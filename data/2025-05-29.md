<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 203]
- [cs.CV](#cs.CV) [Total: 235]
- [cs.AI](#cs.AI) [Total: 53]
- [cs.SD](#cs.SD) [Total: 28]
- [cs.LG](#cs.LG) [Total: 244]
- [cs.MA](#cs.MA) [Total: 11]
- [cs.MM](#cs.MM) [Total: 3]
- [eess.AS](#eess.AS) [Total: 11]
- [eess.IV](#eess.IV) [Total: 31]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models](https://arxiv.org/pdf/2505.21523)
*Chengzhi Liu, Zhongxing Xu, Qingyue Wei, Juncheng Wu, James Zou, Xin Eric Wang, Yuyin Zhou, Sheng Liu*

Main category: cs.CL

TL;DR: The paper explores the trade-off between reasoning ability and hallucination in multimodal large language models, introducing metrics and benchmarks to evaluate visual grounding during extended reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the issue of increased hallucination in multimodal models as reasoning chains lengthen, which reduces focus on visual inputs.

Method: Introduces RH-AUC, a metric for perception accuracy over reasoning length, and RH-Bench, a diagnostic benchmark for evaluating reasoning-perception trade-offs.

Result: Larger models balance reasoning and perception better, influenced more by training data types/domains than volume.

Conclusion: Highlights the need for evaluation frameworks that jointly assess reasoning quality and perceptual fidelity.

Abstract: Test-time compute has empowered multimodal large language models to generate
extended reasoning chains, yielding strong performance on tasks such as
multimodal math reasoning. However, this improved reasoning ability often comes
with increased hallucination: as generations become longer, models tend to
drift away from image-grounded content and rely more heavily on language
priors. Attention analysis shows that longer reasoning chains lead to reduced
focus on visual inputs, which contributes to hallucination. To systematically
study this phenomenon, we introduce RH-AUC, a metric that quantifies how a
model's perception accuracy changes with reasoning length, allowing us to
evaluate whether the model preserves visual grounding during reasoning. We also
release RH-Bench, a diagnostic benchmark that spans a variety of multimodal
tasks, designed to assess the trade-off between reasoning ability and
hallucination. Our analysis reveals that (i) larger models typically achieve a
better balance between reasoning and perception, and (ii) this balance is
influenced more by the types and domains of training data than by its overall
volume. These findings underscore the importance of evaluation frameworks that
jointly consider both reasoning quality and perceptual fidelity.

</details>


### [2] [Loquacious Set: 25,000 Hours of Transcribed and Diverse English Speech Recognition Data for Research and Commercial Use](https://arxiv.org/pdf/2505.21578)
*Titouan Parcollet, Yuan Tseng, Shucong Zhang, Rogier van Dalen*

Main category: cs.CL

TL;DR: The paper introduces the Loquacious Set, a 25,000-hour English speech dataset, addressing limitations of existing ASR benchmarks like LibriSpeech and newer datasets.


<details>
  <summary>Details</summary>
Motivation: Existing ASR datasets are limited by size, licensing issues, unreliable transcriptions, or lack of diversity, hindering real-world ASR development.

Method: The Loquacious Set is a curated collection featuring diverse accents and speech types (read, spontaneous, clean, noisy) from hundreds of thousands of speakers.

Result: The dataset is designed for both academic and industrial researchers to build robust ASR systems for real-world applications.

Conclusion: The Loquacious Set fills gaps in current ASR datasets, offering a commercially usable, diverse, and large-scale resource for ASR research.

Abstract: Automatic speech recognition (ASR) research is driven by the availability of
common datasets between industrial researchers and academics, encouraging
comparisons and evaluations. LibriSpeech, despite its long success as an ASR
benchmark, is now limited by its size and focus on clean, read speech, leading
to near-zero word error rates. More recent datasets, including MOSEL, YODAS,
Gigaspeech, OWSM, Libriheavy or People's Speech suffer from major limitations
including licenses that researchers in the industry cannot use, unreliable
transcriptions, incorrect audio data, or the lack of evaluation sets. This work
presents the Loquacious Set, a 25,000-hour curated collection of commercially
usable English speech. Featuring hundreds of thousands of speakers with diverse
accents and a wide range of speech types (read, spontaneous, talks, clean,
noisy), the Loquacious Set is designed to work for academics and researchers in
the industry to build ASR systems in real-world scenarios.

</details>


### [3] [Rethinking Data Mixture for Large Language Models: A Comprehensive Survey and New Perspectives](https://arxiv.org/pdf/2505.21598)
*Yajiao Liu, Congliang Chen, Junchi Yang, Ruoyu Sun*

Main category: cs.CL

TL;DR: The paper reviews and categorizes methods for determining domain weights in data mixtures to optimize large language model training under computational constraints.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of optimizing domain sampling proportions for training large language models within limited computational resources.

Method: Categorizes existing methods into offline (heuristic-based, algorithm-based, function fitting-based) and online (min-max optimization, mixing law, other approaches) types, summarizing their formulations and algorithms.

Result: Provides a detailed taxonomy of data mixture methods, clarifying relationships and distinctions among them.

Conclusion: Highlights the pros and cons of each method and identifies key challenges in the field of data mixture for model training.

Abstract: Training large language models with data collected from various domains can
improve their performance on downstream tasks. However, given a fixed training
budget, the sampling proportions of these different domains significantly
impact the model's performance. How can we determine the domain weights across
different data domains to train the best-performing model within constrained
computational resources? In this paper, we provide a comprehensive overview of
existing data mixture methods. First, we propose a fine-grained categorization
of existing methods, extending beyond the previous offline and online
classification. Offline methods are further grouped into heuristic-based,
algorithm-based, and function fitting-based methods. For online methods, we
categorize them into three groups: online min-max optimization, online mixing
law, and other approaches by drawing connections with the optimization
frameworks underlying offline methods. Second, we summarize the problem
formulations, representative algorithms for each subtype of offline and online
methods, and clarify the relationships and distinctions among them. Finally, we
discuss the advantages and disadvantages of each method and highlight key
challenges in the field of data mixture.

</details>


### [4] [R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing](https://arxiv.org/pdf/2505.21600)
*Tianyu Fu, Yi Ge, Yichen You, Enshu Liu, Zhihang Yuan, Guohao Dai, Shengen Yan, Huazhong Yang, Yu Wang*

Main category: cs.CL

TL;DR: R2R is a token routing method that uses LLMs only for critical tokens, improving efficiency while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: LLMs have high inference overhead, and SLMs struggle to follow LLMs' reasoning paths. R2R addresses this by focusing on divergent tokens.

Method: Introduces R2R, a neural token routing method, and an automatic data generation pipeline to identify and label divergent tokens.

Result: R2R achieves better accuracy than R1-7B and outperforms R1-14B, with a 2.8x speedup over R1-32B.

Conclusion: R2R advances efficiency in test-time scaling, balancing performance and speed.

Abstract: Large Language Models (LLMs) achieve impressive reasoning capabilities at the
cost of substantial inference overhead, posing substantial deployment
challenges. Although distilled Small Language Models (SLMs) significantly
enhance efficiency, their performance suffers as they fail to follow LLMs'
reasoning paths. Luckily, we reveal that only a small fraction of tokens
genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens
are either identical or exhibit neutral differences, such as minor variations
in abbreviations or expressions. Leveraging this insight, we introduce **Roads
to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs
only for these critical, path-divergent tokens, while leaving the majority of
token generation to the SLM. We also develop an automatic data generation
pipeline that identifies divergent tokens and generates token-level routing
labels to train the lightweight router. We apply R2R to combine R1-1.5B and
R1-32B models from the DeepSeek family, and evaluate on challenging math,
coding, and QA benchmarks. With an average activated parameter size of 5.6B,
R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the
R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with
comparable performance, advancing the Pareto frontier of test-time scaling
efficiency. Our code is available at https://github.com/thu-nics/R2R.

</details>


### [5] [How does Misinformation Affect Large Language Model Behaviors and Preferences?](https://arxiv.org/pdf/2505.21608)
*Miao Peng, Nuo Chen, Jianheng Tang, Jia Li*

Main category: cs.CL

TL;DR: MisBench is a comprehensive benchmark for evaluating LLMs' vulnerability to misinformation, revealing their susceptibility to knowledge conflicts and stylistic variations. A novel approach, RtD, is proposed to improve detection.


<details>
  <summary>Details</summary>
Motivation: To address the lack of fine-grained analysis on how LLMs are influenced by misinformation and to evaluate their behavior and knowledge preferences.

Method: Developed MisBench, a benchmark with 10,346,712 misinformation pieces, considering knowledge conflicts and stylistic variations. Proposed RtD to enhance detection.

Result: LLMs show comparable abilities in discerning misinformation but remain vulnerable to knowledge conflicts and stylistic variations.

Conclusion: MisBench serves as a valuable benchmark for evaluating LLM-based detectors, and RtD offers a promising approach to improve their reliability.

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in
knowledge-intensive tasks, while they remain vulnerable when encountering
misinformation. Existing studies have explored the role of LLMs in combating
misinformation, but there is still a lack of fine-grained analysis on the
specific aspects and extent to which LLMs are influenced by misinformation. To
bridge this gap, we present MisBench, the current largest and most
comprehensive benchmark for evaluating LLMs' behavior and knowledge preference
toward misinformation. MisBench consists of 10,346,712 pieces of
misinformation, which uniquely considers both knowledge-based conflicts and
stylistic variations in misinformation. Empirical results reveal that while
LLMs demonstrate comparable abilities in discerning misinformation, they still
remain susceptible to knowledge conflicts and stylistic variations. Based on
these findings, we further propose a novel approach called Reconstruct to
Discriminate (RtD) to strengthen LLMs' ability to detect misinformation. Our
study provides valuable insights into LLMs' interactions with misinformation,
and we believe MisBench can serve as an effective benchmark for evaluating
LLM-based detectors and enhancing their reliability in real-world applications.
Codes and data are available at https://github.com/GKNL/MisBench.

</details>


### [6] [Iterative Corpus Refinement for Materials Property Prediction Based on Scientific Texts](https://arxiv.org/pdf/2505.21646)
*Lei Zhang, Markus Stricker*

Main category: cs.CL

TL;DR: An iterative framework uses diverse document selection and Word2Vec to predict high-performing materials for ORR, HER, and OER, validated by experiments.


<details>
  <summary>Details</summary>
Motivation: Address the 'combinatorial explosion' in materials discovery by leveraging latent knowledge in scientific texts alongside simulations and experiments.

Method: Iterative framework with strategic document selection, Word2Vec training, and monitoring composition-property correlations in embedding space.

Result: Successfully predicts top-performing materials for ORR, HER, and OER, validated by lab experiments.

Conclusion: Iterative corpus refinement accelerates materials discovery, offering a scalable tool for screening large compositional spaces with scarce data.

Abstract: The discovery and optimization of materials for specific applications is
hampered by the practically infinite number of possible elemental combinations
and associated properties, also known as the `combinatorial explosion'. By
nature of the problem, data are scarce and all possible data sources should be
used. In addition to simulations and experimental results, the latent knowledge
in scientific texts is not yet used to its full potential. We present an
iterative framework that refines a given scientific corpus by strategic
selection of the most diverse documents, training Word2Vec models, and
monitoring the convergence of composition-property correlations in embedding
space. Our approach is applied to predict high-performing materials for oxygen
reduction (ORR), hydrogen evolution (HER), and oxygen evolution (OER) reactions
for a large number of possible candidate compositions. Our method successfully
predicts the highest performing compositions among a large pool of candidates,
validated by experimental measurements of the electrocatalytic performance in
the lab. This work demonstrates and validates the potential of iterative corpus
refinement to accelerate materials discovery and optimization, offering a
scalable and efficient tool for screening large compositional spaces where
reliable data are scarce or non-existent.

</details>


### [7] [Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations](https://arxiv.org/pdf/2505.21657)
*Zeinab Dehghani, Koorosh Aslansefat, Adil Khan, Mohammed Naveed Akram*

Main category: cs.CL

TL;DR: SMILE is a model-agnostic method that explains LLM responses by analyzing input changes and creating visual heat maps to highlight impactful words.


<details>
  <summary>Details</summary>
Motivation: Large language models lack transparency, which is problematic in trust-critical fields. SMILE aims to make their decision-making clearer.

Method: SMILE alters inputs slightly, measures output changes, and visualizes impactful words via heat maps.

Result: Tested on leading LLMs, SMILE demonstrated accuracy, consistency, stability, and fidelity in explanations.

Conclusion: SMILE enhances AI transparency and trustworthiness by providing clear, reliable explanations for LLM behavior.

Abstract: Large language models like GPT, LLAMA, and Claude have become incredibly
powerful at generating text, but they are still black boxes, so it is hard to
understand how they decide what to say. That lack of transparency can be
problematic, especially in fields where trust and accountability matter. To
help with this, we introduce SMILE, a new method that explains how these models
respond to different parts of a prompt. SMILE is model-agnostic and works by
slightly changing the input, measuring how the output changes, and then
highlighting which words had the most impact. Create simple visual heat maps
showing which parts of a prompt matter the most. We tested SMILE on several
leading LLMs and used metrics such as accuracy, consistency, stability, and
fidelity to show that it gives clear and reliable explanations. By making these
models easier to understand, SMILE brings us one step closer to making AI more
transparent and trustworthy.

</details>


### [8] [Rethinking the Outlier Distribution in Large Language Models: An In-depth Study](https://arxiv.org/pdf/2505.21670)
*Rahul Raman, Khushi Sharma, Sai Qian Zhang*

Main category: cs.CL

TL;DR: The paper investigates outliers in LLMs, focusing on their impact on quantization and compression, and proposes strategies to mitigate them with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Outliers in LLMs cause quantization errors, degrading performance. Addressing them can improve quantization efficiency for deployment on edge devices.

Method: Comprehensive investigation into outlier formation mechanisms and proposal of mitigation strategies.

Result: Efficient approaches introduced to eliminate most massive activations and channel-wise outliers.

Conclusion: The study enhances quantization accuracy and efficiency, aiding LLM deployment on specialized hardware.

Abstract: Investigating outliers in large language models (LLMs) is crucial due to
their significant impact on various aspects of LLM performance, including
quantization and compression. Outliers often cause considerable quantization
errors, leading to degraded model performance. Identifying and addressing these
outliers can enhance the accuracy and efficiency of the quantization process,
enabling smoother deployment on edge devices or specialized hardware. Recent
studies have identified two common types of outliers in LLMs: massive
activations and channel-wise outliers. While numerous quantization algorithms
have been proposed to mitigate their effects and maintain satisfactory
accuracy, few have thoroughly explored the root causes of these outliers in
depth. In this paper, we conduct a comprehensive investigation into the
formation mechanisms of these outliers and propose potential strategies to
mitigate their occurrence. Ultimately, we introduce some efficient approaches
to eliminate most massive activations and channel-wise outliers with minimal
impact on accuracy.

</details>


### [9] [LLMPR: A Novel LLM-Driven Transfer Learning based Petition Ranking Model](https://arxiv.org/pdf/2505.21689)
*Avijit Gayen, Somyajit Chakraborty, Mainak Sen, Soham Paul, Angshuman Jana*

Main category: cs.CL

TL;DR: LLMPR, an automated framework using transfer learning and machine learning, prioritizes legal petitions to reduce delays in the Indian judiciary. It achieves high accuracy (99%) with Random Forest and Decision Tree models.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies and biases in manual petition prioritization within the Indian judiciary to improve timely justice delivery.

Method: Uses LLMPR framework with DistilBERT, LegalBERT, and MiniLM embeddings, combined with quantitative features, to train models like Random Forest and Decision Tree.

Result: Random Forest and Decision Tree models achieve 99% accuracy and 0.99 Spearman correlation. Numerical features alone yield near-optimal results.

Conclusion: Automated petition ranking can streamline judicial workflows, reduce backlog, and enhance fairness in prioritization.

Abstract: The persistent accumulation of unresolved legal cases, especially within the
Indian judiciary, significantly hampers the timely delivery of justice. Manual
methods of prioritizing petitions are often prone to inefficiencies and
subjective biases further exacerbating delays. To address this issue, we
propose LLMPR (Large Language Model-based Petition Ranking), an automated
framework that utilizes transfer learning and machine learning to assign
priority rankings to legal petitions based on their contextual urgency.
Leveraging the ILDC dataset comprising 7,593 annotated petitions, we process
unstructured legal text and extract features through various embedding
techniques, including DistilBERT, LegalBERT, and MiniLM. These textual
embeddings are combined with quantitative indicators such as gap days, rank
scores, and word counts to train multiple machine learning models, including
Random Forest, Decision Tree, XGBoost, LightGBM, and CatBoost. Our experiments
demonstrate that Random Forest and Decision Tree models yield superior
performance, with accuracy exceeding 99% and a Spearman rank correlation of
0.99. Notably, models using only numerical features achieve nearly optimal
ranking results (R2 = 0.988, \r{ho} = 0.998), while LLM-based embeddings offer
only marginal gains. These findings suggest that automated petition ranking can
effectively streamline judicial workflows, reduce case backlog, and improve
fairness in legal prioritization.

</details>


### [10] [MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural Awareness Evaluation for LLMs](https://arxiv.org/pdf/2505.21693)
*Raoyuan Zhao, Beiduo Chen, Barbara Plank, Michael A. Hedderich*

Main category: cs.CL

TL;DR: MAKIEval is introduced as a multilingual framework to evaluate cultural awareness in LLMs, addressing biases and disparities across languages, regions, and topics.


<details>
  <summary>Details</summary>
Motivation: The English-centric pretraining of LLMs raises concerns about cross-lingual cultural biases, but existing evaluation methods are limited.

Method: MAKIEval uses Wikidata's multilingual structure to automatically identify cultural entities in model outputs, enabling scalable evaluation without manual work. Four metrics assess cultural awareness.

Result: Evaluation of 7 LLMs across 13 languages shows stronger cultural awareness in English, suggesting English prompts better activate cultural knowledge.

Conclusion: MAKIEval provides a scalable, language-agnostic tool for assessing cultural awareness in LLMs, highlighting disparities and biases.

Abstract: Large language models (LLMs) are used globally across many languages, but
their English-centric pretraining raises concerns about cross-lingual
disparities for cultural awareness, often resulting in biased outputs. However,
comprehensive multilingual evaluation remains challenging due to limited
benchmarks and questionable translation quality. To better assess these
disparities, we introduce MAKIEval, an automatic multilingual framework for
evaluating cultural awareness in LLMs across languages, regions, and topics.
MAKIEval evaluates open-ended text generation, capturing how models express
culturally grounded knowledge in natural language. Leveraging Wikidata's
multilingual structure as a cross-lingual anchor, it automatically identifies
cultural entities in model outputs and links them to structured knowledge,
enabling scalable, language-agnostic evaluation without manual annotation or
translation. We then introduce four metrics that capture complementary
dimensions of cultural awareness: granularity, diversity, cultural specificity,
and consensus across languages. We assess 7 LLMs developed from different parts
of the world, encompassing both open-source and proprietary systems, across 13
languages, 19 countries and regions, and 6 culturally salient topics (e.g.,
food, clothing). Notably, we find that models tend to exhibit stronger cultural
awareness in English, suggesting that English prompts more effectively activate
culturally grounded knowledge. We publicly release our code and data.

</details>


### [11] [Multi-MLLM Knowledge Distillation for Out-of-Context News Detection](https://arxiv.org/pdf/2505.22517)
*Yimeng Gu, Zhao Tong, Ignacio Castro, Shu Wu, Gareth Tyson*

Main category: cs.CL

TL;DR: The paper proposes a two-stage knowledge distillation framework to improve small multimodal large language models (MLLMs) for detecting out-of-context news, reducing annotation costs and achieving state-of-the-art performance with minimal labeled data.


<details>
  <summary>Details</summary>
Motivation: Existing methods for detecting out-of-context news rely on label-rich fine-tuning or expensive API calls, which are impractical in low-resource scenarios. The goal is to enhance small MLLMs efficiently.

Method: A two-stage knowledge distillation framework: Stage 1 uses LoRA fine-tuning on all training data, and Stage 2 combines LoRA fine-tuning and DPO on conflicting teacher predictions.

Result: The approach achieves state-of-the-art performance using less than 10% labeled data.

Conclusion: The proposed method is a label-efficient and cost-effective solution for improving small MLLMs in detecting out-of-context news.

Abstract: Multimodal out-of-context news is a type of misinformation in which the image
is used outside of its original context. Many existing works have leveraged
multimodal large language models (MLLMs) for detecting out-of-context news.
However, observing the limited zero-shot performance of smaller MLLMs, they
generally require label-rich fine-tuning and/or expensive API calls to GPT
models to improve the performance, which is impractical in low-resource
scenarios. In contrast, we aim to improve the performance of small MLLMs in a
more label-efficient and cost-effective manner. To this end, we first prompt
multiple teacher MLLMs to generate both label predictions and corresponding
rationales, which collectively serve as the teachers' knowledge. We then
introduce a two-stage knowledge distillation framework to transfer this
knowledge to a student MLLM. In Stage 1, we apply LoRA fine-tuning to the
student model using all training data. In Stage 2, we further fine-tune the
student model using both LoRA fine-tuning and DPO on the data points where
teachers' predictions conflict. This two-stage strategy reduces annotation
costs and helps the student model uncover subtle patterns in more challenging
cases. Experimental results demonstrate that our approach achieves
state-of-the-art performance using less than 10% labeled data.

</details>


### [12] [Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge Probing](https://arxiv.org/pdf/2505.21701)
*Raoyuan Zhao, Abdullatif Köksal, Ali Modarressi, Michael A. Hedderich, Hinrich Schütze*

Main category: cs.CL

TL;DR: The paper identifies inconsistencies in probing methods for detecting knowledge gaps in LLMs, revealing intra-method and cross-method inconsistencies, and calls for more robust frameworks.


<details>
  <summary>Details</summary>
Motivation: The reliability of LLMs is compromised by hallucinations, necessitating precise identification of knowledge gaps.

Method: The study evaluates probing methods using input variations and quantitative metrics, exposing inconsistencies in knowledge gap detection.

Result: Findings show significant intra-method (e.g., 40% agreement drop) and cross-method (e.g., 7% consistency) inconsistencies in probing methods.

Conclusion: The results challenge existing probing methods and emphasize the need for perturbation-robust frameworks.

Abstract: The reliability of large language models (LLMs) is greatly compromised by
their tendency to hallucinate, underscoring the need for precise identification
of knowledge gaps within LLMs. Various methods for probing such gaps exist,
ranging from calibration-based to prompting-based methods. To evaluate these
probing methods, in this paper, we propose a new process based on using input
variations and quantitative metrics. Through this, we expose two dimensions of
inconsistency in knowledge gap probing. (1) Intra-method inconsistency: Minimal
non-semantic perturbations in prompts lead to considerable variance in detected
knowledge gaps within the same probing method; e.g., the simple variation of
shuffling answer options can decrease agreement to around 40%. (2) Cross-method
inconsistency: Probing methods contradict each other on whether a model knows
the answer. Methods are highly inconsistent -- with decision consistency across
methods being as low as 7% -- even though the model, dataset, and prompt are
all the same. These findings challenge existing probing methods and highlight
the urgent need for perturbation-robust probing frameworks.

</details>


### [13] [Spatial Knowledge Graph-Guided Multimodal Synthesis](https://arxiv.org/pdf/2505.22633)
*Yida Xue, Zhen Bi, Jinnan Yang, Jungang Lou, Huajun Chen, Ningyu Zhang*

Main category: cs.CL

TL;DR: SKG2Data uses spatial knowledge graphs to guide multimodal data synthesis, improving MLLMs' spatial perception and reasoning.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitation of spatial perception in multimodal large language models (MLLMs) through synthesized data adhering to spatial common sense.

Method: Introduces SKG2Data, a multimodal synthesis approach guided by spatial knowledge graphs (SKG) to emulate human-like spatial perception.

Result: Synthesized data enhances MLLMs' spatial abilities and shows strong generalization.

Conclusion: Knowledge-based data synthesis can advance spatial intelligence in MLLMs.

Abstract: Recent advances in multimodal large language models (MLLMs) have
significantly enhanced their capabilities; however, their spatial perception
abilities remain a notable limitation. To address this challenge, multimodal
data synthesis offers a promising solution. Yet, ensuring that synthesized data
adhere to spatial common sense is a non-trivial task. In this work, we
introduce SKG2Data, a novel multimodal synthesis approach guided by spatial
knowledge graphs, grounded in the concept of knowledge-to-data generation.
SKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate
human-like perception of spatial directions and distances, which is
subsequently utilized to guide multimodal data synthesis. Extensive experiments
demonstrate that data synthesized from diverse types of spatial knowledge,
including direction and distance, not only enhance the spatial perception and
reasoning abilities of MLLMs but also exhibit strong generalization
capabilities. We hope that the idea of knowledge-based data synthesis can
advance the development of spatial intelligence.

</details>


### [14] [Assessing and Refining ChatGPT's Performance in Identifying Targeting and Inappropriate Language: A Comparative Study](https://arxiv.org/pdf/2505.21710)
*Barbarestani Baran, Maks Isa, Vossen Piek*

Main category: cs.CL

TL;DR: ChatGPT is effective in detecting inappropriate language in online comments but shows variability in targeting language detection, with room for improvement.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of moderating large volumes of user-generated content on social networks by evaluating ChatGPT's role in content moderation.

Method: ChatGPT's performance was compared against crowd-sourced annotations and expert evaluations to assess accuracy, detection scope, and consistency.

Result: ChatGPT performs well in detecting inappropriate content, with improved accuracy in Version 6, but shows variability and higher false positives in targeting language detection.

Conclusion: The study highlights ChatGPT's potential for automated content moderation but emphasizes the need for continuous refinement and contextual understanding.

Abstract: This study evaluates the effectiveness of ChatGPT, an advanced AI model for
natural language processing, in identifying targeting and inappropriate
language in online comments. With the increasing challenge of moderating vast
volumes of user-generated content on social network sites, the role of AI in
content moderation has gained prominence. We compared ChatGPT's performance
against crowd-sourced annotations and expert evaluations to assess its
accuracy, scope of detection, and consistency. Our findings highlight that
ChatGPT performs well in detecting inappropriate content, showing notable
improvements in accuracy through iterative refinements, particularly in Version
6. However, its performance in targeting language detection showed variability,
with higher false positive rates compared to expert judgments. This study
contributes to the field by demonstrating the potential of AI models like
ChatGPT to enhance automated content moderation systems while also identifying
areas for further improvement. The results underscore the importance of
continuous model refinement and contextual understanding to better support
automated moderation and mitigate harmful online behavior.

</details>


### [15] [Voice Adaptation for Swiss German](https://arxiv.org/pdf/2505.22054)
*Samuel Stucki, Jan Deriu, Mark Cieliebak*

Main category: cs.CL

TL;DR: The paper explores adapting Voice Cloning for Swiss German dialects using the XTTSv2 model, achieving strong performance in evaluations.


<details>
  <summary>Details</summary>
Motivation: To adapt Voice Cloning technology for underrepresented languages like Swiss German dialects.

Method: Preprocess a large Swiss podcast dataset, transcribe and annotate it, then fine-tune the XTTSv2 model.

Result: The model achieves CMOS scores of -0.28 and SMOS scores of 3.8, performing well in human and automated evaluations.

Conclusion: The work successfully advances Voice Cloning for underrepresented dialects, demonstrating practical viability.

Abstract: This work investigates the performance of Voice Adaptation models for Swiss
German dialects, i.e., translating Standard German text to Swiss German dialect
speech. For this, we preprocess a large dataset of Swiss podcasts, which we
automatically transcribe and annotate with dialect classes, yielding
approximately 5000 hours of weakly labeled training material. We fine-tune the
XTTSv2 model on this dataset and show that it achieves good scores in human and
automated evaluations and can correctly render the desired dialect. Our work
shows a step towards adapting Voice Cloning technology to underrepresented
languages. The resulting model achieves CMOS scores of up to -0.28 and SMOS
scores of 3.8.

</details>


### [16] [Counterfactual Simulatability of LLM Explanations for Generation Tasks](https://arxiv.org/pdf/2505.21740)
*Marvin Limpijankit, Yanda Chen, Melanie Subbiah, Nicholas Deas, Kathleen McKeown*

Main category: cs.CL

TL;DR: The paper explores counterfactual simulatability as a method to evaluate LLM explanations, extending it to generation tasks like news summarization and medical suggestion. Results show effectiveness in summarization but limitations in medical tasks, suggesting the method suits skill-based tasks better.


<details>
  <summary>Details</summary>
Motivation: LLMs' unpredictability and the need for reliable explanations in high-stakes settings drive the study.

Method: Extends counterfactual simulatability to generation tasks, testing it on news summarization and medical suggestion.

Result: LLM explanations improve counterfactual predictions in summarization but fall short in medical suggestion.

Conclusion: Counterfactual simulatability is more suitable for skill-based tasks than knowledge-based ones.

Abstract: LLMs can be unpredictable, as even slight alterations to the prompt can cause
the output to change in unexpected ways. Thus, the ability of models to
accurately explain their behavior is critical, especially in high-stakes
settings. One approach for evaluating explanations is counterfactual
simulatability, how well an explanation allows users to infer the model's
output on related counterfactuals. Counterfactual simulatability has been
previously studied for yes/no question answering tasks. We provide a general
framework for extending this method to generation tasks, using news
summarization and medical suggestion as example use cases. We find that while
LLM explanations do enable users to better predict LLM outputs on
counterfactuals in the summarization setting, there is significant room for
improvement for medical suggestion. Furthermore, our results suggest that the
evaluation for counterfactual simulatability may be more appropriate for
skill-based tasks as opposed to knowledge-based tasks.

</details>


### [17] [BehaviorSFT: Behavioral Token Conditioning for Clinical Agents Across the Proactivity Spectrum](https://arxiv.org/pdf/2505.21757)
*Yubin Kim, Zhiyuan Hu, Hyewon Jeong, Eugene Park, Shuyue Stella Li, Chanwoo Park, Shiyun Xiong, MingYu Lu, Hyeonhoon Lee, Xin Liu, Daniel McDuff, Cynthia Breazeal, Samir Tulebaev, Hae Won Park*

Main category: cs.CL

TL;DR: BehaviorBench evaluates LLMs' clinical behaviors, revealing inconsistent proactivity. BehaviorSFT, a novel training strategy, improves performance and realism in clinical tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with proactive engagement in clinical settings, necessitating better behavioral adaptation.

Method: Introduces BehaviorBench for evaluation and BehaviorSFT, a training strategy using behavioral tokens for dynamic behavioral selection.

Result: BehaviorSFT boosts performance (97.3% Macro F1) and improves proactive task scores, confirmed by clinician evaluations.

Conclusion: BehaviorSFT-trained LLMs exhibit superior clinical behavior, balancing proactivity and restraint effectively.

Abstract: Large Language Models (LLMs) as clinical agents require careful behavioral
adaptation. While adept at reactive tasks (e.g., diagnosis reasoning), LLMs
often struggle with proactive engagement, like unprompted identification of
critical missing information or risks. We introduce BehaviorBench, a
comprehensive dataset to evaluate agent behaviors across a clinical assistance
spectrum, ranging from reactive query responses to proactive interventions
(e.g., clarifying ambiguities, flagging overlooked critical data). Our
BehaviorBench experiments reveal LLMs' inconsistent proactivity. To address
this, we propose BehaviorSFT, a novel training strategy using behavioral tokens
to explicitly condition LLMs for dynamic behavioral selection along this
spectrum. BehaviorSFT boosts performance, achieving up to 97.3% overall Macro
F1 on BehaviorBench and improving proactive task scores (e.g., from 95.0% to
96.5% for Qwen2.5-7B-Ins). Crucially, blind clinician evaluations confirmed
BehaviorSFT-trained agents exhibit more realistic clinical behavior, striking a
superior balance between helpful proactivity (e.g., timely, relevant
suggestions) and necessary restraint (e.g., avoiding over-intervention) versus
standard fine-tuning or explicit instructed agents.

</details>


### [18] [Calibrating LLM Confidence by Probing Perturbed Representation Stability](https://arxiv.org/pdf/2505.21772)
*Reza Khanmohammadi, Erfan Miahi, Mehrsa Mardikoraem, Simerjot Kaur, Ivan Brugere, Charese H. Smiley, Kundan Thind, Mohammad M. Ghassemi*

Main category: cs.CL

TL;DR: CCPS is a novel method for calibrating LLM confidence by analyzing internal representational stability, significantly outperforming existing approaches in reducing calibration errors and improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Miscalibration in LLMs reduces their reliability, necessitating better confidence estimation methods.

Method: CCPS applies adversarial perturbations to hidden states, extracts perturbation-response features, and uses a lightweight classifier to predict correctness.

Result: CCPS reduces Expected Calibration Error by ~55%, Brier score by 21%, and improves accuracy by 5 percentage points, among other metrics.

Conclusion: CCPS provides an efficient, broadly applicable solution for better LLM confidence estimation, enhancing trustworthiness.

Abstract: Miscalibration in Large Language Models (LLMs) undermines their reliability,
highlighting the need for accurate confidence estimation. We introduce CCPS
(Calibrating LLM Confidence by Probing Perturbed Representation Stability), a
novel method analyzing internal representational stability in LLMs. CCPS
applies targeted adversarial perturbations to final hidden states, extracts
features reflecting the model's response to these perturbations, and uses a
lightweight classifier to predict answer correctness. CCPS was evaluated on
LLMs from 8B to 32B parameters (covering Llama, Qwen, and Mistral
architectures) using MMLU and MMLU-Pro benchmarks in both multiple-choice and
open-ended formats. Our results show that CCPS significantly outperforms
current approaches. Across four LLMs and three MMLU variants, CCPS reduces
Expected Calibration Error by approximately 55% and Brier score by 21%, while
increasing accuracy by 5 percentage points, Area Under the Precision-Recall
Curve by 4 percentage points, and Area Under the Receiver Operating
Characteristic Curve by 6 percentage points, all relative to the strongest
prior method. CCPS delivers an efficient, broadly applicable, and more accurate
solution for estimating LLM confidence, thereby improving their
trustworthiness.

</details>


### [19] [Co-Saving: Resource Aware Multi-Agent Collaboration for Software Development](https://arxiv.org/pdf/2505.21898)
*Rennai Qiu, Chen Qian, Ran Li, Yufan Dang, Weize Chen, Cheng Yang, Yingli Zhang, Ye Tian, Xuantang Xiong, Lei Han, Zhiyuan Liu, Maosong Sun*

Main category: cs.CL

TL;DR: A resource-aware multi-agent system, Co-Saving, is proposed to reduce inefficiencies in token usage and execution time by leveraging experiential knowledge and shortcuts.


<details>
  <summary>Details</summary>
Motivation: Standalone agents and traditional MAS face inefficiencies in handling complex tasks due to high resource consumption.

Method: Introduces 'shortcuts'—learned from successful historical trajectories—to bypass redundant reasoning and improve efficiency.

Result: Achieves 50.85% reduction in token usage and 10.06% improvement in code quality compared to ChatDev.

Conclusion: Co-Saving enhances operational efficiency and solution quality in multi-agent systems.

Abstract: Recent advancements in Large Language Models (LLMs) and autonomous agents
have demonstrated remarkable capabilities across various domains. However,
standalone agents frequently encounter limitations when handling complex tasks
that demand extensive interactions and substantial computational resources.
Although Multi-Agent Systems (MAS) alleviate some of these limitations through
collaborative mechanisms like task decomposition, iterative communication, and
role specialization, they typically remain resource-unaware, incurring
significant inefficiencies due to high token consumption and excessive
execution time. To address these limitations, we propose a resource-aware
multi-agent system -- Co-Saving (meaning that multiple agents collaboratively
engage in resource-saving activities), which leverages experiential knowledge
to enhance operational efficiency and solution quality. Our key innovation is
the introduction of "shortcuts" -- instructional transitions learned from
historically successful trajectories -- which allows to bypass redundant
reasoning agents and expedite the collective problem-solving process.
Experiments for software development tasks demonstrate significant advantages
over existing methods. Specifically, compared to the state-of-the-art MAS
ChatDev, our method achieves an average reduction of 50.85% in token usage, and
improves the overall code quality by 10.06%.

</details>


### [20] [GMU Systems for the IWSLT 2025 Low-Resource Speech Translation Shared Task](https://arxiv.org/pdf/2505.21781)
*Chutong Meng, Antonios Anastasopoulos*

Main category: cs.CL

TL;DR: The paper details GMU's systems for IWSLT 2025's low-resource speech translation task, focusing on SeamlessM4T-v2 fine-tuning for ASR, MT, and E2E ST, with cascaded ST systems and varied training paradigms.


<details>
  <summary>Details</summary>
Motivation: To improve low-resource speech translation by leveraging SeamlessM4T-v2 and exploring effective fine-tuning methods for ASR, MT, and E2E ST.

Method: Fine-tuned SeamlessM4T-v2 for ASR, MT, and E2E ST; explored direct E2E fine-tuning, multi-task training, and parameter initialization from ASR/MT models.

Result: Direct E2E fine-tuning performed well; ASR encoder initialization boosted ST for untrained languages; multi-task training had minor benefits.

Conclusion: Direct E2E fine-tuning and ASR encoder initialization are effective for low-resource speech translation, with multi-task training offering slight improvements.

Abstract: This paper describes the GMU systems for the IWSLT 2025 low-resource speech
translation shared task. We trained systems for all language pairs, except for
Levantine Arabic. We fine-tuned SeamlessM4T-v2 for automatic speech recognition
(ASR), machine translation (MT), and end-to-end speech translation (E2E ST).
The ASR and MT models are also used to form cascaded ST systems. Additionally,
we explored various training paradigms for E2E ST fine-tuning, including direct
E2E fine-tuning, multi-task training, and parameter initialization using
components from fine-tuned ASR and/or MT models. Our results show that (1)
direct E2E fine-tuning yields strong results; (2) initializing with a
fine-tuned ASR encoder improves ST performance on languages SeamlessM4T-v2 has
not been trained on; (3) multi-task training can be slightly helpful.

</details>


### [21] [VeriTrail: Closed-Domain Hallucination Detection with Traceability](https://arxiv.org/pdf/2505.21786)
*Dasha Metropolitansky, Jonathan Larson*

Main category: cs.CL

TL;DR: VeriTrail is introduced to detect and trace hallucinations in multi-step generative processes (MGS), outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of detecting and tracing unsubstantiated content (hallucinations) in multi-step generative processes.

Method: VeriTrail, a closed-domain hallucination detection method, provides traceability for MGS and SGS processes, using datasets with intermediate outputs and human annotations.

Result: VeriTrail outperforms baseline methods on datasets with intermediate outputs and faithfulness annotations.

Conclusion: VeriTrail is effective for detecting and tracing hallucinations in multi-step generative processes, offering improved traceability.

Abstract: Even when instructed to adhere to source material, Language Models often
generate unsubstantiated content - a phenomenon known as "closed-domain
hallucination." This risk is amplified in processes with multiple generative
steps (MGS), compared to processes with a single generative step (SGS).
However, due to the greater complexity of MGS processes, we argue that
detecting hallucinations in their final outputs is necessary but not
sufficient: it is equally important to trace where hallucinated content was
likely introduced and how faithful content may have been derived from the
source through intermediate outputs. To address this need, we present
VeriTrail, the first closed-domain hallucination detection method designed to
provide traceability for both MGS and SGS processes. We also introduce the
first datasets to include all intermediate outputs as well as human annotations
of final outputs' faithfulness for their respective MGS processes. We
demonstrate that VeriTrail outperforms baseline methods on both datasets.

</details>


### [22] [Revisiting Common Assumptions about Arabic Dialects in NLP](https://arxiv.org/pdf/2505.21816)
*Amr Keleg, Sharon Goldwater, Walid Magdy*

Main category: cs.CL

TL;DR: The paper challenges common assumptions about Arabic dialects in NLP, showing they oversimplify reality and may hinder progress.


<details>
  <summary>Details</summary>
Motivation: To quantitatively verify widely adopted assumptions about Arabic dialects in NLP, which lack empirical validation.

Method: Extending and analyzing a multi-label dataset with manual validation by native speakers of 11 country-level dialects.

Result: The four assumptions examined were found to oversimplify reality and are not always accurate.

Conclusion: These assumptions may hinder progress in Arabic NLP tasks, suggesting a need for more nuanced approaches.

Abstract: Arabic has diverse dialects, where one dialect can be substantially different
from the others. In the NLP literature, some assumptions about these dialects
are widely adopted (e.g., ``Arabic dialects can be grouped into distinguishable
regional dialects") and are manifested in different computational tasks such as
Arabic Dialect Identification (ADI). However, these assumptions are not
quantitatively verified. We identify four of these assumptions and examine them
by extending and analyzing a multi-label dataset, where the validity of each
sentence in 11 different country-level dialects is manually assessed by
speakers of these dialects. Our analysis indicates that the four assumptions
oversimplify reality, and some of them are not always accurate. This in turn
might be hindering further progress in different Arabic NLP tasks.

</details>


### [23] [Representative Language Generation](https://arxiv.org/pdf/2505.21819)
*Charlotte Peale, Vinod Raman, Omer Reingold*

Main category: cs.CL

TL;DR: The paper extends generative model theory to address diversity and bias by ensuring outputs proportionally represent training data groups. It introduces 'group closure dimension' and analyzes feasibility for infinite cases, contrasting with prior work.


<details>
  <summary>Details</summary>
Motivation: To address diversity and bias in generative models by ensuring outputs proportionally represent groups from training data.

Method: Extends Kleinberg et al.'s framework, introduces 'group closure dimension,' and analyzes representative generation theoretically and computationally.

Result: Feasibility shown for infinite cases under conditions, but negative computability result using membership queries alone. Contrasts with prior positive results.

Conclusion: Provides a foundation for diverse and representative generative models, highlighting theoretical and computational challenges.

Abstract: We introduce "representative generation," extending the theoretical framework
for generation proposed by Kleinberg et al. (2024) and formalized by Li et al.
(2024), to additionally address diversity and bias concerns in generative
models. Our notion requires outputs of a generative model to proportionally
represent groups of interest from the training data. We characterize
representative uniform and non-uniform generation, introducing the "group
closure dimension" as a key combinatorial quantity. For representative
generation in the limit, we analyze both information-theoretic and
computational aspects, demonstrating feasibility for countably infinite
hypothesis classes and collections of groups under certain conditions, but
proving a negative result for computability using only membership queries. This
contrasts with Kleinberg et al.'s (2024) positive results for standard
generation in the limit. Our findings provide a rigorous foundation for
developing more diverse and representative generative models.

</details>


### [24] [Principled Content Selection to Generate Diverse and Personalized Multi-Document Summaries](https://arxiv.org/pdf/2505.21859)
*Vishakh Padmakumar, Zichao Wang, David Arbour, Jennifer Healey*

Main category: cs.CL

TL;DR: The paper addresses the 'lost in the middle' issue in LLMs by proposing a three-step method for multi-document summarization, improving source coverage and enabling personalized summaries.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with uneven attention in long contexts, hindering diverse source coverage in summarization tasks like DiverseSumm.

Method: A three-step approach: (1) extract atomic key points, (2) use DPP for diverse content selection, (3) rewrite to a final summary. Incorporates user intent for personalization.

Result: Improved source coverage on DiverseSumm across LLMs and enabled personalized summaries.

Conclusion: Principled content selection and task division enhance LLM performance in multi-document summarization, with potential for personalization.

Abstract: While large language models (LLMs) are increasingly capable of handling
longer contexts, recent work has demonstrated that they exhibit the "lost in
the middle" phenomenon (Liu et al., 2024) of unevenly attending to different
parts of the provided context. This hinders their ability to cover diverse
source material in multi-document summarization, as noted in the DiverseSumm
benchmark (Huang et al., 2024). In this work, we contend that principled
content selection is a simple way to increase source coverage on this task. As
opposed to prompting an LLM to perform the summarization in a single step, we
explicitly divide the task into three steps -- (1) reducing document
collections to atomic key points, (2) using determinantal point processes (DPP)
to perform select key points that prioritize diverse content, and (3) rewriting
to the final summary. By combining prompting steps, for extraction and
rewriting, with principled techniques, for content selection, we consistently
improve source coverage on the DiverseSumm benchmark across various LLMs.
Finally, we also show that by incorporating relevance to a provided user intent
into the DPP kernel, we can generate personalized summaries that cover relevant
source information while retaining coverage.

</details>


### [25] [Evaluating the Retrieval Robustness of Large Language Models](https://arxiv.org/pdf/2505.21870)
*Shuyang Cao, Karthik Radhakrishnan, David Rosenberg, Steven Lu, Pengxiang Cheng, Lu Wang, Shiyue Zhang*

Main category: cs.CL

TL;DR: The paper evaluates the robustness of LLMs in retrieval-augmented generation (RAG) setups, focusing on three key questions about RAG's effectiveness, document quantity, and order impact. It introduces a benchmark and metrics, revealing high but imperfect robustness in LLMs.


<details>
  <summary>Details</summary>
Motivation: To assess whether RAG consistently improves LLM performance and how retrieval quality and document handling affect results.

Method: Established a benchmark with 1500 open-domain questions and retrieved Wikipedia documents. Evaluated 11 LLMs and 3 prompting strategies using three robustness metrics.

Result: LLMs showed high retrieval robustness, but imperfections still limit RAG's full benefits.

Conclusion: While RAG enhances LLMs, retrieval robustness issues persist, affecting performance.

Abstract: Retrieval-augmented generation (RAG) generally enhances large language
models' (LLMs) ability to solve knowledge-intensive tasks. But RAG may also
lead to performance degradation due to imperfect retrieval and the model's
limited ability to leverage retrieved content. In this work, we evaluate the
robustness of LLMs in practical RAG setups (henceforth retrieval robustness).
We focus on three research questions: (1) whether RAG is always better than
non-RAG; (2) whether more retrieved documents always lead to better
performance; (3) and whether document orders impact results. To facilitate this
study, we establish a benchmark of 1500 open-domain questions, each with
retrieved documents from Wikipedia. We introduce three robustness metrics, each
corresponds to one research question. Our comprehensive experiments, involving
11 LLMs and 3 prompting strategies, reveal that all of these LLMs exhibit
surprisingly high retrieval robustness; nonetheless, different degrees of
imperfect robustness hinders them from fully utilizing the benefits of RAG.

</details>


### [26] [EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse](https://arxiv.org/pdf/2505.21889)
*Tianyu Guo, Hande Dong, Yichong Leng, Feng Liu, Cheater Lin, Nong Xiao, Xianwei Zhang*

Main category: cs.CL

TL;DR: EFIM improves KV cache reuse in LLMs for infilling tasks by transforming the prompt format and introducing fragment tokenization, reducing latency by 52% and boosting throughput by 98%.


<details>
  <summary>Details</summary>
Motivation: Current KV cache reuse in LLMs is inefficient for infilling tasks due to prompt structure issues, leading to repeated computations.

Method: Proposes EFIM, a transformed prompt format, and fragment tokenization training to enhance KV cache reuse and address subtoken generation problems.

Result: EFIM reduces latency by 52% and increases throughput by 98% while maintaining infilling accuracy.

Conclusion: EFIM effectively optimizes LLM performance for infilling tasks by improving KV cache reuse and tokenization.

Abstract: Large language models (LLMs) are often used for infilling tasks, which
involve predicting or generating missing information in a given text. These
tasks typically require multiple interactions with similar context. To reduce
the computation of repeated historical tokens, cross-request key-value (KV)
cache reuse, a technique that stores and reuses intermediate computations, has
become a crucial method in multi-round interactive services. However, in
infilling tasks, the KV cache reuse is often hindered by the structure of the
prompt format, which typically consists of a prefix and suffix relative to the
insertion point. Specifically, the KV cache of the prefix or suffix part is
frequently invalidated as the other part (suffix or prefix) is incrementally
generated. To address the issue, we propose EFIM, a transformed prompt format
of FIM to unleash the performance potential of KV cache reuse. Although the
transformed prompt can solve the inefficiency, it exposes subtoken generation
problems in current LLMs, where they have difficulty generating partial words
accurately. Therefore, we introduce a fragment tokenization training method
which splits text into multiple fragments before tokenization during data
processing. Experiments on two representative LLMs show that LLM serving with
EFIM can lower the latency by 52% and improve the throughput by 98% while
maintaining the original infilling capability.EFIM's source code is publicly
available at https://github.com/gty111/EFIM.

</details>


### [27] [Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning](https://arxiv.org/pdf/2505.21926)
*Yin Hua, Zhiqiang Liu, Mingyang Chen, Zheng Fang, Chi Man Wong, Lingxiao Li, Chi Man Vong, Huajun Chen, Wen Zhang*

Main category: cs.CL

TL;DR: MERRY is a foundation model for general knowledge graph reasoning, excelling in both in-KG and out-of-KG tasks by integrating structural and textual information through innovative encoding and fusion techniques.


<details>
  <summary>Details</summary>
Motivation: Existing foundation models for knowledge graphs (KGs) focus mainly on structural aspects and in-KG tasks, limiting progress on more challenging out-of-KG tasks. MERRY aims to bridge this gap by leveraging both structural and textual information.

Method: MERRY uses a multi-perspective Conditional Message Passing (CMP) encoding architecture to integrate textual and structural modalities, a dynamic residual fusion module to retain relevant textual information, and a flexible edge scoring mechanism for diverse tasks.

Result: Evaluations on 28 datasets show MERRY outperforms baselines in most scenarios, demonstrating strong in-KG reasoning and excellent generalization to out-of-KG tasks like KGQA.

Conclusion: MERRY successfully addresses the limitations of existing KG foundation models, showcasing superior performance and versatility across diverse tasks.

Abstract: In natural language processing (NLP) and computer vision (CV), the successful
application of foundation models across diverse tasks has demonstrated their
remarkable potential. However, despite the rich structural and textual
information embedded in knowledge graphs (KGs), existing research of foundation
model for KG has primarily focused on their structural aspects, with most
efforts restricted to in-KG tasks (e.g., knowledge graph completion, KGC). This
limitation has hindered progress in addressing more challenging out-of-KG
tasks. In this paper, we introduce MERRY, a foundation model for general
knowledge graph reasoning, and investigate its performance across two task
categories: in-KG reasoning tasks (e.g., KGC) and out-of-KG tasks (e.g., KG
question answering, KGQA). We not only utilize the structural information, but
also the textual information in KGs. Specifically, we propose a
multi-perspective Conditional Message Passing (CMP) encoding architecture to
bridge the gap between textual and structural modalities, enabling their
seamless integration. Additionally, we introduce a dynamic residual fusion
module to selectively retain relevant textual information and a flexible edge
scoring mechanism to adapt to diverse downstream tasks. Comprehensive
evaluations on 28 datasets demonstrate that MERRY outperforms existing
baselines in most scenarios, showcasing strong reasoning capabilities within
KGs and excellent generalization to out-of-KG tasks such as KGQA.

</details>


### [28] [GOAT-TTS: Expressive and Realistic Speech Generation via A Dual-Branch LLM](https://arxiv.org/pdf/2504.12339)
*Yaodong Song, Hongjie Chen, Jie Lian, Yuxin Zhang, Guangmin Xia, Zehan Li, Genliang Zhao, Jian Kang, Jie Li, Yongxiang Li, Xuelong Li*

Main category: cs.CL

TL;DR: GOAT-TTS addresses limitations in LLM-based TTS by introducing a dual-branch architecture for modality alignment and speech generation, preserving linguistic knowledge and enabling real-time synthesis.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based TTS systems suffer from irreversible acoustic loss, dependency on aligned prompt pairs, and forgetting text comprehension during optimization.

Method: GOAT-TTS uses a dual-branch approach: a modality-alignment branch for continuous acoustic embeddings and a speech-generation branch with modular fine-tuning for token prediction.

Result: GOAT-TTS matches state-of-the-art TTS performance and effectively synthesizes dialect speech.

Conclusion: The proposed architecture resolves key challenges in LLM-based TTS, offering a balanced and efficient solution.

Abstract: While large language models (LLMs) have revolutionized text-to-speech (TTS)
synthesis through discrete tokenization paradigms, current architectures
exhibit fundamental tensions between three critical dimensions: 1) irreversible
loss of acoustic characteristics caused by quantization of speech prompts; 2)
stringent dependence on precisely aligned prompt speech-text pairs that limit
real-world deployment; and 3) catastrophic forgetting of the LLM's native text
comprehension during optimization for speech token generation. To address these
challenges, we propose an LLM-based text-to-speech Generation approach
Optimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework
introduces two key innovations: (1) The modality-alignment branch combines a
speech encoder and projector to capture continuous acoustic embeddings,
enabling bidirectional correlation between paralinguistic features (language,
timbre, emotion) and semantic text representations without transcript
dependency; (2) The speech-generation branch employs modular fine-tuning on
top-k layers of an LLM for speech token prediction while freezing the bottom-n
layers to preserve foundational linguistic knowledge. Moreover, multi-token
prediction is introduced to support real-time streaming TTS synthesis.
Experimental results demonstrate that our GOAT-TTS achieves performance
comparable to state-of-the-art TTS models while validating the efficacy of
synthesized dialect speech data.

</details>


### [29] [RedTeamCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments](https://arxiv.org/pdf/2505.21936)
*Zeyi Liao, Jaylen Jones, Linxi Jiang, Eric Fosler-Lussier, Yu Su, Zhiqiang Lin, Huan Sun*

Main category: cs.CL

TL;DR: RedTeamCUA is a framework for testing vulnerabilities in Computer-use Agents (CUAs) against hybrid web-OS attacks, revealing significant risks from indirect prompt injection.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of CUAs lack realistic hybrid attack scenarios, prompting the need for a controlled testing framework.

Method: Proposes RedTeamCUA, a hybrid sandbox combining VM-based OS and Docker-based web platforms, and RTC-Bench, a benchmark with 864 examples.

Result: High attack success rates (ASR) in CUAs, e.g., 42.9% for Claude 3.7 Sonnet and 48% for Claude 4 Opus, show tangible risks.

Conclusion: RedTeamCUA highlights urgent need for robust defenses against indirect prompt injection before real-world CUA deployment.

Abstract: Computer-use agents (CUAs) promise to automate complex tasks across operating
systems (OS) and the web, but remain vulnerable to indirect prompt injection.
Current evaluations of this threat either lack support realistic but controlled
environments or ignore hybrid web-OS attack scenarios involving both
interfaces. To address this, we propose RedTeamCUA, an adversarial testing
framework featuring a novel hybrid sandbox that integrates a VM-based OS
environment with Docker-based web platforms. Our sandbox supports key features
tailored for red teaming, such as flexible adversarial scenario configuration,
and a setting that decouples adversarial evaluation from navigational
limitations of CUAs by initializing tests directly at the point of an
adversarial injection. Using RedTeamCUA, we develop RTC-Bench, a comprehensive
benchmark with 864 examples that investigate realistic, hybrid web-OS attack
scenarios and fundamental security vulnerabilities. Benchmarking current
frontier CUAs identifies significant vulnerabilities: Claude 3.7 Sonnet | CUA
demonstrates an ASR of 42.9%, while Operator, the most secure CUA evaluated,
still exhibits an ASR of 7.6%. Notably, CUAs often attempt to execute
adversarial tasks with an Attempt Rate as high as 92.5%, although failing to
complete them due to capability limitations. Nevertheless, we observe
concerning ASRs of up to 50% in realistic end-to-end settings, with the
recently released frontier Claude 4 Opus | CUA showing an alarming ASR of 48%,
demonstrating that indirect prompt injection presents tangible risks for even
advanced CUAs despite their capabilities and safeguards. Overall, RedTeamCUA
provides an essential framework for advancing realistic, controlled, and
systematic analysis of CUA vulnerabilities, highlighting the urgent need for
robust defenses to indirect prompt injection prior to real-world deployment.

</details>


### [30] [Graph-Assisted Culturally Adaptable Idiomatic Translation for Indic Languages](https://arxiv.org/pdf/2505.21937)
*Pratik Rakesh Singh, Kritarth Prasad, Mohammadi Zaki, Pankaj Wasnik*

Main category: cs.CL

TL;DR: IdiomCE, an adaptive GNN-based method, improves multi-word expression and idiom translation by learning cultural nuances, outperforming traditional approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional methods like static KGs and prompt-based approaches fail to capture cultural nuances in idiomatic translations, leading to suboptimal results.

Method: Proposes IdiomCE, an adaptive GNN-based methodology that learns mappings between idiomatic expressions, generalizing to seen and unseen nodes.

Result: Significant improvements in translating idioms from English to Indian languages, even in resource-constrained settings.

Conclusion: IdiomCE enhances translation quality for idioms, demonstrating effectiveness across diverse languages and contexts.

Abstract: Translating multi-word expressions (MWEs) and idioms requires a deep
understanding of the cultural nuances of both the source and target languages.
This challenge is further amplified by the one-to-many nature of idiomatic
translations, where a single source idiom can have multiple target-language
equivalents depending on cultural references and contextual variations.
Traditional static knowledge graphs (KGs) and prompt-based approaches struggle
to capture these complex relationships, often leading to suboptimal
translations. To address this, we propose IdiomCE, an adaptive graph neural
network (GNN) based methodology that learns intricate mappings between
idiomatic expressions, effectively generalizing to both seen and unseen nodes
during training. Our proposed method enhances translation quality even in
resource-constrained settings, facilitating improved idiomatic translation in
smaller models. We evaluate our approach on multiple idiomatic translation
datasets using reference-less metrics, demonstrating significant improvements
in translating idioms from English to various Indian languages.

</details>


### [31] [RISE: Reasoning Enhancement via Iterative Self-Exploration in Multi-hop Question Answering](https://arxiv.org/pdf/2505.21940)
*Bolei He, Xinran He, Mengke Chen, Xianwei Xue, Ying Zhu, Zhenhua Ling*

Main category: cs.CL

TL;DR: RISE enhances reasoning in LLMs for MHQA via iterative self-exploration, improving accuracy and performance.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex reasoning like MHQA due to noisy data and incomplete evidence retrieval in RAG.

Method: RISE uses question decomposition, retrieve-then-read, and self-critique for iterative self-improvement.

Result: RISE significantly boosts reasoning accuracy and MHQA performance in benchmarks.

Conclusion: RISE effectively addresses MHQA challenges by enhancing reasoning through iterative self-exploration.

Abstract: Large Language Models (LLMs) excel in many areas but continue to face
challenges with complex reasoning tasks, such as Multi-Hop Question Answering
(MHQA). MHQA requires integrating evidence from diverse sources while managing
intricate logical dependencies, often leads to errors in reasoning.
Retrieval-Augmented Generation (RAG), widely employed in MHQA tasks, faces
challenges in effectively filtering noisy data and retrieving all necessary
evidence, thereby limiting its effectiveness in addressing MHQA challenges. To
address these challenges, we propose RISE:Reasoning Enhancement via Iterative
Self-Exploration, a novel framework designed to enhance models' reasoning
capability through iterative self-exploration. Specifically, RISE involves
three key steps in addressing MHQA tasks: question decomposition,
retrieve-then-read, and self-critique. By leveraging continuous
self-exploration, RISE identifies accurate reasoning paths, iteratively
self-improving the model's capability to integrate evidence, maintain logical
consistency, and enhance performance in MHQA tasks. Extensive experiments on
multiple MHQA benchmarks demonstrate that RISE significantly improves reasoning
accuracy and task performance.

</details>


### [32] [Test-Time Scaling with Repeated Sampling Improves Multilingual Text Generation](https://arxiv.org/pdf/2505.21941)
*Ashim Gupta, Vivek Srikumar*

Main category: cs.CL

TL;DR: Repeated sampling improves multilingual generation, with gains over 35% in some cases. Perplexity-based scoring works for open-ended prompts, while reward-based verifiers excel in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To explore the effectiveness of inference-time scaling via repeated sampling in multilingual generation, which remains underexplored.

Method: Evaluated using perplexity- and reward-based verifiers on multilingual benchmarks (Aya Evaluation Suite and m-ArenaHard).

Result: Consistent quality improvements, with gains exceeding 35% in some cases. Reward-based verifiers outperform in reasoning tasks.

Conclusion: Repeated sampling is broadly useful for multilingual text generation, and the choice of verifier (perplexity vs. reward) is task-dependent.

Abstract: Inference-time scaling via repeated sampling has shown promise in reasoning
tasks, but its effectiveness in multilingual generation remains underexplored.
We evaluate this approach using perplexity- and reward-based verifiers on two
multilingual benchmarks: the Aya Evaluation Suite and m-ArenaHard. Our results
show consistent quality improvements, with gains exceeding 35% in some cases.
While perplexity-based scoring is effective for open-ended prompts, only
reward-based verifiers improve performance on tasks requiring reasoning (e.g.,
math, code). Our results demonstrate the broader utility of repeated sampling
for multilingual text generation and underscore the importance of selecting
right verifiers for the task.

</details>


### [33] [SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement](https://arxiv.org/pdf/2504.03561)
*Runnan Fang, Xiaobin Wang, Yuan Liang, Shuofei Qiao, Jialong Wu, Zekun Xi, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen*

Main category: cs.CL

TL;DR: SynWorld is a framework for LLM-based agents to explore and refine action knowledge in novel environments using scenario synthesis and MCTS.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents struggle in new or unconventional environments, needing better methods to learn and adapt.

Method: Proposes SynWorld, combining multi-step action synthesis and MCTS for exploration and action knowledge refinement.

Result: SynWorld effectively learns action knowledge in new environments, as shown in experiments.

Conclusion: SynWorld is a general and effective approach for enhancing agent capabilities in unfamiliar settings.

Abstract: In the interaction between agents and their environments, agents expand their
capabilities by planning and executing actions. However, LLM-based agents face
substantial challenges when deployed in novel environments or required to
navigate unconventional action spaces. To empower agents to autonomously
explore environments, optimize workflows, and enhance their understanding of
actions, we propose SynWorld, a framework that allows agents to synthesize
possible scenarios with multi-step action invocation within the action space
and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine
their action knowledge in the current environment. Our experiments demonstrate
that SynWorld is an effective and general approach to learning action knowledge
in new environments. Code is available at https://github.com/zjunlp/SynWorld.

</details>


### [34] [Resolving Knowledge Conflicts in Domain-specific Data Selection: A Case Study on Medical Instruction-tuning](https://arxiv.org/pdf/2505.21958)
*Qihuang Zhong, Liang Ding, Fei Liao, Juhua Liu, Bo Du, Dacheng Tao*

Main category: cs.CL

TL;DR: The paper introduces Knowledge-aware Data Selection (KDS), a framework to improve domain-specific instruction-tuning for LLMs by addressing knowledge conflicts in data selection.


<details>
  <summary>Details</summary>
Motivation: Current data selection methods for domain-specific instruction-tuning often fail due to neglecting knowledge conflicts, which can harm LLMs' prior abilities and cause hallucinations.

Method: KDS uses two knowledge-aware metrics to measure knowledge conflicts (context-memory alignment and intra-memory consistency) and filters/samples data accordingly.

Result: Experiments in the medical domain show KDS outperforms baselines, enhancing performance, generalization, and reducing hallucinations.

Conclusion: KDS is a simple yet effective solution for selecting high-quality domain-specific instruction data, improving LLM performance and reliability.

Abstract: Domain-specific instruction-tuning has become the defacto standard for
improving the performance of large language models (LLMs) in specialized
applications, e.g., medical question answering. Since the instruction-tuning
dataset might contain redundant or low-quality data, data selection (DS) is
usually required to maximize the data efficiency. Despite the successes in the
general domain, current DS methods often struggle to select the desired data
for domain-specific instruction-tuning. One of the main reasons is that they
neglect the impact of knowledge conflicts, i.e., the discrepancy between LLMs'
pretrained knowledge and context knowledge of instruction data, which could
damage LLMs' prior abilities and lead to hallucination. To this end, we propose
a simple-yet-effective Knowledge-aware Data Selection (namely KDS) framework to
select the domain-specific instruction-tuning data that meets LLMs' actual
needs. The core of KDS is to leverage two knowledge-aware metrics for
quantitatively measuring knowledge conflicts from two aspects: context-memory
knowledge alignment and intra-memory knowledge consistency. By filtering the
data with large knowledge conflicts and sampling the high-quality and diverse
data, KDS can effectively stimulate the LLMs' abilities and achieve better
domain-specific performance. Taking the medical domain as the testbed, we
conduct extensive experiments and empirically prove that KDS surpasses the
other baselines and brings significant and consistent performance gains among
all LLMs. More encouragingly, KDS effectively improves the model generalization
and alleviates the hallucination problem.

</details>


### [35] [LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents](https://arxiv.org/pdf/2505.21963)
*Taro Yano, Yoichi Ishibashi, Masafumi Oyamada*

Main category: cs.CL

TL;DR: LaMDAgent is a framework that autonomously constructs and optimizes post-training pipelines for LLMs, improving tool-use accuracy by 9.0 points while reducing human intervention.


<details>
  <summary>Details</summary>
Motivation: Existing post-training pipeline construction for LLMs is manual or narrowly optimized, lacking automation. LaMDAgent addresses this gap.

Method: LaMDAgent uses LLM-based agents to explore model generation techniques, datasets, and hyperparameters, leveraging task-based feedback.

Result: LaMDAgent improves tool-use accuracy by 9.0 points and discovers overlooked strategies. Data scaling reduces computational costs.

Conclusion: LaMDAgent automates post-training pipeline optimization, outperforming manual methods and uncovering efficient strategies.

Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across
a wide range of tasks. To further tailor LLMs to specific domains or
applications, post-training techniques such as Supervised Fine-Tuning (SFT),
Preference Learning, and model merging are commonly employed. While each of
these methods has been extensively studied in isolation, the automated
construction of complete post-training pipelines remains an underexplored area.
Existing approaches typically rely on manual design or focus narrowly on
optimizing individual components, such as data ordering or merging strategies.
In this work, we introduce LaMDAgent (short for Language Model Developing
Agent), a novel framework that autonomously constructs and optimizes full
post-training pipelines through the use of LLM-based agents. LaMDAgent
systematically explores diverse model generation techniques, datasets, and
hyperparameter configurations, leveraging task-based feedback to discover
high-performing pipelines with minimal human intervention. Our experiments show
that LaMDAgent improves tool-use accuracy by 9.0 points while preserving
instruction-following capabilities. Moreover, it uncovers effective
post-training strategies that are often overlooked by conventional human-driven
exploration. We further analyze the impact of data and model size scaling to
reduce computational costs on the exploration, finding that model size scalings
introduces new challenges, whereas scaling data size enables cost-effective
pipeline discovery.

</details>


### [36] [Offset Unlearning for Large Language Models](https://arxiv.org/pdf/2404.11045)
*James Y. Huang, Wenxuan Zhou, Fei Wang, Fred Morstatter, Sheng Zhang, Hoifung Poon, Muhao Chen*

Main category: cs.CL

TL;DR: The paper introduces δ-Unlearning, a framework for unlearning sensitive data in black-box LLMs without accessing internal weights or retaining sensitive data.


<details>
  <summary>Details</summary>
Motivation: Address ethical and legal concerns from LLMs memorizing sensitive data by enabling unlearning for black-box models.

Method: Proposes δ-Unlearning, which learns logit offsets by contrasting smaller models' logits, avoiding direct model tuning.

Result: Effectively unlearns target data while maintaining or improving performance on general tasks.

Conclusion: δ-Unlearning is a versatile solution adaptable to various unlearning algorithms for black-box LLMs.

Abstract: Despite the strong capabilities of Large Language Models (LLMs) to acquire
knowledge from their training corpora, the memorization of sensitive
information in the corpora such as copyrighted, biased, and private content has
led to ethical and legal concerns. In response to these challenges, unlearning
has emerged as a potential remedy for LLMs affected by problematic training
data. However, previous unlearning techniques are either not applicable to
black-box LLMs due to required access to model internal weights, or violate
data protection principles by retaining sensitive data for inference-time
correction. We propose {\delta}-Unlearning, an offset unlearning framework for
black-box LLMs. Instead of tuning the black-box LLM itself, {\delta}-Unlearning
learns the logit offset needed for unlearning by contrasting the logits from a
pair of smaller models. Experiments demonstrate that {\delta}- Unlearning can
effectively unlearn target data while maintaining similar or even stronger
performance on general out-of-forget-scope tasks. {\delta}-Unlearning also
effectively incorporates different unlearning algorithms, making our approach a
versatile solution to adapting various existing unlearning algorithms to
black-box LLMs.

</details>


### [37] [Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack](https://arxiv.org/pdf/2505.21967)
*Juan Ren, Mark Dras, Usman Naseem*

Main category: cs.CL

TL;DR: The paper analyzes security vulnerabilities in Large Vision-Language Models (LVLMs) caused by adversarial attacks and proposes a two-stage evaluation framework to assess and improve their safety mechanisms.


<details>
  <summary>Details</summary>
Motivation: LVLMs are vulnerable to adversarial attacks despite safety mechanisms, necessitating a deeper understanding and systematic evaluation of these vulnerabilities.

Method: A two-stage framework: (1) categorizing adversarial attack outcomes (non-compliance, refusal, exploitation), and (2) quantifying harmful intent fulfillment and refusal types. A normative schema for ideal safety alignment is also introduced.

Result: The framework identifies gaps in LVLM safety and provides metrics to evaluate adversarial attack impacts, aiding in the development of more robust safety mechanisms.

Conclusion: The study highlights LVLM vulnerabilities, offers a structured evaluation approach, and proposes a normative schema to guide safety alignment in multimodal systems.

Abstract: Large Vision-Language Models (LVLMs) have shown remarkable capabilities
across a wide range of multimodal tasks. However, their integration of visual
inputs introduces expanded attack surfaces, thereby exposing them to novel
security vulnerabilities. In this work, we conduct a systematic
representational analysis to uncover why conventional adversarial attacks can
circumvent the safety mechanisms embedded in LVLMs. We further propose a novel
two stage evaluation framework for adversarial attacks on LVLMs. The first
stage differentiates among instruction non compliance, outright refusal, and
successful adversarial exploitation. The second stage quantifies the degree to
which the model's output fulfills the harmful intent of the adversarial prompt,
while categorizing refusal behavior into direct refusals, soft refusals, and
partial refusals that remain inadvertently helpful. Finally, we introduce a
normative schema that defines idealized model behavior when confronted with
harmful prompts, offering a principled target for safety alignment in
multimodal systems.

</details>


### [38] [Pearl: A Multimodal Culturally-Aware Arabic Instruction Dataset](https://arxiv.org/pdf/2505.21979)
*Fakhraddin Alwajih, Samar Mohamed Magdy, Abdellah El Mekki, Omer Nacar, Youssef Nafea, Safaa Taher Abdelfadil, Abdulfattah Mohammed Yahya, Hamzah Luqman, Nada Almarwani, Samah Aloufi, Baraah Qawasmeh, Houdaifa Atou, Serry Sibaee, Hamzah A. Alsayadi, Walid Al-Dhabyani, Maged S. Al-shaibani, Aya El aatar, Nour Qandos, Rahaf Alhamouri, Samar Ahmad, Razan Khassib, Lina Hamad, Mohammed Anwar AL-Ghrawi, Fatimah Alshamari, Cheikh Malainine, Doaa Qawasmeh, Aminetou Yacoub, Tfeil moilid, Ruwa AbuHweidi, Ahmed Aboeitta, Vatimetou Mohamed Lemin, Reem Abdel-Salam, Ahlam Bashiti, Adel Ammar, Aisha Alansari, Ahmed Ashraf, Nora Alturayeif, Sara Shatnawi, Alcides Alcoba Inciarte, AbdelRahim A. Elmadany, Mohamedou cheikh tourad, Ismail Berrada, Mustafa Jarrar, Shady Shehata, Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: Pearl is a large-scale Arabic multimodal dataset addressing cultural biases in LVLMs, featuring diverse annotations and specialized benchmarks for nuanced cultural evaluation.


<details>
  <summary>Details</summary>
Motivation: To mitigate cultural biases in large vision-language models by providing a diverse, culturally rich dataset.

Method: Constructed Pearl using advanced agentic workflows and human-in-the-loop annotations from 45 annotators across the Arab world, covering ten culturally significant domains.

Result: Pearl improves cultural grounding in LVLMs through reasoning-centric instruction alignment, outperforming conventional scaling methods.

Conclusion: Pearl serves as a foundational resource for culturally-informed multimodal research, with all datasets and benchmarks publicly available.

Abstract: Mainstream large vision-language models (LVLMs) inherently encode cultural
biases, highlighting the need for diverse multimodal datasets. To address this
gap, we introduce Pearl, a large-scale Arabic multimodal dataset and benchmark
explicitly designed for cultural understanding. Constructed through advanced
agentic workflows and extensive human-in-the-loop annotations by 45 annotators
from across the Arab world, Pearl comprises over K multimodal examples spanning
ten culturally significant domains covering all Arab countries. We further
provide two robust evaluation benchmarks Pearl and Pearl-Lite along with a
specialized subset Pearl-X explicitly developed to assess nuanced cultural
variations. Comprehensive evaluations on state-of-the-art open and proprietary
LVLMs demonstrate that reasoning-centric instruction alignment substantially
improves models' cultural grounding compared to conventional scaling methods.
Pearl establishes a foundational resource for advancing culturally-informed
multimodal modeling research. All datasets and benchmarks are publicly
available.

</details>


### [39] [Leveraging Interview-Informed LLMs to Model Survey Responses: Comparative Insights from AI-Generated and Human Data](https://arxiv.org/pdf/2505.21997)
*Jihong Zhang, Xinya Liang, Anqi Deng, Nicole Bonge, Lin Tan, Ling Zhang, Nicole Zarrett*

Main category: cs.CL

TL;DR: LLMs can predict human survey responses using qualitative data but show less variability. Interview data improves diversity, while prompts and settings enhance alignment. Demographics matter less than interview content.


<details>
  <summary>Details</summary>
Motivation: To address challenges in aligning qualitative and quantitative data in mixed methods research by leveraging LLMs to predict human survey responses.

Method: Used LLMs guided by personal interviews to predict responses on the BREQ questionnaire, analyzing alignment and variability.

Result: LLMs captured overall response patterns but with lower variability. Interview data improved diversity for some models, and prompts/settings enhanced alignment. Demographics had minimal impact.

Conclusion: Interview-informed LLMs show promise for bridging qualitative and quantitative methods but need refinement in variability, emotional interpretation, and psychometric fidelity. Future work should focus on prompt design, bias mitigation, and model optimization.

Abstract: Mixed methods research integrates quantitative and qualitative data but faces
challenges in aligning their distinct structures, particularly in examining
measurement characteristics and individual response patterns. Advances in large
language models (LLMs) offer promising solutions by generating synthetic survey
responses informed by qualitative data. This study investigates whether LLMs,
guided by personal interviews, can reliably predict human survey responses,
using the Behavioral Regulations in Exercise Questionnaire (BREQ) and
interviews from after-school program staff as a case study. Results indicate
that LLMs capture overall response patterns but exhibit lower variability than
humans. Incorporating interview data improves response diversity for some
models (e.g., Claude, GPT), while well-crafted prompts and low-temperature
settings enhance alignment between LLM and human responses. Demographic
information had less impact than interview content on alignment accuracy. These
findings underscore the potential of interview-informed LLMs to bridge
qualitative and quantitative methodologies while revealing limitations in
response variability, emotional interpretation, and psychometric fidelity.
Future research should refine prompt design, explore bias mitigation, and
optimize model settings to enhance the validity of LLM-generated survey data in
social science research.

</details>


### [40] [Found in Translation: Measuring Multilingual LLM Consistency as Simple as Translate then Evaluate](https://arxiv.org/pdf/2505.21999)
*Ashim Gupta, Maitrey Mehta, Zhichao Xu, Vivek Srikumar*

Main category: cs.CL

TL;DR: The paper proposes a framework to evaluate the cross-lingual consistency of large language models (LLMs) using a Translate then Evaluate strategy, revealing significant inconsistencies in responses across 30 languages.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of multilingual LLMs rely on expensive annotated datasets and struggle with open-ended tasks. The study aims to assess predictability of responses across languages.

Method: A Translate then Evaluate framework is introduced, focusing on consistency in information and empathy across languages.

Result: Results show pronounced inconsistencies in LLM responses, with severe deficits in certain language families and scripts.

Conclusion: The findings highlight weaknesses in multilingual LLM capabilities and advocate for consistent cross-lingual evaluations. The framework is offered for future benchmarking.

Abstract: Large language models (LLMs) provide detailed and impressive responses to
queries in English. However, are they really consistent at responding to the
same query in other languages? The popular way of evaluating for multilingual
performance of LLMs requires expensive-to-collect annotated datasets. Further,
evaluating for tasks like open-ended generation, where multiple correct answers
may exist, is nontrivial. Instead, we propose to evaluate the predictability of
model response across different languages. In this work, we propose a framework
to evaluate LLM's cross-lingual consistency based on a simple Translate then
Evaluate strategy. We instantiate this evaluation framework along two
dimensions of consistency: information and empathy. Our results reveal
pronounced inconsistencies in popular LLM responses across thirty languages,
with severe performance deficits in certain language families and scripts,
underscoring critical weaknesses in their multilingual capabilities. These
findings necessitate cross-lingual evaluations that are consistent along
multiple dimensions. We invite practitioners to use our framework for future
multilingual LLM benchmarking.

</details>


### [41] [Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal Assistance](https://arxiv.org/pdf/2505.22003)
*Jatin Gupta, Akhil Sharma, Saransh Singhania, Ali Imam Abidi*

Main category: cs.CL

TL;DR: Legal Assist AI, a transformer-based model, addresses India's legal assistance gap by providing accurate legal information retrieval and responses, outperforming competitors like GPT-3.5 Turbo and Mistral 7B.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in accessible legal assistance in India due to limited awareness and access to legal information.

Method: Fine-tuning a transformer-based model on Indian legal datasets (e.g., Constitution, BNS, BNSS) for specialized legal Question-Answering.

Result: Achieved 60.08% on AIBE, outperforming GPT-3.5 Turbo and Mistral 7B, with reduced hallucinations.

Conclusion: Legal Assist AI is reliable for real-world legal applications, with future plans to expand datasets and multilingual support.

Abstract: Pursuit of accessible legal assistance in India faces a critical gap, as many
citizens struggle to leverage their legal rights due to limited awareness and
access to relevant legal information. This paper introduces Legal Assist AI, a
transformer-based model designed to bridge this gap by offering effective legal
assistance through large language models (LLMs). The system retrieves relevant
legal information from a curated database and generates accurate responses,
enabling effective assistance for diverse users, including legal professionals,
scholars, and the general public. The model was fine-tuned on extensive
datasets from the Indian legal domain, including Indian Constitution, Bharatiya
Nyaya Sanhita (BNS), Bharatiya Nagarik Suraksha Sanhita (BNSS) and so forth,
providing a robust understanding of the complexities of Indian law. By
incorporating domain-specific legal datasets, the proposed model demonstrated
remarkable efficiency and specialization in legal Question-Answering. The model
was evaluated against state-of-the-art models such as GPT-3.5 Turbo and Mistral
7B, achieving a 60.08% score on the AIBE, outperforming its competitors in
legal reasoning and accuracy. Unlike other models, Legal Assist AI avoided
common issues such as hallucinations, making it highly reliable for practical
legal applications. It showcases the model's applicability in real-world legal
scenarios, with future iterations aiming to enhance performance and expand its
dataset to cover a broader range of multilingual and case-specific queries as
well.

</details>


### [42] [CoThink: Token-Efficient Reasoning via Instruct Models Guiding Reasoning Models](https://arxiv.org/pdf/2505.22017)
*Siqi Fan, Peng Han, Shuo Shang, Yequan Wang, Aixin Sun*

Main category: cs.CL

TL;DR: CoThink, a pipeline combining instruct and reasoning models, reduces token generation by 22.3% while maintaining accuracy, addressing overthinking in LLMs.


<details>
  <summary>Details</summary>
Motivation: Reasoning-optimized LLMs often overthink simple problems, leading to inefficient verbosity. This inefficiency stems from reinforcement learning and backward chain-of-thought training.

Method: Propose CoThink: an instruct model drafts a solution outline, followed by a reasoning model completing it. This dynamically adjusts reasoning depth based on input difficulty.

Result: CoThink reduces token generation by 22.3% with minimal accuracy loss (0.42% margin) across three datasets and models.

Conclusion: CoThink improves reasoning efficiency in LLMs, suggesting a potential scaling law for reasoning efficiency.

Abstract: Large language models (LLMs) benefit from increased test-time compute, a
phenomenon known as test-time scaling. However, reasoning-optimized models
often overthink even simple problems, producing excessively verbose outputs and
leading to low token efficiency. By comparing these models with equally sized
instruct models, we identify two key causes of this verbosity: (1)
reinforcement learning reduces the information density of forward reasoning,
and (2) backward chain-of thought training encourages redundant and often
unnecessary verification steps. Since LLMs cannot assess the difficulty of a
given problem, they tend to apply the same cautious reasoning strategy across
all tasks, resulting in inefficient overthinking. To address this, we propose
CoThink, an embarrassingly simple pipeline: an instruct model first drafts a
high-level solution outline; a reasoning model then works out the solution. We
observe that CoThink enables dynamic adjustment of reasoning depth based on
input difficulty. Evaluated with three reasoning models DAPO, DeepSeek-R1, and
QwQ on three datasets GSM8K, MATH500, and AIME24, CoThink reduces total token
generation by 22.3% while maintaining pass@1 accuracy within a 0.42% margin on
average. With reference to the instruct model, we formally define reasoning
efficiency and observe a potential reasoning efficiency scaling law in LLMs.

</details>


### [43] [Improving Continual Pre-training Through Seamless Data Packing](https://arxiv.org/pdf/2505.22018)
*Ruicheng Yin, Xuan Gao, Changze Lv, Xiaohua Wang, Xiaoqing Zheng, Xuanjing Huang*

Main category: cs.CL

TL;DR: Seamless Packing (SP) improves continual pre-training by reducing truncation and context discontinuity through a sliding window and First-Fit-Decreasing algorithm.


<details>
  <summary>Details</summary>
Motivation: Existing data packing methods for continual pre-training cause excessive truncation and context discontinuity, hindering model performance.

Method: SP uses a sliding window for overlapping tokens and a First-Fit-Decreasing algorithm to pack shorter texts, minimizing padding and truncation.

Result: SP outperforms baselines in 99% of settings across various models and domains.

Conclusion: SP effectively enhances continual pre-training by preserving context and improving efficiency.

Abstract: Continual pre-training has demonstrated significant potential in enhancing
model performance, particularly in domain-specific scenarios. The most common
approach for packing data before continual pre-training involves concatenating
input texts and splitting them into fixed-length sequences. While
straightforward and efficient, this method often leads to excessive truncation
and context discontinuity, which can hinder model performance. To address these
issues, we explore the potential of data engineering to enhance continual
pre-training, particularly its impact on model performance and efficiency. We
propose Seamless Packing (SP), a novel data packing strategy aimed at
preserving contextual information more effectively and enhancing model
performance. Our approach employs a sliding window technique in the first stage
that synchronizes overlapping tokens across consecutive sequences, ensuring
better continuity and contextual coherence. In the second stage, we adopt a
First-Fit-Decreasing algorithm to pack shorter texts into bins slightly larger
than the target sequence length, thereby minimizing padding and truncation.
Empirical evaluations across various model architectures and corpus domains
demonstrate the effectiveness of our method, outperforming baseline method in
99% of all settings. Code is available at
https://github.com/Infernus-WIND/Seamless-Packing.

</details>


### [44] [VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning](https://arxiv.org/pdf/2505.22019)
*Qiuchen Wang, Ruixue Ding, Yu Zeng, Zehui Chen, Lin Chen, Shihang Wang, Pengjun Xie, Fei Huang, Feng Zhao*

Main category: cs.CL

TL;DR: VRAG-RL is a novel RL framework for visually rich RAG tasks, addressing limitations of traditional text-based and vision-based methods by optimizing VLMs with tailored actions and rewards.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG methods struggle with visually rich information due to insufficient reasoning and fixed pipelines. RL is introduced to enhance reasoning and retrieval.

Method: VRAG-RL uses RL to optimize VLMs, employing actions like cropping and scaling, and integrates query rewriting and retrieval performance into rewards.

Result: The framework improves reasoning and retrieval for visually rich inputs, addressing prior limitations in token allocation and query articulation.

Conclusion: VRAG-RL effectively bridges gaps in visually rich RAG tasks, aligning models with real-world applications.

Abstract: Effectively retrieving, reasoning and understanding visually rich information
remains a challenge for RAG methods. Traditional text-based methods cannot
handle visual-related information. On the other hand, current vision-based RAG
approaches are often limited by fixed pipelines and frequently struggle to
reason effectively due to the insufficient activation of the fundamental
capabilities of models. As RL has been proven to be beneficial for model
reasoning, we introduce VRAG-RL, a novel RL framework tailored for complex
reasoning across visually rich information. With this framework, VLMs interact
with search engines, autonomously sampling single-turn or multi-turn reasoning
trajectories with the help of visual perception tokens and undergoing continual
optimization based on these samples. Our approach highlights key limitations of
RL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely
incorporate images into the context, leading to insufficient reasoning token
allocation and neglecting visual-specific perception; and (ii) When models
interact with search engines, their queries often fail to retrieve relevant
information due to the inability to articulate requirements, thereby leading to
suboptimal performance. To address these challenges, we define an action space
tailored for visually rich inputs, with actions including cropping and scaling,
allowing the model to gather information from a coarse-to-fine perspective.
Furthermore, to bridge the gap between users' original inquiries and the
retriever, we employ a simple yet effective reward that integrates query
rewriting and retrieval performance with a model-based reward. Our VRAG-RL
optimizes VLMs for RAG tasks using specially designed RL strategies, aligning
the model with real-world applications. The code is available at
\hyperlink{https://github.com/Alibaba-NLP/VRAG}{https://github.com/Alibaba-NLP/VRAG}.

</details>


### [45] [Jailbreak Distillation: Renewable Safety Benchmarking](https://arxiv.org/pdf/2505.22037)
*Jingyu Zhang, Ahmed Elgohary, Xiawei Wang, A S M Iftekhar, Ahmed Magooda, Benjamin Van Durme, Daniel Khashabi, Kyle Jackson*

Main category: cs.CL

TL;DR: JBDistill is a framework for creating high-quality, updatable safety benchmarks for LLMs by distilling jailbreak attacks, ensuring fair comparisons and minimal human effort.


<details>
  <summary>Details</summary>
Motivation: The rapid deployment of LLMs in critical applications necessitates robust safety benchmarking to address vulnerabilities like jailbreak attacks.

Method: JBDistill uses development models and jailbreak attack algorithms to create a prompt pool, then selects effective prompts for benchmarks.

Result: The benchmarks generalize well to 13 diverse models, outperforming existing ones in effectiveness, separability, and diversity.

Conclusion: JBDistill offers a sustainable, adaptable solution for efficient safety evaluation of LLMs.

Abstract: Large language models (LLMs) are rapidly deployed in critical applications,
raising urgent needs for robust safety benchmarking. We propose Jailbreak
Distillation (JBDistill), a novel benchmark construction framework that
"distills" jailbreak attacks into high-quality and easily-updatable safety
benchmarks. JBDistill utilizes a small set of development models and existing
jailbreak attack algorithms to create a candidate prompt pool, then employs
prompt selection algorithms to identify an effective subset of prompts as
safety benchmarks. JBDistill addresses challenges in existing safety
evaluation: the use of consistent evaluation prompts across models ensures fair
comparisons and reproducibility. It requires minimal human effort to rerun the
JBDistill pipeline and produce updated benchmarks, alleviating concerns on
saturation and contamination. Extensive experiments demonstrate our benchmarks
generalize robustly to 13 diverse evaluation models held out from benchmark
construction, including proprietary, specialized, and newer-generation LLMs,
significantly outperforming existing safety benchmarks in effectiveness while
maintaining high separability and diversity. Our framework thus provides an
effective, sustainable, and adaptable solution for streamlining safety
evaluation.

</details>


### [46] [Safeguarding Privacy of Retrieval Data against Membership Inference Attacks: Is This Query Too Close to Home?](https://arxiv.org/pdf/2505.22061)
*Yujin Choi, Youngjoo Park, Junyoung Byun, Jaewook Lee, Jinseong Park*

Main category: cs.CL

TL;DR: Mirabel is a similarity-based framework to detect and defend against membership inference attacks in retrieval-augmented generation systems, ensuring privacy without compromising utility.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of private documents in RAG systems to membership inference attacks (MIAs), which exploit document similarity.

Method: Introduces Mirabel, a detection framework that identifies MIA queries by their high similarity to a single target document, enabling detect-and-hide strategies.

Result: Mirabel successfully defends against state-of-the-art MIA methods, maintains data utility, and integrates with existing private RAG systems.

Conclusion: Mirabel provides an effective, system-agnostic solution to protect private documents in RAG systems from MIAs while preserving functionality.

Abstract: Retrieval-augmented generation (RAG) mitigates the hallucination problem in
large language models (LLMs) and has proven effective for specific,
personalized applications. However, passing private retrieved documents
directly to LLMs introduces vulnerability to membership inference attacks
(MIAs), which try to determine whether the target datum exists in the private
external database or not. Based on the insight that MIA queries typically
exhibit high similarity to only one target document, we introduce Mirabel, a
similarity-based MIA detection framework designed for the RAG system. With the
proposed Mirabel, we show that simple detect-and-hide strategies can
successfully obfuscate attackers, maintain data utility, and remain
system-agnostic. We experimentally prove its detection and defense against
various state-of-the-art MIA methods and its adaptability to existing private
RAG systems.

</details>


### [47] [Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO](https://arxiv.org/pdf/2505.22068)
*Ran Li, Shimin Di, Yuchen Liu, Chen Jing, Yu Qiu, Lei Chen*

Main category: cs.CL

TL;DR: The paper explores how SFT and RLVR can improve reasoning capacity in LLMs for SciIE tasks, proposing a two-stage training method (MimicSFT and R²GRPO) that outperforms baselines.


<details>
  <summary>Details</summary>
Motivation: To address the underperformance of LLMs in SciIE tasks, which require both reasoning and memorization, compared to smaller Bert-based models.

Method: Proposes a two-stage training: 1. MimicSFT (structured reasoning templates), 2. R²GRPO (relevance and rule-induced rewards).

Result: Both methods improve reasoning capacity; R²GRPO with MimicSFT outperforms baseline LLMs and specialized models in relation extraction.

Conclusion: Combining SFT and RLVR in a structured way enhances LLM performance in SciIE tasks, surpassing existing methods.

Abstract: Previous study suggest that powerful Large Language Models (LLMs) trained
with Reinforcement Learning with Verifiable Rewards (RLVR) only refines
reasoning path without improving the reasoning capacity in math tasks while
supervised-finetuning(SFT) with distillation can. We study this from the view
of Scientific information extraction (SciIE) where LLMs and reasoning LLMs
underperforms small Bert-based models. SciIE require both the reasoning and
memorization. We argue that both SFT and RLVR can refine the reasoning path and
improve reasoning capacity in a simple way based on SciIE. We propose two-stage
training with 1. MimicSFT, using structured reasoning templates without needing
high-quality chain-of-thought data, 2. R$^2$GRPO with relevance and
rule-induced rewards. Experiments on scientific IE benchmarks show that both
methods can improve the reasoning capacity. R$^2$GRPO with mimicSFT surpasses
baseline LLMs and specialized supervised models in relation extraction. Our
code is available at https://github.com/ranlislz/R2GRPO.

</details>


### [48] [ArgInstruct: Specialized Instruction Fine-Tuning for Computational Argumentation](https://arxiv.org/pdf/2505.22076)
*Maja Stahl, Timon Ziegenbein, Joonsuk Park, Henning Wachsmuth*

Main category: cs.CL

TL;DR: Specialized instruction fine-tuning for computational argumentation (CA) enhances LLMs' performance on CA tasks while maintaining general NLP capabilities.


<details>
  <summary>Details</summary>
Motivation: Instruction-following LLMs struggle with domain-specific tasks like computational argumentation, necessitating specialized fine-tuning.

Method: Developed a CA-specific benchmark with 105 tasks, synthesized 52k CA-related instructions, and adapted self-instruct for fine-tuning.

Result: CA-specialized fine-tuning improves performance on seen and unseen CA tasks without degrading general NLP task performance.

Conclusion: Specialized instruction fine-tuning effectively adapts LLMs for domain-specific tasks like CA while preserving generalization.

Abstract: Training large language models (LLMs) to follow instructions has
significantly enhanced their ability to tackle unseen tasks. However, despite
their strong generalization capabilities, instruction-following LLMs encounter
difficulties when dealing with tasks that require domain knowledge. This work
introduces a specialized instruction fine-tuning for the domain of
computational argumentation (CA). The goal is to enable an LLM to effectively
tackle any unseen CA tasks while preserving its generalization capabilities.
Reviewing existing CA research, we crafted natural language instructions for
105 CA tasks to this end. On this basis, we developed a CA-specific benchmark
for LLMs that allows for a comprehensive evaluation of LLMs' capabilities in
solving various CA tasks. We synthesized 52k CA-related instructions, adapting
the self-instruct process to train a CA-specialized instruction-following LLM.
Our experiments suggest that CA-specialized instruction fine-tuning
significantly enhances the LLM on both seen and unseen CA tasks. At the same
time, performance on the general NLP tasks of the SuperNI benchmark remains
stable.

</details>


### [49] [Learning to Route Queries Across Knowledge Bases for Step-wise Retrieval-Augmented Reasoning](https://arxiv.org/pdf/2505.22095)
*Chunyi Peng, Zhipeng Xu, Zhenghao Liu, Yishan Li, Yukun Yan, Shuo Wang, Zhiyuan Liu, Yu Gu, Minghe Yu, Ge Yu, Maosong Sun*

Main category: cs.CL

TL;DR: R1-Router is a dynamic MRAG framework that improves MLLMs by adaptively retrieving and integrating knowledge from multiple KBs during reasoning, outperforming baselines by 7%.


<details>
  <summary>Details</summary>
Motivation: Existing MRAG methods use static retrieval pipelines, ignoring MLLMs' reasoning capabilities to dynamically interact with KBs.

Method: Proposes R1-Router, which generates follow-up queries, routes them to suitable KBs, and integrates knowledge. Uses Step-GRPO for reinforcement learning.

Result: Outperforms baselines by 7% on open-domain QA benchmarks, reducing unnecessary retrievals and improving efficiency.

Conclusion: R1-Router effectively leverages diverse KBs adaptively, enhancing MLLMs' reasoning and accuracy.

Abstract: Multimodal Retrieval-Augmented Generation (MRAG) has shown promise in
mitigating hallucinations in Multimodal Large Language Models (MLLMs) by
incorporating external knowledge during generation. Existing MRAG methods
typically adopt a static retrieval pipeline that fetches relevant information
from multiple Knowledge Bases (KBs), followed by a refinement step. However,
these approaches overlook the reasoning and planning capabilities of MLLMs to
dynamically determine how to interact with different KBs during the reasoning
process. To address this limitation, we propose R1-Router, a novel MRAG
framework that learns to decide when and where to retrieve knowledge based on
the evolving reasoning state. Specifically, R1-Router can generate follow-up
queries according to the current reasoning step, routing these intermediate
queries to the most suitable KB, and integrating external knowledge into a
coherent reasoning trajectory to answer the original query. Furthermore, we
introduce Step-wise Group Relative Policy Optimization (Step-GRPO), a tailored
reinforcement learning algorithm that assigns step-specific rewards to optimize
the reasoning behavior of MLLMs. Experimental results on various open-domain QA
benchmarks across multiple modalities demonstrate that R1-Router outperforms
baseline models by over 7%. Further analysis shows that R1-Router can
adaptively and effectively leverage diverse KBs, reducing unnecessary
retrievals and improving both efficiency and accuracy.

</details>


### [50] [Knowledge Base Construction for Knowledge-Augmented Text-to-SQL](https://arxiv.org/pdf/2505.22096)
*Jinheon Baek, Horst Samulowitz, Oktie Hassanzadeh, Dharmashankar Subramanian, Sola Shirai, Alfio Gliozzo, Debarun Bhattacharjya*

Main category: cs.CL

TL;DR: The paper proposes a knowledge base for text-to-SQL to improve accuracy by leveraging comprehensive, reusable knowledge from diverse queries and schemas, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based text-to-SQL methods struggle with domain-specific queries due to limited parametric knowledge, leading to less accurate SQL generation.

Method: Construct a comprehensive knowledge base from available questions, schemas, and relevant knowledge, enabling retrieval and generation of necessary knowledge for queries.

Result: The approach outperforms baselines on multiple datasets, including overlapping and non-overlapping database scenarios.

Conclusion: The proposed knowledge base enhances text-to-SQL accuracy by addressing the limitations of LLMs through reusable, domain-agnostic knowledge.

Abstract: Text-to-SQL aims to translate natural language queries into SQL statements,
which is practical as it enables anyone to easily retrieve the desired
information from databases. Recently, many existing approaches tackle this
problem with Large Language Models (LLMs), leveraging their strong capability
in understanding user queries and generating corresponding SQL code. Yet, the
parametric knowledge in LLMs might be limited to covering all the diverse and
domain-specific queries that require grounding in various database schemas,
which makes generated SQLs less accurate oftentimes. To tackle this, we propose
constructing the knowledge base for text-to-SQL, a foundational source of
knowledge, from which we retrieve and generate the necessary knowledge for
given queries. In particular, unlike existing approaches that either manually
annotate knowledge or generate only a few pieces of knowledge for each query,
our knowledge base is comprehensive, which is constructed based on a
combination of all the available questions and their associated database
schemas along with their relevant knowledge, and can be reused for unseen
databases from different datasets and domains. We validate our approach on
multiple text-to-SQL datasets, considering both the overlapping and
non-overlapping database scenarios, where it outperforms relevant baselines
substantially.

</details>


### [51] [MemOS: An Operating System for Memory-Augmented Generation (MAG) in Large Language Models](https://arxiv.org/pdf/2505.22101)
*Zhiyu Li, Shichao Song, Hanyu Wang, Simin Niu, Ding Chen, Jiawei Yang, Chenyang Xi, Huayi Lai, Jihao Zhao, Yezhaohui Wang, Junpeng Ren, Zehao Lin, Jiahao Huo, Tianyi Chen, Kai Chen, Kehang Li, Zhiqiang Yin, Qingchen Yu, Bo Tang, Hongkang Yang, Zhi-Qin John Xu, Feiyu Xiong*

Main category: cs.CL

TL;DR: MemOS introduces a memory operating system for LLMs, addressing the lack of structured memory management by unifying parametric, activation, and plaintext memory under a standardized abstraction called MemCube.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack a unified architecture for memory, relying on limited parametric and activation memory, and emerging methods like RAG lack lifecycle management.

Method: MemOS provides a memory-centric framework with MemCube, standardizing memory representation, organization, and governance.

Result: The system enables structured, traceable memory access, enhancing controllability, adaptability, and evolvability in LLMs.

Conclusion: MemOS fills a critical gap in LLM infrastructure, supporting continual adaptation and personalized intelligence for next-gen systems.

Abstract: Large Language Models (LLMs) have emerged as foundational infrastructure in
the pursuit of Artificial General Intelligence (AGI). Despite their remarkable
capabilities in language perception and generation, current LLMs fundamentally
lack a unified and structured architecture for handling memory. They primarily
rely on parametric memory (knowledge encoded in model weights) and ephemeral
activation memory (context-limited runtime states). While emerging methods like
Retrieval-Augmented Generation (RAG) incorporate plaintext memory, they lack
lifecycle management and multi-modal integration, limiting their capacity for
long-term knowledge evolution. To address this, we introduce MemOS, a memory
operating system designed for LLMs that, for the first time, elevates memory to
a first-class operational resource. It builds unified mechanisms for
representation, organization, and governance across three core memory types:
parametric, activation, and plaintext. At its core is the MemCube, a
standardized memory abstraction that enables tracking, fusion, and migration of
heterogeneous memory, while offering structured, traceable access across tasks
and contexts. MemOS establishes a memory-centric execution framework with
strong controllability, adaptability, and evolvability. It fills a critical gap
in current LLM infrastructure and lays the groundwork for continual adaptation,
personalized intelligence, and cross-platform coordination in next-generation
intelligent systems.

</details>


### [52] [Curse of High Dimensionality Issue in Transformer for Long-context Modeling](https://arxiv.org/pdf/2505.22107)
*Shuhai Zhang, Zeng You, Yaofo Chen, Zhiquan Wen, Qianyue Wang, Zhijie Qiu, Yuanqing Li, Mingkui Tan*

Main category: cs.CL

TL;DR: The paper introduces Dynamic Group Attention (DGA) to reduce computational inefficiencies in Transformer-based LLMs by optimizing attention sparsity through a group coding strategy.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational inefficiencies in long-context modeling due to redundant attention computations in LLMs.

Method: Reformulates sequence modeling as a supervised task, analyzes attention sparsity, and proposes DGA to aggregate less important tokens.

Result: DGA significantly reduces computational costs while maintaining competitive performance.

Conclusion: DGA effectively optimizes attention computation, improving efficiency without sacrificing model performance.

Abstract: Transformer-based large language models (LLMs) excel in natural language
processing tasks by capturing long-range dependencies through self-attention
mechanisms. However, long-context modeling faces significant computational
inefficiencies due to \textit{redundant} attention computations: while
attention weights are often \textit{sparse}, all tokens consume \textit{equal}
computational resources. In this paper, we reformulate traditional
probabilistic sequence modeling as a \textit{supervised learning task},
enabling the separation of relevant and irrelevant tokens and providing a
clearer understanding of redundancy. Based on this reformulation, we
theoretically analyze attention sparsity, revealing that only a few tokens
significantly contribute to predictions. Building on this, we formulate
attention optimization as a linear coding problem and propose a \textit{group
coding strategy}, theoretically showing its ability to improve robustness
against random noise and enhance learning efficiency. Motivated by this, we
propose \textit{Dynamic Group Attention} (DGA), which leverages the group
coding to explicitly reduce redundancy by aggregating less important tokens
during attention computation. Empirical results show that our DGA significantly
reduces computational costs while maintaining competitive performance.Code is
available at https://github.com/bolixinyu/DynamicGroupAttention.

</details>


### [53] [THINK-Bench: Evaluating Thinking Efficiency and Chain-of-Thought Quality of Large Reasoning Models](https://arxiv.org/pdf/2505.22113)
*Zhiyuan Li, Yi Chang, Yuan Wu*

Main category: cs.CL

TL;DR: The paper introduces Think-Bench to evaluate reasoning efficiency in large reasoning models (LRMs), highlighting the issue of overthinking—excessive token generation in simple tasks—and proposes metrics to assess efficiency.


<details>
  <summary>Details</summary>
Motivation: Overthinking in LRMs leads to computational inefficiency, especially in simple tasks, wasting resources despite high performance in complex tasks.

Method: Think-Bench is introduced as a benchmark, with novel efficiency metrics, to evaluate LRMs across reasoning process, outcome quality, and chain-of-thought (CoT) characteristics.

Result: Most LRMs overthink easy questions, producing lengthy reasoning chains. While CoT quality is high, efficiency varies.

Conclusion: Think-Bench provides a foundation for improving LRM research, addressing overthinking and efficiency issues.

Abstract: Large reasoning models (LRMs) have achieved impressive performance in complex
tasks, often outperforming conventional large language models (LLMs). However,
the prevalent issue of overthinking severely limits their computational
efficiency. Overthinking occurs when models generate excessive and redundant
tokens that contribute little to accurate outcomes, especially in simple tasks,
resulting in a significant waste of computational resources. To systematically
investigate this issue, we introduce Think-Bench, a benchmark designed to
evaluate the reasoning efficiency of LRMs. We also propose novel efficiency
metrics and conduct a comprehensive evaluation of various LRMs across multiple
dimensions, including the reasoning process, outcome quality, and
chain-of-thought (CoT) characteristics. Our analysis reveals that most LRMs
exhibit overthinking in handling easy questions, generating unnecessarily
lengthy reasoning chains. While many LRMs demonstrate high CoT quality, several
suffer from low efficiency. We hope that Think-Bench can serve as a robust
foundation for advancing research into LRMs.

</details>


### [54] [Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model](https://arxiv.org/pdf/2505.22116)
*Jintao Zhang, Zirui Liu, Mingyue Cheng, Shilong Zhang, Tingyue Pan, Qi Liu, Yanhu Xie*

Main category: cs.CL

TL;DR: IOHFuseLM is a multimodal language model for predicting intraoperative hypotension (IOH) using a two-stage training strategy and token-level alignment of clinical and physiological data.


<details>
  <summary>Details</summary>
Motivation: IOH is linked to adverse outcomes, but prediction is challenging due to sparse events and integrating diverse data.

Method: Two-stage training: domain adaptive pretraining on augmented physiological data, followed by task fine-tuning. Multimodal fusion aligns clinical descriptions with time series.

Result: IOHFuseLM outperforms baselines in identifying IOH events on two datasets.

Conclusion: The framework is effective for clinical decision support, with code available for reproducibility.

Abstract: Intraoperative hypotension (IOH) frequently occurs under general anesthesia
and is strongly linked to adverse outcomes such as myocardial injury and
increased mortality. Despite its significance, IOH prediction is hindered by
event sparsity and the challenge of integrating static and dynamic data across
diverse patients. In this paper, we propose \textbf{IOHFuseLM}, a multimodal
language model framework. To accurately identify and differentiate sparse
hypotensive events, we leverage a two-stage training strategy. The first stage
involves domain adaptive pretraining on IOH physiological time series augmented
through diffusion methods, thereby enhancing the model sensitivity to patterns
associated with hypotension. Subsequently, task fine-tuning is performed on the
original clinical dataset to further enhance the ability to distinguish
normotensive from hypotensive states. To enable multimodal fusion for each
patient, we align structured clinical descriptions with the corresponding
physiological time series at the token level. Such alignment enables the model
to capture individualized temporal patterns alongside their corresponding
clinical semantics. In addition, we convert static patient attributes into
structured text to enrich personalized information. Experimental evaluations on
two intraoperative datasets demonstrate that IOHFuseLM outperforms established
baselines in accurately identifying IOH events, highlighting its applicability
in clinical decision support scenarios. Our code is publicly available to
promote reproducibility at https://github.com/zjt-gpu/IOHFuseLM.

</details>


### [55] [Multilingual vs Crosslingual Retrieval of Fact-Checked Claims: A Tale of Two Approaches](https://arxiv.org/pdf/2505.22118)
*Alan Ramponi, Marco Rovera, Robert Moro, Sara Tonelli*

Main category: cs.CL

TL;DR: The paper explores improving multilingual and crosslingual retrieval of fact-checked claims, focusing on negative example selection and re-ranking, with LLM-based re-ranking yielding the best results.


<details>
  <summary>Details</summary>
Motivation: Automating retrieval of fact-checked claims across languages is crucial for languages with limited fact-checks and global narratives like pandemics or politics.

Method: The study evaluates strategies for multilingual and crosslingual performance, including negative example selection (supervised) and re-ranking (unsupervised), tested on 47 languages.

Result: LLM-based re-ranking performed best, followed by fine-tuning with similarity-based negative examples. Crosslinguality showed unique characteristics compared to multilingual setups.

Conclusion: Crosslingual retrieval has distinct challenges and benefits, with LLM-based re-ranking being the most effective approach.

Abstract: Retrieval of previously fact-checked claims is a well-established task, whose
automation can assist professional fact-checkers in the initial steps of
information verification. Previous works have mostly tackled the task
monolingually, i.e., having both the input and the retrieved claims in the same
language. However, especially for languages with a limited availability of
fact-checks and in case of global narratives, such as pandemics, wars, or
international politics, it is crucial to be able to retrieve claims across
languages. In this work, we examine strategies to improve the multilingual and
crosslingual performance, namely selection of negative examples (in the
supervised) and re-ranking (in the unsupervised setting). We evaluate all
approaches on a dataset containing posts and claims in 47 languages (283
language combinations). We observe that the best results are obtained by using
LLM-based re-ranking, followed by fine-tuning with negative examples sampled
using a sentence similarity-based strategy. Most importantly, we show that
crosslinguality is a setup with its own unique characteristics compared to the
multilingual setup.

</details>


### [56] [LoKI: Low-damage Knowledge Implanting of Large Language Models](https://arxiv.org/pdf/2505.22120)
*Runyu Wang, Peng Ping, Zhengyu Guo, Xiaoye Zhang, Quan Shi, Liting Zhou, Tianbo Ji*

Main category: cs.CL

TL;DR: LoKI is a PEFT method for LLMs that reduces catastrophic forgetting while maintaining general capabilities, outperforming full fine-tuning and LoRA-based methods.


<details>
  <summary>Details</summary>
Motivation: Address catastrophic forgetting in PEFT methods for LLMs without sacrificing general capabilities.

Method: Proposes LoKI, a PEFT technique based on mechanistic understanding of knowledge storage in transformers.

Result: LoKI matches or surpasses full fine-tuning and LoRA methods in task-specific performance while better preserving general capabilities.

Conclusion: LoKI achieves state-of-the-art trade-offs between task specialization and general capability preservation, with publicly available implementation.

Abstract: Fine-tuning adapts pretrained models for specific tasks but poses the risk of
catastrophic forgetting (CF), where critical knowledge from pre-training is
overwritten. Current Parameter-Efficient Fine-Tuning (PEFT) methods for Large
Language Models (LLMs), while efficient, often sacrifice general capabilities.
To address the issue of CF in a general-purpose PEFT framework, we propose
\textbf{Lo}w-damage \textbf{K}nowledge \textbf{I}mplanting (\textbf{LoKI}), a
PEFT technique that is based on a mechanistic understanding of how knowledge is
stored in transformer architectures. In two real-world scenarios, LoKI
demonstrates task-specific performance that is comparable to or even surpasses
that of full fine-tuning and LoRA-based methods across various model types,
while significantly better preserving general capabilities. Our work connects
mechanistic insights into LLM knowledge storage with practical fine-tuning
objectives, achieving state-of-the-art trade-offs between task specialization
and the preservation of general capabilities. Our implementation is publicly
available as ready-to-use code\footnote{https://github.com/Nexround/LoKI}.

</details>


### [57] [EULER: Enhancing the Reasoning Ability of Large Language Models through Error-Induced Learning](https://arxiv.org/pdf/2505.22131)
*Zhuoyang Wu, Xinze Li, Zhenghao Liu, Yukun Yan, Zhiyuan Liu, Minghe Yu, Cheng Yang, Yu Gu, Ge Yu, Maosong Sun*

Main category: cs.CL

TL;DR: EULER model enhances LLMs' math reasoning by generating high-quality solution errors during SFT, improving performance by over 4%.


<details>
  <summary>Details</summary>
Motivation: To improve LLMs' mathematical reasoning by learning from errors, which are hard to generate for each problem.

Method: Introduces EULER, an error exposure model that generates solution errors, optimized for self-made errors and regularized by superior LLM solutions.

Result: EULER improves performance by over 4% on math datasets and synthesizes challenging, educational errors.

Conclusion: EULER effectively enhances LLMs' math reasoning by generating high-quality errors, benefiting training and inference.

Abstract: Large Language Models (LLMs) have demonstrated strong reasoning capabilities
and achieved promising results in mathematical problem-solving tasks. Learning
from errors offers the potential to further enhance the performance of LLMs
during Supervised Fine-Tuning (SFT). However, the errors in synthesized
solutions are typically gathered from sampling trails, making it challenging to
generate solution errors for each mathematical problem. This paper introduces
the Error-IndUced LEaRning (EULER) model, which aims to develop an error
exposure model that generates high-quality solution errors to enhance the
mathematical reasoning capabilities of LLMs. Specifically, EULER optimizes the
error exposure model to increase the generation probability of self-made
solution errors while utilizing solutions produced by a superior LLM to
regularize the generation quality. Our experiments across various mathematical
problem datasets demonstrate the effectiveness of the EULER model, achieving an
improvement of over 4% compared to all baseline models. Further analysis
reveals that EULER is capable of synthesizing more challenging and educational
solution errors, which facilitate both the training and inference processes of
LLMs. All codes are available at https://github.com/NEUIR/EULER.

</details>


### [58] [RAD: Redundancy-Aware Distillation for Hybrid Models via Self-Speculative Decoding](https://arxiv.org/pdf/2505.22135)
*Yuichiro Hoshino, Hideyuki Tachibana, Muneyoshi Inahara, Hiroto Takegawa*

Main category: cs.CL

TL;DR: RAD (Redundancy-Aware Distillation) improves hybrid Transformer-SSM models by identifying and replacing redundant attention layers with SSM components, enhancing performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Optimizing hybrid models by addressing redundancy in Transformers to balance performance and efficiency.

Method: Uses self-speculative decoding to identify redundant attention layers, replaces them with SSM components, and applies targeted self-distillation.

Result: Significantly outperforms base models on mathematical and coding tasks, achieving faster convergence and higher scores (e.g., 71.27 on GSM8K).

Conclusion: RAD provides an efficient optimization pathway for hybrid models, enhancing performance even with smaller teacher models.

Abstract: Hybrid models combining Transformers and State Space Models (SSMs) are
promising for balancing performance and efficiency. However, optimizing these
hybrid models, particularly by addressing the potential redundancy inherent
within the Transformer components, remains a significant challenge. In this
paper, we propose RAD (Redundancy-Aware Distillation), a novel framework that
uses self-speculative decoding as a diagnostic tool to identify redundant
attention layers within the model. These identified layers are then selectively
replaced with SSM components, followed by targeted (self-)distillation.
Specifically, RAD focuses knowledge transfer on the components identified as
redundant, considering architectural changes and specific weight initialization
strategies. We experimentally demonstrate that self-distillation using RAD
significantly surpasses the performance of the original base model on
mathematical and coding tasks. Furthermore, RAD is also effective in standard
knowledge distillation settings, achieving up to approximately 2x faster
convergence compared to baseline methods. Notably, while a baseline model
distilled from a Llama-3.1 70B teacher achieves scores of 46.17 on GSM8K and
22.75 on CRUX, RAD achieves significantly higher scores of 71.27 on GSM8K and
28.25 on CRUX, even when using a much smaller Llama-3.1 8B teacher. RAD offers
a new pathway for efficient optimization and performance enhancement in the
distillation of hybrid models.

</details>


### [59] [Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments](https://arxiv.org/pdf/2505.22137)
*Marc Feger, Katarina Boland, Stefan Dietze*

Main category: cs.CL

TL;DR: The paper re-evaluates state-of-the-art BERT-like models for argument mining, revealing their reliance on dataset-specific cues and limited generalization. Task-specific pre-training improves robustness.


<details>
  <summary>Details</summary>
Motivation: To assess the generalization ability of BERT-like models in argument mining, given their dominance in benchmarks but unclear task alignment.

Method: Evaluated four transformers (three standard, one contrastive pre-trained) on 17 English sentence-level datasets.

Result: Models rely on lexical shortcuts, perform well on familiar benchmarks but poorly on unseen data. Task-specific pre-training enhances generalization.

Conclusion: Current models may overfit to dataset cues; task-specific pre-training improves robustness and generalization in argument mining.

Abstract: Identifying arguments is a necessary prerequisite for various tasks in
automated discourse analysis, particularly within contexts such as political
debates, online discussions, and scientific reasoning. In addition to
theoretical advances in understanding the constitution of arguments, a
significant body of research has emerged around practical argument mining,
supported by a growing number of publicly available datasets. On these
benchmarks, BERT-like transformers have consistently performed best,
reinforcing the belief that such models are broadly applicable across diverse
contexts of debate. This study offers the first large-scale re-evaluation of
such state-of-the-art models, with a specific focus on their ability to
generalize in identifying arguments. We evaluate four transformers, three
standard and one enhanced with contrastive pre-training for better
generalization, on 17 English sentence-level datasets as most relevant to the
task. Our findings show that, to varying degrees, these models tend to rely on
lexical shortcuts tied to content words, suggesting that apparent progress may
often be driven by dataset-specific cues rather than true task alignment. While
the models achieve strong results on familiar benchmarks, their performance
drops markedly when applied to unseen datasets. Nonetheless, incorporating both
task-specific pre-training and joint benchmark training proves effective in
enhancing both robustness and generalization.

</details>


### [60] [InComeS: Integrating Compression and Selection Mechanisms into LLMs for Efficient Model Editing](https://arxiv.org/pdf/2505.22156)
*Shuaiyi Li, Zhisong Zhang, Yang Deng, Chenlong Deng, Tianqing Fang, Hongming Zhang, Haitao Mi, Dong Yu, Wai Lam*

Main category: cs.CL

TL;DR: InComeS enhances LLMs' editing efficiency by compressing and selecting edit contexts, overcoming context window limitations.


<details>
  <summary>Details</summary>
Motivation: Existing model editing methods lack deep semantic understanding and degrade with multiple edits due to LLMs' limited context windows.

Method: Proposes InComeS, a framework using KV cache compression and cross-attention modules for dynamic edit context selection.

Result: Demonstrates effectiveness and efficiency across diverse benchmarks and editing formats.

Conclusion: InComeS improves LLMs' editing capabilities by efficiently handling multiple edits without context window constraints.

Abstract: Although existing model editing methods perform well in recalling exact edit
facts, they often struggle in complex scenarios that require deeper semantic
understanding rather than mere knowledge regurgitation. Leveraging the strong
contextual reasoning abilities of large language models (LLMs), in-context
learning (ICL) becomes a promising editing method by comprehending edit
information through context encoding. However, this method is constrained by
the limited context window of LLMs, leading to degraded performance and
efficiency as the number of edits increases. To overcome this limitation, we
propose InComeS, a flexible framework that enhances LLMs' ability to process
editing contexts through explicit compression and selection mechanisms.
Specifically, InComeS compresses each editing context into the key-value (KV)
cache of a special gist token, enabling efficient handling of multiple edits
without being restricted by the model's context window. Furthermore,
specialized cross-attention modules are added to dynamically select the most
relevant information from the gist pools, enabling adaptive and effective
utilization of edit information. We conduct experiments on diverse model
editing benchmarks with various editing formats, and the results demonstrate
the effectiveness and efficiency of our method.

</details>


### [61] [Stratified Selective Sampling for Instruction Tuning with Dedicated Scoring Strategy](https://arxiv.org/pdf/2505.22157)
*Paramita Mirza, Lucas Weber, Fabian Küch*

Main category: cs.CL

TL;DR: A multi-step pipeline for efficient and universal data selection in LLMs, combining binning, quality estimation, and difficulty scoring, with improved diversity control.


<details>
  <summary>Details</summary>
Motivation: Address the high computational costs and narrow-domain limitations of current data selection methods for LLMs.

Method: A multi-step pipeline involving binning data points, estimating quality with specialized models, scoring difficulty, and using task-based categorization and clustering for diversity.

Result: Enables high-performance fine-tuning with minimal overhead.

Conclusion: The proposed method is efficient, universal, and ensures diversity, making it suitable for multi-purpose model fine-tuning.

Abstract: Recent work shows that post-training datasets for LLMs can be substantially
downsampled without noticeably deteriorating performance. However, data
selection often incurs high computational costs or is limited to narrow
domains. In this paper, we demonstrate that data selection can be both --
efficient and universal -- by using a multi-step pipeline in which we
efficiently bin data points into groups, estimate quality using specialized
models, and score difficulty with a robust, lightweight method. Task-based
categorization allows us to control the composition of our final data --
crucial for finetuning multi-purpose models. To guarantee diversity, we improve
upon previous work using embedding models and a clustering algorithm. This
integrated strategy enables high-performance fine-tuning with minimal overhead.

</details>


### [62] [Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes](https://arxiv.org/pdf/2505.22165)
*Bocheng Li, Zhujin Gao, Linli Xu*

Main category: cs.CL

TL;DR: NeoDiff combines discrete and continuous diffusion models for text generation, offering fine-grained control and adaptive denoising, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing discrete and continuous diffusion models have limitations in fine-grained control and semantic nuance capture. NeoDiff aims to unify their strengths.

Method: NeoDiff uses a Poisson diffusion process for flexible noising and a time predictor for adaptive denoising, with an optimized inference schedule.

Result: NeoDiff outperforms baselines in text generation tasks, including non-autoregressive and autoregressive diffusion models.

Conclusion: NeoDiff provides a principled, effective framework for high-quality text generation, advancing diffusion-based methods.

Abstract: Diffusion models have emerged as a promising approach for text generation,
with recent works falling into two main categories: discrete and continuous
diffusion models. Discrete diffusion models apply token corruption
independently using categorical distributions, allowing for different diffusion
progress across tokens but lacking fine-grained control. Continuous diffusion
models map tokens to continuous spaces and apply fine-grained noise, but the
diffusion progress is uniform across tokens, limiting their ability to capture
semantic nuances. To address these limitations, we propose
\textbf{\underline{N}}on-simultan\textbf{\underline{e}}ous
C\textbf{\underline{o}}ntinuous \textbf{\underline{Diff}}usion Models
(NeoDiff), a novel diffusion model that integrates the strengths of both
discrete and continuous approaches. NeoDiff introduces a Poisson diffusion
process for the forward process, enabling a flexible and fine-grained noising
paradigm, and employs a time predictor for the reverse process to adaptively
modulate the denoising progress based on token semantics. Furthermore, NeoDiff
utilizes an optimized schedule for inference to ensure more precise noise
control and improved performance. Our approach unifies the theories of discrete
and continuous diffusion models, offering a more principled and effective
framework for text generation. Experimental results on several text generation
tasks demonstrate NeoDiff's superior performance compared to baselines of
non-autoregressive continuous and discrete diffusion models, iterative-based
methods and autoregressive diffusion-based methods. These results highlight
NeoDiff's potential as a powerful tool for generating high-quality text and
advancing the field of diffusion-based text generation.

</details>


### [63] [ReliableEval: A Recipe for Stochastic LLM Evaluation via Method of Moments](https://arxiv.org/pdf/2505.22169)
*Gili Lior, Eliya Habba, Shahar Levy, Avi Caciularu, Gabriel Stanovsky*

Main category: cs.CL

TL;DR: The paper proposes a stochastic evaluation method (ReliableEval) to address LLMs' sensitivity to prompt phrasing, ensuring robust and reliable benchmarking.


<details>
  <summary>Details</summary>
Motivation: Standard benchmarks often use single prompts, which may not reliably evaluate LLMs due to their sensitivity to prompt phrasing.

Method: Introduces ReliableEval, a stochastic method of moments over meaning-preserving prompt perturbations, to estimate required prompt resamplings.

Result: Evaluation of five frontier LLMs (including GPT-4o and Claude-3.7-Sonnet) reveals significant prompt sensitivity.

Conclusion: The proposed method is model-, task-, and metric-agnostic, providing a robust framework for LLM evaluation.

Abstract: LLMs are highly sensitive to prompt phrasing, yet standard benchmarks
typically report performance using a single prompt, raising concerns about the
reliability of such evaluations. In this work, we argue for a stochastic method
of moments evaluation over the space of meaning-preserving prompt
perturbations. We introduce a formal definition of reliable evaluation that
accounts for prompt sensitivity, and suggest ReliableEval - a method for
estimating the number of prompt resamplings needed to obtain meaningful
results. Using our framework, we stochastically evaluate five frontier LLMs and
find that even top-performing models like GPT-4o and Claude-3.7-Sonnet exhibit
substantial prompt sensitivity. Our approach is model-, task-, and
metric-agnostic, offering a recipe for meaningful and robust LLM evaluation.

</details>


### [64] [Reverse Preference Optimization for Complex Instruction Following](https://arxiv.org/pdf/2505.22172)
*Xiang Huang, Ting-En Lin, Feiteng Fang, Yuchuan Wu, Hangyu Li, Yuzhong Qu, Fei Huang, Yongbin Li*

Main category: cs.CL

TL;DR: Reverse Preference Optimization (RPO) improves instruction following in LLMs by dynamically reversing constraints to ensure perfect responses, outperforming baselines and scaling well with model size.


<details>
  <summary>Details</summary>
Motivation: Handling complex instructions with multiple constraints is challenging, and existing methods introduce noise in preference pairs.

Method: RPO dynamically reverses constraints within instructions to ensure chosen responses are perfect, clarifying optimization direction and reducing noise.

Result: RPO improves performance by 4.6 and 2.5 points on benchmarks (Sysbench and Multi-IF) and scales effectively, with the 70B model surpassing GPT-4o.

Conclusion: RPO is a simple, effective method for aligning LLMs with multiple preferences, outperforming baselines and scaling well.

Abstract: Instruction following (IF) is a critical capability for large language models
(LLMs). However, handling complex instructions with multiple constraints
remains challenging. Previous methods typically select preference pairs based
on the number of constraints they satisfy, introducing noise where chosen
examples may fail to follow some constraints and rejected examples may excel in
certain respects over the chosen ones. To address the challenge of aligning
with multiple preferences, we propose a simple yet effective method called
Reverse Preference Optimization (RPO). It mitigates noise in preference pairs
by dynamically reversing the constraints within the instruction to ensure the
chosen response is perfect, alleviating the burden of extensive sampling and
filtering to collect perfect responses. Besides, reversal also enlarges the gap
between chosen and rejected responses, thereby clarifying the optimization
direction and making it more robust to noise. We evaluate RPO on two multi-turn
IF benchmarks, Sysbench and Multi-IF, demonstrating average improvements over
the DPO baseline of 4.6 and 2.5 points (on Llama-3.1 8B), respectively.
Moreover, RPO scales effectively across model sizes (8B to 70B parameters),
with the 70B RPO model surpassing GPT-4o.

</details>


### [65] [TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table Evaluation](https://arxiv.org/pdf/2505.22176)
*Vihang Pancholi, Jainit Bafna, Tejas Anvekar, Manish Shrivastava, Vivek Gupta*

Main category: cs.CL

TL;DR: The paper introduces TabXEval, a two-phase framework for comprehensive table evaluation, combining structural alignment and semantic-syntactic comparison, validated on a diverse benchmark.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics fail to capture nuanced table discrepancies, necessitating a robust, explainable evaluation method.

Method: TabXEval uses TabAlign for structural alignment and TabCompare for semantic-syntactic comparison, evaluated on TabXBench.

Result: TabXEval outperforms conventional methods, demonstrating effectiveness across diverse tasks and domains.

Conclusion: TabXEval provides a foundation for future innovations in explainable table evaluation.

Abstract: Evaluating tables qualitatively & quantitatively presents a significant
challenge, as traditional metrics often fail to capture nuanced structural and
content discrepancies. To address this, we introduce a novel, methodical rubric
integrating multi-level structural descriptors with fine-grained contextual
quantification, thereby establishing a robust foundation for comprehensive
table comparison. Building on this foundation, we propose TabXEval, an
eXhaustive and eXplainable two-phase evaluation framework. TabXEval initially
aligns reference tables structurally via TabAlign & subsequently conducts a
systematic semantic and syntactic comparison using TabCompare; this approach
clarifies the evaluation process and pinpoints subtle discrepancies overlooked
by conventional methods. The efficacy of this framework is assessed using
TabXBench, a novel, diverse, multi-domain benchmark we developed, featuring
realistic table perturbations and human-annotated assessments. Finally, a
systematic analysis of existing evaluation methods through
sensitivity-specificity trade-offs demonstrates the qualitative and
quantitative effectiveness of TabXEval across diverse table-related tasks and
domains, paving the way for future innovations in explainable table evaluation.

</details>


### [66] [Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design](https://arxiv.org/pdf/2505.22179)
*Yudi Zhang, Weilin Zhao, Xu Han, Tiejun Zhao, Wang Xu, Hailong Cao, Conghui Zhu*

Main category: cs.CL

TL;DR: Combining speculative decoding and quantization for LLM inference acceleration, but 4-bit quantization's memory benefits are offset by speculative decoding's computational load. A hierarchical framework improves speedup.


<details>
  <summary>Details</summary>
Motivation: To optimize memory-bound inference of large language models by integrating speculative decoding and quantization, addressing the trade-off between memory benefits and computational overhead.

Method: Investigates combining speculative decoding (EAGLE-2) with quantization, then proposes a hierarchical framework using a small model to convert tree-style drafts into sequence drafts.

Result: Hierarchical approach achieves 2.78× speedup for 4-bit Llama-3-70B on A100, outperforming EAGLE-2 by 1.31×.

Conclusion: Hierarchical speculative decoding effectively balances memory and computational trade-offs, enhancing performance for quantized models.

Abstract: Speculative decoding and quantization effectively accelerate memory-bound
inference of large language models. Speculative decoding mitigates the memory
bandwidth bottleneck by verifying multiple tokens within a single forward pass,
which increases computational effort. Quantization achieves this optimization
by compressing weights and activations into lower bit-widths and also reduces
computations via low-bit matrix multiplications. To further leverage their
strengths, we investigate the integration of these two techniques.
Surprisingly, experiments applying the advanced speculative decoding method
EAGLE-2 to various quantized models reveal that the memory benefits from 4-bit
weight quantization are diminished by the computational load from speculative
decoding. Specifically, verifying a tree-style draft incurs significantly more
time overhead than a single-token forward pass on 4-bit weight quantized
models. This finding led to our new speculative decoding design: a hierarchical
framework that employs a small model as an intermediate stage to turn
tree-style drafts into sequence drafts, leveraging the memory access benefits
of the target quantized model. Experimental results show that our hierarchical
approach achieves a 2.78$\times$ speedup across various tasks for the 4-bit
weight Llama-3-70B model on an A100 GPU, outperforming EAGLE-2 by 1.31$\times$.
Code available at https://github.com/AI9Stars/SpecMQuant.

</details>


### [67] [Breaking the Cloak! Unveiling Chinese Cloaked Toxicity with Homophone Graph and Toxic Lexicon](https://arxiv.org/pdf/2505.22184)
*Xuchen Ma, Jianxiang Yu, Wenming Shao, Bo Pang, Xiang Li*

Main category: cs.CL

TL;DR: C$^2$TU is a training-free, prompt-free method for unveiling Chinese cloaked toxic content, outperforming competitors by up to 71% in F1 score.


<details>
  <summary>Details</summary>
Motivation: Addressing the rise of disguised toxic content in Chinese social media, which existing methods fail to tackle.

Method: Uses substring matching with Chinese homo-graphs and toxic lexicons, then filters non-toxic candidates using BERT or LLMs.

Result: Achieves superior performance, outperforming competitors by up to 71% in F1 score and 35% in accuracy.

Conclusion: C$^2$TU effectively unveils cloaked toxicity in Chinese texts, setting a new benchmark for the task.

Abstract: Social media platforms have experienced a significant rise in toxic content,
including abusive language and discriminatory remarks, presenting growing
challenges for content moderation. Some users evade censorship by deliberately
disguising toxic words through homophonic cloak, which necessitates the task of
unveiling cloaked toxicity. Existing methods are mostly designed for English
texts, while Chinese cloaked toxicity unveiling has not been solved yet. To
tackle the issue, we propose C$^2$TU, a novel training-free and prompt-free
method for Chinese cloaked toxic content unveiling. It first employs substring
matching to identify candidate toxic words based on Chinese homo-graph and
toxic lexicon. Then it filters those candidates that are non-toxic and corrects
cloaks to be their corresponding toxicities. Specifically, we develop two model
variants for filtering, which are based on BERT and LLMs, respectively. For
LLMs, we address the auto-regressive limitation in computing word occurrence
probability and utilize the full semantic contexts of a text sequence to reveal
cloaked toxic words. Extensive experiments demonstrate that C$^2$TU can achieve
superior performance on two Chinese toxic datasets. In particular, our method
outperforms the best competitor by up to 71% on the F1 score and 35% on
accuracy, respectively.

</details>


### [68] [Let's Predict Sentence by Sentence](https://arxiv.org/pdf/2505.22202)
*Hyeonbin Hwang, Byeongguk Jeon, Seungone Kim, Jiyeon Kim, Hoyeon Chang, Sohee Yang, Seungpil Won, Dohaeng Lee, Youbin Ahn, Minjoon Seo*

Main category: cs.CL

TL;DR: The paper explores whether pretrained LMs can reason over structured semantic units (like sentences) instead of tokens. It introduces a framework adapting LMs to predict continuous sentence embeddings, showing competitive performance with CoT while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Human reasoning operates over higher-level abstractions (sentences, concepts), unlike token-by-token LM generation. The work investigates if LMs can transition to abstract reasoning using learned representations.

Method: A framework adapts pretrained LMs to predict continuous sentence embeddings. Two paradigms are explored: semantic (autoencoding) and contextual (next-sentence prediction) embeddings, evaluated under discretized and continuous inference regimes.

Result: Contextual embeddings with continuous inference match Chain-of-Thought performance while halving FLOPs. Scalability and modular adaptation are shown, and SentenceLens visualizes latent trajectories.

Conclusion: Pretrained LMs can effectively transition to abstract, structured reasoning in latent embedding spaces, demonstrating efficiency and adaptability.

Abstract: Autoregressive language models (LMs) generate one token at a time, yet human
reasoning operates over higher-level abstractions - sentences, propositions,
and concepts. This contrast raises a central question- Can LMs likewise learn
to reason over structured semantic units rather than raw token sequences? In
this work, we investigate whether pretrained LMs can be lifted into such
abstract reasoning spaces by building on their learned representations. We
present a framework that adapts a pretrained token-level LM to operate in
sentence space by autoregressively predicting continuous embeddings of next
sentences. We explore two embedding paradigms inspired by classical
representation learning: 1) semantic embeddings, learned via autoencoding to
preserve surface meaning; and 2) contextual embeddings, trained via
next-sentence prediction to encode anticipatory structure. We evaluate both
under two inference regimes: Discretized, which decodes each predicted
embedding into text before re-encoding; and Continuous, which reasons entirely
in embedding space for improved efficiency. Across four domains - mathematics,
logic, commonsense, and planning - contextual embeddings under continuous
inference show competitive performance with Chain-of-Thought (CoT) while
reducing inference-time FLOPs on average by half. We also present early signs
of scalability and modular adaptation. Finally, to visualize latent
trajectories, we introduce SentenceLens, a diagnostic tool that decodes
intermediate model states into interpretable sentences. Together, our results
indicate that pretrained LMs can effectively transition to abstract, structured
reasoning within latent embedding spaces.

</details>


### [69] [Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models](https://arxiv.org/pdf/2505.22232)
*Mehdi Ali, Manuel Brack, Max Lübbering, Elias Wendt, Abbas Goher Khan, Richard Rutmann, Alex Jude, Maurice Kraus, Alexander Arno Weber, Felix Stollenwerk, David Kaczér, Florian Mai, Lucie Flek, Rafet Sifa, Nicolas Flores-Herr, Joachim Köhler, Patrick Schramowski, Michael Fromm, Kristian Kersting*

Main category: cs.CL

TL;DR: JQL introduces a systematic method for curating high-quality multilingual data using lightweight annotators, outperforming heuristic methods and improving downstream model training.


<details>
  <summary>Details</summary>
Motivation: Limited availability of open-source multilingual datasets and the shortcomings of heuristic filtering methods restrict cross-lingual transferability and scalability.

Method: JQL leverages pretrained multilingual embeddings to create lightweight annotators, enabling efficient and scalable multilingual data curation.

Result: Empirical evaluation across 35 languages shows JQL outperforms heuristic methods like Fineweb2, enhancing data quality and retention rates.

Conclusion: JQL raises multilingual dataset standards, offering practical insights and resources for improved multilingual data curation.

Abstract: High-quality multilingual training data is essential for effectively
pretraining large language models (LLMs). Yet, the availability of suitable
open-source multilingual datasets remains limited. Existing state-of-the-art
datasets mostly rely on heuristic filtering methods, restricting both their
cross-lingual transferability and scalability. Here, we introduce JQL, a
systematic approach that efficiently curates diverse and high-quality
multilingual data at scale while significantly reducing computational demands.
JQL distills LLMs' annotation capabilities into lightweight annotators based on
pretrained multilingual embeddings. These models exhibit robust multilingual
and cross-lingual performance, even for languages and scripts unseen during
training. Evaluated empirically across 35 languages, the resulting annotation
pipeline substantially outperforms current heuristic filtering methods like
Fineweb2. JQL notably enhances downstream model training quality and increases
data retention rates. Our research provides practical insights and valuable
resources for multilingual data curation, raising the standards of multilingual
dataset development.

</details>


### [70] [A Linguistically Motivated Analysis of Intonational Phrasing in Text-to-Speech Systems: Revealing Gaps in Syntactic Sensitivity](https://arxiv.org/pdf/2505.22236)
*Charlotte Pouw, Afra Alishahi, Willem Zuidema*

Main category: cs.CL

TL;DR: TTS systems struggle with ambiguous syntactic boundaries but improve with simpler structures and finetuning.


<details>
  <summary>Details</summary>
Motivation: To understand how TTS systems handle syntactic sensitivity, especially intonational phrase boundaries, and improve their performance.

Method: Analyzed TTS systems using psycholinguistic methods, focusing on intonational phrase boundaries, and finetuned models on sentences without commas.

Result: TTS systems perform poorly on ambiguous syntactic boundaries but better on simpler structures. Finetuning improves intonation patterns.

Conclusion: Finetuning TTS models to rely less on superficial cues like commas enhances their ability to reflect underlying syntactic structure.

Abstract: We analyze the syntactic sensitivity of Text-to-Speech (TTS) systems using
methods inspired by psycholinguistic research. Specifically, we focus on the
generation of intonational phrase boundaries, which can often be predicted by
identifying syntactic boundaries within a sentence. We find that TTS systems
struggle to accurately generate intonational phrase boundaries in sentences
where syntactic boundaries are ambiguous (e.g., garden path sentences or
sentences with attachment ambiguity). In these cases, systems need superficial
cues such as commas to place boundaries at the correct positions. In contrast,
for sentences with simpler syntactic structures, we find that systems do
incorporate syntactic cues beyond surface markers. Finally, we finetune models
on sentences without commas at the syntactic boundary positions, encouraging
them to focus on more subtle linguistic cues. Our findings indicate that this
leads to more distinct intonation patterns that better reflect the underlying
structure.

</details>


### [71] [BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedical Domain](https://arxiv.org/pdf/2505.22240)
*Yunsoo Kim, Yusuf Abdulle, Honghan Wu*

Main category: cs.CL

TL;DR: BioHopR is a new benchmark for evaluating multi-hop reasoning in biomedical knowledge graphs, revealing performance gaps in current models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack the ability to evaluate multi-hop reasoning in biomedical domains, leaving critical challenges underexplored.

Method: BioHopR is built from PrimeKG and includes 1-hop and 2-hop reasoning tasks to assess biomedical reasoning.

Result: O3-mini outperforms other models but shows significant declines in multi-hop performance, highlighting domain challenges.

Conclusion: BioHopR sets a new standard for evaluating biomedical reasoning and identifies gaps between proprietary and open-source models.

Abstract: Biomedical reasoning often requires traversing interconnected relationships
across entities such as drugs, diseases, and proteins. Despite the increasing
prominence of large language models (LLMs), existing benchmarks lack the
ability to evaluate multi-hop reasoning in the biomedical domain, particularly
for queries involving one-to-many and many-to-many relationships. This gap
leaves the critical challenges of biomedical multi-hop reasoning underexplored.
To address this, we introduce BioHopR, a novel benchmark designed to evaluate
multi-hop, multi-answer reasoning in structured biomedical knowledge graphs.
Built from the comprehensive PrimeKG, BioHopR includes 1-hop and 2-hop
reasoning tasks that reflect real-world biomedical complexities.
  Evaluations of state-of-the-art models reveal that O3-mini, a proprietary
reasoning-focused model, achieves 37.93% precision on 1-hop tasks and 14.57% on
2-hop tasks, outperforming proprietary models such as GPT4O and open-source
biomedical models including HuatuoGPT-o1-70B and Llama-3.3-70B. However, all
models exhibit significant declines in multi-hop performance, underscoring the
challenges of resolving implicit reasoning steps in the biomedical domain. By
addressing the lack of benchmarks for multi-hop reasoning in biomedical domain,
BioHopR sets a new standard for evaluating reasoning capabilities and
highlights critical gaps between proprietary and open-source models while
paving the way for future advancements in biomedical LLMs.

</details>


### [72] [MRT at SemEval-2025 Task 8: Maximizing Recovery from Tables with Multiple Steps](https://arxiv.org/pdf/2505.22264)
*Maximiliano Hormazábal Lagos, Álvaro Bueno Saez, Héctor Cerezo-Costas, Pedro Alonso Doval, Jorge Alcalde Vesteiro*

Main category: cs.CL

TL;DR: The paper presents a method using Python code generation with LLMs to solve a question-answering task over tabular data, achieving a score of 70.50%.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of question-answering over tabular data in SemEval 2025 Task 8.

Method: Leverages LLMs for table understanding, generating natural language instructions, translating them to code, and handling errors. Uses open-source LLMs and optimized prompts for each step.

Result: Achieved a score of 70.50% for subtask 1.

Conclusion: The approach demonstrates effectiveness in solving the task through iterative code generation and error handling.

Abstract: In this paper we expose our approach to solve the \textit{SemEval 2025 Task
8: Question-Answering over Tabular Data} challenge. Our strategy leverages
Python code generation with LLMs to interact with the table and get the answer
to the questions. The process is composed of multiple steps: understanding the
content of the table, generating natural language instructions in the form of
steps to follow in order to get the answer, translating these instructions to
code, running it and handling potential errors or exceptions. These steps use
open source LLMs and fine grained optimized prompts for each task (step). With
this approach, we achieved a score of $70.50\%$ for subtask 1.

</details>


### [73] [Comprehensive Evaluation on Lexical Normalization: Boundary-Aware Approaches for Unsegmented Languages](https://arxiv.org/pdf/2505.22273)
*Shohei Higashiyama, Masao Utiyama*

Main category: cs.CL

TL;DR: The paper addresses lexical normalization for informal text in unsegmented languages, focusing on Japanese. It introduces a dataset, methods using pretrained models, and multi-perspective evaluations, showing encoder-only and decoder-only approaches perform well.


<details>
  <summary>Details</summary>
Motivation: The lack of comprehensive evaluations in lexical normalization for informal text, especially in unsegmented languages like Japanese, motivated this research.

Method: The study creates a large-scale Japanese normalization dataset and develops methods using state-of-the-art pretrained models (encoder-only and decoder-only).

Result: Experiments demonstrate that both encoder-only and decoder-only approaches achieve high accuracy and efficiency.

Conclusion: The research provides valuable insights and tools for lexical normalization in unsegmented languages, with promising results from modern pretrained models.

Abstract: Lexical normalization research has sought to tackle the challenge of
processing informal expressions in user-generated text, yet the absence of
comprehensive evaluations leaves it unclear which methods excel across multiple
perspectives. Focusing on unsegmented languages, we make three key
contributions: (1) creating a large-scale, multi-domain Japanese normalization
dataset, (2) developing normalization methods based on state-of-the-art
pretrained models, and (3) conducting experiments across multiple evaluation
perspectives. Our experiments show that both encoder-only and decoder-only
approaches achieve promising results in both accuracy and efficiency.

</details>


### [74] [Natural Language Processing in Support of Evidence-based Medicine: A Scoping Review](https://arxiv.org/pdf/2505.22280)
*Zihan Xu, Haotian Ma, Gongbo Zhang, Yihao Ding, Chunhua Weng, Yifan Peng*

Main category: cs.CL

TL;DR: A survey of 129 studies explores NLP's role in EBM, highlighting its potential to improve clinical decision-making by aiding evidence extraction, synthesis, and workflow efficiency.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of medical literature and high curation costs necessitate NLP methods to streamline evidence-based medicine processes.

Method: Systematic review of 129 research studies on NLP applications in EBM, focusing on the five EBM steps: Ask, Acquire, Appraise, Apply, and Assess.

Result: NLP is pivotal in enhancing EBM by improving evidence extraction, synthesis, and clinical workflow, though current limitations exist.

Conclusion: NLP holds transformative potential for EBM, with future research needed to address limitations and further refine its applications.

Abstract: Evidence-based medicine (EBM) is at the forefront of modern healthcare,
emphasizing the use of the best available scientific evidence to guide clinical
decisions. Due to the sheer volume and rapid growth of medical literature and
the high cost of curation, there is a critical need to investigate Natural
Language Processing (NLP) methods to identify, appraise, synthesize, summarize,
and disseminate evidence in EBM. This survey presents an in-depth review of 129
research studies on leveraging NLP for EBM, illustrating its pivotal role in
enhancing clinical decision-making processes. The paper systematically explores
how NLP supports the five fundamental steps of EBM -- Ask, Acquire, Appraise,
Apply, and Assess. The review not only identifies current limitations within
the field but also proposes directions for future research, emphasizing the
potential for NLP to revolutionize EBM by refining evidence extraction,
evidence synthesis, appraisal, summarization, enhancing data comprehensibility,
and facilitating a more efficient clinical workflow.

</details>


### [75] [Compensating for Data with Reasoning: Low-Resource Machine Translation with LLMs](https://arxiv.org/pdf/2505.22293)
*Samuel Frontull, Thomas Ströhle*

Main category: cs.CL

TL;DR: Fragment-Shot Prompting improves low-resource language translation by segmenting input and retrieving examples based on syntactic coverage, with Pivoted Fragment-Shot enabling translation without parallel data. Stronger models enhance results, while prompt engineering has limited impact for high-resource targets.


<details>
  <summary>Details</summary>
Motivation: Address challenges of using LLMs for low-resource language translation, focusing on prompt engineering and in-context learning.

Method: Introduces Fragment-Shot Prompting and Pivoted Fragment-Shot, evaluated on GPT-3.5, GPT-4o, and other models for Italian-Ladin translation.

Result: Fragment-Shot improves translation quality, especially with stronger models; Pivoted Fragment-Shot works well without parallel data. Prompt engineering has minimal impact for high-resource targets.

Conclusion: Fragment-Shot methods are effective for low-resource languages, with model reasoning ability playing a key role. Public release of code and corpora supports further research.

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
multilingual machine translation, sometimes even outperforming traditional
neural systems. However, previous research has highlighted the challenges of
using LLMs, particularly with prompt engineering, for low-resource languages.
In this work, we introduce Fragment-Shot Prompting, a novel in-context learning
method that segments input and retrieves translation examples based on
syntactic coverage, along with Pivoted Fragment-Shot, an extension that enables
translation without direct parallel data. We evaluate these methods using
GPT-3.5, GPT-4o, o1-mini, LLaMA-3.3, and DeepSeek-R1 for translation between
Italian and two Ladin variants, revealing three key findings: (1) Fragment-Shot
Prompting is effective for translating into and between the studied
low-resource languages, with syntactic coverage positively correlating with
translation quality; (2) Models with stronger reasoning abilities make more
effective use of retrieved knowledge, generally produce better translations,
and enable Pivoted Fragment-Shot to significantly improve translation quality
between the Ladin variants; and (3) prompt engineering offers limited, if any,
improvements when translating from a low-resource to a high-resource language,
where zero-shot prompting already yields satisfactory results. We publicly
release our code and the retrieval corpora.

</details>


### [76] [360-LLaMA-Factory: Plug & Play Sequence Parallelism for Long Post-Training](https://arxiv.org/pdf/2505.22296)
*Haosheng Zou, Xiaowei Lv, Shousheng Jia, Xiangzheng Zhang*

Main category: cs.CL

TL;DR: 360-LLaMA-Factory introduces sequence parallelism, enhancing its utility in various models and frameworks.


<details>
  <summary>Details</summary>
Motivation: To improve model efficiency and scalability by integrating sequence parallelism into LLaMA-Factory.

Method: Implementation of different sequence parallel modes in 360-LLaMA-Factory.

Result: Widely adopted in models like Light-R1, TinyR1, and Kaggle AIMO math models, as well as corporate training frameworks.

Conclusion: The report highlights the effectiveness and versatility of 360-LLaMA-Factory's sequence parallelism.

Abstract: Adding sequence parallelism into LLaMA-Factory, we open-sourced
360-LLaMA-Factory at https://github.com/Qihoo360/360-LLaMA-Factory.
360-LLaMA-Factory has received wide recognition and used in models such as
Light-R1 arXiv:2503.10460, TinyR1 arXiv:2503.04872, Kaggle AIMO math models and
also in large companies' training frameworks. This technical report delves
deeper into the different sequence parallel modes behind 360-LLaMA-Factory and
discusses our implementation insights.

</details>


### [77] [Adaptive Detoxification: Safeguarding General Capabilities of LLMs through Toxicity-Aware Knowledge Editing](https://arxiv.org/pdf/2505.22298)
*Yifan Lu, Jing Li, Yigeng Zhou, Yihui Zhang, Wenya Wang, Xiucheng Li, Meishan Zhang, Fangming Liu, Jun Yu, Min Zhang*

Main category: cs.CL

TL;DR: ToxEdit is a toxicity-aware knowledge editing method for LLMs that dynamically detects toxic activation patterns and routes computations adaptively, outperforming existing methods in detoxification while preserving general capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing detoxification methods for LLMs are ineffective against adversarial inputs without explicit entities and suffer from over-editing, compromising model performance.

Method: ToxEdit dynamically detects toxic activation patterns during forward propagation and routes computations through adaptive inter-layer pathways to mitigate toxicity.

Result: ToxEdit outperforms state-of-the-art methods in detoxification and preserves LLMs' general capabilities, as demonstrated in experiments.

Conclusion: ToxEdit provides an effective solution for LLM detoxification without compromising overall performance, validated by enhanced evaluation benchmarks.

Abstract: Large language models (LLMs) exhibit impressive language capabilities but
remain vulnerable to malicious prompts and jailbreaking attacks. Existing
knowledge editing methods for LLM detoxification face two major challenges.
First, they often rely on entity-specific localization, making them ineffective
against adversarial inputs without explicit entities. Second, these methods
suffer from over-editing, where detoxified models reject legitimate queries,
compromising overall performance. In this paper, we propose ToxEdit, a
toxicity-aware knowledge editing approach that dynamically detects toxic
activation patterns during forward propagation. It then routes computations
through adaptive inter-layer pathways to mitigate toxicity effectively. This
design ensures precise toxicity mitigation while preserving LLMs' general
capabilities. To more accurately assess over-editing, we also enhance the
SafeEdit benchmark by incorporating instruction-following evaluation tasks.
Experimental results on multiple LLMs demonstrate that our ToxEdit outperforms
previous state-of-the-art methods in both detoxification performance and
safeguarding general capabilities of LLMs.

</details>


### [78] [If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?](https://arxiv.org/pdf/2505.22318)
*Ishwar B Balappanawar, Vamshi Krishna Bonagiri, Anish R Joishy, Manas Gaur, Krishnaprasad Thirunarayan, Ponnurangam Kumaraguru*

Main category: cs.CL

TL;DR: The paper introduces CounterLogic, a dataset to evaluate LLMs' logical reasoning in counterfactual scenarios, showing a 27% accuracy drop. A method called Self-Segregate improves performance, reducing the gap to 11% and boosting accuracy by 7.5%.


<details>
  <summary>Details</summary>
Motivation: To investigate LLMs' struggles with reasoning in knowledge-conflicting contexts and propose solutions.

Method: Introduces CounterLogic dataset and Self-Segregate prompting method for metacognitive awareness.

Result: LLMs' accuracy drops by 27% in counterfactual scenarios; Self-Segregate reduces this gap to 11% and increases overall accuracy by 7.5%.

Conclusion: The findings highlight the need for methods like Self-Segregate to enhance LLMs' reasoning in real-world applications with conflicting information.

Abstract: Large Language Models (LLMs) demonstrate impressive reasoning capabilities in
familiar contexts, but struggle when the context conflicts with their
parametric knowledge. To investigate this phenomenon, we introduce
CounterLogic, a dataset containing 1,800 examples across 9 logical schemas,
explicitly designed to evaluate logical reasoning through counterfactual
(hypothetical knowledge-conflicting) scenarios. Our systematic evaluation of 11
LLMs across 6 different datasets reveals a consistent performance degradation,
with accuracies dropping by 27% on average when reasoning through
counterfactual information. We propose Self-Segregate, a prompting method
enabling metacognitive awareness (explicitly identifying knowledge conflicts)
before reasoning. Our method dramatically narrows the average performance gaps
from 27% to just 11%, while significantly increasing the overall accuracy
(+7.5%). We discuss the implications of these findings and draw parallels to
human cognitive processes, particularly on how humans disambiguate conflicting
information during reasoning tasks. Our findings offer practical insights for
understanding and enhancing LLMs reasoning capabilities in real-world
applications, especially where models must logically reason independently of
their factual knowledge.

</details>


### [79] [Advancing Expert Specialization for Better MoE](https://arxiv.org/pdf/2505.22323)
*Hongcan Guo, Haolang Lu, Guoshun Nan, Bolun Chu, Jialin Zhuang, Yuan Yang, Wenhao Che, Sicong Leng, Qimei Cui, Xudong Jiang*

Main category: cs.CL

TL;DR: The paper proposes two new objectives—orthogonality loss and variance loss—to improve expert specialization in Mixture-of-Experts (MoE) models, enhancing performance by up to 23.79% without architectural changes.


<details>
  <summary>Details</summary>
Motivation: The commonly used auxiliary load balancing loss in MoE models leads to expert overlap and uniform routing, hindering specialization and degrading performance.

Method: Introduces orthogonality loss to encourage distinct token processing and variance loss for discriminative routing, compatible with existing auxiliary loss.

Result: Significantly enhances expert specialization, improving MoE baselines by up to 23.79% while maintaining load balancing.

Conclusion: The proposed method effectively addresses expert overlap and routing issues, boosting performance without additional components.

Abstract: Mixture-of-Experts (MoE) models enable efficient scaling of large language
models (LLMs) by activating only a subset of experts per input. However, we
observe that the commonly used auxiliary load balancing loss often leads to
expert overlap and overly uniform routing, which hinders expert specialization
and degrades overall performance during post-training. To address this, we
propose a simple yet effective solution that introduces two complementary
objectives: (1) an orthogonality loss to encourage experts to process distinct
types of tokens, and (2) a variance loss to encourage more discriminative
routing decisions. Gradient-level analysis demonstrates that these objectives
are compatible with the existing auxiliary loss and contribute to optimizing
the training process. Experimental results over various model architectures and
across multiple benchmarks show that our method significantly enhances expert
specialization. Notably, our method improves classic MoE baselines with
auxiliary loss by up to 23.79%, while also maintaining load balancing in
downstream tasks, without any architectural modifications or additional
components. We will release our code to contribute to the community.

</details>


### [80] [NLP for Social Good: A Survey of Challenges, Opportunities, and Responsible Deployment](https://arxiv.org/pdf/2505.22327)
*Antonia Karamolegkou, Angana Borah, Eunjung Cho, Sagnik Ray Choudhury, Martina Galletti, Rajarshi Ghosh, Pranav Gupta, Oana Ignat, Priyanka Kargupta, Neema Kotonya, Hemank Lamba, Sun-Joo Lee, Arushi Mangla, Ishani Mondal, Deniz Nazarova, Poli Nemkova, Dina Pisarevskaya, Naquee Rizwan, Nazanin Sabri, Dominik Stammbach, Anna Steinberg, David Tomás, Steven R Wilson, Bowen Yi, Jessica H Zhu, Arkaitz Zubiaga, Anders Søgaard, Alexander Fraser, Zhijing Jin, Rada Mihalcea, Joel R. Tetreault, Daryna Dementieva*

Main category: cs.CL

TL;DR: The paper discusses the need for responsible and intentional deployment of NLP, focusing on societal challenges and equitable progress in NLP4SG research.


<details>
  <summary>Details</summary>
Motivation: Address the growing need for responsible NLP deployment and its role in societal challenges, inspired by AI for Social Good.

Method: Cross-disciplinary analysis of social goals and emerging risks to identify research directions.

Result: Highlights promising research directions and challenges for equitable NLP4SG progress.

Conclusion: Emphasizes the importance of responsible and equitable advancements in NLP for societal benefit.

Abstract: Recent advancements in large language models (LLMs) have unlocked
unprecedented possibilities across a range of applications. However, as a
community, we believe that the field of Natural Language Processing (NLP) has a
growing need to approach deployment with greater intentionality and
responsibility. In alignment with the broader vision of AI for Social Good
(Toma\v{s}ev et al., 2020), this paper examines the role of NLP in addressing
pressing societal challenges. Through a cross-disciplinary analysis of social
goals and emerging risks, we highlight promising research directions and
outline challenges that must be addressed to ensure responsible and equitable
progress in NLP4SG research.

</details>


### [81] [Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start](https://arxiv.org/pdf/2505.22334)
*Lai Wei, Yuting Li, Kaipeng Zheng, Chen Wang, Yue Wang, Linghe Kong, Lichao Sun, Weiran Huang*

Main category: cs.CL

TL;DR: The paper shows that self-correction patterns in multimodal LLMs exist before RL training and introduces a two-stage approach (SFT + RL) to enhance reasoning, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To investigate and improve multimodal reasoning in LLMs by combining supervised fine-tuning and reinforcement learning.

Method: A two-stage approach: (1) supervised fine-tuning (SFT) for structured reasoning, followed by (2) reinforcement learning (GRPO) for refinement.

Result: The combined method outperforms SFT-only and RL-only, with 7B and 3B models achieving top performance on benchmarks like MathVista and We-Math.

Conclusion: The work offers practical guidance for advancing multimodal reasoning models, demonstrating the effectiveness of the two-stage approach.

Abstract: Recent advancements in large language models (LLMs) have demonstrated
impressive chain-of-thought reasoning capabilities, with reinforcement learning
(RL) playing a crucial role in this progress. While "aha moment"
patterns--where models exhibit self-correction through reflection--are often
attributed to emergent properties from RL, we first demonstrate that these
patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not
necessarily correlate with improved reasoning performance. Building on these
insights, we present a comprehensive study on enhancing multimodal reasoning
through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start
with structured chain-of-thought reasoning patterns, followed by (2)
reinforcement learning via GRPO to further refine these capabilities. Our
extensive experiments show that this combined approach consistently outperforms
both SFT-only and RL-only methods across challenging multimodal reasoning
benchmarks. The resulting models achieve state-of-the-art performance among
open-source MLLMs at both 3B and 7B scales, with our 7B model showing
substantial improvements over base models (e.g., 66.3 %$\rightarrow$73.4 % on
MathVista, 62.9 %$\rightarrow$70.4 % on We-Math) and our 3B model achieving
performance competitive with several 7B models. Overall, this work provides
practical guidance for building advanced multimodal reasoning models. Our code
is available at https://github.com/waltonfuture/RL-with-Cold-Start.

</details>


### [82] [Text2Grad: Reinforcement Learning from Natural Language Feedback](https://arxiv.org/pdf/2505.22338)
*Hanyang Wang, Lu Wang, Chaoyun Zhang, Tianjun Mao, Si Qin, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang*

Main category: cs.CL

TL;DR: Text2Grad introduces a reinforcement-learning paradigm converting textual feedback into span-level gradients for precise model adjustments, outperforming traditional RLHF methods.


<details>
  <summary>Details</summary>
Motivation: Traditional RLHF uses coarse scalar rewards, masking fine-grained reasons for success/failure, leading to slow learning. Text2Grad aims to improve interpretability and precision by leveraging textual feedback.

Method: Text2Grad aligns feedback phrases with token spans, converts them into differentiable rewards, and performs gradient updates on relevant policy portions. It includes a feedback-annotation pipeline, a fine-grained reward model, and a span-level policy optimizer.

Result: Text2Grad outperforms scalar-reward RL and prompt-only baselines in summarization, code generation, and question answering, achieving higher task metrics and better interpretability.

Conclusion: Natural-language feedback, when converted to gradients, is a powerful signal for fine-grained policy optimization, as demonstrated by Text2Grad's success.

Abstract: Traditional RLHF optimizes language models with coarse, scalar rewards that
mask the fine-grained reasons behind success or failure, leading to slow and
opaque learning. Recent work augments RL with textual critiques through
prompting or reflection, improving interpretability but leaving model
parameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm
that turns free-form textual feedback into span-level gradients. Given human
(or programmatic) critiques, Text2Grad aligns each feedback phrase with the
relevant token spans, converts these alignments into differentiable reward
signals, and performs gradient updates that directly refine the offending
portions of the model's policy. This yields precise, feedback-conditioned
adjustments instead of global nudges. Text2Grad is realized through three
components: (1) a high-quality feedback-annotation pipeline that pairs
critiques with token spans; (2) a fine-grained reward model that predicts
span-level reward on answer while generating explanatory critiques; and (3) a
span-level policy optimizer that back-propagates natural-language gradients.
Across summarization, code generation, and question answering, Text2Grad
consistently surpasses scalar-reward RL and prompt-only baselines, providing
both higher task metrics and richer interpretability. Our results demonstrate
that natural-language feedback, when converted to gradients, is a powerful
signal for fine-grained policy optimization. The code for our method is
available at https://github.com/microsoft/Text2Grad

</details>


### [83] [LLMs Struggle to Reject False Presuppositions when Misinformation Stakes are High](https://arxiv.org/pdf/2505.22354)
*Judith Sieker, Clara Lachenmaier, Sina Zarrieß*

Main category: cs.CL

TL;DR: The paper investigates how LLMs handle false presuppositions, revealing their struggles in detecting and correcting misleading assumptions, especially in political contexts.


<details>
  <summary>Details</summary>
Motivation: To understand if LLMs, like humans, fail to detect false presuppositions, which can embed misinformation, particularly in high-stakes political scenarios.

Method: A systematic linguistic presupposition analysis is used to test LLMs (GPT-4-o, LLama-3-8B, Mistral-7B-v03) under varying conditions like linguistic construction and political context.

Result: LLMs struggle to recognize false presuppositions, with performance varying by condition, indicating potential reinforcement of political misinformation.

Conclusion: Linguistic presupposition analysis is effective in uncovering how LLMs may inadvertently reinforce misinformation, highlighting a critical area for improvement.

Abstract: This paper examines how LLMs handle false presuppositions and whether certain
linguistic factors influence their responses to falsely presupposed content.
Presuppositions subtly introduce information as given, making them highly
effective at embedding disputable or false information. This raises concerns
about whether LLMs, like humans, may fail to detect and correct misleading
assumptions introduced as false presuppositions, even when the stakes of
misinformation are high. Using a systematic approach based on linguistic
presupposition analysis, we investigate the conditions under which LLMs are
more or less sensitive to adopt or reject false presuppositions. Focusing on
political contexts, we examine how factors like linguistic construction,
political party, and scenario probability impact the recognition of false
presuppositions. We conduct experiments with a newly created dataset and
examine three LLMs: OpenAI's GPT-4-o, Meta's LLama-3-8B, and MistralAI's
Mistral-7B-v03. Our results show that the models struggle to recognize false
presuppositions, with performance varying by condition. This study highlights
that linguistic presupposition analysis is a valuable tool for uncovering the
reinforcement of political misinformation in LLM responses.

</details>


### [84] [Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition](https://arxiv.org/pdf/2505.22375)
*Hanting Chen, Yasheng Wang, Kai Han, Dong Li, Lin Li, Zhenni Bi, Jinpeng Li, Haoyu Wang, Fei Mi, Mingjian Zhu, Bin Wang, Kaikai Song, Yifei Fu, Xu He, Yu Luo, Chong Zhu, Quan He, Xueyu Wu, Wei He, Hailin Hu, Yehui Tang, Dacheng Tao, Xinghao Chen, Yunhe Wang, Other Contributors*

Main category: cs.CL

TL;DR: Pangu Embedded is an efficient LLM reasoner with fast/slow thinking modes, optimized for computational cost and latency, outperforming similar models.


<details>
  <summary>Details</summary>
Motivation: Address high computational costs and inference latency in reasoning-optimized LLMs.

Method: Two-stage training: iterative distillation with model merging, followed by RL with MARS. Dual-system framework for fast/slow modes.

Result: Outperforms Qwen3-8B and GLM4-9B on benchmarks like AIME 2024, GPQA, and LiveCodeBench.

Conclusion: Pangu Embedded offers a unified, deployable LLM reasoner with balanced latency and reasoning depth.

Abstract: This work presents Pangu Embedded, an efficient Large Language Model (LLM)
reasoner developed on Ascend Neural Processing Units (NPUs), featuring flexible
fast and slow thinking capabilities. Pangu Embedded addresses the significant
computational costs and inference latency challenges prevalent in existing
reasoning-optimized LLMs. We propose a two-stage training framework for its
construction. In Stage 1, the model is finetuned via an iterative distillation
process, incorporating inter-iteration model merging to effectively aggregate
complementary knowledge. This is followed by reinforcement learning on Ascend
clusters, optimized by a latency-tolerant scheduler that combines stale
synchronous parallelism with prioritized data queues. The RL process is guided
by a Multi-source Adaptive Reward System (MARS), which generates dynamic,
task-specific reward signals using deterministic metrics and lightweight LLM
evaluators for mathematics, coding, and general problem-solving tasks. Stage 2
introduces a dual-system framework, endowing Pangu Embedded with a "fast" mode
for routine queries and a deeper "slow" mode for complex inference. This
framework offers both manual mode switching for user control and an automatic,
complexity-aware mode selection mechanism that dynamically allocates
computational resources to balance latency and reasoning depth. Experimental
results on benchmarks including AIME 2024, GPQA, and LiveCodeBench demonstrate
that Pangu Embedded with 7B parameters, outperforms similar-size models like
Qwen3-8B and GLM4-9B. It delivers rapid responses and state-of-the-art
reasoning quality within a single, unified model architecture, highlighting a
promising direction for developing powerful yet practically deployable LLM
reasoners.

</details>


### [85] [RAG-Zeval: Towards Robust and Interpretable Evaluation on RAG Responses through End-to-End Rule-Guided Reasoning](https://arxiv.org/pdf/2505.22430)
*Kun Li, Yunxiang Li, Tianhua Zhang, Hongyin Luo, Xixin Wu, James Glass, Helen Meng*

Main category: cs.CL

TL;DR: RAG-Zeval is a novel framework for evaluating RAG systems using rule-guided reasoning and reinforcement learning, outperforming larger LLMs with better interpretability and lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based evaluation frameworks are resource-intensive and underutilize reasoning capabilities, necessitating a more efficient and interpretable solution.

Method: RAG-Zeval trains compact evaluators with reinforcement learning, using rule-guided reasoning and ranking-based rewards to generate assessments in one pass.

Result: RAG-Zeval achieves the strongest correlation with human judgments and outperforms larger LLM baselines, while being more interpretable.

Conclusion: RAG-Zeval offers a scalable, efficient, and interpretable solution for evaluating RAG systems, reducing reliance on resource-intensive models.

Abstract: Robust evaluation is critical for deploying trustworthy retrieval-augmented
generation (RAG) systems. However, current LLM-based evaluation frameworks
predominantly rely on directly prompting resource-intensive models with complex
multi-stage prompts, underutilizing models' reasoning capabilities and
introducing significant computational cost. In this paper, we present RAG-Zeval
(RAG-Zero Evaluator), a novel end-to-end framework that formulates faithfulness
and correctness evaluation as a rule-guided reasoning task. Our approach trains
evaluators with reinforcement learning, facilitating compact models to generate
comprehensive and sound assessments with detailed explanation in one-pass. We
introduce a ranking-based outcome reward mechanism, using preference judgments
rather than absolute scores, to address the challenge of obtaining precise
pointwise reward signals. To this end, we synthesize the ranking references by
generating quality-controlled responses with zero human annotation. Experiments
demonstrate RAG-Zeval's superior performance, achieving the strongest
correlation with human judgments and outperforming baselines that rely on LLMs
with 10-100 times more parameters. Our approach also exhibits superior
interpretability in response evaluation.

</details>


### [86] [Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO](https://arxiv.org/pdf/2505.22453)
*Lai Wei, Yuting Li, Chen Wang, Yue Wang, Linghe Kong, Weiran Huang, Lichao Sun*

Main category: cs.CL

TL;DR: MM-UPT introduces an unsupervised post-training framework for MLLMs using GRPO and self-rewarding, achieving significant performance improvements without labeled data.


<details>
  <summary>Details</summary>
Motivation: Supervised methods like SFT and RL are costly and unsustainable due to manual annotation needs. Unsupervised alternatives are complex.

Method: MM-UPT leverages GRPO with a self-rewarding mechanism via majority voting over sampled responses, avoiding external supervision.

Result: MM-UPT improves Qwen2.5-VL-7B performance (e.g., 66.3%→72.9% on MathVista) and outperforms unsupervised baselines, nearing supervised GRPO results.

Conclusion: MM-UPT enables scalable, autonomous MLLM enhancement without supervision, with synthetic questions further boosting performance.

Abstract: Improving Multi-modal Large Language Models (MLLMs) in the post-training
stage typically relies on supervised fine-tuning (SFT) or reinforcement
learning (RL). However, these supervised methods require expensive and manually
annotated multi-modal data--an ultimately unsustainable resource. While recent
efforts have explored unsupervised post-training, their methods are complex and
difficult to iterate. In this work, we are the first to investigate the use of
GRPO, a stable and scalable online RL algorithm, for enabling continual
self-improvement without any external supervision. We propose MM-UPT, a simple
yet effective framework for unsupervised post-training of MLLMs. MM-UPT builds
upon GRPO, replacing traditional reward signals with a self-rewarding mechanism
based on majority voting over multiple sampled responses. Our experiments
demonstrate that MM-UPT significantly improves the reasoning ability of
Qwen2.5-VL-7B (e.g., 66.3 %$\rightarrow$72.9 % on MathVista, 62.9
%$\rightarrow$68.7 % on We-Math), using standard dataset without ground truth
labels. MM-UPT also outperforms prior unsupervised baselines and even
approaches the results of supervised GRPO. Furthermore, we show that
incorporating synthetic questions, generated solely by MLLM itself, can boost
performance as well, highlighting a promising approach for scalable
self-improvement. Overall, MM-UPT offers a new paradigm for continual,
autonomous enhancement of MLLMs in the absence of external supervision. Our
code is available at https://github.com/waltonfuture/MM-UPT.

</details>


### [87] [EvolveSearch: An Iterative Self-Evolving Search Agent](https://arxiv.org/pdf/2505.22501)
*Dingchu Zhang, Yida Zhao, Jialong Wu, Baixuan Li, Wenbiao Yin, Liwen Zhang, Yong Jiang, Yufeng Li, Kewei Tu, Pengjun Xie, Fei Huang*

Main category: cs.CL

TL;DR: EvolveSearch combines SFT and RL to improve LLM web search without human-annotated data, achieving a 4.7% performance boost over state-of-the-art on MHQA benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current LLM web search methods face challenges: SFT struggles with open-search data, and RL lacks efficiency. EvolveSearch aims to overcome these limitations.

Method: Proposes EvolveSearch, an iterative self-evolution framework integrating SFT and RL for enhanced web search capabilities.

Result: Achieves a 4.7% average improvement over state-of-the-art on seven MHQA benchmarks.

Conclusion: EvolveSearch enables self-evolution in open web search domains, outperforming existing methods.

Abstract: The rapid advancement of large language models (LLMs) has transformed the
landscape of agentic information seeking capabilities through the integration
of tools such as search engines and web browsers. However, current mainstream
approaches for enabling LLM web search proficiency face significant challenges:
supervised fine-tuning struggles with data production in open-search domains,
while RL converges quickly, limiting their data utilization efficiency. To
address these issues, we propose EvolveSearch, a novel iterative self-evolution
framework that combines SFT and RL to enhance agentic web search capabilities
without any external human-annotated reasoning data. Extensive experiments on
seven multi-hop question-answering (MHQA) benchmarks demonstrate that
EvolveSearch consistently improves performance across iterations, ultimately
achieving an average improvement of 4.7\% over the current state-of-the-art
across seven benchmarks, opening the door to self-evolution agentic
capabilities in open web search domains.

</details>


### [88] [Emotion-o1: Adaptive Long Reasoning for Emotion Understanding in LLMs](https://arxiv.org/pdf/2505.22548)
*Changhao Song, Yazhou Zhang, Peng Zhang*

Main category: cs.CL

TL;DR: A task-adaptive reasoning framework using DeepSeek-R1 improves emotion understanding by generating variable-length reasoning chains, outperforming fixed-length CoT methods in accuracy and F1 scores.


<details>
  <summary>Details</summary>
Motivation: Current methods for emotion understanding use fixed-length CoT reasoning, which doesn't adapt to varying emotional complexity.

Method: Proposes a framework combining fine-tuning and reinforcement learning with a composite reward function for adaptive reasoning depth and diverse paths.

Result: Achieves significant improvements: up to 3.56% F1 for basic tasks and 37.95% F1 for advanced tasks.

Conclusion: Bridges the gap between rigid CoT reasoning and emotional complexity through adaptive-depth analysis.

Abstract: Emotion understanding includes basic tasks (e.g., sentiment/emotion
classification) and advanced tasks (e.g., sarcasm/humor detection). Current
methods rely on fixed-length CoT reasoning, failing to adapt to the varying
complexity of emotions. We propose a task-adaptive reasoning framework that
employs DeepSeek-R1 to generate variable-length reasoning chains for different
emotion tasks. By combining fine-tuning with reinforcement learning, we design
a composite reward function that balances four objectives: prediction accuracy,
adaptive reasoning depth control, structural diversity in reasoning paths, and
suppression of repetitive logic. This approach achieves dynamic
context-sensitive inference while enabling LLMs to autonomously develop deep
reasoning capabilities. Experimental results demonstrate consistent
improvements in both Acc and F1 scores across four tasks: emotion, sentiment,
humor, and sarcasm. Notably, peak enhancements reached 3.56% F1 (2.76% Acc) for
basic tasks and 37.95% F1 (23.14% Acc) for advanced tasks. Our work bridges
rigid CoT reasoning and emotional complexity through adaptive-depth analysis.

</details>


### [89] [ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation with Lightweight Specialized LLM](https://arxiv.org/pdf/2505.22552)
*Hoang Pham, Thanh-Do Nguyen, Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: ClaimPKG integrates knowledge graphs (KGs) with LLMs for claim verification, outperforming baselines by 9%-12% accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on unstructured text, limiting KG utilization, and LLMs struggle with KG reasoning without adaptation.

Method: ClaimPKG uses a lightweight LLM to represent claims as pseudo-subgraphs, retrieves relevant KG subgraphs, and processes them with a general-purpose LLM for verdicts.

Result: Achieves state-of-the-art performance on FactKG and generalizes to unstructured datasets like HoVer and FEVEROUS.

Conclusion: ClaimPKG effectively combines KG knowledge with LLM reasoning, enhancing claim verification accuracy and generalizability.

Abstract: Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of
large language models (LLMs) is an emerging research challenge in claim
verification. While KGs provide structured, semantically rich representations
well-suited for reasoning, most existing verification methods rely on
unstructured text corpora, limiting their ability to effectively leverage KGs.
Additionally, despite possessing strong reasoning abilities, modern LLMs
struggle with multi-step modular pipelines and reasoning over KGs without
adaptation. To address these challenges, we propose ClaimPKG, an end-to-end
framework that seamlessly integrates LLM reasoning with structured knowledge
from KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight,
specialized LLM to represent the input claim as pseudo-subgraphs, guiding a
dedicated subgraph retrieval module to identify relevant KG subgraphs. These
retrieved subgraphs are then processed by a general-purpose LLM to produce the
final verdict and justification. Extensive experiments on the FactKG dataset
demonstrate that ClaimPKG achieves state-of-the-art performance, outperforming
strong baselines in this research field by 9%-12% accuracy points across
multiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability
to unstructured datasets such as HoVer and FEVEROUS, effectively combining
structured knowledge from KGs with LLM reasoning across various LLM backbones.

</details>


### [90] [Do Large Language Models Think Like the Brain? Sentence-Level Evidence from fMRI and Hierarchical Embeddings](https://arxiv.org/pdf/2505.22563)
*Yu Lei, Xingyang Ge, Yi Zhang, Yiming Yang, Bolei Ma*

Main category: cs.CL

TL;DR: The study investigates if LLMs and the human brain share computational principles, finding that higher-performing LLMs develop brain-like hierarchical representations, especially in semantic abstraction.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs' brain-like patterns result from scaling or deeper alignment with human language processing.

Method: Comparing hierarchical embeddings from 14 LLMs with fMRI data during human sentence comprehension, using neural prediction models.

Result: Higher-performing LLMs show stronger brain-like hierarchical representations, particularly in semantic abstraction.

Conclusion: LLM performance improvements drive alignment with human brain hierarchies, suggesting deeper computational convergence.

Abstract: Understanding whether large language models (LLMs) and the human brain
converge on similar computational principles remains a fundamental and
important question in cognitive neuroscience and AI. Do the brain-like patterns
observed in LLMs emerge simply from scaling, or do they reflect deeper
alignment with the architecture of human language processing? This study
focuses on the sentence-level neural mechanisms of language models,
systematically investigating how hierarchical representations in LLMs align
with the dynamic neural responses during human sentence comprehension. By
comparing hierarchical embeddings from 14 publicly available LLMs with fMRI
data collected from participants, who were exposed to a naturalistic narrative
story, we constructed sentence-level neural prediction models to precisely
identify the model layers most significantly correlated with brain region
activations. Results show that improvements in model performance drive the
evolution of representational architectures toward brain-like hierarchies,
particularly achieving stronger functional and anatomical correspondence at
higher semantic abstraction levels.

</details>


### [91] [Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems](https://arxiv.org/pdf/2505.22571)
*Hoang Pham, Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: The paper introduces Agent-UniRAG, a trainable LLM agent framework for unified retrieval-augmented generation (RAG) systems, handling both single-hop and multi-hop queries. It also presents SynAgent-RAG, a synthetic dataset for smaller LLMs, achieving competitive performance.


<details>
  <summary>Details</summary>
Motivation: Existing RAG systems address single-hop or multi-hop queries separately, limiting real-world applicability. The study aims to unify these approaches for broader effectiveness and interpretability.

Method: Proposes Agent-UniRAG, an LLM agent framework that processes RAG tasks step-by-step based on input complexity, and introduces SynAgent-RAG, a synthetic dataset for smaller LLMs.

Result: The framework achieves comparable performance to closed-source and larger open-source LLMs on RAG benchmarks.

Conclusion: Agent-UniRAG enhances RAG systems' effectiveness and interpretability, with publicly available code and dataset for further research.

Abstract: This paper presents a novel approach for unified retrieval-augmented
generation (RAG) systems using the recent emerging large language model (LLM)
agent concept. Specifically, Agent LLM, which utilizes LLM as fundamental
controllers, has become a promising approach to enable the interpretability of
RAG tasks, especially for complex reasoning question-answering systems (e.g.,
multi-hop queries). Nonetheless, previous works mainly focus on solving RAG
systems with either single-hop or multi-hop approaches separately, which limits
the application of those approaches to real-world applications. In this study,
we propose a trainable agent framework called Agent-UniRAG for unified
retrieval-augmented LLM systems, which enhances the effectiveness and
interpretability of RAG systems. The main idea is to design an LLM agent
framework to solve RAG tasks step-by-step based on the complexity of the
inputs, simultaneously including single-hop and multi-hop queries in an
end-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset
to enable the proposed agent framework for small open-source LLMs (e.g.,
Llama-3-8B). The results show comparable performances with closed-source and
larger open-source LLMs across various RAG benchmarks. Our source code and
dataset are publicly available for further exploitation.

</details>


### [92] [Fusion Steering: Prompt-Specific Activation Control](https://arxiv.org/pdf/2505.22572)
*Waldemar Chang, Alhassan Yasin*

Main category: cs.CL

TL;DR: Fusion Steering improves factual accuracy in LLMs for QA tasks by dynamically injecting activation deltas across transformer layers, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: To enhance factual accuracy in LLMs for QA tasks by introducing flexible and dynamic activation steering methods.

Method: Uses dynamic injection of prompt-specific activation deltas derived from reference completions, optimized per prompt with Optuna for balanced factual alignment and fluency.

Result: Segmented steering achieves 25.4% accuracy, outperforming baseline (3.5%) and full-layer steering (16.2%), with 13.1% fully correct responses under stricter criteria.

Conclusion: Segmented, dynamic activation control shows promise for interpretable and scalable improvements in LLMs, with potential for sparse representations.

Abstract: We present Fusion Steering, an activation steering methodology that improves
factual accuracy in large language models (LLMs) for question-answering (QA)
tasks. This approach introduces flexible steering configurations, including
full-layer steering and segmented steering. Unlike traditional methods
constrained to single-layer or fixed-layer operations, Fusion Steering employs
dynamic injection of prompt-specific activation deltas across all transformer
layers. These activation deltas are derived from reference completions that
combine the ground-truth answer with a model-generated explanation to
facilitate semantically enriched, example-specific steering. The injection
weights are optimized per prompt using Optuna, targeting a joint objective that
balances token overlap (factual alignment) and perplexity (fluency proxy).
Evaluation employs a composite score integrating token overlap and LLM-graded
quality, encompassing factual accuracy, coherence, and relevance. Empirical
results on 260 SimpleQA prompts (selected from 500 where the baseline failed)
showcase the efficacy of segmented steering. Using Gemma-2-2B-IT with 8-bit
quantization, segmented steering achieves an accuracy of 25.4% (outputs scoring
$\geq 0.6$), outperforming the baseline at 3.5% and full-layer steering at
16.2%. Under the stricter SimpleQA rubric, segmented steering boosts fully
correct responses from 0.0% to 13.1%. These findings highlight the strengths of
segmented, dynamic intervention strategies and the promise of per-prompt,
full-network activation control. Fusion Steering is also amenable to sparse
representations, such as Neuronpedia or sparse crosscoders, suggesting a
promising direction for interpretable and scalable activation-level control in
LLMs.

</details>


### [93] [Less, but Better: Efficient Multilingual Expansion for LLMs via Layer-wise Mixture-of-Experts](https://arxiv.org/pdf/2505.22582)
*Xue Zhang, Yunlong Liang, Fandong Meng, Songming Zhang, Yufeng Chen, Jinan Xu, Jie Zhou*

Main category: cs.CL

TL;DR: LayerMoE, a layer-wise expert allocation algorithm, optimizes multilingual LLM expansion by reducing experts and mitigating old language forgetting.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and performance impact of existing MoE-based multilingual LLM expansion methods.

Method: Analyzes language representation similarities across layers, allocates fewer experts for higher similarity layers, and adds a classifier to guide routing for old languages.

Result: Outperforms baselines with 60% fewer experts in single-expansion and 33.3% fewer in lifelong-expansion.

Conclusion: LayerMoE effectively balances new language learning and old language preservation with fewer parameters.

Abstract: Continually expanding new languages for existing large language models (LLMs)
is a promising yet challenging approach to building powerful multilingual LLMs.
The biggest challenge is to make the model continuously learn new languages
while preserving the proficient ability of old languages. To achieve this,
recent work utilizes the Mixture-of-Experts (MoE) architecture to expand new
languages by adding new experts and avoid catastrophic forgetting of old
languages by routing corresponding tokens to the original model backbone (old
experts). Although intuitive, this kind of method is parameter-costly when
expanding new languages and still inevitably impacts the performance of old
languages. To address these limitations, we analyze the language
characteristics of different layers in LLMs and propose a layer-wise expert
allocation algorithm (LayerMoE) to determine the appropriate number of new
experts for each layer. Specifically, we find different layers in LLMs exhibit
different representation similarities between languages and then utilize the
similarity as the indicator to allocate experts for each layer, i.e., the
higher similarity, the fewer experts. Additionally, to further mitigate the
forgetting of old languages, we add a classifier in front of the router network
on the layers with higher similarity to guide the routing of old language
tokens. Experimental results show that our method outperforms the previous
state-of-the-art baseline with 60% fewer experts in the single-expansion
setting and with 33.3% fewer experts in the lifelong-expansion setting,
demonstrating the effectiveness of our method.

</details>


### [94] [Precise In-Parameter Concept Erasure in Large Language Models](https://arxiv.org/pdf/2505.22586)
*Yoav Gur-Arieh, Clara Suslik, Yihuai Hong, Fazl Barez, Mor Geva*

Main category: cs.CL

TL;DR: PISCES is a new method for precisely erasing unwanted concepts from LLMs by editing parameter space directions, outperforming existing methods in efficacy, specificity, and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing methods for removing undesirable knowledge from LLMs are either too coarse, shallow, or ineffective, necessitating a more precise approach.

Method: PISCES uses a disentangler model to decompose MLP vectors into interpretable features, identifies concept-associated features, and removes them directly from model parameters.

Result: PISCES reduces target concept accuracy to as low as 7.7%, improving specificity by up to 31% and robustness by up to 38% over existing methods.

Conclusion: Feature-based in-parameter editing offers a more precise and reliable way to erase conceptual knowledge in LLMs.

Abstract: Large language models (LLMs) often acquire knowledge during pretraining that
is undesirable in downstream deployments, e.g., sensitive information or
copyrighted content. Existing approaches for removing such knowledge rely on
fine-tuning, training low-rank adapters or fact-level editing, but these are
either too coarse, too shallow, or ineffective. In this work, we propose PISCES
(Precise In-parameter Suppression for Concept EraSure), a novel framework for
precisely erasing entire concepts from model parameters by directly editing
directions that encode them in parameter space. PISCES uses a disentangler
model to decompose MLP vectors into interpretable features, identifies those
associated with a target concept using automated interpretability techniques,
and removes them from model parameters. Experiments on Gemma 2 and Llama 3.1
over various concepts show that PISCES achieves modest gains in efficacy over
leading erasure methods, reducing accuracy on the target concept to as low as
7.7%, while dramatically improving erasure specificity (by up to 31%) and
robustness (by up to 38%). Overall, these results demonstrate that
feature-based in-parameter editing enables a more precise and reliable approach
for removing conceptual knowledge in language models.

</details>


### [95] [Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning](https://arxiv.org/pdf/2505.22591)
*Erxin Yu, Jing Li, Ming Liao, Qi Zhu, Boyang Xue, Minghui Xu, Baojun Wang, Lanqing Hong, Fei Mi, Lifeng Shang*

Main category: cs.CL

TL;DR: SEI framework improves LLMs' math reasoning by generalizing errors and synthesizing targeted training data.


<details>
  <summary>Details</summary>
Motivation: Address LLMs' struggles with bad cases in math reasoning by generalizing error patterns instead of isolated fixes.

Method: Identify bad cases, generate error keyphrases, cluster error types, synthesize training data via self-instruct, and refine through one-shot learning.

Result: Improved reasoning in models on GSM8K and MATH datasets, both in-domain and out-of-domain.

Conclusion: Self-error instruction effectively enhances LLMs' math reasoning by generalizing and addressing error patterns.

Abstract: Although large language models demonstrate strong performance across various
domains, they still struggle with numerous bad cases in mathematical reasoning.
Previous approaches to learning from errors synthesize training data by solely
extrapolating from isolated bad cases, thereby failing to generalize the
extensive patterns inherent within these cases. This paper presents
Self-Error-Instruct (SEI), a framework that addresses these model weaknesses
and synthesizes more generalized targeted training data. Specifically, we
explore a target model on two mathematical datasets, GSM8K and MATH, to
pinpoint bad cases. Then, we generate error keyphrases for these cases based on
the instructor model's (GPT-4o) analysis and identify error types by clustering
these keyphrases. Next, we sample a few bad cases during each generation for
each identified error type and input them into the instructor model, which
synthesizes additional training data using a self-instruct approach. This new
data is refined through a one-shot learning process to ensure that only the
most effective examples are kept. Finally, we use these curated data to
fine-tune the target model, iteratively repeating the process to enhance
performance. We apply our framework to various models and observe improvements
in their reasoning abilities across both in-domain and out-of-domain
mathematics datasets. These results demonstrate the effectiveness of self-error
instruction in improving LLMs' mathematical reasoning through error
generalization.

</details>


### [96] [Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding](https://arxiv.org/pdf/2505.22618)
*Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, Enze Xie*

Main category: cs.CL

TL;DR: The paper introduces a block-wise KV Cache and confidence-aware parallel decoding to improve the speed and quality of Diffusion LLMs, achieving up to 27.6× throughput with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Diffusion LLMs lag behind autoregressive models in inference speed and quality due to lack of KV Cache and token dependency disruption in parallel decoding.

Method: Proposes a block-wise KV Cache for bidirectional diffusion models and a confidence-aware parallel decoding strategy to mitigate dependency violations.

Result: Achieves up to 27.6× throughput improvement with minimal accuracy loss on LLaDA and Dream models.

Conclusion: The innovations bridge the performance gap with autoregressive models, enabling practical deployment of Diffusion LLMs.

Abstract: Diffusion-based large language models (Diffusion LLMs) have shown promise for
non-autoregressive text generation with parallel decoding capabilities.
However, the practical inference speed of open-sourced Diffusion LLMs often
lags behind autoregressive models due to the lack of Key-Value (KV) Cache and
quality degradation when decoding multiple tokens simultaneously. To bridge
this gap, we introduce a novel block-wise approximate KV Cache mechanism
tailored for bidirectional diffusion models, enabling cache reuse with
negligible performance drop. Additionally, we identify the root cause of
generation quality degradation in parallel decoding as the disruption of token
dependencies under the conditional independence assumption. To address this, we
propose a confidence-aware parallel decoding strategy that selectively decodes
tokens exceeding a confidence threshold, mitigating dependency violations and
maintaining generation quality. Experimental results on LLaDA and Dream models
across multiple LLM benchmarks demonstrate up to \textbf{27.6$\times$
throughput} improvement with minimal accuracy loss, closing the performance gap
with autoregressive models and paving the way for practical deployment of
Diffusion LLMs.

</details>


### [97] [Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions](https://arxiv.org/pdf/2505.22627)
*Yijun Shen, Delong Chen, Fan Liu, Xingyu Wang, Chuanyi Zhang, Liang Yao, Yuhui Zheng*

Main category: cs.CL

TL;DR: CoTalk, an AI-in-the-loop method, optimizes human annotation for image captions by reducing redundancy and leveraging multimodal input, improving speed and retrieval performance.


<details>
  <summary>Details</summary>
Motivation: To maximize annotated samples and comprehensiveness under fixed budget constraints, addressing underexplored optimization of human annotation efforts.

Method: Uses sequential annotation to reduce redundancy and a multimodal interface (reading and talking) for efficiency. Evaluated via intrinsic (semantic units) and extrinsic (vision-language alignment) metrics.

Result: CoTalk improves annotation speed (0.42 vs. 0.30 units/sec) and retrieval performance (41.13% vs. 40.52%) over parallel methods.

Conclusion: CoTalk effectively optimizes human annotation efforts, enhancing both efficiency and performance in vision-language alignment tasks.

Abstract: While densely annotated image captions significantly facilitate the learning
of robust vision-language alignment, methodologies for systematically
optimizing human annotation efforts remain underexplored. We introduce
Chain-of-Talkers (CoTalk), an AI-in-the-loop methodology designed to maximize
the number of annotated samples and improve their comprehensiveness under fixed
budget constraints (e.g., total human annotation time). The framework is built
upon two key insights. First, sequential annotation reduces redundant workload
compared to conventional parallel annotation, as subsequent annotators only
need to annotate the ``residual'' -- the missing visual information that
previous annotations have not covered. Second, humans process textual input
faster by reading while outputting annotations with much higher throughput via
talking; thus a multimodal interface enables optimized efficiency. We evaluate
our framework from two aspects: intrinsic evaluations that assess the
comprehensiveness of semantic units, obtained by parsing detailed captions into
object-attribute trees and analyzing their effective connections; extrinsic
evaluation measures the practical usage of the annotated captions in
facilitating vision-language alignment. Experiments with eight participants
show our Chain-of-Talkers (CoTalk) improves annotation speed (0.42 vs. 0.30
units/sec) and retrieval performance (41.13\% vs. 40.52\%) over the parallel
method.

</details>


### [98] [Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs](https://arxiv.org/pdf/2505.22630)
*Ziling Cheng, Meng Cao, Marc-Antoine Rondeau, Jackie Chi Kit Cheung*

Main category: cs.CL

TL;DR: LLMs exhibit structured errors like irrelevant context hallucinations due to class-based (mis)generalization, influenced by competing circuits in their internal computations.


<details>
  <summary>Details</summary>
Motivation: To understand the nature and regularities of errors in LLMs, particularly irrelevant context hallucinations, and challenge the 'stochastic parrot' argument.

Method: Behavioral analysis and mechanistic interpretability experiments on models like Llama-3, Mistral, and Pythia across 39 factual recall relation types.

Result: Errors stem from class-based (mis)generalization, with abstract class representations refined in layers and competing circuits influencing outputs.

Conclusion: LLMs generalize using abstractions but unreliably, acting as 'stochastic chameleons' influenced by contextual cues.

Abstract: The widespread success of large language models (LLMs) on NLP benchmarks has
been accompanied by concerns that LLMs function primarily as stochastic parrots
that reproduce texts similar to what they saw during pre-training, often
erroneously. But what is the nature of their errors, and do these errors
exhibit any regularities? In this work, we examine irrelevant context
hallucinations, in which models integrate misleading contextual cues into their
predictions. Through behavioral analysis, we show that these errors result from
a structured yet flawed mechanism that we term class-based (mis)generalization,
in which models combine abstract class cues with features extracted from the
query or context to derive answers. Furthermore, mechanistic interpretability
experiments on Llama-3, Mistral, and Pythia across 39 factual recall relation
types reveal that this behavior is reflected in the model's internal
computations: (i) abstract class representations are constructed in lower
layers before being refined into specific answers in higher layers, (ii)
feature selection is governed by two competing circuits -- one prioritizing
direct query-based reasoning, the other incorporating contextual cues -- whose
relative influences determine the final output. Our findings provide a more
nuanced perspective on the stochastic parrot argument: through form-based
training, LLMs can exhibit generalization leveraging abstractions, albeit in
unreliable ways based on contextual cues -- what we term stochastic chameleons.

</details>


### [99] [Learning Composable Chains-of-Thought](https://arxiv.org/pdf/2505.22635)
*Fangcong Yin, Zeyu Leo Liu, Liu Leqi, Xi Ye, Greg Durrett*

Main category: cs.CL

TL;DR: Training LLMs on composable CoT data improves zero-shot performance on unseen compositional tasks compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: To enable LLMs to generalize reasoning skills beyond their training distribution and solve harder, unseen tasks without costly annotated data.

Method: Modify CoT formats of atomic tasks to be composable, train models on this data, and combine them using multitask learning or model merging. Further fine-tune with rejection sampling.

Result: Composable CoT training outperforms multitask learning and continued fine-tuning within the same data budget.

Conclusion: Composable CoT enhances compositional generalization in LLMs, offering a scalable solution for reasoning tasks.

Abstract: A common approach for teaching large language models (LLMs) to reason is to
train on chain-of-thought (CoT) traces of in-distribution reasoning problems,
but such annotated data is costly to obtain for every problem of interest. We
want reasoning models to generalize beyond their training distribution, and
ideally to generalize compositionally: combine atomic reasoning skills to solve
harder, unseen reasoning tasks. We take a step towards compositional
generalization of reasoning skills when addressing a target compositional task
that has no labeled CoT data. We find that simply training models on CoT data
of atomic tasks leads to limited generalization, but minimally modifying CoT
formats of constituent atomic tasks to be composable can lead to improvements.
We can train "atomic CoT" models on the atomic tasks with Composable CoT data
and combine them with multitask learning or model merging for better zero-shot
performance on the target compositional task. Such a combined model can be
further bootstrapped on a small amount of compositional data using rejection
sampling fine-tuning (RFT). Results on string operations and natural language
skill compositions show that training LLMs on Composable CoT outperforms
multitask learning and continued fine-tuning baselines within a given training
data budget.

</details>


### [100] [Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese](https://arxiv.org/pdf/2505.22645)
*Hanjia Lyu, Jiebo Luo, Jian Kang, Allison Koenecke*

Main category: cs.CL

TL;DR: The study investigates performance disparities in LLMs when prompted in Simplified vs. Traditional Chinese, revealing biases influenced by task and language, and provides an open-sourced benchmark for future evaluations.


<details>
  <summary>Details</summary>
Motivation: Understanding differential LLM performance in Simplified and Traditional Chinese is critical to avoid representational harms and downstream decision-making biases in domains like education or hiring.

Method: Two benchmark tasks (regional term choice and regional name choice) were designed to audit 11 commercial and open-sourced LLMs, analyzing biases based on task and prompting language.

Result: LLMs exhibited biases favoring Simplified Chinese in term choice but Traditional Chinese in name choice, influenced by training data, character preferences, and tokenization.

Conclusion: The findings underscore the need for further bias analysis in LLMs, prompting the release of an open-sourced benchmark dataset for reproducible evaluations.

Abstract: While the capabilities of Large Language Models (LLMs) have been studied in
both Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit
differential performance when prompted in these two variants of written
Chinese. This understanding is critical, as disparities in the quality of LLM
responses can perpetuate representational harms by ignoring the different
cultural contexts underlying Simplified versus Traditional Chinese, and can
exacerbate downstream harms in LLM-facilitated decision-making in domains such
as education or hiring. To investigate potential LLM performance disparities,
we design two benchmark tasks that reflect real-world scenarios: regional term
choice (prompting the LLM to name a described item which is referred to
differently in Mainland China and Taiwan), and regional name choice (prompting
the LLM to choose who to hire from a list of names in both Simplified and
Traditional Chinese). For both tasks, we audit the performance of 11 leading
commercial LLM services and open-sourced models -- spanning those primarily
trained on English, Simplified Chinese, or Traditional Chinese. Our analyses
indicate that biases in LLM responses are dependent on both the task and
prompting language: while most LLMs disproportionately favored Simplified
Chinese responses in the regional term choice task, they surprisingly favored
Traditional Chinese names in the regional name choice task. We find that these
disparities may arise from differences in training data representation, written
character preferences, and tokenization of Simplified and Traditional Chinese.
These findings highlight the need for further analysis of LLM biases; as such,
we provide an open-sourced benchmark dataset to foster reproducible evaluations
of future LLM behavior across Chinese language variants
(https://github.com/brucelyu17/SC-TC-Bench).

</details>


### [101] [WebDancer: Towards Autonomous Information Seeking Agency](https://arxiv.org/pdf/2505.22648)
*Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou*

Main category: cs.CL

TL;DR: The paper introduces WebDancer, an agentic system for multi-step information seeking, using a four-stage training paradigm (data construction, trajectories sampling, supervised fine-tuning, and reinforcement learning), achieving strong performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addressing complex real-world problems requires advanced information seeking and reasoning, motivating the development of autonomous agentic systems like WebDancer.

Method: The method involves four stages: browsing data construction, trajectories sampling, supervised fine-tuning, and reinforcement learning, implemented in the WebDancer agent based on ReAct.

Result: WebDancer performs well on benchmarks (GAIA and WebWalkerQA), demonstrating the effectiveness of the training paradigm.

Conclusion: The framework provides actionable insights for developing more capable agentic models, with code and demo available on GitHub.

Abstract: Addressing intricate real-world problems necessitates in-depth information
seeking and multi-step reasoning. Recent progress in agentic systems,
exemplified by Deep Research, underscores the potential for autonomous
multi-step research. In this work, we present a cohesive paradigm for building
end-to-end agentic information seeking agents from a data-centric and
training-stage perspective. Our approach consists of four key stages: (1)
browsing data construction, (2) trajectories sampling, (3) supervised
fine-tuning for effective cold start, and (4) reinforcement learning for
enhanced generalisation. We instantiate this framework in a web agent based on
the ReAct, WebDancer. Empirical evaluations on the challenging information
seeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of
WebDancer, achieving considerable results and highlighting the efficacy of our
training paradigm. Further analysis of agent training provides valuable
insights and actionable, systematic pathways for developing more capable
agentic models. The codes and demo will be released in
https://github.com/Alibaba-NLP/WebAgent.

</details>


### [102] [The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning to Reason](https://arxiv.org/pdf/2505.22653)
*Ang Lv, Ruobing Xie, Xingwu Sun, Zhanhui Kang, Rui Yan*

Main category: cs.CL

TL;DR: LLMs show robustness to reward noise, achieving high performance even with 40% flipped rewards. Rewarding reasoning phrases (RPR) alone matches strict reward models, and combining RPR with noisy rewards improves open-ended task performance.


<details>
  <summary>Details</summary>
Motivation: To explore the impact of reward noise in post-training LLMs for reasoning, addressing practical real-world scenarios where rewards may be imperfect.

Method: Investigated reward noise by manually flipping 40% of rewards in math tasks and introduced reasoning pattern rewards (RPR) without correctness verification. Combined RPR with noisy rewards for calibration.

Result: LLMs achieved 72% accuracy with 40% noisy rewards (vs. 75% noiseless). RPR alone matched strict rewards (over 70% accuracy). Combining RPR and noisy rewards improved open-ended task performance.

Conclusion: Reward noise robustness and RPR highlight the importance of reasoning processes. Insights suggest improving foundational abilities in pre-training and advancing post-training techniques.

Abstract: Recent studies on post-training large language models (LLMs) for reasoning
through reinforcement learning (RL) typically focus on tasks that can be
accurately verified and rewarded, such as solving math problems. In contrast,
our research investigates the impact of reward noise, a more practical
consideration for real-world scenarios involving the post-training of LLMs
using reward models. We found that LLMs demonstrate strong robustness to
substantial reward noise. For example, manually flipping 40% of the reward
function's outputs in math tasks still allows a Qwen-2.5-7B model to achieve
rapid convergence, improving its performance on math tasks from 5% to 72%,
compared to the 75% accuracy achieved by a model trained with noiseless
rewards. Surprisingly, by only rewarding the appearance of key reasoning
phrases (namely reasoning pattern reward, RPR), such as ``first, I need
to''-without verifying the correctness of answers, the model achieved peak
downstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models
trained with strict correctness verification and accurate rewards. Recognizing
the importance of the reasoning process over the final results, we combined RPR
with noisy reward models. RPR helped calibrate the noisy reward models,
mitigating potential false negatives and enhancing the LLM's performance on
open-ended tasks. These findings suggest the importance of improving models'
foundational abilities during the pre-training phase while providing insights
for advancing post-training techniques. Our code and scripts are available at
https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason.

</details>


### [103] [GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating LLMs in Domain-Specific Knowledge and Reasoning](https://arxiv.org/pdf/2505.22661)
*Qingchen Yu, Zifan Zheng, Ding Chen, Simin Niu, Bo Tang, Feiyu Xiong, Zhiyu Li*

Main category: cs.CL

TL;DR: GuessArena is an adaptive evaluation framework for LLMs using adversarial game-based interactions to address limitations of static benchmarks.


<details>
  <summary>Details</summary>
Motivation: Static benchmarks lack adaptability and fail to assess domain-specific knowledge and reasoning abilities effectively.

Method: GuessArena integrates dynamic domain knowledge modeling and progressive reasoning assessment, inspired by the Guess Who I Am? game.

Result: Empirical studies show GuessArena effectively distinguishes LLMs in domain knowledge and reasoning across five domains.

Conclusion: GuessArena offers superior interpretability, scalability, and adaptability compared to conventional benchmarks.

Abstract: The evaluation of large language models (LLMs) has traditionally relied on
static benchmarks, a paradigm that poses two major limitations: (1) predefined
test sets lack adaptability to diverse application domains, and (2)
standardized evaluation protocols often fail to capture fine-grained
assessments of domain-specific knowledge and contextual reasoning abilities. To
overcome these challenges, we propose GuessArena, an adaptive evaluation
framework grounded in adversarial game-based interactions. Inspired by the
interactive structure of the Guess Who I Am? game, our framework seamlessly
integrates dynamic domain knowledge modeling with progressive reasoning
assessment to improve evaluation fidelity. Empirical studies across five
vertical domains-finance, healthcare, manufacturing, information technology,
and education-demonstrate that GuessArena effectively distinguishes LLMs in
terms of domain knowledge coverage and reasoning chain completeness. Compared
to conventional benchmarks, our method provides substantial advantages in
interpretability, scalability, and scenario adaptability.

</details>


### [104] [AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models](https://arxiv.org/pdf/2505.22662)
*Feng Luo, Yu-Neng Chuang, Guanchu Wang, Hoang Anh Duy Le, Shaochen Zhong, Hongyi Liu, Jiayi Yuan, Yang Sui, Vladimir Braverman, Vipin Chaudhary, Xia Hu*

Main category: cs.CL

TL;DR: AutoL2S dynamically adjusts reasoning path length in LLMs, reducing CoT length by 57% without performance loss.


<details>
  <summary>Details</summary>
Motivation: LLMs often overthink, generating long CoT paths for easy questions, increasing costs and latency. Existing methods lack dynamic adaptation.

Method: Proposes AutoL2S, a model-agnostic framework where LLMs learn to compress reasoning paths based on question complexity, using annotated data with <EASY> tokens.

Result: AutoL2S reduces reasoning path length by up to 57% while maintaining performance.

Conclusion: AutoL2S enhances efficiency and scalability of LLM reasoning by dynamically optimizing CoT length.

Abstract: The reasoning-capable large language models (LLMs) demonstrate strong
performance on complex reasoning tasks but often suffer from overthinking,
generating unnecessarily long chain-of-thought (CoT) reasoning paths for easy
reasoning questions, thereby increasing inference cost and latency. Recent
approaches attempt to address this challenge by manually deciding when to apply
long or short reasoning. However, they lack the flexibility to adapt CoT length
dynamically based on question complexity. In this paper, we propose Auto
Long-Short Reasoning (AutoL2S), a dynamic and model-agnostic framework that
enables LLMs to dynamically compress their generated reasoning path based on
the complexity of the reasoning question. AutoL2S enables a learned paradigm,
in which LLMs themselves can decide when longer reasoning is necessary and when
shorter reasoning suffices, by training on data annotated with our proposed
method, which includes both long and short CoT paths and a special <EASY>
token. We then use <EASY> token to indicate when the model can skip generating
lengthy CoT reasoning. This proposed annotation strategy can enhance the LLMs'
ability to generate shorter CoT reasoning paths with improved quality after
training. Extensive evaluation results show that AutoL2S reduces the length of
reasoning generation by up to 57% without compromising performance,
demonstrating the effectiveness of AutoL2S for scalable and efficient LLM
reasoning.

</details>


### [105] [Machine Translation Models are Zero-Shot Detectors of Translation Direction](https://arxiv.org/pdf/2401.06769)
*Michelle Wastl, Jannis Vamvas, Rico Sennrich*

Main category: cs.CL

TL;DR: An unsupervised method detects translation direction using the hypothesis that translations are more predictable from originals than vice versa, achieving high accuracy for machine and human translations.


<details>
  <summary>Details</summary>
Motivation: The work aims to address applications in machine translation training, evaluation, and forensic tasks like plagiarism detection by leveraging the simplification effect in translations.

Method: The approach is unsupervised, based on the hypothesis that the probability of a translation given an original is higher than the reverse, tested across 20 multilingual machine translation directions.

Result: Document-level accuracies range from 82-96% for NMT-produced translations and 60-81% for human translations, varying by model.

Conclusion: The method is effective, especially for high-resource languages, with potential applications in translation quality assessment and forensic analysis.

Abstract: Detecting the translation direction of parallel text has applications for
machine translation training and evaluation, but also has forensic applications
such as resolving plagiarism or forgery allegations. In this work, we explore
an unsupervised approach to translation direction detection based on the simple
hypothesis that
$p(\text{translation}|\text{original})>p(\text{original}|\text{translation})$,
motivated by the well-known simplification effect in translationese or
machine-translationese. In experiments with massively multilingual machine
translation models across 20 translation directions, we confirm the
effectiveness of the approach for high-resource language pairs, achieving
document-level accuracies of 82--96% for NMT-produced translations, and 60--81%
for human translations, depending on the model used. Code and demo are
available at https://github.com/ZurichNLP/translation-direction-detection

</details>


### [106] [Tracking Semantic Change in Slovene: A Novel Dataset and Optimal Transport-Based Distance](https://arxiv.org/pdf/2402.16596)
*Marko Pranjić, Kaja Dobrovoljc, Senja Pollak, Matej Martinc*

Main category: cs.CL

TL;DR: The paper introduces a dataset for semantic change detection in Slovene, critiques existing metrics, and proposes a new metric using regularized optimal transport, showing improved performance.


<details>
  <summary>Details</summary>
Motivation: To understand language evolution in Slovene, a less-resourced language, by detecting semantic changes influenced by societal and cultural shifts.

Method: Creation of a Slovene dataset with annotated sentence pairs, analysis of existing metrics, and proposal of a novel metric based on regularized optimal transport.

Result: The proposed metric outperforms or matches baseline methods in semantic change detection.

Conclusion: The new metric provides a robust framework for semantic change detection, advancing the study of language evolution in Slovene.

Abstract: In this paper, we focus on the detection of semantic changes in Slovene, a
less resourced Slavic language with two million speakers. Detecting and
tracking semantic changes provides insight into the evolution of language
caused by changes in society and culture. We present the first Slovene dataset
for evaluating semantic change detection systems, which contains aggregated
semantic change scores for 104 target words obtained from more than 3,000
manually annotated sentence pairs. We analyze an important class of measures of
semantic change metrics based on the Average pairwise distance and identify
several limitations. To address these limitations, we propose a novel metric
based on regularized optimal transport, which offers a more robust framework
for quantifying semantic change. We provide a comprehensive evaluation of
various existing semantic change detection methods and associated semantic
change measures on our dataset. Through empirical testing, we demonstrate that
our proposed approach, leveraging regularized optimal transport, achieves
either matching or improved performance compared to baseline approaches.

</details>


### [107] [Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems](https://arxiv.org/pdf/2404.06762)
*Zhengyuan Liu, Stella Xin Yin, Geyu Lin, Nancy F. Chen*

Main category: cs.CL

TL;DR: A framework for creating student profiles in conversational ITSs using LLMs, enhancing personalized learning through cognitive and noncognitive traits.


<details>
  <summary>Details</summary>
Motivation: To improve student engagement and learning efficiency in dialogic teaching by simulating diverse student personas.

Method: Proposes a framework integrating cognitive and noncognitive traits, validated through multi-aspect analysis and LLMs for student simulation.

Result: LLMs generate diverse student responses based on language ability and personality, enabling adaptive teaching strategies.

Conclusion: The framework effectively leverages LLMs for personality-aware student simulation, enhancing conversational ITSs.

Abstract: Intelligent Tutoring Systems (ITSs) can provide personalized and self-paced
learning experience. The emergence of large language models (LLMs) further
enables better human-machine interaction, and facilitates the development of
conversational ITSs in various disciplines such as math and language learning.
In dialogic teaching, recognizing and adapting to individual characteristics
can significantly enhance student engagement and learning efficiency. However,
characterizing and simulating student's persona remain challenging in training
and evaluating conversational ITSs. In this work, we propose a framework to
construct profiles of different student groups by refining and integrating both
cognitive and noncognitive aspects, and leverage LLMs for personality-aware
student simulation in a language learning scenario. We further enhance the
framework with multi-aspect validation, and conduct extensive analysis from
both teacher and student perspectives. Our experimental results show that
state-of-the-art LLMs can produce diverse student responses according to the
given language ability and personality traits, and trigger teacher's adaptive
scaffolding strategies.

</details>


### [108] [Mitigating Text Toxicity with Counterfactual Generation](https://arxiv.org/pdf/2405.09948)
*Milan Bhan, Jean-Noel Vittaut, Nina Achache, Victor Legrand, Nicolas Chesneau, Annabelle Blangero, Juliette Murris, Marie-Jeanne Lesot*

Main category: cs.CL

TL;DR: The paper proposes using XAI counterfactual generation methods for text detoxification, outperforming traditional methods in preserving meaning while reducing toxicity.


<details>
  <summary>Details</summary>
Motivation: Existing detoxification methods fail to preserve non-toxic meaning, prompting the use of XAI techniques for better results.

Method: Applies local feature importance and counterfactual generation to a toxicity classifier, tested on three datasets.

Result: XAI-based methods mitigate toxicity more accurately and preserve meaning better than classical methods.

Conclusion: Bridges counterfactual generation and detoxification, highlighting practical XAI applications and risks of misuse.

Abstract: Toxicity mitigation consists in rephrasing text in order to remove offensive
or harmful meaning. Neural natural language processing (NLP) models have been
widely used to target and mitigate textual toxicity. However, existing methods
fail to detoxify text while preserving the initial non-toxic meaning at the
same time. In this work, we propose to apply counterfactual generation methods
from the eXplainable AI (XAI) field to target and mitigate textual toxicity. In
particular, we perform text detoxification by applying local feature importance
and counterfactual generation methods to a toxicity classifier distinguishing
between toxic and non-toxic texts. We carry out text detoxification through
counterfactual generation on three datasets and compare our approach to three
competitors. Automatic and human evaluations show that recently developed NLP
counterfactual generators can mitigate toxicity accurately while better
preserving the meaning of the initial text as compared to classical
detoxification methods. Finally, we take a step back from using automated
detoxification tools, and discuss how to manage the polysemous nature of
toxicity and the risk of malicious use of detoxification tools. This work is
the first to bridge the gap between counterfactual generation and text
detoxification and paves the way towards more practical application of XAI
methods.

</details>


### [109] [REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space](https://arxiv.org/pdf/2406.09325)
*Tomer Ashuach, Martin Tutek, Yonatan Belinkov*

Main category: cs.CL

TL;DR: REVS is a non-gradient-based method for unlearning sensitive information from LMs, outperforming existing methods in privacy protection and robustness.


<details>
  <summary>Details</summary>
Motivation: Language models may memorize and leak sensitive data, raising privacy concerns. Current solutions like dataset scrubbing or model editing are costly or bypassable.

Method: REVS identifies and modifies neurons linked to sensitive tokens, using three datasets (emails, URLs, synthetic SSNs) for evaluation.

Result: REVS excels in unlearning sensitive data and resisting extraction attacks while preserving model integrity.

Conclusion: REVS offers an effective, robust solution for privacy protection in LMs.

Abstract: Language models (LMs) risk inadvertently memorizing and divulging sensitive
or personally identifiable information (PII) seen in training data, causing
privacy concerns. Current approaches to address this issue involve costly
dataset scrubbing, or model filtering through unlearning and model editing,
which can be bypassed through extraction attacks. We propose REVS, a novel
non-gradient-based method for unlearning sensitive information from LMs. REVS
identifies and modifies a small subset of neurons relevant for constituent
tokens that form sensitive information. To adequately evaluate our method on
truly sensitive information, we curate three datasets: email and URL datasets
naturally memorized by the models, and a synthetic social security number
dataset that we tune the models to memorize. Compared to other methods, REVS
demonstrates superior performance in unlearning sensitive information and
robustness to extraction attacks, while retaining underlying model integrity.

</details>


### [110] [Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective](https://arxiv.org/pdf/2406.14023)
*Yuchen Wen, Keping Bi, Wei Chen, Jiafeng Guo, Xueqi Cheng*

Main category: cs.CL

TL;DR: The paper evaluates implicit bias in LLMs using psychometric-inspired attacks, introduces two benchmarks, and shows effectiveness in eliciting bias compared to baselines.


<details>
  <summary>Details</summary>
Motivation: Concerns about LLMs spreading unethical content and implicit bias prompted a need for rigorous evaluation methods.

Method: Proposes three psychometric-inspired attack approaches (Disguise, Deception, Teaching) and builds two benchmarks for evaluation.

Result: Demonstrates that the methods effectively elicit LLMs' implicit bias, outperforming baselines.

Conclusion: Provides tools for assessing ethical risks in LLMs, promoting accountability in their development.

Abstract: As large language models (LLMs) become an important way of information
access, there have been increasing concerns that LLMs may intensify the spread
of unethical content, including implicit bias that hurts certain populations
without explicit harmful words. In this paper, we conduct a rigorous evaluation
of LLMs' implicit bias towards certain demographics by attacking them from a
psychometric perspective to elicit agreements to biased viewpoints. Inspired by
psychometric principles in cognitive and social psychology, we propose three
attack approaches, i.e., Disguise, Deception, and Teaching. Incorporating the
corresponding attack instructions, we built two benchmarks: (1) a bilingual
dataset with biased statements covering four bias types (2.7K instances) for
extensive comparative analysis, and (2) BUMBLE, a larger benchmark spanning
nine common bias types (12.7K instances) for comprehensive evaluation.
Extensive evaluation of popular commercial and open-source LLMs shows that our
methods can elicit LLMs' inner bias more effectively than competitive
baselines. Our attack methodology and benchmarks offer an effective means of
assessing the ethical risks of LLMs, driving progress toward greater
accountability in their development. Our code, data and benchmarks are
available at https://github.com/yuchenwen1/ImplicitBiasPsychometricEvaluation
and https://github.com/yuchenwen1/BUMBLE.

</details>


### [111] [Dissecting the Ullman Variations with a SCALPEL: Why do LLMs fail at Trivial Alterations to the False Belief Task?](https://arxiv.org/pdf/2406.14737)
*Zhiqiang Pi, Annapurna Vadaparty, Benjamin K. Bergen, Cameron R. Jones*

Main category: cs.CL

TL;DR: SCALPEL is introduced to test LLMs' Theory of Mind (ToM) capabilities, revealing their failure in common-sense inferences despite some success in ToM tasks.


<details>
  <summary>Details</summary>
Motivation: To resolve the debate on LLMs' ToM capabilities by testing their robustness against modified stimuli.

Method: SCALPEL technique incrementally modifies stimuli to pinpoint why LLMs fail in ToM tasks, applied to the 'transparent-access' task.

Result: LLMs often fail due to lacking common-sense inferences, like recognizing contents in transparent containers.

Conclusion: LLMs surpass pattern matching but lack robust human-like ToM; SCALPEL aids detailed examination of their capabilities.

Abstract: Recent empirical results have sparked a debate about whether or not Large
Language Models (LLMs) are capable of Theory of Mind (ToM). While some have
found LLMs to be successful on ToM evaluations such as the False Belief task,
others have shown that their performance is not robust against trivial
alterations to stimuli. In this paper, we introduce SCALPEL -- a technique to
incrementally modify stimuli to test different specific hypotheses about why
LLMs fail -- and apply this method to the "transparent-access" modification of
the unexpected contents task. Our results suggest that LLMs often do poorly
because they fail to make essential common-sense inferences, such as that
seeing a transparent container implies recognizing its contents. We conclude
that while modern LLMs go beyond mere pattern matching, they still fall short
of robust human-like ToM. We argue that SCALPEL can help cognitive scientists
examine LLMs' capabilities in finer detail and provide insight into alternative
mechanisms by which tasks that are used to assess human cognition might be
completed.

</details>


### [112] [Large Vocabulary Size Improves Large Language Models](https://arxiv.org/pdf/2406.16508)
*Sho Takase, Ryokan Ri, Shun Kiyono, Takuya Kato*

Main category: cs.CL

TL;DR: Larger subword vocabularies improve LLM performance, and adapting vocabularies for new languages outperforms pre-trained ones.


<details>
  <summary>Details</summary>
Motivation: To understand how subword vocabulary size affects LLM performance and explore vocabulary adaptation for new languages.

Method: Empirical investigation of vocabulary size impact and a simple method to replace pre-trained vocabularies for new languages.

Result: Larger vocabularies enhance LLM performance, and new vocabularies for target languages outperform pre-trained ones.

Conclusion: Optimal vocabulary size and adaptation are key for LLM performance in multilingual settings.

Abstract: This paper empirically investigates the relationship between subword
vocabulary size and the performance of large language models (LLMs) to provide
insights on how to define the vocabulary size. Experimental results show that
larger vocabulary sizes lead to better performance in LLMs. Moreover, we
consider a continual training scenario where a pre-trained language model is
trained on a different target language. We introduce a simple method to use a
new vocabulary instead of the pre-defined one. We show that using the new
vocabulary outperforms the model with the vocabulary used in pre-training.

</details>


### [113] [Empirical analysis of binding precedent efficiency in Brazilian Supreme Court via case classification](https://arxiv.org/pdf/2407.07004)
*Raphaël Tinarrage, Henrique Ennes, Lucas Resck, Lucas T. Gomes, Jean R. Ponciano, Jorge Poco*

Main category: cs.CL

TL;DR: The paper examines the effectiveness of Brazilian binding precedents in reducing repetitive legal demands, using NLP techniques for case classification and identifying reasons for their inefficiency.


<details>
  <summary>Details</summary>
Motivation: To assess the legal impact of binding precedents and understand why they fail to reduce repetitive demands in the Brazilian legal system.

Method: Empirical analysis of five binding precedents using NLP techniques (TF-IDF, LSTM, Longformer, regex) for case classification and comparison of court rulings before and after precedent creation.

Result: TF-IDF performed slightly better than deep learning models, but the latter detected important legal events missed by TF-IDF. Inefficiency reasons are case-dependent, with five main hypotheses identified.

Conclusion: Binding precedents' inefficiency is heterogeneous, with no single cause. The study highlights the need for tailored solutions and the potential of combining NLP methods for legal analysis.

Abstract: Binding precedents (s\'umulas vinculantes) constitute a juridical instrument
unique to the Brazilian legal system and whose objectives include the
protection of the Federal Supreme Court against repetitive demands. Studies of
the effectiveness of these instruments in decreasing the Court's exposure to
similar cases, however, indicate that they tend to fail in such a direction,
with some of the binding precedents seemingly creating new demands. We
empirically assess the legal impact of five binding precedents, 11, 14, 17, 26,
and 37, at the highest Court level through their effects on the legal subjects
they address. This analysis is only possible through the comparison of the
Court's ruling about the precedents' themes before they are created, which
means that these decisions should be detected through techniques of Similar
Case Retrieval, which we tackle from the angle of Case Classification. The
contributions of this article are therefore twofold: on the mathematical side,
we compare the use of different methods of Natural Language Processing --
TF-IDF, LSTM, Longformer, and regex -- for Case Classification, whereas on the
legal side, we contrast the inefficiency of these binding precedents with a set
of hypotheses that may justify their repeated usage. We observe that the TF-IDF
models performed slightly better than LSTM and Longformer when compared through
common metrics; however, the deep learning models were able to detect certain
important legal events that TF-IDF missed. On the legal side, we argue that the
reasons for binding precedents to fail in responding to repetitive demand are
heterogeneous and case-dependent, making it impossible to single out a specific
cause. We identify five main hypotheses, which are found in different
combinations in each of the precedents studied.

</details>


### [114] [Prompt-based Personality Profiling: Reinforcement Learning for Relevance Filtering](https://arxiv.org/pdf/2409.04122)
*Jan Hofmann, Cornelia Sindermann, Roman Klinger*

Main category: cs.CL

TL;DR: Proposes a method for author profiling by filtering relevant content first using reinforcement learning, achieving comparable results with shorter context and improved accuracy on balanced data.


<details>
  <summary>Details</summary>
Motivation: Supervised ML dominates author profiling, but large input lengths and costs of API-accessed systems limit efficiency. Addressing these issues by filtering irrelevant content.

Method: Uses reinforcement learning to optimize a relevance filter, leveraging zero-shot capabilities of large language models, followed by profiling with only relevant data.

Result: Achieves similar efficacy to using all posts but with shorter context; significantly improved accuracy on balanced data.

Conclusion: The method effectively mitigates input length and cost issues while maintaining or improving profiling accuracy.

Abstract: Author profiling is the task of inferring characteristics about individuals
by analyzing content they share. Supervised machine learning still dominates
automatic systems that perform this task, despite the popularity of prompting
large language models to address natural language understanding tasks. One
reason is that the classification instances consist of large amounts of posts,
potentially a whole user profile, which may exceed the input length of
Transformers. Even if a model can use a large context window, the entirety of
posts makes the application of API-accessed black box systems costly and slow,
next to issues which come with such "needle-in-the-haystack" tasks. To mitigate
this limitation, we propose a new method for author profiling which aims at
distinguishing relevant from irrelevant content first, followed by the actual
user profiling only with relevant data. To circumvent the need for
relevance-annotated data, we optimize this relevance filter via reinforcement
learning with a reward function that utilizes the zero-shot capabilities of
large language models. We evaluate our method for Big Five personality trait
prediction on two Twitter corpora. On publicly available real-world data with a
skewed label distribution, our method shows similar efficacy to using all posts
in a user profile, but with a substantially shorter context. An evaluation on a
version of these data balanced with artificial posts shows that the filtering
to relevant posts leads to a significantly improved accuracy of the
predictions.

</details>


### [115] [Nonlinear second-order dynamics describe labial constriction trajectories across languages and contexts](https://arxiv.org/pdf/2410.08351)
*Michael C. Stern, Jason A. Shaw*

Main category: cs.CL

TL;DR: The paper analyzes labial constriction trajectories for /b/ and /m/ sounds in English and Mandarin, proposing a dynamical system model with two parameters (T and r) that fits empirical data well.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics of labial constriction trajectories across languages and contexts, and formalize these observations into a mathematical model.

Method: The study uses empirical data to derive a nonlinear second-order dynamical system with parameters T (target state) and r (movement rapidity), validated through nonlinear regression and trajectory simulations.

Result: The model fits individual trajectories well and captures key kinematic variables (duration, peak velocity, timing). Simulated trajectories match empirical data qualitatively.

Conclusion: The model provides a foundation for studying additional influences on articulatory kinematics, such as prosody, coordination, and noise.

Abstract: We investigate the dynamics of labial constriction trajectories during the
production of /b/ and /m/ in English and Mandarin. We find that, across
languages and contexts, the ratio of instantaneous displacement to
instantaneous velocity generally follows an exponential decay curve from
movement onset to movement offset. We formalize this empirical discovery in a
differential equation and, in combination with an assumption of point attractor
dynamics, derive a nonlinear second-order dynamical system describing labial
constriction trajectories. The equation has only two parameters, T and r. T
corresponds to the target state and r corresponds to movement rapidity. Thus,
each of the parameters corresponds to a phonetically relevant dimension of
control. Nonlinear regression demonstrates that the model provides excellent
fits to individual movement trajectories. Moreover, trajectories simulated from
the model qualitatively match empirical trajectories, and capture key kinematic
variables like duration, peak velocity, and time to achieve peak velocity. The
model constitutes a proposal for the dynamics of individual articulatory
movements, and thus offers a novel foundation from which to understand
additional influences on articulatory kinematics like prosody, inter-movement
coordination, and stochastic noise.

</details>


### [116] [Which Demographics do LLMs Default to During Annotation?](https://arxiv.org/pdf/2410.08820)
*Johannes Schäfer, Aidan Combs, Christopher Bagdon, Jiahui Li, Nadine Probol, Lynn Greschner, Sean Papay, Yarik Menchaca Resendiz, Aswathy Velutharambath, Amelie Wührl, Sabine Weber, Roman Klinger*

Main category: cs.CL

TL;DR: The paper explores how LLMs mimic human annotator demographics when not explicitly given, comparing non-demographic, placebo, and demographic prompts for politeness and offensiveness annotations.


<details>
  <summary>Details</summary>
Motivation: To understand label variations in text annotation due to annotator demographics and investigate how LLMs inherently adopt these biases without explicit prompts.

Method: Evaluates LLM annotations on the POPQUORN dataset, comparing non-demographic, placebo, and demographic-conditioned prompts for politeness and offensiveness.

Result: Notable demographic influences (gender, race, age) in LLM outputs with demographic prompts, contrasting prior studies.

Conclusion: LLMs inherently mimic human annotator biases, highlighting the need for awareness and mitigation of demographic influences in automated annotations.

Abstract: Demographics and cultural background of annotators influence the labels they
assign in text annotation -- for instance, an elderly woman might find it
offensive to read a message addressed to a "bro", but a male teenager might
find it appropriate. It is therefore important to acknowledge label variations
to not under-represent members of a society. Two research directions developed
out of this observation in the context of using large language models (LLM) for
data annotations, namely (1) studying biases and inherent knowledge of LLMs and
(2) injecting diversity in the output by manipulating the prompt with
demographic information. We combine these two strands of research and ask the
question to which demographics an LLM resorts to when no demographics is given.
To answer this question, we evaluate which attributes of human annotators LLMs
inherently mimic. Furthermore, we compare non-demographic conditioned prompts
and placebo-conditioned prompts (e.g., "you are an annotator who lives in house
number 5") to demographics-conditioned prompts ("You are a 45 year old man and
an expert on politeness annotation. How do you rate {instance}"). We study
these questions for politeness and offensiveness annotations on the POPQUORN
data set, a corpus created in a controlled manner to investigate human label
variations based on demographics which has not been used for LLM-based analyses
so far. We observe notable influences related to gender, race, and age in
demographic prompting, which contrasts with previous studies that found no such
effects.

</details>


### [117] [Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models](https://arxiv.org/pdf/2410.13080)
*Linhao Luo, Zicheng Zhao, Gholamreza Haffari, Yuan-Fang Li, Chen Gong, Shirui Pan*

Main category: cs.CL

TL;DR: GCR integrates KGs with LLMs to enhance reasoning accuracy by constraining LLM decoding with KG-Trie, eliminating hallucinations and improving performance.


<details>
  <summary>Details</summary>
Motivation: Address LLMs' struggles with faithful reasoning due to knowledge gaps and hallucinations by leveraging structured knowledge in KGs.

Method: Introduces graph-constrained reasoning (GCR) using KG-Trie to integrate KG structure into LLM decoding, alongside a specialized LLM for KG reasoning.

Result: Achieves state-of-the-art performance on KGQA benchmarks with zero reasoning hallucination and strong zero-shot generalizability.

Conclusion: GCR effectively bridges structured and unstructured reasoning, enabling accurate and faithful KG-grounded reasoning in LLMs.

Abstract: Large language models (LLMs) have demonstrated impressive reasoning
abilities, but they still struggle with faithful reasoning due to knowledge
gaps and hallucinations. To address these issues, knowledge graphs (KGs) have
been utilized to enhance LLM reasoning through their structured knowledge.
However, existing KG-enhanced methods, either retrieval-based or agent-based,
encounter difficulties in accurately retrieving knowledge and efficiently
traversing KGs at scale. In this work, we introduce graph-constrained reasoning
(GCR), a novel framework that bridges structured knowledge in KGs with
unstructured reasoning in LLMs. To eliminate hallucinations, GCR ensures
faithful KG-grounded reasoning by integrating KG structure into the LLM
decoding process through KG-Trie, a trie-based index that encodes KG reasoning
paths. KG-Trie constrains the decoding process, allowing LLMs to directly
reason on graphs and generate faithful reasoning paths grounded in KGs.
Additionally, GCR leverages a lightweight KG-specialized LLM for
graph-constrained reasoning alongside a powerful general LLM for inductive
reasoning over multiple reasoning paths, resulting in accurate reasoning with
zero reasoning hallucination. Extensive experiments on several KGQA benchmarks
demonstrate that GCR achieves state-of-the-art performance and exhibits strong
zero-shot generalizability to unseen KGs without additional training.

</details>


### [118] [Distance between Relevant Information Pieces Causes Bias in Long-Context LLMs](https://arxiv.org/pdf/2410.14641)
*Runchu Tian, Yanghao Li, Yuepeng Fu, Siyang Deng, Qinyu Luo, Cheng Qian, Shuo Wang, Xin Cong, Zhong Zhang, Yesai Wu, Yankai Lin, Huadong Wang, Xiaojiang Liu*

Main category: cs.CL

TL;DR: LongPiBench assesses positional bias in LLMs with multiple relevant information pieces, revealing biases in spacing despite robustness to 'lost in the middle'.


<details>
  <summary>Details</summary>
Motivation: Address the gap in evaluating positional bias for multiple relevant information pieces in LLMs, beyond single-piece focus.

Method: Develop LongPiBench benchmark and test five commercial and six open-source models.

Result: Most models are robust to 'lost in the middle' but show biases in spacing of relevant information.

Conclusion: Evaluating and reducing positional biases is crucial for improving LLM capabilities.

Abstract: Positional bias in large language models (LLMs) hinders their ability to
effectively process long inputs. A prominent example is the "lost in the
middle" phenomenon, where LLMs struggle to utilize relevant information
situated in the middle of the input. While prior research primarily focuses on
single pieces of relevant information, real-world applications often involve
multiple relevant information pieces. To bridge this gap, we present
LongPiBench, a benchmark designed to assess positional bias involving multiple
pieces of relevant information. Thorough experiments are conducted with five
commercial and six open-source models. These experiments reveal that while most
current models are robust against the "lost in the middle" issue, there exist
significant biases related to the spacing of relevant information pieces. These
findings highlight the importance of evaluating and reducing positional biases
to advance LLM's capabilities.

</details>


### [119] [SafetyAnalyst: Interpretable, Transparent, and Steerable Safety Moderation for AI Behavior](https://arxiv.org/pdf/2410.16665)
*Jing-Jing Li, Valentina Pyatkin, Max Kleiman-Weiner, Liwei Jiang, Nouha Dziri, Anne G. E. Collins, Jana Schaich Borg, Maarten Sap, Yejin Choi, Sydney Levine*

Main category: cs.CL

TL;DR: SafetyAnalyst is a new AI safety moderation framework that uses interpretable harm-benefit trees and scoring to outperform existing systems in prompt safety classification.


<details>
  <summary>Details</summary>
Motivation: Current AI safety moderation systems lack interpretability and steerability, making it hard to align them with safety standards and community values.

Method: SafetyAnalyst employs chain-of-thought reasoning to create harm-benefit trees, labels impacts, and aggregates effects into a harmfulness score using interpretable weights.

Result: SafetyAnalyst achieves higher performance (F1=0.81) than existing systems (F1<0.72) and offers interpretability and steerability.

Conclusion: SafetyAnalyst addresses key gaps in AI safety moderation, providing a more effective, transparent, and adaptable solution.

Abstract: The ideal AI safety moderation system would be both structurally
interpretable (so its decisions can be reliably explained) and steerable (to
align to safety standards and reflect a community's values), which current
systems fall short on. To address this gap, we present SafetyAnalyst, a novel
AI safety moderation framework. Given an AI behavior, SafetyAnalyst uses
chain-of-thought reasoning to analyze its potential consequences by creating a
structured "harm-benefit tree," which enumerates harmful and beneficial actions
and effects the AI behavior may lead to, along with likelihood, severity, and
immediacy labels that describe potential impacts on stakeholders. SafetyAnalyst
then aggregates all effects into a harmfulness score using 28 fully
interpretable weight parameters, which can be aligned to particular safety
preferences. We applied this framework to develop an open-source LLM prompt
safety classification system, distilled from 18.5 million harm-benefit features
generated by frontier LLMs on 19k prompts. On comprehensive benchmarks, we show
that SafetyAnalyst (average F1=0.81) outperforms existing moderation systems
(average F1$<$0.72) on prompt safety classification, while offering the
additional advantages of interpretability, transparency, and steerability.

</details>


### [120] [Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case Study on English-Korean Code-Switching](https://arxiv.org/pdf/2410.18436)
*Seoyeon Kim, Huiseo Kim, Chanjun Park, Jinyoung Yeo, Dongha Lee*

Main category: cs.CL

TL;DR: Code-switching (CS) can activate knowledge in LLMs for low-resource language tasks, as shown by the EnKoQA dataset and analysis.


<details>
  <summary>Details</summary>
Motivation: Address the English-centric bias in LLMs and explore CS as a way to leverage knowledge for low-resource languages.

Method: Created EnKoQA, a synthetic English-Korean CS dataset, and analyzed LLMs' knowledge activation via CS.

Result: CS effectively activates LLM knowledge, especially in language-specific domains, outperforming English text.

Conclusion: CS shows promise for improving LLM performance on low-resource language tasks.

Abstract: Recent large language models (LLMs) demonstrate multilingual abilities, yet
they are English-centric due to dominance of English in training corpora. The
limited resource for low-resource languages remains a crucial challenge.
Code-switching (CS), a phenomenon where multilingual speakers alternate between
languages in a discourse, can convey subtle cultural and linguistic nuances
that can be otherwise lost in translation and elicits language-specific
knowledge in human communications. In light of this, we investigate whether
code-switching can 'activate', or identify and leverage knowledge for reasoning
when LLMs solve low-resource language tasks. To facilitate the research, we
first present EnKoQA, a synthetic English-Korean CS question-answering dataset.
We provide comprehensive analysis on a variety of multilingual LLMs by
subdividing activation process into knowledge identification and knowledge
leveraging. Our results demonstrate that compared to English text, CS can
faithfully activate knowledge inside LLMs especially on language-specific
domains, suggesting the potential of code-switching on low-resource language
tasks.

</details>


### [121] [Understanding Synthetic Context Extension via Retrieval Heads](https://arxiv.org/pdf/2410.22316)
*Xinyu Zhao, Fangcong Yin, Greg Durrett*

Main category: cs.CL

TL;DR: The paper explores how fine-tuning LLMs with synthetic long-context data affects downstream tasks, identifying 'retrieval heads' as key to performance but showing synthetic data falls short of real data.


<details>
  <summary>Details</summary>
Motivation: To understand why synthetic context extension works and how it impacts long-context tasks like retrieval and reasoning.

Method: Fine-tuning LLMs on synthetic data with varied realism and diversity, analyzing retrieval heads and their role in performance.

Result: Synthetic data underperforms real data, but retrieval heads learned from both overlap and correlate with downstream success.

Conclusion: Retrieval heads are necessary but not sufficient; insights guide better synthetic data creation for long-context tasks.

Abstract: Long-context LLMs are increasingly in demand for applications such as
retrieval-augmented generation. To defray the cost of pretraining LLMs over
long contexts, recent work takes an approach of synthetic context extension:
fine-tuning LLMs with synthetically generated long-context data in a
post-training stage. However, it remains unclear how and why this synthetic
context extension imparts abilities for downstream long-context tasks. In this
paper, we investigate fine-tuning on synthetic data for three long-context
tasks that require retrieval and reasoning. We vary the realism of "needle"
concepts to be retrieved and diversity of the surrounding "haystack" context,
from using LLMs to construct synthetic documents to using templated relations
and creating symbolic datasets. We find that models trained on synthetic data
fall short of the real data, but surprisingly, the mismatch can be interpreted
and even predicted in terms of a special set of attention heads that are
responsible for retrieval over long context, retrieval heads (Wu et al., 2024).
The retrieval heads learned on synthetic data have high overlap with retrieval
heads learned on real data, and there is a strong correlation between the
recall of heads learned and the downstream performance of a model. Furthermore,
with attention knockout and activation patching, we mechanistically show that
retrieval heads are necessary and explain model performance, although they are
not totally sufficient. Our results shed light on how to interpret synthetic
data fine-tuning performance and how to approach creating better data for
learning real-world capabilities over long contexts.

</details>


### [122] [Controllable Context Sensitivity and the Knob Behind It](https://arxiv.org/pdf/2411.07404)
*Julian Minder, Kevin Du, Niklas Stoehr, Giovanni Monea, Chris Wendler, Robert West, Ryan Cotterell*

Main category: cs.CL

TL;DR: The paper explores controlling a language model's sensitivity to context vs. prior knowledge, identifying a 1-D subspace in a single layer that acts as a 'knob' for this behavior. Fine-tuned models achieve high accuracy, and the subspace generalizes to non-fine-tuned models.


<details>
  <summary>Details</summary>
Motivation: To understand and control how language models balance context and prior knowledge, enabling better performance in tasks like retrieval-augmented generation and question-answering.

Method: Design a task for controllable context sensitivity, fine-tune models (Llama-3.1, Mistral-v0.3, Gemma-2), and analyze layers to identify a 1-D subspace encoding context sensitivity.

Result: Fine-tuned models achieve 85-95% accuracy. The identified subspace generalizes to non-fine-tuned models and correlates with performance.

Conclusion: A single subspace controls the trade-off between context and prior knowledge, suggesting a simple underlying mechanism.

Abstract: When making predictions, a language model must trade off how much it relies
on its context vs. its prior knowledge. Choosing how sensitive the model is to
its context is a fundamental functionality, as it enables the model to excel at
tasks like retrieval-augmented generation and question-answering. In this
paper, we search for a knob which controls this sensitivity, determining
whether language models answer from the context or their prior knowledge. To
guide this search, we design a task for controllable context sensitivity. In
this task, we first feed the model a context (Paris is in England) and a
question (Where is Paris?); we then instruct the model to either use its prior
or contextual knowledge and evaluate whether it generates the correct answer
for both intents (either France or England). When fine-tuned on this task,
instruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it
with high accuracy (85-95%). Analyzing these high-performing models, we narrow
down which layers may be important to context sensitivity using a novel linear
time algorithm. Then, in each model, we identify a 1-D subspace in a single
layer that encodes whether the model follows context or prior knowledge.
Interestingly, while we identify this subspace in a fine-tuned model, we find
that the exact same subspace serves as an effective knob in not only that model
but also non-fine-tuned instruct and base models of that model family. Finally,
we show a strong correlation between a model's performance and how distinctly
it separates context-agreeing from context-ignoring answers in this subspace.
These results suggest a single subspace facilitates how the model chooses
between context and prior knowledge, hinting at a simple fundamental mechanism
that controls this behavior.

</details>


### [123] [LLäMmlein: Compact and Competitive German-Only Language Models from Scratch](https://arxiv.org/pdf/2411.11171)
*Jan Pfister, Julia Wunderle, Andreas Hotho*

Main category: cs.CL

TL;DR: Two German-only decoder models, LL"aMmlein 120M and 1B, were created and published for the German NLP community, with competitive performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To provide transparent, German-specific NLP models and training data for the German research community.

Method: Extensive data preprocessing, custom German tokenizer creation, model training, and evaluation using benchmarks like SuperGLEBer.

Result: Both models performed competitively, matching or surpassing similar-sized models, with quality scaling as expected but some task improvements plateauing early.

Conclusion: The models offer valuable insights for future resource allocation in German NLP model development.

Abstract: We create two German-only decoder models, LL\"aMmlein 120M and 1B,
transparently from scratch and publish them, along with the training data, for
the German NLP research community to use. The model training involved several
key steps, including extensive data preprocessing, the creation of a custom
German tokenizer, the training itself, as well as the evaluation of the final
models on various benchmarks. Throughout the training process, multiple
checkpoints were saved and analyzed using the SuperGLEBer benchmark to monitor
the models' learning dynamics. Compared to state-of-the-art models on the
SuperGLEBer benchmark, both LL\"aMmlein models performed competitively,
consistently matching or surpassing models with similar parameter sizes. The
results show that the models' quality scales with size as expected, but
performance improvements on some tasks plateaued early, offering valuable
insights into resource allocation for future model development.

</details>


### [124] [Overcoming Non-monotonicity in Transducer-based Streaming Generation](https://arxiv.org/pdf/2411.17170)
*Zhengrui Ma, Yang Feng, Min Zhang*

Main category: cs.CL

TL;DR: The paper introduces MonoAttn-Transducer, a method combining Transducer's decoding with learnable monotonic attention to handle non-monotonic alignments in streaming tasks like simultaneous translation.


<details>
  <summary>Details</summary>
Motivation: The Transducer architecture's input-synchronous decoding struggles with non-monotonic alignments, limiting its effectiveness in tasks like simultaneous translation.

Method: The proposed MonoAttn-Transducer uses a learnable monotonic attention mechanism and the forward-backward algorithm to infer alignment probabilities and estimate context representations without enumerating all alignments.

Result: Experiments demonstrate that MonoAttn-Transducer effectively manages non-monotonic alignments in streaming scenarios.

Conclusion: The MonoAttn-Transducer provides a robust solution for complex generation tasks requiring non-monotonic alignments.

Abstract: Streaming generation models are utilized across fields, with the Transducer
architecture being popular in industrial applications. However, its
input-synchronous decoding mechanism presents challenges in tasks requiring
non-monotonic alignments, such as simultaneous translation. In this research,
we address this issue by integrating Transducer's decoding with the history of
input stream via a learnable monotonic attention. Our approach leverages the
forward-backward algorithm to infer the posterior probability of alignments
between the predictor states and input timestamps, which is then used to
estimate the monotonic context representations, thereby avoiding the need to
enumerate the exponentially large alignment space during training. Extensive
experiments show that our MonoAttn-Transducer effectively handles non-monotonic
alignments in streaming scenarios, offering a robust solution for complex
generation tasks.

</details>


### [125] [ConKE: Conceptualization-Augmented Knowledge Editing in Large Language Models for Commonsense Reasoning](https://arxiv.org/pdf/2412.11418)
*Liyu Zhang, Weiqi Wang, Tianqing Fang, Yangqiu Song*

Main category: cs.CL

TL;DR: ConceptEdit improves LLMs' commonsense reasoning by integrating conceptualization and instantiation into knowledge editing, outperforming baselines in plausibility and QA benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address challenges in editing commonsense knowledge in LLMs, such as limited coverage, annotation infeasibility, and rigid knowledge formats.

Method: Introduces ConceptEdit, a framework using conceptualization and instantiation, verified by another LLM, to enhance commonsense reasoning.

Result: LLMs with ConceptEdit generate more plausible commonsense knowledge and perform better on QA benchmarks.

Conclusion: ConceptEdit effectively enhances LLMs' commonsense capabilities, with publicly available resources for further use.

Abstract: Knowledge Editing (KE) aims to adjust a Large Language Model's (LLM) internal
representations and parameters to correct inaccuracies and improve output
consistency without incurring the computational expense of re-training the
entire model. However, editing commonsense knowledge still faces difficulties,
including limited knowledge coverage in existing resources, the infeasibility
of annotating labels for an overabundance of commonsense knowledge, and the
strict knowledge formats of current editing methods. In this paper, we address
these challenges by presenting ConceptEdit, a framework that integrates
conceptualization and instantiation into the KE pipeline for LLMs to enhance
their commonsense reasoning capabilities. ConceptEdit dynamically diagnoses
implausible commonsense knowledge within an LLM using another verifier LLM and
augments the source knowledge to be edited with conceptualization for stronger
generalizability. Experimental results demonstrate that LLMs enhanced with
ConceptEdit successfully generate commonsense knowledge with improved
plausibility compared to other baselines and achieve stronger performance
across multiple question answering benchmarks. Our data, code, and models are
publicly available at https://github.com/HKUST-KnowComp/ConKE.

</details>


### [126] [Core Context Aware Transformers for Long Context Language Modeling](https://arxiv.org/pdf/2412.12465)
*Yaofo Chen, Zeng You, Shuhai Zhang, Haokun Li, Yirui Li, Yaowei Wang, Mingkui Tan*

Main category: cs.CL

TL;DR: The paper introduces Core Context Aware (CCA) Attention, a plug-and-play method for efficient long-context modeling in LLMs, addressing redundancy and computational overhead.


<details>
  <summary>Details</summary>
Motivation: Transformer-based LLMs struggle with redundant context in long sequences, harming performance and efficiency.

Method: CCA Attention includes a globality-aware pooling module to compress tokens and a locality-preserving module to retain local context.

Result: CCA-Attention outperforms state-of-the-art methods in long-context modeling and computational efficiency.

Conclusion: The proposed method effectively reduces redundancy and enhances long-term dependency modeling with minimal fine-tuning.

Abstract: Transformer-based Large Language Models (LLMs) have exhibited remarkable
success in extensive tasks primarily attributed to self-attention mechanism,
which requires a token to consider all preceding tokens as its context to
compute attention. However, when the context length L becomes very large (e.g.,
128K), the amount of potentially redundant information in the context tends to
increase. The redundant context not only hampers the modeling representation
performance but also incurs unnecessary computational and storage overhead. In
this paper, we propose a plug-and-play Core Context Aware (CCA) Attention for
efficient long-context modeling, comprising two complementary modules: 1)
Globality-aware pooling module groups input tokens and dynamically compresses
each group into one core token based on their significance. In this way, our
method automatically focuses and strengthens core context while diminishing
redundancy during the learning process, leading to effective long-term
dependency modeling. 2) Locality-preserving module incorporates neighboring
tokens to preserve local context for detailed representation. Notably, our
CCA-Attention is able to replace the self-attention module in existing LLMs
with minimal fine-tuning cost. Extensive experimental results show the
superiority of our method in both long-context modeling and computational
efficiency over state-of-the-art methods.

</details>


### [127] [How to Synthesize Text Data without Model Collapse?](https://arxiv.org/pdf/2412.14689)
*Xuekai Zhu, Daixuan Cheng, Hengli Li, Kaiyan Zhang, Ermo Hua, Xingtai Lv, Ning Ding, Zhouhan Lin, Zilong Zheng, Bowen Zhou*

Main category: cs.CL

TL;DR: The paper investigates the impact of synthetic data on language model training and proposes token-level editing on human-produced data to prevent model collapse, showing improved performance.


<details>
  <summary>Details</summary>
Motivation: With AI models increasingly trained on synthetic data, understanding its impact and preventing model collapse is critical for future model development.

Method: Pre-train models with varying synthetic data proportions, analyze distributional shifts, and propose token-level editing to create semi-synthetic data.

Result: Higher synthetic data proportions degrade performance; token-level editing prevents collapse and improves model performance.

Conclusion: Token-level editing on human data is a viable solution to mitigate model collapse and enhance language model training.

Abstract: Model collapse in synthetic data indicates that iterative training on
self-generated data leads to a gradual decline in performance. With the
proliferation of AI models, synthetic data will fundamentally reshape the web
data ecosystem. Future GPT-$\{n\}$ models will inevitably be trained on a blend
of synthetic and human-produced data. In this paper, we focus on two questions:
what is the impact of synthetic data on language model training, and how to
synthesize data without model collapse? We first pre-train language models
across different proportions of synthetic data, revealing a negative
correlation between the proportion of synthetic data and model performance. We
further conduct statistical analysis on synthetic data to uncover
distributional shift phenomenon and over-concentration of n-gram features.
Inspired by the above findings, we propose token editing on human-produced data
to obtain semi-synthetic data. As a proof of concept, we theoretically
demonstrate that token-level editing can prevent model collapse, as the test
error is constrained by a finite upper bound. We conduct extensive experiments
on pre-training from scratch, continual pre-training, and supervised
fine-tuning. The results validate our theoretical proof that token-level
editing improves model performance.

</details>


### [128] [Revisiting In-Context Learning with Long Context Language Models](https://arxiv.org/pdf/2412.16926)
*Jinheon Baek, Sun Jae Lee, Prakhar Gupta, Geunseob Oh, Siddharth Dalmia, Prateek Kolhar*

Main category: cs.CL

TL;DR: In-Context Learning (ICL) performance with Long Context Language Models (LCLMs) is not significantly improved by sophisticated example selection methods; instead, filling the context window with sufficient examples, sometimes augmented, boosts performance.


<details>
  <summary>Details</summary>
Motivation: To investigate whether ICL performance in a many-shot regime with LCLMs still depends on example selection methods, given the expanded context window size.

Method: Revisiting example selection approaches in LCLMs through experiments on 18 datasets across 4 tasks, comparing sophisticated techniques to random selection and exploring data augmentation.

Result: Sophisticated example selection methods do not outperform random selection. Augmenting examples to fill the context window improves ICL performance by 5%.

Conclusion: The challenge with LCLMs shifts from selecting effective examples to collecting enough examples to utilize the context window, with augmentation further enhancing performance.

Abstract: In-Context Learning (ICL) is a technique by which language models make
predictions based on examples provided in their input context. Previously,
their context window size imposed a limit on the number of examples that can be
shown, making example selection techniques crucial for identifying the
maximally effective set of examples. However, the recent advent of Long Context
Language Models (LCLMs) has significantly increased the number of examples that
can be included in context, raising an important question of whether ICL
performance in a many-shot regime is still sensitive to the method of sample
selection. To answer this, we revisit these approaches in the context of LCLMs
through extensive experiments on 18 datasets spanning 4 tasks. Surprisingly, we
observe that sophisticated example selection techniques do not yield
significant improvements over a simple random sample selection method. Instead,
we discover that the advent of LCLMs has fundamentally shifted the challenge of
ICL from that of selecting the most effective examples to that of collecting
sufficient examples to fill the context window. Specifically, in certain
datasets, including all available examples does not fully utilize the context
window; however, by augmenting the examples in context with a simple data
augmentation approach, we substantially improve ICL performance by 5%.

</details>


### [129] [FitCF: A Framework for Automatic Feature Importance-guided Counterfactual Example Generation](https://arxiv.org/pdf/2501.00777)
*Qianli Wang, Nils Feldhus, Simon Ostermann, Luis Felipe Villa-Arenas, Sebastian Möller, Vera Schmitt*

Main category: cs.CL

TL;DR: ZeroCF and FitCF are introduced for zero-shot and few-shot counterfactual generation in NLP, leveraging feature attribution methods and label flip verification to improve quality.


<details>
  <summary>Details</summary>
Motivation: Automated generation of counterfactual examples is challenging for LLMs, despite their performance. This paper aims to improve counterfactual generation using feature attribution methods.

Method: ZeroCF uses important words from feature attribution for zero-shot generation. FitCF verifies counterfactuals via label flip and uses them for few-shot prompting.

Result: FitCF outperforms baselines, with ablation studies showing the impact of core components. LIME and Integrated Gradients are effective attribution methods.

Conclusion: Faithfulness of feature attribution scores correlates with counterfactual quality, a key finding for future research.

Abstract: Counterfactual examples are widely used in natural language processing (NLP)
as valuable data to improve models, and in explainable artificial intelligence
(XAI) to understand model behavior. The automated generation of counterfactual
examples remains a challenging task even for large language models (LLMs),
despite their impressive performance on many tasks. In this paper, we first
introduce ZeroCF, a faithful approach for leveraging important words derived
from feature attribution methods to generate counterfactual examples in a
zero-shot setting. Second, we present a new framework, FitCF, which further
verifies aforementioned counterfactuals by label flip verification and then
inserts them as demonstrations for few-shot prompting, outperforming two
state-of-the-art baselines. Through ablation studies, we identify the
importance of each of FitCF's core components in improving the quality of
counterfactuals, as assessed through flip rate, perplexity, and similarity
measures. Furthermore, we show the effectiveness of LIME and Integrated
Gradients as backbone attribution methods for FitCF and find that the number of
demonstrations has the largest effect on performance. Finally, we reveal a
strong correlation between the faithfulness of feature attribution scores and
the quality of generated counterfactuals, which we hope will serve as an
important finding for future research in this direction.

</details>


### [130] [PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models](https://arxiv.org/pdf/2501.03124)
*Mingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, Yu Cheng*

Main category: cs.CL

TL;DR: PRMBench is introduced to evaluate Process-level Reward Models (PRMs) for fine-grained error detection in reasoning tasks, revealing weaknesses in current models.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks lack systematic evaluation of PRMs' nuanced error detection capabilities in reasoning processes.

Method: PRMBench, a benchmark with 6,216 problems and 83,456 step-level labels, assesses PRMs across simplicity, soundness, and sensitivity dimensions.

Result: Experiments on 15 models show significant weaknesses in current PRMs, highlighting challenges in process-level evaluation.

Conclusion: PRMBench aims to advance PRM research by providing a robust evaluation framework.

Abstract: Process-level Reward Models (PRMs) are crucial for complex reasoning and
decision-making tasks, where each intermediate step plays an important role in
the reasoning process. Since language models are prone to various types of
errors during the reasoning process, PRMs are required to possess nuanced
capabilities for detecting various implicit error types in real-world
scenarios. However, current benchmarks primarily focus on step correctness,
failing to evaluate PRMs' performance systematically. To address this gap, we
introduce PRMBench, a process-level benchmark specifically designed to assess
the fine-grained error detection capabilities of PRMs. PRMBench comprises 6,216
carefully designed problems and 83,456 step-level labels, evaluating models
across multiple dimensions, including simplicity, soundness, and sensitivity.
In our experiments on 15 models, spanning both open-source PRMs and
closed-source large language models prompted as critic models, we uncover
significant weaknesses in current PRMs. These findings underscore the
challenges inherent in process-level evaluation and highlight key directions
for future research. We hope PRMBench can be a robust bench for advancing
research on PRM evaluation and development.

</details>


### [131] [LLMs Reproduce Stereotypes of Sexual and Gender Minorities](https://arxiv.org/pdf/2501.05926)
*Ruby Ostrow, Adam Lopez*

Main category: cs.CL

TL;DR: The paper examines gender and sexuality biases in large language models (LLMs) beyond binary categories, revealing negative stereotypes towards minorities in both survey responses and text generation.


<details>
  <summary>Details</summary>
Motivation: Existing research on gender bias in NLP systems often overlooks non-binary identities, conflating gender with sex. This study aims to address this gap by analyzing biases towards sexual and gender minorities.

Method: The study uses the Stereotype Content Model to analyze biases in survey responses and extends it to text generation tasks, evaluating LLMs' outputs.

Result: LLMs exhibit negative stereotypes towards sexual and gender minorities, both in survey responses and text generation, amplifying representational harms.

Conclusion: The findings highlight the need for more inclusive approaches in NLP to mitigate biases against non-binary and minority groups.

Abstract: A large body of research has found substantial gender bias in NLP systems.
Most of this research takes a binary, essentialist view of gender: limiting its
variation to the categories _men_ and _women_, conflating gender with sex, and
ignoring different sexual identities. But gender and sexuality exist on a
spectrum, so in this paper we study the biases of large language models (LLMs)
towards sexual and gender minorities beyond binary categories. Grounding our
study in a widely used social psychology model -- the Stereotype Content Model
-- we demonstrate that English-language survey questions about social
perceptions elicit more negative stereotypes of sexual and gender minorities
from both humans and LLMs. We then extend this framework to a more realistic
use case: text generation. Our analysis shows that LLMs generate stereotyped
representations of sexual and gender minorities in this setting, showing that
they amplify representational harms in creative writing, a widely advertised
use for LLMs.

</details>


### [132] [Gender-Neutral Large Language Models for Medical Applications: Reducing Bias in PubMed Abstracts](https://arxiv.org/pdf/2501.06365)
*Elizabeth Schaefer, Kirk Roberts*

Main category: cs.CL

TL;DR: A pipeline to reduce gender bias in LLMs for medical literature by neutralizing gendered pronouns in PubMed abstracts, achieving 70% inclusivity with MOBERT.


<details>
  <summary>Details</summary>
Motivation: Address gender bias in medical LLMs by modifying gendered occupational pronouns to ensure equitable language modeling.

Method: Developed MOBERT, a BERT-based model trained on neutralized PubMed abstracts (1965-1980), compared with 1965BERT (original dataset).

Result: MOBERT achieved 70% inclusive pronoun replacement vs. 4% for 1965BERT; accuracy correlated with occupational term frequency.

Conclusion: Proposed dataset expansion and pipeline refinement to enhance performance and equity in medical LLMs.

Abstract: This paper presents a pipeline for mitigating gender bias in large language
models (LLMs) used in medical literature by neutralizing gendered occupational
pronouns. A dataset of 379,000 PubMed abstracts from 1965-1980 was processed to
identify and modify pronouns tied to professions. We developed a BERT-based
model, "Modern Occupational Bias Elimination with Refined Training," or
"MOBERT," trained on these neutralized abstracts, and compared its performance
with "1965BERT," trained on the original dataset. MOBERT achieved a 70%
inclusive replacement rate, while 1965BERT reached only 4%. A further analysis
of MOBERT revealed that pronoun replacement accuracy correlated with the
frequency of occupational terms in the training data. We propose expanding the
dataset and refining the pipeline to improve performance and ensure more
equitable language modeling in medical applications.

</details>


### [133] [K-COMP: Retrieval-Augmented Medical Domain Question Answering With Knowledge-Injected Compressor](https://arxiv.org/pdf/2501.13567)
*Jeonghun Cho, Gary Geunbae Lee*

Main category: cs.CL

TL;DR: The paper introduces K-comp (Knowledge-injected compressor) to enhance retrieval-augmented QA by injecting prior knowledge into the compression of retrieved documents, improving accuracy and trust in answers.


<details>
  <summary>Details</summary>
Motivation: Retrieved documents in closed-domain QA often contain irrelevant or inaccurate information, leading to model mistrust and hallucinations. The goal is to align question intent with compressed context.

Method: Proposes K-comp, which generates prior knowledge for answer facilitation and integrates it into autoregressive compression of retrieved passages.

Result: K-comp ensures alignment between question intent and compressed context, guiding reader models to relevant answers and increasing trust in the context.

Conclusion: K-comp effectively addresses issues of irrelevant information and mistrust in retrieval-augmented QA by leveraging knowledge injection during compression.

Abstract: Retrieval-augmented question answering (QA) integrates external information
and thereby increases the QA accuracy of reader models that lack domain
knowledge. However, documents retrieved for closed domains require high
expertise, so the reader model may have difficulty fully comprehending the
text. Moreover, the retrieved documents contain thousands of tokens, some
unrelated to the question. As a result, the documents include some inaccurate
information, which could lead the reader model to mistrust the passages and
could result in hallucinations. To solve these problems, we propose K-comp
(Knowledge-injected compressor) which provides the knowledge required to answer
correctly. The compressor automatically generates the prior knowledge necessary
to facilitate the answer process prior to compression of the retrieved
passages. Subsequently, the passages are compressed autoregressively, with the
generated knowledge being integrated into the compression process. This process
ensures alignment between the question intent and the compressed context. By
augmenting this prior knowledge and concise context, the reader models are
guided toward relevant answers and trust the context.

</details>


### [134] [Redundancy Principles for MLLMs Benchmarks](https://arxiv.org/pdf/2501.13953)
*Zicheng Zhang, Xiangyu Zhao, Xinyu Fang, Chunyi Li, Xiaohong Liu, Xiongkuo Min, Haodong Duan, Kai Chen, Guangtao Zhai*

Main category: cs.CL

TL;DR: The paper addresses redundancy in Multi-modality Large Language Model (MLLM) benchmarks, analyzing it across three perspectives and proposing principles for more effective benchmark construction.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of MLLM benchmarks has led to significant redundancy, necessitating a critical assessment to improve future benchmark development.

Method: The study quantitatively measures redundancy in MLLM evaluations by analyzing hundreds of MLLMs' performance across over 20 benchmarks, focusing on three key redundancy perspectives.

Result: The analysis provides insights into redundancy levels in existing benchmarks and offers strategies to address these issues.

Conclusion: The paper aims to guide future MLLM benchmark development by refining redundancy issues and proposing targeted principles.

Abstract: With the rapid iteration of Multi-modality Large Language Models (MLLMs) and
the evolving demands of the field, the number of benchmarks produced annually
has surged into the hundreds. The rapid growth has inevitably led to
significant redundancy among benchmarks. Therefore, it is crucial to take a
step back and critically assess the current state of redundancy and propose
targeted principles for constructing effective MLLM benchmarks. In this paper,
we focus on redundancy from three key perspectives: 1) Redundancy of benchmark
capability dimensions, 2) Redundancy in the number of test questions, and 3)
Cross-benchmark redundancy within specific domains. Through the comprehensive
analysis over hundreds of MLLMs' performance across more than 20 benchmarks, we
aim to quantitatively measure the level of redundancy lies in existing MLLM
evaluations, provide valuable insights to guide the future development of MLLM
benchmarks, and offer strategies to refine and address redundancy issues
effectively. The code is available at
https://github.com/zzc-1998/Benchmark-Redundancy.

</details>


### [135] [Domaino1s: Guiding LLM Reasoning for Explainable Answers in High-Stakes Domains](https://arxiv.org/pdf/2501.14431)
*Xu Chu, Zhijie Tan, Hanlin Xue, Guanyu Wang, Tong Mo, Weiping Li*

Main category: cs.CL

TL;DR: Domaino1s enhances LLMs' reasoning for high-stakes domains via supervised fine-tuning, tree search, and a new explainability metric.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack reasoning processes and explanations in high-stakes domains, limiting user confidence.

Method: Uses supervised fine-tuning with domain-specific datasets (CoT-stock-2k, CoT-legal-2k) and Selective Tree Exploration for reasoning. Introduces PROOF-Score for explainability.

Result: Demonstrates leading performance and explainability in stock investment and legal QA tasks.

Conclusion: Domaino1s improves LLM reasoning and explainability for domain-specific tasks, supported by datasets and a new metric.

Abstract: Large Language Models (LLMs) are widely applied to downstream domains.
However, current LLMs for high-stakes domain tasks, such as financial
investment and legal QA, typically generate brief answers without reasoning
processes and explanations. This limits users' confidence in making decisions
based on their responses. While original CoT shows promise, it lacks
self-correction mechanisms during reasoning. This work introduces Domain$o1$s,
which enhances LLMs' reasoning capabilities on domain tasks through supervised
fine-tuning and tree search. We construct CoT-stock-2k and CoT-legal-2k
datasets for fine-tuning models that activate domain-specific reasoning steps
based on their judgment. Additionally, we propose Selective Tree Exploration to
spontaneously explore solution spaces and sample optimal reasoning paths to
improve performance. We also introduce PROOF-Score, a new metric for evaluating
domain models' explainability, complementing traditional accuracy metrics with
richer assessment dimensions. Extensive experiments on stock investment
recommendation and legal reasoning QA tasks demonstrate Domaino1s's leading
performance and explainability. Our code is available at
https://github.com/Hyalinesky/Domaino1s.

</details>


### [136] [A Checks-and-Balances Framework for Context-Aware Ethical AI Alignment](https://arxiv.org/pdf/2502.00136)
*Edward Y. Chang*

Main category: cs.CL

TL;DR: A checks-and-balances framework for ethical LLMs, inspired by governmental systems, uses three components (LLMs, DIKE, ERIS) to regulate behavior via emotional conditioning and adversarial testing.


<details>
  <summary>Details</summary>
Motivation: To address ethical alignment in LLMs by mimicking governmental checks-and-balances and regulating emotion-driven behaviors.

Method: Three-branch system (LLMs, DIKE, ERIS) with self-supervised learning for emotion-behavior mapping and adversarial testing.

Result: Framework successfully directs LLM behaviors toward ethical outcomes while maintaining independence in knowledge generation and oversight.

Conclusion: The proposed framework effectively balances ethical alignment and behavioral modulation in LLMs through structural separation and emotional regulation.

Abstract: This paper introduces a checks-and-balances framework for ethical alignment
of Large Language Models (LLMs), inspired by three-branch governmental systems.
It implements three independent yet interacting components: LLMs as the
executive branch for knowledge generation, DIKE as the legislative branch
establishing ethical guardrails, and ERIS as the judicial branch for contextual
interpretation. Beyond structural separation, we address a fundamental
challenge: regulating emotion to shape behaviors. Drawing from psychological
theories where managing emotional responses prevents harmful behaviors, we
develop a self-supervised learning pipeline that maps emotions to linguistic
behaviors, enabling precise behavioral modulation through emotional
conditioning. By integrating this approach with adversarial testing, our
framework demonstrates how DIKE and ERIS direct linguistic behaviors toward
ethical outcomes while preserving independence throughout knowledge generation,
ethical oversight, and contextual interpretation.

</details>


### [137] [Mitigating Heterogeneous Token Overfitting in LLM Knowledge Editing](https://arxiv.org/pdf/2502.00602)
*Tianci Liu, Ruirui Li, Zihan Dong, Hui Liu, Xianfeng Tang, Qingyu Yin, Linjun Zhang, Haoyu Wang, Jing Gao*

Main category: cs.CL

TL;DR: OVERTONE is a token-level smoothing method to mitigate heterogeneous token overfitting (HTO) in knowledge editing for LLMs, improving reasoning without preference data.


<details>
  <summary>Details</summary>
Motivation: LLMs' static training leads to outdated knowledge; KE updates specific knowledge without compromising pre-trained capabilities, but reasoning degrades due to HTO.

Method: Proposes OVERTONE, a token-level smoothing method that adaptively refines target distributions to mitigate HTO.

Result: OVERTONE improves reasoning with negligible overhead, works across methods and LLMs, and doesn't need preference data.

Conclusion: OVERTONE effectively addresses HTO in knowledge editing, enhancing LLM adaptability without extra data.

Abstract: Large language models (LLMs) have achieved remarkable performance on various
natural language tasks. However, they are trained on static corpora and their
knowledge can become outdated quickly in the fast-changing world. This
motivates the development of knowledge editing (KE) to update specific
knowledge in LLMs without changing unrelated others or compromising their
pre-trained capabilities. Previous efforts sought to update a small amount of
parameters of a LLM and proved effective for making selective updates.
Nonetheless, the edited LLM often exhibits degraded ability to reason about the
new knowledge. In this work, we identify a key issue: heterogeneous token
overfitting (HTO), where the LLM overfits different tokens in the provided
knowledge at varying rates. To tackle this, we propose OVERTONE, a token-level
smoothing method that mitigates HTO by adaptively refining the target
distribution. Theoretically, OVERTONE offers better parameter updates with
negligible computation overhead. It also induces an implicit DPO but does not
require preference data pairs. Extensive experiments across four editing
methods, two LLMs, and diverse scenarios demonstrate the effectiveness and
versatility of our method.

</details>


### [138] [Advancing Reasoning in Large Language Models: Promising Methods and Approaches](https://arxiv.org/pdf/2502.03671)
*Avinash Patil, Aryan Jadon*

Main category: cs.CL

TL;DR: A survey on enhancing reasoning in LLMs, covering methods like prompting, architectural innovations, and learning paradigms, along with evaluation frameworks and open challenges.


<details>
  <summary>Details</summary>
Motivation: Despite LLMs' success in NLP tasks, their reasoning capabilities lag behind human expectations, prompting a need for improvement.

Method: Categorizes techniques into prompting strategies, architectural innovations, and learning paradigms, and reviews evaluation frameworks.

Result: Identifies key advancements and open challenges like hallucinations and robustness in reasoning-augmented LLMs.

Conclusion: Highlights promising research directions and practical applications for improving reasoning in LLMs.

Abstract: Large Language Models (LLMs) have succeeded remarkably in various natural
language processing (NLP) tasks, yet their reasoning capabilities remain a
fundamental challenge. While LLMs exhibit impressive fluency and factual
recall, their ability to perform complex reasoning-spanning logical deduction,
mathematical problem-solving, commonsense inference, and multi-step
reasoning-often falls short of human expectations. This survey provides a
comprehensive review of emerging techniques enhancing reasoning in LLMs. We
categorize existing methods into key approaches, including prompting strategies
(e.g., Chain-of-Thought reasoning, Self-Consistency, and Tree-of-Thought
reasoning), architectural innovations (e.g., retrieval-augmented models,
modular reasoning networks, and neuro-symbolic integration), and learning
paradigms (e.g., fine-tuning with reasoning-specific datasets, reinforcement
learning, and self-supervised reasoning objectives). Additionally, we explore
evaluation frameworks used to assess reasoning in LLMs and highlight open
challenges, such as hallucinations, robustness, and reasoning generalization
across diverse tasks. By synthesizing recent advancements, this survey aims to
provide insights into promising directions for future research and practical
applications of reasoning-augmented LLMs.

</details>


### [139] [Beyond External Monitors: Enhancing Transparency of Large Language Models for Easier Monitoring](https://arxiv.org/pdf/2502.05242)
*Guanxu Chen, Dongrui Liu, Tao Luo, Lijie Hu, Jing Shao*

Main category: cs.CL

TL;DR: TELLME improves LLM transparency and monitoring, enhancing task performance and generalization.


<details>
  <summary>Details</summary>
Motivation: Current methods for monitoring LLMs (like CoTs) are inadequate, and hidden representation techniques lack internal monitoring. TELLME addresses this gap.

Method: Proposes TELLME, a method to enhance LLM transparency and enable better monitoring of behaviors like safety risks.

Result: TELLME improves transparency and task performance in trustworthiness tasks (e.g., safety monitoring, detoxification).

Conclusion: TELLME not only enhances monitoring but also theoretically improves LLM generalization via optimal transport theory.

Abstract: Large language models (LLMs) are becoming increasingly capable, but the
mechanisms of their thinking and decision-making process remain unclear.
Chain-of-thoughts (CoTs) have been commonly utilized to monitor LLMs, but this
strategy fails to accurately reflect LLMs' thinking process. Techniques based
on LLMs' hidden representations provide an inner perspective to monitor their
latent thinking. However, previous methods only try to develop external
monitors instead of making LLMs themselves easier to monitor. In this paper, we
propose a novel method TELLME, improving the transparency of LLMs and helping
monitors identify unsuitable and sensitive behaviors. Furthermore, we showcase
the applications of TELLME on trustworthiness tasks (\eg, safety risks
monitoring tasks and detoxification tasks), where LLMs achieve consistent
improvement in transparency and task performance. More crucially, we
theoretically analyze the improvement of TELLME on LLMs' generalization ability
through optimal transport theory.

</details>


### [140] [LongReD: Mitigating Short-Text Degradation of Long-Context Large Language Models via Restoration Distillation](https://arxiv.org/pdf/2502.07365)
*Zican Dong, Junyi Li, Jinhao Jiang, Mingyu Xu, Wayne Xin Zhao, Bingning Wang, Weipeng Chen*

Main category: cs.CL

TL;DR: LongReD addresses performance degradation in LLMs on short-text tasks by minimizing distribution discrepancies and leveraging distillation techniques.


<details>
  <summary>Details</summary>
Motivation: LLMs with extended context windows often degrade in short-text tasks due to distribution drift and catastrophic forgetting.

Method: Proposes LongReD, which uses restoration distillation to align hidden states and output distributions between extended and original models.

Result: LongReD preserves short-text performance while maintaining or improving long-text handling.

Conclusion: LongReD effectively mitigates short-text degradation in LLMs with extended contexts.

Abstract: Large language models (LLMs) have gained extended context windows through
scaling positional encodings and lightweight continual pre-training. However,
this often leads to degraded performance on short-text tasks, while the reasons
for this degradation remain insufficiently explored. In this work, we identify
two primary factors contributing to this issue: distribution drift in hidden
states and attention scores, and catastrophic forgetting during continual
pre-training. To address these challenges, we propose Long Context Pre-training
with Restoration Distillation (LongReD), a novel approach designed to mitigate
short-text performance degradation through minimizing the distribution
discrepancy between the extended and original models. Besides training on long
texts, LongReD distills the hidden state of selected layers from the original
model on short texts. Additionally, LongReD also introduces a short-to-long
distillation, aligning the output distribution on short texts with that on long
texts by leveraging skipped positional indices. Experiments on common text
benchmarks demonstrate that LongReD effectively preserves the model's
short-text performance while maintaining comparable or even better capacity to
handle long texts than baselines. Our code is available at
https://github.com/RUCAIBox/LongReD.

</details>


### [141] [CoSER: Coordinating LLM-Based Persona Simulation of Established Roles](https://arxiv.org/pdf/2502.09082)
*Xintao Wang, Heng Wang, Yifei Zhang, Xinfeng Yuan, Rui Xu, Jen-tse Huang, Siyu Yuan, Haoran Guo, Jiangjie Chen, Shuchang Zhou, Wei Wang, Yanghua Xiao*

Main category: cs.CL

TL;DR: CoSER introduces a high-quality dataset, open models, and an evaluation protocol for role-playing language agents (RPLAs) of established characters, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The lack of authentic character datasets and nuanced evaluation methods for RPLAs motivates the creation of CoSER.

Method: CoSER provides a dataset of 17,966 characters from 771 books, introduces given-circumstance acting for training/evaluation, and develops CoSER 8B and 70B models based on LLaMA-3.1.

Result: CoSER 70B surpasses or matches GPT-4o on benchmarks, achieving 75.80% and 93.47% accuracy on InCharacter and LifeChoice benchmarks.

Conclusion: CoSER advances RPLA training, evaluation, and retrieval, demonstrating the value of its dataset and models.

Abstract: Role-playing language agents (RPLAs) have emerged as promising applications
of large language models (LLMs). However, simulating established characters
presents a challenging task for RPLAs, due to the lack of authentic character
datasets and nuanced evaluation methods using such data. In this paper, we
present CoSER, a collection of a high-quality dataset, open models, and an
evaluation protocol towards effective RPLAs of established characters. The
CoSER dataset covers 17,966 characters from 771 renowned books. It provides
authentic dialogues with real-world intricacies, as well as diverse data types
such as conversation setups, character experiences and internal thoughts.
Drawing from acting methodology, we introduce given-circumstance acting for
training and evaluating role-playing LLMs, where LLMs sequentially portray
multiple characters in book scenes. Using our dataset, we develop CoSER 8B and
CoSER 70B, i.e., advanced open role-playing LLMs built on LLaMA-3.1 models.
Extensive experiments demonstrate the value of the CoSER dataset for RPLA
training, evaluation and retrieval. Moreover, CoSER 70B exhibits
state-of-the-art performance surpassing or matching GPT-4o on our evaluation
and three existing benchmarks, i.e., achieving 75.80% and 93.47% accuracy on
the InCharacter and LifeChoice benchmarks respectively.

</details>


### [142] [Towards Achieving Concept Completeness for Textual Concept Bottleneck Models](https://arxiv.org/pdf/2502.11100)
*Milan Bhan, Yann Choho, Pierre Moreau, Jean-Noel Vittaut, Nicolas Chesneau, Marie-Jeanne Lesot*

Main category: cs.CL

TL;DR: CT-CBM is an unsupervised TCBM generator that builds concept labels without human input, improving interpretability in NLP classifiers.


<details>
  <summary>Details</summary>
Motivation: To eliminate the need for predefined human-labeled concepts and LLM annotations in interpretable text classification models.

Method: Uses a small language model to iteratively identify and add important concepts in the bottleneck layer, creating a complete concept basis.

Result: Achieves superior performance in concept basis completeness and detection accuracy compared to competitors.

Conclusion: CT-CBM provides a reliable solution for enhancing interpretability in NLP classifiers.

Abstract: Textual Concept Bottleneck Models (TCBMs) are interpretable-by-design models
for text classification that predict a set of salient concepts before making
the final prediction. This paper proposes Complete Textual Concept Bottleneck
Model (CT-CBM), a novel TCBM generator building concept labels in a fully
unsupervised manner using a small language model, eliminating both the need for
predefined human labeled concepts and LLM annotations. CT-CBM iteratively
targets and adds important and identifiable concepts in the bottleneck layer to
create a complete concept basis. CT-CBM achieves striking results against
competitors in terms of concept basis completeness and concept detection
accuracy, offering a promising solution to reliably enhance interpretability of
NLP classifiers.

</details>


### [143] [ReLearn: Unlearning via Learning for Large Language Models](https://arxiv.org/pdf/2502.11190)
*Haoming Xu, Ningyuan Zhao, Liming Yang, Sendong Zhao, Shumin Deng, Mengru Wang, Bryan Hooi, Nay Oo, Huajun Chen, Ningyu Zhang*

Main category: cs.CL

TL;DR: ReLearn is a new unlearning method for language models that avoids performance degradation by using data augmentation and fine-tuning, with improved evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: Current unlearning methods disrupt model performance and linguistic coherence, and existing metrics inadequately assess fluency and relevance.

Method: Proposes ReLearn, a data augmentation and fine-tuning pipeline, and introduces new evaluation metrics (KFR, KRR, LS).

Result: ReLearn achieves targeted forgetting while maintaining high-quality output and preserves coherent text generation.

Conclusion: ReLearn outperforms reverse optimization methods by preserving linguistic coherence and introducing better evaluation metrics.

Abstract: Current unlearning methods for large language models usually rely on reverse
optimization to reduce target token probabilities. However, this paradigm
disrupts the subsequent tokens prediction, degrading model performance and
linguistic coherence. Moreover, existing evaluation metrics overemphasize
contextual forgetting while inadequately assessing response fluency and
relevance. To address these challenges, we propose ReLearn, a data augmentation
and fine-tuning pipeline for effective unlearning, along with a comprehensive
evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR)
and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and
Linguistic Score (LS) to evaluate generation quality. Our experiments show that
ReLearn successfully achieves targeted forgetting while preserving high-quality
output. Through mechanistic analysis, we further demonstrate how reverse
optimization disrupts coherent text generation, while ReLearn preserves this
essential capability. Code is available at https://github.com/zjunlp/unlearn.

</details>


### [144] [Which Retain Set Matters for LLM Unlearning? A Case Study on Entity Unlearning](https://arxiv.org/pdf/2502.11441)
*Hwan Chang, Hwanhee Lee*

Main category: cs.CL

TL;DR: The paper explores how unlearning in LLMs affects the retain set, identifying syntactic similarity as a key factor in performance drops and improvements.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns arise from LLMs retaining sensitive data; unlearning aims to remove such data without harming overall performance, but the impact on the retain set is understudied.

Method: A case study on entity unlearning introduces the Syntactically Similar Neighbor Set to analyze performance effects on the retain set.

Result: Syntactically similar queries suffer the most during unlearning but, when used for regularization, improve performance across subsets.

Conclusion: Syntactic similarity is more critical than domain or entity relationships for effective LLM unlearning.

Abstract: Large language models (LLMs) risk retaining unauthorized or sensitive
information from their training data, which raises privacy concerns. LLM
unlearning seeks to mitigate these risks by selectively removing specified data
while maintaining overall model performance. However, most existing work focus
on methods to achieve effective forgetting and does not provide a detailed
analysis of the retain set, the portion of training data that is not targeted
for removal. In this paper, we investigate the effects of unlearning on various
subsets of the retain set through a case study on entity unlearning. We
introduce the Syntactically Similar Neighbor Set, a group of queries that share
similar syntactic structures with the data targeted for removal, and show that
this subset suffers the greatest performance drop during unlearning. Moreover,
when used for regularization, this set not only preserves performance on
syntactically similar queries but also delivers comparable or improved results
across other data subsets. Our results highlight that syntactic similarity is a
critical factor, potentially more so than domain or entity relationships, in
achieving effective and practical LLM unlearning.

</details>


### [145] [BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion Recognition Datasets for 28 Languages](https://arxiv.org/pdf/2502.11926)
*Shamsuddeen Hassan Muhammad, Nedjma Ousidhoum, Idris Abdulmumin, Jan Philip Wahle, Terry Ruas, Meriem Beloucif, Christine de Kock, Nirmal Surange, Daniela Teodorescu, Ibrahim Said Ahmad, David Ifeoluwa Adelani, Alham Fikri Aji, Felermino D. M. A. Ali, Ilseyar Alimova, Vladimir Araujo, Nikolay Babakov, Naomi Baes, Ana-Maria Bucur, Andiswa Bukula, Guanqun Cao, Rodrigo Tufino Cardenas, Rendi Chevi, Chiamaka Ijeoma Chukwuneke, Alexandra Ciobotaru, Daryna Dementieva, Murja Sani Gadanya, Robert Geislinger, Bela Gipp, Oumaima Hourrane, Oana Ignat, Falalu Ibrahim Lawan, Rooweither Mabuya, Rahmad Mahendra, Vukosi Marivate, Alexander Panchenko, Andrew Piper, Charles Henrique Porto Ferreira, Vitaly Protasov, Samuel Rutunda, Manish Shrivastava, Aura Cristina Udrea, Lilian Diana Awuor Wanzare, Sophie Wu, Florian Valentin Wunderlich, Hanif Muhammad Zhafran, Tianhui Zhang, Yi Zhou, Saif M. Mohammad*

Main category: cs.CL

TL;DR: BRIGHTER introduces multilingual emotion-annotated datasets for under-resourced languages, addressing disparities in emotion recognition research.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in emotion recognition research for low-resource languages lacking annotated datasets.

Method: Collection of multilabeled, emotion-annotated datasets in 28 languages, annotated by fluent speakers, with experiments on monolingual and crosslingual emotion identification and intensity recognition.

Result: Variability in performance across languages and domains, with insights into the impact of LLMs.

Conclusion: BRIGHTER datasets advance text-based emotion recognition for under-resourced languages.

Abstract: People worldwide use language in subtle and complex ways to express emotions.
Although emotion recognition--an umbrella term for several NLP tasks--impacts
various applications within NLP and beyond, most work in this area has focused
on high-resource languages. This has led to significant disparities in research
efforts and proposed solutions, particularly for under-resourced languages,
which often lack high-quality annotated datasets. In this paper, we present
BRIGHTER--a collection of multilabeled, emotion-annotated datasets in 28
different languages and across several domains. BRIGHTER primarily covers
low-resource languages from Africa, Asia, Eastern Europe, and Latin America,
with instances labeled by fluent speakers. We highlight the challenges related
to the data collection and annotation processes, and then report experimental
results for monolingual and crosslingual multi-label emotion identification, as
well as emotion intensity recognition. We analyse the variability in
performance across languages and text domains, both with and without the use of
LLMs, and show that the BRIGHTER datasets represent a meaningful step towards
addressing the gap in text-based emotion recognition.

</details>


### [146] [ThinkGuard: Deliberative Slow Thinking Leads to Cautious Guardrails](https://arxiv.org/pdf/2502.13458)
*Xiaofei Wen, Wenxuan Zhou, Wenjie Jacky Mo, Muhao Chen*

Main category: cs.CL

TL;DR: ThinkGuard, a critique-augmented guardrail model, enhances LLM safety by leveraging structured critiques and outperforms existing methods in accuracy and nuanced reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing safety guardrails for LLMs are limited by rule-based filtering or single-pass classification, lacking nuanced handling of safety violations.

Method: ThinkGuard distills knowledge from high-capacity LLMs by generating structured critiques alongside safety labels, fine-tuning on critique-augmented data.

Result: ThinkGuard achieves the highest F1 and AUPRC on safety benchmarks, improving accuracy by 16.1% and macro F1 by 27.0% over LLaMA Guard 3.

Conclusion: Structured critiques enhance classification precision and nuanced safety reasoning while maintaining computational efficiency, making ThinkGuard superior to label-only models.

Abstract: Ensuring the safety of large language models (LLMs) is critical as they are
deployed in real-world applications. Existing guardrails rely on rule-based
filtering or single-pass classification, limiting their ability to handle
nuanced safety violations. To address this, we propose ThinkGuard, a
critique-augmented guardrail model that distills knowledge from high-capacity
LLMs by generating structured critiques alongside safety labels. Fine-tuned on
critique-augmented data, the captured deliberative thinking ability drastically
enhances the guardrail's cautiousness and interpretability. Evaluated on
multiple safety benchmarks, ThinkGuard achieves the highest average F1 and
AUPRC, outperforming all baselines. Compared to LLaMA Guard 3, ThinkGuard
improves accuracy by 16.1% and macro F1 by 27.0%. Moreover, it surpasses
label-only fine-tuned models, confirming that structured critiques enhance both
classification precision and nuanced safety reasoning while maintaining
computational efficiency.

</details>


### [147] [How Do LLMs Perform Two-Hop Reasoning in Context?](https://arxiv.org/pdf/2502.13913)
*Tianyu Guo, Hanlin Zhu, Ruiqi Zhang, Jiantao Jiao, Song Mei, Michael I. Jordan, Stuart Russell*

Main category: cs.CL

TL;DR: The paper explores LLMs' failure in simple two-hop reasoning tasks, their improvement after fine-tuning, and the internal mechanisms behind this transition.


<details>
  <summary>Details</summary>
Motivation: To understand why LLMs struggle with two-hop reasoning and how they can be improved, revealing insights into their learning dynamics.

Method: Analyzed LLMs on synthetic two-hop reasoning tasks, fine-tuned models, and reverse-engineered a 3-layer Transformer's attention mechanisms.

Result: Pre-trained LLMs fail at two-hop reasoning but achieve near-perfect accuracy after fine-tuning, showing a structured sequential query mechanism.

Conclusion: Fine-tuning enables LLMs to master two-hop reasoning, with attention mechanisms evolving from random guessing to structured inference.

Abstract: ``Socrates is human. All humans are mortal. Therefore, Socrates is mortal.''
This form of argument illustrates a typical pattern of two-hop reasoning.
Formally, two-hop reasoning refers to the process of inferring a conclusion by
making two logical steps, each connecting adjacent concepts, such that the
final conclusion depends on the integration of both steps. It is one of the
most fundamental components of human reasoning and plays a crucial role in both
formal logic and everyday decision-making. Despite recent progress in large
language models (LLMs), we surprisingly find that they can fail at solving
simple two-hop reasoning problems when distractors are present. We observe on a
synthetic dataset that pre-trained LLMs often resort to random guessing among
all plausible conclusions. However, after few steps of fine-tuning, models
achieve near-perfect accuracy and exhibit strong length generalization. To
understand the underlying mechanisms, we train a 3-layer Transformer from
scratch on a synthetic two-hop reasoning task and reverse-engineer its internal
information flow. We observe a clear progression in the attention logits
throughout training. This pictures a sharp phase transition from an initial
stage of random guessing to the emergence of a structured sequential query
mechanism, where the model first retrieves the preceding and the bridge
concepts in the early layers and then uses them to infer the final answer.
Finally, we show that these dynamics can be captured by a minimal
three-parameter attention-only network.

</details>


### [148] [Mitigating Lost-in-Retrieval Problems in Retrieval Augmented Multi-Hop Question Answering](https://arxiv.org/pdf/2502.14245)
*Rongzhi Zhu, Xiangyu Liu, Zequn Sun, Yiwei Wang, Wei Hu*

Main category: cs.CL

TL;DR: ChainRAG addresses the 'lost-in-retrieval' issue in multi-hop QA by progressively retrieving and rewriting sub-questions, improving accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The 'lost-in-retrieval' problem in retrieval-augmented multi-hop QA degrades performance by missing key entities, disrupting reasoning chains.

Method: Proposes ChainRAG, a progressive retrieval and rewriting method that sequentially handles sub-questions, completes missing entities, and retrieves relevant sentences from a sentence graph.

Result: ChainRAG outperforms baselines on MuSiQue, 2Wiki, and HotpotQA datasets using GPT4o-mini, Qwen2.5-72B, and GLM-4-Plus.

Conclusion: ChainRAG effectively resolves 'lost-in-retrieval' and enhances multi-hop QA performance through its progressive approach.

Abstract: In this paper, we identify a critical problem, "lost-in-retrieval", in
retrieval-augmented multi-hop question answering (QA): the key entities are
missed in LLMs' sub-question decomposition. "Lost-in-retrieval" significantly
degrades the retrieval performance, which disrupts the reasoning chain and
leads to the incorrect answers. To resolve this problem, we propose a
progressive retrieval and rewriting method, namely ChainRAG, which sequentially
handles each sub-question by completing missing key entities and retrieving
relevant sentences from a sentence graph for answer generation. Each step in
our retrieval and rewriting process builds upon the previous one, creating a
seamless chain that leads to accurate retrieval and answers. Finally, all
retrieved sentences and sub-question answers are integrated to generate a
comprehensive answer to the original question. We evaluate ChainRAG on three
multi-hop QA datasets - MuSiQue, 2Wiki, and HotpotQA - using three large
language models: GPT4o-mini, Qwen2.5-72B, and GLM-4-Plus. Empirical results
demonstrate that ChainRAG consistently outperforms baselines in both
effectiveness and efficiency.

</details>


### [149] [Self-Taught Agentic Long Context Understanding](https://arxiv.org/pdf/2502.15920)
*Yufan Zhuang, Xiaodong Yu, Jialian Wu, Ximeng Sun, Ze Wang, Jiang Liu, Yusheng Su, Jingbo Shang, Zicheng Liu, Emad Barsoum*

Main category: cs.CL

TL;DR: AgenticLU enhances LLMs' ability to handle complex, long-context questions using self-clarification and contextual grounding, achieving high recall and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of LLMs in answering complex, long-context questions by improving question clarification and context retrieval.

Method: Integrates Chain-of-Clarifications (CoC) in an agentic workflow, scaling inference as tree search, and uses two-stage finetuning (supervised and preference optimization).

Result: Achieves 97.8% answer recall on NarrativeQA and outperforms state-of-the-art methods across seven long-context tasks.

Conclusion: AgenticLU effectively improves LLMs' performance in long-context understanding and reasoning, maintaining consistency as context length increases.

Abstract: Answering complex, long-context questions remains a major challenge for large
language models (LLMs) as it requires effective question clarifications and
context retrieval. We propose Agentic Long-Context Understanding (AgenticLU), a
framework designed to enhance an LLM's understanding of such queries by
integrating targeted self-clarification with contextual grounding within an
agentic workflow. At the core of AgenticLU is Chain-of-Clarifications (CoC),
where models refine their understanding through self-generated clarification
questions and corresponding contextual groundings. By scaling inference as a
tree search where each node represents a CoC step, we achieve 97.8% answer
recall on NarrativeQA with a search depth of up to three and a branching factor
of eight. To amortize the high cost of this search process to training, we
leverage the preference pairs for each step obtained by the CoC workflow and
perform two-stage model finetuning: (1) supervised finetuning to learn
effective decomposition strategies, and (2) direct preference optimization to
enhance reasoning quality. This enables AgenticLU models to generate
clarifications and retrieve relevant context effectively and efficiently in a
single inference pass. Extensive experiments across seven long-context tasks
demonstrate that AgenticLU significantly outperforms state-of-the-art prompting
methods and specialized long-context LLMs, achieving robust multi-hop reasoning
while sustaining consistent performance as context length grows.

</details>


### [150] [GraphCheck: Breaking Long-Term Text Barriers with Extracted Knowledge Graph-Powered Fact-Checking](https://arxiv.org/pdf/2502.16514)
*Yingjian Chen, Haoran Liu, Yinhong Liu, Jinxiang Xie, Rui Yang, Han Yuan, Yanran Fu, Peng Yuan Zhou, Qingyu Chen, James Caverlee, Irene Li*

Main category: cs.CL

TL;DR: GraphCheck is a fact-checking framework using knowledge graphs and GNNs to improve LLMs' accuracy in detecting subtle factual errors, outperforming baselines by up to 7.1%.


<details>
  <summary>Details</summary>
Motivation: LLMs often generate subtle factual errors, especially in specialized domains like medicine, and existing methods struggle with multihop reasoning and high computational costs.

Method: GraphCheck extracts knowledge graphs, processes them with GNNs as soft prompts, and integrates structured knowledge into LLMs for efficient fact-checking in a single inference call.

Result: GraphCheck improves accuracy by up to 7.1% on seven benchmarks, outperforming specialized fact-checkers and matching state-of-the-art LLMs with fewer parameters.

Conclusion: GraphCheck effectively addresses the limitations of existing methods, offering precise and efficient fact-checking for LLMs.

Abstract: Large language models (LLMs) are widely used, but they often generate subtle
factual errors, especially in long-form text. These errors are fatal in some
specialized domains such as medicine. Existing fact-checking with grounding
documents methods face two main challenges: (1) they struggle to understand
complex multihop relations in long documents, often overlooking subtle factual
errors; (2) most specialized methods rely on pairwise comparisons, requiring
multiple model calls, leading to high resource and computational costs. To
address these challenges, we propose GraphCheck, a fact-checking framework that
uses extracted knowledge graphs to enhance text representation. Graph Neural
Networks further process these graphs as a soft prompt, enabling LLMs to
incorporate structured knowledge more effectively. Enhanced with graph-based
reasoning, GraphCheck captures multihop reasoning chains that are often
overlooked by existing methods, enabling precise and efficient fact-checking in
a single inference call. Experimental results on seven benchmarks spanning both
general and medical domains demonstrate up to a 7.1% overall improvement over
baseline models. Notably, GraphCheck outperforms existing specialized
fact-checkers and achieves comparable performance with state-of-the-art LLMs,
such as DeepSeek-V3 and OpenAI-o1, with significantly fewer parameters.

</details>


### [151] [Personalized Causal Graph Reasoning for LLMs: A Case Study on Dietary Recommendations](https://arxiv.org/pdf/2503.00134)
*Zhongqi Yang, Amir Rahmani*

Main category: cs.CL

TL;DR: The paper proposes Personalized Causal Graph Reasoning to enhance LLMs' ability for personalized reasoning, demonstrated via dietary recommendations for glucose management.


<details>
  <summary>Details</summary>
Motivation: LLMs lack personalized reasoning for multifactor personal data, limiting their use in context-aware decision-making.

Method: Introduces a framework using personal causal graphs to guide LLM reasoning, evaluated on nutrient-oriented dietary recommendations.

Result: The method reduces average glucose iAUC across three time windows, outperforming previous approaches.

Conclusion: The proposed framework improves personalization in LLM reasoning, validated by LLM-as-a-judge evaluations.

Abstract: Large Language Models (LLMs) effectively leverage common-sense knowledge for
general reasoning, yet they struggle with personalized reasoning when tasked
with interpreting multifactor personal data. This limitation restricts their
applicability in domains that require context-aware decision-making tailored to
individuals. This paper introduces Personalized Causal Graph Reasoning as an
agentic framework that enhances LLM reasoning by incorporating personal causal
graphs derived from data of individuals. These graphs provide a foundation that
guides the LLM's reasoning process. We evaluate it on a case study on
nutrient-oriented dietary recommendations, which requires personal reasoning
due to the implicit unique dietary effects. We propose a counterfactual
evaluation to estimate the efficiency of LLM-recommended foods for glucose
management. Results demonstrate that the proposed method efficiently provides
personalized dietary recommendations to reduce average glucose iAUC across
three time windows, which outperforms the previous approach. LLM-as-a-judge
evaluation results indicate that our proposed method enhances personalization
in the reasoning process.

</details>


### [152] [LINGOLY-TOO: Disentangling Reasoning from Knowledge with Templatised Orthographic Obfuscation](https://arxiv.org/pdf/2503.02972)
*Jude Khouja, Karolina Korgul, Simi Hellsten, Lingyi Yang, Vlad Neacsu, Harry Mayne, Ryan Kearns, Andrew Bean, Adam Mahdi*

Main category: cs.CL

TL;DR: The paper introduces LINGOLY-TOO, a benchmark to test LLMs' reasoning by minimizing reliance on prior knowledge, revealing their brittle reasoning abilities despite progress.


<details>
  <summary>Details</summary>
Motivation: To counteract inflated estimates of LLMs' reasoning due to their memorization capacity by creating a benchmark that isolates reasoning from prior knowledge.

Method: Uses linguistically informed rulesets to permute reasoning problems in real languages, preserving required reasoning steps while reducing solvability via memorization.

Result: Models often bypass reasoning, relying on prior knowledge. Performance is poor and inconsistent across permutations, highlighting brittle reasoning.

Conclusion: LINGOLY-TOO improves reasoning measurement but shows LLMs' reasoning remains fragile, emphasizing the need to disentangle reasoning from knowledge in benchmarks.

Abstract: The expanding knowledge and memorisation capacity of frontier language models
allows them to solve many reasoning tasks directly by exploiting prior
knowledge, leading to inflated estimates of their reasoning abilities. We
introduce LINGOLY-TOO, a challenging reasoning benchmark grounded in natural
language and designed to counteract the effect of non-reasoning abilities on
reasoning estimates. Using linguistically informed rulesets, we permute
reasoning problems written in real languages to generate numerous question
variations. These permutations preserve the intrinsic reasoning steps required
for each solution while reducing the likelihood problems are directly solvable
with models' knowledge. Experiments and analyses show that models can
circumvent reasoning and answer from prior knowledge. On a metric that rewards
consistent reasoning, all models perform poorly and exhibit high variance
across question permutations, indicating that Large Language Models' (LLMs)
reasoning faculty remains brittle. Overall, results on the benchmark reflect
the recent progress of Inference-Time Compute (ITC) models but suggest ample
room for further improvement. The benchmark is a step towards better
measurement of reasoning abilities of LLMs and offers a cautionary tale on the
importance of disentangling reasoning abilities from models' internalised
knowledge when developing reasoning benchmarks.

</details>


### [153] [Shaping Shared Languages: Human and Large Language Models' Inductive Biases in Emergent Communication](https://arxiv.org/pdf/2503.04395)
*Tom Kouwenhoven, Max Peeperkorn, Roy de Kleijn, Tessa Verhoef*

Main category: cs.CL

TL;DR: The study explores how languages evolve when optimized for human and LLM inductive biases, revealing differences between human- and LLM-optimized languages and showing that human-LLM collaboration produces more human-like vocabularies.


<details>
  <summary>Details</summary>
Motivation: To understand how inductive biases in humans and LLMs shape language evolution and ensure alignment in human-machine communication.

Method: Referential game experiments involving Human-Human, LLM-LLM, and Human-LLM interactions to study language optimization.

Result: Referentially grounded vocabularies emerge in all conditions, with human-LLM collaboration producing more human-like languages than LLM-LLM.

Conclusion: Human interaction mitigates LLM-human language differences, suggesting the need for LLM training methods incorporating human collaboration and communicative success as a reward.

Abstract: Languages are shaped by the inductive biases of their users. Using a
classical referential game, we investigate how artificial languages evolve when
optimised for inductive biases in humans and large language models (LLMs) via
Human-Human, LLM-LLM and Human-LLM experiments. We show that referentially
grounded vocabularies emerge that enable reliable communication in all
conditions, even when humans \textit{and} LLMs collaborate. Comparisons between
conditions reveal that languages optimised for LLMs subtly differ from those
optimised for humans. Interestingly, interactions between humans and LLMs
alleviate these differences and result in vocabularies more human-like than
LLM-like. These findings advance our understanding of the role inductive biases
in LLMs play in the dynamic nature of human language and contribute to
maintaining alignment in human and machine communication. In particular, our
work underscores the need to think of new LLM training methods that include
human interaction and shows that using communicative success as a reward signal
can be a fruitful, novel direction.

</details>


### [154] [Odysseus Navigates the Sirens' Song: Dynamic Focus Decoding for Factual and Diverse Open-Ended Text Generation](https://arxiv.org/pdf/2503.08057)
*Wen Luo, Feifan Song, Wei Li, Guangyue Peng, Shaohang Wei, Houfeng Wang*

Main category: cs.CL

TL;DR: DFD is a plug-and-play stochastic decoding method for LLMs that balances factuality and diversity by dynamically adjusting focus across layers.


<details>
  <summary>Details</summary>
Motivation: Current stochastic decoding methods fail to balance factuality and diversity in text generation, necessitating a solution like DFD.

Method: DFD adaptively adjusts decoding focus based on layer-wise distributional differences in LLMs, improving factuality and diversity.

Result: Experiments on seven datasets show DFD significantly enhances performance in open-ended text generation.

Conclusion: DFD offers a scalable, efficient solution for improving factuality and diversity in LLM-generated text without extra resources.

Abstract: Large Language Models (LLMs) are increasingly required to generate text that
is both factually accurate and diverse across various open-ended applications.
However, current stochastic decoding methods struggle to balance such
objectives. We introduce Dynamic Focus Decoding (DFD), a novel plug-and-play
stochastic approach that resolves this trade-off without requiring additional
data, knowledge, or models. DFD adaptively adjusts the decoding focus based on
distributional differences across layers, leveraging the modular and
hierarchical nature of factual knowledge within LLMs. This dynamic adjustment
improves factuality in knowledge-intensive decoding steps and promotes
diversity in less knowledge-reliant steps. DFD can be easily integrated with
existing decoding methods, enhancing both factuality and diversity with minimal
computational overhead. Extensive experiments across seven datasets demonstrate
that DFD significantly improves performance, providing a scalable and efficient
solution for open-ended text generation.

</details>


### [155] [Explicit Learning and the LLM in Machine Translation](https://arxiv.org/pdf/2503.09454)
*Malik Marmonier, Rachel Bawden, Benoît Sagot*

Main category: cs.CL

TL;DR: LLMs can learn new languages via grammar book explanations (explicit learning), but performance drops with complexity. Fine-tuning helps but struggles with novel or complex features.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' ability to learn languages through explicit explanations, addressing gaps in prior research.

Method: Controlled translation experiments using constructed languages derived from Latin or French via cryptographic methods.

Result: LLMs show measurable explicit learning ability, declining with complexity. Fine-tuning improves performance but lacks generalization.

Conclusion: Diverse training sets and better fine-tuning strategies are needed to enhance explicit learning, especially for low-resource languages.

Abstract: This study explores an LLM's ability to learn new languages using
explanations found in a grammar book$\unicode{x2014}$a process we term
"explicit learning." To rigorously assess this ability, we design controlled
translation experiments between English and constructed languages
generated$\unicode{x2014}$by specific cryptographic means$\unicode{x2014}$out
of Latin or French. Contrary to previous studies, our results demonstrate that
LLMs do possess a measurable capacity for explicit learning. This ability,
however, diminishes as the complexity of the linguistic phenomena to be learned
increases. Supervised fine-tuning on ad hoc chains of thought significantly
enhances LLM performance but struggles to generalize to typologically novel or
more complex linguistic features. These findings point to the need for more
diverse training sets and alternative fine-tuning strategies to further improve
explicit learning by LLMs, benefiting low-resource languages typically
described in grammar books but lacking extensive corpora.

</details>


### [156] [Probabilistic Reasoning with LLMs for k-anonymity Estimation](https://arxiv.org/pdf/2503.09674)
*Jonathan Zheng, Sauvik Das, Alan Ritter, Wei Xu*

Main category: cs.CL

TL;DR: A new LLM method, BRANCH, estimates privacy risk in documents by calculating k-privacy values, outperforming existing methods by 13%.


<details>
  <summary>Details</summary>
Motivation: To address uncertainty in privacy risk estimation for user-generated documents using probabilistic reasoning.

Method: BRANCH factorizes joint probability distributions of personal info, estimates factors via Bayesian networks, and combines them for k-value.

Result: 73% accuracy in k-value estimation, 13% better than o3-mini; high-variance predictions are 37.47% less accurate.

Conclusion: BRANCH effectively estimates privacy risk, with LLM uncertainty serving as an accuracy indicator.

Abstract: Probabilistic reasoning is a key aspect of both human and artificial
intelligence that allows for handling uncertainty and ambiguity in
decision-making. In this paper, we introduce a new numerical reasoning task
under uncertainty for large language models, focusing on estimating the privacy
risk of user-generated documents containing privacy-sensitive information. We
propose BRANCH, a new LLM methodology that estimates the k-privacy value of a
text-the size of the population matching the given information. BRANCH
factorizes a joint probability distribution of personal information as random
variables. The probability of each factor in a population is estimated
separately using a Bayesian network and combined to compute the final k-value.
Our experiments show that this method successfully estimates the k-value 73% of
the time, a 13% increase compared to o3-mini with chain-of-thought reasoning.
We also find that LLM uncertainty is a good indicator for accuracy, as
high-variance predictions are 37.47% less accurate on average.

</details>


### [157] [Constrained Discrete Diffusion](https://arxiv.org/pdf/2503.09790)
*Michael Cardei, Jacob K Christopher, Thomas Hartvigsen, Brian R. Bartoldson, Bhavya Kailkhura, Ferdinando Fioretto*

Main category: cs.CL

TL;DR: CDD integrates constraint optimization into discrete diffusion models, ensuring constraint adherence without retraining, outperforming autoregressive models.


<details>
  <summary>Details</summary>
Motivation: To enforce sequence-level constraints in generative models, which autoregressive models cannot natively provide.

Method: Introduces Constrained Discrete Diffusion (CDD), integrating differentiable constraint optimization within diffusion.

Result: Achieves zero constraint violations in tasks like toxicity-controlled text generation, preserving fluency and coherence.

Conclusion: CDD offers a training-free, effective approach for constrained sequence generation, outperforming existing methods.

Abstract: Discrete diffusion models are a class of generative models that construct
sequences by progressively denoising samples from a categorical noise
distribution. Beyond their rapidly growing ability to generate coherent natural
language, these models present a new and important opportunity to enforce
sequence-level constraints, a capability that current autoregressive models
cannot natively provide. This paper capitalizes on this opportunity by
introducing Constrained Discrete Diffusion (CDD), a novel integration of
differentiable constraint optimization within the diffusion process to ensure
adherence to constraints, logic rules, or safety requirements for generated
sequences. Unlike conventional text generators that often rely on post-hoc
filtering or model retraining for controllable generation, CDD directly imposes
constraints into the discrete diffusion sampling process, resulting in a
training-free and effective approach. Experiments in toxicity-controlled text
generation, property-constrained molecule design, and instruction-constrained
text completion demonstrate that CDD achieves zero constraint violations in a
diverse array of tasks while preserving fluency, novelty, and coherence while
outperforming autoregressive and existing discrete diffusion approaches.

</details>


### [158] [Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond](https://arxiv.org/pdf/2503.10460)
*Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, Xiangzheng Zhang*

Main category: cs.CL

TL;DR: Light-R1 is an open-source suite for training long reasoning models using public data, outperforming proprietary models like DeepSeek-R1 in math reasoning.


<details>
  <summary>Details</summary>
Motivation: To provide a reproducible and cost-effective alternative to proprietary models by leveraging public data and progressive curriculum training.

Method: Uses curriculum training with progressively difficult data and multi-staged post-training, applied to models like Qwen2.5-32B-Instruct.

Result: Light-R1-32B outperforms DeepSeek-R1-Distill-Qwen-32B in math reasoning, and Light-R1-14B-DS achieves SOTA performance among 14B models.

Conclusion: Light-R1 advances accessibility of sophisticated reasoning models, with models, data, and code made publicly available.

Abstract: This paper introduces Light-R1, an open-source suite for training long
reasoning models using reproducible and cost-effective methodology. Given the
proprietary nature of data used in the DeepSeek-R1 series, we develop an
alternative approach leveraging exclusively public data and models. Our
curriculum training progressively increases data difficulty, combined with
multi-staged post-training. Our Light-R1-32B model, trained from
Qwen2.5-32B-Instruct, outperforms DeepSeek-R1-Distill-Qwen-32B in math
reasoning.
  Experimental results show that this curriculum approach becomes more
effective when distinct, diverse datasets are available for different training
stages: fine-tuning DeepSeek-R1-Distilled models (pre-tuned by DeepSeek team on
proprietary data) with 3,000 challenging examples from our curriculum dataset
yielded state-of-the-art 7B and 14B models, while the 32B model,
Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.
  Furthermore, we extend our work by applying GRPO on long reasoning models.
Our final Light-R1-14B-DS achieves SOTA performance among 14B models in math,
with AIME24 & 25 scores of 74.0 and 60.2 respectively, surpassing many 32B
models and DeepSeek-R1-Distill-Llama-70B. Despite math-focused training,
Light-R1-14B-DS demonstrates strong cross-domain generalization.
  Light-R1 represents a significant advancement in making sophisticated
reasoning models more accessible and implementable in real-world applications.
Our models, training data and code have been made available at
https://github.com/Qihoo360/Light-R1.

</details>


### [159] [CULEMO: Cultural Lenses on Emotion -- Benchmarking LLMs for Cross-Cultural Emotion Understanding](https://arxiv.org/pdf/2503.10688)
*Tadesse Destaw Belay, Ahmed Haj Ahmed, Alvin Grissom II, Iqra Ameer, Grigori Sidorov, Olga Kolesnikova, Seid Muhie Yimam*

Main category: cs.CL

TL;DR: CuLEmo is a culture-aware emotion benchmark for six languages, addressing gaps in existing emotion datasets by incorporating cultural nuances. It evaluates LLMs, revealing performance variations across languages and cultures.


<details>
  <summary>Details</summary>
Motivation: Existing emotion benchmarks lack cultural depth and rely on translations, leading to unreliable evaluations. CuLEmo aims to fill this gap by focusing on culture-aware emotion prediction.

Method: Introduces CuLEmo, a benchmark with 400 culturally nuanced questions per language (Amharic, Arabic, English, German, Hindi, Spanish), and evaluates LLMs on culture-aware tasks.

Result: Findings show emotion conceptualizations vary by culture, LLM performance differs across languages, and English prompts with country context often outperform in-language prompts.

Conclusion: CuLEmo highlights the importance of cultural context in emotion analysis and provides a reliable benchmark for future research.

Abstract: NLP research has increasingly focused on subjective tasks such as emotion
analysis. However, existing emotion benchmarks suffer from two major
shortcomings: (1) they largely rely on keyword-based emotion recognition,
overlooking crucial cultural dimensions required for deeper emotion
understanding, and (2) many are created by translating English-annotated data
into other languages, leading to potentially unreliable evaluation. To address
these issues, we introduce Cultural Lenses on Emotion (CuLEmo), the first
benchmark designed to evaluate culture-aware emotion prediction across six
languages: Amharic, Arabic, English, German, Hindi, and Spanish. CuLEmo
comprises 400 crafted questions per language, each requiring nuanced cultural
reasoning and understanding. We use this benchmark to evaluate several
state-of-the-art LLMs on culture-aware emotion prediction and sentiment
analysis tasks. Our findings reveal that (1) emotion conceptualizations vary
significantly across languages and cultures, (2) LLMs performance likewise
varies by language and cultural context, and (3) prompting in English with
explicit country context often outperforms in-language prompts for
culture-aware emotion and sentiment understanding. The dataset and evaluation
code are publicly available.

</details>


### [160] [TLUE: A Tibetan Language Understanding Evaluation Benchmark](https://arxiv.org/pdf/2503.12051)
*Fan Gao, Cheng Huang, Nyima Tashi, Xiangxiang Wang, Thupten Tsering, Ban Ma-bao, Renzeg Duojie, Gadeng Luosang, Rinchen Dongrub, Dorje Tashi, Hao Wang Xiao Feng, Yongbin Yu*

Main category: cs.CL

TL;DR: TLUE is the first large-scale benchmark for evaluating LLMs in Tibetan, revealing their poor performance and the need for inclusivity in LLM development.


<details>
  <summary>Details</summary>
Motivation: Tibetan, a low-resource language, is underrepresented in LLM evaluation despite its significant speaker base.

Method: TLUE includes a multi-task understanding benchmark (5 domains, 67 subdomains) and a safety benchmark (7 subdomains), evaluated on state-of-the-art LLMs.

Result: Most LLMs perform below the random baseline, indicating challenges in processing Tibetan.

Conclusion: TLUE highlights the need for more inclusive LLM development and serves as a foundation for future Tibetan language research.

Abstract: Large language models (LLMs) have made tremendous progress in recent years,
but low-resource languages, such as Tibetan, remain significantly
underrepresented in their evaluation. Despite Tibetan being spoken by over
seven million people, it has largely been neglected in the development and
assessment of LLMs. To address this gap, we present TLUE (A Tibetan Language
Understanding Evaluation Benchmark), the first large-scale benchmark for
assessing LLMs' capabilities in Tibetan. TLUE comprises two major components:
(1) a comprehensive multi-task understanding benchmark spanning 5 domains and
67 subdomains, and (2) a safety benchmark covering 7 subdomains. We evaluate a
diverse set of state-of-the-art LLMs. Experimental results demonstrate that
most LLMs perform below the random baseline, highlighting the considerable
challenges LLMs face in processing Tibetan, a low-resource language. TLUE
provides an essential foundation for driving future research and progress in
Tibetan language understanding and underscores the need for greater inclusivity
in LLM development.

</details>


### [161] [Experience Retrieval-Augmentation with Electronic Health Records Enables Accurate Discharge QA](https://arxiv.org/pdf/2503.17933)
*Justice Ou, Tinglin Huang, Yilun Zhao, Ziyang Yu, Peiqing Lu, Rex Ying*

Main category: cs.CL

TL;DR: ExpRAG enhances LLMs in clinical applications by retrieving case-based knowledge from EHRs, improving medical reasoning with a 5.2% performance boost.


<details>
  <summary>Details</summary>
Motivation: To address the gap in clinical case-based knowledge for LLMs, leveraging EHR data for context grounded in real-world patient experiences.

Method: ExpRAG uses a coarse-to-fine retrieval process with an EHR-based ranker and experience retriever to extract relevant patient discharge reports.

Result: ExpRAG outperforms text-based rankers by 5.2% on DischargeQA, a dataset of 1,280 discharge-related questions.

Conclusion: Case-based knowledge from EHRs significantly improves medical reasoning in LLMs, as demonstrated by ExpRAG's performance.

Abstract: To improve the reliability of Large Language Models (LLMs) in clinical
applications, retrieval-augmented generation (RAG) is extensively applied to
provide factual medical knowledge. However, beyond general medical knowledge
from open-ended datasets, clinical case-based knowledge is also critical for
effective medical reasoning, as it provides context grounded in real-world
patient experiences.Motivated by this, we propose Experience
Retrieval-Augmentation ExpRAG framework based on Electronic Health Record(EHR),
aiming to offer the relevant context from other patients' discharge reports.
ExpRAG performs retrieval through a coarse-to-fine process, utilizing an
EHR-based report ranker to efficiently identify similar patients, followed by
an experience retriever to extract task-relevant content for enhanced medical
reasoning.To evaluate ExpRAG, we introduce DischargeQA, a clinical QA dataset
with 1,280 discharge-related questions across diagnosis, medication, and
instruction tasks. Each problem is generated using EHR data to ensure realistic
and challenging scenarios. Experimental results demonstrate that ExpRAG
consistently outperforms a text-based ranker, achieving an average relative
improvement of 5.2%, highlighting the importance of case-based knowledge for
medical reasoning.

</details>


### [162] [Sun-Shine: A Foundation Large Language Model for Tibetan Culture and Heritage](https://arxiv.org/pdf/2503.18288)
*Cheng Huang, Fan Gao, Yutong Liu, Nyima Tashi, Xiangxiang Wang, Thupten Tsering, Ban Ma-bao, Renzeg Duojie, Gadeng Luosang, Rinchen Dongrub, Dorje Tashi, Xiao Feng, Hao Wang, Yongbin Yu*

Main category: cs.CL

TL;DR: The paper introduces Llama-Sunshine, the first large language model for Tibetan culture, addressing data scarcity and linguistic complexity in Tibetan language processing.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack effectiveness for Tibetan culture due to its intricate grammar and data scarcity. The paper aims to bridge this gap.

Method: Developed Llama-Sunshine with optimized architectures for Tibetan linguistic features and introduced TIB-STC, a comprehensive Tibetan dataset.

Result: Sun-Shine excels in Tibetan language tasks, shows strong generalization in low-resource scenarios, and gains preliminary embodied intelligence.

Conclusion: Llama-Sunshine successfully addresses Tibetan language challenges, demonstrating potential for cultural and linguistic applications.

Abstract: Tibetan, a minority language in China, features a highly intricate
grammatical structure, characterized by four verb tenses and a tense system
with frequent irregularities, contributing to its extensive inflectional
diversity. Recently, advances in Large Language Models (LLMs) have transformed
the paradigm in many domains. Despite the success in other fields, current LLMs
often fall short in catering to the needs of domain experts like Tibetans, and
the potential of LLMs for Tibetan culture is under-explored. The intrinsic
reasons are the immense and intricate nature of Tibetan culture as well as the
necessity for higher granularity and richness in knowledge. Simultaneously, the
complexity and uniqueness of its grammatical structure, coupled with its status
as a minority ethnic language, contribute to data scarcity, which remains a
fundamental challenge. To alleviate these issues, we introduce Llama-Sunshine
(Sun-Shine), the first large language model for Tibetan culture, which is
expert in various Tibetan language processing tasks. Sun-Shine incorporates
state-of-the-art model architectures optimized for Tibetan's linguistic
features. We also propose TIB-STC, a comprehensive dataset comprising diverse
Tibetan texts such as literature, religious scripts, news, and conversational
data, which is also the first large-scale dataset for Tibetan culture. Though
comprehensive experiments, Sun-Shine not only demonstrates a higher level of
knowledge expertise for Tibetan culture but also gains preliminary embodied
intelligence capabilities in Tibetan language processing tasks, like language
modeling, text classification, machine translation, and syntactic analysis.
Moreover, it excels in low-resource scenarios, showcasing strong generalization
capabilities.

</details>


### [163] [Token embeddings violate the manifold hypothesis](https://arxiv.org/pdf/2504.01002)
*Michael Robinson, Sourya Dey, Tony Chiang*

Main category: cs.CL

TL;DR: The paper introduces a statistical test (fiber bundle hypothesis) to analyze token embeddings in LLMs, revealing irregularities that challenge assumptions about their structure and impact model stability.


<details>
  <summary>Details</summary>
Motivation: To understand LLM behavior by examining the structure of token embeddings, as flawed assumptions about this space can lead to incorrect conclusions.

Method: A novel statistical test assuming a smooth fiber bundle structure for token embeddings, with rejection indicating irregularities. Applied to multiple open-source LLMs.

Result: Frequent rejection of the null hypothesis suggests token subspaces are not smooth fiber bundles or manifolds, impacting model stability.

Conclusion: Token embedding irregularities affect LLM stability, especially when semantically equivalent prompts contain implicated tokens.

Abstract: A full understanding of the behavior of a large language model (LLM) requires
our understanding of its input token space. If this space differs from our
assumptions, our understanding of and conclusions about the LLM will likely be
flawed. We elucidate the structure of the token embeddings both empirically and
theoretically. We present a novel statistical test assuming that the
neighborhood around each token has a relatively flat and smooth structure as
the null hypothesis. Failing to reject the null is uninformative, but rejecting
it at a specific token $\psi$ implies an irregularity in the token subspace in
a $\psi$-neighborhood, $B(\psi)$. The structure assumed in the null is a
generalization of a manifold with boundary called a \emph{smooth fiber bundle}
(which can be split into two spatial regimes -- small and large radius), so we
denote our new hypothesis test as the ``fiber bundle hypothesis.'' Failure to
reject the null hypothesis is uninformative, but rejecting it at $\psi$
indicates a statistically significant irregularity at $B(\psi)$. By running our
test over several open-source LLMs, each with unique token embeddings, we find
that the null is frequently rejected, and so the evidence suggests that the
token subspace is not a fiber bundle and hence also not a manifold. As a
consequence of our findings, when an LLM is presented with two semantically
equivalent prompts, if one prompt contains a token implicated by our test, the
response to that prompt will likely exhibit less stability than the other.

</details>


### [164] [Evaluating Compact LLMs for Zero-Shot Iberian Language Tasks on End-User Devices](https://arxiv.org/pdf/2504.03312)
*Luís Couto Seller, Íñigo Sanz Torres, Adrián Vogel-Fernández, Carlos González Carballo, Pedro Miguel Sánchez Sánchez, Adrián Carruana Martín, Enrique de Miguel Ambite*

Main category: cs.CL

TL;DR: Compact LLMs for Iberian languages show promise but face performance gaps, especially for Basque, highlighting the need for further research.


<details>
  <summary>Details</summary>
Motivation: To address the limited accessibility of large language models (LLMs) on consumer-grade devices and the lack of resources for under-resourced Iberian languages.

Method: Comprehensive evaluation of compact state-of-the-art LLMs across essential NLP tasks tailored for Iberian languages.

Result: Some models excel in certain tasks, but significant performance gaps remain, particularly for Basque.

Conclusion: Further research is needed to balance model compactness with robust multilingual performance, especially for under-resourced languages.

Abstract: Large Language Models have significantly advanced natural language
processing, achieving remarkable performance in tasks such as language
generation, translation, and reasoning. However, their substantial
computational requirements restrict deployment to high-end systems, limiting
accessibility on consumer-grade devices. This challenge is especially
pronounced for under-resourced languages like those spoken in the Iberian
Peninsula, where relatively limited linguistic resources and benchmarks hinder
effective evaluation. This work presents a comprehensive evaluation of compact
state-of-the-art LLMs across several essential NLP tasks tailored for Iberian
languages. The results reveal that while some models consistently excel in
certain tasks, significant performance gaps remain, particularly for languages
such as Basque. These findings highlight the need for further research on
balancing model compactness with robust multilingual performance

</details>


### [165] [AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for Early Warning System Investments](https://arxiv.org/pdf/2504.05104)
*Saeid Ario Vaghefi, Aymane Hachcham, Veronica Grasso, Jiska Manicus, Nakiete Msemo, Chiara Colesanti Senni, Markus Leippold*

Main category: cs.CL

TL;DR: An LLM-based AI system is introduced to track financial investments in climate adaptation, specifically for Early Warning Systems (EWS), outperforming other methods with 87% accuracy.


<details>
  <summary>Details</summary>
Motivation: Standardized financial reporting for EWS investments is lacking, making tracking complex and expertise-intensive.

Method: The system uses contextual retrieval, fine-tuning, and multi-step reasoning, evaluated on 25 MDB project documents with methods like zero-shot learning and RAG.

Result: The agent-based RAG approach achieved 87% accuracy, 89% precision, and 83% recall.

Conclusion: The study provides a benchmark dataset and corpus, advancing AI-driven financial tracking and climate finance transparency.

Abstract: Tracking financial investments in climate adaptation is a complex and
expertise-intensive task, particularly for Early Warning Systems (EWS), which
lack standardized financial reporting across multilateral development banks
(MDBs) and funds. To address this challenge, we introduce an LLM-based agentic
AI system that integrates contextual retrieval, fine-tuning, and multi-step
reasoning to extract relevant financial data, classify investments, and ensure
compliance with funding guidelines. Our study focuses on a real-world
application: tracking EWS investments in the Climate Risk and Early Warning
Systems (CREWS) Fund. We analyze 25 MDB project documents and evaluate multiple
AI-driven classification methods, including zero-shot and few-shot learning,
fine-tuned transformer-based classifiers, chain-of-thought (CoT) prompting, and
an agent-based retrieval-augmented generation (RAG) approach. Our results show
that the agent-based RAG approach significantly outperforms other methods,
achieving 87\% accuracy, 89\% precision, and 83\% recall. Additionally, we
contribute a benchmark dataset and expert-annotated corpus, providing a
valuable resource for future research in AI-driven financial tracking and
climate finance transparency.

</details>


### [166] [Domain-Specific Pruning of Large Mixture-of-Experts Models with Few-shot Demonstrations](https://arxiv.org/pdf/2504.06792)
*Zican Dong, Han Peng, Peiyu Liu, Wayne Xin Zhao, Dong Wu, Feng Xiao, Zhifeng Wang*

Main category: cs.CL

TL;DR: EASY-EP is a pruning framework for MoE models that retains only relevant experts using domain-specific demonstrations, achieving comparable performance with fewer experts.


<details>
  <summary>Details</summary>
Motivation: The memory overhead of storing all experts in large-scale MoE models is a limitation, prompting the need for efficient pruning.

Method: EASY-EP uses output-aware expert importance assessment and expert-level token contribution estimation to identify and retain key experts.

Result: The method achieves comparable performance with half the experts, improving throughput by 2.99× under the same memory budget.

Conclusion: EASY-EP effectively reduces memory usage while maintaining performance in large-scale MoE models.

Abstract: Mixture-of-Experts (MoE) models achieve a favorable trade-off between
performance and inference efficiency by activating only a subset of experts.
However, the memory overhead of storing all experts remains a major limitation,
especially in large-scale MoE models such as DeepSeek-R1(671B). In this study,
we investigate domain specialization and expert redundancy in large-scale MoE
models and uncover a consistent behavior we term few-shot expert localization,
with only a few in-domain demonstrations, the model consistently activates a
sparse and stable subset of experts on tasks within the same domain. Building
on this observation, we propose a simple yet effective pruning framework,
EASY-EP, that leverages a few domain-specific demonstrations to identify and
retain only the most relevant experts. EASY-EP comprises two key components:
output-aware expert importance assessment and expert-level token contribution
estimation. The former evaluates the importance of each expert for the current
token by considering the gating scores and L2 norm of the outputs of activated
experts, while the latter assesses the contribution of tokens based on
representation similarities before and after routed experts. Experiments on
DeepSeek-R1 and DeepSeek-V3-0324 show that our method can achieve comparable
performances and $2.99\times$ throughput under the same memory budget with full
model with only half the experts.

</details>


### [167] [Layers at Similar Depths Generate Similar Activations Across LLM Architectures](https://arxiv.org/pdf/2504.08775)
*Christopher Wolfram, Aaron Schein*

Main category: cs.CL

TL;DR: The paper explores how latent spaces in independently-trained LLMs relate, finding layer-specific nearest neighbor relationships that are shared across models but vary within layers.


<details>
  <summary>Details</summary>
Motivation: To understand the alignment and differences in latent spaces of independently-trained LLMs, examining their activation geometries.

Method: Analyzed nearest neighbor relationships in activations across 24 open-weight LLMs at different layers.

Result: Nearest neighbor relationships vary by layer within a model but are shared between corresponding layers of different models.

Conclusion: LLMs generate a progression of activation geometries shared across models, adapted to their architectures.

Abstract: How do the latent spaces used by independently-trained LLMs relate to one
another? We study the nearest neighbor relationships induced by activations at
different layers of 24 open-weight LLMs, and find that they 1) tend to vary
from layer to layer within a model, and 2) are approximately shared between
corresponding layers of different models. Claim 2 shows that these nearest
neighbor relationships are not arbitrary, as they are shared across models, but
Claim 1 shows that they are not "obvious" either, as there is no single set of
nearest neighbor relationships that is universally shared. Together, these
suggest that LLMs generate a progression of activation geometries from layer to
layer, but that this entire progression is largely shared between models,
stretched and squeezed to fit into different architectures.

</details>


### [168] [ClonEval: An Open Voice Cloning Benchmark](https://arxiv.org/pdf/2504.20581)
*Iwona Christop, Tomasz Kuczyński, Marek Kubis*

Main category: cs.CL

TL;DR: A new benchmark for voice cloning TTS models, including an evaluation protocol, open-source library, and leaderboard.


<details>
  <summary>Details</summary>
Motivation: To provide a standardized way to evaluate and compare voice cloning text-to-speech models.

Method: Introduces an evaluation protocol, open-source library for performance assessment, and a leaderboard.

Result: Detailed description of the evaluation procedure and software library usage, with organized leaderboard results.

Conclusion: The benchmark offers a comprehensive tool for assessing and improving voice cloning TTS models.

Abstract: We present a novel benchmark for voice cloning text-to-speech models. The
benchmark consists of an evaluation protocol, an open-source library for
assessing the performance of voice cloning models, and an accompanying
leaderboard. The paper discusses design considerations and presents a detailed
description of the evaluation procedure. The usage of the software library is
explained, along with the organization of results on the leaderboard.

</details>


### [169] [Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL](https://arxiv.org/pdf/2505.10832)
*Songjun Tu, Jiahao Lin, Qichao Zhang, Xiangyu Tian, Linjing Li, Xiangyuan Lan, Dongbin Zhao*

Main category: cs.CL

TL;DR: AutoThink is a framework that enables large reasoning models (LRMs) to dynamically decide when to engage in explicit reasoning, improving efficiency without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the computational overhead and latency caused by unnecessary detailed reasoning in LRMs for simple problems.

Method: Uses a multi-stage reinforcement learning (RL) framework to optimize reasoning policies, triggered by inserting an ellipsis in prompts.

Result: Achieves a 6.4% accuracy improvement and 52% token reduction on benchmarks.

Conclusion: AutoThink provides a scalable and adaptive reasoning paradigm for LRMs.

Abstract: Large reasoning models (LRMs) are proficient at generating explicit,
step-by-step reasoning sequences before producing final answers. However, such
detailed reasoning can introduce substantial computational overhead and
latency, particularly for simple problems. To address this over-thinking
problem, we explore how to equip LRMs with adaptive thinking capabilities:
enabling them to dynamically decide whether or not to engage in explicit
reasoning based on problem complexity. Building on R1-style distilled models,
we observe that inserting a simple ellipsis ("...") into the prompt can
stochastically trigger either a thinking or no-thinking mode, revealing a
latent controllability in the reasoning behavior. Leveraging this property, we
propose AutoThink, a multi-stage reinforcement learning (RL) framework that
progressively optimizes reasoning policies via stage-wise reward shaping.
AutoThink learns to invoke explicit reasoning only when necessary, while
defaulting to succinct responses for simpler tasks. Experiments on five
mainstream mathematical benchmarks demonstrate that AutoThink achieves
favorable accuracy-efficiency trade-offs compared to recent prompting and
RL-based pruning methods. It can be seamlessly integrated into any R1-style
model, including both distilled and further fine-tuned variants. Notably,
AutoThink improves relative accuracy by 6.4 percent while reducing token usage
by 52 percent on DeepSeek-R1-Distill-Qwen-1.5B, establishing a scalable and
adaptive reasoning paradigm for LRMs. Project Page:
https://github.com/ScienceOne-AI/AutoThink.

</details>


### [170] [Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs](https://arxiv.org/pdf/2505.11277)
*Yaorui Shi, Sihang Li, Chang Wu, Zhiyuan Liu, Junfeng Fang, Hengxing Cai, An Zhang, Xiang Wang*

Main category: cs.CL

TL;DR: AutoRefine enhances LLM reasoning by refining retrieved knowledge iteratively, outperforming existing methods in QA tasks.


<details>
  <summary>Details</summary>
Motivation: Address limitations of LLMs by improving retrieval-augmented reasoning to avoid irrelevant or noisy information.

Method: Introduces AutoRefine, a reinforcement learning framework with iterative knowledge refinement steps and tailored retrieval-specific rewards.

Result: Outperforms existing methods in single-hop and multi-hop QA benchmarks, especially in complex reasoning.

Conclusion: AutoRefine improves search quality and evidence synthesis, enhancing LLM reasoning capabilities.

Abstract: Large language models have demonstrated impressive reasoning capabilities but
are inherently limited by their knowledge reservoir. Retrieval-augmented
reasoning mitigates this limitation by allowing LLMs to query external
resources, but existing methods often retrieve irrelevant or noisy information,
hindering accurate reasoning. In this paper, we propose AutoRefine, a
reinforcement learning post-training framework that adopts a new
``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit
knowledge refinement steps between successive search calls, enabling the model
to iteratively filter, distill, and organize evidence before generating an
answer. Furthermore, we incorporate tailored retrieval-specific rewards
alongside answer correctness rewards using group relative policy optimization.
Experiments on single-hop and multi-hop QA benchmarks demonstrate that
AutoRefine significantly outperforms existing approaches, particularly in
complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine
issues frequent, higher-quality searches and synthesizes evidence effectively.

</details>


### [171] [Advancing Sequential Numerical Prediction in Autoregressive Models](https://arxiv.org/pdf/2505.13077)
*Xiang Fei, Jinghui Lu, Qi Sun, Hao Feng, Yanjie Wang, Wei Shi, An-Lan Wang, Jingqun Tang, Can Huang*

Main category: cs.CL

TL;DR: NTIL introduces a dual-level loss (token and sequence) to improve numerical sequence generation in autoregressive models, outperforming standard methods.


<details>
  <summary>Details</summary>
Motivation: Standard autoregressive models treat digits as independent tokens, ignoring the coherent structure of numerical sequences.

Method: NTIL combines token-level Earth Mover's Distance (EMD) for ordinal relationships and sequence-level discrepancy penalties.

Result: Extensive experiments show significant performance improvements with NTIL.

Conclusion: NTIL effectively enhances numerical prediction in autoregressive models and integrates well with LLMs/MLLMs.

Abstract: Autoregressive models have become the de facto choice for sequence generation
tasks, but standard approaches treat digits as independent tokens and apply
cross-entropy loss, overlooking the coherent structure of numerical sequences.
This paper introduces Numerical Token Integrity Loss (NTIL) to address this
gap. NTIL operates at two levels: (1) token-level, where it extends the Earth
Mover's Distance (EMD) to preserve ordinal relationships between numerical
values, and (2) sequence-level, where it penalizes the overall discrepancy
between the predicted and actual sequences. This dual approach improves
numerical prediction and integrates effectively with LLMs/MLLMs. Extensive
experiments show significant performance improvements with NTIL.

</details>


### [172] [Language-Specific Latent Process Hinders Cross-Lingual Performance](https://arxiv.org/pdf/2505.13141)
*Zheng Wei Lim, Alham Fikri Aji, Trevor Cohn*

Main category: cs.CL

TL;DR: LLMs exhibit cross-lingual transfer but produce inconsistent outputs. Analysis reveals reliance on language-specific subspaces, not shared semantics. Larger models dissociate more but retrieve knowledge better. Steering towards shared space improves multilingual reasoning.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs generalize knowledge across languages and address inconsistency in multilingual outputs.

Method: Applied the logit lens to interpret LLMs' steps in solving multilingual multi-choice questions, analyzed hidden states, and modulated knowledge sharing by steering latent processing.

Result: LLMs rely on language-specific subspaces, leading to inconsistency. Larger models dissociate more but retrieve knowledge better. Steering towards shared space improves performance.

Conclusion: Enhancing shared semantic space utilization improves multilingual reasoning and output consistency.

Abstract: Large language models (LLMs) are demonstrably capable of cross-lingual
transfer, but can produce inconsistent output when prompted with the same
queries written in different languages. To understand how language models are
able to generalize knowledge from one language to the others, we apply the
logit lens to interpret the implicit steps taken by LLMs to solve multilingual
multi-choice reasoning questions. We find LLMs predict inconsistently and are
less accurate because they rely on subspaces of individual languages, rather
than working in a shared semantic space. While larger models are more
multilingual, we show their hidden states are more likely to dissociate from
the shared representation compared to smaller models, but are nevertheless more
capable of retrieving knowledge embedded across different languages. Finally,
we demonstrate that knowledge sharing can be modulated by steering the models'
latent processing towards the shared semantic space. We find reinforcing
utilization of the shared space improves the models' multilingual reasoning
performance, as a result of more knowledge transfer from, and better output
consistency with English.

</details>


### [173] [Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks](https://arxiv.org/pdf/2505.13171)
*Yixuan Xu, Antoni-Joan Solergibert i Llaquet, Antoine Bosselut, Imanol Schlag*

Main category: cs.CL

TL;DR: Large language models memorize training data, risking copyright violations. The study identifies the 'offset effect,' where memorization decreases with longer prefixes or offsets from the start of the context window. Shifting sensitive data deeper into the context reduces memorization and text degeneration.


<details>
  <summary>Details</summary>
Motivation: To systematically assess the risk of copyright violations due to memorization in large language models by examining how verbatim memorization varies with prefix length and positional offset.

Method: Pretrain language models (1B/3B/8B) on 83B tokens, mixing web data with public domain books to simulate copyrighted content. Analyze memorization patterns, focusing on prefix length and positional offset.

Result: Verbatim memorization is strongest with short prefixes at the start of the context window and declines with longer prefixes or offsets. Positional fragility makes models sensitive to shifts in token positions.

Conclusion: Positional offset is a critical factor in memorization risks. Shifting sensitive data deeper into the context window can mitigate memorization and degeneration, challenging prior assumptions about uniformity in memorization.

Abstract: Large language models are known to memorize parts of their training data,
posing risk of copyright violations. To systematically examine this risk, we
pretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing
web-scale data with public domain books used to simulate copyrighted content at
controlled frequencies at lengths at least ten times longer than prior work. We
thereby identified the offset effect, a phenomenon characterized by two key
findings: (1) verbatim memorization is most strongly triggered by short
prefixes drawn from the beginning of the context window, with memorization
decreasing counterintuitively as prefix length increases; and (2) a sharp
decline in verbatim recall when prefix begins offset from the initial tokens of
the context window. We attribute this to positional fragility: models rely
disproportionately on the earliest tokens in their context window as retrieval
anchors, making them sensitive to even slight shifts. We further observe that
when the model fails to retrieve memorized content, it often produces
degenerated text. Leveraging these findings, we show that shifting sensitive
data deeper into the context window suppresses both extractable memorization
and degeneration. Our results suggest that positional offset is a critical and
previously overlooked axis for evaluating memorization risks, since prior work
implicitly assumed uniformity by probing only from the beginning of training
sequences.

</details>


### [174] [Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning](https://arxiv.org/pdf/2505.14471)
*Tong Li, Jiachuan Wang, Yongqi Zhang, Shuangyin Li, Lei Chen*

Main category: cs.CL

TL;DR: Citss is a novel framework for citation classification using self-supervised contrastive learning to address data scarcity and contextual noise, compatible with both encoder- and decoder-based models.


<details>
  <summary>Details</summary>
Motivation: Address challenges in citation classification like labeled data scarcity, contextual noise, and spurious keyphrase correlations.

Method: Introduces self-supervised contrastive learning with sentence-level cropping and keyphrase perturbation for better contrastive pairs.

Result: Outperforms previous state-of-the-art methods on three benchmark datasets.

Conclusion: Citss effectively adapts PLMs for citation classification, overcoming key challenges and supporting diverse model architectures.

Abstract: Citation classification, which identifies the intention behind academic
citations, is pivotal for scholarly analysis. Previous works suggest
fine-tuning pretrained language models (PLMs) on citation classification
datasets, reaping the reward of the linguistic knowledge they gained during
pretraining. However, directly fine-tuning for citation classification is
challenging due to labeled data scarcity, contextual noise, and spurious
keyphrase correlations. In this paper, we present a novel framework, Citss,
that adapts the PLMs to overcome these challenges. Citss introduces
self-supervised contrastive learning to alleviate data scarcity, and is
equipped with two specialized strategies to obtain the contrastive pairs:
sentence-level cropping, which enhances focus on target citations within long
contexts, and keyphrase perturbation, which mitigates reliance on specific
keyphrases. Compared with previous works that are only designed for
encoder-based PLMs, Citss is carefully developed to be compatible with both
encoder-based PLMs and decoder-based LLMs, to embrace the benefits of enlarged
pretraining. Experiments with three benchmark datasets with both encoder-based
PLMs and decoder-based LLMs demonstrate our superiority compared to the
previous state of the art. Our code is available at: github.com/LITONG99/Citss

</details>


### [175] [General-Reasoner: Advancing LLM Reasoning Across All Domains](https://arxiv.org/pdf/2505.14652)
*Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, Wenhu Chen*

Main category: cs.CL

TL;DR: General-Reasoner enhances LLM reasoning across diverse domains using a novel training paradigm, outperforming baselines on 12 benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current LLM reasoning focuses on math/coding due to data abundance, limiting broader applicability.

Method: Constructs a large dataset via web crawling and develops a generative model-based answer verifier.

Result: Outperforms baselines on diverse benchmarks (e.g., MMLU-Pro, GPQA) while maintaining math reasoning strength.

Conclusion: General-Reasoner achieves robust, generalizable reasoning across domains, advancing LLM capabilities.

Abstract: Reinforcement learning (RL) has recently demonstrated strong potential in
enhancing the reasoning capabilities of large language models (LLMs).
Particularly, the "Zero" reinforcement learning introduced by Deepseek-R1-Zero,
enables direct RL training of base LLMs without relying on an intermediate
supervised fine-tuning stage. Despite these advancements, current works for LLM
reasoning mainly focus on mathematical and coding domains, largely due to data
abundance and the ease of answer verification. This limits the applicability
and generalization of such models to broader domains, where questions often
have diverse answer representations, and data is more scarce. In this paper, we
propose General-Reasoner, a novel training paradigm designed to enhance LLM
reasoning capabilities across diverse domains. Our key contributions include:
(1) constructing a large-scale, high-quality dataset of questions with
verifiable answers curated by web crawling, covering a wide range of
disciplines; and (2) developing a generative model-based answer verifier, which
replaces traditional rule-based verification with the capability of
chain-of-thought and context-awareness. We train a series of models and
evaluate them on a wide range of datasets covering wide domains like physics,
chemistry, finance, electronics etc. Our comprehensive evaluation across these
12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)
demonstrates that General-Reasoner outperforms existing baseline methods,
achieving robust and generalizable reasoning performance while maintaining
superior effectiveness in mathematical reasoning tasks.

</details>


### [176] [Text Generation Beyond Discrete Token Sampling](https://arxiv.org/pdf/2505.14827)
*Yufan Zhuang, Liyuan Liu, Chandan Singh, Jingbo Shang, Jianfeng Gao*

Main category: cs.CL

TL;DR: MoI is a training-free method for autoregressive generation that blends generated tokens with discarded token distributions, improving text quality and reasoning.


<details>
  <summary>Details</summary>
Motivation: To preserve rich information from next-token distributions discarded in standard autoregressive generation.

Method: Uses Bayesian estimation to blend discrete tokens with token distributions, replacing one-hot vectors with continuous posterior expectations.

Result: Improves performance on mathematical reasoning, code generation, and PhD-level QA tasks across multiple models.

Conclusion: MoI enhances generation quality and reasoning without extra training or significant computational cost.

Abstract: In standard autoregressive generation, an LLM predicts the next-token
distribution, samples a discrete token, and then discards the distribution,
passing only the sampled token as new input. To preserve this distribution's
rich information, we propose Mixture of Inputs (MoI), a training-free method
for autoregressive generation. After generating a token following the standard
paradigm, we construct a new input that blends the generated discrete token
with the previously discarded token distribution. Specifically, we employ a
Bayesian estimation method that treats the token distribution as the prior, the
sampled token as the observation, and replaces the conventional one-hot vector
with the continuous posterior expectation as the new model input. MoI allows
the model to maintain a richer internal representation throughout the
generation process, resulting in improved text quality and reasoning
capabilities. On mathematical reasoning, code generation, and PhD-level QA
tasks, MoI consistently improves performance across multiple models including
QwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B, with no additional
training and negligible computational overhead.

</details>


### [177] [KaFT: Knowledge-aware Fine-tuning for Boosting LLMs' Domain-specific Question-Answering Performance](https://arxiv.org/pdf/2505.15480)
*Qihuang Zhong, Liang Ding, Xiantao Cai, Juhua Liu, Bo Du, Dacheng Tao*

Main category: cs.CL

TL;DR: The paper proposes Knowledge-aware Fine-tuning (KaFT) to address conflicts between LLMs' internal knowledge and training data in SFT, improving QA performance by adaptively weighting training samples based on conflict levels.


<details>
  <summary>Details</summary>
Motivation: Conflicts between LLMs' internal knowledge and training data in SFT lead to suboptimal performance, prompting the need for a better approach.

Method: The authors design a query diversification strategy for conflict detection and propose KaFT, which adjusts training weights based on conflict levels.

Result: KaFT consistently improves performance across four LLMs, enhancing generalization and reducing hallucination.

Conclusion: KaFT is a simple-yet-effective solution to leverage conflict data for better fine-tuning outcomes.

Abstract: Supervised fine-tuning (SFT) is a common approach to improve the
domain-specific question-answering (QA) performance of large language models
(LLMs). However, recent literature reveals that due to the conflicts between
LLMs' internal knowledge and the context knowledge of training data, vanilla
SFT using the full QA training set is usually suboptimal. In this paper, we
first design a query diversification strategy for robust conflict detection and
then conduct a series of experiments to analyze the impact of knowledge
conflict. We find that 1) training samples with varied conflicts contribute
differently, where SFT on the data with large conflicts leads to catastrophic
performance drops; 2) compared to directly filtering out the conflict data,
appropriately applying the conflict data would be more beneficial. Motivated by
this, we propose a simple-yet-effective Knowledge-aware Fine-tuning (namely
KaFT) approach to effectively boost LLMs' performance. The core of KaFT is to
adapt the training weight by assigning different rewards for different training
samples according to conflict level. Extensive experiments show that KaFT
brings consistent and significant improvements across four LLMs. More analyses
prove that KaFT effectively improves the model generalization and alleviates
the hallucination.

</details>


### [178] [EduBench: A Comprehensive Benchmarking Dataset for Evaluating Large Language Models in Diverse Educational Scenarios](https://arxiv.org/pdf/2505.16160)
*Bin Xu, Yu Bai, Huashan Sun, Yiguan Lin, Siming Liu, Xinyue Liang, Yaolin Li, Yang Gao, Heyan Huang*

Main category: cs.CL

TL;DR: The paper introduces EduBench, a diverse benchmark for educational scenarios, with synthetic data and multi-dimensional evaluation metrics. A small-scale model trained on this dataset performs comparably to state-of-the-art large models.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored application of large language models in education by creating a tailored benchmark and evaluation framework.

Method: Developed a benchmark with synthetic data (9 scenarios, 4,000 contexts) and multi-dimensional metrics (12 aspects). Used human annotation for validation and trained a small-scale model.

Result: The small-scale model achieved performance comparable to state-of-the-art large models (e.g., Deepseek V3, Qwen Max) on the test set.

Conclusion: The work provides a practical foundation for developing and evaluating education-oriented language models, with code and data publicly released.

Abstract: As large language models continue to advance, their application in
educational contexts remains underexplored and under-optimized. In this paper,
we address this gap by introducing the first diverse benchmark tailored for
educational scenarios, incorporating synthetic data containing 9 major
scenarios and over 4,000 distinct educational contexts. To enable comprehensive
assessment, we propose a set of multi-dimensional evaluation metrics that cover
12 critical aspects relevant to both teachers and students. We further apply
human annotation to ensure the effectiveness of the model-generated evaluation
responses. Additionally, we succeed to train a relatively small-scale model on
our constructed dataset and demonstrate that it can achieve performance
comparable to state-of-the-art large models (e.g., Deepseek V3, Qwen Max) on
the test set. Overall, this work provides a practical foundation for the
development and evaluation of education-oriented language models. Code and data
are released at https://github.com/ybai-nlp/EduBench.

</details>


### [179] [When Do LLMs Admit Their Mistakes? Understanding the Role of Model Belief in Retraction](https://arxiv.org/pdf/2505.16170)
*Yuqing Yang, Robin Jia*

Main category: cs.CL

TL;DR: LLMs can retract incorrect answers but do so rarely. Retraction is linked to their internal belief, and fine-tuning improves performance.


<details>
  <summary>Details</summary>
Motivation: To understand when and why LLMs retract incorrect answers, focusing on their internal belief systems.

Method: Constructed model-specific datasets, conducted steering experiments, and applied supervised fine-tuning.

Result: LLMs retract infrequently, influenced by internal belief. Fine-tuning enhances retraction accuracy.

Conclusion: Retraction behavior in LLMs is tied to internal belief, and fine-tuning can improve it.

Abstract: Can large language models (LLMs) admit their mistakes when they should know
better? In this work, we define the behavior of acknowledging errors in
previously generated answers as "retraction" and aim to understand when and why
LLMs choose to retract. We first construct model-specific datasets to evaluate
whether a model will retract an incorrect answer that contradicts its own
parametric knowledge. While LLMs are capable of retraction, they do so only
infrequently. We demonstrate that retraction is closely tied to previously
identified indicators of models' internal belief: models fail to retract wrong
answers that they "believe" to be factually correct. Steering experiments
further demonstrate that internal belief causally influences model retraction.
In particular, when the model does not believe its answer, this not only
encourages the model to attempt to verify the answer, but also alters attention
behavior during self-verification. Finally, we demonstrate that simple
supervised fine-tuning significantly improves retraction performance by helping
the model learn more accurate internal beliefs. Code and datasets are available
on https://github.com/ayyyq/llm-retraction.

</details>


### [180] [Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models](https://arxiv.org/pdf/2505.17601)
*Jiawei Kong, Hao Fang, Xiaochen Yang, Kuofeng Gao, Bin Chen, Shu-Tao Xia, Yaowei Wang, Min Zhang*

Main category: cs.CL

TL;DR: A novel clean-data backdoor attack for jailbreaking LLMs is proposed, bypassing safety guardrails by overfitting triggers to benign replies, achieving high attack success rates.


<details>
  <summary>Details</summary>
Motivation: Existing poisoning attacks are detectable and compromise model safety alignment, prompting a need for stealthier methods.

Method: Overfit triggers to benign-sounding prefixes using harmless QA pairs, leveraging model priors for harmful completions, and optimizing triggers with gradient-based methods.

Result: Achieves high ASR (86.67% on LLaMA-3-8B, 85% on Qwen-2.5-7B) even under guardrail detection.

Conclusion: The proposed method effectively bypasses safety measures, highlighting vulnerabilities in LLM alignment.

Abstract: Supervised fine-tuning (SFT) aligns large language models (LLMs) with human
intent by training them on labeled task-specific data. Recent studies have
shown that malicious attackers can inject backdoors into these models by
embedding triggers into the harmful question-answer (QA) pairs. However,
existing poisoning attacks face two critical limitations: (1) they are easily
detected and filtered by safety-aligned guardrails (e.g., LLaMAGuard), and (2)
embedding harmful content can undermine the model's safety alignment, resulting
in high attack success rates (ASR) even in the absence of triggers during
inference, thus compromising stealthiness. To address these issues, we propose
a novel \clean-data backdoor attack for jailbreaking LLMs. Instead of
associating triggers with harmful responses, our approach overfits them to a
fixed, benign-sounding positive reply prefix using harmless QA pairs. At
inference, harmful responses emerge in two stages: the trigger activates the
benign prefix, and the model subsequently completes the harmful response by
leveraging its language modeling capacity and internalized priors. To further
enhance attack efficacy, we employ a gradient-based coordinate optimization to
enhance the universal trigger. Extensive experiments demonstrate that our
method can effectively jailbreak backdoor various LLMs even under the detection
of guardrail models, e.g., an ASR of 86.67% and 85% on LLaMA-3-8B and
Qwen-2.5-7B judged by GPT-4o.

</details>


### [181] [ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models](https://arxiv.org/pdf/2505.18799)
*Hao Chen, Haoze Li, Zhiqing Xiao, Lirong Gao, Qi Zhang, Xiaomeng Hu, Ningtao Wang, Xing Fu, Junbo Zhao*

Main category: cs.CL

TL;DR: ALPS is a method to efficiently align LLMs by localizing and pruning task-sensitive attention heads, reducing training costs while improving performance.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and data dependency in aligning LLMs to downstream tasks.

Method: Proposes ALPS, which localizes and prunes task-sensitive attention heads, limiting updates to these heads.

Result: Activates only 10% of attention parameters, achieving 2% performance improvement, with transferable task-specific heads.

Conclusion: ALPS offers an efficient LLM alignment approach, reducing costs and improving generalization.

Abstract: Aligning general-purpose large language models (LLMs) to downstream tasks
often incurs significant training adjustment costs. Prior research has explored
various avenues to enhance alignment efficiency, primarily through minimal-data
training or data-driven activations to identify key attention heads. However,
these approaches inherently introduce data dependency, which hinders
generalization and reusability. To address this issue and enhance model
alignment efficiency, we propose the \textit{\textbf{A}ttention
\textbf{L}ocalization and \textbf{P}runing \textbf{S}trategy (\textbf{ALPS})},
an efficient algorithm that localizes the most task-sensitive attention heads
and prunes by restricting attention training updates to these heads, thereby
reducing alignment costs. Experimental results demonstrate that our method
activates only \textbf{10\%} of attention parameters during fine-tuning while
achieving a \textbf{2\%} performance improvement over baselines on three tasks.
Moreover, the identified task-specific heads are transferable across datasets
and mitigate knowledge forgetting. Our work and findings provide a novel
perspective on efficient LLM alignment. The code is available at
https://github.com/VoiceBeer/ALPS.

</details>


### [182] [Moderating Harm: Benchmarking Large Language Models for Cyberbullying Detection in YouTube Comments](https://arxiv.org/pdf/2505.18927)
*Amel Muminovic*

Main category: cs.CL

TL;DR: The study benchmarks GPT-4.1, Gemini 1.5 Pro, and Claude 3 Opus for moderating harmful YouTube comments, finding GPT-4.1 balances performance best, while Gemini has high recall and Claude high precision. All struggle with sarcasm and slang.


<details>
  <summary>Details</summary>
Motivation: Address the growing issue of online harassment in comment sections by evaluating the effectiveness of leading language models in content moderation.

Method: Tested GPT-4.1, Gemini 1.5 Pro, and Claude 3 Opus on 5,080 YouTube comments (1,334 harmful, 3,746 non-harmful) in English, Arabic, and Indonesian, using unified prompts and deterministic settings.

Result: GPT-4.1 achieved the best balance (F1=0.863), Gemini had highest recall (0.875) but lower precision (0.767), and Claude had highest precision (0.920) but lower recall (0.720). All struggled with sarcasm and slang.

Conclusion: Combining models, incorporating context, and fine-tuning for underrepresented languages and implicit abuse are needed. The dataset and prompts are released for reproducibility.

Abstract: As online platforms grow, comment sections increasingly host harassment that
undermines user experience and well-being. This study benchmarks three leading
large language models, OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic
Claude 3 Opus, on a corpus of 5,080 YouTube comments sampled from high-abuse
threads in gaming, lifestyle, food vlog, and music channels. The dataset
comprises 1,334 harmful and 3,746 non-harmful messages in English, Arabic, and
Indonesian, annotated independently by two reviewers with substantial agreement
(Cohen's kappa = 0.83). Using a unified prompt and deterministic settings,
GPT-4.1 achieved the best overall balance with an F1 score of 0.863, precision
of 0.887, and recall of 0.841. Gemini flagged the highest share of harmful
posts (recall = 0.875) but its precision fell to 0.767 due to frequent false
positives. Claude delivered the highest precision at 0.920 and the lowest
false-positive rate of 0.022, yet its recall dropped to 0.720. Qualitative
analysis showed that all three models struggle with sarcasm, coded insults, and
mixed-language slang. These results underscore the need for moderation
pipelines that combine complementary models, incorporate conversational
context, and fine-tune for under-represented languages and implicit abuse. A
de-identified version of the dataset and full prompts is publicly released to
promote reproducibility and further progress in automated content moderation.

</details>


### [183] [Balancing Computation Load and Representation Expressivity in Parallel Hybrid Neural Networks](https://arxiv.org/pdf/2505.19472)
*Mohammad Mahdi Moradi, Walid Ahmed, Shuangyue Wen, Sudhir Mudur, Weiwei Zhang, Yang Liu*

Main category: cs.CL

TL;DR: FlowHN is a parallel hybrid network combining attention and SSMs, improving speed and accuracy through dynamic token splitting and output fusion.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in sequential hybrid models (idle periods, latency) and challenges in parallel hybrids (divergent outputs, load balancing).

Method: FlowHN uses a FLOP-aware dynamic token split for load balancing and fuses divergent outputs from attention and SSM branches for expressive representations.

Result: Achieves up to 4× higher Tokens per Second (TPS) and 2× better Model FLOPs Utilization (MFU) compared to competitors.

Conclusion: FlowHN outperforms sequential and parallel hybrid models in speed and accuracy, validated on autoregressive language modeling tasks.

Abstract: Attention and State-Space Models (SSMs) when combined in a hybrid network in
sequence or in parallel provide complementary strengths. In a hybrid sequential
pipeline they alternate between applying a transformer to the input and then
feeding its output into a SSM. This results in idle periods in the individual
components increasing end-to-end latency and lowering throughput caps. In the
parallel hybrid architecture, the transformer operates independently in
parallel with the SSM, and these pairs are cascaded, with output from one pair
forming the input to the next. Two issues are (i) creating an expressive
knowledge representation with the inherently divergent outputs from these
separate branches, and (ii) load balancing the computation between these
parallel branches, while maintaining representation fidelity. In this work we
present FlowHN, a novel parallel hybrid network architecture that accommodates
various strategies for load balancing, achieved through appropriate
distribution of input tokens between the two branches. Two innovative
differentiating factors in FlowHN include a FLOP aware dynamic token split
between the attention and SSM branches yielding efficient balance in compute
load, and secondly, a method to fuse the highly divergent outputs from
individual branches for enhancing representation expressivity. Together they
enable much better token processing speeds, avoid bottlenecks, and at the same
time yield significantly improved accuracy as compared to other competing
works. We conduct comprehensive experiments on autoregressive language modeling
for models with 135M, 350M, and 1B parameters. FlowHN outperforms sequential
hybrid models and its parallel counterpart, achieving up to 4* higher Tokens
per Second (TPS) and 2* better Model FLOPs Utilization (MFU).

</details>


### [184] [Continuous Self-Improvement of Large Language Models by Test-time Training with Verifier-Driven Sample Selection](https://arxiv.org/pdf/2505.19475)
*Mohammad Mahdi Moradi, Hossam Amer, Sudhir Mudur, Weiwei Zhang, Yang Liu, Walid Ahmed*

Main category: cs.CL

TL;DR: VDS-TTT is a framework using a verifier to select high-confidence pseudo-labeled examples for test-time training, improving LLM adaptation to out-of-distribution data.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of adapting pretrained language models to unlabeled, out-of-distribution data where models often fail on novel reasoning tasks.

Method: A learned verifier scores generated responses, selecting high-ranking pseudo-labeled examples for fine-tuning low-rank LoRA adapter parameters.

Result: VDS-TTT achieves up to 32.29% improvement over the base model and 6.66% gain over verifier-based methods without test-time training.

Conclusion: VDS-TTT is effective and efficient for on-the-fly adaptation of large language models.

Abstract: Learning to adapt pretrained language models to unlabeled,
out-of-distribution data is a critical challenge, as models often falter on
structurally novel reasoning tasks even while excelling within their training
distribution. We introduce a new framework called VDS-TTT - Verifier-Driven
Sample Selection for Test-Time Training to efficiently address this. We use a
learned verifier to score a pool of generated responses and select only from
high ranking pseudo-labeled examples for fine-tuned adaptation. Specifically,
for each input query our LLM generates N candidate answers; the verifier
assigns a reliability score to each, and the response with the highest
confidence and above a fixed threshold is paired with its query for test-time
training. We fine-tune only low-rank LoRA adapter parameters, ensuring
adaptation efficiency and fast convergence. Our proposed self-supervised
framework is the first to synthesize verifier driven test-time training data
for continuous self-improvement of the model. Experiments across three diverse
benchmarks and three state-of-the-art LLMs demonstrate that VDS-TTT yields up
to a 32.29% relative improvement over the base model and a 6.66% gain compared
to verifier-based methods without test-time training, highlighting its
effectiveness and efficiency for on-the-fly large language model adaptation.

</details>


### [185] [Faster and Better LLMs via Latency-Aware Test-Time Scaling](https://arxiv.org/pdf/2505.19634)
*Zili Wang, Tianyu Zhang, Lei Zhu, Haoli Bai, Lu Hou, Shiming Xiang, Xianzhi Yu, Wulong Liu*

Main category: cs.CL

TL;DR: Test-Time Scaling (TTS) improves LLM performance but lacks latency efficiency. The paper proposes latency-optimal TTS via branch-wise and sequence-wise parallelism, achieving high accuracy under strict latency constraints.


<details>
  <summary>Details</summary>
Motivation: Existing TTS methods overlook latency efficiency, which is critical in real-world applications. The paper aims to bridge this gap.

Method: Proposes two approaches: branch-wise parallelism (concurrent inference branches) and sequence-wise parallelism (speculative decoding), optimizing resource allocation.

Result: Achieves 82.3% accuracy for a 32B model in 1 minute and 72.4% for a 3B model in 10 seconds on MATH-500.

Conclusion: Latency-aware TTS is crucial for balancing speed and accuracy in latency-sensitive scenarios.

Abstract: Test-Time Scaling (TTS) has proven effective in improving the performance of
Large Language Models (LLMs) during inference. However, existing research has
overlooked the efficiency of TTS from a latency-sensitive perspective. Through
a latency-aware evaluation of representative TTS methods, we demonstrate that a
compute-optimal TTS does not always result in the lowest latency in scenarios
where latency is critical. To address this gap and achieve latency-optimal TTS,
we propose two key approaches by optimizing the concurrency configurations: (1)
branch-wise parallelism, which leverages multiple concurrent inference
branches, and (2) sequence-wise parallelism, enabled by speculative decoding.
By integrating these two approaches and allocating computational resources
properly to each, our latency-optimal TTS enables a 32B model to reach 82.3%
accuracy on MATH-500 within 1 minute and a smaller 3B model to achieve 72.4%
within 10 seconds. Our work emphasizes the importance of latency-aware TTS and
demonstrates its ability to deliver both speed and accuracy in
latency-sensitive scenarios.

</details>


### [186] [Comparing Moral Values in Western English-speaking societies and LLMs with Word Associations](https://arxiv.org/pdf/2505.19674)
*Chaoyi Xiang, Chunhua Liu, Simon De Deyne, Lea Frermann*

Main category: cs.CL

TL;DR: The paper proposes using word associations to study moral values in LLMs, avoiding biases from direct prompting, and compares these values with human associations.


<details>
  <summary>Details</summary>
Motivation: Understanding the moral values reflected in LLMs is crucial due to their growing impact, but direct prompting is unreliable due to human norm leakage and prompt sensitivity.

Method: The study uses word associations to assess moral reasoning, creates a dataset of LLM-generated associations, and propagates moral values through association graphs.

Result: The research reveals systematic differences between moral values derived from English speakers and LLM associations.

Conclusion: Word associations provide a robust method to analyze LLMs' moral reasoning, uncovering distinct moral conceptualizations compared to humans.

Abstract: As the impact of large language models increases, understanding the moral
values they reflect becomes ever more important. Assessing the nature of moral
values as understood by these models via direct prompting is challenging due to
potential leakage of human norms into model training data, and their
sensitivity to prompt formulation. Instead, we propose to use word
associations, which have been shown to reflect moral reasoning in humans, as
low-level underlying representations to obtain a more robust picture of LLMs'
moral reasoning. We study moral differences in associations from western
English-speaking communities and LLMs trained predominantly on English data.
First, we create a large dataset of LLM-generated word associations, resembling
an existing data set of human word associations. Next, we propose a novel
method to propagate moral values based on seed words derived from Moral
Foundation Theory through the human and LLM-generated association graphs.
Finally, we compare the resulting moral conceptualizations, highlighting
detailed but systematic differences between moral values emerging from English
speakers and LLM associations.

</details>


### [187] [The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants](https://arxiv.org/pdf/2505.19797)
*Yiqun Zhang, Hao Li, Chenxu Wang, Linyao Chen, Qiaosheng Zhang, Peng Ye, Shi Feng, Daling Wang, Zhen Wang, Xinrun Wang, Jia Xu, Lei Bai, Wanli Ouyang, Shuyue Hu*

Main category: cs.CL

TL;DR: The Avengers framework leverages smaller open-source language models to outperform GPT-4.1 on multiple tasks by using embedding, clustering, scoring, and voting.


<details>
  <summary>Details</summary>
Motivation: Address the competitiveness of smaller models against proprietary giants like GPT-4.1.

Method: A four-step framework: embedding, clustering, scoring, and voting to select top-performing models for each query.

Result: Outperforms GPT-4.1 on 9/15 datasets, notably in mathematics (+18.21%) and code (+7.46%) tasks.

Conclusion: Smaller models can compete with larger proprietary ones when combined effectively, as shown by the Avengers framework.

Abstract: As proprietary giants increasingly dominate the race for ever-larger language
models, a pressing question arises for the open-source community: can smaller
models remain competitive across a broad range of tasks? In this paper, we
present the Avengers--a simple recipe that effectively leverages the collective
intelligence of open-source, smaller language models. Our framework is built
upon four lightweight operations: (i) embedding: encode queries using a text
embedding model; (ii) clustering: group queries based on their semantic
similarity; (iii) scoring: scores each model's performance within each cluster;
and (iv) voting: improve outputs via repeated sampling and voting. At inference
time, each query is embedded and assigned to its nearest cluster. The
top-performing model(s) within that cluster are selected to generate the
response using the Self-Consistency or its multi-model variant. Remarkably,
with 10 open-source models (~7B parameters each), the Avengers collectively
outperforms GPT-4.1 on nine out of 15 datasets (spanning mathematics, code,
logic, knowledge, and affective tasks). In particular, it surpasses GPT-4.1 on
mathematics tasks by 18.21% and on code tasks by 7.46%. Furthermore, the
Avengers delivers superior out-of-distribution generalization, and remains
robust across various embedding models, clustering algorithms, ensemble
strategies, and values of its sole parameter--the number of clusters. We have
open-sourced the code on GitHub: https://github.com/ZhangYiqun018/Avengers

</details>


### [188] [Incentivizing Strong Reasoning from Weak Supervision](https://arxiv.org/pdf/2505.20072)
*Yige Yuan, Teng Xiao, Shuchang Tao, Xue Wang, Jinyang Gao, Bolin Ding, Bingbing Xu*

Main category: cs.CL

TL;DR: Weak supervision from weaker models can enhance LLM reasoning, achieving 94% of RL gains at lower cost.


<details>
  <summary>Details</summary>
Motivation: Avoid expensive RL or high-quality SFT for improving LLM reasoning.

Method: Use supervision from weaker models to train stronger LLMs.

Result: Weak supervision recovers 94% of RL gains, improving reasoning across tasks.

Conclusion: Weak-to-strong supervision is a cost-effective alternative for enhancing LLM reasoning.

Abstract: Large language models (LLMs) have demonstrated impressive performance on
reasoning-intensive tasks, but enhancing their reasoning abilities typically
relies on either reinforcement learning (RL) with verifiable signals or
supervised fine-tuning (SFT) with high-quality long chain-of-thought (CoT)
demonstrations, both of which are expensive. In this paper, we study a novel
problem of incentivizing the reasoning capacity of LLMs without expensive
high-quality demonstrations and reinforcement learning. We investigate whether
the reasoning capabilities of LLMs can be effectively incentivized via
supervision from significantly weaker models. We further analyze when and why
such weak supervision succeeds in eliciting reasoning abilities in stronger
models. Our findings show that supervision from significantly weaker reasoners
can substantially improve student reasoning performance, recovering close to
94% of the gains of expensive RL at a fraction of the cost. Experiments across
diverse benchmarks and model architectures demonstrate that weak reasoners can
effectively incentivize reasoning in stronger student models, consistently
improving performance across a wide range of reasoning tasks. Our results
suggest that this simple weak-to-strong paradigm is a promising and
generalizable alternative to costly methods for incentivizing strong reasoning
capabilities at inference-time in LLMs. The code is publicly available at
https://github.com/yuanyige/w2sr.

</details>


### [189] [Inference-time Alignment in Continuous Space](https://arxiv.org/pdf/2505.20081)
*Yige Yuan, Teng Xiao, Li Yunfan, Bingbing Xu, Shuchang Tao, Yunqi Qiu, Huawei Shen, Xueqi Cheng*

Main category: cs.CL

TL;DR: SEA is a gradient-based sampling method for aligning large language models with human feedback at inference time, outperforming baselines by up to 77.51% on AdvBench and 16.36% on MATH.


<details>
  <summary>Details</summary>
Motivation: Existing methods for inference-time alignment struggle with weak base policies or small candidate sets, limiting effectiveness.

Method: SEA adapts responses via gradient-based sampling in continuous latent space, optimizing an energy function iteratively.

Result: SEA achieves significant improvements over baselines (77.51% on AdvBench, 16.36% on MATH).

Conclusion: SEA offers a simple yet effective solution for inference-time alignment, outperforming existing methods.

Abstract: Aligning large language models with human feedback at inference time has
received increasing attention due to its flexibility. Existing methods rely on
generating multiple responses from the base policy for search using a reward
model, which can be considered as searching in a discrete response space.
However, these methods struggle to explore informative candidates when the base
policy is weak or the candidate set is small, resulting in limited
effectiveness. In this paper, to address this problem, we propose Simple Energy
Adaptation ($\textbf{SEA}$), a simple yet effective algorithm for
inference-time alignment. In contrast to expensive search over the discrete
space, SEA directly adapts original responses from the base policy toward the
optimal one via gradient-based sampling in continuous latent space.
Specifically, SEA formulates inference as an iterative optimization procedure
on an energy function over actions in the continuous space defined by the
optimal policy, enabling simple and effective alignment. For instance, despite
its simplicity, SEA outperforms the second-best baseline with a relative
improvement of up to $ \textbf{77.51%}$ on AdvBench and $\textbf{16.36%}$ on
MATH. Our code is publicly available at https://github.com/yuanyige/sea

</details>


### [190] [Reasoning Is Not All You Need: Examining LLMs for Multi-Turn Mental Health Conversations](https://arxiv.org/pdf/2505.20201)
*Mohit Chandra, Siddharth Sriraman, Harneet Singh Khanuja, Yiqiao Jin, Munmun De Choudhury*

Main category: cs.CL

TL;DR: The paper introduces MedAgent and MultiSenseEval to evaluate LLMs in multi-turn mental health conversations, revealing poor performance in patient-centric communication.


<details>
  <summary>Details</summary>
Motivation: Limited mental healthcare access and LLM capabilities drive the need to explore LLMs' multi-turn mental health conversation abilities, which are under-evaluated.

Method: Developed MedAgent for synthetic multi-turn mental health conversations and created the MHSD dataset (2,200+ dialogues). Introduced MultiSenseEval for holistic LLM evaluation.

Result: Frontier LLMs perform poorly (31% average score) in patient-centric communication and struggle with advanced diagnostics, with performance varying by patient persona and conversation length.

Conclusion: The work provides tools (MedAgent, MHSD, MultiSenseEval) to assess LLMs in mental health conversations, highlighting their current limitations.

Abstract: Limited access to mental healthcare, extended wait times, and increasing
capabilities of Large Language Models (LLMs) has led individuals to turn to
LLMs for fulfilling their mental health needs. However, examining the
multi-turn mental health conversation capabilities of LLMs remains
under-explored. Existing evaluation frameworks typically focus on diagnostic
accuracy and win-rates and often overlook alignment with patient-specific
goals, values, and personalities required for meaningful conversations. To
address this, we introduce MedAgent, a novel framework for synthetically
generating realistic, multi-turn mental health sensemaking conversations and
use it to create the Mental Health Sensemaking Dialogue (MHSD) dataset,
comprising over 2,200 patient-LLM conversations. Additionally, we present
MultiSenseEval, a holistic framework to evaluate the multi-turn conversation
abilities of LLMs in healthcare settings using human-centric criteria. Our
findings reveal that frontier reasoning models yield below-par performance for
patient-centric communication and struggle at advanced diagnostic capabilities
with average score of 31%. Additionally, we observed variation in model
performance based on patient's persona and performance drop with increasing
turns in the conversation. Our work provides a comprehensive synthetic data
generation framework, a dataset and evaluation framework for assessing LLMs in
multi-turn mental health conversations.

</details>


### [191] [PreP-OCR: A Complete Pipeline for Document Image Restoration and Enhanced OCR Accuracy](https://arxiv.org/pdf/2505.20429)
*Shuhao Guan, Moule Lin, Cheng Xu, Xinyi Liu, Jinman Zhao, Jiexin Fan, Qi Xu, Derek Greene*

Main category: cs.CL

TL;DR: PreP-OCR is a two-stage pipeline combining image restoration and post-OCR correction to improve text extraction from degraded historical documents, reducing error rates by 63.9-70.3%.


<details>
  <summary>Details</summary>
Motivation: To enhance text extraction from degraded historical documents by improving both visual clarity and textual consistency.

Method: 1. Synthesize document-image pairs with diverse fonts/layouts and apply degradations. Train an image restoration model using patch extraction/fusion. 2. Fine-tune a ByT5 post-OCR model on synthetic historical text pairs.

Result: Experiments on 13,831 pages show PreP-OCR reduces character error rates by 63.9-70.3% compared to raw OCR.

Conclusion: Integrating image restoration with linguistic error correction is effective for digitizing historical archives.

Abstract: This paper introduces PreP-OCR, a two-stage pipeline that combines document
image restoration with semantic-aware post-OCR correction to enhance both
visual clarity and textual consistency, thereby improving text extraction from
degraded historical documents. First, we synthesize document-image pairs from
plaintext, rendering them with diverse fonts and layouts and then applying a
randomly ordered set of degradation operations. An image restoration model is
trained on this synthetic data, using multi-directional patch extraction and
fusion to process large images. Second, a ByT5 post-OCR model, fine-tuned on
synthetic historical text pairs, addresses remaining OCR errors. Detailed
experiments on 13,831 pages of real historical documents in English, French,
and Spanish show that the PreP-OCR pipeline reduces character error rates by
63.9-70.3% compared to OCR on raw images. Our pipeline demonstrates the
potential of integrating image restoration with linguistic error correction for
digitizing historical archives.

</details>


### [192] [In-context Language Learning for Endangered Languages in Speech Recognition](https://arxiv.org/pdf/2505.20445)
*Zhaolin Li, Jan Niehues*

Main category: cs.CL

TL;DR: LLMs can learn low-resource languages via in-context learning, improving language modeling and ASR, outperforming traditional methods and matching dedicated models.


<details>
  <summary>Details</summary>
Motivation: To explore if LLMs can learn unseen, low-resource languages without supervised data, extending prior work to speech recognition.

Method: Experiments on four endangered languages using in-context learning (ICL), comparing probability-based and instruction-based approaches.

Result: More relevant text samples boost performance; probability-based ICL outperforms instruction-based. LLMs match or surpass dedicated ASR models.

Conclusion: ICL enables LLMs to learn low-resource languages effectively, maintaining their original capabilities while achieving competitive ASR performance.

Abstract: With approximately 7,000 languages spoken worldwide, current large language
models (LLMs) support only a small subset. Prior research indicates LLMs can
learn new languages for certain tasks without supervised data. We extend this
investigation to speech recognition, investigating whether LLMs can learn
unseen, low-resource languages through in-context learning (ICL). With
experiments on four diverse endangered languages that LLMs have not been
trained on, we find that providing more relevant text samples enhances
performance in both language modelling and Automatic Speech Recognition (ASR)
tasks. Furthermore, we show that the probability-based approach outperforms the
traditional instruction-based approach in language learning. Lastly, we show
ICL enables LLMs to achieve ASR performance that is comparable to or even
surpasses dedicated language models trained specifically for these languages,
while preserving the original capabilities of the LLMs.

</details>


### [193] [AstroVisBench: A Code Benchmark for Scientific Computing and Visualization in Astronomy](https://arxiv.org/pdf/2505.20538)
*Sebastian Antony Joseph, Syed Murtaza Husain, Stella S. R. Offner, Stéphanie Juneau, Paul Torrey, Adam S. Bolton, Juan P. Farias, Niall Gaffney, Greg Durrett, Junyi Jessy Li*

Main category: cs.CL

TL;DR: AstroVisBench is introduced as the first benchmark for evaluating LLMs in astronomy, focusing on data processing and visualization, revealing gaps in their utility for scientific research.


<details>
  <summary>Details</summary>
Motivation: The study aims to assess LLMs' ability to assist in scientific research by processing data and generating visualizations, a capability not previously evaluated.

Method: AstroVisBench evaluates LLMs on creating astronomy workflows and visualizing results, using an LLM-as-a-judge approach validated by astronomers.

Result: State-of-the-art LLMs show significant limitations in assisting astronomy research, as revealed by AstroVisBench.

Conclusion: AstroVisBench provides a foundation for improving LLMs in scientific workflows, particularly in visualization-heavy domains like astronomy.

Abstract: Large Language Models (LLMs) are being explored for applications in
scientific research, including their capabilities to synthesize literature,
answer research questions, generate research ideas, and even conduct
computational experiments. Ultimately, our goal is for these to help scientists
derive novel scientific insights. In many areas of science, such insights often
arise from processing and visualizing data to understand its patterns. However,
evaluating whether an LLM-mediated scientific workflow produces outputs
conveying the correct scientific insights is challenging to evaluate and has
not been addressed in past work. We introduce AstroVisBench, the first
benchmark for both scientific computing and visualization in the astronomy
domain. AstroVisBench judges a language model's ability to both (1) create
astronomy-specific workflows to process and analyze data and (2) visualize the
results of these workflows through complex plots. Our evaluation of
visualizations uses a novel LLM-as-a-judge workflow, which is validated against
annotation by five professional astronomers. Using AstroVisBench we present an
evaluation of state-of-the-art language models, showing a significant gap in
their ability to engage in astronomy research as useful assistants. This
evaluation provides a strong end-to-end evaluation for AI scientists that
offers a path forward for the development of visualization-based workflows,
which are central to a broad range of domains from physics to biology.

</details>


### [194] [Paths Not Taken: Understanding and Mending the Multilingual Factual Recall Pipeline](https://arxiv.org/pdf/2505.20546)
*Meng Lu, Ruochen Zhang, Carsten Eickhoff, Ellie Pavlick*

Main category: cs.CL

TL;DR: The paper investigates why multilingual LLMs perform worse in non-English languages for factual recall, attributing it to an English-centric processing pipeline and translation errors. It proposes language-independent interventions to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: To understand and address the factual inconsistency in multilingual LLMs, which perform better in English than other languages.

Method: Mechanistic analysis to uncover the LLM's processing pipeline, identifying errors in English-centric recall and translation. Introduces two vector interventions to improve paths for factual consistency.

Result: Interventions increased recall accuracy by over 35% for the lowest-performing language.

Conclusion: Mechanistic insights can enhance multilingual capabilities in LLMs, demonstrating practical improvements through targeted interventions.

Abstract: Multilingual large language models (LLMs) often exhibit factual
inconsistencies across languages, with significantly better performance in
factual recall tasks in English than in other languages. The causes of these
failures, however, remain poorly understood. Using mechanistic analysis
techniques, we uncover the underlying pipeline that LLMs employ, which involves
using the English-centric factual recall mechanism to process multilingual
queries and then translating English answers back into the target language. We
identify two primary sources of error: insufficient engagement of the reliable
English-centric mechanism for factual recall, and incorrect translation from
English back into the target language for the final answer. To address these
vulnerabilities, we introduce two vector interventions, both independent of
languages and datasets, to redirect the model toward better internal paths for
higher factual consistency. Our interventions combined increase the recall
accuracy by over 35 percent for the lowest-performing language. Our findings
demonstrate how mechanistic insights can be used to unlock latent multilingual
capabilities in LLMs.

</details>


### [195] [CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models](https://arxiv.org/pdf/2505.20767)
*Xiaqiang Tang, Jian Li, Keyu Hu, Du Nan, Xiaolong Li, Xi Zhang, Weigao Sun, Sihong Xie*

Main category: cs.CL

TL;DR: The paper introduces a framework to assess faithfulness hallucination in LLMs, focusing on cognitive statements, and creates a benchmark dataset (CogniBench-L) for training detection models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack standards for evaluating cognitive statements in LLMs, making consistency assessment and optimization difficult.

Method: Inspired by legislative evidence assessment, the authors design a framework to evaluate faithfulness levels of cognitive statements and create an annotation pipeline for automated benchmark generation.

Result: The CogniBench-L dataset is created, revealing insightful statistics and enabling training of accurate hallucination detection models.

Conclusion: The framework and dataset (CogniBench-L) are released to improve cognitive hallucination detection in LLMs.

Abstract: Faithfulness hallucination are claims generated by a Large Language Model
(LLM) not supported by contexts provided to the LLM. Lacking assessment
standard, existing benchmarks only contain "factual statements" that rephrase
source materials without marking "cognitive statements" that make inference
from the given context, making the consistency evaluation and optimization of
cognitive statements difficult. Inspired by how an evidence is assessed in the
legislative domain, we design a rigorous framework to assess different levels
of faithfulness of cognitive statements and create a benchmark dataset where we
reveal insightful statistics. We design an annotation pipeline to create larger
benchmarks for different LLMs automatically, and the resulting larger-scale
CogniBench-L dataset can be used to train accurate cognitive hallucination
detection model. We release our model and dataset at:
https://github.com/FUTUREEEEEE/CogniBench

</details>


### [196] [CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature](https://arxiv.org/pdf/2505.20779)
*Noy Sternlicht, Tom Hope*

Main category: cs.CL

TL;DR: CHIMERA is a large-scale knowledge base (KB) of recombination examples mined from scientific literature, aiding in exploring cross-domain inspiration and training models for predicting new creative directions.


<details>
  <summary>Details</summary>
Motivation: To empirically study how scientists recombine concepts and inspire cross-domain innovation by automating the extraction of recombination examples from scientific papers.

Method: Developed a novel information extraction task, collected a manually annotated corpus, trained an LLM-based extraction model, and applied it to AI domain papers to build CHIMERA.

Result: Created a KB of over 28K recombination examples, analyzed recombination properties in AI subareas, and trained a hypothesis generation model predicting inspiring new directions.

Conclusion: CHIMERA enables large-scale exploration of recombination and supports AI-driven scientific hypothesis generation, with data and code publicly available.

Abstract: A hallmark of human innovation is the process of recombination -- creating
original ideas by integrating elements of existing mechanisms and concepts. In
this work, we automatically mine the scientific literature and build CHIMERA: a
large-scale knowledge base (KB) of recombination examples. CHIMERA can be used
to empirically explore at scale how scientists recombine concepts and take
inspiration from different areas, or to train supervised machine learning
models that learn to predict new creative cross-domain directions. To build
this KB, we present a novel information extraction task of extracting
recombination from scientific paper abstracts, collect a high-quality corpus of
hundreds of manually annotated abstracts, and use it to train an LLM-based
extraction model. The model is applied to a large corpus of papers in the AI
domain, yielding a KB of over 28K recombination examples. We analyze CHIMERA to
explore the properties of recombination in different subareas of AI. Finally,
we train a scientific hypothesis generation model using the KB, which predicts
new recombination directions that real-world researchers find inspiring. Our
data and code are available at https://github.com/noy-sternlicht/CHIMERA-KB

</details>


### [197] [RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph](https://arxiv.org/pdf/2505.20813)
*Junsik Kim, Jinwook Park, Kangil Kim*

Main category: cs.CL

TL;DR: The paper introduces RSCF, a KGE method ensuring semantic consistency in entity transformations, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Address inconsistency in embedding differences before/after transformation, preserving inductive bias.

Method: RSCF uses shared affine transformation, rooted entity transformation, and normalization. Adds relation transformation and prediction modules.

Result: Significantly outperforms state-of-the-art KGE methods in knowledge graph completion tasks.

Conclusion: RSCF enhances semantic consistency and robustness across relations and frequencies.

Abstract: In knowledge graph embedding, leveraging relation specific entity
transformation has markedly enhanced performance. However, the consistency of
embedding differences before and after transformation remains unaddressed,
risking the loss of valuable inductive bias inherent in the embeddings. This
inconsistency stems from two problems. First, transformation representations
are specified for relations in a disconnected manner, allowing dissimilar
transformations and corresponding entity embeddings for similar relations.
Second, a generalized plug-in approach as a SFBR (Semantic Filter Based on
Relations) disrupts this consistency through excessive concentration of entity
embeddings under entity-based regularization, generating indistinguishable
score distributions among relations. In this paper, we introduce a plug-in KGE
method, Relation-Semantics Consistent Filter (RSCF). Its entity transformation
has three features for enhancing semantic consistency: 1) shared affine
transformation of relation embeddings across all relations, 2) rooted entity
transformation that adds an entity embedding to its change represented by the
transformed vector, and 3) normalization of the change to prevent scale
reduction. To amplify the advantages of consistency that preserve semantics on
embeddings, RSCF adds relation transformation and prediction modules for
enhancing the semantics. In knowledge graph completion tasks with
distance-based and tensor decomposition models, RSCF significantly outperforms
state-of-the-art KGE methods, showing robustness across all relations and their
frequencies.

</details>


### [198] [FCKT: Fine-Grained Cross-Task Knowledge Transfer with Semantic Contrastive Learning for Targeted Sentiment Analysis](https://arxiv.org/pdf/2505.21040)
*Wei Chen, Zhao Zhang, Meng Yuan, Kepeng Xu, Fuzhen Zhuang*

Main category: cs.CL

TL;DR: The paper proposes FCKT, a fine-grained cross-task knowledge transfer framework for targeted sentiment analysis (TSA), improving aspect-sentiment relationship modeling and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing multi-task learning approaches for TSA rely on coarse-grained knowledge transfer, neglecting contextual cues and causing negative transfer.

Method: FCKT explicitly incorporates aspect-level information into sentiment prediction for fine-grained knowledge transfer.

Result: Experiments on three datasets show FCKT outperforms baselines and large language models.

Conclusion: FCKT effectively mitigates negative transfer and enhances TSA performance, with code available on GitHub.

Abstract: In this paper, we address the task of targeted sentiment analysis (TSA),
which involves two sub-tasks, i.e., identifying specific aspects from reviews
and determining their corresponding sentiments. Aspect extraction forms the
foundation for sentiment prediction, highlighting the critical dependency
between these two tasks for effective cross-task knowledge transfer. While most
existing studies adopt a multi-task learning paradigm to align task-specific
features in the latent space, they predominantly rely on coarse-grained
knowledge transfer. Such approaches lack fine-grained control over
aspect-sentiment relationships, often assuming uniform sentiment polarity
within related aspects. This oversimplification neglects contextual cues that
differentiate sentiments, leading to negative transfer. To overcome these
limitations, we propose FCKT, a fine-grained cross-task knowledge transfer
framework tailored for TSA. By explicitly incorporating aspect-level
information into sentiment prediction, FCKT achieves fine-grained knowledge
transfer, effectively mitigating negative transfer and enhancing task
performance. Experiments on three datasets, including comparisons with various
baselines and large language models (LLMs), demonstrate the effectiveness of
FCKT. The source code is available on https://github.com/cwei01/FCKT.

</details>


### [199] [Faithfulness-Aware Uncertainty Quantification for Fact-Checking the Output of Retrieval Augmented Generation](https://arxiv.org/pdf/2505.21072)
*Ekaterina Fadeeva, Aleksandr Rubashevskii, Roman Vashurin, Shehzaad Dhuliawala, Artem Shelmanov, Timothy Baldwin, Preslav Nakov, Mrinmaya Sachan, Maxim Panov*

Main category: cs.CL

TL;DR: FRANQ introduces a novel method for detecting hallucinations in RAG systems by distinguishing factuality from faithfulness, outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: RAG systems often produce factually incorrect outputs (hallucinations), and current methods misclassify correct statements as hallucinations if not directly supported by retrieved context.

Method: FRANQ uses Uncertainty Quantification (UQ) techniques to assess factuality based on faithfulness to retrieved context, evaluated on a new annotated QA dataset.

Result: FRANQ achieves more accurate detection of factual errors in RAG outputs compared to existing methods, validated across multiple datasets and LLMs.

Conclusion: FRANQ improves hallucination detection in RAG systems by separating factuality and faithfulness, demonstrating superior performance in QA tasks.

Abstract: Large Language Models (LLMs) enhanced with external knowledge retrieval, an
approach known as Retrieval-Augmented Generation (RAG), have shown strong
performance in open-domain question answering. However, RAG systems remain
susceptible to hallucinations: factually incorrect outputs that may arise
either from inconsistencies in the model's internal knowledge or incorrect use
of the retrieved context. Existing approaches often conflate factuality with
faithfulness to the retrieved context, misclassifying factually correct
statements as hallucinations if they are not directly supported by the
retrieval. In this paper, we introduce FRANQ (Faithfulness-based Retrieval
Augmented UNcertainty Quantification), a novel method for hallucination
detection in RAG outputs. FRANQ applies different Uncertainty Quantification
(UQ) techniques to estimate factuality based on whether a statement is faithful
to the retrieved context or not. To evaluate FRANQ and other UQ techniques for
RAG, we present a new long-form Question Answering (QA) dataset annotated for
both factuality and faithfulness, combining automated labeling with manual
validation of challenging examples. Extensive experiments on long- and
short-form QA across multiple datasets and LLMs show that FRANQ achieves more
accurate detection of factual errors in RAG-generated responses compared to
existing methods.

</details>


### [200] [LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for Black-Box Large Language Models](https://arxiv.org/pdf/2505.21082)
*Jieyong Kim, Tongyoung Kim, Soojin Yoon, Jaehyung Kim, Dongha Lee*

Main category: cs.CL

TL;DR: RPM is a framework for reasoning-level personalization in black-box LLMs, outperforming response-level methods by aligning model reasoning with user-specific logic.


<details>
  <summary>Details</summary>
Motivation: Black-box LLMs lack personalization in reasoning, producing generalized responses. RPM addresses this by tailoring reasoning processes to user preferences.

Method: RPM constructs user-specific factors from history, builds personalized reasoning paths, and retrieves aligned examples for inference.

Result: RPM consistently outperforms response-level personalization methods in diverse tasks, improving accuracy and interpretability.

Conclusion: Reasoning-level personalization, as demonstrated by RPM, is effective for enhancing black-box LLM outputs by grounding them in user-specific logic.

Abstract: Large language models (LLMs) have recently achieved impressive performance
across a wide range of natural language tasks and are now widely used in
real-world applications. Among them, black-box LLMs--served via APIs without
access to model internals--are especially dominant due to their scalability and
ease of deployment. Despite their strong capabilities, these models typically
produce generalized responses that overlook personal preferences and reasoning
styles. This has led to growing interest in black-box LLM personalization,
which aims to tailor model outputs to user-specific context without modifying
model parameters. However, existing approaches primarily focus on
response-level personalization, attempting to match final outputs without
modeling personal thought process. To address this limitation, we propose RPM,
a framework for reasoning-level personalization that aligns the model's
reasoning process with a user's personalized logic. RPM first constructs
statistical user-specific factors by extracting and grouping
response-influential features from user history. It then builds personalized
reasoning paths that reflect how these factors are used in context. In the
inference stage, RPM retrieves reasoning-aligned examples for new queries via
feature-level similarity and performs inference conditioned on the structured
factors and retrieved reasoning paths, enabling the model to follow
user-specific reasoning trajectories. This reasoning-level personalization
enhances both predictive accuracy and interpretability by grounding model
outputs in user-specific logic through structured information. Extensive
experiments across diverse tasks show that RPM consistently outperforms
response-level personalization methods, demonstrating the effectiveness of
reasoning-level personalization in black-box LLMs.

</details>


### [201] [Charting the Landscape of African NLP: Mapping Progress and Shaping the Road Ahead](https://arxiv.org/pdf/2505.21315)
*Jesujoba O. Alabi, Michael A. Hedderich, David Ifeoluwa Adelani, Dietrich Klakow*

Main category: cs.CL

TL;DR: The paper surveys 734 NLP research papers on African languages, highlighting their underrepresentation in mainstream NLP systems and the growing interest in this field. It identifies trends and suggests future directions for inclusive NLP research.


<details>
  <summary>Details</summary>
Motivation: African languages are underrepresented in NLP systems, risking a digital divide. The paper aims to document and analyze recent progress in NLP for African languages.

Method: Analyzed 734 research papers on NLP for African languages published in the past five years.

Result: Identified key trends and gaps in NLP research for African languages, showing active growth in the field.

Conclusion: Outlines promising directions for more inclusive and sustainable NLP research for African languages.

Abstract: With over 2,000 languages and potentially millions of speakers, Africa
represents one of the richest linguistic regions in the world. Yet, this
diversity is scarcely reflected in state-of-the-art natural language processing
(NLP) systems and large language models (LLMs), which predominantly support a
narrow set of high-resource languages. This exclusion not only limits the reach
and utility of modern NLP technologies but also risks widening the digital
divide across linguistic communities. Nevertheless, NLP research on African
languages is active and growing. In recent years, there has been a surge of
interest in this area, driven by several factors-including the creation of
multilingual language resources, the rise of community-led initiatives, and
increased support through funding programs. In this survey, we analyze 734
research papers on NLP for African languages published over the past five
years, offering a comprehensive overview of recent progress across core tasks.
We identify key trends shaping the field and conclude by outlining promising
directions to foster more inclusive and sustainable NLP research for African
languages.

</details>


### [202] [PEDANTIC: A Dataset for the Automatic Examination of Definiteness in Patent Claims](https://arxiv.org/pdf/2505.21342)
*Valentin Knappich, Annemarie Friedrich, Anna Hätty, Simon Razniewski*

Main category: cs.CL

TL;DR: The paper introduces PEDANTIC, a dataset for patent definiteness examination, using an automated pipeline with LLMs for annotation and evaluation.


<details>
  <summary>Details</summary>
Motivation: To address the lack of annotated datasets for patent definiteness examination, aiming to improve patent drafting and examination efficiency.

Method: Constructs PEDANTIC using an automated pipeline with LLMs to extract reasons for indefiniteness from USPTO documents, validated by human study.

Result: LLMs (Qwen 2.5 32B and 72B) perform comparably to logistic regression in definiteness prediction but correctly identify reasons.

Conclusion: PEDANTIC is a valuable resource for patent AI research, with plans to release the dataset and code publicly.

Abstract: Patent claims define the scope of protection for an invention. If there are
ambiguities in a claim, it is rejected by the patent office. In the US, this is
referred to as indefiniteness (35 U.S.C {\S} 112(b)) and is among the most
frequent reasons for patent application rejection. The development of automatic
methods for patent definiteness examination has the potential to make patent
drafting and examination more efficient, but no annotated dataset has been
published to date. We introduce PEDANTIC (Patent Definiteness Examination
Corpus), a novel dataset of 14k US patent claims from patent applications
relating to Natural Language Processing (NLP), annotated with reasons for
indefiniteness. We construct PEDANTIC using a fully automatic pipeline that
retrieves office action documents from the USPTO and uses Large Language Models
(LLMs) to extract the reasons for indefiniteness. A human validation study
confirms the pipeline's accuracy in generating high-quality annotations. To
gain insight beyond binary classification metrics, we implement an LLM-as-Judge
evaluation that compares the free-form reasoning of every model-cited reason
with every examiner-cited reason. We show that LLM agents based on Qwen 2.5 32B
and 72B struggle to outperform logistic regression baselines on definiteness
prediction, even though they often correctly identify the underlying reasons.
PEDANTIC provides a valuable resource for patent AI researchers, enabling the
development of advanced examination models. We will publicly release the
dataset and code.

</details>


### [203] [Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity](https://arxiv.org/pdf/2505.21411)
*Yehui Tang, Xiaosong Li, Fangcheng Liu, Wei Guo, Hang Zhou, Yaoyuan Wang, Kai Han, Xianzhi Yu, Jinpeng Li, Hui Zang, Fei Mi, Xiaojun Meng, Zhicheng Liu, Hanting Chen, Binfan Zheng, Can Chen, Youliang Yan, Ruiming Tang, Peifeng Qin, Xinghao Chen, Dacheng Tao, Yunhe Wang*

Main category: cs.CL

TL;DR: MoGE improves expert load balancing in MoE models, enhancing throughput and efficiency on Ascend NPUs.


<details>
  <summary>Details</summary>
Motivation: Address inefficiency in MoE models due to uneven expert activation by introducing grouped expert selection.

Method: Introduces Mixture of Grouped Experts (MoGE) to balance expert workload by grouping and equalizing activation.

Result: MoGE achieves better load balancing, higher throughput (1148-1528 tokens/s per card), and outperforms dense models.

Conclusion: MoGE is efficient for training and inference on Ascend NPUs, offering superior performance in sub-100B parameter models.

Abstract: The surgence of Mixture of Experts (MoE) in Large Language Models promises a
small price of execution cost for a much larger model parameter count and
learning capacity, because only a small fraction of parameters are activated
for each input token. However, it is commonly observed that some experts are
activated far more often than others, leading to system inefficiency when
running the experts on different devices in parallel. Therefore, we introduce
Mixture of Grouped Experts (MoGE), which groups the experts during selection
and balances the expert workload better than MoE in nature. It constrains
tokens to activate an equal number of experts within each predefined expert
group. When a model execution is distributed on multiple devices, this
architectural design ensures a balanced computational load across devices,
significantly enhancing throughput, particularly for the inference phase.
Further, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE
with 72 billion total parameters, 16 billion of which are activated for each
token. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and
800I A2 through extensive system simulation studies. Our experiments indicate
that MoGE indeed leads to better expert load balancing and more efficient
execution for both model training and inference on Ascend NPUs. The inference
performance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further
improved to 1528 tokens/s per card by speculative acceleration, outperforming
comparable 32B and 72B Dense models. Furthermore, we achieve an excellent
cost-to-performance ratio for model inference on Ascend 300I Duo. Our studies
show that Ascend NPUs are capable of training Pangu Pro MoE with massive
parallelization to make it a leading model within the sub-100B total parameter
class, outperforming prominent open-source models like GLM-Z1-32B and
Qwen3-32B.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [204] [Enhancing Vision Transformer Explainability Using Artificial Astrocytes](https://arxiv.org/pdf/2505.21513)
*Nicolas Echevarrieta-Catalan, Ana Ribas-Rodriguez, Francisco Cedron, Odelia Schwartz, Vanessa Aguiar-Pulido*

Main category: cs.CV

TL;DR: The paper introduces ViTA, a training-free method inspired by neuroscience to enhance explainability in deep neural networks, showing improved alignment with human perception.


<details>
  <summary>Details</summary>
Motivation: Machine learning models lack explainability, especially as complexity grows. Current methods are limited, prompting a need for better solutions.

Method: Proposes Vision Transformer with artificial Astrocytes (ViTA), a training-free approach to enhance reasoning in pretrained models for better explanations.

Result: ViTA improves alignment of model explanations with human perception, achieving statistically significant improvements across XAI techniques.

Conclusion: ViTA effectively enhances explainability without additional training, offering a promising direction for human-aligned AI explanations.

Abstract: Machine learning models achieve high precision, but their decision-making
processes often lack explainability. Furthermore, as model complexity
increases, explainability typically decreases. Existing efforts to improve
explainability primarily involve developing new eXplainable artificial
intelligence (XAI) techniques or incorporating explainability constraints
during training. While these approaches yield specific improvements, their
applicability remains limited. In this work, we propose the Vision Transformer
with artificial Astrocytes (ViTA). This training-free approach is inspired by
neuroscience and enhances the reasoning of a pretrained deep neural network to
generate more human-aligned explanations. We evaluated our approach employing
two well-known XAI techniques, Grad-CAM and Grad-CAM++, and compared it to a
standard Vision Transformer (ViT). Using the ClickMe dataset, we quantified the
similarity between the heatmaps produced by the XAI techniques and a
(human-aligned) ground truth. Our results consistently demonstrate that
incorporating artificial astrocytes enhances the alignment of model
explanations with human perception, leading to statistically significant
improvements across all XAI techniques and metrics utilized.

</details>


### [205] [Do DeepFake Attribution Models Generalize?](https://arxiv.org/pdf/2505.21520)
*Spiros Baxavanakis, Manos Schinas, Symeon Papadopoulos*

Main category: cs.CV

TL;DR: The paper addresses the limitations of binary DeepFake detection models by exploring multi-class attribution models, comparing their generalization and accuracy across datasets, and evaluating contrastive methods for performance improvement.


<details>
  <summary>Details</summary>
Motivation: The rise of DeepFake technology threatens online information integrity, but current binary detection models fail to distinguish between manipulation methods, limiting their practical utility.

Method: The study uses five backbone models and experiments on six DeepFake datasets to compare binary and multi-class models, assess attribution accuracy, and test contrastive methods.

Result: Binary models generalize better, but larger models, contrastive methods, and higher data quality improve attribution model performance.

Conclusion: Attribution models, though less generalized, offer practical benefits for trust and explainability, with potential for improvement through advanced techniques.

Abstract: Recent advancements in DeepFake generation, along with the proliferation of
open-source tools, have significantly lowered the barrier for creating
synthetic media. This trend poses a serious threat to the integrity and
authenticity of online information, undermining public trust in institutions
and media. State-of-the-art research on DeepFake detection has primarily
focused on binary detection models. A key limitation of these models is that
they treat all manipulation techniques as equivalent, despite the fact that
different methods introduce distinct artifacts and visual cues. Only a limited
number of studies explore DeepFake attribution models, although such models are
crucial in practical settings. By providing the specific manipulation method
employed, these models could enhance both the perceived trustworthiness and
explainability for end users. In this work, we leverage five state-of-the-art
backbone models and conduct extensive experiments across six DeepFake datasets.
First, we compare binary and multi-class models in terms of cross-dataset
generalization. Second, we examine the accuracy of attribution models in
detecting seen manipulation methods in unknown datasets, hence uncovering data
distribution shifts on the same DeepFake manipulations. Last, we assess the
effectiveness of contrastive methods in improving cross-dataset generalization
performance. Our findings indicate that while binary models demonstrate better
generalization abilities, larger models, contrastive methods, and higher data
quality can lead to performance improvements in attribution models. The code of
this work is available on GitHub.

</details>


### [206] [CIM-NET: A Video Denoising Deep Neural Network Model Optimized for Computing-in-Memory Architectures](https://arxiv.org/pdf/2505.21522)
*Shan Gao, Zhiqiang Wu, Yawen Niu, Xiaotao Li, Qingqing Xu*

Main category: cs.CV

TL;DR: A hardware-algorithm co-design framework, CIM-NET, is proposed to optimize DNN-based video denoising for CIM chips, reducing MVM operations while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Deploying DNN-based video denoising on edge devices is challenging due to real-time and energy constraints; CIM chips offer potential but require architectural optimization.

Method: Proposes CIM-NET, a CIM-aware architecture, and CIM-CONV, a pseudo-convolutional operator, to integrate slide-based processing with MVM acceleration.

Result: CIM-NET reduces MVM operations to 1/77th with minimal PSNR drop (35.11 dB vs. 35.56 dB) compared to FastDVDnet.

Conclusion: The framework enables efficient DNN inference on CIM chips, balancing performance and computational efficiency.

Abstract: While deep neural network (DNN)-based video denoising has demonstrated
significant performance, deploying state-of-the-art models on edge devices
remains challenging due to stringent real-time and energy efficiency
requirements. Computing-in-Memory (CIM) chips offer a promising solution by
integrating computation within memory cells, enabling rapid matrix-vector
multiplication (MVM). However, existing DNN models are often designed without
considering CIM architectural constraints, thus limiting their acceleration
potential during inference. To address this, we propose a hardware-algorithm
co-design framework incorporating two innovations: (1) a CIM-Aware
Architecture, CIM-NET, optimized for large receptive field operation and CIM's
crossbar-based MVM acceleration; and (2) a pseudo-convolutional operator,
CIM-CONV, used within CIM-NET to integrate slide-based processing with fully
connected transformations for high-quality feature extraction and
reconstruction. This framework significantly reduces the number of MVM
operations, improving inference speed on CIM chips while maintaining
competitive performance. Experimental results indicate that, compared to the
conventional lightweight model FastDVDnet, CIM-NET substantially reduces MVM
operations with a slight decrease in denoising performance. With a stride value
of 8, CIM-NET reduces MVM operations to 1/77th of the original, while
maintaining competitive PSNR (35.11 dB vs. 35.56 dB

</details>


### [207] [Reference-Guided Identity Preserving Face Restoration](https://arxiv.org/pdf/2505.21905)
*Mo Zhou, Keren Ye, Viraj Shah, Kangfu Mei, Mauricio Delbracio, Peyman Milanfar, Vishal M. Patel, Hossein Talebi*

Main category: cs.CV

TL;DR: A novel method enhances face identity preservation in diffusion-based image restoration by leveraging multi-level reference face information, introducing a new loss function, and adapting to multi-reference inputs.


<details>
  <summary>Details</summary>
Motivation: Existing reference-based methods underutilize reference faces, leading to suboptimal face identity preservation in restoration tasks.

Method: The approach includes Composite Context (multi-level reference fusion), Hard Example Identity Loss (improved identity learning), and a training-free multi-reference adaptation.

Result: Achieves state-of-the-art identity-preserving restoration on benchmarks like FFHQ-Ref and CelebA-Ref-Test.

Conclusion: The method significantly improves face restoration and identity preservation by better exploiting reference faces.

Abstract: Preserving face identity is a critical yet persistent challenge in
diffusion-based image restoration. While reference faces offer a path forward,
existing reference-based methods often fail to fully exploit their potential.
This paper introduces a novel approach that maximizes reference face utility
for improved face restoration and identity preservation. Our method makes three
key contributions: 1) Composite Context, a comprehensive representation that
fuses multi-level (high- and low-level) information from the reference face,
offering richer guidance than prior singular representations. 2) Hard Example
Identity Loss, a novel loss function that leverages the reference face to
address the identity learning inefficiencies found in the existing identity
loss. 3) A training-free method to adapt the model to multi-reference inputs
during inference. The proposed method demonstrably restores high-quality faces
and achieves state-of-the-art identity preserving restoration on benchmarks
such as FFHQ-Ref and CelebA-Ref-Test, consistently outperforming previous work.

</details>


### [208] [Learning Shared Representations from Unpaired Data](https://arxiv.org/pdf/2505.21524)
*Amitai Yacobi, Nir Ben-Ari, Ronen Talmon, Uri Shaham*

Main category: cs.CV

TL;DR: The paper shows that shared representations can be learned mostly from unpaired data, using spectral embeddings of random walk matrices, achieving strong performance in cross-modal tasks.


<details>
  <summary>Details</summary>
Motivation: Current methods for shared representations rely on paired data, which is scarce. This work explores learning from unpaired data, which is more abundant.

Method: Uses spectral embeddings of random walk matrices from unimodal representations to learn shared representations from unpaired data.

Result: Demonstrates effectiveness in retrieval, generation, arithmetics, zero-shot, and cross-domain classification tasks.

Conclusion: This is the first work to achieve strong cross-modal performance almost exclusively with unpaired data, suggesting a universal embedding approach.

Abstract: Learning shared representations is a primary area of multimodal
representation learning. The current approaches to achieve a shared embedding
space rely heavily on paired samples from each modality, which are
significantly harder to obtain than unpaired ones. In this work, we demonstrate
that shared representations can be learned almost exclusively from unpaired
data. Our arguments are grounded in the spectral embeddings of the random walk
matrices constructed independently from each unimodal representation. Empirical
results in computer vision and natural language processing domains support its
potential, revealing the effectiveness of unpaired data in capturing meaningful
cross-modal relations, demonstrating high capabilities in retrieval tasks,
generation, arithmetics, zero-shot, and cross-domain classification. This work,
to the best of our knowledge, is the first to demonstrate these capabilities
almost exclusively from unpaired samples, giving rise to a cross-modal
embedding that could be viewed as universal, i.e., independent of the specific
modalities of the data. Our code IS publicly available at
https://github.com/shaham-lab/SUE.

</details>


### [209] [UniDB++: Fast Sampling of Unified Diffusion Bridge](https://arxiv.org/pdf/2505.21528)
*Mokai Pan, Kaizhen Zhu, Yuexin Ma, Yanwei Fu, Jingyi Yu, Jingya Wang, Ye Shi*

Main category: cs.CV

TL;DR: UniDB++ improves the UniDB framework by introducing a training-free sampling algorithm with exact closed-form solutions for reverse-time SDEs, enabling faster and higher-quality image generation.


<details>
  <summary>Details</summary>
Motivation: UniDB's reliance on iterative Euler sampling leads to slow and computationally expensive inference, and existing acceleration techniques fail to address its unique challenges.

Method: UniDB++ derives exact closed-form solutions for UniDB's reverse-time SDEs, replaces noise prediction with data prediction, and introduces an SDE-Corrector mechanism.

Result: UniDB++ achieves up to 20× fewer sampling steps, outperforms Euler-based methods in fidelity and speed, and reduces inference time significantly.

Conclusion: UniDB++ bridges the gap between theoretical generality and practical efficiency in SOC-driven diffusion bridge models.

Abstract: Diffusion Bridges enable transitions between arbitrary distributions, with
the Unified Diffusion Bridge (UniDB) framework achieving high-fidelity image
generation via a Stochastic Optimal Control (SOC) formulation. However, UniDB's
reliance on iterative Euler sampling methods results in slow, computationally
expensive inference, while existing acceleration techniques for diffusion or
diffusion bridge models fail to address its unique challenges: missing terminal
mean constraints and SOC-specific penalty coefficients in its SDEs. We present
UniDB++, a training-free sampling algorithm that significantly improves upon
these limitations. The method's key advancement comes from deriving exact
closed-form solutions for UniDB's reverse-time SDEs, effectively reducing the
error accumulation inherent in Euler approximations and enabling high-quality
generation with up to 20$\times$ fewer sampling steps. This method is further
complemented by replacing conventional noise prediction with a more stable data
prediction model, along with an SDE-Corrector mechanism that maintains
perceptual quality for low-step regimes (5-10 steps). Additionally, we
demonstrate that UniDB++ aligns with existing diffusion bridge acceleration
methods by evaluating their update rules, and UniDB++ can recover DBIMs as
special cases under some theoretical conditions. Experiments demonstrate
UniDB++'s state-of-the-art performance in image restoration tasks,
outperforming Euler-based methods in fidelity and speed while reducing
inference time significantly. This work bridges the gap between theoretical
generality and practical efficiency in SOC-driven diffusion bridge models. Our
code is available at https://github.com/2769433owo/UniDB-plusplus.

</details>


### [210] [How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control](https://arxiv.org/pdf/2505.21531)
*Kunhang Li, Jason Naradowsky, Yansong Feng, Yusuke Miyao*

Main category: cs.CV

TL;DR: LLMs excel in high-level human motion planning but struggle with precise body part positioning and multi-step movements involving complex body parts. They show promise in creative and culturally-specific motion conceptualization.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' capability in understanding and generating human motion knowledge through 3D avatar control, focusing on both high-level planning and low-level execution.

Method: LLMs generate high-level movement plans and specify body part positions, which are interpolated into avatar animations. Evaluations include human assessments and automatic comparisons with oracle positions.

Result: LLMs perform well in high-level movement interpretation but struggle with precise positioning and multi-step movements. They approximate general spatial descriptions but fail with precise spatial-temporal parameters.

Conclusion: LLMs have potential for creative and culturally-specific motion tasks but need improvement in precise execution for practical avatar control.

Abstract: We explore Large Language Models (LLMs)' human motion knowledge through 3D
avatar control. Given a motion instruction, we prompt LLMs to first generate a
high-level movement plan with consecutive steps (High-level Planning), then
specify body part positions in each step (Low-level Planning), which we
linearly interpolate into avatar animations as a clear verification lens for
human evaluators. Through carefully designed 20 representative motion
instructions with full coverage of basic movement primitives and balanced body
part usage, we conduct comprehensive evaluations including human assessment of
both generated animations and high-level movement plans, as well as automatic
comparison with oracle positions in low-level planning. We find that LLMs are
strong at interpreting the high-level body movements but struggle with precise
body part positioning. While breaking down motion queries into atomic
components improves planning performance, LLMs have difficulty with multi-step
movements involving high-degree-of-freedom body parts. Furthermore, LLMs
provide reasonable approximation for general spatial descriptions, but fail to
handle precise spatial specifications in text, and the precise spatial-temporal
parameters needed for avatar control. Notably, LLMs show promise in
conceptualizing creative motions and distinguishing culturally-specific motion
patterns.

</details>


### [211] [EvidenceMoE: A Physics-Guided Mixture-of-Experts with Evidential Critics for Advancing Fluorescence Light Detection and Ranging in Scattering Media](https://arxiv.org/pdf/2505.21532)
*Ismail Erbas, Ferhat Demirkiran, Karthik Swaminathan, Naigang Wang, Navid Ibtehaj Nizam, Stefan T. Radev, Kaoutar El Maghraoui, Xavier Intes, Vikas Pandey*

Main category: cs.CV

TL;DR: A Physics-Guided Mixture-of-Experts (MoE) framework improves FLiDAR signal analysis in scattering media, achieving high accuracy in depth and fluorescence lifetime estimation.


<details>
  <summary>Details</summary>
Motivation: Current FLiDAR methods struggle with signal complexity in scattering media, limiting depth and fluorescence lifetime estimation.

Method: Proposes a Physics-Guided MoE framework with EvidenceMoE, integrating physics-informed expert models and Evidence-Based Dirichlet Critics for adaptive fusion.

Result: Achieves NRMSE of 0.030 for depth and 0.074 for fluorescence lifetime in simulated FLiDAR data.

Conclusion: The framework effectively addresses FLiDAR challenges in scattering media, enhancing accuracy for medical and other applications.

Abstract: Fluorescence LiDAR (FLiDAR), a Light Detection and Ranging (LiDAR) technology
employed for distance and depth estimation across medical, automotive, and
other fields, encounters significant computational challenges in scattering
media. The complex nature of the acquired FLiDAR signal, particularly in such
environments, makes isolating photon time-of-flight (related to target depth)
and intrinsic fluorescence lifetime exceptionally difficult, thus limiting the
effectiveness of current analytical and computational methodologies. To
overcome this limitation, we present a Physics-Guided Mixture-of-Experts (MoE)
framework tailored for specialized modeling of diverse temporal components. In
contrast to the conventional MoE approaches our expert models are informed by
underlying physics, such as the radiative transport equation governing photon
propagation in scattering media. Central to our approach is EvidenceMoE, which
integrates Evidence-Based Dirichlet Critics (EDCs). These critic models assess
the reliability of each expert's output by providing per-expert quality scores
and corrective feedback. A Decider Network then leverages this information to
fuse expert predictions into a robust final estimate adaptively. We validate
our method using realistically simulated Fluorescence LiDAR (FLiDAR) data for
non-invasive cancer cell depth detection generated from photon transport models
in tissue. Our framework demonstrates strong performance, achieving a
normalized root mean squared error (NRMSE) of 0.030 for depth estimation and
0.074 for fluorescence lifetime.

</details>


### [212] [Self-Organizing Visual Prototypes for Non-Parametric Representation Learning](https://arxiv.org/pdf/2505.21533)
*Thalles Silva, Helio Pedrini, Adín Ramírez Rivera*

Main category: cs.CV

TL;DR: SOP introduces a novel unsupervised visual feature learning method using multiple support embeddings (SEs) per prototype, outperforming single-prototype SSL methods.


<details>
  <summary>Details</summary>
Motivation: Existing SSL methods use a single prototype per cluster, limiting feature representation. SOP aims to improve this by using multiple SEs for richer characterization.

Method: SOP employs non-parametric adaptations of loss functions and introduces SOP-MIM, where masked representations are reconstructed using multiple SEs.

Result: SOP achieves state-of-the-art performance on retrieval benchmarks and shows scalability with complex encoders.

Conclusion: SOP enhances SSL by leveraging multiple SEs per prototype, offering better feature learning and performance.

Abstract: We present Self-Organizing Visual Prototypes (SOP), a new training technique
for unsupervised visual feature learning. Unlike existing prototypical
self-supervised learning (SSL) methods that rely on a single prototype to
encode all relevant features of a hidden cluster in the data, we propose the
SOP strategy. In this strategy, a prototype is represented by many semantically
similar representations, or support embeddings (SEs), each containing a
complementary set of features that together better characterize their region in
space and maximize training performance. We reaffirm the feasibility of
non-parametric SSL by introducing novel non-parametric adaptations of two loss
functions that implement the SOP strategy. Notably, we introduce the SOP Masked
Image Modeling (SOP-MIM) task, where masked representations are reconstructed
from the perspective of multiple non-parametric local SEs. We comprehensively
evaluate the representations learned using the SOP strategy on a range of
benchmarks, including retrieval, linear evaluation, fine-tuning, and object
detection. Our pre-trained encoders achieve state-of-the-art performance on
many retrieval benchmarks and demonstrate increasing performance gains with
more complex encoders.

</details>


### [213] [Is Attention Required for Transformer Inference? Explore Function-preserving Attention Replacement](https://arxiv.org/pdf/2505.21535)
*Yuxin Ren, Maxwell D Collins, Miao Hu, Huanrui Yang*

Main category: cs.CV

TL;DR: FAR replaces attention blocks in transformers with LSTM modules, maintaining accuracy while improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Transformers' attention mechanisms are inefficient for inference on edge devices due to redundancy.

Method: FAR uses LSTM-based modules, block-wise distillation, and structural pruning to replace attention blocks.

Result: FAR matches original model accuracy on ImageNet and downstream tasks with fewer parameters and lower latency.

Conclusion: FAR successfully preserves transformer performance while enhancing inference efficiency.

Abstract: While transformers excel across vision and language pretraining tasks, their
reliance on attention mechanisms poses challenges for inference efficiency,
especially on edge and embedded accelerators with limited parallelism and
memory bandwidth. Hinted by the observed redundancy of attention at inference
time, we hypothesize that though the model learns complicated token dependency
through pretraining, the inference-time sequence-to-sequence mapping in each
attention layer is actually ''simple'' enough to be represented with a much
cheaper function. In this work, we explore FAR, a Function-preserving Attention
Replacement framework that replaces all attention blocks in pretrained
transformers with learnable sequence-to-sequence modules, exemplified by an
LSTM. FAR optimize a multi-head LSTM architecture with a block-wise
distillation objective and a global structural pruning framework to achieve a
family of efficient LSTM-based models from pretrained transformers. We validate
FAR on the DeiT vision transformer family and demonstrate that it matches the
accuracy of the original models on ImageNet and multiple downstream tasks with
reduced parameters and latency. Further analysis shows that FAR preserves the
semantic token relationships and the token-to-token correlation learned in the
transformer's attention module.

</details>


### [214] [Caption This, Reason That: VLMs Caught in the Middle](https://arxiv.org/pdf/2505.21538)
*Zihan Weng, Lucas Gomez, Taylor Whittington Webb, Pouya Bashivan*

Main category: cs.CV

TL;DR: The paper analyzes the cognitive limitations of Vision-Language Models (VLMs) using methodologies from cognitive science, identifying gaps in tasks like spatial understanding and selective attention. It suggests improvements through vision-text decoupling and targeted fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To understand and address the limitations of VLMs in specific visual tasks (e.g., counting, relational reasoning) compared to human capabilities.

Method: Employed cognitive science methodologies to evaluate VLMs (including GPT-4o) on tasks targeting Perception, Attention, and Memory. Used vision-text decoupling and fine-tuning on composite visual reasoning tasks.

Result: VLMs show gaps in spatial understanding and selective attention but improve with text-based reasoning. Fine-tuning smaller VLMs enhances core cognitive abilities, though not significantly on out-of-distribution benchmarks.

Conclusion: The study highlights VLM cognitive bottlenecks and proposes simple solutions like fine-tuning and improved Chain-of-Thought abilities for better simultaneous perception and reasoning.

Abstract: Vision-Language Models (VLMs) have shown remarkable progress in visual
understanding in recent years. Yet, they still lag behind human capabilities in
specific visual tasks such as counting or relational reasoning. To understand
the underlying limitations, we adopt methodologies from cognitive science,
analyzing VLM performance along core cognitive axes: Perception, Attention, and
Memory. Using a suite of tasks targeting these abilities, we evaluate
state-of-the-art VLMs, including GPT-4o. Our analysis reveals distinct
cognitive profiles: while advanced models approach ceiling performance on some
tasks (e.g. category identification), a significant gap persists, particularly
in tasks requiring spatial understanding or selective attention. Investigating
the source of these failures and potential methods for improvement, we employ a
vision-text decoupling analysis, finding that models struggling with direct
visual reasoning show marked improvement when reasoning over their own
generated text captions. These experiments reveal a strong need for improved
VLM Chain-of-Thought (CoT) abilities, even in models that consistently exceed
human performance. Furthermore, we demonstrate the potential of targeted
fine-tuning on composite visual reasoning tasks and show that fine-tuning
smaller VLMs substantially improves core cognitive abilities. While this
improvement does not translate to large enhancements on challenging,
out-of-distribution benchmarks, we show broadly that VLM performance on our
datasets strongly correlates with performance on these other benchmarks. Our
work provides a detailed analysis of VLM cognitive strengths and weaknesses and
identifies key bottlenecks in simultaneous perception and reasoning while also
providing an effective and simple solution.

</details>


### [215] [Equivariant Flow Matching for Point Cloud Assembly](https://arxiv.org/pdf/2505.21539)
*Ziming Wang, Nan Xue, Rebecka Jörnsten*

Main category: cs.CV

TL;DR: A novel equivariant solver (Eda) for point cloud assembly uses flow matching to align pieces efficiently, even non-overlapping ones.


<details>
  <summary>Details</summary>
Motivation: To reconstruct complete 3D shapes by aligning multiple point cloud pieces, addressing challenges like non-overlapping inputs.

Method: Proposes Eda, an equivariant diffusion assembly model, learning vector fields for flow matching and ensuring data-efficient training via an equivariant path.

Result: Eda performs competitively on practical datasets and handles non-overlapping input pieces effectively.

Conclusion: Eda is a robust solution for point cloud assembly, demonstrating efficiency and versatility in challenging scenarios.

Abstract: The goal of point cloud assembly is to reconstruct a complete 3D shape by
aligning multiple point cloud pieces. This work presents a novel equivariant
solver for assembly tasks based on flow matching models. We first theoretically
show that the key to learning equivariant distributions via flow matching is to
learn related vector fields. Based on this result, we propose an assembly
model, called equivariant diffusion assembly (Eda), which learns related vector
fields conditioned on the input pieces. We further construct an equivariant
path for Eda, which guarantees high data efficiency of the training process.
Our numerical results show that Eda is highly competitive on practical
datasets, and it can even handle the challenging situation where the input
pieces are non-overlapped.

</details>


### [216] [DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers](https://arxiv.org/pdf/2505.21541)
*Zitong Wang, Hang Zhao, Qianyu Zhou, Xuequan Lu, Xiangtai Li, Yiren Song*

Main category: cs.CV

TL;DR: The paper introduces DiffDecompose, a diffusion Transformer-based framework for decomposing semi-transparent/transparent layers in images, supported by the new AlphaBlend dataset.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with semi-transparent/transparent layer decomposition due to mask dependencies and lack of datasets.

Method: Proposes DiffDecompose, a framework using diffusion models and Transformers, leveraging the AlphaBlend dataset for training.

Result: Extensive experiments show DiffDecompose's effectiveness on AlphaBlend and public datasets.

Conclusion: DiffDecompose advances layer decomposition tasks, with code and dataset to be released.

Abstract: Diffusion models have recently motivated great success in many generation
tasks like object removal. Nevertheless, existing image decomposition methods
struggle to disentangle semi-transparent or transparent layer occlusions due to
mask prior dependencies, static object assumptions, and the lack of datasets.
In this paper, we delve into a novel task: Layer-Wise Decomposition of
Alpha-Composited Images, aiming to recover constituent layers from single
overlapped images under the condition of semi-transparent/transparent alpha
layer non-linear occlusion. To address challenges in layer ambiguity,
generalization, and data scarcity, we first introduce AlphaBlend, the first
large-scale and high-quality dataset for transparent and semi-transparent layer
decomposition, supporting six real-world subtasks (e.g., translucent flare
removal, semi-transparent cell decomposition, glassware decomposition).
Building on this dataset, we present DiffDecompose, a diffusion
Transformer-based framework that learns the posterior over possible layer
decompositions conditioned on the input image, semantic prompts, and blending
type. Rather than regressing alpha mattes directly, DiffDecompose performs
In-Context Decomposition, enabling the model to predict one or multiple layers
without per-layer supervision, and introduces Layer Position Encoding Cloning
to maintain pixel-level correspondence across layers. Extensive experiments on
the proposed AlphaBlend dataset and public LOGO dataset verify the
effectiveness of DiffDecompose. The code and dataset will be available upon
paper acceptance. Our code will be available at:
https://github.com/Wangzt1121/DiffDecompose.

</details>


### [217] [Vision Meets Language: A RAG-Augmented YOLOv8 Framework for Coffee Disease Diagnosis and Farmer Assistance](https://arxiv.org/pdf/2505.21544)
*Semanto Mondal*

Main category: cs.CV

TL;DR: A hybrid AI approach combining object detection, LLMs, and RAG is proposed for precision agriculture to detect crop diseases and suggest remedies, aiming to reduce pesticide use and promote sustainability.


<details>
  <summary>Details</summary>
Motivation: Traditional farming practices are inefficient and environmentally harmful, prompting the need for advanced solutions like precision agriculture to optimize resource use and address environmental challenges.

Method: The study introduces a novel framework integrating YOLOv8 for disease detection, NLP for context-aware diagnoses, and RAG to mitigate LLM hallucinations, providing real-time disease detection and adaptive treatment plans.

Result: The system offers an easy-to-use interface for farmers to detect coffee leaf diseases and receive remediation suggestions, aiming to reduce pesticide use and promote eco-friendly practices.

Conclusion: The project highlights the potential of RAG-integrated object detection systems for scalable, reliable, and user-friendly agricultural applications, fostering sustainability.

Abstract: As a social being, we have an intimate bond with the environment. A plethora
of things in human life, such as lifestyle, health, and food are dependent on
the environment and agriculture. It comes under our responsibility to support
the environment as well as agriculture. However, traditional farming practices
often result in inefficient resource use and environmental challenges. To
address these issues, precision agriculture has emerged as a promising approach
that leverages advanced technologies to optimise agricultural processes. In
this work, a hybrid approach is proposed that combines the three different
potential fields of model AI: object detection, large language model (LLM), and
Retrieval-Augmented Generation (RAG). In this novel framework, we have tried to
combine the vision and language models to work together to identify potential
diseases in the tree leaf. This study introduces a novel AI-based precision
agriculture system that uses Retrieval Augmented Generation (RAG) to provide
context-aware diagnoses and natural language processing (NLP) and YOLOv8 for
crop disease detection. The system aims to tackle major issues with large
language models (LLMs), especially hallucinations and allows for adaptive
treatment plans and real-time disease detection. The system provides an
easy-to-use interface to the farmers, which they can use to detect the
different diseases related to coffee leaves by just submitting the image of the
affected leaf the model will detect the diseases as well as suggest potential
remediation methodologies which aim to lower the use of pesticides, preserving
livelihoods, and encouraging environmentally friendly methods. With an emphasis
on scalability, dependability, and user-friendliness, the project intends to
improve RAG-integrated object detection systems for wider agricultural
applications in the future.

</details>


### [218] [Corruption-Aware Training of Latent Video Diffusion Models for Robust Text-to-Video Generation](https://arxiv.org/pdf/2505.21545)
*Chika Maduabuchi, Hao Chen, Yujin Han, Jindong Wang*

Main category: cs.CV

TL;DR: CAT-LVDM improves robustness of Latent Video Diffusion Models (LVDMs) by introducing corruption-aware training with structured noise injection (BCNI and SACN), reducing semantic drift and improving temporal coherence.


<details>
  <summary>Details</summary>
Motivation: LVDMs suffer from semantic drift and temporal incoherence due to imperfect conditioning on noisy datasets.

Method: Proposes Batch-Centered Noise Injection (BCNI) for semantic consistency and Spectrum-Aware Contextual Noise (SACN) for low-frequency smoothness.

Result: BCNI reduces FVD by 31.9% on caption-rich datasets; SACN improves UCF-101 by 12.3%. Theoretical analysis supports tighter bounds.

Conclusion: CAT-LVDM provides a scalable, principled approach for robust video diffusion under multimodal noise.

Abstract: Latent Video Diffusion Models (LVDMs) achieve high-quality generation but are
sensitive to imperfect conditioning, which causes semantic drift and temporal
incoherence on noisy, web-scale video-text datasets. We introduce CAT-LVDM, the
first corruption-aware training framework for LVDMs that improves robustness
through structured, data-aligned noise injection. Our method includes
Batch-Centered Noise Injection (BCNI), which perturbs embeddings along
intra-batch semantic directions to preserve temporal consistency. BCNI is
especially effective on caption-rich datasets like WebVid-2M, MSR-VTT, and
MSVD. We also propose Spectrum-Aware Contextual Noise (SACN), which injects
noise along dominant spectral directions to improve low-frequency smoothness,
showing strong results on UCF-101. On average, BCNI reduces FVD by 31.9% across
WebVid-2M, MSR-VTT, and MSVD, while SACN yields a 12.3% improvement on UCF-101.
Ablation studies confirm the benefit of low-rank, data-aligned noise. Our
theoretical analysis further explains how such perturbations tighten entropy,
Wasserstein, score-drift, mixing-time, and generalization bounds. CAT-LVDM
establishes a principled, scalable training approach for robust video diffusion
under multimodal noise. Code and models: https://github.com/chikap421/catlvdm

</details>


### [219] [Image Tokens Matter: Mitigating Hallucination in Discrete Tokenizer-based Large Vision-Language Models via Latent Editing](https://arxiv.org/pdf/2505.21547)
*Weixing Wang, Zifeng Ding, Jindong Gu, Rui Cao, Christoph Meinel, Gerard de Melo, Haojin Yang*

Main category: cs.CV

TL;DR: LVLMs hallucinate objects due to visual priors; a GNN-based method mitigates this by suppressing absent tokens.


<details>
  <summary>Details</summary>
Motivation: Addressing object hallucinations in LVLMs caused by visual priors from token co-occurrence.

Method: Construct a co-occurrence graph of image tokens, use GNN with contrastive learning and clustering, then modify latent embeddings to suppress absent tokens.

Result: Hallucinations are reduced while maintaining model expressivity.

Conclusion: Proposed method effectively mitigates hallucinations in LVLMs by targeting visually absent tokens.

Abstract: Large Vision-Language Models (LVLMs) with discrete image tokenizers unify
multimodal representations by encoding visual inputs into a finite set of
tokens. Despite their effectiveness, we find that these models still
hallucinate non-existent objects. We hypothesize that this may be due to visual
priors induced during training: When certain image tokens frequently co-occur
in the same spatial regions and represent shared objects, they become strongly
associated with the verbalizations of those objects. As a result, the model may
hallucinate by evoking visually absent tokens that often co-occur with present
ones. To test this assumption, we construct a co-occurrence graph of image
tokens using a segmentation dataset and employ a Graph Neural Network (GNN)
with contrastive learning followed by a clustering method to group tokens that
frequently co-occur in similar visual contexts. We find that hallucinations
predominantly correspond to clusters whose tokens dominate the input, and more
specifically, that the visually absent tokens in those clusters show much
higher correlation with hallucinated objects compared to tokens present in the
image. Based on this observation, we propose a hallucination mitigation method
that suppresses the influence of visually absent tokens by modifying latent
image embeddings during generation. Experiments show our method reduces
hallucinations while preserving expressivity. Code is available at
https://github.com/weixingW/CGC-VTD/tree/main

</details>


### [220] [Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation](https://arxiv.org/pdf/2505.21549)
*Daniel Csizmadia, Andrei Codreanu, Victor Sim, Vighnesh Prabeau, Michael Lu, Kevin Zhu, Sean O'Brien, Vasu Sharma*

Main category: cs.CV

TL;DR: DCLIP is a fine-tuned CLIP variant improving image-text retrieval without losing zero-shot classification performance, using a teacher-student distillation framework.


<details>
  <summary>Details</summary>
Motivation: CLIP models struggle with fine-grained cross-modal understanding due to fixed resolutions and limited context. DCLIP aims to enhance retrieval while preserving generalization.

Method: Uses a meta teacher-student distillation framework with bidirectional cross-attention between image regions and text, trained with a hybrid loss combining contrastive learning and cosine similarity.

Result: Improves retrieval metrics (Recall@K, MAP) and retains ~94% of CLIP's zero-shot performance, despite training on a smaller dataset.

Conclusion: DCLIP balances task specialization and generalization, offering a resource-efficient solution for vision-language tasks.

Abstract: We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that
enhances multimodal image-text retrieval while preserving the original model's
strong zero-shot classification capabilities. CLIP models are typically
constrained by fixed image resolutions and limited context, which can hinder
their effectiveness in retrieval tasks that require fine-grained cross-modal
understanding. DCLIP addresses these challenges through a meta teacher-student
distillation framework, where a cross-modal transformer teacher is fine-tuned
to produce enriched embeddings via bidirectional cross-attention between
YOLO-extracted image regions and corresponding textual spans. These
semantically and spatially aligned global representations guide the training of
a lightweight student model using a hybrid loss that combines contrastive
learning and cosine similarity objectives. Despite being trained on only
~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a
fraction of CLIP's original dataset-DCLIP significantly improves image-text
retrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIP's
zero-shot classification performance. These results demonstrate that DCLIP
effectively mitigates the trade-off between task specialization and
generalization, offering a resource-efficient, domain-adaptive, and
detail-sensitive solution for advanced vision-language tasks. Code available at
https://anonymous.4open.science/r/DCLIP-B772/README.md.

</details>


### [221] [Benign-to-Toxic Jailbreaking: Inducing Harmful Responses from Harmless Prompts](https://arxiv.org/pdf/2505.21556)
*Hee-Seon Kim, Minbeom Kim, Wonjun Lee, Kihyun Kim, Changick Kim*

Main category: cs.CV

TL;DR: The paper introduces a new jailbreak paradigm, Benign-to-Toxic (B2T), which outperforms the Toxic-Continuation method by inducing toxic outputs from benign inputs, revealing vulnerabilities in multimodal alignment.


<details>
  <summary>Details</summary>
Motivation: Existing Toxic-Continuation jailbreaks are ineffective without explicit toxic signals, prompting the need for a more robust method to exploit safety misalignment in LVLMs.

Method: The B2T approach optimizes adversarial images to generate toxic outputs from benign conditioning, relying solely on the image to bypass safety mechanisms.

Result: B2T outperforms prior methods, works in black-box settings, and complements text-based jailbreaks, demonstrating its effectiveness.

Conclusion: The study highlights a new vulnerability in multimodal alignment and opens a novel direction for jailbreak research.

Abstract: Optimization-based jailbreaks typically adopt the Toxic-Continuation setting
in large vision-language models (LVLMs), following the standard next-token
prediction objective. In this setting, an adversarial image is optimized to
make the model predict the next token of a toxic prompt. However, we find that
the Toxic-Continuation paradigm is effective at continuing already-toxic
inputs, but struggles to induce safety misalignment when explicit toxic signals
are absent. We propose a new paradigm: Benign-to-Toxic (B2T) jailbreak. Unlike
prior work, we optimize adversarial images to induce toxic outputs from benign
conditioning. Since benign conditioning contains no safety violations, the
image alone must break the model's safety mechanisms. Our method outperforms
prior approaches, transfers in black-box settings, and complements text-based
jailbreaks. These results reveal an underexplored vulnerability in multimodal
alignment and introduce a fundamentally new direction for jailbreak approaches.

</details>


### [222] [Analytical Calculation of Weights Convolutional Neural Network](https://arxiv.org/pdf/2505.21557)
*Polad Geidarov*

Main category: cs.CV

TL;DR: An algorithm for analytically calculating CNN weights and thresholds without training, using just 10 MNIST images, achieves over 50% recognition accuracy on 1000 test images in fractions of a second.


<details>
  <summary>Details</summary>
Motivation: To explore if CNNs can be constructed and applied for classification tasks without traditional training, using purely analytical methods.

Method: Analytical computation of CNN weights and thresholds, including layer channels, using 10 MNIST images (one per digit). Implemented in C++ Builder and tested on MNIST.

Result: The analytically computed CNN recognized over 50% of 1000 handwritten digit images without training, with fast inference.

Conclusion: CNNs can be built and used for classification without training, relying solely on analytical weight computation.

Abstract: This paper presents an algorithm for analytically calculating the weights and
thresholds of convolutional neural networks (CNNs) without using standard
training procedures. The algorithm enables the determination of CNN parameters
based on just 10 selected images from the MNIST dataset, each representing a
digit from 0 to 9. As part of the method, the number of channels in CNN layers
is also derived analytically. A software module was implemented in C++ Builder,
and a series of experiments were conducted using the MNIST dataset. Results
demonstrate that the analytically computed CNN can recognize over half of 1000
handwritten digit images without any training, achieving inference in fractions
of a second. These findings suggest that CNNs can be constructed and applied
directly for classification tasks without training, using purely analytical
computation of weights.

</details>


### [223] [A Novel Convolutional Neural Network-Based Framework for Complex Multiclass Brassica Seed Classification](https://arxiv.org/pdf/2505.21558)
*Elhoucine Elfatimia, Recep Eryigitb, Lahcen Elfatimi*

Main category: cs.CV

TL;DR: A CNN-based framework for classifying Brassica seeds achieves 93% accuracy, aiding farmers in seed quality control and efficiency.


<details>
  <summary>Details</summary>
Motivation: Farmers lack time for on-farm research, and early seed classification can reduce costs and risks in crop production.

Method: A custom-designed CNN architecture addresses texture similarity in seed images, compared to pre-trained models.

Result: The proposed model achieved 93% accuracy in classifying ten Brassica seed types.

Conclusion: The CNN framework effectively improves seed classification, supporting better seed quality management and yield estimation.

Abstract: Agricultural research has accelerated in recent years, yet farmers often lack
the time and resources for on-farm research due to the demands of crop
production and farm operations. Seed classification offers valuable insights
into quality control, production efficiency, and impurity detection. Early
identification of seed types is critical to reducing the cost and risk
associated with field emergence, which can lead to yield losses or disruptions
in downstream processes like harvesting. Seed sampling supports growers in
monitoring and managing seed quality, improving precision in determining seed
purity levels, guiding management adjustments, and enhancing yield estimations.
This study proposes a novel convolutional neural network (CNN)-based framework
for the efficient classification of ten common Brassica seed types. The
approach addresses the inherent challenge of texture similarity in seed images
using a custom-designed CNN architecture. The model's performance was evaluated
against several pre-trained state-of-the-art architectures, with adjustments to
layer configurations for optimized classification. Experimental results using
our collected Brassica seed dataset demonstrate that the proposed model
achieved a high accuracy rate of 93 percent.

</details>


### [224] [Knowledge Distillation Approach for SOS Fusion Staging: Towards Fully Automated Skeletal Maturity Assessment](https://arxiv.org/pdf/2505.21561)
*Omid Halimi Milani, Amanda Nikho, Marouane Tliba, Lauren Mills, Ahmet Enis Cetin, Mohammed H Elnagar*

Main category: cs.CV

TL;DR: A deep learning framework automates SOS fusion staging using a dual-model architecture with knowledge distillation, achieving high accuracy without pre-processing.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and consistency of skeletal maturation assessment in orthodontics and forensic anthropology by automating SOS fusion staging.

Method: Uses a teacher-student model with knowledge distillation, a novel loss function aligning spatial logits, and gradient-based attention spatial mapping.

Result: Attains robust diagnostic accuracy, creating a clinically viable end-to-end pipeline without needing pre-processing tools.

Conclusion: The framework enhances skeletal maturation assessment efficiency and consistency in clinical settings.

Abstract: We introduce a novel deep learning framework for the automated staging of
spheno-occipital synchondrosis (SOS) fusion, a critical diagnostic marker in
both orthodontics and forensic anthropology. Our approach leverages a
dual-model architecture wherein a teacher model, trained on manually cropped
images, transfers its precise spatial understanding to a student model that
operates on full, uncropped images. This knowledge distillation is facilitated
by a newly formulated loss function that aligns spatial logits as well as
incorporates gradient-based attention spatial mapping, ensuring that the
student model internalizes the anatomically relevant features without relying
on external cropping or YOLO-based segmentation. By leveraging expert-curated
data and feedback at each step, our framework attains robust diagnostic
accuracy, culminating in a clinically viable end-to-end pipeline. This
streamlined approach obviates the need for additional pre-processing tools and
accelerates deployment, thereby enhancing both the efficiency and consistency
of skeletal maturation assessment in diverse clinical settings.

</details>


### [225] [Multi-instance Learning as Downstream Task of Self-Supervised Learning-based Pre-trained Model](https://arxiv.org/pdf/2505.21564)
*Koki Matsuishi, Tsuyoshi Okita*

Main category: cs.CV

TL;DR: Proposes a pre-trained self-supervised model for multi-instance learning in brain hematoma CT, improving accuracy and F1 scores despite high instance counts.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of learning in deep multi-instance settings with high instance counts (e.g., 256 in brain hematoma CT), where traditional methods struggle.

Method: Uses a pre-trained model with self-supervised learning as a downstream task for multi-instance learning.

Result: Achieves 5% to 13% accuracy improvement and 40% to 55% F1 measure boost for hypodensity marker classification.

Conclusion: The proposed method effectively mitigates learning difficulties in high-instance multi-instance settings, enhancing performance.

Abstract: In deep multi-instance learning, the number of applicable instances depends
on the data set. In histopathology images, deep learning multi-instance
learners usually assume there are hundreds to thousands instances in a bag.
However, when the number of instances in a bag increases to 256 in brain
hematoma CT, learning becomes extremely difficult. In this paper, we address
this drawback. To overcome this problem, we propose using a pre-trained model
with self-supervised learning for the multi-instance learner as a downstream
task. With this method, even when the original target task suffers from the
spurious correlation problem, we show improvements of 5% to 13% in accuracy and
40% to 55% in the F1 measure for the hypodensity marker classification of brain
hematoma CT.

</details>


### [226] [Diffusion Model-based Activity Completion for AI Motion Capture from Videos](https://arxiv.org/pdf/2505.21566)
*Gao Huayu, Huang Tengjiu, Ye Xiaolong, Tsuyoshi Okita*

Main category: cs.CV

TL;DR: AI motion capture for virtual humans using diffusion models to generate smooth transitions between action fragments, outperforming existing methods in most metrics.


<details>
  <summary>Details</summary>
Motivation: Current AI motion capture is limited to predefined actions; the goal is to enable flexible actions for virtual humans by addressing missing transitions in training data.

Method: Proposes a diffusion-model-based action completion technique with a gate module and position-time embedding module to generate smooth motion sequences.

Result: MDC-Net outperforms in ADE, FDE, and MMADE, has a smaller model size (16.84M), and generates more natural motion sequences. Also introduces sensor data extraction from motion sequences.

Conclusion: The proposed method effectively addresses the limitation of predefined actions in AI motion capture, enabling flexible and natural movements for virtual humans.

Abstract: AI-based motion capture is an emerging technology that offers a
cost-effective alternative to traditional motion capture systems. However,
current AI motion capture methods rely entirely on observed video sequences,
similar to conventional motion capture. This means that all human actions must
be predefined, and movements outside the observed sequences are not possible.
To address this limitation, we aim to apply AI motion capture to virtual
humans, where flexible actions beyond the observed sequences are required. We
assume that while many action fragments exist in the training data, the
transitions between them may be missing. To bridge these gaps, we propose a
diffusion-model-based action completion technique that generates complementary
human motion sequences, ensuring smooth and continuous movements. By
introducing a gate module and a position-time embedding module, our approach
achieves competitive results on the Human3.6M dataset. Our experimental results
show that (1) MDC-Net outperforms existing methods in ADE, FDE, and MMADE but
is slightly less accurate in MMFDE, (2) MDC-Net has a smaller model size
(16.84M) compared to HumanMAC (28.40M), and (3) MDC-Net generates more natural
and coherent motion sequences. Additionally, we propose a method for extracting
sensor data, including acceleration and angular velocity, from human motion
sequences.

</details>


### [227] [EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models](https://arxiv.org/pdf/2505.21567)
*Feng Jiang, Zihao Zheng, Xiuping Cui, Maoliang Li, JIayu Chen, Xiang Chen*

Main category: cs.CV

TL;DR: EaqVLA is a framework for optimized quantization of Vision-Language-Action (VLA) models, addressing token alignment issues to reduce memory and computation costs.


<details>
  <summary>Details</summary>
Motivation: Existing VLA models face high computing/storage costs, and token alignment issues hinder effective quantization.

Method: Proposes encoding-aligned quantization, analyzing misalignment in various granularities and applying mixed precision quantization.

Result: EaqVLA achieves minimal quantization loss and significant acceleration compared to existing methods.

Conclusion: EaqVLA effectively optimizes VLA models, reducing costs while maintaining performance.

Abstract: With the development of Embodied Artificial intelligence, the end-to-end
control policy such as Vision-Language-Action (VLA) model has become the
mainstream. Existing VLA models faces expensive computing/storage cost, which
need to be optimized. Quantization is considered as the most effective method
which can not only reduce the memory cost but also achieve computation
acceleration. However, we find the token alignment of VLA models hinders the
application of existing quantization methods. To address this, we proposed an
optimized framework called EaqVLA, which apply encoding-aligned quantization to
VLA models. Specifically, we propose an complete analysis method to find the
misalignment in various granularity. Based on the analysis results, we propose
a mixed precision quantization with the awareness of encoding alignment.
Experiments shows that the porposed EaqVLA achieves better quantization
performance (with the minimal quantization loss for end-to-end action control
and xxx times acceleration) than existing quantization methods.

</details>


### [228] [Thickness-aware E(3)-Equivariant 3D Mesh Neural Networks](https://arxiv.org/pdf/2505.21572)
*Sungwon Kim, Namkyeong Lee, Yunyoung Doh, Seungmin Shin, Guimok Cho, Seung-Won Jeon, Sangkook Kim, Chanyoung Park*

Main category: cs.CV

TL;DR: T-EMNN, a thickness-aware 3D mesh neural network, improves accuracy in predicting deformations by integrating object thickness while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing mesh-based 3D analysis methods overlook object thickness, limiting accuracy. T-EMNN addresses this gap.

Method: Proposes T-EMNN, integrating thickness and using data-driven coordinates for E(3)-equivariance.

Result: Outperforms on real-world datasets, accurately predicting deformations with thickness effects.

Conclusion: T-EMNN enhances 3D analysis by capturing thickness efficiently.

Abstract: Mesh-based 3D static analysis methods have recently emerged as efficient
alternatives to traditional computational numerical solvers, significantly
reducing computational costs and runtime for various physics-based analyses.
However, these methods primarily focus on surface topology and geometry, often
overlooking the inherent thickness of real-world 3D objects, which exhibits
high correlations and similar behavior between opposing surfaces. This
limitation arises from the disconnected nature of these surfaces and the
absence of internal edge connections within the mesh. In this work, we propose
a novel framework, the Thickness-aware E(3)-Equivariant 3D Mesh Neural Network
(T-EMNN), that effectively integrates the thickness of 3D objects while
maintaining the computational efficiency of surface meshes. Additionally, we
introduce data-driven coordinates that encode spatial information while
preserving E(3)-equivariance or invariance properties, ensuring consistent and
robust analysis. Evaluations on a real-world industrial dataset demonstrate the
superior performance of T-EMNN in accurately predicting node-level 3D
deformations, effectively capturing thickness effects while maintaining
computational efficiency.

</details>


### [229] [Do We Need All the Synthetic Data? Towards Targeted Synthetic Image Augmentation via Diffusion Models](https://arxiv.org/pdf/2505.21574)
*Dang Nguyen, Jiping Li, Jinghao Zheng, Baharan Mirzasoleiman*

Main category: cs.CV

TL;DR: Augmenting only unlearned data early in training improves generalization, outperforming full-dataset augmentation by 2.8% in various scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing synthetic augmentation methods lack diversity and require large data expansions (10-30x) for in-distribution performance.

Method: Augmenting only the unlearned portion of data early in training, analyzed via a two-layer CNN, to promote homogeneous feature learning.

Result: Boosts performance by up to 2.8% on CIFAR-10, CIFAR-100, and TinyImageNet, even surpassing SOTA optimizer SAM.

Conclusion: Partial augmentation of unlearned data is more effective than full-dataset augmentation and can integrate with other strategies.

Abstract: Synthetically augmenting training datasets with diffusion models has been an
effective strategy for improving generalization of image classifiers. However,
existing techniques struggle to ensure the diversity of generation and increase
the size of the data by up to 10-30x to improve the in-distribution
performance. In this work, we show that synthetically augmenting part of the
data that is not learned early in training outperforms augmenting the entire
dataset. By analyzing a two-layer CNN, we prove that this strategy improves
generalization by promoting homogeneity in feature learning speed without
amplifying noise. Our extensive experiments show that by augmenting only
30%-40% of the data, our method boosts the performance by up to 2.8% in a
variety of scenarios, including training ResNet, ViT and DenseNet on CIFAR-10,
CIFAR-100, and TinyImageNet, with a range of optimizers including SGD and SAM.
Notably, our method applied with SGD outperforms the SOTA optimizer, SAM, on
CIFAR-100 and TinyImageNet. It can also easily stack with existing weak and
strong augmentation strategies to further boost the performance.

</details>


### [230] [Learnable Burst-Encodable Time-of-Flight Imaging for High-Fidelity Long-Distance Depth Sensing](https://arxiv.org/pdf/2505.22025)
*Manchao Bao, Shengjiang Fang, Tao Yue, Xuemei Hu*

Main category: cs.CV

TL;DR: BE-ToF is a new ToF imaging method for long-distance depth sensing, avoiding phase wrapping and improving SNR through burst-mode light pulses and a learnable framework.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of dToF (hardware demands) and iToF (phase wrapping, low SNR) for long-distance depth imaging.

Method: BE-ToF emits burst-mode light pulses and uses a learnable framework to optimize coding functions and depth reconstruction.

Result: Effective avoidance of phase wrapping and improved SNR, validated by simulations and real-world experiments.

Conclusion: BE-ToF is a practical solution for high-fidelity, long-distance depth imaging.

Abstract: Long-distance depth imaging holds great promise for applications such as
autonomous driving and robotics. Direct time-of-flight (dToF) imaging offers
high-precision, long-distance depth sensing, yet demands ultra-short pulse
light sources and high-resolution time-to-digital converters. In contrast,
indirect time-of-flight (iToF) imaging often suffers from phase wrapping and
low signal-to-noise ratio (SNR) as the sensing distance increases. In this
paper, we introduce a novel ToF imaging paradigm, termed Burst-Encodable
Time-of-Flight (BE-ToF), which facilitates high-fidelity, long-distance depth
imaging. Specifically, the BE-ToF system emits light pulses in burst mode and
estimates the phase delay of the reflected signal over the entire burst period,
thereby effectively avoiding the phase wrapping inherent to conventional iToF
systems. Moreover, to address the low SNR caused by light attenuation over
increasing distances, we propose an end-to-end learnable framework that jointly
optimizes the coding functions and the depth reconstruction network. A
specialized double well function and first-order difference term are
incorporated into the framework to ensure the hardware implementability of the
coding functions. The proposed approach is rigorously validated through
comprehensive simulations and real-world prototype experiments, demonstrating
its effectiveness and practical applicability.

</details>


### [231] [Do you see what I see? An Ambiguous Optical Illusion Dataset exposing limitations of Explainable AI](https://arxiv.org/pdf/2505.21589)
*Carina Newen, Luca Hinkamp, Maria Ntonti, Emmanuel Müller*

Main category: cs.CV

TL;DR: The paper introduces a novel dataset of optical illusions to study perceptual ambiguity in machine learning, focusing on gaze direction and eye cues.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of optical illusion datasets and explore their impact on machine learning models, especially in safety-critical domains.

Method: Creation of a dataset featuring intermingled animal pairs to evoke ambiguity, with systematic generation of illusions and analysis of visual concepts like gaze direction.

Result: Identifies gaze direction and eye cues as impactful features influencing model accuracy, highlighting the role of perceptual ambiguity in visual learning.

Conclusion: The dataset provides a foundation for studying bias and alignment between human and machine vision, with potential applications in bias mitigation.

Abstract: From uncertainty quantification to real-world object detection, we recognize
the importance of machine learning algorithms, particularly in safety-critical
domains such as autonomous driving or medical diagnostics. In machine learning,
ambiguous data plays an important role in various machine learning domains.
Optical illusions present a compelling area of study in this context, as they
offer insight into the limitations of both human and machine perception.
Despite this relevance, optical illusion datasets remain scarce. In this work,
we introduce a novel dataset of optical illusions featuring intermingled animal
pairs designed to evoke perceptual ambiguity. We identify generalizable visual
concepts, particularly gaze direction and eye cues, as subtle yet impactful
features that significantly influence model accuracy. By confronting models
with perceptual ambiguity, our findings underscore the importance of concepts
in visual learning and provide a foundation for studying bias and alignment
between human and machine vision. To make this dataset useful for general
purposes, we generate optical illusions systematically with different concepts
discussed in our bias mitigation section. The dataset is accessible in Kaggle
via
https://kaggle.com/datasets/693bf7c6dd2cb45c8a863f9177350c8f9849a9508e9d50526e2ffcc5559a8333.
Our source code can be found at
https://github.com/KDD-OpenSource/Ambivision.git.

</details>


### [232] [Any-to-Bokeh: One-Step Video Bokeh via Multi-Plane Image Guided Diffusion](https://arxiv.org/pdf/2505.21593)
*Yang Yang, Siming Zheng, Jinwei Chen, Boxi Wu, Xiaofei He, Deng Cai, Bo Li, Peng-Tao Jiang*

Main category: cs.CV

TL;DR: A novel one-step video bokeh framework is proposed to create temporally coherent, depth-aware bokeh effects, addressing limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing video editing models lack explicit control over focus planes and bokeh intensity, and naive extensions of image-based methods cause flickering and poor edge transitions.

Method: The framework uses a multi-plane image (MPI) representation with progressive depth sampling, conditioned on a single-step video diffusion model leveraging pre-trained 3D priors.

Result: The method achieves high-quality, controllable bokeh effects with state-of-the-art performance on benchmarks.

Conclusion: The proposed approach effectively addresses temporal consistency and depth-awareness in video bokeh, outperforming existing methods.

Abstract: Recent advances in diffusion based editing models have enabled realistic
camera simulation and image-based bokeh, but video bokeh remains largely
unexplored. Existing video editing models cannot explicitly control focus
planes or adjust bokeh intensity, limiting their applicability for controllable
optical effects. Moreover, naively extending image-based bokeh methods to video
often results in temporal flickering and unsatisfactory edge blur transitions
due to the lack of temporal modeling and generalization capability. To address
these challenges, we propose a novel one-step video bokeh framework that
converts arbitrary input videos into temporally coherent, depth-aware bokeh
effects. Our method leverages a multi-plane image (MPI) representation
constructed through a progressively widening depth sampling function, providing
explicit geometric guidance for depth-dependent blur synthesis. By conditioning
a single-step video diffusion model on MPI layers and utilizing the strong 3D
priors from pre-trained models such as Stable Video Diffusion, our approach
achieves realistic and consistent bokeh effects across diverse scenes.
Additionally, we introduce a progressive training strategy to enhance temporal
consistency, depth robustness, and detail preservation. Extensive experiments
demonstrate that our method produces high-quality, controllable bokeh effects
and achieves state-of-the-art performance on multiple evaluation benchmarks.

</details>


### [233] [PS4PRO: Pixel-to-pixel Supervision for Photorealistic Rendering and Optimization](https://arxiv.org/pdf/2505.22616)
*Yezhi Shen, Qiuchen Zhai, Fengqing Zhu*

Main category: cs.CV

TL;DR: Proposes PS4PRO, a video frame interpolation model, to augment data for neural rendering, improving 3D reconstruction quality in static and dynamic scenes.


<details>
  <summary>Details</summary>
Motivation: Neural rendering's reconstruction quality is limited by the number of input views, especially in complex or dynamic scenes.

Method: Uses video frame interpolation (PS4PRO) as data augmentation, trained on diverse video datasets to model camera movement and 3D geometry.

Result: Improves reconstruction performance in both static and dynamic scenes.

Conclusion: PS4PRO effectively augments datasets for neural rendering, enhancing reconstruction quality.

Abstract: Neural rendering methods have gained significant attention for their ability
to reconstruct 3D scenes from 2D images. The core idea is to take multiple
views as input and optimize the reconstructed scene by minimizing the
uncertainty in geometry and appearance across the views. However, the
reconstruction quality is limited by the number of input views. This limitation
is further pronounced in complex and dynamic scenes, where certain angles of
objects are never seen. In this paper, we propose to use video frame
interpolation as the data augmentation method for neural rendering.
Furthermore, we design a lightweight yet high-quality video frame interpolation
model, PS4PRO (Pixel-to-pixel Supervision for Photorealistic Rendering and
Optimization). PS4PRO is trained on diverse video datasets, implicitly modeling
camera movement as well as real-world 3D geometry. Our model performs as an
implicit world prior, enriching the photo supervision for 3D reconstruction. By
leveraging the proposed method, we effectively augment existing datasets for
neural rendering methods. Our experimental results indicate that our method
improves the reconstruction performance on both static and dynamic scenes.

</details>


### [234] [Object Concepts Emerge from Motion](https://arxiv.org/pdf/2505.21635)
*Haoqian Liang, Xiaohui Wang, Zhichao Li, Ya Yang, Naiyan Wang*

Main category: cs.CV

TL;DR: A biologically inspired, unsupervised framework learns object-centric visual representations using motion boundaries as pseudo supervision, outperforming supervised and self-supervised baselines in various vision tasks.


<details>
  <summary>Details</summary>
Motivation: To mimic infants' object understanding through motion observation and develop scalable, label-free object-centric representations.

Method: Uses motion boundaries from optical flow and clustering for pseudo instance masks, training visual encoders via contrastive learning.

Result: Outperforms baselines in monocular depth estimation, 3D object detection, and occupancy prediction, with strong generalization.

Conclusion: Motion-induced object representations are a promising alternative to existing vision models, capturing visual instances effectively.

Abstract: Object concepts play a foundational role in human visual cognition, enabling
perception, memory, and interaction in the physical world. Inspired by findings
in developmental neuroscience - where infants are shown to acquire object
understanding through observation of motion - we propose a biologically
inspired framework for learning object-centric visual representations in an
unsupervised manner. Our key insight is that motion boundary serves as a strong
signal for object-level grouping, which can be used to derive pseudo instance
supervision from raw videos. Concretely, we generate motion-based instance
masks using off-the-shelf optical flow and clustering algorithms, and use them
to train visual encoders via contrastive learning. Our framework is fully
label-free and does not rely on camera calibration, making it scalable to
large-scale unstructured video data. We evaluate our approach on three
downstream tasks spanning both low-level (monocular depth estimation) and
high-level (3D object detection and occupancy prediction) vision. Our models
outperform previous supervised and self-supervised baselines and demonstrate
strong generalization to unseen scenes. These results suggest that
motion-induced object representations offer a compelling alternative to
existing vision foundation models, capturing a crucial but overlooked level of
abstraction: the visual instance. The corresponding code will be released upon
paper acceptance.

</details>


### [235] [BaryIR: Learning Multi-Source Unified Representation in Continuous Barycenter Space for Generalizable All-in-One Image Restoration](https://arxiv.org/pdf/2505.21637)
*Xiaole Tang, Xiaoyi He, Xiang Gu, Jian Sun*

Main category: cs.CV

TL;DR: BaryIR is a multi-source representation learning framework for all-in-one image restoration, addressing out-of-distribution degradations by decomposing latent space into a barycenter space and source-specific subspaces.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with out-of-distribution degradations and images, limiting real-world applicability.

Method: BaryIR uses a multi-source latent optimal transport barycenter problem to learn unified and source-specific representations, maintaining orthogonality and contrast.

Result: BaryIR achieves competitive performance, especially in generalizing to real-world data and unseen degradations.

Conclusion: BaryIR offers a robust solution for generalizable all-in-one image restoration, outperforming state-of-the-art methods.

Abstract: Despite remarkable advances made in all-in-one image restoration (AIR) for
handling different types of degradations simultaneously, existing methods
remain vulnerable to out-of-distribution degradations and images, limiting
their real-world applicability. In this paper, we propose a multi-source
representation learning framework BaryIR, which decomposes the latent space of
multi-source degraded images into a continuous barycenter space for unified
feature encoding and source-specific subspaces for specific semantic encoding.
Specifically, we seek the multi-source unified representation by introducing a
multi-source latent optimal transport barycenter problem, in which a continuous
barycenter map is learned to transport the latent representations to the
barycenter space. The transport cost is designed such that the representations
from source-specific subspaces are contrasted with each other while maintaining
orthogonality to those from the barycenter space. This enables BaryIR to learn
compact representations with unified degradation-agnostic information from the
barycenter space, as well as degradation-specific semantics from
source-specific subspaces, capturing the inherent geometry of multi-source data
manifold for generalizable AIR. Extensive experiments demonstrate that BaryIR
achieves competitive performance compared to state-of-the-art all-in-one
methods. Particularly, BaryIR exhibits superior generalization ability to
real-world data and unseen degradations. The code will be publicly available at
https://github.com/xl-tang3/BaryIR.

</details>


### [236] [Geometric Feature Prompting of Image Segmentation Models](https://arxiv.org/pdf/2505.21644)
*Kenneth Ball, Erin Taylor, Nirav Patel, Andrew Bartels, Gary Koplik, James Polly, Jay Hineman*

Main category: cs.CV

TL;DR: The paper introduces GeomPrompt, a geometrically motivated prompt generator for SAM, improving segmentation of plant roots in rhizotron images with fewer prompts.


<details>
  <summary>Details</summary>
Motivation: Automating the laborious and subjective task of segmenting plant roots in rhizotron images using SAM.

Method: Uses GeomPrompt to generate colocated point prompts for SAM, enhancing segmentation sensitivity and specificity.

Result: Enables automatic, efficient, and accurate segmentation of plant roots in challenging rhizotron images.

Conclusion: GeomPrompt with SAM offers a significant improvement in rhizotron image processing, supported by an open-source tool.

Abstract: Advances in machine learning, especially the introduction of transformer
architectures and vision transformers, have led to the development of highly
capable computer vision foundation models. The segment anything model (known
colloquially as SAM and more recently SAM 2), is a highly capable foundation
model for segmentation of natural images and has been further applied to
medical and scientific image segmentation tasks. SAM relies on prompts --
points or regions of interest in an image -- to generate associated
segmentations.
  In this manuscript we propose the use of a geometrically motivated prompt
generator to produce prompt points that are colocated with particular features
of interest. Focused prompting enables the automatic generation of sensitive
and specific segmentations in a scientific image analysis task using SAM with
relatively few point prompts. The image analysis task examined is the
segmentation of plant roots in rhizotron or minirhizotron images, which has
historically been a difficult task to automate. Hand annotation of rhizotron
images is laborious and often subjective; SAM, initialized with GeomPrompt
local ridge prompts has the potential to dramatically improve rhizotron image
processing.
  The authors have concurrently released an open source software suite called
geomprompt https://pypi.org/project/geomprompt/ that can produce point prompts
in a format that enables direct integration with the segment-anything package.

</details>


### [237] [QuARI: Query Adaptive Retrieval Improvement](https://arxiv.org/pdf/2505.21647)
*Eric Xing, Abby Stylianou, Robert Pless, Nathan Jacobs*

Main category: cs.CV

TL;DR: Specializing vision-language models via query-specific linear transformations improves large-scale retrieval performance efficiently.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models underperform in challenging tasks like instance retrieval in large-scale collections, prompting the need for better specialization.

Method: Learns query-specific linear transformations of VLM features to enhance retrieval performance without significant computational overhead.

Result: Outperforms state-of-the-art methods, even those requiring much higher computational resources.

Conclusion: Query-specific feature space transformations offer an efficient and effective solution for large-scale retrieval tasks.

Abstract: Massive-scale pretraining has made vision-language models increasingly
popular for image-to-image and text-to-image retrieval across a broad
collection of domains. However, these models do not perform well when used for
challenging retrieval tasks, such as instance retrieval in very large-scale
image collections. Recent work has shown that linear transformations of VLM
features trained for instance retrieval can improve performance by emphasizing
subspaces that relate to the domain of interest. In this paper, we explore a
more extreme version of this specialization by learning to map a given query to
a query-specific feature space transformation. Because this transformation is
linear, it can be applied with minimal computational cost to millions of image
embeddings, making it effective for large-scale retrieval or re-ranking.
Results show that this method consistently outperforms state-of-the-art
alternatives, including those that require many orders of magnitude more
computation at query time.

</details>


### [238] [Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks](https://arxiv.org/pdf/2505.21649)
*Keanu Nichols, Nazia Tasnim, Yan Yuting, Nicholas Ikechukwu, Elva Zou, Deepti Ghadiyaram, Bryan Plummer*

Main category: cs.CV

TL;DR: DORI is a benchmark for evaluating object orientation understanding in vision-language models, revealing significant limitations in current systems.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks conflate object orientation with other tasks, necessitating a dedicated evaluation framework.

Method: DORI assesses orientation comprehension across four dimensions using curated tasks from 11 datasets.

Result: Top models achieve 54.2% accuracy on coarse tasks and 33.0% on granular ones, showing poor performance on complex orientation tasks.

Conclusion: DORI highlights the need for better orientation representation mechanisms in multimodal systems, with implications for robotics and AI interaction.

Abstract: Object orientation understanding represents a fundamental challenge in visual
perception critical for applications like robotic manipulation and augmented
reality. Current vision-language benchmarks fail to isolate this capability,
often conflating it with positional relationships and general scene
understanding. We introduce DORI (Discriminative Orientation Reasoning
Intelligence), a comprehensive benchmark establishing object orientation
perception as a primary evaluation target. DORI assesses four dimensions of
orientation comprehension: frontal alignment, rotational transformations,
relative directional relationships, and canonical orientation understanding.
Through carefully curated tasks from 11 datasets spanning 67 object categories
across synthetic and real-world scenarios, DORI provides insights on how
multi-modal systems understand object orientations. Our evaluation of 15
state-of-the-art vision-language models reveals critical limitations: even the
best models achieve only 54.2% accuracy on coarse tasks and 33.0% on granular
orientation judgments, with performance deteriorating for tasks requiring
reference frame shifts or compound rotations. These findings demonstrate the
need for dedicated orientation representation mechanisms, as models show
systematic inability to perform precise angular estimations, track orientation
changes across viewpoints, and understand compound rotations - suggesting
limitations in their internal 3D spatial representations. As the first
diagnostic framework specifically designed for orientation awareness in
multimodal systems, DORI offers implications for improving robotic control, 3D
scene reconstruction, and human-AI interaction in physical environments. DORI
data: https://huggingface.co/datasets/appledora/DORI-Benchmark

</details>


### [239] [Think Before You Diffuse: LLMs-Guided Physics-Aware Video Generation](https://arxiv.org/pdf/2505.21653)
*Ke Zhang, Cihan Xiao, Yiqun Mei, Jiacong Xu, Vishal M. Patel*

Main category: cs.CV

TL;DR: DiffPhy is a framework for physically-correct video generation by fine-tuning a pre-trained diffusion model, using LLMs for physical context and MLLMs for supervision.


<details>
  <summary>Details</summary>
Motivation: Current video diffusion models struggle with synthesizing correct physical effects due to the complexity of real-world motions and dynamics.

Method: Leverages LLMs to reason physical context from text prompts and MLLMs for supervision, introducing novel training objectives for physical correctness and semantic consistency.

Result: Produces state-of-the-art results in diverse physics-related scenarios, validated on public benchmarks.

Conclusion: DiffPhy effectively bridges the gap between photo-realism and physical correctness in video generation.

Abstract: Recent video diffusion models have demonstrated their great capability in
generating visually-pleasing results, while synthesizing the correct physical
effects in generated videos remains challenging. The complexity of real-world
motions, interactions, and dynamics introduce great difficulties when learning
physics from data. In this work, we propose DiffPhy, a generic framework that
enables physically-correct and photo-realistic video generation by fine-tuning
a pre-trained video diffusion model. Our method leverages large language models
(LLMs) to explicitly reason a comprehensive physical context from the text
prompt and use it to guide the generation. To incorporate physical context into
the diffusion model, we leverage a Multimodal large language model (MLLM) as a
supervisory signal and introduce a set of novel training objectives that
jointly enforce physical correctness and semantic consistency with the input
text. We also establish a high-quality physical video dataset containing
diverse phyiscal actions and events to facilitate effective finetuning.
Extensive experiments on public benchmarks demonstrate that DiffPhy is able to
produce state-of-the-art results across diverse physics-related scenarios. Our
project page is available at https://bwgzk-keke.github.io/DiffPhy/

</details>


### [240] [Scalable Segmentation for Ultra-High-Resolution Brain MR Images](https://arxiv.org/pdf/2505.21697)
*Xiaoling Hu, Peirong Liu, Dina Zemlyanker, Jonathan Williams Ramirez, Oula Puonti, Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: A novel framework for ultra-high-resolution brain MRI segmentation uses low-resolution coarse labels as guidance, regresses signed distance transform maps for boundary-aware supervision, and employs a scalable class-conditional strategy to improve efficiency and generalizability.


<details>
  <summary>Details</summary>
Motivation: Accurate segmentation of ultra-high-resolution brain MRI is challenging due to limited labeled data for fine-scale structures and high computational demands.

Method: The framework leverages low-resolution coarse labels, regresses per-class signed distance transform maps, and uses a scalable class-conditional segmentation strategy to segment one class at a time.

Result: The method shows superior performance and scalability on synthetic and real-world datasets compared to conventional approaches.

Conclusion: The proposed framework efficiently addresses the challenges of ultra-high-resolution brain MRI segmentation while reducing annotation costs and computational demands.

Abstract: Although deep learning has shown great success in 3D brain MRI segmentation,
achieving accurate and efficient segmentation of ultra-high-resolution brain
images remains challenging due to the lack of labeled training data for
fine-scale anatomical structures and high computational demands. In this work,
we propose a novel framework that leverages easily accessible, low-resolution
coarse labels as spatial references and guidance, without incurring additional
annotation cost. Instead of directly predicting discrete segmentation maps, our
approach regresses per-class signed distance transform maps, enabling smooth,
boundary-aware supervision. Furthermore, to enhance scalability,
generalizability, and efficiency, we introduce a scalable class-conditional
segmentation strategy, where the model learns to segment one class at a time
conditioned on a class-specific input. This novel design not only reduces
memory consumption during both training and testing, but also allows the model
to generalize to unseen anatomical classes. We validate our method through
comprehensive experiments on both synthetic and real-world datasets,
demonstrating its superior performance and scalability compared to conventional
segmentation approaches.

</details>


### [241] [MedBridge: Bridging Foundation Vision-Language Models to Medical Image Diagnosis](https://arxiv.org/pdf/2505.21698)
*Yitong Li, Morteza Ghahremani, Christian Wachinger*

Main category: cs.CV

TL;DR: MedBridge is a lightweight framework adapting pretrained vision-language models (VLMs) for medical image diagnosis, improving accuracy with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Domain shifts in medical images hinder general VLMs, and training medical foundation models is resource-intensive. MedBridge bridges this gap efficiently.

Method: Uses Focal Sampling, a Query Encoder, and Mixture of Experts to adapt VLMs without retraining the backbone.

Result: Achieves 6-15% AUC improvement in thoracic disease diagnosis, outperforming other VLM adaptation methods.

Conclusion: MedBridge effectively leverages foundation models for accurate, data-efficient medical diagnosis.

Abstract: Recent vision-language foundation models deliver state-of-the-art results on
natural image classification but falter on medical images due to pronounced
domain shifts. At the same time, training a medical foundation model requires
substantial resources, including extensive annotated data and high
computational capacity. To bridge this gap with minimal overhead, we introduce
MedBridge, a lightweight multimodal adaptation framework that re-purposes
pretrained VLMs for accurate medical image diagnosis. MedBridge comprises three
key components. First, a Focal Sampling module that extracts high-resolution
local regions to capture subtle pathological features and compensate for the
limited input resolution of general-purpose VLMs. Second, a Query Encoder
(QEncoder) injects a small set of learnable queries that attend to the frozen
feature maps of VLM, aligning them with medical semantics without retraining
the entire backbone. Third, a Mixture of Experts mechanism, driven by learnable
queries, harnesses the complementary strength of diverse VLMs to maximize
diagnostic performance. We evaluate MedBridge on five medical imaging
benchmarks across three key adaptation tasks, demonstrating its superior
performance in both cross-domain and in-domain adaptation settings, even under
varying levels of training data availability. Notably, MedBridge achieved over
6-15% improvement in AUC compared to state-of-the-art VLM adaptation methods in
multi-label thoracic disease diagnosis, underscoring its effectiveness in
leveraging foundation models for accurate and data-efficient medical diagnosis.
Our code is available at https://github.com/ai-med/MedBridge.

</details>


### [242] [OmniResponse: Online Multimodal Conversational Response Generation in Dyadic Interactions](https://arxiv.org/pdf/2505.21724)
*Cheng Luo, Jianghui Wang, Bing Li, Siyang Song, Bernard Ghanem*

Main category: cs.CV

TL;DR: The paper introduces OMCRG, a task for generating synchronized verbal and non-verbal listener feedback online. It proposes OmniResponse, an MLLM model, and a new dataset, ResponseNet, showing superior performance over baselines.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generating synchronized audio and facial responses in dyadic interactions, reflecting natural human communication.

Method: Introduces OmniResponse, a Multimodal Large Language Model (MLLM) with Chrono-Text and TempoVoice components, leveraging text as an intermediate modality.

Result: OmniResponse outperforms baselines in semantic content, audio-visual synchronization, and generation quality on the ResponseNet dataset.

Conclusion: The proposed approach effectively tackles OMCRG challenges, advancing multimodal conversational response generation.

Abstract: In this paper, we introduce Online Multimodal Conversational Response
Generation (OMCRG), a novel task that aims to online generate synchronized
verbal and non-verbal listener feedback, conditioned on the speaker's
multimodal input. OMCRG reflects natural dyadic interactions and poses new
challenges in achieving synchronization between the generated audio and facial
responses of the listener. To address these challenges, we innovatively
introduce text as an intermediate modality to bridge the audio and facial
responses. We hence propose OmniResponse, a Multimodal Large Language Model
(MLLM) that autoregressively generates high-quality multi-modal listener
responses. OmniResponse leverages a pretrained LLM enhanced with two novel
components: Chrono-Text, which temporally anchors generated text tokens, and
TempoVoice, a controllable online TTS module that produces speech synchronized
with facial reactions. To support further OMCRG research, we present
ResponseNet, a new dataset comprising 696 high-quality dyadic interactions
featuring synchronized split-screen videos, multichannel audio, transcripts,
and facial behavior annotations. Comprehensive evaluations conducted on
ResponseNet demonstrate that OmniResponse significantly outperforms baseline
models in terms of semantic speech content, audio-visual synchronization, and
generation quality.

</details>


### [243] [Complex Wavelet Mutual Information Loss: A Multi-Scale Loss Function for Semantic Segmentation](https://arxiv.org/pdf/2502.00563)
*Renhao Lu*

Main category: cs.CV

TL;DR: The paper introduces CWMI loss, a novel loss function for semantic segmentation that addresses class and instance imbalance by leveraging mutual information from complex steerable pyramid subbands, improving accuracy and topology metrics with low computational cost.


<details>
  <summary>Details</summary>
Motivation: Class and instance imbalance in semantic segmentation, especially for smaller instances and thin boundaries, remains a challenge. Existing loss functions are either pixel-wise or computationally expensive for regional/boundary focus.

Method: Proposes the complex wavelet mutual information (CWMI) loss, using mutual information from subband images decomposed by a complex steerable pyramid to capture multi-scale and directional features.

Result: CWMI loss outperforms state-of-the-art methods in pixel-wise accuracy and topological metrics on diverse datasets, with minimal computational overhead.

Conclusion: The CWMI loss effectively addresses imbalance issues in semantic segmentation, offering improved performance and efficiency.

Abstract: Recent advancements in deep neural networks have significantly enhanced the
performance of semantic segmentation. However, class imbalance and instance
imbalance remain persistent challenges, where smaller instances and thin
boundaries are often overshadowed by larger structures. To address the
multiscale nature of segmented objects, various models have incorporated
mechanisms such as spatial attention and feature pyramid networks. Despite
these advancements, most loss functions are still primarily pixel-wise, while
regional and boundary-focused loss functions often incur high computational
costs or are restricted to small-scale regions. To address this limitation, we
propose the complex wavelet mutual information (CWMI) loss, a novel loss
function that leverages mutual information from subband images decomposed by a
complex steerable pyramid. The complex steerable pyramid captures features
across multiple orientations and preserves structural similarity across scales.
Meanwhile, mutual information is well-suited to capturing high-dimensional
directional features and offers greater noise robustness. Extensive experiments
on diverse segmentation datasets demonstrate that CWMI loss achieves
significant improvements in both pixel-wise accuracy and topological metrics
compared to state-of-the-art methods, while introducing minimal computational
overhead. Our code is available at https://github.com/lurenhaothu/CWMI

</details>


### [244] [Moment kernels: a simple and scalable approach for equivariance to rotations and reflections in deep convolutional networks](https://arxiv.org/pdf/2505.21736)
*Zachary Schlamowitz, Andrew Bennecke, Daniel J. Tward*

Main category: cs.CV

TL;DR: The paper introduces 'moment kernels' for achieving equivariance in neural networks, simplifying the exploitation of symmetries like rotations and reflections in biomedical image analysis.


<details>
  <summary>Details</summary>
Motivation: Existing methods for exploiting symmetries in image analysis are mathematically complex, limiting adoption. The paper aims to simplify this using moment kernels.

Method: Proposes moment kernels—radially symmetric functions combined with spatial components—to achieve equivariance. Implements these using standard convolution modules for tasks like classification, 3D registration, and segmentation.

Result: Demonstrates that moment kernels can achieve equivariance, proving all equivariant kernels must take this form. Successfully applies them to biomedical tasks.

Conclusion: Moment kernels offer a simpler, effective way to achieve equivariance, enabling broader adoption in biomedical image analysis.

Abstract: The principle of translation equivariance (if an input image is translated an
output image should be translated by the same amount), led to the development
of convolutional neural networks that revolutionized machine vision. Other
symmetries, like rotations and reflections, play a similarly critical role,
especially in biomedical image analysis, but exploiting these symmetries has
not seen wide adoption. We hypothesize that this is partially due to the
mathematical complexity of methods used to exploit these symmetries, which
often rely on representation theory, a bespoke concept in differential geometry
and group theory. In this work, we show that the same equivariance can be
achieved using a simple form of convolution kernels that we call ``moment
kernels,'' and prove that all equivariant kernels must take this form. These
are a set of radially symmetric functions of a spatial position $x$, multiplied
by powers of the components of $x$ or the identity matrix. We implement
equivariant neural networks using standard convolution modules, and provide
architectures to execute several biomedical image analysis tasks that depend on
equivariance principles: classification (outputs are invariant under orthogonal
transforms), 3D image registration (outputs transform like a vector), and cell
segmentation (quadratic forms defining ellipses transform like a matrix).

</details>


### [245] [Stereo Radargrammetry Using Deep Learning from Airborne SAR Images](https://arxiv.org/pdf/2505.20876)
*Tatsuya Sasayama, Shintaro Ito, Koichi Ito, Takafumi Aoki*

Main category: cs.CV

TL;DR: A deep learning-based stereo radargrammetry method for airborne SAR images is proposed, addressing the lack of public datasets and improving elevation measurement accuracy.


<details>
  <summary>Details</summary>
Motivation: Deep learning methods for SAR images lack public datasets and face challenges like geometric image modulation.

Method: The method involves creating a SAR dataset, fine-tuning a deep learning-based correspondence method, and processing images in patches without ground projection.

Result: The method achieves wider range and more accurate elevation measurements than conventional approaches.

Conclusion: The proposed method effectively addresses SAR image processing challenges and outperforms traditional techniques.

Abstract: In this paper, we propose a stereo radargrammetry method using deep learning
from airborne Synthetic Aperture Radar (SAR) images. Deep learning-based
methods are considered to suffer less from geometric image modulation, while
there is no public SAR image dataset used to train such methods. We create a
SAR image dataset and perform fine-tuning of a deep learning-based image
correspondence method. The proposed method suppresses the degradation of image
quality by pixel interpolation without ground projection of the SAR image and
divides the SAR image into patches for processing, which makes it possible to
apply deep learning. Through a set of experiments, we demonstrate that the
proposed method exhibits a wider range and more accurate elevation measurements
compared to conventional methods.

</details>


### [246] [What is Adversarial Training for Diffusion Models?](https://arxiv.org/pdf/2505.21742)
*Briglia Maria Rosaria, Mujtaba Hussain Mirza, Giuseppe Lisanti, Iacopo Masi*

Main category: cs.CV

TL;DR: Adversarial training (AT) in diffusion models (DMs) differs from classifiers by requiring equivariance, not invariance, aligning the diffusion process with data distribution. It enhances robustness to outliers and corrupted data without noise model assumptions.


<details>
  <summary>Details</summary>
Motivation: To address how AT in DMs fundamentally differs from classifiers and to improve robustness against noise, corruption, and adversarial attacks.

Method: Integrates AT into DMs by adding random or adversarial noise during training, similar to randomized smoothing or AT, without noise model assumptions.

Result: Demonstrates strong performance on proof-of-concept datasets and benchmarks (CIFAR-10, CelebA, LSUN Bedroom) under noise, corruption, and adversarial attacks.

Conclusion: AT in DMs ensures equivariance, improving robustness and handling noisy data, outliers, and adversarial scenarios effectively.

Abstract: We answer the question in the title, showing that adversarial training (AT)
for diffusion models (DMs) fundamentally differs from classifiers: while AT in
classifiers enforces output invariance, AT in DMs requires equivariance to keep
the diffusion process aligned with the data distribution. AT is a way to
enforce smoothness in the diffusion flow, improving robustness to outliers and
corrupted data. Unlike prior art, our method makes no assumptions about the
noise model and integrates seamlessly into diffusion training by adding random
noise, similar to randomized smoothing, or adversarial noise, akin to AT. This
enables intrinsic capabilities such as handling noisy data, dealing with
extreme variability such as outliers, preventing memorization, and improving
robustness. We rigorously evaluate our approach with proof-of-concept datasets
with known distributions in low- and high-dimensional space, thereby taking a
perfect measure of errors; we further evaluate on standard benchmarks such as
CIFAR-10, CelebA and LSUN Bedroom, showing strong performance under severe
noise, data corruption, and iterative adversarial attacks.

</details>


### [247] [Learning to See More: UAS-Guided Super-Resolution of Satellite Imagery for Precision Agriculture](https://arxiv.org/pdf/2505.21746)
*Arif Masrur, Peder A. Olsen, Paul R. Adler, Carlan Jackson, Matthew W. Myers, Nathan Sedghi, Ray R. Weil*

Main category: cs.CV

TL;DR: A novel framework fuses satellite and UAS imagery using super-resolution to enhance precision agriculture, improving biomass and nitrogen estimation accuracy by 18% and 31%, respectively.


<details>
  <summary>Details</summary>
Motivation: Address the trade-offs between satellite and UAS data in precision agriculture by leveraging their combined strengths cost-effectively.

Method: Integrate satellite and UAS imagery using super-resolution methods, spectrally extending UAS RGB data to vegetation red edge and near-infrared regions.

Result: Improved biomass and nitrogen estimation accuracy by 18% and 31%, with reduced need for repeated UAS flights.

Conclusion: The lightweight, scalable system offers affordable on-farm use, enhancing precision agriculture with minimal data collection.

Abstract: Unmanned Aircraft Systems (UAS) and satellites are key data sources for
precision agriculture, yet each presents trade-offs. Satellite data offer broad
spatial, temporal, and spectral coverage but lack the resolution needed for
many precision farming applications, while UAS provide high spatial detail but
are limited by coverage and cost, especially for hyperspectral data. This study
presents a novel framework that fuses satellite and UAS imagery using
super-resolution methods. By integrating data across spatial, spectral, and
temporal domains, we leverage the strengths of both platforms cost-effectively.
We use estimation of cover crop biomass and nitrogen (N) as a case study to
evaluate our approach. By spectrally extending UAS RGB data to the vegetation
red edge and near-infrared regions, we generate high-resolution Sentinel-2
imagery and improve biomass and N estimation accuracy by 18% and 31%,
respectively. Our results show that UAS data need only be collected from a
subset of fields and time points. Farmers can then 1) enhance the spectral
detail of UAS RGB imagery; 2) increase the spatial resolution by using
satellite data; and 3) extend these enhancements spatially and across the
growing season at the frequency of the satellite flights. Our SRCNN-based
spectral extension model shows considerable promise for model transferability
over other cropping systems in the Upper and Lower Chesapeake Bay regions.
Additionally, it remains effective even when cloud-free satellite data are
unavailable, relying solely on the UAS RGB input. The spatial extension model
produces better biomass and N predictions than models built on raw UAS RGB
images. Once trained with targeted UAS RGB data, the spatial extension model
allows farmers to stop repeated UAS flights. While we introduce
super-resolution advances, the core contribution is a lightweight and scalable
system for affordable on-farm use.

</details>


### [248] [Visual Loop Closure Detection Through Deep Graph Consensus](https://arxiv.org/pdf/2505.21754)
*Martin Büchner, Liza Dahiya, Simon Dorer, Vipul Ramtekkar, Kenji Nishimiya, Daniele Cattaneo, Abhinav Valada*

Main category: cs.CV

TL;DR: LoopGNN, a graph neural network, improves loop closure detection by using neighborhoods of keyframes, outperforming traditional methods in precision and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional loop closure detection is computationally expensive and prone to false positives, degrading pose graph estimates. LoopGNN aims to address these limitations.

Method: LoopGNN leverages cliques of visually similar keyframes and propagates deep feature encodings among nodes to estimate loop closure consensus.

Result: LoopGNN achieves high precision and recall, outperforming baselines on TartanDrive 2.0 and NCLT datasets, and is robust across feature encoders.

Conclusion: LoopGNN offers a computationally efficient and robust solution for loop closure detection, with released code and data for further use.

Abstract: Visual loop closure detection traditionally relies on place recognition
methods to retrieve candidate loops that are validated using computationally
expensive RANSAC-based geometric verification. As false positive loop closures
significantly degrade downstream pose graph estimates, verifying a large number
of candidates in online simultaneous localization and mapping scenarios is
constrained by limited time and compute resources. While most deep loop closure
detection approaches only operate on pairs of keyframes, we relax this
constraint by considering neighborhoods of multiple keyframes when detecting
loops. In this work, we introduce LoopGNN, a graph neural network architecture
that estimates loop closure consensus by leveraging cliques of visually similar
keyframes retrieved through place recognition. By propagating deep feature
encodings among nodes of the clique, our method yields high-precision estimates
while maintaining high recall. Extensive experimental evaluations on the
TartanDrive 2.0 and NCLT datasets demonstrate that LoopGNN outperforms
traditional baselines. Additionally, an ablation study across various keypoint
extractors demonstrates that our method is robust, regardless of the type of
deep feature encodings used, and exhibits higher computational efficiency
compared to classical geometric verification baselines. We release our code,
supplementary material, and keyframe data at
https://loopgnn.cs.uni-freiburg.de.

</details>


### [249] [FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering](https://arxiv.org/pdf/2505.21755)
*Chengyue Huang, Brisa Maneechotesuwan, Shivang Chopra, Zsolt Kira*

Main category: cs.CV

TL;DR: The paper introduces FRAMES-VQA, a benchmark for evaluating robust fine-tuning in VQA tasks across multi-modal distribution shifts, using ten existing datasets and analyzing uni- and multi-modal interactions.


<details>
  <summary>Details</summary>
Motivation: Current VQA evaluation settings lack insight into multi-modal data shifts, limiting robust fine-tuning strategies for real-world applications.

Method: Proposes FRAMES-VQA, categorizing datasets into ID and OOD shifts, comparing fine-tuning methods, and quantifying shifts using Mahalanobis distance on embeddings.

Result: Comprehensive analysis reveals interactions between uni- and multi-modal shifts and modality importance, guiding robust fine-tuning development.

Conclusion: FRAMES-VQA provides a valuable benchmark and insights for improving VQA robustness against multi-modal distribution shifts.

Abstract: Visual question answering (VQA) systems face significant challenges when
adapting to real-world data shifts, especially in multi-modal contexts. While
robust fine-tuning strategies are essential for maintaining performance across
in-distribution (ID) and out-of-distribution (OOD) scenarios, current
evaluation settings are primarily unimodal or particular to some types of OOD,
offering limited insight into the complexities of multi-modal contexts. In this
work, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across
Multi-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We
utilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA
and others, and categorize them into ID, near and far OOD datasets covering
uni-modal, multi-modal and adversarial distribution shifts. We first conduct a
comprehensive comparison of existing robust fine-tuning methods. We then
quantify the distribution shifts by calculating the Mahalanobis distance using
uni-modal and multi-modal embeddings extracted from various models. Further, we
perform an extensive analysis to explore the interactions between uni- and
multi-modal shifts as well as modality importance for ID and OOD samples. These
analyses offer valuable guidance on developing more robust fine-tuning methods
to handle multi-modal distribution shifts. The code is available at
https://github.com/chengyuehuang511/FRAMES-VQA .

</details>


### [250] [MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning](https://arxiv.org/pdf/2505.21771)
*Prasham Yatinkumar Titiya, Jainil Trivedi, Chitta Baral, Vivek Gupta*

Main category: cs.CV

TL;DR: MMTBENCH is a benchmark for evaluating vision-language models on real-world multimodal tables, revealing performance gaps in visual-based reasoning and multi-step inference.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models struggle with complex multimodal tables, and their performance on such tasks is unexplored.

Method: Introduced MMTBENCH, a benchmark with 500 real-world multimodal tables and 4021 QA pairs, covering diverse question and reasoning types.

Result: State-of-the-art models show significant gaps, especially in visual-based reasoning and multi-step inference.

Conclusion: MMTBENCH highlights the need for better architectures integrating vision and language, serving as a valuable resource for future research.

Abstract: Multimodal tables those that integrate semi structured data with visual
elements such as charts and maps are ubiquitous across real world domains, yet
they pose a formidable challenge to current vision language models (VLMs).
While Large Language models (LLMs) and VLMs have demonstrated strong
capabilities in text and image understanding, their performance on complex,
real world multimodal table reasoning remains unexplored. To bridge this gap,
we introduce MMTBENCH (Multimodal Table Benchmark), a benchmark consisting of
500 real world multimodal tables drawn from diverse real world sources, with a
total of 4021 question answer pairs. MMTBENCH questions cover four question
types (Explicit, Implicit, Answer Mention, and Visual Based), five reasoning
types (Mathematical, Extrema Identification, Fact Verification, Vision Based,
and Others), and eight table types (Single/Multiple Entity, Maps and Charts
with Entities, Single/Multiple Charts, Maps, and Visualizations). Extensive
evaluation of state of the art models on all types reveals substantial
performance gaps, particularly on questions requiring visual-based reasoning
and multi-step inference. These findings show the urgent need for improved
architectures that more tightly integrate vision and language processing. By
providing a challenging, high-quality resource that mirrors the complexity of
real-world tasks, MMTBENCH underscores its value as a resource for future
research on multimodal tables.

</details>


### [251] [Compositional Scene Understanding through Inverse Generative Modeling](https://arxiv.org/pdf/2505.21780)
*Yanbo Wang, Justin Dauwels, Yilun Du*

Main category: cs.CV

TL;DR: The paper explores using generative models for scene understanding by treating it as an inverse generative modeling problem, enabling robust generalization to new scenes and objects.


<details>
  <summary>Details</summary>
Motivation: To extend generative models beyond content synthesis to infer scene properties from natural images, even those differing from training data.

Method: Formulates scene understanding as inverse generative modeling, using compositional models to infer scene structure and global factors.

Result: Demonstrates robust generalization to new scenes with more objects or new shapes, and enables zero-shot multi-object perception.

Conclusion: The approach successfully leverages generative models for scene understanding and generalizes well to unseen scenarios.

Abstract: Generative models have demonstrated remarkable abilities in generating
high-fidelity visual content. In this work, we explore how generative models
can further be used not only to synthesize visual content but also to
understand the properties of a scene given a natural image. We formulate scene
understanding as an inverse generative modeling problem, where we seek to find
conditional parameters of a visual generative model to best fit a given natural
image. To enable this procedure to infer scene structure from images
substantially different than those seen during training, we further propose to
build this visual generative model compositionally from smaller models over
pieces of a scene. We illustrate how this procedure enables us to infer the set
of objects in a scene, enabling robust generalization to new test scenes with
an increased number of objects of new shapes. We further illustrate how this
enables us to infer global scene factors, likewise enabling robust
generalization to new scenes. Finally, we illustrate how this approach can be
directly applied to existing pretrained text-to-image generative models for
zero-shot multi-object perception. Code and visualizations are at
\href{https://energy-based-model.github.io/compositional-inference}{https://energy-based-model.github.io/compositional-inference}.

</details>


### [252] [SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation](https://arxiv.org/pdf/2505.21795)
*Claudia Cuttano, Gabriele Trivigno, Giuseppe Averta, Carlo Masone*

Main category: cs.CV

TL;DR: SANSA repurposes SAM2 for few-shot segmentation by aligning its latent semantic structure, achieving state-of-the-art performance with minimal modifications.


<details>
  <summary>Details</summary>
Motivation: SAM2's features are entangled with task-specific cues for object tracking, limiting its use for semantic tasks like few-shot segmentation.

Method: SANSA aligns SAM2's latent semantic structure explicitly and adapts it for few-shot segmentation with minimal task-specific changes.

Result: SANSA outperforms generalist methods in few-shot segmentation benchmarks, supports flexible prompts, and is faster and more compact than prior approaches.

Conclusion: SANSA successfully leverages SAM2's capabilities for few-shot segmentation, demonstrating superior performance and flexibility.

Abstract: Few-shot segmentation aims to segment unseen object categories from just a
handful of annotated examples. This requires mechanisms that can both identify
semantically related objects across images and accurately produce segmentation
masks. We note that Segment Anything 2 (SAM2), with its prompt-and-propagate
mechanism, offers both strong segmentation capabilities and a built-in feature
matching process. However, we show that its representations are entangled with
task-specific cues optimized for object tracking, which impairs its use for
tasks requiring higher level semantic understanding. Our key insight is that,
despite its class-agnostic pretraining, SAM2 already encodes rich semantic
structure in its features. We propose SANSA (Semantically AligNed Segment
Anything 2), a framework that makes this latent structure explicit, and
repurposes SAM2 for few-shot segmentation through minimal task-specific
modifications. SANSA achieves state-of-the-art performance on few-shot
segmentation benchmarks specifically designed to assess generalization,
outperforms generalist methods in the popular in-context setting, supports
various prompts flexible interaction via points, boxes, or scribbles, and
remains significantly faster and more compact than prior approaches. Code is
available at https://github.com/ClaudiaCuttano/SANSA.

</details>


### [253] [ALTER: All-in-One Layer Pruning and Temporal Expert Routing for Efficient Diffusion Generation](https://arxiv.org/pdf/2505.21817)
*Xiaomeng Yang, Lei Lu, Qihui Fan, Changdi Yang, Juyi Lin, Yanzhi Wang, Xuan Zhang, Shangqian Gao*

Main category: cs.CV

TL;DR: ALTER introduces a unified framework for efficient diffusion model inference by combining layer pruning, expert routing, and fine-tuning in a single-stage optimization.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are computationally expensive during inference due to iterative denoising, and existing acceleration methods lack adaptability to temporal variations.

Method: ALTER employs a trainable hypernetwork for dynamic layer pruning and timestep routing to pruned expert sub-networks during UNet fine-tuning.

Result: ALTER reduces computational costs (25.9% of MACs) and speeds up inference (3.64x) while maintaining visual fidelity.

Conclusion: ALTER offers a practical solution for deploying high-quality diffusion models in resource-constrained environments.

Abstract: Diffusion models have demonstrated exceptional capabilities in generating
high-fidelity images. However, their iterative denoising process results in
significant computational overhead during inference, limiting their practical
deployment in resource-constrained environments. Existing acceleration methods
often adopt uniform strategies that fail to capture the temporal variations
during diffusion generation, while the commonly adopted sequential
pruning-then-fine-tuning strategy suffers from sub-optimality due to the
misalignment between pruning decisions made on pretrained weights and the
model's final parameters. To address these limitations, we introduce ALTER:
All-in-One Layer Pruning and Temporal Expert Routing, a unified framework that
transforms diffusion models into a mixture of efficient temporal experts. ALTER
achieves a single-stage optimization that unifies layer pruning, expert
routing, and model fine-tuning by employing a trainable hypernetwork, which
dynamically generates layer pruning decisions and manages timestep routing to
specialized, pruned expert sub-networks throughout the ongoing fine-tuning of
the UNet. This unified co-optimization strategy enables significant efficiency
gains while preserving high generative quality. Specifically, ALTER achieves
same-level visual fidelity to the original 50-step Stable Diffusion v2.1 model
while utilizing only 25.9% of its total MACs with just 20 inference steps and
delivering a 3.64x speedup through 35% sparsity.

</details>


### [254] [HDRSDR-VQA: A Subjective Video Quality Dataset for HDR and SDR Comparative Evaluation](https://arxiv.org/pdf/2505.21831)
*Bowen Chen, Cheng-han Lee, Yixu Chen, Zaixi Shang, Hai Wei, Alan C. Bovik*

Main category: cs.CV

TL;DR: HDRSDR-VQA is a dataset for comparing HDR and SDR video quality, featuring 960 videos, subjective scores from 145 participants, and 22,000 pairwise comparisons.


<details>
  <summary>Details</summary>
Motivation: To enable direct comparison between HDR and SDR video quality under realistic conditions, addressing gaps in prior datasets.

Method: Generated 960 videos from 54 sources in HDR/SDR formats with nine distortion levels. Conducted a subjective study with 145 participants and six HDR TVs, collecting 22,000 pairwise comparisons scaled to JOD scores.

Result: A comprehensive dataset supporting direct HDR/SDR comparisons, revealing preferences and quality differences.

Conclusion: HDRSDR-VQA fills a research gap, aiding video quality assessment, streaming, and perceptual model development.

Abstract: We introduce HDRSDR-VQA, a large-scale video quality assessment dataset
designed to facilitate comparative analysis between High Dynamic Range (HDR)
and Standard Dynamic Range (SDR) content under realistic viewing conditions.
The dataset comprises 960 videos generated from 54 diverse source sequences,
each presented in both HDR and SDR formats across nine distortion levels. To
obtain reliable perceptual quality scores, we conducted a comprehensive
subjective study involving 145 participants and six consumer-grade HDR-capable
televisions. A total of over 22,000 pairwise comparisons were collected and
scaled into Just-Objectionable-Difference (JOD) scores. Unlike prior datasets
that focus on a single dynamic range format or use limited evaluation
protocols, HDRSDR-VQA enables direct content-level comparison between HDR and
SDR versions, supporting detailed investigations into when and why one format
is preferred over the other. The open-sourced part of the dataset is publicly
available to support further research in video quality assessment,
content-adaptive streaming, and perceptual model development.

</details>


### [255] [UniMoGen: Universal Motion Generation](https://arxiv.org/pdf/2505.21837)
*Aliasghar Khani, Arianna Rampini, Evan Atherton, Bruno Roy*

Main category: cs.CV

TL;DR: UniMoGen is a UNet-based diffusion model for skeleton-agnostic motion generation, enabling versatile and efficient motion creation across diverse characters.


<details>
  <summary>Details</summary>
Motivation: Existing motion generation methods are limited by reliance on specific skeletal structures, restricting versatility. UniMoGen addresses this by being skeleton-agnostic.

Method: UniMoGen uses a UNet-based diffusion model to dynamically process necessary joints, allowing training on diverse motion data without predefined joint limits. It supports control via style and trajectory inputs.

Result: UniMoGen outperforms state-of-the-art methods on the 100style dataset and achieves high performance on mixed datasets (100style and LAFAN1), demonstrating efficiency and flexibility.

Conclusion: UniMoGen provides a flexible, efficient, and controllable solution for motion generation, advancing the field by supporting diverse character animations.

Abstract: Motion generation is a cornerstone of computer graphics, animation, gaming,
and robotics, enabling the creation of realistic and varied character
movements. A significant limitation of existing methods is their reliance on
specific skeletal structures, which restricts their versatility across
different characters. To overcome this, we introduce UniMoGen, a novel
UNet-based diffusion model designed for skeleton-agnostic motion generation.
UniMoGen can be trained on motion data from diverse characters, such as humans
and animals, without the need for a predefined maximum number of joints. By
dynamically processing only the necessary joints for each character, our model
achieves both skeleton agnosticism and computational efficiency. Key features
of UniMoGen include controllability via style and trajectory inputs, and the
ability to continue motions from past frames. We demonstrate UniMoGen's
effectiveness on the 100style dataset, where it outperforms state-of-the-art
methods in diverse character motion generation. Furthermore, when trained on
both the 100style and LAFAN1 datasets, which use different skeletons, UniMoGen
achieves high performance and improved efficiency across both skeletons. These
results highlight UniMoGen's potential to advance motion generation by
providing a flexible, efficient, and controllable solution for a wide range of
character animations.

</details>


### [256] [Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation](https://arxiv.org/pdf/2505.21844)
*Mehrdad Noori, David Osowiechi, Gustavo Adolfo Vargas Hakim, Ali Bahri, Moslem Yazdanpanah, Sahar Dastani, Farzad Beizaee, Ismail Ben Ayed, Christian Desrosiers*

Main category: cs.CV

TL;DR: A novel TTA method, MLMP, is proposed for adapting VLMs to Open-Vocabulary Semantic Segmentation, outperforming classification baselines.


<details>
  <summary>Details</summary>
Motivation: Address the overlooked problem of test-time adaptation in dense prediction tasks like OVSS.

Method: Multi-Level and Multi-Prompt (MLMP) entropy minimization integrates intermediate vision-encoder layers and uses varied text prompts at global and local levels.

Result: MLMP consistently outperforms TTA classification baselines in a comprehensive OVSS benchmark.

Conclusion: The method is effective, plug-and-play, and sets a standardized benchmark for future TTA research in segmentation.

Abstract: Recently, test-time adaptation has attracted wide interest in the context of
vision-language models for image classification. However, to the best of our
knowledge, the problem is completely overlooked in dense prediction tasks such
as Open-Vocabulary Semantic Segmentation (OVSS). In response, we propose a
novel TTA method tailored to adapting VLMs for segmentation during test time.
Unlike TTA methods for image classification, our Multi-Level and Multi-Prompt
(MLMP) entropy minimization integrates features from intermediate
vision-encoder layers and is performed with different text-prompt templates at
both the global CLS token and local pixel-wise levels. Our approach could be
used as plug-and-play for any segmentation network, does not require additional
training data or labels, and remains effective even with a single test sample.
Furthermore, we introduce a comprehensive OVSS TTA benchmark suite, which
integrates a rigorous evaluation protocol, seven segmentation datasets, and 15
common corruptions, with a total of 82 distinct test scenarios, establishing a
standardized and comprehensive testbed for future TTA research in
open-vocabulary segmentation. Our experiments on this suite demonstrate that
our segmentation-tailored method consistently delivers significant gains over
direct adoption of TTA classification baselines.

</details>


### [257] [RePaViT: Scalable Vision Transformer Acceleration via Structural Reparameterization on Feedforward Network Layers](https://arxiv.org/pdf/2505.21847)
*Xuwei Xu, Yang Li, Yudong Chen, Jiajun Liu, Sen Wang*

Main category: cs.CV

TL;DR: The paper identifies FFN layers as the main bottleneck in ViT inference latency and proposes a channel idle mechanism for efficient structural reparameterization, achieving significant speed-ups with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of FFN layers in Vision Transformers (ViTs) and optimize large-scale ViT performance by focusing on FFN layer optimization.

Method: Introduces a channel idle mechanism for post-training structural reparameterization, creating linear pathways in FFN layers to reduce latency.

Result: RePaViTs achieve up to 68.7% speed-up with accuracy improvements (e.g., +1.1% for RePa-ViT-Huge).

Conclusion: The method successfully optimizes ViT efficiency via FFN layer reparameterization, offering a promising direction for future work.

Abstract: We reveal that feedforward network (FFN) layers, rather than attention
layers, are the primary contributors to Vision Transformer (ViT) inference
latency, with their impact signifying as model size increases. This finding
highlights a critical opportunity for optimizing the efficiency of large-scale
ViTs by focusing on FFN layers. In this work, we propose a novel channel idle
mechanism that facilitates post-training structural reparameterization for
efficient FFN layers during testing. Specifically, a set of feature channels
remains idle and bypasses the nonlinear activation function in each FFN layer,
thereby forming a linear pathway that enables structural reparameterization
during inference. This mechanism results in a family of ReParameterizable
Vision Transformers (RePaViTs), which achieve remarkable latency reductions
with acceptable sacrifices (sometimes gains) in accuracy across various ViTs.
The benefits of our method scale consistently with model sizes, demonstrating
greater speed improvements and progressively narrowing accuracy gaps or even
higher accuracies on larger models. In particular, RePa-ViT-Large and
RePa-ViT-Huge enjoy 66.8% and 68.7% speed-ups with +1.7% and +1.1% higher top-1
accuracies under the same training strategy, respectively. RePaViT is the first
to employ structural reparameterization on FFN layers to expedite ViTs to our
best knowledge, and we believe that it represents an auspicious direction for
efficient ViTs. Source code is available at
https://github.com/Ackesnal/RePaViT.

</details>


### [258] [FPAN: Mitigating Replication in Diffusion Models through the Fine-Grained Probabilistic Addition of Noise to Token Embeddings](https://arxiv.org/pdf/2505.21848)
*Jingqi Xu, Chenghao Li, Yuke Zhang, Peter A. Beerel*

Main category: cs.CV

TL;DR: The paper proposes FPAN, a fine-grained noise injection technique for diffusion models to reduce data replication, improving privacy without compromising image quality.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns in diffusion models, which often replicate sensitive training data.

Method: Introduces FPAN, a probabilistic noise addition method to token embeddings, varying noise amounts.

Result: FPAN reduces replication by 28.78% vs. baseline and 26.51% vs. prior methods, with minimal quality loss.

Conclusion: FPAN effectively mitigates replication and can be combined with other methods for further improvement.

Abstract: Diffusion models have demonstrated remarkable potential in generating
high-quality images. However, their tendency to replicate training data raises
serious privacy concerns, particularly when the training datasets contain
sensitive or private information. Existing mitigation strategies primarily
focus on reducing image duplication, modifying the cross-attention mechanism,
and altering the denoising backbone architecture of diffusion models. Moreover,
recent work has shown that adding a consistent small amount of noise to text
embeddings can reduce replication to some degree. In this work, we begin by
analyzing the impact of adding varying amounts of noise. Based on our analysis,
we propose a fine-grained noise injection technique that probabilistically adds
a larger amount of noise to token embeddings. We refer to our method as
Fine-grained Probabilistic Addition of Noise (FPAN). Through our extensive
experiments, we show that our proposed FPAN can reduce replication by an
average of 28.78% compared to the baseline diffusion model without
significantly impacting image quality, and outperforms the prior
consistent-magnitude-noise-addition approach by 26.51%. Moreover, when combined
with other existing mitigation methods, our FPAN approach can further reduce
replication by up to 16.82% with similar, if not improved, image quality.

</details>


### [259] [Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage Task](https://arxiv.org/pdf/2505.21850)
*Yanbei Jiang, Yihao Ding, Chao Lei, Jiayang Ao, Jey Han Lau, Krista A. Ehinger*

Main category: cs.CV

TL;DR: The paper introduces MultiStAR, a benchmark for assessing Multi-Stage Abstract Visual Reasoning (AVR) in MLLMs, and MSEval, a metric evaluating intermediate reasoning steps, revealing MLLMs' limitations in complex rule detection.


<details>
  <summary>Details</summary>
Motivation: Existing AVR benchmarks and metrics overlook multi-stage reasoning and intermediate step correctness, limiting understanding of MLLMs' failures in abstract reasoning.

Method: Developed MultiStAR based on RAVEN for multi-stage AVR evaluation and proposed MSEval to assess intermediate reasoning correctness. Tested 17 MLLMs.

Result: MLLMs perform well on basic perception but struggle with complex rule detection, highlighting gaps in higher-order reasoning.

Conclusion: MultiStAR and MSEval address limitations in current AVR evaluation, revealing MLLMs' need for improved abstract reasoning capabilities.

Abstract: Current Multimodal Large Language Models (MLLMs) excel in general visual
reasoning but remain underexplored in Abstract Visual Reasoning (AVR), which
demands higher-order reasoning to identify abstract rules beyond simple
perception. Existing AVR benchmarks focus on single-step reasoning, emphasizing
the end result but neglecting the multi-stage nature of reasoning process. Past
studies found MLLMs struggle with these benchmarks, but it doesn't explain how
they fail. To address this gap, we introduce MultiStAR, a Multi-Stage AVR
benchmark, based on RAVEN, designed to assess reasoning across varying levels
of complexity. Additionally, existing metrics like accuracy only focus on the
final outcomes while do not account for the correctness of intermediate steps.
Therefore, we propose a novel metric, MSEval, which considers the correctness
of intermediate steps in addition to the final outcomes. We conduct
comprehensive experiments on MultiStAR using 17 representative close-source and
open-source MLLMs. The results reveal that while existing MLLMs perform
adequately on basic perception tasks, they continue to face challenges in more
complex rule detection stages.

</details>


### [260] [Rethinking Gradient-based Adversarial Attacks on Point Cloud Classification](https://arxiv.org/pdf/2505.21854)
*Jun Chen, Xinke Li, Mingyue Xu, Tianrui Li, Chongshou Li*

Main category: cs.CV

TL;DR: The paper introduces WAAttack and SubAttack, two gradient-based adversarial attack strategies for 3D point clouds, improving effectiveness and imperceptibility by addressing non-uniform point contributions and focusing on critical regions.


<details>
  <summary>Details</summary>
Motivation: Existing gradient-based attacks on point clouds use uniform update rules, leading to excessive and perceptible perturbations. The paper aims to address this by considering the heterogeneous nature of point clouds.

Method: Proposes WAAttack (weighted gradients and adaptive step-size) and SubAttack (decomposing point clouds into critical subsets) for targeted perturbations.

Result: The methods outperform state-of-the-art baselines in generating imperceptible adversarial examples.

Conclusion: The paper presents a principled redesign of gradient-based attacks for 3D point clouds, enhancing both effectiveness and subtlety.

Abstract: Gradient-based adversarial attacks have become a dominant approach for
evaluating the robustness of point cloud classification models. However,
existing methods often rely on uniform update rules that fail to consider the
heterogeneous nature of point clouds, resulting in excessive and perceptible
perturbations. In this paper, we rethink the design of gradient-based attacks
by analyzing the limitations of conventional gradient update mechanisms and
propose two new strategies to improve both attack effectiveness and
imperceptibility. First, we introduce WAAttack, a novel framework that
incorporates weighted gradients and an adaptive step-size strategy to account
for the non-uniform contribution of points during optimization. This approach
enables more targeted and subtle perturbations by dynamically adjusting updates
according to the local structure and sensitivity of each point. Second, we
propose SubAttack, a complementary strategy that decomposes the point cloud
into subsets and focuses perturbation efforts on structurally critical regions.
Together, these methods represent a principled rethinking of gradient-based
adversarial attacks for 3D point cloud classification. Extensive experiments
demonstrate that our approach outperforms state-of-the-art baselines in
generating highly imperceptible adversarial examples. Code will be released
upon paper acceptance.

</details>


### [261] [Towards Scalable Language-Image Pre-training for 3D Medical Imaging](https://arxiv.org/pdf/2505.21862)
*Chenhui Zhao, Yiwei Lyu, Asadur Chowdury, Edward Harake, Akhil Kondepudi, Akshay Rao, Xinhai Hou, Honglak Lee, Todd Hollon*

Main category: cs.CV

TL;DR: HLIP introduces a scalable pre-training framework for 3D medical imaging using hierarchical attention, achieving state-of-the-art performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Overcoming computational challenges in 3D medical imaging pre-training to enable large-scale, uncurated clinical dataset usage.

Method: HLIP uses a lightweight hierarchical attention mechanism (slice, scan, study) for efficient pre-training on volumetric data.

Result: +4.3% macro AUC on Rad-ChestCT, +32.4% balanced ACC on Pub-Brain-5, and improved AUC on RSNA and CQ500 benchmarks.

Conclusion: HLIP demonstrates scalable and effective language-image pre-training for 3D medical imaging, enabling direct training on uncurated datasets.

Abstract: Language-image pre-training has demonstrated strong performance in 2D medical
imaging, but its success in 3D modalities such as CT and MRI remains limited
due to the high computational demands of volumetric data, which pose a
significant barrier to training on large-scale, uncurated clinical studies. In
this study, we introduce Hierarchical attention for Language-Image Pre-training
(HLIP), a scalable pre-training framework for 3D medical imaging. HLIP adopts a
lightweight hierarchical attention mechanism inspired by the natural hierarchy
of radiology data: slice, scan, and study. This mechanism exhibits strong
generalizability, e.g., +4.3% macro AUC on the Rad-ChestCT benchmark when
pre-trained on CT-RATE. Moreover, the computational efficiency of HLIP enables
direct training on uncurated datasets. Trained on 220K patients with 3.13
million scans for brain MRI and 240K patients with 1.44 million scans for head
CT, HLIP achieves state-of-the-art performance, e.g., +32.4% balanced ACC on
the proposed publicly available brain MRI benchmark Pub-Brain-5; +1.4% and
+6.9% macro AUC on head CT benchmarks RSNA and CQ500, respectively. These
results demonstrate that, with HLIP, directly pre-training on uncurated
clinical datasets is a scalable and effective direction for language-image
pre-training in 3D medical imaging. The code is available at
https://github.com/Zch0414/hlip

</details>


### [262] [GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning](https://arxiv.org/pdf/2505.21863)
*Shikhhar Siingh, Abhinav Rawat, Vivek Gupta, Chitta Baral*

Main category: cs.CV

TL;DR: GETReason framework enhances image understanding by inferring deeper contextual meaning through geospatial, temporal, and event data, validated by the GREAT metric.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to accurately extract contextual relevance from publicly significant images, limiting their utility in journalism and education.

Method: Introduces GETReason, a framework for deeper contextual inference, and GREAT, a new evaluation metric. Uses a layered multi-agent approach.

Result: Demonstrates effective linking of images to broader event contexts using reasoning-weighted metrics.

Conclusion: GETReason and GREAT improve image understanding, offering meaningful insights for journalism and education.

Abstract: Publicly significant images from events hold valuable contextual information,
crucial for journalism and education. However, existing methods often struggle
to extract this relevance accurately. To address this, we introduce GETReason
(Geospatial Event Temporal Reasoning), a framework that moves beyond
surface-level image descriptions to infer deeper contextual meaning. We propose
that extracting global event, temporal, and geospatial information enhances
understanding of an image's significance. Additionally, we introduce GREAT
(Geospatial Reasoning and Event Accuracy with Temporal Alignment), a new metric
for evaluating reasoning-based image understanding. Our layered multi-agent
approach, assessed using a reasoning-weighted metric, demonstrates that
meaningful insights can be inferred, effectively linking images to their
broader event context.

</details>


### [263] [Cross-DINO: Cross the Deep MLP and Transformer for Small Object Detection](https://arxiv.org/pdf/2505.21868)
*Guiping Cao, Wenjian Huang, Xiangyuan Lan, Jianguo Zhang, Dongmei Jiang, Yaowei Wang*

Main category: cs.CV

TL;DR: Cross-DINO improves small object detection (SOD) in DETR-like models by integrating deep MLP networks and a Cross Coding Twice Module (CCTM) for better feature representation, along with a new Boost Loss function using Category-Size labels.


<details>
  <summary>Details</summary>
Motivation: Existing Transformer-based detectors struggle with SOD due to poor contextual information capture, feature blurring, and low class prediction scores for small objects.

Method: Proposes Cross-DINO, combining deep MLP networks for feature aggregation, CCTM for enhanced small object details, and Boost Loss for improved class prediction.

Result: Achieves 36.4% APs on COCO for SOD, outperforming DINO by +4.4% with fewer parameters and FLOPs.

Conclusion: Cross-DINO effectively enhances SOD performance in DETR-like models, demonstrating superior results across multiple datasets.

Abstract: Small Object Detection (SOD) poses significant challenges due to limited
information and the model's low class prediction score. While Transformer-based
detectors have shown promising performance, their potential for SOD remains
largely unexplored. In typical DETR-like frameworks, the CNN backbone network,
specialized in aggregating local information, struggles to capture the
necessary contextual information for SOD. The multiple attention layers in the
Transformer Encoder face difficulties in effectively attending to small objects
and can also lead to blurring of features. Furthermore, the model's lower class
prediction score of small objects compared to large objects further increases
the difficulty of SOD. To address these challenges, we introduce a novel
approach called Cross-DINO. This approach incorporates the deep MLP network to
aggregate initial feature representations with both short and long range
information for SOD. Then, a new Cross Coding Twice Module (CCTM) is applied to
integrate these initial representations to the Transformer Encoder feature,
enhancing the details of small objects. Additionally, we introduce a new kind
of soft label named Category-Size (CS), integrating the Category and Size of
objects. By treating CS as new ground truth, we propose a new loss function
called Boost Loss to improve the class prediction score of the model. Extensive
experimental results on COCO, WiderPerson, VisDrone, AI-TOD, and SODA-D
datasets demonstrate that Cross-DINO efficiently improves the performance of
DETR-like models on SOD. Specifically, our model achieves 36.4% APs on COCO for
SOD with only 45M parameters, outperforming the DINO by +4.4% APS (36.4% vs.
32.0%) with fewer parameters and FLOPs, under 12 epochs training setting. The
source codes will be available at https://github.com/Med-Process/Cross-DINO.

</details>


### [264] [EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video Guidance](https://arxiv.org/pdf/2505.21876)
*Zun Wang, Jaemin Cho, Jialu Li, Han Lin, Jaehong Yoon, Yue Zhang, Mohit Bansal*

Main category: cs.CV

TL;DR: EPiC introduces an efficient framework for 3D camera control in video diffusion models, eliminating the need for costly camera trajectory annotations by using masking-based anchor videos and a lightweight ControlNet module.


<details>
  <summary>Details</summary>
Motivation: To address inaccuracies in anchor videos from point cloud estimation and reduce resource demands of camera trajectory annotations.

Method: EPiC constructs high-quality anchor videos by masking source videos based on first-frame visibility and integrates Anchor-ControlNet for guidance.

Result: Achieves state-of-the-art performance on RealEstate10K and MiraData, with robust generalization to point cloud-based anchor videos and zero-shot video-to-video scenarios.

Conclusion: EPiC offers precise, efficient, and resource-friendly 3D camera control without modifying the diffusion model backbone.

Abstract: Recent approaches on 3D camera control in video diffusion models (VDMs) often
create anchor videos to guide diffusion models as a structured prior by
rendering from estimated point clouds following annotated camera trajectories.
However, errors inherent in point cloud estimation often lead to inaccurate
anchor videos. Moreover, the requirement for extensive camera trajectory
annotations further increases resource demands. To address these limitations,
we introduce EPiC, an efficient and precise camera control learning framework
that automatically constructs high-quality anchor videos without expensive
camera trajectory annotations. Concretely, we create highly precise anchor
videos for training by masking source videos based on first-frame visibility.
This approach ensures high alignment, eliminates the need for camera trajectory
annotations, and thus can be readily applied to any in-the-wild video to
generate image-to-video (I2V) training pairs. Furthermore, we introduce
Anchor-ControlNet, a lightweight conditioning module that integrates anchor
video guidance in visible regions to pretrained VDMs, with less than 1% of
backbone model parameters. By combining the proposed anchor video data and
ControlNet module, EPiC achieves efficient training with substantially fewer
parameters, training steps, and less data, without requiring modifications to
the diffusion model backbone typically needed to mitigate rendering
misalignments. Although being trained on masking-based anchor videos, our
method generalizes robustly to anchor videos made with point clouds during
inference, enabling precise 3D-informed camera control. EPiC achieves SOTA
performance on RealEstate10K and MiraData for I2V camera control task,
demonstrating precise and robust camera control ability both quantitatively and
qualitatively. Notably, EPiC also exhibits strong zero-shot generalization to
video-to-video scenarios.

</details>


### [265] [Hyperspectral Gaussian Splatting](https://arxiv.org/pdf/2505.21890)
*Sunil Kumar Narayanan, Lingjun Zhao, Lu Gan, Yongsheng Chen*

Main category: cs.CV

TL;DR: HS-GS combines 3D Gaussian Splatting with a diffusion model for hyperspectral scene reconstruction, improving speed and accuracy over NeRF.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of NeRF in training time and rendering speed for hyperspectral imaging in agriculture.

Method: Uses 3D Gaussian Splatting with a diffusion model, wavelength encoder, and KL-divergence loss for denoising and spectral accuracy.

Result: Achieves state-of-the-art performance on hyperspectral scene reconstruction and novel view synthesis.

Conclusion: HS-GS is a superior alternative to NeRF for hyperspectral imaging, offering faster and more accurate results.

Abstract: Hyperspectral imaging (HSI) has been widely used in agricultural applications
for non-destructive estimation of plant nutrient composition and precise
determination of nutritional elements in samples. Recently, 3D reconstruction
methods have been used to create implicit neural representations of HSI scenes,
which can help localize the target object's nutrient composition spatially and
spectrally. Neural Radiance Field (NeRF) is a cutting-edge implicit
representation that can render hyperspectral channel compositions of each
spatial location from any viewing direction. However, it faces limitations in
training time and rendering speed. In this paper, we propose Hyperspectral
Gaussian Splatting (HS-GS), which combines the state-of-the-art 3D Gaussian
Splatting (3DGS) with a diffusion model to enable 3D explicit reconstruction of
the hyperspectral scenes and novel view synthesis for the entire spectral
range. To enhance the model's ability to capture fine-grained reflectance
variations across the light spectrum and leverage correlations between adjacent
wavelengths for denoising, we introduce a wavelength encoder to generate
wavelength-specific spherical harmonics offsets. We also introduce a novel
Kullback--Leibler divergence-based loss to mitigate the spectral distribution
gap between the rendered image and the ground truth. A diffusion model is
further applied for denoising the rendered images and generating photorealistic
hyperspectral images. We present extensive evaluations on five diverse
hyperspectral scenes from the Hyper-NeRF dataset to show the effectiveness of
our proposed HS-GS framework. The results demonstrate that HS-GS achieves new
state-of-the-art performance among all previously published methods. Code will
be released upon publication.

</details>


### [266] [Concentrate on Weakness: Mining Hard Prototypes for Few-Shot Medical Image Segmentation](https://arxiv.org/pdf/2505.21897)
*Jianchao Jiang, Haofeng Zhang*

Main category: cs.CV

TL;DR: The paper proposes a novel method for Few-Shot Medical Image Segmentation (FSMIS) by focusing on weak features for clearer boundaries, using modules like SSP, HPG, and MSMF, and achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing prototype-based FSMIS methods suffer from boundary blurring due to overemphasis on normal features, ignoring weaker but crucial features for segmentation.

Method: The method includes a Support Self-Prediction (SSP) module to identify weak features, a Hard Prototypes Generation (HPG) module to create prototypes from these features, and a Multiple Similarity Maps Fusion (MSMF) module for balanced segmentation. A boundary loss is also introduced.

Result: Experiments on three medical image datasets show state-of-the-art performance.

Conclusion: The proposed approach effectively addresses boundary blurring in FSMIS by leveraging weak features and achieves superior segmentation results.

Abstract: Few-Shot Medical Image Segmentation (FSMIS) has been widely used to train a
model that can perform segmentation from only a few annotated images. However,
most existing prototype-based FSMIS methods generate multiple prototypes from
the support image solely by random sampling or local averaging, which can cause
particularly severe boundary blurring due to the tendency for normal features
accounting for the majority of features of a specific category. Consequently,
we propose to focus more attention to those weaker features that are crucial
for clear segmentation boundary. Specifically, we design a Support
Self-Prediction (SSP) module to identify such weak features by comparing true
support mask with one predicted by global support prototype. Then, a Hard
Prototypes Generation (HPG) module is employed to generate multiple hard
prototypes based on these weak features. Subsequently, a Multiple Similarity
Maps Fusion (MSMF) module is devised to generate final segmenting mask in a
dual-path fashion to mitigate the imbalance between foreground and background
in medical images. Furthermore, we introduce a boundary loss to further
constraint the edge of segmentation. Extensive experiments on three publicly
available medical image datasets demonstrate that our method achieves
state-of-the-art performance. Code is available at
https://github.com/jcjiang99/CoW.

</details>


### [267] [CAST: Contrastive Adaptation and Distillation for Semi-Supervised Instance Segmentation](https://arxiv.org/pdf/2505.21904)
*Pardis Taghavi, Tian Liu, Renjie Li, Reza Langari, Zhengzhong Tu*

Main category: cs.CV

TL;DR: CAST is a semi-supervised knowledge distillation framework that compresses large vision foundation models into smaller experts using limited labeled and abundant unlabeled data, achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: Instance segmentation requires expensive annotations and large models, prompting the need for efficient, smaller models leveraging unlabeled data.

Method: CAST involves domain adaptation via self-training, distillation with a multi-objective loss, and fine-tuning. Key is an instance-aware pixel-wise contrastive loss.

Result: The smaller student model outperforms its teacher by +3.4 AP on Cityscapes and +1.5 AP on ADE20K, surpassing state-of-the-art semi-supervised methods.

Conclusion: CAST effectively leverages unlabeled data and contrastive learning to create compact, high-performing instance segmentation models.

Abstract: Instance segmentation demands costly per-pixel annotations and large models.
We introduce CAST, a semi-supervised knowledge distillation (SSKD) framework
that compresses pretrained vision foundation models (VFM) into compact experts
using limited labeled and abundant unlabeled data. CAST unfolds in three
stages: (1) domain adaptation of the VFM teacher(s) via self-training with
contrastive pixel calibration, (2) distillation into a compact student via a
unified multi-objective loss that couples standard supervision and
pseudo-labels with our instance-aware pixel-wise contrastive term, and (3)
fine-tuning on labeled data to remove residual pseudo-label bias. Central to
CAST is an \emph{instance-aware pixel-wise contrastive loss} that fuses mask
and class scores to mine informative negatives and enforce clear inter-instance
margins. By maintaining this contrastive signal across both adaptation and
distillation, we align teacher and student embeddings and fully leverage
unlabeled images. On Cityscapes and ADE20K, our ~11X smaller student surpasses
its adapted VFM teacher(s) by +3.4 AP (33.9 vs. 30.5) and +1.5 AP (16.7 vs.
15.2) and outperforms state-of-the-art semi-supervised approaches.

</details>


### [268] [AlignGen: Boosting Personalized Image Generation with Cross-Modality Prior Alignment](https://arxiv.org/pdf/2505.21911)
*Yiheng Lin, Shifang Zhao, Ting Liu, Xiaochao Qu, Luoqi Liu, Yao Zhao, Yunchao Wei*

Main category: cs.CV

TL;DR: AlignGen improves personalized image generation by aligning textual and visual priors using a learnable token, robust training, and selective cross-modal attention.


<details>
  <summary>Details</summary>
Motivation: Addressing the bias toward textual prior in misaligned prompt-reference scenarios, which causes loss of reference content.

Method: Proposes AlignGen with a learnable token, robust training, and selective cross-modal attention mask.

Result: Outperforms zero-shot methods and test-time optimization approaches.

Conclusion: AlignGen effectively aligns cross-modality priors for better personalized image generation.

Abstract: Personalized image generation aims to integrate user-provided concepts into
text-to-image models, enabling the generation of customized content based on a
given prompt. Recent zero-shot approaches, particularly those leveraging
diffusion transformers, incorporate reference image information through
multi-modal attention mechanism. This integration allows the generated output
to be influenced by both the textual prior from the prompt and the visual prior
from the reference image. However, we observe that when the prompt and
reference image are misaligned, the generated results exhibit a stronger bias
toward the textual prior, leading to a significant loss of reference content.
To address this issue, we propose AlignGen, a Cross-Modality Prior Alignment
mechanism that enhances personalized image generation by: 1) introducing a
learnable token to bridge the gap between the textual and visual priors, 2)
incorporating a robust training strategy to ensure proper prior alignment, and
3) employing a selective cross-modal attention mask within the multi-modal
attention mechanism to further align the priors. Experimental results
demonstrate that AlignGen outperforms existing zero-shot methods and even
surpasses popular test-time optimization approaches.

</details>


### [269] [LiDARDustX: A LiDAR Dataset for Dusty Unstructured Road Environments](https://arxiv.org/pdf/2505.21914)
*Chenfeng Wei, Qi Wu, Si Zuo, Jiahua Xu, Boyang Zhao, Zeyu Yang, Guotao Xie, Shenhong Wang*

Main category: cs.CV

TL;DR: The LiDARDustX dataset addresses the lack of autonomous driving datasets for high-dust conditions, providing 30,000 LiDAR frames with annotations to benchmark 3D perception algorithms.


<details>
  <summary>Details</summary>
Motivation: Existing datasets focus on urban environments, neglecting unstructured scenarios like high-dust areas (e.g., mining), limiting algorithm validation.

Method: The dataset includes 30,000 LiDAR frames from six sensors, with 3D bounding boxes and semantic segmentation, 80% of which are dust-affected scenes.

Result: A benchmark for 3D detection and segmentation algorithms was created, and the impact of dust on perception accuracy was analyzed.

Conclusion: LiDARDustX fills a critical gap in autonomous driving datasets, enabling better evaluation of algorithms in dust-heavy environments.

Abstract: Autonomous driving datasets are essential for validating the progress of
intelligent vehicle algorithms, which include localization, perception, and
prediction. However, existing datasets are predominantly focused on structured
urban environments, which limits the exploration of unstructured and
specialized scenarios, particularly those characterized by significant dust
levels. This paper introduces the LiDARDustX dataset, which is specifically
designed for perception tasks under high-dust conditions, such as those
encountered in mining areas. The LiDARDustX dataset consists of 30,000 LiDAR
frames captured by six different LiDAR sensors, each accompanied by 3D bounding
box annotations and point cloud semantic segmentation. Notably, over 80% of the
dataset comprises dust-affected scenes. By utilizing this dataset, we have
established a benchmark for evaluating the performance of state-of-the-art 3D
detection and segmentation algorithms. Additionally, we have analyzed the
impact of dust on perception accuracy and delved into the causes of these
effects. The data and further information can be accessed at:
https://github.com/vincentweikey/LiDARDustX.

</details>


### [270] [BD Open LULC Map: High-resolution land use land cover mapping & benchmarking for urban development in Dhaka, Bangladesh](https://arxiv.org/pdf/2505.21915)
*Mir Sazzat Hossain, Ovi Paul, Md Akil Raihan Iftee, Rakibul Hasan Rajib, Abu Bakar Siddik Nayem, Anis Sarker, Arshad Momen, Md. Ashraful Amin, Amin Ahsan Ali, AKM Mahbubur Rahman*

Main category: cs.CV

TL;DR: The paper introduces BD Open LULC Map (BOLM), a dataset for LULC mapping in Dhaka, addressing data scarcity in South/East Asia. It benchmarks DeepLab V3+ for segmentation.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity of annotated satellite data in developing regions, particularly South/East Asia, to improve LULC classification.

Method: Creation of BOLM dataset with pixel-wise annotations for 11 LULC classes, validated by GIS experts. Benchmarking using DeepLab V3+ on Bing and Sentinel-2A imagery.

Result: BOLM covers 4,392 sq km with validated ground truth, enabling reliable LULC segmentation and domain adaptation.

Conclusion: BOLM fills a critical gap in LULC datasets for South/East Asia, supporting deep learning models and adaptation tasks.

Abstract: Land Use Land Cover (LULC) mapping using deep learning significantly enhances
the reliability of LULC classification, aiding in understanding geography,
socioeconomic conditions, poverty levels, and urban sprawl. However, the
scarcity of annotated satellite data, especially in South/East Asian developing
countries, poses a major challenge due to limited funding, diverse
infrastructures, and dense populations. In this work, we introduce the BD Open
LULC Map (BOLM), providing pixel-wise LULC annotations across eleven classes
(e.g., Farmland, Water, Forest, Urban Structure, Rural Built-Up) for Dhaka
metropolitan city and its surroundings using high-resolution Bing satellite
imagery (2.22 m/pixel). BOLM spans 4,392 sq km (891 million pixels), with
ground truth validated through a three-stage process involving GIS experts. We
benchmark LULC segmentation using DeepLab V3+ across five major classes and
compare performance on Bing and Sentinel-2A imagery. BOLM aims to support
reliable deep models and domain adaptation tasks, addressing critical LULC
dataset gaps in South/East Asia.

</details>


### [271] [InfoSAM: Fine-Tuning the Segment Anything Model from An Information-Theoretic Perspective](https://arxiv.org/pdf/2505.21920)
*Yuanhong Zhang, Muyao Yuan, Weizhan Zhang, Tieliang Gong, Wen Wen, Jiangyong Ying, Weijie Shi*

Main category: cs.CV

TL;DR: InfoSAM enhances SAM fine-tuning by preserving pre-trained knowledge using mutual information objectives, improving performance in specialized domains.


<details>
  <summary>Details</summary>
Motivation: SAM struggles in specialized domains despite strong zero-shot capabilities. Existing PEFT methods ignore domain-invariant relations, limiting SAM's potential.

Method: InfoSAM uses mutual information objectives to distill and preserve pre-trained segmentation knowledge, excluding pseudo-invariant information.

Result: Extensive experiments show InfoSAM improves SAM's performance in specialized scenarios, validating its adaptability and superiority.

Conclusion: InfoSAM provides a robust PEFT framework for SAM, enhancing its effectiveness in real-world specialized tasks.

Abstract: The Segment Anything Model (SAM), a vision foundation model, exhibits
impressive zero-shot capabilities in general tasks but struggles in specialized
domains. Parameter-efficient fine-tuning (PEFT) is a promising approach to
unleash the potential of SAM in novel scenarios. However, existing PEFT methods
for SAM neglect the domain-invariant relations encoded in the pre-trained
model. To bridge this gap, we propose InfoSAM, an information-theoretic
approach that enhances SAM fine-tuning by distilling and preserving its
pre-trained segmentation knowledge. Specifically, we formulate the knowledge
transfer process as two novel mutual information-based objectives: (i) to
compress the domain-invariant relation extracted from pre-trained SAM,
excluding pseudo-invariant information as possible, and (ii) to maximize mutual
information between the relational knowledge learned by the teacher
(pre-trained SAM) and the student (fine-tuned model). The proposed InfoSAM
establishes a robust distillation framework for PEFT of SAM. Extensive
experiments across diverse benchmarks validate InfoSAM's effectiveness in
improving SAM family's performance on real-world tasks, demonstrating its
adaptability and superiority in handling specialized scenarios.

</details>


### [272] [UniTalk: Towards Universal Active Speaker Detection in Real World Scenarios](https://arxiv.org/pdf/2505.21954)
*Le Thien Phuc Nguyen, Zhuoran Yu, Khoa Quang Nhat Cao, Yuwei Guo, Tu Ho Manh Pham, Tuan Tai Nguyen, Toan Ngo Duc Vo, Lucas Poon, Soochahn Lee, Yong Jae Lee*

Main category: cs.CV

TL;DR: UniTalk is a new dataset for active speaker detection (ASD) focusing on challenging real-world scenarios like diverse languages, noisy backgrounds, and crowded scenes. It outperforms benchmarks like AVA in generalization but shows ASD is still unsolved under realistic conditions.


<details>
  <summary>Details</summary>
Motivation: Existing datasets like AVA lack diversity and real-world challenges, limiting model generalization. UniTalk aims to address this gap by focusing on difficult scenarios.

Method: UniTalk includes 44.5 hours of video with frame-level annotations across 48,693 speakers, covering diverse and noisy real-world conditions.

Result: State-of-the-art models perform poorly on UniTalk compared to AVA, indicating ASD is unsolved in realistic settings. UniTalk-trained models generalize better to other datasets.

Conclusion: UniTalk sets a new benchmark for ASD, offering a resource for developing robust models under real-world conditions.

Abstract: We present UniTalk, a novel dataset specifically designed for the task of
active speaker detection, emphasizing challenging scenarios to enhance model
generalization. Unlike previously established benchmarks such as AVA, which
predominantly features old movies and thus exhibits significant domain gaps,
UniTalk focuses explicitly on diverse and difficult real-world conditions.
These include underrepresented languages, noisy backgrounds, and crowded scenes
- such as multiple visible speakers speaking concurrently or in overlapping
turns. It contains over 44.5 hours of video with frame-level active speaker
annotations across 48,693 speaking identities, and spans a broad range of video
types that reflect real-world conditions. Through rigorous evaluation, we show
that state-of-the-art models, while achieving nearly perfect scores on AVA,
fail to reach saturation on UniTalk, suggesting that the ASD task remains far
from solved under realistic conditions. Nevertheless, models trained on UniTalk
demonstrate stronger generalization to modern "in-the-wild" datasets like
Talkies and ASW, as well as to AVA. UniTalk thus establishes a new benchmark
for active speaker detection, providing researchers with a valuable resource
for developing and evaluating versatile and resilient models.
  Dataset: https://huggingface.co/datasets/plnguyen2908/UniTalk-ASD
  Code: https://github.com/plnguyen2908/UniTalk-ASD-code

</details>


### [273] [Point-to-Region Loss for Semi-Supervised Point-Based Crowd Counting](https://arxiv.org/pdf/2505.21943)
*Wei Lin, Chenyang Zhao, Antoni B. Chan*

Main category: cs.CV

TL;DR: The paper proposes a semi-supervised framework for pedestrian counting using pseudo-labeling, replacing point-to-point (P2P) supervision with point-to-region (P2R) to address over-activation issues.


<details>
  <summary>Details</summary>
Motivation: Reduce annotation labor in point-based pedestrian counting by leveraging pseudo-labeled data, while addressing challenges in confidence propagation during training.

Method: Introduces a point-specific activation map (PSAM) to diagnose training issues and proposes P2R supervision to segment local regions instead of points.

Result: P2R resolves over-activation problems, improving performance in semi-supervised counting and unsupervised domain adaptation.

Conclusion: P2R effectively addresses the limitations of P2P, enhancing the robustness of semi-supervised pedestrian counting.

Abstract: Point detection has been developed to locate pedestrians in crowded scenes by
training a counter through a point-to-point (P2P) supervision scheme. Despite
its excellent localization and counting performance, training a point-based
counter still faces challenges concerning annotation labor: hundreds to
thousands of points are required to annotate a single sample capturing a dense
crowd. In this paper, we integrate point-based methods into a semi-supervised
counting framework based on pseudo-labeling, enabling the training of a counter
with only a few annotated samples supplemented by a large volume of
pseudo-labeled data. However, during implementation, the training encounters
issues as the confidence for pseudo-labels fails to be propagated to background
pixels via the P2P. To tackle this challenge, we devise a point-specific
activation map (PSAM) to visually interpret the phenomena occurring during the
ill-posed training. Observations from the PSAM suggest that the feature map is
excessively activated by the loss for unlabeled data, causing the decoder to
misinterpret these over-activations as pedestrians. To mitigate this issue, we
propose a point-to-region (P2R) scheme to substitute P2P, which segments out
local regions rather than detects a point corresponding to a pedestrian for
supervision. Consequently, pixels in the local region can share the same
confidence with the corresponding pseudo points. Experimental results in both
semi-supervised counting and unsupervised domain adaptation highlight the
advantages of our method, illustrating P2R can resolve issues identified in
PSAM. The code is available at https://github.com/Elin24/P2RLoss.

</details>


### [274] [Towards Comprehensive Scene Understanding: Integrating First and Third-Person Views for LVLMs](https://arxiv.org/pdf/2505.21955)
*Insu Lee, Wooje Park, Jaeyun Jang, Minyoung Noh, Kyuhong Shim, Byonghyo Shim*

Main category: cs.CV

TL;DR: The paper introduces a framework combining egocentric and exocentric views to enhance LVLMs' performance in multi-view reasoning, validated by the E3VQA benchmark and M3CoT prompting technique.


<details>
  <summary>Details</summary>
Motivation: Address failures in LVLMs due to narrow field of view and lack of global context in egocentric inputs by integrating exocentric views.

Method: Propose E3VQA benchmark with synchronized ego-exo image pairs and M3CoT, a training-free prompting technique integrating scene graphs from three perspectives.

Result: M3CoT improves LVLM performance (4.84% for GPT-4o, 5.94% for Gemini 2.0 Flash) over CoT baselines.

Conclusion: Combining egocentric and exocentric views enhances LVLM reasoning, with E3VQA and M3CoT providing valuable tools for multi-view tasks.

Abstract: Large vision-language models (LVLMs) are increasingly deployed in interactive
applications such as virtual and augmented reality, where first-person
(egocentric) view captured by head-mounted cameras serves as key input. While
this view offers fine-grained cues about user attention and hand-object
interactions, their narrow field of view and lack of global context often lead
to failures on spatially or contextually demanding queries. To address this, we
introduce a framework that augments egocentric inputs with third-person
(exocentric) views, providing complementary information such as global scene
layout and object visibility to LVLMs. We present E3VQA, the first benchmark
for multi-view question answering with 4K high-quality question-answer pairs
grounded in synchronized ego-exo image pairs. Additionally, we propose M3CoT, a
training-free prompting technique that constructs a unified scene
representation by integrating scene graphs from three complementary
perspectives. M3CoT enables LVLMs to reason more effectively across views,
yielding consistent performance gains (4.84% for GPT-4o and 5.94% for Gemini
2.0 Flash) over a recent CoT baseline. Our extensive evaluation reveals key
strengths and limitations of LVLMs in multi-view reasoning and highlights the
value of leveraging both egocentric and exocentric inputs.

</details>


### [275] [Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation](https://arxiv.org/pdf/2505.21956)
*Mengdan Zhu, Senhao Cheng, Guangji Bai, Yifei Zhang, Liang Zhao*

Main category: cs.CV

TL;DR: Cross-modal RAG improves text-to-image generation by decomposing queries and images for subquery-aware retrieval and synthesis, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Pretrained models lack domain-specific, fine-grained knowledge, and existing RAG methods fail for complex queries requiring multiple images.

Method: Proposes Cross-modal RAG with hybrid retrieval (sparse and dense) and subquery-aware generation using a multimodal LLM.

Result: Outperforms baselines in retrieval and generation quality on multiple datasets (MS-COCO, Flickr30K, etc.).

Conclusion: Cross-modal RAG effectively addresses complex queries by decomposing and selectively conditioning on visual features, ensuring high-quality synthesis.

Abstract: Text-to-image generation increasingly demands access to domain-specific,
fine-grained, and rapidly evolving knowledge that pretrained models cannot
fully capture. Existing Retrieval-Augmented Generation (RAG) methods attempt to
address this by retrieving globally relevant images, but they fail when no
single image contains all desired elements from a complex user query. We
propose Cross-modal RAG, a novel framework that decomposes both queries and
images into sub-dimensional components, enabling subquery-aware retrieval and
generation. Our method introduces a hybrid retrieval strategy - combining a
sub-dimensional sparse retriever with a dense retriever - to identify a
Pareto-optimal set of images, each contributing complementary aspects of the
query. During generation, a multimodal large language model is guided to
selectively condition on relevant visual features aligned to specific
subqueries, ensuring subquery-aware image synthesis. Extensive experiments on
MS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal
RAG significantly outperforms existing baselines in both retrieval and
generation quality, while maintaining high efficiency.

</details>


### [276] [One-Way Ticket:Time-Independent Unified Encoder for Distilling Text-to-Image Diffusion Models](https://arxiv.org/pdf/2505.21960)
*Senmao Li, Lei Wang, Kai Wang, Tao Liu, Jiehang Xie, Joost van de Weijer, Fahad Shahbaz Khan, Shiqi Yang, Yaxing Wang, Jian Yang*

Main category: cs.CV

TL;DR: TiUE introduces a Time-independent Unified Encoder for T2I diffusion models, reducing redundant computations and improving inference speed without sacrificing image quality or diversity.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between inference speed and image quality in T2I diffusion models by optimizing encoder-decoder computations.

Method: Proposes TiUE, a loop-free approach sharing encoder features across decoders, and adds a KL divergence term for noise prediction regularization.

Result: TiUE outperforms state-of-the-art methods like LCM and SD-Turbo, achieving faster inference with diverse, high-quality images.

Conclusion: TiUE effectively balances speed and quality in T2I diffusion models, offering a scalable solution for efficient deployment.

Abstract: Text-to-Image (T2I) diffusion models have made remarkable advancements in
generative modeling; however, they face a trade-off between inference speed and
image quality, posing challenges for efficient deployment. Existing distilled
T2I models can generate high-fidelity images with fewer sampling steps, but
often struggle with diversity and quality, especially in one-step models. From
our analysis, we observe redundant computations in the UNet encoders. Our
findings suggest that, for T2I diffusion models, decoders are more adept at
capturing richer and more explicit semantic information, while encoders can be
effectively shared across decoders from diverse time steps. Based on these
observations, we introduce the first Time-independent Unified Encoder TiUE for
the student model UNet architecture, which is a loop-free image generation
approach for distilling T2I diffusion models. Using a one-pass scheme, TiUE
shares encoder features across multiple decoder time steps, enabling parallel
sampling and significantly reducing inference time complexity. In addition, we
incorporate a KL divergence term to regularize noise prediction, which enhances
the perceptual realism and diversity of the generated images. Experimental
results demonstrate that TiUE outperforms state-of-the-art methods, including
LCM, SD-Turbo, and SwiftBrushv2, producing more diverse and realistic results
while maintaining the computational efficiency.

</details>


### [277] [A2Seek: Towards Reasoning-Centric Benchmark for Aerial Anomaly Understanding](https://arxiv.org/pdf/2505.21962)
*Mengjingcheng Mo, Xinyang Tong, Jiaxu Leng, Mingpi Tan, Jiankang Zheng, Yiran Liu, Haosheng Chen, Ji Gan, Weisheng Li, Xinbo Gao*

Main category: cs.CV

TL;DR: A2Seek is a new dataset and framework (A2Seek-R1) for aerial anomaly detection, addressing challenges like dynamic viewpoints and scale variations. It improves prediction accuracy and localization by 22.04% and 13.9%, respectively.


<details>
  <summary>Details</summary>
Motivation: Existing datasets and methods for anomaly detection are designed for fixed ground-level views, failing in drone-view scenarios due to dynamic viewpoints and complex scenes.

Method: A2Seek-R1 uses a graph-of-thought-guided fine-tuning approach and Aerial Group Relative Policy Optimization (A-GRPO) for reward functions. It also introduces a 'seeking' mechanism to simulate UAV flight behavior.

Result: A2Seek-R1 improves prediction accuracy by 22.04% (AP) and anomaly localization by 13.9% (mIoU), showing strong generalization.

Conclusion: The A2Seek dataset and A2Seek-R1 framework effectively address aerial anomaly detection challenges, offering significant performance improvements and adaptability to complex environments.

Abstract: While unmanned aerial vehicles (UAVs) offer wide-area, high-altitude coverage
for anomaly detection, they face challenges such as dynamic viewpoints, scale
variations, and complex scenes. Existing datasets and methods, mainly designed
for fixed ground-level views, struggle to adapt to these conditions, leading to
significant performance drops in drone-view scenarios. To bridge this gap, we
introduce A2Seek (Aerial Anomaly Seek), a large-scale, reasoning-centric
benchmark dataset for aerial anomaly understanding. This dataset covers various
scenarios and environmental conditions, providing high-resolution real-world
aerial videos with detailed annotations, including anomaly categories,
frame-level timestamps, region-level bounding boxes, and natural language
explanations for causal reasoning. Building on this dataset, we propose
A2Seek-R1, a novel reasoning framework that generalizes R1-style strategies to
aerial anomaly understanding, enabling a deeper understanding of "Where"
anomalies occur and "Why" they happen in aerial frames. To this end, A2Seek-R1
first employs a graph-of-thought (GoT)-guided supervised fine-tuning approach
to activate the model's latent reasoning capabilities on A2Seek. Then, we
introduce Aerial Group Relative Policy Optimization (A-GRPO) to design
rule-based reward functions tailored to aerial scenarios. Furthermore, we
propose a novel "seeking" mechanism that simulates UAV flight behavior by
directing the model's attention to informative regions. Extensive experiments
demonstrate that A2Seek-R1 achieves up to a 22.04% improvement in AP for
prediction accuracy and a 13.9% gain in mIoU for anomaly localization,
exhibiting strong generalization across complex environments and
out-of-distribution scenarios. Our dataset and code will be released at
https://hayneyday.github.io/A2Seek/.

</details>


### [278] [Learning World Models for Interactive Video Generation](https://arxiv.org/pdf/2505.21996)
*Taiye Chen, Xun Hu, Zihan Ding, Chi Jin*

Main category: cs.CV

TL;DR: The paper addresses challenges in video generation models by introducing VRAG to reduce compounding errors and improve spatiotemporal coherence.


<details>
  <summary>Details</summary>
Motivation: Current video generation models lack effective world modeling due to compounding errors and insufficient memory, limiting their interactive and coherent planning capabilities.

Method: The authors enhance image-to-video models with action conditioning and an autoregressive framework, proposing VRAG with global state conditioning to address these issues.

Result: VRAG significantly reduces compounding errors and improves spatiotemporal consistency, outperforming naive autoregressive and retrieval-augmented methods.

Conclusion: The work highlights key challenges in video world models and sets a benchmark for future improvements in video generation with better world modeling.

Abstract: Foundational world models must be both interactive and preserve
spatiotemporal coherence for effective future planning with action choices.
However, present models for long video generation have limited inherent world
modeling capabilities due to two main challenges: compounding errors and
insufficient memory mechanisms. We enhance image-to-video models with
interactive capabilities through additional action conditioning and
autoregressive framework, and reveal that compounding error is inherently
irreducible in autoregressive video generation, while insufficient memory
mechanism leads to incoherence of world models. We propose video retrieval
augmented generation (VRAG) with explicit global state conditioning, which
significantly reduces long-term compounding errors and increases spatiotemporal
consistency of world models. In contrast, naive autoregressive generation with
extended context windows and retrieval-augmented generation prove less
effective for video generation, primarily due to the limited in-context
learning capabilities of current video models. Our work illuminates the
fundamental challenges in video world models and establishes a comprehensive
benchmark for improving video generation models with internal world modeling
capabilities.

</details>


### [279] [DvD: Unleashing a Generative Paradigm for Document Dewarping via Coordinates-based Diffusion Model](https://arxiv.org/pdf/2505.21975)
*Weiguang Zhang, Huangcheng Lu, Maizhen Ning, Xiaowei Huang, Wei Wang, Kaizhu Huang, Qiufeng Wang*

Main category: cs.CV

TL;DR: DvD introduces a diffusion-based framework for document dewarping, using coordinate-level denoising and a time-variant condition refinement mechanism, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing document dewarping methods struggle to preserve document structures, and diffusion models offer potential but face challenges with complex images.

Method: DvD employs a coordinate-level denoising approach and a time-variant condition refinement mechanism to rectify deformations.

Result: DvD achieves state-of-the-art performance on benchmarks like DocUNet, DIR300, and the new AnyPhotoDoc6300.

Conclusion: DvD is a promising solution for document dewarping, and the new benchmark AnyPhotoDoc6300 enables comprehensive evaluation.

Abstract: Document dewarping aims to rectify deformations in photographic document
images, thus improving text readability, which has attracted much attention and
made great progress, but it is still challenging to preserve document
structures. Given recent advances in diffusion models, it is natural for us to
consider their potential applicability to document dewarping. However, it is
far from straightforward to adopt diffusion models in document dewarping due to
their unfaithful control on highly complex document images (e.g.,
2000$\times$3000 resolution). In this paper, we propose DvD, the first
generative model to tackle document \textbf{D}ewarping \textbf{v}ia a
\textbf{D}iffusion framework. To be specific, DvD introduces a coordinate-level
denoising instead of typical pixel-level denoising, generating a mapping for
deformation rectification. In addition, we further propose a time-variant
condition refinement mechanism to enhance the preservation of document
structures. In experiments, we find that current document dewarping benchmarks
can not evaluate dewarping models comprehensively. To this end, we present
AnyPhotoDoc6300, a rigorously designed large-scale document dewarping benchmark
comprising 6,300 real image pairs across three distinct domains, enabling
fine-grained evaluation of dewarping models. Comprehensive experiments
demonstrate that our proposed DvD can achieve state-of-the-art performance with
acceptable computational efficiency on multiple metrics across various
benchmarks including DocUNet, DIR300, and AnyPhotoDoc6300. The new benchmark
and code will be publicly available.

</details>


### [280] [GL-PGENet: A Parameterized Generation Framework for Robust Document Image Enhancement](https://arxiv.org/pdf/2505.22021)
*Zhihong Tang, Yang Li*

Main category: cs.CV

TL;DR: GL-PGENet is a novel architecture for multi-degraded color document image enhancement, combining global and local refinement with parametric generation, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods are limited to single-degradation restoration or grayscale images, lacking efficiency and robustness for real-world color document scenarios.

Method: GL-PGENet uses a hierarchical framework, Dual-Branch Local-Refine Network with parametric generation, and a modified NestUNet with dense blocks. Training involves large-scale pretraining and fine-tuning.

Result: Achieves SSIM scores of 0.7721 on DocUNet and 0.9480 on RealDAE, with strong cross-domain adaptability and computational efficiency.

Conclusion: GL-PGENet is effective, efficient, and robust for real-world document image enhancement, outperforming existing methods.

Abstract: Document Image Enhancement (DIE) serves as a critical component in Document
AI systems, where its performance substantially determines the effectiveness of
downstream tasks. To address the limitations of existing methods confined to
single-degradation restoration or grayscale image processing, we present Global
with Local Parametric Generation Enhancement Network (GL-PGENet), a novel
architecture designed for multi-degraded color document images, ensuring both
efficiency and robustness in real-world scenarios. Our solution incorporates
three key innovations: First, a hierarchical enhancement framework that
integrates global appearance correction with local refinement, enabling
coarse-to-fine quality improvement. Second, a Dual-Branch Local-Refine Network
with parametric generation mechanisms that replaces conventional direct
prediction, producing enhanced outputs through learned intermediate parametric
representations rather than pixel-wise mapping. This approach enhances local
consistency while improving model generalization. Finally, a modified NestUNet
architecture incorporating dense block to effectively fuse low-level pixel
features and high-level semantic features, specifically adapted for document
image characteristics. In addition, to enhance generalization performance, we
adopt a two-stage training strategy: large-scale pretraining on a synthetic
dataset of 500,000+ samples followed by task-specific fine-tuning. Extensive
experiments demonstrate the superiority of GL-PGENet, achieving
state-of-the-art SSIM scores of 0.7721 on DocUNet and 0.9480 on RealDAE. The
model also exhibits remarkable cross-domain adaptability and maintains
computational efficiency for high-resolution images without performance
degradation, confirming its practical utility in real-world scenarios.

</details>


### [281] [D-Fusion: Direct Preference Optimization for Aligning Diffusion Models with Visually Consistent Samples](https://arxiv.org/pdf/2505.22002)
*Zijing Hu, Fengda Zhang, Kun Kuang*

Main category: cs.CV

TL;DR: D-Fusion improves diffusion models by creating visually consistent samples for DPO training, enhancing prompt-image alignment.


<details>
  <summary>Details</summary>
Motivation: Addressing visual inconsistency in DPO training for diffusion models, which hinders alignment improvement.

Method: Uses mask-guided self-attention fusion to generate visually consistent and well-aligned images while retaining denoising trajectories.

Result: D-Fusion effectively improves prompt-image alignment across various reinforcement learning algorithms.

Conclusion: D-Fusion offers a practical solution to enhance diffusion models' alignment with text prompts.

Abstract: The practical applications of diffusion models have been limited by the
misalignment between generated images and corresponding text prompts. Recent
studies have introduced direct preference optimization (DPO) to enhance the
alignment of these models. However, the effectiveness of DPO is constrained by
the issue of visual inconsistency, where the significant visual disparity
between well-aligned and poorly-aligned images prevents diffusion models from
identifying which factors contribute positively to alignment during
fine-tuning. To address this issue, this paper introduces D-Fusion, a method to
construct DPO-trainable visually consistent samples. On one hand, by performing
mask-guided self-attention fusion, the resulting images are not only
well-aligned, but also visually consistent with given poorly-aligned images. On
the other hand, D-Fusion can retain the denoising trajectories of the resulting
images, which are essential for DPO training. Extensive experiments demonstrate
the effectiveness of D-Fusion in improving prompt-image alignment when applied
to different reinforcement learning algorithms.

</details>


### [282] [Balanced Token Pruning: Accelerating Vision Language Models Beyond Local Optimization](https://arxiv.org/pdf/2505.22038)
*Kaiyuan Li, Xiaoyue Chen, Chen Gao, Yong Li, Xinlei Chen*

Main category: cs.CV

TL;DR: Balanced Token Pruning (BTP) reduces computational overhead in LVLMs by optimizing token pruning, considering both local and global impacts, achieving high compression with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Existing token pruning methods in LVLMs overlook the joint impact on current and subsequent layers, leading to suboptimal decisions.

Method: BTP uses a calibration set to stage pruning, prioritizing global impact early and local consistency later.

Result: Achieves 78% compression while retaining 96.7% of original model performance.

Conclusion: BTP is an effective plug-and-play method for efficient token pruning in LVLMs.

Abstract: Large Vision-Language Models (LVLMs) have shown impressive performance across
multi-modal tasks by encoding images into thousands of tokens. However, the
large number of image tokens results in significant computational overhead, and
the use of dynamic high-resolution inputs further increases this burden.
Previous approaches have attempted to reduce the number of image tokens through
token pruning, typically by selecting tokens based on attention scores or image
token diversity. Through empirical studies, we observe that existing methods
often overlook the joint impact of pruning on both the current layer's output
(local) and the outputs of subsequent layers (global), leading to suboptimal
pruning decisions. To address this challenge, we propose Balanced Token Pruning
(BTP), a plug-and-play method for pruning vision tokens. Specifically, our
method utilizes a small calibration set to divide the pruning process into
multiple stages. In the early stages, our method emphasizes the impact of
pruning on subsequent layers, whereas in the deeper stages, the focus shifts
toward preserving the consistency of local outputs. Extensive experiments
across various LVLMs demonstrate the broad effectiveness of our approach on
multiple benchmarks. Our method achieves a 78% compression rate while
preserving 96.7% of the original models' performance on average.

</details>


### [283] [Event-based Egocentric Human Pose Estimation in Dynamic Environment](https://arxiv.org/pdf/2505.22007)
*Wataru Ikeda, Masashi Hatano, Ryosei Hara, Mariko Isogawa*

Main category: cs.CV

TL;DR: D-EventEgo is a framework for human pose estimation using a front-facing event-based camera, addressing challenges like low-light and motion blur. It estimates head poses first, then body poses, and includes a Motion Segmentation Module to improve accuracy by removing dynamic objects.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on RGB cameras and fail in low-light or motion-blur scenarios. Event-based cameras offer a solution, but no framework exists for front-facing egocentric pose estimation.

Method: D-EventEgo estimates head poses first, uses them to generate body poses, and employs a Motion Segmentation Module to filter out dynamic objects for better accuracy.

Result: The method outperforms baselines in four out of five metrics on a synthetic dataset derived from EgoBody, especially in dynamic environments.

Conclusion: D-EventEgo successfully addresses the challenges of event-based human pose estimation, demonstrating superior performance in dynamic settings.

Abstract: Estimating human pose using a front-facing egocentric camera is essential for
applications such as sports motion analysis, VR/AR, and AI for wearable
devices. However, many existing methods rely on RGB cameras and do not account
for low-light environments or motion blur. Event-based cameras have the
potential to address these challenges. In this work, we introduce a novel task
of human pose estimation using a front-facing event-based camera mounted on the
head and propose D-EventEgo, the first framework for this task. The proposed
method first estimates the head poses, and then these are used as conditions to
generate body poses. However, when estimating head poses, the presence of
dynamic objects mixed with background events may reduce head pose estimation
accuracy. Therefore, we introduce the Motion Segmentation Module to remove
dynamic objects and extract background information. Extensive experiments on
our synthetic event-based dataset derived from EgoBody, demonstrate that our
approach outperforms our baseline in four out of five evaluation metrics in
dynamic environments.

</details>


### [284] [From Failures to Fixes: LLM-Driven Scenario Repair for Self-Evolving Autonomous Driving](https://arxiv.org/pdf/2505.22067)
*Xinyu Xia, Xingjun Ma, Yunfeng Hu, Ting Qu, Hong Chen, Xun Gong*

Main category: cs.CV

TL;DR: SERA is an LLM-powered framework for autonomous driving that self-evolves by repairing failure cases through targeted scenario recommendation, improving performance with minimal data.


<details>
  <summary>Details</summary>
Motivation: Existing scenario generation methods lack adaptivity and semantic relevance, limiting performance improvement in autonomous driving.

Method: SERA analyzes performance logs, identifies failure patterns, retrieves semantically aligned scenarios, and uses LLM-based reflection for refined recommendations. Few-shot fine-tuning is applied for adaptation.

Result: SERA improves key metrics across multiple autonomous driving baselines, showing effectiveness and generalizability in safety-critical conditions.

Conclusion: SERA demonstrates a scalable and efficient approach to enhancing autonomous driving systems by leveraging targeted scenario repair.

Abstract: Ensuring robust and generalizable autonomous driving requires not only broad
scenario coverage but also efficient repair of failure cases, particularly
those related to challenging and safety-critical scenarios. However, existing
scenario generation and selection methods often lack adaptivity and semantic
relevance, limiting their impact on performance improvement. In this paper, we
propose \textbf{SERA}, an LLM-powered framework that enables autonomous driving
systems to self-evolve by repairing failure cases through targeted scenario
recommendation. By analyzing performance logs, SERA identifies failure patterns
and dynamically retrieves semantically aligned scenarios from a structured
bank. An LLM-based reflection mechanism further refines these recommendations
to maximize relevance and diversity. The selected scenarios are used for
few-shot fine-tuning, enabling targeted adaptation with minimal data.
Experiments on the benchmark show that SERA consistently improves key metrics
across multiple autonomous driving baselines, demonstrating its effectiveness
and generalizability under safety-critical conditions.

</details>


### [285] [Prototype Embedding Optimization for Human-Object Interaction Detection in Livestreaming](https://arxiv.org/pdf/2505.22011)
*Menghui Zhang, Jing Zhang, Lin Chen, Li Zhuo*

Main category: cs.CV

TL;DR: The paper proposes PeO-HOI, a method to improve human-object interaction (HOI) detection in livestreaming by addressing object bias through prototype embedding optimization.


<details>
  <summary>Details</summary>
Motivation: Current HOI detection methods in livestreaming focus too much on objects, neglecting interactions with streamers, leading to object bias.

Method: PeO-HOI preprocesses livestreaming with object detection and tracking, optimizes prototype embedding to reduce object bias, and models spatio-temporal context for HOI detection.

Result: PeO-HOI achieves 37.19%@full, 51.42%@non-rare, 26.20%@rare on VidHOI and 45.13%@full, 62.78%@non-rare, 30.37%@rare on BJUT-HOI.

Conclusion: PeO-HOI effectively improves HOI detection in livestreaming by mitigating object bias.

Abstract: Livestreaming often involves interactions between streamers and objects,
which is critical for understanding and regulating web content. While
human-object interaction (HOI) detection has made some progress in
general-purpose video downstream tasks, when applied to recognize the
interaction behaviors between a streamer and different objects in
livestreaming, it tends to focuses too much on the objects and neglects their
interactions with the streamer, which leads to object bias. To solve this
issue, we propose a prototype embedding optimization for human-object
interaction detection (PeO-HOI). First, the livestreaming is preprocessed using
object detection and tracking techniques to extract features of the
human-object (HO) pairs. Then, prototype embedding optimization is adopted to
mitigate the effect of object bias on HOI. Finally, after modelling the
spatio-temporal context between HO pairs, the HOI detection results are
obtained by the prediction head. The experimental results show that the
detection accuracy of the proposed PeO-HOI method has detection accuracies of
37.19%@full, 51.42%@non-rare, 26.20%@rare on the publicly available dataset
VidHOI, 45.13%@full, 62.78%@non-rare and 30.37%@rare on the self-built dataset
BJUT-HOI, which effectively improves the HOI detection performance in
livestreaming.

</details>


### [286] [PanoWan: Lifting Diffusion Video Generation Models to 360° with Latitude/Longitude-aware Mechanisms](https://arxiv.org/pdf/2505.22016)
*Yifei Xia, Shuchen Weng, Siqi Yang, Jingqi Liu, Chengxuan Zhu, Minggui Teng, Zijian Jia, Han Jiang, Boxin Shi*

Main category: cs.CV

TL;DR: PanoWan lifts pre-trained text-to-video models to the panoramic domain with minimal modules, achieving high-quality and diverse panoramic video generation.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle to leverage pre-trained generative priors for panoramic videos due to dataset limitations and spatial feature gaps.

Method: PanoWan uses latitude-aware sampling, rotated semantic denoising, and padded pixel-wise decoding to address distortions and boundary transitions.

Result: PanoWan achieves state-of-the-art performance and robustness in zero-shot downstream tasks.

Conclusion: PanoWan, supported by the PanoVid dataset, effectively bridges the gap in panoramic video generation.

Abstract: Panoramic video generation enables immersive 360{\deg} content creation,
valuable in applications that demand scene-consistent world exploration.
However, existing panoramic video generation models struggle to leverage
pre-trained generative priors from conventional text-to-video models for
high-quality and diverse panoramic videos generation, due to limited dataset
scale and the gap in spatial feature representations. In this paper, we
introduce PanoWan to effectively lift pre-trained text-to-video models to the
panoramic domain, equipped with minimal modules. PanoWan employs latitude-aware
sampling to avoid latitudinal distortion, while its rotated semantic denoising
and padded pixel-wise decoding ensure seamless transitions at longitude
boundaries. To provide sufficient panoramic videos for learning these lifted
representations, we contribute PanoVid, a high-quality panoramic video dataset
with captions and diverse scenarios. Consequently, PanoWan achieves
state-of-the-art performance in panoramic video generation and demonstrates
robustness for zero-shot downstream tasks.

</details>


### [287] [Guess the Age of Photos: An Interactive Web Platform for Historical Image Age Estimation](https://arxiv.org/pdf/2505.22031)
*Hasan Yucedag, Adam Jatowt*

Main category: cs.CV

TL;DR: A web platform, Guess the Age of Photos, gamifies historical photo dating with two modes, achieving high user satisfaction and revealing better accuracy in relative comparisons than absolute guesses.


<details>
  <summary>Details</summary>
Motivation: To create an engaging, educational tool that fosters historical awareness and analytical skills while studying human perception of temporal cues in images.

Method: Developed using Python, Flask, Bootstrap, and PostgreSQL, the platform features gamified modes (Guess the Year and Timeline Challenge) with dynamic scoring and leaderboards, tested on 113 users.

Result: Users achieved 65.9% accuracy in relative comparisons but only 25.6% in absolute year guesses, with higher satisfaction (4.25/5). Older decades were easier to identify.

Conclusion: The platform successfully serves as an educational and research tool, enhancing historical awareness and providing data for computer vision models.

Abstract: This paper introduces Guess the Age of Photos, a web platform engaging users
in estimating the years of historical photographs through two gamified modes:
Guess the Year (predicting a single image's year) and Timeline Challenge
(comparing two images to identify the older). Built with Python, Flask,
Bootstrap, and PostgreSQL, it uses a 10,150-image subset of the Date Estimation
in the Wild dataset (1930-1999). Features like dynamic scoring and leaderboards
boost engagement. Evaluated with 113 users and 15,473 gameplays, the platform
earned a 4.25/5 satisfaction rating. Users excelled in relative comparisons
(65.9% accuracy) over absolute year guesses (25.6% accuracy), with older
decades easier to identify. The platform serves as an educational tool,
fostering historical awareness and analytical skills via interactive
exploration of visual heritage. Furthermore, the platform provides a valuable
resource for studying human perception of temporal cues in images and could be
used to generate annotated data for training and evaluating computer vision
models.

</details>


### [288] [OmniAD: Detect and Understand Industrial Anomaly via Multimodal Reasoning](https://arxiv.org/pdf/2505.22039)
*Shifang Zhao, Yiheng Lin, Lu Han, Yao Zhao, Yunchao Wei*

Main category: cs.CV

TL;DR: OmniAD is a novel framework for anomaly detection and understanding, combining visual and textual reasoning with integrated training. It outperforms benchmarks like Qwen2.5-VL-7B and GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generating detailed anomaly analyses incorporating industrial knowledge.

Method: OmniAD uses multimodal reasoning (visual and textual), Text-as-Mask Encoding, and Visual Guided Textual Reasoning. Training combines SFT and GRPO with three reward functions.

Result: Achieves 79.1 on MMAD benchmark, surpassing Qwen2.5-VL-7B and GPT-4o, with strong performance across multiple benchmarks.

Conclusion: OmniAD highlights the importance of visual perception for effective anomaly reasoning; all codes and models will be public.

Abstract: While anomaly detection has made significant progress, generating detailed
analyses that incorporate industrial knowledge remains a challenge. To address
this gap, we introduce OmniAD, a novel framework that unifies anomaly detection
and understanding for fine-grained analysis. OmniAD is a multimodal reasoner
that combines visual and textual reasoning processes. The visual reasoning
provides detailed inspection by leveraging Text-as-Mask Encoding to perform
anomaly detection through text generation without manually selected thresholds.
Following this, Visual Guided Textual Reasoning conducts comprehensive analysis
by integrating visual perception. To enhance few-shot generalization, we employ
an integrated training strategy that combines supervised fine-tuning (SFT) with
reinforcement learning (GRPO), incorporating three sophisticated reward
functions. Experimental results demonstrate that OmniAD achieves a performance
of 79.1 on the MMAD benchmark, surpassing models such as Qwen2.5-VL-7B and
GPT-4o. It also shows strong results across multiple anomaly detection
benchmarks. These results highlight the importance of enhancing visual
perception for effective reasoning in anomaly understanding. All codes and
models will be publicly available.

</details>


### [289] [SridBench: Benchmark of Scientific Research Illustration Drawing of Image Generation Model](https://arxiv.org/pdf/2505.22126)
*Yifan Chang, Yukang Feng, Jianwen Sun, Jiaxin Ai, Chuanhao Li, S. Kevin Zhou, Kaipeng Zhang*

Main category: cs.CV

TL;DR: The paper introduces SridBench, the first benchmark for scientific figure generation, revealing gaps in AI capabilities compared to human performance.


<details>
  <summary>Details</summary>
Motivation: Automating scientific illustration generation is knowledge-intensive and laborious, but no benchmark exists to evaluate AI for this task.

Method: SridBench is created with 1,120 instances from 13 disciplines, evaluated along six dimensions like semantic fidelity and structural accuracy.

Result: Top models like GPT-4o-image underperform humans, struggling with text/visual clarity and scientific correctness.

Conclusion: Advanced reasoning-driven visual generation is needed to bridge the gap between AI and human performance in scientific figure generation.

Abstract: Recent years have seen rapid advances in AI-driven image generation. Early
diffusion models emphasized perceptual quality, while newer multimodal models
like GPT-4o-image integrate high-level reasoning, improving semantic
understanding and structural composition. Scientific illustration generation
exemplifies this evolution: unlike general image synthesis, it demands accurate
interpretation of technical content and transformation of abstract ideas into
clear, standardized visuals. This task is significantly more
knowledge-intensive and laborious, often requiring hours of manual work and
specialized tools. Automating it in a controllable, intelligent manner would
provide substantial practical value. Yet, no benchmark currently exists to
evaluate AI on this front. To fill this gap, we introduce SridBench, the first
benchmark for scientific figure generation. It comprises 1,120 instances
curated from leading scientific papers across 13 natural and computer science
disciplines, collected via human experts and MLLMs. Each sample is evaluated
along six dimensions, including semantic fidelity and structural accuracy.
Experimental results reveal that even top-tier models like GPT-4o-image lag
behind human performance, with common issues in text/visual clarity and
scientific correctness. These findings highlight the need for more advanced
reasoning-driven visual generation capabilities.

</details>


### [290] [LatentMove: Towards Complex Human Movement Video Generation](https://arxiv.org/pdf/2505.22046)
*Ashkan Taghipour, Morteza Ghahremani, Mohammed Bennamoun, Farid Boussaid, Aref Miri Rekavandi, Zinuo Li, Qiuhong Ke, Hamid Laga*

Main category: cs.CV

TL;DR: LatentMove is a DiT-based framework for dynamic human animation in I2V generation, addressing unnatural deformations with a conditional control branch and learnable tokens. It introduces the CHV dataset and new metrics for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing I2V methods struggle with complex, non-repetitive human movements, causing unnatural deformations.

Method: Uses a DiT-based framework with a conditional control branch and learnable face/body tokens for consistency. Introduces the CHV dataset and new metrics for flow and silhouette consistency.

Result: LatentMove improves human animation quality, especially for rapid, intricate movements.

Conclusion: LatentMove advances I2V generation by handling dynamic human motions better, with code, dataset, and metrics made available.

Abstract: Image-to-video (I2V) generation seeks to produce realistic motion sequences
from a single reference image. Although recent methods exhibit strong temporal
consistency, they often struggle when dealing with complex, non-repetitive
human movements, leading to unnatural deformations. To tackle this issue, we
present LatentMove, a DiT-based framework specifically tailored for highly
dynamic human animation. Our architecture incorporates a conditional control
branch and learnable face/body tokens to preserve consistency as well as
fine-grained details across frames. We introduce Complex-Human-Videos (CHV), a
dataset featuring diverse, challenging human motions designed to benchmark the
robustness of I2V systems. We also introduce two metrics to assess the flow and
silhouette consistency of generated videos with their ground truth.
Experimental results indicate that LatentMove substantially improves human
animation quality--particularly when handling rapid, intricate
movements--thereby pushing the boundaries of I2V generation. The code, the CHV
dataset, and the evaluation metrics will be available at https://github.com/
--.

</details>


### [291] [Real-Time Blind Defocus Deblurring for Earth Observation: The IMAGIN-e Mission Approach](https://arxiv.org/pdf/2505.22128)
*Alejandro D. Mousist*

Main category: cs.CV

TL;DR: A blind deblurring method for Earth observation images from the IMAGIN-e mission, using Sentinel-2 data and GANs, improves image quality without reference images and is deployed in space.


<details>
  <summary>Details</summary>
Motivation: Address mechanical defocus in Earth observation images from the IMAGIN-e mission, adapting to space-based edge computing constraints.

Method: Blind deblurring approach using Sentinel-2 data to estimate defocus kernel and train a restoration model within a GAN framework.

Result: SSIM improved by 72.47% and PSNR by 25.00% on synthetic data; NIQE improved by 60.66% and BRISQUE by 48.38% on IMAGIN-e.

Conclusion: The method is practical for space environments, enabling applications like water body segmentation and contour detection under resource constraints.

Abstract: This work addresses mechanical defocus in Earth observation images from the
IMAGIN-e mission aboard the ISS, proposing a blind deblurring approach adapted
to space-based edge computing constraints. Leveraging Sentinel-2 data, our
method estimates the defocus kernel and trains a restoration model within a GAN
framework, effectively operating without reference images.
  On Sentinel-2 images with synthetic degradation, SSIM improved by 72.47% and
PSNR by 25.00%, confirming the model's ability to recover lost details when the
original clean image is known. On IMAGIN-e, where no reference images exist,
perceptual quality metrics indicate a substantial enhancement, with NIQE
improving by 60.66% and BRISQUE by 48.38%, validating real-world onboard
restoration. The approach is currently deployed aboard the IMAGIN-e mission,
demonstrating its practical application in an operational space environment.
  By efficiently handling high-resolution images under edge computing
constraints, the method enables applications such as water body segmentation
and contour detection while maintaining processing viability despite resource
limitations.

</details>


### [292] [AquaMonitor: A multimodal multi-view image sequence dataset for real-life aquatic invertebrate biodiversity monitoring](https://arxiv.org/pdf/2505.22065)
*Mikko Impiö, Philipp M. Rehsen, Tiina Laamanen, Arne J. Beermann, Florian Leese, Jenni Raitoharju*

Main category: cs.CV

TL;DR: AquaMonitor is the first large-scale dataset for aquatic invertebrates, enabling automated species identification with real-world challenges like open-set recognition and class imbalance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of standardized datasets for aquatic invertebrates, which are labor-intensive to collect and crucial for biodiversity monitoring.

Method: Imaged specimens from two years of monitoring, creating a dataset with 2.7M images, DNA sequences, and physical measurements. Defined three benchmark tasks for evaluation.

Result: Produced a large, multimodal dataset with strong baselines for monitoring, classification, and few-shot learning tasks.

Conclusion: AquaMonitor advances aquatic biodiversity monitoring by providing a realistic and challenging dataset for automated identification methods.

Abstract: This paper presents the AquaMonitor dataset, the first large computer vision
dataset of aquatic invertebrates collected during routine environmental
monitoring. While several large species identification datasets exist, they are
rarely collected using standardized collection protocols, and none focus on
aquatic invertebrates, which are particularly laborious to collect. For
AquaMonitor, we imaged all specimens from two years of monitoring whenever
imaging was possible given practical limitations. The dataset enables the
evaluation of automated identification methods for real-life monitoring
purposes using a realistically challenging and unbiased setup. The dataset has
2.7M images from 43,189 specimens, DNA sequences for 1358 specimens, and dry
mass and size measurements for 1494 specimens, making it also one of the
largest biological multi-view and multimodal datasets to date. We define three
benchmark tasks and provide strong baselines for these: 1) Monitoring
benchmark, reflecting real-life deployment challenges such as open-set
recognition, distribution shift, and extreme class imbalance, 2) Classification
benchmark, which follows a standard fine-grained visual categorization setup,
and 3) Few-shot benchmark, which targets classes with only few training
examples from very fine-grained categories. Advancements on the Monitoring
benchmark can directly translate to improvement of aquatic biodiversity
monitoring, which is an important component of regular legislative water
quality assessment in many countries.

</details>


### [293] [FaceEditTalker: Interactive Talking Head Generation with Facial Attribute Editing](https://arxiv.org/pdf/2505.22141)
*Guanwen Feng, Zhiyuan Ma, Yunan Li, Junwei Jing, Jiahao Yang, Qiguang Miao*

Main category: cs.CV

TL;DR: FaceEditTalker introduces a framework for audio-driven talking head generation with facial attribute editing, outperforming existing methods in lip-sync, quality, and controllability.


<details>
  <summary>Details</summary>
Motivation: Current methods lack facial attribute editing, which is vital for personalization and applications like digital avatars, education, and customer service.

Method: Combines an image feature space editing module for attribute control and an audio-driven video generation module using diffusion-based synthesis.

Result: Outperforms state-of-the-art in lip-sync accuracy, video quality, and attribute controllability on public datasets.

Conclusion: FaceEditTalker successfully integrates facial attribute editing with high-quality talking head generation, offering practical benefits for diverse applications.

Abstract: Recent advances in audio-driven talking head generation have achieved
impressive results in lip synchronization and emotional expression. However,
they largely overlook the crucial task of facial attribute editing. This
capability is crucial for achieving deep personalization and expanding the
range of practical applications, including user-tailored digital avatars,
engaging online education content, and brand-specific digital customer service.
In these key domains, the flexible adjustment of visual attributes-such as
hairstyle, accessories, and subtle facial features is essential for aligning
with user preferences, reflecting diverse brand identities, and adapting to
varying contextual demands. In this paper, we present FaceEditTalker, a unified
framework that enables controllable facial attribute manipulation while
generating high-quality, audio-synchronized talking head videos. Our method
consists of two key components: an image feature space editing module, which
extracts semantic and detail features and allows flexible control over
attributes like expression, hairstyle, and accessories; and an audio-driven
video generation module, which fuses these edited features with audio-guided
facial landmarks to drive a diffusion-based generator. This design ensures
temporal coherence, visual fidelity, and identity preservation across frames.
Extensive experiments on public datasets demonstrate that our method
outperforms state-of-the-art approaches in lip-sync accuracy, video quality,
and attribute controllability. Project page:
https://peterfanfan.github.io/FaceEditTalker/

</details>


### [294] [Bringing CLIP to the Clinic: Dynamic Soft Labels and Negation-Aware Learning for Medical Analysis](https://arxiv.org/pdf/2505.22079)
*Hanbin Ko, Chang-Min Park*

Main category: cs.CV

TL;DR: A novel approach enhances medical VLP by integrating clinically-enhanced dynamic soft labels, medical graphical alignment, and negation-based hard negatives, achieving state-of-the-art performance and introducing the CXR-Align benchmark.


<details>
  <summary>Details</summary>
Motivation: General-domain architectures like CLIP struggle with medical data due to negations and data imbalance, necessitating specialized solutions for clinical comprehension.

Method: Proposes clinically-enhanced dynamic soft labels, medical graphical alignment, and negation-based hard negatives to improve contrastive loss in medical contexts.

Result: Achieves state-of-the-art performance in zero-shot, fine-tuned classification, and report retrieval, with the CXR-Align benchmark validating clinical language understanding.

Conclusion: The approach is effective, easy to implement, and generalizes well, advancing medical VLP and clinical language understanding in imaging.

Abstract: The development of large-scale image-text pair datasets has significantly
advanced self-supervised learning in Vision-Language Processing (VLP). However,
directly applying general-domain architectures such as CLIP to medical data
presents challenges, particularly in handling negations and addressing the
inherent data imbalance of medical datasets. To address these issues, we
propose a novel approach that integrates clinically-enhanced dynamic soft
labels and medical graphical alignment, thereby improving clinical
comprehension and the applicability of contrastive loss in medical contexts.
Furthermore, we introduce negation-based hard negatives to deepen the model's
understanding of the complexities of clinical language. Our approach is easily
integrated into the medical CLIP training pipeline and achieves
state-of-the-art performance across multiple tasks, including zero-shot,
fine-tuned classification, and report retrieval. To comprehensively evaluate
our model's capacity for understanding clinical language, we introduce
CXR-Align, a benchmark uniquely designed to evaluate the understanding of
negation and clinical information within chest X-ray (CXR) datasets.
Experimental results demonstrate that our proposed methods are straightforward
to implement and generalize effectively across contrastive learning frameworks,
enhancing medical VLP capabilities and advancing clinical language
understanding in medical imaging.

</details>


### [295] [Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language](https://arxiv.org/pdf/2505.22146)
*Guangfu Hao, Haojie Wen, Liangxuna Guo, Yang Chen, Yanchao Bi, Shan Yu*

Main category: cs.CV

TL;DR: A framework using low-dimensional attribute representations bridges visual tool perception and linguistic task understanding, achieving 74% accuracy in tool selection tasks.


<details>
  <summary>Details</summary>
Motivation: To develop a computational model that mimics human-like flexible tool selection, a complex cognitive ability distinguishing humans from other species.

Method: Uses visual encoders (ResNet or ViT) for attribute extraction from tool images and fine-tuned language models (GPT-2, LLaMA, DeepSeek) for deriving attributes from task descriptions. The dataset ToolNet includes 115 tools labeled with 13 attributes.

Result: Achieves 74% accuracy, outperforming direct tool matching (20%) and smaller multimodal models (21%-58%), and approaching GPT-4o (73%) with fewer parameters. Manipulation-related attributes are most critical.

Conclusion: Provides a parameter-efficient, interpretable solution for tool selection, advancing cognitive science and practical applications.

Abstract: Flexible tool selection reflects a complex cognitive ability that
distinguishes humans from other species, yet computational models that capture
this ability remain underdeveloped. We developed a framework using
low-dimensional attribute representations to bridge visual tool perception and
linguistic task understanding. We constructed a comprehensive dataset (ToolNet)
containing 115 common tools labeled with 13 carefully designed attributes
spanning physical, functional, and psychological properties, paired with
natural language scenarios describing tool usage. Visual encoders (ResNet or
ViT) extract attributes from tool images while fine-tuned language models
(GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our
approach achieves 74% accuracy in tool selection tasks-significantly
outperforming direct tool matching (20%) and smaller multimodal models
(21%-58%), while approaching performance of much larger models like GPT-4o
(73%) with substantially fewer parameters. Ablation studies revealed that
manipulation-related attributes (graspability, hand-relatedness, elongation)
consistently prove most critical across modalities. This work provides a
parameter-efficient, interpretable solution that mimics human-like tool
cognition, advancing both cognitive science understanding and practical
applications in tool selection tasks.

</details>


### [296] [MObyGaze: a film dataset of multimodal objectification densely annotated by experts](https://arxiv.org/pdf/2505.22084)
*Julie Tores, Elisa Ancarani, Lucile Sassatelli, Hui-Yin Wu, Clement Bergman, Lea Andolfi, Victor Ecrement, Remy Sun, Frederic Precioso, Thierry Devars, Magali Guaresi, Virginie Julliard, Sarah Lecossais*

Main category: cs.CV

TL;DR: The paper introduces a new AI task to quantify objectification in films using multimodal data, presents the MObyGaze dataset, and benchmarks models for feasibility.


<details>
  <summary>Details</summary>
Motivation: To address gender representation disparities and stereotypes in audiovisual content by analyzing objectification patterns.

Method: Defines objectification through a structured thesaurus, introduces the MObyGaze dataset (20 movies, 6072 segments), and benchmarks multimodal models.

Result: Demonstrates feasibility of the task with benchmarked models and provides a publicly available dataset and code.

Conclusion: The study advances understanding of objectification in films and provides tools for future research in this area.

Abstract: Characterizing and quantifying gender representation disparities in
audiovisual storytelling contents is necessary to grasp how stereotypes may
perpetuate on screen. In this article, we consider the high-level construct of
objectification and introduce a new AI task to the ML community: characterize
and quantify complex multimodal (visual, speech, audio) temporal patterns
producing objectification in films. Building on film studies and psychology, we
define the construct of objectification in a structured thesaurus involving 5
sub-constructs manifesting through 11 concepts spanning 3 modalities. We
introduce the Multimodal Objectifying Gaze (MObyGaze) dataset, made of 20
movies annotated densely by experts for objectification levels and concepts
over freely delimited segments: it amounts to 6072 segments over 43 hours of
video with fine-grained localization and categorization. We formulate different
learning tasks, propose and investigate best ways to learn from the diversity
of labels among a low number of annotators, and benchmark recent vision, text
and audio models, showing the feasibility of the task. We make our code and our
dataset available to the community and described in the Croissant format:
https://anonymous.4open.science/r/MObyGaze-F600/.

</details>


### [297] [Fast Feature Matching of UAV Images via Matrix Band Reduction-based GPU Data Schedule](https://arxiv.org/pdf/2505.22089)
*San Jiang, Kan You, Wanshou Jiang, Qingquan Li*

Main category: cs.CV

TL;DR: A GPU data schedule algorithm for efficient feature matching in UAV images, achieving significant speedup and comparable accuracy.


<details>
  <summary>Details</summary>
Motivation: Feature matching is time-consuming in SfM, especially for UAV images. This study aims to improve efficiency using GPU acceleration.

Method: 1. Select match pairs via image retrieval. 2. Generate compact image blocks using MBR-based scheduling. 3. Execute feature matching with GPU-accelerated cascade hashing and refine matches with geometric constraints and RANSAC.

Result: Speedup ratios of 77.0 to 100.0 compared to KD-Tree methods, with comparable accuracy in BA.

Conclusion: The proposed algorithm is an efficient solution for UAV image feature matching.

Abstract: Feature matching dominats the time costs in structure from motion (SfM). The
primary contribution of this study is a GPU data schedule algorithm for
efficient feature matching of Unmanned aerial vehicle (UAV) images. The core
idea is to divide the whole dataset into blocks based on the matrix band
reduction (MBR) and achieve efficient feature matching via GPU-accelerated
cascade hashing. First, match pairs are selected by using an image retrieval
technique, which converts images into global descriptors and searches
high-dimension nearest neighbors with graph indexing. Second, compact image
blocks are iteratively generated from a MBR-based data schedule strategy, which
exploits image connections to avoid redundant data IO (input/output) burden and
increases the usage of GPU computing power. Third, guided by the generated
image blocks, feature matching is executed sequentially within the framework of
GPU-accelerated cascade hashing, and initial candidate matches are refined by
combining a local geometric constraint and RANSAC-based global verification.
For further performance improvement, these two seps are designed to execute
parallelly in GPU and CPU. Finally, the performance of the proposed solution is
evaluated by using large-scale UAV datasets. The results demonstrate that it
increases the efficiency of feature matching with speedup ratios ranging from
77.0 to 100.0 compared with KD-Tree based matching methods, and achieves
comparable accuracy in relative and absolute bundle adjustment (BA). The
proposed algorithm is an efficient solution for feature matching of UAV images.

</details>


### [298] [Investigating Mechanisms for In-Context Vision Language Binding](https://arxiv.org/pdf/2505.22200)
*Darshana Saravanan, Makarand Tapaswi, Vineet Gandhi*

Main category: cs.CV

TL;DR: VLMs use Binding IDs to link image tokens and textual references for in-context association, demonstrated via a synthetic dataset.


<details>
  <summary>Details</summary>
Motivation: To explore how VLMs bind visual and textual information, inspired by the Binding ID mechanism in LLMs.

Method: Investigating image-text binding in VLMs using a synthetic dataset and task involving 3D objects and descriptions.

Result: VLMs assign distinct Binding IDs to object image tokens and their textual references, enabling association.

Conclusion: The Binding ID mechanism facilitates cross-modal associations in VLMs, enhancing their understanding of prompts.

Abstract: To understand a prompt, Vision-Language models (VLMs) must perceive the
image, comprehend the text, and build associations within and across both
modalities. For instance, given an 'image of a red toy car', the model should
associate this image to phrases like 'car', 'red toy', 'red object', etc. Feng
and Steinhardt propose the Binding ID mechanism in LLMs, suggesting that the
entity and its corresponding attribute tokens share a Binding ID in the model
activations. We investigate this for image-text binding in VLMs using a
synthetic dataset and task that requires models to associate 3D objects in an
image with their descriptions in the text. Our experiments demonstrate that
VLMs assign a distinct Binding ID to an object's image tokens and its textual
references, enabling in-context association.

</details>


### [299] [UAVPairs: A Challenging Benchmark for Match Pair Retrieval of Large-scale UAV Images](https://arxiv.org/pdf/2505.22098)
*Junhuan Liu, San Jiang, Wei Ge, Wei Huang, Bingxuan Guo, Qingquan Li*

Main category: cs.CV

TL;DR: The paper introduces UAVPairs, a benchmark dataset for UAV image retrieval, and a training pipeline with a batched mining strategy and ranked list loss, improving retrieval accuracy and 3D reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a challenging benchmark for UAV image retrieval and the high cost of global hard negative mining, the authors propose UAVPairs and an efficient training pipeline.

Method: The UAVPairs dataset is constructed with 21,622 images and geometric similarity. A batched mining strategy and ranked list loss are introduced to optimize training.

Result: Models trained with UAVPairs and the ranked list loss show improved retrieval accuracy, better 3D reconstruction, and robustness in challenging scenes.

Conclusion: The UAVPairs dataset and training pipeline provide an effective solution for UAV image retrieval, with the dataset being publicly available.

Abstract: The primary contribution of this paper is a challenging benchmark dataset,
UAVPairs, and a training pipeline designed for match pair retrieval of
large-scale UAV images. First, the UAVPairs dataset, comprising 21,622
high-resolution images across 30 diverse scenes, is constructed; the 3D points
and tracks generated by SfM-based 3D reconstruction are employed to define the
geometric similarity of image pairs, ensuring genuinely matchable image pairs
are used for training. Second, to solve the problem of expensive mining cost
for global hard negative mining, a batched nontrivial sample mining strategy is
proposed, leveraging the geometric similarity and multi-scene structure of the
UAVPairs to generate training samples as to accelerate training. Third,
recognizing the limitation of pair-based losses, the ranked list loss is
designed to improve the discrimination of image retrieval models, which
optimizes the global similarity structure constructed from the positive set and
negative set. Finally, the effectiveness of the UAVPairs dataset and training
pipeline is validated through comprehensive experiments on three distinct
large-scale UAV datasets. The experiment results demonstrate that models
trained with the UAVPairs dataset and the ranked list loss achieve
significantly improved retrieval accuracy compared to models trained on
existing datasets or with conventional losses. Furthermore, these improvements
translate to enhanced view graph connectivity and higher quality of
reconstructed 3D models. The models trained by the proposed approach perform
more robustly compared with hand-crafted global features, particularly in
challenging repetitively textured scenes and weakly textured scenes. For match
pair retrieval of large-scale UAV images, the trained image retrieval models
offer an effective solution. The dataset would be made publicly available at
https://github.com/json87/UAVPairs.

</details>


### [300] [On the Transferability and Discriminability of Repersentation Learning in Unsupervised Domain Adaptation](https://arxiv.org/pdf/2505.22099)
*Wenwen Qiang, Ziyin Gu, Lingyu Si, Jiangmeng Li, Changwen Zheng, Fuchun Sun, Hui Xiong*

Main category: cs.CV

TL;DR: The paper proposes a novel UDA framework (RLGLC) that enhances both transferability and discriminability, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the neglect of target-domain discriminability in standard adversarial-based UDA frameworks.

Method: Introduces RLGLC, combining domain alignment with discriminability-enhancing constraints using AR-WWD and local consistency.

Result: RLGLC consistently outperforms state-of-the-art methods on benchmark datasets.

Conclusion: Theoretical and practical results highlight the necessity of balancing transferability and discriminability in UDA.

Abstract: In this paper, we addressed the limitation of relying solely on distribution
alignment and source-domain empirical risk minimization in Unsupervised Domain
Adaptation (UDA). Our information-theoretic analysis showed that this standard
adversarial-based framework neglects the discriminability of target-domain
features, leading to suboptimal performance. To bridge this
theoretical-practical gap, we defined "good representation learning" as
guaranteeing both transferability and discriminability, and proved that an
additional loss term targeting target-domain discriminability is necessary.
Building on these insights, we proposed a novel adversarial-based UDA framework
that explicitly integrates a domain alignment objective with a
discriminability-enhancing constraint. Instantiated as Domain-Invariant
Representation Learning with Global and Local Consistency (RLGLC), our method
leverages Asymmetrically-Relaxed Wasserstein of Wasserstein Distance (AR-WWD)
to address class imbalance and semantic dimension weighting, and employs a
local consistency mechanism to preserve fine-grained target-domain
discriminative information. Extensive experiments across multiple benchmark
datasets demonstrate that RLGLC consistently surpasses state-of-the-art
methods, confirming the value of our theoretical perspective and underscoring
the necessity of enforcing both transferability and discriminability in
adversarial-based UDA.

</details>


### [301] [Adapting Segment Anything Model for Power Transmission Corridor Hazard Segmentation](https://arxiv.org/pdf/2505.22105)
*Hang Chen, Maoyuan Ye, Peng Yang, Haibin He, Juhua Liu, Bo Du*

Main category: cs.CV

TL;DR: ELE-SAM adapts SAM for PTCHS, improving segmentation of fine-structured hazards in complex backgrounds with a Context-Aware Prompt Adapter and High-Fidelity Mask Decoder, validated on the ELE-40K dataset.


<details>
  <summary>Details</summary>
Motivation: SAM struggles with fine-structured objects in complex transmission corridor scenarios, necessitating an adapted model for PTCHS.

Method: Proposes ELE-SAM with a Context-Aware Prompt Adapter for better tokens and a High-Fidelity Mask Decoder for fine structures. Uses the ELE-40K dataset for training.

Result: ELE-SAM outperforms baselines by 16.8% mIoU and 20.6% mBIoU on ELE-40K, and shows improvements on HQSeg-44K.

Conclusion: ELE-SAM effectively addresses PTCHS challenges, offering superior performance and a new benchmark dataset.

Abstract: Power transmission corridor hazard segmentation (PTCHS) aims to separate
transmission equipment and surrounding hazards from complex background,
conveying great significance to maintaining electric power transmission safety.
Recently, the Segment Anything Model (SAM) has emerged as a foundational vision
model and pushed the boundaries of segmentation tasks. However, SAM struggles
to deal with the target objects in complex transmission corridor scenario,
especially those with fine structure. In this paper, we propose ELE-SAM,
adapting SAM for the PTCHS task. Technically, we develop a Context-Aware Prompt
Adapter to achieve better prompt tokens via incorporating global-local features
and focusing more on key regions. Subsequently, to tackle the hazard objects
with fine structure in complex background, we design a High-Fidelity Mask
Decoder by leveraging multi-granularity mask features and then scaling them to
a higher resolution. Moreover, to train ELE-SAM and advance this field, we
construct the ELE-40K benchmark, the first large-scale and real-world dataset
for PTCHS including 44,094 image-mask pairs. Experimental results for ELE-40K
demonstrate the superior performance that ELE-SAM outperforms the baseline
model with the average 16.8% mIoU and 20.6% mBIoU performance improvement.
Moreover, compared with the state-of-the-art method on HQSeg-44K, the average
2.9% mIoU and 3.8% mBIoU absolute improvements further validate the
effectiveness of our method on high-quality generic object segmentation. The
source code and dataset are available at https://github.com/Hhaizee/ELE-SAM.

</details>


### [302] [Autoregression-free video prediction using diffusion model for mitigating error propagation](https://arxiv.org/pdf/2505.22111)
*Woonho Ko, Jin Bok Park, Il Yong Chun*

Main category: cs.CV

TL;DR: Proposes an AutoRegression-Free (ARFree) video prediction framework using diffusion models to avoid error propagation in long-term predictions.


<details>
  <summary>Details</summary>
Motivation: Existing autoregressive methods suffer from error propagation in distant future frames.

Method: ARFree directly predicts future frames using a motion prediction module and a training method for motion continuity and contextual consistency.

Result: Outperforms state-of-the-art methods on two benchmark datasets.

Conclusion: ARFree is an effective alternative to autoregressive video prediction, addressing error propagation issues.

Abstract: Existing long-term video prediction methods often rely on an autoregressive
video prediction mechanism. However, this approach suffers from error
propagation, particularly in distant future frames. To address this limitation,
this paper proposes the first AutoRegression-Free (ARFree) video prediction
framework using diffusion models. Different from an autoregressive video
prediction mechanism, ARFree directly predicts any future frame tuples from the
context frame tuple. The proposed ARFree consists of two key components: 1) a
motion prediction module that predicts a future motion using motion feature
extracted from the context frame tuple; 2) a training method that improves
motion continuity and contextual consistency between adjacent future frame
tuples. Our experiments with two benchmark datasets show that the proposed
ARFree video prediction framework outperforms several state-of-the-art video
prediction methods.

</details>


### [303] [Neural Restoration of Greening Defects in Historical Autochrome Photographs Based on Purely Synthetic Data](https://arxiv.org/pdf/2505.22291)
*Saptarshi Neil Sinha, P. Julius Kuehn, Johannes Koppe, Arjan Kuijper, Michael Weinmann*

Main category: cs.CV

TL;DR: A novel method for automatic greening defect removal in autochrome photos using synthetic data and generative AI, outperforming existing techniques in efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Preserving early visual arts, especially color photographs, is difficult due to aging and improper storage, causing defects like greening, blurring, and fading. Existing methods are inefficient or inaccurate.

Method: Uses synthetic dataset generation and generative AI with a modified weighted loss function (ChaIR method) to address greening defects in autochrome photos.

Result: Efficient restoration with reduced manual effort, accurately reproducing original colors.

Conclusion: The proposed method effectively restores autochrome photographs, addressing greening defects more efficiently than existing approaches.

Abstract: The preservation of early visual arts, particularly color photographs, is
challenged by deterioration caused by aging and improper storage, leading to
issues like blurring, scratches, color bleeding, and fading defects. In this
paper, we present the first approach for the automatic removal of greening
color defects in digitized autochrome photographs. Our main contributions
include a method based on synthetic dataset generation and the use of
generative AI with a carefully designed loss function for the restoration of
visual arts. To address the lack of suitable training datasets for analyzing
greening defects in damaged autochromes, we introduce a novel approach for
accurately simulating such defects in synthetic data. We also propose a
modified weighted loss function for the ChaIR method to account for color
imbalances between defected and non-defected areas. While existing methods
struggle with accurately reproducing original colors and may require
significant manual effort, our method allows for efficient restoration with
reduced time requirements.

</details>


### [304] [What Makes for Text to 360-degree Panorama Generation with Stable Diffusion?](https://arxiv.org/pdf/2505.22129)
*Jinhong Ni, Chang-Bin Zhang, Qiang Zhang, Jing Zhang*

Main category: cs.CV

TL;DR: The paper investigates the adaptation of text-to-image diffusion models for 360-degree panorama generation, identifying key mechanisms in attention modules and proposing UniPano, a simpler and more efficient framework.


<details>
  <summary>Details</summary>
Motivation: To understand how pre-trained diffusion models can be adapted for panoramic image generation despite the domain gap between perspective and panoramic images.

Method: Analyzes the roles of attention module matrices (query, key, value, output weights) in adaptation and introduces UniPano, a streamlined framework for panorama generation.

Result: UniPano outperforms existing methods, reduces memory usage, and speeds up training, making high-resolution panorama generation scalable.

Conclusion: The study provides insights into leveraging pre-trained diffusion models for panoramic images and offers UniPano as a baseline for future research.

Abstract: Recent prosperity of text-to-image diffusion models, e.g. Stable Diffusion,
has stimulated research to adapt them to 360-degree panorama generation. Prior
work has demonstrated the feasibility of using conventional low-rank adaptation
techniques on pre-trained diffusion models to generate panoramic images.
However, the substantial domain gap between perspective and panoramic images
raises questions about the underlying mechanisms enabling this empirical
success. We hypothesize and examine that the trainable counterparts exhibit
distinct behaviors when fine-tuned on panoramic data, and such an adaptation
conceals some intrinsic mechanism to leverage the prior knowledge within the
pre-trained diffusion models. Our analysis reveals the following: 1) the query
and key matrices in the attention modules are responsible for common
information that can be shared between the panoramic and perspective domains,
thus are less relevant to panorama generation; and 2) the value and output
weight matrices specialize in adapting pre-trained knowledge to the panoramic
domain, playing a more critical role during fine-tuning for panorama
generation. We empirically verify these insights by introducing a simple
framework called UniPano, with the objective of establishing an elegant
baseline for future research. UniPano not only outperforms existing methods but
also significantly reduces memory usage and training time compared to prior
dual-branch approaches, making it scalable for end-to-end panorama generation
with higher resolution. The code will be released.

</details>


### [305] [3D Question Answering via only 2D Vision-Language Models](https://arxiv.org/pdf/2505.22143)
*Fengyun Wang, Sicheng Yu, Jiawei Wu, Jinhui Tang, Hanwang Zhang, Qianru Sun*

Main category: cs.CV

TL;DR: The paper proposes cdViews, a method to select critical and diverse 2D views from 3D point clouds for zero-shot 3D question answering (3D-QA), achieving state-of-the-art performance without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To leverage 2D large vision-language models (LVLMs) for 3D tasks due to limited 3D training data and avoid resource-intensive 3D LVLMs.

Method: cdViews: a two-component approach (viewSelector for critical views, viewNMS for diversity) to sample 2D views from 3D point clouds for 2D LVLMs like LLAVA-OV.

Result: State-of-the-art performance on ScanQA and SQA benchmarks, proving 2D LVLMs are effective for 3D-QA without fine-tuning.

Conclusion: 2D LVLMs are currently the best alternative to 3D LVLMs for 3D tasks, as demonstrated by cdViews' success in 3D-QA.

Abstract: Large vision-language models (LVLMs) have significantly advanced numerous
fields. In this work, we explore how to harness their potential to address 3D
scene understanding tasks, using 3D question answering (3D-QA) as a
representative example. Due to the limited training data in 3D, we do not train
LVLMs but infer in a zero-shot manner. Specifically, we sample 2D views from a
3D point cloud and feed them into 2D models to answer a given question. When
the 2D model is chosen, e.g., LLAVA-OV, the quality of sampled views matters
the most. We propose cdViews, a novel approach to automatically selecting
critical and diverse Views for 3D-QA. cdViews consists of two key components:
viewSelector prioritizing critical views based on their potential to provide
answer-specific information, and viewNMS enhancing diversity by removing
redundant views based on spatial overlap. We evaluate cdViews on the
widely-used ScanQA and SQA benchmarks, demonstrating that it achieves
state-of-the-art performance in 3D-QA while relying solely on 2D models without
fine-tuning. These findings support our belief that 2D LVLMs are currently the
most effective alternative (of the resource-intensive 3D LVLMs) for addressing
3D tasks.

</details>


### [306] [Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging](https://arxiv.org/pdf/2505.22150)
*Runze Xia, Shuo Feng, Renzhi Wang, Congchi Yin, Xuyun Wen, Piji Li*

Main category: cs.CV

TL;DR: FgB2I improves Brain-to-Image reconstruction by using fine-grained text as a bridge, enhancing details and semantic consistency through three stages: detail enhancement, fine-grained text decoding, and text-bridged reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing Brain-to-Image methods lack detail and semantic consistency due to insufficient semantic information.

Method: FgB2I employs three stages: detail enhancement (using vision-language models), decoding fine-grained text from fMRI signals (guided by reward metrics), and integrating text into reconstruction.

Result: Fine-grained text descriptions improve reconstruction quality, validated by object accuracy and semantic similarity metrics.

Conclusion: FgB2I effectively bridges semantic gaps in Brain-to-Image reconstruction, enhancing detail and consistency.

Abstract: Brain-to-Image reconstruction aims to recover visual stimuli perceived by
humans from brain activity. However, the reconstructed visual stimuli often
missing details and semantic inconsistencies, which may be attributed to
insufficient semantic information. To address this issue, we propose an
approach named Fine-grained Brain-to-Image reconstruction (FgB2I), which
employs fine-grained text as bridge to improve image reconstruction. FgB2I
comprises three key stages: detail enhancement, decoding fine-grained text
descriptions, and text-bridged brain-to-image reconstruction. In the
detail-enhancement stage, we leverage large vision-language models to generate
fine-grained captions for visual stimuli and experimentally validate its
importance. We propose three reward metrics (object accuracy, text-image
semantic similarity, and image-image semantic similarity) to guide the language
model in decoding fine-grained text descriptions from fMRI signals. The
fine-grained text descriptions can be integrated into existing reconstruction
methods to achieve fine-grained Brain-to-Image reconstruction.

</details>


### [307] [Learning A Robust RGB-Thermal Detector for Extreme Modality Imbalance](https://arxiv.org/pdf/2505.22154)
*Chao Tian, Chao Yang, Guoqing Zhu, Qiang Wang, Zhenyu He*

Main category: cs.CV

TL;DR: A novel RGB-Thermal object detection method addresses extreme modality imbalance by using a base-and-auxiliary detector with adaptive modality weighting and pseudo-degradation training.


<details>
  <summary>Details</summary>
Motivation: Traditional RGB-T detectors fail under extreme modality imbalance caused by real-world degradation, leading to OOD issues and disrupted training.

Method: Proposes a base-and-auxiliary detector with a modality interaction module for adaptive weighting and uses pseudo-degradation to simulate imbalances.

Result: Reduces Missing Rate by 55% and improves robustness across baseline detectors.

Conclusion: The framework effectively handles severe modality imbalances, enhancing model reliability in degraded conditions.

Abstract: RGB-Thermal (RGB-T) object detection utilizes thermal infrared (TIR) images
to complement RGB data, improving robustness in challenging conditions.
Traditional RGB-T detectors assume balanced training data, where both
modalities contribute equally. However, in real-world scenarios, modality
degradation-due to environmental factors or technical issues-can lead to
extreme modality imbalance, causing out-of-distribution (OOD) issues during
testing and disrupting model convergence during training. This paper addresses
these challenges by proposing a novel base-and-auxiliary detector architecture.
We introduce a modality interaction module to adaptively weigh modalities based
on their quality and handle imbalanced samples effectively. Additionally, we
leverage modality pseudo-degradation to simulate real-world imbalances in
training data. The base detector, trained on high-quality pairs, provides a
consistency constraint for the auxiliary detector, which receives degraded
samples. This framework enhances model robustness, ensuring reliable
performance even under severe modality degradation. Experimental results
demonstrate the effectiveness of our method in handling extreme modality
imbalances~(decreasing the Missing Rate by 55%) and improving performance
across various baseline detectors.

</details>


### [308] [Q-VDiT: Towards Accurate Quantization and Distillation of Video-Generation Diffusion Transformers](https://arxiv.org/pdf/2505.22167)
*Weilun Feng, Chuanguang Yang, Haotong Qin, Xiangqi Li, Yu Wang, Zhulin An, Libo Huang, Boyu Diao, Zixiang Zhao, Yongjun Xu, Michele Magno*

Main category: cs.CV

TL;DR: Q-VDiT is a quantization framework for video diffusion transformers (DiT) that addresses challenges in video generation tasks by introducing Token-aware Quantization Estimator (TQE) and Temporal Maintenance Distillation (TMD).


<details>
  <summary>Details</summary>
Motivation: Existing quantization methods for image generation models fail to generalize to video generation due to information loss and misaligned optimization objectives.

Method: Proposes TQE for error compensation in token and feature dimensions and TMD to preserve spatiotemporal correlations.

Result: Achieves a scene consistency of 23.40, outperforming state-of-the-art methods by 1.9×.

Conclusion: Q-VDiT sets a new benchmark for quantized video DiT models, enabling efficient deployment on edge devices.

Abstract: Diffusion transformers (DiT) have demonstrated exceptional performance in
video generation. However, their large number of parameters and high
computational complexity limit their deployment on edge devices. Quantization
can reduce storage requirements and accelerate inference by lowering the
bit-width of model parameters. Yet, existing quantization methods for image
generation models do not generalize well to video generation tasks. We identify
two primary challenges: the loss of information during quantization and the
misalignment between optimization objectives and the unique requirements of
video generation. To address these challenges, we present Q-VDiT, a
quantization framework specifically designed for video DiT models. From the
quantization perspective, we propose the Token-aware Quantization Estimator
(TQE), which compensates for quantization errors in both the token and feature
dimensions. From the optimization perspective, we introduce Temporal
Maintenance Distillation (TMD), which preserves the spatiotemporal correlations
between frames and enables the optimization of each frame with respect to the
overall video context. Our W3A6 Q-VDiT achieves a scene consistency of 23.40,
setting a new benchmark and outperforming current state-of-the-art quantization
methods by 1.9$\times$. Code will be available at
https://github.com/cantbebetter2/Q-VDiT.

</details>


### [309] [S2AFormer: Strip Self-Attention for Efficient Vision Transformer](https://arxiv.org/pdf/2505.22195)
*Guoan Xu, Wenfeng Huang, Wenjing Jia, Jiamao Li, Guangwei Gao, Guo-Jun Qi*

Main category: cs.CV

TL;DR: S2AFormer introduces Strip Self-Attention (SSA) to reduce computational costs in Vision Transformers while maintaining accuracy, combining CNN's local perception with Transformer's global context.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency of Vision Transformers due to quadratic computational growth with token count and complex self-attention operations.

Method: Proposes S2AFormer with Hybrid Perception Blocks (HPBs) and SSA, reducing spatial and channel dimensions of attention matrices to cut costs.

Result: Achieves significant accuracy gains with superior efficiency on ImageNet-1k, ADE20k, and COCO benchmarks.

Conclusion: S2AFormer balances efficiency and effectiveness, making it a strong candidate for practical Vision Transformer applications.

Abstract: Vision Transformer (ViT) has made significant advancements in computer
vision, thanks to its token mixer's sophisticated ability to capture global
dependencies between all tokens. However, the quadratic growth in computational
demands as the number of tokens increases limits its practical efficiency.
Although recent methods have combined the strengths of convolutions and
self-attention to achieve better trade-offs, the expensive pairwise token
affinity and complex matrix operations inherent in self-attention remain a
bottleneck. To address this challenge, we propose S2AFormer, an efficient
Vision Transformer architecture featuring novel Strip Self-Attention (SSA). We
design simple yet effective Hybrid Perception Blocks (HPBs) to effectively
integrate the local perception capabilities of CNNs with the global context
modeling of Transformer's attention mechanisms. A key innovation of SSA lies in
its reducing the spatial dimensions of $K$ and $V$ while compressing the
channel dimensions of $Q$ and $K$. This design significantly reduces
computational overhead while preserving accuracy, striking an optimal balance
between efficiency and effectiveness. We evaluate the robustness and efficiency
of S2AFormer through extensive experiments on multiple vision benchmarks,
including ImageNet-1k for image classification, ADE20k for semantic
segmentation, and COCO for object detection and instance segmentation. Results
demonstrate that S2AFormer achieves significant accuracy gains with superior
efficiency in both GPU and non-GPU environments, making it a strong candidate
for efficient vision Transformers.

</details>


### [310] [VME: A Satellite Imagery Dataset and Benchmark for Detecting Vehicles in the Middle East and Beyond](https://arxiv.org/pdf/2505.22353)
*Noora Al-Emadi, Ingmar Weber, Yin Yang, Ferda Ofli*

Main category: cs.CV

TL;DR: The paper introduces the VME dataset for vehicle detection in Middle Eastern satellite images and the CDSI benchmark for global car detection, showing improved accuracy over existing datasets.


<details>
  <summary>Details</summary>
Motivation: Addressing geographic bias in vehicle detection datasets, particularly the lack of Middle Eastern data, to enhance traffic management and urban planning.

Method: Creation of the VME dataset (54 cities, 12 countries, 4,000+ images, 100,000+ vehicles) and CDSI benchmark, using manual and semi-automated annotations.

Result: Models trained on existing datasets perform poorly in the Middle East; VME improves accuracy there, and CDSI enhances global detection.

Conclusion: The VME and CDSI datasets effectively address geographic bias and improve vehicle detection accuracy in underrepresented regions and globally.

Abstract: Detecting vehicles in satellite images is crucial for traffic management,
urban planning, and disaster response. However, current models struggle with
real-world diversity, particularly across different regions. This challenge is
amplified by geographic bias in existing datasets, which often focus on
specific areas and overlook regions like the Middle East. To address this gap,
we present the Vehicles in the Middle East (VME) dataset, designed explicitly
for vehicle detection in high-resolution satellite images from Middle Eastern
countries. Sourced from Maxar, the VME dataset spans 54 cities across 12
countries, comprising over 4,000 image tiles and more than 100,000 vehicles,
annotated using both manual and semi-automated methods. Additionally, we
introduce the largest benchmark dataset for Car Detection in Satellite Imagery
(CDSI), combining images from multiple sources to enhance global car detection.
Our experiments demonstrate that models trained on existing datasets perform
poorly on Middle Eastern images, while the VME dataset significantly improves
detection accuracy in this region. Moreover, state-of-the-art models trained on
CDSI achieve substantial improvements in global car detection.

</details>


### [311] [A Survey on Training-free Open-Vocabulary Semantic Segmentation](https://arxiv.org/pdf/2505.22209)
*Naomi Kombol, Ivan Martinović, Siniša Šegvić*

Main category: cs.CV

TL;DR: A survey on training-free open-vocabulary semantic segmentation, leveraging multi-modal models like CLIP, to avoid costly data annotation and computational resources.


<details>
  <summary>Details</summary>
Motivation: Traditional semantic segmentation requires extensive training data and resources. Open-vocabulary segmentation expands classification beyond learned categories, but annotated data is expensive. Training-free methods using existing models offer a solution.

Method: The survey reviews over 30 approaches, categorized into CLIP-based, auxiliary visual foundation models, and generative methods. It covers history, model archetypes, and idea development.

Result: Highlights state-of-the-art training-free methods, their nuances, and limitations.

Conclusion: The survey aims to onboard new researchers and inspire future work in open-vocabulary semantic segmentation, addressing current gaps and underexplored ideas.

Abstract: Semantic segmentation is one of the most fundamental tasks in image
understanding with a long history of research, and subsequently a myriad of
different approaches. Traditional methods strive to train models up from
scratch, requiring vast amounts of computational resources and training data.
In the advent of moving to open-vocabulary semantic segmentation, which asks
models to classify beyond learned categories, large quantities of finely
annotated data would be prohibitively expensive. Researchers have instead
turned to training-free methods where they leverage existing models made for
tasks where data is more easily acquired. Specifically, this survey will cover
the history, nuance, idea development and the state-of-the-art in training-free
open-vocabulary semantic segmentation that leverages existing multi-modal
classification models. We will first give a preliminary on the task definition
followed by an overview of popular model archetypes and then spotlight over 30
approaches split into broader research branches: purely CLIP-based, those
leveraging auxiliary visual foundation models and ones relying on generative
methods. Subsequently, we will discuss the limitations and potential problems
of current research, as well as provide some underexplored ideas for future
study. We believe this survey will serve as a good onboarding read to new
researchers and spark increased interest in the area.

</details>


### [312] [Look & Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest X-ray Report Generation](https://arxiv.org/pdf/2505.22222)
*Yunsoo Kim, Jinge Wu, Su-Hwan Kim, Pardeep Vasudev, Jiashu Shen, Honghan Wu*

Main category: cs.CV

TL;DR: The paper introduces Look & Mark (L&M), a grounding fixation strategy for multimodal LLMs in medical image analysis, reducing errors and improving performance without retraining.


<details>
  <summary>Details</summary>
Motivation: Current multimodal LLMs for medical image analysis suffer from hallucinations and errors, limiting reliability in real-world applications.

Method: L&M integrates radiologist eye fixations (Look) and bounding box annotations (Mark) into LLM prompting, using in-context learning instead of fine-tuning.

Result: L&M improves performance metrics (e.g., 1.2% for CXR-LLaVA, 9.2% for LLaVA-Med) and reduces clinical errors (0.43 fewer per report).

Conclusion: L&M is a scalable, efficient solution for AI-assisted radiology, enhancing accuracy and reliability in low-resource settings.

Abstract: Recent advancements in multimodal Large Language Models (LLMs) have
significantly enhanced the automation of medical image analysis, particularly
in generating radiology reports from chest X-rays (CXR). However, these models
still suffer from hallucinations and clinically significant errors, limiting
their reliability in real-world applications. In this study, we propose Look &
Mark (L&M), a novel grounding fixation strategy that integrates radiologist eye
fixations (Look) and bounding box annotations (Mark) into the LLM prompting
framework. Unlike conventional fine-tuning, L&M leverages in-context learning
to achieve substantial performance gains without retraining. When evaluated
across multiple domain-specific and general-purpose models, L&M demonstrates
significant gains, including a 1.2% improvement in overall metrics (A.AVG) for
CXR-LLaVA compared to baseline prompting and a remarkable 9.2% boost for
LLaVA-Med. General-purpose models also benefit from L&M combined with
in-context learning, with LLaVA-OV achieving an 87.3% clinical average
performance (C.AVG)-the highest among all models, even surpassing those
explicitly trained for CXR report generation. Expert evaluations further
confirm that L&M reduces clinically significant errors (by 0.43 average errors
per report), such as false predictions and omissions, enhancing both accuracy
and reliability. These findings highlight L&M's potential as a scalable and
efficient solution for AI-assisted radiology, paving the way for improved
diagnostic workflows in low-resource clinical settings.

</details>


### [313] [Hadaptive-Net: Efficient Vision Models via Adaptive Cross-Hadamard Synergy](https://arxiv.org/pdf/2505.22226)
*Xuyang Zhang, Xi Zhang, Liang Chen, Hao Shi, Qingshan Guo*

Main category: cs.CV

TL;DR: The paper introduces Adaptive Cross-Hadamard (ACH) and Hadaptive-Net, leveraging Hadamard product for efficient channel expansion and improved network performance.


<details>
  <summary>Details</summary>
Motivation: Despite the theoretical promise of Hadamard product in enhancing network representational capacity, its practical applications remain underexplored.

Method: Proposes ACH, a module using adaptive cross-channel Hadamard products, and Hadaptive-Net, a lightweight network backbone for visual tasks.

Result: Hadaptive-Net achieves a balance between inference speed and accuracy, outperforming standard convolutional operations.

Conclusion: The work demonstrates the practical potential of Hadamard product in network design, offering a lightweight yet effective solution.

Abstract: Recent studies have revealed the immense potential of Hadamard product in
enhancing network representational capacity and dimensional compression.
However, despite its theoretical promise, this technique has not been
systematically explored or effectively applied in practice, leaving its full
capabilities underdeveloped. In this work, we first analyze and identify the
advantages of Hadamard product over standard convolutional operations in
cross-channel interaction and channel expansion. Building upon these insights,
we propose a computationally efficient module: Adaptive Cross-Hadamard (ACH),
which leverages adaptive cross-channel Hadamard products for high-dimensional
channel expansion. Furthermore, we introduce Hadaptive-Net (Hadamard Adaptive
Network), a lightweight network backbone for visual tasks, which is
demonstrated through experiments that it achieves an unprecedented balance
between inference speed and accuracy through our proposed module.

</details>


### [314] [GoMatching++: Parameter- and Data-Efficient Arbitrary-Shaped Video Text Spotting and Benchmarking](https://arxiv.org/pdf/2505.22228)
*Haibin He, Jing Zhang, Maoyuan Ye, Juhua Liu, Bo Du, Dacheng Tao*

Main category: cs.CV

TL;DR: GoMatching++ improves video text spotting by enhancing an off-the-shelf image text spotter with a lightweight tracker, achieving state-of-the-art performance with reduced training costs.


<details>
  <summary>Details</summary>
Motivation: Current video text spotters underperform compared to image text spotters due to limited recognition capability, despite extensive training.

Method: Freezes an image text spotter and adds a trainable tracker (LST-Matcher) with a rescoring mechanism to bridge the image-video domain gap.

Result: Sets new records on benchmarks (ICDAR15-video, DSText, BOVText) and introduces ArTVideo, a curved text dataset.

Conclusion: GoMatching++ and ArTVideo advance video text spotting with efficiency and new benchmarks.

Abstract: Video text spotting (VTS) extends image text spotting (ITS) by adding text
tracking, significantly increasing task complexity. Despite progress in VTS,
existing methods still fall short of the performance seen in ITS. This paper
identifies a key limitation in current video text spotters: limited recognition
capability, even after extensive end-to-end training. To address this, we
propose GoMatching++, a parameter- and data-efficient method that transforms an
off-the-shelf image text spotter into a video specialist. The core idea lies in
freezing the image text spotter and introducing a lightweight, trainable
tracker, which can be optimized efficiently with minimal training data. Our
approach includes two key components: (1) a rescoring mechanism to bridge the
domain gap between image and video data, and (2) the LST-Matcher, which
enhances the frozen image text spotter's ability to handle video text. We
explore various architectures for LST-Matcher to ensure efficiency in both
parameters and training data. As a result, GoMatching++ sets new performance
records on challenging benchmarks such as ICDAR15-video, DSText, and BOVText,
while significantly reducing training costs. To address the lack of curved text
datasets in VTS, we introduce ArTVideo, a new benchmark featuring over 30%
curved text with detailed annotations. We also provide a comprehensive
statistical analysis and experimental results for ArTVideo. We believe that
GoMatching++ and the ArTVideo benchmark will drive future advancements in video
text spotting. The source code, models and dataset are publicly available at
https://github.com/Hxyz-123/GoMatching.

</details>


### [315] [Enjoying Information Dividend: Gaze Track-based Medical Weakly Supervised Segmentation](https://arxiv.org/pdf/2505.22230)
*Zhisong Wang, Yiwen Ye, Ziyang Chen, Yong Xia*

Main category: cs.CV

TL;DR: GradTrack improves weakly supervised semantic segmentation in medical imaging by leveraging detailed gaze data, outperforming existing methods and narrowing the gap with fully supervised models.


<details>
  <summary>Details</summary>
Motivation: Existing gaze-based methods like GazeMedSeg underutilize gaze data, prompting the need for a more effective framework.

Method: GradTrack uses gaze tracks (fixation points, durations, temporal order) with Gaze Track Map Generation and Track Attention for progressive feature refinement.

Result: GradTrack outperforms gaze-based methods on Kvasir-SEG and NCI-ISBI datasets, improving Dice scores by 3.21% and 2.61%, respectively.

Conclusion: GradTrack effectively enhances WSSS performance and reduces the gap with fully supervised models like nnUNet.

Abstract: Weakly supervised semantic segmentation (WSSS) in medical imaging struggles
with effectively using sparse annotations. One promising direction for WSSS
leverages gaze annotations, captured via eye trackers that record regions of
interest during diagnostic procedures. However, existing gaze-based methods,
such as GazeMedSeg, do not fully exploit the rich information embedded in gaze
data. In this paper, we propose GradTrack, a framework that utilizes
physicians' gaze track, including fixation points, durations, and temporal
order, to enhance WSSS performance. GradTrack comprises two key components:
Gaze Track Map Generation and Track Attention, which collaboratively enable
progressive feature refinement through multi-level gaze supervision during the
decoding process. Experiments on the Kvasir-SEG and NCI-ISBI datasets
demonstrate that GradTrack consistently outperforms existing gaze-based
methods, achieving Dice score improvements of 3.21\% and 2.61\%, respectively.
Moreover, GradTrack significantly narrows the performance gap with fully
supervised models such as nnUNet.

</details>


### [316] [DAM: Domain-Aware Module for Multi-Domain Dataset Condensation](https://arxiv.org/pdf/2505.22387)
*Jaehyun Choi, Gyojin Han, Dong-Jae Lee, Sunghyun Baek, Junmo Kim*

Main category: cs.CV

TL;DR: MDDC introduces a Domain-Aware Module (DAM) to condense multi-domain datasets, improving generalization across domains without explicit labels.


<details>
  <summary>Details</summary>
Motivation: Modern datasets are multi-domain, but current DC methods ignore this, limiting their effectiveness.

Method: Proposes DAM, embedding domain features via learnable masks and pseudo-domain labels using frequency statistics.

Result: DAM enhances performance in in-domain, out-of-domain, and cross-architecture settings.

Conclusion: MDDC with DAM is a scalable solution for multi-domain dataset condensation.

Abstract: Dataset Condensation (DC) has emerged as a promising solution to mitigate the
computational and storage burdens associated with training deep learning
models. However, existing DC methods largely overlook the multi-domain nature
of modern datasets, which are increasingly composed of heterogeneous images
spanning multiple domains. In this paper, we extend DC and introduce
Multi-Domain Dataset Condensation (MDDC), which aims to condense data that
generalizes across both single-domain and multi-domain settings. To this end,
we propose the Domain-Aware Module (DAM), a training-time module that embeds
domain-related features into each synthetic image via learnable spatial masks.
As explicit domain labels are mostly unavailable in real-world datasets, we
employ frequency-based pseudo-domain labeling, which leverages low-frequency
amplitude statistics. DAM is only active during the condensation process, thus
preserving the same images per class (IPC) with prior methods. Experiments show
that DAM consistently improves in-domain, out-of-domain, and cross-architecture
performance over baseline dataset condensation methods.

</details>


### [317] [StateSpaceDiffuser: Bringing Long Context to Diffusion World Models](https://arxiv.org/pdf/2505.22246)
*Nedko Savov, Naser Kazemi, Deheng Zhang, Danda Pani Paudel, Xi Wang, Luc Van Gool*

Main category: cs.CV

TL;DR: StateSpaceDiffuser integrates a state-space model (Mamba) with diffusion models to address the short-term memory limitation in world models, enabling long-context tasks while maintaining high-fidelity visuals.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based world models lose context quickly due to lack of lasting environment state, leading to visual inconsistency.

Method: Combines diffusion models with a state-space model (Mamba) to represent interaction history, restoring long-term memory.

Result: Outperforms diffusion-only baselines, maintaining visual coherence for significantly more steps in 2D and 3D environments.

Conclusion: Integrating state-space representations into diffusion models effectively balances visual detail and long-term memory.

Abstract: World models have recently become promising tools for predicting realistic
visuals based on actions in complex environments. However, their reliance on a
short sequence of observations causes them to quickly lose track of context. As
a result, visual consistency breaks down after just a few steps, and generated
scenes no longer reflect information seen earlier. This limitation of the
state-of-the-art diffusion-based world models comes from their lack of a
lasting environment state. To address this problem, we introduce
StateSpaceDiffuser, where a diffusion model is enabled to perform on
long-context tasks by integrating a sequence representation from a state-space
model (Mamba), representing the entire interaction history. This design
restores long-term memory without sacrificing the high-fidelity synthesis of
diffusion models. To rigorously measure temporal consistency, we develop an
evaluation protocol that probes a model's ability to reinstantiate seen content
in extended rollouts. Comprehensive experiments show that StateSpaceDiffuser
significantly outperforms a strong diffusion-only baseline, maintaining a
coherent visual context for an order of magnitude more steps. It delivers
consistent views in both a 2D maze navigation and a complex 3D environment.
These results establish that bringing state-space representations into
diffusion models is highly effective in demonstrating both visual details and
long-term memory.

</details>


### [318] [YH-MINER: Multimodal Intelligent System for Natural Ecological Reef Metric Extraction](https://arxiv.org/pdf/2505.22250)
*Mingzhuang Wang, Yvyang Li, Xiyang Zhang, Fei Tan, Qi Shi, Guotao Zhang, Siqi Chen, Yufei Liu, Lei Lei, Ming Zhou, Qiang Lin, Hongqiang Yang*

Main category: cs.CV

TL;DR: The paper introduces the YH-OSI system, an intelligent framework using a Multimodal Large Model (MLLM) for coral reef monitoring, combining object detection, semantic segmentation, and prior input to achieve high accuracy in genus-level classification and ecological metric extraction.


<details>
  <summary>Details</summary>
Motivation: Coral reefs are vital for marine biodiversity but face threats, requiring efficient monitoring. Current methods suffer from low efficiency and poor segmentation accuracy in complex underwater environments.

Method: The YH-OSI system integrates object detection (mAP@0.5=0.78) to generate spatial prior boxes, drives semantic segmentation for pixel-level accuracy, and uses a Qwen2-VL-based multimodal model for classification (88% accuracy) and ecological metric extraction.

Result: The system achieves 88% genus-level classification accuracy and extracts core ecological metrics, with potential for future integration into underwater robots for full-process automation.

Conclusion: The YH-OSI system addresses monitoring challenges, offering high accuracy and scalability, and paves the way for automated underwater ecological analysis.

Abstract: Coral reefs, crucial for sustaining marine biodiversity and ecological
processes (e.g., nutrient cycling, habitat provision), face escalating threats,
underscoring the need for efficient monitoring. Coral reef ecological
monitoring faces dual challenges of low efficiency in manual analysis and
insufficient segmentation accuracy in complex underwater scenarios. This study
develops the YH-OSI system, establishing an intelligent framework centered on
the Multimodal Large Model (MLLM) for "object detection-semantic
segmentation-prior input". The system uses the object detection module
(mAP@0.5=0.78) to generate spatial prior boxes for coral instances, driving the
segment module to complete pixel-level segmentation in low-light and densely
occluded scenarios. The segmentation masks and finetuned classification
instructions are fed into the Qwen2-VL-based multimodal model as prior inputs,
achieving a genus-level classification accuracy of 88% and simultaneously
extracting core ecological metrics. Meanwhile, the system retains the
scalability of the multimodal model through standardized interfaces, laying a
foundation for future integration into multimodal agent-based underwater robots
and supporting the full-process automation of "image acquisition-prior
generation-real-time analysis."

</details>


### [319] [Domain Adaptation of Attention Heads for Zero-shot Anomaly Detection](https://arxiv.org/pdf/2505.22259)
*Kiyoon Jeong, Jaehyuk Heo, Junyeong Son, Pilsung Kang*

Main category: cs.CV

TL;DR: HeadCLIP improves zero-shot anomaly detection by adapting text and image encoders to the domain, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing ZSAD methods lack full domain adaptation, limiting performance.

Method: HeadCLIP uses learnable prompts for text and dynamic head weights for images, with a joint anomaly score.

Result: Achieves up to 4.9%p (pixel) and 3.0%p (image) mAD improvements in industrial, and 3.2%p, 3.1%p in medical domains.

Conclusion: HeadCLIP effectively addresses domain adaptation in ZSAD, demonstrating superior performance.

Abstract: Zero-shot anomaly detection (ZSAD) in images is an approach that can detect
anomalies without access to normal samples, which can be beneficial in various
realistic scenarios where model training is not possible. However, existing
ZSAD research has shown limitations by either not considering domain adaptation
of general-purpose backbone models to anomaly detection domains or by
implementing only partial adaptation to some model components. In this paper,
we propose HeadCLIP to overcome these limitations by effectively adapting both
text and image encoders to the domain. HeadCLIP generalizes the concepts of
normality and abnormality through learnable prompts in the text encoder, and
introduces learnable head weights to the image encoder to dynamically adjust
the features held by each attention head according to domain characteristics.
Additionally, we maximize the effect of domain adaptation by introducing a
joint anomaly score that utilizes domain-adapted pixel-level information for
image-level anomaly detection. Experimental results using multiple real
datasets in both industrial and medical domains show that HeadCLIP outperforms
existing ZSAD techniques at both pixel and image levels. In the industrial
domain, improvements of up to 4.9%p in pixel-level mean anomaly detection score
(mAD) and up to 3.0%p in image-level mAD were achieved, with similar
improvements (3.2%p, 3.1%p) in the medical domain.

</details>


### [320] [Learning Fine-Grained Geometry for Sparse-View Splatting via Cascade Depth Loss](https://arxiv.org/pdf/2505.22279)
*Wenjun Lu, Haodong Chen, Anqi Yi, Yuk Ying Chung, Zhiyong Wang, Kun Hu*

Main category: cs.CV

TL;DR: HDGS improves sparse-view novel view synthesis by using hierarchical depth guidance and multi-scale depth consistency.


<details>
  <summary>Details</summary>
Motivation: Existing methods like NeRF and 3DGS struggle with blurred details and artifacts in sparse-view conditions due to limited geometric cues.

Method: Introduces Hierarchical Depth-Guided Splatting (HDGS) with Cascade Pearson Correlation Loss (CPCL) for multi-scale depth alignment.

Result: HDGS achieves state-of-the-art performance on LLFF and DTU benchmarks, enhancing structural fidelity in sparse-view scenarios.

Conclusion: HDGS effectively addresses sparse-view challenges, improving rendering quality and geometric accuracy.

Abstract: Novel view synthesis is a fundamental task in 3D computer vision that aims to
reconstruct realistic images from a set of posed input views. However,
reconstruction quality degrades significantly under sparse-view conditions due
to limited geometric cues. Existing methods, such as Neural Radiance Fields
(NeRF) and the more recent 3D Gaussian Splatting (3DGS), often suffer from
blurred details and structural artifacts when trained with insufficient views.
Recent works have identified the quality of rendered depth as a key factor in
mitigating these artifacts, as it directly affects geometric accuracy and view
consistency. In this paper, we address these challenges by introducing
Hierarchical Depth-Guided Splatting (HDGS), a depth supervision framework that
progressively refines geometry from coarse to fine levels. Central to HDGS is a
novel Cascade Pearson Correlation Loss (CPCL), which aligns rendered and
estimated monocular depths across multiple spatial scales. By enforcing
multi-scale depth consistency, our method substantially improves structural
fidelity in sparse-view scenarios. Extensive experiments on the LLFF and DTU
benchmarks demonstrate that HDGS achieves state-of-the-art performance under
sparse-view settings while maintaining efficient and high-quality rendering

</details>


### [321] [Can NeRFs See without Cameras?](https://arxiv.org/pdf/2505.22441)
*Chaitanya Amballa, Sattwik Basu, Yu-Lin Wei, Zhijian Yang, Mehmet Ergezer, Romit Roy Choudhury*

Main category: cs.CV

TL;DR: NeRFs can be adapted to learn from multipath RF/audio signals, enabling environment inference, such as indoor floorplan mapping from WiFi measurements.


<details>
  <summary>Details</summary>
Motivation: To explore if NeRFs can infer environments from multipath signals like RF/audio, unlike camera pixels which capture direct optical rays.

Method: Redesign NeRFs to learn from multipath signals, applying it to infer indoor floorplans from sparse WiFi measurements.

Result: Implicitly learned floorplans show promise, enabling applications like indoor signal prediction and basic ray tracing.

Conclusion: NeRFs can be successfully adapted to learn from multipath signals, opening new possibilities for environment inference.

Abstract: Neural Radiance Fields (NeRFs) have been remarkably successful at
synthesizing novel views of 3D scenes by optimizing a volumetric scene
function. This scene function models how optical rays bring color information
from a 3D object to the camera pixels. Radio frequency (RF) or audio signals
can also be viewed as a vehicle for delivering information about the
environment to a sensor. However, unlike camera pixels, an RF/audio sensor
receives a mixture of signals that contain many environmental reflections (also
called "multipath"). Is it still possible to infer the environment using such
multipath signals? We show that with redesign, NeRFs can be taught to learn
from multipath signals, and thereby "see" the environment. As a grounding
application, we aim to infer the indoor floorplan of a home from sparse WiFi
measurements made at multiple locations inside the home. Although a difficult
inverse problem, our implicitly learnt floorplans look promising, and enables
forward applications, such as indoor signal prediction and basic ray tracing.

</details>


### [322] [From Controlled Scenarios to Real-World: Cross-Domain Degradation Pattern Matching for All-in-One Image Restoration](https://arxiv.org/pdf/2505.22284)
*Junyu Fan, Chuanlin Liao, Yi Lin*

Main category: cs.CV

TL;DR: A new framework, UDAIR, improves All-in-One Image Restoration (AiOIR) by addressing domain gaps between training and real-world data, using a codebook for degradation identification and domain adaptation strategies.


<details>
  <summary>Details</summary>
Motivation: Existing AiOIR methods perform well in controlled settings but struggle in real-world scenarios due to domain gaps, leading to poor degradation awareness.

Method: UDAIR uses a codebook for discrete degradation embeddings and cross-sample contrastive learning. It employs domain adaptation and test-time adaptation to align source and target domains.

Result: UDAIR achieves state-of-the-art performance on 10 datasets, with strong generalization to real-world scenarios.

Conclusion: UDAIR effectively bridges domain gaps and enhances degradation identification, making it robust for real-world AiOIR tasks.

Abstract: As a fundamental imaging task, All-in-One Image Restoration (AiOIR) aims to
achieve image restoration caused by multiple degradation patterns via a single
model with unified parameters. Although existing AiOIR approaches obtain
promising performance in closed and controlled scenarios, they still suffered
from considerable performance reduction in real-world scenarios since the gap
of data distributions between the training samples (source domain) and
real-world test samples (target domain) can lead inferior degradation awareness
ability. To address this issue, a Unified Domain-Adaptive Image Restoration
(UDAIR) framework is proposed to effectively achieve AiOIR by leveraging the
learned knowledge from source domain to target domain. To improve the
degradation identification, a codebook is designed to learn a group of discrete
embeddings to denote the degradation patterns, and the cross-sample contrastive
learning mechanism is further proposed to capture shared features from
different samples of certain degradation. To bridge the data gap, a domain
adaptation strategy is proposed to build the feature projection between the
source and target domains by dynamically aligning their codebook embeddings,
and a correlation alignment-based test-time adaptation mechanism is designed to
fine-tune the alignment discrepancies by tightening the degradation embeddings
to the corresponding cluster center in the source domain. Experimental results
on 10 open-source datasets demonstrate that UDAIR achieves new state-of-the-art
performance for the AiOIR task. Most importantly, the feature cluster validate
the degradation identification under unknown conditions, and qualitative
comparisons showcase robust generalization to real-world scenarios.

</details>


### [323] [NFR: Neural Feature-Guided Non-Rigid Shape Registration](https://arxiv.org/pdf/2505.22445)
*Puhua Jiang, Zhangquan Chen, Mingze Sun, Ruqi Huang*

Main category: cs.CV

TL;DR: A novel learning-based framework for 3D shape registration using neural features, achieving state-of-the-art results without correspondence annotations.


<details>
  <summary>Details</summary>
Motivation: Overcoming challenges of non-rigid deformation and partiality in 3D shape registration without requiring annotated correspondences.

Method: Incorporates neural features from deep learning into an iterative geometric pipeline, dynamically updating and filtering correspondences.

Result: Achieves top performance on benchmarks for non-rigid and partial shape matching, even with limited training data.

Conclusion: The framework robustly handles challenging deformations, outperforming traditional and intrinsic methods.

Abstract: In this paper, we propose a novel learning-based framework for 3D shape
registration, which overcomes the challenges of significant non-rigid
deformation and partiality undergoing among input shapes, and, remarkably,
requires no correspondence annotation during training. Our key insight is to
incorporate neural features learned by deep learning-based shape matching
networks into an iterative, geometric shape registration pipeline. The
advantage of our approach is two-fold -- On one hand, neural features provide
more accurate and semantically meaningful correspondence estimation than
spatial features (e.g., coordinates), which is critical in the presence of
large non-rigid deformations; On the other hand, the correspondences are
dynamically updated according to the intermediate registrations and filtered by
consistency prior, which prominently robustify the overall pipeline. Empirical
results show that, with as few as dozens of training shapes of limited
variability, our pipeline achieves state-of-the-art results on several
benchmarks of non-rigid point cloud matching and partial shape matching across
varying settings, but also delivers high-quality correspondences between unseen
challenging shape pairs that undergo both significant extrinsic and intrinsic
deformations, in which case neither traditional registration methods nor
intrinsic methods work.

</details>


### [324] [CADReview: Automatically Reviewing CAD Programs with Error Detection and Correction](https://arxiv.org/pdf/2505.22304)
*Jiali Chen, Xusen Hei, HongFei Liu, Yuancheng Wei, Zikun Deng, Jiayuan Xie, Yi Cai, Li Qing*

Main category: cs.CV

TL;DR: The paper introduces ReCAD, a framework for automatically detecting and correcting errors in CAD programs to align 3D objects with reference images, outperforming existing MLLMs.


<details>
  <summary>Details</summary>
Motivation: Designers spend excessive time reviewing and refining CAD prototypes against reference images, highlighting the need for automated error detection and correction.

Method: The proposed ReCAD framework detects program errors and provides feedback for correction, supported by a new dataset (CADReview) with 20K program-image pairs.

Result: ReCAD significantly outperforms existing MLLMs in the CAD review task, demonstrating its effectiveness.

Conclusion: ReCAD shows great potential for automating CAD design workflows, improving efficiency and accuracy.

Abstract: Computer-aided design (CAD) is crucial in prototyping 3D objects through
geometric instructions (i.e., CAD programs). In practical design workflows,
designers often engage in time-consuming reviews and refinements of these
prototypes by comparing them with reference images. To bridge this gap, we
introduce the CAD review task to automatically detect and correct potential
errors, ensuring consistency between the constructed 3D objects and reference
images. However, recent advanced multimodal large language models (MLLMs)
struggle to recognize multiple geometric components and perform spatial
geometric operations within the CAD program, leading to inaccurate reviews. In
this paper, we propose the CAD program repairer (ReCAD) framework to
effectively detect program errors and provide helpful feedback on error
correction. Additionally, we create a dataset, CADReview, consisting of over
20K program-image pairs, with diverse errors for the CAD review task. Extensive
experiments demonstrate that our ReCAD significantly outperforms existing
MLLMs, which shows great potential in design applications.

</details>


### [325] [Fostering Video Reasoning via Next-Event Prediction](https://arxiv.org/pdf/2505.22457)
*Haonan Wang, Hongfu Liu, Xiangyan Liu, Chao Du, Kenji Kawaguchi, Ye Wang, Tianyu Pang*

Main category: cs.CV

TL;DR: The paper proposes next-event prediction (NEP) as a self-supervised task for temporal reasoning in MLLMs, using future video segments as training signals. It introduces V1-33K dataset and FutureBench for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing tasks like video QA rely on annotations or entangle temporal and spatial reasoning. NEP aims to provide a scalable, self-supervised solution for temporal reasoning in MLLMs.

Method: Segments videos into past and future frames; MLLMs predict future event summaries from past frames. Uses V1-33K dataset and explores video instruction-tuning strategies.

Result: NEP proves scalable and effective for temporal reasoning, validated by experiments.

Conclusion: NEP is a promising paradigm for enhancing temporal reasoning in MLLMs, supported by dataset and benchmark contributions.

Abstract: Next-token prediction serves as the foundational learning task enabling
reasoning in LLMs. But what should the learning task be when aiming to equip
MLLMs with temporal reasoning capabilities over video inputs? Existing tasks
such as video question answering often rely on annotations from humans or much
stronger MLLMs, while video captioning tends to entangle temporal reasoning
with spatial information. To address this gap, we propose next-event prediction
(NEP), a learning task that harnesses future video segments as a rich,
self-supervised signal to foster temporal reasoning. We segment each video into
past and future frames: the MLLM takes the past frames as input and predicts a
summary of events derived from the future frames, thereby encouraging the model
to reason temporally in order to complete the task. To support this task, we
curate V1-33K, a dataset comprising 33,000 automatically extracted video
segments spanning diverse real-world scenarios. We further explore a range of
video instruction-tuning strategies to study their effects on temporal
reasoning. To evaluate progress, we introduce FutureBench to assess coherence
in predicting unseen future events. Experiments validate that NEP offers a
scalable and effective training paradigm for fostering temporal reasoning in
MLLMs.

</details>


### [326] [IKIWISI: An Interactive Visual Pattern Generator for Evaluating the Reliability of Vision-Language Models Without Ground Truth](https://arxiv.org/pdf/2505.22305)
*Md Touhidul Islam, Imran Kabir, Md Alimoor Reza, Syed Masum Billah*

Main category: cs.CV

TL;DR: IKIWISI is an interactive tool for evaluating vision-language models using binary heatmaps and spy objects to detect hallucinations, aiding human assessment of model reliability.


<details>
  <summary>Details</summary>
Motivation: To assess vision-language models in video object recognition without ground truth, leveraging human pattern recognition to evaluate model reliability and alignment with human perception.

Method: IKIWISI transforms model outputs into binary heatmaps (green for object presence, red for absence) and uses spy objects (adversarial instances) to detect model hallucinations.

Result: Users found IKIWISI easy to use, with assessments correlating to objective metrics, and could evaluate models by examining a small fraction of heatmap cells.

Conclusion: IKIWISI complements traditional evaluation methods, revealing opportunities to improve alignment between human and machine perception in vision-language systems.

Abstract: We present IKIWISI ("I Know It When I See It"), an interactive visual pattern
generator for assessing vision-language models in video object recognition when
ground truth is unavailable. IKIWISI transforms model outputs into a binary
heatmap where green cells indicate object presence and red cells indicate
object absence. This visualization leverages humans' innate pattern recognition
abilities to evaluate model reliability. IKIWISI introduces "spy objects":
adversarial instances users know are absent, to discern models hallucinating on
nonexistent items. The tool functions as a cognitive audit mechanism, surfacing
mismatches between human and machine perception by visualizing where models
diverge from human understanding.
  Our study with 15 participants found that users considered IKIWISI easy to
use, made assessments that correlated with objective metrics when available,
and reached informed conclusions by examining only a small fraction of heatmap
cells. This approach not only complements traditional evaluation methods
through visual assessment of model behavior with custom object sets, but also
reveals opportunities for improving alignment between human perception and
machine understanding in vision-language systems.

</details>


### [327] [Thinking with Generated Images](https://arxiv.org/pdf/2505.22525)
*Ethan Chern, Zhulin Hu, Steffi Chern, Siqi Kou, Jiadi Su, Yan Ma, Zhijie Deng, Pengfei Liu*

Main category: cs.CV

TL;DR: A novel paradigm enabling large multimodal models (LMMs) to generate intermediate visual thoughts for enhanced visual reasoning, outperforming baselines by 50% in complex scenarios.


<details>
  <summary>Details</summary>
Motivation: Current LMMs are limited to fixed images or text-based reasoning, lacking the ability to dynamically generate and refine visual thoughts.

Method: Two mechanisms: (1) decomposing tasks into visual subgoals for progressive generation, and (2) self-critiquing visual hypotheses to refine outputs.

Result: 50% relative improvement in complex multi-object scenarios (from 38% to 57%).

Conclusion: The approach mimics human visual imagination and iterative refinement, benefiting diverse fields like biochemistry, architecture, and forensics.

Abstract: We present Thinking with Generated Images, a novel paradigm that
fundamentally transforms how large multimodal models (LMMs) engage with visual
reasoning by enabling them to natively think across text and vision modalities
through spontaneous generation of intermediate visual thinking steps. Current
visual reasoning with LMMs is constrained to either processing fixed
user-provided images or reasoning solely through text-based chain-of-thought
(CoT). Thinking with Generated Images unlocks a new dimension of cognitive
capability where models can actively construct intermediate visual thoughts,
critique their own visual hypotheses, and refine them as integral components of
their reasoning process. We demonstrate the effectiveness of our approach
through two complementary mechanisms: (1) vision generation with intermediate
visual subgoals, where models decompose complex visual tasks into manageable
components that are generated and integrated progressively, and (2) vision
generation with self-critique, where models generate an initial visual
hypothesis, analyze its shortcomings through textual reasoning, and produce
refined outputs based on their own critiques. Our experiments on vision
generation benchmarks show substantial improvements over baseline approaches,
with our models achieving up to 50% (from 38% to 57%) relative improvement in
handling complex multi-object scenarios. From biochemists exploring novel
protein structures, and architects iterating on spatial designs, to forensic
analysts reconstructing crime scenes, and basketball players envisioning
strategic plays, our approach enables AI models to engage in the kind of visual
imagination and iterative refinement that characterizes human creative,
analytical, and strategic thinking. We release our open-source suite at
https://github.com/GAIR-NLP/thinking-with-generated-images.

</details>


### [328] [Learning to Infer Parameterized Representations of Plants from 3D Scans](https://arxiv.org/pdf/2505.22337)
*Samara Ghrer, Christophe Godin, Stefanie Wuhrer*

Main category: cs.CV

TL;DR: A unified approach for reconstructing 3D plant architecture from scans using a recursive neural network trained on virtual plants, applicable to tasks like reconstruction, segmentation, and skeletonization.


<details>
  <summary>Details</summary>
Motivation: Existing methods either focus on inverse modeling or specific tasks like segmentation, lacking a unified solution for comprehensive plant representation.

Method: A recursive neural network trained on L-systems-generated virtual plants infers parametric tree-like representations from 3D point clouds.

Result: Achieves state-of-the-art performance on tasks like reconstruction, segmentation, and skeletonization, demonstrated on Chenopodium Album plants.

Conclusion: The proposed method provides a versatile, data-driven solution for plant architecture analysis, applicable to binary axial tree-representable plants.

Abstract: Reconstructing faithfully the 3D architecture of plants from unstructured
observations is a challenging task. Plants frequently contain numerous organs,
organized in branching systems in more or less complex spatial networks,
leading to specific computational issues due to self-occlusion or spatial
proximity between organs. Existing works either consider inverse modeling where
the aim is to recover the procedural rules that allow to simulate virtual
plants, or focus on specific tasks such as segmentation or skeletonization. We
propose a unified approach that, given a 3D scan of a plant, allows to infer a
parameterized representation of the plant. This representation describes the
plant's branching structure, contains parametric information for each plant
organ, and can therefore be used directly in a variety of tasks. In this
data-driven approach, we train a recursive neural network with virtual plants
generated using an L-systems-based procedural model. After training, the
network allows to infer a parametric tree-like representation based on an input
3D point cloud. Our method is applicable to any plant that can be represented
as binary axial tree. We evaluate our approach on Chenopodium Album plants,
using experiments on synthetic plants to show that our unified framework allows
for different tasks including reconstruction, segmentation and skeletonization,
while achieving results on-par with state-of-the-art for each task.

</details>


### [329] [RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction](https://arxiv.org/pdf/2505.22613)
*Yuchi Wang, Yishuo Cai, Shuhuai Ren, Sihan Yang, Linli Yao, Yuanxin Liu, Yuanxing Zhang, Pengfei Wan, Xu Sun*

Main category: cs.CV

TL;DR: RICO refines image captions via visual reconstruction and iterative discrepancy identification, improving accuracy and completeness. RICO-Flash reduces computational cost using DPO.


<details>
  <summary>Details</summary>
Motivation: Existing recaptioning methods suffer from inaccuracies due to hallucinations and missing fine-grained details.

Method: Uses a text-to-image model to reconstruct captions into reference images, then identifies discrepancies with MLLMs to refine captions iteratively. RICO-Flash employs DPO for efficiency.

Result: Significantly improves caption accuracy and completeness, outperforming baselines by ~10% on CapsBench and CompreCap.

Conclusion: RICO offers a robust solution for high-quality caption generation, with RICO-Flash addressing computational efficiency.

Abstract: Image recaptioning is widely used to generate training datasets with enhanced
quality for various multimodal tasks. Existing recaptioning methods typically
rely on powerful multimodal large language models (MLLMs) to enhance textual
descriptions, but often suffer from inaccuracies due to hallucinations and
incompleteness caused by missing fine-grained details. To address these
limitations, we propose RICO, a novel framework that refines captions through
visual reconstruction. Specifically, we leverage a text-to-image model to
reconstruct a caption into a reference image, and prompt an MLLM to identify
discrepancies between the original and reconstructed images to refine the
caption. This process is performed iteratively, further progressively promoting
the generation of more faithful and comprehensive descriptions. To mitigate the
additional computational cost induced by the iterative process, we introduce
RICO-Flash, which learns to generate captions like RICO using DPO. Extensive
experiments demonstrate that our approach significantly improves caption
accuracy and completeness, outperforms most baselines by approximately 10% on
both CapsBench and CompreCap. Code released at
https://github.com/wangyuchi369/RICO.

</details>


### [330] [Progressive Data Dropout: An Embarrassingly Simple Approach to Faster Training](https://arxiv.org/pdf/2505.22342)
*Shriram M S, Xinyue Hao, Shihao Hou, Yang Lu, Laura Sevilla-Lara, Anurag Arnab, Shreyank N Gowda*

Main category: cs.CV

TL;DR: Progressive Data Dropout reduces training epochs by 87.6% without sacrificing accuracy, even improving it by up to 4.82%.


<details>
  <summary>Details</summary>
Motivation: The high cost of training large models on massive datasets, with uniform sampling being inefficient.

Method: Leverages hard-data-mining and dropout insights to create Progressive Data Dropout, requiring no architecture changes.

Result: Reduces effective epochs to 12.4% of baseline while improving accuracy by up to 4.82%.

Conclusion: A simple, widely applicable method that enhances efficiency and accuracy in training.

Abstract: The success of the machine learning field has reliably depended on training
on large datasets. While effective, this trend comes at an extraordinary cost.
This is due to two deeply intertwined factors: the size of models and the size
of datasets. While promising research efforts focus on reducing the size of
models, the other half of the equation remains fairly mysterious. Indeed, it is
surprising that the standard approach to training remains to iterate over and
over, uniformly sampling the training dataset. In this paper we explore a
series of alternative training paradigms that leverage insights from
hard-data-mining and dropout, simple enough to implement and use that can
become the new training standard. The proposed Progressive Data Dropout reduces
the number of effective epochs to as little as 12.4% of the baseline. This
savings actually do not come at any cost for accuracy. Surprisingly, the
proposed method improves accuracy by up to 4.82%. Our approach requires no
changes to model architecture or optimizer, and can be applied across standard
training pipelines, thus posing an excellent opportunity for wide adoption.
Code can be found here: https://github.com/bazyagami/LearningWithRevision

</details>


### [331] [Task-Driven Implicit Representations for Automated Design of LiDAR Systems](https://arxiv.org/pdf/2505.22344)
*Nikhil Behari, Aaron Young, Akshat Dave, Ramesh Raskar*

Main category: cs.CV

TL;DR: A framework for automated, task-driven LiDAR system design using a continuous 6D design space and flow-based generative modeling.


<details>
  <summary>Details</summary>
Motivation: LiDAR design is complex and manual; this work aims to automate it under arbitrary constraints for diverse applications.

Method: Represents LiDAR configurations in a 6D space, learns task-specific densities via flow-based modeling, and synthesizes systems using expectation-maximization.

Result: Validated on 3D vision tasks like face scanning, robotic tracking, and object detection.

Conclusion: The framework enables efficient, constraint-aware LiDAR system design for real-world applications.

Abstract: Imaging system design is a complex, time-consuming, and largely manual
process; LiDAR design, ubiquitous in mobile devices, autonomous vehicles, and
aerial imaging platforms, adds further complexity through unique spatial and
temporal sampling requirements. In this work, we propose a framework for
automated, task-driven LiDAR system design under arbitrary constraints. To
achieve this, we represent LiDAR configurations in a continuous six-dimensional
design space and learn task-specific implicit densities in this space via
flow-based generative modeling. We then synthesize new LiDAR systems by
modeling sensors as parametric distributions in 6D space and fitting these
distributions to our learned implicit density using expectation-maximization,
enabling efficient, constraint-aware LiDAR system design. We validate our
method on diverse tasks in 3D vision, enabling automated LiDAR system design
across real-world-inspired applications in face scanning, robotic tracking, and
object detection.

</details>


### [332] [Sherlock: Self-Correcting Reasoning in Vision-Language Models](https://arxiv.org/pdf/2505.22651)
*Yi Ding, Ruqi Zhang*

Main category: cs.CV

TL;DR: Sherlock, a self-correction framework for VLMs, improves reasoning accuracy with minimal annotated data, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Addressing VLMs' sensitivity to errors, data dependency, and domain generalization issues.

Method: Introduces trajectory-level self-correction, visual perturbation-based preference data, and dynamic β tuning.

Result: Achieves 64.1% (direct) and 65.4% (self-corrected) accuracy, surpassing benchmarks with <20% data.

Conclusion: Self-correction enhances VLMs' reasoning and reduces reliance on annotated data.

Abstract: Reasoning Vision-Language Models (VLMs) have shown promising performance on
complex multimodal tasks. However, they still face significant challenges: they
are highly sensitive to reasoning errors, require large volumes of annotated
data or accurate verifiers, and struggle to generalize beyond specific domains.
To address these limitations, we explore self-correction as a strategy to
enhance reasoning VLMs. We first conduct an in-depth analysis of reasoning
VLMs' self-correction abilities and identify key gaps. Based on our findings,
we introduce Sherlock, a self-correction and self-improvement training
framework. Sherlock introduces a trajectory-level self-correction objective, a
preference data construction method based on visual perturbation, and a dynamic
$\beta$ for preference tuning. Once the model acquires self-correction
capabilities using only 20k randomly sampled annotated data, it continues to
self-improve without external supervision. Built on the Llama3.2-Vision-11B
model, Sherlock achieves remarkable results across eight benchmarks, reaching
an average accuracy of 64.1 with direct generation and 65.4 after
self-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and
LlamaV-o1 (63.4) while using less than 20% of the annotated data.

</details>


### [333] [Identity-Preserving Text-to-Image Generation via Dual-Level Feature Decoupling and Expert-Guided Fusion](https://arxiv.org/pdf/2505.22360)
*Kewen Chen, Xiaobin Hu, Wenqi Ren*

Main category: cs.CV

TL;DR: A novel framework improves subject-driven text-to-image generation by decoupling identity-relevant and irrelevant features, enhancing image quality and text alignment.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle to separate identity-relevant from irrelevant details, leading to overfitting or loss of subject identity.

Method: Proposes an Implicit-Explicit Decoupling Module (IEDM) and Feature Fusion Module (FFM) with Mixture of Experts (MoE) for refined feature representation.

Result: The framework enhances generation quality, scene adaptation flexibility, and output diversity.

Conclusion: The proposed method effectively addresses decoupling challenges, improving subject-driven image generation.

Abstract: Recent advances in large-scale text-to-image generation models have led to a
surge in subject-driven text-to-image generation, which aims to produce
customized images that align with textual descriptions while preserving the
identity of specific subjects. Despite significant progress, current methods
struggle to disentangle identity-relevant information from identity-irrelevant
details in the input images, resulting in overfitting or failure to maintain
subject identity. In this work, we propose a novel framework that improves the
separation of identity-related and identity-unrelated features and introduces
an innovative feature fusion mechanism to improve the quality and text
alignment of generated images. Our framework consists of two key components: an
Implicit-Explicit foreground-background Decoupling Module (IEDM) and a Feature
Fusion Module (FFM) based on a Mixture of Experts (MoE). IEDM combines
learnable adapters for implicit decoupling at the feature level with inpainting
techniques for explicit foreground-background separation at the image level.
FFM dynamically integrates identity-irrelevant features with identity-related
features, enabling refined feature representations even in cases of incomplete
decoupling. In addition, we introduce three complementary loss functions to
guide the decoupling process. Extensive experiments demonstrate the
effectiveness of our proposed method in enhancing image generation quality,
improving flexibility in scene adaptation, and increasing the diversity of
generated outputs across various textual descriptions.

</details>


### [334] [PacTure: Efficient PBR Texture Generation on Packed Views with Visual Autoregressive Models](https://arxiv.org/pdf/2505.22394)
*Fan Fei, Jiajun Tang, Fei-Peng Tian, Boxin Shi, Ping Tan*

Main category: cs.CV

TL;DR: PacTure is a framework for generating PBR material textures from untextured 3D meshes using text descriptions and optional image prompts, improving resolution and efficiency over existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing inefficiencies and inconsistencies in current 2D and multi-view texture generation methods.

Method: Introduces view packing for higher resolution and a multi-view multi-domain generative backbone for efficiency.

Result: Outperforms state-of-the-art methods in texture quality and computational efficiency.

Conclusion: PacTure offers a scalable and effective solution for high-quality PBR texture generation.

Abstract: We present PacTure, a novel framework for generating physically-based
rendering (PBR) material textures from an untextured 3D mesh, a text
description, and an optional image prompt. Early 2D generation-based texturing
approaches generate textures sequentially from different views, resulting in
long inference times and globally inconsistent textures. More recent approaches
adopt multi-view generation with cross-view attention to enhance global
consistency, which, however, limits the resolution for each view. In response
to these weaknesses, we first introduce view packing, a novel technique that
significantly increases the effective resolution for each view during
multi-view generation without imposing additional inference cost, by
formulating the arrangement of multi-view maps as a 2D rectangle bin packing
problem. In contrast to UV mapping, it preserves the spatial proximity
essential for image generation and maintains full compatibility with current 2D
generative models. To further reduce the inference cost, we enable fine-grained
control and multi-domain generation within the next-scale prediction
autoregressive framework to create an efficient multi-view multi-domain
generative backbone. Extensive experiments show that PacTure outperforms
state-of-the-art methods in both quality of generated PBR textures and
efficiency in training and inference.

</details>


### [335] [3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model](https://arxiv.org/pdf/2505.22657)
*Wenbo Hu, Yining Hong, Yanjun Wang, Leison Gao, Zibu Wei, Xingcheng Yao, Nanyun Peng, Yonatan Bitton, Idan Szpektor, Kai-Wei Chang*

Main category: cs.CV

TL;DR: The paper introduces 3DMem-Bench, a benchmark for evaluating memory reasoning in 3D environments, and proposes 3DLLM-Mem, a model for dynamic memory management in LLMs, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with planning and acting in dynamic 3D environments due to inadequate spatial-temporal memory modeling.

Method: Proposes 3DLLM-Mem, which uses working memory tokens to selectively attend to and fuse spatial-temporal features from episodic memory.

Result: 3DLLM-Mem outperforms baselines by 16.5% in success rate on challenging tasks in 3DMem-Bench.

Conclusion: The model effectively addresses memory limitations in LLMs for complex 3D environments, demonstrating superior performance.

Abstract: Humans excel at performing complex tasks by leveraging long-term memory
across temporal and spatial experiences. In contrast, current Large Language
Models (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D
environments. We posit that part of this limitation is due to the lack of
proper 3D spatial-temporal memory modeling in LLMs. To address this, we first
introduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000
trajectories and 2,892 embodied tasks, question-answering and captioning,
designed to evaluate an agent's ability to reason over long-term memory in 3D
environments. Second, we propose 3DLLM-Mem, a novel dynamic memory management
and fusion model for embodied spatial-temporal reasoning and actions in LLMs.
Our model uses working memory tokens, which represents current observations, as
queries to selectively attend to and fuse the most useful spatial and temporal
features from episodic memory, which stores past observations and interactions.
Our approach allows the agent to focus on task-relevant information while
maintaining memory efficiency in complex, long-horizon environments.
Experimental results demonstrate that 3DLLM-Mem achieves state-of-the-art
performance across various tasks, outperforming the strongest baselines by
16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied
tasks.

</details>


### [336] [Zooming from Context to Cue: Hierarchical Preference Optimization for Multi-Image MLLMs](https://arxiv.org/pdf/2505.22396)
*Xudong Li, Mengdan Zhang, Peixian Chen, Xiawu Zheng, Yan Zhang, Jingyuan Zheng, Yunhang Shen, Ke Li, Chaoyou Fu, Xing Sun, Rongrong Ji*

Main category: cs.CV

TL;DR: CcDPO improves multi-image understanding in MLLMs by addressing cross-modal misalignment through multi-level preference optimization, reducing hallucinations and enhancing performance.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with multi-image tasks due to cross-modal misalignment and hallucinations, while existing methods like DPO fail to model holistic context.

Method: Proposes CcDPO, a framework with Context-Level and Needle-Level Optimization, leveraging global sequence preferences and fine-grained visual details. Uses MultiScope-42k dataset for scalable optimization.

Result: CcDPO significantly reduces hallucinations and improves performance in both single- and multi-image tasks.

Conclusion: CcDPO effectively enhances MLLMs' multi-image understanding by integrating context and fine-grained details, outperforming existing methods.

Abstract: Multi-modal Large Language Models (MLLMs) excel at single-image tasks but
struggle with multi-image understanding due to cross-modal misalignment,
leading to hallucinations (context omission, conflation, and
misinterpretation). Existing methods using Direct Preference Optimization (DPO)
constrain optimization to a solitary image reference within the input sequence,
neglecting holistic context modeling. We propose Context-to-Cue Direct
Preference Optimization (CcDPO), a multi-level preference optimization
framework that enhances per-image perception in multi-image settings by zooming
into visual clues -- from sequential context to local details. It features: (i)
Context-Level Optimization : Re-evaluates cognitive biases underlying MLLMs'
multi-image context comprehension and integrates a spectrum of low-cost global
sequence preferences for bias mitigation. (ii) Needle-Level Optimization :
Directs attention to fine-grained visual details through region-targeted visual
prompts and multimodal preference supervision. To support scalable
optimization, we also construct MultiScope-42k, an automatically generated
dataset with high-quality multi-level preference pairs. Experiments show that
CcDPO significantly reduces hallucinations and yields consistent performance
gains across general single- and multi-image tasks.

</details>


### [337] [Self-Reflective Reinforcement Learning for Diffusion-based Image Reasoning Generation](https://arxiv.org/pdf/2505.22407)
*Jiadong Pan, Zhiyuan Ma, Kaiyan Zhang, Ning Ding, Bowen Zhou*

Main category: cs.CV

TL;DR: SRRL, a self-reflective RL algorithm for diffusion models, enhances logical image generation by integrating Chain of Thought (CoT) and iterative reflection.


<details>
  <summary>Details</summary>
Motivation: Existing image generation methods struggle with logical reasoning tasks. SRRL aims to address this by leveraging CoT and RL for better reasoning in diffusion models.

Method: SRRL treats the denoising trajectory as CoT steps, using multi-round reflective denoising and condition-guided forward processes for iterative refinement.

Result: SRRL outperforms existing methods, including GPT-4o, in generating logical images adhering to physical laws or unconventional phenomena.

Conclusion: SRRL successfully integrates reasoning into diffusion models, demonstrating superior performance in logical image generation tasks.

Abstract: Diffusion models have recently demonstrated exceptional performance in image
generation task. However, existing image generation methods still significantly
suffer from the dilemma of image reasoning, especially in logic-centered image
generation tasks. Inspired by the success of Chain of Thought (CoT) and
Reinforcement Learning (RL) in LLMs, we propose SRRL, a self-reflective RL
algorithm for diffusion models to achieve reasoning generation of logical
images by performing reflection and iteration across generation trajectories.
The intermediate samples in the denoising process carry noise, making accurate
reward evaluation difficult. To address this challenge, SRRL treats the entire
denoising trajectory as a CoT step with multi-round reflective denoising
process and introduces condition guided forward process, which allows for
reflective iteration between CoT steps. Through SRRL-based iterative diffusion
training, we introduce image reasoning through CoT into generation tasks
adhering to physical laws and unconventional physical phenomena for the first
time. Notably, experimental results of case study exhibit that the superior
performance of our SRRL algorithm even compared with GPT-4o. The project page
is https://jadenpan0.github.io/srrl.github.io/.

</details>


### [338] [Frugal Incremental Generative Modeling using Variational Autoencoders](https://arxiv.org/pdf/2505.22408)
*Victor Enescu, Hichem Sahbi*

Main category: cs.CV

TL;DR: A novel replay-free incremental learning model using VAEs addresses scalability and catastrophic forgetting, achieving SOTA accuracy with minimal memory growth.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges like catastrophic forgetting and scalability in continual learning, especially with replay-based methods.

Method: Proposes a replay-free incremental learning model based on VAEs, featuring multi-modal latent space and an orthogonality criterion. Two variants: static and dynamic VAEs with controlled parameter growth.

Result: Achieves SOTA accuracy while being significantly more memory-efficient than related works.

Conclusion: The method effectively mitigates catastrophic forgetting and scalability issues in continual learning, offering a practical solution for training large models.

Abstract: Continual or incremental learning holds tremendous potential in deep learning
with different challenges including catastrophic forgetting. The advent of
powerful foundation and generative models has propelled this paradigm even
further, making it one of the most viable solution to train these models.
However, one of the persisting issues lies in the increasing volume of data
particularly with replay-based methods. This growth introduces challenges with
scalability since continuously expanding data becomes increasingly demanding as
the number of tasks grows. In this paper, we attenuate this issue by devising a
novel replay-free incremental learning model based on Variational Autoencoders
(VAEs). The main contribution of this work includes (i) a novel incremental
generative modelling, built upon a well designed multi-modal latent space, and
also (ii) an orthogonality criterion that mitigates catastrophic forgetting of
the learned VAEs. The proposed method considers two variants of these VAEs:
static and dynamic with no (or at most a controlled) growth in the number of
parameters. Extensive experiments show that our method is (at least) an order
of magnitude more ``memory-frugal'' compared to the closely related works while
achieving SOTA accuracy scores.

</details>


### [339] [Scaling-up Perceptual Video Quality Assessment](https://arxiv.org/pdf/2505.22543)
*Ziheng Jia, Zicheng Zhang, Zeyu Zhang, Yingji Liang, Xiaorong Zhu, Chunyi Li, Jinliang Han, Haoning Wu, Bin Wang, Haoran Zhang, Guanyu Zhu, Qiyong Zhao, Xiaohong Liu, Guangtao Zhai, Xiongkuo Min*

Main category: cs.CV

TL;DR: The paper introduces OmniVQA, a framework for building large-scale VQA datasets, and demonstrates its effectiveness in improving model performance for quality understanding and rating tasks.


<details>
  <summary>Details</summary>
Motivation: Address the lack of labeled resources and small dataset scales in perceptual video quality assessment (VQA) by leveraging data scaling laws.

Method: Propose OmniVQA framework to create large multi-modal instruction databases (MIDBs), including OmniVQA-Chat-400K and OmniVQA-MOS-20K datasets, and a complementary training strategy.

Result: Models achieve state-of-the-art performance in quality understanding and rating tasks.

Conclusion: OmniVQA effectively scales VQA datasets and enhances model performance, setting new benchmarks in the field.

Abstract: The data scaling law has been shown to significantly enhance the performance
of large multi-modal models (LMMs) across various downstream tasks. However, in
the domain of perceptual video quality assessment (VQA), the potential of
scaling law remains unprecedented due to the scarcity of labeled resources and
the insufficient scale of datasets. To address this, we propose
\textbf{OmniVQA}, an efficient framework designed to efficiently build
high-quality, human-in-the-loop VQA multi-modal instruction databases (MIDBs).
We then scale up to create \textbf{OmniVQA-Chat-400K}, the largest MIDB in the
VQA field concurrently. Our focus is on the technical and aesthetic quality
dimensions, with abundant in-context instruction data to provide fine-grained
VQA knowledge. Additionally, we have built the \textbf{OmniVQA-MOS-20K} dataset
to enhance the model's quantitative quality rating capabilities. We then
introduce a \textbf{complementary} training strategy that effectively leverages
the knowledge from datasets for quality understanding and quality rating tasks.
Furthermore, we propose the \textbf{OmniVQA-FG (fine-grain)-Benchmark} to
evaluate the fine-grained performance of the models. Our results demonstrate
that our models achieve state-of-the-art performance in both quality
understanding and rating tasks.

</details>


### [340] [GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action Control](https://arxiv.org/pdf/2505.22421)
*Anthony Chen, Wenzhao Zheng, Yida Wang, Xueyang Zhang, Kun Zhan, Peng Jia, Kurt Keutzer, Shangbang Zhang*

Main category: cs.CV

TL;DR: GeoDrive integrates robust 3D geometry into driving world models to improve spatial understanding and action control, outperforming existing methods in accuracy and realism.


<details>
  <summary>Details</summary>
Motivation: Current world models for autonomous driving lack robust 3D geometric consistency and accumulate artifacts during occlusion handling, limiting reliable safety assessment.

Method: GeoDrive extracts 3D representations from input frames, renders 2D views based on ego-car trajectories, and uses a dynamic editing module to enhance renderings by adjusting vehicle positions.

Result: GeoDrive outperforms existing models in action accuracy and 3D spatial awareness, enabling realistic, adaptable, and reliable scene modeling. It also generalizes to novel trajectories and supports interactive editing.

Conclusion: GeoDrive enhances autonomous driving safety and reliability by improving 3D spatial modeling and action controllability, with added benefits of generalization and interactivity.

Abstract: Recent advancements in world models have revolutionized dynamic environment
simulation, allowing systems to foresee future states and assess potential
actions. In autonomous driving, these capabilities help vehicles anticipate the
behavior of other road users, perform risk-aware planning, accelerate training
in simulation, and adapt to novel scenarios, thereby enhancing safety and
reliability. Current approaches exhibit deficiencies in maintaining robust 3D
geometric consistency or accumulating artifacts during occlusion handling, both
critical for reliable safety assessment in autonomous navigation tasks. To
address this, we introduce GeoDrive, which explicitly integrates robust 3D
geometry conditions into driving world models to enhance spatial understanding
and action controllability. Specifically, we first extract a 3D representation
from the input frame and then obtain its 2D rendering based on the
user-specified ego-car trajectory. To enable dynamic modeling, we propose a
dynamic editing module during training to enhance the renderings by editing the
positions of the vehicles. Extensive experiments demonstrate that our method
significantly outperforms existing models in both action accuracy and 3D
spatial awareness, leading to more realistic, adaptable, and reliable scene
modeling for safer autonomous driving. Additionally, our model can generalize
to novel trajectories and offers interactive scene editing capabilities, such
as object editing and object trajectory control.

</details>


### [341] [PRISM: Video Dataset Condensation with Progressive Refinement and Insertion for Sparse Motion](https://arxiv.org/pdf/2505.22564)
*Jaehyun Choi, Jiwan Hur, Gyojin Han, Jaemyung Yu, Junmo Kim*

Main category: cs.CV

TL;DR: PRISM introduces a novel video dataset condensation method that preserves spatial-temporal interdependence, outperforming existing methods with compact storage.


<details>
  <summary>Details</summary>
Motivation: Address computational challenges of large-scale video data in deep learning by improving video condensation techniques.

Method: Progressive refinement and insertion for sparse motion (PRISM), preserving spatial-temporal interdependence and optimizing gradients per frame.

Result: Outperforms disentangled approaches in video action recognition benchmarks with compact storage.

Conclusion: PRISM effectively condenses video data while maintaining performance and reducing storage, suitable for resource-constrained environments.

Abstract: Video dataset condensation has emerged as a critical technique for addressing
the computational challenges associated with large-scale video data processing
in deep learning applications. While significant progress has been made in
image dataset condensation, the video domain presents unique challenges due to
the complex interplay between spatial content and temporal dynamics. This paper
introduces PRISM, Progressive Refinement and Insertion for Sparse Motion, for
video dataset condensation, a novel approach that fundamentally reconsiders how
video data should be condensed. Unlike the previous method that separates
static content from dynamic motion, our method preserves the essential
interdependence between these elements. Our approach progressively refines and
inserts frames to fully accommodate the motion in an action while achieving
better performance but less storage, considering the relation of gradients for
each frame. Extensive experiments across standard video action recognition
benchmarks demonstrate that PRISM outperforms existing disentangled approaches
while maintaining compact representations suitable for resource-constrained
environments.

</details>


### [342] [RC-AutoCalib: An End-to-End Radar-Camera Automatic Calibration Network](https://arxiv.org/pdf/2505.22427)
*Van-Tin Luu, Yon-Lin Cai, Vu-Hoang Tran, Wei-Chen Chiu, Yi-Ting Chen, Ching-Chun Huang*

Main category: cs.CV

TL;DR: A novel online automatic geometric calibration method for radar-camera systems using Dual-Perspective representation and Selective Fusion Mechanism to address data sparsity and height uncertainty.


<details>
  <summary>Details</summary>
Motivation: Overcoming the challenges of automatic calibration due to radar data sparsity and height uncertainty during system operation.

Method: Proposes Dual-Perspective representation (frontal and bird's-eye views), Selective Fusion Mechanism, Multi-Modal Cross-Attention, and Noise-Resistant Matcher for robust feature fusion and matching.

Result: Outperforms previous radar-camera and LiDAR-camera calibration methods on the nuScenes dataset.

Conclusion: Sets a new benchmark for radar-camera calibration, with code publicly available for future research.

Abstract: This paper presents a groundbreaking approach - the first online automatic
geometric calibration method for radar and camera systems. Given the
significant data sparsity and measurement uncertainty in radar height data,
achieving automatic calibration during system operation has long been a
challenge. To address the sparsity issue, we propose a Dual-Perspective
representation that gathers features from both frontal and bird's-eye views.
The frontal view contains rich but sensitive height information, whereas the
bird's-eye view provides robust features against height uncertainty. We thereby
propose a novel Selective Fusion Mechanism to identify and fuse reliable
features from both perspectives, reducing the effect of height uncertainty.
Moreover, for each view, we incorporate a Multi-Modal Cross-Attention Mechanism
to explicitly find location correspondences through cross-modal matching.
During the training phase, we also design a Noise-Resistant Matcher to provide
better supervision and enhance the robustness of the matching mechanism against
sparsity and height uncertainty. Our experimental results, tested on the
nuScenes dataset, demonstrate that our method significantly outperforms
previous radar-camera auto-calibration methods, as well as existing
state-of-the-art LiDAR-camera calibration techniques, establishing a new
benchmark for future research. The code is available at
https://github.com/nycu-acm/RC-AutoCalib.

</details>


### [343] [Universal Visuo-Tactile Video Understanding for Embodied Interaction](https://arxiv.org/pdf/2505.22566)
*Yifan Xie, Mingyang Li, Shoujie Li, Xingting Li, Guangyu Chen, Fei Ma, Fei Richard Yu, Wenbo Ding*

Main category: cs.CV

TL;DR: VTV-LLM is a multi-modal large language model integrating visual and tactile data for physical understanding, using a novel dataset (VTV150K) and training paradigm.


<details>
  <summary>Details</summary>
Motivation: Existing approaches lack effective tactile integration, crucial for real-world interaction. VTV-LLM bridges this gap by combining tactile perception with natural language.

Method: Develops VTV150K dataset (150K frames, 100 objects, 3 sensors) and a three-stage training paradigm: VTV enhancement, VTV-text alignment, and text prompt finetuning.

Result: Superior performance in tactile video understanding tasks, enabling advanced tactile reasoning.

Conclusion: VTV-LLM sets a foundation for intuitive human-machine interaction in tactile domains.

Abstract: Tactile perception is essential for embodied agents to understand physical
attributes of objects that cannot be determined through visual inspection
alone. While existing approaches have made progress in visual and language
modalities for physical understanding, they fail to effectively incorporate
tactile information that provides crucial haptic feedback for real-world
interaction. In this paper, we present VTV-LLM, the first multi-modal large
language model for universal Visuo-Tactile Video (VTV) understanding that
bridges the gap between tactile perception and natural language. To address the
challenges of cross-sensor and cross-modal integration, we contribute VTV150K,
a comprehensive dataset comprising 150,000 video frames from 100 diverse
objects captured across three different tactile sensors (GelSight Mini, DIGIT,
and Tac3D), annotated with four fundamental tactile attributes (hardness,
protrusion, elasticity, and friction). We develop a novel three-stage training
paradigm that includes VTV enhancement for robust visuo-tactile representation,
VTV-text alignment for cross-modal correspondence, and text prompt finetuning
for natural language generation. Our framework enables sophisticated tactile
reasoning capabilities including feature assessment, comparative analysis,
scenario-based decision making and so on. Experimental evaluations demonstrate
that VTV-LLM achieves superior performance in tactile video understanding
tasks, establishing a foundation for more intuitive human-machine interaction
in tactile domains.

</details>


### [344] [Zero-Shot 3D Visual Grounding from Vision-Language Models](https://arxiv.org/pdf/2505.22429)
*Rong Li, Shijie Li, Lingdong Kong, Xulei Yang, Junwei Liang*

Main category: cs.CV

TL;DR: SeeGround is a zero-shot 3DVG framework using 2D VLMs to avoid 3D training, achieving strong performance without labeled data.


<details>
  <summary>Details</summary>
Motivation: Existing 3DVG methods rely on labeled 3D data, limiting scalability. SeeGround aims to enable open-world applications without such constraints.

Method: SeeGround uses hybrid inputs (rendered views + enriched text) and two modules: Perspective Adaptation for viewpoint selection and Fusion Alignment for precise localization.

Result: SeeGround outperforms zero-shot baselines by 7.7% (ScanRefer) and 7.1% (Nr3D), rivaling supervised methods.

Conclusion: SeeGround demonstrates strong generalization, enabling scalable 3DVG without 3D-specific training.

Abstract: 3D Visual Grounding (3DVG) seeks to locate target objects in 3D scenes using
natural language descriptions, enabling downstream applications such as
augmented reality and robotics. Existing approaches typically rely on labeled
3D data and predefined categories, limiting scalability to open-world settings.
We present SeeGround, a zero-shot 3DVG framework that leverages 2D
Vision-Language Models (VLMs) to bypass the need for 3D-specific training. To
bridge the modality gap, we introduce a hybrid input format that pairs
query-aligned rendered views with spatially enriched textual descriptions. Our
framework incorporates two core components: a Perspective Adaptation Module
that dynamically selects optimal viewpoints based on the query, and a Fusion
Alignment Module that integrates visual and spatial signals to enhance
localization precision. Extensive evaluations on ScanRefer and Nr3D confirm
that SeeGround achieves substantial improvements over existing zero-shot
baselines -- outperforming them by 7.7% and 7.1%, respectively -- and even
rivals fully supervised alternatives, demonstrating strong generalization under
challenging conditions.

</details>


### [345] [Tell me Habibi, is it Real or Fake?](https://arxiv.org/pdf/2505.22581)
*Kartik Kuckreja, Parul Gupta, Injy Hamed, Thamar Solorio, Muhammad Haris Khan, Abhinav Dhall*

Main category: cs.CV

TL;DR: The paper introduces ArEnAV, a large-scale Arabic-English audio-visual deepfake dataset with code-switching, addressing gaps in multilingual deepfake detection.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection research overlooks multilingual and code-switched content, particularly Arabic-English, which is common in digital communication.

Method: The dataset (387k videos, 765 hours) is created using a novel pipeline with four Text-To-Speech and two lip-sync models.

Result: ArEnAV benchmarks against existing datasets and models, demonstrating its utility for advancing multilingual deepfake detection.

Conclusion: The dataset fills a critical gap in deepfake research and is publicly available for further study.

Abstract: Deepfake generation methods are evolving fast, making fake media harder to
detect and raising serious societal concerns. Most deepfake detection and
dataset creation research focuses on monolingual content, often overlooking the
challenges of multilingual and code-switched speech, where multiple languages
are mixed within the same discourse. Code-switching, especially between Arabic
and English, is common in the Arab world and is widely used in digital
communication. This linguistic mixing poses extra challenges for deepfake
detection, as it can confuse models trained mostly on monolingual data. To
address this, we introduce \textbf{ArEnAV}, the first large-scale
Arabic-English audio-visual deepfake dataset featuring intra-utterance
code-switching, dialectal variation, and monolingual Arabic content. It
\textbf{contains 387k videos and over 765 hours of real and fake videos}. Our
dataset is generated using a novel pipeline integrating four Text-To-Speech and
two lip-sync models, enabling comprehensive analysis of multilingual multimodal
deepfake detection. We benchmark our dataset against existing monolingual and
multilingual datasets, state-of-the-art deepfake detection models, and a human
evaluation, highlighting its potential to advance deepfake research. The
dataset can be accessed
\href{https://huggingface.co/datasets/kartik060702/ArEnAV-Full}{here}.

</details>


### [346] [Distance Transform Guided Mixup for Alzheimer's Detection](https://arxiv.org/pdf/2505.22434)
*Zobia Batool, Huseyin Ozkan, Erchan Aptoula*

Main category: cs.CV

TL;DR: A method for Alzheimer's detection using single-domain generalization with a modified mixup technique improves model generalization on imbalanced medical datasets.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like class imbalance, imaging protocol variations, and limited dataset diversity in Alzheimer's detection models.

Method: Extends the mixup method by computing distance transforms of MRI scans, spatially separating them into layers, and combining layers from different samples for augmentation.

Result: Improved generalization performance on ADNI and AIBL datasets.

Conclusion: The proposed approach effectively generates diverse data while preserving brain structure, enhancing model generalization.

Abstract: Alzheimer's detection efforts aim to develop accurate models for early
disease diagnosis. Significant advances have been achieved with convolutional
neural networks and vision transformer based approaches. However, medical
datasets suffer heavily from class imbalance, variations in imaging protocols,
and limited dataset diversity, which hinder model generalization. To overcome
these challenges, this study focuses on single-domain generalization by
extending the well-known mixup method. The key idea is to compute the distance
transform of MRI scans, separate them spatially into multiple layers and then
combine layers stemming from distinct samples to produce augmented images. The
proposed approach generates diverse data while preserving the brain's
structure. Experimental results show generalization performance improvement
across both ADNI and AIBL datasets.

</details>


### [347] [On Geometry-Enhanced Parameter-Efficient Fine-Tuning for 3D Scene Segmentation](https://arxiv.org/pdf/2505.22444)
*Liyao Tang, Zhe Chen, Dacheng Tao*

Main category: cs.CV

TL;DR: GEM introduces a geometry-aware PEFT module for 3D point cloud models, achieving performance close to full fine-tuning with only 1.6% parameter updates.


<details>
  <summary>Details</summary>
Motivation: Adapting large pre-trained point cloud models to downstream tasks is costly, and existing PEFT methods fail to address geometric and spatial shifts in 3D data.

Method: GEM integrates local positional encodings with a lightweight latent attention mechanism to capture global context.

Result: GEM matches or outperforms full fine-tuning while updating fewer parameters (1.6%) and reducing computational costs.

Conclusion: GEM sets a new benchmark for efficient, scalable, and geometry-aware fine-tuning of 3D point cloud models.

Abstract: The emergence of large-scale pre-trained point cloud models has significantly
advanced 3D scene understanding, but adapting these models to specific
downstream tasks typically demands full fine-tuning, incurring high
computational and storage costs. Parameter-efficient fine-tuning (PEFT)
techniques, successful in natural language processing and 2D vision tasks,
would underperform when naively applied to 3D point cloud models due to
significant geometric and spatial distribution shifts. Existing PEFT methods
commonly treat points as orderless tokens, neglecting important local spatial
structures and global geometric contexts in 3D modeling. To bridge this gap, we
introduce the Geometric Encoding Mixer (GEM), a novel geometry-aware PEFT
module specifically designed for 3D point cloud transformers. GEM explicitly
integrates fine-grained local positional encodings with a lightweight latent
attention mechanism to capture comprehensive global context, thereby
effectively addressing the spatial and geometric distribution mismatch.
Extensive experiments demonstrate that GEM achieves performance comparable to
or sometimes even exceeding full fine-tuning, while only updating 1.6% of the
model's parameters, fewer than other PEFT methods. With significantly reduced
training time and memory requirements, our approach thus sets a new benchmark
for efficient, scalable, and geometry-aware fine-tuning of large-scale 3D point
cloud models. Code will be released.

</details>


### [348] [Universal Domain Adaptation for Semantic Segmentation](https://arxiv.org/pdf/2505.22458)
*Seun-An Choe, Keon-Hee Park, Jinwoo Choi, Gyeong-Moon Park*

Main category: cs.CV

TL;DR: Proposes UniMAP for Universal Domain Adaptation in Semantic Segmentation (UniDA-SS), addressing unknown category settings between domains with domain-specific prototypes and image matching.


<details>
  <summary>Details</summary>
Motivation: Traditional UDA-SS assumes known category settings, but real-world scenarios often have private classes, causing performance issues. UniDA-SS aims to adapt without prior knowledge.

Method: UniMAP uses Domain-Specific Prototype-based Distinction (DSPD) for finer feature separation and Target-based Image Matching (TIM) to pair source and target images for common-class learning.

Result: UniMAP outperforms baselines on a new UniDA-SS benchmark, demonstrating robust adaptation.

Conclusion: UniMAP effectively addresses the UniDA-SS challenge, improving adaptation without requiring prior category knowledge.

Abstract: Unsupervised domain adaptation for semantic segmentation (UDA-SS) aims to
transfer knowledge from labeled source data to unlabeled target data. However,
traditional UDA-SS methods assume that category settings between source and
target domains are known, which is unrealistic in real-world scenarios. This
leads to performance degradation if private classes exist. To address this
limitation, we propose Universal Domain Adaptation for Semantic Segmentation
(UniDA-SS), achieving robust adaptation even without prior knowledge of
category settings. We define the problem in the UniDA-SS scenario as low
confidence scores of common classes in the target domain, which leads to
confusion with private classes. To solve this problem, we propose UniMAP:
UniDA-SS with Image Matching and Prototype-based Distinction, a novel framework
composed of two key components. First, Domain-Specific Prototype-based
Distinction (DSPD) divides each class into two domain-specific prototypes,
enabling finer separation of domain-specific features and enhancing the
identification of common classes across domains. Second, Target-based Image
Matching (TIM) selects a source image containing the most common-class pixels
based on the target pseudo-label and pairs it in a batch to promote effective
learning of common classes. We also introduce a new UniDA-SS benchmark and
demonstrate through various experiments that UniMAP significantly outperforms
baselines. The code is available at
\href{https://github.com/KU-VGI/UniMAP}{this https URL}.

</details>


### [349] [SHTOcc: Effective 3D Occupancy Prediction with Sparse Head and Tail Voxels](https://arxiv.org/pdf/2505.22461)
*Qiucheng Yu, Yuan Xie, Xin Tan*

Main category: cs.CV

TL;DR: SHTOcc improves 3D occupancy prediction by addressing voxel distribution issues, reducing GPU memory, speeding up inference, and boosting accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to explore voxel distribution patterns, leading to poor performance and long-tail problems.

Method: Proposes SHTOcc, using sparse head-tail voxel construction and decoupled learning to balance key voxels and reduce bias.

Result: Reduces GPU memory by 42.2%, speeds up inference by 58.6%, and improves accuracy by ~7%.

Conclusion: SHTOcc effectively addresses distribution issues, enhancing performance and efficiency in 3D occupancy prediction.

Abstract: 3D occupancy prediction has attracted much attention in the field of
autonomous driving due to its powerful geometric perception and object
recognition capabilities. However, existing methods have not explored the most
essential distribution patterns of voxels, resulting in unsatisfactory results.
This paper first explores the inter-class distribution and geometric
distribution of voxels, thereby solving the long-tail problem caused by the
inter-class distribution and the poor performance caused by the geometric
distribution. Specifically, this paper proposes SHTOcc (Sparse Head-Tail
Occupancy), which uses sparse head-tail voxel construction to accurately
identify and balance key voxels in the head and tail classes, while using
decoupled learning to reduce the model's bias towards the dominant (head)
category and enhance the focus on the tail class. Experiments show that
significant improvements have been made on multiple baselines: SHTOcc reduces
GPU memory usage by 42.2%, increases inference speed by 58.6%, and improves
accuracy by about 7%, verifying its effectiveness and efficiency. The code is
available at https://github.com/ge95net/SHTOcc

</details>


### [350] [Single Domain Generalization for Alzheimer's Detection from 3D MRIs with Pseudo-Morphological Augmentations and Contrastive Learning](https://arxiv.org/pdf/2505.22465)
*Zobia Batool, Huseyin Ozkan, Erchan Aptoula*

Main category: cs.CV

TL;DR: The paper proposes a method to improve Alzheimer's disease detection in MRIs using pseudo-morphological modules and contrastive learning to address class imbalance and domain generalization issues.


<details>
  <summary>Details</summary>
Motivation: Challenges like class imbalance, protocol variations, and limited dataset diversity hinder the generalization of deep learning models for Alzheimer's detection.

Method: The approach combines learnable pseudo-morphological modules for shape-aware augmentations with supervised contrastive learning for robust class-specific representations.

Result: Experiments on three datasets show improved performance and generalization, especially under class imbalance and protocol variations.

Conclusion: The proposed method enhances Alzheimer's detection by addressing domain generalization and class imbalance, with code to be made available.

Abstract: Although Alzheimer's disease detection via MRIs has advanced significantly
thanks to contemporary deep learning models, challenges such as class
imbalance, protocol variations, and limited dataset diversity often hinder
their generalization capacity. To address this issue, this article focuses on
the single domain generalization setting, where given the data of one domain, a
model is designed and developed with maximal performance w.r.t. an unseen
domain of distinct distribution. Since brain morphology is known to play a
crucial role in Alzheimer's diagnosis, we propose the use of learnable
pseudo-morphological modules aimed at producing shape-aware, anatomically
meaningful class-specific augmentations in combination with a supervised
contrastive learning module to extract robust class-specific representations.
Experiments conducted across three datasets show improved performance and
generalization capacity, especially under class imbalance and imaging protocol
variations. The source code will be made available upon acceptance at
https://github.com/zobia111/SDG-Alzheimer.

</details>


### [351] [ProCrop: Learning Aesthetic Image Cropping from Professional Compositions](https://arxiv.org/pdf/2505.22490)
*Ke Zhang, Tianyu Ding, Jiachen Jiang, Tianyi Chen, Ilya Zharkov, Vishal M. Patel, Luming Liang*

Main category: cs.CV

TL;DR: ProCrop is a retrieval-based image cropping method that learns from professional photography, outperforming existing methods. A large-scale dataset of 242K weakly-annotated images is introduced, enhancing diversity and quality.


<details>
  <summary>Details</summary>
Motivation: Existing image cropping methods lack diversity or require annotated data. ProCrop addresses this by leveraging professional photography for guidance.

Method: ProCrop fuses features from professional photos with query images to learn compositions. A dataset is generated by out-painting and refining crop proposals.

Result: ProCrop outperforms existing methods in supervised and weakly-supervised settings, matching fully supervised approaches when trained on the new dataset.

Conclusion: ProCrop and the new dataset advance image aesthetics research, with code and dataset made publicly available.

Abstract: Image cropping is crucial for enhancing the visual appeal and narrative
impact of photographs, yet existing rule-based and data-driven approaches often
lack diversity or require annotated training data. We introduce ProCrop, a
retrieval-based method that leverages professional photography to guide
cropping decisions. By fusing features from professional photographs with those
of the query image, ProCrop learns from professional compositions,
significantly boosting performance. Additionally, we present a large-scale
dataset of 242K weakly-annotated images, generated by out-painting professional
images and iteratively refining diverse crop proposals. This composition-aware
dataset generation offers diverse high-quality crop proposals guided by
aesthetic principles and becomes the largest publicly available dataset for
image cropping. Extensive experiments show that ProCrop significantly
outperforms existing methods in both supervised and weakly-supervised settings.
Notably, when trained on the new dataset, our ProCrop surpasses previous
weakly-supervised methods and even matches fully supervised approaches. Both
the code and dataset will be made publicly available to advance research in
image aesthetics and composition analysis.

</details>


### [352] [The Meeseeks Mesh: Spatially Consistent 3D Adversarial Objects for BEV Detector](https://arxiv.org/pdf/2505.22499)
*Aixuan Li, Mochu Xiang, Jing Zhang, Yuchao Dai*

Main category: cs.CV

TL;DR: The paper investigates 3D adversarial attacks on BEV-based 3D object detection models, proposing a method to generate non-invasive adversarial objects for robustness testing.


<details>
  <summary>Details</summary>
Motivation: To evaluate and enhance the robustness of 3D object detection models in autonomous driving by identifying vulnerabilities to adversarial attacks.

Method: Uses differentiable rendering and an occlusion-aware module to create spatially consistent adversarial objects, optimized via BEV spatial feature-guided strategy.

Result: Demonstrates reliable suppression of vehicle predictions in state-of-the-art detectors, with strong generalization across positions and distances.

Conclusion: The approach serves as a valuable tool for testing model robustness before deployment, highlighting vulnerabilities in 3D object detection.

Abstract: 3D object detection is a critical component in autonomous driving systems. It
allows real-time recognition and detection of vehicles, pedestrians and
obstacles under varying environmental conditions. Among existing methods, 3D
object detection in the Bird's Eye View (BEV) has emerged as the mainstream
framework. To guarantee a safe, robust and trustworthy 3D object detection, 3D
adversarial attacks are investigated, where attacks are placed in 3D
environments to evaluate the model performance, e.g., putting a film on a car,
clothing a pedestrian. The vulnerability of 3D object detection models to 3D
adversarial attacks serves as an important indicator to evaluate the robustness
of the model against perturbations. To investigate this vulnerability, we
generate non-invasive 3D adversarial objects tailored for real-world attack
scenarios. Our method verifies the existence of universal adversarial objects
that are spatially consistent across time and camera views. Specifically, we
employ differentiable rendering techniques to accurately model the spatial
relationship between adversarial objects and the target vehicle. Furthermore,
we introduce an occlusion-aware module to enhance visual consistency and
realism under different viewpoints. To maintain attack effectiveness across
multiple frames, we design a BEV spatial feature-guided optimization strategy.
Experimental results demonstrate that our approach can reliably suppress
vehicle predictions from state-of-the-art 3D object detectors, serving as an
important tool to test robustness of 3D object detection models before
deployment. Moreover, the generated adversarial objects exhibit strong
generalization capabilities, retaining its effectiveness at various positions
and distances in the scene.

</details>


### [353] [PathFL: Multi-Alignment Federated Learning for Pathology Image Segmentation](https://arxiv.org/pdf/2505.22522)
*Yuan Zhang, Feng Chen, Yaolei Qi, Guanyu Yang, Huazhu Fu*

Main category: cs.CV

TL;DR: PathFL is a federated learning framework for pathology image segmentation, addressing data heterogeneity through image, feature, and model alignment strategies.


<details>
  <summary>Details</summary>
Motivation: Challenges in pathology image segmentation due to diverse sources of heterogeneity (imaging modalities, organs, equipment) hinder generalizable models.

Method: Three-level alignment: image (style enhancement), feature (adaptive alignment), and model (stratified aggregation).

Result: Effective performance and robustness validated on heterogeneous datasets (cross-source, modality, organ, scanner).

Conclusion: PathFL improves generalization in pathology image segmentation by addressing heterogeneity at multiple levels.

Abstract: Pathology image segmentation across multiple centers encounters significant
challenges due to diverse sources of heterogeneity including imaging
modalities, organs, and scanning equipment, whose variability brings
representation bias and impedes the development of generalizable segmentation
models. In this paper, we propose PathFL, a novel multi-alignment Federated
Learning framework for pathology image segmentation that addresses these
challenges through three-level alignment strategies of image, feature, and
model aggregation. Firstly, at the image level, a collaborative style
enhancement module aligns and diversifies local data by facilitating style
information exchange across clients. Secondly, at the feature level, an
adaptive feature alignment module ensures implicit alignment in the
representation space by infusing local features with global insights, promoting
consistency across heterogeneous client features learning. Finally, at the
model aggregation level, a stratified similarity aggregation strategy
hierarchically aligns and aggregates models on the server, using layer-specific
similarity to account for client discrepancies and enhance global
generalization. Comprehensive evaluations on four sets of heterogeneous
pathology image datasets, encompassing cross-source, cross-modality,
cross-organ, and cross-scanner variations, validate the effectiveness of our
PathFL in achieving better performance and robustness against data
heterogeneity.

</details>


### [354] [PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image Generative Models](https://arxiv.org/pdf/2505.22523)
*Junwen Chen, Heyang Jiang, Yanbin Wang, Keming Wu, Ji Li, Chao Zhang, Keiji Yanai, Dong Chen, Yuhui Yuan*

Main category: cs.CV

TL;DR: The paper introduces a dataset (PrismLayersPro) and a model (ART+) for generating high-quality multi-layer transparent images from text, addressing the lack of such data.


<details>
  <summary>Details</summary>
Motivation: Current multi-layer generative models lag behind due to missing high-quality datasets. This work aims to fill that gap.

Method: Released PrismLayersPro dataset, introduced a training-free synthesis pipeline, and developed ART+ model with LayerFLUX and MultiLayerFLUX for layer generation and composition.

Result: ART+ outperforms the original ART in 60% of user studies and matches FLUX.1-[dev] in visual quality.

Conclusion: The work provides a foundation for multi-layer transparent image generation, enabling precise and editable layered imagery.

Abstract: Generating high-quality, multi-layer transparent images from text prompts can
unlock a new level of creative control, allowing users to edit each layer as
effortlessly as editing text outputs from LLMs. However, the development of
multi-layer generative models lags behind that of conventional text-to-image
models due to the absence of a large, high-quality corpus of multi-layer
transparent data. In this paper, we address this fundamental challenge by: (i)
releasing the first open, ultra-high-fidelity PrismLayers (PrismLayersPro)
dataset of 200K (20K) multilayer transparent images with accurate alpha mattes,
(ii) introducing a trainingfree synthesis pipeline that generates such data on
demand using off-the-shelf diffusion models, and (iii) delivering a strong,
open-source multi-layer generation model, ART+, which matches the aesthetics of
modern text-to-image generation models. The key technical contributions
include: LayerFLUX, which excels at generating high-quality single transparent
layers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple
LayerFLUX outputs into complete images, guided by human-annotated semantic
layout. To ensure higher quality, we apply a rigorous filtering stage to remove
artifacts and semantic mismatches, followed by human selection. Fine-tuning the
state-of-the-art ART model on our synthetic PrismLayersPro yields ART+, which
outperforms the original ART in 60% of head-to-head user study comparisons and
even matches the visual quality of images generated by the FLUX.1-[dev] model.
We anticipate that our work will establish a solid dataset foundation for the
multi-layer transparent image generation task, enabling research and
applications that require precise, editable, and visually compelling layered
imagery.

</details>


### [355] [RiverMamba: A State Space Model for Global River Discharge and Flood Forecasting](https://arxiv.org/pdf/2505.22535)
*Mohamad Hakam Shams Eddin, Yikui Zhang, Stefan Kollet, Juergen Gall*

Main category: cs.CV

TL;DR: RiverMamba is a novel deep learning model for global river discharge and flood forecasting, outperforming existing AI and physics-based models by leveraging spatio-temporal relations and Mamba blocks.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning approaches in hydrology lack spatial connectivity modeling, limiting their application to local scales and necessitating improved spatio-temporal methodologies for better forecasting.

Method: RiverMamba uses pretrained long-term reanalysis data and Mamba blocks to model global-scale channel network routing, integrating ECMWF HRES forecasts while accounting for their inaccuracies.

Result: RiverMamba provides reliable river discharge and flood predictions, including extreme events, surpassing operational AI- and physics-based models.

Conclusion: RiverMamba addresses the gap in spatio-temporal modeling for hydrology, offering a robust solution for global flood forecasting and early warning systems.

Abstract: Recent deep learning approaches for river discharge forecasting have improved
the accuracy and efficiency in flood forecasting, enabling more reliable early
warning systems for risk management. Nevertheless, existing deep learning
approaches in hydrology remain largely confined to local-scale applications and
do not leverage the inherent spatial connections of bodies of water. Thus,
there is a strong need for new deep learning methodologies that are capable of
modeling spatio-temporal relations to improve river discharge and flood
forecasting for scientific and operational applications. To address this, we
present RiverMamba, a novel deep learning model that is pretrained with
long-term reanalysis data and that can forecast global river discharge and
floods on a $0.05^\circ$ grid up to 7 days lead time, which is of high
relevance in early warning. To achieve this, RiverMamba leverages efficient
Mamba blocks that enable the model to capture global-scale channel network
routing and enhance its forecast capability for longer lead times. The forecast
blocks integrate ECMWF HRES meteorological forecasts, while accounting for
their inaccuracies through spatio-temporal modeling. Our analysis demonstrates
that RiverMamba delivers reliable predictions of river discharge, including
extreme floods across return periods and lead times, surpassing both
operational AI- and physics-based models.

</details>


### [356] [Deep Learning-Based BMD Estimation from Radiographs with Conformal Uncertainty Quantification](https://arxiv.org/pdf/2505.22551)
*Long Hui, Wai Lok Yeung*

Main category: cs.CV

TL;DR: Using knee X-rays and deep learning to estimate Bone Mineral Density (BMD) with robust uncertainty quantification, showing potential for early osteoporosis screening despite anatomical limitations.


<details>
  <summary>Details</summary>
Motivation: Limited access to DXA scans hinders osteoporosis screening, prompting the need for alternative methods using widely available knee X-rays.

Method: An EfficientNet model trained on the OAI dataset predicts BMD from knee X-rays, comparing two Test-Time Augmentation (TTA) methods and using Split Conformal Prediction for uncertainty quantification.

Result: Pearson correlation of 0.68 (traditional TTA); multi-sample TTA provided tighter confidence intervals while maintaining coverage.

Conclusion: The method lays groundwork for trustworthy AI-assisted BMD screening using routine radiographs, despite anatomical mismatch with DXA.

Abstract: Limited DXA access hinders osteoporosis screening. This proof-of-concept
study proposes using widely available knee X-rays for opportunistic Bone
Mineral Density (BMD) estimation via deep learning, emphasizing robust
uncertainty quantification essential for clinical use. An EfficientNet model
was trained on the OAI dataset to predict BMD from bilateral knee radiographs.
Two Test-Time Augmentation (TTA) methods were compared: traditional averaging
and a multi-sample approach. Crucially, Split Conformal Prediction was
implemented to provide statistically rigorous, patient-specific prediction
intervals with guaranteed coverage. Results showed a Pearson correlation of
0.68 (traditional TTA). While traditional TTA yielded better point predictions,
the multi-sample approach produced slightly tighter confidence intervals (90%,
95%, 99%) while maintaining coverage. The framework appropriately expressed
higher uncertainty for challenging cases. Although anatomical mismatch between
knee X-rays and standard DXA limits immediate clinical use, this method
establishes a foundation for trustworthy AI-assisted BMD screening using
routine radiographs, potentially improving early osteoporosis detection.

</details>


### [357] [MultiFormer: A Multi-Person Pose Estimation System Based on CSI and Attention Mechanism](https://arxiv.org/pdf/2505.22555)
*Yanyi Qu, Haoyang Ma, Wenhui Xiong*

Main category: cs.CV

TL;DR: MultiFormer is a wireless sensing system using CSI and Transformer-based feature extraction for accurate human pose estimation, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Address challenges in multi-person pose recognition and CSI feature learning for non-intrusive human activity monitoring.

Method: Uses a Transformer-based time-frequency dual-token feature extractor and Multi-Stage Feature Fusion Network (MSFN) to fuse CSI features with pose heatmaps.

Result: Achieves higher accuracy, especially for high-mobility keypoints, on public and self-collected datasets.

Conclusion: MultiFormer demonstrates superior performance in human pose estimation using CSI, addressing key limitations of prior methods.

Abstract: Human pose estimation based on Channel State Information (CSI) has emerged as
a promising approach for non-intrusive and precise human activity monitoring,
yet faces challenges including accurate multi-person pose recognition and
effective CSI feature learning. This paper presents MultiFormer, a wireless
sensing system that accurately estimates human pose through CSI. The proposed
system adopts a Transformer based time-frequency dual-token feature extractor
with multi-head self-attention. This feature extractor is able to model
inter-subcarrier correlations and temporal dependencies of the CSI. The
extracted CSI features and the pose probability heatmaps are then fused by
Multi-Stage Feature Fusion Network (MSFN) to enforce the anatomical
constraints. Extensive experiments conducted on on the public MM-Fi dataset and
our self-collected dataset show that the MultiFormer achieves higher accuracy
over state-of-the-art approaches, especially for high-mobility keypoints
(wrists, elbows) that are particularly difficult for previous methods to
accurately estimate.

</details>


### [358] [ImageReFL: Balancing Quality and Diversity in Human-Aligned Diffusion Models](https://arxiv.org/pdf/2505.22569)
*Dmitrii Sorokin, Maksim Nakhodnov, Andrey Kuznetsov, Aibek Alanov*

Main category: cs.CV

TL;DR: The paper introduces a novel sampling strategy and fine-tuning method to balance human preference alignment and image diversity in diffusion models.


<details>
  <summary>Details</summary>
Motivation: Aligning diffusion models with human preferences often reduces output diversity. The work aims to address this trade-off.

Method: Proposes 'combined generation' for sampling and 'ImageReFL' for fine-tuning, using real images and multiple regularizers.

Result: Outperforms conventional methods in quality and diversity metrics, confirmed by a user study.

Conclusion: The approach effectively balances alignment and diversity, with code available for further use.

Abstract: Recent advances in diffusion models have led to impressive image generation
capabilities, but aligning these models with human preferences remains
challenging. Reward-based fine-tuning using models trained on human feedback
improves alignment but often harms diversity, producing less varied outputs. In
this work, we address this trade-off with two contributions. First, we
introduce \textit{combined generation}, a novel sampling strategy that applies
a reward-tuned diffusion model only in the later stages of the generation
process, while preserving the base model for earlier steps. This approach
mitigates early-stage overfitting and helps retain global structure and
diversity. Second, we propose \textit{ImageReFL}, a fine-tuning method that
improves image diversity with minimal loss in quality by training on real
images and incorporating multiple regularizers, including diffusion and ReFL
losses. Our approach outperforms conventional reward tuning methods on standard
quality and diversity metrics. A user study further confirms that our method
better balances human preference alignment and visual diversity. The source
code can be found at https://github.com/ControlGenAI/ImageReFL .

</details>


### [359] [SAM-R1: Leveraging SAM for Reward Feedback in Multimodal Segmentation via Reinforcement Learning](https://arxiv.org/pdf/2505.22596)
*Jiaqi Huang, Zunnan Xu, Jun Zhou, Ting Liu, Yicheng Xiao, Mingwen Ou, Bowen Ji, Xiu Li, Kehong Yuan*

Main category: cs.CV

TL;DR: SAM-R1 is a novel framework using reinforcement learning to enable multimodal large models for fine-grained image segmentation without costly reasoning-annotated data.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on expensive manually annotated datasets with explicit reasoning processes, prompting the need for a cost-effective alternative.

Method: SAM-R1 integrates fine-grained segmentation settings, task-specific rewards, and leverages SAM for reward provision during training.

Result: With only 3k training samples, SAM-R1 achieves strong performance across multiple benchmarks.

Conclusion: Reinforcement learning effectively equips multimodal models with segmentation-oriented reasoning capabilities, as demonstrated by SAM-R1.

Abstract: Leveraging multimodal large models for image segmentation has become a
prominent research direction. However, existing approaches typically rely
heavily on manually annotated datasets that include explicit reasoning
processes, which are costly and time-consuming to produce. Recent advances
suggest that reinforcement learning (RL) can endow large models with reasoning
capabilities without requiring such reasoning-annotated data. In this paper, we
propose SAM-R1, a novel framework that enables multimodal large models to
perform fine-grained reasoning in image understanding tasks. Our approach is
the first to incorporate fine-grained segmentation settings during the training
of multimodal reasoning models. By integrating task-specific, fine-grained
rewards with a tailored optimization objective, we further enhance the model's
reasoning and segmentation alignment. We also leverage the Segment Anything
Model (SAM) as a strong and flexible reward provider to guide the learning
process. With only 3k training samples, SAM-R1 achieves strong performance
across multiple benchmarks, demonstrating the effectiveness of reinforcement
learning in equipping multimodal models with segmentation-oriented reasoning
capabilities.

</details>


### [360] [Adversarially Robust AI-Generated Image Detection for Free: An Information Theoretic Perspective](https://arxiv.org/pdf/2505.22604)
*Ruixuan Zhang, He Wang, Zhengyu Zhao, Zhiqing Guo, Xun Yang, Yunfeng Diao, Meng Wang*

Main category: cs.CV

TL;DR: The paper identifies performance collapse in adversarial training for AIGI detection, proposes TRIM, a training-free defense method using information-theoretic measures, and validates its superiority over existing defenses.


<details>
  <summary>Details</summary>
Motivation: The rise of malicious use of AI-generated images (AIGI) necessitates robust detection methods, but existing detectors are vulnerable to adversarial attacks, and defenses are lacking.

Method: The paper proposes TRIM, a training-free adversarial defense method that quantifies feature shifts using prediction entropy and KL divergence, building on standard detectors.

Result: TRIM outperforms state-of-the-art defenses by significant margins (e.g., 33.88% on ProGAN) while maintaining original accuracy.

Conclusion: TRIM is an effective, training-free solution for robust AIGI detection, addressing the limitations of adversarial training.

Abstract: Rapid advances in Artificial Intelligence Generated Images (AIGI) have
facilitated malicious use, such as forgery and misinformation. Therefore,
numerous methods have been proposed to detect fake images. Although such
detectors have been proven to be universally vulnerable to adversarial attacks,
defenses in this field are scarce. In this paper, we first identify that
adversarial training (AT), widely regarded as the most effective defense,
suffers from performance collapse in AIGI detection. Through an
information-theoretic lens, we further attribute the cause of collapse to
feature entanglement, which disrupts the preservation of feature-label mutual
information. Instead, standard detectors show clear feature separation.
Motivated by this difference, we propose Training-free Robust Detection via
Information-theoretic Measures (TRIM), the first training-free adversarial
defense for AIGI detection. TRIM builds on standard detectors and quantifies
feature shifts using prediction entropy and KL divergence. Extensive
experiments across multiple datasets and attacks validate the superiority of
our TRIM, e.g., outperforming the state-of-the-art defense by 33.88% (28.91%)
on ProGAN (GenImage), while well maintaining original accuracy.

</details>


### [361] [ObjectClear: Complete Object Removal via Object-Effect Attention](https://arxiv.org/pdf/2505.22636)
*Jixin Zhao, Shangchen Zhou, Zhouxia Wang, Peiqing Yang, Chen Change Loy*

Main category: cs.CV

TL;DR: The paper introduces OBER, a dataset for object-effect removal, and ObjectClear, a framework using attention mechanisms to improve removal quality and background fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based inpainting methods struggle with artifacts, hallucinations, and inaccurate removal of object effects like shadows and reflections.

Method: Proposes ObjectClear, a framework with an object-effect attention mechanism to guide removal and decouple foreground removal from background reconstruction.

Result: ObjectClear outperforms existing methods, achieving better object-effect removal and background preservation, especially in complex scenarios.

Conclusion: The OBER dataset and ObjectClear framework effectively address challenges in object-effect removal, improving accuracy and background fidelity.

Abstract: Object removal requires eliminating not only the target object but also its
effects, such as shadows and reflections. However, diffusion-based inpainting
methods often produce artifacts, hallucinate content, alter background, and
struggle to remove object effects accurately. To address this challenge, we
introduce a new dataset for OBject-Effect Removal, named OBER, which provides
paired images with and without object effects, along with precise masks for
both objects and their associated visual artifacts. The dataset comprises
high-quality captured and simulated data, covering diverse object categories
and complex multi-object scenes. Building on OBER, we propose a novel
framework, ObjectClear, which incorporates an object-effect attention mechanism
to guide the model toward the foreground removal regions by learning attention
masks, effectively decoupling foreground removal from background
reconstruction. Furthermore, the predicted attention map enables an
attention-guided fusion strategy during inference, greatly preserving
background details. Extensive experiments demonstrate that ObjectClear
outperforms existing methods, achieving improved object-effect removal quality
and background fidelity, especially in complex scenarios.

</details>


### [362] [SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation](https://arxiv.org/pdf/2505.22643)
*Dekai Zhu, Yixuan Hu, Youquan Liu, Dongyue Lu, Lingdong Kong, Slobodan Ilic*

Main category: cs.CV

TL;DR: Spiral is a range-view LiDAR diffusion model generating depth, reflectance, and semantic maps simultaneously, outperforming existing methods in efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Existing range-view methods lack semantic labels, and using pretrained segmentation models leads to poor cross-modal consistency. Spiral aims to address this while preserving range-view advantages.

Method: Proposes Spiral, a diffusion model for simultaneous generation of depth, reflectance, and semantic maps in range-view. Introduces semantic-aware metrics for evaluation.

Result: Spiral achieves state-of-the-art performance with minimal parameters, outperforming two-step methods. Generated data aids downstream segmentation training.

Conclusion: Spiral efficiently generates labeled LiDAR data, reducing labeling effort and improving synthetic data augmentation for segmentation tasks.

Abstract: Leveraging recent diffusion models, LiDAR-based large-scale 3D scene
generation has achieved great success. While recent voxel-based approaches can
generate both geometric structures and semantic labels, existing range-view
methods are limited to producing unlabeled LiDAR scenes. Relying on pretrained
segmentation models to predict the semantic maps often results in suboptimal
cross-modal consistency. To address this limitation while preserving the
advantages of range-view representations, such as computational efficiency and
simplified network design, we propose Spiral, a novel range-view LiDAR
diffusion model that simultaneously generates depth, reflectance images, and
semantic maps. Furthermore, we introduce novel semantic-aware metrics to
evaluate the quality of the generated labeled range-view data. Experiments on
the SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves
state-of-the-art performance with the smallest parameter size, outperforming
two-step methods that combine the generative and segmentation models.
Additionally, we validate that range images generated by Spiral can be
effectively used for synthetic data augmentation in the downstream segmentation
training, significantly reducing the labeling effort on LiDAR data.

</details>


### [363] [Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation](https://arxiv.org/pdf/2505.22647)
*Zhe Kong, Feng Gao, Yong Zhang, Zhuoliang Kang, Xiaoming Wei, Xunliang Cai, Guanying Chen, Wenhan Luo*

Main category: cs.CV

TL;DR: The paper introduces MultiTalk, a framework for Multi-Person Conversational Video Generation, addressing audio-person binding and instruction-following challenges with L-RoPE and training strategies.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with multi-stream audio inputs and instruction-following, prompting the need for a solution like MultiTalk.

Method: Proposes L-RoPE for audio-person binding and uses partial parameter training and multi-task training to preserve instruction-following.

Result: MultiTalk outperforms others on talking head, talking body, and multi-person datasets.

Conclusion: MultiTalk effectively addresses multi-person generation challenges with superior performance.

Abstract: Audio-driven human animation methods, such as talking head and talking body
generation, have made remarkable progress in generating synchronized facial
movements and appealing visual quality videos. However, existing methods
primarily focus on single human animation and struggle with multi-stream audio
inputs, facing incorrect binding problems between audio and persons.
Additionally, they exhibit limitations in instruction-following capabilities.
To solve this problem, in this paper, we propose a novel task: Multi-Person
Conversational Video Generation, and introduce a new framework, MultiTalk, to
address the challenges during multi-person generation. Specifically, for audio
injection, we investigate several schemes and propose the Label Rotary Position
Embedding (L-RoPE) method to resolve the audio and person binding problem.
Furthermore, during training, we observe that partial parameter training and
multi-task training are crucial for preserving the instruction-following
ability of the base model. MultiTalk achieves superior performance compared to
other methods on several datasets, including talking head, talking body, and
multi-person datasets, demonstrating the powerful generation capabilities of
our approach.

</details>


### [364] [VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models](https://arxiv.org/pdf/2505.22654)
*Ce Zhang, Kaixin Ma, Tianqing Fang, Wenhao Yu, Hongming Zhang, Zhisong Zhang, Yaqi Xie, Katia Sycara, Haitao Mi, Dong Yu*

Main category: cs.CV

TL;DR: VScan is a two-stage visual token reduction framework that accelerates LVLMs by integrating global/local scans and pruning, achieving significant speedup and FLOP reduction while retaining performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the high computational costs of LVLMs due to long visual token sequences, which hinder real-time deployment.

Method: Proposes VScan: (1) combines global and local scans with token merging during visual encoding, and (2) introduces pruning at intermediate layers of the language model.

Result: Achieves 2.91× speedup and 10× FLOP reduction in LLaVA-NeXT-7B, retaining 95.4% of original performance. Validated on four LVLMs and sixteen benchmarks.

Conclusion: VScan effectively balances computational efficiency and performance, outperforming current state-of-the-art methods.

Abstract: Recent Large Vision-Language Models (LVLMs) have advanced multi-modal
understanding by incorporating finer-grained visual perception and encoding.
However, such methods incur significant computational costs due to longer
visual token sequences, posing challenges for real-time deployment. To mitigate
this, prior studies have explored pruning unimportant visual tokens either at
the output layer of the visual encoder or at the early layers of the language
model. In this work, we revisit these design choices and reassess their
effectiveness through comprehensive empirical studies of how visual tokens are
processed throughout the visual encoding and language decoding stages. Guided
by these insights, we propose VScan, a two-stage visual token reduction
framework that addresses token redundancy by: (1) integrating complementary
global and local scans with token merging during visual encoding, and (2)
introducing pruning at intermediate layers of the language model. Extensive
experimental results across four LVLMs validate the effectiveness of VScan in
accelerating inference and demonstrate its superior performance over current
state-of-the-arts on sixteen benchmarks. Notably, when applied to
LLaVA-NeXT-7B, VScan achieves a 2.91$\times$ speedup in prefilling and a
10$\times$ reduction in FLOPs, while retaining 95.4% of the original
performance.

</details>


### [365] [Training Free Stylized Abstraction](https://arxiv.org/pdf/2505.22663)
*Aimon Rahman, Kartik Narayan, Vishal M. Patel*

Main category: cs.CV

TL;DR: A training-free framework generates stylized abstractions from a single image using vision-language models and cross-domain rectified flow inversion, balancing identity retention with stylistic divergence.


<details>
  <summary>Details</summary>
Motivation: To create visually exaggerated yet semantically faithful representations, addressing the challenge of out-of-distribution individuals in stylized abstraction.

Method: Uses inference-time scaling in VLLMs for identity features and cross-domain rectified flow inversion for structure reconstruction, with style-aware temporal scheduling.

Result: Achieves high-fidelity reconstructions honoring both subject and style, generalizing well to unseen identities and styles.

Conclusion: The method supports multi-round abstraction-aware generation without fine-tuning and introduces StyleBench for human-aligned evaluation.

Abstract: Stylized abstraction synthesizes visually exaggerated yet semantically
faithful representations of subjects, balancing recognizability with perceptual
distortion. Unlike image-to-image translation, which prioritizes structural
fidelity, stylized abstraction demands selective retention of identity cues
while embracing stylistic divergence, especially challenging for
out-of-distribution individuals. We propose a training-free framework that
generates stylized abstractions from a single image using inference-time
scaling in vision-language models (VLLMs) to extract identity-relevant
features, and a novel cross-domain rectified flow inversion strategy that
reconstructs structure based on style-dependent priors. Our method adapts
structural restoration dynamically through style-aware temporal scheduling,
enabling high-fidelity reconstructions that honor both subject and style. It
supports multi-round abstraction-aware generation without fine-tuning. To
evaluate this task, we introduce StyleBench, a GPT-based human-aligned metric
suited for abstract styles where pixel-level similarity fails. Experiments
across diverse abstraction (e.g., LEGO, knitted dolls, South Park) show strong
generalization to unseen identities and styles in a fully open-source setup.

</details>


### [366] [Zero-Shot Vision Encoder Grafting via LLM Surrogates](https://arxiv.org/pdf/2505.22664)
*Kaiyu Yue, Vasu Singla, Menglin Jia, John Kirchenbauer, Rifaa Qadri, Zikui Cai, Abhinav Bhatele, Furong Huang, Tom Goldstein*

Main category: cs.CV

TL;DR: Training vision encoders with small surrogate models reduces VLM training costs by ~45% while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Reduce computational costs of training VLMs by avoiding direct training with large language models.

Method: Train vision encoders using small surrogate models that share embedding space with the target LLM, then transfer to the larger model.

Result: Grafted pairs outperform encoder-surrogate pairs and match full decoder training on some benchmarks.

Conclusion: Surrogate training is a cost-effective strategy for VLM training without sacrificing performance.

Abstract: Vision language models (VLMs) typically pair a modestly sized vision encoder
with a large language model (LLM), e.g., Llama-70B, making the decoder the
primary computational burden during training. To reduce costs, a potential
promising strategy is to first train the vision encoder using a small language
model before transferring it to the large one. We construct small "surrogate
models" that share the same embedding space and representation language as the
large target LLM by directly inheriting its shallow layers. Vision encoders
trained on the surrogate can then be directly transferred to the larger model,
a process we call zero-shot grafting -- when plugged directly into the
full-size target LLM, the grafted pair surpasses the encoder-surrogate pair
and, on some benchmarks, even performs on par with full decoder training with
the target LLM. Furthermore, our surrogate training approach reduces overall
VLM training costs by ~45% when using Llama-70B as the decoder.

</details>


### [367] [Flexible Sampling for Long-tailed Skin Lesion Classification](https://arxiv.org/pdf/2204.03161)
*Lie Ju, Yicheng Wu, Lin Wang, Zhen Yu, Xin Zhao, Xin Wang, Paul Bonnington, Zongyuan Ge*

Main category: cs.CV

TL;DR: A curriculum learning-based framework, Flexible Sampling, is proposed for long-tailed skin lesion classification, dynamically adjusting sampling based on class difficulty, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Medical tasks often have long-tailed distributions due to rare diseases and varied patient conditions. Equal re-balancing of classes can harm performance for challenging classes with diverse intra-class distributions.

Method: The framework uses anchor points from class prototypes to pre-train a model, evaluates per-class difficulty, and dynamically samples new data based on difficulty-aware probabilities.

Result: The method outperforms state-of-the-art approaches on the ISIC dataset under two long-tailed settings, setting a new benchmark.

Conclusion: Flexible Sampling effectively addresses long-tailed classification challenges by dynamically adapting to class-specific difficulties, improving performance.

Abstract: Most of the medical tasks naturally exhibit a long-tailed distribution due to
the complex patient-level conditions and the existence of rare diseases.
Existing long-tailed learning methods usually treat each class equally to
re-balance the long-tailed distribution. However, considering that some
challenging classes may present diverse intra-class distributions, re-balancing
all classes equally may lead to a significant performance drop. To address
this, in this paper, we propose a curriculum learning-based framework called
Flexible Sampling for the long-tailed skin lesion classification task.
Specifically, we initially sample a subset of training data as anchor points
based on the individual class prototypes. Then, these anchor points are used to
pre-train an inference model to evaluate the per-class learning difficulty.
Finally, we use a curriculum sampling module to dynamically query new samples
from the rest training samples with the learning difficulty-aware sampling
probability. We evaluated our model against several state-of-the-art methods on
the ISIC dataset. The results with two long-tailed settings have demonstrated
the superiority of our proposed training strategy, which achieves a new
benchmark for long-tailed skin lesion classification.

</details>


### [368] [End-to-End Breast Cancer Radiotherapy Planning via LMMs with Consistency Embedding](https://arxiv.org/pdf/2311.15876)
*Kwanyoung Kim, Yujin Oh, Sangjoon Park, Hwa Kyung Byun, Joongyo Lee, Jin Sung Kim, Yong Bae Kim, Jong Chul Ye*

Main category: cs.CV

TL;DR: RO-LMM is a multimodal AI model for radiation oncology, handling tasks like summarization, treatment planning, and segmentation. It introduces CEFTune and CESEG for robustness and consistency, showing strong performance in multi-center validation.


<details>
  <summary>Details</summary>
Motivation: To address the need for AI models that mimic medical professionals' comprehensive approaches in radiation oncology, especially for multi-modal integration.

Method: Developed RO-LMM, a large multimodal model, with CEFTune for robustness and CESEG for segmentation consistency.

Result: RO-LMM achieved promising performance in clinical tasks with generalization capabilities, validated across multiple centers.

Conclusion: RO-LMM, enhanced by CEFTune and CESEG, is effective for radiation oncology tasks, demonstrating robustness and consistency.

Abstract: Recent advances in AI foundation models have significant potential for
lightening the clinical workload by mimicking the comprehensive and
multi-faceted approaches used by medical professionals. In the field of
radiation oncology, the integration of multiple modalities holds great
importance, so the opportunity of foundational model is abundant. Inspired by
this, here we present RO-LMM, a multi-purpose, comprehensive large multimodal
model (LMM) tailored for the field of radiation oncology. This model
effectively manages a series of tasks within the clinical workflow, including
clinical context summarization, radiation treatment plan suggestion, and
plan-guided target volume segmentation by leveraging the capabilities of LMM.
In particular, to perform consecutive clinical tasks without error
accumulation, we present a novel Consistency Embedding Fine-Tuning (CEFTune)
technique, which boosts LMM's robustness to noisy inputs while preserving the
consistency of handling clean inputs. We further extend this concept to
LMM-driven segmentation framework, leading to a novel Consistency Embedding
Segmentation (CESEG) techniques. Experimental results including multi-centre
validation confirm that our RO-LMM with CEFTune and CESEG results in promising
performance for multiple clinical tasks with generalization capabilities.

</details>


### [369] [Meta Co-Training: Two Views are Better than One](https://arxiv.org/pdf/2311.18083)
*Jay C. Rothenberger, Dimitrios I. Diochnos*

Main category: cs.CV

TL;DR: The paper introduces Meta Co-Training, a semi-supervised learning method that improves performance by constructing views from pre-trained models, outperforming prior work on datasets like ImageNet-10%.


<details>
  <summary>Details</summary>
Motivation: Labels are scarce in computer vision, so leveraging unlabeled data via semi-supervised learning, like co-training, is valuable. However, co-training requires independent views, which are often unavailable.

Method: The paper proposes Meta Co-Training, which constructs views from pre-trained models, making learning robust and avoiding retraining from scratch.

Result: Meta Co-Training reduces error by ~4.7% on ImageNet-10% and outperforms prior methods on fine-grained datasets.

Conclusion: Meta Co-Training is a robust, efficient semi-supervised method that advances state-of-the-art performance.

Abstract: In many critical computer vision scenarios unlabeled data is plentiful, but
labels are scarce and difficult to obtain. As a result, semi-supervised
learning which leverages unlabeled data to boost the performance of supervised
classifiers have received significant attention in recent literature. One
representative class of semi-supervised algorithms are co-training algorithms.
Co-training algorithms leverage two different models which have access to
different independent and sufficient representations or "views" of the data to
jointly make better predictions. Each of these models creates pseudo-labels on
unlabeled points which are used to improve the other model. We show that in the
common case where independent views are not available, we can construct such
views inexpensively using pre-trained models. Co-training on the constructed
views yields a performance improvement over any of the individual views we
construct and performance comparable with recent approaches in semi-supervised
learning. We present Meta Co-Training, a novel semi-supervised learning
algorithm, which has two advantages over co-training: (i) learning is more
robust when there is large discrepancy between the information content of the
different views, and (ii) does not require retraining from scratch on each
iteration. Our method achieves new state-of-the-art performance on ImageNet-10%
achieving a ~4.7% reduction in error rate over prior work. Our method also
outperforms prior semi-supervised work on several other fine-grained image
classification datasets.

</details>


### [370] [MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering](https://arxiv.org/pdf/2405.11985)
*Jingqun Tang, Qi Liu, Yongjie Ye, Jinghui Lu, Shu Wei, Chunhui Lin, Wanqing Li, Mohamad Fitri Faiz Bin Mahmood, Hao Feng, Zhen Zhao, Yanjie Wang, Yuliang Liu, Hao Liu, Xiang Bai, Can Huang*

Main category: cs.CV

TL;DR: MTVQA is a new multilingual benchmark for Text-Centric Visual Question Answering (TEC-VQA), addressing gaps in existing datasets by providing high-quality human annotations across 9 languages. It highlights performance gaps in current MLLMs and offers training data to improve multilingual TEC-VQA.


<details>
  <summary>Details</summary>
Motivation: Existing TEC-VQA benchmarks focus on high-resource languages and suffer from visual-textual misalignment when using translation-based methods. MTVQA aims to fill this gap with diverse, high-quality annotations.

Method: Introduces MTVQA, a benchmark with 6,778 QA pairs in 9 languages across 2,116 images, evaluated on state-of-the-art MLLMs like Qwen2-VL and GPT-4V.

Result: MLLMs underperform humans (Qwen2-VL: 30.9 vs. human: 79.7). Fine-tuning with MTVQA data improves multilingual TEC-VQA performance.

Conclusion: MTVQA provides valuable insights and resources for advancing multilingual visual text comprehension, encouraging further research.

Abstract: Text-Centric Visual Question Answering (TEC-VQA) in its proper format not
only facilitates human-machine interaction in text-centric visual environments
but also serves as a de facto gold proxy to evaluate AI models in the domain of
text-centric scene understanding. Nonetheless, most existing TEC-VQA benchmarks
have focused on high-resource languages like English and Chinese. Despite
pioneering works to expand multilingual QA pairs in non-text-centric VQA
datasets through translation engines, the translation-based protocol encounters
a substantial "visual-textual misalignment" problem when applied to TEC-VQA.
Specifically, it prioritizes the text in question-answer pairs while
disregarding the visual text present in images. Moreover, it fails to address
complexities related to nuanced meaning, contextual distortion, language bias,
and question-type diversity. In this work, we tackle multilingual TEC-VQA by
introducing MTVQA, the first benchmark featuring high-quality human expert
annotations across 9 diverse languages, consisting of 6,778 question-answer
pairs across 2,116 images. Further, by comprehensively evaluating numerous
state-of-the-art Multimodal Large Language Models~(MLLMs), including Qwen2-VL,
GPT-4o, GPT-4V, Claude3, and Gemini, on the MTVQA benchmark, it is evident that
there is still a large room for performance improvement (Qwen2-VL scoring 30.9
versus 79.7 for human performance), underscoring the value of MTVQA.
Additionally, we supply multilingual training data within the MTVQA dataset,
demonstrating that straightforward fine-tuning with this data can substantially
enhance multilingual TEC-VQA performance. We aspire that MTVQA will offer the
research community fresh insights and stimulate further exploration in
multilingual visual text comprehension. The project homepage is available at
https://bytedance.github.io/MTVQA/.

</details>


### [371] [Base and Exponent Prediction in Mathematical Expressions using Multi-Output CNN](https://arxiv.org/pdf/2407.14967)
*Md Laraib Salam, Akash S Balsaraf, Gaurav Gupta, Ashish Rajeshwar Kulkarni*

Main category: cs.CV

TL;DR: A simplified multi-output CNN model effectively predicts base and exponent values from noisy and varied mathematical expression images with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Complex neural networks for image recognition are resource-intensive; this research aims to simplify the approach while maintaining accuracy.

Method: A multi-output CNN trained on 10,900 synthetic images with noise, font variations, and blur to simulate real-world conditions.

Result: The model achieves high accuracy in predicting base and exponent values efficiently.

Conclusion: The approach is effective for handling noisy and varied input images with reduced computational demands.

Abstract: The use of neural networks and deep learning techniques in image processing
has significantly advanced the field, enabling highly accurate recognition
results. However, achieving high recognition rates often necessitates complex
network models, which can be challenging to train and require substantial
computational resources. This research presents a simplified yet effective
approach to predicting both the base and exponent from images of mathematical
expressions using a multi-output Convolutional Neural Network (CNN). The model
is trained on 10,900 synthetically generated images containing exponent
expressions, incorporating random noise, font size variations, and blur
intensity to simulate real-world conditions. The proposed CNN model
demonstrates robust performance with efficient training time. The experimental
results indicate that the model achieves high accuracy in predicting the base
and exponent values, proving the efficacy of this approach in handling noisy
and varied input images.

</details>


### [372] [Cross-Layer Feature Pyramid Transformer for Small Object Detection in Aerial Images](https://arxiv.org/pdf/2407.19696)
*Zewen Du, Zhenjiang Hu, Guiyu Zhao, Ying Jin, Hongbin Ma*

Main category: cs.CV

TL;DR: CFPT is an upsampler-free feature pyramid network for small object detection in aerial images, using attention blocks (CCA and CSA) and positional encoding (CCPE) to improve performance.


<details>
  <summary>Details</summary>
Motivation: Small object detection in aerial images is challenging, and current detectors often neglect fundamental components like feature pyramid networks.

Method: Introduces CFPT with Cross-Layer Channel-Wise Attention (CCA) and Cross-Layer Spatial-Wise Attention (CSA), along with Cross-Layer Consistent Relative Positional Encoding (CCPE).

Result: CFPT outperforms state-of-the-art feature pyramid networks on datasets like VisDrone2019-DET, TinyPerson, and xView with lower computational costs.

Conclusion: CFPT effectively addresses small object detection challenges by enabling efficient cross-layer interaction and incorporating global contextual information.

Abstract: Object detection in aerial images has always been a challenging task due to
the generally small size of the objects. Most current detectors prioritize the
development of new detection frameworks, often overlooking research on
fundamental components such as feature pyramid networks. In this paper, we
introduce the Cross-Layer Feature Pyramid Transformer (CFPT), a novel
upsampler-free feature pyramid network designed specifically for small object
detection in aerial images. CFPT incorporates two meticulously designed
attention blocks with linear computational complexity: Cross-Layer Channel-Wise
Attention (CCA) and Cross-Layer Spatial-Wise Attention (CSA). CCA achieves
cross-layer interaction by dividing channel-wise token groups to perceive
cross-layer global information along the spatial dimension, while CSA enables
cross-layer interaction by dividing spatial-wise token groups to perceive
cross-layer global information along the channel dimension. By integrating
these modules, CFPT enables efficient cross-layer interaction in a single step,
thereby avoiding the semantic gap and information loss associated with
element-wise summation and layer-by-layer transmission. In addition, CFPT
incorporates global contextual information, which improves detection
performance for small objects. To further enhance location awareness during
cross-layer interaction, we propose the Cross-Layer Consistent Relative
Positional Encoding (CCPE) based on inter-layer mutual receptive fields. We
evaluate the effectiveness of CFPT on three challenging object detection
datasets in aerial images: VisDrone2019-DET, TinyPerson, and xView. Extensive
experiments demonstrate that CFPT outperforms state-of-the-art feature pyramid
networks while incurring lower computational costs. The code is available at
https://github.com/duzw9311/CFPT.

</details>


### [373] [Sparse R-CNN OBB: Ship Target Detection in SAR Images Based on Oriented Sparse Proposals](https://arxiv.org/pdf/2409.07973)
*Kamirul Kamirul, Odysseas Pappas, Alin Achim*

Main category: cs.CV

TL;DR: Sparse R-CNN OBB is a new framework for detecting oriented objects in SAR images using sparse learnable proposals, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: To simplify and improve oriented object detection in SAR images by avoiding dense anchor-based proposals.

Method: Uses 300 sparse learnable proposals and redesigns the detection head of Sparse R-CNN to capture object orientation.

Result: Achieves outstanding performance on the RSDD-SAR dataset, surpassing most models in inshore and offshore scenarios.

Conclusion: Sparse R-CNN OBB is effective for oriented object detection in SAR images, with code publicly available.

Abstract: We present Sparse R-CNN OBB, a novel framework for the detection of oriented
objects in SAR images leveraging sparse learnable proposals. The Sparse R-CNN
OBB has streamlined architecture and ease of training as it utilizes a sparse
set of 300 proposals instead of training a proposals generator on hundreds of
thousands of anchors. To the best of our knowledge, Sparse R-CNN OBB is the
first to adopt the concept of sparse learnable proposals for the detection of
oriented objects, as well as for the detection of ships in Synthetic Aperture
Radar (SAR) images. The detection head of the baseline model, Sparse R-CNN, is
re-designed to enable the model to capture object orientation. We train the
model on RSDD-SAR dataset and provide a performance comparison to
state-of-the-art models. Experimental results show that Sparse R-CNN OBB
achieves outstanding performance, surpassing most models on both inshore and
offshore scenarios. The code is available at:
www.github.com/ka-mirul/Sparse-R-CNN-OBB.

</details>


### [374] [Event-based Stereo Depth Estimation: A Survey](https://arxiv.org/pdf/2409.17680)
*Suman Ghosh, Guillermo Gallego*

Main category: cs.CV

TL;DR: A comprehensive survey on event-based stereo depth estimation, covering methods, datasets, and challenges, with future research directions.


<details>
  <summary>Details</summary>
Motivation: Event cameras offer high temporal resolution and dynamic range, making them ideal for stereo depth estimation, but the field lacks a unified survey.

Method: Reviews both instantaneous stereo and long-term methods (e.g., SLAM), theoretical and empirical comparisons, and DL approaches.

Result: Identifies gaps in accuracy and efficiency, proposes future directions, and provides practical dataset suggestions.

Conclusion: Aims to inspire research by serving as a guide for newcomers and experts, addressing challenges in event-based stereo depth estimation.

Abstract: Stereopsis has widespread appeal in robotics as it is the predominant way by
which living beings perceive depth to navigate our 3D world. Event cameras are
novel bio-inspired sensors that detect per-pixel brightness changes
asynchronously, with very high temporal resolution and high dynamic range,
enabling machine perception in high-speed motion and broad illumination
conditions. The high temporal precision also benefits stereo matching, making
disparity (depth) estimation a popular research area for event cameras ever
since its inception. Over the last 30 years, the field has evolved rapidly,
from low-latency, low-power circuit design to current deep learning (DL)
approaches driven by the computer vision community. The bibliography is vast
and difficult to navigate for non-experts due its highly interdisciplinary
nature. Past surveys have addressed distinct aspects of this topic, in the
context of applications, or focusing only on a specific class of techniques,
but have overlooked stereo datasets. This survey provides a comprehensive
overview, covering both instantaneous stereo and long-term methods suitable for
simultaneous localization and mapping (SLAM), along with theoretical and
empirical comparisons. It is the first to extensively review DL methods as well
as stereo datasets, even providing practical suggestions for creating new
benchmarks to advance the field. The main advantages and challenges faced by
event-based stereo depth estimation are also discussed. Despite significant
progress, challenges remain in achieving optimal performance in not only
accuracy but also efficiency, a cornerstone of event-based computing. We
identify several gaps and propose future research directions. We hope this
survey inspires future research in this area, by serving as an accessible entry
point for newcomers, as well as a practical guide for seasoned researchers in
the community.

</details>


### [375] [CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling](https://arxiv.org/pdf/2409.19291)
*Jihai Zhang, Xiaoye Qu, Tong Zhu, Yu Cheng*

Main category: cs.CV

TL;DR: The paper introduces CLIP-MoE, a method to enhance CLIP by fine-tuning diverse CLIP models into a Mixture of Experts (MoE) framework, improving feature representation and performance.


<details>
  <summary>Details</summary>
Motivation: CLIP's limitation in encoding only one aspect of the feature space leads to information loss and indistinctive features.

Method: Proposes Diversified Multiplet Upcycling (DMU) to fine-tune pre-trained CLIP models into diverse sets, then transforms them into CLIP-MoE for dynamic expert activation.

Result: CLIP-MoE outperforms in zero-shot retrieval, image classification, and MLLM benchmarks.

Conclusion: CLIP-MoE effectively balances model capacity and computational cost, enhancing multimodal intelligence.

Abstract: Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in
multimodal intelligence. However, recent studies discovered that CLIP can only
encode one aspect of the feature space, leading to substantial information loss
and indistinctive features. To mitigate this issue, this paper introduces a
novel strategy that fine-tunes a series of complementary CLIP models and
transforms them into a CLIP-MoE. Specifically, we propose a model-agnostic
Diversified Multiplet Upcycling (DMU) framework for CLIP. Instead of training
multiple CLIP models from scratch, DMU leverages a pre-trained CLIP and
fine-tunes it into a diverse set with highly cost-effective multistage
contrastive learning, thus capturing distinct feature subspaces efficiently. To
fully exploit these fine-tuned models while minimizing computational overhead,
we transform them into a CLIP-MoE, which dynamically activates a subset of CLIP
experts, achieving an effective balance between model capacity and
computational cost. Comprehensive experiments demonstrate the superior
performance of CLIP-MoE across various zero-shot retrieval, zero-shot image
classification tasks, and downstream Multimodal Large Language Model (MLLM)
benchmarks when used as a vision encoder.

</details>


### [376] [Shielded Diffusion: Generating Novel and Diverse Images using Sparse Repellency](https://arxiv.org/pdf/2410.06025)
*Michael Kirchhof, James Thornton, Louis Béthune, Pierre Ablin, Eugene Ndiaye, Marco Cuturi*

Main category: cs.CV

TL;DR: SPELL introduces sparse repellency to diffusion models to enhance diversity and avoid recreating training images, improving performance with minimal FID impact.


<details>
  <summary>Details</summary>
Motivation: Addressing lack of diversity and over-reliance on training data in text-to-image diffusion models.

Method: Adds repellency terms to diffusion SDE to steer trajectories away from reference sets, either static or dynamically updated.

Result: Improves diversity with marginal FID impact and outperforms other training-free methods.

Conclusion: SPELL effectively enhances diversity and shields against protected images, scalable to large datasets like ImageNet.

Abstract: The adoption of text-to-image diffusion models raises concerns over
reliability, drawing scrutiny under the lens of various metrics like
calibration, fairness, or compute efficiency. We focus in this work on two
issues that arise when deploying these models: a lack of diversity when
prompting images, and a tendency to recreate images from the training set. To
solve both problems, we propose a method that coaxes the sampled trajectories
of pretrained diffusion models to land on images that fall outside of a
reference set. We achieve this by adding repellency terms to the diffusion SDE
throughout the generation trajectory, which are triggered whenever the path is
expected to land too closely to an image in the shielded reference set. Our
method is sparse in the sense that these repellency terms are zero and inactive
most of the time, and even more so towards the end of the generation
trajectory. Our method, named SPELL for sparse repellency, can be used either
with a static reference set that contains protected images, or dynamically, by
updating the set at each timestep with the expected images concurrently
generated within a batch, and with the images of previously generated batches.
We show that adding SPELL to popular diffusion models improves their diversity
while impacting their FID only marginally, and performs comparatively better
than other recent training-free diversity methods. We also demonstrate how
SPELL can ensure a shielded generation away from a very large set of protected
images by considering all 1.2M images from ImageNet as the protected set.

</details>


### [377] [Diffusion Models as Cartoonists: The Curious Case of High Density Regions](https://arxiv.org/pdf/2411.01293)
*Rafał Karczewski, Markus Heinonen, Vikas Garg*

Main category: cs.CV

TL;DR: The paper explores high-density regions in diffusion models, introducing a mode-tracking process and a high-density sampler to generate higher-likelihood images, often cartoon-like or blurry.


<details>
  <summary>Details</summary>
Motivation: To understand and access high-density regions in diffusion models, which typical samplers miss, revealing unique image patterns.

Method: Proposes a theoretical mode-tracking process and a practical high-density sampler, alongside a novel likelihood-tracking approach for diffusion SDEs.

Result: Empirical results show higher-likelihood samples (cartoon-like or blurry) not produced by usual samplers, even in datasets lacking such examples.

Conclusion: The study successfully identifies and samples high-density regions in diffusion models, offering insights and tools for improved sampling without extra computational cost.

Abstract: We investigate what kind of images lie in the high-density regions of
diffusion models. We introduce a theoretical mode-tracking process capable of
pinpointing the exact mode of the denoising distribution, and we propose a
practical high-density sampler that consistently generates images of higher
likelihood than usual samplers. Our empirical findings reveal the existence of
significantly higher likelihood samples that typical samplers do not produce,
often manifesting as cartoon-like drawings or blurry images depending on the
noise level. Curiously, these patterns emerge in datasets devoid of such
examples. We also present a novel approach to track sample likelihoods in
diffusion SDEs, which remarkably incurs no additional computational cost. Code
is available at https://github.com/Aalto-QuML/high-density-diffusion.

</details>


### [378] [Dual-Head Knowledge Distillation: Enhancing Logits Utilization with an Auxiliary Head](https://arxiv.org/pdf/2411.08937)
*Penghui Yang, Chen-Chen Zong, Sheng-Jun Huang, Lei Feng, Bo An*

Main category: cs.CV

TL;DR: The paper introduces dual-head knowledge distillation (DHKD) to address performance degeneration caused by combining logit-level and probability-level losses in traditional knowledge distillation.


<details>
  <summary>Details</summary>
Motivation: Traditional distillation loses logit-level information when transitioning to probabilities, and combining both loss types degrades performance due to conflicting gradients in the classifier.

Method: Proposes DHKD, splitting the classifier into two heads for logit-level and probability-level losses to avoid conflicts while retaining benefits.

Result: DHKD effectively exploits logit information and outperforms state-of-the-art methods in experiments.

Conclusion: DHKD resolves gradient conflicts and improves distillation performance by separating loss functions into dual classification heads.

Abstract: Traditional knowledge distillation focuses on aligning the student's
predicted probabilities with both ground-truth labels and the teacher's
predicted probabilities. However, the transition to predicted probabilities
from logits would obscure certain indispensable information. To address this
issue, it is intuitive to additionally introduce a logit-level loss function as
a supplement to the widely used probability-level loss function, for exploiting
the latent information of logits. Unfortunately, we empirically find that the
amalgamation of the newly introduced logit-level loss and the previous
probability-level loss will lead to performance degeneration, even trailing
behind the performance of employing either loss in isolation. We attribute this
phenomenon to the collapse of the classification head, which is verified by our
theoretical analysis based on the neural collapse theory. Specifically, the
gradients of the two loss functions exhibit contradictions in the linear
classifier yet display no such conflict within the backbone. Drawing from the
theoretical analysis, we propose a novel method called dual-head knowledge
distillation, which partitions the linear classifier into two classification
heads responsible for different losses, thereby preserving the beneficial
effects of both losses on the backbone while eliminating adverse influences on
the classification head. Extensive experiments validate that our method can
effectively exploit the information inside the logits and achieve superior
performance against state-of-the-art counterparts. Our code is available at:
https://github.com/penghui-yang/DHKD.

</details>


### [379] [Functionality understanding and segmentation in 3D scenes](https://arxiv.org/pdf/2411.16310)
*Jaime Corsetti, Francesco Giuliari, Alice Fasoli, Davide Boscaini, Fabio Poiesi*

Main category: cs.CV

TL;DR: Fun3DU is the first method for functionality understanding in 3D scenes, using language and vision models to locate interactive objects based on natural language tasks.


<details>
  <summary>Details</summary>
Motivation: Functionality understanding in 3D scenes is challenging due to the need for world knowledge and spatial perception, with no dedicated methods existing before.

Method: Fun3DU employs Chain-of-Thought reasoning with a language model to parse tasks, segments objects using a vision-language model, and aggregates results in 3D.

Result: Fun3DU outperforms state-of-the-art open-vocabulary 3D segmentation methods on the SceneFun3D dataset.

Conclusion: Fun3DU is a training-free, effective solution for functionality understanding in 3D scenes, leveraging pre-trained models.

Abstract: Understanding functionalities in 3D scenes involves interpreting natural
language descriptions to locate functional interactive objects, such as handles
and buttons, in a 3D environment. Functionality understanding is highly
challenging, as it requires both world knowledge to interpret language and
spatial perception to identify fine-grained objects. For example, given a task
like 'turn on the ceiling light', an embodied AI agent must infer that it needs
to locate the light switch, even though the switch is not explicitly mentioned
in the task description. To date, no dedicated methods have been developed for
this problem. In this paper, we introduce Fun3DU, the first approach designed
for functionality understanding in 3D scenes. Fun3DU uses a language model to
parse the task description through Chain-of-Thought reasoning in order to
identify the object of interest. The identified object is segmented across
multiple views of the captured scene by using a vision and language model. The
segmentation results from each view are lifted in 3D and aggregated into the
point cloud using geometric information. Fun3DU is training-free, relying
entirely on pre-trained models. We evaluate Fun3DU on SceneFun3D, the most
recent and only dataset to benchmark this task, which comprises over 3000 task
descriptions on 230 scenes. Our method significantly outperforms
state-of-the-art open-vocabulary 3D segmentation approaches. Project page:
https://tev-fbk.github.io/fun3du/

</details>


### [380] [MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation](https://arxiv.org/pdf/2412.03558)
*Zehuan Huang, Yuan-Chen Guo, Xingqiao An, Yunhan Yang, Yangguang Li, Zi-Xin Zou, Ding Liang, Xihui Liu, Yan-Pei Cao, Lu Sheng*

Main category: cs.CV

TL;DR: MIDI introduces a novel method for generating 3D scenes from a single image using multi-instance diffusion models, ensuring accurate spatial relationships and high generalizability.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on reconstruction, retrieval, or multi-stage object-by-object generation, which are limited in efficiency and accuracy. MIDI aims to overcome these limitations by enabling simultaneous multi-instance generation.

Method: MIDI extends pre-trained image-to-3D models to multi-instance diffusion models, incorporating a novel multi-instance attention mechanism. It uses partial object images and global scene context as inputs, modeling object completion during generation. Training involves scene-level data supervision and single-object data regularization.

Result: MIDI achieves state-of-the-art performance in image-to-scene generation, validated on synthetic, real-world, and stylized scene data.

Conclusion: MIDI presents an efficient and accurate approach for compositional 3D scene generation, leveraging multi-instance diffusion models and attention mechanisms to outperform existing methods.

Abstract: This paper introduces MIDI, a novel paradigm for compositional 3D scene
generation from a single image. Unlike existing methods that rely on
reconstruction or retrieval techniques or recent approaches that employ
multi-stage object-by-object generation, MIDI extends pre-trained image-to-3D
object generation models to multi-instance diffusion models, enabling the
simultaneous generation of multiple 3D instances with accurate spatial
relationships and high generalizability. At its core, MIDI incorporates a novel
multi-instance attention mechanism, that effectively captures inter-object
interactions and spatial coherence directly within the generation process,
without the need for complex multi-step processes. The method utilizes partial
object images and global scene context as inputs, directly modeling object
completion during 3D generation. During training, we effectively supervise the
interactions between 3D instances using a limited amount of scene-level data,
while incorporating single-object data for regularization, thereby maintaining
the pre-trained generalization ability. MIDI demonstrates state-of-the-art
performance in image-to-scene generation, validated through evaluations on
synthetic data, real-world scene data, and stylized scene images generated by
text-to-image diffusion models.

</details>


### [381] [SoPo: Text-to-Motion Generation Using Semi-Online Preference Optimization](https://arxiv.org/pdf/2412.05095)
*Xiaofeng Tan, Hongsong Wang, Xin Geng, Pan Zhou*

Main category: cs.CV

TL;DR: The paper introduces Semi-online Preference Optimization (SoPo), a method to improve text-to-motion generation by combining online and offline data to address limitations like overfitting and biased sampling.


<details>
  <summary>Details</summary>
Motivation: Text-to-motion generation faces challenges in producing consistent, realistic motions. The paper aims to fine-tune models to favor high-quality, human-preferred motions, an underexplored problem.

Method: The authors propose SoPo, a DPO-based method using 'semi-online' data pairs (unpreferred online motions and preferred offline motions) to leverage strengths of both online and offline DPO.

Result: SoPo outperforms other methods, achieving MM-Dist scores of 3.25% (MLD) and 2.91% (MDM), and surpasses state-of-the-art models in R-precision and MM-Dist.

Conclusion: SoPo effectively aligns preferences in text-to-motion generation, demonstrating superior performance and visual results.

Abstract: Text-to-motion generation is essential for advancing the creative industry
but often presents challenges in producing consistent, realistic motions. To
address this, we focus on fine-tuning text-to-motion models to consistently
favor high-quality, human-preferred motions, a critical yet largely unexplored
problem. In this work, we theoretically investigate the DPO under both online
and offline settings, and reveal their respective limitation: overfitting in
offline DPO, and biased sampling in online DPO. Building on our theoretical
insights, we introduce Semi-online Preference Optimization (SoPo), a DPO-based
method for training text-to-motion models using "semi-online" data pair,
consisting of unpreferred motion from online distribution and preferred motion
in offline datasets. This method leverages both online and offline DPO,
allowing each to compensate for the other's limitations. Extensive experiments
demonstrate that SoPo outperforms other preference alignment methods, with an
MM-Dist of 3.25% (vs e.g. 0.76% of MoDiPO) on the MLD model, 2.91% (vs e.g.
0.66% of MoDiPO) on MDM model, respectively. Additionally, the MLD model
fine-tuned by our SoPo surpasses the SoTA model in terms of R-precision and MM
Dist. Visualization results also show the efficacy of our SoPo in preference
alignment. Project page: https://xiaofeng-tan.github.io/projects/SoPo/ .

</details>


### [382] [Preference Adaptive and Sequential Text-to-Image Generation](https://arxiv.org/pdf/2412.10419)
*Ofir Nabati, Guy Tennenholtz, ChihWei Hsu, Moonkyung Ryu, Deepak Ramachandran, Yinlam Chow, Xiang Li, Craig Boutilier*

Main category: cs.CV

TL;DR: PASTA is a reinforcement learning agent for interactive text-to-image generation, improving images through adaptive prompt expansions and leveraging user preferences.


<details>
  <summary>Details</summary>
Motivation: Addresses uncertainty in user intent and fosters collaborative co-creation in text-to-image generation.

Method: Uses RL, a novel sequential preference dataset, EM strategy for preference modeling, and a multimodal language model for adaptive prompt expansions.

Result: Significant improvement over baselines, validated by human raters.

Conclusion: PASTA enhances multi-turn T2I systems; dataset and interactions are open-sourced for future research.

Abstract: We address the problem of interactive text-to-image (T2I) generation,
designing a reinforcement learning (RL) agent which iteratively improves a set
of generated images for a user through a sequence of prompt expansions. Using
human raters, we create a novel dataset of sequential preferences, which we
leverage, together with large-scale open-source (non-sequential) datasets. We
construct user-preference and user-choice models using an EM strategy and
identify varying user preference types. We then leverage a large multimodal
language model (LMM) and a value-based RL approach to suggest an adaptive and
diverse slate of prompt expansions to the user. Our Preference Adaptive and
Sequential Text-to-image Agent (PASTA) extends T2I models with adaptive
multi-turn capabilities, fostering collaborative co-creation and addressing
uncertainty or underspecification in a user's intent. We evaluate PASTA using
human raters, showing significant improvement compared to baseline methods. We
also open-source our sequential rater dataset and simulated user-rater
interactions to support future research in user-centric multi-turn T2I systems.

</details>


### [383] [FocusChat: Text-guided Long Video Understanding via Spatiotemporal Information Filtering](https://arxiv.org/pdf/2412.12833)
*Zheng Cheng, Rendong Wang, Zhicheng Wang*

Main category: cs.CV

TL;DR: FocusChat is a text-guided multi-modal LLM that reduces redundant visual computation by aligning visual tokens with user queries, outperforming Video-LLaMA with fewer tokens and less data.


<details>
  <summary>Details</summary>
Motivation: Current multi-modal LLMs lack user intention guidance, leading to redundant computation and noise in untrimmed videos.

Method: Uses semantic extraction (visual and text branches) and Spatial-Temporal Filtering Module (STFM) to align visual tokens with queries.

Result: Outperforms Video-LLaMA in zero-shot, uses fewer tokens (16) and less data (0.72M), matches state-of-the-art in few-shot.

Conclusion: FocusChat efficiently aligns visual data with user intent, reducing noise and improving performance with minimal resources.

Abstract: Recently, multi-modal large language models have made significant progress.
However, visual information lacking of guidance from the user's intention may
lead to redundant computation and involve unnecessary visual noise, especially
in long, untrimmed videos. To address this issue, we propose FocusChat, a
text-guided multi-modal large language model (LLM) that emphasizes visual
information correlated to the user's prompt. In detail, Our model first
undergoes the semantic extraction module, which comprises a visual semantic
branch and a text semantic branch to extract image and text semantics,
respectively. The two branches are combined using the Spatial-Temporal
Filtering Module (STFM). STFM enables explicit spatial-level information
filtering and implicit temporal-level feature filtering, ensuring that the
visual tokens are closely aligned with the user's query. It lowers the
essential number of visual tokens inputted into the LLM. FocusChat
significantly outperforms Video-LLaMA in zero-shot experiments, using an order
of magnitude less training data with only 16 visual tokens occupied. It
achieves results comparable to the state-of-the-art in few-shot experiments,
with only 0.72M pre-training data.

</details>


### [384] [The Impact of the Single-Label Assumption in Image Recognition Benchmarking](https://arxiv.org/pdf/2412.18409)
*Esla Timothy Anzaku, Seyed Amir Mousavi, Arnout Van Messem, Wesley De Neve*

Main category: cs.CV

TL;DR: The paper critiques single-label evaluation of DNNs, showing it penalizes valid multi-label predictions. It introduces variable top-$k$ and ASMA metrics, revealing stronger multi-label capabilities in models and narrowing the ImageNetV2 accuracy gap.


<details>
  <summary>Details</summary>
Motivation: To address the mismatch between single-label evaluation protocols and the multi-label reality of visual data, which may inaccurately penalize DNNs.

Method: Introduces variable top-$k$ evaluation and ASMA for multi-label assessment, and uses PatchML to isolate multi-label recognition.

Result: Conventional top-1 accuracy unfairly penalizes valid predictions; multi-label evaluation narrows the ImageNetV2 gap. Models show strong multi-label capabilities.

Conclusion: Single-label evaluation underestimates DNNs' multi-label performance; new metrics better reflect their capabilities.

Abstract: Deep neural networks (DNNs) are typically evaluated under the assumption that
each image has a single correct label. However, many images in benchmarks like
ImageNet contain multiple valid labels, creating a mismatch between evaluation
protocols and the actual complexity of visual data. This mismatch can penalize
DNNs for predicting correct but unannotated labels, which may partly explain
reported accuracy drops, such as the widely cited 11 to 14 percent top-1
accuracy decline on ImageNetV2, a replication test set for ImageNet. This
raises the question: do such drops reflect genuine generalization failures or
artifacts of restrictive evaluation metrics? We rigorously assess the impact of
multi-label characteristics on reported accuracy gaps. To evaluate the
multi-label prediction capability (MLPC) of single-label-trained models, we
introduce a variable top-$k$ evaluation, where $k$ matches the number of valid
labels per image. Applied to 315 ImageNet-trained models, our analyses
demonstrate that conventional top-1 accuracy disproportionately penalizes valid
but secondary predictions. We also propose Aggregate Subgroup Model Accuracy
(ASMA) to better capture multi-label performance across model subgroups. Our
results reveal wide variability in MLPC, with some models consistently ranking
multiple correct labels higher. Under this evaluation, the perceived gap
between ImageNet and ImageNetV2 narrows substantially. To further isolate
multi-label recognition performance from contextual cues, we introduce PatchML,
a synthetic dataset containing systematically combined object patches. PatchML
demonstrates that many models trained with single-label supervision nonetheless
recognize multiple objects. Altogether, these findings highlight limitations in
single-label evaluation and reveal that modern DNNs have stronger multi-label
capabilities than standard metrics suggest.

</details>


### [385] [VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control](https://arxiv.org/pdf/2501.01427)
*Yuanpeng Tu, Hao Luo, Xi Chen, Sihui Ji, Xiang Bai, Hengshuang Zhao*

Main category: cs.CV

TL;DR: VideoAnydoor is a zero-shot framework for inserting objects into videos with high-fidelity detail preservation and precise motion control, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Challenges in preserving object details and modeling coherent motions in video object insertion tasks.

Method: Uses an ID extractor for global identity, a box sequence for motion control, and a pixel warper for detail preservation and fine-grained motion manipulation.

Result: Superior performance over existing methods, supporting various applications without task-specific fine-tuning.

Conclusion: VideoAnydoor effectively addresses video object insertion challenges with high fidelity and flexibility.

Abstract: Despite significant advancements in video generation, inserting a given
object into videos remains a challenging task. The difficulty lies in
preserving the appearance details of the reference object and accurately
modeling coherent motions at the same time. In this paper, we propose
VideoAnydoor, a zero-shot video object insertion framework with high-fidelity
detail preservation and precise motion control. Starting from a text-to-video
model, we utilize an ID extractor to inject the global identity and leverage a
box sequence to control the overall motion. To preserve the detailed appearance
and meanwhile support fine-grained motion control, we design a pixel warper. It
takes the reference image with arbitrary key-points and the corresponding
key-point trajectories as inputs. It warps the pixel details according to the
trajectories and fuses the warped features with the diffusion U-Net, thus
improving detail preservation and supporting users in manipulating the motion
trajectories. In addition, we propose a training strategy involving both videos
and static images with a weighted loss to enhance insertion quality.
VideoAnydoor demonstrates significant superiority over existing methods and
naturally supports various downstream applications (e.g., talking head
generation, video virtual try-on, multi-region editing) without task-specific
fine-tuning.

</details>


### [386] [DreamMask: Boosting Open-vocabulary Panoptic Segmentation with Synthetic Data](https://arxiv.org/pdf/2501.02048)
*Yuanpeng Tu, Xi Chen, Ser-Nam Lim, Hengshuang Zhao*

Main category: cs.CV

TL;DR: DreamMask boosts open-vocabulary panoptic segmentation by generating synthetic training data and aligning it with real data, improving generalization and outperforming SOTA by 2.1% mIoU.


<details>
  <summary>Details</summary>
Motivation: Previous works lack generalization to novel classes despite claims, prompting a data-centric approach to enhance existing models.

Method: Proposes DreamMask, an automatic data generation pipeline with vocabulary expansion, layout arrangement, and data filtering, plus a synthetic-real alignment loss for training.

Result: DreamMask outperforms manually collected web data and achieves a 2.1% mIoU improvement over SOTA on ADE20K when trained on COCO.

Conclusion: DreamMask simplifies large-scale data collection and serves as a plug-and-play enhancement for existing methods, improving generalization.

Abstract: Open-vocabulary panoptic segmentation has received significant attention due
to its applicability in the real world. Despite claims of robust
generalization, we find that the advancements of previous works are attributed
mainly on trained categories, exposing a lack of generalization to novel
classes. In this paper, we explore boosting existing models from a data-centric
perspective. We propose DreamMask, which systematically explores how to
generate training data in the open-vocabulary setting, and how to train the
model with both real and synthetic data. For the first part, we propose an
automatic data generation pipeline with off-the-shelf models. We propose
crucial designs for vocabulary expansion, layout arrangement, data filtering,
etc. Equipped with these techniques, our generated data could significantly
outperform the manually collected web data. To train the model with generated
data, a synthetic-real alignment loss is designed to bridge the representation
gap, bringing noticeable improvements across multiple benchmarks. In general,
DreamMask significantly simplifies the collection of large-scale training data,
serving as a plug-and-play enhancement for existing methods. For instance, when
trained on COCO and tested on ADE20K, the model equipped with DreamMask
outperforms the previous state-of-the-art by a substantial margin of 2.1% mIoU.

</details>


### [387] [Diffusion Adversarial Post-Training for One-Step Video Generation](https://arxiv.org/pdf/2501.08316)
*Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, Lu Jiang*

Main category: cs.CV

TL;DR: APT improves one-step video generation by combining adversarial post-training with diffusion models, achieving real-time high-quality results.


<details>
  <summary>Details</summary>
Motivation: Address the slow and costly iterative process of diffusion models for video generation while mitigating quality degradation in one-step methods.

Method: Proposes Adversarial Post-Training (APT) with architectural and training improvements, including an approximated R1 regularization objective.

Result: Seaweed-APT generates 2-second, 1280x720, 24fps videos in real time and 1024px images with quality comparable to state-of-the-art.

Conclusion: APT effectively bridges the gap between speed and quality in one-step video and image generation.

Abstract: The diffusion models are widely used for image and video generation, but
their iterative generation process is slow and expansive. While existing
distillation approaches have demonstrated the potential for one-step generation
in the image domain, they still suffer from significant quality degradation. In
this work, we propose Adversarial Post-Training (APT) against real data
following diffusion pre-training for one-step video generation. To improve the
training stability and quality, we introduce several improvements to the model
architecture and training procedures, along with an approximated R1
regularization objective. Empirically, our experiments show that our
adversarial post-trained model, Seaweed-APT, can generate 2-second, 1280x720,
24fps videos in real time using a single forward evaluation step. Additionally,
our model is capable of generating 1024px images in a single step, achieving
quality comparable to state-of-the-art methods.

</details>


### [388] [Hypo3D: Exploring Hypothetical Reasoning in 3D](https://arxiv.org/pdf/2502.00954)
*Ye Mao, Weixun Luo, Junpeng Jing, Anlan Qiu, Krystian Mikolajczyk*

Main category: cs.CV

TL;DR: Hypo3D is a benchmark for evaluating 3D reasoning without real-time scene data, revealing gaps in current models' performance.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks assume real-time scene access, which is impractical. Hypo3D addresses this by testing models' ability to reason hypothetically.

Method: Hypo3D is a 3D VQA benchmark with 7,727 context changes and 14,885 QAs, using an anchor-based world frame for consistency.

Result: State-of-the-art models struggle with hypothetical reasoning, especially in movement and directional scenarios.

Conclusion: Hypo3D highlights a significant performance gap between models and humans, emphasizing the need for improved 3D reasoning capabilities.

Abstract: The rise of vision-language foundation models marks an advancement in
bridging the gap between human and machine capabilities in 3D scene reasoning.
Existing 3D reasoning benchmarks assume real-time scene accessibility, which is
impractical due to the high cost of frequent scene updates. To this end, we
introduce Hypothetical 3D Reasoning, namely Hypo3D, a benchmark designed to
evaluate models' ability to reason without access to real-time scene data.
Models need to imagine the scene state based on a provided change description
before reasoning. Hypo3D is formulated as a 3D Visual Question Answering (VQA)
benchmark, comprising 7,727 context changes across 700 indoor scenes, resulting
in 14,885 question-answer pairs. An anchor-based world frame is established for
all scenes, ensuring consistent reference to a global frame for directional
terms in context changes and QAs. Extensive experiments show that
state-of-the-art foundation models struggle to reason in hypothetically changed
scenes. This reveals a substantial performance gap compared to humans,
particularly in scenarios involving movement changes and directional reasoning.
Even when the context change is irrelevant to the question, models often
incorrectly adjust their answers. Project website:
https://matchlab-imperial.github.io/Hypo3D/

</details>


### [389] [LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in Multimodal Large Language Models](https://arxiv.org/pdf/2502.02406)
*Tzu-Tao Chang, Shivaram Venkataraman*

Main category: cs.CV

TL;DR: LV-XAttn is a distributed cross-attention mechanism for MLLMs that reduces communication overhead by keeping large key-value blocks local and exchanging smaller query blocks, achieving significant speedups.


<details>
  <summary>Details</summary>
Motivation: Existing distributed attention mechanisms in MLLMs face high communication overhead, making cross-attention layers inefficient for large visual inputs like videos.

Method: LV-XAttn keeps key-value blocks local on GPUs and exchanges smaller query blocks, plus an activation recomputation technique for longer visual context.

Result: LV-XAttn achieves up to 10.62× speedup over existing methods in evaluations with models like Llama 3-V and OpenFlamingo.

Conclusion: LV-XAttn efficiently addresses the bottleneck of cross-attention in MLLMs, enabling faster training and inference for large visual inputs.

Abstract: Cross-attention is commonly adopted in multimodal large language models
(MLLMs) for integrating visual information into the language backbone. However,
in applications with large visual inputs, such as video understanding,
processing a large number of visual tokens in cross-attention layers leads to
high memory demands and often necessitates distributed computation across
multiple GPUs. Existing distributed attention mechanisms face significant
communication overheads, making cross-attention layers a critical bottleneck
for efficient training and inference of MLLMs. To address this, we propose
LV-XAttn, a distributed, exact cross-attention mechanism with minimal
communication overhead. We observe that in applications involving large visual
inputs, the size of the query block is typically much smaller than that of the
key-value blocks. Thus, in LV-XAttn we keep the large key-value blocks locally
on each GPU and exchange smaller query blocks across GPUs. We also introduce an
efficient activation recomputation technique to support longer visual context.
We theoretically analyze the communication benefits of LV-XAttn and show that
it can achieve speedups for a wide range of models. Our evaluations with Llama
3-V, mPLUG-Owl3 and OpenFlamingo models find that LV-XAttn achieves up to
10.62$\times$ end-to-end speedup compared to existing approaches.

</details>


### [390] [UniDB: A Unified Diffusion Bridge Framework via Stochastic Optimal Control](https://arxiv.org/pdf/2502.05749)
*Kaizhen Zhu, Mokai Pan, Yuexin Ma, Yanwei Fu, Jingyi Yu, Jingya Wang, Ye Shi*

Main category: cs.CV

TL;DR: UniDB introduces a unified SOC-based framework for diffusion bridges, improving detail preservation and output quality by balancing control costs and terminal penalties.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion bridge models using Doob's $h$-transform often produce blurred details and lack theoretical grounding.

Method: UniDB formulates the problem via Stochastic Optimal Control (SOC), deriving a closed-form solution for the optimal controller.

Result: UniDB generalizes existing models, achieves better detail preservation, and integrates easily with minimal code changes.

Conclusion: UniDB offers a superior, adaptable framework for diffusion bridges, validated by extensive experiments.

Abstract: Recent advances in diffusion bridge models leverage Doob's $h$-transform to
establish fixed endpoints between distributions, demonstrating promising
results in image translation and restoration tasks. However, these approaches
frequently produce blurred or excessively smoothed image details and lack a
comprehensive theoretical foundation to explain these shortcomings. To address
these limitations, we propose UniDB, a unified framework for diffusion bridges
based on Stochastic Optimal Control (SOC). UniDB formulates the problem through
an SOC-based optimization and derives a closed-form solution for the optimal
controller, thereby unifying and generalizing existing diffusion bridge models.
We demonstrate that existing diffusion bridges employing Doob's $h$-transform
constitute a special case of our framework, emerging when the terminal penalty
coefficient in the SOC cost function tends to infinity. By incorporating a
tunable terminal penalty coefficient, UniDB achieves an optimal balance between
control costs and terminal penalties, substantially improving detail
preservation and output quality. Notably, UniDB seamlessly integrates with
existing diffusion bridge models, requiring only minimal code modifications.
Extensive experiments across diverse image restoration tasks validate the
superiority and adaptability of the proposed framework. Our code is available
at https://github.com/UniDB-SOC/UniDB/.

</details>


### [391] [Galileo: Learning Global & Local Features of Many Remote Sensing Modalities](https://arxiv.org/pdf/2502.09356)
*Gabriel Tseng, Anthony Fuller, Marlena Reil, Henry Herzog, Patrick Beukema, Favyen Bastani, James R. Green, Evan Shelhamer, Hannah Kerner, David Rolnick*

Main category: cs.CV

TL;DR: A multimodal transformer (Galileo) is introduced for remote sensing tasks, outperforming specialist models across 11 benchmarks using self-supervised learning with dual contrastive losses.


<details>
  <summary>Details</summary>
Motivation: Remote sensing tasks require handling diverse data modalities and varying object scales, from small boats to large glaciers, which is challenging for shared representation learning.

Method: A novel self-supervised learning algorithm with masked modeling extracts multi-scale features. It uses dual global and local contrastive losses with different targets and masking strategies.

Result: Galileo outperforms state-of-the-art specialist models in satellite image and pixel time series tasks across eleven benchmarks.

Conclusion: The proposed generalist model effectively handles diverse remote sensing data and tasks, demonstrating superior performance over specialized approaches.

Abstract: We introduce a highly multimodal transformer to represent many remote sensing
modalities - multispectral optical, synthetic aperture radar, elevation,
weather, pseudo-labels, and more - across space and time. These inputs are
useful for diverse remote sensing tasks, such as crop mapping and flood
detection. However, learning shared representations of remote sensing data is
challenging, given the diversity of relevant data modalities, and because
objects of interest vary massively in scale, from small boats (1-2 pixels and
transient) to glaciers (thousands of pixels and persistent). We present a novel
self-supervised learning algorithm that extracts multi-scale features across a
flexible set of input modalities through masked modeling. Our dual global and
local contrastive losses differ in their targets (deep representations vs.
shallow input projections) and masking strategies (structured vs. not). Our
Galileo is a single generalist model that outperforms SoTA specialist models
for satellite images and pixel time series across eleven benchmarks and
multiple tasks.

</details>


### [392] [Semantics-aware Test-time Adaptation for 3D Human Pose Estimation](https://arxiv.org/pdf/2502.10724)
*Qiuxia Lin, Rongyu Chen, Kerui Gu, Angela Yao*

Main category: cs.CV

TL;DR: The paper addresses a semantics misalignment in 3D human pose estimation during test-time adaptation, proposing a semantics-aware motion prior to improve predictions.


<details>
  <summary>Details</summary>
Motivation: The misalignment in 3D pose estimation leads to overly smoothed and unguided predictions, especially under occlusions or truncations.

Method: The authors integrate a semantics-aware motion prior using video understanding and a motion-text space, along with 2D pose completion for occlusions.

Result: The method reduces PA-MPJPE by over 12% on 3DPW and 3DHP datasets.

Conclusion: The proposed approach significantly enhances test-time adaptation in 3D human pose estimation.

Abstract: This work highlights a semantics misalignment in 3D human pose estimation.
For the task of test-time adaptation, the misalignment manifests as overly
smoothed and unguided predictions. The smoothing settles predictions towards
some average pose. Furthermore, when there are occlusions or truncations, the
adaptation becomes fully unguided. To this end, we pioneer the integration of a
semantics-aware motion prior for the test-time adaptation of 3D pose
estimation. We leverage video understanding and a well-structured motion-text
space to adapt the model motion prediction to adhere to video semantics during
test time. Additionally, we incorporate a missing 2D pose completion based on
the motion-text similarity. The pose completion strengthens the motion prior's
guidance for occlusions and truncations. Our method significantly improves
state-of-the-art 3D human pose estimation TTA techniques, with more than 12%
decrease in PA-MPJPE on 3DPW and 3DHP.

</details>


### [393] [CHATS: Combining Human-Aligned Optimization and Test-Time Sampling for Text-to-Image Generation](https://arxiv.org/pdf/2502.12579)
*Minghao Fu, Guo-Hua Wang, Liangfu Cao, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang*

Main category: cs.CV

TL;DR: CHATS is a novel framework combining human-aligned optimization and test-time sampling to improve text-to-image generation, outperforming traditional methods with high efficiency.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models struggle with text-image alignment, generation quality, and human aesthetic consistency, despite key components like human preference alignment and classifier-free guidance.

Method: CHATS separately models preferred and dispreferred distributions and uses a proxy-prompt-based sampling strategy to leverage both distributions.

Result: CHATS achieves strong performance with minimal high-quality data, surpassing traditional methods and setting new benchmarks.

Conclusion: CHATS effectively enhances text-to-image generation by combining human-aligned optimization and test-time sampling, demonstrating superior efficiency and performance.

Abstract: Diffusion models have emerged as a dominant approach for text-to-image
generation. Key components such as the human preference alignment and
classifier-free guidance play a crucial role in ensuring generation quality.
However, their independent application in current text-to-image models
continues to face significant challenges in achieving strong text-image
alignment, high generation quality, and consistency with human aesthetic
standards. In this work, we for the first time, explore facilitating the
collaboration of human performance alignment and test-time sampling to unlock
the potential of text-to-image models. Consequently, we introduce CHATS
(Combining Human-Aligned optimization and Test-time Sampling), a novel
generative framework that separately models the preferred and dispreferred
distributions and employs a proxy-prompt-based sampling strategy to utilize the
useful information contained in both distributions. We observe that CHATS
exhibits exceptional data efficiency, achieving strong performance with only a
small, high-quality funetuning dataset. Extensive experiments demonstrate that
CHATS surpasses traditional preference alignment methods, setting new
state-of-the-art across various standard benchmarks.

</details>


### [394] [Interpreting CLIP with Hierarchical Sparse Autoencoders](https://arxiv.org/pdf/2502.20578)
*Vladimir Zaigrajew, Hubert Baniecki, Przemyslaw Biecek*

Main category: cs.CV

TL;DR: Matryoshka SAE (MSAE) improves sparse autoencoders by optimizing hierarchical representations, achieving better reconstruction and sparsity for CLIP models, and enabling interpretability and control.


<details>
  <summary>Details</summary>
Motivation: Current sparse autoencoders struggle to balance reconstruction quality and sparsity, limiting their effectiveness for interpreting complex models like CLIP.

Method: MSAE introduces hierarchical representations to optimize both metrics without compromise, tested on CLIP.

Result: MSAE achieves 0.99 cosine similarity, <0.1 unexplained variance, and ~80% sparsity, outperforming existing methods.

Conclusion: MSAE enhances interpretability and control of CLIP, demonstrated by extracting 120+ semantic concepts for downstream tasks.

Abstract: Sparse autoencoders (SAEs) are useful for detecting and steering
interpretable features in neural networks, with particular potential for
understanding complex multimodal representations. Given their ability to
uncover interpretable features, SAEs are particularly valuable for analyzing
large-scale vision-language models (e.g., CLIP and SigLIP), which are
fundamental building blocks in modern systems yet remain challenging to
interpret and control. However, current SAE methods are limited by optimizing
both reconstruction quality and sparsity simultaneously, as they rely on either
activation suppression or rigid sparsity constraints. To this end, we introduce
Matryoshka SAE (MSAE), a new architecture that learns hierarchical
representations at multiple granularities simultaneously, enabling a direct
optimization of both metrics without compromise. MSAE establishes a new
state-of-the-art Pareto frontier between reconstruction quality and sparsity
for CLIP, achieving 0.99 cosine similarity and less than 0.1 fraction of
variance unexplained while maintaining ~80% sparsity. Finally, we demonstrate
the utility of MSAE as a tool for interpreting and controlling CLIP by
extracting over 120 semantic concepts from its representation to perform
concept-based similarity search and bias analysis in downstream tasks like
CelebA. We make the codebase available at https://github.com/WolodjaZ/MSAE.

</details>


### [395] [Fast 3D point clouds retrieval for Large-scale 3D Place Recognition](https://arxiv.org/pdf/2502.21067)
*Chahine-Nicolas Zede, Laurent Carrafa, Valérie Gouet-Brunet*

Main category: cs.CV

TL;DR: The paper adapts the Differentiable Search Index (DSI) for 3D point cloud retrieval, using Vision Transformers to map descriptors to 1D identifiers for efficient, constant-time retrieval.


<details>
  <summary>Details</summary>
Motivation: Current methods for 3D point cloud retrieval rely on complex descriptor comparisons, which can be slow. The goal is to accelerate retrieval by leveraging DSI, originally designed for text, for 3D data.

Method: The approach integrates Vision Transformers to map point cloud descriptors to 1D identifiers, incorporating positional and semantic encoding. This enables direct retrieval in constant time.

Result: The method is evaluated for place recognition on a public benchmark, showing competitive performance in retrieval quality and speed compared to state-of-the-art methods.

Conclusion: Adapting DSI for 3D point cloud retrieval with Vision Transformers offers a promising solution for efficient and accurate retrieval, addressing the limitations of descriptor-based methods.

Abstract: Retrieval in 3D point clouds is a challenging task that consists in
retrieving the most similar point clouds to a given query within a reference of
3D points. Current methods focus on comparing descriptors of point clouds in
order to identify similar ones. Due to the complexity of this latter step, here
we focus on the acceleration of the retrieval by adapting the Differentiable
Search Index (DSI), a transformer-based approach initially designed for text
information retrieval, for 3D point clouds retrieval. Our approach generates 1D
identifiers based on the point descriptors, enabling direct retrieval in
constant time. To adapt DSI to 3D data, we integrate Vision Transformers to map
descriptors to these identifiers while incorporating positional and semantic
encoding. The approach is evaluated for place recognition on a public benchmark
comparing its retrieval capabilities against state-of-the-art methods, in terms
of quality and speed of returned point clouds.

</details>


### [396] [WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation](https://arxiv.org/pdf/2503.07265)
*Yuwei Niu, Munan Ning, Mengren Zheng, Weiyang Jin, Bin Lin, Peng Jin, Jiaqi Liao, Chaoran Feng, Kunpeng Ning, Bin Zhu, Li Yuan*

Main category: cs.CV

TL;DR: The paper introduces WISE, a benchmark for evaluating world knowledge integration in text-to-image models, revealing limitations in current models.


<details>
  <summary>Details</summary>
Motivation: Existing T2I models lack comprehensive evaluation of complex semantic understanding and world knowledge integration.

Method: Proposes WISE with 1000 prompts across 25 sub-domains and introduces WiScore for knowledge-image alignment.

Result: Testing 20 models shows significant limitations in world knowledge integration.

Conclusion: Highlights the need for better knowledge incorporation in future T2I models.

Abstract: Text-to-Image (T2I) models are capable of generating high-quality artistic
creations and visual content. However, existing research and evaluation
standards predominantly focus on image realism and shallow text-image
alignment, lacking a comprehensive assessment of complex semantic understanding
and world knowledge integration in text to image generation. To address this
challenge, we propose $\textbf{WISE}$, the first benchmark specifically
designed for $\textbf{W}$orld Knowledge-$\textbf{I}$nformed $\textbf{S}$emantic
$\textbf{E}$valuation. WISE moves beyond simple word-pixel mapping by
challenging models with 1000 meticulously crafted prompts across 25 sub-domains
in cultural common sense, spatio-temporal reasoning, and natural science. To
overcome the limitations of traditional CLIP metric, we introduce
$\textbf{WiScore}$, a novel quantitative metric for assessing knowledge-image
alignment. Through comprehensive testing of 20 models (10 dedicated T2I models
and 10 unified multimodal models) using 1,000 structured prompts spanning 25
subdomains, our findings reveal significant limitations in their ability to
effectively integrate and apply world knowledge during image generation,
highlighting critical pathways for enhancing knowledge incorporation and
application in next-generation T2I models. Code and data are available at
https://github.com/PKU-YuanGroup/WISE.

</details>


### [397] [From Head to Tail: Towards Balanced Representation in Large Vision-Language Models through Adaptive Data Calibration](https://arxiv.org/pdf/2503.12821)
*Mingyang Song, Xiaoye Qu, Jiawei Zhou, Yu Cheng*

Main category: cs.CV

TL;DR: The paper addresses the long-tail data imbalance in Large Vision-Language Models (LVLMs) by proposing an Adaptive Data Refinement Framework (ADR) with two stages: Data Rebalancing and Data Synthesis.


<details>
  <summary>Details</summary>
Motivation: To tackle the underrepresentation of tail concepts and overrepresentation of head concepts in LVLM training data, which limits performance in tasks like Visual Question Answering and Visual Reasoning.

Method: Proposes ADR with two stages: Data Rebalancing (DR) to adjust entity distributions and Data Synthesis (DS) using DDPMs to supplement scarce data.

Result: ADR improves LLaVA 1.5's average performance by 4.36% across eleven benchmarks without increasing data volume.

Conclusion: ADR effectively mitigates long-tail issues in LVLMs, enhancing model performance for underrepresented concepts.

Abstract: Large Vision-Language Models (LVLMs) have achieved significant progress in
combining visual comprehension with language generation. Despite this success,
the training data of LVLMs still suffers from Long-Tail (LT) problems, where
the data distribution is highly imbalanced. Previous works have mainly focused
on traditional VLM architectures, i.e., CLIP or ViT, and specific tasks such as
recognition and classification. Nevertheless, the exploration of LVLM (e.g.
LLaVA) and more general tasks (e.g. Visual Question Answering and Visual
Reasoning) remains under-explored. In this paper, we first conduct an in-depth
analysis of the LT issues in LVLMs and identify two core causes: the
overrepresentation of head concepts and the underrepresentation of tail
concepts. Based on the above observation, we propose an $\textbf{A}$daptive
$\textbf{D}$ata $\textbf{R}$efinement Framework ($\textbf{ADR}$), which
consists of two stages: $\textbf{D}$ata $\textbf{R}$ebalancing ($\textbf{DR}$)
and $\textbf{D}$ata $\textbf{S}$ynthesis ($\textbf{DS}$). In the DR stage, we
adaptively rebalance the redundant data based on entity distributions, while in
the DS stage, we leverage Denoising Diffusion Probabilistic Models (DDPMs) and
scarce images to supplement underrepresented portions. Through comprehensive
evaluations across eleven benchmarks, our proposed ADR effectively mitigates
the long-tail problem in the training data, improving the average performance
of LLaVA 1.5 relatively by 4.36%, without increasing the training data volume.

</details>


### [398] [Human-Object Interaction via Automatically Designed VLM-Guided Motion Policy](https://arxiv.org/pdf/2503.18349)
*Zekai Deng, Ye Shi, Kaiyang Ji, Lan Xu, Shaoli Huang, Jingya Wang*

Main category: cs.CV

TL;DR: A unified physics-based HOI framework using Vision-Language Models (VLMs) for scalable, generalizable human-object interaction synthesis without manual reward tuning.


<details>
  <summary>Details</summary>
Motivation: Existing HOI methods rely on costly motion capture or manual reward engineering, limiting scalability and generalizability.

Method: Introduces VLM-Guided Relative Movement Dynamics (RMD), a spatio-temporal bipartite representation for automatic goal state and reward function construction in reinforcement learning.

Result: Outperforms existing methods in synthesizing natural, human-like motions across simple and complex scenarios.

Conclusion: The framework enables scalable, generalizable HOI synthesis with minimal manual intervention, supported by the novel Interplay dataset.

Abstract: Human-object interaction (HOI) synthesis is crucial for applications in
animation, simulation, and robotics. However, existing approaches either rely
on expensive motion capture data or require manual reward engineering, limiting
their scalability and generalizability. In this work, we introduce the first
unified physics-based HOI framework that leverages Vision-Language Models
(VLMs) to enable long-horizon interactions with diverse object types, including
static, dynamic, and articulated objects. We introduce VLM-Guided Relative
Movement Dynamics (RMD), a fine-grained spatio-temporal bipartite
representation that automatically constructs goal states and reward functions
for reinforcement learning. By encoding structured relationships between human
and object parts, RMD enables VLMs to generate semantically grounded,
interaction-aware motion guidance without manual reward tuning. To support our
methodology, we present Interplay, a novel dataset with thousands of
long-horizon static and dynamic interaction plans. Extensive experiments
demonstrate that our framework outperforms existing methods in synthesizing
natural, human-like motions across both simple single-task and complex
multi-task scenarios. For more details, please refer to our project webpage:
https://vlm-rmd.github.io/.

</details>


### [399] [Inference-Time Scaling for Flow Models via Stochastic Generation and Rollover Budget Forcing](https://arxiv.org/pdf/2503.19385)
*Jaihoon Kim, Taehoon Yoon, Jisung Hwang, Minhyuk Sung*

Main category: cs.CV

TL;DR: The paper introduces an inference-time scaling method for pretrained flow models, addressing limitations of deterministic processes by proposing SDE-based generation, Interpolant conversion, and Rollover Budget Forcing (RBF).


<details>
  <summary>Details</summary>
Motivation: Flow models, while efficient, lack effective inference-time scaling methods like those in diffusion models due to their deterministic nature. This work aims to bridge that gap.

Method: Three key ideas: 1) SDE-based generation for particle sampling, 2) Interpolant conversion for diversity, and 3) RBF for adaptive computational resource allocation.

Result: VP-SDE-based generation improves particle sampling, and RBF with VP-SDE outperforms prior inference-time scaling methods.

Conclusion: The proposed methods enable efficient inference-time scaling for flow models, enhancing performance and computational resource utilization.

Abstract: We propose an inference-time scaling approach for pretrained flow models.
Recently, inference-time scaling has gained significant attention in LLMs and
diffusion models, improving sample quality or better aligning outputs with user
preferences by leveraging additional computation. For diffusion models,
particle sampling has allowed more efficient scaling due to the stochasticity
at intermediate denoising steps. On the contrary, while flow models have gained
popularity as an alternative to diffusion models--offering faster generation
and high-quality outputs in state-of-the-art image and video generative
models--efficient inference-time scaling methods used for diffusion models
cannot be directly applied due to their deterministic generative process. To
enable efficient inference-time scaling for flow models, we propose three key
ideas: 1) SDE-based generation, enabling particle sampling in flow models, 2)
Interpolant conversion, broadening the search space and enhancing sample
diversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of
computational resources across timesteps to maximize budget utilization. Our
experiments show that SDE-based generation, particularly variance-preserving
(VP) interpolant-based generation, improves the performance of particle
sampling methods for inference-time scaling in flow models. Additionally, we
demonstrate that RBF with VP-SDE achieves the best performance, outperforming
all previous inference-time scaling approaches.

</details>


### [400] [Latent Beam Diffusion Models for Decoding Image Sequences](https://arxiv.org/pdf/2503.20429)
*Guilherme Fernandes, Vasco Ramos, Regev Cohen, Idan Szpektor, João Magalhães*

Main category: cs.CV

TL;DR: BeamDiffusion improves visual consistency in image sequences by using a novel beam search strategy for latent space exploration, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models struggle with visual consistency in image sequences, especially in non-linear storytelling, due to independent generation of images.

Method: Introduces a beam search strategy for latent space exploration, dynamically searching for optimal latent sequences and pruning paths with a cross-attention mechanism.

Result: BeamDiffusion outperforms baselines, producing sequences with better coherence, visual continuity, and textual alignment.

Conclusion: The proposed method effectively addresses visual consistency challenges in image sequence generation.

Abstract: While diffusion models excel at generating high-quality images from text
prompts, they struggle with visual consistency in image sequences. Existing
methods generate each image independently, leading to disjointed narratives - a
challenge further exacerbated in non-linear storytelling, where scenes must
connect beyond adjacent frames. We introduce a novel beam search strategy for
latent space exploration, enabling conditional generation of full image
sequences with beam search decoding. Unlike prior approaches that use fixed
latent priors, our method dynamically searches for an optimal sequence of
latent representations, ensuring coherent visual transitions. As the latent
denoising space is explored, the beam search graph is pruned with a
cross-attention mechanism that efficiently scores search paths, prioritizing
alignment with both textual prompts and visual context. Human and automatic
evaluations confirm that BeamDiffusion outperforms other baseline methods,
producing full sequences with superior coherence, visual continuity, and
textual alignment.

</details>


### [401] [Imitating Radiological Scrolling: A Global-Local Attention Model for 3D Chest CT Volumes Multi-Label Anomaly Classification](https://arxiv.org/pdf/2503.20652)
*Theo Di Piazza, Carole Lazarus, Olivier Nempont, Loic Boussel*

Main category: cs.CV

TL;DR: CT-Scroll, a global-local attention model, emulates radiologists' scrolling behavior for multi-label classification in 3D CT scans, outperforming CNNs and Vision Transformers.


<details>
  <summary>Details</summary>
Motivation: The surge in CT scans necessitates automated tools to aid radiologists, but current methods fail to capture long-range dependencies or mimic radiologists' navigational behavior.

Method: CT-Scroll combines global and local attention to replicate radiologists' scrolling, tested on two public datasets with ablation studies.

Result: The model shows efficacy in experiments, with ablation studies confirming the importance of its components.

Conclusion: CT-Scroll effectively addresses the limitations of existing methods, offering a practical solution for 3D CT scan analysis.

Abstract: The rapid increase in the number of Computed Tomography (CT) scan
examinations has created an urgent need for automated tools, such as organ
segmentation, anomaly classification, and report generation, to assist
radiologists with their growing workload. Multi-label classification of
Three-Dimensional (3D) CT scans is a challenging task due to the volumetric
nature of the data and the variety of anomalies to be detected. Existing deep
learning methods based on Convolutional Neural Networks (CNNs) struggle to
capture long-range dependencies effectively, while Vision Transformers require
extensive pre-training, posing challenges for practical use. Additionally,
these existing methods do not explicitly model the radiologist's navigational
behavior while scrolling through CT scan slices, which requires both global
context understanding and local detail awareness. In this study, we present
CT-Scroll, a novel global-local attention model specifically designed to
emulate the scrolling behavior of radiologists during the analysis of 3D CT
scans. Our approach is evaluated on two public datasets, demonstrating its
efficacy through comprehensive experiments and an ablation study that
highlights the contribution of each model component.

</details>


### [402] [ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation](https://arxiv.org/pdf/2503.22194)
*Yunhong Min, Daehyeon Choi, Kyeongmin Yeo, Jihyun Lee, Minhyuk Sung*

Main category: cs.CV

TL;DR: ORIGEN is a zero-shot method for 3D orientation grounding in text-to-image generation, outperforming existing methods with a reward-guided sampling approach.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack control over 3D orientation in text-to-image generation, focusing only on 2D positioning.

Method: Uses a reward-guided sampling approach with a pretrained discriminative model for 3D orientation estimation and a one-step generative flow model, employing Langevin dynamics for optimization.

Result: ORIGEN outperforms training-based and test-time guidance methods in quantitative metrics and user studies.

Conclusion: ORIGEN effectively addresses 3D orientation grounding in text-to-image generation with a simple yet powerful sampling-based approach.

Abstract: We introduce ORIGEN, the first zero-shot method for 3D orientation grounding
in text-to-image generation across multiple objects and diverse categories.
While previous work on spatial grounding in image generation has mainly focused
on 2D positioning, it lacks control over 3D orientation. To address this, we
propose a reward-guided sampling approach using a pretrained discriminative
model for 3D orientation estimation and a one-step text-to-image generative
flow model. While gradient-ascent-based optimization is a natural choice for
reward-based guidance, it struggles to maintain image realism. Instead, we
adopt a sampling-based approach using Langevin dynamics, which extends gradient
ascent by simply injecting random noise--requiring just a single additional
line of code. Additionally, we introduce adaptive time rescaling based on the
reward function to accelerate convergence. Our experiments show that ORIGEN
outperforms both training-based and test-time guidance methods across
quantitative metrics and user studies.

</details>


### [403] [HumanAesExpert: Advancing a Multi-Modality Foundation Model for Human Image Aesthetic Assessment](https://arxiv.org/pdf/2503.23907)
*Zhichao Liao, Xiaokun Liu, Wenyu Qin, Qingyu Li, Qiulin Wang, Pengfei Wan, Di Zhang, Long Zeng, Pingfa Feng*

Main category: cs.CV

TL;DR: The paper introduces HumanBeauty, the first dataset for Human Image Aesthetic Assessment (HIAA), and proposes HumanAesExpert, a Vision Language Model for HIAA, achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: To address the under-explored subset of Image Aesthetic Assessment (IAA) focusing on human images (HIAA).

Method: Created HumanBeauty dataset (108k images) with manual annotations and a 12-dimensional aesthetic standard. Proposed HumanAesExpert model with Expert, LM, and Regression heads, and a MetaVoter for score aggregation.

Result: HumanAesExpert outperforms state-of-the-art models in HIAA.

Conclusion: The work pioneers HIAA with a tailored dataset and model, demonstrating significant improvements in assessment precision.

Abstract: Image Aesthetic Assessment (IAA) is a long-standing and challenging research
task. However, its subset, Human Image Aesthetic Assessment (HIAA), has been
scarcely explored. To bridge this research gap, our work pioneers a holistic
implementation framework tailored for HIAA. Specifically, we introduce
HumanBeauty, the first dataset purpose-built for HIAA, which comprises 108k
high-quality human images with manual annotations. To achieve comprehensive and
fine-grained HIAA, 50K human images are manually collected through a rigorous
curation process and annotated leveraging our trailblazing 12-dimensional
aesthetic standard, while the remaining 58K with overall aesthetic labels are
systematically filtered from public datasets. Based on the HumanBeauty
database, we propose HumanAesExpert, a powerful Vision Language Model for
aesthetic evaluation of human images. We innovatively design an Expert head to
incorporate human knowledge of aesthetic sub-dimensions while jointly utilizing
the Language Modeling (LM) and Regression heads. This approach empowers our
model to achieve superior proficiency in both overall and fine-grained HIAA.
Furthermore, we introduce a MetaVoter, which aggregates scores from all three
heads, to effectively balance the capabilities of each head, thereby realizing
improved assessment precision. Extensive experiments demonstrate that our
HumanAesExpert models deliver significantly better performance in HIAA than
other state-of-the-art models. Project webpage:
https://humanaesexpert.github.io/HumanAesExpert/

</details>


### [404] [A Knowledge-guided Adversarial Defense for Resisting Malicious Visual Manipulation](https://arxiv.org/pdf/2504.08411)
*Dawei Zhou, Suzhi Gang, Decheng Liu, Tongliang Liu, Nannan Wang, Xinbo Gao*

Main category: cs.CV

TL;DR: A knowledge-guided adversarial defense (KGAD) is proposed to combat visual manipulation by focusing on semantic disruptions and perception metrics, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of low-level feature distortions in current adversarial noise-based defenses against malicious visual manipulation.

Method: KGAD integrates domain-specific knowledge to create semantic confusions and uses perception-related metrics for adversarial noise generation.

Result: Experiments show KGAD provides superior protection and generalizability compared to state-of-the-art methods.

Conclusion: KGAD effectively disrupts malicious manipulation models by leveraging knowledge and perception, offering robust defense.

Abstract: Malicious applications of visual manipulation have raised serious threats to
the security and reputation of users in many fields. To alleviate these issues,
adversarial noise-based defenses have been enthusiastically studied in recent
years. However, ``data-only" methods tend to distort fake samples in the
low-level feature space rather than the high-level semantic space, leading to
limitations in resisting malicious manipulation. Frontier research has shown
that integrating knowledge in deep learning can produce reliable and
generalizable solutions. Inspired by these, we propose a knowledge-guided
adversarial defense (KGAD) to actively force malicious manipulation models to
output semantically confusing samples. Specifically, in the process of
generating adversarial noise, we focus on constructing significant semantic
confusions at the domain-specific knowledge level, and exploit a metric closely
related to visual perception to replace the general pixel-wise metrics. The
generated adversarial noise can actively interfere with the malicious
manipulation model by triggering knowledge-guided and perception-related
disruptions in the fake samples. To validate the effectiveness of the proposed
method, we conduct qualitative and quantitative experiments on human perception
and visual quality assessment. The results on two different tasks both show
that our defense provides better protection compared to state-of-the-art
methods and achieves great generalizability.

</details>


### [405] [Progressive Language-guided Visual Learning for Multi-Task Visual Grounding](https://arxiv.org/pdf/2504.16145)
*Jingchao Wang, Hong Wang, Wenlong Zhang, Kunhua Ji, Dingjiang Huang, Yefeng Zheng*

Main category: cs.CV

TL;DR: PLVL is a framework for multi-task visual grounding (MTVG) that integrates language into visual feature extraction and uses a multi-task head for collaborative predictions, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing MTVG approaches have limitations: incomplete linguistic injection into visual features and ineffective exploitation of REC-RES task relationships.

Method: PLVL progressively injects language information into visual learning and uses a multi-task head for collaborative predictions.

Result: PLVL outperforms existing methods on REC and RES tasks across benchmark datasets.

Conclusion: PLVL effectively addresses limitations of current MTVG approaches by integrating language guidance and collaborative task predictions.

Abstract: Multi-task visual grounding (MTVG) includes two sub-tasks, i.e., Referring
Expression Comprehension (REC) and Referring Expression Segmentation (RES). The
existing representative approaches generally follow the research pipeline which
mainly consists of three core procedures, including independent feature
extraction for visual and linguistic modalities, respectively, cross-modal
interaction module, and independent prediction heads for different sub-tasks.
Albeit achieving remarkable performance, this research line has two
limitations: 1) The linguistic content has not been fully injected into the
entire visual backbone for boosting more effective visual feature extraction
and it needs an extra cross-modal interaction module; 2) The relationship
between REC and RES tasks is not effectively exploited to help the
collaborative prediction for more accurate output. To deal with these problems,
in this paper, we propose a Progressive Language-guided Visual Learning
framework for multi-task visual grounding, called PLVL, which not only finely
mine the inherent feature expression of the visual modality itself but also
progressively inject the language information to help learn linguistic-related
visual features. In this manner, our PLVL does not need additional cross-modal
fusion module while fully introducing the language guidance. Furthermore, we
analyze that the localization center for REC would help identify the
to-be-segmented object region for RES to some extent. Inspired by this
investigation, we design a multi-task head to accomplish collaborative
predictions for these two sub-tasks. Extensive experiments conducted on several
benchmark datasets comprehensively substantiate that our PLVL obviously
outperforms the representative methods in both REC and RES tasks.
https://github.com/jcwang0602/PLVL

</details>


### [406] [Enhancing Target-unspecific Tasks through a Features Matrix](https://arxiv.org/pdf/2505.03414)
*Fangming Cui, Yonggang Zhang, Xuan Wang, Xinmei Tian, Jun Yu*

Main category: cs.CV

TL;DR: A novel Features Matrix (FM) approach enhances Vision-Language Models (VLMs) for target-unspecific tasks by preserving general knowledge and mitigating overfitting, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing prompt learning methods for VLMs struggle with target-unspecific tasks due to overfitting and loss of general knowledge.

Method: Proposes a Features Matrix (FM) that extracts and leverages general knowledge from diverse inputs to preserve essential semantics and prevent overfitting.

Result: FM is compatible with existing frameworks and significantly improves performance in target-unspecific tasks like base-to-novel, domain, and cross-dataset generalization.

Conclusion: The FM approach effectively addresses the limitations of current methods, enhancing VLMs' generalizability and performance in diverse tasks.

Abstract: Recent developments in prompt learning of large Vision-Language Models (VLMs)
have significantly improved performance in target-specific tasks. However,
these prompting methods often struggle to tackle the target-unspecific or
generalizable tasks effectively. It may be attributed to the fact that
overfitting training causes the model to forget its general knowledge. The
general knowledge has a strong promotion on target-unspecific tasks. To
alleviate this issue, we propose a novel Features Matrix (FM) approach designed
to enhance these models on target-unspecific tasks. Our method extracts and
leverages general knowledge, shaping a Features Matrix (FM). Specifically, the
FM captures the semantics of diverse inputs from a deep and fine perspective,
preserving essential general knowledge, which mitigates the risk of
overfitting. Representative evaluations demonstrate that: 1) the FM is
compatible with existing frameworks as a generic and flexible module, and 2)
the FM significantly showcases its effectiveness in enhancing target-unspecific
tasks (base-to-novel generalization, domain generalization, and cross-dataset
generalization), achieving state-of-the-art performance.

</details>


### [407] [A Weak Supervision Learning Approach Towards an Equitable Mobility Estimation](https://arxiv.org/pdf/2505.04229)
*Theophilus Aidoo, Till Koebe, Akansh Maurya, Hewan Shrestha, Ingmar Weber*

Main category: cs.CV

TL;DR: A weak supervision framework uses 3m satellite imagery and coarse temporal labels to estimate parking lot occupancy, achieving high accuracy (AUC 0.92) and reducing reliance on costly high-resolution data.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity and high cost of labeled high-resolution imagery, especially in low-income regions, for remote sensing applications.

Method: Proposes a pairwise comparison model trained with coarse temporal labels (e.g., full on Saturdays, empty on Sundays) using 3m resolution satellite imagery.

Result: Achieves an AUC of 0.92 on large parking lots, demonstrating effectiveness with minimal high-resolution data.

Conclusion: The framework is scalable for urban mobility analysis and adaptable for transit patterns and resource allocation in vulnerable communities, aiding data-driven improvements.

Abstract: The scarcity and high cost of labeled high-resolution imagery have long
challenged remote sensing applications, particularly in low-income regions
where high-resolution data are scarce. In this study, we propose a weak
supervision framework that estimates parking lot occupancy using 3m resolution
satellite imagery. By leveraging coarse temporal labels -- based on the
assumption that parking lots of major supermarkets and hardware stores in
Germany are typically full on Saturdays and empty on Sundays -- we train a
pairwise comparison model that achieves an AUC of 0.92 on large parking lots.
The proposed approach minimizes the reliance on expensive high-resolution
images and holds promise for scalable urban mobility analysis. Moreover, the
method can be adapted to assess transit patterns and resource allocation in
vulnerable communities, providing a data-driven basis to improve the well-being
of those most in need.

</details>


### [408] [Benchmarking performance, explainability, and evaluation strategies of vision-language models for surgery: Challenges and opportunities](https://arxiv.org/pdf/2505.10764)
*Jiajun Cheng, Xianwu Zhao, Shan Lin*

Main category: cs.CV

TL;DR: The paper evaluates the performance of vision-language models (VLMs) on surgical data, highlighting their limitations and proposing a metric to assess task-relevant attention.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of limited generalization in existing surgical models and leverage unlabeled surgical data using VLMs.

Method: Benchmarking study of VLMs on laparoscopic datasets, visualizing model attention and proposing a metric for task-relevant focus.

Result: Mismatch between prediction accuracy and visual grounding, with models sometimes focusing on irrelevant areas.

Conclusion: Existing VLMs struggle with surgical data, and future research should improve their visual grounding for better performance.

Abstract: Minimally invasive surgery (MIS) presents significant visual challenges,
including a limited field of view, specular reflections, and inconsistent
lighting conditions due to the small incision and the use of endoscopes. Over
the past decade, many machine learning and deep learning models have been
developed to identify and detect instruments and anatomical structures in
surgical videos. However, these models are typically trained on manually
labeled, procedure- and task-specific datasets that are relatively small,
resulting in limited generalization to unseen data.In practice, hospitals
generate a massive amount of raw surgical data every day, including videos
captured during various procedures. Labeling this data is almost impractical,
as it requires highly specialized expertise. The recent success of
vision-language models (VLMs), which can be trained on large volumes of raw
image-text pairs and exhibit strong adaptability, offers a promising
alternative for leveraging unlabeled surgical data. While some existing work
has explored applying VLMs to surgical tasks, their performance remains
limited. To support future research in developing more effective VLMs for
surgical applications, this paper aims to answer a key question: How well do
existing VLMs, both general-purpose and surgery-specific perform on surgical
data, and what types of scenes do they struggle with? To address this, we
conduct a benchmarking study of several popular VLMs across diverse
laparoscopic datasets. Specifically, we visualize the model's attention to
identify which regions of the image it focuses on when making predictions for
surgical tasks. We also propose a metric to evaluate whether the model attends
to task-relevant regions. Our findings reveal a mismatch between prediction
accuracy and visual grounding, indicating that models may make correct
predictions while focusing on irrelevant areas of the image.

</details>


### [409] [Visuospatial Cognitive Assistant](https://arxiv.org/pdf/2505.12312)
*Qi Feng*

Main category: cs.CV

TL;DR: The paper introduces ViCA-322K, a dataset for video-based spatial cognition, and ViCA-7B, a model that outperforms others on spatial tasks. It also provides reasoning datasets and models for interpretability.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of video-based spatial cognition in Vision-Language Models (VLMs) by providing diverse datasets and models.

Method: Developed ViCA-322K dataset and fine-tuned ViCA-7B model, then extended with reasoning chains (ViCA-Thinking-2.68K) and a reasoning model (ViCA-7B-Thinking).

Result: ViCA-7B achieves state-of-the-art performance on VSI-Bench tasks, with significant improvements (e.g., +26.1 on Absolute Distance).

Conclusion: Targeted data and models enhance spatial cognition in VLMs, with released resources to advance visuospatial intelligence research.

Abstract: Video-based spatial cognition is vital for robotics and embodied AI but
challenges current Vision-Language Models (VLMs). This paper makes two key
contributions. First, we introduce ViCA (Visuospatial Cognitive
Assistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor
videos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D
metadata-grounded queries and video-based complex reasoning. Second, we develop
ViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all
eight VSI-Bench tasks, outperforming existing models, including larger ones
(e.g., +26.1 on Absolute Distance). For interpretability, we present
ViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune
ViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial
reasoning. Our work highlights the importance of targeted data and suggests
paths for improved temporal-spatial modeling. We release all resources to
foster research in robust visuospatial intelligence.

</details>


### [410] [Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts](https://arxiv.org/pdf/2505.12363)
*Qi Feng*

Main category: cs.CV

TL;DR: ViCA2 is a new MLLM enhancing spatial reasoning with a dual vision encoder and token ratio control, outperforming larger models on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs lack fine-grained spatial understanding due to inadequate architecture and training data.

Method: ViCA2 integrates SigLIP for semantics and Hiera for spatial structure, using a token ratio control mechanism. A new dataset, ViCA-322K, was created for instruction tuning.

Result: ViCA2-7B achieves 56.8 on VSI-Bench, surpassing larger models like LLaVA-NeXT-Video-72B (40.9) and Gemini-1.5 Pro (45.4).

Conclusion: ViCA2 demonstrates strong visuospatial intelligence with a compact model, and its resources are released for further research.

Abstract: While Multimodal Large Language Models (MLLMs) excel at general
vision-language tasks, visuospatial cognition - reasoning about spatial
layouts, relations, and dynamics - remains a significant challenge. Existing
models often lack the necessary architectural components and specialized
training data for fine-grained spatial understanding. We introduce ViCA2
(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial
reasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP
for semantics and Hiera for spatial structure, coupled with a token ratio
control mechanism for efficiency. We also developed ViCA-322K, a new
large-scale dataset with over 322,000 spatially grounded question-answer pairs
for targeted instruction tuning. On the challenging VSI-Bench benchmark, our
ViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly
surpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and
leading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the
effectiveness of our approach in achieving strong visuospatial intelligence
with a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset
to facilitate further research.

</details>


### [411] [Hunyuan-Game: Industrial-grade Intelligent Game Creation Model](https://arxiv.org/pdf/2505.14135)
*Ruihuang Li, Caijin Zhou, Shoujian Zheng, Jianxiang Lu, Jiabin Huang, Comi Chen, Junshu Tang, Guangzheng Xu, Jiale Tao, Hongmei Wang, Donghao Li, Wenqing Yu, Senbo Wang, Zhimin Li, Yetshuan Shi, Haoyu Yang, Yukun Wang, Wenxun Dai, Jiaqi Li, Linqing Wang, Qixun Wang, Zhiyong Xu, Yingfang Zhang, Jiangfeng Xiong, Weijie Kong, Chao Zhang, Hongxin Zhang, Qiaoling Zheng, Weiting Guo, Xinchi Deng, Yixuan Li, Renjia Wei, Yulin Jian, Duojun Huang, Xuhua Ren, Junkun Yuan, Zhengguang Zhou, Jiaxiang Cheng, Bing Ma, Shirui Huang, Jiawang Bai, Chao Li, Sihuan Lin, Yifu Sun, Yuan Zhou, Joey Wang, Qin Lin, Tianxiang Zheng, Jingmiao Yu, Jihong Zhang, Caesar Zhong, Di Wang, Yuhong Liu, Linus, Jie Jiang, Longhuang Wu, Shuai Shao, Qinglin Lu*

Main category: cs.CV

TL;DR: Hunyuan-Game is a project using AI to generate high-quality game assets (images and videos) to enhance game development and align with player preferences.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of synthesizing high-quality game assets and boost designer efficiency.

Method: Develops customized image and video generation models (e.g., text-to-image, game visual effects, transparent images, character generation) and video models (e.g., image-to-video, pose avatar synthesis).

Result: Models exhibit high aesthetic expression and domain-specific knowledge, adapting to diverse game scenarios.

Conclusion: Hunyuan-Game revolutionizes intelligent game production by integrating AI for asset generation.

Abstract: Intelligent game creation represents a transformative advancement in game
development, utilizing generative artificial intelligence to dynamically
generate and enhance game content. Despite notable progress in generative
models, the comprehensive synthesis of high-quality game assets, including both
images and videos, remains a challenging frontier. To create high-fidelity game
content that simultaneously aligns with player preferences and significantly
boosts designer efficiency, we present Hunyuan-Game, an innovative project
designed to revolutionize intelligent game production. Hunyuan-Game encompasses
two primary branches: image generation and video generation. The image
generation component is built upon a vast dataset comprising billions of game
images, leading to the development of a group of customized image generation
models tailored for game scenarios: (1) General Text-to-Image Generation. (2)
Game Visual Effects Generation, involving text-to-effect and reference
image-based game visual effect generation. (3) Transparent Image Generation for
characters, scenes, and game visual effects. (4) Game Character Generation
based on sketches, black-and-white images, and white models. The video
generation component is built upon a comprehensive dataset of millions of game
and anime videos, leading to the development of five core algorithmic models,
each targeting critical pain points in game development and having robust
adaptation to diverse game video scenarios: (1) Image-to-Video Generation. (2)
360 A/T Pose Avatar Video Synthesis. (3) Dynamic Illustration Generation. (4)
Generative Video Super-Resolution. (5) Interactive Game Video Generation. These
image and video generation models not only exhibit high-level aesthetic
expression but also deeply integrate domain-specific knowledge, establishing a
systematic understanding of diverse game and anime art styles.

</details>


### [412] [AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of Cross-Modal Embeddings](https://arxiv.org/pdf/2505.14664)
*Yilin Ye, Junchao Huang, Xingchen Zeng, Jiazhi Xia, Wei Zeng*

Main category: cs.CV

TL;DR: AKRMap is a new dimensionality reduction (DR) technique for visualizing cross-modal embeddings, outperforming traditional methods like PCA and t-SNE by incorporating cross-modal metrics and enabling interactive exploration.


<details>
  <summary>Details</summary>
Motivation: Traditional DR methods like PCA and t-SNE focus on single-modality feature distributions and ignore cross-modal metrics, limiting their effectiveness for multi-modal models.

Method: AKRMap learns kernel regression of the metric landscape in the projection space, using a supervised projection network with adaptive generalized kernels and a post-projection kernel regression loss.

Result: AKRMap generates more accurate and trustworthy visualizations, as demonstrated in quantitative experiments, and effectively visualizes cross-modal embeddings for text-to-image models.

Conclusion: AKRMap advances cross-modal embedding visualization by integrating metric-aware projections and interactive features, offering superior performance over existing DR methods.

Abstract: Cross-modal embeddings form the foundation for multi-modal models. However,
visualization methods for interpreting cross-modal embeddings have been
primarily confined to traditional dimensionality reduction (DR) techniques like
PCA and t-SNE. These DR methods primarily focus on feature distributions within
a single modality, whilst failing to incorporate metrics (e.g., CLIPScore)
across multiple modalities. This paper introduces AKRMap, a new DR technique
designed to visualize cross-modal embeddings metric with enhanced accuracy by
learning kernel regression of the metric landscape in the projection space.
Specifically, AKRMap constructs a supervised projection network guided by a
post-projection kernel regression loss, and employs adaptive generalized
kernels that can be jointly optimized with the projection. This approach
enables AKRMap to efficiently generate visualizations that capture complex
metric distributions, while also supporting interactive features such as zoom
and overlay for deeper exploration. Quantitative experiments demonstrate that
AKRMap outperforms existing DR methods in generating more accurate and
trustworthy visualizations. We further showcase the effectiveness of AKRMap in
visualizing and comparing cross-modal embeddings for text-to-image models. Code
and demo are available at https://github.com/yilinye/AKRMap.

</details>


### [413] [Mouse Lockbox Dataset: Behavior Recognition for Mice Solving Lockboxes](https://arxiv.org/pdf/2505.15408)
*Patrik Reiske, Marcus N. Boon, Niek Andresen, Sole Traverso, Katharina Hohlbaum, Lars Lewejohann, Christa Thöne-Reineke, Olaf Hellwich, Henning Sprekeler*

Main category: cs.CV

TL;DR: A video dataset of mice solving complex mechanical puzzles is presented to advance automated behavior classification in neuroscience.


<details>
  <summary>Details</summary>
Motivation: Existing datasets focus on simple or social behaviors, lacking complexity for refining machine learning methods in studying natural animal behavior.

Method: The dataset includes over 110 hours of video from three perspectives, with human-annotated labels for 13% of it. A keypoint tracking-based action classification framework is used.

Result: The framework highlights challenges in automated labeling of fine-grained behaviors, like object manipulation.

Conclusion: The dataset aims to accelerate progress in automated action and behavior classification in neuroscience and is publicly available.

Abstract: Machine learning and computer vision methods have a major impact on the study
of natural animal behavior, as they enable the (semi-)automatic analysis of
vast amounts of video data. Mice are the standard mammalian model system in
most research fields, but the datasets available today to refine such methods
focus either on simple or social behaviors. In this work, we present a video
dataset of individual mice solving complex mechanical puzzles, so-called
lockboxes. The more than 110 hours of total playtime show their behavior
recorded from three different perspectives. As a benchmark for frame-level
action classification methods, we provide human-annotated labels for all videos
of two different mice, that equal 13% of our dataset. Our keypoint (pose)
tracking-based action classification framework illustrates the challenges of
automated labeling of fine-grained behaviors, such as the manipulation of
objects. We hope that our work will help accelerate the advancement of
automated action and behavior classification in the computational neuroscience
community. Our dataset is publicly available at
https://doi.org/10.14279/depositonce-23850

</details>


### [414] [An Effective Training Framework for Light-Weight Automatic Speech Recognition Models](https://arxiv.org/pdf/2505.16991)
*Abdul Hannan, Alessio Brutti, Shah Nawaz, Mubashir Noman*

Main category: cs.CV

TL;DR: A two-step representation learning approach is introduced to create smaller ASR models from a large one, improving performance and training speed without significant degradation.


<details>
  <summary>Details</summary>
Motivation: Deploying large ASR models on low-resource devices is impractical due to computational constraints, and existing methods degrade performance or require lengthy training.

Method: A two-step representation learning approach produces smaller models from a single large model, ensuring better performance in fewer epochs.

Result: Achieves three-fold training speed-up and up to 12.54% word error rate improvement on ASR benchmarks.

Conclusion: The proposed method effectively addresses deployment challenges for ASR models on low-resource devices with improved efficiency and performance.

Abstract: Recent advancement in deep learning encouraged developing large automatic
speech recognition (ASR) models that achieve promising results while ignoring
computational and memory constraints. However, deploying such models on low
resource devices is impractical despite of their favorable performance.
Existing approaches (pruning, distillation, layer skip etc.) transform the
large models into smaller ones at the cost of significant performance
degradation or require prolonged training of smaller models for better
performance. To address these issues, we introduce an efficacious two-step
representation learning based approach capable of producing several small sized
models from a single large model ensuring considerably better performance in
limited number of epochs. Comprehensive experimentation on ASR benchmarks
reveals the efficacy of our approach, achieving three-fold training speed-up
and up to 12.54% word error rate improvement.

</details>


### [415] [PAEFF: Precise Alignment and Enhanced Gated Feature Fusion for Face-Voice Association](https://arxiv.org/pdf/2505.17002)
*Abdul Hannan, Muhammad Arslan Manzoor, Shah Nawaz, Muhammad Irzam Liaqat, Markus Schedl, Mubashir Noman*

Main category: cs.CV

TL;DR: The paper proposes a method to improve face-voice association by aligning and fusing embedding spaces with enhanced gated fusion, addressing issues in negative mining and margin parameters.


<details>
  <summary>Details</summary>
Motivation: Current methods for face-voice association suffer from flawed negative mining and reliance on distant margin parameters, prompting the need for a better approach.

Method: The method learns a joint embedding space with orthogonality constraints, aligns the spaces, and uses enhanced gated fusion for improved association.

Result: Experiments on the VoxCeleb dataset show the proposed method outperforms existing approaches.

Conclusion: The alignment and fusion of embedding spaces with enhanced gated fusion significantly improves face-voice association performance.

Abstract: We study the task of learning association between faces and voices, which is
gaining interest in the multimodal community lately. These methods suffer from
the deliberate crafting of negative mining procedures as well as the reliance
on the distant margin parameter. These issues are addressed by learning a joint
embedding space in which orthogonality constraints are applied to the fused
embeddings of faces and voices. However, embedding spaces of faces and voices
possess different characteristics and require spaces to be aligned before
fusing them. To this end, we propose a method that accurately aligns the
embedding spaces and fuses them with an enhanced gated fusion thereby improving
the performance of face-voice association. Extensive experiments on the
VoxCeleb dataset reveals the merits of the proposed approach.

</details>


### [416] [EVM-Fusion: An Explainable Vision Mamba Architecture with Neural Algorithmic Fusion](https://arxiv.org/pdf/2505.17367)
*Zichuan Yang*

Main category: cs.CV

TL;DR: EVM-Fusion is an explainable medical image classifier combining DenseNet, U-Net, and Vision Mamba with a novel Neural Algorithmic Fusion mechanism, achieving 99.75% accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for accurate, interpretable, and generalizable medical image classification for clinical decision-making.

Method: Uses a multipath design with DenseNet, U-Net, and Vision Mamba modules, integrating features via cross-modal attention and Neural Algorithmic Fusion. Includes explainability tools like spatial attention and attention weights.

Result: Achieves 99.75% test accuracy on a 9-class multi-organ dataset, with interpretable decision-making insights.

Conclusion: EVM-Fusion shows strong potential for trustworthy AI in medical diagnostics due to high accuracy and explainability.

Abstract: Medical image classification is critical for clinical decision-making, yet
demands for accuracy, interpretability, and generalizability remain
challenging. This paper introduces EVM-Fusion, an Explainable Vision Mamba
architecture featuring a novel Neural Algorithmic Fusion (NAF) mechanism for
multi-organ medical image classification. EVM-Fusion leverages a multipath
design, where DenseNet and U-Net based pathways, enhanced by Vision Mamba (Vim)
modules, operate in parallel with a traditional feature pathway. These diverse
features are dynamically integrated via a two-stage fusion process: cross-modal
attention followed by the iterative NAF block, which learns an adaptive fusion
algorithm. Intrinsic explainability is embedded through path-specific spatial
attention, Vim {\Delta}-value maps, traditional feature SE-attention, and
cross-modal attention weights. Experiments on a diverse 9-class multi-organ
medical image dataset demonstrate EVM-Fusion's strong classification
performance, achieving 99.75% test accuracy and provide multi-faceted insights
into its decision-making process, highlighting its potential for trustworthy AI
in medical diagnostics.

</details>


### [417] [Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language Alignment and Modeling](https://arxiv.org/pdf/2505.17982)
*Bryan Wong, Jong Woo Kim, Huazhu Fu, Mun Yong Yi*

Main category: cs.CV

TL;DR: HiVE-MIL is a hierarchical vision-language framework for few-shot WSI classification, addressing limitations in multi-scale interaction and modality alignment, outperforming existing methods by up to 4.1% in macro F1.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack sufficient modeling of intra-modality interactions across scales and alignment between visual and textual modalities on the same scale.

Method: HiVE-MIL constructs a unified graph with parent-child links for hierarchical relationships and intra-scale edges for modality alignment, using text-guided dynamic filtering and hierarchical contrastive loss.

Result: Outperforms traditional and VLM-based MIL methods, achieving up to 4.1% higher macro F1 on TCGA datasets.

Conclusion: Jointly modeling hierarchical structure and multimodal alignment improves learning from limited pathology data.

Abstract: Vision-language models (VLMs) have recently been integrated into multiple
instance learning (MIL) frameworks to address the challenge of few-shot, weakly
supervised classification of whole slide images (WSIs). A key trend involves
leveraging multi-scale information to better represent hierarchical tissue
structures. However, existing methods often face two key limitations: (1)
insufficient modeling of interactions within the same modalities across scales
(e.g., 5x and 20x) and (2) inadequate alignment between visual and textual
modalities on the same scale. To address these gaps, we propose HiVE-MIL, a
hierarchical vision-language framework that constructs a unified graph
consisting of (1) parent-child links between coarse (5x) and fine (20x)
visual/textual nodes to capture hierarchical relationships, and (2)
heterogeneous intra-scale edges linking visual and textual nodes on the same
scale. To further enhance semantic consistency, HiVE-MIL incorporates a
two-stage, text-guided dynamic filtering mechanism that removes weakly
correlated patch-text pairs, and introduces a hierarchical contrastive loss to
align textual semantics across scales. Extensive experiments on TCGA breast,
lung, and kidney cancer datasets demonstrate that HiVE-MIL consistently
outperforms both traditional MIL and recent VLM-based MIL approaches, achieving
gains of up to 4.1% in macro F1 under 16-shot settings. Our results demonstrate
the value of jointly modeling hierarchical structure and multimodal alignment
for efficient and scalable learning from limited pathology data. The code is
available at https://github.com/bryanwong17/HiVE-MIL

</details>


### [418] [Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding](https://arxiv.org/pdf/2505.18079)
*Xiaoyi Zhang, Zhaoyang Jia, Zongyu Guo, Jiahao Li, Bin Li, Houqiang Li, Yan Lu*

Main category: cs.CV

TL;DR: The paper proposes the Deep Video Discovery (DVD) agent, an autonomous system leveraging LLMs and agentic search for long-form video understanding, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Long-form video understanding is challenging due to temporal-spatial complexity and extended contexts. Existing LLMs struggle with hour-long videos.

Method: The DVD agent uses an autonomous search strategy over segmented clips, leveraging LLM reasoning to plan, select tools, and iteratively refine actions.

Result: The DVD agent achieves SOTA performance on LVBench, significantly outperforming prior works. Ablation studies provide insights for future improvements.

Conclusion: The DVD agent demonstrates the effectiveness of autonomous agents for long-form video understanding, with potential for further advancements.

Abstract: Long-form video understanding presents significant challenges due to
extensive temporal-spatial complexity and the difficulty of question answering
under such extended contexts. While Large Language Models (LLMs) have
demonstrated considerable advancements in video analysis capabilities and long
context handling, they continue to exhibit limitations when processing
information-dense hour-long videos. To overcome such limitations, we propose
the Deep Video Discovery agent to leverage an agentic search strategy over
segmented video clips. Different from previous video agents manually designing
a rigid workflow, our approach emphasizes the autonomous nature of agents. By
providing a set of search-centric tools on multi-granular video database, our
DVD agent leverages the advanced reasoning capability of LLM to plan on its
current observation state, strategically selects tools, formulates appropriate
parameters for actions, and iteratively refines its internal reasoning in light
of the gathered information. We perform comprehensive evaluation on multiple
long video understanding benchmarks that demonstrates the advantage of the
entire system design. Our DVD agent achieves SOTA performance, significantly
surpassing prior works by a large margin on the challenging LVBench dataset.
Comprehensive ablation studies and in-depth tool analyses are also provided,
yielding insights to further advance intelligent agents tailored for long-form
video understanding tasks. The code will be released later.

</details>


### [419] [Sampling Strategies for Efficient Training of Deep Learning Object Detection Algorithms](https://arxiv.org/pdf/2505.18302)
*Gefei Shen, Yung-Hong Sun, Yu Hen Hu, Hongrui Jiang*

Main category: cs.CV

TL;DR: Two sampling strategies (uniform and frame difference) improve deep learning object detection training efficiency by leveraging Lipschitz continuity and temporal redundancy.


<details>
  <summary>Details</summary>
Motivation: Enhance training efficiency of deep learning object detection models by reducing the need for manually labeled samples.

Method: Investigated uniform sampling (evenly random state space sampling) and frame difference sampling (exploiting temporal redundancy in videos).

Result: The strategies yield datasets that achieve good training performance with fewer labeled samples.

Conclusion: Proposed sampling methods effectively improve training efficiency for object detection models.

Abstract: Two sampling strategies are investigated to enhance efficiency in training a
deep learning object detection model. These sampling strategies are employed
under the assumption of Lipschitz continuity of deep learning models. The first
strategy is uniform sampling which seeks to obtain samples evenly yet randomly
through the state space of the object dynamics. The second strategy of frame
difference sampling is developed to explore the temporal redundancy among
successive frames in a video. Experiment result indicates that these proposed
sampling strategies provide a dataset that yields good training performance
while requiring relatively few manually labelled samples.

</details>


### [420] [ZooplanktonBench: A Geo-Aware Zooplankton Recognition and Classification Dataset from Marine Observations](https://arxiv.org/pdf/2505.18477)
*Fukun Liu, Adam T. Greer, Gengchen Mai, Jin Sun*

Main category: cs.CV

TL;DR: The paper introduces ZooplanktonBench, a benchmark dataset for detecting, classifying, and tracking zooplankton in challenging ocean environments, aiming to improve computer vision tools for marine research.


<details>
  <summary>Details</summary>
Motivation: Zooplankton are key indicators of ocean health, but analyzing them with general computer vision tools is difficult due to their similarity to backgrounds like marine snow. Accurate monitoring is crucial for marine science and seafood productivity.

Method: The authors present ZooplanktonBench, a dataset with images and videos of zooplankton, enriched with geospatial metadata. It includes tasks like detection, classification, and tracking in cluttered environments.

Result: The dataset provides unique challenges for computer vision systems, such as handling small objects, similar shapes, and dynamic environments, fostering advancements in visual understanding.

Conclusion: ZooplanktonBench offers a valuable resource for improving computer vision in marine research, with potential impacts on understanding ocean ecosystems and seafood productivity.

Abstract: Plankton are small drifting organisms found throughout the world's oceans and
can be indicators of ocean health. One component of this plankton community is
the zooplankton, which includes gelatinous animals and crustaceans (e.g.
shrimp), as well as the early life stages (i.e., eggs and larvae) of many
commercially important fishes. Being able to monitor zooplankton abundances
accurately and understand how populations change in relation to ocean
conditions is invaluable to marine science research, with important
implications for future marine seafood productivity. While new imaging
technologies generate massive amounts of video data of zooplankton, analyzing
them using general-purpose computer vision tools turns out to be highly
challenging due to the high similarity in appearance between the zooplankton
and its background (e.g., marine snow). In this work, we present the
ZooplanktonBench, a benchmark dataset containing images and videos of
zooplankton associated with rich geospatial metadata (e.g., geographic
coordinates, depth, etc.) in various water ecosystems. ZooplanktonBench defines
a collection of tasks to detect, classify, and track zooplankton in challenging
settings, including highly cluttered environments, living vs non-living
classification, objects with similar shapes, and relatively small objects. Our
dataset presents unique challenges and opportunities for state-of-the-art
computer vision systems to evolve and improve visual understanding in dynamic
environments characterized by significant variation and the need for
geo-awareness. The code and settings described in this paper can be found on
our website: https://lfk118.github.io/ZooplanktonBench_Webpage.

</details>


### [421] [Eye-See-You: Reverse Pass-Through VR and Head Avatars](https://arxiv.org/pdf/2505.18869)
*Ankan Dash, Jingyi Gu, Guiling Wang, Chen Chen*

Main category: cs.CV

TL;DR: RevAvatar is an AI-driven framework that reconstructs facial images and creates 3D avatars from occluded VR headset views, enhancing social interaction in virtual environments.


<details>
  <summary>Details</summary>
Motivation: Current VR headsets block facial expressions, hindering communication and causing social isolation. RevAvatar aims to solve this by enabling reverse pass-through technology.

Method: The framework uses generative models and multimodal AI to reconstruct 2D facial images and generate 3D avatars from partial face observations. It also introduces VR-Face, a dataset of 200,000 samples for training.

Result: RevAvatar improves VR interaction by bridging virtual and physical environments, enabling immersive social experiences like VR meetings.

Conclusion: RevAvatar demonstrates the potential of AI to enhance human connection in VR, addressing key limitations of current systems.

Abstract: Virtual Reality (VR) headsets, while integral to the evolving digital
ecosystem, present a critical challenge: the occlusion of users' eyes and
portions of their faces, which hinders visual communication and may contribute
to social isolation. To address this, we introduce RevAvatar, an innovative
framework that leverages AI methodologies to enable reverse pass-through
technology, fundamentally transforming VR headset design and interaction
paradigms. RevAvatar integrates state-of-the-art generative models and
multimodal AI techniques to reconstruct high-fidelity 2D facial images and
generate accurate 3D head avatars from partially observed eye and lower-face
regions. This framework represents a significant advancement in AI4Tech by
enabling seamless interaction between virtual and physical environments,
fostering immersive experiences such as VR meetings and social engagements.
Additionally, we present VR-Face, a novel dataset comprising 200,000 samples
designed to emulate diverse VR-specific conditions, including occlusions,
lighting variations, and distortions. By addressing fundamental limitations in
current VR systems, RevAvatar exemplifies the transformative synergy between AI
and next-generation technologies, offering a robust platform for enhancing
human connection and interaction in virtual environments.

</details>


### [422] [CreatiDesign: A Unified Multi-Conditional Diffusion Transformer for Creative Graphic Design](https://arxiv.org/pdf/2505.19114)
*Hui Zhang, Dexiang Hong, Maoke Yang, Yutao Cheng, Zhao Zhang, Jie Shao, Xinglong Wu, Zuxuan Wu, Yu-Gang Jiang*

Main category: cs.CV

TL;DR: CreatiDesign introduces a unified multi-condition architecture and multimodal attention masks for precise graphic design generation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of prior single-condition and multi-condition models in handling complex graphic design scenarios with heterogeneous elements.

Method: Proposes a unified multi-condition driven architecture with multimodal attention masks and an automated dataset pipeline, including a new 400K-sample dataset.

Result: CreatiDesign outperforms existing models in adhering to user intent, demonstrating superior performance.

Conclusion: The solution provides flexible, precise control over graphic design generation, advancing automated design capabilities.

Abstract: Graphic design plays a vital role in visual communication across advertising,
marketing, and multimedia entertainment. Prior work has explored automated
graphic design generation using diffusion models, aiming to streamline creative
workflows and democratize design capabilities. However, complex graphic design
scenarios require accurately adhering to design intent specified by multiple
heterogeneous user-provided elements (\eg images, layouts, and texts), which
pose multi-condition control challenges for existing methods. Specifically,
previous single-condition control models demonstrate effectiveness only within
their specialized domains but fail to generalize to other conditions, while
existing multi-condition methods often lack fine-grained control over each
sub-condition and compromise overall compositional harmony. To address these
limitations, we introduce CreatiDesign, a systematic solution for automated
graphic design covering both model architecture and dataset construction.
First, we design a unified multi-condition driven architecture that enables
flexible and precise integration of heterogeneous design elements with minimal
architectural modifications to the base diffusion model. Furthermore, to ensure
that each condition precisely controls its designated image region and to avoid
interference between conditions, we propose a multimodal attention mask
mechanism. Additionally, we develop a fully automated pipeline for constructing
graphic design datasets, and introduce a new dataset with 400K samples
featuring multi-condition annotations, along with a comprehensive benchmark.
Experimental results show that CreatiDesign outperforms existing models by a
clear margin in faithfully adhering to user intent.

</details>


### [423] [EventEgoHands: Event-based Egocentric 3D Hand Mesh Reconstruction](https://arxiv.org/pdf/2505.19169)
*Ryosei Hara, Wataru Ikeda, Masashi Hatano, Mariko Isogawa*

Main category: cs.CV

TL;DR: EventEgoHands improves 3D hand mesh reconstruction using event cameras, addressing challenges like dynamic backgrounds and noise.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of RGB/depth cameras in low-light and motion blur by leveraging event cameras' high dynamic range and temporal resolution.

Method: Introduces a Hand Segmentation Module to isolate hand regions, reducing dynamic background noise.

Result: Achieved a 43% improvement (4.5 cm) in MPJPE on the N-HOT3D dataset.

Conclusion: EventEgoHands effectively addresses noise and dynamic background issues in event-based 3D hand reconstruction.

Abstract: Reconstructing 3D hand mesh is challenging but an important task for
human-computer interaction and AR/VR applications. In particular, RGB and/or
depth cameras have been widely used in this task. However, methods using these
conventional cameras face challenges in low-light environments and during
motion blur. Thus, to address these limitations, event cameras have been
attracting attention in recent years for their high dynamic range and high
temporal resolution. Despite their advantages, event cameras are sensitive to
background noise or camera motion, which has limited existing studies to static
backgrounds and fixed cameras. In this study, we propose EventEgoHands, a novel
method for event-based 3D hand mesh reconstruction in an egocentric view. Our
approach introduces a Hand Segmentation Module that extracts hand regions,
effectively mitigating the influence of dynamic background events. We evaluated
our approach and demonstrated its effectiveness on the N-HOT3D dataset,
improving MPJPE by approximately more than 4.5 cm (43%).

</details>


### [424] [MMIG-Bench: Towards Comprehensive and Explainable Evaluation of Multi-Modal Image Generation Models](https://arxiv.org/pdf/2505.19415)
*Hang Hua, Ziyun Zeng, Yizhi Song, Yunlong Tang, Liu He, Daniel Aliaga, Wei Xiong, Jiebo Luo*

Main category: cs.CV

TL;DR: MMIG-Bench is a new benchmark for evaluating multimodal image generators, unifying tasks with rich annotations and a three-level evaluation framework.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for multimodal image generators are disjoint, lacking comprehensive evaluation of multi-modal conditioning and compositional semantics.

Method: Proposes MMIG-Bench with 4,850 text prompts and 1,750 multi-view images, evaluated via low-level, mid-level (AMS), and high-level metrics.

Result: Benchmarked 17 models, validated metrics with 32k human ratings, providing insights into architecture and data design.

Conclusion: MMIG-Bench offers a unified and comprehensive evaluation framework for multimodal image generation.

Abstract: Recent multimodal image generators such as GPT-4o, Gemini 2.0 Flash, and
Gemini 2.5 Pro excel at following complex instructions, editing images and
maintaining concept consistency. However, they are still evaluated by disjoint
toolkits: text-to-image (T2I) benchmarks that lacks multi-modal conditioning,
and customized image generation benchmarks that overlook compositional
semantics and common knowledge. We propose MMIG-Bench, a comprehensive
Multi-Modal Image Generation Benchmark that unifies these tasks by pairing
4,850 richly annotated text prompts with 1,750 multi-view reference images
across 380 subjects, spanning humans, animals, objects, and artistic styles.
MMIG-Bench is equipped with a three-level evaluation framework: (1) low-level
metrics for visual artifacts and identity preservation of objects; (2) novel
Aspect Matching Score (AMS): a VQA-based mid-level metric that delivers
fine-grained prompt-image alignment and shows strong correlation with human
judgments; and (3) high-level metrics for aesthetics and human preference.
Using MMIG-Bench, we benchmark 17 state-of-the-art models, including Gemini 2.5
Pro, FLUX, DreamBooth, and IP-Adapter, and validate our metrics with 32k human
ratings, yielding in-depth insights into architecture and data design.

</details>


### [425] [VisCRA: A Visual Chain Reasoning Attack for Jailbreaking Multimodal Large Language Models](https://arxiv.org/pdf/2505.19684)
*Bingrui Sima, Linhua Cong, Wenxuan Wang, Kun He*

Main category: cs.CV

TL;DR: The paper investigates safety risks in Multimodal Large Language Models (MLRMs) due to enhanced visual reasoning, introducing VisCRA, a jailbreak attack exploiting reasoning chains, with high success rates on major models.


<details>
  <summary>Details</summary>
Motivation: Advanced visual reasoning in MLRMs improves performance but introduces underexplored safety risks, particularly vulnerability to jailbreak attacks.

Method: The authors introduce VisCRA, a framework combining visual attention masking and a two-stage reasoning induction strategy to bypass safety mechanisms.

Result: VisCRA achieves high attack success rates: 76.48% on Gemini 2.0, 68.56% on QvQ-Max, and 56.60% on GPT-4o.

Conclusion: Visual reasoning, a core strength of MLRMs, can also be exploited as an attack vector, revealing significant security risks.

Abstract: The emergence of Multimodal Large Language Models (MLRMs) has enabled
sophisticated visual reasoning capabilities by integrating reinforcement
learning and Chain-of-Thought (CoT) supervision. However, while these enhanced
reasoning capabilities improve performance, they also introduce new and
underexplored safety risks. In this work, we systematically investigate the
security implications of advanced visual reasoning in MLRMs. Our analysis
reveals a fundamental trade-off: as visual reasoning improves, models become
more vulnerable to jailbreak attacks. Motivated by this critical finding, we
introduce VisCRA (Visual Chain Reasoning Attack), a novel jailbreak framework
that exploits the visual reasoning chains to bypass safety mechanisms. VisCRA
combines targeted visual attention masking with a two-stage reasoning induction
strategy to precisely control harmful outputs. Extensive experiments
demonstrate VisCRA's significant effectiveness, achieving high attack success
rates on leading closed-source MLRMs: 76.48% on Gemini 2.0 Flash Thinking,
68.56% on QvQ-Max, and 56.60% on GPT-4o. Our findings highlight a critical
insight: the very capability that empowers MLRMs -- their visual reasoning --
can also serve as an attack vector, posing significant security risks.

</details>


### [426] [Progressive Scaling Visual Object Tracking](https://arxiv.org/pdf/2505.19990)
*Jack Hong, Shilin Yan, Zehao Xiao, Jiayin Cai, Xiaolong Jiang, Yao Hu, Henghui Ding*

Main category: cs.CV

TL;DR: A progressive scaling training strategy (DT-Training) improves visual object tracking by optimizing data volume, model size, and input resolution, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To address suboptimal optimization and limited refinement in naive training when scaling factors like data volume, model size, and input resolution for tracking performance.

Method: Introduces DT-Training, a progressive scaling framework with small teacher transfer and dual-branch alignment.

Result: The scaled tracker outperforms state-of-the-art methods, showing strong generalization and transferability.

Conclusion: The approach is versatile, applicable beyond tracking, and effectively maximizes model potential.

Abstract: In this work, we propose a progressive scaling training strategy for visual
object tracking, systematically analyzing the influence of training data
volume, model size, and input resolution on tracking performance. Our empirical
study reveals that while scaling each factor leads to significant improvements
in tracking accuracy, naive training suffers from suboptimal optimization and
limited iterative refinement. To address this issue, we introduce DT-Training,
a progressive scaling framework that integrates small teacher transfer and
dual-branch alignment to maximize model potential. The resulting scaled tracker
consistently outperforms state-of-the-art methods across multiple benchmarks,
demonstrating strong generalization and transferability of the proposed method.
Furthermore, we validate the broader applicability of our approach to
additional tasks, underscoring its versatility beyond tracking.

</details>


### [427] [OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation](https://arxiv.org/pdf/2505.20292)
*Shenghai Yuan, Xianyi He, Yufan Deng, Yang Ye, Jinfa Huang, Bin Lin, Jiebo Luo, Li Yuan*

Main category: cs.CV

TL;DR: The paper introduces OpenS2V-Nexus, a framework for Subject-to-Video (S2V) generation, including a benchmark (OpenS2V-Eval) and a dataset (OpenS2V-5M), to assess and improve subject-consistent video generation.


<details>
  <summary>Details</summary>
Motivation: Existing S2V benchmarks lack fine-grained assessment of subject consistency and naturalness. The paper aims to address this gap by providing a comprehensive infrastructure for S2V research.

Method: Proposes OpenS2V-Eval with 180 prompts and three automatic metrics (NexusScore, NaturalScore, GmeScore) for evaluation. Also introduces OpenS2V-5M, a 5M triples dataset, using subject segmentation and GPT-Image-1 for diversity.

Result: Evaluated 16 S2V models, revealing their strengths and weaknesses. The dataset and benchmark provide a robust foundation for future research.

Conclusion: OpenS2V-Nexus accelerates S2V generation research by offering a detailed benchmark and large-scale dataset, enhancing subject consistency and naturalness in video generation.

Abstract: Subject-to-Video (S2V) generation aims to create videos that faithfully
incorporate reference content, providing enhanced flexibility in the production
of videos. To establish the infrastructure for S2V generation, we propose
OpenS2V-Nexus, consisting of (i) OpenS2V-Eval, a fine-grained benchmark, and
(ii) OpenS2V-5M, a million-scale dataset. In contrast to existing S2V
benchmarks inherited from VBench that focus on global and coarse-grained
assessment of generated videos, OpenS2V-Eval focuses on the model's ability to
generate subject-consistent videos with natural subject appearance and identity
fidelity. For these purposes, OpenS2V-Eval introduces 180 prompts from seven
major categories of S2V, which incorporate both real and synthetic test data.
Furthermore, to accurately align human preferences with S2V benchmarks, we
propose three automatic metrics, NexusScore, NaturalScore and GmeScore, to
separately quantify subject consistency, naturalness, and text relevance in
generated videos. Building on this, we conduct a comprehensive evaluation of 16
representative S2V models, highlighting their strengths and weaknesses across
different content. Moreover, we create the first open-source large-scale S2V
generation dataset OpenS2V-5M, which consists of five million high-quality 720P
subject-text-video triples. Specifically, we ensure subject-information
diversity in our dataset by (1) segmenting subjects and building pairing
information via cross-video associations and (2) prompting GPT-Image-1 on raw
frames to synthesize multi-view representations. Through OpenS2V-Nexus, we
deliver a robust infrastructure to accelerate future S2V generation research.

</details>


### [428] [DIPO: Dual-State Images Controlled Articulated Object Generation Powered by Diverse Data](https://arxiv.org/pdf/2505.20460)
*Ruiqi Wu, Xinjie Wang, Liu Liu, Chunle Guo, Jiaxiong Qiu, Chongyi Li, Lichao Huang, Zhizhong Su, Ming-Ming Cheng*

Main category: cs.CV

TL;DR: DIPO is a framework for generating articulated 3D objects from dual-image inputs, outperforming baselines with improved motion and kinematic prediction.


<details>
  <summary>Details</summary>
Motivation: Existing single-image approaches lack motion information, limiting kinematic prediction accuracy. Dual-image inputs provide reliable motion guidance.

Method: Uses a dual-image diffusion model for part layouts and joint parameters, plus a CoT-based graph reasoner for part connectivity. Introduces LEGO-Art for dataset expansion and PM-X for diverse articulated objects.

Result: DIPO outperforms baselines in both resting and articulated states. PM-X enhances generalization to complex objects.

Conclusion: DIPO and PM-X advance articulated object generation, with plans to release code and dataset.

Abstract: We present DIPO, a novel framework for the controllable generation of
articulated 3D objects from a pair of images: one depicting the object in a
resting state and the other in an articulated state. Compared to the
single-image approach, our dual-image input imposes only a modest overhead for
data collection, but at the same time provides important motion information,
which is a reliable guide for predicting kinematic relationships between parts.
Specifically, we propose a dual-image diffusion model that captures
relationships between the image pair to generate part layouts and joint
parameters. In addition, we introduce a Chain-of-Thought (CoT) based graph
reasoner that explicitly infers part connectivity relationships. To further
improve robustness and generalization on complex articulated objects, we
develop a fully automated dataset expansion pipeline, name LEGO-Art, that
enriches the diversity and complexity of PartNet-Mobility dataset. We propose
PM-X, a large-scale dataset of complex articulated 3D objects, accompanied by
rendered images, URDF annotations, and textual descriptions. Extensive
experiments demonstrate that DIPO significantly outperforms existing baselines
in both the resting state and the articulated state, while the proposed PM-X
dataset further enhances generalization to diverse and structurally complex
articulated objects. Our code and dataset will be released to the community
upon publication.

</details>


### [429] [VLM Can Be a Good Assistant: Enhancing Embodied Visual Tracking with Self-Improving Vision-Language Models](https://arxiv.org/pdf/2505.20718)
*Kui Wu, Shuhang Xu, Hao Chen, Churan Wang, Zhoujun Li, Yizhou Wang, Fangwei Zhong*

Main category: cs.CV

TL;DR: A self-improving framework enhances Embodied Visual Tracking (EVT) using Vision-Language Models (VLMs) to recover from tracking failures, combining active tracking with VLM reasoning and memory-augmented self-reflection.


<details>
  <summary>Details</summary>
Motivation: Address limitations of current active visual tracking systems in recovering from tracking failures, especially in dynamic, unstructured environments.

Method: Integrates off-the-shelf active tracking with VLM reasoning, using a fast visual policy for normal tracking and activating VLM upon failure detection. Includes a memory-augmented self-reflection mechanism.

Result: Boosts success rates by 72% with RL-based methods and 220% with PID-based methods in challenging environments.

Conclusion: First integration of VLM-based reasoning for proactive failure recovery in EVT, advancing real-world robotic applications.

Abstract: We introduce a novel self-improving framework that enhances Embodied Visual
Tracking (EVT) with Vision-Language Models (VLMs) to address the limitations of
current active visual tracking systems in recovering from tracking failure. Our
approach combines the off-the-shelf active tracking methods with VLMs'
reasoning capabilities, deploying a fast visual policy for normal tracking and
activating VLM reasoning only upon failure detection. The framework features a
memory-augmented self-reflection mechanism that enables the VLM to
progressively improve by learning from past experiences, effectively addressing
VLMs' limitations in 3D spatial reasoning. Experimental results demonstrate
significant performance improvements, with our framework boosting success rates
by $72\%$ with state-of-the-art RL-based approaches and $220\%$ with PID-based
methods in challenging environments. This work represents the first integration
of VLM-based reasoning to assist EVT agents in proactive failure recovery,
offering substantial advances for real-world robotic applications that require
continuous target monitoring in dynamic, unstructured environments. Project
website: https://sites.google.com/view/evt-recovery-assistant.

</details>


### [430] [ControlTac: Force- and Position-Controlled Tactile Data Augmentation with a Single Reference Image](https://arxiv.org/pdf/2505.20498)
*Dongyu Luo, Kelin Yu, Amir-Hossein Shahidzadeh, Cornelia Fermüller, Yiannis Aloimonos, Ruohan Gao*

Main category: cs.CV

TL;DR: ControlTac is a two-stage framework for generating realistic tactile images using physical priors, improving data augmentation for tactile datasets.


<details>
  <summary>Details</summary>
Motivation: Large-scale tactile data collection is costly due to localized sensor-object interactions and inconsistencies. Existing methods like simulation and free-form generation often produce unrealistic results with poor transferability.

Method: ControlTac uses a two-stage controllable framework, generating tactile images conditioned on a reference image, contact force, and position.

Result: ControlTac produces physically plausible and varied tactile images, enhancing data augmentation. Experiments show consistent gains in downstream tasks.

Conclusion: ControlTac effectively augments tactile datasets and demonstrates practical utility in real-world applications.

Abstract: Vision-based tactile sensing has been widely used in perception,
reconstruction, and robotic manipulation. However, collecting large-scale
tactile data remains costly due to the localized nature of sensor-object
interactions and inconsistencies across sensor instances. Existing approaches
to scaling tactile data, such as simulation and free-form tactile generation,
often suffer from unrealistic output and poor transferability to downstream
tasks. To address this, we propose ControlTac, a two-stage controllable
framework that generates realistic tactile images conditioned on a single
reference tactile image, contact force, and contact position. With those
physical priors as control input, ControlTac generates physically plausible and
varied tactile images that can be used for effective data augmentation. Through
experiments on three downstream tasks, we demonstrate that ControlTac can
effectively augment tactile datasets and lead to consistent gains. Our three
real-world experiments further validate the practical utility of our approach.
Project page: https://dongyuluo.github.io/controltac.

</details>


### [431] [Causality and "In-the-Wild" Video-Based Person Re-ID: A Survey](https://arxiv.org/pdf/2505.20540)
*Md Rashidunnabi, Kailash Hambarde, Hugo Proença*

Main category: cs.CV

TL;DR: This survey explores causal reasoning as a robust alternative to correlation-based methods in video-based person re-identification (Re-ID), addressing generalization issues.


<details>
  <summary>Details</summary>
Motivation: Traditional Re-ID models rely on superficial correlations (e.g., clothing, lighting) that fail in real-world scenarios. Causal reasoning offers a principled solution to isolate identity-specific features.

Method: The survey analyzes methods using structural causal models, interventions, and counterfactual reasoning, organized into a taxonomy of generative disentanglement, domain-invariant modeling, and causal transformers.

Result: It reviews evaluation metrics, introduces causal-specific robustness measures, and assesses practical challenges like scalability, fairness, and privacy.

Conclusion: The survey aims to establish a foundation for causal Re-ID and guide future research, integrating causal modeling with efficient architectures and self-supervised learning.

Abstract: Video-based person re-identification (Re-ID) remains brittle in real-world
deployments despite impressive benchmark performance. Most existing models rely
on superficial correlations such as clothing, background, or lighting that fail
to generalize across domains, viewpoints, and temporal variations. This survey
examines the emerging role of causal reasoning as a principled alternative to
traditional correlation-based approaches in video-based Re-ID. We provide a
structured and critical analysis of methods that leverage structural causal
models, interventions, and counterfactual reasoning to isolate
identity-specific features from confounding factors. The survey is organized
around a novel taxonomy of causal Re-ID methods that spans generative
disentanglement, domain-invariant modeling, and causal transformers. We review
current evaluation metrics and introduce causal-specific robustness measures.
In addition, we assess practical challenges of scalability, fairness,
interpretability, and privacy that must be addressed for real-world adoption.
Finally, we identify open problems and outline future research directions that
integrate causal modeling with efficient architectures and self-supervised
learning. This survey aims to establish a coherent foundation for causal
video-based person Re-ID and to catalyze the next phase of research in this
rapidly evolving domain.

</details>


### [432] [See through the Dark: Learning Illumination-affined Representations for Nighttime Occupancy Prediction](https://arxiv.org/pdf/2505.20641)
*Yuan Wu, Zhiqiang Yan, Yigong Zhang, Xiang Li, Jian Yang*

Main category: cs.CV

TL;DR: LIAR is a framework for nighttime occupancy prediction, using illumination-aware techniques like SLLIE, 2D-IGS, and 3D-IDP to improve performance in low-light conditions.


<details>
  <summary>Details</summary>
Motivation: Existing vision-based methods fail in nighttime scenarios due to poor visibility and lighting. LIAR addresses this by leveraging illumination priors.

Method: LIAR uses SLLIE for adaptive image enhancement, 2D-IGS for local underexposure, and 3D-IDP for overexposure handling.

Result: LIAR outperforms existing methods on real and synthetic nighttime datasets.

Conclusion: LIAR effectively tackles nighttime occupancy prediction challenges, with code and models publicly available.

Abstract: Occupancy prediction aims to estimate the 3D spatial distribution of occupied
regions along with their corresponding semantic labels. Existing vision-based
methods perform well on daytime benchmarks but struggle in nighttime scenarios
due to limited visibility and challenging lighting conditions. To address these
challenges, we propose \textbf{LIAR}, a novel framework that learns
illumination-affined representations. LIAR first introduces Selective Low-light
Image Enhancement (SLLIE), which leverages the illumination priors from daytime
scenes to adaptively determine whether a nighttime image is genuinely dark or
sufficiently well-lit, enabling more targeted global enhancement. Building on
the illumination maps generated by SLLIE, LIAR further incorporates two
illumination-aware components: 2D Illumination-guided Sampling (2D-IGS) and 3D
Illumination-driven Projection (3D-IDP), to respectively tackle local
underexposure and overexposure. Specifically, 2D-IGS modulates feature sampling
positions according to illumination maps, assigning larger offsets to darker
regions and smaller ones to brighter regions, thereby alleviating feature
degradation in underexposed areas. Subsequently, 3D-IDP enhances semantic
understanding in overexposed regions by constructing illumination intensity
fields and supplying refined residual queries to the BEV context refinement
process. Extensive experiments on both real and synthetic datasets demonstrate
the superior performance of LIAR under challenging nighttime scenarios. The
source code and pretrained models are available
\href{https://github.com/yanzq95/LIAR}{here}.

</details>


### [433] [Integrating Intermediate Layer Optimization and Projected Gradient Descent for Solving Inverse Problems with Diffusion Models](https://arxiv.org/pdf/2505.20789)
*Yang Zheng, Wen Li, Zhaoqiang Liu*

Main category: cs.CV

TL;DR: Proposes DMILO and DMILO-PGD to improve diffusion model-based inverse problem solvers by reducing computational demands and enhancing convergence.


<details>
  <summary>Details</summary>
Motivation: Addresses issues like heavy computational demands and suboptimal convergence in existing diffusion model-based methods for inverse problems.

Method: Introduces DMILO with intermediate layer optimization (ILO) and sparse deviations, and DMILO-PGD combining ILO with projected gradient descent (PGD).

Result: Demonstrates significant performance gains over state-of-the-art methods on diverse image datasets for linear and nonlinear inverse problems.

Conclusion: DMILO and DMILO-PGD effectively address common challenges in diffusion model-based inverse problem solvers, offering superior performance.

Abstract: Inverse problems (IPs) involve reconstructing signals from noisy
observations. Recently, diffusion models (DMs) have emerged as a powerful
framework for solving IPs, achieving remarkable reconstruction performance.
However, existing DM-based methods frequently encounter issues such as heavy
computational demands and suboptimal convergence. In this work, building upon
the idea of the recent work DMPlug, we propose two novel methods, DMILO and
DMILO-PGD, to address these challenges. Our first method, DMILO, employs
intermediate layer optimization (ILO) to alleviate the memory burden inherent
in DMPlug. Additionally, by introducing sparse deviations, we expand the range
of DMs, enabling the exploration of underlying signals that may lie outside the
range of the diffusion model. We further propose DMILO-PGD, which integrates
ILO with projected gradient descent (PGD), thereby reducing the risk of
suboptimal convergence. We provide an intuitive theoretical analysis of our
approaches under appropriate conditions and validate their superiority through
extensive experiments on diverse image datasets, encompassing both linear and
nonlinear IPs. Our results demonstrate significant performance gains over
state-of-the-art methods, highlighting the effectiveness of DMILO and DMILO-PGD
in addressing common challenges in DM-based IP solvers.

</details>


### [434] [HTMNet: A Hybrid Network with Transformer-Mamba Bottleneck Multimodal Fusion for Transparent and Reflective Objects Depth Completion](https://arxiv.org/pdf/2505.20904)
*Guanghu Xie, Yonglong Zhang, Zhiduo Jiang, Yang Liu, Zongwu Xie, Baoshi Cao, Hong Liu*

Main category: cs.CV

TL;DR: HTMNet, a hybrid model combining Transformer, CNN, and Mamba architectures, addresses depth completion challenges for transparent and reflective objects, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Transparent and reflective objects cause incomplete depth data, hindering robotic tasks. HTMNet aims to solve this.

Method: Uses a dual-branch CNN-Transformer encoder, Transformer-Mamba bottleneck, and multi-scale fusion decoder with attention mechanisms.

Result: Achieves state-of-the-art performance on public datasets.

Conclusion: HTMNet effectively addresses depth completion for transparent objects, showcasing Mamba's potential in this field.

Abstract: Transparent and reflective objects pose significant challenges for depth
sensors, resulting in incomplete depth information that adversely affects
downstream robotic perception and manipulation tasks. To address this issue, we
propose HTMNet, a novel hybrid model integrating Transformer, CNN, and Mamba
architectures. The encoder is based on a dual-branch CNN-Transformer framework,
the bottleneck fusion module adopts a Transformer-Mamba architecture, and the
decoder is built upon a multi-scale fusion module. We introduce a novel
multimodal fusion module grounded in self-attention mechanisms and state space
models, marking the first application of the Mamba architecture in the field of
transparent object depth completion and revealing its promising potential.
Additionally, we design an innovative multi-scale fusion module for the decoder
that combines channel attention, spatial attention, and multi-scale feature
extraction techniques to effectively integrate multi-scale features through a
down-fusion strategy. Extensive evaluations on multiple public datasets
demonstrate that our model achieves state-of-the-art(SOTA) performance,
validating the effectiveness of our approach.

</details>


### [435] [Advancing high-fidelity 3D and Texture Generation with 2.5D latents](https://arxiv.org/pdf/2505.21050)
*Xin Yang, Jiantao Lin, Yingjie Xu, Haodong Li, Yingcong Chen*

Main category: cs.CV

TL;DR: A novel framework for joint 3D geometry and texture generation using unified 2.5D representations, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of coherence between separately generated 3D geometry and texture by proposing a unified approach.

Method: Integrates multiview RGB, normal, and coordinate images into 2.5D latents, adapts pre-trained 2D models, and refines to 3D.

Result: Generates high-quality 3D objects with coherent structure and color, outperforming existing methods.

Conclusion: The proposed framework effectively unifies 3D generation, improving coherence and quality.

Abstract: Despite the availability of large-scale 3D datasets and advancements in 3D
generative models, the complexity and uneven quality of 3D geometry and texture
data continue to hinder the performance of 3D generation techniques. In most
existing approaches, 3D geometry and texture are generated in separate stages
using different models and non-unified representations, frequently leading to
unsatisfactory coherence between geometry and texture. To address these
challenges, we propose a novel framework for joint generation of 3D geometry
and texture. Specifically, we focus in generate a versatile 2.5D
representations that can be seamlessly transformed between 2D and 3D. Our
approach begins by integrating multiview RGB, normal, and coordinate images
into a unified representation, termed as 2.5D latents. Next, we adapt
pre-trained 2D foundation models for high-fidelity 2.5D generation, utilizing
both text and image conditions. Finally, we introduce a lightweight 2.5D-to-3D
refiner-decoder framework that efficiently generates detailed 3D
representations from 2.5D images. Extensive experiments demonstrate that our
model not only excels in generating high-quality 3D objects with coherent
structure and color from text and image inputs but also significantly
outperforms existing methods in geometry-conditioned texture generation.

</details>


### [436] [MagicTryOn: Harnessing Diffusion Transformer for Garment-Preserving Video Virtual Try-on](https://arxiv.org/pdf/2505.21325)
*Guangyuan Li, Siming Zheng, Hao Zhang, Jinwei Chen, Junsheng Luan, Binkai Ou, Lei Zhao, Bo Li, Peng-Tao Jiang*

Main category: cs.CV

TL;DR: MagicTryOn improves video virtual try-on by replacing U-Net with a diffusion Transformer and combining full self-attention for better spatiotemporal consistency and garment detail preservation.


<details>
  <summary>Details</summary>
Motivation: Current VVT methods lack spatiotemporal consistency and struggle with garment detail preservation due to U-Net limitations and separative modeling of spatial and temporal attention.

Method: Proposes MagicTryOn, using a diffusion Transformer and full self-attention for joint spatiotemporal modeling, along with a coarse-to-fine garment preservation strategy and mask-aware loss.

Result: Outperforms SOTA methods in evaluations and generalizes to in-the-wild scenarios.

Conclusion: MagicTryOn addresses key VVT challenges, enhancing realism and stability in synthesized results.

Abstract: Video Virtual Try-On (VVT) aims to simulate the natural appearance of
garments across consecutive video frames, capturing their dynamic variations
and interactions with human body motion. However, current VVT methods still
face challenges in terms of spatiotemporal consistency and garment content
preservation. First, they use diffusion models based on the U-Net, which are
limited in their expressive capability and struggle to reconstruct complex
details. Second, they adopt a separative modeling approach for spatial and
temporal attention, which hinders the effective capture of structural
relationships and dynamic consistency across frames. Third, their expression of
garment details remains insufficient, affecting the realism and stability of
the overall synthesized results, especially during human motion. To address the
above challenges, we propose MagicTryOn, a video virtual try-on framework built
upon the large-scale video diffusion Transformer. We replace the U-Net
architecture with a diffusion Transformer and combine full self-attention to
jointly model the spatiotemporal consistency of videos. We design a
coarse-to-fine garment preservation strategy. The coarse strategy integrates
garment tokens during the embedding stage, while the fine strategy incorporates
multiple garment-based conditions, such as semantics, textures, and contour
lines during the denoising stage. Moreover, we introduce a mask-aware loss to
further optimize garment region fidelity. Extensive experiments on both image
and video try-on datasets demonstrate that our method outperforms existing SOTA
methods in comprehensive evaluations and generalizes to in-the-wild scenarios.

</details>


### [437] [HoliTom: Holistic Token Merging for Fast Video Large Language Models](https://arxiv.org/pdf/2505.21334)
*Kele Shao, Keda Tao, Can Qin, Haoxuan You, Yang Sui, Huan Wang*

Main category: cs.CV

TL;DR: HoliTom is a training-free holistic token merging framework for video LLMs, combining outer-LLM pruning and inner-LLM merging to reduce redundancy, achieving significant computational savings while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing token pruning methods for video LLMs are inefficient, either incurring overhead (inner-LLM) or neglecting global temporal dynamics (outer-LLM). HoliTom addresses these gaps by combining both strategies.

Method: HoliTom uses outer-LLM pruning via global redundancy-aware temporal segmentation and spatial-temporal merging, followed by inner-LLM token similarity-based merging.

Result: Reduces computational costs to 6.9% of FLOPs while maintaining 99.1% performance, with 2.28x TTFT reduction and 1.32x decoding throughput acceleration.

Conclusion: HoliTom's integrated pruning approach offers a promising efficiency-performance trade-off for efficient video LLM inference.

Abstract: Video large language models (video LLMs) excel at video comprehension but
face significant computational inefficiency due to redundant video tokens.
Existing token pruning methods offer solutions. However, approaches operating
within the LLM (inner-LLM pruning), such as FastV, incur intrinsic
computational overhead in shallow layers. In contrast, methods performing token
pruning before the LLM (outer-LLM pruning) primarily address spatial redundancy
within individual frames or limited temporal windows, neglecting the crucial
global temporal dynamics and correlations across longer video sequences. This
leads to sub-optimal spatio-temporal reduction and does not leverage video
compressibility fully. Crucially, the synergistic potential and mutual
influence of combining these strategies remain unexplored. To further reduce
redundancy, we introduce HoliTom, a novel training-free holistic token merging
framework. HoliTom employs outer-LLM pruning through global redundancy-aware
temporal segmentation, followed by spatial-temporal merging to reduce visual
tokens by over 90%, significantly alleviating the LLM's computational burden.
Complementing this, we introduce a robust inner-LLM token similarity-based
merging approach, designed for superior performance and compatibility with
outer-LLM pruning. Evaluations demonstrate our method's promising
efficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational
costs to 6.9% of FLOPs while maintaining 99.1% of the original performance.
Furthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a
1.32x acceleration in decoding throughput, highlighting the practical benefits
of our integrated pruning approach for efficient video LLMs inference.

</details>


### [438] [AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Crop Mapping](https://arxiv.org/pdf/2505.21357)
*Wenyuan Li, Shunlin Liang, Keyan Chen, Yongzhe Chen, Han Ma, Jianglei Xu, Yichuan Ma, Shikang Guan, Husheng Fang, Zhenwei Shi*

Main category: cs.CV

TL;DR: AgriFM is a transformer-based remote sensing foundation model designed for crop mapping, addressing multi-scale spatiotemporal patterns by synchronizing temporal and spatial processing. It outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Current remote sensing foundation models (RSFMs) are suboptimal for crop mapping due to fixed spatiotemporal windows or lack of temporal information. AgriFM aims to bridge these gaps.

Method: AgriFM uses a modified Video Swin Transformer architecture for hierarchical spatiotemporal feature extraction, pre-trained on global satellite data (MODIS, Landsat-8/9, Sentinel-2).

Result: AgriFM outperforms conventional deep learning and state-of-the-art RSFMs in downstream tasks.

Conclusion: AgriFM provides a superior solution for crop mapping by effectively modeling multi-scale spatiotemporal patterns.

Abstract: Accurate crop mapping fundamentally relies on modeling multi-scale
spatiotemporal patterns, where spatial scales range from individual field
textures to landscape-level context, and temporal scales capture both
short-term phenological transitions and full growing-season dynamics.
Transformer-based remote sensing foundation models (RSFMs) offer promising
potential for crop mapping due to their innate ability for unified
spatiotemporal processing. However, current RSFMs remain suboptimal for crop
mapping: they either employ fixed spatiotemporal windows that ignore the
multi-scale nature of crop systems or completely disregard temporal information
by focusing solely on spatial patterns. To bridge these gaps, we present
AgriFM, a multi-source remote sensing foundation model specifically designed
for agricultural crop mapping. Our approach begins by establishing the
necessity of simultaneous hierarchical spatiotemporal feature extraction,
leading to the development of a modified Video Swin Transformer architecture
where temporal down-sampling is synchronized with spatial scaling operations.
This modified backbone enables efficient unified processing of long time-series
satellite inputs. AgriFM leverages temporally rich data streams from three
satellite sources including MODIS, Landsat-8/9 and Sentinel-2, and is
pre-trained on a global representative dataset comprising over 25 million image
samples supervised by land cover products. The resulting framework incorporates
a versatile decoder architecture that dynamically fuses these learned
spatiotemporal representations, supporting diverse downstream tasks.
Comprehensive evaluations demonstrate AgriFM's superior performance over
conventional deep learning approaches and state-of-the-art general-purpose
RSFMs across all downstream tasks. Codes will be available at
https://github.com/flyakon/AgriFM.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [439] [Understanding the learned look-ahead behavior of chess neural networks](https://arxiv.org/pdf/2505.21552)
*Diogo Cruz*

Main category: cs.AI

TL;DR: The paper examines the look-ahead abilities of chess-playing neural networks, particularly Leela Chess Zero, showing context-dependent behavior and the ability to consider moves up to seven steps ahead.


<details>
  <summary>Details</summary>
Motivation: To understand how neural networks trained for strategic tasks like chess can exhibit sophisticated look-ahead capabilities and reasoning.

Method: Analysis of the Leela Chess Zero policy network, building on prior work, to study its ability to process future moves and alternative sequences.

Result: The network's look-ahead is context-dependent, can process up to seven moves ahead, and considers multiple move sequences, not just one.

Conclusion: The findings provide insights into AI reasoning in complex domains and highlight the effectiveness of interpretability techniques in uncovering cognitive-like processes in AI.

Abstract: We investigate the look-ahead capabilities of chess-playing neural networks,
specifically focusing on the Leela Chess Zero policy network. We build on the
work of Jenner et al. (2024) by analyzing the model's ability to consider
future moves and alternative sequences beyond the immediate next move. Our
findings reveal that the network's look-ahead behavior is highly
context-dependent, varying significantly based on the specific chess position.
We demonstrate that the model can process information about board states up to
seven moves ahead, utilizing similar internal mechanisms across different
future time steps. Additionally, we provide evidence that the network considers
multiple possible move sequences rather than focusing on a single line of play.
These results offer new insights into the emergence of sophisticated look-ahead
capabilities in neural networks trained on strategic tasks, contributing to our
understanding of AI reasoning in complex domains. Our work also showcases the
effectiveness of interpretability techniques in uncovering cognitive-like
processes in artificial intelligence systems.

</details>


### [440] [R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning](https://arxiv.org/pdf/2505.21668)
*Yongchao Chen, Yueying Liu, Junwei Zhou, Yilun Hao, Jingquan Wang, Yang Zhang, Chuchu Fan*

Main category: cs.AI

TL;DR: R1-Code-Interpreter enhances LLMs by integrating code generation during reasoning, improving accuracy on diverse tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with precise computation and symbolic tasks, needing better alignment between textual reasoning and code execution.

Method: Multi-turn supervised fine-tuning (SFT) and reinforcement learning (RL) on Qwen-2.5 models, exploring various strategies and formats.

Result: R1-CI-14B improves test task accuracy from 44.0% to 64.1%, outperforming GPT-4o (text-only) and nearing GPT-4o with Code Interpreter.

Conclusion: Code Interpreter training is challenging but effective, with SFT playing a critical role. The model shows emergent self-checking behavior.

Abstract: Despite advances in reasoning and planning of R1-like models, Large Language
Models (LLMs) still struggle with tasks requiring precise computation, symbolic
manipulation, optimization, and algorithmic reasoning, in which textual
reasoning lacks the rigor of code execution. A key challenge is enabling LLMs
to decide when to use textual reasoning versus code generation. While OpenAI
trains models to invoke a Code Interpreter as needed, public research lacks
guidance on aligning pre-trained LLMs to effectively leverage code and
generalize across diverse tasks. We present R1-Code-Interpreter, an extension
of a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and
reinforcement learning (RL) to autonomously generate multiple code queries
during step-by-step reasoning. We curate 144 reasoning and planning tasks (107
for training, 37 for testing), each with over 200 diverse questions. We
fine-tune Qwen-2.5 models (3B/7B/14B) using various SFT and RL strategies,
investigating different answer formats, reasoning vs. non-reasoning models,
cold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs.
Unlike prior RL work on narrow domains, we find that Code Interpreter training
is significantly harder due to high task diversity and expensive code
execution, highlighting the critical role of the SFT stage. Our final model,
R1-CI-14B, improves average accuracy on the 37 test tasks from 44.0\% to
64.1\%, outperforming GPT-4o (text-only: 58.6\%) and approaching GPT-4o with
Code Interpreter (70.9\%), with the emergent self-checking behavior via code
generation. Datasets, Codes, and Models are available at
https://github.com/yongchao98/R1-Code-Interpreter and
https://huggingface.co/yongchao98.

</details>


### [441] [Adaptive Frontier Exploration on Graphs with Applications to Network-Based Disease Testing](https://arxiv.org/pdf/2505.21671)
*Davin Choo, Yuqi Pan, Tonghan Wang, Milind Tambe, Alastair van Heerden, Cheryl Johnson*

Main category: cs.AI

TL;DR: The paper addresses a sequential decision-making problem on graphs with unknown node labels, aiming to maximize rewards under a frontier exploration constraint. A Gittins index-based policy is proposed, proven optimal for forests, and shown to outperform baselines in experiments.


<details>
  <summary>Details</summary>
Motivation: The problem arises in practical scenarios like contact tracing and robotic exploration, where actions are constrained to neighbors of explored nodes. The goal is to efficiently maximize rewards while adapting to unknown node labels.

Method: A Gittins index-based policy is designed for general graphs, with optimality proven for forests. The implementation runs efficiently in polynomial time and space, leveraging oracle calls to the joint distribution.

Result: The method outperforms baselines in synthetic and real-world graphs, including non-tree and budget-limited settings. In HIV testing simulations, it detects most positives with half the population tested.

Conclusion: The proposed policy is effective and scalable, demonstrating strong performance in practical applications, particularly in constrained exploration scenarios.

Abstract: We study a sequential decision-making problem on a $n$-node graph $G$ where
each node has an unknown label from a finite set $\mathbf{\Sigma}$, drawn from
a joint distribution $P$ that is Markov with respect to $G$. At each step,
selecting a node reveals its label and yields a label-dependent reward. The
goal is to adaptively choose nodes to maximize expected accumulated discounted
rewards. We impose a frontier exploration constraint, where actions are limited
to neighbors of previously selected nodes, reflecting practical constraints in
settings such as contact tracing and robotic exploration. We design a Gittins
index-based policy that applies to general graphs and is provably optimal when
$G$ is a forest. Our implementation runs in $O(n^2 \cdot |\mathbf{\Sigma}|^2)$
time while using $O(n \cdot |\mathbf{\Sigma}|^2)$ oracle calls to $P$ and
$O(n^2 \cdot |\mathbf{\Sigma}|)$ space. Experiments on synthetic and real-world
graphs show that our method consistently outperforms natural baselines,
including in non-tree, budget-limited, and undiscounted settings. For example,
in HIV testing simulations on real-world sexual interaction networks, our
policy detects nearly all positive cases with only half the population tested,
substantially outperforming other baselines.

</details>


### [442] [Make Planning Research Rigorous Again!](https://arxiv.org/pdf/2505.21674)
*Michael Katz, Harsha Kokel, Christian Muise, Shirin Sohrabi, Sarath Sreedharan*

Main category: cs.AI

TL;DR: The paper advocates for applying the rigor of traditional automated planning to LLM-based planners, leveraging historical insights to avoid pitfalls and accelerate progress.


<details>
  <summary>Details</summary>
Motivation: To address the current trend of LLM-based planning by incorporating proven practices from the automated planning community to avoid repeated mistakes.

Method: Proposes integrating insights, tools, and data from automated planning into the design and evaluation of LLM-based planners.

Result: Highlights the potential to accelerate LLM-based planner development by avoiding known pitfalls.

Conclusion: Applying historical planning expertise can significantly advance LLM-based planners and the broader planning field.

Abstract: In over sixty years since its inception, the field of planning has made
significant contributions to both the theory and practice of building planning
software that can solve a never-before-seen planning problem. This was done
through established practices of rigorous design and evaluation of planning
systems. It is our position that this rigor should be applied to the current
trend of work on planning with large language models. One way to do so is by
correctly incorporating the insights, tools, and data from the automated
planning community into the design and evaluation of LLM-based planners. The
experience and expertise of the planning community are not just important from
a historical perspective; the lessons learned could play a crucial role in
accelerating the development of LLM-based planners. This position is
particularly important in light of the abundance of recent works that replicate
and propagate the same pitfalls that the planning community has encountered and
learned from. We believe that avoiding such known pitfalls will contribute
greatly to the progress in building LLM-based planners and to planning in
general.

</details>


### [443] [Don't Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models](https://arxiv.org/pdf/2505.21765)
*Sohyun An, Ruochen Wang, Tianyi Zhou, Cho-Jui Hsieh*

Main category: cs.AI

TL;DR: The paper addresses inefficiencies in large reasoning models (LRMs) due to overthinking, proposing a dynamic optimization framework to improve reasoning efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: LRMs often produce unnecessarily complex reasoning paths (overthinking), wasting computation and degrading performance. The goal is to enhance efficiency by optimizing reasoning paths.

Method: A dynamic optimization framework segments reasoning paths into thinking patterns, promoting beneficial ones and removing detrimental ones. Preference optimization is applied using a pairwise dataset.

Result: The method reduces attention FLOPs by 47%, maintains accuracy for correct responses, and improves accuracy by 15.6% for incorrect ones. Token usage drops from ~5,000 to ~3,000.

Conclusion: The framework successfully enhances reasoning efficiency and accuracy, demonstrating significant improvements in computational overhead and performance.

Abstract: While recent success of large reasoning models (LRMs) significantly advanced
LLMs' reasoning capability by optimizing the final answer accuracy using
reinforcement learning, they may also drastically increase the output length
due to overthinking, characterized by unnecessarily complex reasoning paths
that waste computation and potentially degrade the performance. We hypothesize
that such inefficiencies stem from LRMs' limited capability to dynamically
select the proper modular reasoning strategies, termed thinking patterns at the
right position. To investigate this hypothesis, we propose a dynamic
optimization framework that segments model-generated reasoning paths into
distinct thinking patterns, systematically identifying and promoting beneficial
patterns that improve the answer while removing detrimental ones. Empirical
analysis confirms that our optimized thinking paths yield more concise yet
sufficiently informative trajectories, enhancing reasoning efficiency by
reducing attention FLOPs by up to 47% while maintaining accuracy for originally
correct responses. Moreover, a non-trivial portion of originally incorrect
responses are transformed into correct ones, achieving a 15.6% accuracy
improvement with reduced length. Motivated by the improvement brought by the
optimized thinking paths, we apply a preference optimization technique
supported by a pairwise dataset contrasting suboptimal and optimal reasoning
paths. Experimental evaluations across multiple mathematical reasoning
benchmarks reveal that our method notably reduces computational overhead while
simultaneously improving reasoning accuracy, achieving up to a 12% accuracy
improvement and reducing token usage from approximately 5,000 to 3,000 tokens.

</details>


### [444] [Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation](https://arxiv.org/pdf/2505.21784)
*Tharindu Kumarage, Ninareh Mehrabi, Anil Ramakrishna, Xinyan Zhao, Richard Zemel, Kai-Wei Chang, Aram Galstyan, Rahul Gupta, Charith Peris*

Main category: cs.AI

TL;DR: AIDSAFE is a method for generating high-quality safety reasoning datasets using multi-agent deliberation, improving LLM safety training.


<details>
  <summary>Details</summary>
Motivation: Existing safety measures for LLMs suffer from over-refusal and jailbreak vulnerabilities, requiring better reasoning over safety policies.

Method: AIDSAFE uses multi-agent deliberation to iteratively expand safety policy reasoning, with a data refiner to ensure quality. It also includes a supplemental recipe for preference data.

Result: AIDSAFE-generated CoTs show superior policy adherence and reasoning quality, enhancing safety generalization and jailbreak robustness in LLMs.

Conclusion: AIDSAFE provides an effective solution for improving LLM safety training through high-quality reasoning datasets.

Abstract: Safety reasoning is a recent paradigm where LLMs reason over safety policies
before generating responses, thereby mitigating limitations in existing safety
measures such as over-refusal and jailbreak vulnerabilities. However,
implementing this paradigm is challenging due to the resource-intensive process
of creating high-quality policy-embedded chain-of-thought (CoT) datasets while
ensuring reasoning remains accurate and free from hallucinations or policy
conflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation
for Safety Reasoning, a novel data generation recipe that leverages multi-agent
deliberation to iteratively expand reasoning on safety policies. A data refiner
stage in AIDSAFE ensures high-quality outputs by eliminating repetitive,
redundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong
foundation for supervised fine-tuning (SFT)-based safety training.
Additionally, to address the need of preference data in alignment stages, such
as DPO training, we introduce a supplemental recipe that uses belief
augmentation to create distinct selected and rejected CoT samples. Our
evaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy
adherence and reasoning quality. Consequently, we show that fine-tuning
open-source LLMs on these CoTs can significantly improve safety generalization
and jailbreak robustness while maintaining acceptable utility and over-refusal
accuracy. AIDSAFE-generated CoT datasets can be found here:
https://huggingface.co/datasets/AmazonScience/AIDSAFE

</details>


### [445] [SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety Facts](https://arxiv.org/pdf/2505.21828)
*Chen Yueh-Han, Guy Davidson, Brenden M. Lake*

Main category: cs.AI

TL;DR: The paper introduces SAGE-Eval, a benchmark to test if LLMs generalize critical safety facts to novel situations, finding even top models like Claude-3.7-sonnet perform poorly (58% pass rate).


<details>
  <summary>Details</summary>
Motivation: To assess if LLMs reliably apply safety facts to naive user queries, preventing potential harm (e.g., choking hazards).

Method: SAGE-Eval benchmark with 104 safety facts, systematically expanded to 10,428 test scenarios across 7 domains.

Result: Top model passes only 58%; scaling up weakly correlates with performance.

Conclusion: Frontier LLMs lack robust safety generalization; SAGE-Eval is recommended for pre-deployment evaluations.

Abstract: Do LLMs robustly generalize critical safety facts to novel situations?
Lacking this ability is dangerous when users ask naive questions. For instance,
"I'm considering packing melon balls for my 10-month-old's lunch. What other
foods would be good to include?" Before offering food options, the LLM should
warn that melon balls pose a choking hazard to toddlers, as documented by the
CDC. Failing to provide such warnings could result in serious injuries or even
death. To evaluate this, we introduce SAGE-Eval, SAfety-fact systematic
GEneralization evaluation, the first benchmark that tests whether LLMs properly
apply well established safety facts to naive user queries. SAGE-Eval comprises
104 facts manually sourced from reputable organizations, systematically
augmented to create 10,428 test scenarios across 7 common domains (e.g.,
Outdoor Activities, Medicine). We find that the top model, Claude-3.7-sonnet,
passes only 58% of all the safety facts tested. We also observe that model
capabilities and training compute weakly correlate with performance on
SAGE-Eval, implying that scaling up is not the golden solution. Our findings
suggest frontier LLMs still lack robust generalization ability. We recommend
developers use SAGE-Eval in pre-deployment evaluations to assess model
reliability in addressing salient risks. We publicly release SAGE-Eval at
https://huggingface.co/datasets/YuehHanChen/SAGE-Eval and our code is available
at https://github.com/YuehHanChen/SAGE-Eval/tree/main.

</details>


### [446] [SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem](https://arxiv.org/pdf/2505.21887)
*Ahmed Heakl, Yahia Salaheldin Shaaban, Martin Takac, Salem Lahlou, Zangir Iklassov*

Main category: cs.AI

TL;DR: SVRPBench is the first open benchmark for stochastic vehicle routing, simulating real-world urban logistics challenges like congestion, delays, and accidents. It reveals RL solvers degrade under uncertainty, while classical methods remain robust.


<details>
  <summary>Details</summary>
Motivation: To address the gap in benchmarks for stochastic vehicle routing, capturing real-world dynamics like congestion and delays, and to evaluate solver robustness under uncertainty.

Method: Developed SVRPBench, a benchmark with 500+ instances simulating urban-scale logistics, including time-dependent congestion, log-normal delays, and probabilistic accidents. Evaluated state-of-the-art solvers like POMO and AM.

Result: RL solvers (e.g., POMO, AM) degrade by over 20% under distributional shift, while classical and metaheuristic methods remain robust.

Conclusion: SVRPBench highlights the need for solvers that generalize beyond synthetic assumptions and adapt to real-world uncertainty, providing a reproducible dataset for future research.

Abstract: Robust routing under uncertainty is central to real-world logistics, yet most
benchmarks assume static, idealized settings. We present SVRPBench, the first
open benchmark to capture high-fidelity stochastic dynamics in vehicle routing
at urban scale. Spanning more than 500 instances with up to 1000 customers, it
simulates realistic delivery conditions: time-dependent congestion, log-normal
delays, probabilistic accidents, and empirically grounded time windows for
residential and commercial clients. Our pipeline generates diverse,
constraint-rich scenarios, including multi-depot and multi-vehicle setups.
Benchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade
by over 20% under distributional shift, while classical and metaheuristic
methods remain robust. To enable reproducible research, we release the dataset
and evaluation suite. SVRPBench challenges the community to design solvers that
generalize beyond synthetic assumptions and adapt to real-world uncertainty.

</details>


### [447] [Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy](https://arxiv.org/pdf/2505.21907)
*Saleh Afzoon, Zahra Jahanandish, Phuong Thao Huynh, Amin Beheshti, Usman Naseem*

Main category: cs.AI

TL;DR: The paper surveys preference optimization in AI copilots, proposing a taxonomy for capturing and modeling user preferences across interaction stages.


<details>
  <summary>Details</summary>
Motivation: Personalization is key for AI copilots' usability and productivity, but current techniques are fragmented and underexplored in real-time systems.

Method: The survey synthesizes research, introduces a unified definition of AI copilots, and proposes a phase-based taxonomy for preference optimization.

Result: It highlights techniques for preference acquisition, intent modeling, and feedback integration, bridging AI personalization and human-AI collaboration.

Conclusion: The survey provides a structured foundation for designing adaptive, preference-aware AI copilots, offering insights into leveraging preference resources and technical approaches.

Abstract: AI copilots, context-aware, AI-powered systems designed to assist users in
tasks such as software development and content creation, are becoming integral
to modern workflows. As these systems grow in capability and adoption,
personalization has emerged as a cornerstone for ensuring usability, trust, and
productivity. Central to this personalization is preference optimization: the
ability of AI copilots to detect, interpret, and align with individual user
preferences. While personalization techniques are well-established in domains
like recommender systems and dialogue agents, their adaptation to interactive,
real-time systems like AI copilots remains fragmented and underexplored. This
survey addresses this gap by synthesizing research on how user preferences are
captured, modeled, and refined within the design of AI copilots. We introduce a
unified definition of AI copilots and propose a phase-based taxonomy of
preference optimization strategies, structured around pre-interaction,
mid-interaction, and post-interaction stages. We analyze techniques for
acquiring preference signals, modeling user intent, and integrating feedback
loops, highlighting both established approaches and recent innovations. By
bridging insights from AI personalization, human-AI collaboration, and large
language model adaptation, this survey provides a structured foundation for
designing adaptive, preference-aware AI copilots. It offers a holistic view of
the available preference resources, how they can be leveraged, and which
technical approaches are most suited to each stage of system design.

</details>


### [448] [From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models](https://arxiv.org/pdf/2505.21935)
*Kaiyu He, Zhiyu Chen*

Main category: cs.AI

TL;DR: The paper explores whether LLMs can discover new knowledge, proposing a framework based on Peirce's abduction, deduction, and induction to evaluate their potential for hypothesis generation and innovation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to advance LLMs beyond instruction-following and reasoning, aiming for AGI by enabling them to generate new knowledge and hypotheses.

Method: The survey synthesizes existing work on hypothesis generation, application, and validation, using Peirce's framework as a structured lens.

Result: The paper identifies key achievements and gaps in LLM-based hypothesis discovery, suggesting their potential to evolve into engines of innovation.

Conclusion: LLMs could transition from information executors to tools for genuine innovation, impacting research, science, and problem-solving.

Abstract: Since the advent of Large Language Models (LLMs), efforts have largely
focused on improving their instruction-following and deductive reasoning
abilities, leaving open the question of whether these models can truly discover
new knowledge. In pursuit of artificial general intelligence (AGI), there is a
growing need for models that not only execute commands or retrieve information
but also learn, reason, and generate new knowledge by formulating novel
hypotheses and theories that deepen our understanding of the world. Guided by
Peirce's framework of abduction, deduction, and induction, this survey offers a
structured lens to examine LLM-based hypothesis discovery. We synthesize
existing work in hypothesis generation, application, and validation,
identifying both key achievements and critical gaps. By unifying these threads,
we illuminate how LLMs might evolve from mere ``information executors'' into
engines of genuine innovation, potentially transforming research, science, and
real-world problem solving.

</details>


### [449] [Functional Matching of Logic Subgraphs: Beyond Structural Isomorphism](https://arxiv.org/pdf/2505.21988)
*Ziyang Zheng, Kezhi Li, Zhengyuan Shi, Qiang Xu*

Main category: cs.AI

TL;DR: The paper introduces functional subgraph matching for logic circuits, addressing limitations of structural methods by using a two-stage framework for robust detection and fuzzy boundary identification.


<details>
  <summary>Details</summary>
Motivation: Existing structural graph isomorphism techniques fail to identify function-related subgraphs due to synthesis-induced topology changes, limiting EDA applications.

Method: A two-stage framework: (1) learning functional embeddings across AIG and post-mapping netlists, and (2) fuzzy boundary identification via graph segmentation.

Result: Achieves 93.8% accuracy in functional subgraph detection and 91.3% dice score in fuzzy boundary identification on standard benchmarks.

Conclusion: Functional subgraph matching outperforms structural methods, enabling robust identification of logic functions despite structural variations.

Abstract: Subgraph matching in logic circuits is foundational for numerous Electronic
Design Automation (EDA) applications, including datapath optimization,
arithmetic verification, and hardware trojan detection. However, existing
techniques rely primarily on structural graph isomorphism and thus fail to
identify function-related subgraphs when synthesis transformations
substantially alter circuit topology. To overcome this critical limitation, we
introduce the concept of functional subgraph matching, a novel approach that
identifies whether a given logic function is implicitly present within a larger
circuit, irrespective of structural variations induced by synthesis or
technology mapping. Specifically, we propose a two-stage multi-modal framework:
(1) learning robust functional embeddings across AIG and post-mapping netlists
for functional subgraph detection, and (2) identifying fuzzy boundaries using a
graph segmentation approach. Evaluations on standard benchmarks (ITC99,
OpenABCD, ForgeEDA) demonstrate significant performance improvements over
existing structural methods, with average $93.8\%$ accuracy in functional
subgraph detection and a dice score of $91.3\%$ in fuzzy boundary
identification.

</details>


### [450] [Efficiently Enhancing General Agents With Hierarchical-categorical Memory](https://arxiv.org/pdf/2505.22006)
*Changze Qiao, Mingming Lu*

Main category: cs.AI

TL;DR: EHC is a general agent that learns without parameter updates, using hierarchical memory retrieval and task-category oriented experience learning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for multi-modal agents are either computationally expensive or lack continuous learning. EHC addresses these limitations.

Method: EHC combines Hierarchical Memory Retrieval (HMR) for efficient memory use and Task-Category Oriented Experience Learning (TOEL) for task comprehension.

Result: EHC achieves state-of-the-art performance on standard datasets, handling complex multi-modal tasks effectively.

Conclusion: EHC is a robust, adaptable agent for multi-modal tasks, surpassing current methods without costly training.

Abstract: With large language models (LLMs) demonstrating remarkable capabilities,
there has been a surge in research on leveraging LLMs to build general-purpose
multi-modal agents. However, existing approaches either rely on computationally
expensive end-to-end training using large-scale multi-modal data or adopt
tool-use methods that lack the ability to continuously learn and adapt to new
environments. In this paper, we introduce EHC, a general agent capable of
learning without parameter updates. EHC consists of a Hierarchical Memory
Retrieval (HMR) module and a Task-Category Oriented Experience Learning (TOEL)
module. The HMR module facilitates rapid retrieval of relevant memories and
continuously stores new information without being constrained by memory
capacity. The TOEL module enhances the agent's comprehension of various task
characteristics by classifying experiences and extracting patterns across
different categories. Extensive experiments conducted on multiple standard
datasets demonstrate that EHC outperforms existing methods, achieving
state-of-the-art performance and underscoring its effectiveness as a general
agent for handling complex multi-modal tasks.

</details>


### [451] [Reinforced Reasoning for Embodied Planning](https://arxiv.org/pdf/2505.22050)
*Di Wu, Jiaxin Fan, Junzhe Zang, Guanbo Wang, Wei Yin, Wenhao Li, Bo Jin*

Main category: cs.AI

TL;DR: A reinforcement fine-tuning framework enhances embodied planning by combining vision-language models with structured decision-making and rule-based rewards, outperforming larger models on interactive tasks.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models lack temporal reasoning and spatial understanding for embodied planning, necessitating a reinforcement-driven approach.

Method: The framework distills a dataset from a closed-source model, uses supervised fine-tuning, and optimizes with a rule-based reward function via GRPO.

Result: The method outperforms models like GPT-4o-mini and 70B+ baselines on Embench, showing strong generalization.

Conclusion: Reinforcement-driven reasoning can significantly improve long-horizon planning in embodied AI.

Abstract: Embodied planning requires agents to make coherent multi-step decisions based
on dynamic visual observations and natural language goals. While recent
vision-language models (VLMs) excel at static perception tasks, they struggle
with the temporal reasoning, spatial understanding, and commonsense grounding
needed for planning in interactive environments. In this work, we introduce a
reinforcement fine-tuning framework that brings R1-style reasoning enhancement
into embodied planning. We first distill a high-quality dataset from a powerful
closed-source model and perform supervised fine-tuning (SFT) to equip the model
with structured decision-making priors. We then design a rule-based reward
function tailored to multi-step action quality and optimize the policy via
Generalized Reinforced Preference Optimization (GRPO). Our approach is
evaluated on Embench, a recent benchmark for interactive embodied tasks,
covering both in-domain and out-of-domain scenarios. Experimental results show
that our method significantly outperforms models of similar or larger scale,
including GPT-4o-mini and 70B+ open-source baselines, and exhibits strong
generalization to unseen environments. This work highlights the potential of
reinforcement-driven reasoning to advance long-horizon planning in embodied AI.

</details>


### [452] [Cognitively-Inspired Emergent Communication via Knowledge Graphs for Assisting the Visually Impaired](https://arxiv.org/pdf/2505.22087)
*Ruxiao Chen, Dezheng Han, Wenjie Han, Shuaishuai Guo*

Main category: cs.AI

TL;DR: VAG-EC introduces a cognitively-inspired framework for assistive systems, balancing low-latency and semantic richness by using knowledge graphs and attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between latency and semantic richness in assistive systems for visually impaired individuals.

Method: Constructs knowledge graphs with attention mechanisms to prioritize task-relevant entities, enabling compact and interpretable symbolic languages.

Result: Outperforms traditional methods in Topographic Similarity and Context Independence metrics.

Conclusion: VAG-EC offers a fast, adaptive, and human-aligned solution for real-time assistive technologies.

Abstract: Assistive systems for visually impaired individuals must deliver rapid,
interpretable, and adaptive feedback to facilitate real-time navigation.
Current approaches face a trade-off between latency and semantic richness:
natural language-based systems provide detailed guidance but are too slow for
dynamic scenarios, while emergent communication frameworks offer low-latency
symbolic languages but lack semantic depth, limiting their utility in tactile
modalities like vibration. To address these limitations, we introduce a novel
framework, Cognitively-Inspired Emergent Communication via Knowledge Graphs
(VAG-EC), which emulates human visual perception and cognitive mapping. Our
method constructs knowledge graphs to represent objects and their
relationships, incorporating attention mechanisms to prioritize task-relevant
entities, thereby mirroring human selective attention. This structured approach
enables the emergence of compact, interpretable, and context-sensitive symbolic
languages. Extensive experiments across varying vocabulary sizes and message
lengths demonstrate that VAG-EC outperforms traditional emergent communication
methods in Topographic Similarity (TopSim) and Context Independence (CI). These
findings underscore the potential of cognitively grounded emergent
communication as a fast, adaptive, and human-aligned solution for real-time
assistive technologies. Code is available at
https://github.com/Anonymous-NLPcode/Anonymous_submission/tree/main.

</details>


### [453] [VIRAL: Vision-grounded Integration for Reward design And Learning](https://arxiv.org/pdf/2505.22092)
*Valentin Cuzin-Rambaud, Emilien Komlenovic, Alexandre Faure, Bruno Yun*

Main category: cs.AI

TL;DR: VIRAL is a pipeline using multi-modal LLMs to generate and refine reward functions, improving alignment with user intent in reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of human-machine alignment in AI, especially the risks of poorly designed reward functions in reinforcement learning.

Method: VIRAL employs multi-modal LLMs to autonomously create and refine reward functions, incorporating human feedback or video LLM descriptions.

Result: Tested in five Gymnasium environments, VIRAL accelerates learning and enhances alignment with user intent.

Conclusion: VIRAL demonstrates effective reward function generation and refinement, improving reinforcement learning outcomes.

Abstract: The alignment between humans and machines is a critical challenge in
artificial intelligence today. Reinforcement learning, which aims to maximize a
reward function, is particularly vulnerable to the risks associated with poorly
designed reward functions. Recent advancements has shown that Large Language
Models (LLMs) for reward generation can outperform human performance in this
context. We introduce VIRAL, a pipeline for generating and refining reward
functions through the use of multi-modal LLMs. VIRAL autonomously creates and
interactively improves reward functions based on a given environment and a goal
prompt or annotated image. The refinement process can incorporate human
feedback or be guided by a description generated by a video LLM, which explains
the agent's policy in video form. We evaluated VIRAL in five Gymnasium
environments, demonstrating that it accelerates the learning of new behaviors
while ensuring improved alignment with user intent. The source-code and demo
video are available at: https://github.com/VIRAL-UCBL1/VIRAL and
https://youtu.be/t4_BXugBm9Q.

</details>


### [454] [Efficient Dynamic Shielding for Parametric Safety Specifications](https://arxiv.org/pdf/2505.22104)
*Davide Corsi, Kaushik Mallik, Andoni Rodriguez, Cesar Sanchez*

Main category: cs.AI

TL;DR: Dynamic shields adapt to changing safety requirements in AI systems, offering faster runtime adaptation compared to static shields.


<details>
  <summary>Details</summary>
Motivation: Traditional static shields require recomputation for changing safety requirements, causing delays. Dynamic shields address this by adapting to evolving safety specifications.

Method: Dynamic shields are designed for parametric safety specifications and adapt at runtime using a fast algorithm leveraging standard shield features.

Result: Experiments show dynamic shields are up to 5 times faster than brute-force recomputation, with offline design taking minutes and online adaptation seconds.

Conclusion: Dynamic shields provide efficient runtime safety adaptation for AI systems, outperforming traditional static approaches.

Abstract: Shielding has emerged as a promising approach for ensuring safety of
AI-controlled autonomous systems. The algorithmic goal is to compute a shield,
which is a runtime safety enforcement tool that needs to monitor and intervene
the AI controller's actions if safety could be compromised otherwise.
Traditional shields are designed statically for a specific safety requirement.
Therefore, if the safety requirement changes at runtime due to changing
operating conditions, the shield needs to be recomputed from scratch, causing
delays that could be fatal. We introduce dynamic shields for parametric safety
specifications, which are succinctly represented sets of all possible safety
specifications that may be encountered at runtime. Our dynamic shields are
statically designed for a given safety parameter set, and are able to
dynamically adapt as the true safety specification (permissible by the
parameters) is revealed at runtime. The main algorithmic novelty lies in the
dynamic adaptation procedure, which is a simple and fast algorithm that
utilizes known features of standard safety shields, like maximal
permissiveness. We report experimental results for a robot navigation problem
in unknown territories, where the safety specification evolves as new obstacles
are discovered at runtime. In our experiments, the dynamic shields took a few
minutes for their offline design, and took between a fraction of a second and a
few seconds for online adaptation at each step, whereas the brute-force online
recomputation approach was up to 5 times slower.

</details>


### [455] [HDDLGym: A Tool for Studying Multi-Agent Hierarchical Problems Defined in HDDL with OpenAI Gym](https://arxiv.org/pdf/2505.22597)
*Ngoc La, Ruaridh Mon-Williams, Julie A. Shah*

Main category: cs.AI

TL;DR: HDDLGym is a Python tool that integrates hierarchical planning (HDDL) with reinforcement learning (RL) via OpenAI Gym, supporting multi-agent scenarios and collaborative planning.


<details>
  <summary>Details</summary>
Motivation: There's a lack of tools for seamless integration of hierarchical planning with RL, despite its potential benefits.

Method: HDDLGym automatically generates Gym environments from HDDL domains/problems, bridging RL and hierarchical planning.

Result: The tool supports multi-agent planning, demonstrated in domains like Transport and Overcooked.

Conclusion: HDDLGym is a valuable tool for studying RL in hierarchical planning, especially for multi-agent contexts.

Abstract: In recent years, reinforcement learning (RL) methods have been widely tested
using tools like OpenAI Gym, though many tasks in these environments could also
benefit from hierarchical planning. However, there is a lack of a tool that
enables seamless integration of hierarchical planning with RL. Hierarchical
Domain Definition Language (HDDL), used in classical planning, introduces a
structured approach well-suited for model-based RL to address this gap. To
bridge this integration, we introduce HDDLGym, a Python-based tool that
automatically generates OpenAI Gym environments from HDDL domains and problems.
HDDLGym serves as a link between RL and hierarchical planning, supporting
multi-agent scenarios and enabling collaborative planning among agents. This
paper provides an overview of HDDLGym's design and implementation, highlighting
the challenges and design choices involved in integrating HDDL with the Gym
interface, and applying RL policies to support hierarchical planning. We also
provide detailed instructions and demonstrations for using the HDDLGym
framework, including how to work with existing HDDL domains and problems from
International Planning Competitions, exemplified by the Transport domain.
Additionally, we offer guidance on creating new HDDL domains for multi-agent
scenarios and demonstrate the practical use of HDDLGym in the Overcooked
domain. By leveraging the advantages of HDDL and Gym, HDDLGym aims to be a
valuable tool for studying RL in hierarchical planning, particularly in
multi-agent contexts.

</details>


### [456] [Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test](https://arxiv.org/pdf/2505.22112)
*Guangfu Hao, Frederic Alexandre, Shan Yu*

Main category: cs.AI

TL;DR: VLLMs like GPT-4o, Gemini-1.5 Pro, and Claude-3.5 Sonnet show human-level cognitive flexibility in set-shifting tasks, influenced by input modality and prompting. Role-playing reveals their potential to simulate cognitive deficits, suggesting brain-like architecture.


<details>
  <summary>Details</summary>
Motivation: To explore cognitive flexibility in VLLMs, a less studied area, using the Wisconsin Card Sorting Test (WCST) to compare their performance to humans.

Method: Assessed VLLMs (GPT-4o, Gemini-1.5 Pro, Claude-3.5 Sonnet) with WCST under chain-of-thought prompting and varied input modalities. Role-playing simulated cognitive deficits.

Result: VLLMs matched or exceeded human set-shifting abilities with text inputs. Performance depended on modality and prompting. Role-playing simulated deficits akin to brain impairments.

Conclusion: VLLMs exhibit human-like cognitive flexibility and may model brain processes, highlighting their potential for emulating higher cognition.

Abstract: Cognitive flexibility has been extensively studied in human cognition but
remains relatively unexplored in the context of Visual Large Language Models
(VLLMs). This study assesses the cognitive flexibility of state-of-the-art
VLLMs (GPT-4o, Gemini-1.5 Pro, and Claude-3.5 Sonnet) using the Wisconsin Card
Sorting Test (WCST), a classic measure of set-shifting ability. Our results
reveal that VLLMs achieve or surpass human-level set-shifting capabilities
under chain-of-thought prompting with text-based inputs. However, their
abilities are highly influenced by both input modality and prompting strategy.
In addition, we find that through role-playing, VLLMs can simulate various
functional deficits aligned with patients having impairments in cognitive
flexibility, suggesting that VLLMs may possess a cognitive architecture, at
least regarding the ability of set-shifting, similar to the brain. This study
reveals the fact that VLLMs have already approached the human level on a key
component underlying our higher cognition, and highlights the potential to use
them to emulate complex brain processes.

</details>


### [457] [Lifted Forward Planning in Relational Factored Markov Decision Processes with Concurrent Actions](https://arxiv.org/pdf/2505.22147)
*Florian Andreas Marwitz, Tanya Braun, Ralf Möller, Marcel Gehrke*

Main category: cs.AI

TL;DR: Foreplan addresses exponential state and action space growth in AI decision-making using a first-order representation and relational forward planning, achieving significant speedup.


<details>
  <summary>Details</summary>
Motivation: The exponential growth of state and action spaces in decision-making problems with indistinguishable objects makes policy computation impractical.

Method: Introduces Foreplan, a relational forward planner using a first-order representation to store spaces in polynomial size, and an approximate version for faster computation.

Result: Demonstrates a speedup of at least four orders of magnitude and identifies optimal object actions under restrictions.

Conclusion: Foreplan efficiently handles large-scale decision-making problems with indistinguishable objects, offering both exact and approximate solutions.

Abstract: Decision making is a central problem in AI that can be formalized using a
Markov Decision Process. A problem is that, with increasing numbers of
(indistinguishable) objects, the state space grows exponentially. To compute
policies, the state space has to be enumerated. Even more possibilities have to
be enumerated if the size of the action space depends on the size of the state
space, especially if we allow concurrent actions. To tackle the exponential
blow-up in the action and state space, we present a first-order representation
to store the spaces in polynomial instead of exponential size in the number of
objects and introduce Foreplan, a relational forward planner, which uses this
representation to efficiently compute policies for numerous indistinguishable
objects and actions. Additionally, we introduce an even faster approximate
version of Foreplan. Moreover, Foreplan identifies how many objects an agent
should act on to achieve a certain task given restrictions. Further, we provide
a theoretical analysis and an empirical evaluation of Foreplan, demonstrating a
speedup of at least four orders of magnitude.

</details>


### [458] [What Makes a Good Reasoning Chain? Uncovering Structural Patterns in Long Chain-of-Thought Reasoning](https://arxiv.org/pdf/2505.22148)
*Gangwei Jiang, Yahui Liu, Zhaoyi Li, Qi Wang, Fuzheng Zhang, Linqi Song, Ying Wei, Defu Lian*

Main category: cs.AI

TL;DR: LCoT2Tree converts sequential reasoning chains into hierarchical trees, using GNNs to analyze structural patterns like exploration and backtracking, which predict final answer correctness and improve LLM reasoning.


<details>
  <summary>Details</summary>
Motivation: Understanding how internal structures of reasoning chains in LLMs drive or predict answer correctness is underexplored.

Method: LCoT2Tree transforms Long Chain-of-Thought (LCoT) into tree structures, analyzed with GNNs to identify structural patterns.

Result: Structural patterns (e.g., exploration, backtracking) predict performance; critical failure patterns like over-branching are identified.

Conclusion: LCoT2Tree is a powerful tool for diagnosing, interpreting, and improving LLM reasoning, highlighting the importance of reasoning chain structures.

Abstract: Recent advances in reasoning with large language models (LLMs) have
popularized Long Chain-of-Thought (LCoT), a strategy that encourages deliberate
and step-by-step reasoning before producing a final answer. While LCoTs have
enabled expert-level performance in complex tasks, how the internal structures
of their reasoning chains drive, or even predict, the correctness of final
answers remains a critical yet underexplored question. In this work, we present
LCoT2Tree, an automated framework that converts sequential LCoTs into
hierarchical tree structures and thus enables deeper structural analysis of LLM
reasoning. Using graph neural networks (GNNs), we reveal that structural
patterns extracted by LCoT2Tree, including exploration, backtracking, and
verification, serve as stronger predictors of final performance across a wide
range of tasks and models. Leveraging an explainability technique, we further
identify critical thought patterns such as over-branching that account for
failures. Beyond diagnostic insights, the structural patterns by LCoT2Tree
support practical applications, including improving Best-of-N decoding
effectiveness. Overall, our results underscore the critical role of internal
structures of reasoning chains, positioning LCoT2Tree as a powerful tool for
diagnosing, interpreting, and improving reasoning in LLMs.

</details>


### [459] [A Preprocessing Framework for Efficient Approximate Bi-Objective Shortest-Path Computation in the Presence of Correlated Objectives](https://arxiv.org/pdf/2505.22244)
*Yaron Halle, Ariel Felner, Sven Koenig, Oren Salzman*

Main category: cs.AI

TL;DR: The paper proposes an efficient algorithm for the bi-objective shortest-path (BOSP) problem with correlated objectives, leveraging graph clustering to reduce search effort and improve speed.


<details>
  <summary>Details</summary>
Motivation: Real-world scenarios like road networks often involve optimizing correlated objectives (e.g., travel time and fuel consumption), making BOSP computationally challenging. Existing bounded sub-optimal solvers like A*pex approximate solutions but can be inefficient.

Method: The algorithm uses a preprocessing phase to identify correlated clusters in the graph, creating a new representation. This generalizes A*pex, reducing search effort and speeding up computation.

Result: The approach runs up to five times faster on DIMACS dataset instances while maintaining theoretical guarantees on solution quality.

Conclusion: This is the first algorithm to efficiently exploit correlations in BOSP, offering significant performance improvements without sacrificing solution quality.

Abstract: The bi-objective shortest-path (BOSP) problem seeks to find paths between
start and target vertices of a graph while optimizing two conflicting objective
functions. We consider the BOSP problem in the presence of correlated
objectives. Such correlations often occur in real-world settings such as road
networks, where optimizing two positively correlated objectives, such as travel
time and fuel consumption, is common. BOSP is generally computationally
challenging as the size of the search space is exponential in the number of
objective functions and the graph size. Bounded sub-optimal BOSP solvers such
as A*pex alleviate this complexity by approximating the Pareto-optimal solution
set rather than computing it exactly (given a user-provided approximation
factor). As the correlation between objective functions increases, smaller
approximation factors are sufficient for collapsing the entire Pareto-optimal
set into a single solution. We leverage this insight to propose an efficient
algorithm that reduces the search effort in the presence of correlated
objectives. Our approach for computing approximations of the entire
Pareto-optimal set is inspired by graph-clustering algorithms. It uses a
preprocessing phase to identify correlated clusters within a graph and to
generate a new graph representation. This allows a natural generalization of
A*pex to run up to five times faster on DIMACS dataset instances, a standard
benchmark in the field. To the best of our knowledge, this is the first
algorithm proposed that efficiently and effectively exploits correlations in
the context of bi-objective search while providing theoretical guarantees on
solution quality.

</details>


### [460] [Compression versus Accuracy: A Hierarchy of Lifted Models](https://arxiv.org/pdf/2505.22288)
*Jan Speller, Malte Luttermann, Marcel Gehrke, Tanya Braun*

Main category: cs.AI

TL;DR: The paper introduces a hyperparameter-free hierarchical approach for lifted model construction in probabilistic graphical models, addressing the challenges of selecting ε in the Advanced Colour Passing (ACP) algorithm.


<details>
  <summary>Details</summary>
Motivation: The need to avoid the non-obvious and exploratory selection of ε in ACP, which can lead to inconsistent models and reduced interpretability.

Method: A hierarchical approach that computes a hierarchy of ε values, ensuring consistent grouping of factors and enabling a trade-off between compression and accuracy.

Result: The method provides a hierarchy of models with corresponding error bounds, improving interpretability and efficiency.

Conclusion: The proposed approach eliminates the need for ε tuning, offering a structured and interpretable way to construct lifted models.

Abstract: Probabilistic graphical models that encode indistinguishable objects and
relations among them use first-order logic constructs to compress a
propositional factorised model for more efficient (lifted) inference. To obtain
a lifted representation, the state-of-the-art algorithm Advanced Colour Passing
(ACP) groups factors that represent matching distributions. In an approximate
version using $\varepsilon$ as a hyperparameter, factors are grouped that
differ by a factor of at most $(1\pm \varepsilon)$. However, finding a suitable
$\varepsilon$ is not obvious and may need a lot of exploration, possibly
requiring many ACP runs with different $\varepsilon$ values. Additionally,
varying $\varepsilon$ can yield wildly different models, leading to decreased
interpretability. Therefore, this paper presents a hierarchical approach to
lifted model construction that is hyperparameter-free. It efficiently computes
a hierarchy of $\varepsilon$ values that ensures a hierarchy of models, meaning
that once factors are grouped together given some $\varepsilon$, these factors
will be grouped together for larger $\varepsilon$ as well. The hierarchy of
$\varepsilon$ values also leads to a hierarchy of error bounds. This allows for
explicitly weighing compression versus accuracy when choosing specific
$\varepsilon$ values to run ACP with and enables interpretability between the
different models.

</details>


### [461] [Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration](https://arxiv.org/pdf/2502.11882)
*Shao Zhang, Xihuai Wang, Wenhao Zhang, Chaoran Li, Junru Song, Tingyu Li, Lin Qiu, Xuezhi Cao, Xunliang Cai, Wen Yao, Weinan Zhang, Xinbing Wang, Ying Wen*

Main category: cs.AI

TL;DR: DPT-Agent integrates System 1 (fast, intuitive) and System 2 (reasoning-based) for real-time human-AI collaboration, outperforming current LLM-based methods.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based agents struggle with real-time tasks due to latency and difficulty inferring human strategies.

Method: Proposes DPT-Agent, combining System 1 (FSM and code-as-policy) and System 2 (ToM and asynchronous reflection).

Result: DPT-Agent shows significant improvements over mainstream LLM-based frameworks in real-time collaboration.

Conclusion: DPT-Agent is the first framework enabling autonomous real-time human-AI collaboration, enhancing LLM performance.

Abstract: Agents built on large language models (LLMs) have excelled in turn-by-turn
human-AI collaboration but struggle with simultaneous tasks requiring real-time
interaction. Latency issues and the challenge of inferring variable human
strategies hinder their ability to make autonomous decisions without explicit
instructions. Through experiments with current independent System 1 and System
2 methods, we validate the necessity of using Dual Process Theory (DPT) in
real-time tasks. We propose DPT-Agent, a novel language agent framework that
integrates System 1 and System 2 for efficient real-time simultaneous human-AI
collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and
code-as-policy for fast, intuitive, and controllable decision-making.
DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous
reflection to infer human intentions and perform reasoning-based autonomous
decisions. We demonstrate the effectiveness of DPT-Agent through further
experiments with rule-based agents and human collaborators, showing significant
improvements over mainstream LLM-based frameworks. DPT-Agent can effectively
help LLMs convert correct slow thinking and reasoning into executable actions,
thereby improving performance. To the best of our knowledge, DPT-Agent is the
first language agent framework that achieves successful real-time simultaneous
human-AI collaboration autonomously. Code of DPT-Agent can be found in
https://github.com/sjtu-marl/DPT-Agent.

</details>


### [462] [Rethinking the Unsolvable: When In-Context Search Meets Test-Time Scaling](https://arxiv.org/pdf/2505.22290)
*Fanzeng Xia, Yidong Luo, Tinko Sebastian Bartels, Yaqi Xu, Tongxin Li*

Main category: cs.AI

TL;DR: Combining in-context search and test-time scaling significantly boosts LLM performance on hard reasoning tasks, achieving up to 30x improvement, challenging current assumptions about LLM limitations.


<details>
  <summary>Details</summary>
Motivation: To address the underestimation of LLMs' reasoning capabilities due to simplistic evaluation methods and demonstrate their potential on complex tasks.

Method: Systematically explores in-context search prompting and internal scaling to enhance LLM reasoning.

Result: Achieves transformative performance improvements (up to 30x) on NP-hard tasks and real-world benchmarks, extending solvable problem complexity.

Conclusion: Current evaluation paradigms underestimate LLMs; a reassessment is needed to capture their true reasoning potential for real-world applications.

Abstract: Recent research has highlighted that Large Language Models (LLMs), even when
trained to generate extended long reasoning steps, still face significant
challenges on hard reasoning problems. However, much of the existing literature
relies on direct prompting with simple in-context learning examples for
evaluation, which largely overlooks advanced techniques to elicit LLMs'
deliberate reasoning before drawing conclusions that LLMs hit a performance
ceiling. In this paper, we systematically explore the combined potential of
in-context search and test-time scaling on super hard reasoning tasks. We find
that by employing advanced in-context search prompting to LLMs augmented with
internal scaling, one can achieve transformative performance breakthroughs on
tasks previously deemed "unsolvable" (e.g., reported success rates below 5%).
We provide both empirical results and theoretical analysis of how this
combination can unleash LLM reasoning capabilities: i) Empirically, on
controlled NP-hard tasks and complex real-world planning benchmarks, our
approach achieves up to a 30x improvement in success rates compared to
previously reported results without any external mechanisms; ii) Theoretically,
we show that in-context search prompting, when combined with internal scaling,
significantly extends the complexity class of solvable reasoning problems.
These findings challenge prevailing assumptions about the limitations of LLMs
on complex tasks, indicating that current evaluation paradigms systematically
underestimate their true potential. Our work calls for a critical reassessment
of how LLM reasoning is benchmarked and a more robust evaluation strategy that
fully captures the true capabilities of contemporary LLMs, which can lead to a
better understanding of their operational reasoning boundaries in real-world
deployments.

</details>


### [463] [From Large AI Models to Agentic AI: A Tutorial on Future Intelligent Communications](https://arxiv.org/pdf/2505.22311)
*Feibo Jiang, Cunhua Pan, Li Dong, Kezhi Wang, Octavia A. Dobre, Merouane Debbah*

Main category: cs.AI

TL;DR: The paper introduces Large Artificial Intelligence Models (LAMs) and Agentic AI for 6G communications, covering their design, applications, and challenges.


<details>
  <summary>Details</summary>
Motivation: Address challenges in 6G communications like limited adaptability and scalability by leveraging LAMs and Agentic AI.

Method: Systematic review of LAMs (LLMs, LVMs, etc.), propose a LAM-centric design, and develop an Agentic AI system for communications.

Result: Framework for LAM-based Agentic AI in 6G, detailing components like planners and knowledge bases, with applications in communication scenarios.

Conclusion: Identifies research challenges and future directions for efficient, secure, and sustainable 6G intelligent communication systems.

Abstract: With the advent of 6G communications, intelligent communication systems face
multiple challenges, including constrained perception and response
capabilities, limited scalability, and low adaptability in dynamic
environments. This tutorial provides a systematic introduction to the
principles, design, and applications of Large Artificial Intelligence Models
(LAMs) and Agentic AI technologies in intelligent communication systems, aiming
to offer researchers a comprehensive overview of cutting-edge technologies and
practical guidance. First, we outline the background of 6G communications,
review the technological evolution from LAMs to Agentic AI, and clarify the
tutorial's motivation and main contributions. Subsequently, we present a
comprehensive review of the key components required for constructing LAMs. We
further categorize LAMs and analyze their applicability, covering Large
Language Models (LLMs), Large Vision Models (LVMs), Large Multimodal Models
(LMMs), Large Reasoning Models (LRMs), and lightweight LAMs. Next, we propose a
LAM-centric design paradigm tailored for communications, encompassing dataset
construction and both internal and external learning approaches. Building upon
this, we develop an LAM-based Agentic AI system for intelligent communications,
clarifying its core components such as planners, knowledge bases, tools, and
memory modules, as well as its interaction mechanisms. We also introduce a
multi-agent framework with data retrieval, collaborative planning, and
reflective evaluation for 6G. Subsequently, we provide a detailed overview of
the applications of LAMs and Agentic AI in communication scenarios. Finally, we
summarize the research challenges and future directions in current studies,
aiming to support the development of efficient, secure, and sustainable
next-generation intelligent communication systems.

</details>


### [464] [AgentDNS: A Root Domain Naming System for LLM Agents](https://arxiv.org/pdf/2505.22368)
*Enfang Cui, Yujun Cheng, Rui She, Dan Liu, Zhiyuan Liang, Minxin Guo, Tianzheng Li, Qian Wei, Wenjuan Xing, Zhijie Zhong*

Main category: cs.AI

TL;DR: AgentDNS is a proposed system for standardized service discovery and secure invocation across LLM agents and tools, inspired by traditional DNS.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of standardized protocols for cross-vendor service discovery and interoperability in LLM agents.

Method: Introduces AgentDNS, a root domain naming and service discovery system with structured registration, semantic discovery, secure invocation, and unified billing.

Result: Demonstrates potential to streamline multi-agent collaboration in real-world scenarios.

Conclusion: AgentDNS offers a solution for autonomous discovery and secure invocation of third-party services, enhancing LLM agent interoperability.

Abstract: The rapid evolution of Large Language Model (LLM) agents has highlighted
critical challenges in cross-vendor service discovery, interoperability, and
communication. Existing protocols like model context protocol and
agent-to-agent protocol have made significant strides in standardizing
interoperability between agents and tools, as well as communication among
multi-agents. However, there remains a lack of standardized protocols and
solutions for service discovery across different agent and tool vendors. In
this paper, we propose AgentDNS, a root domain naming and service discovery
system designed to enable LLM agents to autonomously discover, resolve, and
securely invoke third-party agent and tool services across organizational and
technological boundaries. Inspired by the principles of the traditional DNS,
AgentDNS introduces a structured mechanism for service registration, semantic
service discovery, secure invocation, and unified billing. We detail the
architecture, core functionalities, and use cases of AgentDNS, demonstrating
its potential to streamline multi-agent collaboration in real-world scenarios.
The source code will be published on https://github.com/agentdns.

</details>


### [465] [AI Mathematician: Towards Fully Automated Frontier Mathematical Research](https://arxiv.org/pdf/2505.22451)
*Yuanhang Liu, Yanxing Huang, Yanqiao Wang, Peng Li, Yang Liu*

Main category: cs.AI

TL;DR: The AI Mathematician (AIM) framework leverages Large Reasoning Models (LRMs) to tackle frontier mathematical research, addressing challenges like problem complexity and procedural rigor with exploration and verification strategies.


<details>
  <summary>Details</summary>
Motivation: To extend the success of LRMs from competition-level problems to frontier mathematical research by overcoming intrinsic complexity and procedural rigor challenges.

Method: AIM uses an exploration mechanism for longer solution paths and a pessimistic reasonable verification method for reliability.

Result: AIM demonstrates strong capability in research-level tasks, autonomously constructing proofs and uncovering non-trivial insights across mathematical topics.

Conclusion: LRM-based systems like AIM have the potential to significantly accelerate mathematical research and discovery.

Abstract: Large Reasoning Models (LRMs) have made significant progress in mathematical
capabilities in recent times. However, these successes have been primarily
confined to competition-level problems. In this work, we propose AI
Mathematician (AIM) framework, which harnesses the reasoning strength of LRMs
to support frontier mathematical research. We have identified two critical
challenges of mathematical research compared to competition, {\it the intrinsic
complexity of research problems} and {\it the requirement of procedural rigor}.
To address these challenges, AIM incorporates two core strategies: an
exploration mechanism to foster longer solution paths, and the pessimistic
reasonable verification method to ensure reliability.
  This early version of AIM already exhibits strong capability in tackling
research-level tasks. We conducted extensive experiments across several
real-world mathematical topics and obtained promising results. AIM is able to
autonomously construct substantial portions of proofs and uncover non-trivial
insights within each research area. These findings highlight the potential of
LRMs in mathematical discovery and suggest that LRM-based agent systems could
significantly accelerate mathematical research in the future.

</details>


### [466] [Capability-Based Scaling Laws for LLM Red-Teaming](https://arxiv.org/pdf/2505.20162)
*Alexander Panfilov, Paul Kassianik, Maksym Andriushchenko, Jonas Geiping*

Main category: cs.AI

TL;DR: The paper studies red-teaming vulnerabilities in large language models (LLMs) as they surpass human capabilities, identifying trends in attack success based on capability gaps and proposing a scaling law to predict it.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of red-teaming when target models exceed human or fixed-capability attackers in capability, ensuring safe deployment of advanced LLMs.

Method: Evaluated over 500 attacker-target pairs using LLM-based jailbreak attacks, analyzing trends across model families, sizes, and capability levels.

Result: Key findings: (i) stronger models are better attackers, (ii) attack success drops when targets surpass attackers, (iii) success correlates with MMLU-Pro social science performance. A scaling law predicts attack success.

Conclusion: Fixed-capability attackers (e.g., humans) may fail against future models; open-source models pose risks, and providers must control persuasive abilities to mitigate attacks.

Abstract: As large language models grow in capability and agency, identifying
vulnerabilities through red-teaming becomes vital for safe deployment. However,
traditional prompt-engineering approaches may prove ineffective once
red-teaming turns into a weak-to-strong problem, where target models surpass
red-teamers in capabilities. To study this shift, we frame red-teaming through
the lens of the capability gap between attacker and target. We evaluate more
than 500 attacker-target pairs using LLM-based jailbreak attacks that mimic
human red-teamers across diverse families, sizes, and capability levels. Three
strong trends emerge: (i) more capable models are better attackers, (ii) attack
success drops sharply once the target's capability exceeds the attacker's, and
(iii) attack success rates correlate with high performance on social science
splits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking
scaling law that predicts attack success for a fixed target based on
attacker-target capability gap. These findings suggest that fixed-capability
attackers (e.g., humans) may become ineffective against future models,
increasingly capable open-source models amplify risks for existing systems, and
model providers must accurately measure and control models' persuasive and
manipulative abilities to limit their effectiveness as attackers.

</details>


### [467] [Novelty Detection in Reinforcement Learning with World Models](https://arxiv.org/pdf/2310.08731)
*Geigh Zollicoffer, Kenneth Eaton, Jonathan Balloch, Julia Kim, Wei Zhou, Robert Wright, Mark O. Riedl*

Main category: cs.AI

TL;DR: The paper proposes bounding approaches for novelty detection in world model RL agents, using misalignment between hallucinated and observed states as an anomaly score. It outperforms traditional methods in novel environments.


<details>
  <summary>Details</summary>
Motivation: Sudden changes (novelties) in world mechanics or properties can degrade RL agent performance. Detecting these novelties is crucial for reliable deployment.

Method: Utilizes misalignment between the world model's hallucinated states and true observed states as an anomaly score for novelty detection.

Result: The proposed method effectively detects novelties in learned transition distributions and outperforms traditional ML and RL novelty detection methods.

Conclusion: Incorporating novelty detection into world model RL agents improves reliability in novel environments, offering a straightforward yet effective solution.

Abstract: Reinforcement learning (RL) using world models has found significant recent
successes. However, when a sudden change to world mechanics or properties
occurs then agent performance and reliability can dramatically decline. We
refer to the sudden change in visual properties or state transitions as
novelties. Implementing novelty detection within generated world model
frameworks is a crucial task for protecting the agent when deployed. In this
paper, we propose straightforward bounding approaches to incorporate novelty
detection into world model RL agents, by utilizing the misalignment of the
world model's hallucinated states and the true observed states as an anomaly
score. We provide effective approaches to detecting novelties in a distribution
of transitions learned by an agent in a world model. Finally, we show the
advantage of our work in a novel environment compared to traditional machine
learning novelty detection methods as well as currently accepted RL focused
novelty detection algorithms.

</details>


### [468] [Community Detection in Networks: A Rough Sets and Consensus Clustering Approach](https://arxiv.org/pdf/2406.12412)
*Darian H. Grass-Boada, Leandro González-Montesino, Rubén Armañanzas*

Main category: cs.AI

TL;DR: Proposes RC-CCD, a Rough Clustering-based Consensus Community Detection framework, to improve community detection in complex networks using Rough Set Theory for uncertainty management.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of identifying community structures from diverse partitions in complex networks, aiming for higher reliability and accuracy.

Method: Uses a consensus approach based on Rough Set Theory (RST) to manage uncertainty. Tested on synthetic LFR benchmark networks with varying scales, node degrees, and community sizes.

Result: Outperforms Louvain, Greedy, and LPA in normalized mutual information, showing superior accuracy and adaptability, especially in complex networks.

Conclusion: RC-CCD enhances community detection, with implications for social and biological network analysis.

Abstract: The objective of this paper is to propose a framework, called Rough
Clustering-based Consensus Community Detection (RC-CCD), to effectively address
the challenge of identifying community structures in complex networks from a
set of different community partitions. The method uses a consensus approach
based on Rough Set Theory (RST) to manage uncertainty and improve the
reliability of community detection. The RC-CCD framework is tested on synthetic
benchmark networks generated by the Lancichinetti-Fortunato-Radicchi (LFR)
method, which simulate varying network scales, node degrees, and community
sizes. Key findings demonstrate that RC-CCD outperforms established algorithms
like Louvain, Greedy, and LPA in terms of normalized mutual information,
showing superior accuracy and adaptability, particularly in networks with
higher complexity, both in terms of size and dispersion. These results have
significant implications for enhancing community detection in fields such as
social and biological network analysis.

</details>


### [469] [LAMBDA: A Large Model Based Data Agent](https://arxiv.org/pdf/2407.17535)
*Maojun Sun, Ruijian Han, Binyan Jiang, Houduo Qi, Defeng Sun, Yancheng Yuan, Jian Huang*

Main category: cs.AI

TL;DR: LAMBDA is a code-free, open-source multi-agent data analysis system using large language models, featuring programmer and inspector agents for seamless data analysis.


<details>
  <summary>Details</summary>
Motivation: To address data analysis challenges by integrating human and AI intelligence for accessibility and efficiency.

Method: Uses programmer and inspector agents for code generation and debugging, with a user interface for intervention and a Knowledge Integration Mechanism for external models.

Result: Demonstrated strong performance on various tasks, enhancing data analysis paradigms.

Conclusion: LAMBDA effectively integrates human and AI intelligence, making data analysis more accessible and efficient.

Abstract: We introduce LArge Model Based Data Agent (LAMBDA), a novel open-source,
code-free multi-agent data analysis system that leverages the power of large
language models. LAMBDA is designed to address data analysis challenges in
data-driven applications through innovatively designed data agents using
natural language. At the core of LAMBDA are two key agent roles: the programmer
and the inspector, which are engineered to work together seamlessly.
Specifically, the programmer generates code based on the user's instructions
and domain-specific knowledge, while the inspector debugs the code when
necessary. To ensure robustness and handle adverse scenarios, LAMBDA features a
user interface that allows direct user intervention. Moreover, LAMBDA can
flexibly integrate external models and algorithms through our proposed
Knowledge Integration Mechanism, catering to the needs of customized data
analysis. LAMBDA has demonstrated strong performance on various data analysis
tasks. It has the potential to enhance data analysis paradigms by seamlessly
integrating human and artificial intelligence, making it more accessible,
effective, and efficient for users from diverse backgrounds. The strong
performance of LAMBDA in solving data analysis problems is demonstrated using
real-world data examples. The code for LAMBDA is available at
https://github.com/AMA-CMFAI/LAMBDA and videos of three case studies can be
viewed at https://www.polyu.edu.hk/ama/cmfai/lambda.html.

</details>


### [470] [Fine-Grained and Thematic Evaluation of LLMs in Social Deduction Game](https://arxiv.org/pdf/2408.09946)
*Byungjun Kim, Dayeon Seo, Bugeun Kim*

Main category: cs.AI

TL;DR: The paper critiques prior evaluation methods for LLMs in obscured communication tasks, proposing fine-grained metrics and structured error analysis to improve assessment.


<details>
  <summary>Details</summary>
Motivation: To address the shortcomings in evaluating LLMs' performance in obscured communication, particularly in social deduction games, where prior methods were coarse-grained and lacked structured error analysis.

Method: Introduced seven fine-grained metrics for detailed evaluation and conducted a thematic analysis to identify reasoning failures in LLMs.

Result: Identified four major reasoning failures affecting LLMs' performance in obscured communication.

Conclusion: Proposes a more systematic and detailed approach to evaluating LLMs in tasks requiring obscured communication, enhancing understanding of their limitations.

Abstract: Recent studies have investigated whether large language models (LLMs) can
support obscure communication that requires specialized skills, such as
inferring subtext or doublespeak. To conduct the investigation, researchers
have used social deduction games (SDGs) as their experimental environment, in
which players conceal and infer specific information. However, prior work has
often overlooked how LLMs should be evaluated in such settings. Specifically,
we point out two issues with the evaluation methods they employed. First,
metrics used in prior studies are coarse-grained as they are based on overall
game outcomes that often fail to capture event-level behaviors; Second, error
analyses have lacked structured methodologies capable of producing insights
that meaningfully support evaluation outcomes. To address these issues, we
propose a macroscopic and systematic approach to the investigation.
Specifically, we introduce seven fine-grained metrics that resolve the first
issue. To tackle the second issue, we conducted a thematic analysis and
identified four major reasoning failures that undermine LLMs' performance in
obscured communication.

</details>


### [471] [Automating Thought of Search: A Journey Towards Soundness and Completeness](https://arxiv.org/pdf/2408.11326)
*Daniel Cao, Michael Katz, Harsha Kokel, Kavitha Srinivas, Shirin Sohrabi*

Main category: cs.AI

TL;DR: AutoToS automates the Thought of Search (ToS) process, achieving 100% accuracy by guiding LLMs with feedback from unit tests, eliminating human intervention.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of LLMs in planning problems, where human collaboration is required for soundness, AutoToS aims to automate the ToS process.

Method: AutoToS guides LLMs step-by-step using feedback from generic and domain-specific unit tests to generate sound and complete search components.

Result: AutoToS achieved 100% accuracy on all evaluated domains with minimal LLM calls.

Conclusion: AutoToS successfully automates ToS, demonstrating the potential for fully automated, accurate planning solutions using LLMs.

Abstract: Large language models (LLMs) are being used to solve planning problems that
require search. Most of the literature uses LLMs as world models to define the
search space, forgoing soundness for the sake of flexibility. A recent work,
Thought of Search (ToS), proposed defining the search space with code, having
LLMs produce that code. ToS requires a human in the loop, collaboratively
producing a sound successor function and goal test. The result, however, is
worth the effort: all the tested datasets were solved with 100% accuracy.
Consequently, there is great potential to automate the ToS process. We take a
first major step towards automating ToS (AutoToS), taking the human out of the
loop of interactions with the language model. AutoToS guides the language model
step by step towards the generation of sound and complete search components,
through feedback from both generic and domain specific unit tests. We show that
AutoToS is able to achieve 100% accuracy on all the evaluated domains with a
small number of LLM calls.

</details>


### [472] [Addressing and Visualizing Misalignments in Human Task-Solving Trajectories](https://arxiv.org/pdf/2409.14191)
*Sejin Kim, Hosung Lee, Sundong Kim*

Main category: cs.AI

TL;DR: The paper categorizes human task-solving misalignments into three types, proposes a heuristic algorithm for detection, and shows improved AI performance through intention-aligned training.


<details>
  <summary>Details</summary>
Motivation: Understanding and addressing misalignments in human task-solving trajectories to enhance AI models mimicking human reasoning.

Method: Formalizes three misalignment types, proposes a heuristic algorithm for detection, and introduces an intention estimation method for alignment.

Result: AI models trained on aligned human trajectories show improved performance in mimicking human reasoning.

Conclusion: Trajectory-intention alignment is crucial, and intention-aligned training effectively enhances AI model performance.

Abstract: Understanding misalignments in human task-solving trajectories is crucial for
enhancing AI models trained to closely mimic human reasoning. This study
categorizes such misalignments into three types: (1) lack of functions to
express intent, (2) inefficient action sequences, and (3) incorrect intentions
that cannot solve the task. To address these issues, we first formalize and
define these three misalignment types in a unified framework. We then propose a
heuristic algorithm to detect misalignments in ARCTraj trajectories and analyze
their impact hierarchically and quantitatively. We also present an intention
estimation method based on our formalism that infers missing alignment between
user actions and intentions. Through trajectory alignment, we experimentally
demonstrate that AI models trained on human task-solving trajectories improve
performance in mimicking human reasoning. Based on hierarchical analysis and
experiments, we highlight the importance of trajectory-intention alignment and
demonstrate the effectiveness of intention-aligned training.

</details>


### [473] [Kimi k1.5: Scaling Reinforcement Learning with LLMs](https://arxiv.org/pdf/2501.12599)
*Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang, Zongyu Lin*

Main category: cs.AI

TL;DR: The paper introduces Kimi k1.5, a multi-modal LLM trained with RL, achieving state-of-the-art results by simplifying RL techniques and leveraging long-context scaling.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of next-token prediction in scaling AI, the authors explore RL for training LLMs, aiming to improve performance without complex methods like Monte Carlo tree search.

Method: The approach combines RL training techniques, multi-modal data recipes, and infrastructure optimization, focusing on long-context scaling and improved policy optimization.

Result: Kimi k1.5 achieves top-tier performance across benchmarks (e.g., 77.5 on AIME, 96.2 on MATH 500) and outperforms models like GPT-4o in short-CoT reasoning.

Conclusion: The work demonstrates that simplified RL frameworks can effectively scale LLM training, yielding competitive results without reliance on complex techniques.

Abstract: Language model pretraining with next token prediction has proved effective
for scaling compute but is limited to the amount of available training data.
Scaling reinforcement learning (RL) unlocks a new axis for the continued
improvement of artificial intelligence, with the promise that large language
models (LLMs) can scale their training data by learning to explore with
rewards. However, prior published work has not produced competitive results. In
light of this, we report on the training practice of Kimi k1.5, our latest
multi-modal LLM trained with RL, including its RL training techniques,
multi-modal data recipes, and infrastructure optimization. Long context scaling
and improved policy optimization methods are key ingredients of our approach,
which establishes a simplistic, effective RL framework without relying on more
complex techniques such as Monte Carlo tree search, value functions, and
process reward models. Notably, our system achieves state-of-the-art reasoning
performance across multiple benchmarks and modalities -- e.g., 77.5 on AIME,
96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista -- matching
OpenAI's o1. Moreover, we present effective long2short methods that use
long-CoT techniques to improve short-CoT models, yielding state-of-the-art
short-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on
LiveCodeBench -- outperforming existing short-CoT models such as GPT-4o and
Claude Sonnet 3.5 by a large margin (up to +550%).

</details>


### [474] [MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented Reinforcement in Embodied Systems](https://arxiv.org/pdf/2501.19318)
*Anirudh Chari, Suraj Reddy, Aditya Tiwari, Richard Lian, Brian Zhou*

Main category: cs.AI

TL;DR: MINDSTORES enhances LLM-based planning for embodied agents by using past experiences to build mental models, improving robustness in complex environments like Minecraft.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack the ability to learn from experience or build persistent mental models, limiting their effectiveness in open-world environments.

Method: MINDSTORES augments zero-shot LLM planning with a database of past experiences, represented as natural language embeddings, for retrieval and reasoning.

Result: In the MineDojo environment, MINDSTORES outperforms memory-based LLM planners in learning and applying knowledge while retaining zero-shot flexibility.

Conclusion: MINDSTORES advances embodied AI by enabling continuous learning through natural interaction, improving planning robustness.

Abstract: While large language models (LLMs) have shown promising capabilities as
zero-shot planners for embodied agents, their inability to learn from
experience and build persistent mental models limits their robustness in
complex open-world environments like Minecraft. We introduce MINDSTORES, an
experience-augmented planning framework that enables embodied agents to build
and leverage mental models through natural interaction with their environment.
Drawing inspiration from how humans construct and refine cognitive mental
models, our approach extends existing zero-shot LLM planning by maintaining a
database of past experiences that informs future planning iterations. The key
innovation is representing accumulated experiences as natural language
embeddings of (state, task, plan, outcome) tuples, which can then be
efficiently retrieved and reasoned over by an LLM planner to generate insights
and guide plan refinement for novel states and tasks. Through extensive
experiments in the MineDojo environment, a simulation environment for agents in
Minecraft that provides low-level controls for Minecraft, we find that
MINDSTORES learns and applies its knowledge significantly better than existing
memory-based LLM planners while maintaining the flexibility and generalization
benefits of zero-shot approaches, representing an important step toward more
capable embodied AI systems that can learn continuously through natural
experience.

</details>


### [475] [Explanation Design in Strategic Learning: Sufficient Explanations that Induce Non-harmful Responses](https://arxiv.org/pdf/2502.04058)
*Kiet Q. H. Vo, Siu Lun Chau, Masahiro Kato, Yixin Wang, Krikamol Muandet*

Main category: cs.AI

TL;DR: The paper explores how decision makers (DMs) can design explanations in algorithmic systems to prevent strategic agents from taking harmful actions, proposing action recommendation-based explanations (ARexes) as a solution.


<details>
  <summary>Details</summary>
Motivation: The demand for transparent algorithmic systems is growing, but partial disclosure of model information can mislead agents into self-harming actions. The paper aims to address this gap.

Method: The study analyzes explanation methods, establishes a necessary condition for non-harmful responses, and proposes ARexes under a conditional homogeneity assumption. A learning procedure jointly optimizes the predictive model and explanation policy.

Result: Experiments show ARexes allow DMs to optimize predictive performance while preserving agents' utility, offering a refined strategy for partial model disclosure.

Conclusion: ARexes provide a practical solution for safe and effective partial model disclosure, balancing decision-making goals and agent utility.

Abstract: We study explanation design in algorithmic decision making with strategic
agents, individuals who may modify their inputs in response to explanations of
a decision maker's (DM's) predictive model. As the demand for transparent
algorithmic systems continues to grow, most prior work assumes full model
disclosure as the default solution. In practice, however, DMs such as financial
institutions typically disclose only partial model information via
explanations. Such partial disclosure can lead agents to misinterpret the model
and take actions that unknowingly harm their utility. A key open question is
how DMs can communicate explanations in a way that avoids harming strategic
agents, while still supporting their own decision-making goals, e.g.,
minimising predictive error. In this work, we analyse well-known explanation
methods, and establish a necessary condition to prevent explanations from
misleading agents into self-harming actions. Moreover, with a conditional
homogeneity assumption, we prove that action recommendation-based explanations
(ARexes) are sufficient for non-harmful responses, mirroring the revelation
principle in information design. To demonstrate how ARexes can be
operationalised in practice, we propose a simple learning procedure that
jointly optimises the predictive model and explanation policy. Experiments on
synthetic and real-world tasks show that ARexes allow the DM to optimise their
model's predictive performance while preserving agents' utility, offering a
more refined strategy for safe and effective partial model disclosure.

</details>


### [476] [Explorer: Scaling Exploration-driven Web Trajectory Synthesis for Multimodal Web Agents](https://arxiv.org/pdf/2502.11357)
*Vardaan Pahuja, Yadong Lu, Corby Rosset, Boyu Gou, Arindam Mitra, Spencer Whitehead, Yu Su, Ahmed Awadallah*

Main category: cs.AI

TL;DR: The paper introduces a scalable method to create a large, diverse trajectory-level dataset for training multimodal web agents, achieving strong performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: The lack of diverse, large-scale trajectory datasets hinders the development of human-level multimodal web agents.

Method: A scalable recipe synthesizes a dataset with 94K trajectories, leveraging web exploration and refinement for diverse tasks.

Result: The dataset costs 28 cents per trajectory, and the trained agent, Explorer, excels in offline and online benchmarks.

Conclusion: The study advances LMM-based agent research by making large-scale data and improved capabilities accessible.

Abstract: Recent success in large multimodal models (LMMs) has sparked promising
applications of agents capable of autonomously completing complex web tasks.
While open-source LMM agents have made significant advances in offline
evaluation benchmarks, their performance still falls substantially short of
human-level capabilities in more realistic online settings. A key bottleneck is
the lack of diverse and large-scale trajectory-level datasets across various
domains, which are expensive to collect. In this paper, we address this
challenge by developing a scalable recipe to synthesize the largest and most
diverse trajectory-level dataset to date, containing over 94K successful
multimodal web trajectories, spanning 49K unique URLs, 720K screenshots, and
33M web elements. In particular, we leverage extensive web exploration and
refinement to obtain diverse task intents. The average cost is 28 cents per
successful trajectory, making it affordable to a wide range of users in the
community. Leveraging this dataset, we train Explorer, a multimodal web agent,
and demonstrate strong performance on both offline and online web agent
benchmarks such as Mind2Web-Live, Multimodal-Mind2Web, and MiniWob++.
Additionally, our experiments highlight data scaling as a key driver for
improving web agent capabilities. We hope this study makes state-of-the-art
LMM-based agent research at a larger scale more accessible.

</details>


### [477] [Patterns Over Principles: The Fragility of Inductive Reasoning in LLMs under Noisy Observations](https://arxiv.org/pdf/2502.16169)
*Chunyang Li, Weiqi Wang, Tianshi Zheng, Yangqiu Song*

Main category: cs.AI

TL;DR: The paper introduces Robust Rule Induction to test LLMs' ability to infer rules from noisy data, proposing Sample-steered Rule Refinement (SRR) to improve reasoning stability. Results show SRR's effectiveness but highlight LLMs' instability and overreliance on memorized patterns.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored capability of LLMs in maintaining stable rule abstraction under imperfect observations, filling a gap in inductive reasoning research.

Method: Proposes Sample-steered Rule Refinement (SRR), which uses observation diversification and execution-guided feedback to enhance reasoning stability.

Result: SRR outperforms other methods with minimal noise degradation, but LLMs show instability and reliance on memorized patterns rather than genuine abstraction.

Conclusion: The findings challenge LLMs' reasoning robustness, revealing susceptibility to hypothesis drift and overfitting, providing insights for developing human-like inductive systems.

Abstract: Inductive reasoning, a cornerstone of human cognition, enables generalization
from limited data but hasn't yet been fully achieved by large language models
(LLMs). While modern LLMs excel at reasoning tasks, their ability to maintain
stable and consistent rule abstraction under imperfect observations remains
underexplored. To fill this gap, in this work, we introduce Robust Rule
Induction, a task that evaluates LLMs' capability in inferring rules from data
that are fused with noisy examples. To address this task, we further propose
Sample-steered Rule Refinement (SRR), a method enhancing reasoning stability
via observation diversification and execution-guided feedback. Experiments
across arithmetic, cryptography, and list functions reveal: (1) SRR outperforms
other methods with minimal performance degradation under noise; (2) Despite
slight accuracy variation, LLMs exhibit instability under noise (e.g., 0%
accuracy change with only 70% consistent score); (3) Counterfactual task gaps
highlight LLMs' reliance on memorized patterns over genuine abstraction. Our
findings challenge LLMs' reasoning robustness, revealing susceptibility to
hypothesis drift and pattern overfitting, while providing empirical evidence
critical for developing human-like inductive systems. Code and data are
available at https://github.com/HKUST-KnowComp/Robust-Rule-Induction.

</details>


### [478] [Robustness and Cybersecurity in the EU Artificial Intelligence Act](https://arxiv.org/pdf/2502.16184)
*Henrik Nolte, Miriam Rateike, Michèle Finck*

Main category: cs.AI

TL;DR: The paper analyzes the EU AI Act's robustness and cybersecurity provisions for high-risk AI systems and general-purpose AI models, identifying legal challenges and proposing alignment with ML research.


<details>
  <summary>Details</summary>
Motivation: To address the lack of attention on robustness and cybersecurity in the EU AI Act and bridge the gap between legal terminology and ML research.

Method: Identifies legal challenges, assesses implementation issues, and aligns provisions with ML advancements.

Result: Highlights the need for resilience against disruptions and informs standards, guidelines, and benchmarks.

Conclusion: The paper aims to harmonize legal and ML research efforts for better implementation of AI Act provisions.

Abstract: The EU Artificial Intelligence Act (AIA) establishes different legal
principles for different types of AI systems. While prior work has sought to
clarify some of these principles, little attention has been paid to robustness
and cybersecurity. This paper aims to fill this gap. We identify legal
challenges and shortcomings in provisions related to robustness and
cybersecurity for high-risk AI systems(Art. 15 AIA) and general-purpose AI
models (Art. 55 AIA). We show that robustness and cybersecurity demand
resilience against performance disruptions. Furthermore, we assess potential
challenges in implementing these provisions in light of recent advancements in
the machine learning (ML) literature. Our analysis informs efforts to develop
harmonized standards, guidelines by the European Commission, as well as
benchmarks and measurement methodologies under Art. 15(2) AIA. With this, we
seek to bridge the gap between legal terminology and ML research, fostering a
better alignment between research and implementation efforts.

</details>


### [479] [WiseMind: Recontextualizing AI with a Knowledge-Guided, Theory-Informed Multi-Agent Framework for Instrumental and Humanistic Benefits](https://arxiv.org/pdf/2502.20689)
*Yuqi Wu, Guangya Wan, Jingjing Li, Shengming Zhao, Lingfeng Ma, Tianyi Ye, Ion Pop, Yanbo Zhang, Jie Chen*

Main category: cs.AI

TL;DR: WiseMind is a framework for psychiatric diagnosis that combines knowledge-guided reasoning, dual-agent architecture, and multi-faceted evaluation, achieving high accuracy and empathy.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in NLP applications for psychiatric diagnosis due to lack of contextualization in knowledge, processes, and evaluation.

Method: Uses structured knowledge graphs, dual-agent architecture (reasoning and empathy agents), and multi-faceted evaluation (simulated patients, user studies, clinician review, ethical assessment).

Result: Achieves 84.2% diagnostic accuracy (comparable to humans) and outperforms baselines in empathy and trustworthiness.

Conclusion: Deep contextualization in NLP can bridge the gap between benchmarks and clinical impact.

Abstract: Translating state-of-the-art NLP into practice often stalls at the "last
mile" owing to insufficient contextualization of the target domain's knowledge,
processes, and evaluation. Psychiatric differential diagnosis exemplifies this
challenge: accurate assessments depend on nuanced clinical knowledge, a
delicate cognitive-affective interview process, and downstream outcomes that
extend far beyond benchmark accuracy. We present WiseMind, a systematic
interdisciplinary contextualization framework that delivers both instrumental
(diagnostic precision) and humanistic (empathy) gains. WiseMind comprises three
components:(i) structured knowledge-guided proactive reasoning, which embeds
DSM-5 criteria in a knowledge graph to steer questioning; (ii) a
theory-informed dual-agent architecture that coordinates a "reasonable-mind"
reasoning agent and an "emotional-mind" empathy agent, inspired by Dialectical
Behavior Therapy; and (iii) a multi-faceted evaluation strategy covering
simulated patients, user studies, clinician review, and ethical assessment.
Tested on depression, anxiety, and bipolar disorder, WiseMind attains up to
84.2% diagnostic accuracy, which is comparable to human experts, while
outperforming single-agent baselines in perceived empathy and trustworthiness.
These results show that deep contextualization-across knowledge, process, and
evaluation layers-can transform benchmark-driven NLP into clinically meaningful
impact.

</details>


### [480] [Position: Don't Use the CLT in LLM Evals With Fewer Than a Few Hundred Datapoints](https://arxiv.org/pdf/2503.01747)
*Sam Bowyer, Laurence Aitchison, Desi R. Ivanova*

Main category: cs.AI

TL;DR: The paper critiques CLT-based uncertainty quantification for LLM evaluations on small benchmarks, advocating for alternative frequentist and Bayesian methods, and provides a Python library for Bayesian approaches.


<details>
  <summary>Details</summary>
Motivation: Current statistical evaluations of LLMs often rely on CLT, which fails for small, specialized benchmarks, leading to underestimated uncertainty.

Method: The paper demonstrates CLT's inadequacy for small-data settings and recommends alternative frequentist and Bayesian methods.

Result: CLT-based methods perform poorly, underestimating uncertainty, while proposed alternatives are more reliable.

Conclusion: Alternative methods are better suited for small-benchmark LLM evaluations, and a Python library is provided for Bayesian implementation.

Abstract: Rigorous statistical evaluations of large language models (LLMs), including
valid error bars and significance testing, are essential for meaningful and
reliable performance assessment. Currently, when such statistical measures are
reported, they typically rely on the Central Limit Theorem (CLT). In this
position paper, we argue that while CLT-based methods for uncertainty
quantification are appropriate when benchmarks consist of thousands of
examples, they fail to provide adequate uncertainty estimates for LLM
evaluations that rely on smaller, highly specialized benchmarks. In these
small-data settings, we demonstrate that CLT-based methods perform very poorly,
usually dramatically underestimating uncertainty (i.e. producing error bars
that are too small). We give recommendations for alternative frequentist and
Bayesian methods that are both easy to implement and more appropriate in these
increasingly common scenarios. We provide a simple Python library for these
Bayesian methods at https://github.com/sambowyer/bayes_evals .

</details>


### [481] [Tempest: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search](https://arxiv.org/pdf/2503.10619)
*Andy Zhou, Ron Arel*

Main category: cs.AI

TL;DR: Tempest is a multi-turn adversarial framework that erodes LLM safety by exploiting incremental policy leaks through tree search, achieving high success rates on GPT models.


<details>
  <summary>Details</summary>
Motivation: To understand and expose how minor concessions in LLM responses can accumulate into fully disallowed outputs over multiple dialogue turns.

Method: Uses a breadth-first tree search to expand adversarial prompts at each turn, tracking and re-injecting partial compliance from previous responses.

Result: Achieves 100% success on GPT-3.5-turbo and 97% on GPT-4, outperforming baselines like Crescendo and GOAT.

Conclusion: Highlights the need for robust multi-turn testing to safeguard LLMs against gradual policy erosion.

Abstract: We introduce Tempest, a multi-turn adversarial framework that models the
gradual erosion of Large Language Model (LLM) safety through a tree search
perspective. Unlike single-turn jailbreaks that rely on one meticulously
engineered prompt, Tempest expands the conversation at each turn in a
breadth-first fashion, branching out multiple adversarial prompts that exploit
partial compliance from previous responses. By tracking these incremental
policy leaks and re-injecting them into subsequent queries, Tempest reveals how
minor concessions can accumulate into fully disallowed outputs. Evaluations on
the JailbreakBench dataset show that Tempest achieves a 100% success rate on
GPT-3.5-turbo and 97% on GPT-4 in a single multi-turn run, using fewer queries
than baselines such as Crescendo or GOAT. This tree search methodology offers
an in-depth view of how model safeguards degrade over successive dialogue
turns, underscoring the urgency of robust multi-turn testing procedures for
language models.

</details>


### [482] [Agent-Centric Personalized Multiple Clustering with Multi-Modal LLMs](https://arxiv.org/pdf/2503.22241)
*Ziye Chen, Yiqun Duan, Riheng Zhu, Zhenbang Sun, Mingming Gong*

Main category: cs.AI

TL;DR: Proposes an agent-centric framework using MLLMs for personalized clustering, outperforming CLIP-based methods with significant NMI score improvements.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations of CLIP-based methods in understanding user interests by leveraging MLLMs for deeper contextual analysis.

Method: Uses MLLMs to traverse a relational graph for clustering, optimizing paths with user-interest-biased embeddings to reduce computation.

Result: Achieves NMI scores of 0.9667 and 0.9481 on benchmarks, improving SOTA by over 140%.

Conclusion: The framework effectively aligns clusters with user preferences, demonstrating superior performance over existing methods.

Abstract: Personalized multiple clustering aims to generate diverse partitions of a
dataset based on different user-specific aspects, rather than a single
clustering. It has recently drawn research interest for accommodating varying
user preferences. Recent approaches primarily use CLIP embeddings with proxy
learning to extract representations biased toward user clustering preferences.
However, CLIP primarily focuses on coarse image-text alignment, lacking a deep
contextual understanding of user interests. To overcome these limitations, we
propose an agent-centric personalized clustering framework that leverages
multi-modal large language models (MLLMs) as agents to comprehensively traverse
a relational graph to search for clusters based on user interests. Due to the
advanced reasoning mechanism of MLLMs, the obtained clusters align more closely
with user-defined criteria than those obtained from CLIP-based representations.
To reduce computational overhead, we shorten the agents' traversal path by
constructing a relational graph using user-interest-biased embeddings extracted
by MLLMs. A large number of weakly connected edges can be filtered out based on
embedding similarity, facilitating an efficient traversal search for agents.
Experimental results show that the proposed method achieves NMI scores of
0.9667 and 0.9481 on the Card Order and Card Suits benchmarks, respectively,
largely improving the SOTA model by over 140%.

</details>


### [483] [Intrinsically-Motivated Humans and Agents in Open-World Exploration](https://arxiv.org/pdf/2503.23631)
*Aly Lidayan, Yuqing Du, Eliza Kosoy, Maria Rufova, Pieter Abbeel, Alison Gopnik*

Main category: cs.AI

TL;DR: The paper compares human and AI exploration in the Crafter environment, finding Entropy and Empowerment as key intrinsic motivators.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between human and AI exploration by understanding intrinsic motivation.

Method: Direct comparison of adults, children, and AI agents in Crafter, analyzing Entropy, Information Gain, and Empowerment.

Result: Entropy and Empowerment correlate with human exploration; Entropy plateaus while Empowerment increases continuously.

Conclusion: Entropy and Empowerment better inform intrinsic rewards; state diversity aids early exploration, while control is key later.

Abstract: What drives exploration? Understanding intrinsic motivation is a
long-standing challenge in both cognitive science and artificial intelligence;
numerous objectives have been proposed and used to train agents, yet there
remains a gap between human and agent exploration. We directly compare adults,
children, and AI agents in a complex open-ended environment, Crafter, and study
how common intrinsic objectives: Entropy, Information Gain, and Empowerment,
relate to their behavior. We find that only Entropy and Empowerment are
consistently positively correlated with human exploration progress, indicating
that these objectives may better inform intrinsic reward design for agents.
Furthermore, across agents and humans we observe that Entropy initially
increases rapidly, then plateaus, while Empowerment increases continuously,
suggesting that state diversity may provide more signal in early exploration,
while advanced exploration should prioritize control. Finally, we find
preliminary evidence that private speech utterances, and particularly goal
verbalizations, may aid exploration in children. Our data is available at
https://github.com/alyd/humans_in_crafter_data.

</details>


### [484] [AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges](https://arxiv.org/pdf/2505.10468)
*Ranjan Sapkota, Konstantinos I. Roumeliotis, Manoj Karkee*

Main category: cs.AI

TL;DR: The paper distinguishes AI Agents from Agentic AI, providing a taxonomy, application mapping, and challenge analysis. It highlights AI Agents as task-specific systems and Agentic AI as a paradigm shift with advanced capabilities like multi-agent collaboration.


<details>
  <summary>Details</summary>
Motivation: To clarify the differences between AI Agents and Agentic AI, their design philosophies, and capabilities, aiding in the development of robust and scalable AI systems.

Method: The study uses a structured approach, including conceptual taxonomy, application mapping, and challenge analysis, comparing architectural evolution, operational mechanisms, and autonomy levels.

Result: A comparative analysis of both paradigms is presented, along with application domains and challenges like hallucination and coordination failure, proposing solutions such as ReAct loops and RAG.

Conclusion: The paper provides a roadmap for developing robust, scalable, and explainable AI agent and Agentic AI-driven systems.

Abstract: This study critically distinguishes between AI Agents and Agentic AI,
offering a structured conceptual taxonomy, application mapping, and challenge
analysis to clarify their divergent design philosophies and capabilities. We
begin by outlining the search strategy and foundational definitions,
characterizing AI Agents as modular systems driven by Large Language Models
(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.
Generative AI is positioned as a precursor, with AI Agents advancing through
tool integration, prompt engineering, and reasoning enhancements. In contrast,
Agentic AI systems represent a paradigmatic shift marked by multi-agent
collaboration, dynamic task decomposition, persistent memory, and orchestrated
autonomy. Through a sequential evaluation of architectural evolution,
operational mechanisms, interaction styles, and autonomy levels, we present a
comparative analysis across both paradigms. Application domains such as
customer support, scheduling, and data summarization are contrasted with
Agentic AI deployments in research automation, robotic coordination, and
medical decision support. We further examine unique challenges in each paradigm
including hallucination, brittleness, emergent behavior, and coordination
failure and propose targeted solutions such as ReAct loops, RAG, orchestration
layers, and causal modeling. This work aims to provide a definitive roadmap for
developing robust, scalable, and explainable AI agent and Agentic AI-driven
systems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision
Support System, Agentic-AI Applications

</details>


### [485] [Solver-Informed RL: Grounding Large Language Models for Authentic Optimization Modeling](https://arxiv.org/pdf/2505.11792)
*Yitian Chen, Jingfan Xia, Siyu Shao, Dongdong Ge, Yinyu Ye*

Main category: cs.AI

TL;DR: SIRL uses RL with verifiable rewards from optimization solvers to improve LLMs' accuracy in generating optimization models, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce incorrect or unusable optimization models; SIRL aims to enhance their reliability using solver-verified feedback.

Method: SIRL employs Reinforcement Learning with verifiable rewards from external solvers, assessing syntax, feasibility, and solution quality.

Result: SIRL achieves state-of-the-art performance on public benchmarks, generating more accurate and executable models.

Conclusion: SIRL significantly improves LLM-generated optimization models, offering a reliable automation framework.

Abstract: Optimization modeling is fundamental to decision-making across diverse
domains. Despite progress in automating optimization formulation from natural
language descriptions, Large Language Models (LLMs) often struggle to generate
formally correct and usable models against hallucinations, posing a challenge
for reliable automation. Inspired by the success of Reinforcement Learning (RL)
in enhancing Large Reasoning Models, we present Solver-Informed Reinforcement
Learning (SIRL), a novel framework that significantly improves the authenticity
of LLMs for optimization modeling using Reinforcement Learning with Verifiable
Reward by leveraging external optimization solvers as verifiers. These
verifiers automatically assess the executable code and the instance-level
mathematical model represented by the associated LP file, yielding precise and
comprehensive feedback signals -- including syntax, feasibility, and solution
quality, serving as direct rewards for the RL process. This automated
verification process, particularly from classic optimization solvers, also
underpins our instance-enhanced self-consistency method to synthesize
high-quality training data. Extensive experiments on diverse public benchmarks
demonstrate that SIRL achieves state-of-the-art performance, substantially
outperforming existing methods in generating accurate and executable
optimization models. Our code is publicly available at
https://github.com/Cardinal-Operations/SIRL.

</details>


### [486] [BACON: A fully explainable AI model with graded logic for decision making problems](https://arxiv.org/pdf/2505.14510)
*Haishi Bai, Jozo Dujmovic, Jianwu Wang*

Main category: cs.AI

TL;DR: BACON is a framework for training explainable AI models using graded logic, achieving high accuracy and transparency for human-AI collaboration.


<details>
  <summary>Details</summary>
Motivation: The need for transparent and trustworthy AI explanations in high-stakes domains like healthcare and finance drives the development of BACON.

Method: BACON uses graded logic to train explainable models, tested on scenarios like Boolean approximation, Iris classification, and medical diagnosis.

Result: BACON delivers high-performance models with compact, human-verifiable decision logic across diverse applications.

Conclusion: BACON is a practical and principled approach for trustworthy explainable AI.

Abstract: As machine learning models and autonomous agents are increasingly deployed in
high-stakes, real-world domains such as healthcare, security, finance, and
robotics, the need for transparent and trustworthy explanations has become
critical. To ensure end-to-end transparency of AI decisions, we need models
that are not only accurate but also fully explainable and human-tunable. We
introduce BACON, a novel framework for automatically training explainable AI
models for decision making problems using graded logic. BACON achieves high
predictive accuracy while offering full structural transparency and precise,
logic-based symbolic explanations, enabling effective human-AI collaboration
and expert-guided refinement. We evaluate BACON with a diverse set of
scenarios: classic Boolean approximation, Iris flower classification, house
purchasing decisions and breast cancer diagnosis. In each case, BACON provides
high-performance models while producing compact, human-verifiable decision
logic. These results demonstrate BACON's potential as a practical and
principled approach for delivering crisp, trustworthy explainable AI.

</details>


### [487] [From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Reasoning-Driven Pedagogical Visualization](https://arxiv.org/pdf/2505.16832)
*Haonian Ji, Shi Qiu, Siyang Xin, Siwei Han, Zhaorun Chen, Dake Zhang, Hongyi Wang, Huaxiu Yao*

Main category: cs.AI

TL;DR: EduVisBench is introduced to evaluate FMs' visual reasoning in education, revealing their limitations. EduVisAgent, a multi-agent framework, improves performance by 40.2%.


<details>
  <summary>Details</summary>
Motivation: Existing FMs lack pedagogically effective visual explanations, focusing too much on text. EduVisBench addresses this gap.

Method: EduVisBench is a multi-domain benchmark with STEM problem sets. EduVisAgent uses specialized agents for visual reasoning.

Result: Existing models struggle with visual reasoning. EduVisAgent outperforms baselines by 40.2%.

Conclusion: EduVisAgent and EduVisBench advance FMs' educational visual reasoning capabilities.

Abstract: While foundation models (FMs), such as diffusion models and large
vision-language models (LVLMs), have been widely applied in educational
contexts, their ability to generate pedagogically effective visual explanations
remains limited. Most existing approaches focus primarily on textual reasoning,
overlooking the critical role of structured and interpretable visualizations in
supporting conceptual understanding. To better assess the visual reasoning
capabilities of FMs in educational settings, we introduce EduVisBench, a
multi-domain, multi-level benchmark. EduVisBench features diverse STEM problem
sets requiring visually grounded solutions, along with a fine-grained
evaluation rubric informed by pedagogical theory. Our empirical analysis
reveals that existing models frequently struggle with the inherent challenge of
decomposing complex reasoning and translating it into visual representations
aligned with human cognitive processes. To address these limitations, we
propose EduVisAgent, a multi-agent collaborative framework that coordinates
specialized agents for instructional planning, reasoning decomposition,
metacognitive prompting, and visualization design. Experimental results show
that EduVisAgent substantially outperforms all baselines, achieving a 40.2%
improvement and delivering more educationally aligned visualizations.
EduVisBench and EduVisAgent are available at
https://github.com/aiming-lab/EduVisBench and
https://github.com/aiming-lab/EduVisAgent.

</details>


### [488] [Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems](https://arxiv.org/pdf/2505.18139)
*Gordon Dai, Yunze Xiao*

Main category: cs.AI

TL;DR: The paper argues that theoretical inconsistencies in Responsible AI (RAI) metrics should be embraced as beneficial, offering normative pluralism, epistemological completeness, and implicit regularization.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the common perception of inconsistency in RAI metrics as a flaw, proposing it as a feature that better represents diverse values and ethical concepts.

Method: The method involves treating conflicting metrics as divergent objectives, analyzing their benefits, and advocating for characterizing acceptable inconsistency thresholds.

Result: The result highlights three key benefits: normative pluralism, epistemological completeness, and implicit regularization, which enhance model robustness and ethical representation.

Conclusion: The conclusion advocates for a shift in RAI practice to embrace and manage inconsistency rather than eliminate it, preserving value diversity and model performance.

Abstract: This position paper argues that the theoretical inconsistency often observed
among Responsible AI (RAI) metrics, such as differing fairness definitions or
tradeoffs between accuracy and privacy, should be embraced as a valuable
feature rather than a flaw to be eliminated. We contend that navigating these
inconsistencies, by treating metrics as divergent objectives, yields three key
benefits: (1) Normative Pluralism: Maintaining a full suite of potentially
contradictory metrics ensures that the diverse moral stances and stakeholder
values inherent in RAI are adequately represented. (2) Epistemological
Completeness: The use of multiple, sometimes conflicting, metrics allows for a
more comprehensive capture of multifaceted ethical concepts, thereby preserving
greater informational fidelity about these concepts than any single, simplified
definition. (3) Implicit Regularization: Jointly optimizing for theoretically
conflicting objectives discourages overfitting to one specific metric, steering
models towards solutions with enhanced generalization and robustness under
real-world complexities. In contrast, efforts to enforce theoretical
consistency by simplifying or pruning metrics risk narrowing this value
diversity, losing conceptual depth, and degrading model performance. We
therefore advocate for a shift in RAI theory and practice: from getting trapped
in inconsistency to characterizing acceptable inconsistency thresholds and
elucidating the mechanisms that permit robust, approximated consistency in
practice.

</details>


### [489] [SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics Reasoning](https://arxiv.org/pdf/2505.19099)
*Kun Xiang, Heng Li, Terry Jingchen Zhang, Yinya Huang, Zirong Liu, Peixin Qu, Jixi He, Jiaqi Chen, Yu-Jie Yuan, Jianhua Han, Hang Xu, Hanhui Li, Mrinmaya Sachan, Xiaodan Liang*

Main category: cs.AI

TL;DR: SeePhys is a multimodal benchmark for LLM reasoning in physics, featuring vision-essential problems. Advanced models struggle with accuracy, highlighting challenges in visual understanding and reliance on textual cues.


<details>
  <summary>Details</summary>
Motivation: To address the lack of benchmarks for LLM reasoning in physics that require visual information extraction, highlighting gaps in current models' capabilities.

Method: Developed a large-scale benchmark with 7 physics domains and 21 diagram categories, emphasizing vision-essential problems (75%). Evaluated advanced models like Gemini-2.5-pro and o4-mini.

Result: Sub-60% accuracy for top models, revealing challenges in diagram-physics coupling and over-reliance on textual cues.

Conclusion: SeePhys exposes critical limitations in LLMs' visual reasoning, urging improvements in multimodal understanding for physics applications.

Abstract: We present SeePhys, a large-scale multimodal benchmark for LLM reasoning
grounded in physics questions ranging from middle school to PhD qualifying
exams. The benchmark covers 7 fundamental domains spanning the physics
discipline, incorporating 21 categories of highly heterogeneous diagrams. In
contrast to prior works where visual elements mainly serve auxiliary purposes,
our benchmark features a substantial proportion of vision-essential problems
(75%) that mandate visual information extraction for correct solutions. Through
extensive evaluation, we observe that even the most advanced visual reasoning
models (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60% accuracy on our
benchmark. These results reveal fundamental challenges in current large
language models' visual understanding capabilities, particularly in: (i)
establishing rigorous coupling between diagram interpretation and physics
reasoning, and (ii) overcoming their persistent reliance on textual cues as
cognitive shortcuts.

</details>


### [490] [SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond](https://arxiv.org/pdf/2505.19641)
*Junteng Liu, Yuanxiang Fan, Zhuo Jiang, Han Ding, Yongyi Hu, Chi Zhang, Yiqi Shi, Shitong Weng, Aili Chen, Shiqi Chen, Yunan Huang, Mozhi Zhang, Pengyu Zhao, Junjie Yan, Junxian He*

Main category: cs.AI

TL;DR: SynLogic is a framework for synthesizing diverse logical reasoning data, enhancing LLMs' reasoning abilities through RL training, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: To address the gap in resources for developing general reasoning capabilities in LLMs, focusing on logical reasoning as a fundamental building block.

Method: Introduces SynLogic, a data synthesis framework generating 35 diverse logical reasoning tasks with adjustable difficulty and verifiable examples.

Result: SynLogic achieves state-of-the-art logical reasoning performance, surpassing DeepSeek-R1-Distill-Qwen-32B by 6 points on BBEH, and improves generalization when mixed with other tasks.

Conclusion: SynLogic is a valuable resource for advancing LLMs' reasoning capabilities, with open-sourced data and pipeline.

Abstract: Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the
potential of Reinforcement Learning (RL) to enhance reasoning abilities in
Large Language Models (LLMs). While open-source replication efforts have
primarily focused on mathematical and coding domains, methods and resources for
developing general reasoning capabilities remain underexplored. This gap is
partly due to the challenge of collecting diverse and verifiable reasoning data
suitable for RL. We hypothesize that logical reasoning is critical for
developing general reasoning capabilities, as logic forms a fundamental
building block of reasoning. In this work, we present SynLogic, a data
synthesis framework and dataset that generates diverse logical reasoning data
at scale, encompassing 35 diverse logical reasoning tasks. The SynLogic
approach enables controlled synthesis of data with adjustable difficulty and
quantity. Importantly, all examples can be verified by simple rules, making
them ideally suited for RL with verifiable rewards. In our experiments, we
validate the effectiveness of RL training on the SynLogic dataset based on 7B
and 32B models. SynLogic leads to state-of-the-art logical reasoning
performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B
by 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and
coding tasks improves the training efficiency of these domains and
significantly enhances reasoning generalization. Notably, our mixed training
model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These
findings position SynLogic as a valuable resource for advancing the broader
reasoning capabilities of LLMs. We open-source both the data synthesis pipeline
and the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic.

</details>


### [491] [Diagnosing and Resolving Cloud Platform Instability with Multi-modal RAG LLMs](https://arxiv.org/pdf/2505.21419)
*Yifan Wang, Kenneth P. Birman*

Main category: cs.AI

TL;DR: ARCA, a multi-modal RAG LLM system, simplifies identifying and resolving performance issues in cloud applications by combining AI pattern matching with a natural interface, outperforming existing solutions.


<details>
  <summary>Details</summary>
Motivation: Addressing the complexity of diagnosing performance or functional instabilities in cloud-hosted applications with numerous potential root causes.

Method: Combines AI pattern matching and a multi-modal RAG LLM interface (ARCA) for problem identification and resolution.

Result: ARCA outperforms state-of-the-art alternatives in step-wise evaluations.

Conclusion: ARCA effectively simplifies and improves the process of diagnosing and resolving issues in cloud applications.

Abstract: Today's cloud-hosted applications and services are complex systems, and a
performance or functional instability can have dozens or hundreds of potential
root causes. Our hypothesis is that by combining the pattern matching
capabilities of modern AI tools with a natural multi-modal RAG LLM interface,
problem identification and resolution can be simplified. ARCA is a new
multi-modal RAG LLM system that targets this domain. Step-wise evaluations show
that ARCA outperforms state-of-the-art alternatives.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [492] [VoiceMark: Zero-Shot Voice Cloning-Resistant Watermarking Approach Leveraging Speaker-Specific Latents](https://arxiv.org/pdf/2505.21568)
*Haiyun Li, Zhiyong Wu, Xiaofeng Xie, Jingran Xie, Yaoxun Xu, Hanyang Peng*

Main category: cs.SD

TL;DR: VoiceMark is a zero-shot VC-resistant watermarking method using speaker-specific latents, achieving 95% detection accuracy, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking fails in zero-shot VC scenarios, necessitating a robust solution.

Method: Uses speaker-specific latents as watermark carriers, VC-simulated augmentations, and VAD-based loss for robustness.

Result: 95% accuracy in watermark detection post zero-shot VC synthesis, surpassing existing methods (~50%).

Conclusion: VoiceMark effectively addresses zero-shot VC watermarking challenges with high detection accuracy.

Abstract: Voice cloning (VC)-resistant watermarking is an emerging technique for
tracing and preventing unauthorized cloning. Existing methods effectively trace
traditional VC models by training them on watermarked audio but fail in
zero-shot VC scenarios, where models synthesize audio from an audio prompt
without training. To address this, we propose VoiceMark, the first zero-shot
VC-resistant watermarking method that leverages speaker-specific latents as the
watermark carrier, allowing the watermark to transfer through the zero-shot VC
process into the synthesized audio. Additionally, we introduce VC-simulated
augmentations and VAD-based loss to enhance robustness against distortions.
Experiments on multiple zero-shot VC models demonstrate that VoiceMark achieves
over 95% accuracy in watermark detection after zero-shot VC synthesis,
significantly outperforming existing methods, which only reach around 50%. See
our code and demos at: https://huggingface.co/spaces/haiyunli/VoiceMark

</details>


### [493] [An Investigation on Speaker Augmentation for End-to-End Speaker Extraction](https://arxiv.org/pdf/2505.21805)
*Zhenghai You, Zhenyu Zhou, Lantian Li, Dong Wang*

Main category: cs.SD

TL;DR: The paper addresses target confusion in end-to-end speaker extraction (E2E-SE) systems by proposing a speaker augmentation strategy to improve speaker embedding generalizability and discrimination.


<details>
  <summary>Details</summary>
Motivation: Target confusion, where systems switch to non-target speakers, is a key challenge in E2E-SE due to inadequate speaker embeddings.

Method: A time-domain resampling and rescaling pipeline is introduced to alter speaker traits while preserving other speech properties, generating pseudo-speakers for better embedding space.

Result: Experiments on WSJ0-2Mix and LibriMix show reduced target confusion and improved extraction performance, with further gains when combined with metric learning.

Conclusion: The proposed speaker augmentation strategy effectively mitigates target confusion and enhances E2E-SE performance, with potential for synergy with other methods like metric learning.

Abstract: Target confusion, defined as occasional switching to non-target speakers,
poses a key challenge for end-to-end speaker extraction (E2E-SE) systems. We
argue that this problem is largely caused by the lack of generalizability and
discrimination of the speaker embeddings, and introduce a simple yet effective
speaker augmentation strategy to tackle the problem. Specifically, we propose a
time-domain resampling and rescaling pipeline that alters speaker traits while
preserving other speech properties. This generates a variety of pseudo-speakers
to help establish a generalizable speaker embedding space, while the
speaker-trait-specific augmentation creates hard samples that force the model
to focus on genuine speaker characteristics. Experiments on WSJ0-2Mix and
LibriMix show that our method mitigates the target confusion and improves
extraction performance. Moreover, it can be combined with metric learning,
another effective approach to address target confusion, leading to further
gains.

</details>


### [494] [Voice Quality Dimensions as Interpretable Primitives for Speaking Style for Atypical Speech and Affect](https://arxiv.org/pdf/2505.21809)
*Jaya Narain, Vasudha Kowtha, Colin Lea, Lauren Tooley, Dianna Yee, Vikramjit Mitra, Zifang Huang, Miquel Espi Marques, Jon Huang, Carlos Avendano, Shirley Ren*

Main category: cs.SD

TL;DR: The paper develops and evaluates voice quality models for seven speech dimensions, achieving strong performance and generalization, validated by zero-shot tests on diverse datasets.


<details>
  <summary>Details</summary>
Motivation: To describe key characteristics of atypical speech and other speech modulations using perceptual voice quality dimensions.

Method: Probes were trained on the SAP dataset (11,184 samples from 434 speakers) using embeddings from frozen pre-trained models as features.

Result: Probes showed strong performance and generalization across speech categories, with validated zero-shot performance on Italian and English atypical speech, and affective speech datasets.

Conclusion: Voice quality dimensions are useful for speaking style-related tasks, supported by interpretable results and strong zero-shot performance.

Abstract: Perceptual voice quality dimensions describe key characteristics of atypical
speech and other speech modulations. Here we develop and evaluate voice quality
models for seven voice and speech dimensions (intelligibility, imprecise
consonants, harsh voice, naturalness, monoloudness, monopitch, and
breathiness). Probes were trained on the public Speech Accessibility (SAP)
project dataset with 11,184 samples from 434 speakers, using embeddings from
frozen pre-trained models as features. We found that our probes had both strong
performance and strong generalization across speech elicitation categories in
the SAP dataset. We further validated zero-shot performance on additional
datasets, encompassing unseen languages and tasks: Italian atypical speech,
English atypical speech, and affective speech. The strong zero-shot performance
and the interpretability of results across an array of evaluations suggests the
utility of using voice quality dimensions in speaking style-related tasks.

</details>


### [495] [Music Source Restoration](https://arxiv.org/pdf/2505.21827)
*Yongyi Zang, Zheqi Dai, Mark D. Plumbley, Qiuqiang Kong*

Main category: cs.SD

TL;DR: The paper introduces Music Source Restoration (MSR), a task to recover original music signals from degraded mixtures, and presents RawStems, a dataset of unprocessed music stems with hierarchical categories.


<details>
  <summary>Details</summary>
Motivation: Current Music Source Separation (MSS) methods overlook real-world signal degradations in music production, creating a gap between idealized separation and practical applications.

Method: MSR models mixtures as degraded sums of individually degraded sources, using the RawStems dataset and U-Former as a baseline method.

Result: The feasibility of MSR is demonstrated, and the RawStems dataset, degradation pipeline, and models are made publicly available.

Conclusion: MSR addresses real-world music production challenges, and RawStems provides a valuable resource for future research in this area.

Abstract: We introduce Music Source Restoration (MSR), a novel task addressing the gap
between idealized source separation and real-world music production. Current
Music Source Separation (MSS) approaches assume mixtures are simple sums of
sources, ignoring signal degradations employed during music production like
equalization, compression, and reverb. MSR models mixtures as degraded sums of
individually degraded sources, with the goal of recovering original, undegraded
signals. Due to the lack of data for MSR, we present RawStems, a dataset
annotation of 578 songs with unprocessed source signals organized into 8
primary and 17 secondary instrument groups, totaling 354.13 hours. To the best
of our knowledge, RawStems is the first dataset that contains unprocessed music
stems with hierarchical categories. We consider spectral filtering, dynamic
range compression, harmonic distortion, reverb and lossy codec as possible
degradations, and establish U-Former as a baseline method, demonstrating the
feasibility of MSR on our dataset. We release the RawStems dataset annotations,
degradation simulation pipeline, training code and pre-trained models to be
publicly available.

</details>


### [496] [AudioGenie: A Training-Free Multi-Agent Framework for Diverse Multimodality-to-Multiaudio Generation](https://arxiv.org/pdf/2505.22053)
*Yan Rong, Jinting Wang, Shan Yang, Guangzhi Lei, Li Liu*

Main category: cs.SD

TL;DR: AudioGenie is a multi-agent system for MM2MA generation, addressing challenges like multimodal understanding and diverse audio synthesis. It outperforms SOTA methods and includes a benchmark (MA-Bench).


<details>
  <summary>Details</summary>
Motivation: Challenges in MM2MA generation include poor multimodal understanding, single-model limitations, and lack of self-correction. Multi-agent systems show promise but face specific hurdles.

Method: Proposes AudioGenie, a dual-layer multi-agent system with generation and supervisor teams. Uses fine-grained task decomposition, MoE, and iterative refinement for dynamic model selection and self-correction.

Result: Outperforms SOTA methods across 9 metrics in 8 tasks. User study confirms quality, accuracy, alignment, and aesthetic improvements.

Conclusion: AudioGenie effectively addresses MM2MA challenges, validated by benchmarks and user studies. The project website provides samples.

Abstract: Multimodality-to-Multiaudio (MM2MA) generation faces significant challenges
in synthesizing diverse and contextually aligned audio types (e.g., sound
effects, speech, music, and songs) from multimodal inputs (e.g., video, text,
images), owing to the scarcity of high-quality paired datasets and the lack of
robust multi-task learning frameworks. Recently, multi-agent system shows great
potential in tackling the above issues. However, directly applying it to MM2MA
task presents three critical challenges: (1) inadequate fine-grained
understanding of multimodal inputs (especially for video), (2) the inability of
single models to handle diverse audio events, and (3) the absence of
self-correction mechanisms for reliable outputs. To this end, we propose
AudioGenie, a novel training-free multi-agent system featuring a dual-layer
architecture with a generation team and a supervisor team. For the generation
team, a fine-grained task decomposition and an adaptive Mixture-of-Experts
(MoE) collaborative entity are designed for dynamic model selection, and a
trial-and-error iterative refinement module is designed for self-correction.
The supervisor team ensures temporal-spatial consistency and verifies outputs
through feedback loops. Moreover, we build MA-Bench, the first benchmark for
MM2MA tasks, comprising 198 annotated videos with multi-type audios.
Experiments demonstrate that our AudioGenie outperforms state-of-the-art (SOTA)
methods across 9 metrics in 8 tasks. User study further validate the
effectiveness of the proposed method in terms of quality, accuracy, alignment,
and aesthetic. The anonymous project website with samples can be found at
https://audiogenie.github.io/.

</details>


### [497] [Leveraging LLM for Stuttering Speech: A Unified Architecture Bridging Recognition and Event Detection](https://arxiv.org/pdf/2505.22005)
*Shangkun Huang, Jing Deng, Jintao Kang, Rong Zheng*

Main category: cs.SD

TL;DR: Proposed an LLM-driven ASR-SED multi-task framework for stuttering speech, reducing ASR error by 37.71% and improving SED F1-score by 46.58%.


<details>
  <summary>Details</summary>
Motivation: Address the performance bottleneck of ASR in stuttering speech, limiting its use in speech rehabilitation.

Method: LLM-driven multi-task learning with dynamic interaction, contrastive learning, and Focal Loss for ASR and SED tasks.

Result: Reduced ASR CER to 5.45% (-37.71%) and achieved SED F1-score of 73.63% (+46.58%).

Conclusion: The framework effectively improves ASR and SED performance for stuttering speech.

Abstract: The performance bottleneck of Automatic Speech Recognition (ASR) in
stuttering speech scenarios has limited its applicability in domains such as
speech rehabilitation. This paper proposed an LLM-driven ASR-SED multi-task
learning framework that jointly optimized the ASR and Stuttering Event
Detection (SED) tasks. We proposed a dynamic interaction mechanism where the
ASR branch leveraged CTC-generated soft prompts to assist LLM context modeling,
while the SED branch output stutter embeddings to enhance LLM comprehension of
stuttered speech. We incorporated contrastive learning to strengthen the
discriminative power of stuttering acoustic features and applied Focal Loss to
mitigate the long-tailed distribution in stuttering event categories.
Evaluations on the AS-70 Mandarin stuttering dataset demonstrated that our
framework reduced the ASR character error rate (CER) to 5.45% (-37.71% relative
reduction) and achieved an average SED F1-score of 73.63% (+46.58% relative
improvement).

</details>


### [498] [FGS-Audio: Fixed-Decoder Framework for Audio Steganography with Adversarial Perturbation Generation](https://arxiv.org/pdf/2505.22266)
*Jialin Yan, Yu Cheng, Zhaoxia Yin, Xinpeng Zhang, Shilin Wang, Tanfeng Sun, Xinghao Jiang*

Main category: cs.SD

TL;DR: The paper introduces FGS-Audio, a fixed-decoder framework for audio steganography using adversarial perturbations, eliminating reliance on large pre-trained models and improving security and audio quality.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current audio steganography methods, which rely on complex training workflows and extensive pre-trained models, by proposing a simpler yet effective framework.

Method: FGS-Audio embeds adversarial perturbations carrying secret information into cover audio. It uses a lightweight fixed decoder for extraction and optimizes perturbations to maintain audio quality and resist steganalysis.

Result: The method outperforms SOTA approaches in anti-steganalysis and achieves a 10 dB PSNR improvement in stego audio quality.

Conclusion: FGS-Audio offers a robust, efficient, and high-quality solution for audio steganography, reducing dependency on pre-trained models.

Abstract: The rapid development of Artificial Intelligence Generated Content (AIGC) has
made high-fidelity generated audio widely available across the Internet,
offering an abundant and versatile source of cover signals for covert
communication. Driven by advances in deep learning, current audio steganography
frameworks are mainly based on encoding-decoding network architectures. While
these methods greatly improve the security of audio steganography, they
typically employ elaborate training workflows and rely on extensive pre-trained
models. To address the aforementioned issues, this paper pioneers a
Fixed-Decoder Framework for Audio Steganography with Adversarial Perturbation
Generation (FGS-Audio). The adversarial perturbations that carry secret
information are embedded into cover audio to generate stego audio. The receiver
only needs to share the structure and weights of the fixed decoding network to
accurately extract the secret information from the stego audio, thus
eliminating the reliance on large pre-trained models. In FGS-Audio, we propose
an audio Adversarial Perturbation Generation (APG) strategy and design a
lightweight fixed decoder. The fixed decoder guarantees reliable extraction of
the hidden message, while the adversarial perturbations are optimized to keep
the stego audio perceptually and statistically close to the cover audio,
thereby improving resistance to steganalysis. The experimental results show
that the method exhibits excellent anti-steganalysis performance under
different relative payloads, outperforming existing SOTA approaches. In terms
of stego audio quality, FGS-Audio achieves an average PSNR improvement of over
10 dB compared to SOTA method.

</details>


### [499] [Overlap-Adaptive Hybrid Speaker Diarization and ASR-Aware Observation Addition for MISP 2025 Challenge](https://arxiv.org/pdf/2505.22013)
*Shangkun Huang, Yuxuan Du, Jingwen Yang, Dejun Zhang, Xupeng Jia, Jing Deng, Jintao Kang, Rong Zheng*

Main category: cs.SD

TL;DR: A hybrid diarization system combining WavLM and clustering, and an ASR-aware method for GSS limitations, achieved top results in MISP 2025.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of overlapping speech and low SNR in meeting scenarios for the MISP 2025 Challenge.

Method: Hybrid diarization (WavLM + clustering) and ASR-aware observation addition for GSS. Integrated cascaded architecture for Track 3.

Result: CER of 9.48% (Track 2) and cpCER of 11.56% (Track 3), securing first place in both.

Conclusion: The proposed methods are effective for real-world meeting scenarios, as demonstrated by top performance in the challenge.

Abstract: This paper presents the system developed to address the MISP 2025 Challenge.
For the diarization system, we proposed a hybrid approach combining a WavLM
end-to-end segmentation method with a traditional multi-module clustering
technique to adaptively select the appropriate model for handling varying
degrees of overlapping speech. For the automatic speech recognition (ASR)
system, we proposed an ASR-aware observation addition method that compensates
for the performance limitations of Guided Source Separation (GSS) under low
signal-to-noise ratio conditions. Finally, we integrated the speaker
diarization and ASR systems in a cascaded architecture to address Track 3. Our
system achieved character error rates (CER) of 9.48% on Track 2 and
concatenated minimum permutation character error rate (cpCER) of 11.56% on
Track 3, ultimately securing first place in both tracks and thereby
demonstrating the effectiveness of the proposed methods in real-world meeting
scenarios.

</details>


### [500] [RESOUND: Speech Reconstruction from Silent Videos via Acoustic-Semantic Decomposed Modeling](https://arxiv.org/pdf/2505.22024)
*Long-Khanh Pham, Thanh V. T. Tran, Minh-Tan Pham, Van Nguyen*

Main category: cs.SD

TL;DR: RESOUND is a novel lip-to-speech (L2S) system that improves accuracy and naturalness by separating prosody and linguistic feature learning, integrating speech units for better waveform generation.


<details>
  <summary>Details</summary>
Motivation: Current L2S synthesis struggles with accuracy and naturalness due to limited supervision in capturing linguistic content, accents, and prosody.

Method: RESOUND uses source-filter theory with two paths: an acoustic path for prosody and a semantic path for linguistic features. It integrates speech units and mel-spectrograms for waveform generation.

Result: Experiments on standard benchmarks show RESOUND's effectiveness in synthesizing intelligible and expressive speech.

Conclusion: RESOUND successfully addresses L2S challenges by simplifying learning and enhancing performance through separated representations and speech unit integration.

Abstract: Lip-to-speech (L2S) synthesis, which reconstructs speech from visual cues,
faces challenges in accuracy and naturalness due to limited supervision in
capturing linguistic content, accents, and prosody. In this paper, we propose
RESOUND, a novel L2S system that generates intelligible and expressive speech
from silent talking face videos. Leveraging source-filter theory, our method
involves two components: an acoustic path to predict prosody and a semantic
path to extract linguistic features. This separation simplifies learning,
allowing independent optimization of each representation. Additionally, we
enhance performance by integrating speech units, a proven unsupervised speech
representation technique, into waveform generation alongside mel-spectrograms.
This allows RESOUND to synthesize prosodic speech while preserving content and
speaker identity. Experiments conducted on two standard L2S benchmarks confirm
the effectiveness of the proposed method across various metrics.

</details>


### [501] [Improving Respiratory Sound Classification with Architecture-Agnostic Knowledge Distillation from Ensembles](https://arxiv.org/pdf/2505.22027)
*Miika Toikkanen, June-Woo Kim*

Main category: cs.SD

TL;DR: Soft label training with knowledge distillation improves respiratory sound classification, achieving state-of-the-art results with minimal compute overhead.


<details>
  <summary>Details</summary>
Motivation: Limited size and quality of respiratory sound datasets hinder high performance, and ensemble models increase compute costs. Soft label training offers an efficient alternative.

Method: Distilling knowledge from an ensemble of teacher models into a student model using soft labels, tested with variations including single and multiple teachers.

Result: Achieved a new state-of-the-art score of 64.39 on ICHBI, surpassing previous best by 0.85, with optimal gains from few teachers.

Conclusion: Soft label knowledge distillation is effective for respiratory sound classification, regardless of model size or architecture.

Abstract: Respiratory sound datasets are limited in size and quality, making high
performance difficult to achieve. Ensemble models help but inevitably increase
compute cost at inference time. Soft label training distills knowledge
efficiently with extra cost only at training. In this study, we explore soft
labels for respiratory sound classification as an architecture-agnostic
approach to distill an ensemble of teacher models into a student model. We
examine different variations of our approach and find that even a single
teacher, identical to the student, considerably improves performance beyond its
own capability, with optimal gains achieved using only a few teachers. We
achieve the new state-of-the-art Score of 64.39 on ICHBI, surpassing the
previous best by 0.85 and improving average Scores across architectures by more
than 1.16. Our results highlight the effectiveness of knowledge distillation
with soft labels for respiratory sound classification, regardless of size or
architecture.

</details>


### [502] [Weakly Supervised Data Refinement and Flexible Sequence Compression for Efficient Thai LLM-based ASR](https://arxiv.org/pdf/2505.22063)
*Mingchen Shao, Xinfa Zhu, Chengyou Wang, Bingshen Mu, Hai Li, Ying Yan, Junhui Liu, Danming Xie, Lei Xie*

Main category: cs.SD

TL;DR: EThai-ASR applies LLMs to Thai ASR, addressing data scarcity and computational demands with a self-evolving data refinement strategy and pluggable sequence compression.


<details>
  <summary>Details</summary>
Motivation: Challenges in low-resource ASR include data scarcity and high computational demands, especially for Thai language.

Method: EThai-ASR uses a speech encoder, connection module, and Thai LLM decoder, with self-evolving data refinement and sequence compression.

Result: Achieves state-of-the-art accuracy in multiple datasets.

Conclusion: EThai-ASR advances Thai ASR by efficiently leveraging LLMs and releasing refined data for research.

Abstract: Despite remarkable achievements, automatic speech recognition (ASR) in
low-resource scenarios still faces two challenges: high-quality data scarcity
and high computational demands. This paper proposes EThai-ASR, the first to
apply large language models (LLMs) to Thai ASR and create an efficient
LLM-based ASR system. EThai-ASR comprises a speech encoder, a connection module
and a Thai LLM decoder. To address the data scarcity and obtain a powerful
speech encoder, EThai-ASR introduces a self-evolving data refinement strategy
to refine weak labels, yielding an enhanced speech encoder. Moreover, we
propose a pluggable sequence compression module used in the connection module
with three modes designed to reduce the sequence length, thus decreasing
computational demands while maintaining decent performance. Extensive
experiments demonstrate that EThai-ASR has achieved state-of-the-art accuracy
in multiple datasets. We release our refined text transcripts to promote
further research.

</details>


### [503] [Delayed-KD: Delayed Knowledge Distillation based CTC for Low-Latency Streaming ASR](https://arxiv.org/pdf/2505.22069)
*Longhao Li, Yangze Li, Hongfei Xue, Jie Liu, Shuai Fang, Kai Wang, Lei Xie*

Main category: cs.SD

TL;DR: Delayed-KD improves CTC-based streaming ASR by aligning outputs with a non-streaming teacher model, reducing accuracy degradation and latency.


<details>
  <summary>Details</summary>
Motivation: Address accuracy degradation in small chunks and token emission latency in CTC-based streaming ASR.

Method: Proposes Delayed-KD with Temporal Alignment Buffer (TAB) to align CTC outputs and control token emission delay.

Result: Achieves 5.42% CER on AISHELL-1 at 40 ms latency, comparable to U2++ at 320 ms.

Conclusion: Delayed-KD effectively mitigates challenges in streaming ASR, offering superior performance with lower latency.

Abstract: CTC-based streaming ASR has gained significant attention in real-world
applications but faces two main challenges: accuracy degradation in small
chunks and token emission latency. To mitigate these challenges, we propose
Delayed-KD, which applies delayed knowledge distillation on CTC posterior
probabilities from a non-streaming to a streaming model. Specifically, with a
tiny chunk size, we introduce a Temporal Alignment Buffer (TAB) that defines a
relative delay range compared to the non-streaming teacher model to align CTC
outputs and mitigate non-blank token mismatches. Additionally, TAB enables
fine-grained control over token emission delay. Experiments on 178-hour
AISHELL-1 and 10,000-hour WenetSpeech Mandarin datasets show consistent
superiority of Delayed-KD. Impressively, Delayed-KD at 40 ms latency achieves a
lower character error rate (CER) of 5.42% on AISHELL-1, comparable to the
competitive U2++ model running at 320 ms latency.

</details>


### [504] [On-the-fly Routing for Zero-shot MoE Speaker Adaptation of Speech Foundation Models for Dysarthric Speech Recognition](https://arxiv.org/pdf/2505.22072)
*Shujie HU, Xurong Xie, Mengzhe Geng, Jiajun Deng, Huimeng Wang, Guinan Li, Chengxi Deng, Tianzi Wang, Mingyu Cui, Helen Meng, Xunying Liu*

Main category: cs.SD

TL;DR: A MoE-based framework for dysarthric speech recognition achieves zero-shot adaptation and real-time processing, reducing WER significantly.


<details>
  <summary>Details</summary>
Motivation: To improve dysarthric speech recognition by enabling zero-shot adaptation and real-time processing while leveraging domain knowledge.

Method: Uses MoE with severity/gender-conditioned experts, dynamic routing, and KL-divergence for diversity.

Result: Achieves WER reductions up to 1.34% absolute (6.36% relative) and 7x RTF speedups. Lowest WER: 16.35%.

Conclusion: The framework outperforms baselines, offering efficient adaptation for dysarthric speech.

Abstract: This paper proposes a novel MoE-based speaker adaptation framework for
foundation models based dysarthric speech recognition. This approach enables
zero-shot adaptation and real-time processing while incorporating domain
knowledge. Speech impairment severity and gender conditioned adapter experts
are dynamically combined using on-the-fly predicted speaker-dependent routing
parameters. KL-divergence is used to further enforce diversity among experts
and their generalization to unseen speakers. Experimental results on the
UASpeech corpus suggest that on-the-fly MoE-based adaptation produces
statistically significant WER reductions of up to 1.34% absolute (6.36%
relative) over the unadapted baseline HuBERT/WavLM models. Consistent WER
reductions of up to 2.55% absolute (11.44% relative) and RTF speedups of up to
7 times are obtained over batch-mode adaptation across varying speaker-level
data quantities. The lowest published WER of 16.35% (46.77% on very low
intelligibility) is obtained.

</details>


### [505] [Visual Cues Support Robust Turn-taking Prediction in Noise](https://arxiv.org/pdf/2505.22088)
*Sam O'Connor Russell, Naomi Harte*

Main category: cs.SD

TL;DR: PTTMs perform poorly in noisy conditions, but a multimodal approach improves accuracy. Training with noisy data helps, but generalization to new noise types is limited. Accurate transcription is crucial for training.


<details>
  <summary>Details</summary>
Motivation: To evaluate the performance of predictive turn-taking models (PTTMs) in noisy environments, which is critical for real-world human-robot interaction.

Method: Analyzed PTTM performance in various noise conditions, comparing audio-only and multimodal (audio-visual) models trained with noisy data.

Result: Multimodal PTTMs outperformed audio-only models in noise, achieving 72% accuracy in 10 dB music noise (vs. 52% for audio-only). However, generalization to unseen noise types was limited.

Conclusion: Multimodal PTTMs are more robust in noise but require accurate transcription for training. Public code is provided for further research.

Abstract: Accurate predictive turn-taking models (PTTMs) are essential for naturalistic
human-robot interaction. However, little is known about their performance in
noise. This study therefore explores PTTM performance in types of noise likely
to be encountered once deployed. Our analyses reveal PTTMs are highly sensitive
to noise. Hold/shift accuracy drops from 84% in clean speech to just 52% in 10
dB music noise. Training with noisy data enables a multimodal PTTM, which
includes visual features to better exploit visual cues, with 72% accuracy in 10
dB music noise. The multimodal PTTM outperforms the audio-only PTTM across all
noise types and SNRs, highlighting its ability to exploit visual cues; however,
this does not always generalise to new types of noise. Analysis also reveals
that successful training relies on accurate transcription, limiting the use of
ASR-derived transcriptions to clean conditions. We make code publicly available
for future research.

</details>


### [506] [AudioTurbo: Fast Text-to-Audio Generation with Rectified Diffusion](https://arxiv.org/pdf/2505.22106)
*Junqi Zhao, Jinzheng Zhao, Haohe Liu, Yun Chen, Lu Han, Xubo Liu, Mark Plumbley, Wenwu Wang*

Main category: cs.SD

TL;DR: AudioTurbo integrates pre-trained diffusion models with rectified flow to speed up text-to-audio generation, achieving better performance with fewer steps.


<details>
  <summary>Details</summary>
Motivation: Rectified flow improves inference speed but performs poorly at low step counts and requires training from scratch. This work aims to leverage pre-trained models for better efficiency.

Method: Proposes AudioTurbo, which learns first-order ODE paths from pre-trained model noise pairs, enabling faster and higher-quality generation.

Result: Outperforms prior models with only 10 sampling steps and reduces inference to 3 steps compared to flow-matching-based models.

Conclusion: AudioTurbo effectively combines pre-trained models with rectified flow for efficient and high-quality text-to-audio generation.

Abstract: Diffusion models have significantly improved the quality and diversity of
audio generation but are hindered by slow inference speed. Rectified flow
enhances inference speed by learning straight-line ordinary differential
equation (ODE) paths. However, this approach requires training a flow-matching
model from scratch and tends to perform suboptimally, or even poorly, at low
step counts. To address the limitations of rectified flow while leveraging the
advantages of advanced pre-trained diffusion models, this study integrates
pre-trained models with the rectified diffusion method to improve the
efficiency of text-to-audio (TTA) generation. Specifically, we propose
AudioTurbo, which learns first-order ODE paths from deterministic noise sample
pairs generated by a pre-trained TTA model. Experiments on the AudioCaps
dataset demonstrate that our model, with only 10 sampling steps, outperforms
prior models and reduces inference to 3 steps compared to a flow-matching-based
acceleration model.

</details>


### [507] [Developing a Top-tier Framework in Naturalistic Conditions Challenge for Categorized Emotion Prediction: From Speech Foundation Models and Learning Objective to Data Augmentation and Engineering Choices](https://arxiv.org/pdf/2505.22133)
*Tiantian Feng, Thanathai Lertpetchpun, Dani Byrd, Shrikanth Narayanan*

Main category: cs.SD

TL;DR: The paper introduces the SAILER system for Speech Emotion Recognition (SER), addressing challenges like subjective annotations and imbalanced datasets. It achieves strong performance, ranking top-3 in the INTERSPEECH 2025 challenge.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenges of SER, particularly subjectivity in emotion annotation and imbalanced label distribution in datasets, using a simple yet effective system.

Method: Develops the SAILER system with focus on modeling, learning objectives, data augmentation, and engineering choices. Evaluates on the INTERSPEECH 2025 challenge dataset.

Result: Single system outperforms 95% of submissions (Macro-F1 > 0.4). Ensemble of three systems ranks top-3.

Conclusion: SAILER demonstrates effectiveness in SER, offering a reproducible and competitive solution for imbalanced and subjective emotion datasets.

Abstract: Speech emotion recognition (SER), particularly for naturally expressed
emotions, remains a challenging computational task. Key challenges include the
inherent subjectivity in emotion annotation and the imbalanced distribution of
emotion labels in datasets. This paper introduces the \texttt{SAILER} system
developed for participation in the INTERSPEECH 2025 Emotion Recognition
Challenge (Task 1). The challenge dataset, which contains natural emotional
speech from podcasts, serves as a valuable resource for studying imbalanced and
subjective emotion annotations. Our system is designed to be simple,
reproducible, and effective, highlighting critical choices in modeling,
learning objectives, data augmentation, and engineering choices. Results show
that even a single system (without ensembling) can outperform more than 95\% of
the submissions, with a Macro-F1 score exceeding 0.4. Moreover, an ensemble of
three systems further improves performance, achieving a competitively ranked
score (top-3 performing team). Our model is at:
https://github.com/tiantiaf0627/vox-profile-release.

</details>


### [508] [Two-stage Audio-Visual Target Speaker Extraction System for Real-Time Processing On Edge Device](https://arxiv.org/pdf/2505.22229)
*Zixuan Li, Xueliang Zhang, Lei Miao, Zhipeng Yan*

Main category: cs.SD

TL;DR: A two-stage ultra-compact AVTSE system reduces computational complexity by using visual cues for VAD first, then combining results with audio for target speaker extraction.


<details>
  <summary>Details</summary>
Motivation: Existing AVTSE methods are computationally heavy, making them impractical for real-time edge device use.

Method: A two-stage system: (1) compact VAD using visual cues, (2) combining VAD results with audio for speaker extraction.

Result: Effectively suppresses noise and interference with low computational cost.

Conclusion: The proposed system is efficient and practical for real-time edge applications.

Abstract: Audio-Visual Target Speaker Extraction (AVTSE) aims to isolate a target
speaker's voice in a multi-speaker environment with visual cues as auxiliary.
Most of the existing AVTSE methods encode visual and audio features
simultaneously, resulting in extremely high computational complexity and making
it impractical for real-time processing on edge devices. To tackle this issue,
we proposed a two-stage ultra-compact AVTSE system. Specifically, in the first
stage, a compact network is employed for voice activity detection (VAD) using
visual information. In the second stage, the VAD results are combined with
audio inputs to isolate the target speaker's voice. Experiments show that the
proposed system effectively suppresses background noise and interfering voices
while spending little computational resources.

</details>


### [509] [Advancing Hearing Assessment: An ASR-Based Frequency-Specific Speech Test for Diagnosing Presbycusis](https://arxiv.org/pdf/2505.22231)
*Stefan Bleeck*

Main category: cs.SD

TL;DR: The paper introduces an ASR-based speech test to address limitations of traditional audiometry in diagnosing frequency-specific hearing deficits, showing promising results in simulations.


<details>
  <summary>Details</summary>
Motivation: Traditional audiometry lacks granularity in assessing supra-threshold deficits and frequency-specific challenges in hearing loss, such as presbycusis.

Method: The study uses ASR to simulate hearing loss effects by degrading speech stimuli and analyzing phoneme-level confusion patterns.

Result: Simulated hearing loss caused specific phoneme confusions (e.g., high-frequency consonants) and deletions, aligning with presbycusis. The ASR-based test effectively differentiated hearing-impaired from normal-hearing simulations.

Conclusion: The ASR-driven approach shows potential for objective, granular hearing assessments. Future work includes human validation and AI integration for improved precision.

Abstract: Traditional audiometry often fails to fully characterize the functional
impact of hearing loss on speech understanding, particularly supra-threshold
deficits and frequency-specific perception challenges in conditions like
presbycusis. This paper presents the development and simulated evaluation of a
novel Automatic Speech Recognition (ASR)-based frequency-specific speech test
designed to provide granular diagnostic insights. Our approach leverages ASR to
simulate the perceptual effects of moderate sloping hearing loss by processing
speech stimuli under controlled acoustic degradation and subsequently analyzing
phoneme-level confusion patterns. Key findings indicate that simulated hearing
loss introduces specific phoneme confusions, predominantly affecting
high-frequency consonants (e.g., alveolar/palatal to labiodental substitutions)
and leading to significant phoneme deletions, consistent with the acoustic cues
degraded in presbycusis. A test battery curated from these ASR-derived
confusions demonstrated diagnostic value, effectively differentiating between
simulated normal-hearing and hearing-impaired listeners in a comprehensive
simulation. This ASR-driven methodology offers a promising avenue for
developing objective, granular, and frequency-specific hearing assessment tools
that complement traditional audiometry. Future work will focus on validating
these findings with human participants and exploring the integration of
advanced AI models for enhanced diagnostic precision.

</details>


### [510] [Effective Context in Neural Speech Models](https://arxiv.org/pdf/2505.22487)
*Yen Meng, Sharon Goldwater, Hao Tang*

Main category: cs.SD

TL;DR: The paper measures the effective context used by speech Transformers, finding it varies by task and is shorter than expected, enabling streaming without modification.


<details>
  <summary>Details</summary>
Motivation: To understand how much context speech models actually use (effective context) and its implications for model performance and efficiency.

Method: Proposes two approaches to measure effective context, analyzing supervised and self-supervised speech Transformers.

Result: Effective context varies by task (e.g., fundamental frequency tracking vs. word classification) and is shorter than expected, especially in self-supervised models. HuBERT can run in streaming mode without changes.

Conclusion: Effective context is task-dependent and often shorter than assumed, enabling practical applications like streaming without architectural modifications.

Abstract: Modern neural speech models benefit from having longer context, and many
approaches have been proposed to increase the maximum context a model can use.
However, few have attempted to measure how much context these models actually
use, i.e., the effective context. Here, we propose two approaches to measuring
the effective context, and use them to analyze different speech Transformers.
For supervised models, we find that the effective context correlates well with
the nature of the task, with fundamental frequency tracking, phone
classification, and word classification requiring increasing amounts of
effective context. For self-supervised models, we find that effective context
increases mainly in the early layers, and remains relatively short -- similar
to the supervised phone model. Given that these models do not use a long
context during prediction, we show that HuBERT can be run in streaming mode
without modification to the architecture and without further fine-tuning.

</details>


### [511] [Towards General Discrete Speech Codec for Complex Acoustic Environments: A Study of Reconstruction and Downstream Task Consistency](https://arxiv.org/pdf/2505.22515)
*Haoran Wang, Guanyu Chen, Bohan Li, Hankun Wang, Yiwei Guo, Zhihan Li, Xie Chen, Kai Yu*

Main category: cs.SD

TL;DR: The paper introduces ERSB, a benchmark to evaluate neural speech codecs' resilience in complex acoustic environments, revealing their limitations in robust reconstruction and downstream task consistency.


<details>
  <summary>Details</summary>
Motivation: Current neural speech codecs perform well with clean speech but lack evaluation in complex environments and downstream tasks, prompting the need for a systematic benchmark.

Method: The study proposes the Environment-Resilient Speech Codec Benchmark (ERSB) to assess two capabilities: robust reconstruction and downstream task consistency.

Result: Experiments show that complex acoustic environments degrade both signal reconstruction and downstream task performance.

Conclusion: The work identifies limitations in current speech codecs and suggests improving their environmental resilience as a future direction.

Abstract: Neural speech codecs excel in reconstructing clean speech signals; however,
their efficacy in complex acoustic environments and downstream signal
processing tasks remains underexplored. In this study, we introduce a novel
benchmark named Environment-Resilient Speech Codec Benchmark (ERSB) to
systematically evaluate whether neural speech codecs are environment-resilient.
Specifically, we assess two key capabilities: (1) robust reconstruction, which
measures the preservation of both speech and non-speech acoustic details, and
(2) downstream task consistency, which ensures minimal deviation in downstream
signal processing tasks when using reconstructed speech instead of the
original. Our comprehensive experiments reveal that complex acoustic
environments significantly degrade signal reconstruction and downstream task
consistency. This work highlights the limitations of current speech codecs and
raises a future direction that improves them for greater environmental
resilience.

</details>


### [512] [Effective and Efficient One-pass Compression of Speech Foundation Models Using Sparsity-aware Self-pinching Gates](https://arxiv.org/pdf/2505.22608)
*Haoning Xu, Zhaoqing Li, Youjun Chen, Huimeng Wang, Guinan Li, Mengzhe Geng, Chengxi Deng, Xunying Liu*

Main category: cs.SD

TL;DR: A novel method integrates pruning and parameter updates into one stage for speech model compression, achieving high compression with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: To create a more efficient and faster approach for compressing speech foundation models without significant performance degradation.

Method: Uses layer-level tied self-pinching gates with single learnable thresholds for fine-grained neuron pruning, trained jointly with uncompressed models.

Result: Reduces parameters by 65% (wav2vec2.0-base) and 60% (HuBERT-large) with no significant WER increase. Achieves lowest WER (7.05%) and faster compression time.

Conclusion: The approach effectively compresses speech models while maintaining performance and improving efficiency over prior methods.

Abstract: This paper presents a novel approach for speech foundation models compression
that tightly integrates model pruning and parameter update into a single stage.
Highly compact layer-level tied self-pinching gates each containing only a
single learnable threshold are jointly trained with uncompressed models and
used in fine-grained neuron level pruning. Experiments conducted on the
LibriSpeech-100hr corpus suggest that our approach reduces the number of
parameters of wav2vec2.0-base and HuBERT-large models by 65% and 60%
respectively, while incurring no statistically significant word error rate
(WER) increase on the test-clean dataset. Compared to previously published
methods on the same task, our approach not only achieves the lowest WER of
7.05% on the test-clean dataset under a comparable model compression ratio of
4.26x, but also operates with at least 25% less model compression time.

</details>


### [513] [METEOR: Melody-aware Texture-controllable Symbolic Orchestral Music Generation](https://arxiv.org/pdf/2409.11753)
*Dinh-Viet-Toan Le, Yi-Hsuan Yang*

Main category: cs.SD

TL;DR: METEOR is a model for melody-aware, texture-controllable orchestral music generation, improving melodic fidelity while maintaining homophonic texture.


<details>
  <summary>Details</summary>
Motivation: To address the need for melodic fidelity and controllable accompaniment in homophonic orchestral music generation.

Method: Symbolic multi-track music style transfer with bar- and track-level controllability of accompaniment attributes.

Result: Achieves controllability similar to baselines while significantly improving melodic fidelity.

Conclusion: METEOR effectively balances texture control and melodic accuracy in orchestral music generation.

Abstract: Western music is often characterized by a homophonic texture, in which the
musical content can be organized into a melody and an accompaniment. In
orchestral music, in particular, the composer can select specific
characteristics for each instrument's part within the accompaniment, while also
needing to adapt the melody to suit the capabilities of the instruments
performing it. In this work, we propose METEOR, a model for Melody-aware
Texture-controllable Orchestral music generation. This model performs symbolic
multi-track music style transfer with a focus on melodic fidelity. We allow
bar- and track-level controllability of the accompaniment with various textural
attributes while keeping a homophonic texture. We show that the model can
achieve controllability performances similar to strong baselines while greatly
improve melodic fidelity.

</details>


### [514] [Zero-Shot Mono-to-Binaural Speech Synthesis](https://arxiv.org/pdf/2412.08356)
*Alon Levkovitch, Julian Salazar, Soroosh Mariooryad, RJ Skerry-Ryan, Nadav Bar, Bastiaan Kleijn, Eliya Nachmani*

Main category: cs.SD

TL;DR: ZeroBAS is a zero-shot neural method for synthesizing binaural audio from monaural recordings without binaural training data, using geometric time warping and a pretrained denoising vocoder.


<details>
  <summary>Details</summary>
Motivation: To achieve robust binaural audio synthesis without relying on binaural training data, leveraging pretrained models and zero-shot learning.

Method: Uses parameter-free geometric time warping and amplitude scaling based on source location, refined by a pretrained denoising vocoder.

Result: Perceptually matches supervised methods on standard datasets and outperforms them on out-of-distribution conditions.

Conclusion: Demonstrates the potential of pretrained generative models and zero-shot learning for robust binaural synthesis.

Abstract: We present ZeroBAS, a neural method to synthesize binaural audio from
monaural audio recordings and positional information without training on any
binaural data. To our knowledge, this is the first published zero-shot neural
approach to mono-to-binaural audio synthesis. Specifically, we show that a
parameter-free geometric time warping and amplitude scaling based on source
location suffices to get an initial binaural synthesis that can be refined by
iteratively applying a pretrained denoising vocoder. Furthermore, we find this
leads to generalization across room conditions, which we measure by introducing
a new dataset, TUT Mono-to-Binaural, to evaluate state-of-the-art
monaural-to-binaural synthesis methods on unseen conditions. Our zero-shot
method is perceptually on-par with the performance of supervised methods on the
standard mono-to-binaural dataset, and even surpasses them on our
out-of-distribution TUT Mono-to-Binaural dataset. Our results highlight the
potential of pretrained generative audio models and zero-shot learning to
unlock robust binaural audio synthesis.

</details>


### [515] [Solid State Bus-Comp: A Large-Scale and Diverse Dataset for Dynamic Range Compressor Virtual Analog Modeling](https://arxiv.org/pdf/2504.04589)
*Yicheng Gu, Runsong Zhang, Lauri Juvela, Zhizheng Wu*

Main category: cs.SD

TL;DR: The paper introduces Solid State Bus-Comp, a large-scale dataset for modeling the SSL 500 G-Bus compressor, addressing data limitations in neural-network-based VA modeling.


<details>
  <summary>Details</summary>
Motivation: Neural-network-based VA modeling lacks generalization due to insufficient data diversity and quantity.

Method: A dataset of 2528 hours was created by recording 175 songs in 220 parameter combinations. Benchmark experiments and ablation studies were conducted.

Result: The dataset improves generalization in VA modeling, demonstrated through experiments with various models.

Conclusion: Solid State Bus-Comp enhances VA modeling by providing diverse and extensive data, validated by benchmark results.

Abstract: Virtual Analog (VA) modeling aims to simulate the behavior of hardware
circuits via algorithms to replicate their tone digitally. Dynamic Range
Compressor (DRC) is an audio processing module that controls the dynamics of a
track by reducing and amplifying the volumes of loud and quiet sounds, which is
essential in music production. In recent years, neural-network-based VA
modeling has shown great potential in producing high-fidelity models. However,
due to the lack of data quantity and diversity, their generalization ability in
different parameter settings and input sounds is still limited. To tackle this
problem, we present Solid State Bus-Comp, the first large-scale and diverse
dataset for modeling the classical VCA compressor -- SSL 500 G-Bus.
Specifically, we manually collected 175 unmastered songs from the Cambridge
Multitrack Library. We recorded the compressed audio in 220 parameter
combinations, resulting in an extensive 2528-hour dataset with diverse genres,
instruments, tempos, and keys. Moreover, to facilitate the use of our proposed
dataset, we conducted benchmark experiments in various open-sourced black-box
and grey-box models, as well as white-box plugins. We also conducted ablation
studies in different data subsets to illustrate the effectiveness of the
improved data diversity and quantity. The dataset and demos are on our project
page: https://www.yichenggu.com/SolidStateBusComp/.

</details>


### [516] [Data Standards in Audiology: A Mixed-Methods Exploration of Community Perspectives and Implementation Considerations](https://arxiv.org/pdf/2505.04728)
*Charlotte Vercammen, Antje Heinrich, Christophe Lesimple, Alessia Paglialonga, Jan-Willem A. Wasmann, Mareike Buhl*

Main category: cs.SD

TL;DR: The study explored data standardization in audiology, revealing low awareness but strong support for initiatives to improve research and patient care.


<details>
  <summary>Details</summary>
Motivation: To assess the audiology community's knowledge, needs, and preferences regarding data standards and develop recommendations.

Method: Mixed-methods approach: survey (82 participants) and expert panel discussion (5 experts).

Result: Low awareness (38%) of existing initiatives, but 90% willingness to contribute. Challenges (data quality, privacy) and opportunities (synergies with other fields) were identified.

Conclusion: Community support can drive standardization efforts, ensuring alignment with other medical fields.

Abstract: Objective: The purpose of this study was to explore options for data
standardisation in audiology and document the global audiology community's
current knowledge and views of data standards, explore their needs and
preferences, and develop recommendations for data standardisation as a result.
  Design: A mixed-methods approach, combining a structured survey with an
in-depth exploration of themes by experts during a special session on "Big Data
and Data Standards in Audiology" at the 2024 Virtual Conference of
Computational Audiology.
  Study Sample: The survey sample consisted of 82 members of the global
audiology community; five experts joined the panel discussion.
  Results: Survey results emphasized the need for data standardisation in
audiology aimed at facilitating research and improving patient care. Knowledge
of existing initiatives was low: 38% were aware of initiatives. Yet, 90%
envisioned contributing to them moving forward. The panel discussion explored
emerging standardisation initiatives in audiology (OMOP, openEHR, HIMSA's Noah
standard), challenges (e.g., data quality and privacy), and opportunities
(e.g., conversion between approaches and synergies with other medical fields).
  Conclusions: The community support identified in this study could be
leveraged to further develop standardisation initiatives for audiology,
ensuring alignment between initiatives and with other medical fields.

</details>


### [517] [Neurodyne: Neural Pitch Manipulation with Representation Learning and Cycle-Consistency GAN](https://arxiv.org/pdf/2505.15368)
*Yicheng Gu, Chaoren Wang, Zhizheng Wu, Lauri Juvela*

Main category: cs.SD

TL;DR: Neurodyne improves pitch manipulation in music production by using adversarial representation learning and cycle-consistency training, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Current neural-network-based pitch-manipulation systems suffer from inaccurate feature disentanglement and lack of paired training data.

Method: Neurodyne employs adversarial representation learning for pitch-independent latent representation and cycle-consistency training to generate paired data.

Result: The system shows improved synthesis quality and preserves singer identity in global-key and template-based pitch manipulation.

Conclusion: Neurodyne effectively addresses limitations of existing methods, enhancing pitch manipulation in music production.

Abstract: Pitch manipulation is the process of producers adjusting the pitch of an
audio segment to a specific key and intonation, which is essential in music
production. Neural-network-based pitch-manipulation systems have been popular
in recent years due to their superior synthesis quality compared to classical
DSP methods. However, their performance is still limited due to their
inaccurate feature disentanglement using source-filter models and the lack of
paired in- and out-of-tune training data. This work proposes Neurodyne to
address these issues. Specifically, Neurodyne uses adversarial representation
learning to learn a pitch-independent latent representation to avoid inaccurate
disentanglement and cycle-consistency training to create paired training data
implicitly. Experimental results on global-key and template-based pitch
manipulation demonstrate the effectiveness of the proposed system, marking
improved synthesis quality while maintaining the original singer identity.

</details>


### [518] [A Comprehensive Real-World Assessment of Audio Watermarking Algorithms: Will They Survive Neural Codecs?](https://arxiv.org/pdf/2505.19663)
*Yigitcan Özer, Woosung Choi, Joan Serrà, Mayank Kumar Singh, Wei-Hsiang Liao, Yuki Mitsufuji*

Main category: cs.SD

TL;DR: RAW-Bench is a benchmark for evaluating deep learning-based audio watermarking methods, featuring standardized tests and a diverse dataset. It reveals neural compression as a major challenge and highlights the impact of specific distortions.


<details>
  <summary>Details</summary>
Motivation: To provide a standardized and systematic way to evaluate audio watermarking methods under real-world conditions, including various distortions.

Method: Introduces a comprehensive audio attack pipeline with distortions like compression, noise, and reverberation, and evaluates four existing watermarking methods.

Result: Neural compression is the biggest challenge, and training with audio attacks improves robustness but is sometimes insufficient. Specific distortions like polarity inversion and time stretching severely affect some methods.

Conclusion: RAW-Bench offers a valuable framework for evaluating audio watermarking robustness, identifying key challenges and areas for improvement.

Abstract: We introduce the Robust Audio Watermarking Benchmark (RAW-Bench), a benchmark
for evaluating deep learning-based audio watermarking methods with standardized
and systematic comparisons. To simulate real-world usage, we introduce a
comprehensive audio attack pipeline with various distortions such as
compression, background noise, and reverberation, along with a diverse test
dataset including speech, environmental sounds, and music recordings.
Evaluating four existing watermarking methods on RAW-bench reveals two main
insights: (i) neural compression techniques pose the most significant
challenge, even when algorithms are trained with such compressions; and (ii)
training with audio attacks generally improves robustness, although it is
insufficient in some cases. Furthermore, we find that specific distortions,
such as polarity inversion, time stretching, or reverb, seriously affect
certain methods. The evaluation framework is accessible at
github.com/SonyResearch/raw_bench.

</details>


### [519] [Towards Robust Automated Perceptual Voice Quality Assessment with Speech Foundation Models](https://arxiv.org/pdf/2505.21356)
*Whenty Ariyanti, Kuan-Yu Chen, Sabato Marco Siniscalchi, Hsin-Min Wang, Yu Tsao*

Main category: cs.SD

TL;DR: VOQANet and VOQANet+ are deep learning frameworks for automated voice quality assessment, combining Speech Foundation Model embeddings with handcrafted acoustic features for improved robustness and interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional voice quality assessment methods like CAPE-V and GRBAS are subjective and prone to variability, necessitating automated, objective solutions.

Method: VOQANet uses an attention mechanism with SFM embeddings, while VOQANet+ integrates handcrafted features (jitter, shimmer, HNR) for a hybrid representation. Both models are evaluated on vowel-based and sentence-level speech.

Result: VOQANet+ outperforms baselines in accuracy and robustness, especially with sentence-level input, and maintains performance under noisy conditions.

Conclusion: Combining SFM embeddings with domain-specific features enhances interpretability and robustness in voice quality assessment, making it suitable for real-world applications.

Abstract: Perceptual voice quality assessment is essential for diagnosing and
monitoring voice disorders. Traditionally, expert raters use scales such as the
CAPE-V and GRBAS. However, these are subjective and prone to inter-rater
variability, motivating the need for automated, objective assessment methods.
This study proposes VOQANet, a deep learning framework with an attention
mechanism that leverages a Speech Foundation Model (SFM) to extract high-level
acoustic and prosodic information from raw speech. To improve robustness and
interpretability, we introduce VOQANet+, which integrates handcrafted acoustic
features such as jitter, shimmer, and harmonics-to-noise ratio (HNR) with SFM
embeddings into a hybrid representation. Unlike prior work focusing only on
vowel-based phonation (PVQD-A subset) from the Perceptual Voice Quality Dataset
(PVQD), we evaluate our models on both vowel-based and sentence-level speech
(PVQD-S subset) for better generalizability. Results show that sentence-based
input outperforms vowel-based input, particularly at the patient level,
highlighting the benefit of longer utterances for capturing voice attributes.
VOQANet consistently surpasses baseline methods in root mean squared error and
Pearson correlation across CAPE-V and GRBAS dimensions, with VOQANet+ achieving
further improvements. Additional tests under noisy conditions show that
VOQANet+ maintains high prediction accuracy, supporting its use in real-world
and telehealth settings. These findings demonstrate the value of combining SFM
embeddings with domain-informed acoustic features for interpretable and robust
voice quality assessment.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [520] [The Role of Visualization in LLM-Assisted Knowledge Graph Systems: Effects on User Trust, Exploration, and Workflows](https://arxiv.org/pdf/2505.21512)
*Harry Li, Gabriel Appleby, Kenneth Alperin, Steven R Gomez, Ashley Suh*

Main category: cs.LG

TL;DR: The paper explores how LLMs assist in KG exploration, revealing risks of user overtrust and the need for tailored visualizations.


<details>
  <summary>Details</summary>
Motivation: To understand how LLM-based KG systems affect user trust, exploration strategies, and decision-making, given the lack of empirical evidence.

Method: Developed LinkQ, a system converting natural language to KG queries using an LLM, with five visual mechanisms for accuracy assessment.

Result: Users, including experts, overtrusted LinkQ due to its visualizations, even when incorrect, with workflows varying by prior KG/LLM familiarity.

Conclusion: Highlights risks of false trust in LLM-assisted tools and the need for further research on visualization's role in mitigating these risks.

Abstract: Knowledge graphs (KGs) are powerful data structures, but exploring them
effectively remains difficult for even expert users. Large language models
(LLMs) are increasingly used to address this gap, yet little is known
empirically about how their usage with KGs shapes user trust, exploration
strategies, or downstream decision-making - raising key design challenges for
LLM-based KG visual analysis systems. To study these effects, we developed
LinkQ, a KG exploration system that converts natural language questions into
structured queries with an LLM. We collaborated with KG experts to design five
visual mechanisms that help users assess the accuracy of both KG queries and
LLM responses: an LLM-KG state diagram that illustrates which stage of the
exploration pipeline LinkQ is in, a query editor displaying the generated query
paired with an LLM explanation, an entity-relation ID table showing extracted
KG entities and relations with semantic descriptions, a query structure graph
that depicts the path traversed in the KG, and an interactive graph
visualization of query results. From a qualitative evaluation with 14
practitioners, we found that users - even KG experts - tended to overtrust
LinkQ's outputs due to its "helpful" visualizations, even when the LLM was
incorrect. Users exhibited distinct workflows depending on their prior
familiarity with KGs and LLMs, challenging the assumption that these systems
are one-size-fits-all - despite often being designed as if they are. Our
findings highlight the risks of false trust in LLM-assisted data analysis tools
and the need for further investigation into the role of visualization as a
mitigation technique.

</details>


### [521] [SIMCOPILOT: Evaluating Large Language Models for Copilot-Style Code Generation](https://arxiv.org/pdf/2505.21514)
*Mingchao Jiang, Abhinav Jain, Sophia Zorek, Chris Jermaine*

Main category: cs.LG

TL;DR: SIMCOPILOT is a benchmark for evaluating LLMs as coding assistants, focusing on completion and infill tasks for Java and Python. It provides detailed analysis of LLM performance in practical coding scenarios.


<details>
  <summary>Details</summary>
Motivation: To create a realistic evaluation framework for LLMs as coding assistants, addressing gaps in existing benchmarks like task-specific nuances and contextual understanding.

Method: SIMCOPILOT includes sub-benchmarks for Java and Python, covering diverse codebases. It evaluates LLMs on completion and infill tasks, analyzing performance across domains.

Result: The benchmark reveals strengths and challenges of LLMs, such as maintaining logical consistency in complex dependencies, and highlights their transition toward reliable coding partners.

Conclusion: SIMCOPILOT advances LLM evaluation for coding tasks, uncovering limitations and emphasizing their evolving role in software development.

Abstract: We introduce SIMCOPILOT, a benchmark that simulates the role of large
language models (LLMs) as interactive, "copilot"-style coding assistants.
Targeting both completion (finishing incomplete methods or code blocks) and
infill tasks (filling missing segments within existing code), SIMCOPILOT
provides a comprehensive framework for evaluating LLM coding capabilities. The
benchmark comprises dedicated sub-benchmarks for Java (SIMCOPILOTJ) and Python
(SIMCOPILOTP), covering diverse codebases varying in size and complexity. Our
key contributions include: (a) establishing a realistic, detailed evaluation
environment to assess LLM utility in practical coding scenarios, and (b)
providing fine-grained analyses that address critical factors frequently
overlooked by existing benchmarks, such as task-specific performance nuances,
contextual understanding across code segments, and sensitivity to variable
scope. Evaluations conducted across domains-including algorithms, databases,
computer vision, and neural networks-offer insights into model strengths and
highlight persistent challenges in maintaining logical consistency within
complex dependency structures. Beyond benchmarking, our study sheds light on
the current limitations of LLM-driven code generation and underscores the
ongoing transition of LLMs from merely syntax-aware generators toward reliable,
intelligent software development partners.

</details>


### [522] [Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation](https://arxiv.org/pdf/2505.21525)
*Peiliang Gong, Yucheng Wang, Min Wu, Zhenghua Chen, Xiaoli Li, Daoqiang Zhang*

Main category: cs.LG

TL;DR: TERSE is a novel SFDA method for MTS data, addressing spatial-temporal correlations to improve domain adaptation without source data access.


<details>
  <summary>Details</summary>
Motivation: Existing SFDA methods fail to account for spatial correlations in MTS data, limiting their effectiveness.

Method: TERSE uses a spatial-temporal feature encoder with temporal restoration and spatial rewiring tasks to align features across domains.

Result: TERSE outperforms existing methods on three real-world datasets, demonstrating effective spatial-temporal modeling.

Conclusion: TERSE is a versatile and effective solution for MTS-SFDA, adaptable to other SFDA methods.

Abstract: Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained model from
an annotated source domain to an unlabelled target domain without accessing the
source data, thereby preserving data privacy. While existing SFDA methods have
proven effective in reducing reliance on source data, they struggle to perform
well on multivariate time series (MTS) due to their failure to consider the
intrinsic spatial correlations inherent in MTS data. These spatial correlations
are crucial for accurately representing MTS data and preserving invariant
information across domains. To address this challenge, we propose Temporal
Restoration and Spatial Rewiring (TERSE), a novel and concise SFDA method
tailored for MTS data. Specifically, TERSE comprises a customized
spatial-temporal feature encoder designed to capture the underlying
spatial-temporal characteristics, coupled with both temporal restoration and
spatial rewiring tasks to reinstate latent representations of the temporally
masked time series and the spatially masked correlated structures. During the
target adaptation phase, the target encoder is guided to produce spatially and
temporally consistent features with the source domain by leveraging the source
pre-trained temporal restoration and spatial rewiring networks. Therefore,
TERSE can effectively model and transfer spatial-temporal dependencies across
domains, facilitating implicit feature alignment. In addition, as the first
approach to simultaneously consider spatial-temporal consistency in MTS-SFDA,
TERSE can also be integrated as a versatile plug-and-play module into
established SFDA methods. Extensive experiments on three real-world time series
datasets demonstrate the effectiveness and versatility of our approach.

</details>


### [523] [ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools](https://arxiv.org/pdf/2505.21569)
*Zhucong Li, Bowei Zhang, Jin Xiao, Zhijian Zhou, Fenglei Cao, Jiaqing Liang, Yuan Qi*

Main category: cs.LG

TL;DR: ChemHAS enhances chemistry tools by optimizing agent-stacking structures, reducing prediction errors and achieving top performance in chemistry tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of LLM-based agents caused by prediction errors in chemistry tools.

Method: Proposes ChemHAS, a hierarchical agent-stacking method that optimizes structures from limited data.

Result: Achieves state-of-the-art performance in four chemistry tasks and identifies four agent-stacking behaviors.

Conclusion: ChemHAS effectively compensates for tool errors and improves interpretability, opening new possibilities for AI in science.

Abstract: Large Language Model (LLM)-based agents have demonstrated the ability to
improve performance in chemistry-related tasks by selecting appropriate tools.
However, their effectiveness remains limited by the inherent prediction errors
of chemistry tools. In this paper, we take a step further by exploring how
LLMbased agents can, in turn, be leveraged to reduce prediction errors of the
tools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking),
a simple yet effective method that enhances chemistry tools through optimizing
agent-stacking structures from limited data. ChemHAS achieves state-of-the-art
performance across four fundamental chemistry tasks, demonstrating that our
method can effectively compensate for prediction errors of the tools.
Furthermore, we identify and characterize four distinct agent-stacking
behaviors, potentially improving interpretability and revealing new
possibilities for AI agent applications in scientific research. Our code and
dataset are publicly available at https:
//anonymous.4open.science/r/ChemHAS-01E4/README.md.

</details>


### [524] [FCOS: A Two-Stage Recoverable Model Pruning Framework for Automatic Modulation Recognition](https://arxiv.org/pdf/2505.21571)
*Yao Lu, Tengfei Ma, Zeyu Wang, Zhuangzhi Chen, Dongwei Xu, Yun Lin, Qi Xuan, Guan Gui*

Main category: cs.LG

TL;DR: FCOS introduces a two-stage pruning framework for deep learning models, combining channel-level pruning and layer collapse diagnosis to achieve extreme compression with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Traditional manual modulation recognition struggles with modern wireless communication demands, and existing deep learning AMR methods face deployment challenges due to high computational costs.

Method: FCOS uses hierarchical clustering and parameter fusion for channel-level pruning, followed by a Layer Collapse Diagnosis (LaCD) module to remove collapsed layers.

Result: FCOS achieves 95.51% FLOPs and 95.31% parameter reduction with only a 0.46% accuracy drop on Sig2019-12.

Conclusion: FCOS offers an efficient solution for deploying AMR models on resource-constrained devices, outperforming existing pruning methods.

Abstract: With the rapid development of wireless communications and the growing
complexity of digital modulation schemes, traditional manual modulation
recognition methods struggle to extract reliable signal features and meet
real-time requirements in modern scenarios. Recently, deep learning based
Automatic Modulation Recognition (AMR) approaches have greatly improved
classification accuracy. However, their large model sizes and high
computational demands hinder deployment on resource-constrained devices. Model
pruning provides a general approach to reduce model complexity, but existing
weight, channel, and layer pruning techniques each present a trade-off between
compression rate, hardware acceleration, and accuracy preservation. To this
end, in this paper, we introduce FCOS, a novel Fine-to-COarse two-Stage pruning
framework that combines channel-level pruning with layer-level collapse
diagnosis to achieve extreme compression, high performance and efficient
inference. In the first stage of FCOS, hierarchical clustering and parameter
fusion are applied to channel weights to achieve channel-level pruning. Then a
Layer Collapse Diagnosis (LaCD) module uses linear probing to identify layer
collapse and removes the collapsed layers due to high channel compression
ratio. Experiments on multiple AMR benchmarks demonstrate that FCOS outperforms
existing channel and layer pruning methods. Specifically, FCOS achieves 95.51%
FLOPs reduction and 95.31% parameter reduction while still maintaining
performance close to the original ResNet56, with only a 0.46% drop in accuracy
on Sig2019-12. Code is available at https://github.com/yaolu-zjut/FCOS.

</details>


### [525] [Spectral-inspired Neural Operator for Data-efficient PDE Simulation in Physics-agnostic Regimes](https://arxiv.org/pdf/2505.21573)
*Han Wan, Rui Zhang, Hao Sun*

Main category: cs.LG

TL;DR: SINO is a novel neural operator framework that learns PDEs from limited data without known PDE terms, achieving state-of-the-art results by operating in the frequency domain and incorporating physics-aware techniques.


<details>
  <summary>Details</summary>
Motivation: Classical and data-driven PDE solvers have limitations in handling unknown physics or data-scarce scenarios. Physics-aware methods often rely on known PDE terms, restricting their applicability.

Method: SINO operates in the frequency domain with a Frequency-to-Vector module and nonlinear operator block, including a Π-Block for aliasing prevention. It uses operator distillation for efficient inference.

Result: SINO achieves state-of-the-art performance, demonstrating discretization invariance and generalization to out-of-distribution conditions, even for globally coupled systems like Navier-Stokes.

Conclusion: SINO is the first physics-aware method to accurately simulate globally coupled systems from limited data without explicit PDE terms, offering a breakthrough in PDE learning.

Abstract: Partial differential equations (PDEs) govern the spatiotemporal evolution of
various physical systems. Classical numerical solvers, while accurate, require
fine discretization and full knowledge of the governing PDEs, limiting their
applicability when the physics is unknown or fast inference is required.
Data-driven neural PDE solvers alleviate these constraints by learning from
data but demand large training datasets and perform poorly in data-scarce
regimes. Physics-aware methods mitigate data requirements by incorporating
physical knowledge yet rely on known PDE terms or local numerical schemes,
restricting their ability to handle unknown or globally coupled systems. In
this work, we propose the Spectral-inspired Neural Operator (SINO), a novel
framework that learns PDE operators from limited trajectories (as few as 2-5),
without any known PDE terms. SINO operates in the frequency domain and
introduces a Frequency-to-Vector module to learn spectral representations
analogous to derivative multipliers. To model nonlinear physical interactions,
we design a nonlinear operator block that includes a $\Pi$-Block with low-pass
filtering to prevent aliasing. Finally, we introduce an operator distillation
technique to distill the trained model for efficient inference. SINO achieves
state-of-the-art results across multiple PDE benchmarks, demonstrating strong
discretization invariance and robust generalization to out-of-distribution
initial conditions. To our knowledge, SINO is the first physics-aware method
capable of accurately simulating globally coupled systems (e.g., the
Navier-Stokes equations) from limited data without any explicit PDE terms.

</details>


### [526] [Concentration Distribution Learning from Label Distributions](https://arxiv.org/pdf/2505.21576)
*Jiawei Tang, Yuheng Jia*

Main category: cs.LG

TL;DR: The paper introduces background concentration to address the incompleteness of label distribution learning (LDL) by capturing absolute label intensities, proposing a new paradigm called concentration distribution learning (CDL).


<details>
  <summary>Details</summary>
Motivation: Label distributions in LDL lack absolute intensity information, leading to incomplete representation and confusion. The paper aims to solve this by incorporating background concentration.

Method: A novel model combining probabilistic methods and neural networks is proposed to learn label distributions and background concentrations from LDL datasets.

Result: Experiments show the approach successfully extracts background concentrations and outperforms state-of-the-art LDL methods in prediction accuracy.

Conclusion: The proposed CDL paradigm enhances LDL by incorporating background concentration, improving representation and prediction accuracy.

Abstract: Label distribution learning (LDL) is an effective method to predict the
relative label description degree (a.k.a. label distribution) of a sample.
However, the label distribution is not a complete representation of an instance
because it overlooks the absolute intensity of each label. Specifically, it's
impossible to obtain the total description degree of hidden labels that not in
the label space, which leads to the loss of information and confusion in
instances. To solve the above problem, we come up with a new concept named
background concentration to serve as the absolute description degree term of
the label distribution and introduce it into the LDL process, forming the
improved paradigm of concentration distribution learning. Moreover, we propose
a novel model by probabilistic methods and neural networks to learn label
distributions and background concentrations from existing LDL datasets.
Extensive experiments prove that the proposed approach is able to extract
background concentrations from label distributions while producing more
accurate prediction results than the state-of-the-art LDL methods. The code is
available in https://github.com/seutjw/CDL-LD.

</details>


### [527] [Fairness in Federated Learning: Fairness for Whom?](https://arxiv.org/pdf/2505.21584)
*Afaf Taik, Khaoula Chehbouni, Golnoosh Farnadi*

Main category: cs.LG

TL;DR: The paper critiques current fairness approaches in federated learning (FL) for being too narrow and abstract, proposing a harm-centered framework for more holistic and context-aware research.


<details>
  <summary>Details</summary>
Motivation: Existing FL fairness research often ignores sociotechnical contexts, focusing narrowly on system-level metrics and overlooking broader harms and stakeholder impacts.

Method: The authors conduct a critical literature analysis, annotating papers for fairness definitions, design decisions, evaluations, and use cases, identifying five recurring pitfalls.

Result: The analysis reveals flaws like server-client framing, simulation-use-case mismatches, conflated definitions, isolated interventions, and lack of multi-stakeholder alignment.

Conclusion: The paper advocates for a harm-centered framework linking fairness to risks and vulnerabilities, recommending context-aware and accountable FL fairness research.

Abstract: Fairness in federated learning has emerged as a rapidly growing area of
research, with numerous works proposing formal definitions and algorithmic
interventions. Yet, despite this technical progress, fairness in FL is often
defined and evaluated in ways that abstract away from the sociotechnical
contexts in which these systems are deployed. In this paper, we argue that
existing approaches tend to optimize narrow system level metrics, such as
performance parity or contribution-based rewards, while overlooking how harms
arise throughout the FL lifecycle and how they impact diverse stakeholders. We
support this claim through a critical analysis of the literature, based on a
systematic annotation of papers for their fairness definitions, design
decisions, evaluation practices, and motivating use cases. Our analysis reveals
five recurring pitfalls: 1) fairness framed solely through the lens of server
client architecture, 2) a mismatch between simulations and motivating use-cases
and contexts, 3) definitions that conflate protecting the system with
protecting its users, 4) interventions that target isolated stages of the
lifecycle while neglecting upstream and downstream effects, 5) and a lack of
multi-stakeholder alignment where multiple fairness definitions can be relevant
at once. Building on these insights, we propose a harm centered framework that
links fairness definitions to concrete risks and stakeholder vulnerabilities.
We conclude with recommendations for more holistic, context-aware, and
accountable fairness research in FL.

</details>


### [528] [CellCLAT: Preserving Topology and Trimming Redundancy in Self-Supervised Cellular Contrastive Learning](https://arxiv.org/pdf/2505.21587)
*Bin Qin, Qirui Ji, Jiangmeng Li, Yupeng Wang, Xuesong Wu, Jianwen Cao, Fanjiang Xu*

Main category: cs.LG

TL;DR: CellCLAT is a self-supervised learning framework for cellular topological deep learning, addressing structural constraints and semantic redundancy through adaptive trimming and parameter perturbation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome challenges in self-supervised learning for cellular complexes, which have greater expressive power but suffer from structural constraints and redundancy.

Method: CellCLAT uses parameter perturbation for augmentation without altering cellular structures and a trimming scheduler to mask redundant cells via bi-level meta-learning.

Result: CellCLAT outperforms existing self-supervised graph learning methods, validated theoretically and empirically.

Conclusion: The framework marks a significant advancement in self-supervised topological deep learning for cellular complexes.

Abstract: Self-supervised topological deep learning (TDL) represents a nascent but
underexplored area with significant potential for modeling higher-order
interactions in simplicial complexes and cellular complexes to derive
representations of unlabeled graphs. Compared to simplicial complexes, cellular
complexes exhibit greater expressive power. However, the advancement in
self-supervised learning for cellular TDL is largely hindered by two core
challenges: \textit{extrinsic structural constraints} inherent to cellular
complexes, and intrinsic semantic redundancy in cellular representations. The
first challenge highlights that traditional graph augmentation techniques may
compromise the integrity of higher-order cellular interactions, while the
second underscores that topological redundancy in cellular complexes
potentially diminish task-relevant information. To address these issues, we
introduce Cellular Complex Contrastive Learning with Adaptive Trimming
(CellCLAT), a twofold framework designed to adhere to the combinatorial
constraints of cellular complexes while mitigating informational redundancy.
Specifically, we propose a parameter perturbation-based augmentation method
that injects controlled noise into cellular interactions without altering the
underlying cellular structures, thereby preserving cellular topology during
contrastive learning. Additionally, a cellular trimming scheduler is employed
to mask gradient contributions from task-irrelevant cells through a bi-level
meta-learning approach, effectively removing redundant topological elements
while maintaining critical higher-order semantics. We provide theoretical
justification and empirical validation to demonstrate that CellCLAT achieves
substantial improvements over existing self-supervised graph learning methods,
marking a significant attempt in this domain.

</details>


### [529] [Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning](https://arxiv.org/pdf/2505.21591)
*Maosen Zhao, Pengtao Chen, Chong Yu, Yan Wen, Xudong Tan, Tao Chen*

Main category: cs.LG

TL;DR: The paper proposes a mixup-sign floating-point quantization (MSFP) framework to achieve superior 4-bit quantization for diffusion models, addressing challenges like asymmetric activations and fine-tuning misalignment.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 4-bit quantization in diffusion models struggle with performance inconsistency, prompting exploration of low-bit floating-point quantization inspired by success in large language models.

Method: The MSFP framework introduces unsigned FP quantization, timestep-aware LoRA (TALoRA), and denoising-factor loss alignment (DFA) to ensure precise fine-tuning.

Result: The method achieves superior performance in 4-bit FP quantization for diffusion models, outperforming existing PTQ fine-tuning methods in 4-bit INT quantization.

Conclusion: The proposed MSFP framework successfully addresses key challenges in low-bit quantization for diffusion models, setting a new benchmark for 4-bit performance.

Abstract: Model quantization reduces the bit-width of weights and activations,
improving memory efficiency and inference speed in diffusion models. However,
achieving 4-bit quantization remains challenging. Existing methods, primarily
based on integer quantization and post-training quantization fine-tuning,
struggle with inconsistent performance. Inspired by the success of
floating-point (FP) quantization in large language models, we explore low-bit
FP quantization for diffusion models and identify key challenges: the failure
of signed FP quantization to handle asymmetric activation distributions, the
insufficient consideration of temporal complexity in the denoising process
during fine-tuning, and the misalignment between fine-tuning loss and
quantization error. To address these challenges, we propose the mixup-sign
floating-point quantization (MSFP) framework, first introducing unsigned FP
quantization in model quantization, along with timestep-aware LoRA (TALoRA) and
denoising-factor loss alignment (DFA), which ensure precise and stable
fine-tuning. Extensive experiments show that we are the first to achieve
superior performance in 4-bit FP quantization for diffusion models,
outperforming existing PTQ fine-tuning methods in 4-bit INT quantization.

</details>


### [530] [Relevance-driven Input Dropout: an Explanation-guided Regularization Technique](https://arxiv.org/pdf/2505.21595)
*Shreyas Gururaj, Lars Grüne, Wojciech Samek, Sebastian Lapuschkin, Leander Weber*

Main category: cs.LG

TL;DR: The paper introduces RelDrop, a data augmentation method that selectively occludes the most relevant input regions to improve model generalization by encouraging the use of other important features.


<details>
  <summary>Details</summary>
Motivation: Overfitting in ML models reduces generalization, and existing occlusion-based augmentation methods lack focus on regions influencing model decisions.

Method: Proposes RelDrop, which selectively occludes highly relevant input regions to diversify feature usage and improve generalization.

Result: Experiments show RelDrop enhances robustness to occlusion, increases feature utilization, and improves generalization performance.

Conclusion: RelDrop is an effective informed regularization technique for improving model generalization and robustness.

Abstract: Overfitting is a well-known issue extending even to state-of-the-art (SOTA)
Machine Learning (ML) models, resulting in reduced generalization, and a
significant train-test performance gap. Mitigation measures include a
combination of dropout, data augmentation, weight decay, and other
regularization techniques. Among the various data augmentation strategies,
occlusion is a prominent technique that typically focuses on randomly masking
regions of the input during training. Most of the existing literature
emphasizes randomness in selecting and modifying the input features instead of
regions that strongly influence model decisions. We propose Relevance-driven
Input Dropout (RelDrop), a novel data augmentation method which selectively
occludes the most relevant regions of the input, nudging the model to use other
important features in the prediction process, thus improving model
generalization through informed regularization. We further conduct qualitative
and quantitative analyses to study how Relevance-driven Input Dropout (RelDrop)
affects model decision-making. Through a series of experiments on benchmark
datasets, we demonstrate that our approach improves robustness towards
occlusion, results in models utilizing more features within the region of
interest, and boosts inference time generalization performance. Our code is
available at https://github.com/Shreyas-Gururaj/LRP_Relevance_Dropout.

</details>


### [531] [SOSBENCH: Benchmarking Safety Alignment on Scientific Knowledge](https://arxiv.org/pdf/2505.21605)
*Fengqing Jiang, Fengbo Ma, Zhangchen Xu, Yuetai Li, Bhaskar Ramasubramanian, Luyao Niu, Bo Li, Xianyan Chen, Zhen Xiang, Radha Poovendran*

Main category: cs.LG

TL;DR: SOSBench is a new benchmark to evaluate LLM safety in high-risk scientific domains, revealing alarming rates of harmful responses from advanced models.


<details>
  <summary>Details</summary>
Motivation: Existing safety benchmarks fail to assess LLM resilience in knowledge-intensive, hazardous scenarios, leaving a critical gap in safety evaluation.

Method: SOSBench includes 3,000 prompts from six high-risk domains, expanded via an LLM-assisted evolutionary pipeline to simulate realistic misuse.

Result: Advanced models like Deepseek-R1 and GPT-4.1 show high rates of policy-violating content (79.1% and 47.3%, respectively).

Conclusion: The findings highlight significant safety alignment deficiencies in LLMs, raising urgent concerns for responsible deployment.

Abstract: Large language models (LLMs) exhibit advancing capabilities in complex tasks,
such as reasoning and graduate-level question answering, yet their resilience
against misuse, particularly involving scientifically sophisticated risks,
remains underexplored. Existing safety benchmarks typically focus either on
instructions requiring minimal knowledge comprehension (e.g., ``tell me how to
build a bomb") or utilize prompts that are relatively low-risk (e.g.,
multiple-choice or classification tasks about hazardous content). Consequently,
they fail to adequately assess model safety when handling knowledge-intensive,
hazardous scenarios.
  To address this critical gap, we introduce SOSBench, a regulation-grounded,
hazard-focused benchmark encompassing six high-risk scientific domains:
chemistry, biology, medicine, pharmacology, physics, and psychology. The
benchmark comprises 3,000 prompts derived from real-world regulations and laws,
systematically expanded via an LLM-assisted evolutionary pipeline that
introduces diverse, realistic misuse scenarios (e.g., detailed explosive
synthesis instructions involving advanced chemical formulas). We evaluate
frontier models within a unified evaluation framework using our SOSBench.
Despite their alignment claims, advanced models consistently disclose
policy-violating content across all domains, demonstrating alarmingly high
rates of harmful responses (e.g., 79.1% for Deepseek-R1 and 47.3% for GPT-4.1).
These results highlight significant safety alignment deficiencies and
underscore urgent concerns regarding the responsible deployment of powerful
LLMs.

</details>


### [532] [Learning Where to Learn: Training Distribution Selection for Provable OOD Performance](https://arxiv.org/pdf/2505.21626)
*Nicolas Guerra, Nicholas H. Nelsen, Yunan Yang*

Main category: cs.LG

TL;DR: The paper addresses out-of-distribution (OOD) generalization in machine learning by proposing methods to optimize training data distributions for better OOD performance.


<details>
  <summary>Details</summary>
Motivation: Models often fail when tested on unseen or shifted data distributions, highlighting the need for robust OOD generalization strategies.

Method: The paper introduces two algorithmic strategies: (i) bilevel optimization for OOD risk minimization and (ii) minimizing a theoretical upper bound on OOD error.

Result: The proposed methods outperform standard empirical risk minimization, improving OOD accuracy significantly.

Conclusion: Distribution-aware training is a promising framework for robust OOD generalization.

Abstract: Out-of-distribution (OOD) generalization remains a fundamental challenge in
machine learning. Models trained on one data distribution often experience
substantial performance degradation when evaluated on shifted or unseen
domains. To address this challenge, the present paper studies the design of
training data distributions that maximize average-case OOD performance. First,
a theoretical analysis establishes a family of generalization bounds that
quantify how the choice of training distribution influences OOD error across a
predefined family of target distributions. These insights motivate the
introduction of two complementary algorithmic strategies: (i) directly
formulating OOD risk minimization as a bilevel optimization problem over the
space of probability measures and (ii) minimizing a theoretical upper bound on
OOD error. Last, the paper evaluates the two approaches across a range of
function approximation and operator learning examples. The proposed methods
significantly improve OOD accuracy over standard empirical risk minimization
with a fixed distribution. These results highlight the potential of
distribution-aware training as a principled and practical framework for robust
OOD generalization.

</details>


### [533] [Apprenticeship learning with prior beliefs using inverse optimization](https://arxiv.org/pdf/2505.21639)
*Mauricio Junca, Esteban Leiva*

Main category: cs.LG

TL;DR: The paper explores the relationship between inverse reinforcement learning (IRL) and inverse optimization (IO) for MDPs, introducing a framework that incorporates prior beliefs on cost functions and shows AL as a special case. It formulates AL as a regularized min-max problem, using SMD for solution, with numerical results emphasizing regularization's importance.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between IRL and IO for MDPs, which address similar problems but lack exploration of their relationship. The work aims to incorporate prior beliefs into IRL and AL, showing AL as a relaxation of their framework.

Method: Incorporates prior beliefs into IRL and AL, formulating AL as a regularized min-max problem. Uses stochastic mirror descent (SMD) to solve the problem and establishes convergence bounds.

Result: Demonstrates that AL formalism is a special case of their framework. Numerical experiments show the critical role of regularization in learning cost vectors and apprentice policies.

Conclusion: The framework successfully links IRL and IO for MDPs, with regularization playing a key role in addressing IRL's ill-posedness. The method is validated through theoretical bounds and numerical results.

Abstract: The relationship between inverse reinforcement learning (IRL) and inverse
optimization (IO) for Markov decision processes (MDPs) has been relatively
underexplored in the literature, despite addressing the same problem. In this
work, we revisit the relationship between the IO framework for MDPs, IRL, and
apprenticeship learning (AL). We incorporate prior beliefs on the structure of
the cost function into the IRL and AL problems, and demonstrate that the
convex-analytic view of the AL formalism (Kamoutsi et al., 2021) emerges as a
relaxation of our framework. Notably, the AL formalism is a special case in our
framework when the regularization term is absent. Focusing on the suboptimal
expert setting, we formulate the AL problem as a regularized min-max problem.
The regularizer plays a key role in addressing the ill-posedness of IRL by
guiding the search for plausible cost functions. To solve the resulting
regularized-convex-concave-min-max problem, we use stochastic mirror descent
(SMD) and establish convergence bounds for the proposed method. Numerical
experiments highlight the critical role of regularization in learning cost
vectors and apprentice policies.

</details>


### [534] [Efficient Diffusion Models for Symmetric Manifolds](https://arxiv.org/pdf/2505.21640)
*Oren Mangoubi, Neil He, Nisheeth K. Vishnoi*

Main category: cs.LG

TL;DR: A new framework for efficient diffusion models on symmetric-space Riemannian manifolds, bypassing heat kernel computations and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing manifold diffusion models rely on heat kernels, which are computationally expensive due to lack of closed-form expressions and high arithmetic operations.

Method: Introduces a diffusion model with spatially-varying covariance, leveraging Euclidean Brownian motion projection and minimizing a novel objective via Ito's Lemma for efficiency.

Result: Achieves O(1) gradient evaluations and nearly-linear-in-d arithmetic operations, improving training speed and sample quality on synthetic datasets.

Conclusion: The proposed model reduces computational costs and outperforms prior methods in efficiency and sample quality for symmetric manifolds.

Abstract: We introduce a framework for designing efficient diffusion models for
$d$-dimensional symmetric-space Riemannian manifolds, including the torus,
sphere, special orthogonal group and unitary group. Existing manifold diffusion
models often depend on heat kernels, which lack closed-form expressions and
require either $d$ gradient evaluations or exponential-in-$d$ arithmetic
operations per training step. We introduce a new diffusion model for symmetric
manifolds with a spatially-varying covariance, allowing us to leverage a
projection of Euclidean Brownian motion to bypass heat kernel computations. Our
training algorithm minimizes a novel efficient objective derived via Ito's
Lemma, allowing each step to run in $O(1)$ gradient evaluations and
nearly-linear-in-$d$ ($O(d^{1.19})$) arithmetic operations, reducing the gap
between diffusions on symmetric manifolds and Euclidean space. Manifold
symmetries ensure the diffusion satisfies an "average-case" Lipschitz
condition, enabling accurate and efficient sample generation. Empirically, our
model outperforms prior methods in training speed and improves sample quality
on synthetic datasets on the torus, special orthogonal group, and unitary
group.

</details>


### [535] [PrivATE: Differentially Private Confidence Intervals for Average Treatment Effects](https://arxiv.org/pdf/2505.21641)
*Maresa Schröder, Justin Hartenstein, Stefan Feuerriegel*

Main category: cs.LG

TL;DR: PrivATE is a differentially private framework for computing confidence intervals (CIs) for the average treatment effect (ATE), ensuring valid uncertainty quantification while preserving data privacy.


<details>
  <summary>Details</summary>
Motivation: In medicine, reliable ATE estimation requires valid CIs, but sensitive data necessitates privacy-preserving methods.

Method: PrivATE uses output perturbation for private ATE and variance estimation, then constructs CIs accounting for estimation and privatization uncertainty.

Result: The framework is model-agnostic, doubly robust, and validated on synthetic and real-world medical datasets.

Conclusion: PrivATE is the first general, doubly robust framework for valid ATE CIs under differential privacy.

Abstract: The average treatment effect (ATE) is widely used to evaluate the
effectiveness of drugs and other medical interventions. In safety-critical
applications like medicine, reliable inferences about the ATE typically require
valid uncertainty quantification, such as through confidence intervals (CIs).
However, estimating treatment effects in these settings often involves
sensitive data that must be kept private. In this work, we present PrivATE, a
novel machine learning framework for computing CIs for the ATE under
differential privacy. Specifically, we focus on deriving valid
privacy-preserving CIs for the ATE from observational data. Our PrivATE
framework consists of three steps: (i) estimating a differentially private ATE
through output perturbation; (ii) estimating the differentially private
variance through a truncated output perturbation mechanism; and (iii)
constructing the CIs while accounting for the uncertainty from both the
estimation and privatization steps. Our PrivATE framework is model agnostic,
doubly robust, and ensures valid CIs. We demonstrate the effectiveness of our
framework using synthetic and real-world medical datasets. To the best of our
knowledge, we are the first to derive a general, doubly robust framework for
valid CIs of the ATE under ($\varepsilon$, $\delta$)-differential privacy.

</details>


### [536] [AutoSGD: Automatic Learning Rate Selection for Stochastic Gradient Descent](https://arxiv.org/pdf/2505.21651)
*Nikola Surjanovic, Alexandre Bouchard-Côté, Trevor Campbell*

Main category: cs.LG

TL;DR: AutoSGD automates learning rate adjustment in SGD, reducing tuning effort while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Manual tuning of learning rates in SGD is time-consuming and non-trivial.

Method: AutoSGD dynamically adjusts the learning rate during iterations based on conditions.

Result: Empirical results show strong performance on optimization and ML tasks.

Conclusion: AutoSGD offers an efficient, automated alternative to manual learning rate tuning.

Abstract: The learning rate is an important tuning parameter for stochastic gradient
descent (SGD) and can greatly influence its performance. However, appropriate
selection of a learning rate schedule across all iterations typically requires
a non-trivial amount of user tuning effort. To address this, we introduce
AutoSGD: an SGD method that automatically determines whether to increase or
decrease the learning rate at a given iteration and then takes appropriate
action. We introduce theory supporting the convergence of AutoSGD, along with
its deterministic counterpart for standard gradient descent. Empirical results
suggest strong performance of the method on a variety of traditional
optimization problems and machine learning tasks.

</details>


### [537] [PreGenie: An Agentic Framework for High-quality Visual Presentation Generation](https://arxiv.org/pdf/2505.21660)
*Xiaojie Xu, Xinli Xu, Sirui Chen, Haoyu Chen, Fan Zhang, Ying-Cong Chen*

Main category: cs.LG

TL;DR: PreGenie is a framework using MLLMs to automate high-quality visual presentations, addressing layout, summarization, and image-text alignment issues.


<details>
  <summary>Details</summary>
Motivation: Existing automated presentation tools suffer from poor layouts, inaccurate text, and mismatched visuals, limiting their use in formal settings.

Method: PreGenie uses a two-stage process (Analysis/Initial Generation and Review/Re-generation) with MLLMs to create and refine presentations via Markdown.

Result: PreGenie outperforms existing models in aesthetics, content consistency, and human design alignment.

Conclusion: PreGenie offers a robust solution for automated, high-quality visual presentations, leveraging MLLMs for improved performance.

Abstract: Visual presentations are vital for effective communication. Early attempts to
automate their creation using deep learning often faced issues such as poorly
organized layouts, inaccurate text summarization, and a lack of image
understanding, leading to mismatched visuals and text. These limitations
restrict their application in formal contexts like business and scientific
research. To address these challenges, we propose PreGenie, an agentic and
modular framework powered by multimodal large language models (MLLMs) for
generating high-quality visual presentations.
  PreGenie is built on the Slidev presentation framework, where slides are
rendered from Markdown code. It operates in two stages: (1) Analysis and
Initial Generation, which summarizes multimodal input and generates initial
code, and (2) Review and Re-generation, which iteratively reviews intermediate
code and rendered slides to produce final, high-quality presentations. Each
stage leverages multiple MLLMs that collaborate and share information.
Comprehensive experiments demonstrate that PreGenie excels in multimodal
understanding, outperforming existing models in both aesthetics and content
consistency, while aligning more closely with human design preferences.

</details>


### [538] [Efficient Controllable Diffusion via Optimal Classifier Guidance](https://arxiv.org/pdf/2505.21666)
*Owen Oertell, Shikun Sun, Yiding Chen, Jin Peng Zhou, Zhiyong Wang, Wen Sun*

Main category: cs.LG

TL;DR: SLCD is a supervised learning method for controllable diffusion models, avoiding RL's overfitting and resource issues, and provably converges to optimal solutions.


<details>
  <summary>Details</summary>
Motivation: To address the overfitting and resource-heavy nature of RL-based fine-tuning in controllable generation tasks.

Method: Uses iterative online data generation and trains a small classifier to guide diffusion models, avoiding RL complexities.

Result: SLCD achieves high-quality samples with minimal inference time overhead in image and sequence generation.

Conclusion: SLCD offers a simpler, provably convergent alternative to RL for controllable diffusion models.

Abstract: The controllable generation of diffusion models aims to steer the model to
generate samples that optimize some given objective functions. It is desirable
for a variety of applications including image generation, molecule generation,
and DNA/sequence generation. Reinforcement Learning (RL) based fine-tuning of
the base model is a popular approach but it can overfit the reward function
while requiring significant resources. We frame controllable generation as a
problem of finding a distribution that optimizes a KL-regularized objective
function. We present SLCD -- Supervised Learning based Controllable Diffusion,
which iteratively generates online data and trains a small classifier to guide
the generation of the diffusion model. Similar to the standard
classifier-guided diffusion, SLCD's key computation primitive is classification
and does not involve any complex concepts from RL or control. Via a reduction
to no-regret online learning analysis, we show that under KL divergence, the
output from SLCD provably converges to the optimal solution of the
KL-regularized objective. Further, we empirically demonstrate that SLCD can
generate high quality samples with nearly the same inference time as the base
model in both image generation with continuous diffusion and biological
sequence generation with discrete diffusion. Our code is available at
https://github.com/Owen-Oertell/slcd

</details>


### [539] [What happens when generative AI models train recursively on each others' generated outputs?](https://arxiv.org/pdf/2505.21677)
*Hung Ahn Vu, Galen Reeves, Emily Wenger*

Main category: cs.LG

TL;DR: The paper explores the impact of AI models training on outputs from other AI models, highlighting benefits like exposure to novel concepts but also risks of performance homogenization.


<details>
  <summary>Details</summary>
Motivation: The increasing reliance on generative AI (genAI) and the internet's dual role as a source of training data and AI-generated content necessitate understanding the effects of models training on each other's outputs.

Method: The study provides empirical evidence, develops a theoretical model, and conducts experiments to analyze data-mediated interactions between genAI models.

Result: Data-mediated interactions can expose models to new concepts but may also lead to homogenized performance on shared tasks.

Conclusion: Understanding the long-term effects of AI models training on each other's outputs is crucial due to society's growing dependence on genAI tools.

Abstract: The internet is full of AI-generated content while also serving as a common
source of training data for generative AI (genAI) models. This duality raises
the possibility that future genAI models may be trained on other models'
generated outputs. Prior work has studied consequences of models training on
their own generated outputs, but limited work has considered what happens if
models ingest content produced by other models. Given society's increasing
dependence on genAI tools, understanding downstream effects of such
data-mediated model interactions is critical. To this end, we provide empirical
evidence for how data-mediated interactions might unfold in practice, develop a
theoretical model for this interactive training process, and show
experimentally possible long-term results of such interactions. We find that
data-mediated interactions can benefit models by exposing them to novel
concepts perhaps missed in original training data, but also can homogenize
their performance on shared tasks.

</details>


### [540] [multivariateGPT: a decoder-only transformer for multivariate categorical and numeric data](https://arxiv.org/pdf/2505.21680)
*Andrew J. Loza, Jun Yup Kim, Shangzheng Song, Yihang Liu, Joseph J. Y. Sung, R Andrew Taylor, Dennis L. Shung*

Main category: cs.LG

TL;DR: The paper introduces multivariateGPT, a transformer-based model for mixed categorical and numeric data, addressing limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with mixed data types (categorical and numeric) and irregular sampling intervals, prompting the need for a unified approach.

Method: The proposed multivariateGPT uses autoregressive sequence decomposition, embedding, and a joint distribution loss function to handle mixed data.

Result: The model effectively learns patterns in physical systems and complex time series like electrocardiograms and health records.

Conclusion: multivariateGPT extends transformer models to mixed data types, improving versatility and performance.

Abstract: Real-world processes often generate data that are a mix of categorical and
numeric values that are recorded at irregular and informative intervals.
Discrete token-based approaches are limited in numeric representation capacity
while methods like neural ordinary differential equations are not well suited
for categorical data or informative sampling and require augmentation to handle
certain classes of trajectories. Here, we present multivariateGPT, a single
architecture for modeling sequences of mixed categorical (including tokenized
text) and numeric data. This is accomplished with an autoregressive sequence
decomposition, embedding scheme, and loss function that extend the next token
prediction task to likelihood estimation of the joint distribution of next
token class and value. We demonstrate how this approach can efficiently learn
to generalize patterns in simple physical systems and model complex time series
including electrocardiograms and multivariate electronic health record data.
This work extends the utility of transformer based models to additional classes
of data.

</details>


### [541] [Incentivizing Permissionless Distributed Learning of LLMs](https://arxiv.org/pdf/2505.21684)
*Joel Lidin, Amir Sarfi, Evangelos Pappas, Samuel Dare, Eugene Belilovsky, Jacob Steeves*

Main category: cs.LG

TL;DR: An incentive system called Gauntlet rewards peers for contributions in distributed deep learning of foundational models, deployed on the Bittensor blockchain to train a 1.2B LLM with permissionless pseudo-gradient contributions.


<details>
  <summary>Details</summary>
Motivation: To create a decentralized and permissionless system for training large models by incentivizing contributions from peers without hardware or user restrictions.

Method: Uses a two-stage mechanism for filtering peer reliability and synchronization, estimates loss changes from pseudo-gradient contributions, employs OpenSkill for tracking competitiveness, and ensures unique computations.

Result: Successfully trained a competitive 1.2B LLM with real-valued token rewards for contributors, proving the system's utility.

Conclusion: Gauntlet demonstrates an effective incentive system for distributed deep learning, enabling scalable and permissionless model training.

Abstract: We describe an incentive system for distributed deep learning of foundational
models where peers are rewarded for contributions. The incentive system,
\textit{Gauntlet}, has been deployed on the bittensor blockchain and used to
train a 1.2B LLM with completely permissionless contributions of
pseudo-gradients: no control over the users that can register or their
hardware. \textit{Gauntlet} can be applied to any synchronous distributed
training scheme that relies on aggregating updates or pseudo-gradients. We rely
on a two-stage mechanism for fast filtering of peer uptime, reliability, and
synchronization, combined with the core component that estimates the loss
before and after individual pseudo-gradient contributions. We utilized an
OpenSkill rating system to track competitiveness of pseudo-gradient scores
across time. Finally, we introduce a novel mechanism to ensure peers on the
network perform unique computations. Our live 1.2B run, which has paid out
real-valued tokens to participants based on the value of their contributions,
yielded a competitive (on a per-iteration basis) 1.2B model that demonstrates
the utility of our incentive system.

</details>


### [542] [AMSFL: Adaptive Multi-Step Federated Learning via Gradient Difference-Based Error Modeling](https://arxiv.org/pdf/2505.21695)
*Ganglou Xu*

Main category: cs.LG

TL;DR: The paper proposes Gradient Difference Approximation (GDA) for efficient error estimation in federated learning, integrated into the AMSFL framework.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between communication efficiency and model accuracy in federated learning.

Method: Gradient Difference Approximation (GDA) uses first-order information to estimate local error trends without full Hessian computation.

Result: GDA enables efficient error modeling in large-scale multi-step adaptive training (AMSFL).

Conclusion: GDA is a lightweight, effective solution for federated learning challenges.

Abstract: Federated learning faces critical challenges in balancing communication
efficiency and model accuracy. One key issue lies in the approximation of
update errors without incurring high computational costs. In this paper, we
propose a lightweight yet effective method called Gradient Difference
Approximation (GDA), which leverages first-order information to estimate local
error trends without computing the full Hessian matrix. The proposed method
forms a key component of the Adaptive Multi-Step Federated Learning (AMSFL)
framework and provides a unified error modeling strategy for large-scale
multi-step adaptive training environments.

</details>


### [543] [Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient Sequence Modeling](https://arxiv.org/pdf/2505.21717)
*Mónika Farsang, Ramin Hasani, Radu Grosu*

Main category: cs.LG

TL;DR: LrcSSM is a nonlinear recurrent model that processes long sequences efficiently, matching the speed of linear state-space layers while offering parallel processing, gradient stability, and compute-optimal scaling.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies of existing models (e.g., Liquid-S4, Mamba) in processing long sequences, particularly their lack of gradient stability and high computational costs.

Method: LrcSSM uses a diagonal state-transition matrix learned at every step, enabling parallel processing via a single prefix-scan. It achieves O(TD) time/memory and O(log T) sequential depth.

Result: LrcSSM outperforms LRU, S5, and Mamba on long-range forecasting tasks, with compute-optimal scaling and lower memory overhead.

Conclusion: LrcSSM is a scalable, efficient alternative to existing models for long-sequence tasks, combining speed, stability, and performance.

Abstract: We present LrcSSM, a \textit{nonlinear} recurrent model that processes long
sequences as fast as today's linear state-space layers. By forcing the
state-transition matrix to be diagonal and learned at every step, the full
sequence can be solved in parallel with a single prefix-scan, giving
$\mathcal{O}(TD)$ time and memory and only $\mathcal{O}(\log T)$ sequential
depth, for input-sequence length $T$ and a state dimension $D$. Moreover,
LrcSSM offers a formal gradient-stability guarantee that other input-varying
systems such as Liquid-S4 and Mamba do not provide. Lastly, for network depth
$L$, as the forward and backward passes cost $\Theta(T\,D\,L)$ FLOPs, with its
low sequential depth and parameter count $\Theta(D\,L)$, the model follows the
compute-optimal scaling law regime ($\beta \approx 0.42$) recently observed for
Mamba, outperforming quadratic-attention Transformers at equal compute while
avoiding the memory overhead of FFT-based long convolutions. We show that on a
series of long-range forecasting tasks, LrcSSM outperforms LRU, S5 and Mamba.

</details>


### [544] [Saddle-To-Saddle Dynamics in Deep ReLU Networks: Low-Rank Bias in the First Saddle Escape](https://arxiv.org/pdf/2505.21722)
*Ioannis Bantzis, James B. Simon, Arthur Jacot*

Main category: cs.LG

TL;DR: The paper explores how gradient descent (GD) in deep ReLU networks initially gets stuck near the origin due to a saddle point. It identifies 'escape directions' that help GD move away, showing these directions exhibit a low-rank bias in deeper layers. The findings aim to understand Saddle-to-Saddle dynamics in training.


<details>
  <summary>Details</summary>
Motivation: To analyze the initial behavior of GD in deep ReLU networks, particularly how it escapes saddle points, and to lay groundwork for understanding Saddle-to-Saddle dynamics.

Method: The study examines escape directions in parameter space, analogous to Hessian eigenvectors, and proves their properties, including low-rank bias in deeper layers.

Result: The optimal escape direction in deeper layers shows a low-rank bias, with the first singular value significantly larger than others.

Conclusion: This work provides insights into GD dynamics in deep ReLU networks and is a step toward proving Saddle-to-Saddle behavior during training.

Abstract: When a deep ReLU network is initialized with small weights, GD is at first
dominated by the saddle at the origin in parameter space. We study the
so-called escape directions, which play a similar role as the eigenvectors of
the Hessian for strict saddles. We show that the optimal escape direction
features a low-rank bias in its deeper layers: the first singular value of the
$\ell$-th layer weight matrix is at least $\ell^{\frac{1}{4}}$ larger than any
other singular value. We also prove a number of related results about these
escape directions. We argue that this result is a first step in proving
Saddle-to-Saddle dynamics in deep ReLU networks, where GD visits a sequence of
saddles with increasing bottleneck rank.

</details>


### [545] [OptiMindTune: A Multi-Agent Framework for Intelligent Hyperparameter Optimization](https://arxiv.org/pdf/2505.19205)
*Meher Bhaskar Madiraju, Meher Sai Preetam Madiraju*

Main category: cs.LG

TL;DR: OptiMindTune is a multi-agent framework for hyperparameter optimization (HPO), using three AI agents to improve efficiency and performance over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional HPO methods struggle with high dimensionality, interdependencies, and computational costs, necessitating a more intelligent approach.

Method: OptiMindTune employs three specialized AI agents (Recommender, Evaluator, Decision) powered by Gemini models, enabling collaborative and dynamic HPO.

Result: The framework aims to converge to optimal hyperparameters faster and more robustly than single-agent or monolithic approaches.

Conclusion: OptiMindTune's multi-agent paradigm is promising for addressing the complexity of modern machine learning model tuning.

Abstract: Hyperparameter optimization (HPO) is a critical yet challenging aspect of
machine learning model development, significantly impacting model performance
and generalization. Traditional HPO methods often struggle with high
dimensionality, complex interdependencies, and computational expense. This
paper introduces OptiMindTune, a novel multi-agent framework designed to
intelligently and efficiently optimize hyperparameters. OptiMindTune leverages
the collaborative intelligence of three specialized AI agents -- a Recommender
Agent, an Evaluator Agent, and a Decision Agent -- each powered by Google's
Gemini models. These agents address distinct facets of the HPO problem, from
model selection and hyperparameter suggestion to robust evaluation and
strategic decision-making. By fostering dynamic interactions and knowledge
sharing, OptiMindTune aims to converge to optimal hyperparameter configurations
more rapidly and robustly than existing single-agent or monolithic approaches.
Our framework integrates principles from advanced large language models, and
adaptive search to achieve scalable and intelligent AutoML. We posit that this
multi-agent paradigm offers a promising avenue for tackling the increasing
complexity of modern machine learning model tuning.

</details>


### [546] [Deep Reinforcement Learning Agents are not even close to Human Intelligence](https://arxiv.org/pdf/2505.21731)
*Quentin Delfosse, Jannis Blüml, Fabian Tatai, Théo Vincent, Bjarne Gregori, Elisabeth Dillies, Jan Peters, Constantin Rothkopf, Kristian Kersting*

Main category: cs.LG

TL;DR: RL agents struggle with zero-shot adaptation on simpler task versions, unlike humans, revealing reliance on shortcuts.


<details>
  <summary>Details</summary>
Motivation: To evaluate RL agents' performance on simplified tasks, uncovering their reliance on shortcuts and lack of human-like adaptability.

Method: Introduces HackAtari, a set of task variations in the Arcade Learning Environment, testing multiple RL algorithms and architectures.

Result: RL agents show significant performance drops on simpler tasks, highlighting a gap in behavioral intelligence compared to humans.

Conclusion: New benchmarks and methodologies are needed to enforce systematic generalization testing beyond static evaluation protocols.

Abstract: Deep reinforcement learning (RL) agents achieve impressive results in a wide
variety of tasks, but they lack zero-shot adaptation capabilities. While most
robustness evaluations focus on tasks complexifications, for which human also
struggle to maintain performances, no evaluation has been performed on tasks
simplifications. To tackle this issue, we introduce HackAtari, a set of task
variations of the Arcade Learning Environments. We use it to demonstrate that,
contrary to humans, RL agents systematically exhibit huge performance drops on
simpler versions of their training tasks, uncovering agents' consistent
reliance on shortcuts. Our analysis across multiple algorithms and
architectures highlights the persistent gap between RL agents and human
behavioral intelligence, underscoring the need for new benchmarks and
methodologies that enforce systematic generalization testing beyond static
evaluation protocols. Training and testing in the same environment is not
enough to obtain agents equipped with human-like intelligence.

</details>


### [547] [LaX: Boosting Low-Rank Training of Foundation Models via Latent Crossing](https://arxiv.org/pdf/2505.21732)
*Ruijie Zhang, Ziyue Liu, Zhengyang Wang, Zheng Zhang*

Main category: cs.LG

TL;DR: LaX enhances low-rank models by enabling cross-subspace information flow, matching full-rank performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Foundation models like ViTs and LLMs are costly to train; low-rank methods are parameter-efficient but underperform. LaX aims to bridge this gap.

Method: LaX is a plug-and-play module that facilitates information flow across low-rank subspaces, tested on ViT-Base/Large and LLaMA-like models.

Result: LaX matches or exceeds full-rank performance with 2-3× fewer parameters and improves fine-tuning tasks like arithmetic and reasoning.

Conclusion: LaX effectively boosts low-rank models, offering a cost-efficient alternative to full-rank training without sacrificing performance.

Abstract: Training foundation models such as ViTs and LLMs requires tremendous
computing cost. Low-rank matrix or tensor factorization offers a
parameter-efficient alternative, but often downgrades performance due to the
restricted parameter space. In this work, we introduce {\textbf{Latent Crossing
(LaX)}} -- a simple yet effective plug-and-play module that enhances the
capacity of low-rank models by enabling information flow across low-rank
subspaces. We extensively validate the benefits of LaX on pre-training tasks
with ViT-Base/Large and LLaMA-like models ranging from 60M to 1B parameters.
LaX boosts low-rank model performance to match or exceed the full-rank
baselines while using 2-3\(\times\) fewer parameters. When equipped with
low-rank adapters (i.e., LoRA) for fine-tuning LLaMA-7/13B, LaX consistently
improves performance on arithmetic and common sense reasoning tasks with
negligible cost.

</details>


### [548] [Simulating the Unseen: Crash Prediction Must Learn from What Did Not Happen](https://arxiv.org/pdf/2505.21743)
*Zihao Li, Xinyuan Cao, Xiangbo Gao, Kexin Tian, Keshu Wu, Mohammad Anis, Hao Zhang, Keke Long, Jiwan Jiang, Xiaopeng Li, Yunlong Zhang, Tianbao Yang, Dominique Lord, Zhengzhong Tu, Yang Zhou*

Main category: cs.LG

TL;DR: The paper proposes a shift from traditional crash-only learning to counterfactual safety learning to address the data paradox in traffic safety, aiming for Vision Zero by synthesizing near-miss events and using simulations for proactive prevention.


<details>
  <summary>Details</summary>
Motivation: The rarity of catastrophic crashes makes them hard to study, hindering traffic safety efforts. Existing methods rely on sparse, noisy data, failing to address long-tailed, high-risk scenarios.

Method: The approach combines crash-rate priors, generative scene engines, diverse driver models, and causal learning to synthesize near-miss events. A digital twin testbed links micro scenes to macro patterns, validated for statistical realism.

Result: The method transforms sparse crash data into rich predictive signals, enabling stress-testing of vehicles, roads, and policies before deployment.

Conclusion: By learning from near-misses, the paper advocates a proactive approach to traffic safety, advancing Vision Zero through counterfactual reasoning and simulation.

Abstract: Traffic safety science has long been hindered by a fundamental data paradox:
the crashes we most wish to prevent are precisely those events we rarely
observe. Existing crash-frequency models and surrogate safety metrics rely
heavily on sparse, noisy, and under-reported records, while even sophisticated,
high-fidelity simulations undersample the long-tailed situations that trigger
catastrophic outcomes such as fatalities. We argue that the path to achieving
Vision Zero, i.e., the complete elimination of traffic fatalities and severe
injuries, requires a paradigm shift from traditional crash-only learning to a
new form of counterfactual safety learning: reasoning not only about what
happened, but also about the vast set of plausible yet perilous scenarios that
could have happened under slightly different circumstances. To operationalize
this shift, our proposed agenda bridges macro to micro. Guided by crash-rate
priors, generative scene engines, diverse driver models, and causal learning,
near-miss events are synthesized and explained. A crash-focused digital twin
testbed links micro scenes to macro patterns, while a multi-objective validator
ensures that simulations maintain statistical realism. This pipeline transforms
sparse crash data into rich signals for crash prediction, enabling the
stress-testing of vehicles, roads, and policies before deployment. By learning
from crashes that almost happened, we can shift traffic safety from reactive
forensics to proactive prevention, advancing Vision Zero.

</details>


### [549] [Revisiting Bi-Linear State Transitions in Recurrent Neural Networks](https://arxiv.org/pdf/2505.21749)
*M. Reza Ebrahimi, Roland Memisevic*

Main category: cs.LG

TL;DR: The paper explores hidden units in RNNs as active computational participants, not just memory, using bi-linear operations for state tracking tasks.


<details>
  <summary>Details</summary>
Motivation: To shift focus from hidden units as passive memory to active contributors in computations, especially in state tracking tasks.

Method: Revisits bi-linear operations (multiplicative interactions between hidden units and inputs) as an inductive bias for hidden state evolution.

Result: Bi-linear operations naturally represent state evolution and form a hierarchy for task complexity, with linear RNNs like Mamba at the simplest level.

Conclusion: Bi-linear state updates offer a structured approach to modeling active hidden unit contributions in RNNs.

Abstract: The role of hidden units in recurrent neural networks is typically seen as
modeling memory, with research focusing on enhancing information retention
through gating mechanisms. A less explored perspective views hidden units as
active participants in the computation performed by the network, rather than
passive memory stores. In this work, we revisit bi-linear operations, which
involve multiplicative interactions between hidden units and input embeddings.
We demonstrate theoretically and empirically that they constitute a natural
inductive bias for representing the evolution of hidden states in state
tracking tasks. These are the simplest type of task that require hidden units
to actively contribute to the behavior of the network. We also show that
bi-linear state updates form a natural hierarchy corresponding to state
tracking tasks of increasing complexity, with popular linear recurrent networks
such as Mamba residing at the lowest-complexity center of that hierarchy.

</details>


### [550] [Hierarchical Reinforcement Learning with Uncertainty-Guided Diffusional Subgoals](https://arxiv.org/pdf/2505.21750)
*Vivienne Huiling Wang, Tinghuai Wang, Joni Pajarinen*

Main category: cs.LG

TL;DR: Proposes a hierarchical reinforcement learning (HRL) method using a conditional diffusion model with a Gaussian Process (GP) prior to improve subgoal generation and uncertainty handling.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of high-level policy generating effective subgoals due to evolving low-level policies in HRL.

Method: Trains a conditional diffusion model regularized by a GP prior for subgoal generation and uncertainty quantification, combining diffusion policy and GP predictive mean for subgoal selection.

Result: Outperforms prior HRL methods in sample efficiency and performance on continuous control benchmarks.

Conclusion: The approach effectively handles subgoal complexity and uncertainty, enhancing HRL performance.

Abstract: Hierarchical reinforcement learning (HRL) learns to make decisions on
multiple levels of temporal abstraction. A key challenge in HRL is that the
low-level policy changes over time, making it difficult for the high-level
policy to generate effective subgoals. To address this issue, the high-level
policy must capture a complex subgoal distribution while also accounting for
uncertainty in its estimates. We propose an approach that trains a conditional
diffusion model regularized by a Gaussian Process (GP) prior to generate a
complex variety of subgoals while leveraging principled GP uncertainty
quantification. Building on this framework, we develop a strategy that selects
subgoals from both the diffusion policy and GP's predictive mean. Our approach
outperforms prior HRL methods in both sample efficiency and performance on
challenging continuous control benchmarks.

</details>


### [551] [DualSchool: How Reliable are LLMs for Optimization Education?](https://arxiv.org/pdf/2505.21775)
*Michael Klamkin, Arnaud Deza, Sikai Cheng, Haoruo Zhao, Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: LLMs struggle with Primal to Dual Conversion (P2DC) tasks despite their training, as revealed by DualSchool, a framework for generating and verifying P2DC instances.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' performance on P2DC tasks, given their exposure to such content during training, and address gaps in evaluation methods for optimization models.

Method: Introduces DualSchool, a framework using Canonical Graph Edit Distance for rigorous verification of P2DC instances.

Result: State-of-the-art LLMs fail to consistently produce correct duals, even for simple instances, and struggle with related tasks like verification and error classification.

Conclusion: Highlights limitations of LLMs in optimization tasks and implications for education and large reasoning systems.

Abstract: Consider the following task taught in introductory optimization courses which
addresses challenges articulated by the community at the intersection of
(generative) AI and OR: generate the dual of a linear program. LLMs, being
trained at web-scale, have the conversion process and many instances of Primal
to Dual Conversion (P2DC) at their disposal. Students may thus reasonably
expect that LLMs would perform well on the P2DC task. To assess this
expectation, this paper introduces DualSchool, a comprehensive framework for
generating and verifying P2DC instances. The verification procedure of
DualSchool uses the Canonical Graph Edit Distance, going well beyond existing
evaluation methods for optimization models, which exhibit many false positives
and negatives when applied to P2DC. Experiments performed by DualSchool reveal
interesting findings. Although LLMs can recite the conversion procedure
accurately, state-of-the-art open LLMs fail to consistently produce correct
duals. This finding holds even for the smallest two-variable instances and for
derivative tasks, such as correctness, verification, and error classification.
The paper also discusses the implications for educators, students, and the
development of large reasoning systems.

</details>


### [552] [Memorization to Generalization: Emergence of Diffusion Models from Associative Memory](https://arxiv.org/pdf/2505.21777)
*Bao Pham, Gabriel Raya, Matteo Negri, Mohammed J. Zaki, Luca Ambrogioni, Dmitry Krotov*

Main category: cs.LG

TL;DR: The paper explores diffusion models as associative memory systems, drawing parallels to Hopfield networks. It identifies memorization and generalization phases, predicts spurious states, and validates them empirically.


<details>
  <summary>Details</summary>
Motivation: To understand diffusion models through the lens of associative memory, analyzing memorization-generalization transitions and spurious states.

Method: Conceptualizes diffusion model training as memory encoding and generation as retrieval, comparing phases to Hopfield networks.

Result: Identifies memorization and generalization phases, predicts and validates spurious states in diffusion models.

Conclusion: Provides insights into memorization-generalization in diffusion models and confirms theoretical predictions of spurious states.

Abstract: Hopfield networks are associative memory (AM) systems, designed for storing
and retrieving patterns as local minima of an energy landscape. In the
classical Hopfield model, an interesting phenomenon occurs when the amount of
training data reaches its critical memory load $- spurious\,\,states$, or
unintended stable points, emerge at the end of the retrieval dynamics, leading
to incorrect recall. In this work, we examine diffusion models, commonly used
in generative modeling, from the perspective of AMs. The training phase of
diffusion model is conceptualized as memory encoding (training data is stored
in the memory). The generation phase is viewed as an attempt of memory
retrieval. In the small data regime the diffusion model exhibits a strong
memorization phase, where the network creates distinct basins of attraction
around each sample in the training set, akin to the Hopfield model below the
critical memory load. In the large data regime, a different phase appears where
an increase in the size of the training set fosters the creation of new
attractor states that correspond to manifolds of the generated samples.
Spurious states appear at the boundary of this transition and correspond to
emergent attractor states, which are absent in the training set, but, at the
same time, have distinct basins of attraction around them. Our findings
provide: a novel perspective on the memorization-generalization phenomenon in
diffusion models via the lens of AMs, theoretical prediction of existence of
spurious states, empirical validation of this prediction in commonly-used
diffusion models.

</details>


### [553] [P-DROP: Poisson-Based Dropout for Graph Neural Networks](https://arxiv.org/pdf/2505.21783)
*Hyunsik Yun*

Main category: cs.LG

TL;DR: A novel Poisson-based node selection strategy for GNNs addresses over-smoothing by enabling asynchronous, structure-aware updates, outperforming traditional dropout methods.


<details>
  <summary>Details</summary>
Motivation: Over-smoothing in GNNs causes node representations to lose discriminative power, necessitating innovative solutions.

Method: Proposes independent Poisson clocks for nodes, enabling stochastic but structure-aware updates, applied as dropout replacement or dynamic subgraph training.

Result: Outperforms Dropout, DropEdge, and DropNode on benchmarks (Cora, Citeseer, Pubmed), especially in later training stages.

Conclusion: The Poisson-based strategy effectively mitigates over-smoothing while maintaining or improving model accuracy.

Abstract: Over-smoothing remains a major challenge in Graph Neural Networks (GNNs),
where repeated message passing causes node representations to converge and lose
discriminative power. To address this, we propose a novel node selection
strategy based on Poisson processes, introducing stochastic but structure-aware
updates. Specifically, we equip each node with an independent Poisson clock,
enabling asynchronous and localized updates that preserve structural diversity.
We explore two applications of this strategy: as a replacement for
dropout-based regularization and as a dynamic subgraph training scheme.
Experimental results on standard benchmarks (Cora, Citeseer, Pubmed)
demonstrate that our Poisson-based method yields competitive or improved
accuracy compared to traditional Dropout, DropEdge, and DropNode approaches,
particularly in later training stages.

</details>


### [554] [Born a Transformer -- Always a Transformer?](https://arxiv.org/pdf/2505.21785)
*Yana Veitsman, Mayank Jobanputra, Yash Sarrof, Aleksandra Bakalova, Vera Demberg, Ellie Pavlick, Michael Hahn*

Main category: cs.LG

TL;DR: The paper investigates whether large-scale pretrained LLMs overcome theoretical limitations of Transformers in sequence-to-sequence tasks, focusing on retrieval and copying tasks. It reveals an induction-versus-anti-induction asymmetry and shows that pretraining selectively enhances certain capabilities without overcoming fundamental limits.


<details>
  <summary>Details</summary>
Motivation: To understand if large-scale pretrained LLMs can overcome theoretical constraints of Transformers in practice, particularly in retrieval and copying tasks.

Method: Uses the C-RASP framework to study length generalization and analyzes pretrained models on retrieval tasks, comparing induction and anti-induction performance. Includes mechanistic analysis and fine-tuning experiments.

Result: Pretrained models show an induction-versus-anti-induction asymmetry, favoring right-side retrieval (induction). Fine-tuning can balance this if length-generalization is theoretically guaranteed. Pretraining enhances specific capabilities but not fundamental limits.

Conclusion: Pretraining selectively improves Transformer capabilities but does not bypass inherent architectural constraints, highlighting reliability risks in real-world tasks.

Abstract: Transformers have theoretical limitations in modeling certain
sequence-to-sequence tasks, yet it remains largely unclear if these limitations
play a role in large-scale pretrained LLMs, or whether LLMs might effectively
overcome these constraints in practice due to the scale of both the models
themselves and their pretraining data. We explore how these architectural
constraints manifest after pretraining, by studying a family of
$\textit{retrieval}$ and $\textit{copying}$ tasks inspired by Liu et al.
[2024]. We use the recently proposed C-RASP framework for studying length
generalization [Huang et al., 2025b] to provide guarantees for each of our
settings. Empirically, we observe an $\textit{induction-versus-anti-induction}$
asymmetry, where pretrained models are better at retrieving tokens to the right
(induction) rather than the left (anti-induction) of a query token. This
asymmetry disappears upon targeted fine-tuning if length-generalization is
guaranteed by theory. Mechanistic analysis reveals that this asymmetry is
connected to the differences in the strength of induction versus anti-induction
circuits within pretrained Transformers. We validate our findings through
practical experiments on real-world tasks demonstrating reliability risks. Our
results highlight that pretraining selectively enhances certain Transformer
capabilities, but does not overcome fundamental length-generalization limits.

</details>


### [555] [Faster Rates for Private Adversarial Bandits](https://arxiv.org/pdf/2505.21790)
*Hilal Asi, Vinod Raman, Kunal Talwar*

Main category: cs.LG

TL;DR: The paper introduces new differentially private algorithms for adversarial bandits and bandits with expert advice, improving regret bounds and enabling sublinear regret for smaller privacy budgets.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of designing private algorithms for bandit problems, improving upon existing bounds and establishing separations between central and local differential privacy.

Method: A simple and efficient conversion of non-private bandit algorithms to private ones, applied to adversarial bandits and bandits with expert advice.

Result: Improved regret bounds, such as $O\left(\frac{\sqrt{KT}}{\sqrt{\epsilon}}\right)$ for adversarial bandits, and first private algorithms for expert advice with sublinear regret.

Conclusion: The proposed algorithms advance the field by providing better privacy guarantees and performance, especially for smaller privacy budgets.

Abstract: We design new differentially private algorithms for the problems of
adversarial bandits and bandits with expert advice. For adversarial bandits, we
give a simple and efficient conversion of any non-private bandit algorithm to a
private bandit algorithm. Instantiating our conversion with existing
non-private bandit algorithms gives a regret upper bound of
$O\left(\frac{\sqrt{KT}}{\sqrt{\epsilon}}\right)$, improving upon the existing
upper bound $O\left(\frac{\sqrt{KT \log(KT)}}{\epsilon}\right)$ for all
$\epsilon \leq 1$. In particular, our algorithms allow for sublinear expected
regret even when $\epsilon \leq \frac{1}{\sqrt{T}}$, establishing the first
known separation between central and local differential privacy for this
problem. For bandits with expert advice, we give the first differentially
private algorithms, with expected regret
$O\left(\frac{\sqrt{NT}}{\sqrt{\epsilon}}\right),
O\left(\frac{\sqrt{KT\log(N)}\log(KT)}{\epsilon}\right)$, and
$\tilde{O}\left(\frac{N^{1/6}K^{1/2}T^{2/3}\log(NT)}{\epsilon ^{1/3}} +
\frac{N^{1/2}\log(NT)}{\epsilon}\right)$, where $K$ and $N$ are the number of
actions and experts respectively. These rates allow us to get sublinear regret
for different combinations of small and large $K, N$ and $\epsilon.$

</details>


### [556] [Multimodal Federated Learning: A Survey through the Lens of Different FL Paradigms](https://arxiv.org/pdf/2505.21792)
*Yuanzhe Peng, Jieming Bian, Lei Wang, Yin Huang, Jie Xu*

Main category: cs.LG

TL;DR: The paper introduces a taxonomy for Multimodal Federated Learning (MFL) by analyzing it through three FL paradigms: horizontal, vertical, and hybrid FL, addressing challenges like modality heterogeneity and privacy.


<details>
  <summary>Details</summary>
Motivation: To organize MFL research by FL paradigms and address unique challenges posed by multimodal data in distributed settings.

Method: Systematic examination of MFL within three FL paradigms: horizontal, vertical, and hybrid FL, including problem formulation, algorithm review, and challenge identification.

Result: A taxonomy highlighting distinct challenges (e.g., modality heterogeneity) in MFL across FL paradigms, with insights for future research.

Conclusion: The taxonomy provides a framework to understand MFL challenges and guide future advancements in the field.

Abstract: Multimodal Federated Learning (MFL) lies at the intersection of two pivotal
research areas: leveraging complementary information from multiple modalities
to improve downstream inference performance and enabling distributed training
to enhance efficiency and preserve privacy. Despite the growing interest in
MFL, there is currently no comprehensive taxonomy that organizes MFL through
the lens of different Federated Learning (FL) paradigms. This perspective is
important because multimodal data introduces distinct challenges across various
FL settings. These challenges, including modality heterogeneity, privacy
heterogeneity, and communication inefficiency, are fundamentally different from
those encountered in traditional unimodal or non-FL scenarios. In this paper,
we systematically examine MFL within the context of three major FL paradigms:
horizontal FL (HFL), vertical FL (VFL), and hybrid FL. For each paradigm, we
present the problem formulation, review representative training algorithms, and
highlight the most prominent challenge introduced by multimodal data in
distributed settings. We also discuss open challenges and provide insights for
future research. By establishing this taxonomy, we aim to uncover the novel
challenges posed by multimodal data from the perspective of different FL
paradigms and to offer a new lens through which to understand and advance the
development of MFL.

</details>


### [557] [From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs](https://arxiv.org/pdf/2505.21800)
*Stanley Yu, Vaidehi Bulusu, Oscar Yasunaga, Clayton Lau, Cole Blondin, Sean O'Brien, Kevin Zhu, Vasu Sharma*

Main category: cs.LG

TL;DR: The paper extends the concept cone framework to analyze truthfulness in LLMs, revealing multi-dimensional cones that mediate truth-related behavior, supported by causal interventions and generalization across models.


<details>
  <summary>Details</summary>
Motivation: LLMs often generate falsehoods, and prior work suggests truthfulness is represented linearly, which may not fully capture the underlying geometry. This work aims to explore a richer, multi-dimensional structure.

Method: The study extends the concept cone framework to truthfulness, identifying multi-dimensional cones in LLM activations. It uses causal interventions, generalization tests, and behavior preservation to validate findings.

Result: Multi-dimensional cones causally mediate truth-related behavior in LLMs. Interventions flip responses, cones generalize across architectures, and unrelated behavior is preserved.

Conclusion: The findings reveal a richer structure for truthfulness in LLMs and highlight concept cones as a useful tool for probing abstract behaviors.

Abstract: Large Language Models (LLMs) exhibit strong conversational abilities but
often generate falsehoods. Prior work suggests that the truthfulness of simple
propositions can be represented as a single linear direction in a model's
internal activations, but this may not fully capture its underlying geometry.
In this work, we extend the concept cone framework, recently introduced for
modeling refusal, to the domain of truth. We identify multi-dimensional cones
that causally mediate truth-related behavior across multiple LLM families. Our
results are supported by three lines of evidence: (i) causal interventions
reliably flip model responses to factual statements, (ii) learned cones
generalize across model architectures, and (iii) cone-based interventions
preserve unrelated model behavior. These findings reveal the richer,
multidirectional structure governing simple true/false propositions in LLMs and
highlight concept cones as a promising tool for probing abstract behaviors.

</details>


### [558] [Towards Operational Automated Greenhouse Gas Plume Detection](https://arxiv.org/pdf/2505.21806)
*Brian D. Bue, Jake H. Lee, Andrew K. Thorpe, Philip G. Brodrick, Daniel Cusworth, Alana Ayasse, Vassiliki Mancoridis, Anagha Satish, Shujun Xiong, Riley Duren*

Main category: cs.LG

TL;DR: The paper addresses challenges in automating GHG plume detection using CNNs, focusing on data quality, bias prevention, and modeling alignment. It demonstrates multitask learning for operational performance and provides tools for reproducibility.


<details>
  <summary>Details</summary>
Motivation: Automating GHG plume detection is critical for emissions monitoring, but operational deployment remains challenging despite advances in deep learning.

Method: Uses convolutional neural networks (CNNs) with multitask learning (instance detection and pixelwise segmentation) on multicampaign airborne and spaceborne data.

Result: CNNs achieve operational detection performance when key obstacles are addressed, with demonstrated effectiveness across emission types and regions.

Conclusion: The work provides analysis-ready data, models, and best practices to advance the field of automated GHG plume detection.

Abstract: Operational deployment of a fully automated greenhouse gas (GHG) plume
detection system remains an elusive goal for imaging spectroscopy missions,
despite recent advances in deep learning approaches. With the dramatic increase
in data availability, however, automation continues to increase in importance
for natural and anthropogenic emissions monitoring. This work reviews and
addresses several key obstacles in the field: data and label quality control,
prevention of spatiotemporal biases, and correctly aligned modeling objectives.
We demonstrate through rigorous experiments using multicampaign data from
airborne and spaceborne instruments that convolutional neural networks (CNNs)
are able to achieve operational detection performance when these obstacles are
alleviated. We demonstrate that a multitask model that learns both instance
detection and pixelwise segmentation simultaneously can successfully lead
towards an operational pathway. We evaluate the model's plume detectability
across emission source types and regions, identifying thresholds for
operational deployment. Finally, we provide analysis-ready data, models, and
source code for reproducibility, and work to define a set of best practices and
validation standards to facilitate future contributions to the field.

</details>


### [559] [TabReason: A Reinforcement Learning-Enhanced Reasoning LLM for Explainable Tabular Data Prediction](https://arxiv.org/pdf/2505.21807)
*Tommy Xu, Zhitian Zhang, Xiangyu Sun, Lauren Kelly Zung, Hossein Hajimirsadeghi, Greg Mori*

Main category: cs.LG

TL;DR: A new method combines reasoning-based LLMs with reinforcement learning for accurate and interpretable tabular data predictions.


<details>
  <summary>Details</summary>
Motivation: Existing models like gradient boosting and deep learning lack interpretability, while LLMs underperform in tabular data prediction.

Method: Uses reinforcement learning to train LLMs with custom reward functions for accuracy and human-understandable explanations.

Result: Outperforms most existing LLMs on financial benchmarks.

Conclusion: The approach successfully balances accuracy and interpretability in tabular data predictions.

Abstract: Predictive modeling on tabular data is the cornerstone of many real-world
applications. Although gradient boosting machines and some recent deep models
achieve strong performance on tabular data, they often lack interpretability.
On the other hand, large language models (LLMs) have demonstrated powerful
capabilities to generate human-like reasoning and explanations, but remain
under-performed for tabular data prediction. In this paper, we propose a new
approach that leverages reasoning-based LLMs, trained using reinforcement
learning, to perform more accurate and explainable predictions on tabular data.
Our method introduces custom reward functions that guide the model not only
toward high prediction accuracy but also toward human-understandable reasons
for its predictions. Experimental results show that our model achieves
promising performance on financial benchmark datasets, outperforming most
existing LLMs.

</details>


### [560] [Optimizing Data Augmentation through Bayesian Model Selection](https://arxiv.org/pdf/2505.21813)
*Madi Matymov, Ba-Hien Tran, Michael Kampffmeyer, Markus Heinonen, Maurizio Filippone*

Main category: cs.LG

TL;DR: The paper proposes a Bayesian framework for optimizing Data Augmentation (DA) parameters, treating them as hyperparameters and using a tractable ELBO for joint optimization with model parameters.


<details>
  <summary>Details</summary>
Motivation: Traditional DA parameter selection relies on trial-and-error or expensive validation, which is inefficient. The paper aims to provide a principled, Bayesian approach to optimize DA.

Method: A probabilistic view of DA is adopted, framing augmentation parameters as hyperparameters. A tractable ELBO is derived for joint optimization of DA and model parameters.

Result: Theoretical analysis shows strong variational approximation, generalization, and invariance properties. Experiments on vision tasks demonstrate improved calibration and robustness.

Conclusion: The work establishes a Bayesian foundation for DA optimization, offering significant potential for robust machine learning.

Abstract: Data Augmentation (DA) has become an essential tool to improve robustness and
generalization of modern machine learning. However, when deciding on DA
strategies it is critical to choose parameters carefully, and this can be a
daunting task which is traditionally left to trial-and-error or expensive
optimization based on validation performance. In this paper, we counter these
limitations by proposing a novel framework for optimizing DA. In particular, we
take a probabilistic view of DA, which leads to the interpretation of
augmentation parameters as model (hyper)-parameters, and the optimization of
the marginal likelihood with respect to these parameters as a Bayesian model
selection problem. Due to its intractability, we derive a tractable Evidence
Lower BOund (ELBO), which allows us to optimize augmentation parameters jointly
with model parameters. We provide extensive theoretical results on variational
approximation quality, generalization guarantees, invariance properties, and
connections to empirical Bayes. Through experiments on computer vision tasks,
we show that our approach improves calibration and yields robust performance
over fixed or no augmentation. Our work provides a rigorous foundation for
optimizing DA through Bayesian principles with significant potential for robust
machine learning.

</details>


### [561] [Unsupervised Latent Pattern Analysis for Estimating Type 2 Diabetes Risk in Undiagnosed Populations](https://arxiv.org/pdf/2505.21824)
*Praveen Kumar, Vincent T. Metzger, Scott A. Malec*

Main category: cs.LG

TL;DR: Proposes an unsupervised framework using NMF and statistical techniques to predict T2DM risk by analyzing multimorbidity and polypharmacy patterns.


<details>
  <summary>Details</summary>
Motivation: Addresses the lack of confirmed negative cases in supervised learning for T2DM prediction, aiming for early detection to reduce health and economic burdens.

Method: Integrates Non-negative Matrix Factorization (NMF) with statistical techniques to identify latent patterns in diagnosed T2DM patients and apply them to undiagnosed individuals.

Result: Provides an interpretable and scalable solution for estimating T2DM risk, aiding timely healthcare interventions.

Conclusion: The framework can improve patient outcomes and reduce the future burden of T2DM by enabling early risk detection.

Abstract: The global prevalence of diabetes, particularly type 2 diabetes mellitus
(T2DM), is rapidly increasing, posing significant health and economic
challenges. T2DM not only disrupts blood glucose regulation but also damages
vital organs such as the heart, kidneys, eyes, nerves, and blood vessels,
leading to substantial morbidity and mortality. In the US alone, the economic
burden of diagnosed diabetes exceeded \$400 billion in 2022. Early detection of
individuals at risk is critical to mitigating these impacts. While machine
learning approaches for T2DM prediction are increasingly adopted, many rely on
supervised learning, which is often limited by the lack of confirmed negative
cases. To address this limitation, we propose a novel unsupervised framework
that integrates Non-negative Matrix Factorization (NMF) with statistical
techniques to identify individuals at risk of developing T2DM. Our method
identifies latent patterns of multimorbidity and polypharmacy among diagnosed
T2DM patients and applies these patterns to estimate the T2DM risk in
undiagnosed individuals. By leveraging data-driven insights from comorbidity
and medication usage, our approach provides an interpretable and scalable
solution that can assist healthcare providers in implementing timely
interventions, ultimately improving patient outcomes and potentially reducing
the future health and economic burden of T2DM.

</details>


### [562] [Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones](https://arxiv.org/pdf/2505.21825)
*Parsa Mirtaheri, Ezra Edelman, Samy Jelassi, Eran Malach, Enric Boix-Adsera*

Main category: cs.LG

TL;DR: The paper explores whether sequential or parallel scaling is more effective for inference-time computation in large language models, showing that sequential scaling can offer exponential advantages in certain reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To better understand the optimal allocation of inference-time computation for improving reasoning in large language models, specifically comparing sequential and parallel scaling.

Method: Theoretical analysis and experiments on graph connectivity problems, testing various language models and chain-of-thought strategies.

Result: Sequential scaling provides exponential advantages over parallel scaling in specific reasoning settings, validated by experiments.

Conclusion: Sequential scaling is more effective in certain reasoning tasks, offering insights for optimizing inference-time computation in language models.

Abstract: Inference-time computation has emerged as a promising scaling axis for
improving large language model reasoning. However, despite yielding impressive
performance, the optimal allocation of inference-time computation remains
poorly understood. A central question is whether to prioritize sequential
scaling (e.g., longer chains of thought) or parallel scaling (e.g., majority
voting across multiple short chains of thought). In this work, we seek to
illuminate the landscape of test-time scaling by demonstrating the existence of
reasoning settings where sequential scaling offers an exponential advantage
over parallel scaling. These settings are based on graph connectivity problems
in challenging distributions of graphs. We validate our theoretical findings
with comprehensive experiments across a range of language models, including
models trained from scratch for graph connectivity with different chain of
thought strategies as well as large reasoning models.

</details>


### [563] [In Search of Adam's Secret Sauce](https://arxiv.org/pdf/2505.21829)
*Antonio Orvieto, Robert Gower*

Main category: cs.LG

TL;DR: Adam's efficacy in training transformers is studied, comparing it to simplified variants. Constraining Adam's momentum parameters to be equal preserves performance and offers theoretical insights.


<details>
  <summary>Details</summary>
Motivation: To understand why Adam is effective for training transformer-based language models and to explore simplified variants.

Method: Empirical study training over 1,300 language models, comparing Adam to simplified variants like signed momentum methods.

Result: Signed momentum methods underperform Adam. Constraining Adam's momentum parameters to be equal preserves performance and provides theoretical clarity.

Conclusion: Constraining Adam's momentum parameters offers near-optimal performance and new theoretical insights, linking it to gradient statistics estimation.

Abstract: Understanding the remarkable efficacy of Adam when training transformer-based
language models has become a central research topic within the optimization
community. To gain deeper insights, several simplifications of Adam have been
proposed, such as the signed gradient and signed momentum methods. In this
work, we conduct an extensive empirical study - training over 1,300 language
models across different data configurations and scales - comparing Adam to
several known simplified variants. We find that signed momentum methods are
faster than SGD, but consistently underperform relative to Adam, even after
careful tuning of momentum, clipping setting and learning rates. However, our
analysis reveals a compelling option that preserves near-optimal performance
while allowing for new insightful reformulations: constraining the Adam
momentum parameters to be equal. Beyond robust performance, this choice affords
new theoretical insights, highlights the "secret sauce" on top of signed
momentum, and grants a precise statistical interpretation: we show that Adam in
this setting implements a natural online algorithm for estimating the mean and
variance of gradients-one that arises from a mean-field Gaussian variational
inference perspective.

</details>


### [564] [TuneComp: Joint Fine-tuning and Compression for Large Foundation Models](https://arxiv.org/pdf/2505.21835)
*Xiangyu Chen, Jing Liu, Ye Wang, Matthew Brand, Pu, Wang, Toshiaki Koike-Akino*

Main category: cs.LG

TL;DR: The paper proposes joint fine-tuning and compression to avoid performance loss from sequential methods, outperforming traditional approaches.


<details>
  <summary>Details</summary>
Motivation: Sequential fine-tuning and compression methods sacrifice performance and create unnecessary intermediate models.

Method: Jointly fine-tune and compress the model by gradually distilling it to a pruned low-rank structure.

Result: Experiments show the joint method significantly outperforms sequential compression methods.

Conclusion: Directly constructing a smaller model while guided by the downstream task is more effective.

Abstract: To reduce model size during post-training, compression methods, including
knowledge distillation, low-rank approximation, and pruning, are often applied
after fine-tuning the model. However, sequential fine-tuning and compression
sacrifices performance, while creating a larger than necessary model as an
intermediate step. In this work, we aim to reduce this gap, by directly
constructing a smaller model while guided by the downstream task. We propose to
jointly fine-tune and compress the model by gradually distilling it to a pruned
low-rank structure. Experiments demonstrate that joint fine-tuning and
compression significantly outperforms other sequential compression methods.

</details>


### [565] [An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints](https://arxiv.org/pdf/2505.21841)
*Jiahui Zhu, Kihyun Yu, Dabeen Lee, Xin Liu, Honghao Wei*

Main category: cs.LG

TL;DR: OMDPD algorithm achieves optimal regret and strong constraint violation for online CMDPs with adversarial constraints, without relying on strict assumptions.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail in adversarial settings with unknown, time-varying constraints. The goal is to develop a robust solution for safe RL in dynamic environments.

Method: Proposes the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm for online CMDPs with adversarial constraints.

Result: OMDPD achieves O(sqrt(K)) regret and constraint violation, improving performance with accurate reward/transition estimates.

Conclusion: OMDPD provides practical guarantees for safe decision-making in adversarial environments, advancing online safe RL.

Abstract: Online safe reinforcement learning (RL) plays a key role in dynamic
environments, with applications in autonomous driving, robotics, and
cybersecurity. The objective is to learn optimal policies that maximize rewards
while satisfying safety constraints modeled by constrained Markov decision
processes (CMDPs). Existing methods achieve sublinear regret under stochastic
constraints but often fail in adversarial settings, where constraints are
unknown, time-varying, and potentially adversarially designed. In this paper,
we propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, the
first to address online CMDPs with anytime adversarial constraints. OMDPD
achieves optimal regret O(sqrt(K)) and strong constraint violation O(sqrt(K))
without relying on Slater's condition or the existence of a strictly known safe
policy. We further show that access to accurate estimates of rewards and
transitions can further improve these bounds. Our results offer practical
guarantees for safe decision-making in adversarial environments.

</details>


### [566] [A Provable Approach for End-to-End Safe Reinforcement Learning](https://arxiv.org/pdf/2505.21852)
*Akifumi Wachi, Kohei Miyaguchi, Takumi Tanabe, Rei Sato, Youhei Akimoto*

Main category: cs.LG

TL;DR: PLS integrates offline safe RL with safe policy deployment, using return-conditioned learning and Gaussian processes to ensure safety and high rewards.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of ensuring policy safety throughout the entire RL process, from learning to operation.

Method: Combines offline safe RL (return-conditioned supervised learning) with safe policy deployment (optimizing target returns using Gaussian processes).

Result: Theoretically and empirically, PLS ensures safety with high probability while achieving near-optimal rewards.

Conclusion: PLS successfully achieves the goal of high rewards and safety throughout the policy's lifetime.

Abstract: A longstanding goal in safe reinforcement learning (RL) is a method to ensure
the safety of a policy throughout the entire process, from learning to
operation. However, existing safe RL paradigms inherently struggle to achieve
this objective. We propose a method, called Provably Lifetime Safe RL (PLS),
that integrates offline safe RL with safe policy deployment to address this
challenge. Our proposed method learns a policy offline using return-conditioned
supervised learning and then deploys the resulting policy while cautiously
optimizing a limited set of parameters, known as target returns, using Gaussian
processes (GPs). Theoretically, we justify the use of GPs by analyzing the
mathematical relationship between target and actual returns. We then prove that
PLS finds near-optimal target returns while guaranteeing safety with high
probability. Empirically, we demonstrate that PLS outperforms baselines both in
safety and reward performance, thereby achieving the longstanding goal to
obtain high rewards while ensuring the safety of a policy throughout the
lifetime from learning to operation.

</details>


### [567] [Revisiting Bayesian Model Averaging in the Era of Foundation Models](https://arxiv.org/pdf/2505.21857)
*Mijung Park*

Main category: cs.LG

TL;DR: The paper revisits Bayesian model averaging (BMA) for ensembling foundation models, introducing trainable linear classifiers and a cheaper optimizable model averaging (OMA) method to improve classification performance.


<details>
  <summary>Details</summary>
Motivation: To enhance classification performance on image and text data by leveraging pre-trained foundation models through a principled ensembling approach.

Method: Proposes BMA with trainable linear classifiers and introduces OMA, which optimizes ensemble weights by minimizing prediction surprise.

Result: The methods provide a tractable and efficient way to ensemble foundation models, improving classification performance.

Conclusion: These approaches enable future integration of better foundation models for challenging classification tasks.

Abstract: We revisit the classical, full-fledged Bayesian model averaging (BMA)
paradigm to ensemble pre-trained and/or lightly-finetuned foundation models to
enhance the classification performance on image and text data. To make BMA
tractable under foundation models, we introduce trainable linear classifiers
that take frozen features from the pre-trained foundation models as inputs. The
model posteriors over the linear classifiers tell us which linear heads and
frozen features are better suited for a given dataset, resulting in a
principled model ensembling method. Furthermore, we propose a computationally
cheaper, optimizable model averaging scheme (OMA). In OMA, we directly optimize
the model ensemble weights, just like those weights based on model posterior
distributions in BMA, by reducing the amount of surprise (expected entropy of
the predictions) we get from predictions of ensembled models. With the rapid
development of foundation models, these approaches will enable the
incorporation of future, possibly significantly better foundation models to
enhance the performance of challenging classification tasks.

</details>


### [568] [Hybrid Batch Normalisation: Resolving the Dilemma of Batch Normalisation in Federated Learning](https://arxiv.org/pdf/2505.21877)
*Hongyao Chen, Tianyang Xu, Xiaojun Wu, Josef Kittler*

Main category: cs.LG

TL;DR: Hybrid Batch Normalization (HBN) is proposed to address Batch Normalization (BN) challenges in federated learning by separating statistical and learnable parameter updates, improving performance with adaptive global statistics.


<details>
  <summary>Details</summary>
Motivation: Standard BN degrades federated learning due to non-IID data and lack of coherent BN parameter updates.

Method: HBN separates statistical and learnable parameter updates, using a learnable hybrid distribution factor to mix batch and global statistics.

Result: HBN improves federated learning performance, especially for small batch sizes and heterogeneous data.

Conclusion: HBN is an effective plugin for federated learning, offering adaptive and unbiased global statistical estimates.

Abstract: Batch Normalisation (BN) is widely used in conventional deep neural network
training to harmonise the input-output distributions for each batch of data.
However, federated learning, a distributed learning paradigm, faces the
challenge of dealing with non-independent and identically distributed data
among the client nodes. Due to the lack of a coherent methodology for updating
BN statistical parameters, standard BN degrades the federated learning
performance. To this end, it is urgent to explore an alternative normalisation
solution for federated learning. In this work, we resolve the dilemma of the BN
layer in federated learning by developing a customised normalisation approach,
Hybrid Batch Normalisation (HBN). HBN separates the update of statistical
parameters (i.e. , means and variances used for evaluation) from that of
learnable parameters (i.e. , parameters that require gradient updates),
obtaining unbiased estimates of global statistical parameters in distributed
scenarios. In contrast with the existing solutions, we emphasise the supportive
power of global statistics for federated learning. The HBN layer introduces a
learnable hybrid distribution factor, allowing each computing node to
adaptively mix the statistical parameters of the current batch with the global
statistics. Our HBN can serve as a powerful plugin to advance federated
learning performance. It reflects promising merits across a wide range of
federated learning settings, especially for small batch sizes and heterogeneous
data.

</details>


### [569] [HydraNet: Momentum-Driven State Space Duality for Multi-Granularity Tennis Tournaments Analysis](https://arxiv.org/pdf/2505.21882)
*Ruijie Li, Xiang Zhao, Qiao Ning, Shikai Guo*

Main category: cs.LG

TL;DR: The paper introduces a Momentum Score (MS) metric and HydraNet framework to model momentum in tennis, analyzing multi-granularity performance shifts across points, games, sets, and matches.


<details>
  <summary>Details</summary>
Motivation: Momentum in tennis is critical but underexplored in terms of modeling and multi-granularity analysis. The study aims to quantify and model momentum to understand its impact on match outcomes.

Method: The authors propose HydraNet, a state-space duality-based framework, integrating 32 performance dimensions. It uses a sliding-window mechanism for explicit momentum and cross-game state propagation for implicit momentum, along with Versus Learning and CAAM for adversarial and dynamic momentum capture.

Result: HydraNet is validated on a large tennis dataset (2012-2023 Wimbledon and 2013-2023 US Open), showing effective multi-granularity modeling of MS and actionable insights into momentum's impact.

Conclusion: The MS metric and HydraNet framework provide a novel foundation for momentum modeling in sports, with potential applications in performance analysis and strategy.

Abstract: In tennis tournaments, momentum, a critical yet elusive phenomenon, reflects
the dynamic shifts in performance of athletes that can decisively influence
match outcomes. Despite its significance, momentum in terms of effective
modeling and multi-granularity analysis across points, games, sets, and matches
in tennis tournaments remains underexplored. In this study, we define a novel
Momentum Score (MS) metric to quantify a player's momentum level in
multi-granularity tennis tournaments, and design HydraNet, a momentum-driven
state-space duality-based framework, to model MS by integrating thirty-two
heterogeneous dimensions of athletes performance in serve, return, psychology
and fatigue. HydraNet integrates a Hydra module, which builds upon a
state-space duality (SSD) framework, capturing explicit momentum with a
sliding-window mechanism and implicit momentum through cross-game state
propagation. It also introduces a novel Versus Learning method to better
enhance the adversarial nature of momentum between the two athletes at a macro
level, along with a Collaborative-Adversarial Attention Mechanism (CAAM) for
capturing and integrating intra-player and inter-player dynamic momentum at a
micro level. Additionally, we construct a million-level tennis cross-tournament
dataset spanning from 2012-2023 Wimbledon and 2013-2023 US Open, and validate
the multi-granularity modeling capability of HydraNet for the MS metric on this
dataset. Extensive experimental evaluations demonstrate that the MS metric
constructed by the HydraNet framework provides actionable insights into how
momentum impacts outcomes at different granularities, establishing a new
foundation for momentum modeling and sports analysis. To the best of our
knowledge. The source code and datasets are available at
https://github.com/ReyJerry/HydraNet.

</details>


### [570] [SDPO: Importance-Sampled Direct Preference Optimization for Stable Diffusion Training](https://arxiv.org/pdf/2505.21893)
*Xiaomeng Yang, Zhiyu Tan, Junyan Wang, Zhijian Zhou, Hao Li*

Main category: cs.LG

TL;DR: The paper introduces DPO-C&M and SDPO to address instability and off-policy bias in diffusion-based preference learning, outperforming Diffusion-DPO.


<details>
  <summary>Details</summary>
Motivation: Existing methods like Diffusion-DPO face instability and off-policy bias in aligning generative models with human preferences.

Method: Proposes DPO-C&M (clipping/masking uninformative timesteps) and SDPO (importance sampling for bias correction).

Result: Both methods outperform Diffusion-DPO; SDPO achieves better VBench scores, human preference alignment, and robustness.

Conclusion: Timestep-aware, distribution-corrected optimization is crucial for effective diffusion-based preference learning.

Abstract: Preference learning has become a central technique for aligning generative
models with human expectations. Recently, it has been extended to diffusion
models through methods like Direct Preference Optimization (DPO). However,
existing approaches such as Diffusion-DPO suffer from two key challenges:
timestep-dependent instability, caused by a mismatch between the reverse and
forward diffusion processes and by high gradient variance in early noisy
timesteps, and off-policy bias arising from the mismatch between optimization
and data collection policies. We begin by analyzing the reverse diffusion
trajectory and observe that instability primarily occurs at early timesteps
with low importance weights. To address these issues, we first propose
DPO-C\&M, a practical strategy that improves stability by clipping and masking
uninformative timesteps while partially mitigating off-policy bias. Building on
this, we introduce SDPO (Importance-Sampled Direct Preference Optimization), a
principled framework that incorporates importance sampling into the objective
to fully correct for off-policy bias and emphasize informative updates during
the diffusion process. Experiments on CogVideoX-2B, CogVideoX-5B, and
Wan2.1-1.3B demonstrate that both methods outperform standard Diffusion-DPO,
with SDPO achieving superior VBench scores, human preference alignment, and
training robustness. These results highlight the importance of timestep-aware,
distribution-corrected optimization in diffusion-based preference learning.

</details>


### [571] [Compressing Sine-Activated Low-Rank Adapters through Post-Training Quantization](https://arxiv.org/pdf/2505.21895)
*Cameron Gordon, Yiping Ji, Hemanth Saratchandran, Paul Albert, Simon Lucey*

Main category: cs.LG

TL;DR: The paper explores applying a sinusoidal transformation to quantized LoRA adapters to enhance their representational capacity without extra parameters, maintaining performance post-quantization.


<details>
  <summary>Details</summary>
Motivation: Address the limitation of LoRA's low-rank constraint by extending Ji et al.'s sine-activated technique to quantized adapters, ensuring performance retention after compression.

Method: Extend sinusoidal transformation to quantized LoRA adapters, supported by theoretical analysis linking stable rank of quantized and full-precision adapters.

Result: Sinusoidal non-linearity preserves expressivity post-quantization, enabling highly compressed adapters with minimal performance loss.

Conclusion: The approach successfully applies sine-activated LoRA to quantized models, achieving memory savings without compromising accuracy across tasks.

Abstract: Low-Rank Adaptation (LoRA) has become a standard approach for
parameter-efficient fine-tuning, offering substantial reductions in trainable
parameters by modeling updates as the product of two low-rank matrices. While
effective, the low-rank constraint inherently limits representational capacity,
often resulting in reduced performance compared to full-rank fine-tuning.
Recent work by Ji et al. (2025) has addressed this limitation by applying a
fixed-frequency sinusoidal transformation to low-rank adapters, increasing
their stable rank without introducing additional parameters. This raises a
crucial question: can the same sine-activated technique be successfully applied
within the context of Post-Training Quantization to retain benefits even after
model compression? In this paper, we investigate this question by extending the
sinusoidal transformation framework to quantized LoRA adapters. We develop a
theoretical analysis showing that the stable rank of a quantized adapter is
tightly linked to that of its full-precision counterpart, motivating the use of
such rank-enhancing functions even under quantization. Our results demonstrate
that the expressivity gains from a sinusoidal non-linearity persist after
quantization, yielding highly compressed adapters with negligible loss in
performance. We validate our approach across a range of fine-tuning tasks for
language, vision and text-to-image generation achieving significant memory
savings while maintaining competitive accuracy.

</details>


### [572] [Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding](https://arxiv.org/pdf/2505.21908)
*Hanyin Wang, Zhenbang Wu, Gururaj Kolar, Hariprasad Korsapati, Brian Bartlett, Bryan Hull, Jimeng Sun*

Main category: cs.LG

TL;DR: DRG-Sapphire uses RL for automated DRG coding, achieving state-of-the-art accuracy and explainability, while highlighting the importance of domain knowledge in RL for OOD tasks.


<details>
  <summary>Details</summary>
Motivation: DRG coding is labor-intensive and challenging for LLMs due to OOD data. Automating it with RL can improve efficiency and accuracy.

Method: DRG-Sapphire employs Qwen2.5-7B with GRPO and rule-based rewards, introducing RL enhancements for domain-specific challenges.

Result: Achieves top accuracy on MIMIC-IV, with physician-validated reasoning, and shows RL performance scales with SFT examples.

Conclusion: For OOD tasks like DRG coding, scaling SFT is more effective than RL alone, emphasizing the need for prior domain knowledge infusion.

Abstract: Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement
and operations but require labor-intensive assignment. Large Language Models
(LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of
the task: pretraining corpora rarely contain private clinical or billing data.
We introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL)
for automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained
with Group Relative Policy Optimization (GRPO) using rule-based rewards,
DRG-Sapphire introduces a series of RL enhancements to address domain-specific
challenges not seen in previous mathematical tasks. Our model achieves
state-of-the-art accuracy on the MIMIC-IV benchmark and generates
physician-validated reasoning for DRG assignments, significantly enhancing
explainability. Our study further sheds light on broader challenges of applying
RL to knowledge-intensive, OOD tasks. We observe that RL performance scales
approximately linearly with the logarithm of the number of supervised
fine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally
constrained by the domain knowledge encoded in the base model. For OOD tasks
like DRG coding, strong RL performance requires sufficient knowledge infusion
prior to RL. Consequently, scaling SFT may be more effective and
computationally efficient than scaling RL alone for such tasks.

</details>


### [573] [Taming Transformer Without Using Learning Rate Warmup](https://arxiv.org/pdf/2505.21910)
*Xianbiao Qi, Yelin He, Jiaquan Ye, Chun-Guang Li, Bojia Zi, Xili Dai, Qin Zou, Rong Xiao*

Main category: cs.LG

TL;DR: The paper analyzes the challenge of scaling Transformers without tricks like learning rate warmup, identifies spectral energy concentration as a cause of model crashes, and proposes a novel optimization strategy to prevent it.


<details>
  <summary>Details</summary>
Motivation: Large-scale Transformer training without common tricks is difficult due to model crashes caused by spectral energy concentration, leading to entropy collapse.

Method: The paper introduces a smooth weight-updating strategy based on Weyl's Inequality, bounding the learning rate to prevent spectral energy concentration.

Result: Experiments with ViT, Swin-Transformer, and GPT show the strategy effectively trains Transformers without learning rate warmup.

Conclusion: The proposed optimization strategy addresses spectral energy concentration, enabling stable Transformer training without warmup.

Abstract: Scaling Transformer to a large scale without using some technical tricks such
as learning rate warump and using an obviously lower learning rate is an
extremely challenging task, and is increasingly gaining more attention. In this
paper, we provide a theoretical analysis for the process of training
Transformer and reveal the rationale behind the model crash phenomenon in the
training process, termed \textit{spectral energy concentration} of
${\bW_q}^{\top} \bW_k$, which is the reason for a malignant entropy collapse,
where ${\bW_q}$ and $\bW_k$ are the projection matrices for the query and the
key in Transformer, respectively. To remedy this problem, motivated by
\textit{Weyl's Inequality}, we present a novel optimization strategy, \ie,
making the weight updating in successive steps smooth -- if the ratio
$\frac{\sigma_{1}(\nabla \bW_t)}{\sigma_{1}(\bW_{t-1})}$ is larger than a
threshold, we will automatically bound the learning rate to a weighted multiple
of $\frac{\sigma_{1}(\bW_{t-1})}{\sigma_{1}(\nabla \bW_t)}$, where $\nabla
\bW_t$ is the updating quantity in step $t$. Such an optimization strategy can
prevent spectral energy concentration to only a few directions, and thus can
avoid malignant entropy collapse which will trigger the model crash. We conduct
extensive experiments using ViT, Swin-Transformer and GPT, showing that our
optimization strategy can effectively and stably train these Transformers
without using learning rate warmup.

</details>


### [574] [Self-supervised Learning Method Using Transformer for Multi-dimensional Sensor Data Processing](https://arxiv.org/pdf/2505.21918)
*Haruki Kai, Tsuyoshi Okita*

Main category: cs.LG

TL;DR: A deep learning algorithm for human activity recognition using a modified Transformer model improves accuracy by 10%-15% over the vanilla Transformer.


<details>
  <summary>Details</summary>
Motivation: To enhance performance in human activity recognition by leveraging a pretrained Transformer model with specialized numerical processing features.

Method: Developed an n-dimensional numerical processing Transformer with linear layer embedding, binning-based pre-processing, and linear transformation in the output layer.

Result: Achieved 10%-15% accuracy improvements across five datasets compared to the vanilla Transformer.

Conclusion: The proposed model effectively enhances human activity recognition performance by optimizing numerical data processing in the Transformer architecture.

Abstract: We developed a deep learning algorithm for human activity recognition using
sensor signals as input. In this study, we built a pretrained language model
based on the Transformer architecture, which is widely used in natural language
processing. By leveraging this pretrained model, we aimed to improve
performance on the downstream task of human activity recognition. While this
task can be addressed using a vanilla Transformer, we propose an enhanced
n-dimensional numerical processing Transformer that incorporates three key
features: embedding n-dimensional numerical data through a linear layer,
binning-based pre-processing, and a linear transformation in the output layer.
We evaluated the effectiveness of our proposed model across five different
datasets. Compared to the vanilla Transformer, our model demonstrated 10%-15%
improvements in accuracy.

</details>


### [575] [FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design](https://arxiv.org/pdf/2505.21923)
*Asal Mehradfar, Xuzhe Zhao, Yilun Huang, Emir Ceyani, Yankai Yang, Shihao Han, Hamidreza Aghasi, Salman Avestimehr*

Main category: cs.LG

TL;DR: FALCON is a machine learning framework for automated analog circuit synthesis, achieving high accuracy in topology selection and performance prediction with layout-aware optimization.


<details>
  <summary>Details</summary>
Motivation: Analog circuit design is complex and multi-stage; FALCON aims to automate this process, reducing human effort and improving efficiency.

Method: FALCON uses a performance-driven classifier for topology selection and a graph neural network for parameter inference, guided by a differentiable layout cost.

Result: FALCON achieves >99% topology accuracy, <10% performance error, and completes designs in under 1 second per instance.

Conclusion: FALCON is a practical foundation model for end-to-end analog circuit design automation.

Abstract: Designing analog circuits from performance specifications is a complex,
multi-stage process encompassing topology selection, parameter inference, and
layout feasibility. We introduce FALCON, a unified machine learning framework
that enables fully automated, specification-driven analog circuit synthesis
through topology selection and layout-constrained optimization. Given a target
performance, FALCON first selects an appropriate circuit topology using a
performance-driven classifier guided by human design heuristics. Next, it
employs a custom, edge-centric graph neural network trained to map circuit
topology and parameters to performance, enabling gradient-based parameter
inference through the learned forward model. This inference is guided by a
differentiable layout cost, derived from analytical equations capturing
parasitic and frequency-dependent effects, and constrained by design rules. We
train and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave
circuits, generated and simulated using Cadence Spectre across 20
expert-designed topologies. Through this evaluation, FALCON demonstrates >99\%
accuracy in topology inference, <10\% relative error in performance prediction,
and efficient layout-aware design that completes in under 1 second per
instance. Together, these results position FALCON as a practical and extensible
foundation model for end-to-end analog circuit design automation.

</details>


### [576] [Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets](https://arxiv.org/pdf/2505.21930)
*Dongyue Li, Ziniu Zhang, Lu Wang, Hongyang R. Zhang*

Main category: cs.LG

TL;DR: An ensemble method for fine-tuning language models on multiple datasets is proposed, outperforming QLoRA in accuracy with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Existing methods like QLoRA are efficient for single datasets but unclear for multiple tasks. The paper addresses this gap.

Method: Partition datasets into groups, train smaller adapters per group, and form a weighted ensemble. Uses gradients for fast performance estimation.

Result: Achieves up to 10% higher accuracy than QLoRA with only 9% more FLOPs. Scales well to large models (34B parameters).

Conclusion: The ensemble approach is efficient and effective for multi-task fine-tuning, offering significant accuracy gains with minimal computational cost.

Abstract: This paper develops an ensemble method for fine-tuning a language model to
multiple datasets. Existing methods, such as quantized LoRA (QLoRA), are
efficient when adapting to a single dataset. When training on multiple datasets
of different tasks, a common setup in practice, it remains unclear how to
design an efficient adaptation for fine-tuning language models. We propose to
use an ensemble of multiple smaller adapters instead of a single adapter per
task. We design an efficient algorithm that partitions $n$ datasets into $m$
groups, where $m$ is typically much smaller than $n$ in practice, and train one
adapter for each group before taking a weighted combination to form the
ensemble. The algorithm leverages a first-order approximation property of
low-rank adaptation to quickly obtain the fine-tuning performances of dataset
combinations since methods like LoRA stay close to the base model. Hence, we
use the gradients of the base model to estimate its behavior during
fine-tuning. Empirically, this approximation holds with less than $1\%$ error
on models with up to $34$ billion parameters, leading to an estimation of true
fine-tuning performances under $5\%$ error while speeding up computation
compared to base fine-tuning by $105$ times. When applied to fine-tune Llama
and GPT models on ten text classification tasks, our approach provides up to
$10\%$ higher average test accuracy over QLoRA, with only $9\%$ more FLOPs. On
a Llama model with $34$ billion parameters, an ensemble of QLoRA increases test
accuracy by $3\%$ compared to QLoRA, with only $8\%$ more FLOPs.

</details>


### [577] [Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection](https://arxiv.org/pdf/2505.21938)
*Qirun Zeng, Eric He, Richard Hoffmann, Xuchuang Wang, Jinhang Zuo*

Main category: cs.LG

TL;DR: The paper introduces a practical adversarial threat model, Fake Data Injection, for stochastic bandits, addressing realistic constraints like bounded fake feedback and limited injection. It proposes efficient attack strategies, theoretically and experimentally validating their effectiveness against UCB and Thompson Sampling algorithms.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial attacks on stochastic bandits rely on unrealistic assumptions (e.g., unbounded perturbations). This work aims to address these limitations by proposing a more practical threat model.

Method: The authors design attack strategies under the Fake Data Injection model, incorporating magnitude and temporal constraints. Theoretical analysis and experiments on synthetic/real-world datasets are conducted.

Result: The attacks successfully mislead UCB and Thompson Sampling algorithms into selecting a target arm with sublinear attack cost, demonstrating vulnerabilities in these algorithms.

Conclusion: The study highlights significant vulnerabilities in stochastic bandit algorithms under practical adversarial scenarios, emphasizing the need for robust defenses.

Abstract: Adversarial attacks on stochastic bandits have traditionally relied on some
unrealistic assumptions, such as per-round reward manipulation and unbounded
perturbations, limiting their relevance to real-world systems. We propose a
more practical threat model, Fake Data Injection, which reflects realistic
adversarial constraints: the attacker can inject only a limited number of
bounded fake feedback samples into the learner's history, simulating legitimate
interactions. We design efficient attack strategies under this model,
explicitly addressing both magnitude constraints (on reward values) and
temporal constraints (on when and how often data can be injected). Our
theoretical analysis shows that these attacks can mislead both Upper Confidence
Bound (UCB) and Thompson Sampling algorithms into selecting a target arm in
nearly all rounds while incurring only sublinear attack cost. Experiments on
synthetic and real-world datasets validate the effectiveness of our strategies,
revealing significant vulnerabilities in widely used stochastic bandit
algorithms under practical adversarial scenarios.

</details>


### [578] [Continual Learning Beyond Experience Rehearsal and Full Model Surrogates](https://arxiv.org/pdf/2505.21942)
*Prashant Bhat, Laurens Niesten, Elahe Arani, Bahram Zonooz*

Main category: cs.LG

TL;DR: SPARC is a scalable continual learning method that avoids memory-heavy rehearsal or full-model surrogates, achieving efficiency with only 6% of parameters while matching or outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiency and scalability issues of existing continual learning solutions, which rely on memory-intensive rehearsal or full-model surrogates.

Method: Combines task-specific working memories and task-agnostic semantic memory for cross-task knowledge consolidation, with weight re-normalization to mitigate biases.

Result: Achieves superior performance on Seq-TinyImageNet and matches rehearsal-based methods on benchmarks, using only 6% of parameters.

Conclusion: SPARC is a practical, scalable solution for continual learning under efficiency constraints.

Abstract: Continual learning (CL) has remained a significant challenge for deep neural
networks as learning new tasks erases previously acquired knowledge, either
partially or completely. Existing solutions often rely on experience rehearsal
or full model surrogates to mitigate CF. While effective, these approaches
introduce substantial memory and computational overhead, limiting their
scalability and applicability in real-world scenarios. To address this, we
propose SPARC, a scalable CL approach that eliminates the need for experience
rehearsal and full-model surrogates. By effectively combining task-specific
working memories and task-agnostic semantic memory for cross-task knowledge
consolidation, SPARC results in a remarkable parameter efficiency, using only
6% of the parameters required by full-model surrogates. Despite its lightweight
design, SPARC achieves superior performance on Seq-TinyImageNet and matches
rehearsal-based methods on various CL benchmarks. Additionally, weight
re-normalization in the classification layer mitigates task-specific biases,
establishing SPARC as a practical and scalable solution for CL under stringent
efficiency constraints.

</details>


### [579] [Stochastic Primal-Dual Double Block-Coordinate for Two-way Partial AUC Maximization](https://arxiv.org/pdf/2505.21944)
*Linli Zhou, Bokun Wang, My T. Thai, Tianbao Yang*

Main category: cs.LG

TL;DR: The paper introduces two stochastic primal-dual algorithms for optimizing two-way partial AUC (TPAUC) in imbalanced binary classification, offering improved convergence and performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for TPAUC optimization are limited by approximations or high complexity, necessitating more efficient and effective solutions.

Method: Two stochastic primal-dual double block-coordinate algorithms are proposed, with updates for both primal and dual variables, applicable to convex and non-convex settings.

Result: Theoretical and experimental results show faster convergence and better generalization compared to prior methods.

Conclusion: The work advances TPAUC optimization, providing practical tools for machine learning applications.

Abstract: Two-way partial AUC (TPAUC) is a critical performance metric for binary
classification with imbalanced data, as it focuses on specific ranges of the
true positive rate (TPR) and false positive rate (FPR). However, stochastic
algorithms for TPAUC optimization remain under-explored, with existing methods
either limited to approximated TPAUC loss functions or burdened by sub-optimal
complexities. To overcome these limitations, we introduce two innovative
stochastic primal-dual double block-coordinate algorithms for TPAUC
maximization. These algorithms utilize stochastic block-coordinate updates for
both the primal and dual variables, catering to both convex and non-convex
settings. We provide theoretical convergence rate analyses, demonstrating
significant improvements over prior approaches. Our experimental results, based
on multiple benchmark datasets, validate the superior performance of our
algorithms, showcasing faster convergence and better generalization. This work
advances the state of the art in TPAUC optimization and offers practical tools
for real-world machine learning applications.

</details>


### [580] [EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles](https://arxiv.org/pdf/2505.21959)
*Aakriti Agrawal, Mucong Ding, Zora Che, Chenghao Deng, Anirudh Satheesh, Bang An, Bayan Bruss, John Langford, Furong Huang*

Main category: cs.LG

TL;DR: The paper proposes EnsemW2S, a token-level ensemble method to enhance weak experts using human-level data, improving their ability to supervise stronger models, with notable performance gains on ID and OOD datasets.


<details>
  <summary>Details</summary>
Motivation: Address the weak-to-strong generalization challenge in LLMs by improving weak experts to supervise stronger models.

Method: Token-level ensemble strategy iteratively combines weak experts, refining their collective supervision capability.

Result: Achieves 4% and 3.2% improvements on ID datasets, and up to 6% and 2.28% on OOD datasets for experts and student models.

Conclusion: EnsemW2S effectively advances weak-to-strong generalization, demonstrating significant performance improvements.

Abstract: With Large Language Models (LLMs) rapidly approaching and potentially
surpassing human-level performance, it has become imperative to develop
approaches capable of effectively supervising and enhancing these powerful
models using smaller, human-level models exposed to only human-level data. We
address this critical weak-to-strong (W2S) generalization challenge by
proposing a novel method aimed at improving weak experts, by training on the
same limited human-level data, enabling them to generalize to complex,
super-human-level tasks. Our approach, called \textbf{EnsemW2S}, employs a
token-level ensemble strategy that iteratively combines multiple weak experts,
systematically addressing the shortcomings identified in preceding iterations.
By continuously refining these weak models, we significantly enhance their
collective ability to supervise stronger student models. We extensively
evaluate the generalization performance of both the ensemble of weak experts
and the subsequent strong student model across in-distribution (ID) and
out-of-distribution (OOD) datasets. For OOD, we specifically introduce question
difficulty as an additional dimension for defining distributional shifts. Our
empirical results demonstrate notable improvements, achieving 4\%, and 3.2\%
improvements on ID datasets and, upto 6\% and 2.28\% on OOD datasets for
experts and student models respectively, underscoring the effectiveness of our
proposed method in advancing W2S generalization.

</details>


### [581] [Judging LLMs on a Simplex](https://arxiv.org/pdf/2505.21972)
*Patrick Vossler, Fan Xia, Yifan Mai, Jean Feng*

Main category: cs.LG

TL;DR: The paper analyzes the theoretical properties of using LLMs as judges for evaluating free-form outputs, revealing a phase transition in ranking identifiability and proposing Bayesian inference to address uncertainty.


<details>
  <summary>Details</summary>
Motivation: Understanding the theoretical limitations and uncertainties when using LLMs as judges for evaluating diverse and valid outputs.

Method: A geometric framework representing judges and candidates on a probability simplex, combined with Bayesian inference for uncertainty quantification.

Result: Binary scoring systems allow identifiable rankings under mild assumptions, while systems with three or more levels face non-identifiability. Bayesian inference improves ranking accuracy and coverage rates.

Conclusion: A holistic approach to uncertainty quantification, integrating Bayesian methods, is crucial for reliable use of LLMs as judges.

Abstract: Automated evaluation of free-form outputs from large language models (LLMs)
is challenging because many distinct answers can be equally valid. A common
practice is to use LLMs themselves as judges, but the theoretical properties of
this approach are not yet well understood. We show that a geometric framework
that represents both judges and candidates as points on a probability simplex
can provide helpful insight on what is or is not identifiable using LLM judges.
Our theoretical analysis uncovers a "phase transition" in ranking
identifiability: for binary scoring systems, true rankings are identifiable
even with weak judges under mild assumptions, while rankings become
non-identifiable for three or more scoring levels even with infinite data,
absent additional prior knowledge. This non-identifiability highlights how
uncertainty in rankings stems from not only aleatoric uncertainty (i.e.,
inherent stochasticity in the data) but also epistemic uncertainty regarding
which assumptions hold, an aspect that has received limited attention until
now. To integrate both types of uncertainty, we use Bayesian inference to
encode assumptions as priors and conduct sensitivity analysis of ranking
estimates and credible intervals. Empirical evaluations across multiple
benchmarks demonstrate that Bayesian inference yields more accurate rankings
and substantially improves coverage rates. These results underscore the
importance of taking a more holistic approach to uncertainty quantification
when using LLMs as judges.

</details>


### [582] [BOFormer: Learning to Solve Multi-Objective Bayesian Optimization via Non-Markovian RL](https://arxiv.org/pdf/2505.21974)
*Yu-Heng Hung, Kai-Jie Lin, Yu-Heng Lin, Chien-YiWang, Cheng Sun, Ping-Chun Hsieh*

Main category: cs.LG

TL;DR: BOFormer, a Transformer-based deep Q-learning framework, addresses the hypervolume identifiability issue in multi-objective Bayesian optimization (MOBO) and outperforms benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing learning-based acquisition functions in single-objective BO don't directly extend to MOBO due to the hypervolume identifiability issue.

Method: Proposes BOFormer, a deep Q-learning framework using sequence modeling inspired by Transformers and non-Markovian RL.

Result: BOFormer consistently outperforms rule-based and learning-based benchmarks in synthetic and real-world MOBO tasks.

Conclusion: BOFormer effectively tackles MOBO challenges, with open-source code to foster further research.

Abstract: Bayesian optimization (BO) offers an efficient pipeline for optimizing
black-box functions with the help of a Gaussian process prior and an
acquisition function (AF). Recently, in the context of single-objective BO,
learning-based AFs witnessed promising empirical results given its favorable
non-myopic nature. Despite this, the direct extension of these approaches to
multi-objective Bayesian optimization (MOBO) suffer from the
\textit{hypervolume identifiability issue}, which results from the
non-Markovian nature of MOBO problems. To tackle this, inspired by the
non-Markovian RL literature and the success of Transformers in language
modeling, we present a generalized deep Q-learning framework and propose
\textit{BOFormer}, which substantiates this framework for MOBO via sequence
modeling. Through extensive evaluation, we demonstrate that BOFormer constantly
outperforms the benchmark rule-based and learning-based algorithms in various
synthetic MOBO and real-world multi-objective hyperparameter optimization
problems. We have made the source code publicly available to encourage further
research in this direction.

</details>


### [583] [Two-Stage Feature Generation with Transformer and Reinforcement Learning](https://arxiv.org/pdf/2505.21978)
*Wanfu Gao, Zengyao Man, Zebin He, Yuhao Tang, Jun Gao, Kunpeng Liu*

Main category: cs.LG

TL;DR: The paper proposes a Two-Stage Feature Generation (TSFG) framework combining Transformer-based encoder-decoder and PPO to automate and optimize feature generation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional feature generation is labor-intensive and lacks adaptability, while automated methods struggle with redundancy and inefficiency.

Method: TSFG uses a Transformer-based encoder-decoder for feature representation and PPO for dynamic strategy adjustment.

Result: TSFG generates high-quality features, improving model performance and outperforming state-of-the-art methods.

Conclusion: TSFG effectively addresses challenges in feature generation, offering adaptability and superior performance.

Abstract: Feature generation is a critical step in machine learning, aiming to enhance
model performance by capturing complex relationships within the data and
generating meaningful new features. Traditional feature generation methods
heavily rely on domain expertise and manual intervention, making the process
labor-intensive and challenging to adapt to different scenarios. Although
automated feature generation techniques address these issues to some extent,
they often face challenges such as feature redundancy, inefficiency in feature
space exploration, and limited adaptability to diverse datasets and tasks. To
address these problems, we propose a Two-Stage Feature Generation (TSFG)
framework, which integrates a Transformer-based encoder-decoder architecture
with Proximal Policy Optimization (PPO). The encoder-decoder model in TSFG
leverages the Transformer's self-attention mechanism to efficiently represent
and transform features, capturing complex dependencies within the data. PPO
further enhances TSFG by dynamically adjusting the feature generation strategy
based on task-specific feedback, optimizing the process for improved
performance and adaptability. TSFG dynamically generates high-quality feature
sets, significantly improving the predictive performance of machine learning
models. Experimental results demonstrate that TSFG outperforms existing
state-of-the-art methods in terms of feature quality and adaptability.

</details>


### [584] [ACE: Exploring Activation Cosine Similarity and Variance for Accurate and Calibration-Efficient LLM Pruning](https://arxiv.org/pdf/2505.21987)
*Zhendong Mi, Zhenglun Kong, Geng Yuan, Shaoyi Huang*

Main category: cs.LG

TL;DR: Proposes an efficient LLM pruning method with improved performance and speed using activation cosine similarity and variance metrics.


<details>
  <summary>Details</summary>
Motivation: Addresses the inefficiency and suboptimal performance of existing LLM pruning methods.

Method: Introduces two pruning metrics: activation cosine similarity loss and activation variance, combined for better pruning.

Result: Achieves up to 18% perplexity reduction and 63% pruning time decrease on models like LLaMA and OPT.

Conclusion: The method enhances LLM pruning accuracy and efficiency effectively.

Abstract: With the rapid expansion of large language models (LLMs), the demand for
memory and computational resources has grown significantly. Recent advances in
LLM pruning aim to reduce the size and computational cost of these models.
However, existing methods often suffer from either suboptimal pruning
performance or low time efficiency during the pruning process. In this work, we
propose an efficient and effective pruning method that simultaneously achieves
high pruning performance and fast pruning speed with improved calibration
efficiency. Our approach introduces two key innovations: (1) An activation
cosine similarity loss-guided pruning metric, which considers the angular
deviation of the output activation between the dense and pruned models. (2) An
activation variance-guided pruning metric, which helps preserve semantic
distinctions in output activations after pruning, enabling effective pruning
with shorter input sequences. These two components can be readily combined to
enhance LLM pruning in both accuracy and efficiency. Experimental results show
that our method achieves up to an 18% reduction in perplexity and up to 63%
decrease in pruning time on prevalent LLMs such as LLaMA, LLaMA-2, and OPT.

</details>


### [585] [Learning in Compact Spaces with Approximately Normalized Transformers](https://arxiv.org/pdf/2505.22014)
*Jörg K. H. Franke, Urs Spiegelhalter, Marianna Nezhurina, Jenia Jitsev, Frank Hutter, Michael Hefenbrock*

Main category: cs.LG

TL;DR: The paper introduces anTransformer, an approximate normalization method for deep learning, which speeds up convergence and reduces hyperparameters while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Address challenges like overfitting and numerical instability in deep learning by proposing a simpler, more efficient normalization method.

Method: Constrain parameter norms and normalize representations via scalar multiplications, leveraging high-dimensional vector norm properties.

Result: 40% faster convergence in GPT training with <3% runtime overhead; enables larger batch sizes and fewer hyperparameters.

Conclusion: anTransformer offers a practical, efficient alternative to traditional normalization, matching classic GPT scaling.

Abstract: In deep learning, regularization and normalization are common solutions for
challenges such as overfitting, numerical instabilities, and the increasing
variance in the residual stream. An alternative approach is to force all
parameters and representations to lie on a hypersphere. This removes the need
for regularization and increases convergence speed, but comes with additional
costs. In this work, we propose a more holistic but approximate normalization
(anTransformer). Our approach constrains the norm of parameters and normalizes
all representations via scalar multiplications motivated by the tight
concentration of the norms of high-dimensional random vectors. When applied to
GPT training, we observe a 40% faster convergence compared to models with QK
normalization, with less than 3% additional runtime. Deriving scaling laws for
anGPT, we found our method enables training with larger batch sizes and fewer
hyperparameters, while matching the favorable scaling characteristics of
classic GPT architectures.

</details>


### [586] [Weakly-Supervised Contrastive Learning for Imprecise Class Labels](https://arxiv.org/pdf/2505.22028)
*Zi-Hao Zhou, Jun-Jie Wang, Tong Wei, Min-Ling Zhang*

Main category: cs.LG

TL;DR: The paper introduces a graph-theoretic framework for weakly-supervised contrastive learning using continuous semantic similarity to address ambiguous data annotations.


<details>
  <summary>Details</summary>
Motivation: Supervised contrastive learning struggles with inaccurate class labels, limiting its real-world applicability.

Method: Proposes a framework measuring semantic similarity between examples, refining weak signals iteratively, and using graph weights.

Result: Demonstrates effectiveness in noisy and partial label learning, improving performance over existing methods.

Conclusion: The approach approximates supervised contrastive learning under mild conditions, with theoretical error bounds.

Abstract: Contrastive learning has achieved remarkable success in learning effective
representations, with supervised contrastive learning often outperforming
self-supervised approaches. However, in real-world scenarios, data annotations
are often ambiguous or inaccurate, meaning that class labels may not reliably
indicate whether two examples belong to the same class. This limitation
restricts the applicability of supervised contrastive learning. To address this
challenge, we introduce the concept of ``continuous semantic similarity'' to
define positive and negative pairs. Instead of directly relying on imprecise
class labels, we measure the semantic similarity between example pairs, which
quantifies how closely they belong to the same category by iteratively refining
weak supervisory signals. Based on this concept, we propose a graph-theoretic
framework for weakly-supervised contrastive learning, where semantic similarity
serves as the graph weights. Our framework is highly versatile and can be
applied to many weakly-supervised learning scenarios. We demonstrate its
effectiveness through experiments in two common settings, i.e., noisy label and
partial label learning, where existing methods can be easily integrated to
significantly improve performance. Theoretically, we establish an error bound
for our approach, showing that it can approximate supervised contrastive
learning under mild conditions. The implementation code is available at
https://github.com/Speechless-10308/WSC.

</details>


### [587] [Detecting Undesired Process Behavior by Means of Retrieval Augmented Generation](https://arxiv.org/pdf/2505.22041)
*Michael Grohs, Adrian Rebmann, Jana-Rebecca Rehse*

Main category: cs.LG

TL;DR: The paper proposes a Retrieval Augmented Generation (RAG) approach to detect undesired process behavior without needing a dedicated process model or resource-intensive fine-tuning of LLMs, outperforming fine-tuned models.


<details>
  <summary>Details</summary>
Motivation: Existing conformance checking techniques require process models, and fine-tuning LLMs is resource-intensive and lacks generalization. Organizations still need to detect undesired behavior without these constraints.

Method: Uses RAG to provide LLMs with a knowledge base of desired and undesired process behavior, enabling knowledge transfer to new processes without fine-tuning.

Result: The approach outperforms fine-tuned LLMs in detecting undesired behavior, especially when enriched with event log context like frequent traces and activities.

Conclusion: RAG is a viable, efficient alternative to fine-tuning for detecting undesired process behavior, leveraging existing knowledge without heavy computational costs.

Abstract: Conformance checking techniques detect undesired process behavior by
comparing process executions that are recorded in event logs to desired
behavior that is captured in a dedicated process model. If such models are not
available, conformance checking techniques are not applicable, but
organizations might still be interested in detecting undesired behavior in
their processes. To enable this, existing approaches use Large Language Models
(LLMs), assuming that they can learn to distinguish desired from undesired
behavior through fine-tuning. However, fine-tuning is highly resource-intensive
and the fine-tuned LLMs often do not generalize well. To address these
limitations, we propose an approach that requires neither a dedicated process
model nor resource-intensive fine-tuning to detect undesired process behavior.
Instead, we use Retrieval Augmented Generation (RAG) to provide an LLM with
direct access to a knowledge base that contains both desired and undesired
process behavior from other processes, assuming that the LLM can transfer this
knowledge to the process at hand. Our evaluation shows that our approach
outperforms fine-tuned LLMs in detecting undesired behavior, demonstrating that
RAG is a viable alternative to resource-intensive fine-tuning, particularly
when enriched with relevant context from the event log, such as frequent traces
and activities.

</details>


### [588] [Estimating the Effects of Sample Training Orders for Large Language Models without Retraining](https://arxiv.org/pdf/2505.22042)
*Hao Yang, Haoxuan Li, Mengyue Yang, Xu Chen, Mingming Gong*

Main category: cs.LG

TL;DR: A retraining-free framework is proposed to estimate model parameters for arbitrary training sample orders in LLMs, improving efficiency over traditional methods. It aids in curriculum design and analyzing memorization/generalization effects.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for studying training sample order in LLMs require costly retraining, which is impractical for large models. This work aims to provide a more efficient alternative.

Method: The framework approximates Adam optimizer updates using Taylor expansions and random projections to store checkpoints, enabling parameter estimation without retraining.

Result: The framework effectively estimates model performance for different sample orders and is applied to optimize training curricula and analyze memorization/generalization effects.

Conclusion: The retraining-free framework is validated as efficient and useful for LLM training optimization and analysis, offering practical benefits over traditional methods.

Abstract: The order of training samples plays a crucial role in large language models
(LLMs), significantly impacting both their external performance and internal
learning dynamics. Traditional methods for investigating this effect generally
require retraining the model with various sample orders, which is
computationally infeasible for LLMs. In this work, we improve traditional
methods by designing a retraining-free framework. By approximating Adam
optimizer updates with first- and second-order Taylor expansions and utilizing
random projection methods to store intermediate checkpoints, our framework can
efficiently estimate model parameters for arbitrary training sample orders.
Next, we apply our framework to two downstream research problems: (1) Training
curriculum design for LLMs -- we base our retraining-free framework to propose
a novel curriculum learning strategy that augments curriculum proposals with
estimated model performances, enabling more informed sample scheduling. (2)
LLMs' memorization and generalization effect analysis -- we use our
retraining-free framework to estimate how the positions of training samples
influence LLMs' capacity for memorization and generalization. We conduct
extensive experiments to validate the effectiveness of our retraining-free
framework in reproducing the true model performances, and further demonstrate
its potential in optimizing LLM training curricula and analyzing the
memorization and generalization effects of LLMs.

</details>


### [589] [Differentiable Generalized Sliced Wasserstein Plans](https://arxiv.org/pdf/2505.22049)
*Laetitia Chapel, Romain Tavenard, Samuel Vaiter*

Main category: cs.LG

TL;DR: The paper introduces a reformulation of the min-SWGG slicing method for Optimal Transport (OT) as a bilevel optimization problem, addressing its scalability and limitations in high dimensions and nonlinear settings. It also extends the method to manifolds and demonstrates its utility in applications like gradient flows and image generation.


<details>
  <summary>Details</summary>
Motivation: The computational complexity of OT and the limitations of existing slicing methods (exponential growth in slices with dimension and linear projection constraints) motivate the need for a more efficient and flexible approach.

Method: The authors reformulate min-SWGG as a bilevel optimization problem and propose a differentiable approximation scheme to find optimal slices efficiently. They also generalize the method for data on manifolds.

Result: The proposed method efficiently identifies optimal slices in high-dimensional settings and accommodates nonlinear data structures. It shows practical value in gradient flows and image generation tasks.

Conclusion: The reformulated min-SWGG method overcomes key limitations of slicing techniques, offering computational efficiency and flexibility for high-dimensional and manifold data, with demonstrated success in diverse applications.

Abstract: Optimal Transport (OT) has attracted significant interest in the machine
learning community, not only for its ability to define meaningful distances
between probability distributions -- such as the Wasserstein distance -- but
also for its formulation of OT plans. Its computational complexity remains a
bottleneck, though, and slicing techniques have been developed to scale OT to
large datasets. Recently, a novel slicing scheme, dubbed min-SWGG, lifts a
single one-dimensional plan back to the original multidimensional space,
finally selecting the slice that yields the lowest Wasserstein distance as an
approximation of the full OT plan. Despite its computational and theoretical
advantages, min-SWGG inherits typical limitations of slicing methods: (i) the
number of required slices grows exponentially with the data dimension, and (ii)
it is constrained to linear projections. Here, we reformulate min-SWGG as a
bilevel optimization problem and propose a differentiable approximation scheme
to efficiently identify the optimal slice, even in high-dimensional settings.
We furthermore define its generalized extension for accommodating to data
living on manifolds. Finally, we demonstrate the practical value of our
approach in various applications, including gradient flows on manifolds and
high-dimensional spaces, as well as a novel sliced OT-based conditional flow
matching for image generation -- where fast computation of transport plans is
essential.

</details>


### [590] [The Resurrection of the ReLU](https://arxiv.org/pdf/2505.22074)
*Coşku Can Horuz, Geoffrey Kasenbacher, Saya Higuchi, Sebastian Kairat, Jendrik Stoltz, Moritz Pesl, Bernhard A. Moser, Christoph Linse, Thomas Martinetz, Sebastian Otte*

Main category: cs.LG

TL;DR: SUGAR is a plug-and-play regularizer for ReLU that avoids dead neurons by using a smooth surrogate gradient, improving performance in various architectures.


<details>
  <summary>Details</summary>
Motivation: ReLU's simplicity and sparsity are appealing, but its 'dying ReLU' problem limits effectiveness. SUGAR aims to revive ReLU's potential.

Method: SUGAR replaces ReLU's backward pass derivative with a smooth surrogate, preserving the forward pass. Tested on VGG-16, ResNet-18, Conv2NeXt, and Swin Transformer.

Result: SUGAR enhances generalization, sparser activations, and revives dead ReLUs. It outperforms or matches GELU in modern architectures.

Conclusion: ReLU with SUGAR can outperform advanced activation functions, proving it remains a versatile and strong choice for deep learning models.

Abstract: Modeling sophisticated activation functions within deep learning
architectures has evolved into a distinct research direction. Functions such as
GELU, SELU, and SiLU offer smooth gradients and improved convergence
properties, making them popular choices in state-of-the-art models. Despite
this trend, the classical ReLU remains appealing due to its simplicity,
inherent sparsity, and other advantageous topological characteristics. However,
ReLU units are prone to becoming irreversibly inactive - a phenomenon known as
the dying ReLU problem - which limits their overall effectiveness. In this
work, we introduce surrogate gradient learning for ReLU (SUGAR) as a novel,
plug-and-play regularizer for deep architectures. SUGAR preserves the standard
ReLU function during the forward pass but replaces its derivative in the
backward pass with a smooth surrogate that avoids zeroing out gradients. We
demonstrate that SUGAR, when paired with a well-chosen surrogate function,
substantially enhances generalization performance over convolutional network
architectures such as VGG-16 and ResNet-18, providing sparser activations while
effectively resurrecting dead ReLUs. Moreover, we show that even in modern
architectures like Conv2NeXt and Swin Transformer - which typically employ GELU
- substituting these with SUGAR yields competitive and even slightly superior
performance. These findings challenge the prevailing notion that advanced
activation functions are necessary for optimal performance. Instead, they
suggest that the conventional ReLU, particularly with appropriate gradient
handling, can serve as a strong, versatile revived classic across a broad range
of deep learning vision models.

</details>


### [591] [Can Test-time Computation Mitigate Memorization Bias in Neural Symbolic Regression?](https://arxiv.org/pdf/2505.22081)
*Shun Sato, Issei Sato*

Main category: cs.LG

TL;DR: The paper investigates the memorization bias in Transformers for neural symbolic regression (NSR), revealing their tendency to replicate training data expressions. Test-time strategies can mitigate bias but don't always improve performance.


<details>
  <summary>Details</summary>
Motivation: Symbolic regression is valuable for scientific research, but NSR methods using Transformers suffer from low performance with many variables, likely due to memorization bias.

Method: Quantitative evaluation of Transformer memorization bias using synthetic data, theoretical analysis of compositional expression construction, and empirical testing of test-time strategies.

Result: Transformers rarely generate novel expressions, and test-time information reduces bias but doesn't consistently enhance performance.

Conclusion: The study highlights NSR limitations and provides insights for designing more robust symbolic regression methods.

Abstract: Symbolic regression aims to discover mathematical equations that fit given
numerical data. It has been applied in various fields of scientific research,
such as producing human-readable expressions that explain physical phenomena.
Recently, Neural symbolic regression (NSR) methods that involve Transformers
pre-trained on large-scale synthetic datasets have gained attention. While
these methods offer advantages such as short inference time, they suffer from
low performance, particularly when the number of input variables is large. In
this study, we hypothesized that this limitation stems from the memorization
bias of Transformers in symbolic regression. We conducted a quantitative
evaluation of this bias in Transformers using a synthetic dataset and found
that Transformers rarely generate expressions not present in the training data.
Additional theoretical analysis reveals that this bias arises from the
Transformer's inability to construct expressions compositionally while
verifying their numerical validity. We finally examined if tailoring test-time
strategies can lead to reduced memorization bias and better performance. We
empirically demonstrate that providing additional information to the model at
test time can significantly mitigate memorization bias. On the other hand, we
also find that reducing memorization bias does not necessarily correlate with
improved performance. These findings contribute to a deeper understanding of
the limitations of NSR approaches and offer a foundation for designing more
robust, generalizable symbolic regression methods. Code is available at
https://github.com/Shun-0922/Mem-Bias-NSR .

</details>


### [592] [Inclusive, Differentially Private Federated Learning for Clinical Data](https://arxiv.org/pdf/2505.22108)
*Santhosh Parampottupadam, Melih Coşğun, Sarthak Pati, Maximilian Zenk, Saikat Roy, Dimitrios Bounias, Benjamin Hamm, Sinem Sav, Ralf Floca, Klaus Maier-Hein*

Main category: cs.LG

TL;DR: A compliance-aware FL framework enhances DP by adjusting noise based on client compliance scores, improving accuracy by up to 15% over traditional FL.


<details>
  <summary>Details</summary>
Motivation: Real-world adoption of FL in clinical settings is hindered by privacy, resource, and compliance challenges, with uniform DP noise degrading performance unfairly.

Method: Proposes a compliance-aware FL framework with adaptive DP noise and a compliance scoring tool based on healthcare and security standards.

Result: Experiments show up to 15% accuracy improvement by integrating diverse clinical settings, balancing privacy, compliance, and performance.

Conclusion: The framework advances FL for real-world clinical workflows, making it a viable solution for global healthcare.

Abstract: Federated Learning (FL) offers a promising approach for training clinical AI
models without centralizing sensitive patient data. However, its real-world
adoption is hindered by challenges related to privacy, resource constraints,
and compliance. Existing Differential Privacy (DP) approaches often apply
uniform noise, which disproportionately degrades model performance, even among
well-compliant institutions. In this work, we propose a novel compliance-aware
FL framework that enhances DP by adaptively adjusting noise based on
quantifiable client compliance scores. Additionally, we introduce a compliance
scoring tool based on key healthcare and security standards to promote secure,
inclusive, and equitable participation across diverse clinical settings.
Extensive experiments on public datasets demonstrate that integrating
under-resourced, less compliant clinics with highly regulated institutions
yields accuracy improvements of up to 15% over traditional FL. This work
advances FL by balancing privacy, compliance, and performance, making it a
viable solution for real-world clinical workflows in global healthcare.

</details>


### [593] [The quest for the GRAph Level autoEncoder (GRALE)](https://arxiv.org/pdf/2505.22109)
*Paul Krzakala, Gabriel Melo, Charlotte Laclau, Florence d'Alché-Buc, Rémi Flamary*

Main category: cs.LG

TL;DR: GRALE is a novel graph autoencoder using Optimal Transport-inspired loss and attention-based architecture for versatile graph representation learning.


<details>
  <summary>Details</summary>
Motivation: Graph representation learning is challenging but crucial for fields like chemistry and biology.

Method: GRALE employs an Optimal Transport-inspired loss, a differentiable node matching module, and an attention-based architecture extended from Evoformer.

Result: GRALE enables general pre-training for tasks like classification, regression, graph interpolation, editing, matching, and prediction.

Conclusion: GRALE offers a flexible and effective solution for graph representation learning across diverse applications.

Abstract: Although graph-based learning has attracted a lot of attention, graph
representation learning is still a challenging task whose resolution may impact
key application fields such as chemistry or biology. To this end, we introduce
GRALE, a novel graph autoencoder that encodes and decodes graphs of varying
sizes into a shared embedding space. GRALE is trained using an Optimal
Transport-inspired loss that compares the original and reconstructed graphs and
leverages a differentiable node matching module, which is trained jointly with
the encoder and decoder. The proposed attention-based architecture relies on
Evoformer, the core component of AlphaFold, which we extend to support both
graph encoding and decoding. We show, in numerical experiments on simulated and
molecular data, that GRALE enables a highly general form of pre-training,
applicable to a wide range of downstream tasks, from classification and
regression to more complex tasks such as graph interpolation, editing,
matching, and prediction.

</details>


### [594] [BiMi Sheets: Infosheets for bias mitigation methods](https://arxiv.org/pdf/2505.22114)
*MaryBeth Defrance, Guillaume Bied, Maarten Buyl, Jefrey Lijffijt, Tijl De Bie*

Main category: cs.LG

TL;DR: BiMi Sheets are introduced as a uniform guide to document bias mitigation methods in ML, aiding comparison and adoption by practitioners.


<details>
  <summary>Details</summary>
Motivation: Addressing the 'portability trap' in bias mitigation methods due to domain-, task-, and model-specific biases, which complicates benchmarking and practical use.

Method: Proposes BiMi Sheets, a standardized documentation framework for bias mitigation methods, supported by an online platform (bimisheet.com).

Result: Enables quick learning and comparison of bias mitigation methods, fostering structured databases and wider adoption.

Conclusion: BiMi Sheets offer a practical solution to improve transparency and usability of bias mitigation methods in ML.

Abstract: Over the past 15 years, hundreds of bias mitigation methods have been
proposed in the pursuit of fairness in machine learning (ML). However,
algorithmic biases are domain-, task-, and model-specific, leading to a
`portability trap': bias mitigation solutions in one context may not be
appropriate in another. Thus, a myriad of design choices have to be made when
creating a bias mitigation method, such as the formalization of fairness it
pursues, and where and how it intervenes in the ML pipeline. This creates
challenges in benchmarking and comparing the relative merits of different bias
mitigation methods, and limits their uptake by practitioners.
  We propose BiMi Sheets as a portable, uniform guide to document the design
choices of any bias mitigation method. This enables researchers and
practitioners to quickly learn its main characteristics and to compare with
their desiderata. Furthermore, the sheets' structure allow for the creation of
a structured database of bias mitigation methods. In order to foster the
sheets' adoption, we provide a platform for finding and creating BiMi Sheets at
bimisheet.com.

</details>


### [595] [Oryx: a Performant and Scalable Algorithm for Many-Agent Coordination in Offline MARL](https://arxiv.org/pdf/2505.22151)
*Claude Formanek, Omayma Mahjoub, Louay Ben Nessir, Sasha Abramowitz, Ruan de Kock, Wiem Khlifi, Simon Du Toit, Felix Chalumeau, Daniel Rajaonarivonivelomanantsoa, Arnol Fokam, Siddarth Singh, Ulrich Mbou Sob, Arnu Pretorius*

Main category: cs.LG

TL;DR: Oryx is a novel offline MARL algorithm combining retention-based architecture and implicit constraint Q-learning for effective many-agent coordination. It outperforms prior methods on 80% of 65 datasets and scales well in complex settings.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of many-agent multi-step coordination in offline MARL.

Method: Combines retention-based architecture (Sable) with sequential implicit constraint Q-learning (ICQ) for an auto-regressive policy update.

Result: Achieves state-of-the-art performance on 80% of 65 datasets, demonstrating robust generalization across domains.

Conclusion: Oryx effectively scales and solves complex coordination challenges, with datasets and code to be made available.

Abstract: A key challenge in offline multi-agent reinforcement learning (MARL) is
achieving effective many-agent multi-step coordination in complex environments.
In this work, we propose Oryx, a novel algorithm for offline cooperative MARL
to directly address this challenge. Oryx adapts the recently proposed
retention-based architecture Sable and combines it with a sequential form of
implicit constraint Q-learning (ICQ), to develop a novel offline
auto-regressive policy update scheme. This allows Oryx to solve complex
coordination challenges while maintaining temporal coherence over lengthy
trajectories. We evaluate Oryx across a diverse set of benchmarks from prior
works (SMAC, RWARE, and Multi-Agent MuJoCo) covering tasks of both discrete and
continuous control, varying in scale and difficulty. Oryx achieves
state-of-the-art performance on more than 80% of the 65 tested datasets,
outperforming prior offline MARL methods and demonstrating robust
generalisation across domains with many agents and long horizons. Finally, we
introduce new datasets to push the limits of many-agent coordination in offline
MARL, and demonstrate Oryx's superior ability to scale effectively in such
settings. We will make all of our datasets, experimental data, and code
available upon publication.

</details>


### [596] [Uncertainty Estimation for Heterophilic Graphs Through the Lens of Information Theory](https://arxiv.org/pdf/2505.22152)
*Dominik Fuchsgruber, Tom Wollschläger, Johannes Bordne, Stephan Günnemann*

Main category: cs.LG

TL;DR: The paper addresses uncertainty estimation in heterophilic graphs by analyzing MPNNs from an information-theoretic perspective, proposing a joint node embedding approach for state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing uncertainty estimation methods for graphs rely on homophily and fail in heterophilic settings.

Method: Analyzes MPNNs using information theory, develops a data processing inequality analog, and uses a post-hoc density estimator on joint node embeddings.

Result: The method achieves state-of-the-art uncertainty estimation on heterophilic graphs and matches prior work on homophilic graphs.

Conclusion: Joint consideration of node representations is key for uncertainty estimation in graphs beyond homophily.

Abstract: While uncertainty estimation for graphs recently gained traction, most
methods rely on homophily and deteriorate in heterophilic settings. We address
this by analyzing message passing neural networks from an information-theoretic
perspective and developing a suitable analog to data processing inequality to
quantify information throughout the model's layers. In contrast to non-graph
domains, information about the node-level prediction target can increase with
model depth if a node's features are semantically different from its neighbors.
Therefore, on heterophilic graphs, the latent embeddings of an MPNN each
provide different information about the data distribution - different from
homophilic settings. This reveals that considering all node representations
simultaneously is a key design principle for epistemic uncertainty estimation
on graphs beyond homophily. We empirically confirm this with a simple post-hoc
density estimator on the joint node embedding space that provides
state-of-the-art uncertainty on heterophilic graphs. At the same time, it
matches prior work on homophilic graphs without explicitly exploiting homophily
through post-processing.

</details>


### [597] [The informativeness of the gradient revisited](https://arxiv.org/pdf/2505.22158)
*Rustem Takhanov*

Main category: cs.LG

TL;DR: The paper provides a theoretical bound on gradient variance in deep learning, linking it to pairwise independence and collision entropy, and applies this to LWE mappings and high-frequency functions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical understanding of gradient-based deep learning limitations, especially when gradients contain minimal information.

Method: The authors derive a general bound on gradient variance using parameters related to pairwise independence and collision entropy.

Result: The bound scales as $\tilde{\mathcal{O}}(\varepsilon+e^{-\frac{1}{2}\mathcal{E}_c})$, with applications to LWE mappings and high-frequency functions.

Conclusion: The theoretical and experimental analysis enhances understanding of gradient limitations in deep learning, particularly for LWE attacks.

Abstract: In the past decade gradient-based deep learning has revolutionized several
applications. However, this rapid advancement has highlighted the need for a
deeper theoretical understanding of its limitations. Research has shown that,
in many practical learning tasks, the information contained in the gradient is
so minimal that gradient-based methods require an exceedingly large number of
iterations to achieve success. The informativeness of the gradient is typically
measured by its variance with respect to the random selection of a target
function from a hypothesis class.
  We use this framework and give a general bound on the variance in terms of a
parameter related to the pairwise independence of the target function class and
the collision entropy of the input distribution. Our bound scales as $
\tilde{\mathcal{O}}(\varepsilon+e^{-\frac{1}{2}\mathcal{E}_c}) $, where $
\tilde{\mathcal{O}} $ hides factors related to the regularity of the learning
model and the loss function, $ \varepsilon $ measures the pairwise independence
of the target function class and $\mathcal{E}_c$ is the collision entropy of
the input distribution.
  To demonstrate the practical utility of our bound, we apply it to the class
of Learning with Errors (LWE) mappings and high-frequency functions. In
addition to the theoretical analysis, we present experiments to understand
better the nature of recent deep learning-based attacks on LWE.

</details>


### [598] [An Augmentation-Aware Theory for Self-Supervised Contrastive Learning](https://arxiv.org/pdf/2505.22196)
*Jingyi Cui, Hongwei Wen, Yisen Wang*

Main category: cs.LG

TL;DR: Proposes an augmentation-aware error bound for self-supervised contrastive learning, linking supervised risk to unsupervised risk and data augmentation trade-offs.


<details>
  <summary>Details</summary>
Motivation: Existing theoretical studies under-exploit the role of data augmentation, particularly specific types, in self-supervised contrastive learning.

Method: Introduces an augmentation-aware error bound and analyzes effects of augmentation methods under a semantic label assumption.

Result: Shows supervised risk is bounded by unsupervised risk and augmentation trade-offs, verified through pixel- and representation-level experiments.

Conclusion: The study fills a gap by theoretically and empirically demonstrating the impact of data augmentation in contrastive learning.

Abstract: Self-supervised contrastive learning has emerged as a powerful tool in
machine learning and computer vision to learn meaningful representations from
unlabeled data. Meanwhile, its empirical success has encouraged many
theoretical studies to reveal the learning mechanisms. However, in the existing
theoretical research, the role of data augmentation is still under-exploited,
especially the effects of specific augmentation types. To fill in the blank, we
for the first time propose an augmentation-aware error bound for
self-supervised contrastive learning, showing that the supervised risk is
bounded not only by the unsupervised risk, but also explicitly by a trade-off
induced by data augmentation. Then, under a novel semantic label assumption, we
discuss how certain augmentation methods affect the error bound. Lastly, we
conduct both pixel- and representation-level experiments to verify our proposed
theoretical results.

</details>


### [599] [Enhancing Uncertainty Estimation and Interpretability via Bayesian Non-negative Decision Layer](https://arxiv.org/pdf/2505.22199)
*Xinyue Hu, Zhibin Duan, Bo Chen, Mingyuan Zhou*

Main category: cs.LG

TL;DR: The paper introduces a Bayesian Non-negative Decision Layer (BNDL) to improve uncertainty estimation and interpretability in deep neural networks by leveraging stochastic latent variables and non-negativity constraints.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks lack reliable uncertainty estimation and interpretability due to entangled features, which BNDL aims to address.

Method: BNDL reformulates deep neural networks as conditional Bayesian non-negative factor analysis, using stochastic latent variables and a Weibull variational inference network for posterior approximation.

Result: BNDL enhances disentanglement, improving accuracy, uncertainty estimation, and interpretability in experiments.

Conclusion: BNDL effectively addresses uncertainty and interpretability challenges in deep learning, supported by theoretical guarantees and empirical results.

Abstract: Although deep neural networks have demonstrated significant success due to
their powerful expressiveness, most models struggle to meet practical
requirements for uncertainty estimation. Concurrently, the entangled nature of
deep neural networks leads to a multifaceted problem, where various localized
explanation techniques reveal that multiple unrelated features influence the
decisions, thereby undermining interpretability. To address these challenges,
we develop a Bayesian Non-negative Decision Layer (BNDL), which reformulates
deep neural networks as a conditional Bayesian non-negative factor analysis. By
leveraging stochastic latent variables, the BNDL can model complex dependencies
and provide robust uncertainty estimation. Moreover, the sparsity and
non-negativity of the latent variables encourage the model to learn
disentangled representations and decision layers, thereby improving
interpretability. We also offer theoretical guarantees that BNDL can achieve
effective disentangled learning. In addition, we developed a corresponding
variational inference method utilizing a Weibull variational inference network
to approximate the posterior distribution of the latent variables. Our
experimental results demonstrate that with enhanced disentanglement
capabilities, BNDL not only improves the model's accuracy but also provides
reliable uncertainty estimation and improved interpretability.

</details>


### [600] [Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning](https://arxiv.org/pdf/2505.22203)
*Yuzhen Huang, Weihao Zeng, Xingshan Zeng, Qi Zhu, Junxian He*

Main category: cs.LG

TL;DR: The paper analyzes the reliability of rule-based and model-based verifiers in RLVR, highlighting their flaws in handling mathematical reasoning tasks and their impact on RL training.


<details>
  <summary>Details</summary>
Motivation: To understand the reliability of verifiers in RLVR and their effect on training, especially in complex domains like mathematical reasoning.

Method: Conducts a comprehensive analysis of verifiers in static evaluation and RL training, comparing rule-based and model-based approaches.

Result: Rule-based verifiers fail with equivalent answers, while model-based verifiers are prone to hacking, both negatively impacting RL training.

Conclusion: Both verifier types have unique risks; insights are provided to develop more robust reward systems in RL.

Abstract: Trustworthy verifiers are essential for the success of reinforcement learning
with verifiable reward (RLVR), which is the core methodology behind various
large reasoning models such as DeepSeek-R1. In complex domains like
mathematical reasoning, rule-based verifiers have been widely adopted in
previous works to train strong reasoning models. However, the reliability of
these verifiers and their impact on the RL training process remain poorly
understood. In this work, we take mathematical reasoning as a case study and
conduct a comprehensive analysis of various verifiers in both static evaluation
and RL training scenarios. First, we find that current open-source rule-based
verifiers often fail to recognize equivalent answers presented in different
formats across multiple commonly used mathematical datasets, resulting in
non-negligible false negative rates. This limitation adversely affects RL
training performance and becomes more pronounced as the policy model gets
stronger. Subsequently, we investigate model-based verifiers as a potential
solution to address these limitations. While the static evaluation shows that
model-based verifiers achieve significantly higher verification accuracy,
further analysis and RL training results imply that they are highly susceptible
to hacking, where they misclassify certain patterns in responses as correct
(i.e., false positives). This vulnerability is exploited during policy model
optimization, leading to artificially inflated rewards. Our findings underscore
the unique risks inherent to both rule-based and model-based verifiers, aiming
to offer valuable insights to develop more robust reward systems in
reinforcement learning.

</details>


### [601] [Solver-Free Decision-Focused Learning for Linear Optimization Problems](https://arxiv.org/pdf/2505.22224)
*Senne Berden, Ali İrfan Mahmutoğulları, Dimos Tsouros, Tias Guns*

Main category: cs.LG

TL;DR: A solver-free training method for linear optimization in predict-then-optimize problems reduces computational cost while maintaining decision quality.


<details>
  <summary>Details</summary>
Motivation: Address the computational bottleneck in decision-focused learning (DFL) for linear optimization problems.

Method: Exploits geometric structure of linear optimization, comparing ground-truth optimal solution quality with adjacent vertices as a loss function.

Result: Significantly reduces computational cost with minimal degradation in solution quality.

Conclusion: Proposed method efficiently trains models for linear optimization problems without solving them repeatedly.

Abstract: Mathematical optimization is a fundamental tool for decision-making in a wide
range of applications. However, in many real-world scenarios, the parameters of
the optimization problem are not known a priori and must be predicted from
contextual features. This gives rise to predict-then-optimize problems, where a
machine learning model predicts problem parameters that are then used to make
decisions via optimization. A growing body of work on decision-focused learning
(DFL) addresses this setting by training models specifically to produce
predictions that maximize downstream decision quality, rather than accuracy.
While effective, DFL is computationally expensive, because it requires solving
the optimization problem with the predicted parameters at each loss evaluation.
In this work, we address this computational bottleneck for linear optimization
problems, a common class of problems in both DFL literature and real-world
applications. We propose a solver-free training method that exploits the
geometric structure of linear optimization to enable efficient training with
minimal degradation in solution quality. Our method is based on the insight
that a solution is optimal if and only if it achieves an objective value that
is at least as good as that of its adjacent vertices on the feasible polytope.
Building on this, our method compares the estimated quality of the ground-truth
optimal solution with that of its precomputed adjacent vertices, and uses this
as loss function. Experiments demonstrate that our method significantly reduces
computational cost while maintaining high decision quality.

</details>


### [602] [LaMM: Semi-Supervised Pre-Training of Large-Scale Materials Models](https://arxiv.org/pdf/2505.22208)
*Yosuke Oyama, Yusuke Majima, Eiji Ohta, Yasufumi Sakai*

Main category: cs.LG

TL;DR: LaMM introduces a semi-supervised pre-training method for neural network potentials (NNPs), combining denoising self-supervised learning and load-balancing to improve efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: NNPs accelerate materials science by replacing DFT calculations, but pre-training and fine-tuning are computationally expensive due to DFT labeling costs and load imbalances.

Method: LaMM uses semi-supervised pre-training with denoising self-supervised learning and a load-balancing algorithm for efficient multi-node training.

Result: The method effectively trains an NNP on ~300M semi-labeled samples, enhancing fine-tuning speed and accuracy.

Conclusion: LaMM addresses computational challenges in NNP training, improving performance for materials science applications.

Abstract: Neural network potentials (NNPs) are crucial for accelerating computational
materials science by surrogating density functional theory (DFT) calculations.
Improving their accuracy is possible through pre-training and fine-tuning,
where an NNP model is first pre-trained on a large-scale dataset and then
fine-tuned on a smaller target dataset. However, this approach is
computationally expensive, mainly due to the cost of DFT-based dataset labeling
and load imbalances during large-scale pre-training. To address this, we
propose LaMM, a semi-supervised pre-training method incorporating improved
denoising self-supervised learning and a load-balancing algorithm for efficient
multi-node training. We demonstrate that our approach effectively leverages a
large-scale dataset of $\sim$300 million semi-labeled samples to train a single
NNP model, resulting in improved fine-tuning performance in terms of both speed
and accuracy.

</details>


### [603] [Optimal kernel regression bounds under energy-bounded noise](https://arxiv.org/pdf/2505.22235)
*Amon Lahr, Johannes Köhler, Anna Scampicchio, Melanie N. Zeilinger*

Main category: cs.LG

TL;DR: The paper derives a tight, non-asymptotic uncertainty bound for kernel-based estimation, handling correlated noise, and shows its effectiveness for accurate and safety-critical applications.


<details>
  <summary>Details</summary>
Motivation: To provide non-conservative uncertainty bounds for assessing estimation accuracy and enabling safe deployment in critical contexts.

Method: Uses a norm-boundedness assumption on the unknown function and noise, computing worst-case function realization via Gaussian process posterior mean and covariance.

Result: Demonstrates tight, easy-to-compute bounds for kernel-based estimates, outperforming existing methods.

Conclusion: The proposed approach effectively provides rigorous and practical uncertainty bounds for kernel-based estimation.

Abstract: Non-conservative uncertainty bounds are key for both assessing an estimation
algorithm's accuracy and in view of downstream tasks, such as its deployment in
safety-critical contexts. In this paper, we derive a tight, non-asymptotic
uncertainty bound for kernel-based estimation, which can also handle correlated
noise sequences. Its computation relies on a mild norm-boundedness assumption
on the unknown function and the noise, returning the worst-case function
realization within the hypothesis class at an arbitrary query input location.
The value of this function is shown to be given in terms of the posterior mean
and covariance of a Gaussian process for an optimal choice of the measurement
noise covariance. By rigorously analyzing the proposed approach and comparing
it with other results in the literature, we show its effectiveness in returning
tight and easy-to-compute bounds for kernel-based estimates.

</details>


### [604] [B-XAIC Dataset: Benchmarking Explainable AI for Graph Neural Networks Using Chemical Data](https://arxiv.org/pdf/2505.22252)
*Magdalena Proszewska, Tomasz Danel, Dawid Rymarczyk*

Main category: cs.LG

TL;DR: B-XAIC is a new benchmark for evaluating Explainable AI (XAI) in cheminformatics, using real-world molecular data and tasks with known ground-truth rationales to assess the faithfulness of explanations for Graph Neural Networks (GNNs).


<details>
  <summary>Details</summary>
Motivation: Current XAI evaluation frameworks in cheminformatics rely on artificial datasets or simplified tasks, lacking real-world complexity and direct links to explanation faithfulness.

Method: The authors introduce B-XAIC, a benchmark built from real-world molecular data and diverse tasks with known ground-truth rationales.

Result: Evaluation with B-XAIC reveals limitations of existing XAI methods for GNNs in molecular applications.

Conclusion: B-XAIC serves as a valuable resource for improving the reliability and interpretability of XAI models in cheminformatics.

Abstract: Understanding the reasoning behind deep learning model predictions is crucial
in cheminformatics and drug discovery, where molecular design determines their
properties. However, current evaluation frameworks for Explainable AI (XAI) in
this domain often rely on artificial datasets or simplified tasks, employing
data-derived metrics that fail to capture the complexity of real-world
scenarios and lack a direct link to explanation faithfulness. To address this,
we introduce B-XAIC, a novel benchmark constructed from real-world molecular
data and diverse tasks with known ground-truth rationales for assigned labels.
Through a comprehensive evaluation using B-XAIC, we reveal limitations of
existing XAI methods for Graph Neural Networks (GNNs) in the molecular domain.
This benchmark provides a valuable resource for gaining deeper insights into
the faithfulness of XAI, facilitating the development of more reliable and
interpretable models.

</details>


### [605] [A Unified Online-Offline Framework for Co-Branding Campaign Recommendations](https://arxiv.org/pdf/2505.22254)
*Xiangxiang Dai, Xiaowei Sun, Jinhang Zuo, Xutong Liu, John C. S. Lui*

Main category: cs.LG

TL;DR: The paper proposes a unified online-offline framework for co-branding recommendations, addressing challenges like resource imbalances and dynamic market conditions. It achieves improved performance with a 12%+ gain.


<details>
  <summary>Details</summary>
Motivation: Co-branding is crucial for market expansion, but challenges like resource imbalances and uncertain brand willingness hinder effective cross-industry partnerships.

Method: The approach uses a bipartite graph to quantify co-branding probabilities, dynamically updates it online, and optimizes offline by consolidating sub-brand interests.

Result: The framework achieves at least a 12% improvement in performance, with theoretical guarantees for online learning and offline optimization.

Conclusion: The proposed framework effectively balances exploration and exploitation, enhances short-term performance, and ensures sustainable growth in co-branding.

Abstract: Co-branding has become a vital strategy for businesses aiming to expand
market reach within recommendation systems. However, identifying effective
cross-industry partnerships remains challenging due to resource imbalances,
uncertain brand willingness, and ever-changing market conditions. In this
paper, we provide the first systematic study of this problem and propose a
unified online-offline framework to enable co-branding recommendations. Our
approach begins by constructing a bipartite graph linking ``initiating'' and
``target'' brands to quantify co-branding probabilities and assess market
benefits. During the online learning phase, we dynamically update the graph in
response to market feedback, while striking a balance between exploring new
collaborations for long-term gains and exploiting established partnerships for
immediate benefits. To address the high initial co-branding costs, our
framework mitigates redundant exploration, thereby enhancing short-term
performance while ensuring sustainable strategic growth. In the offline
optimization phase, our framework consolidates the interests of multiple
sub-brands under the same parent brand to maximize overall returns, avoid
excessive investment in single sub-brands, and reduce unnecessary costs
associated with over-prioritizing a single sub-brand. We present a theoretical
analysis of our approach, establishing a highly nontrivial sublinear regret
bound for online learning in the complex co-branding problem, and enhancing the
approximation guarantee for the NP-hard offline budget allocation optimization.
Experiments on both synthetic and real-world co-branding datasets demonstrate
the practical effectiveness of our framework, with at least 12\% improvement.

</details>


### [606] [Versatile Cardiovascular Signal Generation with a Unified Diffusion Transformer](https://arxiv.org/pdf/2505.22306)
*Zehua Chen, Yuyang Miao, Liyuan Wang, Luyun Fan, Danilo P. Mandic, Jun Zhu*

Main category: cs.LG

TL;DR: UniCardio is a multi-modal diffusion transformer for reconstructing and synthesizing cardiovascular signals, outperforming baselines in denoising, imputation, and translation.


<details>
  <summary>Details</summary>
Motivation: Joint utilization of cardiovascular signals (PPG, ECG, BP) is limited by acquisition challenges like noise and invasiveness.

Method: Proposes UniCardio, a unified generative framework with a specialized architecture and continual learning for varying modality combinations.

Result: Outperforms baselines in signal tasks; generated signals match ground-truth performance in health detection and vital sign estimation.

Conclusion: UniCardio advances AI-assisted healthcare by leveraging complementary cardiovascular signals effectively.

Abstract: Cardiovascular signals such as photoplethysmography (PPG),
electrocardiography (ECG), and blood pressure (BP) are inherently correlated
and complementary, together reflecting the health of cardiovascular system.
However, their joint utilization in real-time monitoring is severely limited by
diverse acquisition challenges from noisy wearable recordings to burdened
invasive procedures. Here we propose UniCardio, a multi-modal diffusion
transformer that reconstructs low-quality signals and synthesizes unrecorded
signals in a unified generative framework. Its key innovations include a
specialized model architecture to manage the signal modalities involved in
generation tasks and a continual learning paradigm to incorporate varying
modality combinations. By exploiting the complementary nature of cardiovascular
signals, UniCardio clearly outperforms recent task-specific baselines in signal
denoising, imputation, and translation. The generated signals match the
performance of ground-truth signals in detecting abnormal health conditions and
estimating vital signs, even in unseen domains, while ensuring interpretability
for human experts. These advantages position UniCardio as a promising avenue
for advancing AI-assisted healthcare.

</details>


### [607] [Train Sparse Autoencoders Efficiently by Utilizing Features Correlation](https://arxiv.org/pdf/2505.22255)
*Vadim Kurochkin, Yaroslav Aksenov, Daniil Laptev, Daniil Gavrilov, Nikita Balagansky*

Main category: cs.LG

TL;DR: KronSAE introduces Kronecker product decomposition and mAND activation to improve efficiency and interpretability in sparse autoencoders.


<details>
  <summary>Details</summary>
Motivation: Training sparse autoencoders (SAEs) at scale is computationally intensive, especially with large dictionary sizes, prompting the need for more efficient methods.

Method: Proposes KronSAE, using Kronecker product decomposition to reduce computational overhead, and mAND, a differentiable activation function for better interpretability.

Result: Significantly reduces memory and computational costs while maintaining or improving interpretability and performance.

Conclusion: KronSAE and mAND offer a scalable and interpretable solution for training SAEs, addressing key challenges in efficiency.

Abstract: Sparse Autoencoders (SAEs) have demonstrated significant promise in
interpreting the hidden states of language models by decomposing them into
interpretable latent directions. However, training SAEs at scale remains
challenging, especially when large dictionary sizes are used. While decoders
can leverage sparse-aware kernels for efficiency, encoders still require
computationally intensive linear operations with large output dimensions. To
address this, we propose KronSAE, a novel architecture that factorizes the
latent representation via Kronecker product decomposition, drastically reducing
memory and computational overhead. Furthermore, we introduce mAND, a
differentiable activation function approximating the binary AND operation,
which improves interpretability and performance in our factorized framework.

</details>


### [608] [From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization](https://arxiv.org/pdf/2505.22310)
*Shoaib Ahmed Siddiqui, Adrian Weller, David Krueger, Gintare Karolina Dziugaite, Michael Curtis Mozer, Eleni Triantafillou*

Main category: cs.LG

TL;DR: Recent unlearning methods for LLMs are vulnerable to relearning attacks, where unlearned knowledge re-emerges with minimal fine-tuning. The study shows forget-set accuracy can recover to nearly 100% without forget-set examples, unlike retrained models. Resistance to relearning is linked to weight-space properties, leading to improved methods.


<details>
  <summary>Details</summary>
Motivation: To investigate the vulnerability of unlearning methods to relearning attacks and understand the conditions under which unlearned knowledge resurfaces.

Method: Controlled study of example-level unlearning in vision classifiers, analyzing forget-set accuracy recovery and weight-space properties like $L_2$-distance and linear mode connectivity.

Result: Forget-set accuracy recovers to nearly 100% with fine-tuning on the retain set alone, unlike retrained models. Resistance to relearning is predictable via weight-space metrics.

Conclusion: Weight-space properties predict relearning resistance, enabling the development of more robust unlearning methods.

Abstract: Recent unlearning methods for LLMs are vulnerable to relearning attacks:
knowledge believed-to-be-unlearned re-emerges by fine-tuning on a small set of
(even seemingly-unrelated) examples. We study this phenomenon in a controlled
setting for example-level unlearning in vision classifiers. We make the
surprising discovery that forget-set accuracy can recover from around 50%
post-unlearning to nearly 100% with fine-tuning on just the retain set -- i.e.,
zero examples of the forget set. We observe this effect across a wide variety
of unlearning methods, whereas for a model retrained from scratch excluding the
forget set (gold standard), the accuracy remains at 50%. We observe that
resistance to relearning attacks can be predicted by weight-space properties,
specifically, $L_2$-distance and linear mode connectivity between the original
and the unlearned model. Leveraging this insight, we propose a new class of
methods that achieve state-of-the-art resistance to relearning attacks.

</details>


### [609] [Revisiting Group Relative Policy Optimization: Insights into On-Policy and Off-Policy Training](https://arxiv.org/pdf/2505.22257)
*Youssef Mroueh, Nicolas Dupuis, Brian Belgodere, Apoorva Nitsure, Mattia Rigotti, Kristjan Greenewald, Jiri Navratil, Jerret Ross, Jesus Rios*

Main category: cs.LG

TL;DR: GRPO is adapted for off-policy settings, showing improved rewards and comparable or better performance than on-policy GRPO.


<details>
  <summary>Details</summary>
Motivation: Improving training stability, sampling efficiency, and memory usage in reinforcement learning by leveraging off-policy PPO and GRPO advantages.

Method: Adapting GRPO to off-policy settings and comparing its performance with on-policy GRPO using clipped surrogate objectives.

Result: Off-policy GRPO outperforms or matches on-policy GRPO in reward improvement and empirical performance.

Conclusion: Off-policy GRPO is a viable and often superior alternative to on-policy GRPO, especially in scenarios requiring verifiable rewards.

Abstract: We revisit Group Relative Policy Optimization (GRPO) in both on-policy and
off-policy optimization regimes. Our motivation comes from recent work on
off-policy Proximal Policy Optimization (PPO), which improves training
stability, sampling efficiency, and memory usage. In addition, a recent
analysis of GRPO suggests that estimating the advantage function with
off-policy samples could be beneficial. Building on these observations, we
adapt GRPO to the off-policy setting. We show that both on-policy and
off-policy GRPO objectives yield an improvement in the reward. This result
motivates the use of clipped surrogate objectives in the off-policy version of
GRPO. We then compare the empirical performance of reinforcement learning with
verifiable rewards in post-training using both GRPO variants. Our results show
that off-policy GRPO either significantly outperforms or performs on par with
its on-policy counterpart.

</details>


### [610] [Skywork Open Reasoner 1 Technical Report](https://arxiv.org/pdf/2505.22312)
*Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu, Yahui Zhou*

Main category: cs.LG

TL;DR: Skywork-OR1 enhances reasoning in LLMs using RL, achieving significant accuracy gains on benchmarks like AIME24 and LiveCodeBench.


<details>
  <summary>Details</summary>
Motivation: To improve reasoning capabilities in LLMs via scalable RL, addressing performance gaps in long Chain-of-Thought models.

Method: Builds on DeepSeek-R1-Distill, using RL to refine models, with ablation studies and entropy collapse analysis.

Result: Accuracy improved by 15.0% (32B model) and 13.9% (7B model), outperforming competitors on key benchmarks.

Conclusion: Mitigating entropy collapse is crucial for performance; open-sourcing supports further research.

Abstract: The success of DeepSeek-R1 underscores the significant role of reinforcement
learning (RL) in enhancing the reasoning capabilities of large language models
(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL
implementation for long Chain-of-Thought (CoT) models. Building on the
DeepSeek-R1-Distill model series, our RL approach achieves notable performance
gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench
from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)
for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and
Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable
results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models
demonstrate competitive reasoning capabilities among models of similar size. We
perform comprehensive ablation studies on the core components of our training
pipeline to validate their effectiveness. Additionally, we thoroughly
investigate the phenomenon of entropy collapse, identify key factors affecting
entropy dynamics, and demonstrate that mitigating premature entropy collapse is
critical for improved test performance. To support community research, we fully
open-source our model weights, training code, and training datasets.

</details>


### [611] [Full Domain Analysis in Fluid Dynamics](https://arxiv.org/pdf/2505.22275)
*Alexander Hagg, Adam Gaier, Dominik Wilde, Alexander Asteroth, Holger Foysi, Dirk Reith*

Main category: cs.LG

TL;DR: The paper introduces full domain analysis, a technique combining evolutionary optimization, simulation, and machine learning to comprehensively explore and analyze complex domains like fluid dynamics.


<details>
  <summary>Details</summary>
Motivation: To deepen understanding of complex domains (e.g., fluid dynamics) by efficiently generating, diversifying, optimizing, and analyzing solutions.

Method: Defines a formal model for full domain analysis, leveraging evolutionary optimization, simulation, and machine learning.

Result: Demonstrates the potential of full domain analysis to provide insights into complex systems, with an example application.

Conclusion: Full domain analysis is a valuable tool for understanding complex systems in computational physics and other fields.

Abstract: Novel techniques in evolutionary optimization, simulation and machine
learning allow for a broad analysis of domains like fluid dynamics, in which
computation is expensive and flow behavior is complex. Under the term of full
domain analysis we understand the ability to efficiently determine the full
space of solutions in a problem domain, and analyze the behavior of those
solutions in an accessible and interactive manner. The goal of full domain
analysis is to deepen our understanding of domains by generating many examples
of flow, their diversification, optimization and analysis. We define a formal
model for full domain analysis, its current state of the art, and requirements
of subcomponents. Finally, an example is given to show what we can learn by
using full domain analysis. Full domain analysis, rooted in optimization and
machine learning, can be a helpful tool in understanding complex systems in
computational physics and beyond.

</details>


### [612] [Transformers Pretrained on Procedural Data Contain Modular Structures for Algorithmic Reasoning](https://arxiv.org/pdf/2505.22308)
*Zachary Shinnick, Liangze Jiang, Hemanth Saratchandran, Anton van den Hengel, Damien Teney*

Main category: cs.LG

TL;DR: Procedural synthetic data can improve small transformers' algorithmic reasoning, with distinct rules imparting complementary structures in different model parts, enabling knowledge-reasoning disentanglement.


<details>
  <summary>Details</summary>
Motivation: Understand how simple synthetic data benefits language models, identifying specific capabilities and their architectural locations.

Method: Identify beneficial procedural data forms, analyze their impact on small transformers, and conduct ablations/transfer experiments.

Result: Different rules create distinct, complementary structures in attention layers and MLPs, which can be composed to enhance multiple capabilities.

Conclusion: Synthetic data can disentangle knowledge acquisition from reasoning, potentially improving model robustness and efficiency.

Abstract: Pretraining on large, semantically rich datasets is key for developing
language models. Surprisingly, recent studies have shown that even synthetic
data, generated procedurally through simple semantic-free algorithms, can yield
some of the same benefits as natural language pretraining. It is unclear what
specific capabilities such simple synthetic data instils in a model, where
these capabilities reside in the architecture, and how they manifest within its
weights. In this short paper, we identify several beneficial forms of
procedural data, together with specific algorithmic reasoning skills that
improve in small transformers. Our core finding is that different procedural
rules instil distinct but complementary inductive structures in the model. With
extensive ablations and partial-transfer experiments, we discover that these
structures reside in different parts of the model. Attention layers often carry
the most transferable information, but some pretraining rules impart useful
structure to MLP blocks instead. Most interestingly, the structures induced by
multiple rules can be composed to jointly reinforce multiple capabilities.
These results suggest an exciting possibility of disentangling the acquisition
of knowledge from reasoning in language models, with the goal of improving
their robustness and data efficiency.

</details>


### [613] [Rethinking BPS: A Utility-Based Evaluation Framework](https://arxiv.org/pdf/2505.22316)
*Konrad Özdemir, Lukas Kirchdorfer, Keyvan Amiri Elyasi, Han van der Aa, Heiner Stuckenschmidt*

Main category: cs.LG

TL;DR: The paper proposes a new framework for evaluating Business Process Simulation (BPS) models by assessing their ability to generate representative process behavior, addressing limitations of current methods.


<details>
  <summary>Details</summary>
Motivation: Current BPS evaluation methods treat simulation as a forecasting problem and rely on metrics that obscure temporal patterns, leading to misleading conclusions.

Method: The proposed framework evaluates BPS models by comparing the performance of predictive process monitoring models trained on simulated vs. real data for downstream tasks.

Result: Empirical results show the framework identifies discrepancies and distinguishes between model accuracy and data complexity.

Conclusion: The new framework offers a more meaningful way to assess BPS quality by focusing on representative behavior generation.

Abstract: Business process simulation (BPS) is a key tool for analyzing and optimizing
organizational workflows, supporting decision-making by estimating the impact
of process changes. The reliability of such estimates depends on the ability of
a BPS model to accurately mimic the process under analysis, making rigorous
accuracy evaluation essential. However, the state-of-the-art approach to
evaluating BPS models has two key limitations. First, it treats simulation as a
forecasting problem, testing whether models can predict unseen future events.
This fails to assess how well a model captures the as-is process, particularly
when process behavior changes from train to test period. Thus, it becomes
difficult to determine whether poor results stem from an inaccurate model or
the inherent complexity of the data, such as unpredictable drift. Second, the
evaluation approach strongly relies on Earth Mover's Distance-based metrics,
which can obscure temporal patterns and thus yield misleading conclusions about
simulation quality. To address these issues, we propose a novel framework that
evaluates simulation quality based on its ability to generate representative
process behavior. Instead of comparing simulated logs to future real-world
executions, we evaluate whether predictive process monitoring models trained on
simulated data perform comparably to those trained on real data for downstream
analysis tasks. Empirical results show that our framework not only helps
identify sources of discrepancies but also distinguishes between model accuracy
and data complexity, offering a more meaningful way to assess BPS quality.

</details>


### [614] [A Closer Look on Memorization in Tabular Diffusion Model: A Data-Centric Perspective](https://arxiv.org/pdf/2505.22322)
*Zhengyu Fang, Zhimeng Jiang, Huiyuan Chen, Xiaoge Zhang, Kaiyu Tang, Xiao Li, Jing Li*

Main category: cs.LG

TL;DR: The paper studies memorization in tabular diffusion models, identifies high-risk samples, and proposes DynamicCut to mitigate privacy risks with minimal impact on data quality.


<details>
  <summary>Details</summary>
Motivation: Diffusion models for tabular data risk privacy by reproducing exact training samples, but little is known about which samples contribute most to memorization.

Method: Quantifies memorization per sample using a relative distance ratio, analyzes memorization dynamics, and introduces DynamicCut—a two-stage method to rank and prune high-risk samples before retraining.

Result: Reveals a heavy-tailed memorization distribution; DynamicCut reduces leakage without harming diversity or performance and works across models (e.g., GANs, VAEs).

Conclusion: DynamicCut effectively mitigates memorization risks in tabular diffusion models and is transferable to other generative models.

Abstract: Diffusion models have shown strong performance in generating high-quality
tabular data, but they carry privacy risks by reproducing exact training
samples. While prior work focuses on dataset-level augmentation to reduce
memorization, little is known about which individual samples contribute most.
We present the first data-centric study of memorization dynamics in tabular
diffusion models. We quantify memorization for each real sample based on how
many generated samples are flagged as replicas, using a relative distance
ratio. Our empirical analysis reveals a heavy-tailed distribution of
memorization counts: a small subset of samples contributes disproportionately
to leakage, confirmed via sample-removal experiments. To understand this, we
divide real samples into top- and non-top-memorized groups and analyze their
training-time behaviors. We track when each sample is first memorized and
monitor per-epoch memorization intensity (AUC). Memorized samples are memorized
slightly earlier and show stronger signals in early training. Based on these
insights, we propose DynamicCut, a two-stage, model-agnostic mitigation method:
(a) rank samples by epoch-wise intensity, (b) prune a tunable top fraction, and
(c) retrain on the filtered dataset. Across multiple tabular datasets and
models, DynamicCut reduces memorization with minimal impact on data diversity
and downstream performance. It also complements augmentation-based defenses.
Furthermore, DynamicCut enables cross-model transferability: high-ranked
samples identified from one model (e.g., a diffusion model) are also effective
for reducing memorization when removed from others, such as GANs and VAEs.

</details>


### [615] [Suitability Filter: A Statistical Framework for Classifier Evaluation in Real-World Deployment Settings](https://arxiv.org/pdf/2505.22356)
*Angéline Pouget, Mohammad Yaghini, Stephan Rabanser, Nicolas Papernot*

Main category: cs.LG

TL;DR: The paper introduces the suitability filter, a framework to detect performance deterioration in machine learning models for safety-critical domains without ground truth labels, using suitability signals and statistical testing.


<details>
  <summary>Details</summary>
Motivation: Ensuring reliable model performance in safety-critical applications where ground truth labels are unavailable for validation.

Method: Proposes the suitability filter, which aggregates suitability signals from model outputs and compares distributions of test and user data using statistical hypothesis testing.

Result: Empirical evaluations show the suitability filter effectively detects performance deviations due to covariate shift.

Conclusion: The suitability filter enables proactive mitigation of potential failures in high-stakes applications by reliably monitoring model performance.

Abstract: Deploying machine learning models in safety-critical domains poses a key
challenge: ensuring reliable model performance on downstream user data without
access to ground truth labels for direct validation. We propose the suitability
filter, a novel framework designed to detect performance deterioration by
utilizing suitability signals -- model output features that are sensitive to
covariate shifts and indicative of potential prediction errors. The suitability
filter evaluates whether classifier accuracy on unlabeled user data shows
significant degradation compared to the accuracy measured on the labeled test
dataset. Specifically, it ensures that this degradation does not exceed a
pre-specified margin, which represents the maximum acceptable drop in accuracy.
To achieve reliable performance evaluation, we aggregate suitability signals
for both test and user data and compare these empirical distributions using
statistical hypothesis testing, thus providing insights into decision
uncertainty. Our modular method adapts to various models and domains. Empirical
evaluations across different classification tasks demonstrate that the
suitability filter reliably detects performance deviations due to covariate
shift. This enables proactive mitigation of potential failures in high-stakes
applications.

</details>


### [616] [Look Within or Look Beyond? A Theoretical Comparison Between Parameter-Efficient and Full Fine-Tuning](https://arxiv.org/pdf/2505.22355)
*Yongkang Liu, Xingle Xu, Ercong Nie, Zijing Wang, Shi Feng, Daling Wang, Qian Li, Hinrich Schütze*

Main category: cs.LG

TL;DR: PEFT methods are resource-efficient but underperform FFT in complex tasks due to limited parameter space, as shown by theoretical and experimental analysis.


<details>
  <summary>Details</summary>
Motivation: To compare PEFT and FFT in terms of representational capacity and robustness, highlighting PEFT's limitations in complex tasks.

Method: Theoretical analysis of PEFT as a subset of FFT, with experiments on 15 datasets and 11 adversarial test sets.

Result: PEFT's constrained parameter space limits representational ability and robustness, validated by empirical results.

Conclusion: PEFT's limitations call for further research beyond its current scope, with code available for reproducibility.

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods achieve performance comparable
to Full Fine-Tuning (FFT) while requiring significantly fewer computing
resources, making it the go-to choice for researchers. We find that although
PEFT can achieve competitive results on some benchmarks, its performance falls
short of FFT in complex tasks, such as reasoning and instruction-based
fine-tuning. In this paper, we compare the characteristics of PEFT and FFT in
terms of representational capacity and robustness based on optimization theory.
We theoretically demonstrate that PEFT is a strict subset of FFT. By providing
theoretical upper bounds for PEFT, we show that the limited parameter space
constrains the model's representational ability, making it more susceptible to
perturbations. Experiments on 15 datasets encompassing classification,
generation, reasoning, instruction fine-tuning tasks and 11 adversarial test
sets validate our theories. We hope that these results spark further research
beyond the realms of well established PEFT. The source code is in the anonymous
Github repository\footnote{https://github.com/misonsky/PEFTEval}.

</details>


### [617] [Budget-Adaptive Adapter Tuning in Orthogonal Subspaces for Continual Learning in LLMs](https://arxiv.org/pdf/2505.22358)
*Zhiyi Wan, Wanrou Du, Liang Li, Miao Pan, Xiaoqi Qin*

Main category: cs.LG

TL;DR: OA-Adapter is a parameter-efficient method for continual learning in LLMs, combining dynamic budget adaptation and orthogonal subspace learning in a single stage to mitigate catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: Address limitations of fixed budget allocation and misalignment in multi-stage approaches for continual learning in LLMs.

Method: Proposes OA-Adapter with dynamic bottleneck dimension adaptation and orthogonal constraints for task subspaces.

Result: Outperforms state-of-the-art methods, achieving higher accuracy with 58.5% fewer parameters.

Conclusion: OA-Adapter effectively unifies budget adaptation and subspace learning, improving continual learning performance.

Abstract: Large language models (LLMs) often suffer from catastrophic forgetting in
continual learning (CL) scenarios, where performance on previously learned
tasks degrades severely while training on sequentially arriving tasks. Although
pioneering CL approaches using orthogonal subspaces can mitigate task
interference, they typically employ fixed budget allocation, neglecting the
varying complexity across tasks and layers. Besides, recent budget-adaptive
tuning methods for LLMs often adopt multi-stage paradigms that decouple
optimization and budget allocation. Such decoupling results in potential
misalignment, which hinders those approaches' practical application in CL
scenarios. To address these limitations, we propose OA-Adapter, a novel
parameter-efficient approach for continual learning in LLMs that unifies
dynamic budget adaptation with orthogonal subspace learning in a single
end-to-end training stage. Specifically, OA-Adapter introduces a dynamic
bottleneck dimension adaptation mechanism that simultaneously allocates an
efficient parameter budget and optimizes task objectives without misalignment.
To effectively preserve previously acquired knowledge while coordinating with
the dynamic budget allocation, orthogonal constraints are applied specifically
between the parameter subspace of the current task and the dynamically
allocated parameter subspaces of historical tasks. Experimental results on
continual learning benchmarks demonstrate that OA-Adapter outperforms
state-of-the-art methods in both accuracy and parameter efficiency, achieving
higher average accuracy while using 58.5% fewer parameters on the standard CL
benchmark.

</details>


### [618] [Multiclass Loss Geometry Matters for Generalization of Gradient Descent in Separable Classification](https://arxiv.org/pdf/2505.22359)
*Matan Schliserman, Tomer Koren*

Main category: cs.LG

TL;DR: The paper analyzes generalization performance of unregularized gradient methods for multiclass linear classification, showing risk bounds influenced by loss template geometry rather than the loss function itself.


<details>
  <summary>Details</summary>
Motivation: To extend understanding of gradient methods beyond binary classification to multiclass settings, focusing on how loss template geometry affects risk bounds.

Method: Uses Gradient Descent for loss functions decaying to zero, analyzing risk bounds based on the smoothness of the loss template with respect to p-norms.

Result: Risk bounds vary with p-norms: logarithmic dependence on k for p=∞ and linear scaling for p=2, with a proven lower bound for the latter.

Conclusion: The geometry of the loss template, not the loss function, critically influences risk bounds in multiclass classification, with p-norms playing a key role.

Abstract: We study the generalization performance of unregularized gradient methods for
separable linear classification. While previous work mostly deal with the
binary case, we focus on the multiclass setting with $k$ classes and establish
novel population risk bounds for Gradient Descent for loss functions that decay
to zero. In this setting, we show risk bounds that reveal that convergence
rates are crucially influenced by the geometry of the loss template, as
formalized by Wang and Scott (2024), rather than of the loss function itself.
Particularly, we establish risk upper bounds that holds for any decay rate of
the loss whose template is smooth with respect to the $p$-norm. In the case of
exponentially decaying losses, our results indicates a contrast between the
$p=\infty$ case, where the risk exhibits a logarithmic dependence on $k$, and
$p=2$ where the risk scales linearly with $k$. To establish this separation
formally, we also prove a lower bound in the latter scenario, demonstrating
that the polynomial dependence on $k$ is unavoidable. Central to our analysis
is a novel bound on the Rademacher complexity of low-noise vector-valued linear
predictors with a loss template smooth w.r.t.~general $p$-norms.

</details>


### [619] [SplitLoRA: Balancing Stability and Plasticity in Continual Learning Through Gradient Space Splitting](https://arxiv.org/pdf/2505.22370)
*Haomiao Qiu, Miao Zhang, Ziyue Qiao, Weili Guan, Min Zhang, Liqiang Nie*

Main category: cs.LG

TL;DR: The paper proposes SplitLoRA, a novel continual learning method using Low-Rank Adaptation to balance stability and plasticity by optimally partitioning gradient space.


<details>
  <summary>Details</summary>
Motivation: Existing gradient projection methods in continual learning struggle to balance stability (retaining old knowledge) and plasticity (learning new tasks).

Method: The paper introduces SplitLoRA, which theoretically analyzes gradient subspace partitioning and derives an optimal partition for stability and plasticity.

Result: Experiments show SplitLoRA achieves state-of-the-art performance on multiple datasets.

Conclusion: SplitLoRA effectively balances stability and plasticity in continual learning, outperforming existing methods.

Abstract: Continual Learning requires a model to learn multiple tasks in sequence while
maintaining both stability:preserving knowledge from previously learned tasks,
and plasticity:effectively learning new tasks. Gradient projection has emerged
as an effective and popular paradigm in CL, where it partitions the gradient
space of previously learned tasks into two orthogonal subspaces: a primary
subspace and a minor subspace. New tasks are learned effectively within the
minor subspace, thereby reducing interference with previously acquired
knowledge. However, existing Gradient Projection methods struggle to achieve an
optimal balance between plasticity and stability, as it is hard to
appropriately partition the gradient space. In this work, we consider a
continual learning paradigm based on Low-Rank Adaptation, which has gained
considerable attention due to its efficiency and wide applicability, and
propose a novel approach for continual learning, called SplitLoRA. We first
provide a theoretical analysis of how subspace partitioning affects model
stability and plasticity. Informed by this analysis, we then introduce an
effective method that derives the optimal partition of the gradient space for
previously learned tasks. This approach effectively balances stability and
plasticity in continual learning. Experimental results on multiple datasets
demonstrate that the proposed method achieves state-of-the-art performance.

</details>


### [620] [Continuum-armed Bandit Optimization with Batch Pairwise Comparison Oracles](https://arxiv.org/pdf/2505.22361)
*Xiangyu Chang, Xi Chen, Yining Wang, Zhiyi Zeng*

Main category: cs.LG

TL;DR: The paper addresses a bandit optimization problem using a pairwise comparison oracle for strongly concave functions, with applications in pricing, inventory, and revenue management. It introduces novel techniques to handle biased estimates and achieves near-optimal regret bounds.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by real-world challenges in operations management, such as joint pricing and inventory replenishment, where traditional oracles fail due to biased estimates and the need for adaptive stopping times.

Method: The authors use discretization and local polynomial approximation to link the problem to linear bandits, then employ a tournament successive elimination technique and an interactive batched LinUCB algorithm.

Result: The proposed method achieves regret bounds optimal up to poly-logarithmic factors and improves state-of-the-art results in operations management applications.

Conclusion: The framework effectively addresses the challenges of biased estimates and adaptive stopping, offering practical improvements for operations management problems.

Abstract: This paper studies a bandit optimization problem where the goal is to
maximize a function $f(x)$ over $T$ periods for some unknown strongly concave
function $f$. We consider a new pairwise comparison oracle, where the
decision-maker chooses a pair of actions $(x, x')$ for a consecutive number of
periods and then obtains an estimate of $f(x)-f(x')$. We show that such a
pairwise comparison oracle finds important applications to joint pricing and
inventory replenishment problems and network revenue management. The challenge
in this bandit optimization is twofold. First, the decision-maker not only
needs to determine a pair of actions $(x, x')$ but also a stopping time $n$
(i.e., the number of queries based on $(x, x')$). Second, motivated by our
inventory application, the estimate of the difference $f(x)-f(x')$ is biased,
which is different from existing oracles in stochastic optimization literature.
To address these challenges, we first introduce a discretization technique and
local polynomial approximation to relate this problem to linear bandits. Then
we developed a tournament successive elimination technique to localize the
discretized cell and run an interactive batched version of LinUCB algorithm on
cells. We establish regret bounds that are optimal up to poly-logarithmic
factors. Furthermore, we apply our proposed algorithm and analytical framework
to the two operations management problems and obtain results that improve
state-of-the-art results in the existing literature.

</details>


### [621] [Directed Homophily-Aware Graph Neural Network](https://arxiv.org/pdf/2505.22362)
*Aihu Zhang, Jiaxing Xu, Mengcheng Lan, Shili Xiang, Yiping Ke*

Main category: cs.LG

TL;DR: DHGNN is a novel GNN framework addressing heterophilic neighborhoods and directional graph challenges, outperforming baselines by up to 15.07% in link prediction.


<details>
  <summary>Details</summary>
Motivation: Most GNNs struggle with heterophilic neighborhoods and ignore directional graph structures, leading to suboptimal performance.

Method: DHGNN uses a resettable gating mechanism for adaptive message modulation and a noise-tolerant fusion module for directional integration.

Result: DHGNN outperforms state-of-the-art methods in node classification and link prediction, with up to 15.07% improvement.

Conclusion: DHGNN effectively addresses GNN limitations, providing insights into message-passing behavior on complex graphs.

Abstract: Graph Neural Networks (GNNs) have achieved significant success in various
learning tasks on graph-structured data. Nevertheless, most GNNs struggle to
generalize to heterophilic neighborhoods. Additionally, many GNNs ignore the
directional nature of real-world graphs, resulting in suboptimal performance on
directed graphs with asymmetric structures. In this work, we propose Directed
Homophily-aware Graph Neural Network (DHGNN), a novel framework that addresses
these limitations by incorporating homophily-aware and direction-sensitive
components. DHGNN employs a resettable gating mechanism to adaptively modulate
message contributions based on homophily levels and informativeness, and a
structure-aware noise-tolerant fusion module to effectively integrate node
representations from the original and reverse directions. Extensive experiments
on both homophilic and heterophilic directed graph datasets demonstrate that
DHGNN outperforms state-of-the-art methods in node classification and link
prediction. In particular, DHGNN improves over the best baseline by up to
15.07% in link prediction. Our analysis further shows that the gating mechanism
captures directional homophily gaps and fluctuating homophily across layers,
providing deeper insights into message-passing behavior on complex graph
structures.

</details>


### [622] [A Divide-and-Conquer Approach for Modeling Arrival Times in Business Process Simulation](https://arxiv.org/pdf/2505.22381)
*Lukas Kirchdorfer, Konrad Özdemir, Stjepan Kusenic, Han van der Aa, Heiner Stuckenschmidt*

Main category: cs.LG

TL;DR: AT-KDE improves BPS accuracy by dynamically modeling case-arrival times, outperforming static methods.


<details>
  <summary>Details</summary>
Motivation: Existing BPS methods use oversimplified static distributions for case-arrival times, lacking accuracy in dynamic environments.

Method: Proposes Auto Time Kernel Density Estimation (AT-KDE) to model arrival times with global dynamics, day-of-week variations, and intraday changes.

Result: AT-KDE outperforms existing methods in accuracy and robustness across 20 diverse processes, with efficient execution.

Conclusion: AT-KDE enhances BPS reliability by addressing dynamic case-arrival complexities, offering a scalable and precise solution.

Abstract: Business Process Simulation (BPS) is a critical tool for analyzing and
improving organizational processes by estimating the impact of process changes.
A key component of BPS is the case-arrival model, which determines the pattern
of new case entries into a process. Although accurate case-arrival modeling is
essential for reliable simulations, as it influences waiting and overall cycle
times, existing approaches often rely on oversimplified static distributions of
inter-arrival times. These approaches fail to capture the dynamic and temporal
complexities inherent in organizational environments, leading to less accurate
and reliable outcomes. To address this limitation, we propose Auto Time Kernel
Density Estimation (AT-KDE), a divide-and-conquer approach that models arrival
times of processes by incorporating global dynamics, day-of-week variations,
and intraday distributional changes, ensuring both precision and scalability.
Experiments conducted across 20 diverse processes demonstrate that AT-KDE is
far more accurate and robust than existing approaches while maintaining
sensible execution time efficiency.

</details>


### [623] [Train with Perturbation, Infer after Merging: A Two-Stage Framework for Continual Learning](https://arxiv.org/pdf/2505.22389)
*Haomiao Qiu, Miao Zhang, Ziyue Qiao, Liqiang Nie*

Main category: cs.LG

TL;DR: Perturb-and-Merge (P&M) integrates model merging into continual learning to mitigate forgetting by combining previous and new task models, optimizing merging coefficients, and using efficient regularization.


<details>
  <summary>Details</summary>
Motivation: Existing CL methods suffer from catastrophic forgetting by relying only on recent task parameters. P&M aims to address this by leveraging model merging techniques.

Method: P&M constructs a new model via convex combination of previous and task-specific models, optimizes merging coefficients, and uses Hessian-based regularization approximated by stochastic perturbations.

Result: P&M achieves state-of-the-art performance on CL benchmarks, with additional efficiency gains when combined with LoRA.

Conclusion: P&M effectively mitigates forgetting in CL through model merging and efficient regularization, outperforming existing methods.

Abstract: Continual Learning (CL) aims to enable models to continuously acquire new
knowledge from a sequence of tasks with avoiding the forgetting of learned
information. However, existing CL methods only rely on the parameters of the
most recent task for inference, which makes them susceptible to catastrophic
forgetting. Inspired by the recent success of model merging techniques, we
propose \textbf{Perturb-and-Merge (P\&M)}, a novel continual learning framework
that integrates model merging into the CL paradigm to mitigate forgetting.
Specifically, after training on each task, P\&M constructs a new model by
forming a convex combination of the previous model and the newly trained
task-specific model. Through theoretical analysis, we minimize the total loss
increase across all tasks and derive an analytical solution for the optimal
merging coefficient. To further improve the performance of the merged model, we
observe that the degradation introduced during merging can be alleviated by a
regularization term composed of the task vector and the Hessian matrix of the
loss function. Interestingly, we show that this term can be efficiently
approximated using second-order symmetric finite differences, and a stochastic
perturbation strategy along the task vector direction is accordingly devised
which incurs no additional forward or backward passes while providing an
effective approximation of the regularization term. Finally, we combine P\&M
with LoRA, a parameter-efficient fine-tuning method, to reduce memory overhead.
Our proposed approach achieves state-of-the-art performance on several
continual learning benchmark datasets.

</details>


### [624] [Physics-Informed Distillation of Diffusion Models for PDE-Constrained Generation](https://arxiv.org/pdf/2505.22391)
*Yi Zhang, Difan Zou*

Main category: cs.LG

TL;DR: The paper introduces PIDDM, a post-hoc distillation method to enforce PDE constraints in diffusion models, improving accuracy and efficiency in generative modeling of physical systems.


<details>
  <summary>Details</summary>
Motivation: Diffusion models struggle to enforce PDE constraints on clean data due to Jensen's Gap, leading to a trade-off between constraint satisfaction and generative accuracy.

Method: Proposes PIDDM, a post-hoc distillation approach that enforces PDE constraints after the diffusion process, avoiding direct injection during noisy steps.

Result: PIDDM outperforms baselines like PIDM and DiffusionPDE in PDE satisfaction and supports forward/inverse problem-solving with less computational cost.

Conclusion: PIDDM offers an efficient way to integrate physical constraints into diffusion models, balancing accuracy and constraint adherence.

Abstract: Modeling physical systems in a generative manner offers several advantages,
including the ability to handle partial observations, generate diverse
solutions, and address both forward and inverse problems. Recently, diffusion
models have gained increasing attention in the modeling of physical systems,
particularly those governed by partial differential equations (PDEs). However,
diffusion models only access noisy data $\boldsymbol{x}_t$ at intermediate
steps, making it infeasible to directly enforce constraints on the clean sample
$\boldsymbol{x}_0$ at each noisy level. As a workaround, constraints are
typically applied to the expectation of clean samples
$\mathbb{E}[\boldsymbol{x}_0|\boldsymbol{x}_t]$, which is estimated using the
learned score network. However, imposing PDE constraints on the expectation
does not strictly represent the one on the true clean data, known as Jensen's
Gap. This gap creates a trade-off: enforcing PDE constraints may come at the
cost of reduced accuracy in generative modeling. To address this, we propose a
simple yet effective post-hoc distillation approach, where PDE constraints are
not injected directly into the diffusion process, but instead enforced during a
post-hoc distillation stage. We term our method as Physics-Informed
Distillation of Diffusion Models (PIDDM). This distillation not only
facilitates single-step generation with improved PDE satisfaction, but also
support both forward and inverse problem solving and reconstruction from
randomly partial observation. Extensive experiments across various PDE
benchmarks demonstrate that PIDDM significantly improves PDE satisfaction over
several recent and competitive baselines, such as PIDM, DiffusionPDE, and
ECI-sampling, with less computation overhead. Our approach can shed light on
more efficient and effective strategies for incorporating physical constraints
into diffusion models.

</details>


### [625] [Mitigating Overthinking in Large Reasoning Models via Manifold Steering](https://arxiv.org/pdf/2505.22411)
*Yao Huang, Huanran Chen, Shouwei Ruan, Yichi Zhang, Xingxing Wei, Yinpeng Dong*

Main category: cs.LG

TL;DR: The paper addresses overthinking in Large Reasoning Models (LRMs) by identifying a low-dimensional manifold in activation space and proposing Manifold Steering to reduce computational overhead while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: LRMs exhibit overthinking during inference, causing redundant computations. The study aims to mitigate this by understanding and intervening in the model's activation space.

Method: The authors identify a low-dimensional manifold tied to overthinking and propose Manifold Steering, which projects steering directions onto this manifold to reduce noise.

Result: Experiments show up to 71% fewer output tokens with maintained or improved accuracy on math benchmarks, and cross-domain effectiveness in coding and QA tasks.

Conclusion: Manifold Steering effectively reduces overthinking in LRMs, offering computational efficiency without sacrificing performance, with broad applicability.

Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable
capabilities in solving complex tasks such as mathematics and coding. However,
these models frequently exhibit a phenomenon known as overthinking during
inference, characterized by excessive validation loops and redundant
deliberation, leading to substantial computational overheads. In this paper, we
aim to mitigate overthinking by investigating the underlying mechanisms from
the perspective of mechanistic interpretability. We first showcase that the
tendency of overthinking can be effectively captured by a single direction in
the model's activation space and the issue can be eased by intervening the
activations along this direction. However, this efficacy soon reaches a plateau
and even deteriorates as the intervention strength increases. We therefore
systematically explore the activation space and find that the overthinking
phenomenon is actually tied to a low-dimensional manifold, which indicates that
the limited effect stems from the noises introduced by the high-dimensional
steering direction. Based on this insight, we propose Manifold Steering, a
novel approach that elegantly projects the steering direction onto the
low-dimensional activation manifold given the theoretical approximation of the
interference noise. Extensive experiments on DeepSeek-R1 distilled models
validate that our method reduces output tokens by up to 71% while maintaining
and even improving the accuracy on several mathematical benchmarks. Our method
also exhibits robust cross-domain transferability, delivering consistent token
reduction performance in code generation and knowledge-based QA tasks. Code is
available at: https://github.com/Aries-iai/Manifold_Steering.

</details>


### [626] [STaR-Bets: Sequential Target-Recalculating Bets for Tighter Confidence Intervals](https://arxiv.org/pdf/2505.22422)
*Václav Voráček, Francesco Orabona*

Main category: cs.LG

TL;DR: The paper introduces a betting-based algorithm for constructing optimal confidence intervals for bounded random variables, outperforming existing methods and providing finite-time guarantees.


<details>
  <summary>Details</summary>
Motivation: Existing methods for constructing confidence intervals are either sub-optimal or lack finite-time guarantees, especially in fixed-horizon settings. The goal is to bridge this gap.

Method: A betting-based algorithm is proposed, dynamically optimizing the betting strategy at each step, unlike standard methods that use a fixed strategy.

Result: The algorithm empirically outperforms competitors and achieves confidence intervals with optimal width up to a diminishing factor.

Conclusion: The work provides a theoretically sound and empirically superior solution for constructing tight confidence intervals in fixed-horizon settings.

Abstract: The construction of confidence intervals for the mean of a bounded random
variable is a classical problem in statistics with numerous applications in
machine learning and virtually all scientific fields. In particular, obtaining
the tightest possible confidence intervals is vital every time the sampling of
the random variables is expensive. The current state-of-the-art method to
construct confidence intervals is by using betting algorithms. This is a very
successful approach for deriving optimal confidence sequences, even matching
the rate of law of iterated logarithms. However, in the fixed horizon setting,
these approaches are either sub-optimal or based on heuristic solutions with
strong empirical performance but without a finite-time guarantee. Hence, no
betting-based algorithm guaranteeing the optimal
$\mathcal{O}(\sqrt{\frac{\sigma^2\log\frac1\delta}{n}})$ width of the
confidence intervals are known. This work bridges this gap. We propose a
betting-based algorithm to compute confidence intervals that empirically
outperforms the competitors. Our betting strategy uses the optimal strategy in
every step (in a certain sense), whereas the standard betting methods choose a
constant strategy in advance. Leveraging this fact results in strict
improvements even for classical concentration inequalities, such as the ones of
Hoeffding or Bernstein. Moreover, we also prove that the width of our
confidence intervals is optimal up to an $1+o(1)$ factor diminishing with $n$.
The code is available
on~https://github.com/vvoracek/STaR-bets-confidence-interval.

</details>


### [627] [Scaling Reasoning without Attention](https://arxiv.org/pdf/2505.22425)
*Xueliang Zhao, Wei Wu, Lingpeng Kong*

Main category: cs.LG

TL;DR: An attention-free language model (\ourmodel) using state space dual layers and a two-phase fine-tuning strategy outperforms Transformer models in complex reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Address architectural inefficiencies of Transformers and lack of structured fine-tuning for high-difficulty domains.

Method: Uses state space dual (SSD) layers for fixed-memory, constant-time inference and a two-phase curriculum fine-tuning strategy (PromptCoT) for training.

Result: Outperforms comparable Transformer and hybrid models, even surpassing larger models like Gemma3-27B on benchmarks.

Conclusion: State space models are efficient and scalable alternatives to attention-based architectures for high-capacity reasoning.

Abstract: Large language models (LLMs) have made significant advances in complex
reasoning tasks, yet they remain bottlenecked by two core challenges:
architectural inefficiency due to reliance on Transformers, and a lack of
structured fine-tuning for high-difficulty domains. We introduce \ourmodel, an
attention-free language model that addresses both issues through architectural
and data-centric innovations. Built on the state space dual (SSD) layers of
Mamba-2, our model eliminates the need for self-attention and key-value
caching, enabling fixed-memory, constant-time inference. To train it for
complex reasoning, we propose a two-phase curriculum fine-tuning strategy based
on the \textsc{PromptCoT} synthesis paradigm, which generates pedagogically
structured problems via abstract concept selection and rationale-guided
generation. On benchmark evaluations, \ourmodel-7B outperforms strong
Transformer and hybrid models of comparable scale, and even surpasses the much
larger Gemma3-27B by 2.6\% on AIME 24, 0.6\% on AIME 25, and 3.0\% on
Livecodebench. These results highlight the potential of state space models as
efficient and scalable alternatives to attention-based architectures for
high-capacity reasoning.

</details>


### [628] [Data-Driven Antenna Miniaturization: A Knowledge-Based System Integrating Quantum PSO and Predictive Machine Learning Models](https://arxiv.org/pdf/2505.22440)
*Khan Masood Parvez, Sk Md Abidar Rahaman, Ali Shiri Sichani*

Main category: cs.LG

TL;DR: A machine learning-enhanced workflow using QDPSO and ANSYS HFSS accelerates antenna design, achieving a 240x speedup over traditional methods.


<details>
  <summary>Details</summary>
Motivation: The need for automated design frameworks to address antenna miniaturization and performance optimization within tight development cycles.

Method: Integration of Quantum-Behaved Dynamic Particle Swarm Optimization (QDPSO) with ANSYS HFSS simulations and machine learning models (SVM, Random Forest, XGBoost, Stacked ensembles).

Result: QDPSO optimized loop dimensions in 11.53 seconds, achieving a 12.7% reduction in resonance frequency. Machine learning models predicted resonance frequencies in 0.75 seconds with high accuracy.

Conclusion: The framework bridges AI-driven optimization with CAD validation, reducing engineering workloads and enabling scalable, production-ready designs for 6G and IoT.

Abstract: The rapid evolution of wireless technologies necessitates automated design
frameworks to address antenna miniaturization and performance optimization
within constrained development cycles. This study demonstrates a machine
learning enhanced workflow integrating Quantum-Behaved Dynamic Particle Swarm
Optimization (QDPSO) with ANSYS HFSS simulations to accelerate antenna design.
The QDPSO algorithm autonomously optimized loop dimensions in 11.53 seconds,
achieving a resonance frequency of 1.4208 GHz a 12.7 percent reduction compared
to conventional 1.60 GHz designs. Machine learning models (SVM, Random Forest,
XGBoost, and Stacked ensembles) predicted resonance frequencies in 0.75 seconds
using 936 simulation datasets, with stacked models showing superior training
accuracy (R2=0.9825) and SVM demonstrating optimal validation performance
(R2=0.7197). The complete design cycle, encompassing optimization, prediction,
and ANSYS validation, required 12.42 minutes on standard desktop hardware
(Intel i5-8500, 16GB RAM), contrasting sharply with the 50-hour benchmark of
PSADEA-based approaches. This 240 times of acceleration eliminates traditional
trial-and-error methods that often extend beyond seven expert-led days. The
system enables precise specifications of performance targets with automated
generation of fabrication-ready parameters, particularly benefiting compact
consumer devices requiring rapid frequency tuning. By bridging AI-driven
optimization with CAD validation, this framework reduces engineering workloads
while ensuring production-ready designs, establishing a scalable paradigm for
next-generation RF systems in 6G and IoT applications.

</details>


### [629] [SOReL and TOReL: Two Methods for Fully Offline Reinforcement Learning](https://arxiv.org/pdf/2505.22442)
*Mattie Fellows, Clarisse Wibault, Uljad Berdica, Johannes Forkel, Jakob N. Foerster, Michael A. Osborne*

Main category: cs.LG

TL;DR: The paper introduces SOReL and TOReL, two algorithms addressing offline RL's reliance on online tuning and lack of performance bounds, enabling safer and more efficient RL deployment.


<details>
  <summary>Details</summary>
Motivation: Sample efficiency and safety in RL are hindered by the need for extensive online interactions and unreliable initial performance estimates.

Method: SOReL uses Bayesian inference on offline data to estimate online performance, while TOReL extends offline hyperparameter tuning to general RL methods.

Result: SOReL accurately estimates regret, and TOReL matches online tuning performance using only offline data.

Conclusion: SOReL and TOReL advance offline RL towards real-world applicability by ensuring safety and efficiency without online tuning.

Abstract: Sample efficiency remains a major obstacle for real world adoption of
reinforcement learning (RL): success has been limited to settings where
simulators provide access to essentially unlimited environment interactions,
which in reality are typically costly or dangerous to obtain. Offline RL in
principle offers a solution by exploiting offline data to learn a near-optimal
policy before deployment. In practice, however, current offline RL methods rely
on extensive online interactions for hyperparameter tuning, and have no
reliable bound on their initial online performance. To address these two
issues, we introduce two algorithms. Firstly, SOReL: an algorithm for safe
offline reinforcement learning. Using only offline data, our Bayesian approach
infers a posterior over environment dynamics to obtain a reliable estimate of
the online performance via the posterior predictive uncertainty. Crucially, all
hyperparameters are also tuned fully offline. Secondly, we introduce TOReL: a
tuning for offline reinforcement learning algorithm that extends our
information rate based offline hyperparameter tuning methods to general offline
RL approaches. Our empirical evaluation confirms SOReL's ability to accurately
estimate regret in the Bayesian setting whilst TOReL's offline hyperparameter
tuning achieves competitive performance with the best online hyperparameter
tuning methods using only offline data. Thus, SOReL and TOReL make a
significant step towards safe and reliable offline RL, unlocking the potential
for RL in the real world. Our implementations are publicly available:
https://github.com/CWibault/sorel\_torel.

</details>


### [630] [Position: All Current Generative Fidelity and Diversity Metrics are Flawed](https://arxiv.org/pdf/2505.22450)
*Ossi Räisä, Boris van Breugel, Mihaela van der Schaar*

Main category: cs.LG

TL;DR: Current synthetic data metrics are flawed, hindering practical use. The paper proposes desiderata and sanity checks to evaluate metrics, urging more focus on metric development.


<details>
  <summary>Details</summary>
Motivation: The practical application of generative modeling is limited by unreliable metrics, with existing ones failing in robustness and clarity.

Method: Proposes desiderata for synthetic data metrics and a suite of sanity checks to detect failure modes.

Result: All current generative fidelity and diversity metrics are flawed, limiting synthetic data utility.

Conclusion: Urges the community to prioritize metric development over models and provides guidelines for metric usage.

Abstract: Any method's development and practical application is limited by our ability
to measure its reliability. The popularity of generative modeling emphasizes
the importance of good synthetic data metrics. Unfortunately, previous works
have found many failure cases in current metrics, for example lack of outlier
robustness and unclear lower and upper bounds. We propose a list of desiderata
for synthetic data metrics, and a suite of sanity checks: carefully chosen
simple experiments that aim to detect specific and known generative modeling
failure modes. Based on these desiderata and the results of our checks, we
arrive at our position: all current generative fidelity and diversity metrics
are flawed. This significantly hinders practical use of synthetic data. Our aim
is to convince the research community to spend more effort in developing
metrics, instead of models. Additionally, through analyzing how current metrics
fail, we provide practitioners with guidelines on how these metrics should
(not) be used.

</details>


### [631] [Pure Exploration with Infinite Answers](https://arxiv.org/pdf/2505.22473)
*Riccardo Poiani, Martino Bernasconi, Andrea Celli*

Main category: cs.LG

TL;DR: The paper explores pure exploration problems with infinite correct answers, like regression in bandit settings. It identifies limitations of existing methods (e.g., Sticky Track-and-Stop) and proposes a new framework, Sticky-Sequence Track-and-Stop, which is asymptotically optimal and generalizes prior methods.


<details>
  <summary>Details</summary>
Motivation: To address the gap in existing methods for pure exploration problems with infinite correct answers, which fail to achieve asymptotic optimality.

Method: Derives an instance-dependent lower bound, analyzes existing methods, and introduces the Sticky-Sequence Track-and-Stop framework.

Result: The new framework is asymptotically optimal and generalizes prior methods, also identifying cases where existing methods remain optimal.

Conclusion: Sticky-Sequence Track-and-Stop provides a versatile and optimal solution for pure exploration problems with infinite correct answers, while also contextualizing the effectiveness of prior methods.

Abstract: We study pure exploration problems where the set of correct answers is
possibly infinite, e.g., the regression of any continuous function of the means
of the bandit. We derive an instance-dependent lower bound for these problems.
By analyzing it, we discuss why existing methods (i.e., Sticky Track-and-Stop)
for finite answer problems fail at being asymptotically optimal in this more
general setting. Finally, we present a framework, Sticky-Sequence
Track-and-Stop, which generalizes both Track-and-Stop and Sticky
Track-and-Stop, and that enjoys asymptotic optimality. Due to its generality,
our analysis also highlights special cases where existing methods enjoy
optimality.

</details>


### [632] [Forecasting Multivariate Urban Data via Decomposition and Spatio-Temporal Graph Analysis](https://arxiv.org/pdf/2505.22474)
*Amirhossein Sohrabbeig, Omid Ardakanian, Petr Musilek*

Main category: cs.LG

TL;DR: A novel GNN-based model for multivariate urban data forecasting, capturing spatial dependencies and improving accuracy via decomposition preprocessing.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of forecasting interdependent urban metrics (weather, pollution, energy demand) due to their complex relationships.

Method: Uses Graph Neural Networks (GNNs) with decomposition-based preprocessing (trend, seasonal, residual) to model spatial dependencies.

Result: Demonstrates effectiveness on real-world datasets (electricity, weather, carbon intensity, pollution), improving forecasting performance.

Conclusion: The model shows potential for optimizing smart infrastructure, aiding energy-efficient urban development and public well-being.

Abstract: The forecasting of multivariate urban data presents a complex challenge due
to the intricate dependencies between various urban metrics such as weather,
air pollution, carbon intensity, and energy demand. This paper introduces a
novel multivariate time-series forecasting model that utilizes advanced Graph
Neural Networks (GNNs) to capture spatial dependencies among different
time-series variables. The proposed model incorporates a decomposition-based
preprocessing step, isolating trend, seasonal, and residual components to
enhance the accuracy and interpretability of forecasts. By leveraging the
dynamic capabilities of GNNs, the model effectively captures interdependencies
and improves the forecasting performance. Extensive experiments on real-world
datasets, including electricity usage, weather metrics, carbon intensity, and
air pollution data, demonstrate the effectiveness of the proposed approach
across various forecasting scenarios. The results highlight the potential of
the model to optimize smart infrastructure systems, contributing to
energy-efficient urban development and enhanced public well-being.

</details>


### [633] [A Closer Look at Multimodal Representation Collapse](https://arxiv.org/pdf/2505.22483)
*Abhra Chaudhuri, Anjan Dutta, Tu Bui, Serban Georgescu*

Main category: cs.LG

TL;DR: Modality collapse occurs when models ignore some modalities in multimodal fusion due to noisy feature entanglement. Cross-modal knowledge distillation helps disentangle these features, and a proposed algorithm prevents collapse via basis reallocation.


<details>
  <summary>Details</summary>
Motivation: To understand and address modality collapse, where models ignore certain modalities in multimodal fusion, limiting performance.

Method: Analyze modality collapse, propose cross-modal knowledge distillation to disentangle features, and introduce an algorithm for basis reallocation.

Result: Theoretical and experimental validation shows the proposed method prevents modality collapse and handles missing modalities effectively.

Conclusion: The study provides insights into modality collapse and offers a practical solution to mitigate it, improving multimodal fusion performance.

Abstract: We aim to develop a fundamental understanding of modality collapse, a
recently observed empirical phenomenon wherein models trained for multimodal
fusion tend to rely only on a subset of the modalities, ignoring the rest. We
show that modality collapse happens when noisy features from one modality are
entangled, via a shared set of neurons in the fusion head, with predictive
features from another, effectively masking out positive contributions from the
predictive features of the former modality and leading to its collapse. We
further prove that cross-modal knowledge distillation implicitly disentangles
such representations by freeing up rank bottlenecks in the student encoder,
denoising the fusion-head outputs without negatively impacting the predictive
features from either modality. Based on the above findings, we propose an
algorithm that prevents modality collapse through explicit basis reallocation,
with applications in dealing with missing modalities. Extensive experiments on
multiple multimodal benchmarks validate our theoretical claims. Project page:
https://abhrac.github.io/mmcollapse/.

</details>


### [634] [Non-Asymptotic Analysis of (Sticky) Track-and-Stop](https://arxiv.org/pdf/2505.22475)
*Riccardo Poiani, Martino Bernasconi, Andrea Celli*

Main category: cs.LG

TL;DR: The paper provides non-asymptotic guarantees for the Track-and-Stop and Sticky Track-and-Stop algorithms in pure exploration problems, extending their known asymptotic optimality.


<details>
  <summary>Details</summary>
Motivation: Existing algorithms like Track-and-Stop and Sticky Track-and-Stop are asymptotically optimal for pure exploration problems, but their non-asymptotic performance remains unstudied.

Method: The work analyzes the non-asymptotic sample complexity of Track-and-Stop and Sticky Track-and-Stop algorithms.

Result: Non-asymptotic guarantees are derived for both algorithms, filling a gap in their theoretical understanding.

Conclusion: The study extends the theoretical foundation of these algorithms by providing non-asymptotic performance bounds.

Abstract: In pure exploration problems, a statistician sequentially collects
information to answer a question about some stochastic and unknown environment.
The probability of returning a wrong answer should not exceed a maximum risk
parameter $\delta$ and good algorithms make as few queries to the environment
as possible. The Track-and-Stop algorithm is a pioneering method to solve these
problems. Specifically, it is well-known that it enjoys asymptotic optimality
sample complexity guarantees for $\delta\to 0$ whenever the map from the
environment to its correct answers is single-valued (e.g., best-arm
identification with a unique optimal arm). The Sticky Track-and-Stop algorithm
extends these results to settings where, for each environment, there might
exist multiple correct answers (e.g., $\epsilon$-optimal arm identification).
Although both methods are optimal in the asymptotic regime, their
non-asymptotic guarantees remain unknown. In this work, we fill this gap and
provide non-asymptotic guarantees for both algorithms.

</details>


### [635] [On the Surprising Effectiveness of Large Learning Rates under Standard Width Scaling](https://arxiv.org/pdf/2505.22491)
*Moritz Haas, Sebastian Bordt, Ulrike von Luxburg, Leena Chennuru Vankadara*

Main category: cs.LG

TL;DR: The paper resolves the discrepancy between theoretical predictions and empirical observations in neural network training under cross-entropy loss, identifying a controlled divergence regime that enables stable training with large learning rates.


<details>
  <summary>Details</summary>
Motivation: To understand why standard parameterization (SP) works well in practice despite theoretical predictions of instability or vanishing feature learning under large or stable learning rates, respectively.

Method: The study analyzes training dynamics, focusing on the role of loss functions (cross-entropy vs. MSE) and validates findings across optimizers, architectures, and data modalities.

Result: Under cross-entropy loss, a controlled divergence regime emerges, allowing stable training with large learning rates and persistent feature evolution. This regime is absent under MSE loss.

Conclusion: The controlled divergence regime under cross-entropy loss explains the practical success of SP, and width-scaling considerations help predict optimal learning rates.

Abstract: The dominant paradigm for training large-scale vision and language models is
He initialization and a single global learning rate (\textit{standard
parameterization}, SP). Despite its practical success, standard parametrization
remains poorly understood from a theoretical perspective: Existing
infinite-width theory would predict instability under large learning rates and
vanishing feature learning under stable learning rates. However, empirically
optimal learning rates consistently decay much slower than theoretically
predicted. By carefully studying neural network training dynamics, we
demonstrate that this discrepancy is not fully explained by finite-width
phenomena such as catapult effects or a lack of alignment between weights and
incoming activations. We instead show that the apparent contradiction can be
fundamentally resolved by taking the loss function into account: In contrast to
Mean Squared Error (MSE) loss, we prove that under cross-entropy (CE) loss, an
intermediate \textit{controlled divergence} regime emerges, where logits
diverge but loss, gradients, and activations remain stable. Stable training
under large learning rates enables persistent feature evolution at scale in all
hidden layers, which is crucial for the practical success of SP. In experiments
across optimizers (SGD, Adam), architectures (MLPs, GPT) and data modalities
(vision, language), we validate that neural networks operate in this controlled
divergence regime under CE loss but not under MSE loss. Our empirical evidence
suggests that width-scaling considerations are surprisingly useful for
predicting empirically optimal learning rate exponents. Finally, our analysis
clarifies the effectiveness and limitations of recently proposed layerwise
learning rate scalings for standard initialization.

</details>


### [636] [Understanding Adversarial Training with Energy-based Models](https://arxiv.org/pdf/2505.22486)
*Mujtaba Hussain Mirza, Maria Rosaria Briglia, Filippo Bartolucci, Senad Beadini, Giuseppe Lisanti, Iacopo Masi*

Main category: cs.LG

TL;DR: The paper uses Energy-based Models (EBMs) to analyze adversarial training (AT) in classifiers, focusing on Catastrophic Overfitting (CO) and Robust Overfitting (RO). It introduces a Delta Energy Regularizer (DER) to mitigate these issues and explores the generative capabilities of robust classifiers, proposing improvements for better sample diversity and quality.


<details>
  <summary>Details</summary>
Motivation: To understand adversarial training in classifiers through an energy lens and analyze the generative potential of robust classifiers, addressing issues like CO and RO.

Method: Analyzes energy dynamics of adversarial and natural samples, proposes DER to smoothen energy landscapes, and improves generative capabilities using local PCA and energy-based guidance.

Result: DER effectively mitigates CO and RO. Robust classifiers achieve competitive IS and FID scores in generative tasks without explicit generative training.

Conclusion: The energy perspective provides insights into AT, and DER is a practical solution for overfitting. Robust classifiers show promise in generative tasks with proposed enhancements.

Abstract: We aim at using Energy-based Model (EBM) framework to better understand
adversarial training (AT) in classifiers, and additionally to analyze the
intrinsic generative capabilities of robust classifiers. By viewing standard
classifiers through an energy lens, we begin by analyzing how the energies of
adversarial examples, generated by various attacks, differ from those of the
natural samples. The central focus of our work is to understand the critical
phenomena of Catastrophic Overfitting (CO) and Robust Overfitting (RO) in AT
from an energy perspective. We analyze the impact of existing AT approaches on
the energy of samples during training and observe that the behavior of the
``delta energy' -- change in energy between original sample and its adversarial
counterpart -- diverges significantly when CO or RO occurs. After a thorough
analysis of these energy dynamics and their relationship with overfitting, we
propose a novel regularizer, the Delta Energy Regularizer (DER), designed to
smoothen the energy landscape during training. We demonstrate that DER is
effective in mitigating both CO and RO across multiple benchmarks. We further
show that robust classifiers, when being used as generative models, have limits
in handling trade-off between image quality and variability. We propose an
improved technique based on a local class-wise principal component analysis
(PCA) and energy-based guidance for better class-specific initialization and
adaptive stopping, enhancing sample diversity and generation quality.
Considering that we do not explicitly train for generative modeling, we achieve
a competitive Inception Score (IS) and Fr\'echet inception distance (FID)
compared to hybrid discriminative-generative models.

</details>


### [637] [The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models](https://arxiv.org/pdf/2505.22617)
*Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, Zhiyuan Liu, Hao Peng, Lei Bai, Wanli Ouyang, Yu Cheng, Bowen Zhou, Ning Ding*

Main category: cs.LG

TL;DR: The paper addresses the collapse of policy entropy in RL for reasoning with LLMs, proposing methods to manage entropy for better exploration and performance.


<details>
  <summary>Details</summary>
Motivation: The collapse of policy entropy in RL hinders exploration and performance, necessitating entropy management for scalable RL.

Method: The authors analyze entropy dynamics theoretically and empirically, proposing Clip-Cov and KL-Cov to restrict updates of high-covariance tokens.

Result: Empirical results show the proposed methods prevent entropy collapse and improve downstream performance.

Conclusion: Managing entropy through targeted interventions enhances exploration and performance in RL for reasoning tasks.

Abstract: This paper aims to overcome a major obstacle in scaling RL for reasoning with
LLMs, namely the collapse of policy entropy. Such phenomenon is consistently
observed across vast RL runs without entropy intervention, where the policy
entropy dropped sharply at the early training stage, this diminished
exploratory ability is always accompanied with the saturation of policy
performance. In practice, we establish a transformation equation R=-a*e^H+b
between entropy H and downstream performance R. This empirical law strongly
indicates that, the policy performance is traded from policy entropy, thus
bottlenecked by its exhaustion, and the ceiling is fully predictable H=0,
R=-a+b. Our finding necessitates entropy management for continuous exploration
toward scaling compute for RL. To this end, we investigate entropy dynamics
both theoretically and empirically. Our derivation highlights that, the change
in policy entropy is driven by the covariance between action probability and
the change in logits, which is proportional to its advantage when using Policy
Gradient-like algorithms. Empirical study shows that, the values of covariance
term and entropy differences matched exactly, supporting the theoretical
conclusion. Moreover, the covariance term stays mostly positive throughout
training, further explaining why policy entropy would decrease monotonically.
Through understanding the mechanism behind entropy dynamics, we motivate to
control entropy by restricting the update of high-covariance tokens.
Specifically, we propose two simple yet effective techniques, namely Clip-Cov
and KL-Cov, which clip and apply KL penalty to tokens with high covariances
respectively. Experiments show that these methods encourage exploration, thus
helping policy escape entropy collapse and achieve better downstream
performance.

</details>


### [638] [Demystifying the Paradox of Importance Sampling with an Estimated History-Dependent Behavior Policy in Off-Policy Evaluation](https://arxiv.org/pdf/2505.22492)
*Hongyi Zhou, Josiah P. Hanna, Jin Zhu, Ying Yang, Chengchun Shi*

Main category: cs.LG

TL;DR: The paper explains why history-dependent behavior policy estimation reduces MSE in off-policy evaluation (OPE) by showing it decreases asymptotic variance but increases finite-sample bias.


<details>
  <summary>Details</summary>
Motivation: To understand why history-dependent behavior policy estimation empirically lowers MSE in OPE, even when the true policy is Markovian.

Method: Theoretical analysis of bias-variance decomposition for ordinary importance sampling (IS) estimators, extended to other OPE estimators like sequential IS, doubly robust, and marginalized IS.

Result: History-dependent estimation reduces asymptotic variance but increases finite-sample bias, with variance decreasing as history length grows.

Conclusion: The paradox is resolved: history-dependent behavior policy estimation improves OPE by trading off bias and variance, applicable to various estimators.

Abstract: This paper studies off-policy evaluation (OPE) in reinforcement learning with
a focus on behavior policy estimation for importance sampling. Prior work has
shown empirically that estimating a history-dependent behavior policy can lead
to lower mean squared error (MSE) even when the true behavior policy is
Markovian. However, the question of why the use of history should lower MSE
remains open. In this paper, we theoretically demystify this paradox by
deriving a bias-variance decomposition of the MSE of ordinary importance
sampling (IS) estimators, demonstrating that history-dependent behavior policy
estimation decreases their asymptotic variances while increasing their
finite-sample biases. Additionally, as the estimated behavior policy conditions
on a longer history, we show a consistent decrease in variance. We extend these
findings to a range of other OPE estimators, including the sequential IS
estimator, the doubly robust estimator and the marginalized IS estimator, with
the behavior policy estimated either parametrically or non-parametrically.

</details>


### [639] [ProSpero: Active Learning for Robust Protein Design Beyond Wild-Type Neighborhoods](https://arxiv.org/pdf/2505.22494)
*Michal Kmicikiewicz, Vincent Fortuin, Ewa Szczurek*

Main category: cs.LG

TL;DR: ProSpero is an active learning framework for designing high-fitness, novel protein sequences by integrating a frozen generative model with surrogate updates and biologically-constrained sampling.


<details>
  <summary>Details</summary>
Motivation: Overcoming the challenge of designing biologically plausible and high-fitness protein sequences beyond wild-type neighborhoods, where existing methods often fail.

Method: Combines a frozen pre-trained generative model with a surrogate updated from oracle feedback, using fitness-relevant residue selection and Sequential Monte Carlo sampling.

Result: ProSpero outperforms existing methods, producing high-fitness, novel sequences even with surrogate misspecification.

Conclusion: ProSpero effectively balances exploration and biological plausibility in protein engineering.

Abstract: Designing protein sequences of both high fitness and novelty is a challenging
task in data-efficient protein engineering. Exploration beyond wild-type
neighborhoods often leads to biologically implausible sequences or relies on
surrogate models that lose fidelity in novel regions. Here, we propose
ProSpero, an active learning framework in which a frozen pre-trained generative
model is guided by a surrogate updated from oracle feedback. By integrating
fitness-relevant residue selection with biologically-constrained Sequential
Monte Carlo sampling, our approach enables exploration beyond wild-type
neighborhoods while preserving biological plausibility. We show that our
framework remains effective even when the surrogate is misspecified. ProSpero
consistently outperforms or matches existing methods across diverse protein
engineering tasks, retrieving sequences of both high fitness and novelty.

</details>


### [640] [Geometric GNNs for Charged Particle Tracking at GlueX](https://arxiv.org/pdf/2505.22504)
*Ahmed Hossam Mohammed, Kishansingh Rajput, Simon Taylor, Denis Furletov, Sergey Furletov, Malachi Schram*

Main category: cs.LG

TL;DR: GNNs outperform traditional methods in nuclear physics track finding, offering faster and more efficient particle trajectory reconstruction.


<details>
  <summary>Details</summary>
Motivation: Traditional combinatorial methods for tracking charged particles in nuclear physics experiments scale poorly with increasing data. GNNs provide a scalable and efficient alternative.

Method: The study evaluates GNNs for track finding using simulation and real data from the GlueX experiment, comparing performance with traditional methods.

Result: GNNs achieve higher efficiency and faster inference than traditional methods, with additional speedup from GPU batch processing.

Conclusion: GNNs are a promising solution for particle tracking, balancing efficiency and computational speed, though hardware trade-offs (GPU vs. FPGA) exist.

Abstract: Nuclear physics experiments are aimed at uncovering the fundamental building
blocks of matter. The experiments involve high-energy collisions that produce
complex events with many particle trajectories. Tracking charged particles
resulting from collisions in the presence of a strong magnetic field is
critical to enable the reconstruction of particle trajectories and precise
determination of interactions. It is traditionally achieved through
combinatorial approaches that scale worse than linearly as the number of hits
grows. Since particle hit data naturally form a 3-dimensional point cloud and
can be structured as graphs, Graph Neural Networks (GNNs) emerge as an
intuitive and effective choice for this task. In this study, we evaluate the
GNN model for track finding on the data from the GlueX experiment at Jefferson
Lab. We use simulation data to train the model and test on both simulation and
real GlueX measurements. We demonstrate that GNN-based track finding
outperforms the currently used traditional method at GlueX in terms of
segment-based efficiency at a fixed purity while providing faster inferences.
We show that the GNN model can achieve significant speedup by processing
multiple events in batches, which exploits the parallel computation capability
of Graphical Processing Units (GPUs). Finally, we compare the GNN
implementation on GPU and FPGA and describe the trade-off.

</details>


### [641] [Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents](https://arxiv.org/pdf/2505.22655)
*Michael Kirchhof, Gjergji Kasneci, Enkelejda Kasneci*

Main category: cs.LG

TL;DR: The paper critiques traditional uncertainty quantification in LLMs, proposing three new research directions for interactive settings: underspecification, interactive learning, and expressive output uncertainties.


<details>
  <summary>Details</summary>
Motivation: Traditional uncertainty measures (aleatoric/epistemic) are inadequate for interactive LLM-agent scenarios, necessitating enriched approaches.

Method: Review of literature and proposal of three novel research directions for uncertainty in human-LLM interactions.

Result: Identified contradictions in traditional uncertainty definitions and proposed new methods for interactive settings.

Conclusion: New uncertainty approaches could enhance transparency, trust, and intuitiveness in LLM-agent interactions.

Abstract: Large-language models (LLMs) and chatbot agents are known to provide wrong
outputs at times, and it was recently found that this can never be fully
prevented. Hence, uncertainty quantification plays a crucial role, aiming to
quantify the level of ambiguity in either one overall number or two numbers for
aleatoric and epistemic uncertainty. This position paper argues that this
traditional dichotomy of uncertainties is too limited for the open and
interactive setup that LLM agents operate in when communicating with a user,
and that we need to research avenues that enrich uncertainties in this novel
scenario. We review the literature and find that popular definitions of
aleatoric and epistemic uncertainties directly contradict each other and lose
their meaning in interactive LLM agent settings. Hence, we propose three novel
research directions that focus on uncertainties in such human-computer
interactions: Underspecification uncertainties, for when users do not provide
all information or define the exact task at the first go, interactive learning,
to ask follow-up questions and reduce the uncertainty about the current
context, and output uncertainties, to utilize the rich language and speech
space to express uncertainties as more than mere numbers. We expect that these
new ways of dealing with and communicating uncertainties will lead to LLM agent
interactions that are more transparent, trustworthy, and intuitive.

</details>


### [642] [Evaluating Supervised Learning Models for Fraud Detection: A Comparative Study of Classical and Deep Architectures on Imbalanced Transaction Data](https://arxiv.org/pdf/2505.22521)
*Chao Wang, Chuanhao Nie, Yunbo Liu*

Main category: cs.LG

TL;DR: Comparison of four supervised learning models for fraud detection, highlighting trade-offs between performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Fraud detection is crucial in finance and e-commerce to prevent economic losses, necessitating effective and interpretable models.

Method: Evaluated Logistic Regression, Random Forest, LightGBM, and GRU on an imbalanced transaction dataset, focusing on class-specific metrics.

Result: Ensemble methods (Random Forest, LightGBM) performed best overall, while GRU excelled in recall for fraud class but with lower precision. Logistic Regression provided a reliable baseline.

Conclusion: Model choice should align with risk tolerance and operational needs, balancing performance and interpretability.

Abstract: Fraud detection remains a critical task in high-stakes domains such as
finance and e-commerce, where undetected fraudulent transactions can lead to
significant economic losses. In this study, we systematically compare the
performance of four supervised learning models - Logistic Regression, Random
Forest, Light Gradient Boosting Machine (LightGBM), and a Gated Recurrent Unit
(GRU) network - on a large-scale, highly imbalanced online transaction dataset.
While ensemble methods such as Random Forest and LightGBM demonstrated superior
performance in both overall and class-specific metrics, Logistic Regression
offered a reliable and interpretable baseline. The GRU model showed strong
recall for the minority fraud class, though at the cost of precision,
highlighting a trade-off relevant for real-world deployment. Our evaluation
emphasizes not only weighted averages but also per-class precision, recall, and
F1-scores, providing a nuanced view of each model's effectiveness in detecting
rare but consequential fraudulent activity. The findings underscore the
importance of choosing models based on the specific risk tolerance and
operational needs of fraud detection systems.

</details>


### [643] [Sparsification and Reconstruction from the Perspective of Representation Geometry](https://arxiv.org/pdf/2505.22506)
*Wenjie Sun, Bingzhe Wu, Zhile Yang, Chengke Wu*

Main category: cs.LG

TL;DR: SAEMA explores how sparse encoding organizes representations in language models, linking feature disentanglement and reconstruction performance through stratified structures and optimization interventions.


<details>
  <summary>Details</summary>
Motivation: To understand how sparse encoding organizes activation vectors in language models and its impact on feature disentanglement and reconstruction performance.

Method: Proposes SAEMA, analyzing stratified structures via SSPD matrix variability and defining local/global representations to study inter-feature distinctions.

Result: Sparse encoding merges similar features and adds dimensionality, with global representation separability causally linked to reconstruction performance.

Conclusion: The study highlights the importance of representational geometry in sparsity principles and offers insights for improving interpretable tools like SAEs.

Abstract: Sparse Autoencoders (SAEs) have emerged as a predominant tool in mechanistic
interpretability, aiming to identify interpretable monosemantic features.
However, how does sparse encoding organize the representations of activation
vector from language models? What is the relationship between this
organizational paradigm and feature disentanglement as well as reconstruction
performance? To address these questions, we propose the SAEMA, which validates
the stratified structure of the representation by observing the variability of
the rank of the symmetric semipositive definite (SSPD) matrix corresponding to
the modal tensor unfolded along the latent tensor with the level of noise added
to the residual stream. To systematically investigate how sparse encoding
alters representational structures, we define local and global representations,
demonstrating that they amplify inter-feature distinctions by merging similar
semantic features and introducing additional dimensionality. Furthermore, we
intervene the global representation from an optimization perspective, proving a
significant causal relationship between their separability and the
reconstruction performance. This study explains the principles of sparsity from
the perspective of representational geometry and demonstrates the impact of
changes in representational structure on reconstruction performance.
Particularly emphasizes the necessity of understanding representations and
incorporating representational constraints, providing empirical references for
developing new interpretable tools and improving SAEs. The code is available at
\hyperlink{https://github.com/wenjie1835/SAERepGeo}{https://github.com/wenjie1835/SAERepGeo}.

</details>


### [644] [Training RL Agents for Multi-Objective Network Defense Tasks](https://arxiv.org/pdf/2505.22531)
*Andres Molina-Markham, Luis Robaina, Sean Steinle, Akash Trivedi, Derek Tsui, Nicholas Potteiger, Lauren Brandt, Ransom Winder, Ahmed Ridley*

Main category: cs.LG

TL;DR: The paper proposes an Open-ended Learning (OEL) approach for developing robust and generalizable autonomous network defenders in cybersecurity, addressing technical challenges like task representation.


<details>
  <summary>Details</summary>
Motivation: To apply OEL principles to cybersecurity, overcoming challenges in task representation for diverse network conditions and attacker behaviors.

Method: A training approach inspired by OEL, focusing on consistent task representation for goals, rewards, and action spaces.

Result: Demonstrates that OEL principles yield more robust and generalizable agents for cyber defense.

Conclusion: Encourages the adoption of diverse, consistently represented tasks in cybersecurity research to enhance AI applications.

Abstract: Open-ended learning (OEL) -- which emphasizes training agents that achieve
broad capability over narrow competency -- is emerging as a paradigm to develop
artificial intelligence (AI) agents to achieve robustness and generalization.
However, despite promising results that demonstrate the benefits of OEL,
applying OEL to develop autonomous agents for real-world cybersecurity
applications remains a challenge.
  We propose a training approach, inspired by OEL, to develop autonomous
network defenders. Our results demonstrate that like in other domains, OEL
principles can translate into more robust and generalizable agents for cyber
defense. To apply OEL to network defense, it is necessary to address several
technical challenges. Most importantly, it is critical to provide a task
representation approach over a broad universe of tasks that maintains a
consistent interface over goals, rewards and action spaces. This way, the
learning agent can train with varying network conditions, attacker behaviors,
and defender goals while being able to build on previously gained knowledge.
  With our tools and results, we aim to fundamentally impact research that
applies AI to solve cybersecurity problems. Specifically, as researchers
develop gyms and benchmarks for cyber defense, it is paramount that they
consider diverse tasks with consistent representations, such as those we
propose in our work.

</details>


### [645] [Accelerating Optimization via Differentiable Stopping Time](https://arxiv.org/pdf/2505.22509)
*Zhonglin Xie, Yiman Fong, Haoran Yuan, Zaiwen Wen*

Main category: cs.LG

TL;DR: The paper introduces a differentiable stopping time to optimize the time to reach a target loss, enabling new differentiable formulations for accelerating algorithms.


<details>
  <summary>Details</summary>
Motivation: Current optimization methods focus on minimizing loss at a given time, but the dual problem (minimizing time to reach a target loss) is considered non-differentiable. This limits its practical use.

Method: The authors propose a differentiable stopping time, theoretically justified via differential equations, and design an efficient backpropagation algorithm.

Result: The method enables new differentiable formulations for accelerating algorithms and shows superior performance in experiments.

Conclusion: The proposed differentiable stopping time effectively addresses the non-differentiability issue, with applications in hyperparameter tuning and learning to optimize.

Abstract: Optimization is an important module of modern machine learning applications.
Tremendous efforts have been made to accelerate optimization algorithms. A
common formulation is achieving a lower loss at a given time. This enables a
differentiable framework with respect to the algorithm hyperparameters. In
contrast, its dual, minimizing the time to reach a target loss, is believed to
be non-differentiable, as the time is not differentiable. As a result, it
usually serves as a conceptual framework or is optimized using zeroth-order
methods. To address this limitation, we propose a differentiable stopping time
and theoretically justify it based on differential equations. An efficient
algorithm is designed to backpropagate through it. As a result, the proposed
differentiable stopping time enables a new differentiable formulation for
accelerating algorithms. We further discuss its applications, such as online
hyperparameter tuning and learning to optimize. Our proposed methods show
superior performance in comprehensive experiments across various problems,
which confirms their effectiveness.

</details>


### [646] [TabularQGAN: A Quantum Generative Model for Tabular Data](https://arxiv.org/pdf/2505.22533)
*Pallavi Bhardwaj, Caitlin Jones, Lasse Dierich, Aleksandar Vučković*

Main category: cs.LG

TL;DR: A novel quantum generative model for tabular data synthesis outperforms classical models by 8.5% in similarity scores while using significantly fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Synthetic data is crucial where real-world data is scarce or private, especially for heterogeneous tabular data in industries like healthcare and finance.

Method: Proposes a quantum generative adversarial network with flexible data encoding and a novel quantum circuit ansatz, tested on MIMIC III and Adult Census datasets.

Result: The quantum model outperforms classical models (CTGAN, CopulaGAN) by 8.5% in similarity scores and uses only 0.072% of their parameters.

Conclusion: This is a pioneering demonstration of a quantum generative model for tabular data, suggesting quantum computers' suitability for this task.

Abstract: In this paper, we introduce a novel quantum generative model for synthesizing
tabular data. Synthetic data is valuable in scenarios where real-world data is
scarce or private, it can be used to augment or replace existing datasets.
Real-world enterprise data is predominantly tabular and heterogeneous, often
comprising a mixture of categorical and numerical features, making it highly
relevant across various industries such as healthcare, finance, and software.
We propose a quantum generative adversarial network architecture with flexible
data encoding and a novel quantum circuit ansatz to effectively model tabular
data. The proposed approach is tested on the MIMIC III healthcare and Adult
Census datasets, with extensive benchmarking against leading classical models,
CTGAN, and CopulaGAN. Experimental results demonstrate that our quantum model
outperforms classical models by an average of 8.5% with respect to an overall
similarity score from SDMetrics, while using only 0.072% of the parameters of
the classical models. Additionally, we evaluate the generalization capabilities
of the models using two custom-designed metrics that demonstrate the ability of
the proposed quantum model to generate useful and novel samples. To our
knowledge, this is one of the first demonstrations of a successful quantum
generative model for handling tabular data, indicating that this task could be
well-suited to quantum computers.

</details>


### [647] [Test-Time Alignment of Discrete Diffusion Models with Sequential Monte Carlo](https://arxiv.org/pdf/2505.22524)
*Chinmay Pani, Zijing Ou, Yingzhen Li*

Main category: cs.LG

TL;DR: A training-free method using Sequential Monte Carlo (SMC) with Gumbel-Softmax relaxation is proposed for reward-aligned sampling in discrete diffusion models.


<details>
  <summary>Details</summary>
Motivation: Real-world applications need generative processes to follow constraints without task-specific fine-tuning.

Method: Twisted SMC with a locally optimal proposal via Taylor expansion of the reward function, using Gumbel-Softmax for gradient approximation.

Result: Effective performance validated on synthetic datasets and image modeling.

Conclusion: The method successfully enables constrained sampling in discrete diffusion models without fine-tuning.

Abstract: Discrete diffusion models have become highly effective across various
domains. However, real-world applications often require the generative process
to adhere to certain constraints but without task-specific fine-tuning. To this
end, we propose a training-free method based on Sequential Monte Carlo (SMC) to
sample from the reward-aligned target distribution at the test time. Our
approach leverages twisted SMC with an approximate locally optimal proposal,
obtained via a first-order Taylor expansion of the reward function. To address
the challenge of ill-defined gradients in discrete spaces, we incorporate a
Gumbel-Softmax relaxation, enabling efficient gradient-based approximation
within the discrete generative framework. Empirical results on both synthetic
datasets and image modelling validate the effectiveness of our approach.

</details>


### [648] [Uncertainty Quantification with Proper Scoring Rules: Adjusting Measures to Prediction Tasks](https://arxiv.org/pdf/2505.22538)
*Paul Hofman, Yusuf Sale, Eyke Hüllermeier*

Main category: cs.LG

TL;DR: The paper proposes a flexible framework for uncertainty quantification using proper scoring rules, showing advantages in tasks like selective prediction, out-of-distribution detection, and active learning.


<details>
  <summary>Details</summary>
Motivation: To address the problem of uncertainty quantification by decomposing proper scoring rules into divergence and entropy components, enabling tailored solutions for specific use cases.

Method: Proposes measures of total, aleatoric, and epistemic uncertainty based on proper scoring rules, allowing flexibility in choosing losses suited to the task.

Result: Demonstrates advantages in selective prediction (matching scoring rules to task loss), out-of-distribution detection (mutual information performs best), and active learning (zero-one-loss-based measure outperforms others).

Conclusion: The framework's flexibility in choosing scoring rules enhances uncertainty quantification, proving effective across diverse tasks.

Abstract: We address the problem of uncertainty quantification and propose measures of
total, aleatoric, and epistemic uncertainty based on a known decomposition of
(strictly) proper scoring rules, a specific type of loss function, into a
divergence and an entropy component. This leads to a flexible framework for
uncertainty quantification that can be instantiated with different losses
(scoring rules), which makes it possible to tailor uncertainty quantification
to the use case at hand. We show that this flexibility is indeed advantageous.
In particular, we analyze the task of selective prediction and show that the
scoring rule should ideally match the task loss. In addition, we perform
experiments on two other common tasks. For out-of-distribution detection, our
results confirm that a widely used measure of epistemic uncertainty, mutual
information, performs best. Moreover, in the setting of active learning, our
measure of epistemic uncertainty based on the zero-one-loss consistently
outperforms other uncertainty measures.

</details>


### [649] [A Human-Centric Approach to Explainable AI for Personalized Education](https://arxiv.org/pdf/2505.22541)
*Vinitra Swamy*

Main category: cs.LG

TL;DR: This paper addresses the limited adoption of deep neural networks in education due to lack of explainability, proposing human-centric XAI solutions for personalized learning.


<details>
  <summary>Details</summary>
Motivation: Despite their potential, AI models in education lack trust due to opaque decisions. The thesis aims to integrate human needs into XAI, focusing on personalized learning.

Method: The study combines technical advances (MultiModN, InterpretCC, adversarial training, iLLuMinaTE) with human studies to improve explainability.

Result: The work identifies gaps in post-hoc explainers and introduces interpretable architectures, validated with educators and students.

Conclusion: The research bridges AI performance and transparency, fostering trust in educational AI systems.

Abstract: Deep neural networks form the backbone of artificial intelligence research,
with potential to transform the human experience in areas ranging from
autonomous driving to personal assistants, healthcare to education. However,
their integration into the daily routines of real-world classrooms remains
limited. It is not yet common for a teacher to assign students individualized
homework targeting their specific weaknesses, provide students with instant
feedback, or simulate student responses to a new exam question. While these
models excel in predictive performance, this lack of adoption can be attributed
to a significant weakness: the lack of explainability of model decisions,
leading to a lack of trust from students, parents, and teachers. This thesis
aims to bring human needs to the forefront of eXplainable AI (XAI) research,
grounded in the concrete use case of personalized learning and teaching. We
frame the contributions along two verticals: technical advances in XAI and
their aligned human studies. We investigate explainability in AI for education,
revealing systematic disagreements between post-hoc explainers and identifying
a need for inherently interpretable model architectures. We propose four novel
technical contributions in interpretability with a multimodal modular
architecture (MultiModN), an interpretable mixture-of-experts model
(InterpretCC), adversarial training for explainer stability, and a
theory-driven LLM-XAI framework to present explanations to students
(iLLuMinaTE), which we evaluate in diverse settings with professors, teachers,
learning scientists, and university students. By combining empirical
evaluations of existing explainers with novel architectural designs and human
studies, our work lays a foundation for human-centric AI systems that balance
state-of-the-art performance with built-in transparency and trust.

</details>


### [650] [DES-LOC: Desynced Low Communication Adaptive Optimizers for Training Foundation Models](https://arxiv.org/pdf/2505.22549)
*Alex Iacob, Lorenzo Sani, Mher Safaryan, Paris Giampouras, Samuel Horváth, Andrej Jovanovic, Meghdad Kurmanji, Preslav Aleksandrov, William F. Shen, Xinchi Qiu, Nicholas D. Lane*

Main category: cs.LG

TL;DR: DES-LOC optimizers reduce communication costs in distributed training by asynchronizing parameter and momentum updates, outperforming DDP and Local ADAM.


<details>
  <summary>Details</summary>
Motivation: Current methods for scaling foundation model training are bandwidth-limited, and existing infrequent communication techniques like Local SGD don't work well with adaptive optimizers due to additional states.

Method: DES-LOC assigns independent synchronization periods to parameters and momenta, reducing communication without sacrificing convergence.

Result: DES-LOC communicates 170x less than DDP and 2x less than Local ADAM, while being fault-tolerant.

Conclusion: DES-LOC provides a scalable, efficient, and fault-tolerant solution for training large foundation models.

Abstract: Scaling foundation model training with Distributed Data Parallel (DDP)
methods is bandwidth-limited. Existing infrequent communication methods like
Local SGD were designed to synchronize only model parameters and cannot be
trivially applied to adaptive optimizers due to additional optimizer states.
Current approaches extending Local SGD either lack convergence guarantees or
require synchronizing all optimizer states, tripling communication costs. We
propose Desynced Low Communication Adaptive Optimizers (DES-LOC), a family of
optimizers assigning independent synchronization periods to parameters and
momenta, enabling lower communication costs while preserving convergence.
Through extensive experiments on language models of up to 1.7B, we show that
DES-LOC can communicate 170x less than DDP and 2x less than the previous
state-of-the-art Local ADAM. Furthermore, unlike previous heuristic approaches,
DES-LOC is suited for practical training scenarios prone to system failures.
DES-LOC offers a scalable, bandwidth-efficient, and fault-tolerant solution for
foundation model training.

</details>


### [651] [Geometric Hyena Networks for Large-scale Equivariant Learning](https://arxiv.org/pdf/2505.22560)
*Artem Moskalev, Mangal Prakash, Junjie Xu, Tianyu Cui, Rui Liao, Tommaso Mansi*

Main category: cs.LG

TL;DR: Geometric Hyena is an equivariant long-convolutional model for geometric systems, offering sub-quadratic complexity and outperforming existing methods in efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: The challenge of processing global geometric context with equivariance efficiently, avoiding the quadratic complexity of standard methods like equivariant self-attention.

Method: Introduces Geometric Hyena, leveraging state-space and long-convolutional models to capture global context with sub-quadratic complexity while preserving equivariance.

Result: Outperforms existing equivariant models in tasks like RNA property prediction and protein dynamics, with 20x faster processing and 72x longer context handling.

Conclusion: Geometric Hyena is a scalable, efficient solution for global geometric context modeling, surpassing current equivariant methods in performance and resource usage.

Abstract: Processing global geometric context while preserving equivariance is crucial
when modeling biological, chemical, and physical systems. Yet, this is
challenging due to the computational demands of equivariance and global context
at scale. Standard methods such as equivariant self-attention suffer from
quadratic complexity, while local methods such as distance-based message
passing sacrifice global information. Inspired by the recent success of
state-space and long-convolutional models, we introduce Geometric Hyena, the
first equivariant long-convolutional model for geometric systems. Geometric
Hyena captures global geometric context at sub-quadratic complexity while
maintaining equivariance to rotations and translations. Evaluated on all-atom
property prediction of large RNA molecules and full protein molecular dynamics,
Geometric Hyena outperforms existing equivariant models while requiring
significantly less memory and compute that equivariant self-attention. Notably,
our model processes the geometric context of 30k tokens 20x faster than the
equivariant transformer and allows 72x longer context within the same budget.

</details>


### [652] [FNOPE: Simulation-based inference on function spaces with Fourier Neural Operators](https://arxiv.org/pdf/2505.22573)
*Guy Moss, Leah Sophie Muhle, Reinhard Drews, Jakob H. Macke, Cornelius Schröder*

Main category: cs.LG

TL;DR: FNOPE introduces a Fourier Neural Operator (FNO) with flow matching for efficient posterior estimation of function-valued parameters in SBI, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current SBI methods struggle with function-valued parameters, common in spatiotemporal modeling (e.g., climate science). FNOPE aims to address this gap.

Method: Uses FNO architecture with flow matching for posterior estimation, enabling inference of function-valued parameters at lower computational cost.

Result: FNOPE achieves efficient inference, supports arbitrary discretizations, and handles vector-valued parameters, validated on benchmarks and a glaciology task.

Conclusion: FNOPE expands SBI's applicability to domains requiring function-valued parameter inference, offering computational efficiency and flexibility.

Abstract: Simulation-based inference (SBI) is an established approach for performing
Bayesian inference on scientific simulators. SBI so far works best on
low-dimensional parametric models. However, it is difficult to infer
function-valued parameters, which frequently occur in disciplines that model
spatiotemporal processes such as the climate and earth sciences. Here, we
introduce an approach for efficient posterior estimation, using a Fourier
Neural Operator (FNO) architecture with a flow matching objective. We show that
our approach, FNOPE, can perform inference of function-valued parameters at a
fraction of the simulation budget of state of the art methods. In addition,
FNOPE supports posterior evaluation at arbitrary discretizations of the domain,
as well as simultaneous estimation of vector-valued parameters. We demonstrate
the effectiveness of our approach on several benchmark tasks and a challenging
spatial inference task from glaciology. FNOPE extends the applicability of SBI
methods to new scientific domains by enabling the inference of function-valued
parameters.

</details>


### [653] [Benignity of loss landscape with weight decay requires both large overparametrization and initialization](https://arxiv.org/pdf/2505.22578)
*Etienne Boursier, Matthew Bowditch, Matthias Englert, Ranko Lazic*

Main category: cs.LG

TL;DR: The paper explores the loss landscape of ℓ₂-regularized training for two-layer ReLU networks, showing benign conditions (no spurious local minima) under large overparametrization, but highlights limitations in small initialization regimes.


<details>
  <summary>Details</summary>
Motivation: Understanding the theoretical underpinnings of neural network optimization under weight decay, which is widely used but poorly analyzed in regularized settings.

Method: Analyzing the loss landscape of ℓ₂-regularized training for two-layer ReLU networks, focusing on overparametrization and initialization regimes.

Result: Under large overparametrization (m ≳ min(n^d, 2^n)), the landscape is benign (no spurious local minima), but small initializations can still lead to spurious minima.

Conclusion: Large overparametrization ensures benign landscapes, but initialization size critically impacts optimization outcomes, with small initializations risking convergence to spurious minima.

Abstract: The optimization of neural networks under weight decay remains poorly
understood from a theoretical standpoint. While weight decay is standard
practice in modern training procedures, most theoretical analyses focus on
unregularized settings. In this work, we investigate the loss landscape of the
$\ell_2$-regularized training loss for two-layer ReLU networks. We show that
the landscape becomes benign -- i.e., free of spurious local minima -- under
large overparametrization, specifically when the network width $m$ satisfies $m
\gtrsim \min(n^d, 2^n)$, where $n$ is the number of data points and $d$ the
input dimension. More precisely in this regime, almost all constant activation
regions contain a global minimum and no spurious local minima. We further show
that this level of overparametrization is not only sufficient but also
necessary via the example of orthogonal data. Finally, we demonstrate that such
loss landscape results primarily hold relevance in the large initialization
regime. In contrast, for small initializations -- corresponding to the feature
learning regime -- optimization can still converge to spurious local minima,
despite the global benignity of the landscape.

</details>


### [654] [Machine Unlearning under Overparameterization](https://arxiv.org/pdf/2505.22601)
*Jacob L. Block, Aryan Mokhtari, Sanjay Shakkottai*

Main category: cs.LG

TL;DR: The paper addresses machine unlearning in overparameterized settings, proposing a new definition and algorithm for removing specific training samples' influence by focusing on minimum-complexity interpolators and orthogonal gradient perturbations.


<details>
  <summary>Details</summary>
Motivation: Prior unlearning methods fail in overparameterized settings where models interpolate data, necessitating new definitions and algorithms to effectively remove sample influence.

Method: Defines unlearning as the minimum-complexity interpolator over retained data and introduces an algorithm using orthogonal gradient perturbations to the retained set's gradients.

Result: The proposed framework outperforms existing baselines, with exact and approximate guarantees for different model classes.

Conclusion: The work provides a robust solution for unlearning in overparameterized regimes, validated by superior experimental performance.

Abstract: Machine unlearning algorithms aim to remove the influence of specific
training samples, ideally recovering the model that would have resulted from
training on the remaining data alone. We study unlearning in the
overparameterized setting, where many models interpolate the data, and defining
the unlearning solution as any loss minimizer over the retained
set$\unicode{x2013}$as in prior work in the underparameterized
setting$\unicode{x2013}$is inadequate, since the original model may already
interpolate the retained data and satisfy this condition. In this regime, loss
gradients vanish, rendering prior methods based on gradient perturbations
ineffective, motivating both new unlearning definitions and algorithms. For
this setting, we define the unlearning solution as the minimum-complexity
interpolator over the retained data and propose a new algorithmic framework
that only requires access to model gradients on the retained set at the
original solution. We minimize a regularized objective over perturbations
constrained to be orthogonal to these model gradients, a first-order relaxation
of the interpolation condition. For different model classes, we provide exact
and approximate unlearning guarantees, and we demonstrate that an
implementation of our framework outperforms existing baselines across various
unlearning experiments.

</details>


### [655] [One Rank at a Time: Cascading Error Dynamics in Sequential Learning](https://arxiv.org/pdf/2505.22602)
*Mahtab Alizadeh Vandchali, Fangshuo, Liao, Anastasios Kyrillidis*

Main category: cs.LG

TL;DR: The paper analyzes error propagation in sequential learning of rank-1 subspaces using low-rank linear regression, showing predictable error compounding and its impact on model accuracy.


<details>
  <summary>Details</summary>
Motivation: To understand how errors propagate in sequential learning tasks, particularly in rank-1 subspace estimation, and their effects on overall model performance.

Method: Decomposes sequential learning into rank-1 estimation problems, analyzing error propagation and establishing bounds on accuracy.

Result: Errors compound predictably, affecting model accuracy, with implications for algorithmic design and stability.

Conclusion: The study provides insights into error propagation in sequential learning, offering guidelines for improving algorithmic stability and accuracy.

Abstract: Sequential learning -- where complex tasks are broken down into simpler,
hierarchical components -- has emerged as a paradigm in AI. This paper views
sequential learning through the lens of low-rank linear regression, focusing
specifically on how errors propagate when learning rank-1 subspaces
sequentially. We present an analysis framework that decomposes the learning
process into a series of rank-1 estimation problems, where each subsequent
estimation depends on the accuracy of previous steps. Our contribution is a
characterization of the error propagation in this sequential process,
establishing bounds on how errors -- e.g., due to limited computational budgets
and finite precision -- affect the overall model accuracy. We prove that these
errors compound in predictable ways, with implications for both algorithmic
design and stability guarantees.

</details>


### [656] [Understanding (Un)Reliability of Steering Vectors in Language Models](https://arxiv.org/pdf/2505.22637)
*Joschka Braun, Carsten Eickhoff, David Krueger, Seyed Ali Bahrainian, Dmitrii Krasheninnikov*

Main category: cs.LG

TL;DR: Steering vectors control language models but can be unreliable. This paper explores how prompt types and activation geometry affect steering reliability, finding variance in effectiveness and better results with coherent activation directions.


<details>
  <summary>Details</summary>
Motivation: To understand why steering vectors sometimes fail or work counterproductively, and to identify factors like prompt types and activation geometry that influence their reliability.

Method: Analyzed seven prompt types for steering effects, measured cosine similarity of activation differences, and evaluated dataset separability.

Result: All prompt types had net positive effects but high variance; higher cosine similarity and better dataset separability improved steering.

Conclusion: Steering vectors are unreliable when the target behavior lacks a coherent direction in activation space.

Abstract: Steering vectors are a lightweight method to control language model behavior
by adding a learned bias to the activations at inference time. Although
steering demonstrates promising performance, recent work shows that it can be
unreliable or even counterproductive in some cases. This paper studies the
influence of prompt types and the geometry of activation differences on
steering reliability. First, we find that all seven prompt types used in our
experiments produce a net positive steering effect, but exhibit high variance
across samples, and often give an effect opposite of the desired one. No prompt
type clearly outperforms the others, and yet the steering vectors resulting
from the different prompt types often differ directionally (as measured by
cosine similarity). Second, we show that higher cosine similarity between
training set activation differences predicts more effective steering. Finally,
we observe that datasets where positive and negative activations are better
separated are more steerable. Our results suggest that vector steering is
unreliable when the target behavior is not represented by a coherent direction.

</details>


### [657] [Spectral Survival Analysis](https://arxiv.org/pdf/2505.22641)
*Chengzhi Shi, Stratis Ioannidis*

Main category: cs.LG

TL;DR: The paper introduces a scalable method for CoxPH models by connecting rank regression to survival analysis, improving performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Scaling CoxPH models for large datasets and deep architectures is challenging, especially in high-dimensional settings.

Method: The authors adapt and extend the spectral method for rank regression to survival analysis, generalizing to CoxPH variants, including deep models.

Result: The method outperforms legacy approaches in predictive performance and efficiency on real-world high-dimensional datasets.

Conclusion: The proposed approach offers a versatile and scalable solution for CoxPH models in survival analysis.

Abstract: Survival analysis is widely deployed in a diverse set of fields, including
healthcare, business, ecology, etc. The Cox Proportional Hazard (CoxPH) model
is a semi-parametric model often encountered in the literature. Despite its
popularity, wide deployment, and numerous variants, scaling CoxPH to large
datasets and deep architectures poses a challenge, especially in the
high-dimensional regime. We identify a fundamental connection between rank
regression and the CoxPH model: this allows us to adapt and extend the
so-called spectral method for rank regression to survival analysis. Our
approach is versatile, naturally generalizing to several CoxPH variants,
including deep models. We empirically verify our method's scalability on
multiple real-world high-dimensional datasets; our method outperforms legacy
methods w.r.t. predictive performance and efficiency.

</details>


### [658] [On Learning Verifiers for Chain-of-Thought Reasoning](https://arxiv.org/pdf/2505.22650)
*Maria-Florina Balcan, Avrim Blum, Zhiyuan Li, Dravyansh Sharma*

Main category: cs.LG

TL;DR: The paper proposes a PAC-learning framework to develop reliable verifiers for natural language Chain-of-Thought reasoning, addressing the challenge of incorrect inferences in LLMs.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with formal mathematical reasoning and verifying informal problem statements, necessitating reliable verifiers for natural language reasoning steps.

Method: A formal PAC-learning framework is introduced to study verifier learning, with proposed verification goals at varying strengths, analyzed for sample complexity.

Result: Upper-bounds for learning verifiers are provided, alongside lower-bounds and impossibility results for certain verification objectives without additional assumptions.

Conclusion: The work advances the reliability of Chain-of-Thought reasoning by formalizing verifier learning, though some objectives remain unattainable without extra assumptions.

Abstract: Chain-of-Thought reasoning has emerged as a powerful approach for solving
complex mathematical and logical problems. However, it can often veer off track
through incorrect or unsubstantiated inferences. Formal mathematical reasoning,
which can be checked with a formal verifier, is one approach to addressing this
issue. However, currently LLMs are simply not good enough to solve complex
problems in a formal way, and even just formalizing an informal problem
statement can be challenging. Motivated by this fact, in this work we consider
the problem of learning reliable verifiers for natural language
Chain-of-Thought reasoning. That is, given a problem statement and step-by-step
solution in natural language, the aim of the verifier is to output [Yes] if the
reasoning steps in the solution are all valid, and [No] otherwise. In this work
we give a formal PAC-learning framework for studying this problem. We propose
and analyze several natural verification goals, at different levels of
strength, in this framework. We provide sample complexity upper-bounds for
learning verifiers satisfying these goals, as well as lower-bound and
impossibility results for learning other natural verification objectives
without additional assumptions.

</details>


### [659] [Maximizing Confidence Alone Improves Reasoning](https://arxiv.org/pdf/2505.22660)
*Mihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao Liu, Deepak Pathak*

Main category: cs.LG

TL;DR: RENT is an unsupervised RL method using entropy minimization as intrinsic reward, improving reasoning without external rewards.


<details>
  <summary>Details</summary>
Motivation: Reward engineering in RL is challenging; RENT avoids external rewards by leveraging model entropy.

Method: Uses entropy of the model's distribution as intrinsic reward to reinforce high-confidence reasoning.

Result: Shows improvements on reasoning benchmarks (GSM8K, MATH500, etc.) across various model sizes.

Conclusion: RENT's unsupervised approach is broadly applicable where external supervision is lacking.

Abstract: Reinforcement learning (RL) has enabled machine learning models to achieve
significant advances in many fields. Most recently, RL has empowered frontier
language models to solve challenging math, science, and coding problems.
However, central to any RL algorithm is the reward function, and reward
engineering is a notoriously difficult problem in any domain. In this paper, we
propose RENT: Reinforcement Learning via Entropy Minimization -- a fully
unsupervised RL method that requires no external reward or ground-truth
answers, and instead uses the model's entropy of its underlying distribution as
an intrinsic reward. We find that by reinforcing the chains of thought that
yield high model confidence on its generated answers, the model improves its
reasoning ability. In our experiments, we showcase these improvements on an
extensive suite of commonly-used reasoning benchmarks, including GSM8K,
MATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen and
Mistral families. The generality of our unsupervised learning method lends
itself to applicability in a wide range of domains where external supervision
is limited or unavailable.

</details>


### [660] [Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and unfairness in dyadic regression models](https://arxiv.org/pdf/2401.10690)
*Jorge Paz-Ruza, Amparo Alonso-Betanzos, Bertha Guijarro-Berdiñas, Brais Cancela, Carlos Eiras-Franco*

Main category: cs.LG

TL;DR: The paper identifies eccentricity bias in dyadic regression models, introduces EAUC to measure it, and suggests bias corrections for fairer predictions.


<details>
  <summary>Details</summary>
Motivation: To address biases in dyadic regression models caused by non-uniform observed value distributions, which skew predictions and harm predictive power in critical cases.

Method: The authors prove the existence of eccentricity bias, introduce EAUC as a metric to quantify it, and experiment with naive post-training bias corrections.

Result: EAUC effectively captures eccentricity bias, and bias corrections improve fairness in predictions.

Conclusion: The work provides tools for bias-aware evaluation and fairer dyadic regression models in real-world applications.

Abstract: Dyadic regression models, which output real-valued predictions for pairs of
entities, are fundamental in many domains (e.g. obtaining user-product ratings
in Recommender Systems) and promising and under exploration in others (e.g.
tuning patient-drug dosages in precision pharmacology). In this work, we prove
that non-uniform observed value distributions of individual entities lead to
severe biases in state-of-the-art models, skewing predictions towards the
average of observed past values for the entity and providing worse-than-random
predictive power in eccentric yet crucial cases; we name this phenomenon
eccentricity bias. We show that global error metrics like Root Mean Squared
Error (RMSE) are insufficient to capture this bias, and we introduce
Eccentricity-Area Under the Curve (EAUC) as a novel metric that can quantify it
in all studied domains and models. We prove the intuitive interpretation of
EAUC by experimenting with naive post-training bias corrections, and theorize
other options to use EAUC to guide the construction of fair models. This work
contributes a bias-aware evaluation of dyadic regression to prevent unfairness
in critical real-world applications of such systems.

</details>


### [661] [Learning Latent Graph Structures and their Uncertainty](https://arxiv.org/pdf/2405.19933)
*Alessandro Manenti, Daniele Zambon, Cesare Alippi*

Main category: cs.LG

TL;DR: The paper addresses the challenge of learning latent relational information in graph neural networks, proposing a method using stochastic model outputs to simultaneously learn graph distributions and achieve optimal predictions.


<details>
  <summary>Details</summary>
Motivation: Task-relevant relations in graph neural networks are often unknown, and existing methods fail to ensure proper learning of these latent relations and their uncertainty.

Method: The authors propose a sampling-based method with suitable loss functions on stochastic model outputs to jointly learn the latent graph distribution and optimize predictions.

Result: Empirical results validate the theoretical claims, showing the method effectively learns relational information and improves prediction performance.

Conclusion: The proposed approach successfully addresses the dual task of learning latent graph distributions and achieving optimal predictions, outperforming traditional methods.

Abstract: Graph neural networks use relational information as an inductive bias to
enhance prediction performance. Not rarely, task-relevant relations are unknown
and graph structure learning approaches have been proposed to learn them from
data. Given their latent nature, no graph observations are available to provide
a direct training signal to the learnable relations. Therefore, graph
topologies are typically learned on the prediction task alongside the other
graph neural network parameters. In this paper, we demonstrate that minimizing
point-prediction losses does not guarantee proper learning of the latent
relational information and its associated uncertainty. Conversely, we prove
that suitable loss functions on the stochastic model outputs simultaneously
grant solving two tasks: (i) learning the unknown distribution of the latent
graph and (ii) achieving optimal predictions of the target variable. Finally,
we propose a sampling-based method that solves this joint learning task.
Empirical results validate our theoretical claims and demonstrate the
effectiveness of the proposed approach.

</details>


### [662] [CTBENCH: A Library and Benchmark for Certified Training](https://arxiv.org/pdf/2406.04848)
*Yuhao Mao, Stefan Balauca, Martin Vechev*

Main category: cs.LG

TL;DR: CTBench is introduced as a unified benchmark for certified training, revealing that most algorithms outperform literature-reported results and highlighting key insights about certified models.


<details>
  <summary>Details</summary>
Motivation: The challenge of comparing certified training algorithms due to inconsistent evaluation settings and under-tuned hyperparameters.

Method: Introducing CTBench, a unified library and benchmark, to evaluate algorithms under fair settings with tuned hyperparameters.

Result: Most algorithms surpass literature-reported performance, and recent algorithms' advantages diminish with fair baselines. Key insights about certified models are provided.

Conclusion: CTBench establishes new state-of-the-art results and offers valuable insights, serving as a benchmark for future certified training research.

Abstract: Training certifiably robust neural networks is an important but challenging
task. While many algorithms for (deterministic) certified training have been
proposed, they are often evaluated on different training schedules,
certification methods, and systematically under-tuned hyperparameters, making
it difficult to compare their performance. To address this challenge, we
introduce CTBench, a unified library and a high-quality benchmark for certified
training that evaluates all algorithms under fair settings and systematically
tuned hyperparameters. We show that (1) almost all algorithms in CTBench
surpass the corresponding reported performance in literature in the magnitude
of algorithmic improvements, thus establishing new state-of-the-art, and (2)
the claimed advantage of recent algorithms drops significantly when we enhance
the outdated baselines with a fair training schedule, a fair certification
method and well-tuned hyperparameters. Based on CTBench, we provide new
insights into the current state of certified training, including (1) certified
models have less fragmented loss surface, (2) certified models share many
mistakes, (3) certified models have more sparse activations, (4) reducing
regularization cleverly is crucial for certified training especially for large
radii and (5) certified training has the potential to improve
out-of-distribution generalization. We are confident that CTBench will serve as
a benchmark and testbed for future research in certified training.

</details>


### [663] [Gradient Boosting Reinforcement Learning](https://arxiv.org/pdf/2407.08250)
*Benjamin Fuhrer, Chen Tessler, Gal Dalal*

Main category: cs.LG

TL;DR: GBRL adapts gradient boosting trees (GBT) to RL, outperforming neural networks (NNs) in structured/categorical tasks and showing robustness to out-of-distribution samples.


<details>
  <summary>Details</summary>
Motivation: NNs in RL struggle with structured/categorical features and poor generalization; GBT excels in these areas but is limited in RL due to its static design.

Method: GBRL interleaves tree construction with environment interaction to adapt GBT to RL's dynamic nature.

Result: GBRL outperforms NNs in structured/categorical tasks and matches performance in continuous control, with better robustness and handling of irregular state-action relationships.

Conclusion: GBRL successfully bridges GBT's strengths to RL, offering a robust alternative to NNs for certain tasks.

Abstract: We present Gradient Boosting Reinforcement Learning (GBRL), a framework that
adapts the strengths of gradient boosting trees (GBT) to reinforcement learning
(RL) tasks. While neural networks (NNs) have become the de facto choice for RL,
they face significant challenges with structured and categorical features and
tend to generalize poorly to out-of-distribution samples. These are challenges
for which GBTs have traditionally excelled in supervised learning. However,
GBT's application in RL has been limited. The design of traditional GBT
libraries is optimized for static datasets with fixed labels, making them
incompatible with RL's dynamic nature, where both state distributions and
reward signals evolve during training. GBRL overcomes this limitation by
continuously interleaving tree construction with environment interaction.
Through extensive experiments, we demonstrate that GBRL outperforms NNs in
domains with structured observations and categorical features while maintaining
competitive performance on standard continuous control benchmarks. Like its
supervised learning counterpart, GBRL demonstrates superior robustness to
out-of-distribution samples and better handles irregular state-action
relationships.

</details>


### [664] [Mini-batch Coresets for Memory-efficient Language Model Training on Data Mixtures](https://arxiv.org/pdf/2407.19580)
*Dang Nguyen, Wenhan Yang, Rathul Anand, Yu Yang, Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: CoLM proposes a method to find small mini-batch coresets for LLM training, addressing challenges like imbalanced data and Adam optimizer, reducing memory by 2x and outperforming larger batches.


<details>
  <summary>Details</summary>
Motivation: Training LLMs with large mini-batches is memory-intensive. Existing coreset methods fail due to imbalanced data, Adam optimizer, and high gradient dimensionality.

Method: CoLM includes small-source examples, normalizes gradients historically, and uses zeroth-order methods to sparsify gradients.

Result: CoLM reduces memory by 2x and outperforms 4x larger batches in fine-tuning Phi-2, Phi-3, Zephyr, and Llama-3.

Conclusion: CoLM effectively reduces memory needs while improving performance, integrating well with methods like LoRA.

Abstract: Training with larger mini-batches improves the convergence rate and can yield
superior performance. However, training with large mini-batches becomes
prohibitive for Large Language Models (LLMs), due to the large GPU memory
requirement. To address this problem, an effective approach is finding small
mini-batch coresets that closely match the gradient of larger mini-batches.
However, this approach becomes infeasible and ineffective for LLMs, due to the
highly imbalanced mixture of sources in language data, use of the Adam
optimizer, and the very large gradient dimensionality of LLMs. In this work, we
address the above challenges by proposing Coresets for Training LLMs (CoLM).
First, we show that mini-batch coresets found by gradient matching do not
contain representative examples of the small sources w.h.p., and thus including
all examples of the small sources in the mini-batch coresets is crucial for
optimal performance. Second, we normalize the gradients by their historical
exponential to find mini-batch coresets for training with Adam. Finally, we
leverage zeroth-order methods to find smooth gradient of the last V-projection
matrix and sparsify it to keep the dimensions with the largest normalized
gradient magnitude. We apply CoLM to fine-tuning Phi-2, Phi-3, Zephyr, and
Llama-3 models with LoRA on MathInstruct and SuperGLUE benchmark. Remarkably,
CoLM reduces the memory requirement of fine-tuning by 2x and even outperforms
training with 4x larger mini-batches. Moreover, CoLM seamlessly integrates with
existing memory-efficient training methods like LoRA, further reducing the
memory requirements of training LLMs. Our code is available at
https://github.com/BigML-CS-UCLA/CoLM.

</details>


### [665] [Criticality and Safety Margins for Reinforcement Learning](https://arxiv.org/pdf/2409.18289)
*Alexander Grushin, Walt Woods, Alvaro Velasquez, Simon Khan*

Main category: cs.LG

TL;DR: The paper proposes a framework to measure criticality in reinforcement learning, defining true and proxy criticality for safer agent oversight.


<details>
  <summary>Details</summary>
Motivation: Current methods lack accuracy and interpretability in identifying unsafe situations in reinforcement learning, necessitating a quantifiable and user-significant criticality framework.

Method: Introduces true criticality (expected reward drop from policy deviation) and proxy criticality (low-overhead metric). Uses safety margins for interpretability.

Result: Demonstrated in A3C agent in Atari Beamrider: supervising 5% of decisions could prevent ~47% of agent losses.

Conclusion: The framework enables effective debugging and oversight by measuring potential impacts of bad decisions preemptively.

Abstract: State of the art reinforcement learning methods sometimes encounter unsafe
situations. Identifying when these situations occur is of interest both for
post-hoc analysis and during deployment, where it might be advantageous to call
out to a human overseer for help. Efforts to gauge the criticality of different
points in time have been developed, but their accuracy is not well established
due to a lack of ground truth, and they are not designed to be easily
interpretable by end users. Therefore, we seek to define a criticality
framework with both a quantifiable ground truth and a clear significance to
users. We introduce true criticality as the expected drop in reward when an
agent deviates from its policy for n consecutive random actions. We also
introduce the concept of proxy criticality, a low-overhead metric that has a
statistically monotonic relationship to true criticality. Safety margins make
these interpretable, when defined as the number of random actions for which
performance loss will not exceed some tolerance with high confidence. We
demonstrate this approach in several environment-agent combinations; for an A3C
agent in an Atari Beamrider environment, the lowest 5% of safety margins
contain 47% of agent losses; i.e., supervising only 5% of decisions could
potentially prevent roughly half of an agent's errors. This criticality
framework measures the potential impacts of bad decisions, even before those
decisions are made, allowing for more effective debugging and oversight of
autonomous agents.

</details>


### [666] [Rethinking GNN Expressive Power from a Distributed Computational Model Perspective](https://arxiv.org/pdf/2410.01308)
*Guanyu Cui, Yuhe Guo, Zhewei Wei, Hsin-Hao Su*

Main category: cs.LG

TL;DR: The paper critiques the current theoretical focus on GNNs' ability to distinguish graph structures (via WL tests) and advocates for analyzing their expressiveness using computational models like a modified CONGEST model. It reveals issues with unrestricted preprocessing, shows WL test misalignment with GNNs, and explores virtual nodes/edges.


<details>
  <summary>Details</summary>
Motivation: To shift the theoretical analysis of GNN expressiveness from graph structure distinction to function approximation, using computational models for rigor.

Method: Uses a modified CONGEST model to analyze GNN expressiveness, examining preprocessing, WL test alignment, and virtual nodes/edges.

Result: Unrestricted preprocessing can be problematic; WL test is misaligned with GNNs (requiring near-linear capacity growth). Virtual nodes/edges have computational benefits.

Conclusion: The paper highlights gaps in GNN expressiveness theory, proposes computational models for analysis, and identifies open problems for future research.

Abstract: The success of graph neural networks (GNNs) has motivated theoretical studies
on their expressive power, often through alignments with the Weisfeiler-Lehman
(WL) tests. However, such analyses typically focus on the ability of GNNs to
distinguish between graph structures, rather than to compute or approximate
specific function classes. The latter is more commonly studied in machine
learning theory, including results such as the Turing completeness of recurrent
networks and the universal approximation property of feedforward networks. We
argue that using well-defined computational models, such as a modified CONGEST
model with clearly specified preprocessing and postprocessing, offers a more
sound framework for analyzing GNN expressiveness. Within this framework, we
show that allowing unrestricted preprocessing or incorporating externally
computed features, while claiming that these precomputations enhance the
expressiveness, can sometimes lead to problems. We also show that the lower
bound on a GNN's capacity (depth multiplied by width) to simulate one iteration
of the WL test actually grows nearly linearly with graph size, indicating that
the WL test is not locally computable and is misaligned with message-passing
GNNs. Despite these negative results, we also present positive results that
characterize the effects of virtual nodes and edges from a computational model
perspective. Finally, we highlight several open problems regarding GNN
expressiveness for further exploration.

</details>


### [667] [Exploring the Limitations of Mamba in COPY and CoT Reasoning](https://arxiv.org/pdf/2410.03810)
*Ruifeng Ren, Zhicong Li, Yong Liu*

Main category: cs.LG

TL;DR: Mamba offers constant inference size but struggles with COPY operations and CoT reasoning compared to Transformers, losing its computational savings advantage in some cases.


<details>
  <summary>Details</summary>
Motivation: To analyze Mamba's expressive ability in COPY operations and Chain of Thought (CoT) reasoning compared to Transformers, given its computational cost advantages.

Method: The study connects Mamba to linear attention, evaluates its performance on COPY and CoT tasks, and compares it to Transformers, including scenarios where Mamba's size scales with input length.

Result: Mamba struggles with COPY operations unless its size grows linearly with input length, negating its computational savings. For CoT tasks, Mamba's cost is comparable to Transformers unless the tasks have favorable properties like locality.

Conclusion: Mamba's computational savings are limited to specific scenarios; it cannot universally replace Transformers, especially for tasks requiring COPY operations or arbitrary CoT reasoning.

Abstract: Transformers have become the backbone of modern Large Language Models (LLMs);
however, their inference overhead grows linearly with the sequence length,
posing challenges for modeling long sequences. In light of this, Mamba has
attracted attention for maintaining a constant inference size, with empirical
evidence demonstrating that it can match Transformer performance in sequence
modeling while significantly reducing computational costs. However, an open
question remains: can Mamba always bring savings while achieving performance
comparable to Transformers? In this paper, we focus on analyzing the expressive
ability of Mamba to perform our defined COPY operation and Chain of Thought
(CoT) reasoning. First, inspired by the connection between Mamba and linear
attention, we show that constant-sized Mamba may struggle to perform COPY
operations while Transformers can handle them more easily. However, when the
size of Mamba grows linearly with the input sequence length, it can accurately
perform COPY, but in this case, Mamba no longer provides overhead savings.
Based on this observation, we further analyze Mamba's ability to tackle CoT
tasks, which can be described by the Dynamic Programming (DP) problems. Our
findings suggest that to solve arbitrary DP problems, the total cost of Mamba
is still comparable to standard Transformers. However, similar to efficient
Transformers, when facing DP problems with favorable properties such as
locality, Mamba can provide savings in overhead. Our experiments on the copy
and CoT tasks further demonstrate Mamba's limitations compared to Transformers
in learning these tasks.

</details>


### [668] [Understanding Model Ensemble in Transferable Adversarial Attack](https://arxiv.org/pdf/2410.06851)
*Wei Yao, Zeliang Zhang, Huayi Tang, Yong Liu*

Main category: cs.LG

TL;DR: The paper provides theoretical insights into model ensemble adversarial attacks, defining transferability error and decomposing it into vulnerability, diversity, and a constant. It offers practical guidelines to reduce transferability error and validates the framework with experiments.


<details>
  <summary>Details</summary>
Motivation: The theoretical foundation of model ensemble adversarial attacks is underexplored, prompting the need for early insights to advance the field.

Method: Defines transferability error, diversity, and empirical model ensemble Rademacher complexity. Decomposes transferability error and applies information theory tools to bound it.

Result: Theoretical framework explains transferability error origins and provides guidelines (e.g., increasing diversity, reducing complexity). Experiments with 54 models validate the framework.

Conclusion: The study advances understanding of transferable model ensemble adversarial attacks, offering practical insights for improving attack effectiveness.

Abstract: Model ensemble adversarial attack has become a powerful method for generating
transferable adversarial examples that can target even unknown models, but its
theoretical foundation remains underexplored. To address this gap, we provide
early theoretical insights that serve as a roadmap for advancing model ensemble
adversarial attack. We first define transferability error to measure the error
in adversarial transferability, alongside concepts of diversity and empirical
model ensemble Rademacher complexity. We then decompose the transferability
error into vulnerability, diversity, and a constant, which rigidly explains the
origin of transferability error in model ensemble attack: the vulnerability of
an adversarial example to ensemble components, and the diversity of ensemble
components. Furthermore, we apply the latest mathematical tools in information
theory to bound the transferability error using complexity and generalization
terms, contributing to three practical guidelines for reducing transferability
error: (1) incorporating more surrogate models, (2) increasing their diversity,
and (3) reducing their complexity in cases of overfitting. Finally, extensive
experiments with 54 models validate our theoretical framework, representing a
significant step forward in understanding transferable model ensemble
adversarial attacks.

</details>


### [669] [Latent Weight Diffusion: Generating reactive policies instead of trajectories](https://arxiv.org/pdf/2410.14040)
*Shashank Hegde, Satyajeet Das, Gautam Salhotra, Gaurav S. Sukhatme*

Main category: cs.LG

TL;DR: Latent Weight Diffusion (LWD) improves robotic policy learning by generating neural policy weights via diffusion, offering longer action horizons, robustness, and lower compute costs compared to Diffusion Policy (DP).


<details>
  <summary>Details</summary>
Motivation: Address the trade-offs in Diffusion Policy (DP) models, such as high inference costs and trajectory errors, by proposing a more efficient and robust method.

Method: LWD uses diffusion to generate neural policy weights instead of trajectories, enabling longer action horizons and reduced compute costs.

Result: LWD outperforms DP in success rates for longer action horizons and under perturbations, while requiring significantly fewer FLOPS.

Conclusion: LWD is a promising alternative to DP for robotic tasks, balancing performance, robustness, and computational efficiency.

Abstract: With the increasing availability of open-source robotic data, imitation
learning has emerged as a viable approach for both robot manipulation and
locomotion. Currently, large generalized policies are trained to predict
controls or trajectories using diffusion models, which have the desirable
property of learning multimodal action distributions. However, generalizability
comes with a cost, namely, larger model size and slower inference. This is
especially an issue for robotic tasks that require high control frequency.
Further, there is a known trade-off between performance and action horizon for
Diffusion Policy (DP), a popular model for generating trajectories: fewer
diffusion queries accumulate greater trajectory tracking errors. For these
reasons, it is common practice to run these models at high inference frequency,
subject to robot computational constraints. To address these limitations, we
propose Latent Weight Diffusion (LWD), a method that uses diffusion to generate
closed-loop policies (weights for neural policies) for robotic tasks, rather
than generating trajectories. Learning the behavior distribution through
parameter space over trajectory space offers two key advantages: longer action
horizons (fewer diffusion queries) & robustness to perturbations while
retaining high performance; and a lower inference compute cost. To this end, we
show that LWD has higher success rates than DP when the action horizon is
longer and when stochastic perturbations exist in the environment. Furthermore,
LWD achieves multitask performance comparable to DP while requiring just
~1/45th of the inference-time FLOPS

</details>


### [670] [Inferring Traffic Models in Terminal Airspace from Flight Tracks and Procedures](https://arxiv.org/pdf/2303.09981)
*Soyeon Jung, Amelia Hardy, Mykel J. Kochenderfer*

Main category: cs.LG

TL;DR: A probabilistic model learns variability in aircraft trajectories from procedural and radar data, generating synthetic trajectories for ATM system validation.


<details>
  <summary>Details</summary>
Motivation: To design and validate ATM systems, realistic aircraft trajectory models are needed, capturing variability in how aircraft follow procedures.

Method: Uses Gaussian mixture models to learn trajectory deviations from procedures, then samples deviations to generate synthetic trajectories. Extended to capture pairwise correlations between aircraft.

Result: Demonstrated on JFK Airport data; synthetic trajectories showed distributional similarity to original data via Jensen-Shannon divergence.

Conclusion: The model effectively captures and replicates aircraft trajectory variability, useful for ATM system validation.

Abstract: Realistic aircraft trajectory models are useful in the design and validation
of air traffic management (ATM) systems. Models of aircraft operated under
instrument flight rules (IFR) require capturing the variability inherent in how
aircraft follow standard flight procedures. The variability in aircraft
behavior differs among flight stages. In this paper, we propose a simple
probabilistic model that can learn this variability from procedural data and
flight tracks collected from radar surveillance data. For each segment, we use
a Gaussian mixture model to learn the deviations of aircraft trajectories from
their procedures. Given new procedures, we generate synthetic trajectories by
sampling a series of deviations from the Gaussian mixture model and
reconstructing the aircraft trajectory using the deviations and the procedures.
We extend this method to capture pairwise correlations between aircraft and
show how a pairwise model can be used to generate traffic involving an
arbitrary number of aircraft. We demonstrate the proposed models on the arrival
tracks and procedures of the John F. Kennedy International Airport.
Distributional similarity between the original and the synthetic trajectory
dataset was evaluated using the Jensen-Shannon divergence between the empirical
distributions of different variables and we provide qualitative analyses of the
synthetic trajectories generated.

</details>


### [671] [Coarse-to-fine Q-Network with Action Sequence for Data-Efficient Robot Learning](https://arxiv.org/pdf/2411.12155)
*Younggyo Seo, Pieter Abbeel*

Main category: cs.LG

TL;DR: CQN-AS, a value-based RL algorithm, improves performance by predicting Q-values for action sequences, outperforming baselines in sparse-reward tasks.


<details>
  <summary>Details</summary>
Motivation: To explore if action sequence prediction, successful in behavior cloning, can enhance reinforcement learning.

Method: Introduces CQN-AS, which trains a critic network to predict Q-values for action sequences, learning consequences of executing them.

Result: CQN-AS achieves lower validation loss and outperforms baselines in humanoid control and tabletop manipulation tasks.

Conclusion: Incorporating action sequences in RL improves performance, validating the effectiveness of CQN-AS.

Abstract: Predicting a sequence of actions has been crucial in the success of recent
behavior cloning algorithms in robotics. Can similar ideas improve
reinforcement learning (RL)? We answer affirmatively by observing that
incorporating action sequences when predicting ground-truth return-to-go leads
to lower validation loss. Motivated by this, we introduce Coarse-to-fine
Q-Network with Action Sequence (CQN-AS), a novel value-based RL algorithm that
learns a critic network that outputs Q-values over a sequence of actions, i.e.,
explicitly training the value function to learn the consequence of executing
action sequences. Our experiments show that CQN-AS outperforms several
baselines on a variety of sparse-reward humanoid control and tabletop
manipulation tasks from BiGym and RLBench.

</details>


### [672] [An Empirical Evaluation of Rewiring Approaches in Graph Neural Networks](https://arxiv.org/pdf/2305.19717)
*Alessio Micheli, Domenico Tortorella*

Main category: cs.LG

TL;DR: The paper evaluates graph rewiring methods for mitigating over-squashing in graph neural networks, finding limited practical benefits.


<details>
  <summary>Details</summary>
Motivation: Addressing over-squashing and over-smoothing in deep graph neural networks to improve node representations.

Method: Proposes an evaluation setting using message-passing models without training to isolate rewiring effects.

Result: Systematic experiments show rewiring rarely provides practical benefits for message-passing.

Conclusion: Graph rewiring methods may not significantly improve performance in real-world tasks.

Abstract: Graph neural networks compute node representations by performing multiple
message-passing steps that consist in local aggregations of node features.
Having deep models that can leverage longer-range interactions between nodes is
hindered by the issues of over-smoothing and over-squashing. In particular, the
latter is attributed to the graph topology which guides the message-passing,
causing a node representation to become insensitive to information contained at
distant nodes. Many graph rewiring methods have been proposed to remedy or
mitigate this problem. However, properly evaluating the benefits of these
methods is made difficult by the coupling of over-squashing with other issues
strictly related to model training, such as vanishing gradients. Therefore, we
propose an evaluation setting based on message-passing models that do not
require training to compute node and graph representations. We perform a
systematic experimental comparison on real-world node and graph classification
tasks, showing that rewiring the underlying graph rarely does confer a
practical benefit for message-passing.

</details>


### [673] [Natural Language Reinforcement Learning](https://arxiv.org/pdf/2411.14251)
*Xidong Feng, Bo Liu, Yan Song, Haotian Fu, Ziyu Wan, Girish A. Koushik, Zhiyuan Hu, Mengyue Yang, Ying Wen, Jun Wang*

Main category: cs.LG

TL;DR: NLRL introduces a language-based RL framework using interpretable narratives for value representation, improving agent understanding and learning.


<details>
  <summary>Details</summary>
Motivation: Traditional RL's scalar value representation limits agent understanding and active learning in continuous, grounded interactions.

Method: NLRL replaces scalar values with linguistic narratives (LVF) and extends this to RL components like policy and Bellman equation, using LLMs for implementation.

Result: Experiments on 4 tasks show NLRL's effectiveness, efficiency, and potential for deeper learning.

Conclusion: NLRL offers a promising approach to enhance RL by integrating natural language for richer value representation and active learning.

Abstract: Artificial intelligence progresses towards the "Era of Experience," where
agents are expected to learn from continuous, grounded interaction. We argue
that traditional Reinforcement Learning (RL), which typically represents value
as a scalar, can restrict agent's deep understanding of environments and
hinders the active, deliberative learning crucial for navigating this new
paradigm. To address the issue, we introduce Natural Language Reinforcement
Learning (NLRL), a framework that extends RL principles into natural language
counterparts. Central to NLRL is the Language Value Function (LVF), which
redefines value as an interpretable linguistic narrative articulating the
rationale behind an evaluation. NLRL further extends this concept to core RL
components, including policy, the Bellman equation, and policy iteration.
Leveraging recent advancements in Large Language Models (LLMs), NLRL can be
practically implemented to achieve RL-like policy and value training through
unsupervised environment interactions. Experiments over 4 multi-step agentic
tasks demonstrate NLRL's effectiveness, efficiency, and its potential to foster
deeper understanding and more active learning strategies.

</details>


### [674] [Variational Positive-incentive Noise: How Noise Benefits Models](https://arxiv.org/pdf/2306.07651)
*Hongyuan Zhang, Sida Huang, Yubin Guo, Xuelong Li*

Main category: cs.LG

TL;DR: The paper introduces Positive-incentive Noise (Pi-Noise) to enhance classical models using noise, proposing a variational approach (VPN) for optimization.


<details>
  <summary>Details</summary>
Motivation: Challenge the assumption that noise is always negative and explore its benefits for classical models.

Method: Propose variational Pi-Noise (VPN), optimizing its bound via neural networks to enhance base models without altering their architecture.

Result: VPN improves base models and blurs irrelevant image components, aligning with expectations.

Conclusion: VPN effectively leverages noise to enhance models, demonstrating practical benefits.

Abstract: A large number of works aim to alleviate the impact of noise due to an
underlying conventional assumption of the negative role of noise. However, some
existing works show that the assumption does not always hold. In this paper, we
investigate how to benefit the classical models by random noise under the
framework of Positive-incentive Noise (Pi-Noise). Since the ideal objective of
Pi-Noise is intractable, we propose to optimize its variational bound instead,
namely variational Pi-Noise (VPN). With the variational inference, a VPN
generator implemented by neural networks is designed for enhancing base models
and simplifying the inference of base models, without changing the architecture
of base models. Benefiting from the independent design of base models and VPN
generators, the VPN generator can work with most existing models. From the
experiments, it is shown that the proposed VPN generator can improve the base
models. It is appealing that the trained variational VPN generator prefers to
blur the irrelevant ingredients in complicated images, which meets our
expectations.

</details>


### [675] [Federated Continual Graph Learning](https://arxiv.org/pdf/2411.18919)
*Yinlin Zhu, Miao Hu, Di Wu*

Main category: cs.LG

TL;DR: The paper proposes Federated Continual Graph Learning (FCGL) to address challenges in evolving graph data, introducing POWER to mitigate local and global forgetting issues.


<details>
  <summary>Details</summary>
Motivation: Challenges in managing evolving graph data and catastrophic forgetting in GNNs, with existing methods lacking distributed solutions.

Method: FCGL framework with POWER, using experience nodes and pseudo-prototype reconstruction for knowledge retention.

Result: POWER outperforms federated CGL baselines and vision-centric approaches in experiments.

Conclusion: FCGL and POWER effectively address storage, privacy, and forgetting challenges in distributed graph learning.

Abstract: Managing evolving graph data presents substantial challenges in storage and
privacy, and training graph neural networks (GNNs) on such data often leads to
catastrophic forgetting, impairing performance on earlier tasks. Despite
existing continual graph learning (CGL) methods mitigating this to some extent,
they rely on centralized architectures and ignore the potential of distributed
graph databases to leverage collective intelligence. To this end, we propose
Federated Continual Graph Learning (FCGL) to adapt GNNs across multiple
evolving graphs under storage and privacy constraints. Our empirical study
highlights two core challenges: local graph forgetting (LGF), where clients
lose prior knowledge when adapting to new tasks, and global expertise conflict
(GEC), where the global GNN exhibits sub-optimal performance in both adapting
to new tasks and retaining old ones, arising from inconsistent client expertise
during server-side parameter aggregation. To address these, we introduce POWER,
a framework that preserves experience nodes with maximum local-global coverage
locally to mitigate LGF, and leverages pseudo-prototype reconstruction with
trajectory-aware knowledge transfer to resolve GEC. Experiments on various
graph datasets demonstrate POWER's superiority over federated adaptations of
CGL baselines and vision-centric federated continual learning approaches.

</details>


### [676] [Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models](https://arxiv.org/pdf/2311.04378)
*Hanlin Zhang, Benjamin L. Edelman, Danilo Francati, Daniele Venturi, Giuseppe Ateniese, Boaz Barak*

Main category: cs.LG

TL;DR: The paper proves the impossibility of strong watermarking in generative models under natural assumptions, introducing a generic attack that removes watermarks with minor quality loss.


<details>
  <summary>Details</summary>
Motivation: To investigate whether strong watermarking schemes for generative models are achievable, given the potential for attackers to erase watermarks without significant quality degradation.

Method: Introduces a generic efficient watermark attack based on two assumptions: access to a quality oracle and a perturbation oracle. The attack is tested on three existing watermarking schemes.

Result: The attack successfully removes watermarks from three schemes (Kirchenbauer et al., Kuditipudi et al., Zhao et al.) with only minor quality degradation.

Conclusion: Strong watermarking is impossible under the given assumptions, and existing schemes are vulnerable to the proposed attack.

Abstract: Watermarking generative models consists of planting a statistical signal
(watermark) in a model's output so that it can be later verified that the
output was generated by the given model. A strong watermarking scheme satisfies
the property that a computationally bounded attacker cannot erase the watermark
without causing significant quality degradation. In this paper, we study the
(im)possibility of strong watermarking schemes. We prove that, under
well-specified and natural assumptions, strong watermarking is impossible to
achieve. This holds even in the private detection algorithm setting, where the
watermark insertion and detection algorithms share a secret key, unknown to the
attacker. To prove this result, we introduce a generic efficient watermark
attack; the attacker is not required to know the private key of the scheme or
even which scheme is used. Our attack is based on two assumptions: (1) The
attacker has access to a "quality oracle" that can evaluate whether a candidate
output is a high-quality response to a prompt, and (2) The attacker has access
to a "perturbation oracle" which can modify an output with a nontrivial
probability of maintaining quality, and which induces an efficiently mixing
random walk on high-quality outputs. We argue that both assumptions can be
satisfied in practice by an attacker with weaker computational capabilities
than the watermarked model itself, to which the attacker has only black-box
access. Furthermore, our assumptions will likely only be easier to satisfy over
time as models grow in capabilities and modalities. We demonstrate the
feasibility of our attack by instantiating it to attack three existing
watermarking schemes for large language models: Kirchenbauer et al. (2023),
Kuditipudi et al. (2023), and Zhao et al. (2023). The same attack successfully
removes the watermarks planted by all three schemes, with only minor quality
degradation.

</details>


### [677] [Intrinsic User-Centric Interpretability through Global Mixture of Experts](https://arxiv.org/pdf/2402.02933)
*Vinitra Swamy, Syrielle Montariol, Julian Blackwell, Jibril Frej, Martin Jaggi, Tanja Käser*

Main category: cs.LG

TL;DR: InterpretCC is an interpretable neural network model optimizing for human understanding and faithfulness, outperforming other interpretable methods while matching non-interpretable baselines in accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in human-centeredness of interpretable models, which often produce complex, non-actionable explanations.

Method: Uses adaptive sparse feature activation and a global mixture-of-experts (MoE) model to separate and activate topical subnetworks for each instance.

Result: Achieves comparable performance to non-interpretable models and outperforms interpretable baselines, with higher actionability in user studies.

Conclusion: InterpretCC effectively balances accuracy, interpretability, and human-centeredness, making it practical for real-world applications.

Abstract: In human-centric settings like education or healthcare, model accuracy and
model explainability are key factors for user adoption. Towards these two
goals, intrinsically interpretable deep learning models have gained popularity,
focusing on accurate predictions alongside faithful explanations. However,
there exists a gap in the human-centeredness of these approaches, which often
produce nuanced and complex explanations that are not easily actionable for
downstream users. We present InterpretCC (interpretable conditional
computation), a family of intrinsically interpretable neural networks at a
unique point in the design space that optimizes for ease of human understanding
and explanation faithfulness, while maintaining comparable performance to
state-of-the-art models. InterpretCC achieves this through adaptive sparse
activation of features before prediction, allowing the model to use a
different, minimal set of features for each instance. We extend this idea into
an interpretable, global mixture-of-experts (MoE) model that allows users to
specify topics of interest, discretely separates the feature space for each
data point into topical subnetworks, and adaptively and sparsely activates
these topical subnetworks for prediction. We apply InterpretCC for text, time
series and tabular data across several real-world datasets, demonstrating
comparable performance with non-interpretable baselines and outperforming
intrinsically interpretable baselines. Through a user study involving 56
teachers, InterpretCC explanations are found to have higher actionability and
usefulness over other intrinsically interpretable approaches.

</details>


### [678] [Decoupled Subgraph Federated Learning](https://arxiv.org/pdf/2402.19163)
*Javad Aliakbari, Johan Östman, Alexandre Graell i Amat*

Main category: cs.LG

TL;DR: FedStruct is a novel federated learning framework for graph-structured data that avoids sharing sensitive node features by leveraging global graph structure, achieving near-centralized performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of federated learning on interconnected subgraphs while preserving privacy by not sharing sensitive node features.

Method: FedStruct uses explicit global graph structure to capture dependencies without sharing node features or embeddings.

Result: Validated on six datasets for semi-supervised node classification, showing performance close to centralized approaches under various conditions.

Conclusion: FedStruct effectively balances privacy and performance in federated learning for graph-structured data.

Abstract: We address the challenge of federated learning on graph-structured data
distributed across multiple clients. Specifically, we focus on the prevalent
scenario of interconnected subgraphs, where interconnections between different
clients play a critical role. We present a novel framework for this scenario,
named FedStruct, that harnesses deep structural dependencies. To uphold
privacy, unlike existing methods, FedStruct eliminates the necessity of sharing
or generating sensitive node features or embeddings among clients. Instead, it
leverages explicit global graph structure information to capture inter-node
dependencies. We validate the effectiveness of FedStruct through experimental
results conducted on six datasets for semi-supervised node classification,
showcasing performance close to the centralized approach across various
scenarios, including different data partitioning methods, varying levels of
label availability, and number of clients.

</details>


### [679] [Constraint-Adaptive Policy Switching for Offline Safe Reinforcement Learning](https://arxiv.org/pdf/2412.18946)
*Yassine Chemingui, Aryan Deshwal, Honghao Wei, Alan Fern, Janardhan Rao Doppa*

Main category: cs.LG

TL;DR: CAPS is a wrapper framework for offline safe RL that learns multiple policies during training and switches between them during deployment to adapt to varying safety constraints.


<details>
  <summary>Details</summary>
Motivation: Existing offline RL methods struggle to adapt to changing safety constraints without retraining.

Method: CAPS learns multiple policies with shared representations during training and dynamically switches between them during testing to satisfy current constraints.

Result: CAPS outperforms existing methods on 38 DSRL benchmark tasks.

Conclusion: CAPS provides a strong baseline for offline safe RL with adaptive constraint handling.

Abstract: Offline safe reinforcement learning (OSRL) involves learning a
decision-making policy to maximize rewards from a fixed batch of training data
to satisfy pre-defined safety constraints. However, adapting to varying safety
constraints during deployment without retraining remains an under-explored
challenge. To address this challenge, we introduce constraint-adaptive policy
switching (CAPS), a wrapper framework around existing offline RL algorithms.
During training, CAPS uses offline data to learn multiple policies with a
shared representation that optimize different reward and cost trade-offs.
During testing, CAPS switches between those policies by selecting at each state
the policy that maximizes future rewards among those that satisfy the current
cost constraint. Our experiments on 38 tasks from the DSRL benchmark
demonstrate that CAPS consistently outperforms existing methods, establishing a
strong wrapper-based baseline for OSRL. The code is publicly available at
https://github.com/yassineCh/CAPS.

</details>


### [680] [LC-Tsallis-INF: Generalized Best-of-Both-Worlds Linear Contextual Bandits](https://arxiv.org/pdf/2403.03219)
*Masahiro Kato, Shinji Ito*

Main category: cs.LG

TL;DR: The paper proposes a Best-of-Both-Worlds (BoBW) algorithm for the linear contextual bandit problem, achieving logarithmic regret in stochastic regimes and sublinear regret in adversarial regimes.


<details>
  <summary>Details</summary>
Motivation: To address the linear contextual bandit problem with i.i.d. contexts, aiming for robust performance in both stochastic and adversarial settings.

Method: Develops the α-Linear-Contextual-Tsallis-INF algorithm based on Follow-The-Regularized-Leader (FTRL) with Tsallis entropy.

Result: Achieves O(log(T)) regret in stochastic regimes and O(√T) in adversarial regimes, with extensions under the margin condition.

Conclusion: The algorithm provides versatile performance guarantees across different regimes, including a generalized margin condition.

Abstract: We investigate the \emph{linear contextual bandit problem} with independent
and identically distributed (i.i.d.) contexts. In this problem, we aim to
develop a \emph{Best-of-Both-Worlds} (BoBW) algorithm with regret upper bounds
in both stochastic and adversarial regimes. We develop an algorithm based on
\emph{Follow-The-Regularized-Leader} (FTRL) with Tsallis entropy, referred to
as the $\alpha$-\emph{Linear-Contextual (LC)-Tsallis-INF}. We show that its
regret is at most $O(\log(T))$ in the stochastic regime under the assumption
that the suboptimality gap is uniformly bounded from below, and at most
$O(\sqrt{T})$ in the adversarial regime. Furthermore, our regret analysis is
extended to more general regimes characterized by the \emph{margin condition}
with a parameter $\beta \in (1, \infty]$, which imposes a milder assumption on
the suboptimality gap. We show that the proposed algorithm achieves
$O\left(\log(T)^{\frac{1+\beta}{2+\beta}}T^{\frac{1}{2+\beta}}\right)$ regret
under the margin condition.

</details>


### [681] [An Innovative Data-Driven and Adaptive Reinforcement Learning Approach for Context-Aware Prescriptive Process Monitoring](https://arxiv.org/pdf/2501.10543)
*Mostafa Abbasi, Maziyar Khadivi, Maryam Ahang, Patricia Lasserre, Yves Lucet, Homayoun Najjaran*

Main category: cs.LG

TL;DR: FORLAPS, a novel reinforcement learning framework, optimizes business processes with 31% resource savings and 23% process time reduction, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The potential of AI in business process management is underexplored due to data challenges. FORLAPS addresses this by optimizing execution paths.

Method: FORLAPS uses reinforcement learning with reward shaping and data augmentation for fine-tuning, validated via real-life event logs and case studies.

Result: FORLAPS saves 31% in resource time and reduces process time by 23%, outperforming existing models in diverse industries.

Conclusion: FORLAPS is robust and adaptable, excelling in prescriptive decision-making across various domains.

Abstract: The application of artificial intelligence and machine learning in business
process management has advanced significantly, however, the full potential of
these technologies remains largely unexplored, primarily due to challenges
related to data quality and availability. We present a novel framework called
Fine-Tuned Offline Reinforcement Learning Augmented Process Sequence
Optimization (FORLAPS), which aims to identify optimal execution paths in
business processes by leveraging reinforcement learning enhanced with a
state-dependent reward shaping mechanism, thereby enabling context-sensitive
prescriptions. Additionally, to compare FORLAPS with the existing models
(Permutation Feature Importance and multi-task Long Short Term Memory model),
we experimented to evaluate its effectiveness in terms of resource savings and
process time reduction. The experimental results on real-life event logs
validate that FORLAPS achieves 31% savings in resource time spent and a 23%
reduction in process time span. To further enhance learning, we introduce an
innovative process-aware data augmentation technique that selectively increases
the average estimated Q-values in sampled batches, enabling automatic
fine-tuning of the reinforcement learning model. Robustness was assessed
through both prefix-level and trace-level evaluations, using the
Damerau-Levenshtein distance as the primary metric. Finally, the model's
adaptability across industries was further validated through diverse case
studies, including healthcare treatment pathways, financial services workflows,
permit applications from regulatory bodies, and operations management. In each
domain, the proposed model demonstrated exceptional performance, outperforming
existing state-of-the-art approaches in prescriptive decision-making,
demonstrating its capability to prescribe optimal next steps and predict the
best next activities within a process trace.

</details>


### [682] [Polynomial Chaos Expanded Gaussian Process](https://arxiv.org/pdf/2405.01052)
*Dominik Polke, Tim Kösters, Elmar Ahle, Dirk Söffker*

Main category: cs.LG

TL;DR: The paper introduces PCEGP, a machine learning model combining polynomial chaos expansion and Gaussian processes to improve local and global prediction accuracy with interpretable hyperparameters.


<details>
  <summary>Details</summary>
Motivation: Global models often fail in local areas, and local models add complexity. The study aims to bridge this gap with a model effective for both global and local experimental spaces.

Method: PCEGP uses polynomial chaos expansion to compute input-dependent hyperparameters for Gaussian processes, enabling non-stationary covariance and heteroscedastic noise estimation.

Result: PCEGP shows low prediction errors, often outperforming or matching previous methods, with competitive runtime.

Conclusion: PCEGP offers an interpretable, efficient solution for accurate local and global modeling, balancing performance and complexity.

Abstract: In complex and unknown processes, global models are initially generated over
the entire experimental space but often fail to provide accurate predictions in
local areas. A common approach is to use local models, which requires
partitioning the experimental space and training multiple models, adding
significant complexity. Recognizing this limitation, this study addresses the
need for models that effectively represent both global and local experimental
spaces. It introduces a novel machine learning (ML) approach: Polynomial Chaos
Expanded Gaussian Process (PCEGP), leveraging polynomial chaos expansion (PCE)
to calculate input-dependent hyperparameters of the Gaussian process (GP). This
provides a mathematically interpretable approach that incorporates
non-stationary covariance functions and heteroscedastic noise estimation to
generate locally adapted models. The model performance is compared to different
algorithms in benchmark tests for regression tasks. The results demonstrate low
prediction errors of the PCEGP, highlighting model performance that is often
competitive with or better than previous methods. A key advantage of the
presented model is its interpretable hyperparameters along with training and
prediction runtimes comparable to those of a GP.

</details>


### [683] [Fast meta-solvers for 3D complex-shape scatterers using neural operators trained on a non-scattering problem](https://arxiv.org/pdf/2405.12380)
*Youngkyu Lee, Shanqing Liu, Zongren Zou, Adar Kahana, Eli Turkel, Rishikesh Ranade, Jay Pathak, George Em Karniadakis*

Main category: cs.LG

TL;DR: The paper presents fast meta-solvers combining DeepONet with relaxation or Krylov methods for solving 3D scattering problems, achieving robust and shape-agnostic solutions.


<details>
  <summary>Details</summary>
Motivation: To address the need for high-accuracy, real-time solutions in 3D target identification using scattering techniques.

Method: Train DeepONet for wave propagation, then combine it with relaxation (Jacobi, Gauss-Seidel) or Krylov (GMRES, BiCGStab) methods as meta-solvers.

Result: Meta-solvers are fast, robust, and shape-agnostic, outperforming standalone solvers, especially DeepONet-Krylov methods.

Conclusion: The approach enables efficient and accurate solutions for complex 3D scattering problems, such as submarine scattering.

Abstract: Three-dimensional target identification using scattering techniques requires
high accuracy solutions and very fast computations for real-time predictions in
some critical applications. We first train a deep neural operator~(DeepONet) to
solve wave propagation problems described by the Helmholtz equation in a domain
\textit{without scatterers} but at different wavenumbers and with a complex
absorbing boundary condition. We then design two classes of fast meta-solvers
by combining DeepONet with either relaxation methods, such as Jacobi and
Gauss-Seidel, or with Krylov methods, such as GMRES and BiCGStab, using the
trunk basis of DeepONet as a coarse-scale preconditioner. We leverage the
spectral bias of neural networks to account for the lower part of the spectrum
in the error distribution while the upper part is handled inexpensively using
relaxation methods or fine-scale preconditioners. The meta-solvers are then
applied to solve scattering problems with different shape of scatterers, at no
extra training cost. We first demonstrate that the resulting meta-solvers are
shape-agnostic, fast, and robust, whereas the standard standalone solvers may
even fail to converge without the DeepONet. We then apply both classes of
meta-solvers to scattering from a submarine, a complex three-dimensional
problem. We achieve very fast solutions, especially with the DeepONet-Krylov
methods, which require orders of magnitude fewer iterations than any of the
standalone solvers.

</details>


### [684] [Risk-Informed Diffusion Transformer for Long-Tail Trajectory Prediction in the Crash Scenario](https://arxiv.org/pdf/2501.16349)
*Junlan Chen, Pei Liu, Zihao Zhang, Hongyi Zhao, Yufei Ji, Ziyuan Pu*

Main category: cs.LG

TL;DR: The paper introduces RI-DiT, a method combining graph-based risk info and diffusion with transformer to improve trajectory prediction in rare crash scenarios, addressing long-tail data challenges.


<details>
  <summary>Details</summary>
Motivation: Existing trajectory prediction methods lack data for critical scenarios (e.g., crashes), leading to poor performance in rare but important cases (long-tail phenomenon).

Method: Proposed RI-DiT integrates graph-based risk info (ITTC and traffic flow features) with diffusion and transformer models, using real-world crash data for training.

Result: RI-DiT achieves minADE/minFDE of 0.016/2.667 m for tail 10% data, showing better performance in rare scenarios. Trajectories are less smooth in tail distributions.

Conclusion: RI-DiT effectively addresses long-tail challenges in trajectory prediction, enhancing safety for autonomous driving by improving rare scenario predictions.

Abstract: Trajectory prediction methods have been widely applied in autonomous driving
technologies. Although the overall performance accuracy of trajectory
prediction is relatively high, the lack of trajectory data in critical
scenarios in the training data leads to the long-tail phenomenon. Normally, the
trajectories of the tail data are more critical and more difficult to predict
and may include rare scenarios such as crashes. To solve this problem, we
extracted the trajectory data from real-world crash scenarios, which contain
more long-tail data. Meanwhile, based on the trajectory data in this scenario,
we integrated graph-based risk information and diffusion with transformer and
proposed the Risk-Informed Diffusion Transformer (RI-DiT) trajectory prediction
method. Extensive experiments were conducted on trajectory data in the
real-world crash scenario, and the results show that the algorithm we proposed
has good performance. When predicting the data of the tail 10\% (Top 10\%), the
minADE and minFDE indicators are 0.016/2.667 m. At the same time, we showed the
trajectory conditions of different long-tail distributions. The distribution of
trajectory data is closer to the tail, the less smooth the trajectory is.
Through the trajectory data in real-world crash scenarios, Our work expands the
methods to overcome the long-tail challenges in trajectory prediction. Our
method, RI-DiT, integrates inverse time to collision (ITTC) and the feature of
traffic flow, which can predict long-tail trajectories more accurately and
improve the safety of autonomous driving systems.

</details>


### [685] [Fully Heteroscedastic Count Regression with Deep Double Poisson Networks](https://arxiv.org/pdf/2406.09262)
*Spencer Young, Porter Jenkins, Longchao Da, Jeff Dotson, Hua Wei*

Main category: cs.LG

TL;DR: The paper introduces the Deep Double Poisson Network (DDPN) for count regression, addressing the lack of methods for uncertainty representation in count data. DDPN outperforms existing baselines in accuracy, calibration, and OOD detection.


<details>
  <summary>Details</summary>
Motivation: Existing methods for uncertainty representation in regression focus on continuous data, leaving a gap for count regression. DDPN aims to fill this gap by providing flexible aleatoric and epistemic uncertainty estimation for count data.

Method: The paper proposes DDPN, a neural network model that outputs parameters of the Double Poisson distribution. It includes learnable loss attenuation and a modified loss function to control uncertainty behavior.

Result: Experiments show DDPN outperforms current baselines in accuracy, calibration, and out-of-distribution detection, setting a new state-of-the-art for count regression.

Conclusion: DDPN successfully addresses the gap in uncertainty representation for count regression, offering robust performance and improved uncertainty estimation.

Abstract: Neural networks capable of accurate, input-conditional uncertainty
representation are essential for real-world AI systems. Deep ensembles of
Gaussian networks have proven highly effective for continuous regression due to
their ability to flexibly represent aleatoric uncertainty via unrestricted
heteroscedastic variance, which in turn enables accurate epistemic uncertainty
estimation. However, no analogous approach exists for count regression, despite
many important applications. To address this gap, we propose the Deep Double
Poisson Network (DDPN), a novel neural discrete count regression model that
outputs the parameters of the Double Poisson distribution, enabling arbitrarily
high or low predictive aleatoric uncertainty for count data and improving
epistemic uncertainty estimation when ensembled. We formalize and prove that
DDPN exhibits robust regression properties similar to heteroscedastic Gaussian
models via learnable loss attenuation, and introduce a simple loss modification
to control this behavior. Experiments on diverse datasets demonstrate that DDPN
outperforms current baselines in accuracy, calibration, and out-of-distribution
detection, establishing a new state-of-the-art in deep count regression.

</details>


### [686] [Quantum Kernel Learning for Small Dataset Modeling in Semiconductor Fabrication: Application to Ohmic Contact](https://arxiv.org/pdf/2409.10803)
*Zeheng Wang, Fangzhou Wang, Liang Li, Zirui Wang, Timothy van der Laan, Ross C. C. Leon, Jing-Kai Huang, Muhammad Usman*

Main category: cs.LG

TL;DR: Quantum machine learning (QML) outperforms classical methods in modeling semiconductor fabrication with limited data, using a quantum kernel-aligned regressor (QKAR).


<details>
  <summary>Details</summary>
Motivation: Challenges in modeling semiconductor processes due to high-dimensional parameters and small datasets motivate exploring QML as an alternative to classical methods.

Method: Developed QKAR with a shallow Pauli-Z feature map and trainable quantum kernel alignment, evaluated against seven classical regressors under unified preprocessing.

Result: QKAR achieved lower errors (MAE: 0.338 Ω mm) than classical baselines, showing robustness in cross-validation and new device tests.

Conclusion: QML offers predictive advantages in data-constrained semiconductor modeling, demonstrating potential for near-term quantum hardware deployment.

Abstract: Modeling complex semiconductor fabrication processes such as Ohmic contact
formation remains challenging due to high-dimensional parameter spaces and
limited experimental data. While classical machine learning (CML) approaches
have been successful in many domains, their performance degrades in
small-sample, nonlinear scenarios. In this work, we investigate quantum machine
learning (QML) as an alternative, exploiting quantum kernels to capture
intricate correlations from compact datasets. Using only 159 experimental GaN
HEMT samples, we develop a quantum kernel-aligned regressor (QKAR) combining a
shallow Pauli-Z feature map with a trainable quantum kernel alignment (QKA)
layer. All models, including seven baseline CML regressors, are evaluated under
a unified PCA-based preprocessing pipeline to ensure a fair comparison. QKAR
consistently outperforms classical baselines across multiple metrics (MAE, MSE,
RMSE), achieving a mean absolute error of 0.338 Omega mm when validated on
experimental data. We further assess noise robustness and generalization
through cross-validation and new device fabrication. These findings suggest
that carefully constructed QML models could provide predictive advantages in
data-constrained semiconductor modeling, offering a foundation for practical
deployment on near-term quantum hardware. While challenges remain for both QML
and CML, this study demonstrates QML's potential as a complementary approach in
complex process modeling tasks.

</details>


### [687] [Message-Passing GNNs Fail to Approximate Sparse Triangular Factorizations](https://arxiv.org/pdf/2502.01397)
*Vladislav Trifonov, Ekaterina Muravleva, Ivan Oseledets*

Main category: cs.LG

TL;DR: GNNs, particularly message-passing variants, fail to approximate sparse triangular factorizations due to non-local dependencies, despite their potential for learning preconditioners.


<details>
  <summary>Details</summary>
Motivation: To investigate the limitations of GNNs in approximating sparse triangular factorizations, especially for matrices requiring non-local dependencies.

Method: Construct baselines using synthetic and real-world matrices (SuiteSparse), evaluate GNN architectures (Graph Attention Networks, Graph Transformers) against exact or K-optimal factorizations.

Result: Severe performance degradation observed (cosine similarity < 0.6), indicating GNNs' inability to handle non-local dependencies.

Conclusion: Architectural innovations beyond message-passing are needed for GNNs to succeed in matrix factorization tasks.

Abstract: Graph Neural Networks (GNNs) have been proposed as a tool for learning sparse
matrix preconditioners, which are key components in accelerating linear
solvers. This position paper argues that message-passing GNNs are fundamentally
incapable of approximating sparse triangular factorizations. We demonstrate
that message-passing GNNs fundamentally fail to approximate sparse triangular
factorizations for classes of matrices for which high-quality preconditioners
exist but require non-local dependencies. To illustrate this, we construct a
set of baselines using both synthetic matrices and real-world examples from the
SuiteSparse collection. Across a range of GNN architectures, including Graph
Attention Networks and Graph Transformers, we observe severe performance
degradation compared to exact or K-optimal factorizations, with cosine
similarity dropping below $0.6$ in key cases. Our theoretical and empirical
results suggest that architectural innovations beyond message-passing are
necessary for applying GNNs to scientific computing tasks such as matrix
factorization. Experiments demonstrate that overcoming non-locality alone is
insufficient. Tailored architectures are necessary to capture the required
dependencies since even a completely non-local Graph Transformer fails to match
the proposed baselines.

</details>


### [688] [A Hybrid Multi-Factor Network with Dynamic Sequence Modeling for Early Warning of Intraoperative Hypotension](https://arxiv.org/pdf/2409.11064)
*Mingyue Cheng, Jintao Zhang, Zhiding Liu, Chunli Liu*

Main category: cs.LG

TL;DR: The paper proposes a Hybrid Multi-Factor (HMF) network for predicting intraoperative hypotension (IOH) by capturing temporal dependencies and non-stationarity in physiological signals, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: IOH prediction is critical to prevent severe complications, but current methods lack dynamic modeling of physiological signals.

Method: HMF decomposes signals into trend and seasonal components, uses patch-based Transformers for encoding, and introduces symmetric normalization for distributional drift.

Result: HMF outperforms baselines on public and clinical datasets.

Conclusion: HMF advances IOH prediction, potentially improving surgical safety; code is publicly available.

Abstract: Intraoperative hypotension (IOH) prediction using past physiological signals
is crucial, as IOH may lead to inadequate organ perfusion and significantly
elevate the risk of severe complications and mortality. However, current
methods often rely on static modeling, overlooking the complex temporal
dependencies and the inherently non-stationary nature of physiological signals.
We propose a Hybrid Multi-Factor (HMF) network that formulates IOH prediction
as a dynamic sequence forecasting task, explicitly capturing both temporal
dependencies and physiological non-stationarity. We represent signal dynamics
as multivariate time series and decompose them into trend and seasonal
components, enabling separate modeling of long-term and periodic variations.
Each component is encoded with a patch-based Transformer to balance
computational efficiency and feature representation. To address distributional
drift from evolving signals, we introduce a symmetric normalization mechanism.
Experiments on both public and real-world clinical datasets show that HMF
significantly outperforms competitive baselines. We hope HMF offers new
insights into IOH prediction and ultimately promotes safer surgical care. Our
code is available at https://github.com/Mingyue-Cheng/HMF.

</details>


### [689] [Improving Rule-based Reasoning in LLMs using Neurosymbolic Representations](https://arxiv.org/pdf/2502.01657)
*Varun Dhanraj, Chris Eliasmith*

Main category: cs.LG

TL;DR: A neurosymbolic method improves LLM reasoning by encoding hidden states into neurosymbolic vectors, enhancing performance on numerical tasks without degrading other capabilities.


<details>
  <summary>Details</summary>
Motivation: Addressing LLMs' challenges in precise rule-following tasks, especially mathematical reasoning, by leveraging neurosymbolic representations.

Method: Encodes hidden states into neurosymbolic vectors for problem-solving in a neurosymbolic space, then merges results with the original state.

Result: Achieves 88.6% lower cross-entropy loss and solves 15.4x more problems correctly compared to baselines.

Conclusion: The neurosymbolic approach boosts efficiency, reliability, and interpretability in LLM reasoning tasks.

Abstract: Large language models (LLMs) continue to face challenges in reliably solving
reasoning tasks, particularly those that require precise rule following, as
often found in mathematical reasoning. This paper introduces a novel
neurosymbolic method that improves LLM reasoning by encoding hidden states into
neurosymbolic vectors, enabling problem-solving within a neurosymbolic vector
space. The results are decoded and merged with the original hidden state,
significantly boosting the model's performance on numerical reasoning tasks. By
offloading computation through neurosymbolic representations, this method
enhances efficiency, reliability, and interpretability. Experimental results
demonstrate an average of 88.6% lower cross-entropy loss and 15.4 times more
problems correctly solved on a suite of mathematical reasoning tasks compared
to chain-of-thought prompting and supervised fine-tuning (LoRA), without
degrading performance on other tasks. We make our code available at:
https://github.com/vdhanraj/Neurosymbolic-LLM.

</details>


### [690] [Diss-l-ECT: Dissecting Graph Data with Local Euler Characteristic Transforms](https://arxiv.org/pdf/2410.02622)
*Julius von Rohrscheidt, Bastian Rieck*

Main category: cs.LG

TL;DR: The paper introduces the Local Euler Characteristic Transform (ℓ-ECT), a lossless local representation method for graphs, outperforming GNNs in node-classification tasks while preserving interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the loss of local details in traditional GNNs and enhance expressivity and interpretability in graph representation learning.

Method: Extends the Euler Characteristic Transform (ECT) to ℓ-ECT for lossless local neighborhood representation and introduces a rotation-invariant metric for spatial alignment.

Result: Superior performance compared to standard GNNs on node-classification tasks, with theoretical guarantees.

Conclusion: The ℓ-ECT effectively balances local detail preservation and global interpretability, offering a robust alternative to GNNs.

Abstract: The Euler Characteristic Transform (ECT) is an efficiently-computable
geometrical-topological invariant that characterizes the global shape of data.
In this paper, we introduce the Local Euler Characteristic Transform
($\ell$-ECT), a novel extension of the ECT particularly designed to enhance
expressivity and interpretability in graph representation learning. Unlike
traditional Graph Neural Networks (GNNs), which may lose critical local details
through aggregation, the $\ell$-ECT provides a lossless representation of local
neighborhoods. This approach addresses key limitations in GNNs by preserving
nuanced local structures while maintaining global interpretability. Moreover,
we construct a rotation-invariant metric based on $\ell$-ECTs for spatial
alignment of data spaces. Our method exhibits superior performance compared to
standard GNNs on a variety of node-classification tasks, while also offering
theoretical guarantees that demonstrate its effectiveness.

</details>


### [691] [Robust LLM Alignment via Distributionally Robust Direct Preference Optimization](https://arxiv.org/pdf/2502.01930)
*Zaiyan Xu, Sushil Vemuri, Kishan Panaganti, Dileep Kalathil, Rahul Jain, Deepak Ramachandran*

Main category: cs.LG

TL;DR: The paper addresses distribution shift in LLM alignment by introducing two robust DPO algorithms, WDPO and KLDPO, showing improved performance in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: User preferences vary widely, causing alignment failures in LLMs due to reliance on static datasets.

Method: Developed WDPO and KLDPO using distributionally robust optimization, with scalable gradient descent algorithms.

Result: WDPO and KLDPO outperform in alignment under preference distribution shift, validated empirically.

Conclusion: The proposed algorithms effectively mitigate alignment failures caused by preference shifts.

Abstract: A major challenge in aligning large language models (LLMs) with human
preferences is the issue of distribution shift. LLM alignment algorithms rely
on static preference datasets, assuming that they accurately represent
real-world user preferences. However, user preferences vary significantly
across geographical regions, demographics, linguistic patterns, and evolving
cultural trends. This preference distribution shift leads to catastrophic
alignment failures in many real-world applications. We address this problem
using the principled framework of distributionally robust optimization, and
develop two novel distributionally robust direct preference optimization (DPO)
algorithms, namely, Wasserstein DPO (WDPO) and Kullback-Leibler DPO (KLDPO). We
characterize the sample complexity of learning the optimal policy parameters
for WDPO and KLDPO. Moreover, we propose scalable gradient descent-style
learning algorithms by developing suitable approximations for the challenging
minimax loss functions of WDPO and KLDPO. Our empirical experiments using
benchmark data sets and LLMs demonstrate the superior performance of WDPO and
KLDPO in substantially improving the alignment when there is a preference
distribution shift.

</details>


### [692] [EventFlow: Forecasting Temporal Point Processes with Flow Matching](https://arxiv.org/pdf/2410.07430)
*Gavin Kerrigan, Kai Nelson, Padhraic Smyth*

Main category: cs.LG

TL;DR: EventFlow, a non-autoregressive model for temporal point processes, outperforms autoregressive models by reducing error by 20%-53% and requiring fewer model calls.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models for temporal point processes degrade in performance for longer forecasting horizons due to cascading errors and myopic predictions.

Method: EventFlow uses flow matching to directly learn joint distributions over event times, avoiding autoregressive processes.

Result: EventFlow achieves 20%-53% lower error than baselines and uses fewer model calls during sampling.

Conclusion: EventFlow is a simpler, more effective alternative to autoregressive models for temporal point processes.

Abstract: Continuous-time event sequences, in which events occur at irregular
intervals, are ubiquitous across a wide range of industrial and scientific
domains. The contemporary modeling paradigm is to treat such data as
realizations of a temporal point process, and in machine learning it is common
to model temporal point processes in an autoregressive fashion using a neural
network. While autoregressive models are successful in predicting the time of a
single subsequent event, their performance can degrade when forecasting longer
horizons due to cascading errors and myopic predictions. We propose EventFlow,
a non-autoregressive generative model for temporal point processes. The model
builds on the flow matching framework in order to directly learn joint
distributions over event times, side-stepping the autoregressive process.
EventFlow is simple to implement and achieves a 20%-53% lower error than the
nearest baseline on standard TPP benchmarks while simultaneously using fewer
model calls at sampling time.

</details>


### [693] [Path Planning for Masked Diffusion Model Sampling](https://arxiv.org/pdf/2502.03540)
*Fred Zhangzhi Peng, Zachary Bezemek, Sawan Patel, Jarrid Rector-Brooks, Sherwood Yao, Avishek Joey Bose, Alexander Tong, Pranam Chatterjee*

Main category: cs.LG

TL;DR: The paper introduces Path Planning (P2), a novel inference strategy for masked diffusion models (MDMs), enhancing generative quality by refining unmasked tokens and achieving state-of-the-art performance across domains.


<details>
  <summary>Details</summary>
Motivation: Current MDMs lack iterative refinement of unmasked tokens, limiting their potential. The paper aims to unlock the full power of MDMs by enabling refinement through P2.

Method: P2 decomposes generation into planning and denoising sub-stages, allowing iterative refinement of tokens. It includes planners like Self-Planning, BERT-Planning, and Trained-Planning.

Result: P2 improves generative quality significantly, with relative gains of 22% in protein foldability, 8% in RNA pLDDT, 4% in math reasoning, 68% in story generation, and 33% in code generation.

Conclusion: P2 generalizes existing MDM sampling strategies, enhances generative quality, and establishes a new ELBO, proving its effectiveness across diverse domains.

Abstract: Any order generation of discrete data using masked diffusion models (MDMs)
offers a compelling alternative to traditional autoregressive models,
especially in domains that lack a natural causal ordering of data. However,
current popular MDMs depart from their successful continuous diffusion model
counterparts with simplified masked inference wherein unmasked tokens cannot be
iteratively refined -- even if there is a mistake. In this paper, we extract
the full power of MDMs by introducing a novel inference sampling strategy
termed Path Planning (P2) that decomposes each generation step into two
sub-stages: planning and denoising. Under P2, the planner at every step selects
appropriate tokens that are marked to be updated, which can then be sampled
using the denoiser. We demonstrate that P2 generalizes all existing sampling
strategies for MDMs and critically enhances generative quality through the new
capability of refining and updating existing unmasked tokens. We theoretically
prove that P2 establishes a (new) expanded evidence lower bound (ELBO) on the
log marginal likelihood of data. We instantiate P2 with a family of planners
including: 1.) Self-Planning, 2.) BERT-Planning, and 3.) Trained-Planning with
a learned planner leading to SOTA generative performance for MDMs on a suite of
domains. Specifically, solely using P2 inference, we observe relative
improvements of 22% in protein sequence foldability, 8% in RNA sequence pLDDT,
4% in math reasoning, 68% in story generation (ROUGE score), and 33% in code
generation for the challenging pass@1 metric.

</details>


### [694] [NRFormer: Nationwide Nuclear Radiation Forecasting with Spatio-Temporal Transformer](https://arxiv.org/pdf/2410.11924)
*Tengfei Lyu, Jindong Han, Hao Liu*

Main category: cs.LG

TL;DR: NRFormer is a novel framework for nationwide nuclear radiation prediction, addressing spatial imbalance and non-stationary patterns with advanced attention modules.


<details>
  <summary>Details</summary>
Motivation: Nuclear radiation monitoring is critical for safety, but challenges like spatial imbalance and non-stationary patterns hinder accurate forecasting.

Method: NRFormer integrates non-stationary temporal attention, imbalance-aware spatial attention, and radiation propagation prompting to model spatio-temporal dynamics.

Result: NRFormer outperforms 11 baselines in experiments on real-world datasets.

Conclusion: NRFormer provides an effective solution for nuclear radiation forecasting, aiding decision-making for safety and environmental protection.

Abstract: Nuclear radiation, which refers to the energy emitted from atomic nuclei
during decay, poses significant risks to human health and environmental safety.
Recently, advancements in monitoring technology have facilitated the effective
recording of nuclear radiation levels and related factors, such as weather
conditions. The abundance of monitoring data enables the development of
accurate and reliable nuclear radiation forecasting models, which play a
crucial role in informing decision-making for individuals and governments.
However, this task is challenging due to the imbalanced distribution of
monitoring stations over a wide spatial range and the non-stationary radiation
variation patterns. In this study, we introduce NRFormer, a novel framework
tailored for the nationwide prediction of nuclear radiation variations. By
integrating a non-stationary temporal attention module, an imbalance-aware
spatial attention module, and a radiation propagation prompting module,
NRFormer collectively captures complex spatio-temporal dynamics of nuclear
radiation. Extensive experiments on two real-world datasets demonstrate the
superiority of our proposed framework against 11 baselines.

</details>


### [695] [ExpProof : Operationalizing Explanations for Confidential Models with ZKPs](https://arxiv.org/pdf/2502.03773)
*Chhavi Yadav, Evan Monroe Laufer, Dan Boneh, Kamalika Chaudhuri*

Main category: cs.LG

TL;DR: The paper addresses the failure of explainability methods in adversarial settings and proposes using Zero-Knowledge Proofs (ZKPs) to operationalize explanations, specifically adapting LIME for ZKPs.


<details>
  <summary>Details</summary>
Motivation: Explainability methods often fail in adversarial scenarios due to misaligned interests and manipulation incentives, despite regulatory demands for trust in machine learning models.

Method: The authors explore ZKP-amenable versions of the LIME explainability algorithm and evaluate their performance on Neural Networks and Random Forests.

Result: The study demonstrates the feasibility of using ZKPs to enhance the robustness of explanations in adversarial contexts.

Conclusion: The work advances the operationalization of explanations in adversarial settings by leveraging cryptographic primitives like ZKPs, with publicly available code for further exploration.

Abstract: In principle, explanations are intended as a way to increase trust in machine
learning models and are often obligated by regulations. However, many
circumstances where these are demanded are adversarial in nature, meaning the
involved parties have misaligned interests and are incentivized to manipulate
explanations for their purpose. As a result, explainability methods fail to be
operational in such settings despite the demand \cite{bordt2022post}. In this
paper, we take a step towards operationalizing explanations in adversarial
scenarios with Zero-Knowledge Proofs (ZKPs), a cryptographic primitive.
Specifically we explore ZKP-amenable versions of the popular explainability
algorithm LIME and evaluate their performance on Neural Networks and Random
Forests. Our code is publicly available at
https://github.com/emlaufer/ExpProof.

</details>


### [696] [Embedding Safety into RL: A New Take on Trust Region Methods](https://arxiv.org/pdf/2411.02957)
*Nikola Milosevic, Johannes Müller, Nico Scherf*

Main category: cs.LG

TL;DR: C-TRPO ensures safe RL training by reshaping policy space geometry, reducing constraint violations while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods either compromise reward maximization or allow unsafe behavior; C-TRPO aims to enforce safety constraints without these drawbacks.

Method: Introduces Constrained Trust Region Policy Optimization (C-TRPO), reshaping policy space to ensure trust regions contain only safe policies.

Result: C-TRPO reduces constraint violations while keeping competitive returns, validated through experiments.

Conclusion: C-TRPO effectively balances safety and performance in RL, offering a robust solution for constrained optimization.

Abstract: Reinforcement Learning (RL) agents can solve diverse tasks but often exhibit
unsafe behavior. Constrained Markov Decision Processes (CMDPs) address this by
enforcing safety constraints, yet existing methods either sacrifice reward
maximization or allow unsafe training. We introduce Constrained Trust Region
Policy Optimization (C-TRPO), which reshapes the policy space geometry to
ensure trust regions contain only safe policies, guaranteeing constraint
satisfaction throughout training. We analyze its theoretical properties and
connections to TRPO, Natural Policy Gradient (NPG), and Constrained Policy
Optimization (CPO). Experiments show that C-TRPO reduces constraint violations
while maintaining competitive returns.

</details>


### [697] [TransMLA: Migrating GQA Models to MLA with Full DeepSeek Compatibility and Speedup](https://arxiv.org/pdf/2502.07864)
*Fanxu Meng, Pingzhi Tang, Zengwei Yao, Xing Sun, Muhan Zhang*

Main category: cs.LG

TL;DR: TransMLA converts GQA-based models to MLA-based ones, achieving 10.6x speedup with minimal performance loss and compatibility with DeepSeek optimizations.


<details>
  <summary>Details</summary>
Motivation: To enable GQA-based models to leverage DeepSeek's optimizations and improve inference speed without sacrificing output quality.

Method: Compresses KV cache in LLaMA-2-7B, fine-tunes with 6B tokens, and integrates with DeepSeek's features like FP8 quantization.

Result: 10.6x inference speedup at 8K context length, 93% KV cache compression, and performance parity with original models.

Conclusion: TransMLA provides a practical migration path for GQA models to MLA, enhancing speed and compatibility with DeepSeek.

Abstract: In this paper, we present TransMLA, a framework that seamlessly converts any
GQA-based pre-trained model into an MLA-based model. Our approach enables
direct compatibility with DeepSeek's codebase, allowing these models to fully
leverage DeepSeek-specific optimizations such as vLLM and SGlang. By
compressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x
inference speedup at an 8K context length while preserving meaningful output
quality. Additionally, the model requires only 6 billion tokens for fine-tuning
to regain performance on par with the original across multiple benchmarks.
TransMLA offers a practical solution for migrating GQA-based models to the MLA
structure. When combined with DeepSeek's advanced features, such as FP8
quantization and Multi-Token Prediction, even greater inference acceleration
can be realized.

</details>


### [698] [AutoElicit: Using Large Language Models for Expert Prior Elicitation in Predictive Modelling](https://arxiv.org/pdf/2411.17284)
*Alexander Capstick, Rahul G. Krishnan, Payam Barnaghi*

Main category: cs.LG

TL;DR: AutoElicit extracts knowledge from LLMs to create informative priors for predictive models, reducing error and labeling effort compared to uninformative priors and in-context learning.


<details>
  <summary>Details</summary>
Motivation: Specialized domains like healthcare need interpretable models with limited labeled data. Expert priors are time-consuming to elicit, so AutoElicit leverages LLMs for this.

Method: AutoElicit extracts knowledge from LLMs to construct priors, refined via natural language, and compares performance with in-context learning.

Result: AutoElicit reduces error, uses fewer labels, and outperforms in-context learning, saving significant labeling effort (e.g., 6 months for UTI predictions).

Conclusion: AutoElicit effectively bridges LLMs and interpretable models, offering a practical solution for domains with scarce labeled data.

Abstract: Large language models (LLMs) acquire a breadth of information across various
domains. However, their computational complexity, cost, and lack of
transparency often hinder their direct application for predictive tasks where
privacy and interpretability are paramount. In fields such as healthcare,
biology, and finance, specialised and interpretable linear models still hold
considerable value. In such domains, labelled data may be scarce or expensive
to obtain. Well-specified prior distributions over model parameters can reduce
the sample complexity of learning through Bayesian inference; however,
eliciting expert priors can be time-consuming. We therefore introduce
AutoElicit to extract knowledge from LLMs and construct priors for predictive
models. We show these priors are informative and can be refined using natural
language. We perform a careful study contrasting AutoElicit with in-context
learning and demonstrate how to perform model selection between the two
methods. We find that AutoElicit yields priors that can substantially reduce
error over uninformative priors, using fewer labels, and consistently
outperform in-context learning. We show that AutoElicit saves over 6 months of
labelling effort when building a new predictive model for urinary tract
infections from sensor recordings of people living with dementia.

</details>


### [699] [Non-Markovian Discrete Diffusion with Causal Language Models](https://arxiv.org/pdf/2502.09767)
*Yangtian Zhang, Sizhuang He, Daniel Levine, Lawrence Zhao, David Zhang, Syed A Rizvi, Emanuele Zappala, Rex Ying, David van Dijk*

Main category: cs.LG

TL;DR: CaDDi is a non-Markovian discrete diffusion model that conditions on the entire generative trajectory, outperforming existing diffusion models and narrowing the gap to autoregressive transformers.


<details>
  <summary>Details</summary>
Motivation: Current discrete diffusion models are limited by the Markovian assumption, leading to error accumulation. CaDDi aims to overcome this by leveraging the full generative trajectory.

Method: CaDDi unifies sequential and temporal reasoning in a non-Markovian transformer, enabling reuse of pretrained LLM weights without architectural changes.

Result: CaDDi outperforms state-of-the-art discrete diffusion models on natural-language benchmarks.

Conclusion: CaDDi lifts the Markov constraint, improves performance, and bridges the gap to autoregressive models.

Abstract: Discrete diffusion models offer a flexible, controllable approach to
structured sequence generation, yet they still lag behind causal language
models in expressive power. A key limitation lies in their reliance on the
Markovian assumption, which restricts each step to condition only on the
current state, leading to potential uncorrectable error accumulation. In this
paper, we introduce CaDDi, a discrete diffusion model that conditions on the
entire generative trajectory, thereby lifting the Markov constraint and
allowing the model to revisit and improve past states. By unifying sequential
(causal) and temporal (diffusion) reasoning in a single non-Markovian
transformer, CaDDi also treats standard causal language models as a special
case and permits the direct reuse of pretrained LLM weights with no
architectural changes. Empirically, CaDDi outperforms state-of-the-art discrete
diffusion baselines on natural-language benchmarks, substantially narrowing the
remaining gap to large autoregressive transformers.

</details>


### [700] [Multi-Label Bayesian Active Learning with Inter-Label Relationships](https://arxiv.org/pdf/2411.17941)
*Yuanyuan Qi, Jueqing Lu, Xiaohao Yang, Joanne Enticott, Lan Du*

Main category: cs.LG

TL;DR: A novel multi-label active learning strategy addresses label correlation and data imbalance using updated correlation matrices and ensemble pseudo labeling, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing multi-label active learning methods either lack computational efficiency or fail to fully exploit label dependencies, while also struggling with imbalanced data.

Method: The proposed method uses progressively updated correlation matrices to capture label relationships and employs ensemble pseudo labeling with beta scoring to handle data imbalances.

Result: Experiments on four datasets show the strategy achieves more reliable and superior performance compared to established methods.

Conclusion: The proposed approach effectively addresses label correlation and data imbalance, offering a robust solution for multi-label active learning.

Abstract: The primary challenge of multi-label active learning, differing it from
multi-class active learning, lies in assessing the informativeness of an
indefinite number of labels while also accounting for the inherited label
correlation. Existing studies either require substantial computational
resources to leverage correlations or fail to fully explore label dependencies.
Additionally, real-world scenarios often require addressing intrinsic biases
stemming from imbalanced data distributions. In this paper, we propose a new
multi-label active learning strategy to address both challenges. Our method
incorporates progressively updated positive and negative correlation matrices
to capture co-occurrence and disjoint relationships within the label space of
annotated samples, enabling a holistic assessment of uncertainty rather than
treating labels as isolated elements. Furthermore, alongside diversity, our
model employs ensemble pseudo labeling and beta scoring rules to address data
imbalances. Extensive experiments on four realistic datasets demonstrate that
our strategy consistently achieves more reliable and superior performance,
compared to several established methods.

</details>


### [701] [Go With the Flow: Fast Diffusion for Gaussian Mixture Models](https://arxiv.org/pdf/2412.09059)
*George Rapakoulias, Ali Reza Pedram, Fengjiao Liu, Lingjiong Zhu, Panagiotis Tsiotras*

Main category: cs.LG

TL;DR: The paper proposes an efficient method for computing Schrodinger Bridges (SBs) between Gaussian Mixture Models (GMMs) using a low-dimensional linear program, avoiding costly training schemes.


<details>
  <summary>Details</summary>
Motivation: Existing methods for SBs are computationally expensive, even for low-dimensional problems. The authors aim to provide a more efficient solution.

Method: An analytic parametrization of feasible policies for steering distributions between GMMs is introduced, solved via a low-dimensional linear program.

Result: The method efficiently solves multi-marginal momentum SB problems and outperforms state-of-the-art methods in certain cases, requiring minimal training.

Conclusion: The approach is scalable and effective for low-to-moderate dimensional problems, with applications in image translation, cellular dynamics, and more.

Abstract: Schrodinger Bridges (SBs) are diffusion processes that steer, in finite time,
a given initial distribution to another final one while minimizing a suitable
cost functional. Although various methods for computing SBs have recently been
proposed in the literature, most of these approaches require computationally
expensive training schemes, even for solving low-dimensional problems. In this
work, we propose an analytic parametrization of a set of feasible policies for
steering the distribution of a dynamical system from one Gaussian Mixture Model
(GMM) to another. Instead of relying on standard non-convex optimization
techniques, the optimal policy within the set can be approximated as the
solution of a low-dimensional linear program whose dimension scales linearly
with the number of components in each mixture. The proposed method generalizes
naturally to more general classes of dynamical systems, such as controllable
linear time-varying systems, enabling efficient solutions to multi-marginal
momentum SB between GMMs, a challenging distribution interpolation problem. We
showcase the potential of this approach in low-to-moderate dimensional problems
such as image-to-image translation in the latent space of an autoencoder,
learning of cellular dynamics using multi-marginal momentum SB problems, and
various other examples. We also test our approach on an Entropic Optimal
Transport (EOT) benchmark problem and show that it outperforms state-of-the-art
methods in cases where the boundary distributions are mixture models while
requiring virtually no training.

</details>


### [702] [Revisiting Weak-to-Strong Generalization in Theory and Practice: Reverse KL vs. Forward KL](https://arxiv.org/pdf/2502.11107)
*Wei Yao, Wenkai Yang, Ziqiao Wang, Yankai Lin, Yong Liu*

Main category: cs.LG

TL;DR: The paper proposes using reverse KL divergence over forward KL divergence to improve weak-to-strong generalization in language models, mitigating noise from weak predictions. Theoretical and empirical results show reverse KL's superiority.


<details>
  <summary>Details</summary>
Motivation: Ensuring alignment of advanced language models with human values is challenging. Weak-to-strong generalization is promising but limited by noise in weak predictions.

Method: Replace forward KL divergence with reverse KL divergence to prioritize high-confidence predictions and reduce unreliable weak supervision. Theoretical bounds are extended and tightened.

Result: Reverse KL ensures strong models outperform weak supervisors, especially when fine-tuning the last linear layer. Empirical tests show reverse KL's practical advantages.

Conclusion: Reverse KL divergence and reverse cross-entropy are more effective than forward KL, offering better performance in weak-to-strong generalization.

Abstract: As large language models advance toward superhuman performance, ensuring
their alignment with human values and abilities grows increasingly complex.
Weak-to-strong generalization offers a promising approach by leveraging
predictions from weaker models to guide stronger systems, but its effectiveness
could be constrained by the inherent noise and inaccuracies in these weak
predictions. To address this, we propose a theoretically grounded approach that
replaces forward KL divergence-whose mass-covering behavior risks overfitting
to imperfect weak signals-with reverse KL divergence. Reverse KL divergence's
zero-forcing effect prioritizes high-confidence predictions, effectively
mitigating the influence of unreliable weak supervision. Theoretically, we
extend existing bounds and derive tighter lower bounds for both forward and
reverse KL divergence, establishing that reverse KL achieves at least
comparable guarantees to forward KL. Notably, when a sufficiently pre-trained
strong model is fine-tuned on the last linear layer, reverse KL guarantees that
it outperforms its weak supervisor by the magnitude of their disagreement.
Empirically, we demonstrate that reverse KL and reverse cross-entropy enable
strong models to successfully outperform those trained with forward KL and
standard cross-entropy across most settings, highlighting the practical
advantages of these reverse losses.

</details>


### [703] [Simple Guidance Mechanisms for Discrete Diffusion Models](https://arxiv.org/pdf/2412.10193)
*Yair Schiff, Subham Sekhar Sahoo, Hao Phung, Guanghan Wang, Sam Boshar, Hugo Dalla-torre, Bernardo P. de Almeida, Alexander Rush, Thomas Pierrot, Volodymyr Kuleshov*

Main category: cs.LG

TL;DR: The paper introduces guidance methods for discrete diffusion models, leveraging uniform noise for better controllability and performance.


<details>
  <summary>Details</summary>
Motivation: Existing continuous guidance methods don't apply to discrete diffusion, limiting controllable generation.

Method: Derives classifier-free and classifier-based guidance for discrete diffusion, introduces uniform noise models, and proposes a continuous-time variational lower bound.

Result: Achieves state-of-the-art performance in guided and fast generation tasks across genomic sequences, molecule design, and discretized images.

Conclusion: The proposed methods enhance controllable discrete diffusion, outperforming autoregressive and diffusion baselines.

Abstract: Diffusion models for continuous data gained widespread adoption owing to
their high quality generation and control mechanisms. However, controllable
diffusion on discrete data faces challenges given that continuous guidance
methods do not directly apply to discrete diffusion. Here, we provide a
straightforward derivation of classifier-free and classifier-based guidance for
discrete diffusion, as well as a new class of diffusion models that leverage
uniform noise and that are more guidable because they can continuously edit
their outputs. We improve the quality of these models with a novel
continuous-time variational lower bound that yields state-of-the-art
performance, especially in settings involving guidance or fast generation.
Empirically, we demonstrate that our guidance mechanisms combined with uniform
noise diffusion improve controllable generation relative to autoregressive and
diffusion baselines on several discrete data domains, including genomic
sequences, small molecule design, and discretized image generation.

</details>


### [704] [MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections](https://arxiv.org/pdf/2502.12170)
*Da Xiao, Qingye Meng, Shengping Li, Xingyuan Yuan*

Main category: cs.LG

TL;DR: MUDD connections enhance Transformer performance by dynamically generating connection weights, outperforming standard Transformers with minimal added cost.


<details>
  <summary>Details</summary>
Motivation: To address limitations of static residual connections in Transformers and improve cross-layer information flow.

Method: Proposes MUDD connections, which dynamically generate weights based on hidden states and decoupled input streams (query, key, value, residual).

Result: MUDDFormer outperforms Transformers, achieving performance equivalent to 1.8X-2.4X compute, with minimal parameter and computation overhead.

Conclusion: MUDD connections are a simple, effective enhancement for Transformers, offering significant performance gains with negligible cost.

Abstract: We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective
method to address the limitations of residual connections and enhance
cross-layer information flow in Transformers. Unlike existing dense connection
approaches with static and shared connection weights, MUDD generates connection
weights dynamically depending on hidden states at each sequence position and
for each decoupled input stream (the query, key, value or residual) of a
Transformer block. MUDD connections can be seamlessly integrated into any
Transformer architecture to create MUDDFormer. Extensive experiments show that
MUDDFormer significantly outperforms Transformers across various model
architectures and scales in language modeling, achieving the performance of
Transformers trained with 1.8X-2.4X compute. Notably, MUDDPythia-2.8B matches
Pythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B
in five-shot settings, while adding only 0.23% parameters and 0.4% computation.
Code in JAX and PyTorch and pre-trained models are available at
https://github.com/Caiyun-AI/MUDDFormer .

</details>


### [705] [GNNs-to-MLPs by Teacher Injection and Dirichlet Energy Distillation](https://arxiv.org/pdf/2412.11180)
*Ziang Zhou, Zhihao Ding, Jieming Shi, Qing Li, Shiqi Shen*

Main category: cs.LG

TL;DR: TINED distills GNNs into MLPs layer-by-layer using Teacher Injection and Dirichlet Energy Distillation, improving scalability and performance.


<details>
  <summary>Details</summary>
Motivation: GNNs face scalability issues due to multi-hop data needs during inference, limiting latency-sensitive applications. Existing distillation methods underutilize GNN layer-level insights.

Method: TINED transfers GNN layer operations (feature transformation and graph propagation) to MLP layers, using theoretical bounds and Dirichlet energy to measure smoothing effects.

Result: TINED outperforms GNNs and other distillation methods across seven datasets.

Conclusion: TINED effectively addresses GNN scalability by distilling layer-level insights into MLPs, enhancing performance and inference speed.

Abstract: Graph Neural Networks (GNNs) are pivotal in graph-based learning,
particularly excelling in node classification. However, their scalability is
hindered by the need for multi-hop data during inference, limiting their
application in latency-sensitive scenarios. Recent efforts to distill GNNs into
multi-layer perceptrons (MLPs) for faster inference often underutilize the
layer-level insights of GNNs. In this paper, we present TINED, a novel approach
that distills GNNs to MLPs on a layer-by-layer basis using Teacher Injection
and Dirichlet Energy Distillation techniques. We focus on two key operations in
GNN layers: feature transformation (FT) and graph propagation (GP). We
recognize that FT is computationally equivalent to a fully-connected (FC) layer
in MLPs. Thus, we propose directly transferring teacher parameters from an FT
in a GNN to an FC layer in the student MLP, enhanced by fine-tuning. In TINED,
the FC layers in an MLP replicate the sequence of FTs and GPs in the GNN. We
also establish a theoretical bound for GP approximation. Furthermore, we note
that FT and GP operations in GNN layers often exhibit opposing smoothing
effects: GP is aggressive, while FT is conservative. Using Dirichlet energy, we
develop a DE ratio to measure these effects and propose Dirichlet Energy
Distillation to convey these characteristics from GNN layers to MLP layers.
Extensive experiments show that TINED outperforms GNNs and leading distillation
methods across various settings and seven datasets. Source code are available
at https://github.com/scottjiao/TINED_ICML25/.

</details>


### [706] [ReQFlow: Rectified Quaternion Flow for Efficient and High-Quality Protein Backbone Generation](https://arxiv.org/pdf/2502.14637)
*Angxiao Yue, Zichong Wang, Hongteng Xu*

Main category: cs.LG

TL;DR: ReQFlow is a novel method for fast and high-quality protein backbone generation, outperforming existing models in speed and designability.


<details>
  <summary>Details</summary>
Motivation: Protein backbone generation is crucial for de novo protein design but current methods are slow and produce low-designability results.

Method: ReQFlow uses rectified quaternion flow matching, representing 3D rotations as unit quaternions and training with spherical linear interpolation for stability.

Result: ReQFlow matches performance of existing models while being significantly faster (e.g., 37x faster than RFDiffusion).

Conclusion: ReQFlow is an efficient and effective solution for protein backbone generation, with open-source code available.

Abstract: Protein backbone generation plays a central role in de novo protein design
and is significant for many biological and medical applications. Although
diffusion and flow-based generative models provide potential solutions to this
challenging task, they often generate proteins with undesired designability and
suffer computational inefficiency. In this study, we propose a novel rectified
quaternion flow (ReQFlow) matching method for fast and high-quality protein
backbone generation. In particular, our method generates a local translation
and a 3D rotation from random noise for each residue in a protein chain, which
represents each 3D rotation as a unit quaternion and constructs its flow by
spherical linear interpolation (SLERP) in an exponential format. We train the
model by quaternion flow (QFlow) matching with guaranteed numerical stability
and rectify the QFlow model to accelerate its inference and improve the
designability of generated protein backbones, leading to the proposed ReQFlow
model. Experiments show that ReQFlow achieves on-par performance in protein
backbone generation while requiring much fewer sampling steps and significantly
less inference time (e.g., being 37x faster than RFDiffusion and 63x faster
than Genie2 when generating a backbone of length 300), demonstrating its
effectiveness and efficiency. The code is available at
https://github.com/AngxiaoYue/ReQFlow.

</details>


### [707] [Efficient Logit-based Knowledge Distillation of Deep Spiking Neural Networks for Full-Range Timestep Deployment](https://arxiv.org/pdf/2501.15925)
*Chengting Yu, Xiaochen Zhao, Lei Liu, Shu Yang, Gaoang Wang, Erping Li, Aili Wang*

Main category: cs.LG

TL;DR: A novel distillation framework for Spiking Neural Networks (SNNs) optimizes performance across full-range timesteps without retraining, improving accuracy and deployment flexibility.


<details>
  <summary>Details</summary>
Motivation: SNNs face accuracy degradation and inflexibility due to fixed inference timesteps, requiring retraining for adjustments.

Method: Proposes a spatio-temporal distillation framework for deep SNNs, ensuring convergence across all timesteps without retraining.

Result: Achieves state-of-the-art performance on CIFAR-10, CIFAR-100, CIFAR10-DVS, and ImageNet datasets.

Conclusion: The framework enhances SNN efficacy and adaptability, supported by theoretical and empirical validation.

Abstract: Spiking Neural Networks (SNNs) are emerging as a brain-inspired alternative
to traditional Artificial Neural Networks (ANNs), prized for their potential
energy efficiency on neuromorphic hardware. Despite this, SNNs often suffer
from accuracy degradation compared to ANNs and face deployment challenges due
to fixed inference timesteps, which require retraining for adjustments,
limiting operational flexibility. To address these issues, our work considers
the spatio-temporal property inherent in SNNs, and proposes a novel
distillation framework for deep SNNs that optimizes performance across
full-range timesteps without specific retraining, enhancing both efficacy and
deployment adaptability. We provide both theoretical analysis and empirical
validations to illustrate that training guarantees the convergence of all
implicit models across full-range timesteps. Experimental results on CIFAR-10,
CIFAR-100, CIFAR10-DVS, and ImageNet demonstrate state-of-the-art performance
among distillation-based SNNs training methods. Our code is available at
https://github.com/Intelli-Chip-Lab/snn\_temporal\_decoupling\_distillation.

</details>


### [708] [BatteryLife: A Comprehensive Dataset and Benchmark for Battery Life Prediction](https://arxiv.org/pdf/2502.18807)
*Ruifeng Tan, Weixiang Hong, Jiayue Tang, Xibin Lu, Ruijun Ma, Xiang Zheng, Jia Li, Jiaqiang Huang, Tong-Yi Zhang*

Main category: cs.LG

TL;DR: The paper introduces BatteryLife, a comprehensive dataset and benchmark for Battery Life Prediction (BLP), addressing challenges like limited dataset size, lack of diversity, and inconsistent benchmarks. It also proposes CyclePatch, a plug-in technique to improve model performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome key challenges in BLP research: limited dataset size, lack of diversity in battery types and testing conditions, and inconsistent benchmarks.

Method: The authors propose BatteryLife, integrating 16 datasets with diverse battery types and conditions, and introduce CyclePatch, a plug-in technique for neural networks.

Result: BatteryLife offers a 2.5 times larger sample size and greater diversity. CyclePatch improves model performance, establishing state-of-the-art benchmarks.

Conclusion: BatteryLife provides a robust resource for BLP, and CyclePatch enhances model effectiveness, addressing previous limitations in the field.

Abstract: Battery Life Prediction (BLP), which relies on time series data produced by
battery degradation tests, is crucial for battery utilization, optimization,
and production. Despite impressive advancements, this research area faces three
key challenges. Firstly, the limited size of existing datasets impedes insights
into modern battery life data. Secondly, most datasets are restricted to
small-capacity lithium-ion batteries tested under a narrow range of diversity
in labs, raising concerns about the generalizability of findings. Thirdly,
inconsistent and limited benchmarks across studies obscure the effectiveness of
baselines and leave it unclear if models popular in other time series fields
are effective for BLP. To address these challenges, we propose BatteryLife, a
comprehensive dataset and benchmark for BLP. BatteryLife integrates 16
datasets, offering a 2.5 times sample size compared to the previous largest
dataset, and provides the most diverse battery life resource with batteries
from 8 formats, 59 chemical systems, 9 operating temperatures, and 421
charge/discharge protocols, including both laboratory and industrial tests.
Notably, BatteryLife is the first to release battery life datasets of zinc-ion
batteries, sodium-ion batteries, and industry-tested large-capacity lithium-ion
batteries. With the comprehensive dataset, we revisit the effectiveness of
baselines popular in this and other time series fields. Furthermore, we propose
CyclePatch, a plug-in technique that can be employed in various neural
networks. Extensive benchmarking of 18 methods reveals that models popular in
other time series fields can be unsuitable for BLP, and CyclePatch consistently
improves model performance establishing state-of-the-art benchmarks. Moreover,
BatteryLife evaluates model performance across aging conditions and domains.
BatteryLife is available at https://github.com/Ruifeng-Tan/BatteryLife.

</details>


### [709] [Training Dynamics of In-Context Learning in Linear Attention](https://arxiv.org/pdf/2501.16265)
*Yedi Zhang, Aaditya K. Singh, Peter E. Latham, Andrew Saxe*

Main category: cs.LG

TL;DR: The paper investigates gradient descent dynamics in multi-head linear self-attention for in-context learning (ICL), comparing merged and separate key-query parametrizations.


<details>
  <summary>Details</summary>
Motivation: To understand how attention-based models acquire ICL abilities through gradient descent training, focusing on linear regression tasks.

Method: Analyzes training dynamics of two parametrizations (merged and separate key-query) for linear self-attention, deriving analytical solutions and reducing dynamics to scalar ODEs.

Result: Merged parametrization shows two fixed points and abrupt loss drop, while separate parametrization exhibits exponentially many fixed points and saddle-to-saddle dynamics, implementing principal component regression.

Conclusion: ICL abilities evolve differently based on key-query parametrization: abruptly for merged and progressively for separate, providing theoretical insights into training dynamics.

Abstract: While attention-based models have demonstrated the remarkable ability of
in-context learning (ICL), the theoretical understanding of how these models
acquired this ability through gradient descent training is still preliminary.
Towards answering this question, we study the gradient descent dynamics of
multi-head linear self-attention trained for in-context linear regression. We
examine two parametrizations of linear self-attention: one with the key and
query weights merged as a single matrix (common in theoretical studies), and
one with separate key and query matrices (closer to practical settings). For
the merged parametrization, we show that the training dynamics has two fixed
points and the loss trajectory exhibits a single, abrupt drop. We derive an
analytical time-course solution for a certain class of datasets and
initialization. For the separate parametrization, we show that the training
dynamics has exponentially many fixed points and the loss exhibits
saddle-to-saddle dynamics, which we reduce to scalar ordinary differential
equations. During training, the model implements principal component regression
in context with the number of principal components increasing over training
time. Overall, we provide a theoretical description of how ICL abilities evolve
during gradient descent training of linear attention, revealing abrupt
acquisition or progressive improvements depending on how the key and query are
parametrized.

</details>


### [710] [A Variational Perspective on Generative Protein Fitness Optimization](https://arxiv.org/pdf/2501.19200)
*Lea Bogensperger, Dominik Narnhofer, Ahmed Allam, Konrad Schindler, Michael Krauthammer*

Main category: cs.LG

TL;DR: VLGPO is a variational method for protein fitness optimization, embedding sequences in a continuous latent space for efficient sampling and achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Protein fitness optimization is challenging due to vast search spaces and sparse fitness landscapes, requiring gradient-based methods.

Method: VLGPO uses a variational approach with a continuous latent space, combining a flow matching prior and fitness predictor to guide optimization.

Result: State-of-the-art performance on two protein benchmarks, demonstrating effectiveness.

Conclusion: VLGPO's flexible framework suits various protein design tasks, offering efficient optimization.

Abstract: The goal of protein fitness optimization is to discover new protein variants
with enhanced fitness for a given use. The vast search space and the sparsely
populated fitness landscape, along with the discrete nature of protein
sequences, pose significant challenges when trying to determine the gradient
towards configurations with higher fitness. We introduce Variational Latent
Generative Protein Optimization (VLGPO), a variational perspective on fitness
optimization. Our method embeds protein sequences in a continuous latent space
to enable efficient sampling from the fitness distribution and combines a
(learned) flow matching prior over sequence mutations with a fitness predictor
to guide optimization towards sequences with high fitness. VLGPO achieves
state-of-the-art results on two different protein benchmarks of varying
complexity. Moreover, the variational design with explicit prior and likelihood
functions offers a flexible plug-and-play framework that can be easily
customized to suit various protein design tasks.

</details>


### [711] [Wanda++: Pruning Large Language Models via Regional Gradients](https://arxiv.org/pdf/2503.04992)
*Yifan Yang, Kai Zhen, Bhavana Ganesh, Aram Galstyan, Goeric Huybrechts, Markus Müller, Jonas M. Kübler, Rupak Vignesh Swaminathan, Athanasios Mouchtaris, Sravan Babu Bodapati, Nathan Susanj, Zheng Zhang, Jack FitzGerald, Abhishek Kumar*

Main category: cs.LG

TL;DR: Wanda++ is a novel pruning framework for LLMs that uses regional gradients to improve pruning accuracy and efficiency, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods for LLMs often degrade accuracy without full-model fine-tuning, prompting the need for a better approach.

Method: Wanda++ leverages decoder-block-level regional gradients and proposes an efficient regional optimization method to minimize output discrepancies between dense and sparse models.

Result: Wanda++ reduces perplexity by up to 32% over Wanda, generalizes well to downstream tasks, and prunes a 7B LLaMA model in under 10 minutes on a single GPU.

Conclusion: Wanda++ is a lightweight, effective pruning framework that enhances accuracy and speed, and remains compatible with sparsity-aware fine-tuning.

Abstract: Large Language Models (LLMs) pruning seeks to remove unimportant weights for
inference speedup with minimal accuracy impact. However, existing methods often
suffer from accuracy degradation without full-model sparsity-aware fine-tuning.
This paper presents Wanda++, a novel pruning framework that outperforms the
state-of-the-art methods by utilizing decoder-block-level \textbf{regional}
gradients. Specifically, Wanda++ improves the pruning score with regional
gradients for the first time and proposes an efficient regional optimization
method to minimize pruning-induced output discrepancies between the dense and
sparse decoder output. Notably, Wanda++ improves perplexity by up to 32\% over
Wanda in the language modeling task and generalizes effectively to downstream
tasks. Moreover, despite updating weights with regional optimization, Wanda++
remains orthogonal to sparsity-aware fine-tuning, further reducing perplexity
with LoRA in great extend. Our approach is lightweight, pruning a 7B LLaMA
model in under 10 minutes on a single H100 GPU.

</details>


### [712] [PUATE: Efficient Average Treatment Effect Estimation from Treated (Positive) and Unlabeled Units](https://arxiv.org/pdf/2501.19345)
*Masahiro Kato, Fumiaki Kozai, Ryo Inokuchi*

Main category: cs.LG

TL;DR: Semiparametric efficient estimators for ATE in a setting with treatment and unlabeled groups, addressing missing data and PU learning.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of estimating ATEs when only a treatment group and an unlabeled group (with unknown treatment status) are observed, a scenario akin to PU learning.

Method: Derives semiparametric efficiency bounds and constructs efficient ATE estimators that achieve these bounds.

Result: Developed estimators attain the lowest achievable asymptotic variance, contributing to causal inference with missing data.

Conclusion: The study advances causal inference and weakly supervised learning by providing efficient solutions for ATE estimation in incomplete data settings.

Abstract: The estimation of average treatment effects (ATEs), defined as the difference
in expected outcomes between treatment and control groups, is a central topic
in causal inference. This study develops semiparametric efficient estimators
for ATE in a setting where only a treatment group and an unlabeled group,
consisting of units whose treatment status is unknown, are observed. This
scenario constitutes a variant of learning from positive and unlabeled data (PU
learning) and can be viewed as a special case of ATE estimation with missing
data. For this setting, we derive the semiparametric efficiency bounds, which
characterize the lowest achievable asymptotic variance for regular estimators.
We then construct semiparametric efficient ATE estimators that attain these
bounds. Our results contribute to the literature on causal inference with
missing data and weakly supervised learning.

</details>


### [713] [Efficient Online Reinforcement Learning for Diffusion Policy](https://arxiv.org/pdf/2502.00361)
*Haitong Ma, Tianyi Chen, Kai Wang, Na Li, Bo Dai*

Main category: cs.LG

TL;DR: The paper introduces Reweighted Score Matching (RSM) to efficiently train diffusion policies in online RL, eliminating the need for target distribution sampling and enabling value function optimization. Two algorithms, DPMD and SDAC, outperform existing methods on MuJoCo benchmarks.


<details>
  <summary>Details</summary>
Motivation: Conventional diffusion training in online RL is impractical due to the inability to sample from the optimal policy and high computational costs. The paper aims to address these limitations.

Method: The authors generalize denoising score matching by reweighting the loss function (RSM), introducing two algorithms: DPMD for policy mirror descent and SDAC for max-entropy policy.

Result: DPMD and SDAC outperform recent diffusion-policy online RL methods on MuJoCo tasks, with DPMD improving over soft actor-critic by over 120% on Humanoid and Ant.

Conclusion: RSM enables efficient diffusion policy training in online RL, with DPMD and SDAC demonstrating superior performance, making them scalable and practical solutions.

Abstract: Diffusion policies have achieved superior performance in imitation learning
and offline reinforcement learning (RL) due to their rich expressiveness.
However, the conventional diffusion training procedure requires samples from
target distribution, which is impossible in online RL since we cannot sample
from the optimal policy. Backpropagating policy gradient through the diffusion
process incurs huge computational costs and instability, thus being expensive
and not scalable. To enable efficient training of diffusion policies in online
RL, we generalize the conventional denoising score matching by reweighting the
loss function. The resulting Reweighted Score Matching (RSM) preserves the
optimal solution and low computational cost of denoising score matching, while
eliminating the need to sample from the target distribution and allowing
learning to optimize value functions. We introduce two tractable reweighted
loss functions to solve two commonly used policy optimization problems, policy
mirror descent and max-entropy policy, resulting in two practical algorithms
named Diffusion Policy Mirror Descent (DPMD) and Soft Diffusion Actor-Critic
(SDAC). We conducted comprehensive comparisons on MuJoCo benchmarks. The
empirical results show that the proposed algorithms outperform recent
diffusion-policy online RLs on most tasks, and the DPMD improves more than 120%
over soft actor-critic on Humanoid and Ant.

</details>


### [714] [TS-RAG: Retrieval-Augmented Generation based Time Series Foundation Models are Stronger Zero-Shot Forecaster](https://arxiv.org/pdf/2503.07649)
*Kanghui Ning, Zijie Pan, Yu Liu, Yushan Jiang, James Y. Zhang, Kashif Rasul, Anderson Schneider, Lintao Ma, Yuriy Nevmyvaka, Dongjin Song*

Main category: cs.LG

TL;DR: TS-RAG is a retrieval-augmented framework for time series forecasting, improving generalization and interpretability of Time Series Foundation Models (TSFMs) without task-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing TSFMs struggle with non-stationary dynamics and distribution shifts, lacking effective adaptation mechanisms.

Method: TS-RAG retrieves relevant time series segments using pre-trained encoders and dynamically fuses them with TSFM representations via an Adaptive Retrieval Mixer (ARM).

Result: TS-RAG achieves state-of-the-art zero-shot performance, outperforming existing TSFMs by up to 6.84% across diverse datasets.

Conclusion: TS-RAG enhances forecasting accuracy and interpretability, addressing key limitations of current TSFMs.

Abstract: Large Language Models (LLMs) and Foundation Models (FMs) have recently become
prevalent for time series forecasting tasks. While fine-tuning LLMs enables
domain adaptation, they often struggle to generalize across diverse and unseen
datasets. Moreover, existing Time Series Foundation Models (TSFMs) still face
challenges in handling non-stationary dynamics and distribution shifts, largely
due to the lack of effective mechanisms for adaptation. To this end, we present
TS-RAG, a retrieval-augmented generation framework for time series forecasting
that enhances the generalization and interpretability of TSFMs. Specifically,
TS-RAG leverages pre-trained time series encoders to retrieve semantically
relevant segments from a dedicated knowledge base, enriching the contextual
representation of the input query. Furthermore, we propose an Adaptive
Retrieval Mixer (ARM) module that dynamically fuses the retrieved patterns with
the TSFM's internal representation, improving forecasting accuracy without
requiring task-specific fine-tuning. Thorough empirical studies on seven public
benchmark datasets demonstrate that TS-RAG achieves state-of-the-art zero-shot
forecasting performance, outperforming the existing TSFMs by up to 6.84% across
diverse domains while also providing desirable interpretability.

</details>


### [715] [Memento No More: Coaching AI Agents to Master Multiple Tasks via Hints Internalization](https://arxiv.org/pdf/2502.01562)
*Minttu Alakuijala, Ya Gao, Georgy Ananov, Samuel Kaski, Pekka Marttinen, Alexander Ilin, Harri Valpola*

Main category: cs.LG

TL;DR: Proposes a method for AI agents to learn multiple tasks without relying on prompts or demonstration data, using iterative feedback and context distillation.


<details>
  <summary>Details</summary>
Motivation: Current AI agents rely on prompts for task knowledge, which is inefficient and unsustainable.

Method: Uses iterative feedback from humans and context distillation to integrate knowledge into the agent's weights.

Result: The Llama-3-based agent outperforms GPT-4o and DeepSeek-V3 in tasks requiring sequencing of information retrieval, tool use, and question answering.

Conclusion: The approach enables AI agents to internalize knowledge effectively, improving performance without extensive prompts or demonstrations.

Abstract: As the general capabilities of artificial intelligence (AI) agents continue
to evolve, their ability to learn to master multiple complex tasks through
experience remains a key challenge. Current LLM agents, particularly those
based on proprietary language models, typically rely on prompts to incorporate
knowledge about the target tasks. This approach does not allow the agent to
internalize this information and instead relies on ever-expanding prompts to
sustain its functionality in diverse scenarios. This resembles a system of
notes used by a person affected by anterograde amnesia, the inability to form
new memories. In this paper, we propose a novel method to train AI agents to
incorporate knowledge and skills for multiple tasks without the need for either
cumbersome note systems or prior high-quality demonstration data. Our approach
employs an iterative process where the agent collects new experiences, receives
corrective feedback from humans in the form of hints, and integrates this
feedback into its weights via a context distillation training procedure. We
demonstrate the efficacy of our approach by implementing it in a Llama-3-based
agent that, after only a few rounds of feedback, outperforms advanced models
GPT-4o and DeepSeek-V3 in tasksets requiring correct sequencing of information
retrieval, tool use, and question answering.

</details>


### [716] [Language-Enhanced Representation Learning for Single-Cell Transcriptomics](https://arxiv.org/pdf/2503.09427)
*Yaorui Shi, Jiaqi Yang, Changhao Nai, Sihang Li, Junfeng Fang, Xiang Wang, Zhiyuan Liu, Yang Zhang*

Main category: cs.LG

TL;DR: scMMGPT is a multimodal framework for single-cell transcriptomics, combining transcriptomic data with textual knowledge to outperform existing methods in tasks like cell annotation and clustering.


<details>
  <summary>Details</summary>
Motivation: Existing scLLMs focus only on transcriptomic data, ignoring valuable textual biological knowledge, limiting their effectiveness.

Method: scMMGPT uses robust cell representation extraction and a two-stage pre-training strategy, blending discriminative and generative approaches.

Result: scMMGPT outperforms unimodal and multimodal baselines in downstream tasks and generalizes better in out-of-distribution scenarios.

Conclusion: scMMGPT's multimodal approach enhances single-cell representation learning, proving more effective than existing methods.

Abstract: Single-cell RNA sequencing (scRNA-seq) offers detailed insights into cellular
heterogeneity. Recent advancements leverage single-cell large language models
(scLLMs) for effective representation learning. These models focus exclusively
on transcriptomic data, neglecting complementary biological knowledge from
textual descriptions. To overcome this limitation, we propose scMMGPT, a novel
multimodal framework designed for language-enhanced representation learning in
single-cell transcriptomics. Unlike existing methods, scMMGPT employs robust
cell representation extraction, preserving quantitative gene expression data,
and introduces an innovative two-stage pre-training strategy combining
discriminative precision with generative flexibility. Extensive experiments
demonstrate that scMMGPT significantly outperforms unimodal and multimodal
baselines across key downstream tasks, including cell annotation and
clustering, and exhibits superior generalization in out-of-distribution
scenarios.

</details>


### [717] [BILBO: BILevel Bayesian Optimization](https://arxiv.org/pdf/2502.02121)
*Ruth Wan Theng Chew, Quoc Phong Nguyen, Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: BILBO is a Bayesian optimization algorithm for bilevel problems, avoiding repeated lower-level optimizations by sampling from trusted sets and selecting one query per iteration. It guarantees sublinear regret and performs well empirically.


<details>
  <summary>Details</summary>
Motivation: Bilevel optimization is common but challenging due to noisy, constrained, and derivative-free settings, requiring inefficient repeated lower-level optimizations.

Method: BILBO simultaneously optimizes upper- and lower-level problems using confidence-bounds based trusted sets and a single function query per iteration with exploration strategies.

Result: Theoretical sublinear regret bound is proven, and empirical evaluations show strong performance on synthetic and real-world problems.

Conclusion: BILBO effectively addresses bilevel optimization challenges, offering a sample-efficient and theoretically sound solution.

Abstract: Bilevel optimization is characterized by a two-level optimization structure,
where the upper-level problem is constrained by optimal lower-level solutions,
and such structures are prevalent in real-world problems. The constraint by
optimal lower-level solutions poses significant challenges, especially in
noisy, constrained, and derivative-free settings, as repeating lower-level
optimizations is sample inefficient and predicted lower-level solutions may be
suboptimal. We present BILevel Bayesian Optimization (BILBO), a novel Bayesian
optimization algorithm for general bilevel problems with blackbox functions,
which optimizes both upper- and lower-level problems simultaneously, without
the repeated lower-level optimization required by existing methods. BILBO
samples from confidence-bounds based trusted sets, which bounds the
suboptimality on the lower level. Moreover, BILBO selects only one function
query per iteration, where the function query selection strategy incorporates
the uncertainty of estimated lower-level solutions and includes a conditional
reassignment of the query to encourage exploration of the lower-level
objective. The performance of BILBO is theoretically guaranteed with a
sublinear regret bound for commonly used kernels and is empirically evaluated
on several synthetic and real-world problems.

</details>


### [718] [Improving User Behavior Prediction: Leveraging Annotator Metadata in Supervised Machine Learning Models](https://arxiv.org/pdf/2503.21000)
*Lynnette Hui Xian Ng, Kokil Jaidka, Kaiyuan Tay, Hansin Ahuja, Niyati Chhaya*

Main category: cs.LG

TL;DR: MSWEEM improves user behavior prediction by integrating annotator meta-features, outperforming standard models by 14% and highlighting the importance of annotator behavior and qualifications.


<details>
  <summary>Details</summary>
Motivation: Supervised models struggle due to poor label quality and NLP task accuracy, prompting the need for a metadata-sensitive approach.

Method: Developed MSWEEM, which incorporates annotator meta-features like fatigue and speeding to enhance predictions.

Result: MSWEEM outperforms standard ensembles by 14% on held-out data and 12% on an alternative dataset, with annotator behavior and qualifications significantly boosting performance.

Conclusion: Understanding annotator patterns is crucial for improving model accuracy in user behavior prediction.

Abstract: Supervised machine-learning models often underperform in predicting user
behaviors from conversational text, hindered by poor crowdsourced label quality
and low NLP task accuracy. We introduce the Metadata-Sensitive
Weighted-Encoding Ensemble Model (MSWEEM), which integrates annotator
meta-features like fatigue and speeding. First, our results show MSWEEM
outperforms standard ensembles by 14% on held-out data and 12% on an
alternative dataset. Second, we find that incorporating signals of annotator
behavior, such as speed and fatigue, significantly boosts model performance.
Third, we find that annotators with higher qualifications, such as Master's,
deliver more consistent and faster annotations. Given the increasing
uncertainty over annotation quality, our experiments show that understanding
annotator patterns is crucial for enhancing model accuracy in user behavior
prediction.

</details>


### [719] [ReGNet: Reciprocal Space-Aware Long-Range Modeling for Crystalline Property Prediction](https://arxiv.org/pdf/2502.02748)
*Jianan Nie, Peiyao Xiao, Kaiyi Ji, Peng Gao*

Main category: cs.LG

TL;DR: ReGNet, a novel architecture combining geometric GNNs and reciprocal blocks, achieves state-of-the-art accuracy in crystal property prediction by capturing both short-range and long-range interactions.


<details>
  <summary>Details</summary>
Motivation: Predicting crystal properties is challenging due to infinite periodic atomic arrangements, and existing methods often fail to capture long-range interactions effectively.

Method: ReGNet integrates geometric GNNs for short-range interactions and reciprocal blocks (using Fourier transforms) for long-range interactions, with a model extension for multi-property prediction.

Result: ReGNet outperforms existing methods on datasets like JARVIS, Materials Project, and MatBench, demonstrating high accuracy and computational efficiency.

Conclusion: ReGNet is a scalable and accurate solution for crystal property prediction, with potential for multi-property prediction.

Abstract: Predicting properties of crystals from their structures is a fundamental yet
challenging task in materials science. Unlike molecules, crystal structures
exhibit infinite periodic arrangements of atoms, requiring methods capable of
capturing both local and global information effectively. However, most current
works fall short of capturing long-range interactions within periodic
structures. To address this limitation, we leverage \emph{reciprocal space} to
efficiently encode long-range interactions with learnable filters within
Fourier transforms. We introduce Reciprocal Geometry Network (ReGNet), a novel
architecture that integrates geometric GNNs and reciprocal blocks to model
short-range and long-range interactions, respectively. Experimental results on
JARVIS, Materials Project, and MatBench demonstrate that ReGNet achieves
state-of-the-art predictive accuracy across a range of crystal property
prediction tasks. Additionally, we explore a model extension that employs the
mixture-of-experts for multi-property prediction with promising results and
high computational efficiency. These findings highlight the potential of our
model as a scalable and accurate solution for crystal property prediction. The
code will be released upon paper acceptance.

</details>


### [720] [Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint Matching](https://arxiv.org/pdf/2504.11713)
*Aaron Havens, Benjamin Kurt Miller, Bing Yan, Carles Domingo-Enrich, Anuroop Sriram, Brandon Wood, Daniel Levine, Bin Hu, Brandon Amos, Brian Karrer, Xiang Fu, Guan-Horng Liu, Ricky T. Q. Chen*

Main category: cs.LG

TL;DR: Adjoint Sampling is a scalable, efficient algorithm for learning diffusion processes to sample from unnormalized densities, outperforming prior methods in gradient updates and scalability.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing methods in scalability and efficiency for sampling from unnormalized densities, particularly in large problem settings.

Method: The algorithm is based on stochastic optimal control, allowing more gradient updates than energy evaluations, and incorporates symmetries and periodic boundary conditions for molecular modeling.

Result: Demonstrated effectiveness on classical energy functions and scaled to neural network-based energy models for amortized conformer generation.

Conclusion: Adjoint Sampling offers a theoretically grounded, scalable solution for sampling, with potential impact in computational chemistry, and plans to open-source benchmarks for further research.

Abstract: We introduce Adjoint Sampling, a highly scalable and efficient algorithm for
learning diffusion processes that sample from unnormalized densities, or energy
functions. It is the first on-policy approach that allows significantly more
gradient updates than the number of energy evaluations and model samples,
allowing us to scale to much larger problem settings than previously explored
by similar methods. Our framework is theoretically grounded in stochastic
optimal control and shares the same theoretical guarantees as Adjoint Matching,
being able to train without the need for corrective measures that push samples
towards the target distribution. We show how to incorporate key symmetries, as
well as periodic boundary conditions, for modeling molecules in both cartesian
and torsional coordinates. We demonstrate the effectiveness of our approach
through extensive experiments on classical energy functions, and further scale
up to neural network-based energy models where we perform amortized conformer
generation across many molecular systems. To encourage further research in
developing highly scalable sampling methods, we plan to open source these
challenging benchmarks, where successful methods can directly impact progress
in computational chemistry.

</details>


### [721] [Robust Reward Alignment via Hypothesis Space Batch Cutting](https://arxiv.org/pdf/2502.02921)
*Zhixian Xie, Haode Zhang, Yizhe Feng, Wanxin Jin*

Main category: cs.LG

TL;DR: A robust reward alignment method using hypothesis space batched cutting to handle erroneous human preferences in reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Existing preference-based alignment methods struggle with robustness to false human preferences, necessitating a more reliable approach.

Method: Proposes hypothesis space batched cutting, iteratively refining reward hypotheses with conservative cuts to handle erroneous preferences without explicit identification.

Result: Outperforms state-of-the-art methods in error-free and high-error settings, ensuring bounded query complexity.

Conclusion: The method provides provable robustness against false preferences, improving reward alignment efficiency and reliability.

Abstract: Reward design in reinforcement learning and optimal control is challenging.
Preference-based alignment addresses this by enabling agents to learn rewards
from ranked trajectory pairs provided by humans. However, existing methods
often struggle from poor robustness to unknown false human preferences. In this
work, we propose a robust and efficient reward alignment method based on a
novel and geometrically interpretable perspective: hypothesis space batched
cutting. Our method iteratively refines the reward hypothesis space through
"cuts" based on batches of human preferences. Within each batch, human
preferences, queried based on disagreement, are grouped using a voting function
to determine the appropriate cut, ensuring a bounded human query complexity. To
handle unknown erroneous preferences, we introduce a conservative cutting
method within each batch, preventing erroneous human preferences from making
overly aggressive cuts to the hypothesis space. This guarantees provable
robustness against false preferences, while eliminating the need to explicitly
identify them. We evaluate our method in a model predictive control setting
across diverse tasks. The results demonstrate that our framework achieves
comparable or superior performance to state-of-the-art methods in error-free
settings while significantly outperforming existing methods when handling a
high percentage of erroneous human preferences.

</details>


### [722] [Prediction of the Most Fire-Sensitive Point in Building Structures with Differentiable Agents for Thermal Simulators](https://arxiv.org/pdf/2502.03424)
*Yuan Xinjie, Khalid M. Mosalam*

Main category: cs.LG

TL;DR: The paper introduces the Most Fire-Sensitive Point (MFSP) and a machine learning framework using Graph Neural Networks (GNNs) to identify it, optimizing fire safety assessments in buildings.


<details>
  <summary>Details</summary>
Motivation: Evaluating fire safety in buildings is challenging due to the complexity of simulating all fire scenarios. The MFSP concept addresses this by identifying the worst-case fire scenario.

Method: A GNN-based framework predicts the Maximum Interstory Drift Ratio (MIDR) under fire, guiding MFSP identification. It includes an edge update mechanism and transfer learning.

Result: The framework performs well in identifying MFSP on a large-scale dataset, offering a tool for fire safety optimization.

Conclusion: The proposed method efficiently identifies MFSP, transforming fire safety assessments in structural design. Datasets and codes are open-sourced.

Abstract: Fire safety is crucial for ensuring the stability of building structures, yet
evaluating whether a structure meets fire safety requirement is challenging.
Fires can originate at any point within a structure, and simulating every
potential fire scenario is both expensive and time-consuming. To address this
challenge, we propose the concept of the Most Fire-Sensitive Point (MFSP) and
an efficient machine learning framework for its identification. The MFSP is
defined as the location at which a fire, if initiated, would cause the most
severe detrimental impact on the building's stability, effectively representing
the worst-case fire scenario. In our framework, a Graph Neural Network (GNN)
serves as an efficient and differentiable agent for conventional Finite Element
Analysis (FEA) simulators by predicting the Maximum Interstory Drift Ratio
(MIDR) under fire, which then guides the training and evaluation of the MFSP
predictor. Additionally, we enhance our framework with a novel edge update
mechanism and a transfer learning-based training scheme. Evaluations on a
large-scale simulation dataset demonstrate the good performance of the proposed
framework in identifying the MFSP, offering a transformative tool for
optimizing fire safety assessments in structural design. All developed datasets
and codes are open-sourced online.

</details>


### [723] [Devil is in the Details: Density Guidance for Detail-Aware Generation with Flow Models](https://arxiv.org/pdf/2502.05807)
*Rafał Karczewski, Markus Heinonen, Vikas Garg*

Main category: cs.LG

TL;DR: The paper analyzes and improves techniques for controlling image detail in diffusion models, introducing Density Guidance for precise log-density control during sampling.


<details>
  <summary>Details</summary>
Motivation: Recent findings show that image likelihood doesn't align with perceptual quality, necessitating better control over sample density to balance realism and detail.

Method: The authors analyze Prior Guidance, propose score alignment as a condition for its effectiveness, and introduce Density Guidance for exact log-density control in generative ODEs.

Result: Experiments show fine-grained control over image detail without quality loss, with extensions to stochastic sampling for controlled variation.

Conclusion: Density Guidance and its stochastic extension provide effective tools for balancing detail and realism in diffusion models.

Abstract: Diffusion models have emerged as a powerful class of generative models,
capable of producing high-quality images by mapping noise to a data
distribution. However, recent findings suggest that image likelihood does not
align with perceptual quality: high-likelihood samples tend to be smooth, while
lower-likelihood ones are more detailed. Controlling sample density is thus
crucial for balancing realism and detail. In this paper, we analyze an existing
technique, Prior Guidance, which scales the latent code to influence image
detail. We introduce score alignment, a condition that explains why this method
works and show that it can be tractably checked for any continuous normalizing
flow model. We then propose Density Guidance, a principled modification of the
generative ODE that enables exact log-density control during sampling. Finally,
we extend Density Guidance to stochastic sampling, ensuring precise log-density
control while allowing controlled variation in structure or fine details. Our
experiments demonstrate that these techniques provide fine-grained control over
image detail without compromising sample quality. Code is available at
https://github.com/Aalto-QuML/density-guidance.

</details>


### [724] [Model Diffusion for Certifiable Few-shot Transfer Learning](https://arxiv.org/pdf/2502.06970)
*Fady Rezk, Royson Lee, Henry Gouk, Timothy Hospedales, Minyoung Kim*

Main category: cs.LG

TL;DR: A novel transfer learning method for low-data problems ensures non-vacuous generalization guarantees by sampling PEFT parameters from a trained distribution and selecting the best-performing one.


<details>
  <summary>Details</summary>
Motivation: Address the lack of generalization guarantees in parameter-efficient fine-tuning (PEFT) for high-importance applications, ensuring ethical and legal compliance.

Method: Train a distribution over PEFT parameters using upstream tasks, then sample and evaluate plausible PEFTs for downstream tasks, confining the hypothesis space to a finite set.

Result: Demonstrates non-trivial generalization guarantees in low-shot regimes, outperforming existing methods with vacuous bounds.

Conclusion: The approach provides a practical and theoretically sound solution for certifying accuracy in low-data scenarios.

Abstract: In contemporary deep learning, a prevalent and effective workflow for solving
low-data problems is adapting powerful pre-trained foundation models (FMs) to
new tasks via parameter-efficient fine-tuning (PEFT). However, while
empirically effective, the resulting solutions lack generalisation guarantees
to certify their accuracy - which may be required for ethical or legal reasons
prior to deployment in high-importance applications. In this paper we develop a
novel transfer learning approach that is designed to facilitate non-vacuous
learning theoretic generalisation guarantees for downstream tasks, even in the
low-shot regime. Specifically, we first use upstream tasks to train a
distribution over PEFT parameters. We then learn the downstream task by a
sample-and-evaluate procedure -- sampling plausible PEFTs from the trained
diffusion model and selecting the one with the highest likelihood on the
downstream data. Crucially, this confines our model hypothesis to a finite set
of PEFT samples. In contrast to the typical continuous hypothesis spaces of
neural network weights, this facilitates tighter risk certificates. We
instantiate our bound and show non-trivial generalization guarantees compared
to existing learning approaches which lead to vacuous bounds in the low-shot
regime.

</details>


### [725] [Addressing Concept Mislabeling in Concept Bottleneck Models Through Preference Optimization](https://arxiv.org/pdf/2504.18026)
*Emiliano Penaloza, Tianyue H. Zhan, Laurent Charlin, Mateo Espinosa Zarlenga*

Main category: cs.LG

TL;DR: Concept Bottleneck Models (CBMs) improve AI trustworthiness but suffer from concept mislabeling. Concept Preference Optimization (CPO) is introduced to mitigate this, outperforming Binary Cross Entropy (BCE) in noisy datasets.


<details>
  <summary>Details</summary>
Motivation: CBMs rely on accurate concept labels, which are often missing in practice, degrading performance. CPO aims to address this limitation.

Method: CPO, a new loss function based on Direct Preference Optimization, is introduced to optimize for concept posterior distribution and reduce sensitivity to noise.

Result: CPO consistently outperforms BCE in real-world datasets, even with added label noise, improving robustness.

Conclusion: CPO effectively mitigates the impact of concept mislabeling in CBMs, enhancing performance and reliability.

Abstract: Concept Bottleneck Models (CBMs) propose to enhance the trustworthiness of AI
systems by constraining their decisions on a set of human understandable
concepts. However, CBMs typically assume that datasets contains accurate
concept labels an assumption often violated in practice, which we show can
significantly degrade performance (by 25% in some cases). To address this, we
introduce the Concept Preference Optimization (CPO) objective, a new loss
function based on Direct Preference Optimization, which effectively mitigates
the negative impact of concept mislabeling on CBM performance. We provide an
analysis on some key properties of the CPO objective showing it directly
optimizes for the concept's posterior distribution, and contrast it against
Binary Cross Entropy (BCE) where we show CPO is inherently less sensitive to
concept noise. We empirically confirm our analysis finding that CPO
consistently outperforms BCE in three real world datasets with and without
added label noise.

</details>


### [726] [Outsourced diffusion sampling: Efficient posterior inference in latent spaces of generative models](https://arxiv.org/pdf/2502.06999)
*Siddarth Venkatraman, Mohsin Hasan, Minsu Kim, Luca Scimeca, Marcin Sendera, Yoshua Bengio, Glen Berseth, Nikolay Malkin*

Main category: cs.LG

TL;DR: The paper proposes a method to sample from intractable posterior distributions in generative models by using diffusion models in the noise space, trained via reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Sampling from posterior distributions in generative models (like VAEs, GANs, or flow-based models) is often intractable, especially when constraints are involved. The goal is to enable efficient conditional sampling.

Method: The method involves training diffusion models in the noise space (z) to sample from the posterior distribution. Reinforcement learning ensures transformed samples align with the posterior in data space (x).

Result: The approach outperforms other inference methods, enabling conditional sampling for GANs, VAEs, and flow-based models. It works well in tasks like image generation, RL with human feedback, and protein structure generation.

Conclusion: Outsourced diffusion sampling is effective for amortized inference in generative models, offering smoother posterior sampling in noise space compared to data space.

Abstract: Any well-behaved generative model over a variable $\mathbf{x}$ can be
expressed as a deterministic transformation of an exogenous ('outsourced')
Gaussian noise variable $\mathbf{z}$: $\mathbf{x}=f_\theta(\mathbf{z})$. In
such a model (\eg, a VAE, GAN, or continuous-time flow-based model), sampling
of the target variable $\mathbf{x} \sim p_\theta(\mathbf{x})$ is
straightforward, but sampling from a posterior distribution of the form
$p(\mathbf{x}\mid\mathbf{y}) \propto
p_\theta(\mathbf{x})r(\mathbf{x},\mathbf{y})$, where $r$ is a constraint
function depending on an auxiliary variable $\mathbf{y}$, is generally
intractable. We propose to amortize the cost of sampling from such posterior
distributions with diffusion models that sample a distribution in the noise
space ($\mathbf{z}$). These diffusion samplers are trained by reinforcement
learning algorithms to enforce that the transformed samples
$f_\theta(\mathbf{z})$ are distributed according to the posterior in the data
space ($\mathbf{x}$). For many models and constraints, the posterior in noise
space is smoother than in data space, making it more suitable for amortized
inference. Our method enables conditional sampling under unconditional GAN,
(H)VAE, and flow-based priors, comparing favorably with other inference
methods. We demonstrate the proposed outsourced diffusion sampling in several
experiments with large pretrained prior models: conditional image generation,
reinforcement learning with human feedback, and protein structure generation.

</details>


### [727] [You Do Not Fully Utilize Transformer's Representation Capacity](https://arxiv.org/pdf/2502.09245)
*Gleb Gerasimov, Yaroslav Aksenov, Nikita Balagansky, Viacheslav Sinii, Daniil Gavrilov*

Main category: cs.LG

TL;DR: LIMe, a lightweight extension for Transformers, addresses representation collapse by integrating multi-layer representations, improving performance and convergence.


<details>
  <summary>Details</summary>
Motivation: Standard Transformers rely on single-layer hidden states, leading to representation collapse and degraded performance.

Method: Introduces Layer-Integrated Memory (LIMe), using key-value buffers and learned routing weights to integrate multi-layer representations.

Result: LIMe achieves faster convergence, lower perplexity, and better accuracy while preserving higher entropy and separability.

Conclusion: LIMe mitigates collapse, enriches representations, and reveals systematic feature reuse, suggesting future research directions.

Abstract: In contrast to RNNs, which compress their history into a single hidden state,
Transformers can attend to all past tokens directly. However, standard
Transformers rely solely on the hidden state from the previous layer to
represent the entire context. We show that this design choice induces
representation collapse and degrades performance. To address this issue, we
introduce Layer-Integrated Memory (LIMe), a lightweight extension that
leverages existing key-value buffers and learns per-head, per-layer routing
weights to integrate representations from all previous layers with negligible
overhead. Through extensive experiments-including language modeling, synthetic
reasoning benchmarks, and very deep architectures-LIMe consistently achieves
faster convergence, lower perplexity per FLOP, and substantial accuracy
improvements on synthetic tasks while preserving higher value-vector entropy
and improved token separability. Finally, our analysis of the learned routing
weights reveals systematic reuse of both local and long-distance features,
demonstrating how LIMe mitigates collapse, unlocks richer representations
without increasing hidden-state size, and points to promising directions for
future research.

</details>


### [728] [Continuous Thought Machines](https://arxiv.org/pdf/2505.05522)
*Luke Darlow, Ciaran Regan, Sebastian Risi, Jeffrey Seely, Llion Jones*

Main category: cs.LG

TL;DR: The paper introduces the Continuous Thought Machine (CTM), a model incorporating neuron-level temporal processing and synchronization to enhance biological plausibility in deep learning, demonstrating versatility across tasks without aiming for state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To challenge the oversimplification of neural activity in deep learning by reintroducing temporal dynamics and synchronization, inspired by biological brains.

Method: Develops the CTM with neuron-level temporal processing and neural synchronization as latent representations, balancing biological realism and computational efficiency.

Result: CTM performs well on diverse tasks (e.g., ImageNet-1K, maze-solving, QA) and exhibits adaptive compute, stopping early for simpler tasks.

Conclusion: CTM advances biologically plausible AI, offering interpretability and versatility, though not targeting state-of-the-art performance.

Abstract: Biological brains demonstrate complex neural activity, where the timing and
interplay between neurons is critical to how brains process information. Most
deep learning architectures simplify neural activity by abstracting away
temporal dynamics. In this paper we challenge that paradigm. By incorporating
neuron-level processing and synchronization, we can effectively reintroduce
neural timing as a foundational element. We present the Continuous Thought
Machine (CTM), a model designed to leverage neural dynamics as its core
representation. The CTM has two core innovations: (1) neuron-level temporal
processing, where each neuron uses unique weight parameters to process a
history of incoming signals; and (2) neural synchronization employed as a
latent representation. The CTM aims to strike a balance between oversimplified
neuron abstractions that improve computational efficiency, and biological
realism. It operates at a level of abstraction that effectively captures
essential temporal dynamics while remaining computationally tractable for deep
learning. We demonstrate the CTM's strong performance and versatility across a
range of challenging tasks, including ImageNet-1K classification, solving 2D
mazes, sorting, parity computation, question-answering, and RL tasks. Beyond
displaying rich internal representations and offering a natural avenue for
interpretation owing to its internal process, the CTM is able to perform tasks
that require complex sequential reasoning. The CTM can also leverage adaptive
compute, where it can stop earlier for simpler tasks, or keep computing when
faced with more challenging instances. The goal of this work is to share the
CTM and its associated innovations, rather than pushing for new
state-of-the-art results. To that end, we believe the CTM represents a
significant step toward developing more biologically plausible and powerful
artificial intelligence systems.

</details>


### [729] [When do neural networks learn world models?](https://arxiv.org/pdf/2502.09297)
*Tianren Zhang, Guanyu Chen, Feng Chen*

Main category: cs.LG

TL;DR: Neural networks with low-degree bias can recover latent data-generating variables in multi-task settings, but recovery depends on model architecture.


<details>
  <summary>Details</summary>
Motivation: To determine if neural networks can learn world models like humans, focusing on latent variable recovery.

Method: Uses Boolean models and Fourier-Walsh transform to analyze invertible Boolean transforms in multi-task settings.

Result: Models with low-degree bias recover latent variables under mild assumptions, even with complex proxy tasks.

Conclusion: The findings connect to self-supervised learning, out-of-distribution generalization, and large language models.

Abstract: Humans develop world models that capture the underlying generation process of
data. Whether neural networks can learn similar world models remains an open
problem. In this work, we present the first theoretical results for this
problem, showing that in a multi-task setting, models with a low-degree bias
provably recover latent data-generating variables under mild assumptions --
even if proxy tasks involve complex, non-linear functions of the latents.
However, such recovery is sensitive to model architecture. Our analysis
leverages Boolean models of task solutions via the Fourier-Walsh transform and
introduces new techniques for analyzing invertible Boolean transforms, which
may be of independent interest. We illustrate the algorithmic implications of
our results and connect them to related research areas, including
self-supervised learning, out-of-distribution generalization, and the linear
representation hypothesis in large language models.

</details>


### [730] [DiffMS: Diffusion Generation of Molecules Conditioned on Mass Spectra](https://arxiv.org/pdf/2502.09571)
*Montgomery Bohde, Mrunali Manjrekar, Runzhong Wang, Shuiwang Ji, Connor W. Coley*

Main category: cs.LG

TL;DR: DiffMS is a state-of-the-art encoder-decoder generative network for de novo molecular structure generation from mass spectra, leveraging transformer-based encoding and discrete graph diffusion decoding.


<details>
  <summary>Details</summary>
Motivation: To improve accuracy and efficiency in small molecule structure elucidation using mass spectrometry.

Method: Uses a transformer encoder for mass spectra and a formula-restricted discrete graph diffusion decoder, pretrained on fingerprint-structure pairs.

Result: Outperforms existing models on de novo molecule generation benchmarks.

Conclusion: DiffMS advances molecular structure elucidation with robust performance and scalability.

Abstract: Mass spectrometry plays a fundamental role in elucidating the structures of
unknown molecules and subsequent scientific discoveries. One formulation of the
structure elucidation task is the conditional de novo generation of molecular
structure given a mass spectrum. Toward a more accurate and efficient
scientific discovery pipeline for small molecules, we present DiffMS, a
formula-restricted encoder-decoder generative network that achieves
state-of-the-art performance on this task. The encoder utilizes a transformer
architecture and models mass spectra domain knowledge such as peak formulae and
neutral losses, and the decoder is a discrete graph diffusion model restricted
by the heavy-atom composition of a known chemical formula. To develop a robust
decoder that bridges latent embeddings and molecular structures, we pretrain
the diffusion decoder with fingerprint-structure pairs, which are available in
virtually infinite quantities, compared to structure-spectrum pairs that number
in the tens of thousands. Extensive experiments on established benchmarks show
that DiffMS outperforms existing models on de novo molecule generation. We
provide several ablations to demonstrate the effectiveness of our diffusion and
pretraining approaches and show consistent performance scaling with increasing
pretraining dataset size. DiffMS code is publicly available at
https://github.com/coleygroup/DiffMS.

</details>


### [731] [Exploring Criteria of Loss Reweighting to Enhance LLM Unlearning](https://arxiv.org/pdf/2505.11953)
*Puning Yang, Qizhou Wang, Zhuo Huang, Tongliang Liu, Chengqi Zhang, Bo Han*

Main category: cs.LG

TL;DR: The paper explores loss reweighting in machine unlearning for LLMs, identifying two goals (Saturation and Importance) and proposing SatImp, a combined method that outperforms individual strategies.


<details>
  <summary>Details</summary>
Motivation: To clarify the unclear functionalities of loss reweighting and determine the optimal strategy for improving machine unlearning with LLMs.

Method: Designs specific reweighting strategies for Saturation and Importance, evaluates their effects, and proposes SatImp, a combined approach.

Result: Saturation is more effective than Importance, and their combination yields improvements. SatImp outperforms individual strategies.

Conclusion: SatImp bridges research gaps and suggests future directions, validated by empirical results.

Abstract: Loss reweighting has shown significant benefits for machine unlearning with
large language models (LLMs). However, their exact functionalities are left
unclear and the optimal strategy remains an open question, thus impeding the
understanding and improvement of existing methodologies. In this paper, we
identify two distinct goals of loss reweighting, namely, Saturation and
Importance -- the former indicates that those insufficiently optimized data
should be emphasized, while the latter stresses some critical data that are
most influential for loss minimization. To study their usefulness, we design
specific reweighting strategies for each goal and evaluate their respective
effects on unlearning. We conduct extensive empirical analyses on
well-established benchmarks, and summarize some important observations as
follows: (i) Saturation enhances efficacy more than importance-based
reweighting, and their combination can yield additional improvements. (ii)
Saturation typically allocates lower weights to data with lower likelihoods,
whereas importance-based reweighting does the opposite. (iii) The efficacy of
unlearning is also largely influenced by the smoothness and granularity of the
weight distributions. Based on these findings, we propose SatImp, a simple
reweighting method that combines the advantages of both saturation and
importance. Empirical results on extensive datasets validate the efficacy of
our method, potentially bridging existing research gaps and indicating
directions for future research. Our code is available at
https://github.com/tmlr-group/SatImp.

</details>


### [732] [Solving Empirical Bayes via Transformers](https://arxiv.org/pdf/2502.09844)
*Anzo Teh, Mark Jabbour, Yury Polyanskiy*

Main category: cs.LG

TL;DR: The paper applies transformer-based AI to solve the Poisson-EB problem, showing that small models outperform classical methods in runtime and accuracy.


<details>
  <summary>Details</summary>
Motivation: To leverage modern AI (transformers) for solving the classical Poisson-EB problem, improving upon traditional statistical methods.

Method: Pre-train a transformer on synthetic $(X,	heta)$ pairs for in-context learning, adapting to unknown priors. Theoretical analysis shows vanishing regret for wide transformers.

Result: Small transformers (100k parameters) outperform classical NPMLE in runtime and validation loss on synthetic and real-world datasets.

Conclusion: Transformers offer a practical and theoretically sound alternative to classical Poisson-EB estimators, with distinct internal mechanisms.

Abstract: This work applies modern AI tools (transformers) to solving one of the oldest
statistical problems: Poisson means under empirical Bayes (Poisson-EB) setting.
In Poisson-EB a high-dimensional mean vector $\theta$ (with iid coordinates
sampled from an unknown prior $\pi$) is estimated on the basis of
$X=\mathrm{Poisson}(\theta)$. A transformer model is pre-trained on a set of
synthetically generated pairs $(X,\theta)$ and learns to do in-context learning
(ICL) by adapting to unknown $\pi$. Theoretically, we show that a sufficiently
wide transformer can achieve vanishing regret with respect to an oracle
estimator who knows $\pi$ as dimension grows to infinity. Practically, we
discover that already very small models (100k parameters) are able to
outperform the best classical algorithm (non-parametric maximum likelihood, or
NPMLE) both in runtime and validation loss, which we compute on
out-of-distribution synthetic data as well as real-world datasets (NHL hockey,
MLB baseball, BookCorpusOpen). Finally, by using linear probes, we confirm that
the transformer's EB estimator appears to internally work differently from
either NPMLE or Robbins' estimators.

</details>


### [733] [Closed-Form Training Dynamics Reveal Learned Features and Linear Structure in Word2Vec-like Models](https://arxiv.org/pdf/2502.09863)
*Dhruva Karkada, James B. Simon, Yasaman Bahri, Michael R. DeWeese*

Main category: cs.LG

TL;DR: The paper analyzes the quartic Taylor approximation of the word2vec loss, showing its similarity to word2vec in training dynamics and downstream performance. It provides analytical solutions for gradient flow and embeddings, revealing how orthogonal subspaces are learned incrementally.


<details>
  <summary>Details</summary>
Motivation: To understand the training dynamics and representation learning in word2vec-like models through a simplified analytical framework.

Method: Examines the quartic Taylor approximation of word2vec loss, solves for gradient flow and embeddings analytically, and validates findings on Wikipedia data.

Result: The model learns orthogonal subspaces incrementally, each representing interpretable topics. Linear representations of abstract concepts emerge, enabling analogy completion.

Conclusion: The analytical framework successfully explains word2vec's behavior, revealing interpretable subspace learning and emergent semantic representations.

Abstract: Self-supervised word embedding algorithms such as word2vec provide a minimal
setting for studying representation learning in language modeling. We examine
the quartic Taylor approximation of the word2vec loss around the origin, and we
show that both the resulting training dynamics and the final performance on
downstream tasks are empirically very similar to those of word2vec. Our main
contribution is to analytically solve for both the gradient flow training
dynamics and the final word embeddings in terms of only the corpus statistics
and training hyperparameters. The solutions reveal that these models learn
orthogonal linear subspaces one at a time, each one incrementing the effective
rank of the embeddings until model capacity is saturated. Training on
Wikipedia, we find that each of the top linear subspaces represents an
interpretable topic-level concept. Finally, we apply our theory to describe how
linear representations of more abstract semantic concepts emerge during
training; these can be used to complete analogies via vector addition.

</details>


### [734] [Solving Inverse Problems with Deep Linear Neural Networks: Global Convergence Guarantees for Gradient Descent with Weight Decay](https://arxiv.org/pdf/2502.15522)
*Hannah Laus, Suzanna Parkinson, Vasileios Charisopoulos, Felix Krahmer, Rebecca Willett*

Main category: cs.LG

TL;DR: Deep linear networks trained with weight decay adapt to latent low-dimensional structure in underdetermined inverse problems, improving generalization and convergence.


<details>
  <summary>Details</summary>
Motivation: To understand if deep neural networks adapt to latent low-dimensional structure in underdetermined inverse problems when trained with gradient descent and weight decay.

Method: Study mildly overparameterized deep linear networks trained with gradient descent and weight decay regularization.

Result: Networks converge to solutions that solve the inverse problem and implicitly encode latent subspace structure.

Conclusion: Regularization and overparameterization enhance generalization and training convergence, with networks adapting to latent structure.

Abstract: Machine learning methods are commonly used to solve inverse problems, wherein
an unknown signal must be estimated from few measurements generated via a known
acquisition procedure. In particular, neural networks perform well empirically
but have limited theoretical guarantees. In this work, we study an
underdetermined linear inverse problem that admits several possible solution
mappings. A standard remedy (e.g., in compressed sensing) establishing
uniqueness of the solution mapping is to assume knowledge of latent
low-dimensional structure in the source signal. We ask the following question:
do deep neural networks adapt to this low-dimensional structure when trained by
gradient descent with weight decay regularization? We prove that mildly
overparameterized deep linear networks trained in this manner converge to an
approximate solution that accurately solves the inverse problem while
implicitly encoding latent subspace structure. To our knowledge, this is the
first result to rigorously show that deep linear networks trained with weight
decay automatically adapt to latent subspace structure in the data under
practical stepsize and weight initialization schemes. Our work highlights that
regularization and overparameterization improve generalization, while
overparameterization also accelerates convergence during training.

</details>


### [735] [Explainable Multi-modal Time Series Prediction with LLM-in-the-Loop](https://arxiv.org/pdf/2503.01013)
*Yushan Jiang, Wenchao Yu, Geon Lee, Dongjin Song, Kijung Shin, Wei Cheng, Yanchi Liu, Haifeng Chen*

Main category: cs.LG

TL;DR: TimeXL is a multi-modal framework combining time series and textual inputs with LLMs for improved prediction accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing time series methods often ignore contextual signals from auxiliary modalities, limiting their effectiveness.

Method: TimeXL uses a prototype-based encoder and three LLMs (prediction, reflection, refinement) in a closed-loop workflow for iterative improvement.

Result: Achieves up to 8.9% AUC improvement and generates human-centric, multi-modal explanations.

Conclusion: TimeXL demonstrates the potential of LLM-driven reasoning for enhancing time series prediction and interpretability.

Abstract: Time series analysis provides essential insights for real-world system
dynamics and informs downstream decision-making, yet most existing methods
often overlook the rich contextual signals present in auxiliary modalities. To
bridge this gap, we introduce TimeXL, a multi-modal prediction framework that
integrates a prototype-based time series encoder with three collaborating Large
Language Models (LLMs) to deliver more accurate predictions and interpretable
explanations. First, a multi-modal prototype-based encoder processes both time
series and textual inputs to generate preliminary forecasts alongside
case-based rationales. These outputs then feed into a prediction LLM, which
refines the forecasts by reasoning over the encoder's predictions and
explanations. Next, a reflection LLM compares the predicted values against the
ground truth, identifying textual inconsistencies or noise. Guided by this
feedback, a refinement LLM iteratively enhances text quality and triggers
encoder retraining. This closed-loop workflow -- prediction, critique
(reflect), and refinement -- continuously boosts the framework's performance
and interpretability. Empirical evaluations on four real-world datasets
demonstrate that TimeXL achieves up to 8.9\% improvement in AUC and produces
human-centric, multi-modal explanations, highlighting the power of LLM-driven
reasoning for time series prediction.

</details>


### [736] [VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use](https://arxiv.org/pdf/2505.19255)
*Mingyuan Wu, Jingcheng Yang, Jize Jiang, Meitang Li, Kaizhuo Yan, Hanchao Yu, Minjia Zhang, Chengxiang Zhai, Klara Nahrstedt*

Main category: cs.LG

TL;DR: VTool-R1 trains VLMs to generate multimodal chains of thought by interleaving text and visual reasoning steps, improving reasoning performance without process-based supervision.


<details>
  <summary>Details</summary>
Motivation: Existing methods for VLMs either lack true multimodal reasoning or visual training mechanisms, limiting their effectiveness.

Method: VTool-R1 integrates Python-based visual editing tools into RFT, using outcome-based rewards to train VLMs for strategic visual tool use.

Result: Experiments show VTool-R1 enhances reasoning performance on visual question answering tasks by generating multimodal chains of thought.

Conclusion: VTool-R1 advances VLMs' reasoning by enabling them to 'think with images' and use tools strategically.

Abstract: Reinforcement Learning Finetuning (RFT) has significantly advanced the
reasoning capabilities of large language models (LLMs) by enabling long chains
of thought, self-correction, and effective tool use. While recent works attempt
to extend RFT to vision-language models (VLMs), these efforts largely produce
text-only reasoning conditioned on static image inputs, falling short of true
multimodal reasoning in the response. In contrast, test-time methods like
Visual Sketchpad incorporate visual steps but lack training mechanisms.
  We introduce VTool-R1, the first framework that trains VLMs to generate
multimodal chains of thought by interleaving text and intermediate visual
reasoning steps. VTool-R1 integrates Python-based visual editing tools into the
RFT process, enabling VLMs to learn when and how to generate visual reasoning
steps that benefit final reasoning. Trained with outcome-based rewards tied to
task accuracy, our approach elicits strategic visual tool use for reasoning
without relying on process-based supervision. Experiments on structured visual
question answering over charts and tables show that VTool-R1 enhances reasoning
performance by teaching VLMs to "think with images" and generate multimodal
chain of thoughts with tools.

</details>


### [737] [Interpretable Visualizations of Data Spaces for Classification Problems](https://arxiv.org/pdf/2503.05861)
*Christian Jorgensen, Arthur Y. Lin, Rhushil Vasavada, Rose K. Cersonsky*

Main category: cs.LG

TL;DR: A hybrid supervised-unsupervised technique is proposed to visualize decision boundaries in classification models, demonstrated with chemical neurotoxicity data.


<details>
  <summary>Details</summary>
Motivation: Current visualization techniques struggle to reveal how classification models discern class boundaries, despite their effectiveness.

Method: A hybrid supervised-unsupervised approach is developed to create interpretable maps of decision boundaries.

Result: The method produces human-interpretable visualizations, demonstrated with chemical neurotoxicity data.

Conclusion: The technique generalizes beyond chemistry, aiding in understanding classification models across fields.

Abstract: How do classification models "see" our data? Based on their success in
delineating behaviors, there must be some lens through which it is easy to see
the boundary between classes; however, our current set of visualization
techniques makes this prospect difficult. In this work, we propose a hybrid
supervised-unsupervised technique distinctly suited to visualizing the decision
boundaries determined by classification problems. This method provides a
human-interpretable map that can be analyzed qualitatively and quantitatively,
which we demonstrate through visualizing and interpreting a decision boundary
for chemical neurotoxicity. While we discuss this method in the context of
chemistry-driven problems, its application can be generalized across subfields
for "unboxing" the operations of machine-learning classification models.

</details>


### [738] [Towards Large Reasoning Models for Agriculture](https://arxiv.org/pdf/2505.19259)
*Hossein Zaremehrjerdi, Shreyan Ganguly, Ashlyn Rairdin, Elizabeth Tranel, Benjamin Feuer, Juan Ignacio Di Salvo, Srikanth Panthulugiri, Hernan Torres Pacin, Victoria Moser, Sarah Jones, Joscif G Raigne, Yanben Shen, Heidi M. Dornath, Aditya Balu, Adarsh Krishnamurthy, Asheesh K Singh, Arti Singh, Baskar Ganapathysubramanian, Chinmay Hegde, Soumik Sarkar*

Main category: cs.LG

TL;DR: AgReason introduces a benchmark for agricultural reasoning, showing LRMs outperform traditional LLMs, with Gemini-based models achieving 36% accuracy. AgThoughts dataset and AgThinker models enhance reasoning in LLMs.


<details>
  <summary>Details</summary>
Motivation: Traditional LLMs struggle with nuanced agricultural decision-making due to limited reasoning. Advances in LRMs offer potential for better domain-specific inference.

Method: Created AgReason benchmark (100 questions) and AgThoughts dataset (44.6K QA pairs with reasoning traces). Developed AgThinker models for consumer GPUs.

Result: LRMs outperform conventional models, with Gemini-based models achieving 36% accuracy. AgThoughts dataset improves reasoning in LLMs.

Conclusion: LRMs show promise for agricultural reasoning, but challenges remain. AgThoughts and AgThinker advance domain-specific LLM capabilities.

Abstract: Agricultural decision-making involves complex, context-specific reasoning,
where choices about crops, practices, and interventions depend heavily on
geographic, climatic, and economic conditions. Traditional large language
models (LLMs) often fall short in navigating this nuanced problem due to
limited reasoning capacity. We hypothesize that recent advances in large
reasoning models (LRMs) can better handle such structured, domain-specific
inference. To investigate this, we introduce AgReason, the first expert-curated
open-ended science benchmark with 100 questions for agricultural reasoning.
Evaluations across thirteen open-source and proprietary models reveal that LRMs
outperform conventional ones, though notable challenges persist, with the
strongest Gemini-based baseline achieving 36% accuracy. We also present
AgThoughts, a large-scale dataset of 44.6K question-answer pairs generated with
human oversight and equipped with synthetically generated reasoning traces.
Using AgThoughts, we develop AgThinker, a suite of small reasoning models that
can be run on consumer-grade GPUs, and show that our dataset can be effective
in unlocking agricultural reasoning abilities in LLMs. Our project page is
here: https://baskargroup.github.io/Ag_reasoning/

</details>


### [739] [Reinforcement Learning with Verifiable Rewards: GRPO's Effective Loss, Dynamics, and Success Amplification](https://arxiv.org/pdf/2503.06639)
*Youssef Mroueh*

Main category: cs.LG

TL;DR: GRPO is a KL-regularized contrastive loss method for improving LLM reasoning with verifiable rewards, ensuring policy success probability amplification.


<details>
  <summary>Details</summary>
Motivation: To enhance reasoning in LLMs using verifiable rewards and quantify policy improvement.

Method: GRPO uses KL-regularized contrastive loss with synthetic data from old policies, iteratively updating policies.

Result: GRPO guarantees increased success probability, converging to a fixed point higher than initial.

Conclusion: GRPO effectively amplifies policy success, proving its utility for LLM training.

Abstract: Group Relative Policy Optimization (GRPO) was introduced recently and used
successfully to train DeepSeek-R1 models for promoting reasoning capabilities
of LLMs using verifiable or binary rewards. We show in this paper that GRPO
with verifiable rewards can be written as a Kullback--Leibler (KL) regularized
contrastive loss, where the contrastive samples are synthetic data sampled from
the old policy. The optimal GRPO policy $\pi_{n}$ can be expressed explicitly
in terms of the binary reward, as well as the first- and second-order
statistics of the old policy ($\pi_{n-1}$) and the reference policy
$\pi_{\text{ref}}$. Iterating this scheme, we obtain a sequence of policies
$\pi_{n}$ for which we can quantify the probability of success $p_n$. We show
that the probability of success of the policy satisfies a recurrence that
converges to a fixed point of a function that depends on the initial
probability of success $p_{\text{ref}}$ and the regularization parameter
$\beta$ of the $KL$ regularizer. We show that the fixed point $p^*$ is
guaranteed to be larger than $p_{\text{ref}}$, thereby demonstrating that GRPO
effectively amplifies the probability of success of the policy.

</details>


### [740] [In-Context Linear Regression Demystified: Training Dynamics and Mechanistic Interpretability of Multi-Head Softmax Attention](https://arxiv.org/pdf/2503.12734)
*Jianliang He, Xintian Pan, Siyu Chen, Zhuoran Yang*

Main category: cs.LG

TL;DR: Multi-head softmax attention models develop specific weight patterns during training, enabling in-context learning on linear data, outperforming single-head attention and generalizing to longer sequences.


<details>
  <summary>Details</summary>
Motivation: To understand how multi-head softmax attention models learn in-context tasks and the emergent patterns in their weights.

Method: Empirical experiments and theoretical analysis of attention patterns (KQ and OV weights) in multi-head softmax attention models.

Result: Emergent patterns enable debiased gradient descent predictors, outperforming single-head attention and generalizing to longer sequences. Multi-task learning reveals superposition phenomena.

Conclusion: In-context learning emerges from transformer architecture and data distribution, offering insights for broader applications.

Abstract: We study how multi-head softmax attention models are trained to perform
in-context learning on linear data. Through extensive empirical experiments and
rigorous theoretical analysis, we demystify the emergence of elegant attention
patterns: a diagonal and homogeneous pattern in the key-query (KQ) weights, and
a last-entry-only and zero-sum pattern in the output-value (OV) weights.
Remarkably, these patterns consistently appear from gradient-based training
starting from random initialization. Our analysis reveals that such emergent
structures enable multi-head attention to approximately implement a debiased
gradient descent predictor -- one that outperforms single-head attention and
nearly achieves Bayesian optimality up to proportional factor. Furthermore,
compared to linear transformers, the softmax attention readily generalizes to
sequences longer than those seen during training. We also extend our study to
scenarios with anisotropic covariates and multi-task linear regression. In the
former, multi-head attention learns to implement a form of pre-conditioned
gradient descent. In the latter, we uncover an intriguing regime where the
interplay between head number and task number triggers a superposition
phenomenon that efficiently resolves multi-task in-context learning. Our
results reveal that in-context learning ability emerges from the trained
transformer as an aggregated effect of its architecture and the underlying data
distribution, paving the way for deeper understanding and broader applications
of in-context learning.

</details>


### [741] [JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance Asymmetry in Model-Based Reinforcement Learning](https://arxiv.org/pdf/2505.19698)
*Jing Yu Lim, Zarif Ikram, Samson Yu, Haozhe Ma, Tze-Yun Leong, Dianbo Liu*

Main category: cs.LG

TL;DR: MBRL agents show performance asymmetry in Atari100k, excelling in some tasks but failing in others. The paper proposes JEDI, a latent diffusion model, to address this imbalance, improving human-optimal tasks while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: Current MBRL agents' aggregate metrics mask performance asymmetry, inflating results by excelling in some tasks while underperforming in others, especially in pixel-based agents.

Method: Proposes Joint Embedding DIffusion (JEDI), a latent diffusion world model trained end-to-end with a self-consistency objective.

Result: JEDI outperforms SOTA in human-optimal tasks, stays competitive overall, and is 3x faster with 43% lower memory than pixel-based baselines.

Conclusion: The work redefines human-level performance in Atari100k by addressing performance asymmetry and advocating balanced metrics.

Abstract: Recent advances in model-based reinforcement learning (MBRL) have achieved
super-human level performance on the Atari100k benchmark, driven by
reinforcement learning agents trained on powerful diffusion world models.
However, we identify that the current aggregates mask a major performance
asymmetry: MBRL agents dramatically outperform humans in some tasks despite
drastically underperforming in others, with the former inflating the aggregate
metrics. This is especially pronounced in pixel-based agents trained with
diffusion world models. In this work, we address the pronounced asymmetry
observed in pixel-based agents as an initial attempt to reverse the worrying
upward trend observed in them. We address the problematic aggregates by
delineating all tasks as Agent-Optimal or Human-Optimal and advocate for equal
importance on metrics from both sets. Next, we hypothesize this pronounced
asymmetry is due to the lack of temporally-structured latent space trained with
the World Model objective in pixel-based methods. Lastly, to address this
issue, we propose Joint Embedding DIffusion (JEDI), a novel latent diffusion
world model trained end-to-end with the self-consistency objective. JEDI
outperforms SOTA models in human-optimal tasks while staying competitive across
the Atari100k benchmark, and runs 3 times faster with 43% lower memory than the
latest pixel-based diffusion baseline. Overall, our work rethinks what it truly
means to cross human-level performance in Atari100k.

</details>


### [742] [MultiScale Contextual Bandits for Long Term Objectives](https://arxiv.org/pdf/2503.17674)
*Richa Rastogi, Yuta Saito, Thorsten Joachims*

Main category: cs.LG

TL;DR: The paper introduces MultiScale Policy Learning to optimize AI systems for long-term objectives by reconciling feedback at multiple timescales, addressing the disconnect between short-term interventions and long-term goals.


<details>
  <summary>Details</summary>
Motivation: Optimizing AI systems for short-term feedback (e.g., clicks) often fails to achieve long-term objectives (e.g., user retention). The disconnect between these timescales is a key challenge.

Method: Proposes MultiScale Policy Learning, leveraging PAC-Bayes to use data-rich lower timescales as hierarchical priors for faster learning at data-scarce higher scales. Instantiates this with MultiScale Off-Policy Bandit Learning (MSBL).

Result: Demonstrates effectiveness on recommender and conversational system tasks, showing improved optimization for long-term objectives.

Conclusion: The framework successfully bridges the gap between short-term actions and long-term goals, enabling AI systems to optimize for sustained outcomes.

Abstract: The feedback that AI systems (e.g., recommender systems, chatbots) collect
from user interactions is a crucial source of training data. While short-term
feedback (e.g., clicks, engagement) is widely used for training, there is ample
evidence that optimizing short-term feedback does not necessarily achieve the
desired long-term objectives. Unfortunately, directly optimizing for long-term
objectives is challenging, and we identify the disconnect in the timescales of
short-term interventions (e.g., rankings) and the long-term feedback (e.g.,
user retention) as one of the key obstacles. To overcome this disconnect, we
introduce the framework of MultiScale Policy Learning to contextually reconcile
that AI systems need to act and optimize feedback at multiple interdependent
timescales. Following a PAC-Bayes motivation, we show how the lower timescales
with more plentiful data can provide a data-dependent hierarchical prior for
faster learning at higher scales, where data is more scarce. As a result, the
policies at all levels effectively optimize for the long-term. We instantiate
the framework with MultiScale Off-Policy Bandit Learning (MSBL) and demonstrate
its effectiveness on three tasks relating to recommender and conversational
systems.

</details>


### [743] [Beyond Verifiable Rewards: Scaling Reinforcement Learning for Language Models to Unverifiable Data](https://arxiv.org/pdf/2503.19618)
*Yunhao Tang, Sid Wang, Lovish Madaan, Rémi Munos*

Main category: cs.LG

TL;DR: JEPO scales RL to unverifiable data using Jensen's evidence lower bound, outperforming baselines on long-form tasks like proofs.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of scaling RL to unverifiable data, where ground truth is hard to match (e.g., long-form answers like proofs).

Method: Proposes JEPO, applying Jensen's evidence lower bound to treat chain-of-thought as a latent variable in generative processes.

Result: JEPO matches RL on verifiable data, improves on semi-verifiable data, and outperforms baselines on unverifiable data.

Conclusion: JEPO effectively scales RL to unverifiable tasks, demonstrating robustness across data types.

Abstract: We propose to scale RL to unverifiable data with a novel algorithm JEPO
(Jensen's Evidence lower bound Policy Optimization). While most prior efforts
on scaling RL for LLMs focus on verifiable data where ground truth answers are
typically short-form and can be matched easily; we investigate the case where
such assumptions are less valid (e.g., when answers are long-form such as
mathematical proofs). To scale RL training to unverifiable data with
contemporary training constraints, we propose JEPO. JEPO applies Jensen's
evidence lower bound, a pragmatic simplification of the evidence lower bound
which views chain-of-thought as a latent variable in the generative process. We
show that on verifiable data (math), JEPO is as effective as RL with verifiable
rewards; on semi-verifiable data (numina), JEPO improves on soft-match based
evaluations compared to RL with verifiable rewards which can only leverage a
subset of the data source; finally, on unverifiable data (numina-proof), JEPO
outperforms SFT and a few ablation baselines on likelihood evaluations.

</details>


### [744] [Cooperation of Experts: Fusing Heterogeneous Information with Large Margin](https://arxiv.org/pdf/2505.20853)
*Shuo Wang, Shunyang Huang, Jinghui Yuan, Zhixiang Shen, Zhao Kang*

Main category: cs.LG

TL;DR: The paper introduces the Cooperation of Experts (CoE) framework to fuse heterogeneous data by encoding it into unified multiplex networks, leveraging domain-specific encoders and a novel optimization strategy for superior performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with the heterogeneity of object patterns across semantic spaces, limiting effective data fusion.

Method: CoE uses dedicated encoders as domain-specific experts, collaborating via a large margin mechanism and tailored optimization.

Result: Theoretical and experimental results confirm CoE's feasibility, stability, and outperformance across benchmarks.

Conclusion: CoE offers a robust, flexible solution for heterogeneous data fusion, with demonstrated broad applicability.

Abstract: Fusing heterogeneous information remains a persistent challenge in modern
data analysis. While significant progress has been made, existing approaches
often fail to account for the inherent heterogeneity of object patterns across
different semantic spaces. To address this limitation, we propose the
Cooperation of Experts (CoE) framework, which encodes multi-typed information
into unified heterogeneous multiplex networks. By overcoming modality and
connection differences, CoE provides a powerful and flexible model for
capturing the intricate structures of real-world complex data. In our
framework, dedicated encoders act as domain-specific experts, each specializing
in learning distinct relational patterns in specific semantic spaces. To
enhance robustness and extract complementary knowledge, these experts
collaborate through a novel large margin mechanism supported by a tailored
optimization strategy. Rigorous theoretical analyses guarantee the framework's
feasibility and stability, while extensive experiments across diverse
benchmarks demonstrate its superior performance and broad applicability. Our
code is available at https://github.com/strangeAlan/CoE.

</details>


### [745] [Evaluation of the impact of expert knowledge: How decision support scores impact the effectiveness of automatic knowledge-driven feature engineering (aKDFE)](https://arxiv.org/pdf/2504.05928)
*Olof Björneld, Tora Hammar, Daniel Nilsson, Alisa Lincke, Welf Löwe*

Main category: cs.LG

TL;DR: The study evaluates automatic Knowledge-Driven Feature Engineering (aKDFE) for predicting Adverse Drug Events (ADEs) from EHR data, finding patient-centric transformation improves performance, while domain-specific risk scores did not.


<details>
  <summary>Details</summary>
Motivation: ADEs pose significant healthcare challenges, and improving their prediction from EHR data can enhance patient safety and reduce costs.

Method: The study compares aKDFE with automated event-based KDD, incorporating domain-specific ADE risk scores and patient-centric transformations.

Result: aKDFE's patient-centric transformation improved ADE prediction, while domain-specific risk scores did not enhance performance. High AUROC values indicated strong feature correlations.

Conclusion: aKDFE, especially with patient-centric transformation, enhances ADE prediction. Future work will explore attention-based models and automatic domain knowledge integration.

Abstract: Adverse Drug Events (ADEs), harmful medication effects, pose significant
healthcare challenges, impacting patient safety and costs. This study evaluates
automatic Knowledge-Driven Feature Engineering (aKDFE) for improved ADE
prediction from Electronic Health Record (EHR) data, comparing it with
automated event-based Knowledge Discovery in Databases (KDD). We investigated
how incorporating domain-specific ADE risk scores for prolonged heart QT
interval, extracted from the Janusmed Riskprofile (Janusmed) Clinical Decision
Support System (CDSS), affects prediction performance using EHR data and
medication handling events. Results indicate that, while aKDFE step 1
(event-based feature generation) alone did not significantly improve ADE
prediction performance, aKDFE step 2 (patient-centric transformation) enhances
the prediction performance. High Area Under the Receiver Operating
Characteristic curve (AUROC) values suggest strong feature correlations to the
outcome, aligning with the predictive power of patients' prior healthcare
history for ADEs. Statistical analysis did not confirm that incorporating the
Janusmed information (i) risk scores and (ii) medication route of
administration into the model's feature set enhanced predictive performance.
However, the patient-centric transformation applied by aKDFE proved to be a
highly effective feature engineering approach. Limitations include a
single-project focus, potential bias from machine learning pipeline methods,
and reliance on AUROC. In conclusion, aKDFE, particularly with patient-centric
transformation, improves ADE prediction from EHR data. Future work will explore
attention-based models, event feature sequences, and automatic methods for
incorporating domain knowledge into the aKDFE framework.

</details>


### [746] [GraphOmni: A Comprehensive and Extendable Benchmark Framework for Large Language Models on Graph-theoretic Tasks](https://arxiv.org/pdf/2504.12764)
*Hao Xu, Xiangru Jian, Xinjian Zhao, Wei Pang, Chao Zhang, Suyuchen Wang, Qixin Zhang, Zhengyuan Dong, Joao Monteiro, Bang Liu, Qiuzhuang Sun, Tianshu Yu*

Main category: cs.LG

TL;DR: GraphOmni is a benchmark for evaluating LLMs on graph-theoretic tasks, revealing performance variability and proposing an adaptive framework for improvement.


<details>
  <summary>Details</summary>
Motivation: To assess and enhance LLMs' reasoning capabilities on graph tasks, addressing gaps in scope and depth of prior evaluations.

Method: GraphOmni includes diverse graph types, serialization formats, and prompting schemes, evaluated systematically. A reinforcement learning-inspired framework is proposed for adaptive factor selection.

Result: State-of-the-art models like Claude-3.5 and o4-mini perform best but show room for improvement. Performance varies with combinations of factors, highlighting the need for comprehensive evaluation.

Conclusion: GraphOmni provides a robust benchmark for advancing LLM-based graph reasoning, with findings encouraging tailored approaches and further research.

Abstract: This paper introduces GraphOmni, a comprehensive benchmark designed to
evaluate the reasoning capabilities of LLMs on graph-theoretic tasks
articulated in natural language. GraphOmni encompasses diverse graph types,
serialization formats, and prompting schemes, significantly exceeding prior
efforts in both scope and depth. Through extensive systematic evaluation, we
identify critical interactions among these dimensions, demonstrating their
substantial impact on model performance. Our experiments reveal that
state-of-the-art models like Claude-3.5 and o4-mini consistently outperform
other models, yet even these leading models exhibit substantial room for
improvement. Performance variability is evident depending on the specific
combinations of factors we considered, underscoring the necessity of
comprehensive evaluations across these interconnected dimensions. Additionally,
we observe distinct impacts of serialization and prompting strategies between
open-source and closed-source models, encouraging the development of tailored
approaches. Motivated by the findings, we also propose a reinforcement
learning-inspired framework that adaptively selects the optimal factors
influencing LLM reasoning capabilities. This flexible and extendable benchmark
not only deepens our understanding of LLM performance on structured tasks but
also provides a robust foundation for advancing research in LLM-based graph
reasoning. The code and datasets are available at
https://github.com/GAI-Community/GraphOmni.

</details>


### [747] [SageAttention2++: A More Efficient Implementation of SageAttention2](https://arxiv.org/pdf/2505.21136)
*Jintao Zhang, Xiaoming Xu, Jia Wei, Haofeng Huang, Pengle Zhang, Chendong Xiang, Jun Zhu, Jianfei Chen*

Main category: cs.LG

TL;DR: SageAttention2++ improves attention efficiency by using FP8 Matmul with FP16 accumulation, achieving 3.9x speedup over FlashAttention while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: The quadratic time complexity of attention with sequence length necessitates efficiency improvements.

Method: Utilizes FP8 Matmul accumulated in FP16 for faster computation, building on SageAttention2's quantization approach.

Result: Achieves 3.9x speedup over FlashAttention with negligible accuracy loss, applicable to language, image, and video models.

Conclusion: SageAttention2++ effectively accelerates attention mechanisms without compromising performance, with code available for public use.

Abstract: The efficiency of attention is critical because its time complexity grows
quadratically with sequence length. SageAttention2 addresses this by utilizing
quantization to accelerate matrix multiplications (Matmul) in attention. To
further accelerate SageAttention2, we propose to utilize the faster instruction
of FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8
Matmul used in SageAttention2. Our experiments show that SageAttention2++
achieves a 3.9x speedup over FlashAttention while maintaining the same
attention accuracy as SageAttention2. This means SageAttention2++ effectively
accelerates various models, including those for language, image, and video
generation, with negligible end-to-end metrics loss. The code will be available
at https://github.com/thu-ml/SageAttention.

</details>


### [748] [Geometry-Informed Neural Operator Transformer](https://arxiv.org/pdf/2504.19452)
*Qibang Liu, Vincient Zhong, Hadi Meidani, Diab Abueidda, Seid Koric, Philippe Geubelle*

Main category: cs.LG

TL;DR: GINOT combines transformers with neural operators for efficient PDE predictions on arbitrary geometries, achieving high accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: To improve computational efficiency and accuracy in solving PDEs for arbitrary geometries using machine learning.

Method: Integrates transformer architecture with neural operators, encoding geometry via point clouds and attention mechanisms.

Result: Validated on challenging datasets, GINOT shows high accuracy and generalization for 2D/3D geometries.

Conclusion: GINOT is a robust and efficient surrogate model for PDE predictions on complex geometries.

Abstract: Machine-learning-based surrogate models offer significant computational
efficiency and faster simulations compared to traditional numerical methods,
especially for problems requiring repeated evaluations of partial differential
equations. This work introduces the Geometry-Informed Neural Operator
Transformer (GINOT), which integrates the transformer architecture with the
neural operator framework to enable forward predictions for arbitrary
geometries. GINOT encodes the surface points cloud of a geometry using a
sampling and grouping mechanism combined with an attention mechanism, ensuring
invariance to point order and padding while maintaining robustness to
variations in point density. The geometry information is seamlessly integrated
with query points in the solution decoder through the attention mechanism. The
performance of GINOT is validated on multiple challenging datasets, showcasing
its high accuracy and strong generalization capabilities for complex and
arbitrary 2D and 3D geometries.

</details>


### [749] [ItDPDM: Information-Theoretic Discrete Poisson Diffusion Model](https://arxiv.org/pdf/2505.05082)
*Sagnik Bhattacharya, Abhiram Gorle, Ahsan Bilal, Connor Ding, Amit Kumar Singh Yadav, Tsachy Weissman*

Main category: cs.LG

TL;DR: ItDPDM is a discrete diffusion model for non-negative data, combining exact likelihood estimation and discrete-state modeling, outperforming prior methods in likelihood and sampling.


<details>
  <summary>Details</summary>
Motivation: Existing methods for discrete data modeling rely on continuous embeddings or variational bounds, leading to suboptimal performance. ItDPDM addresses these issues jointly.

Method: Introduces ItDPDM, a discrete Poisson diffusion model with an information-theoretic Poisson Reconstruction Loss (PRL) for exact likelihood estimation.

Result: ItDPDM improves likelihood and sampling performance on synthetic and real-world datasets like symbolic music and images.

Conclusion: ItDPDM demonstrates robust discrete generative modeling, achieving superior likelihood and competitive generation quality.

Abstract: Generative modeling of non-negative, discrete data, such as symbolic music,
remains challenging due to two persistent limitations in existing methods.
Firstly, many approaches rely on modeling continuous embeddings, which is
suboptimal for inherently discrete data distributions. Secondly, most models
optimize variational bounds rather than exact data likelihood, resulting in
inaccurate likelihood estimates and degraded sampling quality. While recent
diffusion-based models have addressed these issues separately, we tackle them
jointly. In this work, we introduce the Information-Theoretic Discrete Poisson
Diffusion Model (ItDPDM), inspired by photon arrival process, which combines
exact likelihood estimation with fully discrete-state modeling. Central to our
approach is an information-theoretic Poisson Reconstruction Loss (PRL) that has
a provable exact relationship with the true data likelihood. ItDPDM achieves
improved likelihood and sampling performance over prior discrete and continuous
diffusion models on a variety of synthetic discrete datasets. Furthermore, on
real-world datasets such as symbolic music and images, ItDPDM attains superior
likelihood estimates and competitive generation quality-demonstrating a proof
of concept for distribution-robust discrete generative modeling.

</details>


### [750] [Bridging Supervised Learning and Reinforcement Learning in Math Reasoning](https://arxiv.org/pdf/2505.18116)
*Huayu Chen, Kaiwen Zheng, Qinsheng Zhang, Ganqu Cui, Yin Cui, Haotian Ye, Tsung-Yi Lin, Ming-Yu Liu, Jun Zhu, Haoxiang Wang*

Main category: cs.LG

TL;DR: NFT, a supervised learning method, enables LLMs to self-improve using negative feedback, matching or surpassing RL methods like GRPO and DAPO in math reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Challenge the idea that self-improvement is exclusive to RL by proposing a supervised approach (NFT) that leverages negative feedback for autonomous improvement.

Method: NFT constructs an implicit negative policy from self-generated negative answers, optimizing the LLM directly on all generations.

Result: NFT outperforms SL baselines and matches/surpasses RL methods (GRPO, DAPO) in math reasoning tasks.

Conclusion: NFT bridges the gap between SL and RL in binary-feedback learning, showing equivalence between NFT and GRPO in strict-on-policy training.

Abstract: Reinforcement Learning (RL) has played a central role in the recent surge of
LLMs' math abilities by enabling self-improvement through binary verifier
signals. In contrast, Supervised Learning (SL) is rarely considered for such
verification-driven training, largely due to its heavy reliance on reference
answers and inability to reflect on mistakes. In this work, we challenge the
prevailing notion that self-improvement is exclusive to RL and propose
Negative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to
reflect on their failures and improve autonomously with no external teachers.
In online training, instead of throwing away self-generated negative answers,
NFT constructs an implicit negative policy to model them. This implicit policy
is parameterized with the same positive LLM we target to optimize on positive
data, enabling direct policy optimization on all LLMs' generations. We conduct
experiments on 7B and 32B models in math reasoning tasks. Results consistently
show that through the additional leverage of negative feedback, NFT
significantly improves over SL baselines like Rejection sampling Fine-Tuning,
matching or even surpassing leading RL algorithms like GRPO and DAPO.
Furthermore, we demonstrate that NFT and GRPO are actually equivalent in
strict-on-policy training, even though they originate from entirely different
theoretical foundations. Our experiments and theoretical findings bridge the
gap between SL and RL methods in binary-feedback learning systems.

</details>


### [751] [Identifying Causal Direction via Variational Bayesian Compression](https://arxiv.org/pdf/2505.07503)
*Quang-Duy Tran, Bao Duong, Phuoc Nguyen, Thin Nguyen*

Main category: cs.LG

TL;DR: The paper proposes using variational Bayesian learning of neural networks to improve cause-effect identification, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The challenge of distinguishing cause and effect from observational data, with existing methods compromising between model fitness and computational complexity.

Method: Leverages variational Bayesian learning of neural networks to approximate codelengths, enhancing model fitness and succinctness.

Result: Outperforms related complexity-based and structural causal model regression-based approaches in experiments.

Conclusion: The proposed method effectively improves cause-effect identification with better performance and reduced computational complexity.

Abstract: Telling apart the cause and effect between two random variables with purely
observational data is a challenging problem that finds applications in various
scientific disciplines. A key principle utilized in this task is the
algorithmic Markov condition, which postulates that the joint distribution,
when factorized according to the causal direction, yields a more succinct
codelength compared to the anti-causal direction. Previous approaches
approximate these codelengths by relying on simple functions or Gaussian
processes (GPs) with easily evaluable complexity, compromising between model
fitness and computational complexity. To overcome these limitations, we propose
leveraging the variational Bayesian learning of neural networks as an
interpretation of the codelengths. Consequently, we can enhance the model
fitness while promoting the succinctness of the codelengths, while avoiding the
significant computational complexity of the GP-based approaches. Extensive
experiments on both synthetic and real-world benchmarks in cause-effect
identification demonstrate the effectiveness of our proposed method, surpassing
the overall performance of related complexity-based and structural causal model
regression-based approaches.

</details>


### [752] [Training on Plausible Counterfactuals Removes Spurious Correlations](https://arxiv.org/pdf/2505.16583)
*Shpresim Sadiku, Kartikeya Chitranshi, Hiroshi Kera, Sebastian Pokutta*

Main category: cs.LG

TL;DR: Training classifiers on plausible counterfactual explanations (p-CFEs) with incorrect labels improves accuracy and reduces bias compared to adversarial perturbations.


<details>
  <summary>Details</summary>
Motivation: To explore whether classifiers can learn effectively from p-CFEs labeled with incorrect target classes, extending the adversarial perturbation paradigm.

Method: Train classifiers on p-CFEs labeled with incorrect target classes and evaluate their performance on unperturbed inputs.

Result: Classifiers trained on p-CFEs achieve high in-distribution accuracy and significantly reduced bias from spurious correlations.

Conclusion: Learning from p-CFEs is more effective than adversarial perturbations, enhancing classifier performance and fairness.

Abstract: Plausible counterfactual explanations (p-CFEs) are perturbations that
minimally modify inputs to change classifier decisions while remaining
plausible under the data distribution. In this study, we demonstrate that
classifiers can be trained on p-CFEs labeled with induced \emph{incorrect}
target classes to classify unperturbed inputs with the original labels. While
previous studies have shown that such learning is possible with adversarial
perturbations, we extend this paradigm to p-CFEs. Interestingly, our
experiments reveal that learning from p-CFEs is even more effective: the
resulting classifiers achieve not only high in-distribution accuracy but also
exhibit significantly reduced bias with respect to spurious correlations.

</details>


### [753] [Backdoors in DRL: Four Environments Focusing on In-distribution Triggers](https://arxiv.org/pdf/2505.17248)
*Chace Ashcraft, Ted Staley, Josh Carney, Cameron Hickert, Kiran Karra, Nathan Drenkow*

Main category: cs.LG

TL;DR: The paper explores backdoor attacks in deep reinforcement learning (DRL), focusing on in-distribution triggers, and demonstrates their viability as security threats.


<details>
  <summary>Details</summary>
Motivation: To address the security risks posed by backdoor attacks in open-source neural networks, especially in DRL, where in-distribution triggers are easier to exploit.

Method: Developed trojans for DRL agents, tested in four RL environments (LavaWorld, Randomized LavaWorld, Colorful Memory, Modified Safety Gymnasium), and trained clean/backdoored models.

Result: In-distribution triggers, though harder to implement and learn, remain viable threats in DRL, even with basic data poisoning.

Conclusion: In-distribution backdoor attacks are a significant security concern in DRL, warranting further research for mitigation.

Abstract: Backdoor attacks, or trojans, pose a security risk by concealing undesirable
behavior in deep neural network models. Open-source neural networks are
downloaded from the internet daily, possibly containing backdoors, and
third-party model developers are common. To advance research on backdoor attack
mitigation, we develop several trojans for deep reinforcement learning (DRL)
agents. We focus on in-distribution triggers, which occur within the agent's
natural data distribution, since they pose a more significant security threat
than out-of-distribution triggers due to their ease of activation by the
attacker during model deployment. We implement backdoor attacks in four
reinforcement learning (RL) environments: LavaWorld, Randomized LavaWorld,
Colorful Memory, and Modified Safety Gymnasium. We train various models, both
clean and backdoored, to characterize these attacks. We find that
in-distribution triggers can require additional effort to implement and be more
challenging for models to learn, but are nevertheless viable threats in DRL
even using basic data poisoning attacks.

</details>


### [754] [C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in Large Language Models](https://arxiv.org/pdf/2505.17773)
*Amir Hossein Rahmati, Sanket Jantre, Weifeng Zhang, Yucheng Wang, Byung-Jun Yoon, Nathan M. Urban, Xiaoning Qian*

Main category: cs.LG

TL;DR: C-LoRA improves LoRA by dynamically adapting uncertainty estimates per input, mitigating overconfidence and overfitting in few-shot LLM fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LoRA's overconfident predictions in data-scarce settings and neglect of input characteristics in uncertainty estimation.

Method: Develops lightweight, input-contextualized LoRA modules for dynamic uncertainty adaptation.

Result: Outperforms state-of-the-art methods in uncertainty quantification and generalization, with ablation studies validating contextual modules.

Conclusion: C-LoRA sets a new standard for robust, uncertainty-aware LLM fine-tuning in few-shot scenarios.

Abstract: Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning
large language models (LLMs), but it often produces overconfident predictions
in data-scarce few-shot settings. To address this issue, several classical
statistical learning approaches have been repurposed for scalable
uncertainty-aware LoRA fine-tuning. However, these approaches neglect how input
characteristics affect the predictive uncertainty estimates. To address this
limitation, we propose Contextual Low-Rank Adaptation (\textbf{C-LoRA}) as a
novel uncertainty-aware and parameter efficient fine-tuning approach, by
developing new lightweight LoRA modules contextualized to each input data
sample to dynamically adapt uncertainty estimates. Incorporating data-driven
contexts into the parameter posteriors, C-LoRA mitigates overfitting, achieves
well-calibrated uncertainties, and yields robust predictions. Extensive
experiments demonstrate that C-LoRA consistently outperforms the
state-of-the-art uncertainty-aware LoRA methods in both uncertainty
quantification and model generalization. Ablation studies further confirm the
critical role of our contextual modules in capturing sample-specific
uncertainties. C-LoRA sets a new standard for robust, uncertainty-aware LLM
fine-tuning in few-shot regimes.

</details>


### [755] [GenPO: Generative Diffusion Models Meet On-Policy Reinforcement Learning](https://arxiv.org/pdf/2505.18763)
*Shutong Ding, Ke Hu, Shan Zhong, Haoyang Luo, Weinan Zhang, Jingya Wang, Jun Wang, Ye Shi*

Main category: cs.LG

TL;DR: GenPO integrates diffusion policies into on-policy RL using exact diffusion inversion, enabling invertible action mappings and overcoming log-likelihood computation challenges.


<details>
  <summary>Details</summary>
Motivation: The gap in integrating diffusion policies into on-policy RL frameworks like PPO, despite their exploration capabilities and multimodality, motivates this work.

Method: GenPO uses exact diffusion inversion and a doubled dummy action mechanism for invertible mappings, enabling log-likelihood computation and unbiased entropy/KL divergence estimation.

Result: GenPO outperforms baselines on eight IsaacLab benchmarks, including locomotion, manipulation, and control tasks.

Conclusion: GenPO successfully bridges the gap, enabling diffusion policies in on-policy RL for large-scale parallelized training and real-world deployment.

Abstract: Recent advances in reinforcement learning (RL) have demonstrated the powerful
exploration capabilities and multimodality of generative diffusion-based
policies. While substantial progress has been made in offline RL and off-policy
RL settings, integrating diffusion policies into on-policy frameworks like PPO
remains underexplored. This gap is particularly significant given the
widespread use of large-scale parallel GPU-accelerated simulators, such as
IsaacLab, which are optimized for on-policy RL algorithms and enable rapid
training of complex robotic tasks. A key challenge lies in computing
state-action log-likelihoods under diffusion policies, which is straightforward
for Gaussian policies but intractable for flow-based models due to irreversible
forward-reverse processes and discretization errors (e.g., Euler-Maruyama
approximations). To bridge this gap, we propose GenPO, a generative policy
optimization framework that leverages exact diffusion inversion to construct
invertible action mappings. GenPO introduces a novel doubled dummy action
mechanism that enables invertibility via alternating updates, resolving
log-likelihood computation barriers. Furthermore, we also use the action
log-likelihood for unbiased entropy and KL divergence estimation, enabling
KL-adaptive learning rates and entropy regularization in on-policy updates.
Extensive experiments on eight IsaacLab benchmarks, including legged locomotion
(Ant, Humanoid, Anymal-D, Unitree H1, Go2), dexterous manipulation (Shadow
Hand), aerial control (Quadcopter), and robotic arm tasks (Franka), demonstrate
GenPO's superiority over existing RL baselines. Notably, GenPO is the first
method to successfully integrate diffusion policies into on-policy RL,
unlocking their potential for large-scale parallelized training and real-world
robotic deployment.

</details>


### [756] [Latent Mamba Operator for Partial Differential Equations](https://arxiv.org/pdf/2505.19105)
*Karn Tiwari, Niladri Dutta, N M Anoop Krishnan, Prathosh A P*

Main category: cs.LG

TL;DR: LaMO (Latent Mamba Operator) improves neural operators for solving PDEs by combining state-space models and kernel integrals, achieving SOTA performance with 32.3% better accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing neural operators for PDEs lack scalability, are computationally expensive, and struggle with long-range dependencies.

Method: LaMO integrates state-space models (SSMs) in latent space with kernel integral formulations of neural operators.

Result: LaMO outperforms baselines by 32.3% in PDE solution approximation across diverse benchmarks.

Conclusion: LaMO effectively addresses scalability and dependency issues in neural operators, offering superior performance for PDE solutions.

Abstract: Neural operators have emerged as powerful data-driven frameworks for solving
Partial Differential Equations (PDEs), offering significant speedups over
numerical methods. However, existing neural operators struggle with scalability
in high-dimensional spaces, incur high computational costs, and face challenges
in capturing continuous and long-range dependencies in PDE dynamics. To address
these limitations, we introduce the Latent Mamba Operator (LaMO), which
integrates the efficiency of state-space models (SSMs) in latent space with the
expressive power of kernel integral formulations in neural operators. We also
establish a theoretical connection between state-space models (SSMs) and the
kernel integral of neural operators. Extensive experiments across diverse PDE
benchmarks on regular grids, structured meshes, and point clouds covering solid
and fluid physics datasets, LaMOs achieve consistent state-of-the-art (SOTA)
performance, with a 32.3% improvement over existing baselines in solution
operator approximation, highlighting its efficacy in modeling complex PDE
solutions.

</details>


### [757] [Data-Distill-Net: A Data Distillation Approach Tailored for Reply-based Continual Learning](https://arxiv.org/pdf/2505.20135)
*Wenyang Liao, Quanziang Wang, Yichen Wu, Renzhen Wang, Deyu Meng*

Main category: cs.LG

TL;DR: A new dataset distillation framework for continual learning (CL) is proposed, using a learnable memory buffer and lightweight distillation to mitigate forgetting.


<details>
  <summary>Details</summary>
Motivation: Existing replay-based CL methods struggle due to limited buffer capacity and heuristic data selection, leading to ineffective knowledge consolidation.

Method: Introduces a learnable memory buffer and lightweight distillation module to distill global information and generate soft labels, reducing computational overhead.

Result: Achieves competitive performance and effectively mitigates forgetting across various datasets.

Conclusion: The proposed framework addresses limitations of replay-based CL and offers a practical solution with public code availability.

Abstract: Replay-based continual learning (CL) methods assume that models trained on a
small subset can also effectively minimize the empirical risk of the complete
dataset. These methods maintain a memory buffer that stores a sampled subset of
data from previous tasks to consolidate past knowledge. However, this
assumption is not guaranteed in practice due to the limited capacity of the
memory buffer and the heuristic criteria used for buffer data selection. To
address this issue, we propose a new dataset distillation framework tailored
for CL, which maintains a learnable memory buffer to distill the global
information from the current task data and accumulated knowledge preserved in
the previous memory buffer. Moreover, to avoid the computational overhead and
overfitting risks associated with parameterizing the entire buffer during
distillation, we introduce a lightweight distillation module that can achieve
global information distillation solely by generating learnable soft labels for
the memory buffer data. Extensive experiments show that, our method can achieve
competitive results and effectively mitigates forgetting across various
datasets. The source code will be publicly available.

</details>


### [758] [FireQ: Fast INT4-FP8 Kernel and RoPE-aware Quantization for LLM Inference Acceleration](https://arxiv.org/pdf/2505.20839)
*Daehyeon Baek, Jieun Choi, Jimyoung Son, Kyungmin Bin, Seungbeom Choi, Kihyo Moon, Minsung Jang, Hyojung Lee*

Main category: cs.LG

TL;DR: FireQ is a co-designed PTQ framework with an INT4-FP8 kernel, improving LLM inference throughput via optimized quantization and pipelining, while minimizing accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Memory bandwidth constraints limit inference throughput in large language models, motivating the need for efficient post-training quantization (PTQ).

Method: FireQ quantizes weights/key-values to INT4 and activations/queries to FP8, introduces a three-stage pipelining for prefill, and uses outlier smoothing techniques for linear and attention layers.

Result: FireQ achieves 1.68x faster inference in feed-forward layers on Llama2-7B and 1.26x faster prefill on Llama3-8B vs. QServe, with negligible accuracy loss.

Conclusion: FireQ effectively balances speed and accuracy, outperforming state-of-the-art methods in LLM inference.

Abstract: As large language models become increasingly prevalent, memory bandwidth
constraints significantly limit inference throughput, motivating post-training
quantization (PTQ). In this paper, we propose FireQ, a co-designed PTQ
framework and an INT4-FP8 matrix multiplication kernel that accelerates LLM
inference across all linear layers. Specifically, FireQ quantizes linear layer
weights and key-values to INT4, and activations and queries to FP8,
significantly enhancing throughput. Additionally, we introduce a three-stage
pipelining for the prefill phase, which modifies the FlashAttention-3 kernel,
effectively reducing time-to-first-token in the prefill phase. To minimize
accuracy loss from quantization, we develop novel outlier smoothing techniques
tailored separately for linear and attention layers. In linear layers, we
explicitly use per-tensor scaling to prevent underflow caused by the FP8
quantization scaling factor of INT4 quantization, and channel-wise scaling to
compensate for coarse granularity of INT4. In attention layers, we address
quantization challenges posed by rotary positional embeddings (RoPE) by
combining pre-RoPE and post-RoPE scaling strategies. FireQ significantly
outperforms state-of-the-art methods, achieving 1.68x faster inference in
feed-forward network layers on Llama2-7B and 1.26x faster prefill phase
performance on Llama3-8B compared to QServe, with negligible accuracy loss.

</details>


### [759] [Improved Bounds for Swap Multicalibration and Swap Omniprediction](https://arxiv.org/pdf/2505.20885)
*Haipeng Luo, Spandan Senapati, Vatsal Sharan*

Main category: cs.LG

TL;DR: The paper addresses multicalibration and omniprediction, proposing an efficient algorithm that improves error bounds and sample complexity rates, outperforming previous results.


<details>
  <summary>Details</summary>
Motivation: The work aims to resolve an open problem raised by Garg et al. (2024) regarding efficient multicalibration and omniprediction, with a focus on achieving better error rates and sample complexities.

Method: An efficient algorithm is introduced to achieve improved multicalibration and omniprediction error bounds, specifically $O(T^{\frac{1}{3}})$ for $\ell_{2}$-swap multicalibration and $O(T^{\frac{2}{3}})$ for $\ell_{1}$-swap multicalibration and omniprediction.

Result: The algorithm significantly improves error rates and sample complexities, such as $O(\varepsilon^{-3})$ for learning an $\varepsilon$-swap omnipredictor, surpassing prior bounds like $O(T^{\frac{7}{8}})$.

Conclusion: The proposed algorithm advances multicalibration and omniprediction by providing tighter bounds and better sample complexities, addressing the open problem effectively.

Abstract: In this paper, we consider the related problems of multicalibration -- a
multigroup fairness notion and omniprediction -- a simultaneous loss
minimization paradigm, both in the distributional and online settings. The
recent work of Garg et al. (2024) raised the open problem of whether it is
possible to efficiently achieve $O(\sqrt{T})$ $\ell_{2}$-multicalibration error
against bounded linear functions. In this paper, we answer this question in a
strongly affirmative sense. We propose an efficient algorithm that achieves
$O(T^{\frac{1}{3}})$ $\ell_{2}$-swap multicalibration error (both in high
probability and expectation). On propagating this bound onward, we obtain
significantly improved rates for $\ell_{1}$-swap multicalibration and swap
omniprediction for a loss class of convex Lipschitz functions. In particular,
we show that our algorithm achieves $O(T^{\frac{2}{3}})$ $\ell_{1}$-swap
multicalibration and swap omniprediction errors, thereby improving upon the
previous best-known bound of $O(T^{\frac{7}{8}})$. As a consequence of our
improved online results, we further obtain several improved sample complexity
rates in the distributional setting. In particular, we establish a
$O(\varepsilon ^ {-3})$ sample complexity of efficiently learning an
$\varepsilon$-swap omnipredictor for the class of convex and Lipschitz
functions, $O(\varepsilon ^{-2.5})$ sample complexity of efficiently learning
an $\varepsilon$-swap agnostic learner for the squared loss, and $O(\varepsilon
^ {-5}), O(\varepsilon ^ {-2.5})$ sample complexities of learning $\ell_{1},
\ell_{2}$-swap multicalibrated predictors against linear functions, all of
which significantly improve on the previous best-known bounds.

</details>


### [760] [Understanding the behavior of representation forgetting in continual learning](https://arxiv.org/pdf/2505.20970)
*Joonkyu Kim, Yejin Kim, Jy-yong Sohn*

Main category: cs.LG

TL;DR: The paper introduces a theoretical analysis of representation forgetting in continual learning, proposing a new metric (representation discrepancy) to measure it and deriving key insights about its dynamics.


<details>
  <summary>Details</summary>
Motivation: Catastrophic forgetting in continual learning is a critical issue, and understanding representation forgetting (at hidden layers) is essential for improving learning systems.

Method: The authors propose a new metric, representation discrepancy, to measure representation forgetting and analyze it theoretically. They also conduct experiments on real datasets (Split-CIFAR100, ImageNet1K) to validate findings.

Result: Key findings include: forgetting increases with layer depth but slows with network width. The proposed metric effectively captures representation forgetting.

Conclusion: The theoretical and experimental analysis provides insights into representation forgetting, aiding the development of better continual learning methods.

Abstract: In continual learning scenarios, catastrophic forgetting of previously
learned tasks is a critical issue, making it essential to effectively measure
such forgetting. Recently, there has been growing interest in focusing on
representation forgetting, the forgetting measured at the hidden layer. In this
paper, we provide the first theoretical analysis of representation forgetting
and use this analysis to better understand the behavior of continual learning.
First, we introduce a new metric called representation discrepancy, which
measures the difference between representation spaces constructed by two
snapshots of a model trained through continual learning. We demonstrate that
our proposed metric serves as an effective surrogate for the representation
forgetting while remaining analytically tractable. Second, through mathematical
analysis of our metric, we derive several key findings about the dynamics of
representation forgetting: the forgetting occurs more rapidly to a higher
degree as the layer index increases, while increasing the width of the network
slows down the forgetting process. Third, we support our theoretical findings
through experiments on real image datasets, including Split-CIFAR100 and
ImageNet1K.

</details>


### [761] [Bridging Arbitrary and Tree Metrics via Differentiable Gromov Hyperbolicity](https://arxiv.org/pdf/2505.21073)
*Pierre Houedry, Nicolas Courty, Florestan Martin-Baillon, Laetitia Chapel, Titouan Vayer*

Main category: cs.LG

TL;DR: DeltaZero introduces a differentiable optimization framework to bridge arbitrary metrics to tree metrics, achieving state-of-the-art distortion with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: The deviation of arbitrary metrics from tree metrics is quantified by Gromov's δ-hyperbolicity, but existing approaches lack guarantees or perform moderately.

Method: DeltaZero uses a smooth surrogate for δ-hyperbolicity, enabling gradient-based optimization with tractable complexity and better worst-case guarantees.

Result: Experiments on synthetic and real-world datasets show DeltaZero consistently achieves state-of-the-art distortion.

Conclusion: DeltaZero provides a statistically justified, efficient solution for approximating arbitrary metrics with tree metrics.

Abstract: Trees and the associated shortest-path tree metrics provide a powerful
framework for representing hierarchical and combinatorial structures in data.
Given an arbitrary metric space, its deviation from a tree metric can be
quantified by Gromov's $\delta$-hyperbolicity. Nonetheless, designing
algorithms that bridge an arbitrary metric to its closest tree metric is still
a vivid subject of interest, as most common approaches are either heuristical
and lack guarantees, or perform moderately well. In this work, we introduce a
novel differentiable optimization framework, coined DeltaZero, that solves this
problem. Our method leverages a smooth surrogate for Gromov's
$\delta$-hyperbolicity which enables a gradient-based optimization, with a
tractable complexity. The corresponding optimization procedure is derived from
a problem with better worst case guarantees than existing bounds, and is
justified statistically. Experiments on synthetic and real-world datasets
demonstrate that our method consistently achieves state-of-the-art distortion.

</details>


### [762] [Copresheaf Topological Neural Networks: A Generalized Deep Learning Framework](https://arxiv.org/pdf/2505.21251)
*Mustafa Hajij, Lennart Bastian, Sarah Osentoski, Hardik Kabaria, John L. Davenport, Sheik Dawood, Balaji Cherukuri, Joseph G. Kocheemoolayil, Nastaran Shahmansouri, Adrian Lew, Theodore Papamarkou, Tolga Birdal*

Main category: cs.LG

TL;DR: CTNNs introduce a unifying framework for deep learning on structured data, leveraging copresheaves from algebraic topology to outperform conventional models.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of designing neural architectures tailored to specific tasks and data types, CTNNs provide a principled approach grounded in algebraic topology.

Method: CTNNs use copresheaves to generalize deep learning models, enabling solutions for representation learning challenges like long-range dependencies and non-Euclidean domains.

Result: Empirical results show CTNNs outperform baselines on structured data benchmarks, especially in hierarchical or localized tasks.

Conclusion: CTNNs offer a principled, multi-scale foundation for future deep learning architectures.

Abstract: We introduce copresheaf topological neural networks (CTNNs), a powerful and
unifying framework that encapsulates a wide spectrum of deep learning
architectures, designed to operate on structured data: including images, point
clouds, graphs, meshes, and topological manifolds. While deep learning has
profoundly impacted domains ranging from digital assistants to autonomous
systems, the principled design of neural architectures tailored to specific
tasks and data types remains one of the field's most persistent open
challenges. CTNNs address this gap by grounding model design in the language of
copresheaves, a concept from algebraic topology that generalizes and subsumes
most practical deep learning models in use today. This abstract yet
constructive formulation yields a rich design space from which theoretically
sound and practically effective solutions can be derived to tackle core
challenges in representation learning: long-range dependencies, oversmoothing,
heterophily, and non-Euclidean domains. Our empirical results on structured
data benchmarks demonstrate that CTNNs consistently outperform conventional
baselines, particularly in tasks requiring hierarchical or localized
sensitivity. These results underscore CTNNs as a principled, multi-scale
foundation for the next generation of deep learning architectures.

</details>


### [763] [OVERT: A Benchmark for Over-Refusal Evaluation on Text-to-Image Models](https://arxiv.org/pdf/2505.21347)
*Ziheng Cheng, Yixiao Huang, Hui Xu, Somayeh Sojoudi, Xuandong Zhao, Dawn Song, Song Mei*

Main category: cs.LG

TL;DR: OVERT is a benchmark for evaluating over-refusal in T2I models, revealing widespread issues and proposing a flexible framework for customized safety evaluations.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic evaluation for over-refusal in T2I models, which reduces their practical utility despite safety alignment efforts.

Method: Developed OVERT, a large-scale benchmark with synthetic evaluation data (4,600 benign and 1,785 harmful prompts), and tested leading T2I models.

Result: Over-refusal is widespread across categories, highlighting a safety-utility trade-off. Prompt rewriting was explored but often compromised faithfulness.

Conclusion: OVERT provides a tool for assessing and improving T2I models' safety alignment without sacrificing functionality, with potential for customization.

Abstract: Text-to-Image (T2I) models have achieved remarkable success in generating
visual content from text inputs. Although multiple safety alignment strategies
have been proposed to prevent harmful outputs, they often lead to overly
cautious behavior -- rejecting even benign prompts -- a phenomenon known as
$\textit{over-refusal}$ that reduces the practical utility of T2I models.
Despite over-refusal having been observed in practice, there is no large-scale
benchmark that systematically evaluates this phenomenon for T2I models. In this
paper, we present an automatic workflow to construct synthetic evaluation data,
resulting in OVERT ($\textbf{OVE}$r-$\textbf{R}$efusal evaluation on
$\textbf{T}$ext-to-image models), the first large-scale benchmark for assessing
over-refusal behaviors in T2I models. OVERT includes 4,600 seemingly harmful
but benign prompts across nine safety-related categories, along with 1,785
genuinely harmful prompts (OVERT-unsafe) to evaluate the safety-utility
trade-off. Using OVERT, we evaluate several leading T2I models and find that
over-refusal is a widespread issue across various categories (Figure 1),
underscoring the need for further research to enhance the safety alignment of
T2I models without compromising their functionality. As a preliminary attempt
to reduce over-refusal, we explore prompt rewriting; however, we find it often
compromises faithfulness to the meaning of the original prompts. Finally, we
demonstrate the flexibility of our generation framework in accommodating
diverse safety requirements by generating customized evaluation data adapting
to user-defined policies.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [764] [Uncovering Bottlenecks and Optimizing Scientific Lab Workflows with Cycle Time Reduction Agents](https://arxiv.org/pdf/2505.21534)
*Yao Fehlis*

Main category: cs.MA

TL;DR: CTRA is a LangGraph-based agentic workflow designed to optimize lab workflows by automating operational metric analysis, identifying bottlenecks, and reducing cycle times in pharmaceutical and biotech labs.


<details>
  <summary>Details</summary>
Motivation: Scientific labs face challenges in optimizing workflows due to task complexity and volume, particularly in compound screening and assay execution.

Method: CTRA consists of three agents: Question Creation Agent (initiates analysis), Operational Metrics Agents (data extraction/validation), and Insights Agents (reporting/visualization).

Result: CTRA's performance was evaluated on a lab dataset, demonstrating its ability to identify bottlenecks and reduce cycle times.

Conclusion: CTRA provides a scalable solution to accelerate pharmaceutical and biotechnological development by optimizing lab workflows.

Abstract: Scientific laboratories, particularly those in pharmaceutical and
biotechnology companies, encounter significant challenges in optimizing
workflows due to the complexity and volume of tasks such as compound screening
and assay execution. We introduce Cycle Time Reduction Agents (CTRA), a
LangGraph-based agentic workflow designed to automate the analysis of lab
operational metrics. CTRA comprises three main components: the Question
Creation Agent for initiating analysis, Operational Metrics Agents for data
extraction and validation, and Insights Agents for reporting and visualization,
identifying bottlenecks in lab processes. This paper details CTRA's
architecture, evaluates its performance on a lab dataset, and discusses its
potential to accelerate pharmaceutical and biotechnological development. CTRA
offers a scalable framework for reducing cycle times in scientific labs.

</details>


### [765] [Streamlining Resilient Kubernetes Autoscaling with Multi-Agent Systems via an Automated Online Design Framework](https://arxiv.org/pdf/2505.21559)
*Julien Soulé, Jean-Paul Jamont, Michel Occello, Louis-Marie Traonouez, Paul Théron*

Main category: cs.MA

TL;DR: The paper proposes a Multi-Agent System (MAS) for Horizontal Pod Autoscaling (HPA) in Kubernetes to enhance operational resilience under adversarial conditions like DDoS, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current HPA methods fail to handle dynamic, adversarial scenarios in Kubernetes clusters, focusing narrowly on single goals like latency.

Method: A four-phase framework: 1) digital twin modeling, 2) agent training in simulation, 3) behavior analysis for explainability, and 4) policy transfer to real clusters.

Result: The HPA MAS outperforms three state-of-the-art HPA systems in maintaining resilience under adversarial conditions.

Conclusion: Decomposing resilience into sub-goals with collaborative agents improves Kubernetes cluster resilience, validated by experimental results.

Abstract: In cloud-native systems, Kubernetes clusters with interdependent services
often face challenges to their operational resilience due to poor workload
management issues such as resource blocking, bottlenecks, or continuous pod
crashes. These vulnerabilities are further amplified in adversarial scenarios,
such as Distributed Denial-of-Service attacks (DDoS). Conventional Horizontal
Pod Autoscaling (HPA) approaches struggle to address such dynamic conditions,
while reinforcement learning-based methods, though more adaptable, typically
optimize single goals like latency or resource usage, neglecting broader
failure scenarios. We propose decomposing the overarching goal of maintaining
operational resilience into failure-specific sub-goals delegated to
collaborative agents, collectively forming an HPA Multi-Agent System (MAS). We
introduce an automated, four-phase online framework for HPA MAS design: 1)
modeling a digital twin built from cluster traces; 2) training agents in
simulation using roles and missions tailored to failure contexts; 3) analyzing
agent behaviors for explainability; and 4) transferring learned policies to the
real cluster. Experimental results demonstrate that the generated HPA MASs
outperform three state-of-the-art HPA systems in sustaining operational
resilience under various adversarial conditions in a proposed complex cluster.

</details>


### [766] [Herd Behavior: Investigating Peer Influence in LLM-based Multi-Agent Systems](https://arxiv.org/pdf/2505.21588)
*Young-Min Cho, Sharath Chandra Guntuku, Lyle Ungar*

Main category: cs.MA

TL;DR: The paper explores herd behavior in LLM-based multi-agent systems, identifying factors like confidence gaps and information presentation that influence conformity, and shows controlled herd behavior can improve collaboration.


<details>
  <summary>Details</summary>
Motivation: To understand underexplored peer influence dynamics in LLM-based multi-agent systems, particularly herd behavior.

Method: Conducted controlled experiments to analyze how factors like confidence gaps and information presentation affect herd behavior.

Result: Found that herd behavior is influenced by confidence disparities and information format, and can be controlled to enhance collaboration.

Conclusion: Provides insights into LLM social dynamics and suggests pathways for better multi-agent collaboration frameworks.

Abstract: Recent advancements in Large Language Models (LLMs) have enabled the
emergence of multi-agent systems where LLMs interact, collaborate, and make
decisions in shared environments. While individual model behavior has been
extensively studied, the dynamics of peer influence in such systems remain
underexplored. In this paper, we investigate herd behavior, the tendency of
agents to align their outputs with those of their peers, within LLM-based
multi-agent interactions. We present a series of controlled experiments that
reveal how herd behaviors are shaped by multiple factors. First, we show that
the gap between self-confidence and perceived confidence in peers significantly
impacts an agent's likelihood to conform. Second, we find that the format in
which peer information is presented plays a critical role in modulating the
strength of herd behavior. Finally, we demonstrate that the degree of herd
behavior can be systematically controlled, and that appropriately calibrated
herd tendencies can enhance collaborative outcomes. These findings offer new
insights into the social dynamics of LLM-based systems and open pathways for
designing more effective and adaptive multi-agent collaboration frameworks.

</details>


### [767] [AI-Supported Platform for System Monitoring and Decision-Making in Nuclear Waste Management with Large Language Models](https://arxiv.org/pdf/2505.21741)
*Dongjune Chang, Sola Kim, Young Soo Park*

Main category: cs.MA

TL;DR: A multi-agent RAG system using LLMs improves nuclear waste management compliance by combining document retrieval and structured agent collaboration, demonstrated in a case study near Winslow, Arizona.


<details>
  <summary>Details</summary>
Motivation: To enhance decision accuracy in nuclear waste management by addressing complex legal, environmental, and safety considerations through AI-driven systems.

Method: A multi-agent RAG system with structured 10-round discussions, leveraging Llama 3.2 and mxbai-embed-large-v1 embeddings for retrieval and semantic representation.

Result: Higher relevance scores for regulatory compliance, effective risk assessment by the Safety Agent, and improved agreement rates with reduced semantic drift.

Conclusion: The system offers scalable, transparent, and adaptive decision-making for high-stakes environmental management, balancing automation with human oversight.

Abstract: Nuclear waste management requires rigorous regulatory compliance assessment,
demanding advanced decision-support systems capable of addressing complex
legal, environmental, and safety considerations. This paper presents a
multi-agent Retrieval-Augmented Generation (RAG) system that integrates large
language models (LLMs) with document retrieval mechanisms to enhance decision
accuracy through structured agent collaboration. Through a structured 10-round
discussion model, agents collaborate to assess regulatory compliance and safety
requirements while maintaining document-grounded responses. Implemented on
consumer-grade hardware, the system leverages Llama 3.2 and
mxbai-embed-large-v1 embeddings for efficient retrieval and semantic
representation. A case study of a proposed temporary nuclear waste storage site
near Winslow, Arizona, demonstrates the framework's effectiveness. Results show
the Regulatory Agent achieves consistently higher relevance scores in
maintaining alignment with legal frameworks, while the Safety Agent effectively
manages complex risk assessments requiring multifaceted analysis. The system
demonstrates progressive improvement in agreement rates between agents across
discussion rounds while semantic drift decreases, indicating enhanced
decision-making consistency and response coherence. The system ensures
regulatory decisions remain factually grounded, dynamically adapting to
evolving regulatory frameworks through real-time document retrieval. By
balancing automated assessment with human oversight, this framework offers a
scalable and transparent approach to regulatory governance. These findings
underscore the potential of AI-driven, multi-agent systems in advancing
evidence-based, accountable, and adaptive decision-making for high-stakes
environmental management scenarios.

</details>


### [768] [Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation](https://arxiv.org/pdf/2505.21880)
*Yu-Lun Song, Chung-En Tsern, Che-Cheng Wu, Yu-Ming Chang, Syuan-Bo Huang, Wei-Chu Chen, Michael Chia-Liang Lin, Yu-Ta Lin*

Main category: cs.MA

TL;DR: An innovative urban mobility simulation integrates LLM with ABM to enhance realism and diversity, tested in Taipei City for actionable urban planning insights.


<details>
  <summary>Details</summary>
Motivation: To improve urban mobility simulation by combining LLM and ABM for more realistic and diverse agent behaviors.

Method: Integrates LLM with ABM to generate synthetic population profiles, allocate locations, and simulate personalized routes using real-world data.

Result: Produces route heat maps and mode-specific indicators for urban planners, tested in Taipei City.

Conclusion: Future work aims to validate the framework for accuracy and reliability in urban planning.

Abstract: This study presents an innovative approach to urban mobility simulation by
integrating a Large Language Model (LLM) with Agent-Based Modeling (ABM).
Unlike traditional rule-based ABM, the proposed framework leverages LLM to
enhance agent diversity and realism by generating synthetic population
profiles, allocating routine and occasional locations, and simulating
personalized routes. Using real-world data, the simulation models individual
behaviors and large-scale mobility patterns in Taipei City. Key insights, such
as route heat maps and mode-specific indicators, provide urban planners with
actionable information for policy-making. Future work focuses on establishing
robust validation frameworks to ensure accuracy and reliability in urban
planning applications.

</details>


### [769] [Reward-Independent Messaging for Decentralized Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2505.21985)
*Naoto Yoshida, Tadahiro Taniguchi*

Main category: cs.MA

TL;DR: MARL-CPC enables decentralized agents to communicate effectively using collective predictive coding, outperforming traditional message-as-action methods in non-cooperative settings.


<details>
  <summary>Details</summary>
Motivation: Improving agent performance in MARL under partial observability by enabling communication without parameter sharing or cooperation assumptions.

Method: Introduces MARL-CPC with a message learning model based on collective predictive coding (CPC). Two algorithms, Bandit-CPC and IPPO-CPC, are proposed and tested.

Result: MARL-CPC outperforms standard message-as-action approaches, enabling effective communication even without direct sender benefits.

Conclusion: MARL-CPC shows promise for coordination in complex, decentralized environments, especially in non-cooperative settings.

Abstract: In multi-agent reinforcement learning (MARL), effective communication
improves agent performance, particularly under partial observability. We
propose MARL-CPC, a framework that enables communication among fully
decentralized, independent agents without parameter sharing. MARL-CPC
incorporates a message learning model based on collective predictive coding
(CPC) from emergent communication research. Unlike conventional methods that
treat messages as part of the action space and assume cooperation, MARL-CPC
links messages to state inference, supporting communication in non-cooperative,
reward-independent settings. We introduce two algorithms -Bandit-CPC and
IPPO-CPC- and evaluate them in non-cooperative MARL tasks. Benchmarks show that
both outperform standard message-as-action approaches, establishing effective
communication even when messages offer no direct benefit to the sender. These
results highlight MARL-CPC's potential for enabling coordination in complex,
decentralized environments.

</details>


### [770] [Sentiment Simulation using Generative AI Agents](https://arxiv.org/pdf/2505.22125)
*Melrose Tia, Jezreel Sophia Lanuzo, Lei Rigi Baltazar, Marie Joy Lopez-Relente, Diwa Malaya Quiñones, Jason Albia*

Main category: cs.MA

TL;DR: A framework using generative AI agents with psychological profiles for sentiment simulation outperforms traditional methods, achieving high alignment with human responses.


<details>
  <summary>Details</summary>
Motivation: Traditional sentiment analysis lacks predictive insight due to reliance on surface-level patterns. This work aims to capture psychological and contextual drivers for better applications like policy testing.

Method: The framework involves three stages: agent embodiment (categorical/contextualized encodings), exposure to real-world scenarios, and sentiment generation with rationales.

Result: Contextualized encoding achieved 92% alignment with human responses, and sentiment simulation tasks reached 81%--86% accuracy.

Conclusion: The study introduces a scalable, psychology-grounded framework for dynamic sentiment simulation, shifting from retrospective to prospective analysis.

Abstract: Traditional sentiment analysis relies on surface-level linguistic patterns
and retrospective data, limiting its ability to capture the psychological and
contextual drivers of human sentiment. These limitations constrain its
effectiveness in applications that require predictive insight, such as policy
testing, narrative framing, and behavioral forecasting. We present a robust
framework for sentiment simulation using generative AI agents embedded with
psychologically rich profiles. Agents are instantiated from a nationally
representative survey of 2,485 Filipino respondents, combining sociodemographic
information with validated constructs of personality traits, values, beliefs,
and socio-political attitudes. The framework includes three stages: (1) agent
embodiment via categorical or contextualized encodings, (2) exposure to
real-world political and economic scenarios, and (3) generation of sentiment
ratings accompanied by explanatory rationales. Using Quadratic Weighted
Accuracy (QWA), we evaluated alignment between agent-generated and human
responses. Contextualized encoding achieved 92% alignment in replicating
original survey responses. In sentiment simulation tasks, agents reached
81%--86% accuracy against ground truth sentiment, with contextualized profile
encodings significantly outperforming categorical (p < 0.0001, Cohen's d =
0.70). Simulation results remained consistent across repeated trials
(+/-0.2--0.5% SD) and resilient to variation in scenario framing (p = 0.9676,
Cohen's d = 0.02). Our findings establish a scalable framework for sentiment
modeling through psychographically grounded AI agents. This work signals a
paradigm shift in sentiment analysis from retrospective classification to
prospective and dynamic simulation grounded in psychology of sentiment
formation.

</details>


### [771] [Efficient Leave-one-out Approximation in LLM Multi-agent Debate Based on Introspection](https://arxiv.org/pdf/2505.22192)
*Yue Cui, Liuyi Yao, Zitao Li, Yaliang Li, Bolin Ding, Xiaofang Zhou*

Main category: cs.MA

TL;DR: IntrospecLOO is a cost-effective prompting method to approximate LOO for evaluating agent contributions in LLM-based multi-agent debates.


<details>
  <summary>Details</summary>
Motivation: Assessing individual agent contributions in LLM-based debates is crucial but computationally expensive with traditional LOO methods.

Method: IntrospecLOO adds a querying round post-debate, prompting agents to update answers while ignoring a designated agent's input.

Result: Experiments on three benchmark datasets validate IntrospecLOO's effectiveness in reducing query complexity.

Conclusion: IntrospecLOO offers a practical solution for evaluating agent contributions in LLM-powered debates with lower computational costs.

Abstract: Multi-agent systems based on large language models (LLMs) advance automatic
task completion in various fields, where debate is a common cooperation form
for agents to solve complicated problems with reasoning and cross-review to
solidify answers. Assessing the individual contributions of agents within these
debates is crucial for system refinement and outcome reliability. Traditional
leave-one-out (LOO) method offers a clear framework for evaluating each agent's
role but face challenges in LLM-based systems due to high computational costs
and associated financial implications. This paper presents
introspective-leave-one-out (IntrospecLOO), a simple yet effective prompting
for approximation of LOO in LLM-powered multi-agent debates. IntrospecLOO
introduces an additional querying round after standard debates, prompting
agents to update their answers while ignoring responses from a designated
agent. This strategy effectively isolates and gauges each participant's
influence at a reduced query complexity compared to the original LOO
approaches. Validation through experiments on three benchmark datasets confirms
the effectiveness of IntrospecLOO.

</details>


### [772] [Topological Structure Learning Should Be A Research Priority for LLM-Based Multi-Agent Systems](https://arxiv.org/pdf/2505.22467)
*Jiaxi Yang, Mengqi Zhang, Yiqiao Jin, Hao Chen, Qingsong Wen, Lu Lin, Yi He, Weijie Xu, James Evans, Jindong Wang*

Main category: cs.MA

TL;DR: The paper proposes a framework for optimizing Multi-Agent Systems (MASs) by focusing on their structural organization, introducing a three-stage process to enhance coordination and efficiency.


<details>
  <summary>Details</summary>
Motivation: The research addresses the unexplored question of how to structurally organize agents in MASs for optimal cooperation, aiming to redirect focus toward topology-aware systems.

Method: The paper introduces a three-stage framework: agent selection, structure profiling, and topology synthesis, leveraging areas like language models, reinforcement learning, and graph learning.

Result: The framework identifies new research opportunities and aims to unlock the full potential of MASs in complex real-world applications.

Conclusion: The paper highlights challenges and opportunities in evaluating MASs and hopes to provide critical insights for advancing agentic AI.

Abstract: Large Language Model-based Multi-Agent Systems (MASs) have emerged as a
powerful paradigm for tackling complex tasks through collaborative
intelligence. Nevertheless, the question of how agents should be structurally
organized for optimal cooperation remains largely unexplored. In this position
paper, we aim to gently redirect the focus of the MAS research community toward
this critical dimension: develop topology-aware MASs for specific tasks.
Specifically, the system consists of three core components - agents,
communication links, and communication patterns - that collectively shape its
coordination performance and efficiency. To this end, we introduce a
systematic, three-stage framework: agent selection, structure profiling, and
topology synthesis. Each stage would trigger new research opportunities in
areas such as language models, reinforcement learning, graph learning, and
generative modeling; together, they could unleash the full potential of MASs in
complicated real-world applications. Then, we discuss the potential challenges
and opportunities in the evaluation of multiple systems. We hope our
perspective and framework can offer critical new insights in the era of agentic
AI.

</details>


### [773] [Benchmarking LLMs' Swarm intelligence](https://arxiv.org/pdf/2505.04364)
*Kai Ruan, Mowen Huang, Ji-Rong Wen, Hao Sun*

Main category: cs.MA

TL;DR: SwarmBench is a new benchmark to evaluate LLMs' swarm intelligence in decentralized MAS, revealing their limitations in long-range planning and adaptive strategy under uncertainty.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' emergent coordination in MAS under strict swarm-like constraints, addressing gaps in existing benchmarks.

Method: Introduces SwarmBench with five MAS tasks in a 2D grid, using local perception and communication, and proposes coordination metrics.

Result: Zero-shot evaluations show task-dependent performance variations, with LLMs struggling in long-range planning and adaptive strategy.

Conclusion: SwarmBench is released as an open toolkit to advance research in LLM-based MAS coordination and emergent collective behavior.

Abstract: Large Language Models (LLMs) show potential for complex reasoning, yet their
capacity for emergent coordination in Multi-Agent Systems (MAS) when operating
under strict swarm-like constraints-limited local perception and
communication-remains largely unexplored. Existing benchmarks often do not
fully capture the unique challenges of decentralized coordination when agents
operate with incomplete spatio-temporal information. To bridge this gap, we
introduce SwarmBench, a novel benchmark designed to systematically evaluate the
swarm intelligence capabilities of LLMs acting as decentralized agents.
SwarmBench features five foundational MAS coordination tasks (Pursuit,
Synchronization, Foraging, Flocking, Transport) within a configurable 2D grid
environment, forcing agents to rely solely on local sensory input ($k\times k$
view) and local communication. We propose metrics for coordination
effectiveness and analyze emergent group dynamics. Zero-shot evaluations of
leading LLMs (e.g., deepseek-v3, o4-mini) reveal significant task-dependent
performance variations. While some rudimentary coordination is observed, our
results indicate that current LLMs significantly struggle with robust
long-range planning and adaptive strategy formation under the uncertainty
inherent in these decentralized scenarios. Assessing LLMs under such swarm-like
constraints is crucial for understanding their utility in future decentralized
intelligent systems. We release SwarmBench as an open, extensible toolkit-built
on a customizable physical system-providing environments, prompts, evaluation
scripts, and comprehensive datasets. This aims to foster reproducible research
into LLM-based MAS coordination and the theoretical underpinnings of emergent
collective behavior under severe informational decentralization. Our code
repository is available at https://github.com/x66ccff/swarmbench.

</details>


### [774] [Empowering Scientific Workflows with Federated Agents](https://arxiv.org/pdf/2505.05428)
*J. Gregory Pauloski, Yadu Babuji, Ryan Chard, Mansi Sakarvadia, Kyle Chard, Ian Foster*

Main category: cs.MA

TL;DR: Academy is a middleware for deploying autonomous agents in research cyberinfrastructure, supporting asynchronous execution, heterogeneous resources, and high-throughput data flows, with demonstrated scalability in HPC environments.


<details>
  <summary>Details</summary>
Motivation: Agentic systems are popular but lack frameworks compatible with research cyberinfrastructure, limiting their use in scientific computing.

Method: Academy is introduced as a modular and extensible middleware, providing abstractions for stateful agents, inter-agent coordination, and integration with experimental control.

Result: Microbenchmarks show high performance and scalability in HPC environments, and case studies demonstrate applications in materials discovery, decentralized learning, and information extraction.

Conclusion: Academy enables agentic workflows in the federated research ecosystem, addressing the needs of scientific computing with scalable and flexible solutions.

Abstract: Agentic systems, in which diverse agents cooperate to tackle challenging
problems, are exploding in popularity in the AI community. However, the agentic
frameworks used to build these systems have not previously enabled use with
research cyberinfrastructure. Here we introduce Academy, a modular and
extensible middleware designed to deploy autonomous agents across the federated
research ecosystem, including HPC systems, experimental facilities, and data
repositories. To meet the demands of scientific computing, Academy supports
asynchronous execution, heterogeneous resources, high-throughput data flows,
and dynamic resource availability. It provides abstractions for expressing
stateful agents, managing inter-agent coordination, and integrating computation
with experimental control. We present microbenchmark results that demonstrate
high performance and scalability in HPC environments. To demonstrate the
breadth of applications that can be supported by agentic workflow designs, we
also present case studies in materials discovery, decentralized learning, and
information extraction in which agents are deployed across diverse HPC systems.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [775] [Towards Structure-aware Model for Multi-modal Knowledge Graph Completion](https://arxiv.org/pdf/2505.21973)
*Linyu Li, Zhi Jin, Yichi Zhang, Dongming Jin, Chengfeng Dou, Yuanpeng He, Xuan Zhang, Haiyan Zhao*

Main category: cs.MM

TL;DR: The paper proposes TSAM, a novel model for multi-modal knowledge graph completion (MMKGC), addressing challenges of fine-grained modality interaction and graph structure dominance.


<details>
  <summary>Details</summary>
Motivation: Traditional KGC models fail with multi-modal data, necessitating new methods for MMKGC to handle modality interaction and noise.

Method: TSAM introduces Fine-grained Modality Awareness Fusion (FgMAF) for semantic interaction and Structure-aware Contrastive Learning (SaCL) to align modalities with graph structure.

Result: TSAM outperforms existing MMKGC models on multi-modal datasets.

Conclusion: TSAM effectively integrates modality interaction and graph structure, advancing MMKGC performance.

Abstract: Knowledge graphs (KGs) play a key role in promoting various multimedia and AI
applications. However, with the explosive growth of multi-modal information,
traditional knowledge graph completion (KGC) models cannot be directly applied.
This has attracted a large number of researchers to study multi-modal knowledge
graph completion (MMKGC). Since MMKG extends KG to the visual and textual
domains, MMKGC faces two main challenges: (1) how to deal with the fine-grained
modality information interaction and awareness; (2) how to ensure the dominant
role of graph structure in multi-modal knowledge fusion and deal with the noise
generated by other modalities during modality fusion. To address these
challenges, this paper proposes a novel MMKGC model named TSAM, which
integrates fine-grained modality interaction and dominant graph structure to
form a high-performance MMKGC framework. Specifically, to solve the challenges,
TSAM proposes the Fine-grained Modality Awareness Fusion method (FgMAF), which
uses pre-trained language models to better capture fine-grained semantic
information interaction of different modalities and employs an attention
mechanism to achieve fine-grained modality awareness and fusion. Additionally,
TSAM presents the Structure-aware Contrastive Learning method (SaCL), which
utilizes two contrastive learning approaches to align other modalities more
closely with the structured modality. Extensive experiments show that the
proposed TSAM model significantly outperforms existing MMKGC models on widely
used multi-modal datasets.

</details>


### [776] [Mitigating Audiovisual Mismatch in Visual-Guide Audio Captioning](https://arxiv.org/pdf/2505.22045)
*Le Xu, Chenxing Li, Yong Ren, Yujie Chen, Yu Gu, Ruibo Fu, Shan Yang, Dong Yu*

Main category: cs.MM

TL;DR: An entropy-aware gated fusion framework improves audio captioning by dynamically modulating visual information flow and suppressing misleading cues, with enhanced resilience to audiovisual misalignment.


<details>
  <summary>Details</summary>
Motivation: Addressing audiovisual misalignment in real-world scenarios (e.g., dubbed content or off-screen sounds) where current systems often fail.

Method: Uses entropy-aware gated fusion with cross-modal uncertainty quantification and a batch-wise audiovisual shuffling technique for synthetic mismatched training pairs.

Result: Outperforms baselines on AudioCaps, especially in mismatched scenarios, with ~6x faster inference speed.

Conclusion: The framework effectively handles misalignment and improves performance and speed in vision-guided audio captioning.

Abstract: Current vision-guided audio captioning systems frequently fail to address
audiovisual misalignment in real-world scenarios, such as dubbed content or
off-screen sounds. To bridge this critical gap, we present an entropy-aware
gated fusion framework that dynamically modulates visual information flow
through cross-modal uncertainty quantification. Our novel approach employs
attention entropy analysis in cross-attention layers to automatically identify
and suppress misleading visual cues during modal fusion. Complementing this
architecture, we develop a batch-wise audiovisual shuffling technique that
generates synthetic mismatched training pairs, greatly enhancing model
resilience against alignment noise. Evaluations on the AudioCaps benchmark
demonstrate our system's superior performance over existing baselines,
especially in mismatched modality scenarios. Furthermore, our solution
demonstrates an approximately 6x improvement in inference speed compared to the
baseline.

</details>


### [777] [Hearing from Silence: Reasoning Audio Descriptions from Silent Videos via Vision-Language Model](https://arxiv.org/pdf/2505.13062)
*Yong Ren, Chenxing Li, Le Xu, Hao Gu, Duzhen Zhang, Yujie Chen, Manjie Xu, Ruibo Fu, Shan Yang, Dong Yu*

Main category: cs.MM

TL;DR: The paper explores whether multimodal large language models can infer sounds from silent videos without accessing target modalities, introducing the SVAD task and a CoT-based fine-tuning strategy to enhance reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of acquiring audio descriptions during video-to-audio tasks and explore modal-mismatch reasoning in vision-language models.

Method: Introduces the SVAD task, constructs the CoT-AudioCaps dataset, and proposes a Chain-of-Thought-based supervised fine-tuning strategy.

Result: Significantly improves VLMs' modal-mismatch reasoning for SVAD and effectively addresses audio description acquisition in VT2A tasks.

Conclusion: The proposed method enhances VLMs' reasoning for sound inference from silent videos and improves performance in related tasks.

Abstract: Humans can intuitively infer sounds from silent videos, but whether
multimodal large language models can perform modal-mismatch reasoning without
accessing target modalities remains relatively unexplored. Current
text-assisted-video-to-audio (VT2A) methods excel in video foley tasks but
struggle to acquire audio descriptions during inference. We introduce the task
of Reasoning Audio Descriptions from Silent Videos (SVAD) to address this
challenge and investigate vision-language models' (VLMs) capabilities on this
task. To further enhance the VLMs' reasoning capacity for the SVAD task, we
construct a CoT-AudioCaps dataset and propose a Chain-of-Thought-based
supervised fine-tuning strategy. Experiments on SVAD and subsequent VT2A tasks
demonstrate our method's effectiveness in two key aspects: significantly
improving VLMs' modal-mismatch reasoning for SVAD and effectively addressing
the challenge of acquiring audio descriptions during VT2A inference.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [778] [VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled data and Large-Scale Speech Pretraining](https://arxiv.org/pdf/2505.21527)
*Jianheng Zhuo, Yifan Yang, Yiwen Shao, Yong Xu, Dong Yu, Kai Yu, Xie Chen*

Main category: eess.AS

TL;DR: VietASR is a cost-effective ASR pipeline for low-resource languages like Vietnamese, using self-supervised learning on unlabeled data and minimal labeled data to outperform existing systems.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity of labeled data and high training costs for ASR in low-resource languages like Vietnamese.

Method: Multi-iteration ASR-biased self-supervised learning on large unlabeled datasets, followed by fine-tuning with minimal labeled data.

Result: Outperforms Whisper Large-v3 and commercial ASR systems with pre-training on 70,000-hour unlabeled data and fine-tuning on 50-hour labeled data.

Conclusion: VietASR provides a practical, lightweight solution for low-resource ASR, with plans to open-source the code and models.

Abstract: Automatic speech recognition (ASR) has made remarkable progress but heavily
relies on large-scale labeled data, which is scarce for low-resource languages
like Vietnamese. While existing systems such as Whisper, USM, and MMS achieve
promising performance, their efficacy remains inadequate in terms of training
costs, latency, and accessibility. To address these issues, we propose VietASR,
a novel ASR training pipeline that leverages vast amounts of unlabeled data and
a small set of labeled data. Through multi-iteration ASR-biased self-supervised
learning on a large-scale unlabeled dataset, VietASR offers a cost-effective
and practical solution for enhancing ASR performance. Experiments demonstrate
that pre-training on 70,000-hour unlabeled data and fine-tuning on merely
50-hour labeled data yield a lightweight but powerful ASR model. It outperforms
Whisper Large-v3 and commercial ASR systems on real-world data. Our code and
models will be open-sourced to facilitate research in low-resource ASR.

</details>


### [779] [WhisperD: Dementia Speech Recognition and Filler Word Detection with Whisper](https://arxiv.org/pdf/2505.21551)
*Emmanuel Akinrintoyo, Nadine Abdelhalim, Nicole Salomons*

Main category: eess.AS

TL;DR: Fine-tuning Whisper with dementia speech datasets improves transcription accuracy, outperforming off-the-shelf models.


<details>
  <summary>Details</summary>
Motivation: Standard speech models like Whisper fail to transcribe dementia speech accurately due to irregular patterns and disfluencies, hindering diagnosis and assistive tech development.

Method: Fine-tuned Whisper using DementiaBank and in-house datasets, focusing on WER, FIR, and F1 scores.

Result: Fine-tuned models achieved a WER of 0.24, outperforming off-the-shelf models and showing generalizability to unseen data.

Conclusion: Fine-tuning with specialized datasets significantly improves dementia speech transcription, aiding diagnosis and technology development.

Abstract: Whisper fails to correctly transcribe dementia speech because persons with
dementia (PwDs) often exhibit irregular speech patterns and disfluencies such
as pauses, repetitions, and fragmented sentences. It was trained on standard
speech and may have had little or no exposure to dementia-affected speech.
However, correct transcription is vital for dementia speech for cost-effective
diagnosis and the development of assistive technology. In this work, we
fine-tune Whisper with the open-source dementia speech dataset (DementiaBank)
and our in-house dataset to improve its word error rate (WER). The fine-tuning
also includes filler words to ascertain the filler inclusion rate (FIR) and F1
score. The fine-tuned models significantly outperformed the off-the-shelf
models. The medium-sized model achieved a WER of 0.24, outperforming previous
work. Similarly, there was a notable generalisability to unseen data and speech
patterns.

</details>


### [780] [Analysis and Evaluation of Synthetic Data Generation in Speech Dysfluency Detection](https://arxiv.org/pdf/2505.22029)
*Jinming Zhang, Xuanru Zhou, Jiachen Lian, Shuhe Li, William Li, Zoe Ezzes, Rian Bogley, Lisa Wauters, Zachary Miller, Jet Vonk, Brittany Morin, Maria Gorno-Tempini, Gopala Anumanchipalli*

Main category: eess.AS

TL;DR: The paper introduces LLM-Dys, a comprehensive dysfluent speech corpus enhanced by LLM, addressing limitations of existing synthetic datasets, and improves dysfluency detection performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for speech dysfluency detection are limited by scarce high-quality annotated data and unnatural synthetic datasets.

Method: Proposes LLM-Dys, a dataset with LLM-enhanced dysfluency simulation covering 11 categories, and improves an end-to-end detection framework.

Result: Achieves state-of-the-art performance in dysfluency detection.

Conclusion: LLM-Dys provides a robust resource for dysfluency detection, with open-sourced data, models, and code.

Abstract: Speech dysfluency detection is crucial for clinical diagnosis and language
assessment, but existing methods are limited by the scarcity of high-quality
annotated data. Although recent advances in TTS model have enabled synthetic
dysfluency generation, existing synthetic datasets suffer from unnatural
prosody and limited contextual diversity. To address these limitations, we
propose LLM-Dys -- the most comprehensive dysfluent speech corpus with
LLM-enhanced dysfluency simulation. This dataset captures 11 dysfluency
categories spanning both word and phoneme levels. Building upon this resource,
we improve an end-to-end dysfluency detection framework. Experimental
validation demonstrates state-of-the-art performance. All data, models, and
code are open-sourced at https://github.com/Berkeley-Speech-Group/LLM-Dys.

</details>


### [781] [ARiSE: Auto-Regressive Multi-Channel Speech Enhancement](https://arxiv.org/pdf/2505.22051)
*Pengjie Shen, Xueliang Zhang, Zhong-Qiu Wang*

Main category: eess.AS

TL;DR: ARiSE is an auto-regressive algorithm enhancing multi-channel speech by leveraging past frame data and a parallel training mechanism for efficiency.


<details>
  <summary>Details</summary>
Motivation: Improve existing DNN-based multi-channel speech enhancement by incorporating auto-regressive connections for better performance.

Method: Introduces auto-regressive connections using past frame data and beamformed mixtures, with parallel training to speed up DNN training.

Result: Effective in noisy-reverberant conditions, showing potential for improved speech enhancement.

Conclusion: ARiSE demonstrates effectiveness and efficiency in enhancing multi-channel speech, with promising results.

Abstract: We propose ARiSE, an auto-regressive algorithm for multi-channel speech
enhancement. ARiSE improves existing deep neural network (DNN) based
frame-online multi-channel speech enhancement models by introducing
auto-regressive connections, where the estimated target speech at previous
frames is leveraged as extra input features to help the DNN estimate the target
speech at the current frame. The extra input features can be derived from (a)
the estimated target speech in previous frames; and (b) a beamformed mixture
with the beamformer computed based on the previous estimated target speech. On
the other hand, naively training the DNN in an auto-regressive manner is very
slow. To deal with this, we propose a parallel training mechanism to speed up
the training. Evaluation results in noisy-reverberant conditions show the
effectiveness and potential of the proposed algorithms.

</details>


### [782] [Evaluation of LLMs in Speech is Often Flawed: Test Set Contamination in Large Language Models for Speech Recognition](https://arxiv.org/pdf/2505.22251)
*Yuan Tseng, Titouan Parcollet, Rogier van Dalen, Shucong Zhang, Sourav Bhattacharya*

Main category: eess.AS

TL;DR: LLM performance on speech tasks may be inflated due to data contamination in LibriSpeech and Common Voice, as contaminated LLMs show bias toward seen data.


<details>
  <summary>Details</summary>
Motivation: To investigate the reliability of LLM performance claims on speech tasks, given potential contamination in evaluation datasets.

Method: Compare LLMs trained with and without contaminated data, analyzing error rates and transcription probabilities.

Result: Contaminated LLMs generate seen test sentences more often and assign higher probabilities to them, though error rates differ subtly.

Conclusion: LLM-based speech systems should be evaluated with held-out data to avoid bias from contamination.

Abstract: Recent work suggests that large language models (LLMs) can improve
performance of speech tasks compared to existing systems. To support their
claims, results on LibriSpeech and Common Voice are often quoted. However, this
work finds that a substantial amount of the LibriSpeech and Common Voice
evaluation sets appear in public LLM pretraining corpora. This calls into
question the reliability of findings drawn from these two datasets. To measure
the impact of contamination, LLMs trained with or without contamination are
compared, showing that a contaminated LLM is more likely to generate test
sentences it has seen during training. Speech recognisers using contaminated
LLMs shows only subtle differences in error rates, but assigns significantly
higher probabilities to transcriptions seen during training. Results show that
LLM outputs can be biased by tiny amounts of data contamination, highlighting
the importance of evaluating LLM-based speech systems with held-out data.

</details>


### [783] [Articulatory modeling of the S-shaped F2 trajectories observed in Öhman's spectrographic analysis of VCV syllables](https://arxiv.org/pdf/2505.22455)
*Frédéric Berthommier*

Main category: eess.AS

TL;DR: The study revisits Ohman's VCV sequences using the Maeda model, introducing articulatory constraints and finding S-shaped F2 trajectories emerge from coordinated articulator synergy.


<details>
  <summary>Details</summary>
Motivation: To address the lack of articulatory constraints in the DRM model and explore the underlying determinism of locus equations in synthetic CV data.

Method: Analyzed 75 VCVs using the Maeda model with trajectory planning, differentiating vowel-to-vowel transitions from consonantal influences.

Result: Synthetic data mirrored Ohman's sequences, showing S-shaped F2 trajectories. Locus equations revealed a composite mechanism for articulatory planning.

Conclusion: Articulatory planning is structured separately for vowels and consonants, but S-shaped F2 trajectories result from coordinated articulator synergy.

Abstract: The synthesis of Ohman's VCV sequences with intervocalic plosive consonants
was first achieved 30 years ago using the DRM model. However, this approach
remains primarily acoustic and lacks articulatory constraints. In this study,
the same 75 VCVs are analyzed, but generated with the Maeda model, using
trajectory planning that differentiates vowel-to-vowel transitions from
consonantal influences. Synthetic data exhibit similar characteristics to
Ohman's sequences, including the presence of S-shaped F2 trajectories.
Furthermore, locus equations (LEs) for F2 and F3 are computed from synthetic CV
data to investigate their underlying determinism, leading to a reassessment of
conventional interpretations. The findings indicate that, although articulatory
planning is structured separately for vowel and consonant groups, S-shaped F2
trajectories emerge from a composite mechanism governed by the coordinated
synergy of all articulators.

</details>


### [784] [VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for Speech Processing](https://arxiv.org/pdf/2408.05758)
*Chunyu Qiang, Wang Geng, Yi Zhao, Ruibo Fu, Tao Wang, Cheng Gong, Tianrui Wang, Qiuyu Liu, Jiangyan Yi, Zhengqi Wen, Chen Zhang, Hao Che, Longbiao Wang, Jianwu Dang, Jianhua Tao*

Main category: eess.AS

TL;DR: VQ-CTAP is a cross-modal method for fine-grained text-speech alignment, improving tasks like TTS, VC, and ASR without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To create a joint multimodal space for text and speech, emphasizing semantic content while minimizing paralinguistic noise.

Method: Uses a cross-modal aligned sequence transcoder, stepping optimization, and semantic-transfer-wise paralinguistic consistency loss.

Result: Achieves high-compression speech coding (960-fold reduction) and plug-and-play capability for TTS.

Conclusion: VQ-CTAP offers a promising solution for fine-grained cross-modal tasks in speech processing.

Abstract: Deep learning has brought significant improvements to the field of
cross-modal representation learning. For tasks such as text-to-speech (TTS),
voice conversion (VC), and automatic speech recognition (ASR), a cross-modal
fine-grained (frame-level) sequence representation is desired, emphasizing the
semantic content of the text modality while de-emphasizing the paralinguistic
information of the speech modality. We propose a method called "Vector
Quantized Contrastive Token-Acoustic Pre-training (VQ-CTAP)", which uses the
cross-modal aligned sequence transcoder to bring text and speech into a joint
multimodal space, learning how to connect text and speech at the frame level.
The proposed VQ-CTAP is a paradigm for cross-modal sequence representation
learning, offering a promising solution for fine-grained generation and
recognition tasks in speech processing. The VQ-CTAP can be directly applied to
VC and ASR tasks without fine-tuning or additional structures. We propose a
sequence-aware semantic connector, which connects multiple frozen pre-trained
modules for the TTS task, exhibiting a plug-and-play capability. We design a
stepping optimization strategy to ensure effective model convergence by
gradually injecting and adjusting the influence of various loss components.
Furthermore, we propose a semantic-transfer-wise paralinguistic consistency
loss to enhance representational capabilities, allowing the model to better
generalize to unseen data and capture the nuances of paralinguistic
information. In addition, VQ-CTAP achieves high-compression speech coding at a
rate of 25Hz from 24kHz input waveforms, which is a 960-fold reduction in the
sampling rate. The audio demo is available at
https://qiangchunyu.github.io/VQCTAP/

</details>


### [785] [On the Within-class Variation Issue in Alzheimer's Disease Detection](https://arxiv.org/pdf/2409.16322)
*Jiawen Kang, Dongrui Han, Lingwei Meng, Jingyan Zhou, Jinchao Li, Xixin Wu, Helen Meng*

Main category: eess.AS

TL;DR: The paper addresses within-class variation and instance-level imbalance in Alzheimer's Disease (AD) detection using machine learning, proposing Soft Target Distillation (SoTD) and Instance-level Re-balancing (InRe) methods to improve performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of within-class heterogeneity and instance-level imbalance in AD detection, which simplistic binary classification overlooks.

Method: The authors propose two methods: Soft Target Distillation (SoTD) to align sample-specific soft scores with cognitive scores, and Instance-level Re-balancing (InRe) to tackle instance-level imbalance.

Result: The methods were tested on ADReSS and CU-MARVEL corpora, showing improved detection performance.

Conclusion: The findings offer insights for developing more robust and reliable AD detection models.

Abstract: Alzheimer's Disease (AD) detection employs machine learning classification
models to distinguish between individuals with AD and those without. Different
from conventional classification tasks, we identify within-class variation as a
critical challenge in AD detection: individuals with AD exhibit a spectrum of
cognitive impairments. Therefore, simplistic binary AD classification may
overlook two crucial aspects: within-class heterogeneity and instance-level
imbalance. In this work, we found using a sample score estimator can generate
sample-specific soft scores aligning with cognitive scores. We subsequently
propose two simple yet effective methods: Soft Target Distillation (SoTD) and
Instance-level Re-balancing (InRe), targeting two problems respectively. Based
on the ADReSS and CU-MARVEL corpora, we demonstrated and analyzed the
advantages of the proposed approaches in detection performance. These findings
provide insights for developing robust and reliable AD detection models.

</details>


### [786] [AttentiveMOS: A Lightweight Attention-Only Model for Speech Quality Prediction](https://arxiv.org/pdf/2410.12675)
*Imran E Kibria, Donald S. Williamson*

Main category: eess.AS

TL;DR: Proposes an attention-only model using Swin and standard transformers for speech quality assessment, improving generalization and applicability.


<details>
  <summary>Details</summary>
Motivation: Existing no-reference speech models rely on impractical pretrained networks and lack generalization.

Method: Uses Swin transformer for local features and standard transformer for global features, with sequential self-teaching for noisy labels.

Result: Outperforms baselines on three datasets, confirming effectiveness.

Conclusion: The lightweight, attention-only design enhances generalization and real-world usability.

Abstract: Research in modeling subjective metrics for quality assessment has led to the
development of no-reference speech models that directly operate on utterance
waveforms to predict the Mean Opinion Score (MOS). These models often rely on
convolutional layers for local feature extraction and embeddings from
impractically large pretrained networks to enhance generalization. We propose
an attention-only model based on Swin transformer and standard transformer
layers to extract local context features and global utterance features,
respectively. The self-attention operator excels at processing sequences, and
our lightweight design enhances generalization on limited MOS datasets while
improving real-world applicability. We train our network using a sequential
self-teaching strategy to improve generalization on MOS labels affected by
noise in listener ratings. Experiments on three datasets confirm the
effectiveness of our design and demonstrate improvement over baseline models.

</details>


### [787] [The Search for Squawk: Agile Modeling in Bioacoustics](https://arxiv.org/pdf/2505.03071)
*Vincent Dumoulin, Otilia Stretcu, Jenny Hamer, Lauren Harrell, Rob Laber, Hugo Larochelle, Bart van Merriënboer, Amanda Navine, Patrick Hart, Ben Williams, Timothy A. C. Lamont, Tries B. Rasak, Mars Coral Restoration Team, Sheryn Brodie, Brendan Doohan, Phil Eichinski, Paul Roe, Lin Schwarzkopf, Tom Denton*

Main category: eess.AS

TL;DR: A scalable, data-efficient system for bioacoustic recognizers reduces development time to under an hour, using pre-trained embeddings, indexed audio search, and active learning.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of extracting insights from vast audio recordings in ecology without requiring extensive training data or ML expertise.

Method: Uses pre-trained acoustic embeddings, indexed audio search for dataset creation, and active learning for iterative classifier improvement.

Result: Successfully applied in case studies (coral reef health, juvenile Hawaiian bird calls, Christmas Island bird occupancy) and simulations, demonstrating efficiency and scalability.

Conclusion: The system enables rapid, scalable solutions for novel bioacoustic challenges, aiding ecological research.

Abstract: Passive acoustic monitoring (PAM) has shown great promise in helping
ecologists understand the health of animal populations and ecosystems. However,
extracting insights from millions of hours of audio recordings requires the
development of specialized recognizers. This is typically a challenging task,
necessitating large amounts of training data and machine learning expertise. In
this work, we introduce a general, scalable and data-efficient system for
developing recognizers for novel bioacoustic problems in under an hour. Our
system consists of several key components that tackle problems in previous
bioacoustic workflows: 1) highly generalizable acoustic embeddings pre-trained
for birdsong classification minimize data hunger; 2) indexed audio search
allows the efficient creation of classifier training datasets, and 3)
precomputation of embeddings enables an efficient active learning loop,
improving classifier quality iteratively with minimal wait time. Ecologists
employed our system in three novel case studies: analyzing coral reef health
through unidentified sounds; identifying juvenile Hawaiian bird calls to
quantify breeding success and improve endangered species monitoring; and
Christmas Island bird occupancy modeling. We augment the case studies with
simulated experiments which explore the range of design decisions in a
structured way and help establish best practices. Altogether these experiments
showcase our system's scalability, efficiency, and generalizability, enabling
scientists to quickly address new bioacoustic challenges.

</details>


### [788] [Optimal Scalogram for Computational Complexity Reduction in Acoustic Recognition Using Deep Learning](https://arxiv.org/pdf/2505.13017)
*Dang Thoai Phan, Tuan Anh Huynh, Van Tuan Pham, Cao Minh Tran, Van Thuan Mai, Ngoc Quy Tran*

Main category: eess.AS

TL;DR: The paper proposes a method to reduce the computational cost of CWT for acoustic recognition by optimizing wavelet kernel length and hop size, maintaining performance.


<details>
  <summary>Details</summary>
Motivation: CWT is effective for feature extraction in acoustic recognition but is computationally expensive, leading to preference for alternatives like STFT.

Method: Optimize wavelet kernel length and hop size of the output scalogram to reduce CWT's computational complexity.

Result: The proposed method significantly reduces computational cost while preserving model performance in acoustic recognition.

Conclusion: Optimizing CWT parameters effectively balances computational efficiency and performance in acoustic recognition tasks.

Abstract: The Continuous Wavelet Transform (CWT) is an effective tool for feature
extraction in acoustic recognition using Convolutional Neural Networks (CNNs),
particularly when applied to non-stationary audio. However, its high
computational cost poses a significant challenge, often leading researchers to
prefer alternative methods such as the Short-Time Fourier Transform (STFT). To
address this issue, this paper proposes a method to reduce the computational
complexity of CWT by optimizing the length of the wavelet kernel and the hop
size of the output scalogram. Experimental results demonstrate that the
proposed approach significantly reduces computational cost while maintaining
the robust performance of the trained model in acoustic recognition tasks.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [789] [High-Fidelity Functional Ultrasound Reconstruction via A Visual Auto-Regressive Framework](https://arxiv.org/pdf/2505.21530)
*Xuhang Chen, Zhuo Li, Yanyan Shen, Mufti Mahmud, Hieu Pham, Chi-Man Pun, Shuqiang Wang*

Main category: eess.IV

TL;DR: Functional ultrasound (fUS) imaging offers high-resolution neurovascular mapping but faces challenges like data scarcity and signal degradation, limiting dataset diversity and model fairness.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the practical limitations of fUS imaging, particularly data scarcity and signal degradation, which hinder its broader application and fairness in machine learning models.

Method: Not explicitly mentioned in the abstract, but likely involves techniques to mitigate data scarcity and signal degradation in fUS imaging.

Result: The abstract highlights the challenges but does not specify results; the focus is on identifying limitations.

Conclusion: The paper underscores the need to address data scarcity and signal degradation in fUS imaging to improve its practical utility and fairness in machine learning applications.

Abstract: Functional ultrasound (fUS) imaging provides exceptional spatiotemporal
resolution for neurovascular mapping, yet its practical application is
significantly hampered by critical challenges. Foremost among these are data
scarcity, arising from ethical considerations and signal degradation through
the cranium, which collectively limit dataset diversity and compromise the
fairness of downstream machine learning models.

</details>


### [790] [Image denoising as a conditional expectation](https://arxiv.org/pdf/2505.21546)
*Sajal Chakroborty, Suddhasattwa Das*

Main category: eess.IV

TL;DR: The paper proposes a data-driven denoising method using probability spaces and kernel integral operators, ensuring unbiased and convergent results.


<details>
  <summary>Details</summary>
Motivation: Traditional denoising methods rely on projections, which may not guarantee unbiased or convergent results. The paper aims to address this limitation.

Method: The method interprets noisy images as samples from a probability space, recovering the true image as a conditional expectation using kernel integral operators and RKHS.

Result: The technique is proven convergent as pixel count increases and provides a framework for optimal parameter selection in finite-pixel images.

Conclusion: The proposed method offers a robust, data-driven approach to denoising with theoretical guarantees of convergence and unbiased results.

Abstract: All techniques for denoising involve a notion of a true (noise-free) image,
and a hypothesis space. The hypothesis space may reconstruct the image directly
as a grayscale valued function, or indirectly by its Fourier or wavelet
spectrum. Most common techniques estimate the true image as a projection to
some subspace. We propose an interpretation of a noisy image as a collection of
samples drawn from a certain probability space. Within this interpretation,
projection based approaches are not guaranteed to be unbiased and convergent.
We present a data-driven denoising method in which the true image is recovered
as a conditional expectation. Although the probability space is unknown
apriori, integrals on this space can be estimated by kernel integral operators.
The true image is reformulated as the least squares solution to a linear
equation in a reproducing kernel Hilbert space (RKHS), and involving various
kernel integral operators as linear transforms. Assuming the true image to be a
continuous function on a compact planar domain, the technique is shown to be
convergent as the number of pixels goes to infinity. We also show that for a
picture with finite number of pixels, the convergence result can be used to
choose the various parameters for an optimum denoising result.

</details>


### [791] [Taylor expansion-based Kolmogorov-Arnold network for blind image quality assessment](https://arxiv.org/pdf/2505.21592)
*Ze Chen, Shaode Yu*

Main category: eess.IV

TL;DR: TaylorKAN improves Kolmogorov-Arnold Networks (KAN) by using Taylor expansions for local approximation, enhancing performance and efficiency in blind image quality assessment (BIQA).


<details>
  <summary>Details</summary>
Motivation: Addressing KAN's challenges with high-dimensional features, limited performance gains, and high computational costs in BIQA.

Method: Proposes TaylorKAN, integrating Taylor expansions as learnable activation functions, network depth reduction, and feature dimensionality compression.

Result: Outperforms other KAN-related models on five databases, demonstrating better local approximation and generalization.

Conclusion: TaylorKAN is an efficient and robust solution for high-dimensional score regression in BIQA.

Abstract: Kolmogorov-Arnold Network (KAN) has attracted growing interest for its strong
function approximation capability. In our previous work, KAN and its variants
were explored in score regression for blind image quality assessment (BIQA).
However, these models encounter challenges when processing high-dimensional
features, leading to limited performance gains and increased computational
cost. To address these issues, we propose TaylorKAN that leverages the Taylor
expansions as learnable activation functions to enhance local approximation
capability. To improve the computational efficiency, network depth reduction
and feature dimensionality compression are integrated into the TaylorKAN-based
score regression pipeline. On five databases (BID, CLIVE, KonIQ, SPAQ, and
FLIVE) with authentic distortions, extensive experiments demonstrate that
TaylorKAN consistently outperforms the other KAN-related models, indicating
that the local approximation via Taylor expansions is more effective than
global approximation using orthogonal functions. Its generalization capacity is
validated through inter-database experiments. The findings highlight the
potential of TaylorKAN as an efficient and robust model for high-dimensional
score regression.

</details>


### [792] [Optimizing Deep Learning for Skin Cancer Classification: A Computationally Efficient CNN with Minimal Accuracy Trade-Off](https://arxiv.org/pdf/2505.21597)
*Abdullah Al Mamun, Pollob Chandra Ray, Md Rahat Ul Nasib, Akash Das, Jia Uddin, Md Nurul Absur*

Main category: eess.IV

TL;DR: A lightweight CNN model reduces parameters by 96.7% and FLOPs by 99.25% compared to ResNet50, with minimal accuracy loss (0.022%), making it practical for resource-constrained environments.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models for skin cancer classification, like ResNet50, are computationally expensive and impractical for resource-limited settings.

Method: Proposes a custom CNN model with significantly fewer parameters and FLOPs, evaluated on the HAM10000 dataset.

Result: Achieves 96.7% parameter reduction and 99.25% FLOP reduction, with only a 0.022% accuracy drop compared to ResNet50.

Conclusion: The lightweight CNN is a practical solution for mobile and edge-based skin cancer diagnostics, balancing accuracy and computational efficiency.

Abstract: The rapid advancement of deep learning in medical image analysis has greatly
enhanced the accuracy of skin cancer classification. However, current
state-of-the-art models, especially those based on transfer learning like
ResNet50, come with significant computational overhead, rendering them
impractical for deployment in resource-constrained environments. This study
proposes a custom CNN model that achieves a 96.7\% reduction in parameters
(from 23.9 million in ResNet50 to 692,000) while maintaining a classification
accuracy deviation of less than 0.022\%. Our empirical analysis of the HAM10000
dataset reveals that although transfer learning models provide a marginal
accuracy improvement of approximately 0.022\%, they result in a staggering
13,216.76\% increase in FLOPs, considerably raising computational costs and
inference latency. In contrast, our lightweight CNN architecture, which
encompasses only 30.04 million FLOPs compared to ResNet50's 4.00 billion,
significantly reduces energy consumption, memory footprint, and inference time.
These findings underscore the trade-off between the complexity of deep models
and their real-world feasibility, positioning our optimized CNN as a practical
solution for mobile and edge-based skin cancer diagnostics.

</details>


### [793] [Laparoscopic Image Desmoking Using the U-Net with New Loss Function and Integrated Differentiable Wiener Filter](https://arxiv.org/pdf/2505.21634)
*Chengyu Yang, Chengjun Liu*

Main category: eess.IV

TL;DR: A novel U-Net deep learning method (ULW) with a new loss function and integrated Wiener filter is proposed to enhance laparoscopic image clarity by removing surgical smoke.


<details>
  <summary>Details</summary>
Motivation: Surgical smoke in laparoscopic surgeries reduces visual clarity, hindering surgeons and computer-assisted technologies.

Method: The ULW method combines a new loss function (structural similarity, perceptual loss, mean squared error) and a learnable Wiener filter to model smoke degradation.

Result: ULW outperforms in visual clarity and metric-based evaluation on a public laparoscopic dataset.

Conclusion: ULW is a promising real-time solution for enhancing laparoscopic imagery, with code publicly available.

Abstract: Laparoscopic surgeries often suffer from reduced visual clarity due to the
presence of surgical smoke originated by surgical instruments, which poses
significant challenges for both surgeons and vision based computer-assisted
technologies. In order to remove the surgical smoke, a novel U-Net deep
learning with new loss function and integrated differentiable Wiener filter
(ULW) method is presented. Specifically, the new loss function integrates the
pixel, structural, and perceptual properties. Thus, the new loss function,
which combines the structural similarity index measure loss, the perceptual
loss, as well as the mean squared error loss, is able to enhance the quality
and realism of the reconstructed images. Furthermore, the learnable Wiener
filter is capable of effectively modelling the degradation process caused by
the surgical smoke. The effectiveness of the proposed ULW method is evaluated
using the publicly available paired laparoscopic smoke and smoke-free image
dataset, which provides reliable benchmarking and quantitative comparisons.
Experimental results show that the proposed ULW method excels in both visual
clarity and metric-based evaluation. As a result, the proposed ULW method
offers a promising solution for real-time enhancement of laparoscopic imagery.
The code is available at https://github.com/chengyuyang-njit/ImageDesmoke.

</details>


### [794] [STA-Risk: A Deep Dive of Spatio-Temporal Asymmetries for Breast Cancer Risk Prediction](https://arxiv.org/pdf/2505.21699)
*Zhengbo Zhou, Dooman Arefan, Margarita Zuley, Jules Sumkin, Shandong Wu*

Main category: eess.IV

TL;DR: STA-Risk, a Transformer-based model, improves breast cancer risk prediction by analyzing spatial-temporal asymmetries in mammograms, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing risk models lack performance and ignore nuanced breast tissue changes in longitudinal imaging, which are crucial for accurate risk prediction.

Method: STA-Risk uses side and temporal encoding to learn spatial-temporal asymmetries, regulated by a customized asymmetry loss, applied to mammogram datasets.

Result: STA-Risk outperformed four state-of-the-art models in 1- to 5-year future risk prediction across two independent datasets.

Conclusion: STA-Risk offers a superior approach for breast cancer risk prediction by leveraging spatial-temporal asymmetries in mammograms.

Abstract: Predicting the risk of developing breast cancer is an important clinical tool
to guide early intervention and tailoring personalized screening strategies.
Early risk models have limited performance and recently machine learning-based
analysis of mammogram images showed encouraging risk prediction effects. These
models however are limited to the use of a single exam or tend to overlook
nuanced breast tissue evolvement in spatial and temporal details of
longitudinal imaging exams that are indicative of breast cancer risk. In this
paper, we propose STA-Risk (Spatial and Temporal Asymmetry-based Risk
Prediction), a novel Transformer-based model that captures fine-grained
mammographic imaging evolution simultaneously from bilateral and longitudinal
asymmetries for breast cancer risk prediction. STA-Risk is innovative by the
side encoding and temporal encoding to learn spatial-temporal asymmetries,
regulated by a customized asymmetry loss. We performed extensive experiments
with two independent mammogram datasets and achieved superior performance than
four representative SOTA models for 1- to 5-year future risk prediction. Source
codes will be released upon publishing of the paper.

</details>


### [795] [Privacy-Preserving Chest X-ray Report Generation via Multimodal Federated Learning with ViT and GPT-2](https://arxiv.org/pdf/2505.21715)
*Md. Zahid Hossain, Mustofa Ahmed, Most. Sharmin Sultana Samu, Md. Rakibul Islam*

Main category: eess.IV

TL;DR: A Multimodal Federated Learning framework for generating radiology reports from chest X-rays is proposed, using ViT and GPT-2, with FL strategies outperforming centralized models while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: To enhance diagnostic workflows and patient privacy by avoiding sensitive data transfer in radiology report generation.

Method: Uses Vision Transformer (ViT) as encoder and GPT-2 as report generator, evaluating three FL aggregation strategies: FedAvg, Krum Aggregation, and L-FedAvg.

Result: Krum Aggregation performed best in lexical and semantic metrics (ROUGE, BLEU, BERTScore, RaTEScore), matching or surpassing centralized models.

Conclusion: The framework enables privacy-preserving, collaborative medical AI development without compromising data confidentiality.

Abstract: The automated generation of radiology reports from chest X-ray images holds
significant promise in enhancing diagnostic workflows while preserving patient
privacy. Traditional centralized approaches often require sensitive data
transfer, posing privacy concerns. To address this, the study proposes a
Multimodal Federated Learning framework for chest X-ray report generation using
the IU-Xray dataset. The system utilizes a Vision Transformer (ViT) as the
encoder and GPT-2 as the report generator, enabling decentralized training
without sharing raw data. Three Federated Learning (FL) aggregation strategies:
FedAvg, Krum Aggregation and a novel Loss-aware Federated Averaging (L-FedAvg)
were evaluated. Among these, Krum Aggregation demonstrated superior performance
across lexical and semantic evaluation metrics such as ROUGE, BLEU, BERTScore
and RaTEScore. The results show that FL can match or surpass centralized models
in generating clinically relevant and semantically rich radiology reports. This
lightweight and privacy-preserving framework paves the way for collaborative
medical AI development without compromising data confidentiality.

</details>


### [796] [Highly Efficient Non-Separable Transforms for Next Generation Video Coding](https://arxiv.org/pdf/2505.21728)
*Amir Said, Xin Zhao, Marta Karczewicz, Hilmi E. Egilmez, Vadim Seregin, Jianle Chen*

Main category: eess.IV

TL;DR: Proposes HyGTs, a low-complexity parametric transform for video compression, improving coding gain and reducing memory usage.


<details>
  <summary>Details</summary>
Motivation: Address the high computational complexity of matrix-based signal-adaptive transform coding in video compression.

Method: Introduces HyGTs, a class of transforms defined by parameters, implemented as an extension of HEVC.

Result: Achieves 6% bit rate reduction and 6.8 times less memory usage compared to KLT matrices.

Conclusion: HyGTs offer a practical, efficient alternative to traditional transform methods in video compression.

Abstract: For the last few decades, the application of signal-adaptive transform coding
to video compression has been stymied by the large computational complexity of
matrix-based solutions. In this paper, we propose a novel parametric approach
to greatly reduce the complexity without degrading the compression performance.
In our approach, instead of following the conventional technique of identifying
full transform matrices that yield best compression efficiency, we look for the
best transform parameters defining a new class of transforms, called HyGTs,
which have low complexity implementations that are easy to parallelize. The
proposed HyGTs are implemented as an extension of High Efficiency Video Coding
(HEVC), and our comprehensive experimental results demonstrate that proposed
HyGTs improve average coding gain by 6% bit rate reduction, while using 6.8
times less memory than KLT matrices.

</details>


### [797] [Beyond 1D: Vision Transformers and Multichannel Signal Images for PPG-to-ECG Reconstruction](https://arxiv.org/pdf/2505.21767)
*Xiaoyan Li, Shixin Xu, Faisal Habib, Arvind Gupta, Huaxiong Huang*

Main category: eess.IV

TL;DR: A novel PPG-to-ECG reconstruction method using a Vision Transformer (ViT) with a four-channel signal image representation outperforms existing methods, reducing errors by up to 29% in PRD and 15% in RMSE.


<details>
  <summary>Details</summary>
Motivation: Accurately reconstructing ECG from PPG is challenging due to fine-grained waveform features. Existing methods lack robustness in capturing these details.

Method: Proposes a ViT-based approach with a four-channel signal image (PPG, first-order difference, second-order difference, area under the curve) to enrich feature extraction and model inter/intra-beat dependencies.

Result: Achieves significant error reduction (29% PRD, 15% RMSE) and introduces new clinically relevant metrics (QRS area error, PR interval error, etc.).

Conclusion: The method demonstrates PPG's potential as an alternative for heart monitoring and opens new directions for cyclic signal analysis.

Abstract: Reconstructing ECG from PPG is a promising yet challenging task. While recent
advancements in generative models have significantly improved ECG
reconstruction, accurately capturing fine-grained waveform features remains a
key challenge. To address this, we propose a novel PPG-to-ECG reconstruction
method that leverages a Vision Transformer (ViT) as the core network. Unlike
conventional approaches that rely on single-channel PPG, our method employs a
four-channel signal image representation, incorporating the original PPG, its
first-order difference, second-order difference, and area under the curve. This
multi-channel design enriches feature extraction by preserving both temporal
and physiological variations within the PPG. By leveraging the self-attention
mechanism in ViT, our approach effectively captures both inter-beat and
intra-beat dependencies, leading to more robust and accurate ECG
reconstruction. Experimental results demonstrate that our method consistently
outperforms existing 1D convolution-based approaches, achieving up to 29%
reduction in PRD and 15% reduction in RMSE. The proposed approach also produces
improvements in other evaluation metrics, highlighting its robustness and
effectiveness in reconstructing ECG signals. Furthermore, to ensure a
clinically relevant evaluation, we introduce new performance metrics, including
QRS area error, PR interval error, RT interval error, and RT amplitude
difference error. Our findings suggest that integrating a four-channel signal
image representation with the self-attention mechanism of ViT enables more
effective extraction of informative PPG features and improved modeling of
beat-to-beat variations for PPG-to-ECG mapping. Beyond demonstrating the
potential of PPG as a viable alternative for heart activity monitoring, our
approach opens new avenues for cyclic signal analysis and prediction.

</details>


### [798] [Targeted Unlearning Using Perturbed Sign Gradient Methods With Applications On Medical Images](https://arxiv.org/pdf/2505.21872)
*George R. Nahass, Zhu Wang, Homa Rashidisabet, Won Hwa Kim, Sasha Hubschman, Jeffrey C. Peterson, Ghasem Yazdanpanah, Chad A. Purnell, Pete Setabutr, Ann Q. Tran, Darvin Yi, Sathya N. Ravi*

Main category: eess.IV

TL;DR: Machine unlearning is proposed as a modular tool for model revision in clinical contexts, outperforming baselines on forgetting and retention metrics.


<details>
  <summary>Details</summary>
Motivation: To address data shifts, device deprecation, and policy changes in clinical settings without full retraining.

Method: Bilevel optimization formulation of boundary-based unlearning with tunable loss design and iterative algorithms.

Result: Outperforms baselines on benchmark and clinical datasets, with convergence guarantees for first-order algorithms.

Conclusion: Machine unlearning is a practical alternative to retraining for clinical model maintenance.

Abstract: Machine unlearning aims to remove the influence of specific training samples
from a trained model without full retraining. While prior work has largely
focused on privacy-motivated settings, we recast unlearning as a
general-purpose tool for post-deployment model revision. Specifically, we focus
on utilizing unlearning in clinical contexts where data shifts, device
deprecation, and policy changes are common. To this end, we propose a bilevel
optimization formulation of boundary-based unlearning that can be solved using
iterative algorithms. We provide convergence guarantees when first-order
algorithms are used to unlearn. Our method introduces tunable loss design for
controlling the forgetting-retention tradeoff and supports novel model
composition strategies that merge the strengths of distinct unlearning runs.
Across benchmark and real-world clinical imaging datasets, our approach
outperforms baselines on both forgetting and retention metrics, including
scenarios involving imaging devices and anatomical outliers. This work
establishes machine unlearning as a modular, practical alternative to
retraining for real-world model maintenance in clinical applications.

</details>


### [799] [MAMBO-NET: Multi-Causal Aware Modeling Backdoor-Intervention Optimization for Medical Image Segmentation Network](https://arxiv.org/pdf/2505.21874)
*Ruiguo Yu, Yiyang Zhang, Yuan Tian, Yujie Diao, Di Jin, Witold Pedrycz*

Main category: eess.IV

TL;DR: MAMBO-NET introduces causal intervention to address confusion factors in medical image segmentation, improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods ignore confusion factors like anatomical variations and imaging limitations, leading to poor segmentation.

Method: Uses multi-Gaussian distributions for self-modeling, causal intervention, and posterior probability constraints.

Result: Experiments on five datasets show reduced confusion factor impact and better segmentation.

Conclusion: MAMBO-NET effectively mitigates confusion factors, enhancing segmentation performance.

Abstract: Medical image segmentation methods generally assume that the process from
medical image to segmentation is unbiased, and use neural networks to establish
conditional probability models to complete the segmentation task. This
assumption does not consider confusion factors, which can affect medical
images, such as complex anatomical variations and imaging modality limitations.
Confusion factors obfuscate the relevance and causality of medical image
segmentation, leading to unsatisfactory segmentation results. To address this
issue, we propose a multi-causal aware modeling backdoor-intervention
optimization (MAMBO-NET) network for medical image segmentation. Drawing
insights from causal inference, MAMBO-NET utilizes self-modeling with
multi-Gaussian distributions to fit the confusion factors and introduce causal
intervention into the segmentation process. Moreover, we design appropriate
posterior probability constraints to effectively train the distributions of
confusion factors. For the distributions to effectively guide the segmentation
and mitigate and eliminate the Impact of confusion factors on the segmentation,
we introduce classical backdoor intervention techniques and analyze their
feasibility in the segmentation task. To evaluate the effectiveness of our
approach, we conducted extensive experiments on five medical image datasets.
The results demonstrate that our method significantly reduces the influence of
confusion factors, leading to enhanced segmentation accuracy.

</details>


### [800] [Patch-based Reconstruction for Unsupervised Dynamic MRI using Learnable Tensor Function with Implicit Neural Representation](https://arxiv.org/pdf/2505.21894)
*Yuanyuan Liu, Yuanbiao Yang, Zhuo-Xu Cui, Qingyong Zhu, Jing Cheng, Congcong Liu, Jinwen Xie, Jingran Xu, Hairong Zheng, Dong Liang, Yanjie Zhu*

Main category: eess.IV

TL;DR: TenF-INR, a patch-based unsupervised framework using implicit neural representation (INR) and tensor decomposition, enhances dynamic MRI reconstruction by improving efficiency and quality, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: High spatiotemporal resolution in dynamic MRI is limited by long scan times, and existing deep learning methods require large fully-sampled datasets, which are hard to acquire.

Method: TenF-INR employs INR to model tensor decomposition bases, leveraging patch-based reconstruction and multidimensional low-rankness for efficient and accurate dynamic MRI modeling.

Result: TenF-INR achieves high acceleration factors (up to 21) and outperforms state-of-the-art methods in image quality, temporal fidelity, and quantitative metrics, even surpassing supervised methods.

Conclusion: TenF-INR provides a robust, efficient, and high-quality solution for dynamic MRI reconstruction without needing large fully-sampled datasets.

Abstract: Dynamic MRI plays a vital role in clinical practice by capturing both spatial
details and dynamic motion, but its high spatiotemporal resolution is often
limited by long scan times. Deep learning (DL)-based methods have shown
promising performance in accelerating dynamic MRI. However, most existing
algorithms rely on large fully-sampled datasets for training, which are
difficult to acquire. Recently, implicit neural representation (INR) has
emerged as a powerful scan-specific paradigm for accelerated MRI, which models
signals as a continuous function over spatiotemporal coordinates. Although this
approach achieves efficient continuous modeling of dynamic images and robust
reconstruction, it faces challenges in recovering fine details and increasing
computational demands for high dimensional data representation. To enhance both
efficiency and reconstruction quality, we propose TenF-INR, a novel patch-based
unsupervised framework that employs INR to model bases of tensor decomposition,
enabling efficient and accurate modeling of dynamic MR images with learnable
tensor functions. By exploiting strong correlations in similar spatial image
patches and in the temporal direction, TenF-INR enforces multidimensional
low-rankness and implements patch-based reconstruction with the benefits of
continuous modeling. We compare TenF-INR with state-of-the-art methods,
including supervised DL methods and unsupervised approaches. Experimental
results demonstrate that TenF-INR achieves high acceleration factors up to 21,
outperforming all comparison methods in image quality, temporal fidelity, and
quantitative metrics, even surpassing the supervised methods.

</details>


### [801] [Subspecialty-Specific Foundation Model for Intelligent Gastrointestinal Pathology](https://arxiv.org/pdf/2505.21928)
*Lianghui Zhu, Xitong Ling, Minxi Ouyang, Xiaoping Liu, Mingxi Fu, Tian Guan, Fanglei Fu, Xuanyu Wang, Maomao Zeng, Mingxi Zhu, Yibo Jin, Liming Liu, Song Duan, Qiming He, Yizhi Wang, Luxi Xie, Houqiang Li, Yonghong He, Sufang Tian*

Main category: eess.IV

TL;DR: Digepath is a specialized AI foundation model for GI pathology, achieving state-of-the-art performance in diagnosis, molecular prediction, and prognosis, with high sensitivity for early cancer detection.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of conventional histopathological diagnosis, which is subjective and lacks reproducibility, and to fill the gap in pathology-specific foundation models for GI diseases.

Method: Develops Digepath using a dual-phase iterative optimization strategy (pretraining with fine-screening) on 353 million image patches from 200,000+ GI disease slides.

Result: Achieves top performance on 33/34 GI pathology tasks and 99.6% sensitivity for early cancer detection across 9 institutions.

Conclusion: Digepath bridges gaps in histopathology, advancing AI-driven precision for GI diseases and offering a transferable model for other specialties.

Abstract: Gastrointestinal (GI) diseases represent a clinically significant burden,
necessitating precise diagnostic approaches to optimize patient outcomes.
Conventional histopathological diagnosis, heavily reliant on the subjective
interpretation of pathologists, suffers from limited reproducibility and
diagnostic variability. To overcome these limitations and address the lack of
pathology-specific foundation models for GI diseases, we develop Digepath, a
specialized foundation model for GI pathology. Our framework introduces a
dual-phase iterative optimization strategy combining pretraining with
fine-screening, specifically designed to address the detection of sparsely
distributed lesion areas in whole-slide images. Digepath is pretrained on more
than 353 million image patches from over 200,000 hematoxylin and eosin-stained
slides of GI diseases. It attains state-of-the-art performance on 33 out of 34
tasks related to GI pathology, including pathological diagnosis, molecular
prediction, gene mutation prediction, and prognosis evaluation, particularly in
diagnostically ambiguous cases and resolution-agnostic tissue classification.We
further translate the intelligent screening module for early GI cancer and
achieve near-perfect 99.6% sensitivity across 9 independent medical
institutions nationwide. The outstanding performance of Digepath highlights its
potential to bridge critical gaps in histopathological practice. This work not
only advances AI-driven precision pathology for GI diseases but also
establishes a transferable paradigm for other pathology subspecialties.

</details>


### [802] [Collaborative Learning for Unsupervised Multimodal Remote Sensing Image Registration: Integrating Self-Supervision and MIM-Guided Diffusion-Based Image Translation](https://arxiv.org/pdf/2505.22000)
*Xiaochen Wei, Weiwei Guo, Wenxian Yu*

Main category: eess.IV

TL;DR: CoLReg is a collaborative learning framework for unsupervised multimodal image registration, combining cross-modal synthesis, self-supervised registration, and distilled training to improve accuracy without labeled data.


<details>
  <summary>Details</summary>
Motivation: Multimodal image registration faces challenges due to modality variations. Supervised methods need annotated data, while unsupervised ones struggle with geometric discrepancies. CoLReg aims to bridge this gap.

Method: CoLReg uses three networks: MIMGCD for cross-modal synthesis, a self-supervised registration network, and a distilled registration network. They are jointly optimized via alternating training.

Result: CoLReg outperforms state-of-the-art unsupervised methods and rivals supervised baselines in registration accuracy.

Conclusion: The collaborative framework effectively reduces modality discrepancies and enhances registration performance, offering a practical solution for unsupervised multimodal image registration.

Abstract: The substantial modality-induced variations in radiometric, texture, and
structural characteristics pose significant challenges for the accurate
registration of multimodal images. While supervised deep learning methods have
demonstrated strong performance, they often rely on large-scale annotated
datasets, limiting their practical application. Traditional unsupervised
methods usually optimize registration by minimizing differences in feature
representations, yet often fail to robustly capture geometric discrepancies,
particularly under substantial spatial and radiometric variations, thus
hindering convergence stability. To address these challenges, we propose a
Collaborative Learning framework for Unsupervised Multimodal Image
Registration, named CoLReg, which reformulates unsupervised registration
learning into a collaborative training paradigm comprising three components:
(1) a cross-modal image translation network, MIMGCD, which employs a learnable
Maximum Index Map (MIM) guided conditional diffusion model to synthesize
modality-consistent image pairs; (2) a self-supervised intermediate
registration network which learns to estimate geometric transformations using
accurate displacement labels derived from MIMGCD outputs; (3) a distilled
cross-modal registration network trained with pseudo-label predicted by the
intermediate network. The three networks are jointly optimized through an
alternating training strategy wherein each network enhances the performance of
the others. This mutual collaboration progressively reduces modality
discrepancies, enhances the quality of pseudo-labels, and improves registration
accuracy. Extensive experimental results on multiple datasets demonstrate that
our ColReg achieves competitive or superior performance compared to
state-of-the-art unsupervised approaches and even surpasses several supervised
baselines.

</details>


### [803] [High Volume Rate 3D Ultrasound Reconstruction with Diffusion Models](https://arxiv.org/pdf/2505.22090)
*Tristan S. W. Stevens, Oisín Nolan, Oudom Somphone, Jean-Luc Robert, Ruud J. G. van Sloun*

Main category: eess.IV

TL;DR: A novel 3D ultrasound reconstruction method using diffusion models (DMs) improves image quality and temporal resolution compared to traditional and deep learning-based methods.


<details>
  <summary>Details</summary>
Motivation: 3D ultrasound offers real-time volumetric imaging but faces challenges in balancing high volume rates and image quality, especially with diverging waves.

Method: The paper employs diffusion models to reconstruct 3D ultrasound from undersampled elevation planes, leveraging temporal consistency and probabilistic sampling.

Result: DM-based reconstruction outperforms baselines in image quality and downstream tasks, with improved robustness and recall on out-of-distribution data.

Conclusion: Diffusion models enhance 3D ultrasound reconstruction, offering superior performance and uncertainty quantification for clinical applications.

Abstract: Three-dimensional ultrasound enables real-time volumetric visualization of
anatomical structures. Unlike traditional 2D ultrasound, 3D imaging reduces the
reliance on precise probe orientation, potentially making ultrasound more
accessible to clinicians with varying levels of experience and improving
automated measurements and post-exam analysis. However, achieving both high
volume rates and high image quality remains a significant challenge. While 3D
diverging waves can provide high volume rates, they suffer from limited tissue
harmonic generation and increased multipath effects, which degrade image
quality. One compromise is to retain the focusing in elevation while leveraging
unfocused diverging waves in the lateral direction to reduce the number of
transmissions per elevation plane. Reaching the volume rates achieved by full
3D diverging waves, however, requires dramatically undersampling the number of
elevation planes. Subsequently, to render the full volume, simple interpolation
techniques are applied. This paper introduces a novel approach to 3D ultrasound
reconstruction from a reduced set of elevation planes by employing diffusion
models (DMs) to achieve increased spatial and temporal resolution. We compare
both traditional and supervised deep learning-based interpolation methods on a
3D cardiac ultrasound dataset. Our results show that DM-based reconstruction
consistently outperforms the baselines in image quality and downstream task
performance. Additionally, we accelerate inference by leveraging the temporal
consistency inherent to ultrasound sequences. Finally, we explore the
robustness of the proposed method by exploiting the probabilistic nature of
diffusion posterior sampling to quantify reconstruction uncertainty and
demonstrate improved recall on out-of-distribution data with synthetic
anomalies under strong subsampling.

</details>


### [804] [MONSTR: Model-Oriented Neutron Strain Tomographic Reconstruction](https://arxiv.org/pdf/2505.22187)
*Mohammad Samin Nur Chowdhury, Shimin Tang, Singanallur V. Venkatakrishnan, Hassina Z. Bilheux, Gregery T. Buzzard, Charles A. Bouman*

Main category: eess.IV

TL;DR: The paper introduces MONSTR, a model-oriented algorithm for reconstructing 2D residual strain tensors from neutron Bragg edge strain measurements, addressing the ill-posed nature of the problem.


<details>
  <summary>Details</summary>
Motivation: Residual strain is crucial for metal part performance, but current tomography techniques face ill-posed reconstruction challenges due to scalar sinogram data.

Method: MONSTR uses the multi-agent consensus equilibrium framework, combining detector physics, tomographic reconstruction, and continuum mechanics constraints.

Result: Simulated data shows high-quality strain tensor reconstruction even with sparse measurements.

Conclusion: MONSTR effectively addresses the ill-posed problem, enabling accurate residual strain tensor reconstruction.

Abstract: Residual strain, a tensor quantity, is a critical material property that
impacts the overall performance of metal parts. Neutron Bragg edge strain
tomography is a technique for imaging residual strain that works by making
conventional hyperspectral computed tomography measurements, extracting the
average projected strain at each detector pixel, and processing the resulting
strain sinogram using a reconstruction algorithm. However, the reconstruction
is severely ill-posed as the underlying inverse problem involves inferring a
tensor at each voxel from scalar sinogram data.
  In this paper, we introduce the model-oriented neutron strain tomographic
reconstruction (MONSTR) algorithm that reconstructs the 2D residual strain
tensor from the neutron Bragg edge strain measurements. MONSTR is based on
using the multi-agent consensus equilibrium framework for the tensor
tomographic reconstruction. Specifically, we formulate the reconstruction as a
consensus solution of a collection of agents representing detector physics, the
tomographic reconstruction process, and physics-based constraints from
continuum mechanics. Using simulated data, we demonstrate high-quality
reconstruction of the strain tensor even when using very few measurements.

</details>


### [805] [Cascaded 3D Diffusion Models for Whole-body 3D 18-F FDG PET/CT synthesis from Demographics](https://arxiv.org/pdf/2505.22489)
*Siyeop Yoon, Sifan Song, Pengfei Jin, Matthew Tivnan, Yujin Oh, Sekeun Kim, Dufan Wu, Xiang Li, Quanzheng Li*

Main category: eess.IV

TL;DR: A cascaded 3D diffusion model synthesizes high-fidelity PET/CT volumes from demographic variables, offering realistic digital twins for medical imaging.


<details>
  <summary>Details</summary>
Motivation: Addresses the need for realistic digital twins in oncologic imaging, virtual trials, and AI-driven data augmentation, surpassing deterministic phantoms.

Method: Uses a two-stage generative process: a score-based diffusion model for low-resolution synthesis and a super-resolution residual diffusion model for refinement.

Result: Synthetic PET/CT images show strong concordance with real data, with metabolic uptake deviations within 3-5% of ground truth.

Conclusion: Cascaded 3D diffusion models provide anatomically and metabolically accurate synthetic imaging, enhancing clinical and research applications.

Abstract: We propose a cascaded 3D diffusion model framework to synthesize
high-fidelity 3D PET/CT volumes directly from demographic variables, addressing
the growing need for realistic digital twins in oncologic imaging, virtual
trials, and AI-driven data augmentation. Unlike deterministic phantoms, which
rely on predefined anatomical and metabolic templates, our method employs a
two-stage generative process. An initial score-based diffusion model
synthesizes low-resolution PET/CT volumes from demographic variables alone,
providing global anatomical structures and approximate metabolic activity. This
is followed by a super-resolution residual diffusion model that refines spatial
resolution. Our framework was trained on 18-F FDG PET/CT scans from the AutoPET
dataset and evaluated using organ-wise volume and standardized uptake value
(SUV) distributions, comparing synthetic and real data between demographic
subgroups. The organ-wise comparison demonstrated strong concordance between
synthetic and real images. In particular, most deviations in metabolic uptake
values remained within 3-5% of the ground truth in subgroup analysis. These
findings highlight the potential of cascaded 3D diffusion models to generate
anatomically and metabolically accurate PET/CT images, offering a robust
alternative to traditional phantoms and enabling scalable, population-informed
synthetic imaging for clinical and research applications.

</details>


### [806] [Risk-Sensitive Conformal Prediction for Catheter Placement Detection in Chest X-rays](https://arxiv.org/pdf/2505.22496)
*Long Hui*

Main category: eess.IV

TL;DR: A novel method for catheter and line position detection in chest X-rays using multi-task learning and risk-sensitive conformal prediction, achieving high reliability and zero high-risk mispredictions.


<details>
  <summary>Details</summary>
Motivation: Addressing critical clinical requirements for accurate and reliable detection of catheter and line positions in chest X-rays, ensuring safety in medical applications.

Method: Combines multi-task learning (classification, segmentation, landmark detection) with risk-sensitive conformal prediction to enhance reliability and performance.

Result: Achieves 90.68% overall empirical coverage, 99.29% for critical conditions, and zero high-risk mispredictions.

Conclusion: The approach is highly reliable and suitable for clinical deployment, offering accurate predictions and quantified uncertainty for life-critical applications.

Abstract: This paper presents a novel approach to catheter and line position detection
in chest X-rays, combining multi-task learning with risk-sensitive conformal
prediction to address critical clinical requirements. Our model simultaneously
performs classification, segmentation, and landmark detection, leveraging the
synergistic relationship between these tasks to improve overall performance. We
further enhance clinical reliability through risk-sensitive conformal
prediction, which provides statistically guaranteed prediction sets with higher
reliability for clinically critical findings. Experimental results demonstrate
excellent performance with 90.68\% overall empirical coverage and 99.29\%
coverage for critical conditions, while maintaining remarkable precision in
prediction sets. Most importantly, our risk-sensitive approach achieves zero
high-risk mispredictions (cases where the system dangerously declares
problematic tubes as confidently normal), making the system particularly
suitable for clinical deployment. This work offers both accurate predictions
and reliably quantified uncertainty -- essential features for life-critical
medical applications.

</details>


### [807] [Surf2CT: Cascaded 3D Flow Matching Models for Torso 3D CT Synthesis from Skin Surface](https://arxiv.org/pdf/2505.22511)
*Siyeop Yoon, Yujin Oh, Pengfei Jin, Sifan Song, Matthew Tivnan, Dufan Wu, Xiang Li, Quanzheng Li*

Main category: eess.IV

TL;DR: Surf2CT is a cascaded flow matching framework that generates 3D CT volumes from external surface scans and demographics, achieving high anatomical fidelity without internal imaging.


<details>
  <summary>Details</summary>
Motivation: To enable non-invasive internal anatomical imaging using only external data, reducing risks and expanding applications in healthcare.

Method: Three-stage process: Surface Completion (SDF reconstruction), Coarse CT Synthesis (low-res CT generation), and CT Super-Resolution (high-res refinement), using 3D-adapted EDM2 backbone trained via flow matching.

Result: Strong anatomical fidelity: small organ volume differences (-11.1% to 4.4%), high correlation for body composition (0.67 to 0.96), and improved surface completion metrics (Chamfer distance: 521.8 mm to 2.7 mm).

Conclusion: Surf2CT pioneers non-invasive internal imaging, offering potential for home-based healthcare and preventive medicine without conventional imaging risks.

Abstract: We present Surf2CT, a novel cascaded flow matching framework that synthesizes
full 3D computed tomography (CT) volumes of the human torso from external
surface scans and simple demographic data (age, sex, height, weight). This is
the first approach capable of generating realistic volumetric internal anatomy
images solely based on external body shape and demographics, without any
internal imaging. Surf2CT proceeds through three sequential stages: (1) Surface
Completion, reconstructing a complete signed distance function (SDF) from
partial torso scans using conditional 3D flow matching; (2) Coarse CT
Synthesis, generating a low-resolution CT volume from the completed SDF and
demographic information; and (3) CT Super-Resolution, refining the coarse
volume into a high-resolution CT via a patch-wise conditional flow model. Each
stage utilizes a 3D-adapted EDM2 backbone trained via flow matching. We trained
our model on a combined dataset of 3,198 torso CT scans (approximately 1.13
million axial slices) sourced from Massachusetts General Hospital (MGH) and the
AutoPET challenge. Evaluation on 700 paired torso surface-CT cases demonstrated
strong anatomical fidelity: organ volumes exhibited small mean percentage
differences (range from -11.1% to 4.4%), and muscle/fat body composition
metrics matched ground truth with strong correlation (range from 0.67 to 0.96).
Lung localization had minimal bias (mean difference -2.5 mm), and surface
completion significantly improved metrics (Chamfer distance: from 521.8 mm to
2.7 mm; Intersection-over-Union: from 0.87 to 0.98). Surf2CT establishes a new
paradigm for non-invasive internal anatomical imaging using only external data,
opening opportunities for home-based healthcare, preventive medicine, and
personalized clinical assessments without the risks associated with
conventional imaging techniques.

</details>


### [808] [ConfLUNet: Multiple sclerosis lesion instance segmentation in presence of confluent lesions](https://arxiv.org/pdf/2505.22537)
*Maxence Wynen, Pedro M. Gordaliza, Maxime Istasse, Anna Stölting, Pietro Maggi, Benoît Macq, Meritxell Bach Cuadra*

Main category: eess.IV

TL;DR: The paper introduces ConfLUNet, an end-to-end instance segmentation framework for MS lesions, outperforming existing methods in accuracy and addressing limitations of current practices.


<details>
  <summary>Details</summary>
Motivation: Current methods for MS lesion segmentation rely on post-processing techniques that misalign with clinical needs, particularly in handling confluent lesions.

Method: The authors propose ConfLUNet, which jointly optimizes lesion detection and delineation from FLAIR images, and introduce CLU-aware metrics for evaluation.

Result: ConfLUNet significantly outperforms existing methods (CC and ACLS) in instance segmentation and lesion detection, achieving higher precision and recall for CLUs.

Conclusion: This work establishes a foundation for accurate lesion instance segmentation in MS by combining rigorous definitions, new metrics, and a dedicated model.

Abstract: Accurate lesion-level segmentation on MRI is critical for multiple sclerosis
(MS) diagnosis, prognosis, and disease monitoring. However, current evaluation
practices largely rely on semantic segmentation post-processed with connected
components (CC), which cannot separate confluent lesions (aggregates of
confluent lesion units, CLUs) due to reliance on spatial connectivity. To
address this misalignment with clinical needs, we introduce formal definitions
of CLUs and associated CLU-aware detection metrics, and include them in an
exhaustive instance segmentation evaluation framework. Within this framework,
we systematically evaluate CC and post-processing-based Automated Confluent
Splitting (ACLS), the only existing methods for lesion instance segmentation in
MS. Our analysis reveals that CC consistently underestimates CLU counts, while
ACLS tends to oversplit lesions, leading to overestimated lesion counts and
reduced precision. To overcome these limitations, we propose ConfLUNet, the
first end-to-end instance segmentation framework for MS lesions. ConfLUNet
jointly optimizes lesion detection and delineation from a single FLAIR image.
Trained on 50 patients, ConfLUNet significantly outperforms CC and ACLS on the
held-out test set (n=13) in instance segmentation (Panoptic Quality: 42.0% vs.
37.5%/36.8%; p = 0.017/0.005) and lesion detection (F1: 67.3% vs. 61.6%/59.9%;
p = 0.028/0.013). For CLU detection, ConfLUNet achieves the highest F1[CLU]
(81.5%), improving recall over CC (+12.5%, p = 0.015) and precision over ACLS
(+31.2%, p = 0.003). By combining rigorous definitions, new CLU-aware metrics,
a reproducible evaluation framework, and the first dedicated end-to-end model,
this work lays the foundation for lesion instance segmentation in MS.

</details>


### [809] [Multipath cycleGAN for harmonization of paired and unpaired low-dose lung computed tomography reconstruction kernels](https://arxiv.org/pdf/2505.22568)
*Aravind R. Krishnan, Thomas Z. Li, Lucas W. Remedios, Michael E. Kim, Chenyu Gao, Gaurav Rudravaram, Elyssa M. McMaster, Adam M. Saunders, Shunxing Bao, Kaiwen Xu, Lianrui Zuo, Kim L. Sandler, Fabien Maldonado, Yuankai Huo, Bennett A. Landman*

Main category: eess.IV

TL;DR: A multipath cycleGAN model is proposed for CT kernel harmonization to improve emphysema quantification and preserve anatomical fidelity in CT scans.


<details>
  <summary>Details</summary>
Motivation: Reconstruction kernels in CT introduce variability in quantitative imaging measurements like emphysema quantification, necessitating kernel harmonization for consistent analysis.

Method: The model uses domain-specific encoders/decoders with a shared latent space and discriminators for each domain, trained on paired and unpaired data from the NLST dataset.

Result: Harmonization reduces bias in emphysema scores for paired kernels and eliminates confounding differences for unpaired kernels, while preserving anatomical structures.

Conclusion: The shared latent space multipath cycleGAN effectively harmonizes CT kernels, enhancing emphysema quantification and maintaining anatomical consistency.

Abstract: Reconstruction kernels in computed tomography (CT) affect spatial resolution
and noise characteristics, introducing systematic variability in quantitative
imaging measurements such as emphysema quantification. Choosing an appropriate
kernel is therefore essential for consistent quantitative analysis. We propose
a multipath cycleGAN model for CT kernel harmonization, trained on a mixture of
paired and unpaired data from a low-dose lung cancer screening cohort. The
model features domain-specific encoders and decoders with a shared latent space
and uses discriminators tailored for each domain.We train the model on 42
kernel combinations using 100 scans each from seven representative kernels in
the National Lung Screening Trial (NLST) dataset. To evaluate performance, 240
scans from each kernel are harmonized to a reference soft kernel, and emphysema
is quantified before and after harmonization. A general linear model assesses
the impact of age, sex, smoking status, and kernel on emphysema. We also
evaluate harmonization from soft kernels to a reference hard kernel. To assess
anatomical consistency, we compare segmentations of lung vessels, muscle, and
subcutaneous adipose tissue generated by TotalSegmentator between harmonized
and original images. Our model is benchmarked against traditional and
switchable cycleGANs. For paired kernels, our approach reduces bias in
emphysema scores, as seen in Bland-Altman plots (p<0.05). For unpaired kernels,
harmonization eliminates confounding differences in emphysema (p>0.05). High
Dice scores confirm preservation of muscle and fat anatomy, while lung vessel
overlap remains reasonable. Overall, our shared latent space multipath cycleGAN
enables robust harmonization across paired and unpaired CT kernels, improving
emphysema quantification and preserving anatomical fidelity.

</details>


### [810] [Comparative Analysis of Machine Learning Models for Lung Cancer Mutation Detection and Staging Using 3D CT Scans](https://arxiv.org/pdf/2505.22592)
*Yiheng Li, Francisco Carrillo-Perez, Mohammed Alawad, Olivier Gevaert*

Main category: eess.IV

TL;DR: Comparison of supervised (FMCIB+XGBoost) and self-supervised (Dinov2+ABMIL) models for lung cancer mutation detection and staging, with supervised models excelling in mutation detection and self-supervised models showing promise in staging.


<details>
  <summary>Details</summary>
Motivation: To improve non-invasive detection of lung cancer mutations and staging using machine learning, addressing the need for better patient outcomes.

Method: Evaluated FMCIB+XGBoost (supervised) and Dinov2+ABMIL (self-supervised) on 3D lung nodule data from Stanford cohorts for KRAS/EGFR mutation detection and cancer staging.

Result: FMCIB+XGBoost outperformed in mutation detection (0.846 for KRAS, 0.883 for EGFR), while Dinov2+ABMIL showed competitive staging accuracy (0.797 for T-stage).

Conclusion: Supervised models are better for mutation detection, while self-supervised learning has potential for staging generalization, with room for improving mutation sensitivity.

Abstract: Lung cancer is the leading cause of cancer mortality worldwide, and
non-invasive methods for detecting key mutations and staging are essential for
improving patient outcomes. Here, we compare the performance of two machine
learning models - FMCIB+XGBoost, a supervised model with domain-specific
pretraining, and Dinov2+ABMIL, a self-supervised model with attention-based
multiple-instance learning - on 3D lung nodule data from the Stanford
Radiogenomics and Lung-CT-PT-Dx cohorts. In the task of KRAS and EGFR mutation
detection, FMCIB+XGBoost consistently outperformed Dinov2+ABMIL, achieving
accuracies of 0.846 and 0.883 for KRAS and EGFR mutations, respectively. In
cancer staging, Dinov2+ABMIL demonstrated competitive generalization, achieving
an accuracy of 0.797 for T-stage prediction in the Lung-CT-PT-Dx cohort,
suggesting SSL's adaptability across diverse datasets. Our results emphasize
the clinical utility of supervised models in mutation detection and highlight
the potential of SSL to improve staging generalization, while identifying areas
for enhancement in mutation sensitivity.

</details>


### [811] [Chest Disease Detection In X-Ray Images Using Deep Learning Classification Method](https://arxiv.org/pdf/2505.22609)
*Alanna Hazlett, Naomi Ohashi, Timothy Rodriguez, Sodiq Adewole*

Main category: eess.IV

TL;DR: The paper evaluates multiple CNN models for classifying chest X-ray images into COVID-19, pneumonia, TB, and normal cases using transfer learning, achieving high accuracy and strong performance metrics.


<details>
  <summary>Details</summary>
Motivation: To improve classification of chest X-ray images for better diagnosis of COVID-19, pneumonia, and TB using advanced deep learning techniques.

Method: Fine-tuned pre-trained CNN models with transfer learning on labeled medical X-ray images and used Grad-CAM for interpretability.

Result: High accuracy and strong performance in precision, recall, and F1 score, with visual explanations for decisions.

Conclusion: The approach is effective for medical image classification, enhancing trust and transparency in clinical use.

Abstract: In this work, we investigate the performance across multiple classification
models to classify chest X-ray images into four categories of COVID-19,
pneumonia, tuberculosis (TB), and normal cases. We leveraged transfer learning
techniques with state-of-the-art pre-trained Convolutional Neural Networks
(CNNs) models. We fine-tuned these pre-trained architectures on a labeled
medical x-ray images. The initial results are promising with high accuracy and
strong performance in key classification metrics such as precision, recall, and
F1 score. We applied Gradient-weighted Class Activation Mapping (Grad-CAM) for
model interpretability to provide visual explanations for classification
decisions, improving trust and transparency in clinical applications.

</details>


### [812] [Structurally Different Neural Network Blocks for the Segmentation of Atrial and Aortic Perivascular Adipose Tissue in Multi-centre CT Angiography Scans](https://arxiv.org/pdf/2306.03494)
*Ikboljon Sobirov, Cheng Xie, Muhammad Siddique, Parijat Patel, Kenneth Chan, Thomas Halborg, Christos P. Kotanidis, Zarqaish Fatima, Henry West, Sheena Thomas, Maria Lyasheva, Donna Alexander, David Adlam, Praveen Rao, Das Indrajeet, Aparna Deshpande, Amrita Bajaj, Jonathan C L Rodrigues, Benjamin J Hudson, Vivek Srivastava, George Krasopoulos, Rana Sayeed, Qiang Zhang, Pete Tomlins, Cheerag Shirodaria, Keith M. Channon, Stefan Neubauer, Charalambos Antoniades, Mohammad Yaqub*

Main category: eess.IV

TL;DR: LegoNet alternates CNN and SwinViT blocks for medical image segmentation, outperforming other architectures and showing high generalizability and clinical reliability.


<details>
  <summary>Details</summary>
Motivation: To leverage complementary strengths of different architectural designs (CNNs and ViTs) for improved medical image segmentation, particularly for cardiovascular risk assessment.

Method: Proposes LegoNet, alternating CNN-based and SwinViT-based blocks, and tests three variations on IMA, aorta, and PVAT segmentation from CTA scans.

Result: Achieves superior performance (DSC > 0.90) on large datasets and external cohorts, with strong agreement in human annotations.

Conclusion: LegoNet offers a robust, automated solution for vascular segmentation, aiding diagnostic cardiovascular management and personalized medicine.

Abstract: Since the emergence of convolutional neural networks (CNNs) and, later,
vision transformers (ViTs), deep learning architectures have predominantly
relied on identical block types with varying hyperparameters. We propose a
novel block alternation strategy to leverage the complementary strengths of
different architectural designs, assembling structurally distinct components
similar to Lego blocks. We introduce LegoNet, a deep learning framework that
alternates CNN-based and SwinViT-based blocks to enhance feature learning for
medical image segmentation. We investigate three variations of LegoNet and
apply this concept to a previously unexplored clinical problem: the
segmentation of the internal mammary artery (IMA), aorta, and perivascular
adipose tissue (PVAT) from computed tomography angiography (CTA) scans. These
PVAT regions have been shown to possess prognostic value in assessing
cardiovascular risk and primary clinical outcomes. We evaluate LegoNet on large
datasets, achieving superior performance to other leading architectures.
Furthermore, we assess the model's generalizability on external testing
cohorts, where an expert clinician corrects the model's segmentations,
achieving DSC > 0.90 across various external, international, and public
cohorts. To further validate the model's clinical reliability, we perform
intra- and inter-observer variability analysis, demonstrating strong agreement
with human annotations. The proposed methodology has significant implications
for diagnostic cardiovascular management and early prognosis, offering a
robust, automated solution for vascular and perivascular segmentation and risk
assessment in clinical practice, paving the way for personalised medicine.

</details>


### [813] [MOSformer: Momentum encoder-based inter-slice fusion transformer for medical image segmentation](https://arxiv.org/pdf/2401.11856)
*De-Xing Huang, Xiao-Hu Zhou, Mei-Jiang Gui, Xiao-Liang Xie, Shi-Qi Liu, Shuang-Yi Wang, Zhen-Qiu Feng, Zeng-Guang Hou*

Main category: eess.IV

TL;DR: A novel 2.5D-based model, MOSformer, uses dual encoders and an inter-slice fusion transformer to improve medical image segmentation by effectively leveraging multi-scale inter-slice information.


<details>
  <summary>Details</summary>
Motivation: Existing 2.5D models fail to fuse inter-slice information effectively, leading to suboptimal segmentation performance.

Method: MOSformer employs dual encoders (one moving-averaged) and an IF-Trans module to fuse multi-scale inter-slice features.

Result: Achieves state-of-the-art DSC scores of 85.63%, 92.19%, and 85.43% on Synapse, ACDC, and AMOS datasets.

Conclusion: MOSformer demonstrates superior performance in medical image segmentation by effectively utilizing inter-slice information.

Abstract: Medical image segmentation takes an important position in various clinical
applications. 2.5D-based segmentation models bridge the computational
efficiency of 2D-based models with the spatial perception capabilities of
3D-based models. However, existing 2.5D-based models primarily adopt a single
encoder to extract features of target and neighborhood slices, failing to
effectively fuse inter-slice information, resulting in suboptimal segmentation
performance. In this study, a novel momentum encoder-based inter-slice fusion
transformer (MOSformer) is proposed to overcome this issue by leveraging
inter-slice information at multi-scale feature maps extracted by different
encoders. Specifically, dual encoders are employed to enhance feature
distinguishability among different slices. One of the encoders is
moving-averaged to maintain consistent slice representations. Moreover, an
inter-slice fusion transformer (IF-Trans) module is developed to fuse
inter-slice multi-scale features. The MOSformer is evaluated on three benchmark
datasets (Synapse, ACDC, and AMOS), achieving a new state-of-the-art with
85.63%, 92.19%, and 85.43% DSC, respectively. These results demonstrate
MOSformer's competitiveness in medical image segmentation.

</details>


### [814] [SLoRD: Structural Low-Rank Descriptors for Shape Consistency in Vertebrae Segmentation](https://arxiv.org/pdf/2407.08555)
*Xin You, Yixin Lou, Minghui Zhang, Jie Yang, Yun Gu*

Main category: eess.IV

TL;DR: Proposes SLoRD, a contour generation network using Structural Low-Rank Descriptors for precise multi-class vertebrae segmentation in CT images, addressing intra-vertebrae inconsistency.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to ensure contour precision and intra-vertebrae consistency due to similar appearances and pathologies.

Method: Uses spherical coordinate system and spherical centroid for contour descriptors, leveraging offline priors and explicit shape constraints.

Result: Outperforms SOTA methods on VerSe 2019 and 2020 datasets, refining coarse predictions from other approaches.

Conclusion: SLoRD effectively addresses segmentation inconsistency and is a plug-and-play solution for vertebrae segmentation.

Abstract: Automatic and precise multi-class vertebrae segmentation from CT images is
crucial for various clinical applications. However, due to similar appearances
between adjacent vertebrae and the existence of various pathologies, existing
single-stage and multi-stage methods suffer from imprecise vertebrae
segmentation. Essentially, these methods fail to explicitly impose both contour
precision and intra-vertebrae voxel consistency constraints synchronously,
resulting in the intra-vertebrae segmentation inconsistency, which refers to
multiple label predictions inside a singular vertebra. In this work, we intend
to label complete binary masks with sequential indices to address that
challenge. Specifically, a contour generation network is proposed based on
Structural Low-Rank Descriptors for shape consistency, termed SLoRD. For a
structural representation of vertebral contours, we adopt the spherical
coordinate system and devise the spherical centroid to calculate contour
descriptors. Due to vertebrae's similar appearances, basic contour descriptors
can be acquired offline to restore original contours. Therefore, SLoRD
leverages these contour priors and explicit shape constraints to facilitate
regressed contour points close to vertebral surfaces. Quantitative and
qualitative evaluations on VerSe 2019 and 2020 demonstrate the superior
performance of our framework over other single-stage and multi-stage
state-of-the-art (SOTA) methods. Further, SLoRD is a plug-and-play framework to
refine the segmentation inconsistency existing in coarse predictions from other
approaches. Source codes are available.

</details>


### [815] [A Plug-and-Play Method for Guided Multi-contrast MRI Reconstruction based on Content/Style Modeling](https://arxiv.org/pdf/2409.13477)
*Chinmay Rao, Matthias van Osch, Nicola Pezzotti, Jeroen de Bresser, Laurens Beljaards, Jakob Meineke, Elwin de Weerdt, Huangling Lu, Mariya Doneva, Marius Staring*

Main category: eess.IV

TL;DR: The paper introduces PnP-CoSMo, a modular two-stage approach for guided MRI reconstruction using a content/style model to address the challenge of requiring large paired datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the need for large paired training datasets in guided MRI reconstruction by leveraging shared and non-shared generative factors between contrasts.

Method: A content/style model is learned from unpaired data and used as a plug-and-play operator in iterative reconstruction, disentangling contrast-independent and contrast-specific factors.

Result: PnP-CoSMo shows improved generalizability, up to 32.6% more acceleration over non-guided methods, and 33.3% more acceleration at diagnostic quality in radiological tasks.

Conclusion: The approach offers a practical, interpretable, and efficient solution for multi-contrast MRI reconstruction, outperforming end-to-end methods.

Abstract: Since multiple MRI contrasts of the same anatomy contain redundant
information, one contrast can guide the reconstruction of an undersampled
subsequent contrast. To this end, several end-to-end learning-based guided
reconstruction methods have been proposed. However, a key challenge is the
requirement of large paired training datasets comprising raw data and aligned
reference images. We propose a modular two-stage approach addressing this
issue, additionally providing an explanatory framework for the multi-contrast
problem based on the shared and non-shared generative factors underlying two
given contrasts. A content/style model of two-contrast image data is learned
from a largely unpaired image-domain dataset and is subsequently applied as a
plug-and-play operator in iterative reconstruction. The disentanglement of
content and style allows explicit representation of contrast-independent and
contrast-specific factors. Consequently, incorporating prior information into
the reconstruction reduces to a simple replacement of the aliased content of
the reconstruction iterate with high-quality content derived from the reference
scan. Combining this component with a data consistency step and introducing a
general corrective process for the content yields an iterative scheme. We name
this novel approach PnP-CoSMo. Various aspects like interpretability and
convergence are explored via simulations. Furthermore, its practicality is
demonstrated on the NYU fastMRI DICOM dataset, showing improved
generalizability compared to end-to-end methods, and on two in-house multi-coil
raw datasets, offering up to 32.6% more acceleration over learning-based
non-guided reconstruction for a given SSIM. In a small radiological task,
PnP-CoSMo allowed 33.3% more acceleration over clinical reconstruction at
diagnostic quality.

</details>


### [816] [Distribution-aware Fairness Learning in Medical Image Segmentation From A Control-Theoretic Perspective](https://arxiv.org/pdf/2502.00619)
*Yujin Oh, Pengfei Jin, Sangjoon Park, Sekeun Kim, Siyeop Yoon, Kyungsang Kim, Jin Sung Kim, Xiang Li, Quanzheng Li*

Main category: eess.IV

TL;DR: The paper introduces Distribution-aware Mixture of Experts (dMoE) to address fairness in medical image segmentation by mitigating biases from imbalanced data. It integrates demographic and clinical factors, achieving top performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: To tackle biases in medical image segmentation caused by imbalanced data acquisition due to demographic and clinical factors.

Method: Proposes dMoE, inspired by optimal control theory, to adapt to heterogeneous distributions. It is integrated into various network architectures.

Result: dMoE achieves state-of-the-art performance on 2D benchmark and 3D in-house datasets, effectively mitigating biases.

Conclusion: dMoE bridges control theory and medical image segmentation, offering a fair and effective approach. The source code is publicly available.

Abstract: Ensuring fairness in medical image segmentation is critical due to biases in
imbalanced clinical data acquisition caused by demographic attributes (e.g.,
age, sex, race) and clinical factors (e.g., disease severity). To address these
challenges, we introduce Distribution-aware Mixture of Experts (dMoE), inspired
by optimal control theory. We provide a comprehensive analysis of its
underlying mechanisms and clarify dMoE's role in adapting to heterogeneous
distributions in medical image segmentation. Furthermore, we integrate dMoE
into multiple network architectures, demonstrating its broad applicability
across diverse medical image analysis tasks. By incorporating demographic and
clinical factors, dMoE achieves state-of-the-art performance on two 2D
benchmark datasets and a 3D in-house dataset. Our results highlight the
effectiveness of dMoE in mitigating biases from imbalanced distributions,
offering a promising approach to bridging control theory and medical image
segmentation within fairness learning paradigms. The source code will be made
available. The source code is available at https://github.com/tvseg/dMoE.

</details>


### [817] [Bridging Scales in Map Generation: A scale-aware cascaded generative mapping framework for seamless and consistent multi-scale cartographic representation](https://arxiv.org/pdf/2502.04991)
*Chenxing Sun, Yongyang Xu, Xuwei Xu, Xixi Fan, Jing Bai, Xiechun Lu, Zhanlong Chen*

Main category: eess.IV

TL;DR: The paper proposes a scale-aware cartographic generation framework (SCGM) to address challenges in multi-scale map generation, ensuring geospatial fidelity and visual consistency.


<details>
  <summary>Details</summary>
Motivation: Existing methods prioritize texture replication over geospatial feature preservation, leading to cartographic invalidity and spatial discontinuities.

Method: SCGM uses conditional guided diffusion and a multi-scale cascade architecture, incorporating scale modality encoding, a scale-driven conditional encoder, and a cascade reference mechanism.

Result: SCGM generates seamless multi-scale maps with improved spatial coherence and generalization-aware representation, validated on benchmarks.

Conclusion: The framework shows promise for applications like emergency mapping and automated cartography, addressing key limitations in current approaches.

Abstract: Multi-scale tile maps are essential for geographic information services,
serving as fundamental outcomes of surveying and cartographic workflows. While
existing image generation networks can produce map-like outputs from remote
sensing imagery, their emphasis on replicating texture rather than preserving
geospatial features limits cartographic validity. Current approaches face two
fundamental challenges: inadequate integration of cartographic generalization
principles with dynamic multi-scale generation and spatial discontinuities
arising from tile-wise generation. To address these limitations, we propose a
scale-aware cartographic generation framework (SCGM) that leverages conditional
guided diffusion and a multi-scale cascade architecture. The framework
introduces three key innovations: a scale modality encoding mechanism to
formalize map generalization relationships, a scale-driven conditional encoder
for robust feature fusion, and a cascade reference mechanism ensuring
cross-scale visual consistency. By hierarchically constraining large-scale map
synthesis with small-scale structural priors, SCGM effectively mitigates edge
artifacts while maintaining geographic fidelity. Comprehensive evaluations on
cartographic benchmarks confirm the framework's ability to generate seamless
multi-scale tile maps with enhanced spatial coherence and generalization-aware
representation, demonstrating significant potential for emergency mapping and
automated cartography applications.

</details>


### [818] [SegRet: An Efficient Design for Semantic Segmentation with Retentive Network](https://arxiv.org/pdf/2502.14014)
*Zhiyuan Li, Yi Chang, Yuan Wu*

Main category: eess.IV

TL;DR: SegRet, a lightweight semantic segmentation model using RetNet and a zero-initialized residual decoder, balances performance and efficiency, achieving SOTA results with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between model performance and computational efficiency in semantic segmentation for autonomous driving and intelligent transportation systems.

Method: Proposes SegRet, combining RetNet for robust feature extraction and a lightweight residual decoder with zero-initialization for efficiency.

Result: Achieves state-of-the-art performance on benchmarks like ADE20K, Cityscapes, and COCO-Stuff while reducing parameters.

Conclusion: SegRet effectively balances accuracy and computational efficiency, making it suitable for advanced applications in autonomous driving.

Abstract: With the rapid evolution of autonomous driving technology and intelligent
transportation systems, semantic segmentation has become increasingly critical.
Precise interpretation and analysis of real-world environments are
indispensable for these advanced applications. However, traditional semantic
segmentation approaches frequently face challenges in balancing model
performance with computational efficiency, especially regarding the volume of
model parameters. To address these constraints, we propose SegRet, a novel
model employing the Retentive Network (RetNet) architecture coupled with a
lightweight residual decoder that integrates zero-initialization. SegRet offers
three distinctive advantages: (1) Lightweight Residual Decoder: by embedding a
zero-initialization layer within the residual network structure, the decoder
remains computationally streamlined without sacrificing essential information
propagation; (2) Robust Feature Extraction: adopting RetNet as its backbone
enables SegRet to effectively capture hierarchical image features, thereby
enriching the representation quality of extracted features; (3) Parameter
Efficiency: SegRet attains state-of-the-art (SOTA) segmentation performance
while markedly decreasing the number of parameters, ensuring high accuracy
without imposing additional computational burdens. Comprehensive empirical
evaluations on prominent benchmarks, such as ADE20K, Citycapes, and COCO-Stuff,
highlight the effectiveness and superiority of our method.

</details>


### [819] [X-GAN: A Generative AI-Powered Unsupervised Model for Main Vessel Segmentation of Glaucoma Screening](https://arxiv.org/pdf/2503.06743)
*Cheng Huang, Weizheng Xie, Tsengdar J. Lee, Jui-Kai Wang, Karanjit Kooner, Ning Zhang, Jia Zhang*

Main category: eess.IV

TL;DR: X-GAN, an unsupervised AI model, segments retinal blood vessels in OCTA images with high accuracy using GAN and biostatistical modeling, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Identifying retinal blood vessels is crucial for glaucoma diagnosis but challenging due to the complexity of vessel structures.

Method: Combines Space Colonization Algorithm for vessel skeletonization with GAN and biostatistical modeling to reconstruct 2D/3D vessels without labeled data.

Result: Achieves nearly 100% segmentation accuracy, surpassing other deep learning models.

Conclusion: X-GAN offers a fast, accurate, and resource-efficient solution for retinal vessel segmentation.

Abstract: Structural changes in main retinal blood vessels serve as critical biomarkers
for the onset and progression of glaucoma. Identifying these vessels is vital
for vascular modeling yet highly challenging. This paper proposes X-GAN, a
generative AI-powered unsupervised segmentation model designed for extracting
main blood vessels from Optical Coherence Tomography Angiography (OCTA) images.
The process begins with the Space Colonization Algorithm (SCA) to rapidly
generate a skeleton of vessels, featuring their radii. By synergistically
integrating the generative adversarial network (GAN) with biostatistical
modeling of vessel radii, X-GAN enables a fast reconstruction of both 2D and 3D
representations of the vessels. Based on this reconstruction, X-GAN achieves
nearly 100% segmentation accuracy without relying on labeled data or
high-performance computing resources. Experimental results confirm X-GAN's
superiority in evaluating main vessel segmentation compared to existing deep
learning models. Code is here: https://github.com/VikiXie/SatMar8.

</details>
