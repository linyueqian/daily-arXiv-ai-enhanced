<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 107]
- [cs.CV](#cs.CV) [Total: 135]
- [cs.AI](#cs.AI) [Total: 56]
- [cs.SD](#cs.SD) [Total: 11]
- [cs.LG](#cs.LG) [Total: 204]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.MM](#cs.MM) [Total: 2]
- [eess.AS](#eess.AS) [Total: 4]
- [eess.IV](#eess.IV) [Total: 16]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Artificial Intelligence Bias on English Language Learners in Automatic Scoring](https://arxiv.org/pdf/2505.10643)
*Shuchen Guo, Yun Wang, Jichao Yu, Xuansheng Wu, Bilgehan Ayik, Field M. Watts, Ehsan Latif, Ninghao Liu, Lei Liu, Xiaoming Zhai*

Main category: cs.CL

TL;DR: The study examines bias in automatic scoring for ELLs in science assessments, finding no significant bias with large training datasets but potential issues with smaller samples.


<details>
  <summary>Details</summary>
Motivation: To investigate scoring biases and disparities toward ELLs in automated systems due to unbalanced training data.

Method: Fine-tuned BERT with four datasets (ELLs, non-ELLs, unbalanced mixed, balanced mixed) and analyzed scoring accuracy and Mean Score Gaps (MSGs) across 21 items.

Result: No AI bias or disparities found with large datasets (30,000 or 1,000 ELL responses), but concerns arose with smaller samples (200 ELL responses).

Conclusion: Training data size impacts bias detection; larger datasets mitigate scoring disparities for ELLs in automated systems.

Abstract: This study investigated potential scoring biases and disparities toward
English Language Learners (ELLs) when using automatic scoring systems for
middle school students' written responses to science assessments. We
specifically focus on examining how unbalanced training data with ELLs
contributes to scoring bias and disparities. We fine-tuned BERT with four
datasets: responses from (1) ELLs, (2) non-ELLs, (3) a mixed dataset reflecting
the real-world proportion of ELLs and non-ELLs (unbalanced), and (4) a balanced
mixed dataset with equal representation of both groups. The study analyzed 21
assessment items: 10 items with about 30,000 ELL responses, five items with
about 1,000 ELL responses, and six items with about 200 ELL responses. Scoring
accuracy (Acc) was calculated and compared to identify bias using Friedman
tests. We measured the Mean Score Gaps (MSGs) between ELLs and non-ELLs and
then calculated the differences in MSGs generated through both the human and AI
models to identify the scoring disparities. We found that no AI bias and
distorted disparities between ELLs and non-ELLs were found when the training
dataset was large enough (ELL = 30,000 and ELL = 1,000), but concerns could
exist if the sample size is limited (ELL = 200).

</details>


### [2] [GeoGrid-Bench: Can Foundation Models Understand Multimodal Gridded Geo-Spatial Data?](https://arxiv.org/pdf/2505.10714)
*Bowen Jiang, Yangxinyu Xie, Xiaomeng Wang, Jiashu He, Joshua Bergerson, John K Hutchison, Jordan Branham, Camillo J Taylor, Tanwi Mallick*

Main category: cs.CL

TL;DR: GeoGrid-Bench is a benchmark for evaluating foundation models on geo-spatial data, featuring 3,200 QA pairs from expert-curated templates. Vision-language models perform best.


<details>
  <summary>Details</summary>
Motivation: To assess foundation models' ability to handle geo-spatial data challenges like dense numerical values, spatiotemporal dependencies, and multimodal representations.

Method: Uses large-scale real-world data (16 climate variables, 150 locations) and 3,200 QA pairs from 8 expert templates, covering basic to complex tasks.

Result: Vision-language models outperform others, with detailed analysis of strengths/limitations in geo-spatial tasks.

Conclusion: GeoGrid-Bench clarifies how foundation models can effectively support geo-spatial data analysis and scientific research.

Abstract: We present GeoGrid-Bench, a benchmark designed to evaluate the ability of
foundation models to understand geo-spatial data in the grid structure.
Geo-spatial datasets pose distinct challenges due to their dense numerical
values, strong spatial and temporal dependencies, and unique multimodal
representations including tabular data, heatmaps, and geographic
visualizations. To assess how foundation models can support scientific research
in this domain, GeoGrid-Bench features large-scale, real-world data covering 16
climate variables across 150 locations and extended time frames. The benchmark
includes approximately 3,200 question-answer pairs, systematically generated
from 8 domain expert-curated templates to reflect practical tasks encountered
by human scientists. These range from basic queries at a single location and
time to complex spatiotemporal comparisons across regions and periods. Our
evaluation reveals that vision-language models perform best overall, and we
provide a fine-grained analysis of the strengths and limitations of different
foundation models in different geo-spatial tasks. This benchmark offers clearer
insights into how foundation models can be effectively applied to geo-spatial
data analysis and used to support scientific research.

</details>


### [3] [Survey of End-to-End Multi-Speaker Automatic Speech Recognition for Monaural Audio](https://arxiv.org/pdf/2505.10975)
*Xinlu He, Jacob Whitehill*

Main category: cs.CL

TL;DR: This survey reviews end-to-end (E2E) neural approaches for monaural multi-speaker ASR, covering architectural paradigms, recent improvements, extensions to long-form speech, and benchmark comparisons, while highlighting open challenges.


<details>
  <summary>Details</summary>
Motivation: The field lacks a comprehensive review of recent E2E advancements in multi-speaker ASR, which is crucial for addressing data scarcity and overlapping speech challenges.

Method: The paper systematically categorizes E2E approaches (SIMO vs. SISO), analyzes architectural and algorithmic improvements, and evaluates methods on standard benchmarks.

Result: The survey provides insights into trade-offs, recent advances, and performance comparisons, aiding in understanding robust multi-speaker ASR solutions.

Conclusion: Open challenges remain in scalability and robustness, guiding future research directions for multi-speaker ASR.

Abstract: Monaural multi-speaker automatic speech recognition (ASR) remains challenging
due to data scarcity and the intrinsic difficulty of recognizing and
attributing words to individual speakers, particularly in overlapping speech.
Recent advances have driven the shift from cascade systems to end-to-end (E2E)
architectures, which reduce error propagation and better exploit the synergy
between speech content and speaker identity. Despite rapid progress in E2E
multi-speaker ASR, the field lacks a comprehensive review of recent
developments. This survey provides a systematic taxonomy of E2E neural
approaches for multi-speaker ASR, highlighting recent advances and comparative
analysis. Specifically, we analyze: (1) architectural paradigms (SIMO vs.~SISO)
for pre-segmented audio, analyzing their distinct characteristics and
trade-offs; (2) recent architectural and algorithmic improvements based on
these two paradigms; (3) extensions to long-form speech, including segmentation
strategy and speaker-consistent hypothesis stitching. Further, we (4) evaluate
and compare methods across standard benchmarks. We conclude with a discussion
of open challenges and future research directions towards building robust and
scalable multi-speaker ASR.

</details>


### [4] [A Modular Approach for Clinical SLMs Driven by Synthetic Data with Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment](https://arxiv.org/pdf/2505.10717)
*Jean-Philippe Corbeil, Amin Dada, Jean-Michel Attendu, Asma Ben Abacha, Alessandro Sordoni, Lucas Caccia, Fran√ßois Beaulieu, Thomas Lin, Jens Kleesiek, Paul Vozila*

Main category: cs.CL

TL;DR: A framework for adapting small language models (SLMs) into high-performing clinical models, outperforming GPT-4 in some tasks.


<details>
  <summary>Details</summary>
Motivation: High computation costs and latency of large models like GPT-4 limit clinical deployment, while SLMs need biomedical domain adaptation.

Method: Pre-instruction tuning, model merging, and clinical-tasks alignment using the MediPhi framework and CLUE+ benchmark.

Result: Relative improvements over base models: 64.3% on medical entities, 49.5% on radiology reports, and 44% on ICD-10 coding (outperforming GPT-4 by 14%).

Conclusion: The MediPhi framework and synthetic dataset MediFlow enable efficient adaptation of SLMs for clinical tasks, achieving significant performance gains.

Abstract: High computation costs and latency of large language models such as GPT-4
have limited their deployment in clinical settings. Small language models
(SLMs) offer a cost-effective alternative, but their limited capacity requires
biomedical domain adaptation, which remains challenging. An additional
bottleneck is the unavailability and high sensitivity of clinical data. To
address these challenges, we propose a novel framework for adapting SLMs into
high-performing clinical models. We introduce the MediPhi collection of
3.8B-parameter SLMs developed with our novel framework: pre-instruction tuning
of experts on relevant medical and clinical corpora (PMC, Medical Guideline,
MedWiki, etc.), model merging, and clinical-tasks alignment. To cover most
clinical tasks, we extended the CLUE benchmark to CLUE+, doubling its size. Our
expert models deliver relative improvements on this benchmark over the base
model without any task-specific fine-tuning: 64.3% on medical entities, 49.5%
on radiology reports, and 44% on ICD-10 coding (outperforming GPT-4-0125 by
14%). We unify the expert models into MediPhi via model merging, preserving
gains across benchmarks. Furthermore, we built the MediFlow collection, a
synthetic dataset of 2.5 million high-quality instructions on 14 medical NLP
tasks, 98 fine-grained document types, and JSON format support. Alignment of
MediPhi using supervised fine-tuning and direct preference optimization
achieves further gains of 18.9% on average.

</details>


### [5] [AI-enhanced semantic feature norms for 786 concepts](https://arxiv.org/pdf/2505.10718)
*Siddharth Suresh, Kushin Mukherjee, Tyler Giallanza, Xizheng Yu, Mia Patil, Jonathan D. Cohen, Timothy T. Rogers*

Main category: cs.CL

TL;DR: The paper introduces NOVA, an AI-enhanced feature norm dataset combining human-generated norms with LLM responses, showing higher quality and predictive power than human-only datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional semantic feature norm methods are limited by labor-intensive processes and trade-offs between coverage and quality.

Method: Augment human-generated feature norms with LLM responses and verify quality against human judgments.

Result: NOVA outperforms human-only datasets and word-embedding models in predicting semantic similarity judgments.

Conclusion: Human conceptual knowledge is richer than previously captured, and validated LLMs can enhance cognitive science research.

Abstract: Semantic feature norms have been foundational in the study of human
conceptual knowledge, yet traditional methods face trade-offs between
concept/feature coverage and verifiability of quality due to the
labor-intensive nature of norming studies. Here, we introduce a novel approach
that augments a dataset of human-generated feature norms with responses from
large language models (LLMs) while verifying the quality of norms against
reliable human judgments. We find that our AI-enhanced feature norm dataset,
NOVA: Norms Optimized Via AI, shows much higher feature density and overlap
among concepts while outperforming a comparable human-only norm dataset and
word-embedding models in predicting people's semantic similarity judgments.
Taken together, we demonstrate that human conceptual knowledge is richer than
captured in previous norm datasets and show that, with proper validation, LLMs
can serve as powerful tools for cognitive science research.

</details>


### [6] [CAMEO: Collection of Multilingual Emotional Speech Corpora](https://arxiv.org/pdf/2505.11051)
*Iwona Christop, Maciej Czajka*

Main category: cs.CL

TL;DR: CAMEO is a multilingual emotional speech dataset collection for emotion recognition research, ensuring accessibility, reproducibility, and standardized benchmarking.


<details>
  <summary>Details</summary>
Motivation: To facilitate research in emotion recognition and speech-related tasks by providing easy access to multilingual emotional speech data.

Method: Describes dataset selection criteria, curation, normalization, and performance evaluation of models.

Result: The collection, metadata, and a leaderboard are publicly available on Hugging Face.

Conclusion: CAMEO serves as a valuable resource for standardized evaluation of speech emotion recognition systems across languages and emotional states.

Abstract: This paper presents CAMEO -- a curated collection of multilingual emotional
speech datasets designed to facilitate research in emotion recognition and
other speech-related tasks. The main objectives were to ensure easy access to
the data, to allow reproducibility of the results, and to provide a
standardized benchmark for evaluating speech emotion recognition (SER) systems
across different emotional states and languages. The paper describes the
dataset selection criteria, the curation and normalization process, and
provides performance results for several models. The collection, along with
metadata, and a leaderboard, is publicly available via the Hugging Face
platform.

</details>


### [7] [Tracr-Injection: Distilling Algorithms into Pre-trained Language Models](https://arxiv.org/pdf/2505.10719)
*Tom√°s Vergara-Browne, √Ålvaro Soto*

Main category: cs.CL

TL;DR: The paper introduces tracr-injection, a method to distill RASP algorithms into pre-trained language models, improving interpretability and out-of-distribution performance.


<details>
  <summary>Details</summary>
Motivation: Address the mismatch between theoretical transformer capabilities (via RASP) and practical learnability from unsupervised data.

Method: Propose tracr-injection to compile RASP algorithms into transformer weights, creating interpretable subspaces in the model.

Result: Successfully injected 3 algorithms, showing interpretable subspaces and improved out-of-distribution performance.

Conclusion: tracr-injection bridges theory and practice, enhancing symbolic mechanisms in language models.

Abstract: Motivated by the surge of large language models, there has been a push to
formally characterize the symbolic abilities intrinsic to the transformer
architecture. A programming language, called RASP, has been proposed, which can
be directly compiled into transformer weights to implement these algorithms.
However, the tasks that can be implemented in RASP are often uncommon to learn
from natural unsupervised data, showing a mismatch between theoretical
capabilities of the transformer architecture, and the practical learnability of
these capabilities from unsupervised data. We propose tracr-injection, a method
that allows us to distill algorithms written in RASP directly into a
pre-trained language model. We showcase our method by injecting 3 different
algorithms into a language model. We show how our method creates an
interpretable subspace within the model's residual stream, which can be decoded
into the variables present in the code of the RASP algorithm. Additionally, we
found that the proposed method can improve out of distribution performance
compared to our baseline, indicating that indeed a more symbolic mechanism is
taking place in the inner workings of the model. We release the code used to
run our experiments.

</details>


### [8] [LegoSLM: Connecting LLM with Speech Encoder using CTC Posteriors](https://arxiv.org/pdf/2505.11352)
*Rao Ma, Tongzhou Chen, Kartik Audhkhasi, Bhuvana Ramabhadran*

Main category: cs.CL

TL;DR: LegoSLM bridges speech encoders and LLMs using ASR posterior matrices, improving performance on ASR and speech translation tasks with modularity and domain adaptation control.


<details>
  <summary>Details</summary>
Motivation: To address suboptimal performance and inflexibility in combining speech encoders and LLMs for spoken language processing tasks.

Method: Uses CTC posteriors over LLM vocabulary to reconstruct pseudo-audio embeddings, concatenated with text embeddings in LLM input space.

Result: Achieves 49% WERR improvement over baseline on ASR tasks and shows modularity and domain adaptation effectiveness.

Conclusion: LegoSLM effectively combines speech encoders and LLMs, offering performance gains and flexibility in various settings.

Abstract: Recently, large-scale pre-trained speech encoders and Large Language Models
(LLMs) have been released, which show state-of-the-art performance on a range
of spoken language processing tasks including Automatic Speech Recognition
(ASR). To effectively combine both models for better performance, continuous
speech prompts, and ASR error correction have been adopted. However, these
methods are prone to suboptimal performance or are inflexible. In this paper,
we propose a new paradigm, LegoSLM, that bridges speech encoders and LLMs using
the ASR posterior matrices. The speech encoder is trained to generate
Connectionist Temporal Classification (CTC) posteriors over the LLM vocabulary,
which are used to reconstruct pseudo-audio embeddings by computing a weighted
sum of the LLM input embeddings. These embeddings are concatenated with text
embeddings in the LLM input space. Using the well-performing USM and Gemma
models as an example, we demonstrate that our proposed LegoSLM method yields
good performance on both ASR and speech translation tasks. By connecting USM
with Gemma models, we can get an average of 49% WERR over the USM-CTC baseline
on 8 MLS testsets. The trained model also exhibits modularity in a range of
settings -- after fine-tuning the Gemma model weights, the speech encoder can
be switched and combined with the LLM in a zero-shot fashion. Additionally, we
propose to control the decode-time influence of the USM and LLM using a softmax
temperature, which shows effectiveness in domain adaptation.

</details>


### [9] [Model Performance-Guided Evaluation Data Selection for Effective Prompt Optimization](https://arxiv.org/pdf/2505.10736)
*Ximing Dong, Shaowei Wang, Dayi Lin, Ahmed E. Hassan*

Main category: cs.CL

TL;DR: IPOMP is a two-stage method for automated prompt optimization, using semantic clustering and iterative refinement with real-time performance data, outperforming baselines in effectiveness and stability.


<details>
  <summary>Details</summary>
Motivation: Manual prompt engineering is inefficient, and existing automated methods rely on unreliable evaluation subsets, leading to suboptimal prompts.

Method: IPOMP uses semantic clustering and boundary analysis to select diverse samples, then iteratively refines them using real-time model performance data.

Result: IPOMP improves effectiveness by 1.6% to 5.3% and stability by 57% over baselines, with minimal computational overhead (<1%).

Conclusion: IPOMP's real-time performance-guided refinement is universally applicable and enhances existing coreset selection methods.

Abstract: Optimizing Large Language Model (LLM) performance requires well-crafted
prompts, but manual prompt engineering is labor-intensive and often
ineffective. Automated prompt optimization techniques address this challenge
but the majority of them rely on randomly selected evaluation subsets, which
fail to represent the full dataset, leading to unreliable evaluations and
suboptimal prompts. Existing coreset selection methods, designed for LLM
benchmarking, are unsuitable for prompt optimization due to challenges in
clustering similar samples, high data collection costs, and the unavailability
of performance data for new or private datasets. To overcome these issues, we
propose IPOMP, an Iterative evaluation data selection for effective Prompt
Optimization using real-time Model Performance. IPOMP is a two-stage approach
that selects representative and diverse samples using semantic clustering and
boundary analysis, followed by iterative refinement with real-time model
performance data to replace redundant samples. Evaluations on the BIG-bench
dataset show that IPOMP improves effectiveness by 1.6% to 5.3% and stability by
at least 57% compared with SOTA baselines, with minimal computational overhead
below 1%. Furthermore, the results demonstrate that our real-time
performance-guided refinement approach can be universally applied to enhance
existing coreset selection methods.

</details>


### [10] [SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval](https://arxiv.org/pdf/2505.10740)
*Qiwei Peng, Robert Moro, Michal Gregor, Ivan Srba, Simon Ostermann, Marian Simko, Juraj Podrou≈æek, Mat√∫≈° Mesarƒç√≠k, Jaroslav Kopƒçan, Anders S√∏gaard*

Main category: cs.CL

TL;DR: The paper discusses a shared task on multilingual claim retrieval at SemEval 2025, addressing the gap in handling disinformation in multilingual and low-resource settings. It reports top-performing systems and common approaches.


<details>
  <summary>Details</summary>
Motivation: To tackle the neglect of multilingual and low-resource languages in disinformation detection, the study aims to improve claim retrieval across languages.

Method: A shared task with two subtracks (monolingual and crosslingual) was conducted, involving 179 participants and 52 test submissions.

Result: 23 teams submitted system papers, with insights into the best-performing systems and effective approaches for multilingual claim retrieval.

Conclusion: The shared task and its outcomes provide valuable resources and insights for future research in automated fact-checking and multilingual disinformation detection.

Abstract: The rapid spread of online disinformation presents a global challenge, and
machine learning has been widely explored as a potential solution. However,
multilingual settings and low-resource languages are often neglected in this
field. To address this gap, we conducted a shared task on multilingual claim
retrieval at SemEval 2025, aimed at identifying fact-checked claims that match
newly encountered claims expressed in social media posts across different
languages. The task includes two subtracks: (1) a monolingual track, where
social posts and claims are in the same language, and (2) a crosslingual track,
where social posts and claims might be in different languages. A total of 179
participants registered for the task contributing to 52 test submissions. 23
out of 31 teams have submitted their system papers. In this paper, we report
the best-performing systems as well as the most common and the most effective
approaches across both subtracks. This shared task, along with its dataset and
participating systems, provides valuable insights into multilingual claim
retrieval and automated fact-checking, supporting future research in this
field.

</details>


### [11] [On the Role of Speech Data in Reducing Toxicity Detection Bias](https://arxiv.org/pdf/2411.08135)
*Samuel J. Bell, Mariano Coria Meglioli, Megan Richards, Eduardo S√°nchez, Christophe Ropers, Skyler Wang, Adina Williams, Levent Sagun, Marta R. Costa-juss√†*

Main category: cs.CL

TL;DR: Speech-based toxicity detection reduces bias compared to text-based systems, especially for ambiguous cases, and improving classifiers is more effective than transcription pipelines.


<details>
  <summary>Details</summary>
Motivation: To investigate if speech-based toxicity detection mitigates biases found in text-based systems.

Method: Produced high-quality group annotations for the MuTox dataset and compared speech- and text-based toxicity classifiers.

Result: Speech data reduces bias, particularly for ambiguous samples, and classifier improvements are more impactful than transcription pipelines.

Conclusion: Speech-based systems help mitigate bias; annotations and recommendations are released for future dataset construction.

Abstract: Text toxicity detection systems exhibit significant biases, producing
disproportionate rates of false positives on samples mentioning demographic
groups. But what about toxicity detection in speech? To investigate the extent
to which text-based biases are mitigated by speech-based systems, we produce a
set of high-quality group annotations for the multilingual MuTox dataset, and
then leverage these annotations to systematically compare speech- and
text-based toxicity classifiers. Our findings indicate that access to speech
data during inference supports reduced bias against group mentions,
particularly for ambiguous and disagreement-inducing samples. Our results also
suggest that improving classifiers, rather than transcription pipelines, is
more helpful for reducing group bias. We publicly release our annotations and
provide recommendations for future toxicity dataset construction.

</details>


### [12] [Ranked Voting based Self-Consistency of Large Language Models](https://arxiv.org/pdf/2505.10772)
*Weiqin Wang, Yile Wang, Hui Huang*

Main category: cs.CL

TL;DR: The paper introduces ranked voting methods (Instant-runoff, Borda count, mean reciprocal rank) to improve chain-of-thought reasoning by considering multiple ranked answers, outperforming single-answer baselines.


<details>
  <summary>Details</summary>
Motivation: Existing chain-of-thought methods generate only one answer per trial, missing potential alternatives. Ranked voting leverages multiple answers for more reliable self-consistency.

Method: Proposes generating ranked answers and using three ranked voting methods (Instant-runoff, Borda count, mean reciprocal rank) to aggregate answers from multiple reasoning paths.

Result: Validated on six datasets, the method outperforms baselines, demonstrating improved reasoning performance with ranked voting.

Conclusion: Ranked voting enhances reasoning reliability by utilizing multiple answers, showing promise for future applications.

Abstract: Majority voting is considered an effective method to enhance chain-of-thought
reasoning, as it selects the answer with the highest "self-consistency" among
different reasoning paths (Wang et al., 2023). However, previous
chain-of-thought reasoning methods typically generate only a single answer in
each trial, thereby ignoring the possibility of other potential answers. As a
result, these alternative answers are often overlooked in subsequent voting
processes. In this work, we propose to generate ranked answers in each
reasoning process and conduct ranked voting among multiple ranked answers from
different responses, thereby making the overall self-consistency more reliable.
Specifically, we use three ranked voting methods: Instant-runoff voting, Borda
count voting, and mean reciprocal rank voting. We validate our methods on six
datasets, including three multiple-choice and three open-ended
question-answering tasks, using both advanced open-source and closed-source
large language models. Extensive experimental results indicate that our
proposed method outperforms the baselines, showcasing the potential of
leveraging the information of ranked answers and using ranked voting to improve
reasoning performance. The code is available at
https://github.com/szu-tera/RankedVotingSC.

</details>


### [13] [A Systematic Analysis of Base Model Choice for Reward Modeling](https://arxiv.org/pdf/2505.10775)
*Kian Ahrabian, Pegah Jandaghi, Negar Mokhberian, Sai Praneeth Karimireddy, Jay Pujara*

Main category: cs.CL

TL;DR: The paper analyzes how base model selection impacts reward modeling in RLHF, showing up to 14% performance improvement over default choices and highlighting the role of benchmarks and post-training steps.


<details>
  <summary>Details</summary>
Motivation: The choice of base models for reward modeling in RLHF is often overlooked, yet critical, given the growing variety of LLMs. This work aims to systematically study its impact.

Method: The study evaluates base model selection's effect on reward modeling performance, uses benchmarks to predict downstream performance, and explores post-training steps and data distributions.

Result: Performance improves by up to 14% with better base model choices. Benchmarks strongly correlate with downstream results, and combining them boosts selection accuracy (+18% in top 5-10). Post-training steps and data distributions also impact performance.

Conclusion: Base model selection significantly affects reward modeling. Leveraging benchmarks and optimizing post-training can enhance performance, offering practical insights for RLHF in LLMs.

Abstract: Reinforcement learning from human feedback (RLHF) and, at its core, reward
modeling have become a crucial part of training powerful large language models
(LLMs). One commonly overlooked factor in training high-quality reward models
(RMs) is the effect of the base model, which is becoming more challenging to
choose given the rapidly growing pool of LLMs. In this work, we present a
systematic analysis of the effect of base model selection on reward modeling
performance. Our results show that the performance can be improved by up to 14%
compared to the most common (i.e., default) choice. Moreover, we showcase the
strong statistical relation between some existing benchmarks and downstream
performances. We also demonstrate that the results from a small set of
benchmarks could be combined to boost the model selection ($+$18% on average in
the top 5-10). Lastly, we illustrate the impact of different post-training
steps on the final performance and explore using estimated data distributions
to reduce performance prediction error.

</details>


### [14] [Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.10792)
*Zhan Peng Lee, Andre Lin, Calvin Tan*

Main category: cs.CL

TL;DR: Finetune-RAG improves factual accuracy in LLMs by fine-tuning with a dataset mimicking real-world retrieval imperfections, achieving a 21.2% boost. Bench-RAG evaluates models under imperfect retrieval scenarios.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of irrelevant retrieved content causing hallucinations in LLMs, aiming to enhance factual accuracy.

Method: Proposes Finetune-RAG, a fine-tuning approach using a dataset simulating real-world retrieval imperfections, and Bench-RAG for evaluation.

Result: Finetune-RAG improves factual accuracy by 21.2% over the base model.

Conclusion: The approach effectively mitigates hallucinations and improves factual grounding, with open-sourced resources for community use.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to
improve factuality in large language models (LLMs) by grounding their outputs
in retrieved documents. However, ensuring perfect retrieval of relevant
information remains challenging, and when irrelevant content is passed
downstream to an LLM, it can lead to hallucinations. In this work, we propose
Finetune-RAG, a simple and effective fine-tuning approach that features the
first-of-its-kind RAG training dataset constructed to mimic real-world
imperfections. Experimental results show that Finetune-RAG improves factual
accuracy by 21.2% over the base model. We also propose a Bench-RAG, an
LLM-as-a-judge evaluation pipeline that stress tests models under realistic
imperfect retrieval scenarios. Our codebase and dataset are fully open sourced
for community use.

</details>


### [15] [Relation Extraction Across Entire Books to Reconstruct Community Networks: The AffilKG Datasets](https://arxiv.org/pdf/2505.10798)
*Erica Cai, Sean McQuade, Kevin Young, Brendan O'Connor*

Main category: cs.CL

TL;DR: AffilKG introduces six datasets pairing book scans with labeled KGs to evaluate KG extraction accuracy, addressing gaps in current datasets.


<details>
  <summary>Details</summary>
Motivation: Current annotated datasets are inadequate for evaluating KG extraction accuracy due to disconnection, small size, or complexity.

Method: AffilKG provides six datasets with affiliation graphs (simple KGs) and expanded KGs, pairing book scans with labeled KGs.

Result: Preliminary experiments show significant model performance variability, highlighting AffilKG's utility for benchmarking and validation.

Conclusion: AffilKG enables benchmarking of extraction errors and validates KG methods for social science research.

Abstract: When knowledge graphs (KGs) are automatically extracted from text, are they
accurate enough for downstream analysis? Unfortunately, current annotated
datasets can not be used to evaluate this question, since their KGs are highly
disconnected, too small, or overly complex. To address this gap, we introduce
AffilKG (https://doi.org/10.5281/zenodo.15427977), which is a collection of six
datasets that are the first to pair complete book scans with large, labeled
knowledge graphs. Each dataset features affiliation graphs, which are simple
KGs that capture Member relationships between Person and Organization entities
-- useful in studies of migration, community interactions, and other social
phenomena. In addition, three datasets include expanded KGs with a wider
variety of relation types. Our preliminary experiments demonstrate significant
variability in model performance across datasets, underscoring AffilKG's
ability to enable two critical advances: (1) benchmarking how extraction errors
propagate to graph-level analyses (e.g., community structure), and (2)
validating KG extraction methods for real-world social science research.

</details>


### [16] [Enhancing Low-Resource Minority Language Translation with LLMs and Retrieval-Augmented Generation for Cultural Nuances](https://arxiv.org/pdf/2505.10829)
*Chen-Chi Chang, Chong-Fu Li, Chu-Hsuan Lee, Hung-Shin Lee*

Main category: cs.CL

TL;DR: Integration of LLMs with RAG improves Hakka translation, achieving BLEU scores up to 31%, with iterative methods and domain knowledge enhancing accuracy and cultural nuance.


<details>
  <summary>Details</summary>
Motivation: To address challenges in translating low-resource languages like Hakka by leveraging advanced language models and retrieval methods.

Method: Tested various configurations, including dictionary-only, RAG with Gemini 2.0, and a two-stage method combining dictionary outputs with Gemini 2.0 refinement.

Result: Best model (Model 4) achieved 31% BLEU, improving lexical coverage and coherence. Iterative methods (Model 3) scored 26%, highlighting domain-specific challenges.

Conclusion: Curated resources, domain knowledge, and ethical collaboration are crucial for accurate, fluent translations and cultural preservation.

Abstract: This study investigates the challenges of translating low-resource languages
by integrating Large Language Models (LLMs) with Retrieval-Augmented Generation
(RAG). Various model configurations were tested on Hakka translations, with
BLEU scores ranging from 12% (dictionary-only) to 31% (RAG with Gemini 2.0).
The best-performing model (Model 4) combined retrieval and advanced language
modeling, improving lexical coverage, particularly for specialized or
culturally nuanced terms, and enhancing grammatical coherence. A two-stage
method (Model 3) using dictionary outputs refined by Gemini 2.0 achieved a BLEU
score of 26%, highlighting iterative correction's value and the challenges of
domain-specific expressions. Static dictionary-based approaches struggled with
context-sensitive content, demonstrating the limitations of relying solely on
predefined resources. These results emphasize the need for curated resources,
domain knowledge, and ethical collaboration with local communities, offering a
framework that improves translation accuracy and fluency while supporting
cultural preservation.

</details>


### [17] [Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL](https://arxiv.org/pdf/2505.10832)
*Songjun Tu, Jiahao Lin, Qichao Zhang, Xiangyu Tian, Linjing Li, Xiangyuan Lan, Dongbin Zhao*

Main category: cs.CL

TL;DR: AutoThink enables large reasoning models to dynamically decide when to use explicit reasoning, improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: To reduce computational overhead and latency in large reasoning models by avoiding unnecessary detailed reasoning for simple problems.

Method: Uses a multi-stage reinforcement learning framework (AutoThink) to optimize reasoning policies, triggered by inserting an ellipsis in prompts.

Result: Achieves better accuracy-efficiency trade-offs, improving accuracy by 6.4% and reducing token usage by 52%.

Conclusion: AutoThink provides a scalable and adaptive reasoning paradigm for large reasoning models.

Abstract: Large reasoning models (LRMs) are proficient at generating explicit,
step-by-step reasoning sequences before producing final answers. However, such
detailed reasoning can introduce substantial computational overhead and
latency, particularly for simple problems. To address this over-thinking
problem, we explore how to equip LRMs with adaptive thinking capabilities:
enabling them to dynamically decide whether or not to engage in explicit
reasoning based on problem complexity. Building on R1-style distilled models,
we observe that inserting a simple ellipsis ("...") into the prompt can
stochastically trigger either a thinking or no-thinking mode, revealing a
latent controllability in the reasoning behavior. Leveraging this property, we
propose AutoThink, a multi-stage reinforcement learning (RL) framework that
progressively optimizes reasoning policies via stage-wise reward shaping.
AutoThink learns to invoke explicit reasoning only when necessary, while
defaulting to succinct responses for simpler tasks. Experiments on five
mainstream mathematical benchmarks demonstrate that AutoThink achieves
favorable accuracy-efficiency trade-offs compared to recent prompting and
RL-based pruning methods. It can be seamlessly integrated into any R1-style
model, including both distilled and further fine-tuned variants. Notably,
AutoThink improves relative accuracy by 6.4 percent while reducing token usage
by 52 percent on DeepSeek-R1-Distill-Qwen-1.5B, establishing a scalable and
adaptive reasoning paradigm for LRMs.

</details>


### [18] [Multimodal Event Detection: Current Approaches and Defining the New Playground through LLMs and VLMs](https://arxiv.org/pdf/2505.10836)
*Abhishek Dey, Aabha Bothera, Samhita Sarikonda, Rishav Aryan, Sanjay Kumar Podishetty, Akshay Havalgi, Gaurav Singh, Saurabh Srivastava*

Main category: cs.CL

TL;DR: The paper explores multimodal and generative models for social media event detection, finding multimodal methods superior but generative models lagging in precision despite handling some challenges better.


<details>
  <summary>Details</summary>
Motivation: Traditional unimodal systems struggle with rapid, multimodal social media data, prompting the need for advanced models.

Method: Employed unimodal (ModernBERT, ConvNeXt-V2), multimodal fusion, and generative models (GPT-4o, LLaVA), testing generative models with single modality.

Result: Multimodal approaches outperform unimodal ones; generative models lag in precision and event class generation but handle social media issues like leet speak better.

Conclusion: Multimodal methods are superior for event detection, but generative models need improvement for precision and class accuracy.

Abstract: In this paper, we study the challenges of detecting events on social media,
where traditional unimodal systems struggle due to the rapid and multimodal
nature of data dissemination. We employ a range of models, including unimodal
ModernBERT and ConvNeXt-V2, multimodal fusion techniques, and advanced
generative models like GPT-4o, and LLaVA. Additionally, we also study the
effect of providing multimodal generative models (such as GPT-4o) with a single
modality to assess their efficacy. Our results indicate that while multimodal
approaches notably outperform unimodal counterparts, generative approaches
despite having a large number of parameters, lag behind supervised methods in
precision. Furthermore, we also found that they lag behind instruction-tuned
models because of their inability to generate event classes correctly. During
our error analysis, we discovered that common social media issues such as leet
speak, text elongation, etc. are effectively handled by generative approaches
but are hard to tackle using supervised approaches.

</details>


### [19] [Have Multimodal Large Language Models (MLLMs) Really Learned to Tell the Time on Analog Clocks?](https://arxiv.org/pdf/2505.10862)
*Tairan Fu, Miguel Gonz√°lez, Javier Conde, Elena Merino-G√≥mez, Pedro Reviriego*

Main category: cs.CL

TL;DR: MLLMs struggle with telling time on analog clocks due to limited training data. This study tests GPT-4.1 to understand the issue and evaluate fine-tuning's effectiveness.


<details>
  <summary>Details</summary>
Motivation: To investigate why MLLMs fail at telling time on analog clocks and assess if fine-tuning can improve performance.

Method: Testing GPT-4.1 with various analog clocks to analyze its ability to generalize and abstract time-telling.

Result: Models show progress but may rely on training data patterns rather than true understanding.

Conclusion: MLLMs still struggle with abstracting and generalizing time-telling, highlighting limitations in their training.

Abstract: Multimodal Large Language Models which can answer complex questions on an
image struggle to tell the time on analog clocks. This is probably due to the
lack of images with clocks at different times in their training set. In this
work we explore this issue with one of the latest MLLMs: GPT-4.1 to understand
why MLLMs fail to tell the time and whether fine-tuning can solve the problem.
The results show how models are making progress in reading the time on analog
clocks. But have they really learned to do it, or have they only learned
patterns in their training datasets? In this work we put the models to the test
with different clocks to illustrate the limitations of MLLMs to abstract and
generalize.

</details>


### [20] [Improve Rule Retrieval and Reasoning with Self-Induction and Relevance ReEstimate](https://arxiv.org/pdf/2505.10870)
*Ziyang Huang, Wangtao Sun, Jun Zhao, Kang Liu*

Main category: cs.CL

TL;DR: The paper introduces SIAR and R¬≥ to improve rule retrieval by addressing semantic gaps and enhancing relevance estimation.


<details>
  <summary>Details</summary>
Motivation: Existing rule retrieval methods suffer from low accuracy due to semantic mismatches between queries and abstract rule representations, impacting reasoning performance.

Method: Proposes SIAR for rule induction via LLMs and R¬≥ for relevance re-estimation by aligning abstract rules with query facts.

Result: Experiments show improved retrieval effectiveness and reasoning performance across various settings.

Conclusion: SIAR and R¬≥ effectively address retrieval challenges, enhancing downstream reasoning tasks.

Abstract: This paper systematically addresses the challenges of rule retrieval, a
crucial yet underexplored area. Vanilla retrieval methods using sparse or dense
retrievers to directly search for relevant rules to support downstream
reasoning, often suffer from low accuracy. This is primarily due to a
significant semantic gap between the instantiated facts in the queries and the
abstract representations of the rules. Such misalignment results in suboptimal
retrieval quality, which in turn negatively impacts reasoning performance. To
overcome these challenges, we propose Self-Induction Augmented Retrieval
(SIAR), a novel approach that utilizes Large Language Models (LLMs) to induce
potential inferential rules that might offer benefits for reasoning by
abstracting the underlying knowledge and logical structure in queries. These
induced rules are then used for query augmentation to improve retrieval
effectiveness. Additionally, we introduce Rule Relevance ReEstimate (R$^3$), a
method that re-estimates the relevance of retrieved rules by assessing whether
the abstract knowledge they contain can be instantiated to align with the facts
in the queries and the helpfulness for reasoning. Extensive experiments across
various settings demonstrate the effectiveness and versatility of our proposed
methods.

</details>


### [21] [A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?](https://arxiv.org/pdf/2505.10924)
*Ada Chen, Yongjiang Wu, Junyuan Zhang, Shu Yang, Jen-tse Huang, Kun Wang, Wenxuan Wang, Shuai Wang*

Main category: cs.CL

TL;DR: The paper systematizes safety and security risks of AI-driven Computer-Using Agents (CUAs), categorizes threats, proposes defensive strategies, and summarizes benchmarks for secure deployment.


<details>
  <summary>Details</summary>
Motivation: The rise of CUAs introduces novel safety and security risks due to vulnerabilities in LLM-driven reasoning and complex software integration, necessitating a structured analysis.

Method: A comprehensive literature review is conducted, focusing on defining CUAs, categorizing threats, proposing defensive taxonomies, and summarizing evaluation metrics.

Result: The study provides a taxonomy of threats and defenses, along with benchmarks, aiding researchers and practitioners in addressing CUA vulnerabilities.

Conclusion: The work offers a foundation for future research on unexplored vulnerabilities and practical guidance for secure CUA deployment.

Abstract: Recently, AI-driven interactions with computing devices have advanced from
basic prototype tools to sophisticated, LLM-based systems that emulate
human-like operations in graphical user interfaces. We are now witnessing the
emergence of \emph{Computer-Using Agents} (CUAs), capable of autonomously
performing tasks such as navigating desktop applications, web pages, and mobile
apps. However, as these agents grow in capability, they also introduce novel
safety and security risks. Vulnerabilities in LLM-driven reasoning, with the
added complexity of integrating multiple software components and multimodal
inputs, further complicate the security landscape. In this paper, we present a
systematization of knowledge on the safety and security threats of CUAs. We
conduct a comprehensive literature review and distill our findings along four
research objectives: \textit{\textbf{(i)}} define the CUA that suits safety
analysis; \textit{\textbf{(ii)} } categorize current safety threats among CUAs;
\textit{\textbf{(iii)}} propose a comprehensive taxonomy of existing defensive
strategies; \textit{\textbf{(iv)}} summarize prevailing benchmarks, datasets,
and evaluation metrics used to assess the safety and performance of CUAs.
Building on these insights, our work provides future researchers with a
structured foundation for exploring unexplored vulnerabilities and offers
practitioners actionable guidance in designing and deploying secure
Computer-Using Agents.

</details>


### [22] [Connecting the Dots: A Chain-of-Collaboration Prompting Framework for LLM Agents](https://arxiv.org/pdf/2505.10936)
*Jiaxing Zhao, Hongbin Xie, Yuzhen Lei, Xuan Song, Zhuoran Shi, Lianxin Li, Shuangxue Liu, Haoran Zhang*

Main category: cs.CL

TL;DR: Cochain is a collaboration prompting framework combining knowledge and prompts to solve business workflow challenges, outperforming baselines and even GPT-4 when paired with a small model.


<details>
  <summary>Details</summary>
Motivation: Address limitations of single-agent chain-of-thought (collaboration challenges) and multi-agent systems (high token cost and problem dilution) in business workflows.

Method: Proposes Cochain, integrating a knowledge graph and a prompts tree to efficiently retrieve and combine relevant prompt information across workflow stages.

Result: Outperforms baselines in prompt engineering and multi-agent LLMs; expert evaluation shows it surpasses GPT-4 when used with a small model.

Conclusion: Cochain effectively addresses collaboration and cost issues in business workflows, demonstrating superior performance over existing approaches.

Abstract: Large Language Models (LLMs) have demonstrated impressive performance in
executing complex reasoning tasks. Chain-of-thought effectively enhances
reasoning capabilities by unlocking the potential of large models, while
multi-agent systems provide more comprehensive solutions by integrating
collective intelligence of multiple agents. However, both approaches face
significant limitations. Single-agent with chain-of-thought, due to the
inherent complexity of designing cross-domain prompts, faces collaboration
challenges. Meanwhile, multi-agent systems consume substantial tokens and
inevitably dilute the primary problem, which is particularly problematic in
business workflow tasks. To address these challenges, we propose Cochain, a
collaboration prompting framework that effectively solves business workflow
collaboration problem by combining knowledge and prompts at a reduced cost.
Specifically, we construct an integrated knowledge graph that incorporates
knowledge from multiple stages. Furthermore, by maintaining and retrieving a
prompts tree, we can obtain prompt information relevant to other stages of the
business workflow. We perform extensive evaluations of Cochain across multiple
datasets, demonstrating that Cochain outperforms all baselines in both prompt
engineering and multi-agent LLMs. Additionally, expert evaluation results
indicate that the use of a small model in combination with Cochain outperforms
GPT-4.

</details>


### [23] [Reasoning with OmniThought: A Large CoT Dataset with Verbosity and Cognitive Difficulty Annotations](https://arxiv.org/pdf/2505.10937)
*Wenrui Cai, Chengyu Wang, Junbing Yan, Jun Huang, Xiangzhong Fang*

Main category: cs.CL

TL;DR: OmniThought is a large-scale dataset with 2 million CoT processes, annotated with RV and CD scores, improving LRM training and reasoning abilities.


<details>
  <summary>Details</summary>
Motivation: Current CoT datasets lack comprehensiveness and fail to describe internal CoT properties, hindering LRM advancement.

Method: Introduce OmniThought, a dataset with RV and CD scores, and a self-reliant curation pipeline. Validate impact using Qwen2.5 models.

Result: OmniThought enhances LRM training, leading to high-performing models with improved reasoning and optimal CoT output.

Conclusion: OmniThought significantly advances LRM development for complex tasks.

Abstract: The emergence of large reasoning models (LRMs) has transformed Natural
Language Processing by excelling in complex tasks such as mathematical
problem-solving and code generation. These models leverage chain-of-thought
(CoT) processes, enabling them to emulate human-like reasoning strategies.
However, the advancement of LRMs is hindered by the lack of comprehensive CoT
datasets. Current resources often fail to provide extensive reasoning problems
with coherent CoT processes distilled from multiple teacher models and do not
account for multifaceted properties describing the internal characteristics of
CoTs. To address these challenges, we introduce OmniThought, a large-scale
dataset featuring 2 million CoT processes generated and validated by two
powerful LRMs as teacher models. Each CoT process in OmniThought is annotated
with novel Reasoning Verbosity (RV) and Cognitive Difficulty (CD) scores, which
describe the appropriateness of CoT verbosity and cognitive difficulty level
for models to comprehend these reasoning processes. We further establish a
self-reliant pipeline to curate this dataset. Extensive experiments using
Qwen2.5 models of various sizes demonstrate the positive impact of our proposed
scores on LRM training effectiveness. Based on the proposed OmniThought
dataset, we further train and release a series of high-performing LRMs,
specifically equipped with stronger reasoning abilities and optimal CoT output
length and difficulty level. Our contributions significantly enhance the
development and training of LRMs for solving complex tasks.

</details>


### [24] [Accurate KV Cache Quantization with Outlier Tokens Tracing](https://arxiv.org/pdf/2505.10938)
*Yi Su, Yuechi Zhou, Quantong Qiu, Juntao Li, Qingrong Xia, Ping Li, Xinyu Duan, Zhefeng Wang, Min Zhang*

Main category: cs.CL

TL;DR: A method to improve KV Cache quantization by identifying and excluding outlier tokens, enhancing accuracy while reducing memory usage and increasing throughput.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) require substantial computational resources, and KV Cache quantization balances memory and accuracy but is affected by unusual tokens.

Method: Identify and exclude outlier tokens during decoding to improve quantization accuracy.

Result: Achieves significant accuracy improvements under 2-bit quantization, reducing memory usage by 6.4x and increasing throughput by 2.3x.

Conclusion: The proposed method effectively addresses quantization challenges, improving efficiency and performance in LLM deployment.

Abstract: The impressive capabilities of Large Language Models (LLMs) come at the cost
of substantial computational resources during deployment. While KV Cache can
significantly reduce recomputation during inference, it also introduces
additional memory overhead. KV Cache quantization presents a promising
solution, striking a good balance between memory usage and accuracy. Previous
research has shown that the Keys are distributed by channel, while the Values
are distributed by token. Consequently, the common practice is to apply
channel-wise quantization to the Keys and token-wise quantization to the
Values. However, our further investigation reveals that a small subset of
unusual tokens exhibit unique characteristics that deviate from this pattern,
which can substantially impact quantization accuracy. To address this, we
develop a simple yet effective method to identify these tokens accurately
during the decoding process and exclude them from quantization as outlier
tokens, significantly improving overall accuracy. Extensive experiments show
that our method achieves significant accuracy improvements under 2-bit
quantization and can deliver a 6.4 times reduction in memory usage and a 2.3
times increase in throughput.

</details>


### [25] [GenKnowSub: Improving Modularity and Reusability of LLMs through General Knowledge Subtraction](https://arxiv.org/pdf/2505.10939)
*Mohammadtaha Bagherifard, Sahar Rajabi, Ali Edalat, Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: The paper proposes GenKnowSub, a modular framework to disentangle general knowledge and task-specific adaptations in LLMs, improving zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: Addressing the entanglement of general knowledge and task-specific adaptations in LLMs to enhance zero-shot performance.

Method: Uses task-specific and general-domain LoRA modules, subtracts general knowledge to create residual modules, and dynamically combines them using the Arrow routing algorithm.

Result: Shows consistent performance gains in monolingual and cross-lingual settings, validated on Phi-3 and Phi-2 models.

Conclusion: GenKnowSub effectively improves LLM generalization without additional training, with code and data publicly available.

Abstract: Large language models often struggle with zero-shot generalization, and
several modular approaches have been proposed to address this challenge. Yet,
we hypothesize that a key limitation remains: the entanglement of general
knowledge and task-specific adaptations. To overcome this, we propose a modular
framework that disentangles these components by constructing a library of
task-specific LoRA modules alongside a general-domain LoRA. By subtracting this
general knowledge component from each task-specific module, we obtain residual
modules that focus more exclusively on task-relevant information, a method we
call general knowledge subtraction (GenKnowSub). Leveraging the refined
task-specific modules and the Arrow routing algorithm
\citep{ostapenko2024towards}, we dynamically select and combine modules for new
inputs without additional training. Our studies on the Phi-3 model and standard
Arrow as baselines reveal that using general knowledge LoRAs derived from
diverse languages, including English, French, and German, yields consistent
performance gains in both monolingual and cross-lingual settings across a wide
set of benchmarks. Further experiments on Phi-2 demonstrate how GenKnowSub
generalizes to weaker LLMs. The complete code and data are available at
https://github.com/saharsamr/Modular-LLM.

</details>


### [26] [Semantic Aware Linear Transfer by Recycling Pre-trained Language Models for Cross-lingual Transfer](https://arxiv.org/pdf/2505.10945)
*Seungyoon Lee, Seongtae Hong, Hyeonseok Moon, Heuiseok Lim*

Main category: cs.CL

TL;DR: SALT, a cross-lingual transfer technique, recycles target language PLM embeddings to enhance LLMs, outperforming other methods in performance and convergence.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitation of current methods that blend source model embeddings, which may constrain expressive capacity in target languages due to English-centric training.

Method: Proposes Semantic Aware Linear Transfer (SALT), which uses regression lines based on vocabulary overlap similarity to handle non-overlapping token embeddings.

Result: SALT significantly outperforms other transfer methods, achieving lower loss and faster convergence, with notable performance in cross-lingual understanding.

Conclusion: SALT effectively enhances LLMs by leveraging PLMs, demonstrating scalability and superior performance in cross-lingual transfer.

Abstract: Large Language Models (LLMs) increasingly incorporate multilingual
capabilities, fueling the demand to transfer them into target language-specific
models. However, most approaches, which blend the source model's embedding by
replacing the source vocabulary with the target language-specific vocabulary,
may constrain expressive capacity in the target language since the source model
is predominantly trained on English data. In this paper, we propose Semantic
Aware Linear Transfer (SALT), a novel cross-lingual transfer technique that
recycles embeddings from target language Pre-trained Language Models (PLMs) to
transmit the deep representational strengths of PLM-derived embedding to LLMs.
SALT derives unique regression lines based on the similarity in the overlap of
the source and target vocabularies, to handle each non-overlapping token's
embedding space. Our extensive experiments show that SALT significantly
outperforms other transfer methods and achieves lower loss with accelerating
faster convergence during language adaptation. Notably, SALT obtains remarkable
performance in cross-lingual understanding setups compared to other methods.
Furthermore, we highlight the scalable use of PLMs to enhance the functionality
of contemporary LLMs by conducting experiments with varying architectures.

</details>


### [27] [The Way We Prompt: Conceptual Blending, Neural Dynamics, and Prompt-Induced Transitions in LLMs](https://arxiv.org/pdf/2505.10948)
*Makoto Sato*

Main category: cs.CL

TL;DR: The paper explores how LLMs blend meaning using Conceptual Blending Theory, revealing parallels between AI and biological cognition through prompt-based methods.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms behind LLMs' personality-like behaviors and bridge gaps between linguistics, neuroscience, and AI.

Method: Uses prompt-based methods (Prompt-Induced Transitions and Hallucinations) to study how LLMs blend and compress meaning.

Result: Uncovers structural similarities and differences between artificial and biological cognition.

Conclusion: Proposes prompt engineering as a scientific method to study meaning, advocating for human-AI collaboration in cognitive science.

Abstract: Large language models (LLMs), inspired by neuroscience, exhibit behaviors
that often evoke a sense of personality and intelligence-yet the mechanisms
behind these effects remain elusive. Here, we operationalize Conceptual
Blending Theory (CBT) as an experimental framework, using prompt-based methods
to reveal how LLMs blend and compress meaning. By systematically investigating
Prompt-Induced Transitions (PIT) and Prompt-Induced Hallucinations (PIH), we
uncover structural parallels and divergences between artificial and biological
cognition. Our approach bridges linguistics, neuroscience, and empirical AI
research, demonstrating that human-AI collaboration can serve as a living
prototype for the future of cognitive science. This work proposes prompt
engineering not just as a technical tool, but as a scientific method for
probing the deep structure of meaning itself.

</details>


### [28] [Illusion or Algorithm? Investigating Memorization, Emergence, and Symbolic Processing in In-Context Learning](https://arxiv.org/pdf/2505.11004)
*Jingcheng Niu, Subhabrata Dutta, Ahmed Elshabrawy, Harish Tayyar Madabushi, Iryna Gurevych*

Main category: cs.CL

TL;DR: The paper investigates in-context learning (ICL) in large-scale Transformer LMs, showing it's not just memorization but also not a fully symbolic algorithm.


<details>
  <summary>Details</summary>
Motivation: To clarify the controversial mechanism behind ICL in LMs, addressing debates on whether it's memorization or algorithmic development.

Method: Uses the Pythia scaling suite with interim checkpoints to analyze ICL performance and mechanistic interpretability of the residual stream.

Result: ICL extends beyond memorization but isn't a standalone symbolic algorithm; insights into training dynamics and model capabilities are provided.

Conclusion: Advances understanding of ICL, offering insights for model improvement and AI security guidelines.

Abstract: Large-scale Transformer language models (LMs) trained solely on next-token
prediction with web-scale data can solve a wide range of tasks after seeing
just a few examples. The mechanism behind this capability, known as in-context
learning (ICL), remains both controversial and poorly understood. Some studies
argue that it is merely the result of memorizing vast amounts of data, while
others contend that it reflects a fundamental, symbolic algorithmic development
in LMs. In this work, we introduce a suite of investigative tasks and a novel
method to systematically investigate ICL by leveraging the full Pythia scaling
suite, including interim checkpoints that capture progressively larger amount
of training data. By carefully exploring ICL performance on downstream tasks
and simultaneously conducting a mechanistic analysis of the residual stream's
subspace, we demonstrate that ICL extends beyond mere "memorization" of the
training corpus, yet does not amount to the implementation of an independent
symbolic algorithm. Our results also clarify several aspects of ICL, including
the influence of training dynamics, model capabilities, and elements of
mechanistic interpretability. Overall, our work advances the understanding of
ICL and its implications, offering model developers insights into potential
improvements and providing AI security practitioners with a basis for more
informed guidelines.

</details>


### [29] [Reconstructing Syllable Sequences in Abugida Scripts with Incomplete Inputs](https://arxiv.org/pdf/2505.11008)
*Ye Kyaw Thu, Thazin Myint Oo*

Main category: cs.CL

TL;DR: Transformer models predict syllable sequences in Abugida languages, showing consonants are key for accuracy, while vowels are harder.


<details>
  <summary>Details</summary>
Motivation: To improve syllable sequence prediction in Abugida languages for applications like text prediction and spelling correction.

Method: Uses Transformer models to reconstruct syllables from incomplete inputs (consonant/vowel sequences, partial/masked syllables) on six ALT languages.

Result: Consonant sequences yield high BLEU scores; vowels are challenging. Model excels in partial/masked syllable tasks.

Conclusion: Advances Abugida language sequence prediction, offering practical insights for text-related applications.

Abstract: This paper explores syllable sequence prediction in Abugida languages using
Transformer-based models, focusing on six languages: Bengali, Hindi, Khmer,
Lao, Myanmar, and Thai, from the Asian Language Treebank (ALT) dataset. We
investigate the reconstruction of complete syllable sequences from various
incomplete input types, including consonant sequences, vowel sequences, partial
syllables (with random character deletions), and masked syllables (with fixed
syllable deletions). Our experiments reveal that consonant sequences play a
critical role in accurate syllable prediction, achieving high BLEU scores,
while vowel sequences present a significantly greater challenge. The model
demonstrates robust performance across tasks, particularly in handling partial
and masked syllable reconstruction, with strong results for tasks involving
consonant information and syllable masking. This study advances the
understanding of sequence prediction for Abugida languages and provides
practical insights for applications such as text prediction, spelling
correction, and data augmentation in these scripts.

</details>


### [30] [Review-Instruct: A Review-Driven Multi-Turn Conversations Generation Method for Large Language Models](https://arxiv.org/pdf/2505.11010)
*Jiangxu Wu, Cong Wang, TianHuang Su, Jun Yang, Haozhi Lin, Chao Zhang, Ming Peng, Kai Shi, SongPan Yang, BinQing Pan, ZiXian Li, Ni Yang, ZhenYu Yang*

Main category: cs.CL

TL;DR: Review-Instruct, a multi-agent framework, improves multi-turn dialogue quality by iteratively refining instructions through an 'Ask-Respond-Review' process, outperforming prior models on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Single-turn SFT data limits LLMs' contextual coherence in multi-turn dialogues, and existing methods lack diversity and quality in instructions.

Method: Proposes Review-Instruct, using three agent roles (Candidate, Reviewers, Chairman) to iteratively refine instructions via feedback, and fine-tunes LLaMA2-13B on a multi-turn dataset.

Result: Achieves absolute gains of 2.9% on MMLU-Pro and 2% on MT-Bench over prior LLaMA2-13B models, with ablation studies confirming the Review stage's importance.

Conclusion: Review-driven, multi-agent frameworks can generate high-quality conversational data at scale, enhancing LLM performance in multi-turn dialogues.

Abstract: The effectiveness of large language models (LLMs) in conversational AI is
hindered by their reliance on single-turn supervised fine-tuning (SFT) data,
which limits contextual coherence in multi-turn dialogues. Existing methods for
generating multi-turn dialogue data struggle to ensure both diversity and
quality in instructions. To address this, we propose Review-Instruct, a novel
framework that synthesizes multi-turn conversations through an iterative
"Ask-Respond-Review" process involving three agent roles: a Candidate, multiple
Reviewers, and a Chairman. The framework iteratively refines instructions by
incorporating Reviewer feedback, enhancing dialogue diversity and difficulty.
We construct a multi-turn dataset using the Alpaca dataset and fine-tune the
LLaMA2-13B model. Evaluations on MT-Bench, MMLU-Pro, and Auto-Arena demonstrate
significant improvements, achieving absolute gains of 2.9\% on MMLU-Pro and 2\%
on MT-Bench compared to prior state-of-the-art models based on LLaMA2-13B.
Ablation studies confirm the critical role of the Review stage and the use of
multiple Reviewers in boosting instruction diversity and difficulty. Our work
highlights the potential of review-driven, multi-agent frameworks for
generating high-quality conversational data at scale.

</details>


### [31] [StRuCom: A Novel Dataset of Structured Code Comments in Russian](https://arxiv.org/pdf/2505.11026)
*Maria Dziuba, Valentin Malykh*

Main category: cs.CL

TL;DR: StRuCom is a new dataset for Russian code documentation, improving model performance over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing models for generating structured code comments perform poorly for Russian compared to English.

Method: StRuCom combines human-written Russian GitHub comments with synthetic ones, validated for compliance with multiple programming languages. Qwen2.5-Coder models (0.5B-7B) are fine-tuned on this dataset.

Result: Statistically significant improvements in chrf++ and BERTScore over baseline models.

Conclusion: StRuCom effectively bridges the gap in Russian code documentation quality.

Abstract: Structured code comments in docstring format are essential for code
comprehension and maintenance, but existing machine learning models for their
generation perform poorly for Russian compared to English. To bridge this gap,
we present StRuCom - the first large-scale dataset (153K examples) specifically
designed for Russian code documentation. Unlike machine-translated English
datasets that distort terminology (e.g., technical loanwords vs. literal
translations) and docstring structures, StRuCom combines human-written comments
from Russian GitHub repositories with synthetically generated ones, ensuring
compliance with Python, Java, JavaScript, C#, and Go standards through
automated validation. Fine-tuning Qwen2.5-Coder models (0.5B-7B) on StRuCom
shows statistically significant improvements of chrf++ and BERTScore over
baseline models.

</details>


### [32] [OntoURL: A Benchmark for Evaluating Large Language Models on Symbolic Ontological Understanding, Reasoning and Learning](https://arxiv.org/pdf/2505.11031)
*Xiao Zhang, Huiyuan Lai, Qianru Meng, Johan Bos*

Main category: cs.CL

TL;DR: The paper introduces OntoURL, a benchmark to evaluate LLMs' ability to handle ontologies, revealing their strengths in understanding but weaknesses in reasoning and learning symbolic knowledge.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' underexplored capability in processing structured symbolic knowledge, specifically ontologies.

Method: Proposes a taxonomy of LLMs' ontological capabilities and develops OntoURL, a benchmark with 15 tasks (58,981 questions) across 8 domains to assess understanding, reasoning, and learning.

Result: Experiments with 20 LLMs show strong performance in understanding but significant weaknesses in reasoning and learning tasks.

Conclusion: OntoURL highlights LLMs' limitations in symbolic knowledge processing and serves as a benchmark for future integration of LLMs with formal knowledge.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
a range of natural language processing tasks, yet their ability to process
structured symbolic knowledge remains underexplored. To address this gap, we
propose a taxonomy of LLMs' ontological capabilities and introduce OntoURL, the
first comprehensive benchmark designed to systematically evaluate LLMs'
proficiency in handling ontologies -- formal, symbolic representations of
domain knowledge through concepts, relationships, and instances. Based on the
proposed taxonomy, OntoURL systematically assesses three dimensions:
understanding, reasoning, and learning through 15 distinct tasks comprising
58,981 questions derived from 40 ontologies across 8 domains. Experiments with
20 open-source LLMs reveal significant performance differences across models,
tasks, and domains, with current LLMs showing proficiency in understanding
ontological knowledge but substantial weaknesses in reasoning and learning
tasks. These findings highlight fundamental limitations in LLMs' capability to
process symbolic knowledge and establish OntoURL as a critical benchmark for
advancing the integration of LLMs with formal knowledge representations.

</details>


### [33] [BLEUBERI: BLEU is a surprisingly effective reward for instruction following](https://arxiv.org/pdf/2505.11080)
*Yapei Chang, Yekyung Kim, Michael Krumdick, Amir Zadeh, Chuan Li, Chris Tanner, Mohit Iyyer*

Main category: cs.CL

TL;DR: BLEUBERI uses BLEU as a reward function for RL-based alignment, matching reward models in performance and improving factual grounding.


<details>
  <summary>Details</summary>
Motivation: High costs of training reward models and availability of synthetic datasets prompt exploration of simpler metrics like BLEU.

Method: BLEUBERI identifies challenging instructions and applies GRPO using BLEU as the reward function.

Result: BLEUBERI-trained models match reward model-aligned models in performance and improve factual grounding.

Conclusion: String-matching metrics like BLEU are cost-effective alternatives to reward models for alignment.

Abstract: Reward models are central to aligning LLMs with human preferences, but they
are costly to train, requiring large-scale human-labeled preference data and
powerful pretrained LLM backbones. Meanwhile, the increasing availability of
high-quality synthetic instruction-following datasets raises the question: can
simpler, reference-based metrics serve as viable alternatives to reward models
during RL-based alignment? In this paper, we show first that BLEU, a basic
string-matching metric, surprisingly matches strong reward models in agreement
with human preferences on general instruction-following datasets. Based on this
insight, we develop BLEUBERI, a method that first identifies challenging
instructions and then applies Group Relative Policy Optimization (GRPO) using
BLEU directly as the reward function. We demonstrate that BLEUBERI-trained
models are competitive with models trained via reward model-guided RL across
four challenging instruction-following benchmarks and three different base
language models. A human evaluation further supports that the quality of
BLEUBERI model outputs is on par with those from reward model-aligned models.
Moreover, BLEUBERI models generate outputs that are more factually grounded
than competing methods. Overall, we show that given access to high-quality
reference outputs (easily obtained via existing instruction-following datasets
or synthetic data generation), string matching-based metrics are cheap yet
effective proxies for reward models during alignment. We release our code and
data at https://github.com/lilakk/BLEUBERI.

</details>


### [34] [Towards Better Evaluation for Generated Patent Claims](https://arxiv.org/pdf/2505.11095)
*Lekang Jiang, Pascal A Scherz, Stephan Goetz*

Main category: cs.CL

TL;DR: Patent-CE is a benchmark for evaluating patent claims, addressing gaps in automated metrics by incorporating expert annotations and introducing PatClaimEval, a method that aligns closely with human assessments.


<details>
  <summary>Details</summary>
Motivation: The complexity of drafting patent claims creates barriers for small enterprises, and existing automated methods lack alignment with human expert evaluations.

Method: Introduces Patent-CE, a benchmark with expert-annotated evaluations, and PatClaimEval, a multi-dimensional evaluation method for patent claims.

Result: PatClaimEval shows the highest correlation with human expert evaluations across all criteria.

Conclusion: This work advances accurate evaluation of automated patent claim generation systems.

Abstract: Patent claims define the scope of protection and establish the legal
boundaries of an invention. Drafting these claims is a complex and
time-consuming process that usually requires the expertise of skilled patent
attorneys, which can form a large access barrier for many small enterprises. To
solve these challenges, researchers have investigated the use of large language
models (LLMs) for automating patent claim generation. However, existing studies
highlight inconsistencies between automated evaluation metrics and human expert
assessments. To bridge this gap, we introduce Patent-CE, the first
comprehensive benchmark for evaluating patent claims. Patent-CE includes
comparative claim evaluations annotated by patent experts, focusing on five key
criteria: feature completeness, conceptual clarity, terminology consistency,
logical linkage, and overall quality. Additionally, we propose PatClaimEval, a
novel multi-dimensional evaluation method specifically designed for patent
claims. Our experiments demonstrate that PatClaimEval achieves the highest
correlation with human expert evaluations across all assessment criteria among
all tested metrics. This research provides the groundwork for more accurate
evaluations of automated patent claim generation systems.

</details>


### [35] [Scaling Reasoning can Improve Factuality in Large Language Models](https://arxiv.org/pdf/2505.11140)
*Mike Zhang, Johannes Bjerva, Russa Biswas*

Main category: cs.CL

TL;DR: The paper investigates whether longer reasoning chains improve factual accuracy in LLMs beyond mathematical tasks, focusing on open-domain QA. It fine-tunes models with enriched reasoning traces and finds smaller models improve accuracy, with test-time compute further boosting performance by 2-8%.


<details>
  <summary>Details</summary>
Motivation: To determine if extended reasoning chains enhance factual accuracy in LLMs, especially in open-domain QA, beyond just mathematical reasoning.

Method: Distills reasoning traces from large models, fine-tunes various models, and enriches traces with knowledge graph paths. Evaluates on six datasets with 168 runs.

Result: Smaller models show improved factual accuracy, and test-time compute boosts accuracy by 2-8%.

Conclusion: Longer reasoning chains and test-time compute enhance factual accuracy in open-domain QA, with smaller models benefiting significantly.

Abstract: Recent studies on large language model (LLM) reasoning capabilities have
demonstrated promising improvements in model performance by leveraging a
lengthy thinking process and additional computational resources during
inference, primarily in tasks involving mathematical reasoning (Muennighoff et
al., 2025). However, it remains uncertain if longer reasoning chains inherently
enhance factual accuracy, particularly beyond mathematical contexts. In this
work, we thoroughly examine LLM reasoning within complex open-domain
question-answering (QA) scenarios. We initially distill reasoning traces from
advanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then
fine-tune a variety of models ranging from smaller, instruction-tuned variants
to larger architectures based on Qwen2.5. To enrich reasoning traces, we
introduce factual information from knowledge graphs in the form of paths into
our reasoning traces. Our experimental setup includes four baseline approaches
and six different instruction-tuned models evaluated across a benchmark of six
datasets, encompassing over 22.6K questions. Overall, we carry out 168
experimental runs and analyze approximately 1.7 million reasoning traces. Our
findings indicate that, within a single run, smaller reasoning models achieve
noticeable improvements in factual accuracy compared to their original
instruction-tuned counterparts. Moreover, our analysis demonstrates that adding
test-time compute and token budgets factual accuracy consistently improves by
2-8%, further confirming the effectiveness of test-time scaling for enhancing
performance and consequently improving reasoning accuracy in open-domain QA
tasks. We release all the experimental artifacts for further research.

</details>


### [36] [SoLoPO: Unlocking Long-Context Capabilities in LLMs via Short-to-Long Preference Optimization](https://arxiv.org/pdf/2505.11166)
*Huashan Sun, Shengyi Liao, Yansen Han, Yu Bai, Yang Gao, Cheng Fu, Weizhou Shen, Fanqi Wan, Ming Yan, Ji Zhang, Fei Huang*

Main category: cs.CL

TL;DR: SoLoPO is a framework that decouples long-context preference optimization into short-context PO and short-to-long reward alignment, improving efficiency and performance in LLMs.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in LLMs' long-context utilization due to data quality, training inefficiencies, and lack of optimization objectives.

Method: Proposes SoLoPO, combining short-context PO and SoLo-RA to align rewards and transfer short-context abilities to long-context scenarios.

Result: Enhances algorithms with better length and domain generalization, improving computational and memory efficiency.

Conclusion: SoLoPO effectively improves LLMs' long-context utilization and training efficiency.

Abstract: Despite advances in pretraining with extended context lengths, large language
models (LLMs) still face challenges in effectively utilizing real-world
long-context information, primarily due to insufficient long-context alignment
caused by data quality issues, training inefficiencies, and the lack of
well-designed optimization objectives. To address these limitations, we propose
a framework named $\textbf{S}$h$\textbf{o}$rt-to-$\textbf{Lo}$ng
$\textbf{P}$reference $\textbf{O}$ptimization ($\textbf{SoLoPO}$), decoupling
long-context preference optimization (PO) into two components: short-context PO
and short-to-long reward alignment (SoLo-RA), supported by both theoretical and
empirical evidence. Specifically, short-context PO leverages preference pairs
sampled from short contexts to enhance the model's contextual knowledge
utilization ability. Meanwhile, SoLo-RA explicitly encourages reward score
consistency utilization for the responses when conditioned on both short and
long contexts that contain identical task-relevant information. This
facilitates transferring the model's ability to handle short contexts into
long-context scenarios. SoLoPO is compatible with mainstream preference
optimization algorithms, while substantially improving the efficiency of data
construction and training processes. Experimental results show that SoLoPO
enhances all these algorithms with respect to stronger length and domain
generalization abilities across various long-context benchmarks, while
achieving notable improvements in both computational and memory efficiency.

</details>


### [37] [Low-Resource Language Processing: An OCR-Driven Summarization and Translation Pipeline](https://arxiv.org/pdf/2505.11177)
*Hrishit Madhavi, Jacob Cherian, Yuvraj Khamkar, Dhananjay Bhagat*

Main category: cs.CL

TL;DR: An end-to-end multilingual system for extracting and processing text from image-based documents, leveraging OCR, LLMs, and additional NLP modules, with a Gradio interface for accessibility.


<details>
  <summary>Details</summary>
Motivation: To bridge the language gap and improve access to information in image media across diverse linguistic environments.

Method: Uses Tesseract OCR for text extraction, Gemini for translation/summarization, and additional modules (sentiment analysis, topic classification, date extraction) for enhanced comprehension.

Result: Demonstrates a practical application combining libraries, models, and APIs to process multilingual documents effectively.

Conclusion: The system successfully enhances accessibility and comprehension of image-based documents in multiple languages.

Abstract: This paper presents an end-to-end suite for multilingual information
extraction and processing from image-based documents. The system uses Optical
Character Recognition (Tesseract) to extract text in languages such as English,
Hindi, and Tamil, and then a pipeline involving large language model APIs
(Gemini) for cross-lingual translation, abstractive summarization, and
re-translation into a target language. Additional modules add sentiment
analysis (TensorFlow), topic classification (Transformers), and date extraction
(Regex) for better document comprehension. Made available in an accessible
Gradio interface, the current research shows a real-world application of
libraries, models, and APIs to close the language gap and enhance access to
information in image media across different linguistic environments

</details>


### [38] [NoPE: The Counting Power of Transformers with No Positional Encodings](https://arxiv.org/pdf/2505.11199)
*Chris K√∂cher, Alexander Kozachinskiy, Anthony Widjaja Lin, Marco S√§lzer, Georg Zetzsche*

Main category: cs.CL

TL;DR: NoPE-transformers with average hard attention can express complex counting languages (semi-algebraic sets) but not simple ones like PARITY, and their analysis is undecidable.


<details>
  <summary>Details</summary>
Motivation: To explore the expressiveness of NoPE-transformers with average hard attention, challenging the assumption that positional encodings are essential for transformers.

Method: Characterizes languages expressible by NoPE-AHATs as semi-algebraic sets, comparing them to other models like counter machines and Petri nets.

Result: NoPE-AHATs can solve Diophantine equations (undecidable problems) but fail at simple tasks like PARITY; their analysis is undecidable.

Conclusion: NoPE-transformers with average hard attention are surprisingly expressive but limited in certain tasks, and their undecidability complicates analysis.

Abstract: Positional Encodings (PEs) seem to be indispensable for ensuring
expressiveness of transformers; without them attention transformers reduce to a
bag-of-word model. NoPE-transformers (i.e. with No PEs) with unique hard
attention mechanisms were very recently shown to only be able to express
regular languages, i.e., with limited counting ability. This paper shows that,
with average hard attention mechanisms, NoPE-transformers are still
surprisingly expressive: they can express counting languages corresponding to
nonnegative integer solutions to multivariate polynomial equations (i.e.
Diophantine equations), reasoning about which is well-known to be undecidable.
In fact, we provide a precise characterization of languages expressible by
Average Hard Attention NoPE-Transformers (NoPE-AHATs): they correspond
precisely to what we call \emph{semi-algebraic sets}, i.e., finite unions of
sets of nonnegative integer solutions to systems of multivariate polynomial
inequations. We obtain several interesting consequences of our
characterization. Firstly, NoPE-transformers can express counting properties
that are far more complex than established models like simplified counter
machines and Petri nets, but cannot express a very simple counting property of
PARITY. Secondly, the problem of analyzing NoPE-transformers is undecidable,
e.g., whether a given NoPE transformer classifies all input strings in one
class. To complement our results, we exhibit a counting language that is not
expressible by average hard attention transformers even with arbitrary PEs but
is expressible in the circuit complexity class TC$^0$, answering an open
problem.

</details>


### [39] [HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization](https://arxiv.org/pdf/2505.11225)
*Chengyu Huang, Zhengxin Zhang, Claire Cardie*

Main category: cs.CL

TL;DR: HAPO improves LLM efficiency by tracking historical response lengths and optimizing for conciseness without significant accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Prior methods for efficient test-time scaling lack historical context, limiting conciseness over time.

Method: HAPO uses a history state and length reward to incentivize shorter, correct solutions, combined with correctness optimization.

Result: HAPO reduces response lengths by 33-59% with only 2-5% accuracy drop on math benchmarks.

Conclusion: HAPO effectively balances correctness and efficiency, enhancing LLM conciseness.

Abstract: While scaling the length of responses at test-time has been shown to markedly
improve the reasoning abilities and performance of large language models
(LLMs), it often results in verbose outputs and increases inference cost. Prior
approaches for efficient test-time scaling, typically using universal budget
constraints or query-level length optimization, do not leverage historical
information from previous encounters with the same problem during training. We
hypothesize that this limits their ability to progressively make solutions more
concise over time. To address this, we present History-Aware Policy
Optimization (HAPO), which keeps track of a history state (e.g., the minimum
length over previously generated correct responses) for each problem. HAPO
employs a novel length reward function based on this history state to
incentivize the discovery of correct solutions that are more concise than those
previously found. Crucially, this reward structure avoids overly penalizing
shorter incorrect responses with the goal of facilitating exploration towards
more efficient solutions. By combining this length reward with a correctness
reward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to
train DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and
Qwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span
various difficulty levels. Experiment results demonstrate that HAPO effectively
induces LLMs' concise reasoning abilities, producing length reductions of
33-59% with accuracy drops of only 2-5%.

</details>


### [40] [Semantic Caching of Contextual Summaries for Efficient Question-Answering with Language Models](https://arxiv.org/pdf/2505.11271)
*Camille Couturier, Spyros Mastorakis, Haiying Shen, Saravan Rajmohan, Victor R√ºhle*

Main category: cs.CL

TL;DR: A semantic caching method for LLMs reduces redundant computations by 50-60% while maintaining accuracy in QA workflows.


<details>
  <summary>Details</summary>
Motivation: Addressing high computational overhead, memory usage, and network bandwidth in distributed LLM systems for real-time QA.

Method: Introduces semantic caching to store and reuse intermediate contextual summaries for similar queries.

Result: Reduces redundant computations by 50-60% with comparable accuracy to full processing, tested on NaturalQuestions, TriviaQA, and ArXiv.

Conclusion: Balances computational cost and response quality, crucial for real-time AI assistants.

Abstract: Large Language Models (LLMs) are increasingly deployed across edge and cloud
platforms for real-time question-answering and retrieval-augmented generation.
However, processing lengthy contexts in distributed systems incurs high
computational overhead, memory usage, and network bandwidth. This paper
introduces a novel semantic caching approach for storing and reusing
intermediate contextual summaries, enabling efficient information reuse across
similar queries in LLM-based QA workflows. Our method reduces redundant
computations by up to 50-60% while maintaining answer accuracy comparable to
full document processing, as demonstrated on NaturalQuestions, TriviaQA, and a
synthetic ArXiv dataset. This approach balances computational cost and response
quality, critical for real-time AI assistants.

</details>


### [41] [Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs](https://arxiv.org/pdf/2505.11277)
*Yaorui Shi, Shihan Li, Chang Wu, Zhiyuan Liu, Junfeng Fang, Hengxing Cai, An Zhang, Xiang Wang*

Main category: cs.CL

TL;DR: AutoRefine is a reinforcement learning framework that improves retrieval-augmented reasoning by refining knowledge between searches, outperforming existing methods in QA tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of LLMs in retrieving noisy or irrelevant information, hindering accurate reasoning.

Method: Introduces a "search-and-refine-during-think" paradigm with iterative knowledge refinement and tailored retrieval-specific rewards.

Result: Significantly outperforms existing approaches, especially in multi-hop reasoning, with higher-quality searches and effective evidence synthesis.

Conclusion: AutoRefine enhances retrieval-augmented reasoning by refining knowledge iteratively, proving effective in complex QA scenarios.

Abstract: Large language models have demonstrated impressive reasoning capabilities but
are inherently limited by their knowledge reservoir. Retrieval-augmented
reasoning mitigates this limitation by allowing LLMs to query external
resources, but existing methods often retrieve irrelevant or noisy information,
hindering accurate reasoning. In this paper, we propose AutoRefine, a
reinforcement learning post-training framework that adopts a new
``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit
knowledge refinement steps between successive search calls, enabling the model
to iteratively filter, distill, and organize evidence before generating an
answer. Furthermore, we incorporate tailored retrieval-specific rewards
alongside answer correctness rewards using group relative policy optimization.
Experiments on single-hop and multi-hop QA benchmarks demonstrate that
AutoRefine significantly outperforms existing approaches, particularly in
complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine
issues frequent, higher-quality searches and synthesizes evidence effectively.

</details>


### [42] [Temporal fine-tuning for early risk detection](https://arxiv.org/pdf/2505.11280)
*Horacio Thompson, Esa√∫ Villatoro-Tello, Manuel Montes-y-G√≥mez, Marcelo Errecalde*

Main category: cs.CL

TL;DR: Proposes temporal fine-tuning for transformer models to optimize Early Risk Detection (ERD) by integrating time into learning, achieving competitive results in depression and eating disorder tasks.


<details>
  <summary>Details</summary>
Motivation: ERD requires balancing precision and speed, but standard metrics and approaches may not suffice. A new method is needed to incorporate time explicitly.

Method: Temporal fine-tuning of transformer models, analyzing complete user post histories and evaluating with temporal metrics.

Result: Competitive performance in Spanish-language depression and eating disorder tasks, optimizing decisions with context and time progress.

Conclusion: Temporal fine-tuning effectively combines precision and speed in ERD, leveraging transformers' power for better outcomes.

Abstract: Early Risk Detection (ERD) on the Web aims to identify promptly users facing
social and health issues. Users are analyzed post-by-post, and it is necessary
to guarantee correct and quick answers, which is particularly challenging in
critical scenarios. ERD involves optimizing classification precision and
minimizing detection delay. Standard classification metrics may not suffice,
resorting to specific metrics such as ERDE(theta) that explicitly consider
precision and delay. The current research focuses on applying a multi-objective
approach, prioritizing classification performance and establishing a separate
criterion for decision time. In this work, we propose a completely different
strategy, temporal fine-tuning, which allows tuning transformer-based models by
explicitly incorporating time within the learning process. Our method allows us
to analyze complete user post histories, tune models considering different
contexts, and evaluate training performance using temporal metrics. We
evaluated our proposal in the depression and eating disorders tasks for the
Spanish language, achieving competitive results compared to the best models of
MentalRiskES 2023. We found that temporal fine-tuning optimized decisions
considering context and time progress. In this way, by properly taking
advantage of the power of transformers, it is possible to address ERD by
combining precision and speed as a single objective.

</details>


### [43] [Probing Subphonemes in Morphology Models](https://arxiv.org/pdf/2505.11297)
*Gal Astrach, Yuval Pinter*

Main category: cs.CL

TL;DR: Transformers perform well in morphological inflection but struggle with generalization. A probing method reveals they capture local phonological features better than long-distance ones, impacting training strategies.


<details>
  <summary>Details</summary>
Motivation: To understand why transformers generalize poorly in morphological tasks by examining their ability to encode phonological and subphonemic features.

Method: A language-agnostic probing method was applied to transformers trained on phonemes across seven languages, analyzing phonological feature encoding.

Result: Local features (e.g., final-obstruent devoicing) are well-captured in phoneme embeddings, while long-distance dependencies (e.g., vowel harmony) rely more on the encoder.

Conclusion: The findings suggest subphonemic feature acquisition is crucial for improving transformer-based morphological models.

Abstract: Transformers have achieved state-of-the-art performance in morphological
inflection tasks, yet their ability to generalize across languages and
morphological rules remains limited. One possible explanation for this behavior
can be the degree to which these models are able to capture implicit phenomena
at the phonological and subphonemic levels. We introduce a language-agnostic
probing method to investigate phonological feature encoding in transformers
trained directly on phonemes, and perform it across seven morphologically
diverse languages. We show that phonological features which are local, such as
final-obstruent devoicing in Turkish, are captured well in phoneme embeddings,
whereas long-distance dependencies like vowel harmony are better represented in
the transformer's encoder. Finally, we discuss how these findings inform
empirical strategies for training morphological models, particularly regarding
the role of subphonemic feature acquisition.

</details>


### [44] [Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI](https://arxiv.org/pdf/2505.10472)
*Agnik Saha, Victoria Churchill, Anny D. Rodriguez, Ugur Kursuncu, Muhammed Y. Idris*

Main category: cs.CL

TL;DR: The study evaluates LLMs for generating cancer-related information, finding general-purpose models excel in linguistic quality and affectiveness, while medical models are more accessible but less safe.


<details>
  <summary>Details</summary>
Motivation: Address gaps in public understanding of cancer prevention and treatment by assessing LLMs' ability to provide accurate, safe, and accessible information.

Method: Mixed-methods framework evaluating five general-purpose and three medical LLMs using quantitative metrics, qualitative expert ratings, and statistical analysis (Welch's ANOVA, Games-Howell, Hedges' g).

Result: General-purpose LLMs outperform in linguistic quality and affectiveness; medical LLMs are more accessible but show higher harm, toxicity, and bias.

Conclusion: Highlights the need for targeted improvements in model design to balance domain-specific knowledge with safety, ensuring accurate and accessible health communication.

Abstract: Effective communication about breast and cervical cancers remains a
persistent health challenge, with significant gaps in public understanding of
cancer prevention, screening, and treatment, potentially leading to delayed
diagnoses and inadequate treatments. This study evaluates the capabilities and
limitations of Large Language Models (LLMs) in generating accurate, safe, and
accessible cancer-related information to support patient understanding. We
evaluated five general-purpose and three medical LLMs using a mixed-methods
evaluation framework across linguistic quality, safety and trustworthiness, and
communication accessibility and affectiveness. Our approach utilized
quantitative metrics, qualitative expert ratings, and statistical analysis
using Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that
general-purpose LLMs produced outputs of higher linguistic quality and
affectiveness, while medical LLMs demonstrate greater communication
accessibility. However, medical LLMs tend to exhibit higher levels of potential
harm, toxicity, and bias, reducing their performance in safety and
trustworthiness. Our findings indicate a duality between domain-specific
knowledge and safety in health communications. The results highlight the need
for intentional model design with targeted improvements, particularly in
mitigating harm and bias, and improving safety and affectiveness. This study
provides a comprehensive evaluation of LLMs for cancer communication, offering
critical insights for improving AI-generated health content and informing
future development of accurate, safe, and accessible digital health tools.

</details>


### [45] [XtraGPT: LLMs for Human-AI Collaboration on Controllable Academic Paper Revision](https://arxiv.org/pdf/2505.11336)
*Nuo Chen, Andre Lin HuiKai, Jiaying Wu, Junyi Hou, Zining Zhang, Qian Wang, Xidong Wang, Bingsheng He*

Main category: cs.CL

TL;DR: The paper introduces XtraGPT, a human-AI collaboration framework for academic paper revision, outperforming existing systems with context-aware, instruction-guided assistance.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of LLMs in supporting high-quality scientific writing, particularly in iterative, revision-driven academic workflows.

Method: Developed a dataset of 7,040 research papers with 140,000 instruction-response pairs, then created XtraGPT, a suite of open-source LLMs for context-aware writing assistance.

Result: XtraGPT outperforms same-scale baselines and approaches proprietary systems in improving scientific drafts, validated by automated and human evaluations.

Conclusion: The proposed framework effectively enhances scientific writing by combining human-AI collaboration and context-aware LLMs.

Abstract: Despite the growing adoption of large language models (LLMs) in academic
workflows, their capabilities remain limited when it comes to supporting
high-quality scientific writing. Most existing systems are designed for
general-purpose scientific text generation and fail to meet the sophisticated
demands of research communication beyond surface-level polishing, such as
conceptual coherence across sections. Furthermore, academic writing is
inherently iterative and revision-driven, a process not well supported by
direct prompting-based paradigms. To address these scenarios, we propose a
human-AI collaboration framework for academic paper revision. We first
introduce a comprehensive dataset of 7,040 research papers from top-tier venues
annotated with over 140,000 instruction-response pairs that reflect realistic,
section-level scientific revisions. Building on the dataset, we develop
XtraGPT, the first suite of open-source LLMs, designed to provide
context-aware, instruction-guided writing assistance, ranging from 1.5B to 14B
parameters. Extensive experiments validate that XtraGPT significantly
outperforms same-scale baselines and approaches the quality of proprietary
systems. Both automated preference assessments and human evaluations confirm
the effectiveness of our models in improving scientific drafts.

</details>


### [46] [Benchmarking Critical Questions Generation: A Challenging Reasoning Task for Large Language Models](https://arxiv.org/pdf/2505.11341)
*Banca Calvo Figueras, Rodrigo Agerri*

Main category: cs.CL

TL;DR: The paper introduces a comprehensive approach for Critical Questions Generation (CQs-Gen), including a new dataset, evaluation methods, and benchmarks for LLMs.


<details>
  <summary>Details</summary>
Motivation: To advance critical thinking by addressing the lack of datasets and evaluation standards for CQs-Gen.

Method: Constructs a large-scale annotated dataset and evaluates automatic methods, favoring LLM-based reference techniques.

Result: Zero-shot evaluation of 11 LLMs sets a baseline, highlighting task difficulty.

Conclusion: Provides resources to encourage research in CQs-Gen for automated reasoning and human critical thinking.

Abstract: The task of Critical Questions Generation (CQs-Gen) aims to foster critical
thinking by enabling systems to generate questions that expose assumptions and
challenge the reasoning in arguments. Despite growing interest in this area,
progress has been hindered by the lack of suitable datasets and automatic
evaluation standards. This work presents a comprehensive approach to support
the development and benchmarking of systems for this task. We construct the
first large-scale manually-annotated dataset. We also investigate automatic
evaluation methods and identify a reference-based technique using large
language models (LLMs) as the strategy that best correlates with human
judgments. Our zero-shot evaluation of 11 LLMs establishes a strong baseline
while showcasing the difficulty of the task. Data, code, and a public
leaderboard are provided to encourage further research not only in terms of
model performance, but also to explore the practical benefits of CQs-Gen for
both automated reasoning and human critical thinking.

</details>


### [47] [GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents](https://arxiv.org/pdf/2505.11368)
*Lingxiao Diao, Xinyue Xu, Wanxuan Sun, Cheng Yang, Zhuosheng Zhang*

Main category: cs.CL

TL;DR: GuideBench is introduced to evaluate LLMs' ability to follow domain-oriented guidelines, addressing gaps in current benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on general domains and commonsense knowledge, lacking evaluation for domain-specific guidelines in LLMs.

Method: GuideBench assesses LLMs on rule adherence, robustness to updates, and human preference alignment.

Result: Experiments show significant room for improvement in LLMs' guideline-following capabilities.

Conclusion: GuideBench provides a crucial tool for advancing LLMs as domain-oriented agents.

Abstract: Large language models (LLMs) have been widely deployed as autonomous agents
capable of following user instructions and making decisions in real-world
applications. Previous studies have made notable progress in benchmarking the
instruction following capabilities of LLMs in general domains, with a primary
focus on their inherent commonsense knowledge. Recently, LLMs have been
increasingly deployed as domain-oriented agents, which rely on domain-oriented
guidelines that may conflict with their commonsense knowledge. These guidelines
exhibit two key characteristics: they consist of a wide range of
domain-oriented rules and are subject to frequent updates. Despite these
challenges, the absence of comprehensive benchmarks for evaluating the
domain-oriented guideline following capabilities of LLMs presents a significant
obstacle to their effective assessment and further development. In this paper,
we introduce GuideBench, a comprehensive benchmark designed to evaluate
guideline following performance of LLMs. GuideBench evaluates LLMs on three
critical aspects: (i) adherence to diverse rules, (ii) robustness to rule
updates, and (iii) alignment with human preferences. Experimental results on a
range of LLMs indicate substantial opportunities for improving their ability to
follow domain-oriented guidelines.

</details>


### [48] [A computational system to handle the orthographic layer of tajwid in contemporary Quranic Orthography](https://arxiv.org/pdf/2505.11379)
*Alicia Gonz√°lez Mart√≠nez*

Main category: cs.CL

TL;DR: The paper explores the systematicity of tajwid rules in the Cairo Quran using a digital edition and a Python module to manipulate orthographic layers. It highlights the potential for computational alignment and comparison of Quranic manuscripts.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the phonetic and prosodic processes in the Quran by analyzing tajwid rules, leveraging the precision of the Cairo Quran's digital edition.

Method: A Python module was developed to add or remove the orthographic layer of tajwid from Quranic texts, enabling systematic analysis of the rules.

Result: The rules of tajwid in the Cairo Quran provide a precise framework for studying phonetic notation and can computationally align and compare Quranic manuscripts.

Conclusion: The study offers a powerful computational tool for analyzing Quranic manuscripts and understanding diacritic notation systems, extending beyond isolated texts to interconnected manuscripts.

Abstract: Contemporary Quranic Orthography (CQO) relies on a precise system of phonetic
notation that can be traced back to the early stages of Islam, when the Quran
was mainly oral in nature and the first written renderings of it served as
memory aids for this oral tradition. The early systems of diacritical marks
created on top of the Quranic Consonantal Text (QCT) motivated the creation and
further development of a fine-grained system of phonetic notation that
represented tajwid-the rules of recitation. We explored the systematicity of
the rules of tajwid, as they are encountered in the Cairo Quran, using a fully
and accurately encoded digital edition of the Quranic text. For this purpose,
we developed a python module that can remove or add the orthographic layer of
tajwid from a Quranic text in CQO. The interesting characteristic of these two
sets of rules is that they address the complete Quranic text of the Cairo
Quran, so they can be used as precise witnesses to study its phonetic and
prosodic processes. From a computational point of view, the text of the Cairo
Quran can be used as a linchpin to align and compare Quranic manuscripts, due
to its richness and completeness. This will let us create a very powerful
framework to work with the Arabic script, not just within an isolated text, but
automatically exploring a specific textual phenomenon in other connected
manuscripts. Having all the texts mapped among each other can serve as a
powerful tool to study the nature of the notation systems of diacritics added
to the consonantal skeleton.

</details>


### [49] [CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in Medical LLMs](https://arxiv.org/pdf/2505.11413)
*Sijia Chen, Xiaomin Li, Mengxue Zhang, Eric Hanchen Jiang, Qingcheng Zeng, Chen-Hsiang Yu*

Main category: cs.CL

TL;DR: CARES is a benchmark for evaluating LLM safety in healthcare, addressing gaps in existing benchmarks by including clinical specificity, graded harm levels, and jailbreak-style attacks. It reveals vulnerabilities in LLMs and proposes a mitigation strategy.


<details>
  <summary>Details</summary>
Motivation: To address the lack of clinical specificity and graded harm levels in existing benchmarks for LLM safety in healthcare, and to evaluate susceptibility to adversarial manipulation.

Method: CARES includes 18,000+ prompts across eight medical safety principles, four harm levels, and four prompting styles. It uses a three-way response evaluation protocol and a Safety Score metric.

Result: State-of-the-art LLMs are vulnerable to jailbreaks and over-refuse safe queries. A lightweight classifier is proposed to mitigate jailbreak attempts.

Conclusion: CARES offers a rigorous framework for improving medical LLM safety under adversarial conditions, highlighting vulnerabilities and proposing solutions.

Abstract: Large language models (LLMs) are increasingly deployed in medical contexts,
raising critical concerns about safety, alignment, and susceptibility to
adversarial manipulation. While prior benchmarks assess model refusal
capabilities for harmful prompts, they often lack clinical specificity, graded
harmfulness levels, and coverage of jailbreak-style attacks. We introduce CARES
(Clinical Adversarial Robustness and Evaluation of Safety), a benchmark for
evaluating LLM safety in healthcare. CARES includes over 18,000 prompts
spanning eight medical safety principles, four harm levels, and four prompting
styles: direct, indirect, obfuscated, and role-play, to simulate both malicious
and benign use cases. We propose a three-way response evaluation protocol
(Accept, Caution, Refuse) and a fine-grained Safety Score metric to assess
model behavior. Our analysis reveals that many state-of-the-art LLMs remain
vulnerable to jailbreaks that subtly rephrase harmful prompts, while also
over-refusing safe but atypically phrased queries. Finally, we propose a
mitigation strategy using a lightweight classifier to detect jailbreak attempts
and steer models toward safer behavior via reminder-based conditioning. CARES
provides a rigorous framework for testing and improving medical LLM safety
under adversarial and ambiguous conditions.

</details>


### [50] [Towards Cultural Bridge by Bahnaric-Vietnamese Translation Using Transfer Learning of Sequence-To-Sequence Pre-training Language Model](https://arxiv.org/pdf/2505.11421)
*Phan Tran Minh Dat, Vo Hoang Nhat Khang, Quan Thanh Tho*

Main category: cs.CL

TL;DR: The paper proposes a transfer learning approach using a sequence-to-sequence model for Bahnaric-Vietnamese translation, addressing resource scarcity through pre-training and data augmentation.


<details>
  <summary>Details</summary>
Motivation: To bridge cultural gaps between Bahnaric and Vietnamese ethnic groups in Vietnam by enabling translation, despite challenges like limited Bahnaric resources.

Method: Leverages a pre-trained Vietnamese sequence-to-sequence model, fine-tuned with limited bilingual data, and uses data augmentation and heuristics for improved translation.

Result: The approach effectively handles resource imbalance and optimizes training, proving highly effective for Bahnaric-Vietnamese translation.

Conclusion: The method successfully aids language preservation and mutual understanding, demonstrating the potential of transfer learning for low-resource languages.

Abstract: This work explores the journey towards achieving Bahnaric-Vietnamese
translation for the sake of culturally bridging the two ethnic groups in
Vietnam. However, translating from Bahnaric to Vietnamese also encounters some
difficulties. The most prominent challenge is the lack of available original
Bahnaric resources source language, including vocabulary, grammar, dialogue
patterns and bilingual corpus, which hinders the data collection process for
training. To address this, we leverage a transfer learning approach using
sequence-to-sequence pre-training language model. First of all, we leverage a
pre-trained Vietnamese language model to capture the characteristics of this
language. Especially, to further serve the purpose of machine translation, we
aim for a sequence-to-sequence model, not encoder-only like BERT or
decoder-only like GPT. Taking advantage of significant similarity between the
two languages, we continue training the model with the currently limited
bilingual resources of Vietnamese-Bahnaric text to perform the transfer
learning from language model to machine translation. Thus, this approach can
help to handle the problem of imbalanced resources between two languages, while
also optimizing the training and computational processes. Additionally, we also
enhanced the datasets using data augmentation to generate additional resources
and defined some heuristic methods to help the translation more precise. Our
approach has been validated to be highly effective for the Bahnaric-Vietnamese
translation model, contributing to the expansion and preservation of languages,
and facilitating better mutual understanding between the two ethnic people.

</details>


### [51] [When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs](https://arxiv.org/pdf/2505.11423)
*Xiaomin Li, Zhou Yu, Zhiwei Zhang, Xupeng Chen, Ziji Zhang, Yingying Zhuang, Narayanan Sadagopan, Anurag Beniwal*

Main category: cs.CL

TL;DR: Explicit chain-of-thought (CoT) reasoning in large language models can degrade instruction-following accuracy, as shown in evaluations of 15 models on IFEval and ComplexBench. The study identifies patterns where reasoning helps or hurts performance and proposes mitigation strategies like selective reasoning.


<details>
  <summary>Details</summary>
Motivation: To uncover the overlooked phenomenon of reasoning-induced performance degradation in instruction-following tasks and develop strategies to mitigate it.

Method: Evaluated 15 models on IFEval and ComplexBench, analyzed attention patterns, and proposed four mitigation strategies (in-context learning, self-reflection, self-selective reasoning, classifier-selective reasoning).

Result: CoT reasoning often diverts attention from instruction-relevant tokens, degrading performance. Selective reasoning strategies, especially classifier-selective reasoning, recover lost performance.

Conclusion: This work systematically exposes reasoning-induced failures in instruction-following and offers practical mitigation strategies, marking the first such study.

Abstract: Reasoning-enhanced large language models (RLLMs), whether explicitly trained
for reasoning or prompted via chain-of-thought (CoT), have achieved
state-of-the-art performance on many complex reasoning tasks. However, we
uncover a surprising and previously overlooked phenomenon: explicit CoT
reasoning can significantly degrade instruction-following accuracy. Evaluating
15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints)
and ComplexBench (with complex, compositional constraints), we consistently
observe performance drops when CoT prompting is applied. Through large-scale
case studies and an attention-based analysis, we identify common patterns where
reasoning either helps (e.g., with formatting or lexical precision) or hurts
(e.g., by neglecting simple constraints or introducing unnecessary content). We
propose a metric, constraint attention, to quantify model focus during
generation and show that CoT reasoning often diverts attention away from
instruction-relevant tokens. To mitigate these effects, we introduce and
evaluate four strategies: in-context learning, self-reflection, self-selective
reasoning, and classifier-selective reasoning. Our results demonstrate that
selective reasoning strategies, particularly classifier-selective reasoning,
can substantially recover lost performance. To our knowledge, this is the first
work to systematically expose reasoning-induced failures in
instruction-following and offer practical mitigation strategies.

</details>


### [52] [GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art](https://arxiv.org/pdf/2505.11436)
*Chenkai Zhang, Yiming Lei, Zeming Liu, Haitao Leng, Shaoguo Liu, Tingting Gao, Qingjie Liu, Yunhong Wang*

Main category: cs.CL

TL;DR: GODBench is a new benchmark for evaluating MLLMs' ability to create creative video comments, and RoT is a framework to enhance their creativity.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs and benchmarks struggle with creative tasks like humor and satire in video comments.

Method: Introduce GODBench for multimodal evaluation and propose Ripple of Thought (RoT) for multi-step reasoning.

Result: RoT improves MLLMs' creative output, while current methods still lag in understanding and generating creative comments.

Conclusion: RoT shows promise for advancing MLLM-based creativity, with GODBench as a tool for future research.

Abstract: Video Comment Art enhances user engagement by providing creative content that
conveys humor, satire, or emotional resonance, requiring a nuanced and
comprehensive grasp of cultural and contextual subtleties. Although Multimodal
Large Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated
strong reasoning abilities in STEM tasks (e.g. mathematics and coding), they
still struggle to generate creative expressions such as resonant jokes and
insightful satire. Moreover, existing benchmarks are constrained by their
limited modalities and insufficient categories, hindering the exploration of
comprehensive creativity in video-based Comment Art creation. To address these
limitations, we introduce GODBench, a novel benchmark that integrates video and
text modalities to systematically evaluate MLLMs' abilities to compose Comment
Art. Furthermore, inspired by the propagation patterns of waves in physics, we
propose Ripple of Thought (RoT), a multi-step reasoning framework designed to
enhance the creativity of MLLMs. Extensive experiments reveal that existing
MLLMs and CoT methods still face significant challenges in understanding and
generating creative video comments. In contrast, RoT provides an effective
approach to improve creative composing, highlighting its potential to drive
meaningful advancements in MLLM-based creativity. GODBench is publicly
available at https://github.com/stan-lei/GODBench-ACL2025.

</details>


### [53] [Is Compression Really Linear with Code Intelligence?](https://arxiv.org/pdf/2505.11441)
*Xianzhen Luo, Shijie Xuyang, Tianhao Cheng, Zheng Chu, Houyi Li, ziqi wang, Siming Huang, Qingfu Zhu, Qiufeng Wang, Xiangyu Zhang, Shuigeng Zhou, Wanxiang Che*

Main category: cs.CL

TL;DR: The paper explores the relationship between data compression and Code LLMs, revealing a logarithmic (not linear) link between compression (BPC) and code intelligence, using a novel evaluation method called Format Annealing.


<details>
  <summary>Details</summary>
Motivation: Prior work assumed a linear relationship between compression and general intelligence in LLMs but failed to account for the complexity of code (multi-language, multi-task) and fair evaluation of modern Code LLMs.

Method: The study evaluates diverse open-source Code LLMs on multi-language, multi-task benchmarks and introduces Format Annealing for fair assessment. Compression efficacy (BPC) is measured using a new GitHub-derived validation set.

Result: Empirical results show a logarithmic (not linear) relationship between code intelligence and BPC, refining prior hypotheses.

Conclusion: The work offers a nuanced understanding of compression's role in code intelligence and provides a robust evaluation framework for the code domain.

Abstract: Understanding the relationship between data compression and the capabilities
of Large Language Models (LLMs) is crucial, especially in specialized domains
like code intelligence. Prior work posited a linear relationship between
compression and general intelligence. However, it overlooked the multifaceted
nature of code that encompasses diverse programming languages and tasks, and
struggled with fair evaluation of modern Code LLMs. We address this by
evaluating a diverse array of open-source Code LLMs on comprehensive
multi-language, multi-task code benchmarks. To address the challenge of
efficient and fair evaluation of pre-trained LLMs' code intelligence, we
introduce \textit{Format Annealing}, a lightweight, transparent training
methodology designed to assess the intrinsic capabilities of these pre-trained
models equitably. Compression efficacy, measured as bits-per-character (BPC),
is determined using a novel, large-scale, and previously unseen code validation
set derived from GitHub. Our empirical results reveal a fundamental logarithmic
relationship between measured code intelligence and BPC. This finding refines
prior hypotheses of linearity, which we suggest are likely observations of the
logarithmic curve's tail under specific, limited conditions. Our work provides
a more nuanced understanding of compression's role in developing code
intelligence and contributes a robust evaluation framework in the code domain.

</details>


### [54] [Disentangling Reasoning and Knowledge in Medical Large Language Models](https://arxiv.org/pdf/2505.11462)
*Rahul Thapa, Qingyang Wu, Kevin Wu, Harrison Zhang, Angela Zhang, Eric Wu, Haotian Ye, Suhana Bedi, Nevin Aresh, Joseph Boen, Shriya Reddy, Ben Athiwaratkun, Shuaiwen Leon Song, James Zou*

Main category: cs.CL

TL;DR: The paper separates biomedical QA benchmarks into reasoning- and knowledge-focused subsets, revealing only 32.8% require complex reasoning. It evaluates models, showing gaps in reasoning vs. knowledge performance, and introduces BioMed-R1 for improved reasoning.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks mix reasoning with factual recall, making it hard to assess true reasoning capabilities in LLMs for medical tasks.

Method: Uses a PubMedBERT classifier to split benchmarks, evaluates models on reasoning/knowledge, and trains BioMed-R1 with fine-tuning and reinforcement learning.

Result: Only 32.8% of questions require complex reasoning. BioMed-R1 outperforms similarly sized models. Biomedical models degrade in adversarial tests, while general models show robustness.

Conclusion: BioMed-R1 improves reasoning performance, and further gains could come from clinical case reports and adversarial training.

Abstract: Medical reasoning in large language models (LLMs) aims to emulate clinicians'
diagnostic thinking, but current benchmarks such as MedQA-USMLE, MedMCQA, and
PubMedQA often mix reasoning with factual recall. We address this by separating
11 biomedical QA benchmarks into reasoning- and knowledge-focused subsets using
a PubMedBERT classifier that reaches 81 percent accuracy, comparable to human
performance. Our analysis shows that only 32.8 percent of questions require
complex reasoning. We evaluate biomedical models (HuatuoGPT-o1, MedReason, m1)
and general-domain models (DeepSeek-R1, o4-mini, Qwen3), finding consistent
gaps between knowledge and reasoning performance. For example, m1 scores 60.5
on knowledge but only 47.1 on reasoning. In adversarial tests where models are
misled with incorrect initial reasoning, biomedical models degrade sharply,
while larger or RL-trained general models show more robustness. To address
this, we train BioMed-R1 using fine-tuning and reinforcement learning on
reasoning-heavy examples. It achieves the strongest performance among similarly
sized models. Further gains may come from incorporating clinical case reports
and training with adversarial and backtracking scenarios.

</details>


### [55] [No Gold Standard, No Problem: Reference-Free Evaluation of Taxonomies](https://arxiv.org/pdf/2505.11470)
*Pascal Wullschleger, Majid Zarharan, Donnacha Daly, Marc Pouly, Jennifer Foster*

Main category: cs.CL

TL;DR: Two reference-free metrics for taxonomy quality evaluation: one measures robustness via semantic-taxonomic correlation, and the other assesses logical adequacy using Natural Language Inference. Both correlate well with gold-standard F1 scores.


<details>
  <summary>Details</summary>
Motivation: Existing metrics for taxonomy quality evaluation lack coverage for certain errors, such as robustness and logical adequacy.

Method: 1. Robustness metric: correlation between semantic and taxonomic similarity. 2. Logical adequacy metric: Natural Language Inference. Both tested on five taxonomies.

Result: Both metrics correlate well with F1 scores against gold-standard taxonomies.

Conclusion: The proposed metrics effectively evaluate taxonomy quality, addressing gaps in existing methods.

Abstract: We introduce two reference-free metrics for quality evaluation of taxonomies.
The first metric evaluates robustness by calculating the correlation between
semantic and taxonomic similarity, covering a type of error not handled by
existing metrics. The second uses Natural Language Inference to assess logical
adequacy. Both metrics are tested on five taxonomies and are shown to correlate
well with F1 against gold-standard taxonomies.

</details>


### [56] [HelpSteer3-Preference: Open Human-Annotated Preference Data across Diverse Tasks and Languages](https://arxiv.org/pdf/2505.11475)
*Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Hoo-Chang Shin, Felipe Soares, Alexander Bukharin, Ellie Evans, Yi Dong, Oleksii Kuchaiev*

Main category: cs.CL

TL;DR: HelpSteer3-Preference is a high-quality, diverse preference dataset for RLHF, improving RM performance by ~10%.


<details>
  <summary>Details</summary>
Motivation: Advancing the quality and diversity of openly available preference data for training instruction-following LLMs.

Method: Introduces HelpSteer3-Preference, a 40,000-sample human-annotated dataset spanning STEM, coding, and multilingual tasks.

Result: Trained RMs achieve 82.4% on RM-Bench and 73.7% on JudgeBench, a ~10% improvement over prior results.

Conclusion: HelpSteer3-Preference enhances RM training and enables effective RLHF alignment for policy models.

Abstract: Preference datasets are essential for training general-domain,
instruction-following language models with Reinforcement Learning from Human
Feedback (RLHF). Each subsequent data release raises expectations for future
data collection, meaning there is a constant need to advance the quality and
diversity of openly available preference data. To address this need, we
introduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0),
high-quality, human-annotated preference dataset comprising of over 40,000
samples. These samples span diverse real-world applications of large language
models (LLMs), including tasks relating to STEM, coding and multilingual
scenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that
achieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This
represents a substantial improvement (~10% absolute) over the previously
best-reported results from existing RMs. We demonstrate HelpSteer3-Preference
can also be applied to train Generative RMs and how policy models can be
aligned with RLHF using our RMs. Dataset (CC-BY-4.0):
https://huggingface.co/datasets/nvidia/HelpSteer3#preference

</details>


### [57] [Improving Assembly Code Performance with Large Language Models via Reinforcement Learning](https://arxiv.org/pdf/2505.11480)
*Anjiang Wei, Tarun Suresh, Huanmi Tan, Yinglun Xu, Gagandeep Singh, Ke Wang, Alex Aiken*

Main category: cs.CL

TL;DR: LLMs trained with reinforcement learning (PPO) can optimize assembly code, achieving 96.0% test pass rates and 1.47x speedup over gcc -O3.


<details>
  <summary>Details</summary>
Motivation: Explore LLMs' potential for optimizing assembly code, where fine-grained control offers performance improvements hard to achieve in high-level languages.

Method: A reinforcement learning framework using PPO, with rewards for functional correctness and execution performance, tested on 8,072 real-world programs.

Result: Qwen2.5-Coder-7B-PPO achieves 96.0% test pass rates and 1.47x speedup, outperforming 20 other models.

Conclusion: Reinforcement learning enables LLMs to effectively optimize assembly code performance.

Abstract: Large language models (LLMs) have demonstrated strong performance across a
wide range of programming tasks, yet their potential for code optimization
remains underexplored. This work investigates whether LLMs can optimize the
performance of assembly code, where fine-grained control over execution enables
improvements that are difficult to express in high-level languages. We present
a reinforcement learning framework that trains LLMs using Proximal Policy
Optimization (PPO), guided by a reward function that considers both functional
correctness, validated through test cases, and execution performance relative
to the industry-standard compiler gcc -O3. To support this study, we introduce
a benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO,
achieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3
baseline, outperforming all 20 other models evaluated, including
Claude-3.7-sonnet. These results indicate that reinforcement learning can
unlock the potential of LLMs to serve as effective optimizers for assembly code
performance.

</details>


### [58] [SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning](https://arxiv.org/pdf/2505.11484)
*Yige Xu, Xu Guo, Zhiwei Zeng, Chunyan Miao*

Main category: cs.CL

TL;DR: SoftCoT++ enhances reasoning by diversifying latent thought exploration in continuous space, outperforming existing methods like SoftCoT and self-consistency scaling.


<details>
  <summary>Details</summary>
Motivation: Existing continuous-space reasoning methods lack diverse exploration due to fixed latent representations. SoftCoT++ addresses this by perturbing latent thoughts and promoting diversity.

Method: SoftCoT++ introduces specialized initial tokens to perturb latent thoughts and uses contrastive learning to diversify soft thought representations.

Result: Experiments on five benchmarks and two LLM architectures show SoftCoT++ significantly improves reasoning performance over SoftCoT and self-consistency scaling.

Conclusion: SoftCoT++ effectively enables diverse exploration in continuous-space reasoning, demonstrating strong compatibility with conventional scaling techniques.

Abstract: Test-Time Scaling (TTS) refers to approaches that improve reasoning
performance by allocating extra computation during inference, without altering
the model's parameters. While existing TTS methods operate in a discrete token
space by generating more intermediate steps, recent studies in Coconut and
SoftCoT have demonstrated that thinking in the continuous latent space can
further enhance the reasoning performance. Such latent thoughts encode
informative thinking without the information loss associated with
autoregressive token generation, sparking increased interest in
continuous-space reasoning. Unlike discrete decoding, where repeated sampling
enables exploring diverse reasoning paths, latent representations in continuous
space are fixed for a given input, which limits diverse exploration, as all
decoded paths originate from the same latent thought. To overcome this
limitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling
paradigm by enabling diverse exploration of thinking paths. Specifically, we
perturb latent thoughts via multiple specialized initial tokens and apply
contrastive learning to promote diversity among soft thought representations.
Experiments across five reasoning benchmarks and two distinct LLM architectures
demonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms
SoftCoT with self-consistency scaling. Moreover, it shows strong compatibility
with conventional scaling techniques such as self-consistency. Source code is
available at https://github.com/xuyige/SoftCoT.

</details>


### [59] [Modeling cognitive processes of natural reading with transformer-based Language Models](https://arxiv.org/pdf/2505.11485)
*Bruno Bianchi, Ferm√≠n Travi, Juan E. Kamienkowski*

Main category: cs.CL

TL;DR: Transformer-based models (GPT2, LLaMA-7B, LLaMA2-7B) outperform older models in explaining Gaze Duration variance in reading but still fall short of human predictability.


<details>
  <summary>Details</summary>
Motivation: To explore how advanced NLP models (transformers) compare to older models in explaining eye movement behaviors (Gaze Duration) during reading, particularly in Rioplantense Spanish.

Method: Evaluated transformer-based models (GPT2, LLaMA-7B, LLaMA2-7B) against earlier models (N-grams, LSTMs) using Gaze Duration data from readers.

Result: Transformers outperform older models but still cannot fully account for human predictability in Gaze Duration.

Conclusion: State-of-the-art language models predict language differently than humans, indicating a gap in their ability to fully replicate human reading behaviors.

Abstract: Recent advances in Natural Language Processing (NLP) have led to the
development of highly sophisticated language models for text generation. In
parallel, neuroscience has increasingly employed these models to explore
cognitive processes involved in language comprehension. Previous research has
shown that models such as N-grams and LSTM networks can partially account for
predictability effects in explaining eye movement behaviors, specifically Gaze
Duration, during reading. In this study, we extend these findings by evaluating
transformer-based models (GPT2, LLaMA-7B, and LLaMA2-7B) to further investigate
this relationship. Our results indicate that these architectures outperform
earlier models in explaining the variance in Gaze Durations recorded from
Rioplantense Spanish readers. However, similar to previous studies, these
models still fail to account for the entirety of the variance captured by human
predictability. These findings suggest that, despite their advancements,
state-of-the-art language models continue to predict language in ways that
differ from human readers.

</details>


### [60] [Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator](https://arxiv.org/pdf/2305.15099)
*Ziwei He, Meng Yang, Minwei Feng, Jingcheng Yin, Xinbing Wang, Jingwen Leng, Zhouhan Lin*

Main category: cs.CL

TL;DR: The paper introduces Fourier Transformer, a method using FFT and DCT to reduce computational costs in transformers while retaining pretrained model compatibility.


<details>
  <summary>Details</summary>
Motivation: Transformers are computationally expensive for long sequences due to quadratic complexity in self-attention. Existing solutions often prevent weight inheritance from pretrained models.

Method: Proposes Fourier Transformer, leveraging FFT for DCT to progressively remove redundancies in sequences, reducing costs without sacrificing pretrained model compatibility.

Result: Achieves state-of-the-art performance on LRA benchmark and outperforms BART in generative tasks, with improved speed and space efficiency.

Conclusion: Fourier Transformer effectively addresses transformer inefficiency, enabling efficient long-sequence modeling and seamless integration with pretrained models.

Abstract: The transformer model is known to be computationally demanding, and
prohibitively costly for long sequences, as the self-attention module uses a
quadratic time and space complexity with respect to sequence length. Many
researchers have focused on designing new forms of self-attention or
introducing new parameters to overcome this limitation, however a large portion
of them prohibits the model to inherit weights from large pretrained models. In
this work, the transformer's inefficiency has been taken care of from another
perspective. We propose Fourier Transformer, a simple yet effective approach by
progressively removing redundancies in hidden sequence using the ready-made
Fast Fourier Transform (FFT) operator to perform Discrete Cosine Transformation
(DCT). Fourier Transformer is able to significantly reduce computational costs
while retain the ability to inherit from various large pretrained models.
Experiments show that our model achieves state-of-the-art performances among
all transformer-based models on the long-range modeling benchmark LRA with
significant improvement in both speed and space. For generative seq-to-seq
tasks including CNN/DailyMail and ELI5, by inheriting the BART weights our
model outperforms the standard BART and other efficient models. Our code is
publicly available at https://github.com/LUMIA-Group/FourierTransformer

</details>


### [61] [Can Authorship Attribution Models Distinguish Speakers in Speech Transcripts?](https://arxiv.org/pdf/2311.07564)
*Cristina Aggazzotti, Nicholas Andrews, Elizabeth Allyn Smith*

Main category: cs.CL

TL;DR: The paper introduces a new benchmark for speaker attribution in transcribed speech, addressing challenges like missing stylistic features and leveraging conversational patterns like filler words. It evaluates neural and non-neural models, showing limitations of text-based methods when topic control increases.


<details>
  <summary>Details</summary>
Motivation: To address the novel challenges of speaker attribution in transcribed speech, where traditional stylistic features (e.g., punctuation) are absent, but other patterns (e.g., filler words) may be informative.

Method: Proposes a benchmark using human-transcribed conversational speech, controlling for topic bias via conversation prompts and shared speakers. Evaluates neural and non-neural models, including fine-tuning on speech transcripts.

Result: Text-based models perform well in some settings but degrade with increased topic control. Fine-tuning on speech transcripts improves performance.

Conclusion: The benchmark highlights the need for specialized models for transcribed speech attribution, as text-based methods are insufficient under controlled topics.

Abstract: Authorship verification is the task of determining if two distinct writing
samples share the same author and is typically concerned with the attribution
of written text. In this paper, we explore the attribution of transcribed
speech, which poses novel challenges. The main challenge is that many stylistic
features, such as punctuation and capitalization, are not informative in this
setting. On the other hand, transcribed speech exhibits other patterns, such as
filler words and backchannels (e.g., 'um', 'uh-huh'), which may be
characteristic of different speakers. We propose a new benchmark for speaker
attribution focused on human-transcribed conversational speech transcripts. To
limit spurious associations of speakers with topic, we employ both conversation
prompts and speakers participating in the same conversation to construct
verification trials of varying difficulties. We establish the state of the art
on this new benchmark by comparing a suite of neural and non-neural baselines,
finding that although written text attribution models achieve surprisingly good
performance in certain settings, they perform markedly worse as conversational
topic is increasingly controlled. We present analyses of the impact of
transcription style on performance as well as the ability of fine-tuning on
speech transcripts to improve performance.

</details>


### [62] [COBIAS: Assessing the Contextual Reliability of Bias Benchmarks for Language Models](https://arxiv.org/pdf/2402.14889)
*Priyanshul Govil, Hemang Jain, Vamshi Krishna Bonagiri, Aman Chadha, Ponnurangam Kumaraguru, Manas Gaur, Sanorita Dey*

Main category: cs.CL

TL;DR: The paper introduces COBIAS, a contextual reliability framework, to better evaluate and mitigate biases in LLMs by considering contextual variance in biased statements.


<details>
  <summary>Details</summary>
Motivation: Current bias evaluation methods lack contextual considerations, leading to unreliable benchmarks. The paper aims to improve bias detection by incorporating context.

Method: Developed COBIAS to measure bias reliability based on contextual variance. Augmented 2,291 stereotyped statements with contextual data for evaluation.

Result: COBIAS aligns with human judgment (Spearman‚Äôs œÅ = 0.65) and improves bias benchmark reliability.

Conclusion: COBIAS enhances bias evaluation and mitigation by contextualizing biased statements, aiding in creating more reliable benchmarks.

Abstract: Large Language Models (LLMs) often inherit biases from the web data they are
trained on, which contains stereotypes and prejudices. Current methods for
evaluating and mitigating these biases rely on bias-benchmark datasets. These
benchmarks measure bias by observing an LLM's behavior on biased statements.
However, these statements lack contextual considerations of the situations they
try to present. To address this, we introduce a contextual reliability
framework, which evaluates model robustness to biased statements by considering
the various contexts in which they may appear. We develop the Context-Oriented
Bias Indicator and Assessment Score (COBIAS) to measure a biased statement's
reliability in detecting bias, based on the variance in model behavior across
different contexts. To evaluate the metric, we augmented 2,291 stereotyped
statements from two existing benchmark datasets by adding contextual
information. We show that COBIAS aligns with human judgment on the contextual
reliability of biased statements (Spearman's $\rho = 0.65, p = 3.4 * 10^{-60}$)
and can be used to create reliable benchmarks, which would assist bias
mitigation works.

</details>


### [63] [ViTextVQA: A Large-Scale Visual Question Answering Dataset for Evaluating Vietnamese Text Comprehension in Images](https://arxiv.org/pdf/2404.10652)
*Quan Van Nguyen, Dan Quang Tran, Huy Quang Pham, Thang Kien-Bao Nguyen, Nghia Hieu Nguyen, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen*

Main category: cs.CL

TL;DR: The paper introduces ViTextVQA, a Vietnamese dataset for scene text understanding in VQA, and proposes ViTextBLIP-2, a novel method combining OCR and LLMs for improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing VQA research overlooks scene text in images, which carries explicit information. The study aims to address this gap by focusing on Vietnamese text comprehension.

Method: Proposes ViTextBLIP-2, integrating a frozen Vision Transformer, SwinTextSpotter OCR, ViT5 LLM, and a trainable Q-Former for multimodal feature fusion.

Result: Experiments show the importance of OCR token processing order, leading to significant performance improvements on the ViTextVQA dataset.

Conclusion: The study advances VQA by emphasizing scene text understanding and provides a valuable dataset for future research.

Abstract: Visual Question Answerinng (VQA) is a complicated task that requires the
capability of simultaneously processing natural language and images. This task
was initially researched with a focus on developing methods to help machines
understand objects and scene contexts in images. However, some scene text that
carries explicit information about the full content of the image is not
mentioned. Along with the continuous development of the AI era, there have been
many studies on the reading comprehension ability of VQA models in the world.
Therefore, we introduce the first large-scale dataset in Vietnamese
specializing in the ability to understand scene text, we call it ViTextVQA
(\textbf{Vi}etnamese \textbf{Text}-based \textbf{V}isual \textbf{Q}uestion
\textbf{A}nswering dataset) which contains \textbf{over 16,000} images and
\textbf{over 50,000} questions with answers. To tackle this task efficiently,
we propose ViTextBLIP-2, an novel multimodal feature fusion Method, which
optimizes Vietnamese OCR-based VQA by integrating a frozen Vision Transformer,
SwinTextSpotter OCR, and ViT5 LLM with a trainable Q-Former for multimodal
feature fusion. Through experiments with various state-of-the-art models, we
uncover the significance of the order in which tokens in OCR text are processed
and selected to formulate answers. This finding helped us significantly improve
the performance of the baseline models on the ViTextVQA dataset. Our dataset is
available (https://github.com/minhquan6203/ViTextVQA-Dataset) for research
purposes.

</details>


### [64] [Towards Adapting Open-Source Large Language Models for Expert-Level Clinical Note Generation](https://arxiv.org/pdf/2405.00715)
*Hanyin Wang, Chufan Gao, Bolun Liu, Qiping Xu, Guleid Hussein, Mohamad El Labban, Kingsley Iheasirim, Hariprasad Korsapati, Chuck Outcalt, Jimeng Sun*

Main category: cs.CL

TL;DR: The study adapts the open-source LLaMA-2 13B model for clinical note generation, achieving physician-level quality with a new reinforcement learning method, DistillDirect, and outperforming physician notes in some aspects.


<details>
  <summary>Details</summary>
Motivation: Healthcare providers prefer small, locally-hosted models over proprietary LLMs due to privacy and cost concerns, necessitating domain-specific adaptations.

Method: Continued pre-training, supervised fine-tuning, and reinforcement learning (DistillDirect) with Gemini 1.0 Pro as the teacher model.

Result: LLaMA-Clinic generated notes rated as 'acceptable' or higher by 90.4% of physicians, with higher real-world readiness scores in some sections.

Conclusion: Pre-defining note formats is crucial for clinical note-generation tasks, and LLaMA-Clinic demonstrates the viability of open-source models in healthcare.

Abstract: Proprietary Large Language Models (LLMs) such as GPT-4 and Gemini have
demonstrated promising capabilities in clinical text summarization tasks.
However, due to patient data privacy concerns and computational costs, many
healthcare providers prefer using small, locally-hosted models over external
generic LLMs. This study presents a comprehensive domain- and task-specific
adaptation process for the open-source LLaMA-2 13 billion parameter model,
enabling it to generate high-quality clinical notes from outpatient
patient-doctor dialogues. Our process incorporates continued pre-training,
supervised fine-tuning, and reinforcement learning from both AI and human
feedback. We introduced a new approach, DistillDirect, for performing on-policy
reinforcement learning with Gemini 1.0 Pro as the teacher model. Our resulting
model, LLaMA-Clinic, can generate clinical notes comparable in quality to those
authored by physicians. In a blinded physician reader study, the majority
(90.4%) of individual evaluations rated the notes generated by LLaMA-Clinic as
"acceptable" or higher across all three criteria: real-world readiness,
completeness, and accuracy. In the more challenging "Assessment and Plan"
section, LLaMA-Clinic scored higher (4.2/5) in real-world readiness than
physician-authored notes (4.1/5). We highlight key considerations for future
clinical note-generation tasks, emphasizing the importance of pre-defining a
best-practice note format, rather than relying on LLMs to determine this for
clinical practice.

</details>


### [65] [Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching](https://arxiv.org/pdf/2406.06326)
*Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Yipeng Zhang, Haitao Mi, Helen Meng*

Main category: cs.CL

TL;DR: Self-Tuning, a learning framework inspired by the Feynman Technique, enhances LLMs' ability to acquire new knowledge from raw documents through self-teaching, outperforming existing methods in knowledge acquisition and retention.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with outdated information due to static training. Existing methods fail in knowledge extraction, prompting the need for a better approach.

Method: Self-Tuning uses self-supervised tasks (memorization, comprehension, self-reflection) to augment documents and improve knowledge acquisition.

Result: Experiments show Self-Tuning outperforms other methods in knowledge tasks and preserves prior knowledge effectively.

Conclusion: Self-Tuning is a promising framework for keeping LLMs current and improving their knowledge acquisition capabilities.

Abstract: Large language models (LLMs) often struggle to provide up-to-date information
due to their one-time training and the constantly evolving nature of the world.
To keep LLMs current, existing approaches typically involve continued
pre-training on new documents. However, they frequently face difficulties in
extracting stored knowledge. Motivated by the remarkable success of the Feynman
Technique in efficient human learning, we introduce Self-Tuning, a learning
framework aimed at improving an LLM's ability to effectively acquire new
knowledge from unseen raw documents through self-teaching. Specifically, we
develop a Self-Teaching strategy that augments the documents with a set of
knowledge-intensive tasks created in a self-supervised manner, focusing on
three crucial aspects: memorization, comprehension, and self-reflection.
Additionally, we introduce three Wiki-Newpages-2023-QA datasets to facilitate
an in-depth analysis of an LLM's knowledge acquisition ability concerning
memorization, extraction, and reasoning. Extensive experimental results on
various models, e.g., Llama2-7B reveal that Self-Tuning consistently exhibits
superior performance across all knowledge acquisition tasks and excels in
preserving previous knowledge.

</details>


### [66] [Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in Language Models](https://arxiv.org/pdf/2408.06518)
*Hila Gonen, Terra Blevins, Alisa Liu, Luke Zettlemoyer, Noah A. Smith*

Main category: cs.CL

TL;DR: The paper introduces 'semantic leakage,' a new phenomenon where language models leak irrelevant prompt information into outputs, and proposes methods to detect and measure it.


<details>
  <summary>Details</summary>
Motivation: To uncover and understand biases and unintended behaviors in language models, focusing on the newly identified issue of semantic leakage.

Method: Proposes an evaluation setting for detecting semantic leakage, curates a diverse test suite, and measures leakage in 13 flagship models.

Result: Significant semantic leakage is found in all tested models, including non-English languages and various settings.

Conclusion: Semantic leakage is a new type of bias affecting language model behavior, highlighting the need for further research and mitigation.

Abstract: Despite their wide adoption, the biases and unintended behaviors of language
models remain poorly understood. In this paper, we identify and characterize a
phenomenon never discussed before, which we call semantic leakage, where models
leak irrelevant information from the prompt into the generation in unexpected
ways. We propose an evaluation setting to detect semantic leakage both by
humans and automatically, curate a diverse test suite for diagnosing this
behavior, and measure significant semantic leakage in 13 flagship models. We
also show that models exhibit semantic leakage in languages besides English and
across different settings and generation scenarios. This discovery highlights
yet another type of bias in language models that affects their generation
patterns and behavior.

</details>


### [67] [Towards understanding evolution of science through language model series](https://arxiv.org/pdf/2409.09636)
*Junjie Dong, Zhuoqi Lyu, Qing Ke*

Main category: cs.CL

TL;DR: AnnualBERT is a series of language models for scientific text evolution, using whole-word tokens and progressive annual training, achieving strong performance in domain-specific tasks and link prediction.


<details>
  <summary>Details</summary>
Motivation: To capture the temporal evolution of scientific text and improve performance on scientific NLP tasks.

Method: Uses whole-word tokens, a base RoBERTa model pretrained on arXiv papers until 2008, and progressively trained models annually.

Result: Comparable performance in standard tasks, state-of-the-art in domain-specific tasks and link prediction, with insights into scientific discourse evolution.

Conclusion: AnnualBERT improves scientific text processing and provides insights into temporal scientific discourse.

Abstract: We introduce AnnualBERT, a series of language models designed specifically to
capture the temporal evolution of scientific text. Deviating from the
prevailing paradigms of subword tokenizations and "one model to rule them all",
AnnualBERT adopts whole words as tokens and is composed of a base RoBERTa model
pretrained from scratch on the full-text of 1.7 million arXiv papers published
until 2008 and a collection of progressively trained models on arXiv papers at
an annual basis. We demonstrate the effectiveness of AnnualBERT models by
showing that they not only have comparable performances in standard tasks but
also achieve state-of-the-art performances on domain-specific NLP tasks as well
as link prediction tasks in the arXiv citation network. We then utilize probing
tasks to quantify the models' behavior in terms of representation learning and
forgetting as time progresses. Our approach enables the pretrained models to
not only improve performances on scientific text processing tasks but also to
provide insights into the development of scientific discourse over time. The
series of the models is available at https://huggingface.co/jd445/AnnualBERTs.

</details>


### [68] [Divided by discipline? A systematic literature review on the quantification of online sexism and misogyny using a semi-automated approach](https://arxiv.org/pdf/2409.20204)
*Aditi Dutta, Susan Banducci, Chico Q. Camargo*

Main category: cs.CL

TL;DR: A systematic literature review synthesizes interdisciplinary research on detecting and addressing online sexism and misogyny, highlighting gaps and proposing future directions.


<details>
  <summary>Details</summary>
Motivation: Addressing the growing concern over gender-based discrimination online, contested definitions of sexism, and the need for interdisciplinary collaboration.

Method: Rigorous semi-automated systematic review process guided by PRISMA, synthesizing literature into five themes.

Result: Reveals disciplinary divides in conceptualizing sexism, gaps in intersectional and non-Western perspectives, and limited proactive design strategies.

Conclusion: Calls for interdisciplinary collaboration, intersectional approaches, and methodological rigor to advance research on online sexism and misogyny.

Abstract: Several computational tools have been developed to detect and identify
sexism, misogyny, and gender-based hate speech, particularly on online
platforms. These tools draw on insights from both social science and computer
science. Given the increasing concern over gender-based discrimination in
digital spaces, the contested definitions and measurements of sexism, and the
rise of interdisciplinary efforts to understand its online manifestations, a
systematic literature review is essential for capturing the current state and
trajectory of this evolving field. In this review, we make four key
contributions: (1) we synthesize the literature into five core themes:
definitions of sexism and misogyny, disciplinary divergences, automated
detection methods, associated challenges, and design-based interventions; (2)
we adopt an interdisciplinary lens, bridging theoretical and methodological
divides across disciplines; (3) we highlight critical gaps, including the need
for intersectional approaches, the under-representation of non-Western
languages and perspectives, and the limited focus on proactive design
strategies beyond text classification; and (4) we offer a methodological
contribution by applying a rigorous semi-automated systematic review process
guided by PRISMA, establishing a replicable standard for future work in this
domain. Our findings reveal a clear disciplinary divide in how sexism and
misogyny are conceptualized and measured. Through an evidence-based synthesis,
we examine how existing studies have attempted to bridge this gap through
interdisciplinary collaboration. Drawing on both social science theories and
computational modeling practices, we assess the strengths and limitations of
current methodologies. Finally, we outline key challenges and future directions
for advancing research on the detection and mitigation of online sexism and
misogyny.

</details>


### [69] [Training of Scaffolded Language Models with Language Supervision: A Survey](https://arxiv.org/pdf/2410.16392)
*Matthieu Lin, Jenny Sheng, Andrew Zhao, Shenzhi Wang, Yang Yue, Victor Shea Jay Huang, Huan Liu, Jun Liu, Gao Huang, Yong-Jin Liu*

Main category: cs.CL

TL;DR: The paper surveys scaffolded LMs, focusing on their design, optimization, and integration with tools, highlighting language supervision and real-time feedback for continuous improvement.


<details>
  <summary>Details</summary>
Motivation: To organize and analyze the literature on scaffolded LMs, emphasizing their semi-parametric nature and the role of language supervision in training and optimization.

Method: The survey examines scaffolded LMs as semi-parametric models, training non-parametric variables (prompts, tools, code) using language supervision and real-time feedback.

Result: Language-based optimization offers rich, interpretable objectives, mitigates issues like catastrophic forgetting, and supports closed-source models. Real-world applications include AI co-workers like Copilot.

Conclusion: Scaffolded LMs, trained with language supervision and real-time feedback, represent a promising paradigm for AI integration in mixed-autonomy settings.

Abstract: This survey organizes the intricate literature on the design and optimization
of emerging structures around post-trained LMs. We refer to this overarching
structure as scaffolded LMs and focus on LMs that are integrated into
multi-step processes with tools. We view scaffolded LMs as semi-parametric
models wherein we train non-parametric variables, including the prompt, tools,
and scaffold's code. In particular, they interpret instructions, use tools, and
receive feedback all in language. Recent works use an LM as an optimizer to
interpret language supervision and update non-parametric variables according to
intricate objectives. In this survey, we refer to this paradigm as training of
scaffolded LMs with language supervision. A key feature of non-parametric
training is the ability to learn from language. Parametric training excels in
learning from demonstration (supervised learning), exploration (reinforcement
learning), or observations (unsupervised learning), using well-defined loss
functions. Language-based optimization enables rich, interpretable, and
expressive objectives, while mitigating issues like catastrophic forgetting and
supporting compatibility with closed-source models. Furthermore, agents are
increasingly deployed as co-workers in real-world applications such as Copilot
in Office tools or software development. In these mixed-autonomy settings,
where control and decision-making are shared between human and AI, users point
out errors or suggest corrections. Accordingly, we discuss agents that
continuously improve by learning from this real-time, language-based feedback
and refer to this setting as streaming learning from language supervision.

</details>


### [70] [ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based Contrastive Framework](https://arxiv.org/pdf/2410.19453)
*Hengyuan Zhang, Chenming Shang, Sizhe Wang, Dongdong Zhang, Feng Yao, Renliang Sun, Yiyao Yu, Yujiu Yang, Furu Wei*

Main category: cs.CL

TL;DR: ShifCon, a Shift-based Contrastive framework, improves non-dominant language performance in LLMs by aligning representations with the dominant language subspace and using contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Address the performance gap between dominant (e.g., English) and non-dominant languages in LLMs due to imbalanced training data.

Method: Shift representations of non-dominant languages into the dominant language subspace, enrich them, and shift back before generation. Uses subspace distance and contrastive learning.

Result: Significant performance improvement for non-dominant languages, especially low-resource ones.

Conclusion: ShifCon effectively enhances multilingual LLM performance and provides insights for future research.

Abstract: Although fine-tuning Large Language Models (LLMs) with multilingual data can
rapidly enhance the multilingual capabilities of LLMs, they still exhibit a
performance gap between the dominant language (e.g., English) and non-dominant
ones due to the imbalance of training data across languages. To further enhance
the performance of non-dominant languages, we propose ShifCon, a Shift-based
Contrastive framework that aligns the internal forward process of other
languages toward that of the dominant one. Specifically, it shifts the
representations of non-dominant languages into the dominant language subspace,
allowing them to access relatively rich information encoded in the model
parameters. The enriched representations are then shifted back into their
original language subspace before generation. Moreover, we introduce a subspace
distance metric to pinpoint the optimal layer area for shifting representations
and employ multilingual contrastive learning to further enhance the alignment
of representations within this area. Experiments demonstrate that our ShifCon
framework significantly enhances the performance of non-dominant languages,
particularly for low-resource ones. Further analysis offers extra insights to
verify the effectiveness of ShifCon and propel future research

</details>


### [71] [How Good is Your Wikipedia? Auditing Data Quality for Low-resource and Multilingual NLP](https://arxiv.org/pdf/2411.05527)
*Kushal Tatariya, Artur Kulmizev, Wessel Poelman, Esther Ploeger, Marcel Bollmann, Johannes Bjerva, Jiaming Luo, Heather Lent, Miryam de Lhoneux*

Main category: cs.CL

TL;DR: The paper examines Wikipedia's data quality in non-English settings, revealing issues like one-line and duplicate articles. Quality filtering improves resource efficiency without harming performance, especially for low-resource languages.


<details>
  <summary>Details</summary>
Motivation: To scrutinize Wikipedia's assumed high quality in low-resource languages and assess the impact of quality filtering on multilingual NLP tasks.

Method: Subjecting Wikipedia to various quality filtering techniques and evaluating the downstream impact on performance.

Result: Quality filtering is effective for resource-efficient training without performance loss, particularly in low-resource languages.

Conclusion: Advocates for language- and task-specific data quality definitions and aims to guide Wikipedia's use in multilingual pretraining.

Abstract: Wikipedia's perceived high quality and broad language coverage have
established it as a fundamental resource in multilingual NLP. In the context of
low-resource languages, however, these quality assumptions are increasingly
being scrutinised. This paper critically examines the data quality of Wikipedia
in a non-English setting by subjecting it to various quality filtering
techniques, revealing widespread issues such as a high percentage of one-line
articles and duplicate articles. We evaluate the downstream impact of quality
filtering on Wikipedia and find that data quality pruning is an effective means
for resource-efficient training without hurting performance, especially for
low-resource languages. Moreover, we advocate for a shift in perspective from
seeking a general definition of data quality towards a more language- and
task-specific one. Ultimately, we aim for this study to serve as a guide to
using Wikipedia for pretraining in a multilingual setting.

</details>


### [72] [UniHR: Hierarchical Representation Learning for Unified Knowledge Graph Link Prediction](https://arxiv.org/pdf/2411.07019)
*Zhiqiang Liu, Yin Hua, Mingyang Chen, Zhuo Chen, Ziqi Liu, Lei Liang, Huajun Chen, Wen Zhang*

Main category: cs.CL

TL;DR: UniHR proposes a unified framework for link prediction in knowledge graphs, handling hyper-relational, temporal, and nested facts via hierarchical representation learning.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with hierarchical fact modeling and generalization across fact types due to complex representations.

Method: UniHR uses a Hierarchical Data Representation (HiDR) module to unify fact types and a Hierarchical Structure Learning (HiSL) module for intra- and inter-fact message passing.

Result: Empirical results show UniHR's effectiveness, emphasizing the potential of unified representations.

Conclusion: UniHR successfully addresses limitations of existing models, offering a scalable and generalizable solution for link prediction.

Abstract: Beyond-triple fact representations including hyper-relational facts with
auxiliary key-value pairs, temporal facts with additional timestamps, and
nested facts implying relationships between facts, are gaining significant
attention. However, constrained by complex fact representation forms, existing
link prediction models for beyond-triple facts have difficulty achieving
hierarchical fact modeling and generalizing the modules for one specific facts
to other fact types. To overcome this limitation, we propose a Unified
Hierarchical Representation learning framework (UniHR) for unified knowledge
graph link prediction. It consists of a unified Hierarchical Data
Representation (HiDR) module and a unified Hierarchical Structure Learning
(HiSL) module as graph encoder. The HiDR module unifies hyper-relational KGs,
temporal KGs, and nested factual KGs into triple-based representations. Then
HiSL incorporates intra-fact and inter-fact message passing, focusing on
enhancing the semantic information within individual facts and enriching the
structural information between facts. Empirical results demonstrate the
effectiveness of UniHR and highlight the strong potential of unified
representations. Code and data are available at
https://github.com/Lza12a/UniHR.

</details>


### [73] [AD-LLM: Benchmarking Large Language Models for Anomaly Detection](https://arxiv.org/pdf/2412.11142)
*Tiankai Yang, Yi Nian, Shawn Li, Ruiyao Xu, Yuangang Li, Jiaqi Li, Zhuo Xiao, Xiyang Hu, Ryan Rossi, Kaize Ding, Xia Hu, Yue Zhao*

Main category: cs.CL

TL;DR: The paper introduces AD-LLM, a benchmark for evaluating LLMs in NLP anomaly detection, covering zero-shot detection, data augmentation, and model selection. Results show LLMs' potential, with future research directions outlined.


<details>
  <summary>Details</summary>
Motivation: Despite LLMs' impact on NLP tasks like text generation, their role in anomaly detection (AD) is understudied. The paper aims to explore LLMs' potential in NLP AD tasks.

Method: The paper evaluates LLMs in three AD tasks: zero-shot detection, data augmentation, and model selection, using experiments across datasets.

Result: LLMs perform well in zero-shot AD, data augmentation helps, but model selection explanations are challenging.

Conclusion: The paper highlights LLMs' promise for AD and suggests six future research directions.

Abstract: Anomaly detection (AD) is an important machine learning task with many
real-world uses, including fraud detection, medical diagnosis, and industrial
monitoring. Within natural language processing (NLP), AD helps detect issues
like spam, misinformation, and unusual user activity. Although large language
models (LLMs) have had a strong impact on tasks such as text generation and
summarization, their potential in AD has not been studied enough. This paper
introduces AD-LLM, the first benchmark that evaluates how LLMs can help with
NLP anomaly detection. We examine three key tasks: (i) zero-shot detection,
using LLMs' pre-trained knowledge to perform AD without tasks-specific
training; (ii) data augmentation, generating synthetic data and category
descriptions to improve AD models; and (iii) model selection, using LLMs to
suggest unsupervised AD models. Through experiments with different datasets, we
find that LLMs can work well in zero-shot AD, that carefully designed
augmentation methods are useful, and that explaining model selection for
specific datasets remains challenging. Based on these results, we outline six
future research directions on LLMs for AD.

</details>


### [74] [When to Speak, When to Abstain: Contrastive Decoding with Abstention](https://arxiv.org/pdf/2412.12527)
*Hyuhng Joon Kim, Youna Kim, Sang-goo Lee, Taeuk Kim*

Main category: cs.CL

TL;DR: The paper introduces Contrastive Decoding with Abstention (CDA), a method to improve LLM robustness by allowing them to abstain when lacking relevant knowledge, enhancing reliability.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored challenge of LLMs lacking relevant knowledge in certain scenarios, aiming to improve robustness and reliability.

Method: Proposes CDA, a training-free decoding method that estimates knowledge relevance and adaptively prioritizes or excludes information.

Result: CDA effectively enables accurate generation and abstention, improving LLM reliability and user trust.

Conclusion: CDA enhances LLM robustness by handling knowledge gaps, ensuring reliable performance and maintaining trust.

Abstract: Large Language Models (LLMs) demonstrate exceptional performance across
diverse tasks by leveraging pre-trained (i.e., parametric) and external (i.e.,
contextual) knowledge. While substantial efforts have been made to enhance the
utilization of both forms of knowledge, situations in which models lack
relevant information remain underexplored. To investigate this challenge, we
first present a controlled testbed featuring four distinct knowledge access
scenarios, including the aforementioned edge case, revealing that conventional
LLM usage exhibits insufficient robustness in handling all instances.
Addressing this limitation, we propose Contrastive Decoding with Abstention
(CDA), a novel training-free decoding method that allows LLMs to generate
responses when relevant knowledge is available and to abstain otherwise. CDA
estimates the relevance of both knowledge sources for a given input, adaptively
deciding which type of information to prioritize and which to exclude. Through
extensive experiments, we demonstrate that CDA can effectively perform accurate
generation and abstention simultaneously, enhancing reliability and preserving
user trust.

</details>


### [75] [What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context](https://arxiv.org/pdf/2412.12632)
*Zhiyuan Chang, Mingyang Li, Xiaojun Jia, Junjie Wang, Yuekai Huang, Qing Wang, Yihao Huang, Yang Liu*

Main category: cs.CL

TL;DR: The paper proposes a Chain of Evidence (CoE) approach to improve LLMs' use of external knowledge in multi-hop QA, enhancing accuracy, faithfulness, and robustness.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of imperfect external knowledge (irrelevant or misinformation) in LLMs, which can impair reliability.

Method: Inspired by criminal procedural law's CoE, the approach ensures knowledge relevance and mutual support. An automated CoE discrimination method is proposed and tested on five LLMs.

Result: CoE improves generation accuracy, answer faithfulness, robustness to conflicts, and boosts RAG performance in three scenarios.

Conclusion: The CoE approach effectively enhances LLMs' reliability and performance in handling imperfect external knowledge for multi-hop QA.

Abstract: Incorporating external knowledge into large language models (LLMs) has
emerged as a promising approach to mitigate outdated knowledge and
hallucination in LLMs. However, external knowledge is often imperfect. In
addition to useful knowledge, external knowledge is rich in irrelevant or
misinformation in the context that can impair the reliability of LLM responses.
This paper focuses on LLMs' preferred external knowledge in imperfect contexts
when handling multi-hop QA. Inspired by criminal procedural law's Chain of
Evidence (CoE), we characterize that knowledge preferred by LLMs should
maintain both relevance to the question and mutual support among knowledge
pieces. Accordingly, we propose an automated CoE discrimination approach and
evaluate LLMs' effectiveness, faithfulness and robustness with CoE, including
its application in the Retrieval-Augmented Generation (RAG). Tests on five LLMs
show CoE improves generation accuracy, answer faithfulness, robustness to
knowledge conflicts, and boosts the performance of existing approaches in three
practical RAG scenarios.

</details>


### [76] [XRAG: eXamining the Core -- Benchmarking Foundational Components in Advanced Retrieval-Augmented Generation](https://arxiv.org/pdf/2412.15529)
*Qianren Mao, Yangyifei Luo, Qili Zhang, Yashuo Luo, Zhilong Cao, Jinlong Zhang, HanWen Hao, Zhijun Chen, Weifeng Jiang, Junnan Liu, Xiaolong Wang, Zhenting Huang, Zhixing Tan, Sun Jie, Bo Li, Xudong Liu, Richong Zhang, Jianxin Li*

Main category: cs.CL

TL;DR: XRAG is an open-source tool for evaluating and optimizing RAG systems, focusing on four core phases to improve performance and address failure points.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy and relevance of RAG systems by identifying and addressing their failure points through systematic evaluation.

Method: XRAG modularly evaluates RAG components across four phases (pre-retrieval, retrieval, post-retrieval, generation) using reconfigured datasets and diagnostic tests.

Result: Provides benchmarks and tailored solutions to improve RAG system performance by pinpointing and mitigating failure points.

Conclusion: XRAG offers a comprehensive framework for optimizing RAG systems, ensuring better contextual relevance and accuracy in generated outputs.

Abstract: Retrieval-augmented generation (RAG) synergizes the retrieval of pertinent
data with the generative capabilities of Large Language Models (LLMs), ensuring
that the generated output is not only contextually relevant but also accurate
and current. We introduce XRAG, an open-source, modular codebase that
facilitates exhaustive evaluation of the performance of foundational components
of advanced RAG modules. These components are systematically categorized into
four core phases: pre-retrieval, retrieval, post-retrieval, and generation. We
systematically analyse them across reconfigured datasets, providing a
comprehensive benchmark for their effectiveness. As the complexity of RAG
systems continues to escalate, we underscore the critical need to identify
potential failure points in RAG systems. We formulate a suite of experimental
methodologies and diagnostic testing protocols to dissect the failure points
inherent in RAG engineering. Subsequently, we proffer bespoke solutions aimed
at bolstering the overall performance of these modules. Our work thoroughly
evaluates the performance of advanced core components in RAG systems, providing
insights into optimizations for prevalent failure points.

</details>


### [77] [Dynamics of Adversarial Attacks on Large Language Model-Based Search Engines](https://arxiv.org/pdf/2501.00745)
*Xiyang Hu*

Main category: cs.CL

TL;DR: The paper studies ranking manipulation attacks in LLM-based search engines, framing the problem as an Infinitely Repeated Prisoners' Dilemma. It analyzes conditions for cooperation, identifies tipping points, and highlights paradoxes in defensive measures.


<details>
  <summary>Details</summary>
Motivation: To understand and mitigate vulnerabilities in LLM-based search engines, particularly ranking manipulation attacks, by analyzing strategic player behavior.

Method: Frames the problem as an Infinitely Repeated Prisoners' Dilemma, analyzing factors like attack costs, discount rates, and trigger strategies.

Result: Identifies conditions for cooperation, tipping points in dynamics, and paradoxes in defensive measures (e.g., reducing attack success rates may incentivize attacks).

Conclusion: Highlights the complexity of securing LLM-based systems and emphasizes adaptive security strategies and ecosystem design.

Abstract: The increasing integration of Large Language Model (LLM) based search engines
has transformed the landscape of information retrieval. However, these systems
are vulnerable to adversarial attacks, especially ranking manipulation attacks,
where attackers craft webpage content to manipulate the LLM's ranking and
promote specific content, gaining an unfair advantage over competitors. In this
paper, we study the dynamics of ranking manipulation attacks. We frame this
problem as an Infinitely Repeated Prisoners' Dilemma, where multiple players
strategically decide whether to cooperate or attack. We analyze the conditions
under which cooperation can be sustained, identifying key factors such as
attack costs, discount rates, attack success rates, and trigger strategies that
influence player behavior. We identify tipping points in the system dynamics,
demonstrating that cooperation is more likely to be sustained when players are
forward-looking. However, from a defense perspective, we find that simply
reducing attack success probabilities can, paradoxically, incentivize attacks
under certain conditions. Furthermore, defensive measures to cap the upper
bound of attack success rates may prove futile in some scenarios. These
insights highlight the complexity of securing LLM-based systems. Our work
provides a theoretical foundation and practical insights for understanding and
mitigating their vulnerabilities, while emphasizing the importance of adaptive
security strategies and thoughtful ecosystem design.

</details>


### [78] [LLM Content Moderation and User Satisfaction: Evidence from Response Refusals in Chatbot Arena](https://arxiv.org/pdf/2501.03266)
*Stefan Pasch*

Main category: cs.CL

TL;DR: The study explores user dissatisfaction with LLM refusals, finding ethical refusals penalize satisfaction more than technical ones, but context and phrasing can mitigate this.


<details>
  <summary>Details</summary>
Motivation: To understand how users react when LLMs refuse prompts, especially for ethical reasons, and the impact on satisfaction.

Method: Analyzed 50,000 model comparisons from Chatbot Arena, using a RoBERTa-based classifier to distinguish ethical from technical refusals.

Result: Ethical refusals have lower win rates than technical refusals or standard responses, but sensitivity and phrasing improve evaluations.

Conclusion: Safety-aligned behaviors may clash with user expectations, suggesting adaptive moderation strategies are needed.

Abstract: LLM safety and ethical alignment are widely discussed, but the impact of
content moderation on user satisfaction remains underexplored. In particular,
little is known about how users respond when models refuse to answer a
prompt-one of the primary mechanisms used to enforce ethical boundaries in
LLMs. We address this gap by analyzing nearly 50,000 model comparisons from
Chatbot Arena, a platform where users indicate their preferred LLM response in
pairwise matchups, providing a large-scale setting for studying real-world user
preferences. Using a novel RoBERTa-based refusal classifier fine-tuned on a
hand-labeled dataset, we distinguish between refusals due to ethical concerns
and technical limitations. Our results reveal a substantial refusal penalty:
ethical refusals yield significantly lower win rates than both technical
refusals and standard responses, indicating that users are especially
dissatisfied when models decline a task for ethical reasons. However, this
penalty is not uniform. Refusals receive more favorable evaluations when the
underlying prompt is highly sensitive (e.g., involving illegal content), and
when the refusal is phrased in a detailed and contextually aligned manner.
These findings underscore a core tension in LLM design: safety-aligned
behaviors may conflict with user expectations, calling for more adaptive
moderation strategies that account for context and presentation.

</details>


### [79] [TreeKV: Smooth Key-Value Cache Compression with Tree Structures](https://arxiv.org/pdf/2501.04987)
*Ziwei He, Jian Yuan, Haoli Bai, Jingwen Leng, Bo Jiang*

Main category: cs.CL

TL;DR: TreeKV is a training-free method for efficient KV cache compression in LLMs, using a tree structure to maintain context quality in long sequences, outperforming baselines with significant cache reduction.


<details>
  <summary>Details</summary>
Motivation: Existing KV cache compression methods suffer from regional biases or miss crucial information, limiting LLM performance in long sequences. Wavelet analysis shows tokens near sequence ends contribute more, inspiring TreeKV.

Method: TreeKV employs a tree structure for smooth cache compression, maintaining a fixed cache size and working in both generation and prefilling stages.

Result: TreeKV outperforms baselines on PG19, OpenWebText2, and Longbench, achieving high performance with 16x cache reduction and 6% budget at optimal efficiency.

Conclusion: TreeKV is an effective, intuitive solution for KV cache compression, enabling LLMs to handle long sequences efficiently without training.

Abstract: Efficient key-value (KV) cache compression is critical for scaling
transformer-based Large Language Models (LLMs) in long sequences and
resource-limited settings. Existing methods evict tokens based on their
positions or importance scores, but position-based strategies can miss crucial
information outside predefined regions, while those relying on global
importance scores resulting in strong regional biases, limiting the KV cache's
overall context retention and potentially impairing the performance of LLMs on
complex tasks. Our wavelet analysis reveals that as tokens approach the end of
sequence, their contributions to generation gradually increase and tends to
diverge more from neighboring tokens, indicating a smooth transition with
increasing complexity and variability from distant to nearby context. Motivated
by this observation, we propose TreeKV, an intuitive, training-free method that
employs a tree structure for smooth cache compression. TreeKV maintains a fixed
cache size, allowing LLMs to deliver high-quality output even in long text
scenarios. Unlike most compression methods, TreeKV is applicable to both the
generation and prefilling stages. TreeKV consistently surpasses all baseline
models in language modeling tasks on PG19 and OpenWebText2, allowing LLMs
trained with short context window to generalize to longer window with a 16x
cache reduction. On the Longbench benchmark, TreeKV achieves the best
performance with only 6\% of the budget at optimal efficiency.

</details>


### [80] [Know Your Mistakes: Towards Preventing Overreliance on Task-Oriented Conversational AI Through Accountability Modeling](https://arxiv.org/pdf/2501.10316)
*Suvodip Dey, Yi-Jyun Sun, Gokhan Tur, Dilek Hakkani-Tur*

Main category: cs.CL

TL;DR: The paper proposes an accountability model for LLM-based task-oriented dialogue agents to reduce user overreliance by introducing friction turns during model uncertainty or errors, improving dialogue state tracking accuracy.


<details>
  <summary>Details</summary>
Motivation: LLMs often hallucinate, leading users to over-rely on incorrect AI suggestions. The paper aims to mitigate this by adding accountability mechanisms.

Method: An augmented LLM with an accountability head (binary classifier) predicts relevant dialogue state slots. Experiments use MultiWOZ and Snips benchmarks with multiple LLMs.

Result: The model improves joint goal accuracy (JGA) by ~3%, with self-correction further boosting JGA to 70.51. User confirmations (friction turns) also enhance performance.

Conclusion: The accountability model effectively reduces user overreliance and improves DST accuracy, achieving state-of-the-art results.

Abstract: Recent LLMs have enabled significant advancements for conversational agents.
However, they are also well known to hallucinate, producing responses that seem
plausible but are factually incorrect. On the other hand, users tend to
over-rely on LLM-based AI agents, accepting AI's suggestion even when it is
wrong. Adding positive friction, such as explanations or getting user
confirmations, has been proposed as a mitigation in AI-supported
decision-making systems. In this paper, we propose an accountability model for
LLM-based task-oriented dialogue agents to address user overreliance via
friction turns in cases of model uncertainty and errors associated with
dialogue state tracking (DST). The accountability model is an augmented LLM
with an additional accountability head that functions as a binary classifier to
predict the relevant slots of the dialogue state mentioned in the conversation.
We perform our experiments with multiple backbone LLMs on two established
benchmarks (MultiWOZ and Snips). Our empirical findings demonstrate that the
proposed approach not only enables reliable estimation of AI agent errors but
also guides the decoder in generating more accurate actions. We observe around
3% absolute improvement in joint goal accuracy (JGA) of DST output by
incorporating accountability heads into modern LLMs. Self-correcting the
detected errors further increases the JGA from 67.13 to 70.51, achieving
state-of-the-art DST performance. Finally, we show that error correction
through user confirmations (friction turn) achieves a similar performance gain,
highlighting its potential to reduce user overreliance.

</details>


### [81] [Med-R$^2$: Crafting Trustworthy LLM Physicians via Retrieval and Reasoning of Evidence-Based Medicine](https://arxiv.org/pdf/2501.11885)
*Keer Lu, Zheng Liang, Zhuoran Zhang, Da Pan, Shusen Zhang, Xin Wu, Zenan Zhou, Guosheng Dong, Bin Cui, Tengjiao Wang, Wentao Zhang*

Main category: cs.CL

TL;DR: Med-R^2 is a novel LLM framework for healthcare, integrating retrieval and reasoning to outperform traditional methods without extra training costs.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs in medical settings face high costs, outdated data, and poor retrieval precision, limiting their effectiveness.

Method: Med-R^2 follows the Evidence-Based Medicine process, combining retrieval, evidence selection, and reasoning.

Result: Med-R^2 improves performance by 14.74% over RAG and 3.32% over fine-tuning, without added costs.

Conclusion: Med-R^2 enhances LLM proficiency in healthcare, offering a cost-effective and trustworthy solution.

Abstract: Large Language Models (LLMs) have exhibited remarkable capabilities in
clinical scenarios. Despite their potential, existing works face challenges
when applying LLMs to medical settings. Strategies relying on training with
medical datasets are highly cost-intensive and may suffer from outdated
training data. Leveraging external knowledge bases is a suitable alternative,
yet it faces obstacles such as limited retrieval precision and poor
effectiveness in answer extraction. These issues collectively prevent LLMs from
demonstrating the expected level of proficiency in mastering medical expertise.
To address these challenges, we introduce Med-R^2, a novel LLM physician
framework that adheres to the Evidence-Based Medicine (EBM) process,
efficiently integrating retrieval mechanisms as well as the selection and
reasoning processes of evidence, thereby enhancing the problem-solving
capabilities of LLMs in healthcare scenarios and fostering a trustworthy LLM
physician. Our comprehensive experiments indicate that Med-R^2 achieves a
14.74\% improvement over vanilla RAG methods and even a 3.32\% enhancement
compared to fine-tuning strategies, without incurring additional training
costs.

</details>


### [82] [Re-ranking Using Large Language Models for Mitigating Exposure to Harmful Content on Social Media Platforms](https://arxiv.org/pdf/2501.13977)
*Rajvardhan Oak, Muhammad Haroon, Claire Jo, Magdalena Wojcieszak, Anshuman Chhabra*

Main category: cs.CL

TL;DR: A novel LLM-based re-ranking method reduces harmful content exposure on social media without needing extensive labeled data, outperforming existing moderation approaches.


<details>
  <summary>Details</summary>
Motivation: Current ML/AI moderation struggles with scalability and adapting to new harms due to reliance on human-annotated data.

Method: Proposes a zero-shot and few-shot LLM-based re-ranking approach to dynamically assess and re-rank content sequences.

Result: Outperforms proprietary moderation in experiments across datasets, models, and configurations.

Conclusion: The LLM-based method offers a scalable, adaptable solution for mitigating harmful content exposure.

Abstract: Social media platforms utilize Machine Learning (ML) and Artificial
Intelligence (AI) powered recommendation algorithms to maximize user
engagement, which can result in inadvertent exposure to harmful content.
Current moderation efforts, reliant on classifiers trained with extensive
human-annotated data, struggle with scalability and adapting to new forms of
harm. To address these challenges, we propose a novel re-ranking approach using
Large Language Models (LLMs) in zero-shot and few-shot settings. Our method
dynamically assesses and re-ranks content sequences, effectively mitigating
harmful content exposure without requiring extensive labeled data. Alongside
traditional ranking metrics, we also introduce two new metrics to evaluate the
effectiveness of re-ranking in reducing exposure to harmful content. Through
experiments on three datasets, three models and across three configurations, we
demonstrate that our LLM-based approach significantly outperforms existing
proprietary moderation approaches, offering a scalable and adaptable solution
for harm mitigation.

</details>


### [83] [Do we really have to filter out random noise in pre-training data for language models?](https://arxiv.org/pdf/2502.06604)
*Jinghan Ru, Yuxin Xie, Xianwei Zhuang, Yuguo Yin, Zhihui Guo, Zhiming Liu, Qianli Ren, Yuexian Zou*

Main category: cs.CL

TL;DR: The paper investigates random noise in web-scale pre-training datasets for LLMs, revealing its minimal impact on next-token prediction loss but potential harm to downstream tasks. It introduces a Local Gradient Matching loss to mitigate adverse effects.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of random noise in pre-training datasets on LLMs, especially its unexpected minimal effect on next-token prediction loss and its potential degradation of downstream task performance.

Method: A cohesive 'What-Why-How' framework is used to systematically study random noise. A novel Local Gradient Matching loss is introduced to enhance denoising capability for downstream tasks.

Result: Random noise minimally affects next-token prediction loss even in large models (2.7B), but harms downstream performance. The proposed loss improves denoising without needing model parameters.

Conclusion: Random noise in pre-training data has nuanced effects, requiring targeted solutions like Local Gradient Matching to preserve downstream task performance.

Abstract: Web-scale pre-training datasets are the cornerstone of LLMs' success.
However, text data curated from the Internet inevitably contains random noise
caused by decoding errors or unregulated web content. In contrast to previous
works that focus on low quality or synthetic data, our study \textbf{provides
the first systematic investigation of such random noise through a cohesive
``What-Why-How'' framework.} Surprisingly, we observed that the resulting
increase in the loss of next-token prediction (NTP) was significantly lower
than the proportion of random noise even when the model was scaled up to 2.7B.
We provide a theoretical justification for this phenomenon, which also
elucidates the success of multilingual models and can be applied to multimodal
models. On the other hand, experiments show that the model's performance in
downstream tasks is not based solely on the NTP loss, which means that random
noise may result in degraded downstream performance. To address the potential
adverse effects, we introduce a novel plug-and-play Local Gradient Matching
loss, which explicitly enhances the denoising capability of the downstream task
head by aligning the gradient of normal and perturbed features without
requiring knowledge of the model's parameters. Additional experiments on 8
language and 14 vision benchmarks further validate its effectiveness.

</details>


### [84] [Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and Harmlessness of Large Language Model via Model Merging](https://arxiv.org/pdf/2502.06876)
*Jinluan Yang, Dingnan Jin, Anke Tang, Li Shen, Didi Zhu, Zhengyu Chen, Ziyu Zhao, Daixin Wang, Qing Cui, Zhiqiang Zhang, Jun Zhou, Fei Wu, Kun Kuang*

Main category: cs.CL

TL;DR: The paper compares model merging and data mixture methods for aligning large language models (LLMs) with Helpfulness, Honesty, and Harmlessness (3H). It introduces RESM, a novel merging method, showing improved performance over existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3H alignment rely heavily on expert knowledge and face optimization conflicts. The potential of model merging for 3H optimization is underexplored.

Method: Proposes RESM, a reweighting-enhanced task singular merging method, using outlier weighting and sparsity-aware rank selection to address noise and sparsity in 3H-aligned LLM merging.

Result: RESM outperforms data mixture (2%-5% gain) and model merging (1%-3% gain) methods, demonstrating effectiveness and robustness in balanced LLM alignment.

Conclusion: Model merging, especially with RESM, offers a promising approach for 3H alignment, addressing limitations of data mixture methods. The study highlights collaborative and conflict relationships among 3H dimensions.

Abstract: Achieving balanced alignment of large language models (LLMs) in terms of
Helpfulness, Honesty, and Harmlessness (3H optimization) constitutes a
cornerstone of responsible AI. Existing methods like data mixture strategies
face limitations, including heavy reliance on expert knowledge and conflicting
optimization signals. While model merging offers parameter-level
conflict-resolution strategies through integrating specialized models'
parameters, its potential for 3H optimization remains underexplored. This paper
systematically compares the effectiveness of model merging and data mixture
methods in constructing 3H-aligned LLMs for the first time, revealing
previously overlooked collaborative and conflict relationships among the 3H
dimensions and discussing the advantages and drawbacks of data mixture
(\textit{data-level}) and model merging (\textit{parameter-level}) methods in
mitigating the conflict for balanced 3H optimization. Specially, we propose a
novel \textbf{R}eweighting \textbf{E}nhanced task \textbf{S}ingular
\textbf{M}erging method, \textbf{RESM}, through outlier weighting and
sparsity-aware rank selection strategies to address the challenges of
preference noise accumulation and layer sparsity adaptation inherent in
3H-aligned LLM merging. Extensive evaluations can verify the effectiveness and
robustness of RESM compared to previous data mixture (2\%-5\% gain) and model
merging (1\%-3\% gain) methods in achieving balanced LLM alignment. We release
our models through \href{https://huggingface.co/Jinluan}{3H\_Merging} for
further investigations.

</details>


### [85] [Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More](https://arxiv.org/pdf/2502.07490)
*Xialie Zhuang, Zhikai Jia, Jianjin Li, Zhenyu Zhang, Li Shen, Zheng Cao, Shiwei Liu*

Main category: cs.CL

TL;DR: MEAP integrates MLM into NTP to improve LLMs' retrieval and reasoning, outperforming NTP without extra computational cost.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with accurate key information retrieval; MEAP aims to enhance this capability.

Method: MEAP randomly masks input tokens and uses NTP with a decoder-only Transformer, avoiding bidirectional attention.

Result: MEAP outperforms NTP in retrieval and reasoning tasks, especially in lost-in-the-middle scenarios (+11.77%).

Conclusion: MEAP is an effective training paradigm for LLMs, improving focus on relevant signals.

Abstract: Large Language Models (LLMs) are discovered to suffer from accurately
retrieving key information. To address this, we propose Mask-Enhanced
Autoregressive Prediction (MEAP), a simple yet effective training paradigm that
seamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction
(NTP) to enhance the latter's in-context retrieval capabilities. Specifically,
MEAP first randomly masks a small fraction of input tokens and then directly
performs the standard next-token prediction autoregressive using a decoder-only
Transformer. MEAP eliminates the need for bidirectional attention or
encoder-decoder architectures for MLM, incurring no additional computational
overhead during pre-training or inference. Intensive experiments demonstrate
that MEAP substantially outperforms NTP on key information retrieval and
long-context reasoning tasks, while performing on par or better on commonsense
reasoning tasks. The benefits of MEAP also extend to supervised fine-tuning,
where it shows remarkable advantages in lost-in-the-middle scenarios,
outperforming NTP by 11.77 percentage points. Our analysis indicates that
MEAP's effectiveness arises from its ability to promote more distinguishable
attention scores by concentrating on a reduced set of non-masked tokens. This
mechanism improves the model's focus on task-relevant signals while mitigating
the influence of peripheral context. These findings position MEAP as a
promising training paradigm for large language models.

</details>


### [86] [Hallucination, Monofacts, and Miscalibration: An Empirical Investigation](https://arxiv.org/pdf/2502.08666)
*Miranda Muqing Miao, Michael Kearns*

Main category: cs.CL

TL;DR: The paper explores the relationship between hallucinated facts in LLMs, monofact rates, and model miscalibration, introducing selective upweighting to reduce hallucination by up to 40%.


<details>
  <summary>Details</summary>
Motivation: To empirically investigate the statistical lower bound of hallucinated facts in LLMs and propose practical interventions to mitigate them.

Method: Uses n-gram models and fine-tuned Transformers, controls monofact rates via Pareto distributions, and introduces selective upweighting to inject miscalibration.

Result: Selective upweighting reduces hallucination by up to 40% without sacrificing accuracy, unlike standard training.

Conclusion: The study highlights a trade-off between accuracy and hallucination, advocating for targeted interventions like selective upweighting.

Abstract: Hallucinated facts in large language models (LLMs) have recently been shown
to obey a statistical lower bound determined by the monofact rate (related to
the classical Good-Turing missing mass estimator) minus model miscalibration
(Kalai & Vempala, 2024). We present the first empirical investigation of this
three-way relationship in classical n-gram models and fine-tuned
encoder-decoder Transformers. By generating training data from Pareto
distributions with varying shape parameters, we systematically control the
monofact rates and establish its positive relationship with hallucination. To
bridge theory and practice, we derive an empirical analog of the hallucination
bound by replacing the population miscalibration term (Section 2.1) with an
empirical bin-wise KL divergence and confirm its practical viability. We then
introduce selective upweighting -- a simple yet effective technique that
strategically repeats as little as 5% of training examples -- to deliberately
inject miscalibration into the model. This intervention reduces hallucination
by up to 40%, challenging universal deduplication policies. Our experiments
reveal a critical trade-off: selective upweighting maintains pre-injection
levels of accuracy while substantially reducing hallucination, whereas standard
training gradually improves accuracy but fails to address persistently high
hallucination, indicating an inherent tension in optimization objectives.

</details>


### [87] [Investigating Language Preference of Multilingual RAG Systems](https://arxiv.org/pdf/2502.11175)
*Jeonghyun Park, Hwanhee Lee*

Main category: cs.CL

TL;DR: The paper investigates language preferences in multilingual RAG systems, identifies inconsistencies, and proposes DKM-RAG to improve performance by fusing translated passages with model knowledge.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of language preferences and inconsistencies in multilingual RAG systems, which hinder retrieval and generation performance.

Method: Systematic experiments to analyze language preferences in retrieval and generation, followed by proposing DKM-RAG, a framework combining translated multilingual passages with model knowledge.

Result: Retrievers favor high-resource and query languages, while generators prefer query languages or Latin scripts, leading to inconsistencies. DKM-RAG mitigates these issues and improves performance.

Conclusion: DKM-RAG effectively addresses language preference biases and enhances multilingual RAG performance across diverse linguistic settings.

Abstract: Multilingual Retrieval-Augmented Generation (mRAG) systems enhance language
models by integrating external multilingual information to produce
context-aware responses. However, mRAG systems struggle with retrieving
relevant information due to linguistic variations between queries and
documents, generating inconsistent responses when multilingual sources
conflict. In this work, we systematically investigate language preferences in
both retrieval and generation of mRAG through a series of experiments. Our
analysis indicates that retrievers tend to prefer high-resource and query
languages, yet this preference does not consistently improve generation
performance. Moreover, we observe that generators prefer the query language or
Latin scripts, leading to inconsistent outputs. To overcome these issues, we
propose Dual Knowledge Multilingual RAG (DKM-RAG), a simple yet effective
framework that fuses translated multilingual passages with complementary model
knowledge. Empirical results demonstrate that DKM-RAG mitigates language
preference in generation and enhances performance across diverse linguistic
settings.

</details>


### [88] [Can Your Uncertainty Scores Detect Hallucinated Entity?](https://arxiv.org/pdf/2502.11948)
*Min-Hsuan Yeh, Max Kamachee, Seongheon Park, Yixuan Li*

Main category: cs.CL

TL;DR: The paper introduces HalluEntity, a dataset for entity-level hallucination detection in LLMs, evaluates uncertainty-based methods, and identifies linguistic trends in hallucinations.


<details>
  <summary>Details</summary>
Motivation: Current hallucination detection methods lack granularity, especially for long-form outputs mixing accurate and fabricated information.

Method: Proposes HalluEntity dataset for entity-level annotation and evaluates 17 LLMs using uncertainty-based approaches.

Result: Token-probability methods over-predict hallucinations, while context-aware methods perform better but suboptimally. Linguistic properties influence hallucination tendencies.

Conclusion: Highlights the need for improved entity-level hallucination detection and suggests future research directions.

Abstract: To mitigate the impact of hallucination nature of LLMs, many studies propose
detecting hallucinated generation through uncertainty estimation. However,
these approaches predominantly operate at the sentence or paragraph level,
failing to pinpoint specific spans or entities responsible for hallucinated
content. This lack of granularity is especially problematic for long-form
outputs that mix accurate and fabricated information. To address this
limitation, we explore entity-level hallucination detection. We propose a new
data set, HalluEntity, which annotates hallucination at the entity level. Based
on the dataset, we comprehensively evaluate uncertainty-based hallucination
detection approaches across 17 modern LLMs. Our experimental results show that
uncertainty estimation approaches focusing on individual token probabilities
tend to over-predict hallucinations, while context-aware methods show better
but still suboptimal performance. Through an in-depth qualitative study, we
identify relationships between hallucination tendencies and linguistic
properties and highlight important directions for future research. HalluEntity:
https://huggingface.co/datasets/samuelyeh/HalluEntity

</details>


### [89] [iAgent: LLM Agent as a Shield between User and Recommender Systems](https://arxiv.org/pdf/2502.14662)
*Wujiang Xu, Yunxiao Shi, Zujie Liang, Xuying Ning, Kai Mei, Kun Wang, Xi Zhu, Min Xu, Yongfeng Zhang*

Main category: cs.CL

TL;DR: The paper critiques traditional recommender systems for prioritizing platform benefits over user interests, proposes a new user-agent-platform paradigm to protect users.


<details>
  <summary>Details</summary>
Motivation: Traditional recommender systems often prioritize commercial goals and overlook individual user preferences, leading to vulnerabilities like lack of control, manipulation, and echo chambers.

Method: Introduces a user-agent-platform paradigm, where an agent acts as a protective intermediary between users and the platform.

Result: The proposed paradigm aims to mitigate issues like lack of personalization and platform manipulation by enabling indirect user exposure.

Conclusion: The new paradigm addresses core flaws in traditional systems, offering better protection and alignment with user interests.

Abstract: Traditional recommender systems usually take the user-platform paradigm,
where users are directly exposed under the control of the platform's
recommendation algorithms. However, the defect of recommendation algorithms may
put users in very vulnerable positions under this paradigm. First, many
sophisticated models are often designed with commercial objectives in mind,
focusing on the platform's benefits, which may hinder their ability to protect
and capture users' true interests. Second, these models are typically optimized
using data from all users, which may overlook individual user's preferences.
Due to these shortcomings, users may experience several disadvantages under the
traditional user-platform direct exposure paradigm, such as lack of control
over the recommender system, potential manipulation by the platform, echo
chamber effects, or lack of personalization for less active users due to the
dominance of active users during collaborative learning. Therefore, there is an
urgent need to develop a new paradigm to protect user interests and alleviate
these issues. Recently, some researchers have introduced LLM agents to simulate
user behaviors, these approaches primarily aim to optimize platform-side
performance, leaving core issues in recommender systems unresolved. To address
these limitations, we propose a new user-agent-platform paradigm, where agent
serves as the protective shield between user and recommender system that
enables indirect exposure.

</details>


### [90] [Unveiling Attractor Cycles in Large Language Models: A Dynamical Systems View of Successive Paraphrasing](https://arxiv.org/pdf/2502.15208)
*Zhilin Wang, Yafu Li, Jianhao Yan, Yu Cheng, Yue Zhang*

Main category: cs.CL

TL;DR: Successive paraphrasing with LLMs converges to stable periodic states (e.g., 2-period attractor cycles), limiting linguistic diversity due to self-reinforcing textual preferences.


<details>
  <summary>Details</summary>
Motivation: To analyze the long-term behaviors of LLMs using dynamical systems theory, focusing on paraphrasing as a testbed.

Method: Apply dynamical systems theory to LLMs, using successive paraphrasing to observe convergence to attractor cycles.

Result: LLMs converge to stable periodic states (e.g., 2-period cycles) in paraphrasing, reducing linguistic diversity.

Conclusion: LLMs exhibit inherent generative constraints, but dynamical systems theory offers a novel perspective for studying their expressive potential.

Abstract: Dynamical systems theory provides a framework for analyzing iterative
processes and evolution over time. Within such systems, repetitive
transformations can lead to stable configurations, known as attractors,
including fixed points and limit cycles. Applying this perspective to large
language models (LLMs), which iteratively map input text to output text,
provides a principled approach to characterizing long-term behaviors.
Successive paraphrasing serves as a compelling testbed for exploring such
dynamics, as paraphrases re-express the same underlying meaning with linguistic
variation. Although LLMs are expected to explore a diverse set of paraphrases
in the text space, our study reveals that successive paraphrasing converges to
stable periodic states, such as 2-period attractor cycles, limiting linguistic
diversity. This phenomenon is attributed to the self-reinforcing nature of
LLMs, as they iteratively favour and amplify certain textual forms over others.
This pattern persists with increasing generation randomness or alternating
prompts and LLMs. These findings underscore inherent constraints in LLM
generative capability, while offering a novel dynamical systems perspective for
studying their expressive potential.

</details>


### [91] [Call for Rigor in Reporting Quality of Instruction Tuning Data](https://arxiv.org/pdf/2503.04807)
*Hyeonseok Moon, Jaehyung Seo, Heuiseok Lim*

Main category: cs.CL

TL;DR: The paper highlights issues with arbitrary hyperparameter selection in instruction tuning (IT) for LLMs, showing it can lead to unreliable conclusions about IT data quality.


<details>
  <summary>Details</summary>
Motivation: To address the lack of justification in hyperparameter selection for IT studies, which can skew evaluations of IT data quality.

Method: Conducted experiments using LIMA data and 1,000 Alpaca data points to analyze the impact of hyperparameter choices.

Result: Found that arbitrary hyperparameter decisions can lead to any arbitrary conclusion about IT data quality.

Conclusion: Emphasizes the need for careful hyperparameter selection to ensure reliable evaluation of IT data quality.

Abstract: Instruction tuning is crucial for adapting large language models (LLMs) to
align with user intentions. Numerous studies emphasize the significance of the
quality of instruction tuning (IT) data, revealing a strong correlation between
IT data quality and the alignment performance of LLMs. In these studies, the
quality of IT data is typically assessed by evaluating the performance of LLMs
trained with that data. However, we identified a prevalent issue in such
practice: hyperparameters for training models are often selected arbitrarily
without adequate justification. We observed significant variations in
hyperparameters applied across different studies, even when training the same
model with the same data. In this study, we demonstrate the potential problems
arising from this practice and emphasize the need for careful consideration in
verifying data quality. Through our experiments on the quality of LIMA data and
a selected set of 1,000 Alpaca data points, we demonstrate that arbitrary
hyperparameter decisions can make any arbitrary conclusion.

</details>


### [92] [TigerLLM -- A Family of Bangla Large Language Models](https://arxiv.org/pdf/2503.10995)
*Nishat Raihan, Marcos Zampieri*

Main category: cs.CL

TL;DR: TigerLLM is introduced as a family of Bangla LLMs, outperforming existing open-source and proprietary models like GPT3.5, setting a new benchmark for Bangla language modeling.


<details>
  <summary>Details</summary>
Motivation: Addressing the linguistic disparity in LLM development, particularly for Bangla, which lacks high-performance open-source models.

Method: Development of TigerLLM, a family of Bangla LLMs.

Result: TigerLLM surpasses all open-source alternatives and outperforms larger proprietary models like GPT3.5 on standard benchmarks.

Conclusion: TigerLLM establishes a new baseline for future Bangla language modeling.

Abstract: The development of Large Language Models (LLMs) remains heavily skewed
towards English and a few other high-resource languages. This linguistic
disparity is particularly evident for Bangla - the 5th most spoken language. A
few initiatives attempted to create open-source Bangla LLMs with performance
still behind high-resource languages and limited reproducibility. To address
this gap, we introduce TigerLLM - a family of Bangla LLMs. Our results
demonstrate that these models surpass all open-source alternatives and also
outperform larger proprietary models like GPT3.5 across standard benchmarks,
establishing TigerLLM as the new baseline for future Bangla language modeling.

</details>


### [93] [KVShare: An LLM Service System with Efficient and Effective Multi-Tenant KV Cache Reuse](https://arxiv.org/pdf/2503.16525)
*Huan Yang, Renji Zhang, Mingzhe Huang, Weijun Wang, Yin Tang, Yuanchun Li, Yunxin Liu, Deyu Zhang*

Main category: cs.CL

TL;DR: KVShare, a KV cache management module, improves LLM serving efficiency by reusing KV caches across requests, reducing TTFT by 9.39x and boosting throughput by 1.2x while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the high computational costs and slow TTFT in long-text LLMs by enabling efficient KV cache reuse across requests without sacrificing accuracy.

Method: Introduces KVShare with a Dual-Stage High Deviation algorithm (DHD) for selective KV cache recomputation and a cache-aware scheduler for request prioritization and batching.

Result: KVShare reduces TTFT by up to 9.39x, increases throughput by 1.2x, and improves accuracy by 20.38% over SOTA methods.

Conclusion: KVShare effectively balances efficiency and accuracy in LLM serving, making it a viable solution for multi-tenant scenarios.

Abstract: Recent advances in long-text understanding have pushed the context length of
large language models (LLMs) up to one million tokens. It boosts LLMs's
accuracy and reasoning capacity but causes exorbitant computational costs and
unsatisfactory Time to First Token (TTFT). KV cache reuse, which reuses the
exact same KV cache of prefixes and templates or shares similar ones but with
extra selective recomputation, offers a promising way to tackle this issue.
However, prior studies overlook the cross-request KV reuse and the attention
deviations introduced by new tokens during the decoding stage. In this paper,
we present a KV cache management module that shares the KV cache across
requests under multi-tenant scenarios without sacrificing model accuracy. Our
system, KVShare, enables accurate and efficient LLM serving by 1) a Dual-Stage
High Deviation algorithm (DHD) that conditionally selects a small portion of KV
cache to be recomputed during both prefill and decode phases, and 2) a
cache-aware scheduler that prioritizes requests based on their KV cache hit
rates and orchestrates continuous batching to achieve enhanced system
efficiency and faster TTFT. Multi-task experiments conducted on models such as
Qwen2.5-7B,Llama3.1-8B and Yi1.5-9B demonstrate that KVShare reduces TTFT by up
to 9.39x and increases 1.2x of the throughput compared to the full KV
recompute. Moreover, KVShare achieves 20.38% boost in terms of accuracy
compared to SOTA methods.

</details>


### [94] [Safety Evaluation and Enhancement of DeepSeek Models in Chinese Contexts](https://arxiv.org/pdf/2503.16529)
*Wenjing Zhang, Xuejiao Lei, Zhaoxiang Liu, Limin Han, Jiaojiao Zhao, Junting Guo, Zhenhong Long, Shu Yang, Meijuan An, Beibei Huang, Rongjia Du, Ning Wang, Kai Wang, Shiguo Lian*

Main category: cs.CL

TL;DR: The study evaluates the safety of DeepSeek-R1 distilled models in Chinese contexts using CHiSafetyBench, implements safety enhancements, and open-sources the improved models.


<details>
  <summary>Details</summary>
Motivation: DeepSeek-R1 has notable safety vulnerabilities, especially in Chinese contexts, but the safety of its distilled models remains unassessed.

Method: Uses CHiSafetyBench to evaluate safety before and after distillation, then implements targeted safety enhancements.

Result: Enhanced models show significant safety improvements without compromising reasoning capabilities.

Conclusion: The open-sourced safety-enhanced models provide a resource for future research and optimization of DeepSeek models.

Abstract: DeepSeek-R1, renowned for its exceptional reasoning capabilities and
open-source strategy, is significantly influencing the global artificial
intelligence landscape. However, it exhibits notable safety shortcomings.
Recent research conducted by Robust Intelligence, a subsidiary of Cisco, in
collaboration with the University of Pennsylvania, revealed that DeepSeek-R1
achieves a 100\% attack success rate when processing harmful prompts.
Furthermore, multiple security firms and research institutions have identified
critical security vulnerabilities within the model. Although China Unicom has
uncovered safety vulnerabilities of R1 in Chinese contexts, the safety
capabilities of the remaining distilled models in the R1 series have not yet
been comprehensively evaluated. To address this gap, this study utilizes the
comprehensive Chinese safety benchmark CHiSafetyBench to conduct an in-depth
safety evaluation of the DeepSeek-R1 series distilled models. The objective is
to assess the safety capabilities of these models in Chinese contexts both
before and after distillation, and to further elucidate the adverse effects of
distillation on model safety. Building on these findings, we implement targeted
safety enhancements for the entire DeepSeek-R1 model series. Evaluation results
indicate that the enhanced models achieve significant improvements in safety
while maintaining reasoning capabilities without notable degradation. We
open-source the safety-enhanced models at
https://github.com/UnicomAI/DeepSeek-R1-Safe to serve as a valuable resource
for future research and optimization of DeepSeek models.

</details>


### [95] [Mixture of Routers](https://arxiv.org/pdf/2503.23362)
*Jia-Chen Zhang, Yu-Jie Xiong, Xi-He Qiu, Chun-Ming Xia, Fei Dai*

Main category: cs.CL

TL;DR: The paper introduces Mixture of Routers (MoR), a parameter-efficient fine-tuning method combining LoRA and MoE to improve performance by addressing routing issues.


<details>
  <summary>Details</summary>
Motivation: To enhance fine-tuning performance of large models by solving MoE routing problems like incorrect assignments and imbalanced expert allocation.

Method: Proposes MoR, using multiple sub-routers and a learnable main router to dynamically select experts, improving accuracy and efficiency.

Result: MoR outperforms baselines, achieving a 1% average performance improvement across tasks.

Conclusion: MoR is a plug-and-play, parameter-efficient method suitable for diverse applications, with code publicly available.

Abstract: Supervised fine-tuning (SFT) is a milestone in aligning large language models
with human instructions and adapting them to downstream tasks. In particular,
Low-Rank Adaptation (LoRA) has gained widespread attention due to its parameter
efficiency. However, its impact on improving the performance of large models
remains limited. Recent studies suggest that combining LoRA with
Mixture-of-Experts (MoE) can significantly enhance fine-tuning performance. MoE
adapts to the diversity and complexity of datasets by dynamically selecting the
most suitable experts, thereby improving task accuracy and efficiency. Despite
impressive results, recent studies reveal issues in the MoE routing mechanism,
such as incorrect assignments and imbalanced expert allocation. Inspired by the
principles of Redundancy and Fault Tolerance Theory. We innovatively integrate
the concept of Mixture of Experts into the routing mechanism and propose an
efficient fine-tuning method called Mixture of Routers (MoR). It employs
multiple sub-routers for joint selection and uses a learnable main router to
determine the weights of the sub-routers. The results show that MoR outperforms
baseline models on most tasks, achieving an average performance improvement of
1%. MoR can serve as a plug-and-play, parameter-efficient fine-tuning method
suitable for a wide range of applications. Our code is available here:
https://anonymous.4open.science/r/MoR-DFC6.

</details>


### [96] [Do Theory of Mind Benchmarks Need Explicit Human-like Reasoning in Language Models?](https://arxiv.org/pdf/2504.01698)
*Yi-Long Lu, Chunhui Zhang, Jiajun Song, Lifeng Fan, Wei Wang*

Main category: cs.CL

TL;DR: The paper examines whether Large Language Models (LLMs) solve Theory of Mind (ToM) benchmarks using human-like reasoning or alternative strategies. It finds that Reinforcement Learning (RL) improves reasoning in larger models but causes "reasoning collapse" in smaller ones, while Supervised Fine-Tuning (SFT) performs competitively without explicit reasoning training.


<details>
  <summary>Details</summary>
Motivation: To determine if ToM benchmarks require human-like reasoning or can be solved via alternative strategies in LLMs, and to evaluate the impact of model scale and training methods (RL vs. SFT).

Method: Applied RL and SFT to LLMs (0.5B to 7B parameters) and evaluated them on multiple ToM datasets.

Result: RL improves reasoning in larger models (7B) but causes reasoning collapse in smaller ones (‚â§3B). SFT achieves competitive accuracy without explicit reasoning training.

Conclusion: Current ToM benchmarks may not require human-like reasoning, as LLMs can solve them using alternative strategies, especially when scale or training signals are limited.

Abstract: Theory of Mind (ToM), the ability to attribute mental states to others, is
fundamental for human social intelligence and a critical capability for
advanced Artificial Intelligence. Recent advancements in Large Language Models
(LLMs) have shown promising performance on ToM benchmarks, raising the
question: Do these benchmarks necessitate explicit human-like reasoning
processes, or can models succeed through alternative strategies? We investigate
this question empirically by applying Reinforcement Learning (RL) and
Supervised Fine-Tuning (SFT) to LLMs of varying scales (0.5B to 7B parameters)
and evaluating them across multiple ToM datasets. Our results reveal a
scale-dependent impact of RL: while RL significantly improves accuracy and
fosters high-quality, interpretable, and transferable belief-tracking reasoning
in larger models (7B), it leads to "reasoning collapse" in smaller models
($\leq$3B), where high accuracy and generalization ability are achieved via
drastically shortened, less meaningful responses. Surprisingly, further SFT
achieves competitive and generalizable performance across these benchmarks,
often matching or exceeding RL models in accuracy, despite not being explicitly
trained to produce structured reasoning traces. These findings highlight a
critical discrepancy between benchmark accuracy and the nature of learned
reasoning. Our work suggests that current ToM benchmarks may be solvable
without requiring the explicit, human-like simulation of mental states they
were designed to probe. LLMs, particularly when scale is limited or training
signals focus solely on output correctness, may leverage alternative rules
effective for benchmark data structures.

</details>


### [97] [Parameterized Synthetic Text Generation with SimpleStories](https://arxiv.org/pdf/2504.09184)
*Lennart Finke, Chandan Sreedhara, Thomas Dooms, Mat Allen, Emerald Zhang, Juan Diego Rodriguez, Noa Nabeshima, Thomas Marshall, Dan Braun*

Main category: cs.CL

TL;DR: SimpleStories is a large synthetic dataset in simple language (2M samples in English and Japanese) with controlled story characteristics. It improves sample efficiency and model interpretability over TinyStories and is open-sourced.


<details>
  <summary>Details</summary>
Motivation: To create a scalable, controlled dataset for studying language model training and improve interpretability and efficiency.

Method: Parameterized prompts at multiple abstraction levels to control story characteristics, with ablations on a trained model suite.

Result: Improved sample efficiency and interpretability, and a minimal-parameter model producing grammatical language.

Conclusion: SimpleStories advances dataset control and model training study, with open-sourced resources for broader research.

Abstract: We present SimpleStories, a large synthetic story dataset in simple language,
consisting of 2 million samples each in English and Japanese. Through
parameterizing prompts at multiple levels of abstraction, we achieve control
over story characteristics at scale, inducing syntactic and semantic diversity.
Ablations on a newly trained model suite show improved sample efficiency and
model interpretability compared to the TinyStories dataset. We open-source all
constituent parts of model creation, hoping to enable novel ways to study the
end-to-end training process. As a byproduct, we move the frontier regarding the
fewest-parameter language model that outputs grammatical natural language.

</details>


### [98] [Safety in Large Reasoning Models: A Survey](https://arxiv.org/pdf/2504.17704)
*Cheng Wang, Yue Liu, Baolong Bi, Duzhen Zhang, Zhongzhi Li, Junfeng Fang, Bryan Hooi*

Main category: cs.CL

TL;DR: A survey on safety risks, attacks, and defenses in Large Reasoning Models (LRMs), organized into a taxonomy for clarity.


<details>
  <summary>Details</summary>
Motivation: Address growing concerns about vulnerabilities and safety in LRMs as their reasoning capabilities advance.

Method: Comprehensive survey and taxonomy of safety risks, attacks, and defense strategies in LRMs.

Result: Structured understanding of LRM safety landscape to guide future research.

Conclusion: The work aims to enhance the security and reliability of LRMs for real-world deployment.

Abstract: Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks
like mathematics and coding, leveraging their advanced reasoning capabilities.
Nevertheless, as these capabilities progress, significant concerns regarding
their vulnerabilities and safety have arisen, which can pose challenges to
their deployment and application in real-world settings. This paper presents a
comprehensive survey of LRMs, meticulously exploring and summarizing the newly
emerged safety risks, attacks, and defense strategies. By organizing these
elements into a detailed taxonomy, this work aims to offer a clear and
structured understanding of the current safety landscape of LRMs, facilitating
future research and development to enhance the security and reliability of
these powerful models.

</details>


### [99] [ZeroSearch: Incentivize the Search Capability of LLMs without Searching](https://arxiv.org/pdf/2505.04588)
*Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Yan Zhang, Fei Huang, Jingren Zhou*

Main category: cs.CL

TL;DR: ZeroSearch is a novel RL framework that improves LLMs' search capabilities by simulating searches during training, addressing challenges like uncontrolled document quality and high API costs.


<details>
  <summary>Details</summary>
Motivation: Enhancing LLMs' reasoning and generation by improving their search capabilities, while overcoming issues like unpredictable document quality and expensive API costs.

Method: Uses lightweight supervised fine-tuning to create a retrieval module, followed by a curriculum-based RL strategy to degrade document quality incrementally.

Result: ZeroSearch effectively improves search capabilities, with larger LLMs (7B, 14B) matching or surpassing real search engine performance.

Conclusion: ZeroSearch is scalable, generalizes well across models, and is compatible with various RL algorithms, offering a cost-effective solution for enhancing LLM search abilities.

Abstract: Effective information searching is essential for enhancing the reasoning and
generation capabilities of large language models (LLMs). Recent research has
explored using reinforcement learning (RL) to improve LLMs' search capabilities
by interacting with live search engines in real-world environments. While these
approaches show promising results, they face two major challenges: (1)
Uncontrolled Document Quality: The quality of documents returned by search
engines is often unpredictable, introducing noise and instability into the
training process. (2) Prohibitively High API Costs: RL training requires
frequent rollouts, potentially involving hundreds of thousands of search
requests, which incur substantial API expenses and severely constrain
scalability. To address these challenges, we introduce ZeroSearch, a novel RL
framework that incentivizes the capabilities of LLMs to use a real search
engine with simulated searches during training. Our approach begins with
lightweight supervised fine-tuning to transform the LLM into a retrieval module
capable of generating both useful and noisy documents in response to a query.
During RL training, we employ a curriculum-based rollout strategy that
incrementally degrades the quality of generated documents, progressively
eliciting the model's reasoning ability by exposing it to increasingly
challenging retrieval scenarios. Extensive experiments demonstrate that
ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B
LLM as the retrieval module. Remarkably, a 7B retrieval module achieves
comparable performance to the real search engine, while a 14B retrieval module
even surpasses it. Furthermore, it generalizes well across both base and
instruction-tuned models of various parameter sizes and is compatible with a
wide range of RL algorithms.

</details>


### [100] [Insertion Language Models: Sequence Generation with Arbitrary-Position Insertions](https://arxiv.org/pdf/2505.05755)
*Dhruvesh Patel, Aishwarya Sahoo, Avinash Amballa, Tahira Naseem, Tim G. J. Rudner, Andrew McCallum*

Main category: cs.CL

TL;DR: Insertion Language Models (ILMs) outperform Autoregressive Models (ARMs) and Masked Diffusion Models (MDMs) in planning tasks and offer flexibility in text infilling.


<details>
  <summary>Details</summary>
Motivation: ARMs and MDMs have limitations in handling sequences with complex constraints or out-of-order dependencies. ILMs aim to address these gaps.

Method: ILMs insert tokens at arbitrary positions, selecting both position and vocabulary element. Training uses a tailored network and denoising objective.

Result: ILMs outperform ARMs and MDMs in planning tasks and match ARMs in text generation while offering better flexibility in infilling.

Conclusion: ILMs provide a flexible and effective alternative to ARMs and MDMs for sequence generation tasks with complex dependencies.

Abstract: Autoregressive models (ARMs), which predict subsequent tokens one-by-one
``from left to right,'' have achieved significant success across a wide range
of sequence generation tasks. However, they struggle to accurately represent
sequences that require satisfying sophisticated constraints or whose sequential
dependencies are better addressed by out-of-order generation. Masked Diffusion
Models (MDMs) address some of these limitations, but the process of unmasking
multiple tokens simultaneously in MDMs can introduce incoherences, and MDMs
cannot handle arbitrary infilling constraints when the number of tokens to be
filled in is not known in advance. In this work, we introduce Insertion
Language Models (ILMs), which learn to insert tokens at arbitrary positions in
a sequence -- that is, they select jointly both the position and the vocabulary
element to be inserted. By inserting tokens one at a time, ILMs can represent
strong dependencies between tokens, and their ability to generate sequences in
arbitrary order allows them to accurately model sequences where token
dependencies do not follow a left-to-right sequential structure. To train ILMs,
we propose a tailored network parameterization and use a simple denoising
objective. Our empirical evaluation demonstrates that ILMs outperform both ARMs
and MDMs on common planning tasks. Furthermore, we show that ILMs outperform
MDMs and perform on par with ARMs in an unconditional text generation task
while offering greater flexibility than MDMs in arbitrary-length text
infilling.

</details>


### [101] [From Rankings to Insights: Evaluation Should Shift Focus from Leaderboard to Feedback](https://arxiv.org/pdf/2505.06698)
*Zongqi Wang, Tianle Gu, Chen Gong, Xin Tian, Siqi Bao, Yujiu Yang*

Main category: cs.CL

TL;DR: The paper critiques current LLM evaluation benchmarks for focusing on replicating human rankings and proposes Feedbacker, a framework for fine-grained feedback to optimize models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks provide limited utility by focusing on overall scores rather than actionable feedback for model improvement.

Method: Feedbacker includes a tree-based query taxonomy, automated query synthesis, visualization tools, and a novel LLM-as-a-Judge method (PC$^{2}$).

Result: Feedbacker's evaluation of 17 LLMs demonstrates its effectiveness in identifying specific strengths and weaknesses.

Conclusion: Feedbacker shifts the evaluation paradigm to provide analytical feedback, aiding model optimization and behavior understanding.

Abstract: Automatic evaluation benchmarks such as MT-Bench, Arena-Hard, and Auto-Arena
are seeing growing adoption for the evaluation of Large Language Models (LLMs).
Existing research has primarily focused on approximating human-based model
rankings using limited data and LLM-as-a-Judge. However, the fundamental
premise of these studies, which attempts to replicate human rankings, is
flawed. Specifically, these benchmarks typically offer only overall scores,
limiting their utility to leaderboard rankings, rather than providing feedback
that can guide model optimization and support model profiling. Therefore, we
advocate for an evaluation paradigm shift from approximating human-based model
rankings to providing feedback with analytical value. To this end, we introduce
\textbf{Feedbacker}, an evaluation framework that provides comprehensive and
fine-grained results, thereby enabling thorough identification of a model's
specific strengths and weaknesses. Such feedback not only supports the targeted
optimization of the model but also enhances the understanding of its behavior.
Feedbacker comprises three key components: an extensible tree-based query
taxonomy builder, an automated query synthesis scheme, and a suite of
visualization and analysis tools. Furthermore, we propose a novel
LLM-as-a-Judge method: PC$^{2}$ (Pre-Comparison-derived Criteria) pointwise
evaluation. This method derives evaluation criteria by pre-comparing the
differences between several auxiliary responses, achieving the accuracy of
pairwise evaluation while maintaining the time complexity of pointwise
evaluation. Finally, leveraging the evaluation results of 17 mainstream LLMs,
we demonstrate the usage of Feedbacker and highlight its effectiveness and
potential. Our project homepage and dataset are available at
https://liudan193.github.io/Feedbacker.

</details>


### [102] [DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.07233)
*Jiashuo Sun, Xianrui Zhong, Sizhe Zhou, Jiawei Han*

Main category: cs.CL

TL;DR: DynamicRAG introduces a reinforcement learning-based reranker for RAG systems, dynamically adjusting document selection to improve generation quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of determining the optimal number of documents for reranking in RAG systems to balance information relevance and noise.

Method: Proposes DynamicRAG, a framework using RL to optimize reranking decisions based on LLM output quality.

Result: Achieves state-of-the-art performance on seven knowledge-intensive datasets.

Conclusion: DynamicRAG effectively enhances RAG systems by dynamically refining document retrieval, improving both quality and explainability.

Abstract: Retrieval-augmented generation (RAG) systems combine large language models
(LLMs) with external knowledge retrieval, making them highly effective for
knowledge-intensive tasks. A crucial but often under-explored component of
these systems is the reranker. Since irrelevant documents in RAG systems can
mislead the generator, the reranker plays a vital role in refining retrieved
documents to enhance generation quality and explainability. However, it is
challenging to determine the appropriate number of documents ($k$) that the
reranker should select: too few may result in missing critical information,
while too many introduce noise and inefficiencies. Although recent studies have
explored LLM-based rerankers, they primarily leverage internal model knowledge
and overlook the rich supervisory signals that LLMs can provide, such as using
response quality as feedback for optimizing reranking decisions. In this paper,
we propose DynamicRAG, a novel RAG framework where the reranker dynamically
adjusts both the order and number of retrieved documents based on the query. We
model the reranker as an agent optimized through reinforcement learning (RL),
using rewards derived from LLM output quality. Across seven knowledge-intensive
datasets, DynamicRAG demonstrates superior performance, achieving
state-of-the-art results among models of same parameter sizes. The model, data
and code are available at https://github.com/GasolSun36/DynamicRAG.

</details>


### [103] [Towards Multi-Agent Reasoning Systems for Collaborative Expertise Delegation: An Exploratory Design Study](https://arxiv.org/pdf/2505.07313)
*Baixuan Xu, Chunyang Li, Weiqi Wang, Wei Fan, Tianshi Zheng, Haochen Shi, Tao Fan, Yangqiu Song, Qiang Yang*

Main category: cs.CL

TL;DR: The paper explores how collaboration structure in multi-agent LLM systems affects collective reasoning, focusing on expertise alignment, collaboration paradigms, and system scale. Key findings include domain-specific benefits of expertise alignment, superior performance of diversity-driven collaboration, and scalability trade-offs.


<details>
  <summary>Details</summary>
Motivation: To enhance collective reasoning in multi-agent LLM systems by investigating the impact of collaboration structure design dimensions.

Method: Systematic investigation of three design dimensions: Expertise-Domain Alignment, Collaboration Paradigm, and System Scale.

Result: Expertise alignment is domain-contingent, diversity-driven collaboration outperforms rigid workflows, and scaling introduces computational trade-offs.

Conclusion: Provides guidelines for configuring multi-agent systems and identifies architectural trade-offs, emphasizing the need for efficient communication protocols.

Abstract: Designing effective collaboration structure for multi-agent LLM systems to
enhance collective reasoning is crucial yet remains under-explored. In this
paper, we systematically investigate how collaborative reasoning performance is
affected by three key design dimensions: (1) Expertise-Domain Alignment, (2)
Collaboration Paradigm (structured workflow vs. diversity-driven integration),
and (3) System Scale. Our findings reveal that expertise alignment benefits are
highly domain-contingent, proving most effective for contextual reasoning
tasks. Furthermore, collaboration focused on integrating diverse knowledge
consistently outperforms rigid task decomposition. Finally, we empirically
explore the impact of scaling the multi-agent system with expertise
specialization and study the computational trade off, highlighting the need for
more efficient communication protocol design. This work provides concrete
guidelines for configuring specialized multi-agent system and identifies
critical architectural trade-offs and bottlenecks for scalable multi-agent
reasoning. The code will be made available upon acceptance.

</details>


### [104] [DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models](https://arxiv.org/pdf/2505.09655)
*Xiwen Chen, Wenhui Zhu, Peijie Qiu, Xuanzhao Dong, Hao Wang, Haiyu Wu, Huayu Li, Aristeidis Sotiras, Yalin Wang, Abolfazl Razi*

Main category: cs.CL

TL;DR: The paper introduces Diversity-aware Reward Adjustment (DRA) to address diversity-quality inconsistency in reinforcement learning for language models, improving performance in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: Current methods like GRPO lack semantic diversity in reward signals, leading to indistinguishable rewards for diverse reasoning paths.

Method: DRA uses Submodular Mutual Information (SMI) to adjust rewards, downweighting redundant completions and amplifying diverse ones. It integrates with GRPO and DR.~GRPO.

Result: DRA achieves state-of-the-art performance (58.2% accuracy) on five mathematical reasoning benchmarks with minimal resources.

Conclusion: DRA effectively balances exploration and exploitation, enhancing model performance in low-resource scenarios.

Abstract: Recent advances in reinforcement learning for language model post-training,
such as Group Relative Policy Optimization (GRPO), have shown promise in
low-resource settings. However, GRPO typically relies on solution-level and
scalar reward signals that fail to capture the semantic diversity among sampled
completions. This leads to what we identify as a diversity-quality
inconsistency, where distinct reasoning paths may receive indistinguishable
rewards. To address this limitation, we propose $\textit{Diversity-aware Reward
Adjustment}$ (DRA), a method that explicitly incorporates semantic diversity
into the reward computation. DRA uses Submodular Mutual Information (SMI) to
downweight redundant completions and amplify rewards for diverse ones. This
encourages better exploration during learning, while maintaining stable
exploitation of high-quality samples. Our method integrates seamlessly with
both GRPO and its variant DR.~GRPO, resulting in $\textit{DRA-GRPO}$ and
$\textit{DGA-DR.~GRPO}$. We evaluate our method on five mathematical reasoning
benchmarks and find that it outperforms recent strong baselines. It achieves
state-of-the-art performance with an average accuracy of 58.2%, using only
7,000 fine-tuning samples and a total training cost of approximately $55. The
code is available at https://github.com/xiwenc1/DRA-GRPO.

</details>


### [105] [An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs](https://arxiv.org/pdf/2505.09724)
*Gino Carmona-D√≠az, William Jim√©nez-Leal, Mar√≠a Alejandra Grisales, Chandra Sripada, Santiago Amaya, Michael Inzlicht, Juan Pablo Berm√∫dez*

Main category: cs.CL

TL;DR: A tutorial on using LLMs for efficient text analysis with iterative collaboration between researchers and LLMs, demonstrated via personal goals categorization.


<details>
  <summary>Details</summary>
Motivation: Text analysis is time-consuming and biased; LLMs offer a solution without quality loss.

Method: Step-by-step tutorial: prompt writing, taxonomy generation, evaluation, refinement, testing, and application.

Result: High intercoder reliability achieved in categorizing datasets.

Conclusion: LLMs are promising for text analysis but have limitations.

Abstract: Analyzing texts such as open-ended responses, headlines, or social media
posts is a time- and labor-intensive process highly susceptible to bias. LLMs
are promising tools for text analysis, using either a predefined (top-down) or
a data-driven (bottom-up) taxonomy, without sacrificing quality. Here we
present a step-by-step tutorial to efficiently develop, test, and apply
taxonomies for analyzing unstructured data through an iterative and
collaborative process between researchers and LLMs. Using personal goals
provided by participants as an example, we demonstrate how to write prompts to
review datasets and generate a taxonomy of life domains, evaluate and refine
the taxonomy through prompt and direct modifications, test the taxonomy and
assess intercoder agreements, and apply the taxonomy to categorize an entire
dataset with high intercoder reliability. We discuss the possibilities and
limitations of using LLMs for text analysis.

</details>


### [106] [From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models](https://arxiv.org/pdf/2505.09924)
*Yidan Wang, Yubing Ren, Yanan Cao, Binxing Fang*

Main category: cs.CL

TL;DR: A hybrid watermarking framework for LLMs combines logits-based and sampling-based schemes to balance robustness, text quality, and security, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-offs in existing LLM watermarking schemes to improve misuse detection without compromising text quality.

Method: Proposes a symbiotic framework with serial, parallel, and hybrid strategies, using token and semantic entropy for adaptive watermarking.

Result: Outperforms baselines in experiments, achieving state-of-the-art performance across datasets and models.

Conclusion: The framework offers a versatile solution for LLM watermarking, balancing detectability, robustness, and quality.

Abstract: The rise of Large Language Models (LLMs) has heightened concerns about the
misuse of AI-generated text, making watermarking a promising solution.
Mainstream watermarking schemes for LLMs fall into two categories: logits-based
and sampling-based. However, current schemes entail trade-offs among
robustness, text quality, and security. To mitigate this, we integrate
logits-based and sampling-based schemes, harnessing their respective strengths
to achieve synergy. In this paper, we propose a versatile symbiotic
watermarking framework with three strategies: serial, parallel, and hybrid. The
hybrid framework adaptively embeds watermarks using token entropy and semantic
entropy, optimizing the balance between detectability, robustness, text
quality, and security. Furthermore, we validate our approach through
comprehensive experiments on various datasets and models. Experimental results
indicate that our method outperforms existing baselines and achieves
state-of-the-art (SOTA) performance. We believe this framework provides novel
insights into diverse watermarking paradigms. Our code is available at
https://github.com/redwyd/SymMark.

</details>


### [107] [LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations](https://arxiv.org/pdf/2505.10354)
*Yile Wang, Zhanyu Shen, Hui Huang*

Main category: cs.CL

TL;DR: LDIR introduces low-dimensional, dense, and interpretable text embeddings using relative representations, balancing performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing text embeddings lack interpretability (e.g., SimCSE) or suffer from poor performance (e.g., bag-of-words). Recent work (Benara et al., 2024) offers interpretability but with high dimensionality.

Method: LDIR uses farthest point sampling to create low-dimensional (under 500) embeddings where values indicate semantic relatedness to anchor texts.

Result: LDIR performs close to black-box models and outperforms interpretable baselines with fewer dimensions.

Conclusion: LDIR successfully combines performance and interpretability, validated on semantic similarity, retrieval, and clustering tasks.

Abstract: Semantic text representation is a fundamental task in the field of natural
language processing. Existing text embedding (e.g., SimCSE and LLM2Vec) have
demonstrated excellent performance, but the values of each dimension are
difficult to trace and interpret. Bag-of-words, as classic sparse interpretable
embeddings, suffers from poor performance. Recently, Benara et al. (2024)
propose interpretable text embeddings using large language models, which forms
"0/1" embeddings based on responses to a series of questions. These
interpretable text embeddings are typically high-dimensional (larger than
10,000). In this work, we propose Low-dimensional (lower than 500) Dense and
Interpretable text embeddings with Relative representations (LDIR). The
numerical values of its dimensions indicate semantic relatedness to different
anchor texts through farthest point sampling, offering both semantic
representation as well as a certain level of traceability and interpretability.
We validate LDIR on multiple semantic textual similarity, retrieval, and
clustering tasks. Extensive experimental results show that LDIR performs close
to the black-box baseline models and outperforms the interpretable embeddings
baselines with much fewer dimensions. Code is available at
https://github.com/szu-tera/LDIR.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [108] [Robust Emotion Recognition via Bi-Level Self-Supervised Continual Learning](https://arxiv.org/pdf/2505.10575)
*Adnan Ahmad, Bahareh Nakisa, Mohammad Naim Rastgoo*

Main category: cs.CV

TL;DR: A novel bi-level self-supervised continual learning framework (SSOCL) is proposed for emotion recognition from continuous, unlabeled EEG data, addressing cross-subject variability and noisy labels.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with cross-subject variability and noisy labels in physiological data, especially in continuous, unlabeled streams.

Method: SSOCL uses a dynamic memory buffer, fast adaptation module, and cluster-mapping module to refine pseudo-labels and retain representative samples.

Result: Outperforms existing approaches in adapting to continuous EEG data streams and generalizing across subjects.

Conclusion: SSOCL effectively handles evolving data streams for robust emotion recognition.

Abstract: Emotion recognition through physiological signals such as
electroencephalogram (EEG) has become an essential aspect of affective
computing and provides an objective way to capture human emotions. However,
physiological data characterized by cross-subject variability and noisy labels
hinder the performance of emotion recognition models. Existing domain
adaptation and continual learning methods struggle to address these issues,
especially under realistic conditions where data is continuously streamed and
unlabeled. To overcome these limitations, we propose a novel bi-level
self-supervised continual learning framework, SSOCL, based on a dynamic memory
buffer. This bi-level architecture iteratively refines the dynamic buffer and
pseudo-label assignments to effectively retain representative samples, enabling
generalization from continuous, unlabeled physiological data streams for
emotion recognition. The assigned pseudo-labels are subsequently leveraged for
accurate emotion prediction. Key components of the framework, including a fast
adaptation module and a cluster-mapping module, enable robust learning and
effective handling of evolving data streams. Experimental validation on two
mainstream EEG tasks demonstrates the framework's ability to adapt to
continuous data streams while maintaining strong generalization across
subjects, outperforming existing approaches.

</details>


### [109] [Bias and Generalizability of Foundation Models across Datasets in Breast Mammography](https://arxiv.org/pdf/2505.10579)
*Germani Elodie, Selin T√ºrk Ilayda, Zeineddine Fatima, Mourad Charbel, Albarqouni Shadi*

Main category: cs.CV

TL;DR: The paper examines fairness and bias in foundation models (FMs) for breast mammography classification, highlighting challenges like data variability and biases. It shows that while pre-training and dataset aggregation improve performance, biases persist, especially in underrepresented subgroups. Fairness-aware techniques are recommended for equitable AI.


<details>
  <summary>Details</summary>
Motivation: To address the limited clinical adoption of computer-aided diagnosis tools due to data variability and biases, the study explores the fairness and bias of FMs in breast cancer screening.

Method: The study leverages diverse datasets, including underrepresented regions, and evaluates FMs with modality-specific pre-training, domain-adaptation, and fairness-aware techniques.

Result: Pre-training and dataset aggregation improve performance but fail to fully mitigate biases, causing disparities in underrepresented subgroups. Fairness-aware techniques provide more stable and equitable results.

Conclusion: Rigorous fairness evaluations and mitigation strategies are essential for inclusive and generalizable AI in breast cancer diagnosis.

Abstract: Over the past decades, computer-aided diagnosis tools for breast cancer have
been developed to enhance screening procedures, yet their clinical adoption
remains challenged by data variability and inherent biases. Although foundation
models (FMs) have recently demonstrated impressive generalizability and
transfer learning capabilities by leveraging vast and diverse datasets, their
performance can be undermined by spurious correlations that arise from
variations in image quality, labeling uncertainty, and sensitive patient
attributes. In this work, we explore the fairness and bias of FMs for breast
mammography classification by leveraging a large pool of datasets from diverse
sources-including data from underrepresented regions and an in-house dataset.
Our extensive experiments show that while modality-specific pre-training of FMs
enhances performance, classifiers trained on features from individual datasets
fail to generalize across domains. Aggregating datasets improves overall
performance, yet does not fully mitigate biases, leading to significant
disparities across under-represented subgroups such as extreme breast densities
and age groups. Furthermore, while domain-adaptation strategies can reduce
these disparities, they often incur a performance trade-off. In contrast,
fairness-aware techniques yield more stable and equitable performance across
subgroups. These findings underscore the necessity of incorporating rigorous
fairness evaluations and mitigation strategies into FM-based models to foster
inclusive and generalizable AI.

</details>


### [110] [Relative Drawing Identification Complexity is Invariant to Modality in Vision-Language Models](https://arxiv.org/pdf/2505.10583)
*Diogo Freitas, Brigt H√•vardstun, C√®sar Ferri, Dar√≠o Garigliotti, Jan Arne Telle, Jos√© Hern√°ndez-Orallo*

Main category: cs.CV

TL;DR: The paper explores whether multimodal language models use common representations for different modalities (e.g., images and text) by teaching them concepts from the Quick, Draw! dataset using images and coordinate-based descriptions. Results show images require fewer teaching examples and achieve higher accuracy, but concept simplicity ranks similarly across modalities.


<details>
  <summary>Details</summary>
Motivation: To investigate if multimodal language models integrate modalities using common representations, focusing on whether visual and textual descriptions of the same concept map similarly in latent space.

Method: Uses machine teaching to evaluate the complexity of teaching visual-language models concepts from Quick, Draw! dataset, comparing raw images (bitmaps) and trace coordinates (TikZ format).

Result: Image-based representations require fewer teaching segments and achieve higher accuracy than coordinate-based ones, but concept simplicity rankings are similar across modalities.

Conclusion: Concept simplicity may be an inherent property transcending modality representations, suggesting multimodal models align modalities meaningfully.

Abstract: Large language models have become multimodal, and many of them are said to
integrate their modalities using common representations. If this were true, a
drawing of a car as an image, for instance, should map to the similar area in
the latent space as a textual description of the strokes that conform the
drawing. To explore this in a black-box access regime to these models, we
propose the use of machine teaching, a theory that studies the minimal set of
examples a teacher needs to choose so that the learner captures the concept. In
this paper we evaluate the complexity of teaching visual-language models a
subset of objects in the Quick, Draw! dataset using two presentations: raw
images as bitmaps and trace coordinates in TikZ format. The results indicate
that image-based representations generally require fewer segments and achieve
higher accuracy than coordinate-based representations. But, surprisingly, the
teaching size usually ranks concepts similarly across both modalities, even
when controlling for (a human proxy of) concept priors, suggesting that the
simplicity of concepts may be an inherent property that transcends modality
representations.

</details>


### [111] [Classifying Shelf Life Quality of Pineapples by Combining Audio and Visual Features](https://arxiv.org/pdf/2505.11020)
*Yi-Lu Jiang, Wen-Chang Chang, Ching-Lin Wang, Kung-Liang Hsu, Chih-Yi Chiu*

Main category: cs.CV

TL;DR: A multimodal and multiview classification model was developed to classify pineapple quality using audio and visual features, achieving 84% accuracy.


<details>
  <summary>Details</summary>
Motivation: To reduce waste and increase income by non-destructively determining pineapple shelf life quality.

Method: Constructed a cross-modal classification model using audio and visual data from the PQC500 dataset, employing contrastive audiovisual masked autoencoder training and compact sampling.

Result: The model achieved 84% accuracy, outperforming unimodal audio (78%) and visual (66%) models.

Conclusion: The cross-modal approach is effective for pineapple quality classification, offering practical benefits for shelf life assessment.

Abstract: Determining the shelf life quality of pineapples using non-destructive
methods is a crucial step to reduce waste and increase income. In this paper, a
multimodal and multiview classification model was constructed to classify
pineapples into four quality levels based on audio and visual characteristics.
For research purposes, we compiled and released the PQC500 dataset consisting
of 500 pineapples with two modalities: one was tapping pineapples to record
sounds by multiple microphones and the other was taking pictures by multiple
cameras at different locations, providing multimodal and multi-view audiovisual
features. We modified the contrastive audiovisual masked autoencoder to train
the cross-modal-based classification model by abundant combinations of audio
and visual pairs. In addition, we proposed to sample a compact size of training
data for efficient computation. The experiments were evaluated under various
data and model configurations, and the results demonstrated that the proposed
cross-modal model trained using audio-major sampling can yield 84% accuracy,
outperforming the unimodal models of only audio and only visual by 6% and 18%,
respectively.

</details>


### [112] [Aquarius: A Family of Industry-Level Video Generation Models for Marketing Scenarios](https://arxiv.org/pdf/2505.10584)
*Huafeng Shi, Jianzhong Liang, Rongchang Xie, Xian Wu, Cheng Chen, Chang Liu*

Main category: cs.CV

TL;DR: Aquarius is a family of industry-level video generation models for marketing, designed for large-scale clusters and models with hundreds of billions of parameters, showcasing high-fidelity, multi-aspect-ratio, and long-duration video synthesis.


<details>
  <summary>Details</summary>
Motivation: To demystify industrial-scale video generation systems and advance the generative video community by disclosing design details and open-sourcing components.

Method: The framework includes distributed data processing, scalable model architectures (Single-DiT and Multimodal-DiT), high-performance training infrastructure, parallel inference acceleration, and marketing-specific applications.

Result: Achieves 36% MFU in training, 2.35x inference speedup, and supports diverse video generation tasks like text-to-video and video inpainting.

Conclusion: Aquarius demonstrates robust performance and scalability, with plans for further downstream applications and updates.

Abstract: This report introduces Aquarius, a family of industry-level video generation
models for marketing scenarios designed for thousands-xPU clusters and models
with hundreds of billions of parameters. Leveraging efficient engineering
architecture and algorithmic innovation, Aquarius demonstrates exceptional
performance in high-fidelity, multi-aspect-ratio, and long-duration video
synthesis. By disclosing the framework's design details, we aim to demystify
industrial-scale video generation systems and catalyze advancements in the
generative video community. The Aquarius framework consists of five components:
Distributed Graph and Video Data Processing Pipeline: Manages tens of thousands
of CPUs and thousands of xPUs via automated task distribution, enabling
efficient video data processing. Additionally, we are about to open-source the
entire data processing framework named "Aquarius-Datapipe". Model Architectures
for Different Scales: Include a Single-DiT architecture for 2B models and a
Multimodal-DiT architecture for 13.4B models, supporting multi-aspect ratios,
multi-resolution, and multi-duration video generation. High-Performance
infrastructure designed for video generation model training: Incorporating
hybrid parallelism and fine-grained memory optimization strategies, this
infrastructure achieves 36% MFU at large scale. Multi-xPU Parallel Inference
Acceleration: Utilizes diffusion cache and attention optimization to achieve a
2.35x inference speedup. Multiple marketing-scenarios applications: Including
image-to-video, text-to-video (avatar), video inpainting and video
personalization, among others. More downstream applications and
multi-dimensional evaluation metrics will be added in the upcoming version
updates.

</details>


### [113] [Towards Robust and Controllable Text-to-Motion via Masked Autoregressive Diffusion](https://arxiv.org/pdf/2505.11013)
*Zongye Zhang, Bohan Kong, Qingjie Liu, Yunhong Wang*

Main category: cs.CV

TL;DR: MoMADiff combines masked modeling and diffusion for robust 3D human motion generation from text, offering fine-grained control and strong generalization.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with out-of-distribution motions and lack fine-grained control, limiting real-world applicability.

Method: MoMADiff integrates masked modeling with diffusion processes for frame-level continuous representations, supporting user-provided keyframes.

Result: Outperforms state-of-the-art models in motion quality, instruction fidelity, and keyframe adherence on novel and benchmark datasets.

Conclusion: MoMADiff addresses limitations of existing methods, providing robust and controllable motion synthesis.

Abstract: Generating 3D human motion from text descriptions remains challenging due to
the diverse and complex nature of human motion. While existing methods excel
within the training distribution, they often struggle with out-of-distribution
motions, limiting their applicability in real-world scenarios. Existing
VQVAE-based methods often fail to represent novel motions faithfully using
discrete tokens, which hampers their ability to generalize beyond seen data.
Meanwhile, diffusion-based methods operating on continuous representations
often lack fine-grained control over individual frames. To address these
challenges, we propose a robust motion generation framework MoMADiff, which
combines masked modeling with diffusion processes to generate motion using
frame-level continuous representations. Our model supports flexible
user-provided keyframe specification, enabling precise control over both
spatial and temporal aspects of motion synthesis. MoMADiff demonstrates strong
generalization capability on novel text-to-motion datasets with sparse
keyframes as motion prompts. Extensive experiments on two held-out datasets and
two standard benchmarks show that our method consistently outperforms
state-of-the-art models in motion quality, instruction fidelity, and keyframe
adherence.

</details>


### [114] [Efficient Malicious UAV Detection Using Autoencoder-TSMamba Integration](https://arxiv.org/pdf/2505.10585)
*Azim Akhtarshenas, Ramin Toosi, David L√≥pez-P√©rez, Tohid Alizadeh, Alireza Hosseini*

Main category: cs.CV

TL;DR: The paper proposes an integrated AE-classifier system using a 4-layer TSMamba architecture to detect malicious UAVs, achieving high recall (99.8%) and reduced computational complexity.


<details>
  <summary>Details</summary>
Motivation: Malicious UAVs threaten next-generation networks with risks like unauthorized surveillance and data theft, necessitating robust detection methods.

Method: The system uses an AE to generate residual values, processed by a ResNet-based classifier for lower complexity and higher accuracy.

Result: Achieves 99.8% recall in binary and multi-class scenarios, outperforming benchmarks (96.7%), with reduced computational complexity.

Conclusion: The approach is robust, scalable, and effective for large-scale malicious UAV detection in NGNs.

Abstract: Malicious Unmanned Aerial Vehicles (UAVs) present a significant threat to
next-generation networks (NGNs), posing risks such as unauthorized
surveillance, data theft, and the delivery of hazardous materials. This paper
proposes an integrated (AE)-classifier system to detect malicious UAVs. The
proposed AE, based on a 4-layer Tri-orientated Spatial Mamba (TSMamba)
architecture, effectively captures complex spatial relationships crucial for
identifying malicious UAV activities. The first phase involves generating
residual values through the AE, which are subsequently processed by a
ResNet-based classifier. This classifier leverages the residual values to
achieve lower complexity and higher accuracy. Our experiments demonstrate
significant improvements in both binary and multi-class classification
scenarios, achieving up to 99.8 % recall compared to 96.7 % in the benchmark.
Additionally, our method reduces computational complexity, making it more
suitable for large-scale deployment. These results highlight the robustness and
scalability of our approach, offering an effective solution for malicious UAV
detection in NGN environments.

</details>


### [115] [Super-Resolution Generative Adversarial Networks based Video Enhancement](https://arxiv.org/pdf/2505.10589)
*Kaƒüan √áETƒ∞N*

Main category: cs.CV

TL;DR: An enhanced video super-resolution method extends SRGAN with 3D Non-Local Blocks for spatio-temporal data, improving temporal coherence and reducing artifacts.


<details>
  <summary>Details</summary>
Motivation: SRGAN excels in single-image super-resolution but lacks temporal continuity for video. This work addresses this gap.

Method: Extends SRGAN with 3D Non-Local Blocks, uses patch-wise learning and advanced data degradation for training.

Result: Improved temporal coherence, sharper textures, fewer artifacts; two model variants balance performance and efficiency.

Conclusion: A practical, learning-based solution for video enhancement, useful in streaming, gaming, and restoration.

Abstract: This study introduces an enhanced approach to video super-resolution by
extending ordinary Single-Image Super-Resolution (SISR) Super-Resolution
Generative Adversarial Network (SRGAN) structure to handle spatio-temporal
data. While SRGAN has proven effective for single-image enhancement, its design
does not account for the temporal continuity required in video processing. To
address this, a modified framework that incorporates 3D Non-Local Blocks is
proposed, which is enabling the model to capture relationships across both
spatial and temporal dimensions. An experimental training pipeline is
developed, based on patch-wise learning and advanced data degradation
techniques, to simulate real-world video conditions and learn from both local
and global structures and details. This helps the model generalize better and
maintain stability across varying video content while maintaining the general
structure besides the pixel-wise correctness. Two model variants-one larger and
one more lightweight-are presented to explore the trade-offs between
performance and efficiency. The results demonstrate improved temporal
coherence, sharper textures, and fewer visual artifacts compared to traditional
single-image methods. This work contributes to the development of practical,
learning-based solutions for video enhancement tasks, with potential
applications in streaming, gaming, and digital restoration.

</details>


### [116] [ARFC-WAHNet: Adaptive Receptive Field Convolution and Wavelet-Attentive Hierarchical Network for Infrared Small Target Detection](https://arxiv.org/pdf/2505.10595)
*Xingye Cui, Junhai Luo, Jiakun Deng, Kexuan Li, Xiangyu Qiu, Zhenming Peng*

Main category: cs.CV

TL;DR: ARFC-WAHNet improves infrared small target detection using adaptive convolution, wavelet enhancement, and attention mechanisms, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Limited texture in infrared images and feature loss in deep learning methods hinder accurate small target detection.

Method: Proposes ARFC-WAHNet with MRFFIConv for adaptive feature extraction, WFED for noise suppression, HLFF for feature fusion, and GMEA for attention.

Result: Outperforms recent methods on SIRST, NUDT-SIRST, and IRSTD-1k datasets, especially in complex backgrounds.

Conclusion: ARFC-WAHNet enhances detection accuracy and robustness, addressing challenges in infrared small target detection.

Abstract: Infrared small target detection (ISTD) is critical in both civilian and
military applications. However, the limited texture and structural information
in infrared images makes accurate detection particularly challenging. Although
recent deep learning-based methods have improved performance, their use of
conventional convolution kernels limits adaptability to complex scenes and
diverse targets. Moreover, pooling operations often cause feature loss and
insufficient exploitation of image information. To address these issues, we
propose an adaptive receptive field convolution and wavelet-attentive
hierarchical network for infrared small target detection (ARFC-WAHNet). This
network incorporates a multi-receptive field feature interaction convolution
(MRFFIConv) module to adaptively extract discriminative features by integrating
multiple convolutional branches with a gated unit. A wavelet frequency
enhancement downsampling (WFED) module leverages Haar wavelet transform and
frequency-domain reconstruction to enhance target features and suppress
background noise. Additionally, we introduce a high-low feature fusion (HLFF)
module for integrating low-level details with high-level semantics, and a
global median enhancement attention (GMEA) module to improve feature diversity
and expressiveness via global attention. Experiments on public datasets SIRST,
NUDT-SIRST, and IRSTD-1k demonstrate that ARFC-WAHNet outperforms recent
state-of-the-art methods in both detection accuracy and robustness,
particularly under complex backgrounds. The code is available at
https://github.com/Leaf2001/ARFC-WAHNet.

</details>


### [117] [MAVOS-DD: Multilingual Audio-Video Open-Set Deepfake Detection Benchmark](https://arxiv.org/pdf/2505.11109)
*Florinel-Alin Croitoru, Vlad Hondru, Marius Popescu, Radu Tudor Ionescu, Fahad Shahbaz Khan, Mubarak Shah*

Main category: cs.CV

TL;DR: A large-scale multilingual audio-video deepfake detection benchmark is introduced, featuring 250+ hours of real and fake videos across eight languages, with 60% generated content. Open-set evaluation reveals state-of-the-art detectors struggle in unseen scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the lack of open-set benchmarks for multilingual audio-video deepfake detection, enabling robust evaluation of detectors in diverse, unseen conditions.

Method: Dataset includes real and fake videos in eight languages, generated by seven models. Training splits exclude some models/languages to simulate open-set challenges. Various pre-trained and fine-tuned detectors are tested.

Result: State-of-the-art detectors fail to maintain performance in open-set scenarios, highlighting their limitations.

Conclusion: The benchmark exposes gaps in current deepfake detection methods, urging development of more robust solutions. Data and code are publicly released.

Abstract: We present the first large-scale open-set benchmark for multilingual
audio-video deepfake detection. Our dataset comprises over 250 hours of real
and fake videos across eight languages, with 60% of data being generated. For
each language, the fake videos are generated with seven distinct deepfake
generation models, selected based on the quality of the generated content. We
organize the training, validation and test splits such that only a subset of
the chosen generative models and languages are available during training, thus
creating several challenging open-set evaluation setups. We perform experiments
with various pre-trained and fine-tuned deepfake detectors proposed in recent
literature. Our results show that state-of-the-art detectors are not currently
able to maintain their performance levels when tested in our open-set
scenarios. We publicly release our data and code at:
https://huggingface.co/datasets/unibuc-cs/MAVOS-DD.

</details>


### [118] [SRMamba: Mamba for Super-Resolution of LiDAR Point Clouds](https://arxiv.org/pdf/2505.10601)
*Chuang Chen, Wenyi Ge*

Main category: cs.CV

TL;DR: SRMamba is a novel method for LiDAR point cloud super-resolution, addressing challenges like sparsity and irregular structure, especially for novel views. It uses Hough Voting, Hole Compensation, Visual State Space, and Multi-Directional Scanning to improve 3D spatial recovery.


<details>
  <summary>Details</summary>
Motivation: The sparsity and irregular structure of LiDAR point clouds make super-resolution challenging, particularly for novel views. Existing methods struggle with recovering 3D spatial structures.

Method: SRMamba employs projection techniques (Hough Voting, Hole Compensation) and a Visual State Space model with Multi-Directional Scanning. An asymmetric U-Net adapts to multi-beam LiDAR inputs.

Result: Experiments on SemanticKITTI and nuScenes show SRMamba outperforms other algorithms in qualitative and quantitative evaluations.

Conclusion: SRMamba effectively addresses LiDAR point cloud super-resolution challenges, demonstrating superior performance in recovering 3D spatial structures from novel views.

Abstract: In recent years, range-view-based LiDAR point cloud super-resolution
techniques attract significant attention as a low-cost method for generating
higher-resolution point cloud data. However, due to the sparsity and irregular
structure of LiDAR point clouds, the point cloud super-resolution problem
remains a challenging topic, especially for point cloud upsampling under novel
views. In this paper, we propose SRMamba, a novel method for super-resolution
of LiDAR point clouds in sparse scenes, addressing the key challenge of
recovering the 3D spatial structure of point clouds from novel views.
Specifically, we implement projection technique based on Hough Voting and Hole
Compensation strategy to eliminate horizontally linear holes in range image. To
improve the establishment of long-distance dependencies and to focus on
potential geometric features in vertical 3D space, we employ Visual State Space
model and Multi-Directional Scanning mechanism to mitigate the loss of 3D
spatial structural information due to the range image. Additionally, an
asymmetric U-Net network adapts to the input characteristics of LiDARs with
different beam counts, enabling super-resolution reconstruction for multi-beam
point clouds. We conduct a series of experiments on multiple challenging public
LiDAR datasets (SemanticKITTI and nuScenes), and SRMamba demonstrates
significant superiority over other algorithms in both qualitative and
quantitative evaluations.

</details>


### [119] [Face Consistency Benchmark for GenAI Video](https://arxiv.org/pdf/2505.11425)
*Michal Podstawski, Malgorzata Kudelska, Haohong Wang*

Main category: cs.CV

TL;DR: The paper introduces the Face Consistency Benchmark (FCB) to evaluate character consistency in AI-generated videos, addressing a key challenge in the field.


<details>
  <summary>Details</summary>
Motivation: Current AI video generation struggles with maintaining character consistency, prompting the need for a standardized evaluation framework.

Method: The paper proposes the FCB framework, providing metrics to assess and compare character consistency in videos.

Result: The benchmark identifies gaps in existing solutions and encourages more reliable approaches.

Conclusion: This work advances AI video generation by improving character consistency through standardized evaluation.

Abstract: Video generation driven by artificial intelligence has advanced
significantly, enabling the creation of dynamic and realistic content. However,
maintaining character consistency across video sequences remains a major
challenge, with current models struggling to ensure coherence in appearance and
attributes. This paper introduces the Face Consistency Benchmark (FCB), a
framework for evaluating and comparing the consistency of characters in
AI-generated videos. By providing standardized metrics, the benchmark
highlights gaps in existing solutions and promotes the development of more
reliable approaches. This work represents a crucial step toward improving
character consistency in AI video generation technologies.

</details>


### [120] [MIRAGE: A Multi-modal Benchmark for Spatial Perception, Reasoning, and Intelligence](https://arxiv.org/pdf/2505.10604)
*Chonghan Liu, Haoran Wang, Felix Henry, Pu Miao, Yajie Zhang, Yu Zhao, Peiran Wu*

Main category: cs.CV

TL;DR: MIRAGE is a multi-modal benchmark evaluating models' abilities in object attribute recognition and spatial relational reasoning, highlighting gaps in current models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks show gaps in models' abilities for object attribute recognition and spatial relational reasoning, which are crucial for dynamic reasoning.

Method: Proposes MIRAGE, a benchmark with diverse scenarios for evaluating Counting, Relation, and Counting with Relation tasks.

Result: MIRAGE reveals limitations in state-of-the-art models, emphasizing the need for better representations and reasoning frameworks.

Conclusion: MIRAGE provides a foundation for advancing spatiotemporal reasoning in future research.

Abstract: Spatial perception and reasoning are core components of human cognition,
encompassing object recognition, spatial relational understanding, and dynamic
reasoning. Despite progress in computer vision, existing benchmarks reveal
significant gaps in models' abilities to accurately recognize object attributes
and reason about spatial relationships, both essential for dynamic reasoning.
To address these limitations, we propose MIRAGE, a multi-modal benchmark
designed to evaluate models' capabilities in Counting (object attribute
recognition), Relation (spatial relational reasoning), and Counting with
Relation. Through diverse and complex scenarios requiring fine-grained
recognition and reasoning, MIRAGE highlights critical limitations in
state-of-the-art models, underscoring the need for improved representations and
reasoning frameworks. By targeting these foundational abilities, MIRAGE
provides a pathway toward spatiotemporal reasoning in future research.

</details>


### [121] [Question-Answering Dense Video Events](https://arxiv.org/pdf/2409.04388)
*Hangyu Qin, Junbin Xiao, Angela Yao*

Main category: cs.CV

TL;DR: The paper introduces a novel task of question-answering on dense video events, presents the DeVE-QA dataset, and proposes DeVi, a training-free MLLM approach that outperforms state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of answering and grounding dense-event questions in long videos, which requires faithful comprehension and reasoning over extended periods.

Method: Proposes DeVi, featuring hierarchical captioning, temporal event memory, and self-consistency checking modules to detect, contextualize, and ground dense events.

Result: DeVi achieves a 4.8% and 2.1% increase in GQA accuracy on DeVE-QA and NExT-GQA datasets, respectively.

Conclusion: DeVi demonstrates superior performance in answering dense-event questions and grounding relevant video moments, with data and code made publicly available.

Abstract: This paper presents question-answering on dense video events, a novel task
that answers and grounds dense-event questions in long videos, thus challenging
MLLMs to faithfully comprehend and reason about multiple events over extended
periods of time. To facilitate the study, we construct DeVE-QA -- a dataset
featuring 78K questions about 26K events on 10.6K long videos. Our benchmarking
shows that state-of-the-art MLLMs struggle on DeVE-QA. For improvement, we
propose DeVi, a novel training-free MLLM approach that highlights a
hierarchical captioning module, a temporal event memory module, and a
self-consistency checking module to respectively detect, contextualize and
memorize, and ground dense-events in long videos for question answering.
Extensive experiments show that DeVi is superior at answering dense-event
questions and grounding relevant video moments. Compared with existing MLLMs,
it achieves a notable increase of 4.8% and 2.1% for G(round)QA accuracy on
DeVE-QA and NExT-GQA, respectively. Data and code are available at
https://github.com/QHUni/DeVE-QA.

</details>


### [122] [MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly](https://arxiv.org/pdf/2505.10610)
*Zhaowei Wang, Wenhao Yu, Xiyu Ren, Jipeng Zhang, Yu Zhao, Rohit Saxena, Liang Cheng, Ginny Wong, Simon See, Pasquale Minervini, Yangqiu Song, Mark Steedman*

Main category: cs.CV

TL;DR: MMLongBench is introduced as the first benchmark for evaluating long-context vision-language models (LCVLMs) across diverse tasks and image types, revealing challenges and insights for future improvements.


<details>
  <summary>Details</summary>
Motivation: The rise of LCVLMs handling hundreds of images and interleaved text tokens necessitates a comprehensive benchmark to evaluate their capabilities thoroughly.

Method: MMLongBench includes 13,331 examples across five task categories and various image types, standardized at five input lengths (8K-128K tokens) via cross-modal tokenization.

Result: Benchmarking 46 LCVLMs shows: i) single-task performance is a weak proxy for overall capability; ii) both closed and open-source models struggle with long-context tasks; iii) reasoning ability correlates with better performance.

Conclusion: MMLongBench provides a foundational tool for diagnosing and advancing LCVLMs, highlighting significant room for improvement in long-context vision-language tasks.

Abstract: The rapid extension of context windows in large vision-language models has
given rise to long-context vision-language models (LCVLMs), which are capable
of handling hundreds of images with interleaved text tokens in a single forward
pass. In this work, we introduce MMLongBench, the first benchmark covering a
diverse set of long-context vision-language tasks, to evaluate LCVLMs
effectively and thoroughly. MMLongBench is composed of 13,331 examples spanning
five different categories of downstream tasks, such as Visual RAG and Many-Shot
ICL. It also provides broad coverage of image types, including various natural
and synthetic images. To assess the robustness of the models to different input
lengths, all examples are delivered at five standardized input lengths (8K-128K
tokens) via a cross-modal tokenization scheme that combines vision patches and
text tokens. Through a thorough benchmarking of 46 closed-source and
open-source LCVLMs, we provide a comprehensive analysis of the current models'
vision-language long-context ability. Our results show that: i) performance on
a single task is a weak proxy for overall long-context capability; ii) both
closed-source and open-source models face challenges in long-context
vision-language tasks, indicating substantial room for future improvement; iii)
models with stronger reasoning ability tend to exhibit better long-context
performance. By offering wide task coverage, various image types, and rigorous
length control, MMLongBench provides the missing foundation for diagnosing and
advancing the next generation of LCVLMs.

</details>


### [123] [Mitigate Language Priors in Large Vision-Language Models by Cross-Images Contrastive Decoding](https://arxiv.org/pdf/2505.10634)
*Jianfei Zhao, Feng Zhang, Xin Sun, Chong Feng*

Main category: cs.CV

TL;DR: The paper proposes Cross-Image Contrastive Decoding (CICD), a training-free method to reduce language priors in Large Vision-Language Models (LVLMs), improving visual consistency without losing textual fluency.


<details>
  <summary>Details</summary>
Motivation: Language priors in LVLMs cause hallucinations by generating visually inconsistent but linguistically plausible content, inherited from pre-trained LLMs.

Method: CICD identifies and contrasts essential vs. detrimental priors, using cross-image information to mitigate hallucinations while preserving coherence.

Result: CICD effectively reduces language priors in LVLMs, particularly in image captioning, as validated on four benchmarks with six models.

Conclusion: CICD is a simple, effective solution for mitigating language priors in LVLMs, enhancing visual-textual alignment without additional training.

Abstract: Language priors constitute one of the primary causes of hallucinations in
Large Vision-Language Models (LVLMs), driving the models to generate
linguistically plausible yet visually inconsistent content. The language priors
in LVLMs originate from the linguistic knowledge inherited from their
pre-trained Large Language Model (LLM) backbone. Consequently, this
characteristic is an intrinsic property of the model that remains independent
of visual inputs. Inspired by the finding that language priors are consistent
across images, we propose Cross-Image Contrastive Decoding (CICD), a simple yet
effective training-free method to alleviate language priors in LVLMs. CICD
first identifies essential and detrimental priors, and then employs contrastive
decoding to eliminate the detrimental ones. This approach simultaneously
prevents LVLMs from generating hallucinated content while maintaining textual
fluency and coherence. Furthermore, the limited information overlap between
images helps prevent visual information loss during contrastive decoding. We
validate the effectiveness of CICD on four benchmarks with six LVLMs. Our
experiments demonstrate that CICD performs remarkably well in mitigating
language priors, especially in the image captioning task, where such priors are
most pronounced. Code will be released once accepted.

</details>


### [124] [Advancing Multiple Instance Learning with Continual Learning for Whole Slide Imaging](https://arxiv.org/pdf/2505.10649)
*Xianrui Li, Yufei Cui, Jun Li, Antoni B. Chan*

Main category: cs.CV

TL;DR: The paper proposes Attention Knowledge Distillation (AKD) and Pseudo-Bag Memory Pool (PMP) to improve continual learning in attention-based MIL models for WSI analysis, addressing forgetting and memory issues.


<details>
  <summary>Details</summary>
Motivation: Conventional MIL models lack adaptability to evolving datasets, and continual learning (CL) often underperforms due to forgetting in attention layers.

Method: Analyzed CL in attention MIL models, identified forgetting in attention layers, and introduced AKD (to retain attention knowledge) and PMP (to reduce memory usage by storing informative patches).

Result: The method outperforms state-of-the-art CL methods, improving accuracy and memory efficiency on diverse WSI datasets.

Conclusion: This work enhances CL for large-scale clinical datasets, enabling more adaptable and resilient diagnostic models.

Abstract: Advances in medical imaging and deep learning have propelled progress in
whole slide image (WSI) analysis, with multiple instance learning (MIL) showing
promise for efficient and accurate diagnostics. However, conventional MIL
models often lack adaptability to evolving datasets, as they rely on static
training that cannot incorporate new information without extensive retraining.
Applying continual learning (CL) to MIL models is a possible solution, but
often sees limited improvements. In this paper, we analyze CL in the context of
attention MIL models and find that the model forgetting is mainly concentrated
in the attention layers of the MIL model. Using the results of this analysis we
propose two components for improving CL on MIL: Attention Knowledge
Distillation (AKD) and the Pseudo-Bag Memory Pool (PMP). AKD mitigates
catastrophic forgetting by focusing on retaining attention layer knowledge
between learning sessions, while PMP reduces the memory footprint by
selectively storing only the most informative patches, or ``pseudo-bags'' from
WSIs. Experimental evaluations demonstrate that our method significantly
improves both accuracy and memory efficiency on diverse WSI datasets,
outperforming current state-of-the-art CL methods. This work provides a
foundation for CL in large-scale, weakly annotated clinical datasets, paving
the way for more adaptable and resilient diagnostic models.

</details>


### [125] [CLIP Embeddings for AI-Generated Image Detection: A Few-Shot Study with Lightweight Classifier](https://arxiv.org/pdf/2505.10664)
*Ziyang Ou*

Main category: cs.CV

TL;DR: The paper explores using CLIP embeddings to classify AI-generated images, achieving high accuracy but revealing challenges with specific image types.


<details>
  <summary>Details</summary>
Motivation: The authenticity of AI-generated images on social media is a growing concern, and the potential of CLIP embeddings for classification is underexplored.

Method: A pipeline extracts CLIP visual embeddings, feeds them to lightweight networks, and fine-tunes the classifier, tested on the CIFAKE benchmark.

Result: Achieves 95% accuracy on CIFAKE and 85% with few-shot adaptation, but struggles with specific styles like wide-angle photos and oil paintings.

Conclusion: The study highlights unexplored challenges in classifying certain AI-generated images, suggesting further research is needed.

Abstract: Verifying the authenticity of AI-generated images presents a growing
challenge on social media platforms these days. While vision-language models
(VLMs) like CLIP outdo in multimodal representation, their capacity for
AI-generated image classification is underexplored due to the absence of such
labels during the pre-training process. This work investigates whether CLIP
embeddings inherently contain information indicative of AI generation. A
proposed pipeline extracts visual embeddings using a frozen CLIP model, feeds
its embeddings to lightweight networks, and fine-tunes only the final
classifier. Experiments on the public CIFAKE benchmark show the performance
reaches 95% accuracy without language reasoning. Few-shot adaptation to curated
custom with 20% of the data results in performance to 85%. A closed-source
baseline (Gemini-2.0) has the best zero-shot accuracy yet fails on specific
styles. Notably, some specific image types, such as wide-angle photographs and
oil paintings, pose significant challenges to classification. These results
indicate previously unexplored difficulties in classifying certain types of
AI-generated images, revealing new and more specific questions in this domain
that are worth further investigation.

</details>


### [126] [From Embeddings to Accuracy: Comparing Foundation Models for Radiographic Classification](https://arxiv.org/pdf/2505.10823)
*Xue Li, Jameson Merkow, Noel C. F. Codella, Alberto Santamaria-Pang, Naiteek Sangani, Alexander Ersoy, Christopher Burt, John W. Garrett, Richard J. Bruce, Joshua D. Warner, Tyler Bradshaw, Ivan Tarapov, Matthew P. Lungren, Alan B. McMillan*

Main category: cs.CV

TL;DR: The study evaluates foundation model embeddings for multi-class radiography classification, finding MedImageInsight with an SVM adapter most effective (93.8% mAUC), with efficient and fair performance.


<details>
  <summary>Details</summary>
Motivation: To assess the utility of general-purpose and medical-specific foundation model embeddings for training lightweight adapters in radiography classification, focusing on tube placement.

Method: Used six foundation models to extract embeddings from 8842 radiographs, trained adapter models with classical ML algorithms, and evaluated performance (mAUC), efficiency, and fairness.

Result: MedImageInsight embeddings with SVM adapter achieved the highest mAUC (93.8%). Most adapters were computationally efficient (training <1 min, inference in seconds) and fair (minimal disparities).

Conclusion: Foundation model embeddings, especially MedImageInsight, enable accurate, efficient, and equitable diagnostic classification in radiographic image analysis.

Abstract: Foundation models, pretrained on extensive datasets, have significantly
advanced machine learning by providing robust and transferable embeddings
applicable to various domains, including medical imaging diagnostics. This
study evaluates the utility of embeddings derived from both general-purpose and
medical domain-specific foundation models for training lightweight adapter
models in multi-class radiography classification, focusing specifically on tube
placement assessment. A dataset comprising 8842 radiographs classified into
seven distinct categories was employed to extract embeddings using six
foundation models: DenseNet121, BiomedCLIP, Med-Flamingo, MedImageInsight,
Rad-DINO, and CXR-Foundation. Adapter models were subsequently trained using
classical machine learning algorithms. Among these combinations,
MedImageInsight embeddings paired with an support vector machine adapter
yielded the highest mean area under the curve (mAUC) at 93.8%, followed closely
by Rad-DINO (91.1%) and CXR-Foundation (89.0%). In comparison, BiomedCLIP and
DenseNet121 exhibited moderate performance with mAUC scores of 83.0% and 81.8%,
respectively, whereas Med-Flamingo delivered the lowest performance at 75.1%.
Notably, most adapter models demonstrated computational efficiency, achieving
training within one minute and inference within seconds on CPU, underscoring
their practicality for clinical applications. Furthermore, fairness analyses on
adapters trained on MedImageInsight-derived embeddings indicated minimal
disparities, with gender differences in performance within 2% and standard
deviations across age groups not exceeding 3%. These findings confirm that
foundation model embeddings-especially those from MedImageInsight-facilitate
accurate, computationally efficient, and equitable diagnostic classification
using lightweight adapters for radiographic image analysis.

</details>


### [127] [GA3CE: Unconstrained 3D Gaze Estimation with Gaze-Aware 3D Context Encoding](https://arxiv.org/pdf/2505.10671)
*Yuki Kawana, Shintaro Shiba, Quan Kong, Norimasa Kobori*

Main category: cs.CV

TL;DR: A novel 3D gaze estimation method, GA3CE, learns spatial relationships in 3D space, improving accuracy in unconstrained settings.


<details>
  <summary>Details</summary>
Motivation: Overcoming limitations of 2D appearance-based or depth-map methods in estimating 3D gaze direction, especially in challenging scenarios like distant subjects or varying poses.

Method: Uses 3D poses and object positions as context, aligns them in egocentric space, and employs D$^3$ positional encoding to capture spatial relationships.

Result: Reduces mean angle error by 13%-37% compared to baselines in single-frame settings.

Conclusion: GA3CE effectively addresses 3D gaze estimation challenges by leveraging 3D context and spatial relationships.

Abstract: We propose a novel 3D gaze estimation approach that learns spatial
relationships between the subject and objects in the scene, and outputs 3D gaze
direction. Our method targets unconstrained settings, including cases where
close-up views of the subject's eyes are unavailable, such as when the subject
is distant or facing away. Previous approaches typically rely on either 2D
appearance alone or incorporate limited spatial cues using depth maps in the
non-learnable post-processing step. Estimating 3D gaze direction from 2D
observations in these scenarios is challenging; variations in subject pose,
scene layout, and gaze direction, combined with differing camera poses, yield
diverse 2D appearances and 3D gaze directions even when targeting the same 3D
scene. To address this issue, we propose GA3CE: Gaze-Aware 3D Context Encoding.
Our method represents subject and scene using 3D poses and object positions,
treating them as 3D context to learn spatial relationships in 3D space.
Inspired by human vision, we align this context in an egocentric space,
significantly reducing spatial complexity. Furthermore, we propose D$^3$
(direction-distance-decomposed) positional encoding to better capture the
spatial relationship between 3D context and gaze direction in direction and
distance space. Experiments demonstrate substantial improvements, reducing mean
angle error by 13%-37% compared to leading baselines on benchmark datasets in
single-frame settings.

</details>


### [128] [HSRMamba: Efficient Wavelet Stripe State Space Model for Hyperspectral Image Super-Resolution](https://arxiv.org/pdf/2505.11062)
*Baisong Li, Xingwang Wang, Haixiao Xu*

Main category: cs.CV

TL;DR: HSRMamba improves hyperspectral image super-resolution by reducing artifacts and modal conflicts, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address potential artifacts in Visual Mamba's 1D scanning and modal conflicts in hyperspectral image super-resolution.

Method: Introduces strip-based scanning and wavelet decomposition to enhance efficiency and performance.

Result: Achieves state-of-the-art results with reduced computational load and model size.

Conclusion: HSRMamba effectively balances performance and efficiency, advancing hyperspectral image super-resolution.

Abstract: Single hyperspectral image super-resolution (SHSR) aims to restore
high-resolution images from low-resolution hyperspectral images. Recently, the
Visual Mamba model has achieved an impressive balance between performance and
computational efficiency. However, due to its 1D scanning paradigm, the model
may suffer from potential artifacts during image generation. To address this
issue, we propose HSRMamba. While maintaining the computational efficiency of
Visual Mamba, we introduce a strip-based scanning scheme to effectively reduce
artifacts from global unidirectional scanning. Additionally, HSRMamba uses
wavelet decomposition to alleviate modal conflicts between high-frequency
spatial features and low-frequency spectral features, further improving
super-resolution performance. Extensive experiments show that HSRMamba not only
excels in reducing computational load and model size but also outperforms
existing methods, achieving state-of-the-art results.

</details>


### [129] [Are Spatial-Temporal Graph Convolution Networks for Human Action Recognition Over-Parameterized?](https://arxiv.org/pdf/2505.10679)
*Jianyang Xie, Yitian Zhao, Yanda Meng, He Zhao, Anh Nguyen, Yalin Zheng*

Main category: cs.CV

TL;DR: Sparse ST-GCNs achieve comparable performance to dense models with significantly fewer parameters, and multi-level sparsity further improves accuracy.


<details>
  <summary>Details</summary>
Motivation: ST-GCNs are over-parameterized for HAR, and sparse architectures can maintain performance while reducing complexity.

Method: Proposed a sparse ST-GCN generator and multi-level sparsity integration, validated on four datasets.

Result: Sparse models with 95% fewer parameters show <1% accuracy drop; multi-level sparsity improves accuracy by >1%.

Conclusion: Sparse ST-GCNs are efficient and effective for HAR, offering a balance between performance and parameter reduction.

Abstract: Spatial-temporal graph convolutional networks (ST-GCNs) showcase impressive
performance in skeleton-based human action recognition (HAR). However, despite
the development of numerous models, their recognition performance does not
differ significantly after aligning the input settings. With this observation,
we hypothesize that ST-GCNs are over-parameterized for HAR, a conjecture
subsequently confirmed through experiments employing the lottery ticket
hypothesis. Additionally, a novel sparse ST-GCNs generator is proposed, which
trains a sparse architecture from a randomly initialized dense network while
maintaining comparable performance levels to the dense components. Moreover, we
generate multi-level sparsity ST-GCNs by integrating sparse structures at
various sparsity levels and demonstrate that the assembled model yields a
significant enhancement in HAR performance. Thorough experiments on four
datasets, including NTU-RGB+D 60(120), Kinetics-400, and FineGYM, demonstrate
that the proposed sparse ST-GCNs can achieve comparable performance to their
dense components. Even with 95% fewer parameters, the sparse ST-GCNs exhibit a
degradation of <1% in top-1 accuracy. Meanwhile, the multi-level sparsity
ST-GCNs, which require only 66% of the parameters of the dense ST-GCNs,
demonstrate an improvement of >1% in top-1 accuracy. The code is available at
https://github.com/davelailai/Sparse-ST-GCN.

</details>


### [130] [GaussianFormer3D: Multi-Modal Gaussian-based Semantic Occupancy Prediction with 3D Deformable Attention](https://arxiv.org/pdf/2505.10685)
*Lingjun Zhao, Sizhe Wei, James Hays, Lu Gan*

Main category: cs.CV

TL;DR: Proposes GaussianFormer3D, a multi-modal Gaussian-based semantic occupancy prediction framework using 3D deformable attention for autonomous driving, achieving high accuracy with reduced memory usage.


<details>
  <summary>Details</summary>
Motivation: 3D semantic occupancy prediction is crucial for autonomous driving. Multi-modal fusion (LiDAR-camera) improves accuracy, and 3D Gaussians offer a compact, continuous representation compared to dense grids.

Method: Introduces voxel-to-Gaussian initialization for geometry priors and LiDAR-guided 3D deformable attention for refining Gaussians with fusion features in a lifted 3D space.

Result: Achieves high prediction accuracy comparable to state-of-the-art multi-modal methods, with reduced memory consumption and improved efficiency.

Conclusion: GaussianFormer3D is an effective framework for semantic occupancy prediction, balancing accuracy and efficiency in autonomous driving.

Abstract: 3D semantic occupancy prediction is critical for achieving safe and reliable
autonomous driving. Compared to camera-only perception systems, multi-modal
pipelines, especially LiDAR-camera fusion methods, can produce more accurate
and detailed predictions. Although most existing works utilize a dense
grid-based representation, in which the entire 3D space is uniformly divided
into discrete voxels, the emergence of 3D Gaussians provides a compact and
continuous object-centric representation. In this work, we propose a
multi-modal Gaussian-based semantic occupancy prediction framework utilizing 3D
deformable attention, named as GaussianFormer3D. We introduce a
voxel-to-Gaussian initialization strategy to provide 3D Gaussians with geometry
priors from LiDAR data, and design a LiDAR-guided 3D deformable attention
mechanism for refining 3D Gaussians with LiDAR-camera fusion features in a
lifted 3D space. We conducted extensive experiments on both on-road and
off-road datasets, demonstrating that our GaussianFormer3D achieves high
prediction accuracy that is comparable to state-of-the-art multi-modal
fusion-based methods with reduced memory consumption and improved efficiency.

</details>


### [131] [Automated Detection of Salvin's Albatrosses: Improving Deep Learning Tools for Aerial Wildlife Surveys](https://arxiv.org/pdf/2505.10737)
*Mitchell Rogers, Theo Thompson, Isla Duporge, Johannes Fischer, Klemens P√ºtz, Thomas Mattern, Bing Xue, Mengjie Zhang*

Main category: cs.CV

TL;DR: The paper evaluates BirdDetector, a deep-learning model, for monitoring Salvin's albatross populations using UAV imagery, showing improved accuracy with fine-tuning and augmentation.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of deep learning models like BirdDetector for wildlife monitoring, especially in remote areas like seabird colonies.

Method: Evaluated BirdDetector in zero-shot and fine-tuned settings using drone imagery, with enhanced inference and augmentation techniques.

Result: Fine-tuning with target-domain annotations and stronger augmentation significantly improved detection accuracy over zero-shot performance.

Conclusion: Pre-trained deep-learning models, when fine-tuned, are highly effective for species-specific monitoring in challenging environments.

Abstract: Recent advancements in deep learning and aerial imaging have transformed
wildlife monitoring, enabling researchers to survey wildlife populations at
unprecedented scales. Unmanned Aerial Vehicles (UAVs) provide a cost-effective
means of capturing high-resolution imagery, particularly for monitoring densely
populated seabird colonies. In this study, we assess the performance of a
general-purpose avian detection model, BirdDetector, in estimating the breeding
population of Salvin's albatross (Thalassarche salvini) on the Bounty Islands,
New Zealand. Using drone-derived imagery, we evaluate the model's effectiveness
in both zero-shot and fine-tuned settings, incorporating enhanced inference
techniques and stronger augmentation methods. Our findings indicate that while
applying the model in a zero-shot setting offers a strong baseline, fine-tuning
with annotations from the target domain and stronger image augmentation leads
to marked improvements in detection accuracy. These results highlight the
potential of leveraging pre-trained deep-learning models for species-specific
monitoring in remote and challenging environments.

</details>


### [132] [IMAGE-ALCHEMY: Advancing subject fidelity in personalised text-to-image generation](https://arxiv.org/pdf/2505.10743)
*Amritanshu Tiwari, Cherish Puniani, Kaustubh Sharma, Ojasva Nema*

Main category: cs.CV

TL;DR: A two-stage pipeline using LoRA-based fine-tuning on SDXL for personalized text-to-image generation, avoiding catastrophic forgetting and overfitting.


<details>
  <summary>Details</summary>
Motivation: Personalizing text-to-image models for novel subjects from few references is challenging due to issues like catastrophic forgetting and computational overhead.

Method: A two-stage approach: (1) generate a generic scene with SDXL, (2) insert the personalized subject using a segmentation-driven Img2Img pipeline with trained LoRA weights.

Result: Achieves a DINO similarity score of 0.789 on SDXL, outperforming existing methods.

Conclusion: The method preserves SDXL's generative capabilities while integrating new subjects with high fidelity.

Abstract: Recent advances in text-to-image diffusion models, particularly Stable
Diffusion, have enabled the generation of highly detailed and semantically rich
images. However, personalizing these models to represent novel subjects based
on a few reference images remains challenging. This often leads to catastrophic
forgetting, overfitting, or large computational overhead.We propose a two-stage
pipeline that addresses these limitations by leveraging LoRA-based fine-tuning
on the attention weights within the U-Net of the Stable Diffusion XL (SDXL)
model. First, we use the unmodified SDXL to generate a generic scene by
replacing the subject with its class label. Then, we selectively insert the
personalized subject through a segmentation-driven image-to-image (Img2Img)
pipeline that uses the trained LoRA weights.This framework isolates the subject
encoding from the overall composition, thus preserving SDXL's broader
generative capabilities while integrating the new subject in a high-fidelity
manner. Our method achieves a DINO similarity score of 0.789 on SDXL,
outperforming existing personalized text-to-image approaches.

</details>


### [133] [Mapping Semantic Segmentation to Point Clouds Using Structure from Motion for Forest Analysis](https://arxiv.org/pdf/2505.10751)
*Francisco Raverta Capua, Pablo De Cristoforis*

Main category: cs.CV

TL;DR: A novel pipeline generates semantically segmented forest point clouds using a custom simulator and modified SfM software, addressing the lack of public annotated datasets.


<details>
  <summary>Details</summary>
Motivation: Public annotated point cloud datasets for forests are scarce due to high costs and technical challenges, especially with SfM algorithms.

Method: A custom forest simulator creates labeled RGB images, processed by modified SfM software to preserve semantic data during 3D reconstruction.

Result: The pipeline produces detailed geometric and semantic point clouds, useful for training deep learning models.

Conclusion: This work provides a valuable resource for advancing forest point cloud segmentation via SfM.

Abstract: Although the use of remote sensing technologies for monitoring forested
environments has gained increasing attention, publicly available point cloud
datasets remain scarce due to the high costs, sensor requirements, and
time-intensive nature of their acquisition. Moreover, as far as we are aware,
there are no public annotated datasets generated through Structure From Motion
(SfM) algorithms applied to imagery, which may be due to the lack of SfM
algorithms that can map semantic segmentation information into an accurate
point cloud, especially in a challenging environment like forests.
  In this work, we present a novel pipeline for generating semantically
segmented point clouds of forest environments. Using a custom-built forest
simulator, we generate realistic RGB images of diverse forest scenes along with
their corresponding semantic segmentation masks. These labeled images are then
processed using modified open-source SfM software capable of preserving
semantic information during 3D reconstruction. The resulting point clouds
provide both geometric and semantic detail, offering a valuable resource for
training and evaluating deep learning models aimed at segmenting real forest
point clouds obtained via SfM.

</details>


### [134] [Benchmarking performance, explainability, and evaluation strategies of vision-language models for surgery: Challenges and opportunities](https://arxiv.org/pdf/2505.10764)
*Jiajun Cheng, Xianwu Zhao, Shan Lin*

Main category: cs.CV

TL;DR: The paper benchmarks vision-language models (VLMs) in surgical contexts, revealing gaps in linking language to surgical scenes.


<details>
  <summary>Details</summary>
Motivation: To assess how general-purpose VLMs perform in the surgical domain, given their success in other visual tasks.

Method: Benchmarking several VLMs on diverse surgical datasets, including laparoscopic and endoscopic procedures.

Result: VLMs struggle to consistently link language to correct regions in surgical scenes.

Conclusion: General-purpose VLMs have limitations in surgical understanding, highlighting the need for domain-specific improvements.

Abstract: Minimally invasive surgery (MIS) presents significant visual and technical
challenges, including surgical instrument classification and understanding
surgical action involving instruments, verbs, and anatomical targets. While
many machine learning-based methods have been developed for surgical
understanding, they typically rely on procedure- and task-specific models
trained on small, manually annotated datasets. In contrast, the recent success
of vision-language models (VLMs) trained on large volumes of raw image-text
pairs has demonstrated strong adaptability to diverse visual data and a range
of downstream tasks. This opens meaningful research questions: how well do
these general-purpose VLMs perform in the surgical domain? In this work, we
explore those questions by benchmarking several VLMs across diverse surgical
datasets, including general laparoscopic procedures and endoscopic submucosal
dissection, to assess their current capabilities and limitations. Our benchmark
reveals key gaps in the models' ability to consistently link language to the
correct regions in surgical scenes.

</details>


### [135] [reBEN: Refined BigEarthNet Dataset for Remote Sensing Image Analysis](https://arxiv.org/pdf/2407.03653)
*Kai Norman Clasen, Leonard Hackel, Tom Burgert, Gencer Sumbul, Beg√ºm Demir, Volker Markl*

Main category: cs.CV

TL;DR: The paper introduces refined BigEarthNet (reBEN), an improved large-scale multi-modal remote sensing dataset for deep learning, featuring higher-quality patches, reduced label noise, and optimized data formats for efficient training.


<details>
  <summary>Details</summary>
Motivation: To address limitations in the original BigEarthNet dataset, such as label noise and spatial correlation, and to support more reliable deep learning evaluations for remote sensing image analysis.

Method: Constructed reBEN by dividing Sentinel-1 and Sentinel-2 tiles into 1200 m x 1200 m patches, applying atmospheric correction, and using the latest CORINE Land Cover map for labeling. Introduced a geographical-based split algorithm to reduce spatial correlation and optimized data formats for DL training.

Result: reBEN provides higher-quality patches, reduced label noise, and minimized spatial correlation, enhancing the reliability of DL model evaluations.

Conclusion: reBEN is a valuable resource for multi-modal multi-label remote sensing tasks, with improved data quality and evaluation reliability, supported by publicly available tools and datasets.

Abstract: This paper presents refined BigEarthNet (reBEN) that is a large-scale,
multi-modal remote sensing dataset constructed to support deep learning (DL)
studies for remote sensing image analysis. The reBEN dataset consists of
549,488 pairs of Sentinel-1 and Sentinel-2 image patches. To construct reBEN,
we initially consider the Sentinel-1 and Sentinel-2 tiles used to construct the
BigEarthNet dataset and then divide them into patches of size 1200 m x 1200 m.
We apply atmospheric correction to the Sentinel-2 patches using the latest
version of the sen2cor tool, resulting in higher-quality patches compared to
those present in BigEarthNet. Each patch is then associated with a pixel-level
reference map and scene-level multi-labels. This makes reBEN suitable for
pixel- and scene-based learning tasks. The labels are derived from the most
recent CORINE Land Cover (CLC) map of 2018 by utilizing the 19-class
nomenclature as in BigEarthNet. The use of the most recent CLC map results in
overcoming the label noise present in BigEarthNet. Furthermore, we introduce a
new geographical-based split assignment algorithm that significantly reduces
the spatial correlation among the train, validation, and test sets with respect
to those present in BigEarthNet. This increases the reliability of the
evaluation of DL models. To minimize the DL model training time, we introduce
software tools that convert the reBEN dataset into a DL-optimized data format.
In our experiments, we show the potential of reBEN for multi-modal multi-label
image classification problems by considering several state-of-the-art DL
models. The pre-trained model weights, associated code, and complete dataset
are available at https://bigearth.net.

</details>


### [136] [Unifying Segment Anything in Microscopy with Multimodal Large Language Model](https://arxiv.org/pdf/2505.10769)
*Manyu Li, Ruian He, Zixian Zhang, Weimin Tan, Bo Yan*

Main category: cs.CV

TL;DR: The paper proposes uLLSAM, a method leveraging MLLMs to enhance SAM's cross-domain segmentation in microscopy by injecting Vision-Language Knowledge (VLK), achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Existing biomedical segmentation models lack generalization on unseen domains due to missing vision-language knowledge. MLLMs' multimodal capabilities inspire integrating VLK to improve SAM's performance.

Method: The paper introduces the Vision-Language Semantic Alignment (VLSA) module to inject VLK into SAM and Semantic Boundary Regularization (SBR) to address boundary perception issues.

Result: uLLSAM improves Dice by 7.71% and SA by 12.10% on in-domain datasets, and Dice by 6.79% and SA by 10.08% on out-of-domain datasets, showing strong generalization.

Conclusion: The proposed uLLSAM, with VLSA and SBR, effectively enhances SAM's cross-domain segmentation, achieving state-of-the-art performance and generalization.

Abstract: Accurate segmentation of regions of interest in biomedical images holds
substantial value in image analysis. Although several foundation models for
biomedical segmentation have currently achieved excellent performance on
certain datasets, they typically demonstrate sub-optimal performance on unseen
domain data. We owe the deficiency to lack of vision-language knowledge before
segmentation. Multimodal Large Language Models (MLLMs) bring outstanding
understanding and reasoning capabilities to multimodal tasks, which inspires us
to leverage MLLMs to inject Vision-Language Knowledge (VLK), thereby enabling
vision models to demonstrate superior generalization capabilities on
cross-domain datasets. In this paper, we propose using MLLMs to guide SAM in
learning microscopy crose-domain data, unifying Segment Anything in Microscopy,
named uLLSAM. Specifically, we propose the Vision-Language Semantic Alignment
(VLSA) module, which injects VLK into Segment Anything Model (SAM). We find
that after SAM receives global VLK prompts, its performance improves
significantly, but there are deficiencies in boundary contour perception.
Therefore, we further propose Semantic Boundary Regularization (SBR) to prompt
SAM. Our method achieves performance improvements of 7.71% in Dice and 12.10%
in SA across 9 in-domain microscopy datasets, achieving state-of-the-art
performance. Our method also demonstrates improvements of 6.79% in Dice and
10.08% in SA across 10 out-ofdomain datasets, exhibiting strong generalization
capabilities. Code is available at https://github.com/ieellee/uLLSAM.

</details>


### [137] [Completely Weakly Supervised Class-Incremental Learning for Semantic Segmentation](https://arxiv.org/pdf/2505.10781)
*David Minkwan Kim, Soeun Lee, Byeongkeun Kang*

Main category: cs.CV

TL;DR: A completely weakly supervised method for class-incremental semantic segmentation (CISS) using image-level labels, outperforming partially supervised methods.


<details>
  <summary>Details</summary>
Motivation: Traditional CISS requires costly pixel-level annotations; this work aims to eliminate this need by using only image-level labels.

Method: Generates robust pseudo-labels via a localizer and foundation models, and uses exemplar-guided data augmentation to prevent catastrophic forgetting.

Result: Outperforms partially supervised methods in 15-5 VOC and 10-10 VOC settings, and achieves competitive accuracy in COCO-to-VOC.

Conclusion: The proposed method is effective for completely weakly supervised CISS, demonstrating superior or competitive performance across settings.

Abstract: This work addresses the task of completely weakly supervised
class-incremental learning for semantic segmentation to learn segmentation for
both base and additional novel classes using only image-level labels. While
class-incremental semantic segmentation (CISS) is crucial for handling diverse
and newly emerging objects in the real world, traditional CISS methods require
expensive pixel-level annotations for training. To overcome this limitation,
partially weakly-supervised approaches have recently been proposed. However, to
the best of our knowledge, this is the first work to introduce a completely
weakly-supervised method for CISS. To achieve this, we propose to generate
robust pseudo-labels by combining pseudo-labels from a localizer and a sequence
of foundation models based on their uncertainty. Moreover, to mitigate
catastrophic forgetting, we introduce an exemplar-guided data augmentation
method that generates diverse images containing both previous and novel classes
with guidance. Finally, we conduct experiments in three common experimental
settings: 15-5 VOC, 10-10 VOC, and COCO-to-VOC, and in two scenarios: disjoint
and overlap. The experimental results demonstrate that our completely weakly
supervised method outperforms even partially weakly supervised methods in the
15-5 VOC and 10-10 VOC settings while achieving competitive accuracy in the
COCO-to-VOC setting.

</details>


### [138] [SynRailObs: A Synthetic Dataset for Obstacle Detection in Railway Scenarios](https://arxiv.org/pdf/2505.10784)
*Qiushi Guo, Jason Rambach*

Main category: cs.CV

TL;DR: SynRailObs is a synthetic dataset for railway obstacle detection, addressing gaps in existing datasets by including diverse conditions and rare obstacles. It shows strong performance in real-world tests and offers zero-shot capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack diversity and precision for railway obstacle detection, limiting safety research.

Method: Created SynRailObs, a synthetic dataset with diverse weather and geographical features, using diffusion models for rare obstacles.

Result: Models trained on SynRailObs perform consistently across distances and conditions, with zero-shot capabilities.

Conclusion: SynRailObs advances railway safety by providing a high-quality dataset for obstacle detection.

Abstract: Detecting potential obstacles in railway environments is critical for
preventing serious accidents. Identifying a broad range of obstacle categories
under complex conditions requires large-scale datasets with precisely
annotated, high-quality images. However, existing publicly available datasets
fail to meet these requirements, thereby hindering progress in railway safety
research. To address this gap, we introduce SynRailObs, a high-fidelity
synthetic dataset designed to represent a diverse range of weather conditions
and geographical features. Furthermore, diffusion models are employed to
generate rare and difficult-to-capture obstacles that are typically challenging
to obtain in real-world scenarios. To evaluate the effectiveness of SynRailObs,
we perform experiments in real-world railway environments, testing on both
ballasted and ballastless tracks across various weather conditions. The results
demonstrate that SynRailObs holds substantial potential for advancing obstacle
detection in railway safety applications. Models trained on this dataset show
consistent performance across different distances and environmental conditions.
Moreover, the model trained on SynRailObs exhibits zero-shot capabilities,
which are essential for applications in security-sensitive domains. The data is
available in https://www.kaggle.com/datasets/qiushi910/synrailobs.

</details>


### [139] [EA-3DGS: Efficient and Adaptive 3D Gaussians with Highly Enhanced Quality for outdoor scenes](https://arxiv.org/pdf/2505.10787)
*Jianlin Guo, Haihong Xiao, Wenxiong Kang*

Main category: cs.CV

TL;DR: EA-3DGS improves outdoor scene rendering by combining mesh-based Gaussian initialization, pruning, densification, and vector quantization for efficiency and quality.


<details>
  <summary>Details</summary>
Motivation: Current NeRF-based methods are slow, and 3DGS struggles with memory and adjustment in outdoor scenes.

Method: Uses adaptive tetrahedral mesh for Gaussian initialization, pruning, densification, and vector quantization.

Result: Achieves high-quality real-time rendering on 13 diverse scenes with reduced memory usage.

Conclusion: EA-3DGS is superior for outdoor scenes, balancing speed, quality, and resource efficiency.

Abstract: Efficient scene representations are essential for many real-world
applications, especially those involving spatial measurement. Although current
NeRF-based methods have achieved impressive results in reconstructing
building-scale scenes, they still suffer from slow training and inference
speeds due to time-consuming stochastic sampling. Recently, 3D Gaussian
Splatting (3DGS) has demonstrated excellent performance with its high-quality
rendering and real-time speed, especially for objects and small-scale scenes.
However, in outdoor scenes, its point-based explicit representation lacks an
effective adjustment mechanism, and the millions of Gaussian points required
often lead to memory constraints during training. To address these challenges,
we propose EA-3DGS, a high-quality real-time rendering method designed for
outdoor scenes. First, we introduce a mesh structure to regulate the
initialization of Gaussian components by leveraging an adaptive tetrahedral
mesh that partitions the grid and initializes Gaussian components on each face,
effectively capturing geometric structures in low-texture regions. Second, we
propose an efficient Gaussian pruning strategy that evaluates each 3D
Gaussian's contribution to the view and prunes accordingly. To retain
geometry-critical Gaussian points, we also present a structure-aware
densification strategy that densifies Gaussian points in low-curvature regions.
Additionally, we employ vector quantization for parameter quantization of
Gaussian components, significantly reducing disk space requirements with only a
minimal impact on rendering quality. Extensive experiments on 13 scenes,
including eight from four public datasets (MatrixCity-Aerial, Mill-19, Tanks \&
Temples, WHU) and five self-collected scenes acquired through UAV
photogrammetry measurement from SCUT-CA and plateau regions, further
demonstrate the superiority of our method.

</details>


### [140] [MoCLIP: Motion-Aware Fine-Tuning and Distillation of CLIP for Human Motion Generation](https://arxiv.org/pdf/2505.10810)
*Gabriel Maldonado, Armin Danesh Pazho, Ghazal Alinezhad Noghre, Vinit Katariya, Hamed Tabkhi*

Main category: cs.CV

TL;DR: MoCLIP is a fine-tuned CLIP model with a motion encoding head, improving text-to-motion alignment by incorporating motion-aware representations.


<details>
  <summary>Details</summary>
Motivation: Existing CLIP-based text encoders lack understanding of temporal and kinematic structures in motion, limiting their effectiveness in motion generation.

Method: MoCLIP adds a motion encoding head to CLIP, trained on motion sequences using contrastive learning and tethering loss.

Result: MoCLIP improves Top-1, Top-2, and Top-3 accuracy while maintaining competitive FID, enhancing motion fidelity.

Conclusion: MoCLIP is a versatile and effective framework for improving motion generation, compatible with existing CLIP-based methods.

Abstract: Human motion generation is essential for fields such as animation, robotics,
and virtual reality, requiring models that effectively capture motion dynamics
from text descriptions. Existing approaches often rely on Contrastive
Language-Image Pretraining (CLIP)-based text encoders, but their training on
text-image pairs constrains their ability to understand temporal and kinematic
structures inherent in motion and motion generation. This work introduces
MoCLIP, a fine-tuned CLIP model with an additional motion encoding head,
trained on motion sequences using contrastive learning and tethering loss. By
explicitly incorporating motion-aware representations, MoCLIP enhances motion
fidelity while remaining compatible with existing CLIP-based pipelines and
seamlessly integrating into various CLIP-based methods. Experiments demonstrate
that MoCLIP improves Top-1, Top-2, and Top-3 accuracy while maintaining
competitive FID, leading to improved text-to-motion alignment results. These
results highlight MoCLIP's versatility and effectiveness, establishing it as a
robust framework for enhancing motion generation.

</details>


### [141] [A High-Performance Thermal Infrared Object Detection Framework with Centralized Regulation](https://arxiv.org/pdf/2505.10825)
*Jinke Li, Yue Wu, Xiaoyan Yang*

Main category: cs.CV

TL;DR: CRT-YOLO is a new TIR object detection framework that improves local-global feature fusion using centralized feature regulation and multi-scale attention, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional TIR object detection methods fail to effectively extract and fuse local-global information, limiting performance.

Method: CRT-YOLO integrates EMA modules for long-range dependencies and a CFP network for global feature regulation.

Result: Experiments show CRT-YOLO outperforms conventional methods on benchmark datasets.

Conclusion: The proposed modules are effective, advancing TIR object detection.

Abstract: Thermal Infrared (TIR) technology involves the use of sensors to detect and
measure infrared radiation emitted by objects, and it is widely utilized across
a broad spectrum of applications. The advancements in object detection methods
utilizing TIR images have sparked significant research interest. However, most
traditional methods lack the capability to effectively extract and fuse
local-global information, which is crucial for TIR-domain feature attention. In
this study, we present a novel and efficient thermal infrared object detection
framework, known as CRT-YOLO, that is based on centralized feature regulation,
enabling the establishment of global-range interaction on TIR information. Our
proposed model integrates efficient multi-scale attention (EMA) modules, which
adeptly capture long-range dependencies while incurring minimal computational
overhead. Additionally, it leverages the Centralized Feature Pyramid (CFP)
network, which offers global regulation of TIR features. Extensive experiments
conducted on two benchmark datasets demonstrate that our CRT-YOLO model
significantly outperforms conventional methods for TIR image object detection.
Furthermore, the ablation study provides compelling evidence of the
effectiveness of our proposed modules, reinforcing the potential impact of our
approach on advancing the field of thermal infrared object detection.

</details>


### [142] [NeuSEditor: From Multi-View Images to Text-Guided Neural Surface Edits](https://arxiv.org/pdf/2505.10827)
*Nail Ibrahimli, Julian F. P. Kooij, Liangliang Nan*

Main category: cs.CV

TL;DR: NeuSEditor is a novel method for text-guided editing of neural implicit surfaces, addressing challenges in identity preservation and geometric consistency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for editing implicit surfaces struggle with preserving identity and geometric consistency, prompting the need for a more efficient solution.

Method: NeuSEditor uses an identity-preserving architecture to separate scenes into foreground and background, coupled with a geometry-aware distillation loss for improved rendering and geometry.

Result: NeuSEditor outperforms state-of-the-art methods like PDS and InstructNeRF2NeRF, delivering better quantitative and qualitative results.

Conclusion: NeuSEditor simplifies editing workflows and enhances geometric quality, making it a superior solution for neural implicit surface editing.

Abstract: Implicit surface representations are valued for their compactness and
continuity, but they pose significant challenges for editing. Despite recent
advancements, existing methods often fail to preserve identity and maintain
geometric consistency during editing. To address these challenges, we present
NeuSEditor, a novel method for text-guided editing of neural implicit surfaces
derived from multi-view images. NeuSEditor introduces an identity-preserving
architecture that efficiently separates scenes into foreground and background,
enabling precise modifications without altering the scene-specific elements.
Our geometry-aware distillation loss significantly enhances rendering and
geometric quality. Our method simplifies the editing workflow by eliminating
the need for continuous dataset updates and source prompting. NeuSEditor
outperforms recent state-of-the-art methods like PDS and InstructNeRF2NeRF,
delivering superior quantitative and qualitative results. For more visual
results, visit: neuseditor.github.io.

</details>


### [143] [RefPose: Leveraging Reference Geometric Correspondences for Accurate 6D Pose Estimation of Unseen Objects](https://arxiv.org/pdf/2505.10841)
*Jaeguk Kim, Jaewoo Park, Keuntek Lee, Nam Ik Cho*

Main category: cs.CV

TL;DR: RefPose is a novel method for 6D pose estimation of unseen objects using reference images and geometric correspondence, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The challenge of estimating 6D poses of unseen objects without prior knowledge drives the need for an adaptable solution.

Method: RefPose uses reference images and geometric correspondence, refining poses iteratively with a render-and-compare approach and a correlation volume-guided attention mechanism.

Result: RefPose outperforms traditional methods on the BOP benchmark, showing robust performance on unseen objects.

Conclusion: RefPose offers a dynamic and effective solution for 6D pose estimation, excelling in adaptability and performance.

Abstract: Estimating the 6D pose of unseen objects from monocular RGB images remains a
challenging problem, especially due to the lack of prior object-specific
knowledge. To tackle this issue, we propose RefPose, an innovative approach to
object pose estimation that leverages a reference image and geometric
correspondence as guidance. RefPose first predicts an initial pose by using
object templates to render the reference image and establish the geometric
correspondence needed for the refinement stage. During the refinement stage,
RefPose estimates the geometric correspondence of the query based on the
generated references and iteratively refines the pose through a
render-and-compare approach. To enhance this estimation, we introduce a
correlation volume-guided attention mechanism that effectively captures
correlations between the query and reference images. Unlike traditional methods
that depend on pre-defined object models, RefPose dynamically adapts to new
object shapes by leveraging a reference image and geometric correspondence.
This results in robust performance across previously unseen objects. Extensive
evaluation on the BOP benchmark datasets shows that RefPose achieves
state-of-the-art results while maintaining a competitive runtime.

</details>


### [144] [A Convolution-Based Gait Asymmetry Metric for Inter-Limb Synergistic Coordination](https://arxiv.org/pdf/2505.10869)
*Go Fukino, Kanta Tachibana*

Main category: cs.CV

TL;DR: The paper proposes a new method using an LTI system and a dissimilarity metric to evaluate gait symmetry, tested on five subjects.


<details>
  <summary>Details</summary>
Motivation: Traditional gait symmetry assessments rely on EMG signals or acceleration differences, which may not fully capture intersegmental coordination.

Method: The study models intersegmental coordination with an LTI system and introduces a dissimilarity metric for symmetry evaluation.

Result: The method was validated on five subjects, including those with symmetric and asymmetric gait.

Conclusion: The proposed approach offers a novel way to assess gait symmetry by focusing on intersegmental coordination.

Abstract: This study focuses on the velocity patterns of various body parts during
walking and proposes a method for evaluating gait symmetry. Traditional motion
analysis studies have assessed gait symmetry based on differences in
electromyographic (EMG) signals or acceleration between the left and right
sides. In contrast, this paper models intersegmental coordination using an LTI
system and proposes a dissimilarity metric to evaluate symmetry. The method was
tested on five subjects with both symmetric and asymmetric gait.

</details>


### [145] [A Light and Smart Wearable Platform with Multimodal Foundation Model for Enhanced Spatial Reasoning in People with Blindness and Low Vision](https://arxiv.org/pdf/2505.10875)
*Alexey Magay, Dhurba Tripathi, Yu Hao, Yi Fang*

Main category: cs.CV

TL;DR: A novel spatial-enhanced MLLM approach for visually impaired individuals improves navigation and object recognition by integrating spatial reasoning and a hardware attachment for glasses.


<details>
  <summary>Details</summary>
Motivation: Current MLLM models lack spatial reasoning for pBLV, and there's a need for lightweight, user-friendly assistive systems.

Method: Fine-tuning MLLM for spatial reasoning, integrating with a glasses attachment for real-time feedback.

Result: Substantial improvements in accuracy and user experience, validated on VizWiz and a custom dataset.

Conclusion: The approach bridges ML models and practical assistive devices, enhancing independence for visually impaired users.

Abstract: People with blindness and low vision (pBLV) face significant challenges,
struggling to navigate environments and locate objects due to limited visual
cues. Spatial reasoning is crucial for these individuals, as it enables them to
understand and interpret the spatial relationships in their surroundings,
enhancing their ability to navigate and interact more safely and independently.
Current multi-modal large language (MLLM) models for low vision people lack the
spatial reasoning capabilities needed to effectively assist in these tasks.
Moreover, there is a notable absence of lightweight, easy-to-use systems that
allow pBLV to effectively perceive and interact with their surrounding
environment. In this paper, we propose a novel spatial enhanced multi-modal
large language model based approach for visually impaired individuals. By
fine-tuning the MLLM to incorporate spatial reasoning capabilities, our method
significantly improves the understanding of environmental context, which is
critical for navigation and object recognition. The innovation extends to a
hardware component, designed as an attachment for glasses, ensuring increased
accessibility and ease of use. This integration leverages advanced VLMs to
interpret visual data and provide real-time, spatially aware feedback to the
user. Our approach aims to bridge the gap between advanced machine learning
models and practical, user-friendly assistive devices, offering a robust
solution for visually impaired users to navigate their surroundings more
effectively and independently. The paper includes an in-depth evaluation using
the VizWiz dataset, demonstrating substantial improvements in accuracy and user
experience. Additionally, we design a comprehensive dataset to evaluate our
method's effectiveness in realworld situations, demonstrating substantial
improvements in accuracy and user experience.

</details>


### [146] [PoseBench3D: A Cross-Dataset Analysis Framework for 3D Human Pose Estimation](https://arxiv.org/pdf/2505.10888)
*Saad Manzur, Bryan Vela, Brandon Vela, Aditya Agrawal, Lan-Anh Dang-Vu, David Li, Wayne Hayes*

Main category: cs.CV

TL;DR: PoseBench3D is a standardized framework for evaluating 3D human pose estimation methods across diverse datasets, ensuring fair comparisons and analyzing generalization.


<details>
  <summary>Details</summary>
Motivation: Prior work lacks adaptability to diverse real-world conditions; PoseBench3D addresses this by enabling cross-dataset evaluations.

Method: Proposes PoseBench3D, a unified framework for evaluating models on four datasets, supporting future additions. Uses MPJPE and PA-MPJPE metrics.

Result: Re-evaluated 18 methods, producing over 100 cross-dataset results, and analyzed preprocessing impacts on generalization.

Conclusion: PoseBench3D provides a robust, adaptable tool for assessing model performance and generalization in 3D human pose estimation.

Abstract: Reliable three-dimensional human pose estimation is becoming increasingly
important for real-world applications, yet much of prior work has focused
solely on the performance within a single dataset. In practice, however,
systems must adapt to diverse viewpoints, environments, and camera setups --
conditions that differ significantly from those encountered during training,
which is often the case in real-world scenarios. To address these challenges,
we present a standardized testing environment in which each method is evaluated
on a variety of datasets, ensuring consistent and fair cross-dataset
comparisons -- allowing for the analysis of methods on previously unseen data.
Therefore, we propose PoseBench3D, a unified framework designed to
systematically re-evaluate prior and future models across four of the most
widely used datasets for human pose estimation -- with the framework able to
support novel and future datasets as the field progresses. Through a unified
interface, our framework provides datasets in a pre-configured yet easily
modifiable format, ensuring compatibility with diverse model architectures. We
re-evaluated the work of 18 methods, either trained or gathered from existing
literature, and reported results using both Mean Per Joint Position Error
(MPJPE) and Procrustes Aligned Mean Per Joint Position Error (PA-MPJPE)
metrics, yielding more than 100 novel cross-dataset evaluation results.
Additionally, we analyze performance differences resulting from various
pre-processing techniques and dataset preparation parameters -- offering
further insight into model generalization capabilities.

</details>


### [147] [Patient-Specific Dynamic Digital-Physical Twin for Coronary Intervention Training: An Integrated Mixed Reality Approach](https://arxiv.org/pdf/2505.10902)
*Shuo Wang, Tong Ren, Nan Cheng, Rong Wang, Li Zhang*

Main category: cs.CV

TL;DR: A dynamic cardiac model framework using 4D-CTA and digital twin technology was developed for precise coronary intervention planning and training, achieving high accuracy in simulations and practical training applications.


<details>
  <summary>Details</summary>
Motivation: To address the lack of accurate simulation of cardiac physiological dynamics in existing training systems and provide personalized tools for interventional cardiology.

Method: Utilized 4D-CTA data to segment cardiac structures, construct dynamic models, and manufacture transparent vascular physical models. Developed cardiac output analysis, virtual angiography, and guidewire 3D reconstruction systems.

Result: Achieved 80.9% morphological consistency in angiography, guidewire motion Dice coefficients of 0.741-0.812, and mean trajectory errors below 1.1 mm. The transparent model proved effective for CABG training.

Conclusion: The patient-specific digital-physical twin approach successfully replicates coronary anatomy and dynamics, offering valuable visual and tactile feedback for education and clinical planning.

Abstract: Background and Objective: Precise preoperative planning and effective
physician training for coronary interventions are increasingly important.
Despite advances in medical imaging technologies, transforming static or
limited dynamic imaging data into comprehensive dynamic cardiac models remains
challenging. Existing training systems lack accurate simulation of cardiac
physiological dynamics. This study develops a comprehensive dynamic cardiac
model research framework based on 4D-CTA, integrating digital twin technology,
computer vision, and physical model manufacturing to provide precise,
personalized tools for interventional cardiology. Methods: Using 4D-CTA data
from a 60-year-old female with three-vessel coronary stenosis, we segmented
cardiac chambers and coronary arteries, constructed dynamic models, and
implemented skeletal skinning weight computation to simulate vessel deformation
across 20 cardiac phases. Transparent vascular physical models were
manufactured using medical-grade silicone. We developed cardiac output analysis
and virtual angiography systems, implemented guidewire 3D reconstruction using
binocular stereo vision, and evaluated the system through angiography
validation and CABG training applications. Results: Morphological consistency
between virtual and real angiography reached 80.9%. Dice similarity
coefficients for guidewire motion ranged from 0.741-0.812, with mean trajectory
errors below 1.1 mm. The transparent model demonstrated advantages in CABG
training, allowing direct visualization while simulating beating heart
challenges. Conclusion: Our patient-specific digital-physical twin approach
effectively reproduces both anatomical structures and dynamic characteristics
of coronary vasculature, offering a dynamic environment with visual and tactile
feedback valuable for education and clinical planning.

</details>


### [148] [VISTA: Enhancing Vision-Text Alignment in MLLMs via Cross-Modal Mutual Information Maximization](https://arxiv.org/pdf/2505.10917)
*Mingxiao Li, Na Su, Fang Qu, Zhizhou Zhong, Ziyang Chen, Zhaopeng Tu, Xiaolong Li*

Main category: cs.CV

TL;DR: The paper identifies a bias in multimodal large language models (MLLMs) favoring text over other modalities. It proposes VISTA, a novel method to improve cross-modal alignment without extra modules or data.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with modality alignment, often prioritizing text over vision, which limits effective multimodal fusion.

Method: The paper analyzes the cross-entropy loss in MLLMs, revealing its limitations, and introduces VISTA, an explicit alignment objective to maximize cross-modal mutual information.

Result: VISTA outperforms baselines on multiple benchmarks (e.g., VQAv2, MMStar, MME) without additional training resources.

Conclusion: VISTA offers an efficient solution to improve MLLM alignment, advancing research in multimodal fusion.

Abstract: Current multimodal large language models (MLLMs) face a critical challenge in
modality alignment, often exhibiting a bias towards textual information at the
expense of other modalities like vision. This paper conducts a systematic
information-theoretic analysis of the widely used cross-entropy loss in MLLMs,
uncovering its implicit alignment objective. Our theoretical investigation
reveals that this implicit objective has inherent limitations, leading to a
degradation of cross-modal alignment as text sequence length increases, thereby
hindering effective multimodal information fusion. To overcome these drawbacks,
we propose Vision-Text Alignment (VISTA), a novel approach guided by our
theoretical insights. VISTA introduces an explicit alignment objective designed
to maximize cross-modal mutual information, preventing the degradation of
visual alignment. Notably, VISTA enhances the visual understanding capabilities
of existing MLLMs without requiring any additional trainable modules or extra
training data, making it both efficient and practical. Our method significantly
outperforms baseline models across more than a dozen benchmark datasets,
including VQAv2, MMStar, and MME, paving the way for new directions in MLLM
modal alignment research.

</details>


### [149] [Towards Cross-modal Retrieval in Chinese Cultural Heritage Documents: Dataset and Solution](https://arxiv.org/pdf/2505.10921)
*Junyi Yuan, Jian Zhang, Fangyu Wu, Dongming Lu, Huanda Lu, Qiufeng Wang*

Main category: cs.CV

TL;DR: The paper introduces CulTi, a multimodal dataset for Chinese cultural heritage, and LACLIP, a training-free local alignment method for cross-modal retrieval, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: The lack of specialized datasets for Chinese cultural heritage limits cross-modal learning. CulTi and LACLIP aim to bridge this gap.

Method: Proposes CulTi dataset (5,726 image-text pairs) and LACLIP, a local alignment strategy using a fine-tuned Chinese-CLIP for weighted similarity scoring.

Result: LACLIP significantly outperforms existing models in cross-modal retrieval, especially for fine-grained semantic associations.

Conclusion: CulTi and LACLIP advance cross-modal retrieval for Chinese cultural heritage, addressing local alignment challenges.

Abstract: China has a long and rich history, encompassing a vast cultural heritage that
includes diverse multimodal information, such as silk patterns, Dunhuang
murals, and their associated historical narratives. Cross-modal retrieval plays
a pivotal role in understanding and interpreting Chinese cultural heritage by
bridging visual and textual modalities to enable accurate text-to-image and
image-to-text retrieval. However, despite the growing interest in multimodal
research, there is a lack of specialized datasets dedicated to Chinese cultural
heritage, limiting the development and evaluation of cross-modal learning
models in this domain. To address this gap, we propose a multimodal dataset
named CulTi, which contains 5,726 image-text pairs extracted from two series of
professional documents, respectively related to ancient Chinese silk and
Dunhuang murals. Compared to existing general-domain multimodal datasets, CulTi
presents a challenge for cross-modal retrieval: the difficulty of local
alignment between intricate decorative motifs and specialized textual
descriptions. To address this challenge, we propose LACLIP, a training-free
local alignment strategy built upon a fine-tuned Chinese-CLIP. LACLIP enhances
the alignment of global textual descriptions with local visual regions by
computing weighted similarity scores during inference. Experimental results on
CulTi demonstrate that LACLIP significantly outperforms existing models in
cross-modal retrieval, particularly in handling fine-grained semantic
associations within Chinese cultural heritage.

</details>


### [150] [M4-SAR: A Multi-Resolution, Multi-Polarization, Multi-Scene, Multi-Source Dataset and Benchmark for Optical-SAR Fusion Object Detection](https://arxiv.org/pdf/2505.10931)
*Chao Wang, Wei Lu, Xiang Li, Jian Yang, Lei Luo*

Main category: cs.CV

TL;DR: The paper introduces M4-SAR, a large-scale dataset for optical-SAR fusion object detection, and proposes E2E-OSDet, a framework improving detection accuracy by 5.7% over single-source methods.


<details>
  <summary>Details</summary>
Motivation: Single-source remote sensing (optical or SAR) struggles in complex environments due to limitations like weather effects or noise. Fusion of both can improve accuracy, but lacks standardized datasets.

Method: The authors create M4-SAR, a dataset with 112,184 aligned optical-SAR pairs and nearly one million labeled instances. They also propose E2E-OSDet, an end-to-end fusion framework, and a benchmarking toolkit.

Result: Fusing optical and SAR data improves mAP by 5.7%, especially in complex environments.

Conclusion: M4-SAR and E2E-OSDet address dataset and method gaps, providing a robust baseline for future optical-SAR fusion research.

Abstract: Single-source remote sensing object detection using optical or SAR images
struggles in complex environments. Optical images offer rich textural details
but are often affected by low-light, cloud-obscured, or low-resolution
conditions, reducing the detection performance. SAR images are robust to
weather, but suffer from speckle noise and limited semantic expressiveness.
Optical and SAR images provide complementary advantages, and fusing them can
significantly improve the detection accuracy. However, progress in this field
is hindered by the lack of large-scale, standardized datasets. To address these
challenges, we propose the first comprehensive dataset for optical-SAR fusion
object detection, named Multi-resolution, Multi-polarization, Multi-scene,
Multi-source SAR dataset (M4-SAR). It contains 112,184 precisely aligned image
pairs and nearly one million labeled instances with arbitrary orientations,
spanning six key categories. To enable standardized evaluation, we develop a
unified benchmarking toolkit that integrates six state-of-the-art multi-source
fusion methods. Furthermore, we propose E2E-OSDet, a novel end-to-end
multi-source fusion detection framework that mitigates cross-domain
discrepancies and establishes a robust baseline for future studies. Extensive
experiments on M4-SAR demonstrate that fusing optical and SAR data can improve
$mAP$ by 5.7\% over single-source inputs, with particularly significant gains
in complex environments. The dataset and code are publicly available at
https://github.com/wchao0601/M4-SAR.

</details>


### [151] [Visual Anomaly Detection under Complex View-Illumination Interplay: A Large-Scale Benchmark](https://arxiv.org/pdf/2505.10996)
*Yunkang Cao, Yuqi Cheng, Xiaohao Xu, Yiheng Zhang, Yihan Sun, Yuxiang Tan, Yuxin Zhang, Xiaonan Huang, Weiming Shen*

Main category: cs.CV

TL;DR: M2AD is a new benchmark for Visual Anomaly Detection (VAD) to test robustness under varying viewpoints and illumination, showing current methods struggle with these real-world complexities.


<details>
  <summary>Details</summary>
Motivation: Current VAD systems are sensitive to real-world imaging variations like viewpoint and illumination, which are overlooked in existing benchmarks.

Method: M2AD introduces a large-scale dataset (119,880 images) with 120 configurations (12 views, 10 illuminations) and two evaluation protocols: M2AD-Synergy and M2AD-Invariant.

Result: State-of-the-art VAD methods perform poorly on M2AD, highlighting the challenge of view-illumination interplay.

Conclusion: M2AD is a critical tool for advancing VAD methods to handle real-world complexities, with the dataset publicly available.

Abstract: The practical deployment of Visual Anomaly Detection (VAD) systems is
hindered by their sensitivity to real-world imaging variations, particularly
the complex interplay between viewpoint and illumination which drastically
alters defect visibility. Current benchmarks largely overlook this critical
challenge. We introduce Multi-View Multi-Illumination Anomaly Detection (M2AD),
a new large-scale benchmark comprising 119,880 high-resolution images designed
explicitly to probe VAD robustness under such interacting conditions. By
systematically capturing 999 specimens across 10 categories using 12
synchronized views and 10 illumination settings (120 configurations total),
M2AD enables rigorous evaluation. We establish two evaluation protocols:
M2AD-Synergy tests the ability to fuse information across diverse
configurations, and M2AD-Invariant measures single-image robustness against
realistic view-illumination effects. Our extensive benchmarking shows that
state-of-the-art VAD methods struggle significantly on M2AD, demonstrating the
profound challenge posed by view-illumination interplay. This benchmark serves
as an essential tool for developing and validating VAD methods capable of
overcoming real-world complexities. Our full dataset and test suite will be
released at https://hustcyq.github.io/M2AD to facilitate the field.

</details>


### [152] [DDAE++: Enhancing Diffusion Models Towards Unified Generative and Discriminative Learning](https://arxiv.org/pdf/2505.10999)
*Weilai Xiang, Hongyu Yang, Di Huang, Yunhong Wang*

Main category: cs.CV

TL;DR: The paper introduces self-conditioning in diffusion models to improve both generative and discriminative performance without significant computational cost.


<details>
  <summary>Details</summary>
Motivation: To address whether diffusion models' representations can enhance their own training and rival self-supervised learners without losing generative capability.

Method: Proposes self-conditioning, a mechanism using denoising network semantics to guide decoding layers, forming a bottleneck for better generation.

Result: Boosts generation FID and recognition accuracy with minimal overhead; outperforms self-supervised models in linear evaluations.

Conclusion: Self-conditioning effectively integrates discriminative techniques into diffusion models, enhancing both generation and representation learning.

Abstract: While diffusion models have gained prominence in image synthesis, their
generative pre-training has been shown to yield discriminative representations,
paving the way towards unified visual generation and understanding. However,
two key questions remain: 1) Can these representations be leveraged to improve
the training of diffusion models themselves, rather than solely benefiting
downstream tasks? 2) Can the feature quality be enhanced to rival or even
surpass modern self-supervised learners, without compromising generative
capability? This work addresses these questions by introducing
self-conditioning, a straightforward yet effective mechanism that internally
leverages the rich semantics inherent in denoising network to guide its own
decoding layers, forming a tighter bottleneck that condenses high-level
semantics to improve generation. Results are compelling: our method boosts both
generation FID and recognition accuracy with 1% computational overhead and
generalizes across diverse diffusion architectures. Crucially,
self-conditioning facilitates an effective integration of discriminative
techniques, such as contrastive self-distillation, directly into diffusion
models without sacrificing generation quality. Extensive experiments on
pixel-space and latent-space datasets show that in linear evaluations, our
enhanced diffusion models, particularly UViT and DiT, serve as strong
representation learners, surpassing various self-supervised models.

</details>


### [153] [ForensicHub: A Unified Benchmark & Codebase for All-Domain Fake Image Detection and Localization](https://arxiv.org/pdf/2505.11003)
*Bo Du, Xuekang Zhu, Xiaochen Ma, Chenfan Qu, Kaiwen Feng, Zhe Yang, Chi-Man Pun, Jian Liu, Jizhe Zhou*

Main category: cs.CV

TL;DR: ForensicHub is the first unified benchmark and codebase for Fake Image Detection and Localization (FIDL), addressing fragmentation across four domains by modular design, baseline implementations, and cross-domain analysis.


<details>
  <summary>Details</summary>
Motivation: The FIDL field is fragmented into four domains with no unified benchmark, leading to silos and hindering cross-domain progress. ForensicHub aims to bridge this gap.

Method: ForensicHub introduces a modular, configuration-driven architecture, implements 10 baseline models, 6 backbones, and integrates existing benchmarks while adding new ones for AIGC and Doc domains.

Result: The framework enables flexible composition across domains, provides 8 actionable insights, and fosters interoperability and comparison.

Conclusion: ForensicHub breaks domain silos in FIDL, offering a foundation for future advancements in fake image detection and localization.

Abstract: The field of Fake Image Detection and Localization (FIDL) is highly
fragmented, encompassing four domains: deepfake detection (Deepfake), image
manipulation detection and localization (IMDL), artificial
intelligence-generated image detection (AIGC), and document image manipulation
localization (Doc). Although individual benchmarks exist in some domains, a
unified benchmark for all domains in FIDL remains blank. The absence of a
unified benchmark results in significant domain silos, where each domain
independently constructs its datasets, models, and evaluation protocols without
interoperability, preventing cross-domain comparisons and hindering the
development of the entire FIDL field. To close the domain silo barrier, we
propose ForensicHub, the first unified benchmark & codebase for all-domain fake
image detection and localization. Considering drastic variations on dataset,
model, and evaluation configurations across all domains, as well as the
scarcity of open-sourced baseline models and the lack of individual benchmarks
in some domains, ForensicHub: i) proposes a modular and configuration-driven
architecture that decomposes forensic pipelines into interchangeable components
across datasets, transforms, models, and evaluators, allowing flexible
composition across all domains; ii) fully implements 10 baseline models, 6
backbones, 2 new benchmarks for AIGC and Doc, and integrates 2 existing
benchmarks of DeepfakeBench and IMDLBenCo through an adapter-based design; iii)
conducts indepth analysis based on the ForensicHub, offering 8 key actionable
insights into FIDL model architecture, dataset characteristics, and evaluation
standards. ForensicHub represents a significant leap forward in breaking the
domain silos in the FIDL field and inspiring future breakthroughs.

</details>


### [154] [WildDoc: How Far Are We from Achieving Comprehensive and Robust Document Understanding in the Wild?](https://arxiv.org/pdf/2505.11015)
*An-Lan Wang, Jingqun Tang, Liao Lei, Hao Feng, Qi Liu, Xiang Fei, Jinghui Lu, Han Wang, Weiwei Liu, Hao Liu, Yuliang Liu, Xiang Bai, Can Huang*

Main category: cs.CV

TL;DR: WildDoc is a new benchmark for evaluating document understanding in natural environments, highlighting the limitations of current MLLMs in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks like DocVQA and ChartQA focus on scanned or digital documents, failing to address real-world challenges like variable illumination and physical distortions.

Method: WildDoc introduces manually captured document images under diverse real-world conditions and evaluates MLLMs on robustness by capturing each document four times under varying conditions.

Result: State-of-the-art MLLMs show significant performance drops on WildDoc, revealing their lack of robustness in real-world document understanding.

Conclusion: WildDoc exposes critical gaps in current MLLMs' capabilities, emphasizing the need for improved models to handle real-world document challenges.

Abstract: The rapid advancements in Multimodal Large Language Models (MLLMs) have
significantly enhanced capabilities in Document Understanding. However,
prevailing benchmarks like DocVQA and ChartQA predominantly comprise
\textit{scanned or digital} documents, inadequately reflecting the intricate
challenges posed by diverse real-world scenarios, such as variable illumination
and physical distortions. This paper introduces WildDoc, the inaugural
benchmark designed specifically for assessing document understanding in natural
environments. WildDoc incorporates a diverse set of manually captured document
images reflecting real-world conditions and leverages document sources from
established benchmarks to facilitate comprehensive comparisons with digital or
scanned documents. Further, to rigorously evaluate model robustness, each
document is captured four times under different conditions. Evaluations of
state-of-the-art MLLMs on WildDoc expose substantial performance declines and
underscore the models' inadequate robustness compared to traditional
benchmarks, highlighting the unique challenges posed by real-world document
understanding. Our project homepage is available at
https://bytedance.github.io/WildDoc.

</details>


### [155] [Rethinking the Mean Teacher Strategy from the Perspective of Self-paced Learning](https://arxiv.org/pdf/2505.11018)
*Pengchen Zhang, Alan J. X. Guo, Sipin Luo, Zhe Han, Lin Guo*

Main category: cs.CV

TL;DR: The paper proposes Dual Teacher-Student Learning (DTSL) for semi-supervised medical image segmentation, leveraging cross-architectural models and Jensen-Shannon divergence for pseudo-labeling, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To reduce manual annotation costs in medical image segmentation by enhancing semi-supervised learning with cross-architectural model agreement.

Method: Introduces DTSL with two teacher-student model groups, using output agreement and a Jensen-Shannon divergence-based consensus label generator for pseudo-labels.

Result: Outperforms state-of-the-art methods on popular datasets; ablation studies confirm module effectiveness.

Conclusion: DTSL offers a flexible, effective approach for semi-supervised medical image segmentation, validated by superior performance.

Abstract: Semi-supervised medical image segmentation has attracted significant
attention due to its potential to reduce manual annotation costs. The mean
teacher (MT) strategy, commonly understood as introducing smoothed, temporally
lagged consistency regularization, has demonstrated strong performance across
various tasks in this field. In this work, we reinterpret the MT strategy on
supervised data as a form of self-paced learning, regulated by the output
agreement between the temporally lagged teacher model and the ground truth
labels. This idea is further extended to incorporate agreement between a
temporally lagged model and a cross-architectural model, which offers greater
flexibility in regulating the learning pace and enables application to
unlabeled data. Specifically, we propose dual teacher-student learning (DTSL),
a framework that introduces two groups of teacher-student models with different
architectures. The output agreement between the cross-group teacher and student
models is used as pseudo-labels, generated via a Jensen-Shannon
divergence-based consensus label generator (CLG). Extensive experiments on
popular datasets demonstrate that the proposed method consistently outperforms
existing state-of-the-art approaches. Ablation studies further validate the
effectiveness of the proposed modules.

</details>


### [156] [CleanPatrick: A Benchmark for Image Data Cleaning](https://arxiv.org/pdf/2505.11034)
*Fabian Gr√∂ger, Simone Lionetti, Philippe Gottfrois, Alvaro Gonzalez-Jimenez, Ludovic Amruthalingam, Elisabeth Victoria Goessinger, Hanna Lindemann, Marie Bargiela, Marie Hofbauer, Omar Badri, Philipp Tschandl, Arash Koochek, Matthew Groh, Alexander A. Navarini, Marc Pouly*

Main category: cs.CV

TL;DR: CleanPatrick is a large-scale benchmark for image data cleaning, built on the Fitzpatrick17k dataset, addressing off-topic samples, near-duplicates, and label errors. It evaluates various methods, revealing strengths in self-supervised representations and classical anomaly detection, while highlighting challenges in label-error detection.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for image data cleaning rely on synthetic noise or limited human studies, lacking real-world relevance and comparability. CleanPatrick aims to provide a robust, large-scale benchmark for systematic evaluation.

Method: The benchmark uses the Fitzpatrick17k dataset, collecting 496,377 binary annotations from 933 medical crowd workers. It identifies off-topic samples (4%), near-duplicates (21%), and label errors (22%), employing an aggregation model and expert review for high-quality ground truth. Issue detection is formalized as a ranking task.

Result: Self-supervised representations excel at near-duplicate detection, classical methods perform well for off-topic detection under budget constraints, and label-error detection remains challenging for fine-grained medical classification.

Conclusion: CleanPatrick enables systematic comparison of image-cleaning strategies, advancing reliable data-centric AI. The dataset and framework are released to support further research.

Abstract: Robust machine learning depends on clean data, yet current image data
cleaning benchmarks rely on synthetic noise or narrow human studies, limiting
comparison and real-world relevance. We introduce CleanPatrick, the first
large-scale benchmark for data cleaning in the image domain, built upon the
publicly available Fitzpatrick17k dermatology dataset. We collect 496,377
binary annotations from 933 medical crowd workers, identify off-topic samples
(4%), near-duplicates (21%), and label errors (22%), and employ an aggregation
model inspired by item-response theory followed by expert review to derive
high-quality ground truth. CleanPatrick formalizes issue detection as a ranking
task and adopts typical ranking metrics mirroring real audit workflows.
Benchmarking classical anomaly detectors, perceptual hashing, SSIM, Confident
Learning, NoiseRank, and SelfClean, we find that, on CleanPatrick,
self-supervised representations excel at near-duplicate detection, classical
methods achieve competitive off-topic detection under constrained review
budgets, and label-error detection remains an open challenge for fine-grained
medical classification. By releasing both the dataset and the evaluation
framework, CleanPatrick enables a systematic comparison of image-cleaning
strategies and paves the way for more reliable data-centric artificial
intelligence.

</details>


### [157] [Artifacts of Idiosyncracy in Global Street View Data](https://arxiv.org/pdf/2505.11046)
*Tim Alpherts, Sennay Ghebreab, Nanne van Noord*

Main category: cs.CV

TL;DR: The paper reveals biases in street view data due to city idiosyncrasies, proposes a method to evaluate coverage, and includes a case study on Amsterdam to address biases.


<details>
  <summary>Details</summary>
Motivation: Street view data is assumed to represent cities systematically, but prior work shows coverage gaps. This study explores how city layouts introduce biases even in densely sampled data.

Method: Quantitative analysis of street view data coverage biases across 28 cities, proposing an evaluation method. A case study in Amsterdam with semi-structured interviews examines collection process impacts.

Result: Biases in street view data distribution are uncovered, linked to city idiosyncrasies. The case study highlights how collection processes affect representation.

Conclusion: City-specific factors cause biases in street view data. The proposed evaluation method and case study provide insights to mitigate these biases at their source.

Abstract: Street view data is increasingly being used in computer vision applications
in recent years. Machine learning datasets are collected for these applications
using simple sampling techniques. These datasets are assumed to be a systematic
representation of cities, especially when densely sampled. Prior works however,
show that there are clear gaps in coverage, with certain cities or regions
being covered poorly or not at all. Here we demonstrate that a cities'
idiosyncracies, such as city layout, may lead to biases in street view data for
28 cities across the globe, even when they are densely covered. We
quantitatively uncover biases in the distribution of coverage of street view
data and propose a method for evaluation of such distributions to get better
insight in idiosyncracies in a cities' coverage. In addition, we perform a case
study of Amsterdam with semi-structured interviews, showing how idiosyncracies
of the collection process impact representation of cities and regions and
allowing us to address biases at their source.

</details>


### [158] [CUBIC: Concept Embeddings for Unsupervised Bias Identification using VLMs](https://arxiv.org/pdf/2505.11060)
*David M√©ndez, Gianpaolo Bontempo, Elisa Ficarra, Roberto Confalonieri, Natalia D√≠az-Rodr√≠guez*

Main category: cs.CV

TL;DR: CUBIC is a novel method for unsupervised bias identification in deep vision models, using concept embeddings without predefined bias candidates or failure examples.


<details>
  <summary>Details</summary>
Motivation: Deep vision models often rely on dataset biases, but identifying these biases is challenging due to the lack of annotated concepts.

Method: CUBIC leverages image-text latent space and linear classifier probes to measure how concepts influence model predictions.

Result: CUBIC effectively uncovers unknown biases in Vision-Language Models without needing prior knowledge or failure samples.

Conclusion: CUBIC provides a scalable, unsupervised solution for bias identification in deep learning models.

Abstract: Deep vision models often rely on biases learned from spurious correlations in
datasets. To identify these biases, methods that interpret high-level,
human-understandable concepts are more effective than those relying primarily
on low-level features like heatmaps. A major challenge for these concept-based
methods is the lack of image annotations indicating potentially bias-inducing
concepts, since creating such annotations requires detailed labeling for each
dataset and concept, which is highly labor-intensive. We present CUBIC (Concept
embeddings for Unsupervised Bias IdentifiCation), a novel method that
automatically discovers interpretable concepts that may bias classifier
behavior. Unlike existing approaches, CUBIC does not rely on predefined bias
candidates or examples of model failures tied to specific biases, as such
information is not always available. Instead, it leverages image-text latent
space and linear classifier probes to examine how the latent representation of
a superclass label$\unicode{x2014}$shared by all instances in the
dataset$\unicode{x2014}$is influenced by the presence of a given concept. By
measuring these shifts against the normal vector to the classifier's decision
boundary, CUBIC identifies concepts that significantly influence model
predictions. Our experiments demonstrate that CUBIC effectively uncovers
previously unknown biases using Vision-Language Models (VLMs) without requiring
the samples in the dataset where the classifier underperforms or prior
knowledge of potential biases.

</details>


### [159] [Towards Self-Improvement of Diffusion Models via Group Preference Optimization](https://arxiv.org/pdf/2505.11070)
*Renjie Chen, Wenfeng Lin, Yichen Zhang, Jiangchuan Wei, Boyuan Liu, Chao Feng, Jiao Ran, Mingyu Guo*

Main category: cs.CV

TL;DR: The paper introduces Group Preference Optimization (GPO), a method to improve text-to-image diffusion models by addressing challenges in Direct Preference Optimization (DPO), such as sensitivity to preference pairs and data collection. GPO extends DPO to groupwise comparisons and uses reward standardization, enhancing performance without extra data or inference overhead.


<details>
  <summary>Details</summary>
Motivation: DPO's sensitivity to preference pairs and the labor-intensive data annotation process limit its effectiveness in text-to-image diffusion models. The paper aims to overcome these challenges.

Method: The authors propose GPO, which extends DPO from pairwise to groupwise comparisons and incorporates reward standardization for reweighting. This leverages the model's own capabilities without external data.

Result: GPO improves performance across diffusion models and tasks, notably enhancing Stable Diffusion 3.5 Medium's counting and text rendering by 20 percentage points when combined with models like YOLO and OCR.

Conclusion: GPO is an effective, plug-and-play method for improving text-to-image diffusion models without additional inference overhead, demonstrating broad applicability and performance gains.

Abstract: Aligning text-to-image (T2I) diffusion models with Direct Preference
Optimization (DPO) has shown notable improvements in generation quality.
However, applying DPO to T2I faces two challenges: the sensitivity of DPO to
preference pairs and the labor-intensive process of collecting and annotating
high-quality data. In this work, we demonstrate that preference pairs with
marginal differences can degrade DPO performance. Since DPO relies exclusively
on relative ranking while disregarding the absolute difference of pairs, it may
misclassify losing samples as wins, or vice versa. We empirically show that
extending the DPO from pairwise to groupwise and incorporating reward
standardization for reweighting leads to performance gains without explicit
data selection. Furthermore, we propose Group Preference Optimization (GPO), an
effective self-improvement method that enhances performance by leveraging the
model's own capabilities without requiring external data. Extensive experiments
demonstrate that GPO is effective across various diffusion models and tasks.
Specifically, combining with widely used computer vision models, such as YOLO
and OCR, the GPO improves the accurate counting and text rendering capabilities
of the Stable Diffusion 3.5 Medium by 20 percentage points. Notably, as a
plug-and-play method, no extra overhead is introduced during inference.

</details>


### [160] [Pseudo-Label Quality Decoupling and Correction for Semi-Supervised Instance Segmentation](https://arxiv.org/pdf/2505.11075)
*Jianghang Lin, Yilin Lu, Yunhang Shen, Chaoyang Zhu, Shengchuan Zhang, Liujuan Cao, Rongrong Ji*

Main category: cs.CV

TL;DR: The paper introduces PL-DC, a framework for Semi-Supervised Instance Segmentation (SSIS) that decouples and corrects pseudo-label quality at instance, category, and pixel levels, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The challenge of unstable performance in SSIS due to noisy pseudo-labels for instance categories and pixel masks motivates the need for a better approach.

Method: PL-DC uses a decoupled dual-threshold filtering mechanism, dynamic instance category correction, and a pixel-level mask uncertainty-aware mechanism.

Result: PL-DC achieves significant improvements, e.g., +11.6 mAP with 1% COCO data and +15.5 mAP with 5% Cityscapes data.

Conclusion: PL-DC effectively addresses pseudo-label noise in SSIS, setting new benchmarks with minimal labeled data.

Abstract: Semi-Supervised Instance Segmentation (SSIS) involves classifying and
grouping image pixels into distinct object instances using limited labeled
data. This learning paradigm usually faces a significant challenge of unstable
performance caused by noisy pseudo-labels of instance categories and pixel
masks. We find that the prevalent practice of filtering instance pseudo-labels
assessing both class and mask quality with a single score threshold, frequently
leads to compromises in the trade-off between the qualities of class and mask
labels. In this paper, we introduce a novel Pseudo-Label Quality Decoupling and
Correction (PL-DC) framework for SSIS to tackle the above challenges. Firstly,
at the instance level, a decoupled dual-threshold filtering mechanism is
designed to decouple class and mask quality estimations for instance-level
pseudo-labels, thereby independently controlling pixel classifying and grouping
qualities. Secondly, at the category level, we introduce a dynamic instance
category correction module to dynamically correct the pseudo-labels of instance
categories, effectively alleviating category confusion. Lastly, we introduce a
pixel-level mask uncertainty-aware mechanism at the pixel level to re-weight
the mask loss for different pixels, thereby reducing the impact of noise
introduced by pixel-level mask pseudo-labels. Extensive experiments on the COCO
and Cityscapes datasets demonstrate that the proposed PL-DC achieves
significant performance improvements, setting new state-of-the-art results for
SSIS. Notably, our PL-DC shows substantial gains even with minimal labeled
data, achieving an improvement of +11.6 mAP with just 1% COCO labeled data and
+15.5 mAP with 5% Cityscapes labeled data. The code will be public.

</details>


### [161] [Hybrid-Emba3D: Geometry-Aware and Cross-Path Feature Hybrid Enhanced State Space Model for Point Cloud Classification](https://arxiv.org/pdf/2505.11099)
*Bin Liu, Chunyang Wang, Xuelian Liu, Guan Xi, Ge Zhang, Ziteng Yao, Mengxue Dong*

Main category: cs.CV

TL;DR: Hybrid-Emba3D, a bidirectional Mamba model, enhances point cloud classification by coupling geometry-features and cross-path hybridization, achieving 95.99% accuracy on ModelNet40 with minimal added complexity.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of efficiently extracting local geometric features in point clouds while balancing model complexity, overcoming limitations of unidirectional Mamba architectures.

Method: Proposes Hybrid-Emba3D with geometry-feature coupling and cross-path hybridization, enhancing local feature discriminative power and handling sparse signals.

Result: Achieves state-of-the-art 95.99% classification accuracy on ModelNet40 with only 0.03M additional parameters.

Conclusion: Hybrid-Emba3D effectively balances local feature extraction and global modeling, outperforming traditional SSMs in point cloud classification.

Abstract: The point cloud classification tasks face the dual challenge of efficiently
extracting local geometric features while maintaining model complexity. The
Mamba architecture utilizes the linear complexity advantage of state space
models (SSMs) to overcome the computational bottleneck of Transformers while
balancing global modeling capabilities. However, the inherent contradiction
between its unidirectional dependency and the unordered nature of point clouds
impedes modeling spatial correlation in local neighborhoods, thus constraining
geometric feature extraction. This paper proposes Hybrid-Emba3D, a
bidirectional Mamba model enhanced by geometry-feature coupling and cross-path
feature hybridization. The Local geometric pooling with geometry-feature
coupling mechanism significantly enhances local feature discriminative power
via coordinated propagation and dynamic aggregation of geometric information
between local center points and their neighborhoods, without introducing
additional parameters. The designed Collaborative feature enhancer adopts
dual-path hybridization, effectively handling local mutations and sparse key
signals, breaking through the limitations of traditional SSM long-range
modeling. Experimental results demonstrate that the proposed model achieves a
new SOTA classification accuracy of 95.99% on ModelNet40 with only 0.03M
additional.

</details>


### [162] [Deepfake Forensic Analysis: Source Dataset Attribution and Legal Implications of Synthetic Media Manipulation](https://arxiv.org/pdf/2505.11110)
*Massimiliano Cassia, Luca Guarnera, Mirko Casu, Ignazio Zangara, Sebastiano Battiato*

Main category: cs.CV

TL;DR: A forensic framework identifies GAN-generated images' training datasets using spectral, color, and local features, achieving high accuracy (98-99%) and addressing legal/ethical concerns.


<details>
  <summary>Details</summary>
Motivation: Challenges in verifying GAN-generated media authenticity and tracing dataset origins, impacting copyright, privacy, and legal compliance.

Method: Uses spectral transforms (Fourier/DCT), color metrics, and SIFT features with supervised classifiers (Random Forest, SVM, XGBoost) for dataset attribution.

Result: Achieves 98-99% accuracy in binary and multi-class classification, highlighting frequency-domain features' dominance.

Conclusion: The framework enhances accountability in generative modeling, with applications in forensics, moderation, and litigation.

Abstract: Synthetic media generated by Generative Adversarial Networks (GANs) pose
significant challenges in verifying authenticity and tracing dataset origins,
raising critical concerns in copyright enforcement, privacy protection, and
legal compliance. This paper introduces a novel forensic framework for
identifying the training dataset (e.g., CelebA or FFHQ) of GAN-generated images
through interpretable feature analysis. By integrating spectral transforms
(Fourier/DCT), color distribution metrics, and local feature descriptors
(SIFT), our pipeline extracts discriminative statistical signatures embedded in
synthetic outputs. Supervised classifiers (Random Forest, SVM, XGBoost) achieve
98-99% accuracy in binary classification (real vs. synthetic) and multi-class
dataset attribution across diverse GAN architectures (StyleGAN, AttGAN, GDWCT,
StarGAN, and StyleGAN2). Experimental results highlight the dominance of
frequency-domain features (DCT/FFT) in capturing dataset-specific artifacts,
such as upsampling patterns and spectral irregularities, while color histograms
reveal implicit regularization strategies in GAN training. We further examine
legal and ethical implications, showing how dataset attribution can address
copyright infringement, unauthorized use of personal data, and regulatory
compliance under frameworks like GDPR and California's AB 602. Our framework
advances accountability and governance in generative modeling, with
applications in digital forensics, content moderation, and intellectual
property litigation.

</details>


### [163] [Redundancy-Aware Pretraining of Vision-Language Foundation Models in Remote Sensing](https://arxiv.org/pdf/2505.11121)
*Mathis J√ºrgen Adler, Leonard Hackel, Gencer Sumbul, Beg√ºm Demir*

Main category: cs.CV

TL;DR: The paper introduces a weighted feature aggregation (WFA) strategy for pretraining vision-language models (VLMs) in remote sensing, reducing redundancy in captions and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing redundancy in multiple captions per image during VLM pretraining, which increases computational costs and inefficiencies.

Method: Proposes WFA with two techniques: (i) non-parametric uniqueness (BLEU-based weights) and (ii) learning-based attention for adaptive caption weighting.

Result: Demonstrates improved efficiency and effectiveness in VLM pretraining, validated by text-to-image retrieval performance.

Conclusion: Provides guidelines for technique selection based on task requirements and resource constraints, with publicly available code.

Abstract: The development of foundation models through pretraining of vision-language
models (VLMs) has recently attracted great attention in remote sensing (RS).
VLM pretraining aims to learn image and language alignments from a large number
of image-text pairs. Each pretraining image is often associated with multiple
captions containing redundant information due to repeated or semantically
similar phrases, resulting in increased pretraining and inference time. To
overcome this, we introduce a weighted feature aggregation (WFA) strategy for
VLM pretraining in RS. Our strategy aims to extract and exploit complementary
information from multiple captions per image while reducing redundancies
through feature aggregation with importance weighting. To calculate adaptive
importance weights for different captions of each image, we propose two
techniques: (i) non-parametric uniqueness and (ii) learning-based attention. In
the first technique, importance weights are calculated based on the bilingual
evaluation understudy (BLEU) scores of the captions to emphasize unique
sentences and reduce the influence of repetitive ones. In the second technique,
importance weights are learned through an attention mechanism instead of
relying on hand-crafted features. The effectiveness of the proposed WFA
strategy with the two techniques is analyzed in terms of downstream performance
on text-to-image retrieval in RS. Experimental results show that the proposed
strategy enables efficient and effective pretraining of VLMs in RS. Based on
the experimental analysis, we derive guidelines for selecting appropriate
techniques depending on downstream task requirements and resource constraints.
The code of this work is publicly available at
https://git.tu-berlin.de/rsim/redundacy-aware-rs-vlm.

</details>


### [164] [PhiNet v2: A Mask-Free Brain-Inspired Vision Foundation Model from Video](https://arxiv.org/pdf/2505.11129)
*Makoto Yamada, Kian Ming A. Chai, Ayoub Rhim, Satoki Ishikawa, Mohammad Sabokrou, Yao-Hung Hubert Tsai*

Main category: cs.CV

TL;DR: PhiNet v2 is a Transformer-based SSL model for temporal visual input, achieving competitive performance without strong augmentation, aligning closer to human visual processing.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between SSL in computer vision and biological visual processing by introducing a model that learns from sequential input like humans.

Method: Uses a Transformer-based architecture with variational inference to process temporal visual input without strong augmentation.

Result: Competes with state-of-the-art vision models while learning from sequential input naturally.

Conclusion: A step toward biologically plausible computer vision systems aligned with human cognition.

Abstract: Recent advances in self-supervised learning (SSL) have revolutionized
computer vision through innovative architectures and learning objectives, yet
they have not fully leveraged insights from biological visual processing
systems. Recently, a brain-inspired SSL model named PhiNet was proposed; it is
based on a ResNet backbone and operates on static image inputs with strong
augmentation. In this paper, we introduce PhiNet v2, a novel Transformer-based
architecture that processes temporal visual input (that is, sequences of
images) without relying on strong augmentation. Our model leverages variational
inference to learn robust visual representations from continuous input streams,
similar to human visual processing. Through extensive experimentation, we
demonstrate that PhiNet v2 achieves competitive performance compared to
state-of-the-art vision foundation models, while maintaining the ability to
learn from sequential input without strong data augmentation. This work
represents a significant step toward more biologically plausible computer
vision systems that process visual information in a manner more closely aligned
with human cognitive processes.

</details>


### [165] [One Image is Worth a Thousand Words: A Usability Preservable Text-Image Collaborative Erasing Framework](https://arxiv.org/pdf/2505.11131)
*Feiran Li, Qianqian Xu, Shilong Bao, Zhiyong Yang, Xiaochun Cao, Qingming Huang*

Main category: cs.CV

TL;DR: A novel text-image collaborative framework (Co-Erasing) improves concept erasure in diffusion models by integrating visual supervision, outperforming existing methods in efficacy and usability.


<details>
  <summary>Details</summary>
Motivation: Current concept erasure methods rely on text prompts, creating a gap between text and image modalities, limiting efficacy and usability.

Method: Co-Erasing combines text prompts and undesirable images for negative guidance, refining visual features with text guidance to minimize disruption to benign concepts.

Result: Co-Erasing significantly outperforms state-of-the-art methods, achieving better trade-offs between erasure efficacy and usability.

Conclusion: The proposed framework effectively bridges the text-image gap, enhancing concept erasure while preserving benign content.

Abstract: Concept erasing has recently emerged as an effective paradigm to prevent
text-to-image diffusion models from generating visually undesirable or even
harmful content. However, current removal methods heavily rely on manually
crafted text prompts, making it challenging to achieve a high erasure
(efficacy) while minimizing the impact on other benign concepts (usability). In
this paper, we attribute the limitations to the inherent gap between the text
and image modalities, which makes it hard to transfer the intricately entangled
concept knowledge from text prompts to the image generation process. To address
this, we propose a novel solution by directly integrating visual supervision
into the erasure process, introducing the first text-image Collaborative
Concept Erasing (Co-Erasing) framework. Specifically, Co-Erasing describes the
concept jointly by text prompts and the corresponding undesirable images
induced by the prompts, and then reduces the generating probability of the
target concept through negative guidance. This approach effectively bypasses
the knowledge gap between text and image, significantly enhancing erasure
efficacy. Additionally, we design a text-guided image concept refinement
strategy that directs the model to focus on visual features most relevant to
the specified text concept, minimizing disruption to other benign concepts.
Finally, comprehensive experiments suggest that Co-Erasing outperforms
state-of-the-art erasure approaches significantly with a better trade-off
between efficacy and usability. Codes are available at
https://github.com/Ferry-Li/Co-Erasing.

</details>


### [166] [Human-Aligned Bench: Fine-Grained Assessment of Reasoning Ability in MLLMs vs. Humans](https://arxiv.org/pdf/2505.11141)
*Yansheng Qiu, Li Xiao, Zhaopan Xu, Pengfei Zhou, Zheng Wang, Kaipeng Zhang*

Main category: cs.CV

TL;DR: The paper introduces Human-Aligned Bench, a benchmark for evaluating multimodal reasoning in AI models against human performance, revealing gaps in current MLLMs.


<details>
  <summary>Details</summary>
Motivation: To assess whether current multimodal large language models (MLLMs) match human reasoning capabilities, given their integration into AGI development.

Method: Created a benchmark with 9,794 multimodal questions (bilingual and text-based) across four reasoning types, incorporating human success rates and error-prone options.

Result: Experiments show significant performance gaps between MLLMs and humans in multimodal reasoning tasks.

Conclusion: The benchmark highlights areas for improvement in MLLMs, guiding future AGI development.

Abstract: The goal of achieving Artificial General Intelligence (AGI) is to imitate
humans and surpass them. Models such as OpenAI's o1, o3, and DeepSeek's R1 have
demonstrated that large language models (LLMs) with human-like reasoning
capabilities exhibit exceptional performance and are being gradually integrated
into multimodal large language models (MLLMs). However, whether these models
possess capabilities comparable to humans in handling reasoning tasks remains
unclear at present. In this paper, we propose Human-Aligned Bench, a benchmark
for fine-grained alignment of multimodal reasoning with human performance.
Specifically, we collected 9,794 multimodal questions that solely rely on
contextual reasoning, including bilingual (Chinese and English) multimodal
questions and pure text-based questions, encompassing four question types:
visual reasoning, definition judgment, analogical reasoning, and logical
judgment. More importantly, each question is accompanied by human success rates
and options that humans are prone to choosing incorrectly. Extensive
experiments on the Human-Aligned Bench reveal notable differences between the
performance of current MLLMs in multimodal reasoning and human performance. The
findings on our benchmark provide insights into the development of the
next-generation models.

</details>


### [167] [Learning Dense Hand Contact Estimation from Imbalanced Data](https://arxiv.org/pdf/2505.11152)
*Daniel Sungho Jung, Kyoung Mu Lee*

Main category: cs.CV

TL;DR: The paper introduces a framework (HACO) for dense hand contact estimation, addressing class and spatial imbalance issues in datasets using balanced sampling and a vertex-level class-balanced loss.


<details>
  <summary>Details</summary>
Motivation: Understanding hand contact is crucial for human interaction, but existing datasets suffer from class and spatial imbalance, hindering effective learning.

Method: Proposes balanced contact sampling for class imbalance and vertex-level class-balanced (VCB) loss for spatial imbalance.

Result: The framework effectively predicts dense hand contact without suffering from imbalance issues.

Conclusion: The approach successfully tackles dataset imbalances, enabling better hand contact estimation.

Abstract: Hands are essential to human interaction, and understanding contact between
hands and the world can promote comprehensive understanding of their function.
Recently, there have been growing number of hand interaction datasets that
cover interaction with object, other hand, scene, and body. Despite the
significance of the task and increasing high-quality data, how to effectively
learn dense hand contact estimation remains largely underexplored. There are
two major challenges for learning dense hand contact estimation. First, there
exists class imbalance issue from hand contact datasets where majority of
samples are not in contact. Second, hand contact datasets contain spatial
imbalance issue with most of hand contact exhibited in finger tips, resulting
in challenges for generalization towards contacts in other hand regions. To
tackle these issues, we present a framework that learns dense HAnd COntact
estimation (HACO) from imbalanced data. To resolve the class imbalance issue,
we introduce balanced contact sampling, which builds and samples from multiple
sampling groups that fairly represent diverse contact statistics for both
contact and non-contact samples. Moreover, to address the spatial imbalance
issue, we propose vertex-level class-balanced (VCB) loss, which incorporates
spatially varying contact distribution by separately reweighting loss
contribution of each vertex based on its contact frequency across dataset. As a
result, we effectively learn to predict dense hand contact estimation with
large-scale hand contact data without suffering from class and spatial
imbalance issue. The codes will be released.

</details>


### [168] [CheX-DS: Improving Chest X-ray Image Classification with Ensemble Learning Based on DenseNet and Swin Transformer](https://arxiv.org/pdf/2505.11168)
*Xinran Li, Yu Liu, Xiujuan Xu, Xiaowei Zhao*

Main category: cs.CV

TL;DR: CheX-DS, a hybrid model combining DenseNet and Swin Transformer, outperforms previous methods in chest X-ray classification with an AUC of 83.76%.


<details>
  <summary>Details</summary>
Motivation: Current CNN-based methods for chest disease diagnosis focus on local features and ignore global features, limiting performance. Self-attention mechanisms (Transformers) offer a solution.

Method: CheX-DS integrates DenseNet (CNN) and Swin Transformer via ensemble deep learning, combining local and global features. It uses weighted binary cross-entropy and asymmetric loss to handle data imbalance.

Result: Achieves an average AUC of 83.76% on the NIH ChestX-ray14 dataset, surpassing prior studies.

Conclusion: CheX-DS effectively combines CNNs and Transformers, addressing data imbalance and improving chest X-ray classification performance.

Abstract: The automatic diagnosis of chest diseases is a popular and challenging task.
Most current methods are based on convolutional neural networks (CNNs), which
focus on local features while neglecting global features. Recently,
self-attention mechanisms have been introduced into the field of computer
vision, demonstrating superior performance. Therefore, this paper proposes an
effective model, CheX-DS, for classifying long-tail multi-label data in the
medical field of chest X-rays. The model is based on the excellent CNN model
DenseNet for medical imaging and the newly popular Swin Transformer model,
utilizing ensemble deep learning techniques to combine the two models and
leverage the advantages of both CNNs and Transformers. The loss function of
CheX-DS combines weighted binary cross-entropy loss with asymmetric loss,
effectively addressing the issue of data imbalance. The NIH ChestX-ray14
dataset is selected to evaluate the model's effectiveness. The model
outperforms previous studies with an excellent average AUC score of 83.76\%,
demonstrating its superior performance.

</details>


### [169] [CompAlign: Improving Compositional Text-to-Image Generation with a Complex Benchmark and Fine-Grained Feedback](https://arxiv.org/pdf/2505.11178)
*Yixin Wan, Kai-Wei Chang*

Main category: cs.CV

TL;DR: CompAlign is a benchmark for evaluating T2I models on compositional image generation, focusing on 3D-spatial relationships. CompQuest provides fine-grained feedback, and an alignment framework improves model performance.


<details>
  <summary>Details</summary>
Motivation: Current T2I models struggle with accurately depicting complex compositional scenes involving multiple objects, attributes, and spatial relations.

Method: CompAlign includes 900 complex prompts, and CompQuest decomposes prompts into sub-questions for evaluation. An alignment framework uses feedback for model improvement.

Result: Evaluation shows models struggle with complex 3D-spatial tasks, and post-alignment models outperform previous approaches.

Conclusion: CompAlign and CompQuest effectively assess and improve T2I models' compositional generation, highlighting performance gaps and enabling scalable improvements.

Abstract: State-of-the-art T2I models are capable of generating high-resolution images
given textual prompts. However, they still struggle with accurately depicting
compositional scenes that specify multiple objects, attributes, and spatial
relations. We present CompAlign, a challenging benchmark with an emphasis on
assessing the depiction of 3D-spatial relationships, for evaluating and
improving models on compositional image generation. CompAlign consists of 900
complex multi-subject image generation prompts that combine numerical and
3D-spatial relationships with varied attribute bindings. Our benchmark is
remarkably challenging, incorporating generation tasks with 3+ generation
subjects with complex 3D-spatial relationships. Additionally, we propose
CompQuest, an interpretable and accurate evaluation framework that decomposes
complex prompts into atomic sub-questions, then utilizes a MLLM to provide
fine-grained binary feedback on the correctness of each aspect of generation
elements in model-generated images. This enables precise quantification of
alignment between generated images and compositional prompts. Furthermore, we
propose an alignment framework that uses CompQuest's feedback as preference
signals to improve diffusion models' compositional image generation abilities.
Using adjustable per-image preferences, our method is easily scalable and
flexible for different tasks. Evaluation of 9 T2I models reveals that: (1)
models remarkable struggle more with compositional tasks with more complex
3D-spatial configurations, and (2) a noticeable performance gap exists between
open-source accessible models and closed-source commercial models. Further
empirical study on using CompAlign for model alignment yield promising results:
post-alignment diffusion models achieve remarkable improvements in
compositional accuracy, especially on complex generation tasks, outperforming
previous approaches.

</details>


### [170] [Imputation-free and Alignment-free: Incomplete Multi-view Clustering Driven by Consensus Semantic Learning](https://arxiv.org/pdf/2505.11182)
*Yuzhuo Dai, Jiaqi Jin, Zhibin Dong, Siwei Wang, Xinwang Liu, En Zhu, Xihong Yang, Xinbiao Gan, Yu Feng*

Main category: cs.CV

TL;DR: FreeCSL is a new IMVC framework that learns consensus semantics without imputation or alignment, addressing limitations in existing methods by leveraging shared prototypes and heuristic graph clustering.


<details>
  <summary>Details</summary>
Motivation: Existing IMVC methods fail to construct a shared semantic space and rely excessively on consistency, leading to unreliable imputation and alignment.

Method: FreeCSL learns consensus prototypes for a shared space and uses heuristic graph clustering to enhance cluster semantics within views.

Result: FreeCSL outperforms state-of-the-art methods, providing more confident and robust clustering assignments.

Conclusion: FreeCSL effectively bridges semantic gaps and enhances cluster structures in IMVC, offering a superior alternative to existing approaches.

Abstract: In incomplete multi-view clustering (IMVC), missing data induce prototype
shifts within views and semantic inconsistencies across views. A feasible
solution is to explore cross-view consistency in paired complete observations,
further imputing and aligning the similarity relationships inherently shared
across views. Nevertheless, existing methods are constrained by two-tiered
limitations: (1) Neither instance- nor cluster-level consistency learning
construct a semantic space shared across views to learn consensus semantics.
The former enforces cross-view instances alignment, and wrongly regards
unpaired observations with semantic consistency as negative pairs; the latter
focuses on cross-view cluster counterparts while coarsely handling fine-grained
intra-cluster relationships within views. (2) Excessive reliance on consistency
results in unreliable imputation and alignment without incorporating
view-specific cluster information. Thus, we propose an IMVC framework,
imputation- and alignment-free for consensus semantics learning (FreeCSL). To
bridge semantic gaps across all observations, we learn consensus prototypes
from available data to discover a shared space, where semantically similar
observations are pulled closer for consensus semantics learning. To capture
semantic relationships within specific views, we design a heuristic graph
clustering based on modularity to recover cluster structure with intra-cluster
compactness and inter-cluster separation for cluster semantics enhancement.
Extensive experiments demonstrate, compared to state-of-the-art competitors,
FreeCSL achieves more confident and robust assignments on IMVC task.

</details>


### [171] [FALCON: False-Negative Aware Learning of Contrastive Negatives in Vision-Language Pretraining](https://arxiv.org/pdf/2505.11192)
*Myunsoo Kim, Seong-Woong Shim, Byung-Jun Lee*

Main category: cs.CV

TL;DR: FALCON is a learning-based mini-batch strategy for VLP that dynamically balances hard and false negatives, improving cross-modal alignment and performance.


<details>
  <summary>Details</summary>
Motivation: False negatives in VLP degrade embedding quality and hard negative sampling effectiveness, necessitating a solution.

Method: FALCON uses a negative mining scheduler to adaptively select negative samples during mini-batch construction, guided by cross-modal alignment improvement.

Result: FALCON enhances performance across ALBEF, BLIP-2 frameworks and various downstream tasks.

Conclusion: FALCON effectively mitigates false negative impact, proving robust and adaptable in VLP.

Abstract: False negatives pose a critical challenge in vision-language pretraining
(VLP) due to the many-to-many correspondence between images and texts in
large-scale datasets. These false negatives introduce conflicting supervision
signals that degrade the learned embedding space and diminish the effectiveness
of hard negative sampling. In this paper, we propose FALCON (False-negative
Aware Learning of COntrastive Negatives), a learning-based mini-batch
construction strategy that adaptively balances the trade-off between hard and
false negatives during VLP. Rather than relying on fixed heuristics, FALCON
employs a negative mining scheduler that dynamically selects negative samples
of appropriate hardness for each anchor instance during mini-batch
construction, guided by a proxy for cross-modal alignment improvement.
Experimental results demonstrate that FALCON significantly improves performance
across two widely adopted VLP frameworks (ALBEF, BLIP-2) and a broad range of
downstream tasks and evaluation settings, underscoring its effectiveness and
robustness in mitigating the impact of false negatives.

</details>


### [172] [DiCo: Revitalizing ConvNets for Scalable and Efficient Diffusion Modeling](https://arxiv.org/pdf/2505.11196)
*Yuang Ai, Qihang Fan, Xuefeng Hu, Zhenheng Yang, Ran He, Huaibo Huang*

Main category: cs.CV

TL;DR: DiCo introduces a ConvNet-based diffusion model with compact channel attention, outperforming DiT in efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational overhead and redundancy in DiT's global self-attention by exploring ConvNets as an alternative.

Method: Replacing self-attention with convolution and introducing a compact channel attention mechanism to enhance feature diversity.

Result: DiCo achieves better FID scores (e.g., 2.05 at 256x256) and faster generation speeds (2.7x speedup) compared to DiT.

Conclusion: DiCo demonstrates that ConvNets can be efficient and expressive for diffusion models, outperforming Transformers in both quality and speed.

Abstract: Diffusion Transformer (DiT), a promising diffusion model for visual
generation, demonstrates impressive performance but incurs significant
computational overhead. Intriguingly, analysis of pre-trained DiT models
reveals that global self-attention is often redundant, predominantly capturing
local patterns-highlighting the potential for more efficient alternatives. In
this paper, we revisit convolution as an alternative building block for
constructing efficient and expressive diffusion models. However, naively
replacing self-attention with convolution typically results in degraded
performance. Our investigations attribute this performance gap to the higher
channel redundancy in ConvNets compared to Transformers. To resolve this, we
introduce a compact channel attention mechanism that promotes the activation of
more diverse channels, thereby enhancing feature diversity. This leads to
Diffusion ConvNet (DiCo), a family of diffusion models built entirely from
standard ConvNet modules, offering strong generative performance with
significant efficiency gains. On class-conditional ImageNet benchmarks, DiCo
outperforms previous diffusion models in both image quality and generation
speed. Notably, DiCo-XL achieves an FID of 2.05 at 256x256 resolution and 2.53
at 512x512, with a 2.7x and 3.1x speedup over DiT-XL/2, respectively.
Furthermore, our largest model, DiCo-H, scaled to 1B parameters, reaches an FID
of 1.90 on ImageNet 256x256-without any additional supervision during training.
Code: https://github.com/shallowdream204/DiCo.

</details>


### [173] [GeoMM: On Geodesic Perspective for Multi-modal Learning](https://arxiv.org/pdf/2505.11216)
*Shibin Mei, Hang Wang, Bingbing Ni*

Main category: cs.CV

TL;DR: The paper introduces geodesic distance as a novel metric in multimodal learning to address limitations of traditional distance metrics, using graph structures and efficient computation strategies.


<details>
  <summary>Details</summary>
Motivation: Traditional distance metrics fail to distinguish semantically different samples in nonlinear manifolds, common in multimodal learning.

Method: Constructs a graph structure for adjacency relationships, applies shortest-path algorithms for geodesic distance, and uses hierarchical clustering with incremental updates for efficiency.

Result: Extensive experiments show the method captures complex sample relationships and improves multimodal learning performance.

Conclusion: Geodesic distance is effective for multimodal learning, addressing traditional metric limitations and enhancing model performance.

Abstract: Geodesic distance serves as a reliable means of measuring distance in
nonlinear spaces, and such nonlinear manifolds are prevalent in the current
multimodal learning. In these scenarios, some samples may exhibit high
similarity, yet they convey different semantics, making traditional distance
metrics inadequate for distinguishing between positive and negative samples.
This paper introduces geodesic distance as a novel distance metric in
multi-modal learning for the first time, to mine correlations between samples,
aiming to address the limitations of common distance metric. Our approach
incorporates a comprehensive series of strategies to adapt geodesic distance
for the current multimodal learning. Specifically, we construct a graph
structure to represent the adjacency relationships among samples by
thresholding distances between them and then apply the shortest-path algorithm
to obtain geodesic distance within this graph. To facilitate efficient
computation, we further propose a hierarchical graph structure through
clustering and combined with incremental update strategies for dynamic status
updates. Extensive experiments across various downstream tasks validate the
effectiveness of our proposed method, demonstrating its capability to capture
complex relationships between samples and improve the performance of multimodal
learning models.

</details>


### [174] [AW-GATCN: Adaptive Weighted Graph Attention Convolutional Network for Event Camera Data Joint Denoising and Object Recognition](https://arxiv.org/pdf/2505.11232)
*Haiyu Li, Charith Abhayaratne*

Main category: cs.CV

TL;DR: The paper proposes an adaptive graph-based framework to remove noisy data in event-based object recognition, achieving superior accuracy and noise reduction.


<details>
  <summary>Details</summary>
Motivation: Event cameras generate redundant and noisy data, making it challenging to retain critical spatial-temporal information for object recognition.

Method: The approach combines adaptive event segmentation, multifactorial edge-weighting, and adaptive graph-based denoising to filter noise while preserving essential features.

Result: The method achieves high recognition accuracies (83.77% to 99.30%) and outperforms existing methods by up to 8.79% in accuracy and 19.57% in noise reduction.

Conclusion: The proposed framework effectively enhances spatiotemporal information integration, improving robustness in event-based object recognition.

Abstract: Event cameras, which capture brightness changes with high temporal
resolution, inherently generate a significant amount of redundant and noisy
data beyond essential object structures. The primary challenge in event-based
object recognition lies in effectively removing this noise without losing
critical spatial-temporal information. To address this, we propose an Adaptive
Graph-based Noisy Data Removal framework for Event-based Object Recognition.
Specifically, our approach integrates adaptive event segmentation based on
normalized density analysis, a multifactorial edge-weighting mechanism, and
adaptive graph-based denoising strategies. These innovations significantly
enhance the integration of spatiotemporal information, effectively filtering
noise while preserving critical structural features for robust recognition.
Experimental evaluations on four challenging datasets demonstrate that our
method achieves superior recognition accuracies of 83.77%, 76.79%, 99.30%, and
96.89%, surpassing existing graph-based methods by up to 8.79%, and improving
noise reduction performance by up to 19.57%, with an additional accuracy gain
of 6.26% compared to traditional Euclidean-based techniques.

</details>


### [175] [Diffusion-NPO: Negative Preference Optimization for Better Preference Aligned Generation of Diffusion Models](https://arxiv.org/pdf/2505.11245)
*Fu-Yun Wang, Yunhao Shui, Jingtan Piao, Keqiang Sun, Hongsheng Li*

Main category: cs.CV

TL;DR: The paper addresses the issue of diffusion models generating outputs misaligned with human preferences and proposes a method to improve alignment by training a model attuned to negative preferences.


<details>
  <summary>Details</summary>
Motivation: Existing preference alignment methods for diffusion models overlook handling unconditional/negative-conditional outputs, limiting the effectiveness of classifier-free guidance (CFG).

Method: The authors propose training a model specifically focused on negative preferences, requiring minor modifications to existing techniques without new training strategies or datasets.

Result: The approach integrates with models like SD1.5, SDXL, and video diffusion models, consistently improving alignment with human preferences.

Conclusion: The proposed method effectively enhances the alignment of diffusion model outputs with human preferences by addressing the oversight of negative-conditional outputs.

Abstract: Diffusion models have made substantial advances in image generation, yet
models trained on large, unfiltered datasets often yield outputs misaligned
with human preferences. Numerous methods have been proposed to fine-tune
pre-trained diffusion models, achieving notable improvements in aligning
generated outputs with human preferences. However, we argue that existing
preference alignment methods neglect the critical role of handling
unconditional/negative-conditional outputs, leading to a diminished capacity to
avoid generating undesirable outcomes. This oversight limits the efficacy of
classifier-free guidance~(CFG), which relies on the contrast between
conditional generation and unconditional/negative-conditional generation to
optimize output quality. In response, we propose a straightforward but
versatile effective approach that involves training a model specifically
attuned to negative preferences. This method does not require new training
strategies or datasets but rather involves minor modifications to existing
techniques. Our approach integrates seamlessly with models such as SD1.5, SDXL,
video diffusion models and models that have undergone preference optimization,
consistently enhancing their alignment with human preferences.

</details>


### [176] [CROC: Evaluating and Training T2I Metrics with Pseudo- and Human-Labeled Contrastive Robustness Checks](https://arxiv.org/pdf/2505.11314)
*Christoph Leiter, Yuki M. Asano, Margret Keuper, Steffen Eger*

Main category: cs.CV

TL;DR: CROC is a scalable framework for automated meta-evaluation of text-to-image metrics, using contrastive test cases to assess robustness and introducing a new metric, CROCScore.


<details>
  <summary>Details</summary>
Motivation: Human-based meta-evaluation is costly and time-intensive, and automated alternatives are lacking for assessing text-to-image metrics.

Method: CROC synthesizes contrastive test cases across image properties, creating a pseudo-labeled dataset (CROC$^{syn}$) and a human-supervised benchmark (CROC$^{hum}$). It also trains CROCScore, a new metric.

Result: CROCScore achieves state-of-the-art performance, while existing metrics show robustness issues, failing on prompts with negation or body part identification.

Conclusion: CROC provides a scalable, automated solution for meta-evaluation, highlighting weaknesses in current metrics and offering a superior alternative.

Abstract: The assessment of evaluation metrics (meta-evaluation) is crucial for
determining the suitability of existing metrics in text-to-image (T2I)
generation tasks. Human-based meta-evaluation is costly and time-intensive, and
automated alternatives are scarce. We address this gap and propose CROC: a
scalable framework for automated Contrastive Robustness Checks that
systematically probes and quantifies metric robustness by synthesizing
contrastive test cases across a comprehensive taxonomy of image properties.
With CROC, we generate a pseudo-labeled dataset (CROC$^{syn}$) of over one
million contrastive prompt-image pairs to enable a fine-grained comparison of
evaluation metrics. We also use the dataset to train CROCScore, a new metric
that achieves state-of-the-art performance among open-source methods,
demonstrating an additional key application of our framework. To complement
this dataset, we introduce a human-supervised benchmark (CROC$^{hum}$)
targeting especially challenging categories. Our results highlight robustness
issues in existing metrics: for example, many fail on prompts involving
negation, and all tested open-source metrics fail on at least 25% of cases
involving correct identification of body parts.

</details>


### [177] [Entropy-Driven Genetic Optimization for Deep-Feature-Guided Low-Light Image Enhancement](https://arxiv.org/pdf/2505.11246)
*Nirjhor Datta, Afroza Akther, M. Sohel Rahman*

Main category: cs.CV

TL;DR: An unsupervised, fuzzy-inspired image enhancement framework using NSGA-II optimizes brightness, contrast, and gamma, balancing visual quality and semantic fidelity without paired training data.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on pixel-level information, neglecting semantic features. This work aims to enhance images while preserving semantic consistency.

Method: Uses a pre-trained deep neural network for feature extraction and a GPU-accelerated NSGA-II algorithm to optimize brightness, contrast, and gamma. Includes a local search phase for fine-tuning.

Result: Achieves BRISQUE score of 19.82 and NIQE score of 3.652, with improved visibility in shadows and preserved fine details.

Conclusion: Proposes a novel unsupervised approach for image enhancement, emphasizing semantic consistency and broad applicability.

Abstract: Image enhancement methods often prioritize pixel level information,
overlooking the semantic features. We propose a novel, unsupervised,
fuzzy-inspired image enhancement framework guided by NSGA-II algorithm that
optimizes image brightness, contrast, and gamma parameters to achieve a balance
between visual quality and semantic fidelity. Central to our proposed method is
the use of a pre trained deep neural network as a feature extractor. To find
the best enhancement settings, we use a GPU-accelerated NSGA-II algorithm that
balances multiple objectives, namely, increasing image entropy, improving
perceptual similarity, and maintaining appropriate brightness. We further
improve the results by applying a local search phase to fine-tune the top
candidates from the genetic algorithm. Our approach operates entirely without
paired training data making it broadly applicable across domains with limited
or noisy labels. Quantitatively, our model achieves excellent performance with
average BRISQUE and NIQE scores of 19.82 and 3.652, respectively, in all
unpaired datasets. Qualitatively, enhanced images by our model exhibit
significantly improved visibility in shadowed regions, natural balance of
contrast and also preserve the richer fine detail without introducing noticable
artifacts. This work opens new directions for unsupervised image enhancement
where semantic consistency is critical.

</details>


### [178] [DRAGON: A Large-Scale Dataset of Realistic Images Generated by Diffusion Models](https://arxiv.org/pdf/2505.11257)
*Giulia Bertazzini, Daniele Baracchi, Dasara Shullani, Isao Echizen, Alessandro Piva*

Main category: cs.CV

TL;DR: DRAGON is a comprehensive dataset for detecting synthetic images from 25 diffusion models, designed to address the limitations of outdated datasets and support forensic research.


<details>
  <summary>Details</summary>
Motivation: The rise of synthetic content from diffusion models necessitates robust detection tools, but current methods lack up-to-date and diverse datasets.

Method: DRAGON includes images from 25 diffusion models, enhanced by a pipeline using a large language model to improve prompt diversity and image quality.

Result: The dataset improves standard quality metrics and is available in multiple sizes, with a dedicated test set for benchmarking.

Conclusion: DRAGON supports the development of detection techniques for synthetic content, providing a versatile and up-to-date resource for researchers.

Abstract: The remarkable ease of use of diffusion models for image generation has led
to a proliferation of synthetic content online. While these models are often
employed for legitimate purposes, they are also used to generate fake images
that support misinformation and hate speech. Consequently, it is crucial to
develop robust tools capable of detecting whether an image has been generated
by such models. Many current detection methods, however, require large volumes
of sample images for training. Unfortunately, due to the rapid evolution of the
field, existing datasets often cover only a limited range of models and quickly
become outdated. In this work, we introduce DRAGON, a comprehensive dataset
comprising images from 25 diffusion models, spanning both recent advancements
and older, well-established architectures. The dataset contains a broad variety
of images representing diverse subjects. To enhance image realism, we propose a
simple yet effective pipeline that leverages a large language model to expand
input prompts, thereby generating more diverse and higher-quality outputs, as
evidenced by improvements in standard quality metrics. The dataset is provided
in multiple sizes (ranging from extra-small to extra-large) to accomodate
different research scenarios. DRAGON is designed to support the forensic
community in developing and evaluating detection and attribution techniques for
synthetic content. Additionally, the dataset is accompanied by a dedicated test
set, intended to serve as a benchmark for assessing the performance of newly
developed methods.

</details>


### [179] [EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large Language Models](https://arxiv.org/pdf/2505.11405)
*Bohao Xing, Xin Liu, Guoying Zhao, Chengyu Liu, Xiaolan Fu, Heikki K√§lvi√§inen*

Main category: cs.CV

TL;DR: The paper introduces EmotionHallucer, the first benchmark for detecting emotion hallucinations in Multimodal Large Language Models (MLLMs), revealing significant issues and proposing the PEP-MEK framework for improvement.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of dedicated evaluation for emotion-related hallucinations in MLLMs, despite their critical role in emotion understanding.

Method: Uses an adversarial binary QA framework to assess emotion hallucinations, leveraging emotion psychology knowledge and real-world multimodal perception.

Result: Most models struggle with emotion hallucinations; closed-source models outperform open-source ones, and reasoning capability helps.

Conclusion: The PEP-MEK framework improves emotion hallucination detection by 9.90%, with resources made publicly available.

Abstract: Emotion understanding is a critical yet challenging task. Recent advances in
Multimodal Large Language Models (MLLMs) have significantly enhanced their
capabilities in this area. However, MLLMs often suffer from hallucinations,
generating irrelevant or nonsensical content. To the best of our knowledge,
despite the importance of this issue, there has been no dedicated effort to
evaluate emotion-related hallucinations in MLLMs. In this work, we introduce
EmotionHallucer, the first benchmark for detecting and analyzing emotion
hallucinations in MLLMs. Unlike humans, whose emotion understanding stems from
the interplay of biology and social learning, MLLMs rely solely on data-driven
learning and lack innate emotional instincts. Fortunately, emotion psychology
provides a solid foundation of knowledge about human emotions. Building on
this, we assess emotion hallucinations from two dimensions: emotion psychology
knowledge and real-world multimodal perception. To support robust evaluation,
we utilize an adversarial binary question-answer (QA) framework, which employs
carefully crafted basic and hallucinated pairs to assess the emotion
hallucination tendencies of MLLMs. By evaluating 38 LLMs and MLLMs on
EmotionHallucer, we reveal that: i) most current models exhibit substantial
issues with emotion hallucinations; ii) closed-source models outperform
open-source ones in detecting emotion hallucinations, and reasoning capability
provides additional advantages; iii) existing models perform better in emotion
psychology knowledge than in multimodal emotion perception. As a byproduct,
these findings inspire us to propose the PEP-MEK framework, which yields an
average improvement of 9.90% in emotion hallucination detection across selected
models. Resources will be available at
https://github.com/xxtars/EmotionHallucer.

</details>


### [180] [Multi-view dense image matching with similarity learning and geometry priors](https://arxiv.org/pdf/2505.11264)
*Mohamed Ali Chebbi, Ewelina Rupnik, Paul Lopes, Marc Pierrot-Deseilligny*

Main category: cs.CV

TL;DR: MV-DeepSimNets is a deep learning suite for multi-view similarity learning, using epipolar geometry and online geometry priors to improve multi-view reconstruction without extensive training data.


<details>
  <summary>Details</summary>
Motivation: To enhance multi-view reconstruction by leveraging geometry-aware features and avoiding laborious dataset creation.

Method: Incorporates epipolar geometry and homography rectification to generate geometry-aware features, aggregates similarities to regularize cost volume.

Result: Superior performance in multi-view reconstruction, especially for aerial and satellite imagery with varied resolutions.

Conclusion: MV-DeepSimNets offers a scalable, geometry-aware solution for multi-view reconstruction, integrated into MicMac software.

Abstract: We introduce MV-DeepSimNets, a comprehensive suite of deep neural networks
designed for multi-view similarity learning, leveraging epipolar geometry for
training. Our approach incorporates an online geometry prior to characterize
pixel relationships, either along the epipolar line or through homography
rectification. This enables the generation of geometry-aware features from
native images, which are then projected across candidate depth hypotheses using
plane sweeping. Our method geometric preconditioning effectively adapts
epipolar-based features for enhanced multi-view reconstruction, without
requiring the laborious multi-view training dataset creation. By aggregating
learned similarities, we construct and regularize the cost volume, leading to
improved multi-view surface reconstruction over traditional dense matching
approaches. MV-DeepSimNets demonstrates superior performance against leading
similarity learning networks and end-to-end regression models, especially in
terms of generalization capabilities across both aerial and satellite imagery
with varied ground sampling distances. Our pipeline is integrated into MicMac
software and can be readily adopted in standard multi-resolution image matching
pipelines.

</details>


### [181] [Equal is Not Always Fair: A New Perspective on Hyperspectral Representation Non-Uniformity](https://arxiv.org/pdf/2505.11267)
*Wuzhou Quan, Mingqiang Wei, Jinhui Tang*

Main category: cs.CV

TL;DR: FairHyp is a fairness-directed framework for hyperspectral image (HSI) representation, addressing non-uniformity through specialized modules for spatial, spectral, and feature adaptation, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing HSI models assume homogeneity, leading to suboptimal performance due to non-uniformity in spectral, spatial, and feature dimensions. FairHyp aims to resolve this by disentangling and addressing these non-uniformities.

Method: FairHyp uses a Runge-Kutta-inspired spatial adapter, multi-receptive field convolution with sparse-aware refinement, and a spectral-context state space model with bidirectional Mamba scanning.

Result: FairHyp outperforms state-of-the-art methods in tasks like classification, denoising, super-resolution, and inpainting, demonstrating its adaptability and effectiveness.

Conclusion: FairHyp redefines fairness as a structural necessity in HSI modeling, offering a balanced approach for high-dimensional vision tasks.

Abstract: Hyperspectral image (HSI) representation is fundamentally challenged by
pervasive non-uniformity, where spectral dependencies, spatial continuity, and
feature efficiency exhibit complex and often conflicting behaviors. Most
existing models rely on a unified processing paradigm that assumes homogeneity
across dimensions, leading to suboptimal performance and biased
representations. To address this, we propose FairHyp, a fairness-directed
framework that explicitly disentangles and resolves the threefold
non-uniformity through cooperative yet specialized modules. We introduce a
Runge-Kutta-inspired spatial variability adapter to restore spatial coherence
under resolution discrepancies, a multi-receptive field convolution module with
sparse-aware refinement to enhance discriminative features while respecting
inherent sparsity, and a spectral-context state space model that captures
stable and long-range spectral dependencies via bidirectional Mamba scanning
and statistical aggregation. Unlike one-size-fits-all solutions, FairHyp
achieves dimension-specific adaptation while preserving global consistency and
mutual reinforcement. This design is grounded in the view that non-uniformity
arises from the intrinsic structure of HSI representations, rather than any
particular task setting. To validate this, we apply FairHyp across four
representative tasks including classification, denoising, super-resolution, and
inpaintin, demonstrating its effectiveness in modeling a shared structural
flaw. Extensive experiments show that FairHyp consistently outperforms
state-of-the-art methods under varied imaging conditions. Our findings redefine
fairness as a structural necessity in HSI modeling and offer a new paradigm for
balancing adaptability, efficiency, and fidelity in high-dimensional vision
tasks.

</details>


### [182] [MTevent: A Multi-Task Event Camera Dataset for 6D Pose Estimation and Moving Object Detection](https://arxiv.org/pdf/2505.11282)
*Shrutarv Awasthi, Anas Gouda, Sven Franke, J√©r√¥me Rutinowski, Frank Hoffmann, Moritz Roidl*

Main category: cs.CV

TL;DR: MTevent is a new dataset for 6D pose estimation and moving object detection in high-speed, dynamic environments using stereo-event and RGB cameras. It highlights the limitations of RGB-based methods and aims to advance event-based vision in robotics.


<details>
  <summary>Details</summary>
Motivation: Current RGB cameras struggle with motion blur and latency in high-speed robotic applications, while event cameras offer a promising alternative. MTevent addresses the lack of datasets for such scenarios.

Method: The dataset includes 75 scenes captured with stereo-event and RGB cameras, featuring 16 unique objects under challenging conditions like extreme angles, lighting changes, and occlusions.

Result: Baseline evaluation using RGB images achieved an Average Recall of 0.22, showing the limitations of RGB-based methods in dynamic settings.

Conclusion: MTevent provides a valuable resource for improving perception models and advancing research in high-speed robotic vision, with the dataset publicly available.

Abstract: Mobile robots are reaching unprecedented speeds, with platforms like Unitree
B2, and Fraunhofer O3dyn achieving maximum speeds between 5 and 10 m/s.
However, effectively utilizing such speeds remains a challenge due to the
limitations of RGB cameras, which suffer from motion blur and fail to provide
real-time responsiveness. Event cameras, with their asynchronous operation, and
low-latency sensing, offer a promising alternative for high-speed robotic
perception. In this work, we introduce MTevent, a dataset designed for 6D pose
estimation and moving object detection in highly dynamic environments with
large detection distances. Our setup consists of a stereo-event camera and an
RGB camera, capturing 75 scenes, each on average 16 seconds, and featuring 16
unique objects under challenging conditions such as extreme viewing angles,
varying lighting, and occlusions. MTevent is the first dataset to combine
high-speed motion, long-range perception, and real-world object interactions,
making it a valuable resource for advancing event-based vision in robotics. To
establish a baseline, we evaluate the task of 6D pose estimation using NVIDIA's
FoundationPose on RGB images, achieving an Average Recall of 0.22 with
ground-truth masks, highlighting the limitations of RGB-based approaches in
such dynamic settings. With MTevent, we provide a novel resource to improve
perception models and foster further research in high-speed robotic vision. The
dataset is available for download
https://huggingface.co/datasets/anas-gouda/MTevent

</details>


### [183] [Breaking the Batch Barrier (B3) of Contrastive Learning via Smart Batch Mining](https://arxiv.org/pdf/2505.11293)
*Raghuveer Thirukovalluru, Rui Meng, Ye Liu, Karthikeyan K, Mingyi Su, Ping Nie, Semih Yavuz, Yingbo Zhou, Wenhu Chen, Bhuwan Dhingra*

Main category: cs.CV

TL;DR: The paper introduces 'Breaking the Batch Barrier' (B3), a batch construction strategy for contrastive learning that improves embedding model performance by curating high-quality batches with strong negatives.


<details>
  <summary>Details</summary>
Motivation: Current contrastive learning methods rely on in-batch negatives, whose quality and batch size significantly impact model effectiveness. The goal is to enhance batch construction for better performance.

Method: B3 uses a pretrained teacher model to rank dataset examples, constructs a sparse similarity graph, applies community detection to identify clusters of strong negatives, and forms batches rich in in-batch negatives.

Result: B3 achieves state-of-the-art results on the MMEB benchmark, outperforming previous methods by +1.3 and +2.9 points at 7B and 2B scales, respectively, even with smaller batch sizes (e.g., 64).

Conclusion: B3 effectively improves contrastive learning by optimizing batch construction, enabling superior performance with smaller batch sizes.

Abstract: Contrastive learning (CL) is a prevalent technique for training embedding
models, which pulls semantically similar examples (positives) closer in the
representation space while pushing dissimilar ones (negatives) further apart. A
key source of negatives are 'in-batch' examples, i.e., positives from other
examples in the batch. Effectiveness of such models is hence strongly
influenced by the size and quality of training batches. In this work, we
propose 'Breaking the Batch Barrier' (B3), a novel batch construction strategy
designed to curate high-quality batches for CL. Our approach begins by using a
pretrained teacher embedding model to rank all examples in the dataset, from
which a sparse similarity graph is constructed. A community detection algorithm
is then applied to this graph to identify clusters of examples that serve as
strong negatives for one another. The clusters are then used to construct
batches that are rich in in-batch negatives. Empirical results on the MMEB
multimodal embedding benchmark (36 tasks) demonstrate that our method sets a
new state of the art, outperforming previous best methods by +1.3 and +2.9
points at the 7B and 2B model scales, respectively. Notably, models trained
with B3 surpass existing state-of-the-art results even with a batch size as
small as 64, which is 4-16x smaller than that required by other methods.

</details>


### [184] [Temporally-Grounded Language Generation: A Benchmark for Real-Time Vision-Language Models](https://arxiv.org/pdf/2505.11326)
*Keunwoo Peter Yu, Joyce Chai*

Main category: cs.CV

TL;DR: The paper introduces Temporally-Grounded Language Generation (TGLG), a benchmark for evaluating real-time VLMs, and proposes VLM-TSI, a model for time-synchronized language generation.


<details>
  <summary>Details</summary>
Motivation: Real-time interactive environments require VLMs to generate semantically accurate and precisely timed utterances, which existing models lack.

Method: Proposes TGLG benchmark and VLM-TSI, a model that interleaves visual and linguistic tokens in a time-synchronized manner.

Result: VLM-TSI outperforms baselines but overall performance remains modest, indicating the challenge of TGLG.

Conclusion: TGLG is a challenging task, motivating further research in real-time VLMs.

Abstract: Vision-language models (VLMs) have shown remarkable progress in offline tasks
such as image captioning and video question answering. However, real-time
interactive environments impose new demands on VLMs, requiring them to generate
utterances that are not only semantically accurate but also precisely timed. We
identify two core capabilities necessary for such settings --
$\textit{perceptual updating}$ and $\textit{contingency awareness}$ -- and
propose a new benchmark task, $\textbf{Temporally-Grounded Language Generation
(TGLG)}$, to evaluate them. TGLG requires models to generate utterances in
response to streaming video such that both content and timing align with
dynamic visual input. To support this benchmark, we curate evaluation datasets
from sports broadcasting and egocentric human interaction domains, and
introduce a new metric, $\textbf{TRACE}$, to evaluate TGLG by jointly measuring
semantic similarity and temporal alignment. Finally, we present
$\textbf{Vision-Language Model with Time-Synchronized Interleaving (VLM-TSI)}$,
a model that interleaves visual and linguistic tokens in a time-synchronized
manner, enabling real-time language generation without relying on turn-based
assumptions. Experimental results show that VLM-TSI significantly outperforms a
strong baseline, yet overall performance remains modest -- highlighting the
difficulty of TGLG and motivating further research in real-time VLMs. Code and
data available $\href{https://github.com/yukw777/tglg}{here}$.

</details>


### [185] [MARRS: Masked Autoregressive Unit-based Reaction Synthesis](https://arxiv.org/pdf/2505.11334)
*Y. B. Wang, S Wang, J. N. Zhang, J. F. Wu, Q. D. He, C. C. Fu, C. J. Wang, Y. Liu*

Main category: cs.CV

TL;DR: The paper introduces MARRS, a framework for human action-reaction synthesis, addressing challenges like fine-grained hand movements and VQ limitations with novel components like UD-VAE, ACF, and AUM.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of autoregressive and VQ-based methods in generating human reactions, especially for fine-grained hand movements, and to improve coordination in motion synthesis.

Method: Proposes MARRS with UD-VAE for independent encoding of body and hand units, ACF for action-conditioned fusion, AUM for adaptive modulation, and a diffusion model with MLP noise predictors.

Result: Quantitative and qualitative results show superior performance in generating coordinated and fine-grained reaction motions.

Conclusion: MARRS effectively addresses the challenges of human action-reaction synthesis, outperforming existing methods.

Abstract: This work aims at a challenging task: human action-reaction synthesis, i.e.,
generating human reactions based on the action sequence of the other as
conditions. Currently, autoregressive modeling approaches have achieved
remarkable performance in motion generation tasks, e.g. text-to-motion.
However, vector quantization (VQ) accompanying autoregressive generation has
inherent disadvantages, including loss of quantization information, low
codebook utilization, etc. Moreover, unlike text-to-motion, which focuses
solely on the movement of body joints, human action-reaction synthesis also
encompasses fine-grained hand movements. In this work, we propose MARRS, a
novel framework designed to generate coordinated and fine-grained reaction
motions in continuous representations. Initially, we present the
Unit-distinguished Motion Variational AutoEncoder (UD-VAE), which segments the
entire body into distinct body and hand units, encoding them independently.
Subsequently, we propose Action-Conditioned Fusion (ACF), which involves
randomly masking a subset of reactive tokens and extracting specific
information about the body and hands from the active tokens. Furthermore, we
introduce Adaptive Unit Modulation (AUM) to facilitate interaction between body
and hand units by using the information from one unit to adaptively modulate
the other. Finally, for the diffusion model, we employ a compact MLP as a noise
predictor for each distinct body unit and incorporate the diffusion loss to
model the probability distribution of each token. Quantitative and qualitative
results demonstrate that our method achieves superior performance. The code
will be released upon acceptance.

</details>


### [186] [Dynamic Base model Shift for Delta Compression](https://arxiv.org/pdf/2505.11344)
*Chenyu Huang, Peng Ye, Shenghe Zheng, Xiaohui Wang, Lei Bai, Tao Chen, Wanli Ouyang*

Main category: cs.CV

TL;DR: DBMS dynamically adapts the base model for delta compression, improving performance under high compression rates.


<details>
  <summary>Details</summary>
Motivation: Existing delta compression methods degrade performance, especially at high compression rates, due to reliance on the pretrained base model.

Method: Proposes Dynamic Base Model Shift (DBMS), adjusting base model shift magnitude and delta compression scale for each task.

Result: DBMS maintains performance under high compression, outperforming existing methods, and works across various model types.

Conclusion: DBMS is a versatile and effective solution for delta compression, enhancing efficiency without sacrificing performance.

Abstract: Transformer-based models with the pretrain-finetune paradigm bring about
significant progress, along with the heavy storage and deployment costs of
finetuned models on multiple tasks. Delta compression attempts to lower the
costs by reducing the redundancy of delta parameters (i.e., the difference
between the finetuned and pre-trained model weights) through pruning or
quantization. However, existing methods by default employ the pretrained model
as the base model and compress the delta parameters for every task, which may
causes significant performance degradation, especially when the compression
rate is extremely high. To tackle this issue, we investigate the impact of
different base models on the performance of delta compression and find that the
pre-trained base model can hardly be optimal. To this end, we propose Dynamic
Base Model Shift (DBMS), which dynamically adapts the base model to the target
task before performing delta compression. Specifically, we adjust two
parameters, which respectively determine the magnitude of the base model shift
and the overall scale of delta compression, to boost the compression
performance on each task. Through low-cost learning of these two parameters,
our DBMS can maintain most of the finetuned model's performance even under an
extremely high compression ratio setting, significantly surpassing existing
methods. Moreover, our DBMS is orthogonal and can be integrated with a variety
of other methods, and it has been evaluated across different types of models
including language, vision transformer, and multi-modal models.

</details>


### [187] [Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation](https://arxiv.org/pdf/2505.11383)
*Zihan Wang, Seungjun Lee, Gim Hee Lee*

Main category: cs.CV

TL;DR: Dynam3D improves VLN tasks by using dynamic layered 3D representations to address challenges like 3D understanding, large-scale exploration, and adaptability in changing environments.


<details>
  <summary>Details</summary>
Motivation: Current Video-VLMs struggle with 3D geometry, large-scale navigation, and dynamic environments in VLN tasks.

Method: Dynam3D projects 2D CLIP features into 3D space, creating hierarchical 3D representations with dynamic updates for better navigation.

Result: Achieves state-of-the-art performance on VLN benchmarks (R2R-CE, REVERIE-CE, NavRAG-CE) and validates practical deployment in real-world robots.

Conclusion: Dynam3D effectively addresses VLN challenges, enhancing 3D understanding and adaptability for embodied agents.

Abstract: Vision-and-Language Navigation (VLN) is a core task where embodied agents
leverage their spatial mobility to navigate in 3D environments toward
designated destinations based on natural language instructions. Recently,
video-language large models (Video-VLMs) with strong generalization
capabilities and rich commonsense knowledge have shown remarkable performance
when applied to VLN tasks. However, these models still encounter the following
challenges when applied to real-world 3D navigation: 1) Insufficient
understanding of 3D geometry and spatial semantics; 2) Limited capacity for
large-scale exploration and long-term environmental memory; 3) Poor
adaptability to dynamic and changing environments.To address these limitations,
we propose Dynam3D, a dynamic layered 3D representation model that leverages
language-aligned, generalizable, and hierarchical 3D representations as visual
input to train 3D-VLM in navigation action prediction. Given posed RGB-D
images, our Dynam3D projects 2D CLIP features into 3D space and constructs
multi-level 3D patch-instance-zone representations for 3D geometric and
semantic understanding with a dynamic and layer-wise update strategy. Our
Dynam3D is capable of online encoding and localization of 3D instances, and
dynamically updates them in changing environments to provide large-scale
exploration and long-term memory capabilities for navigation. By leveraging
large-scale 3D-language pretraining and task-specific adaptation, our Dynam3D
sets new state-of-the-art performance on VLN benchmarks including R2R-CE,
REVERIE-CE and NavRAG-CE under monocular settings. Furthermore, experiments for
pre-exploration, lifelong memory, and real-world robot validate the
effectiveness of practical deployment.

</details>


### [188] [MutualNeRF: Improve the Performance of NeRF under Limited Samples with Mutual Information Theory](https://arxiv.org/pdf/2505.11386)
*Zifan Wang, Jingwei Li, Yitang Li, Yunze Liu*

Main category: cs.CV

TL;DR: MutualNeRF enhances NeRF performance with limited samples using Mutual Information Theory, improving sparse view sampling and few-shot synthesis.


<details>
  <summary>Details</summary>
Motivation: NeRF struggles with limited data and lacks theoretical support for prior knowledge integration. Mutual Information provides a unified metric for correlation.

Method: Uses Mutual Information to select viewpoints (minimizing MI) and improve synthesis (maximizing MI) via a greedy algorithm and plug-and-play regularization.

Result: Consistent improvements over state-of-the-art baselines in limited-sample settings.

Conclusion: MutualNeRF effectively addresses NeRF's limitations with a theoretically robust and practical framework.

Abstract: This paper introduces MutualNeRF, a framework enhancing Neural Radiance Field
(NeRF) performance under limited samples using Mutual Information Theory. While
NeRF excels in 3D scene synthesis, challenges arise with limited data and
existing methods that aim to introduce prior knowledge lack theoretical support
in a unified framework. We introduce a simple but theoretically robust concept,
Mutual Information, as a metric to uniformly measure the correlation between
images, considering both macro (semantic) and micro (pixel) levels.
  For sparse view sampling, we strategically select additional viewpoints
containing more non-overlapping scene information by minimizing mutual
information without knowing ground truth images beforehand. Our framework
employs a greedy algorithm, offering a near-optimal solution.
  For few-shot view synthesis, we maximize the mutual information between
inferred images and ground truth, expecting inferred images to gain more
relevant information from known images. This is achieved by incorporating
efficient, plug-and-play regularization terms.
  Experiments under limited samples show consistent improvement over
state-of-the-art baselines in different settings, affirming the efficacy of our
framework.

</details>


### [189] [Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner](https://arxiv.org/pdf/2505.11404)
*Wenchuan Zhang, Penghao Zhang, Jingru Guo, Tao Cheng, Jie Chen, Shuwan Zhang, Zhang Zhang, Yuhao Yi, Hong Bu*

Main category: cs.CV

TL;DR: The paper introduces Patho-R1, a multimodal RL-based pathology Reasoner, and PathoCLIP, trained on high-quality datasets derived from pathology textbooks and expert input to improve diagnostic accuracy and reasoning in pathology.


<details>
  <summary>Details</summary>
Motivation: Current pathology-specific VLMs lack depth and structured diagnostic reasoning due to limited datasets. The study aims to address this by leveraging expert knowledge and textbooks to create better datasets.

Method: A three-stage pipeline: (1) pretraining on 3.5M image-text pairs, (2) supervised fine-tuning on 500k Chain-of-Thought samples, (3) reinforcement learning for reasoning refinement. PathoCLIP is also introduced for alignment assessment.

Result: Patho-R1 and PathoCLIP show robust performance in zero-shot classification, cross-modal retrieval, VQA, and MCQs.

Conclusion: The approach significantly improves pathology-specific VLMs by enhancing reasoning and diagnostic accuracy through high-quality datasets and advanced training methods.

Abstract: Recent advances in vision language models (VLMs) have enabled broad progress
in the general medical field. However, pathology still remains a more
challenging subdomain, with current pathology specific VLMs exhibiting
limitations in both diagnostic accuracy and reasoning plausibility. Such
shortcomings are largely attributable to the nature of current pathology
datasets, which are primarily composed of image description pairs that lack the
depth and structured diagnostic paradigms employed by real world pathologists.
In this study, we leverage pathology textbooks and real world pathology experts
to construct high-quality, reasoning-oriented datasets. Building on this, we
introduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a
three-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs
for knowledge infusion; (2) supervised fine-tuning on 500k high-quality
Chain-of-Thought samples for reasoning incentivizing; (3) reinforcement
learning using Group Relative Policy Optimization and Decoupled Clip and
Dynamic sAmpling Policy Optimization strategies for multimodal reasoning
quality refinement. To further assess the alignment quality of our dataset, we
propose PathoCLIP, trained on the same figure-caption corpus used for continued
pretraining. Comprehensive experimental results demonstrate that both PathoCLIP
and Patho-R1 achieve robust performance across a wide range of
pathology-related tasks, including zero-shot classification, cross-modal
retrieval, Visual Question Answering, and Multiple Choice Question. Our project
is available at the Patho-R1 repository:
https://github.com/Wenchuan-Zhang/Patho-R1.

</details>


### [190] [Improving Object Detection Performance through YOLOv8: A Comprehensive Training and Evaluation Study](https://arxiv.org/pdf/2505.11424)
*Rana Poureskandar, Shiva Razzagzadeh*

Main category: cs.CV

TL;DR: Evaluation of YOLOv8-based segmentation for wrinkle detection in facial images.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of YOLOv8 in detecting and segmenting facial wrinkles.

Method: Utilized a YOLOv8-based segmentation model for wrinkle detection in facial images.

Result: Performance of the model was evaluated, though specific metrics are not detailed.

Conclusion: The study demonstrates the potential of YOLOv8 for facial wrinkle segmentation.

Abstract: This study evaluated the performance of a YOLOv8-based segmentation model for
detecting and segmenting wrinkles in facial images.

</details>


### [191] [SurgPose: Generalisable Surgical Instrument Pose Estimation using Zero-Shot Learning and Stereo Vision](https://arxiv.org/pdf/2505.11439)
*Utsav Rai, Haozheng Xu, Stamatia Giannarou*

Main category: cs.CV

TL;DR: A novel 6-DoF pose estimation pipeline for surgical tools in RMIS using zero-shot RGB-D models, enhanced with depth estimation and improved segmentation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional marker-based and supervised learning methods for pose estimation in RMIS face limitations like occlusions, reflections, and lack of adaptability to new tools. Zero-shot methods, though successful elsewhere, remain unexplored in RMIS.

Method: Leverages zero-shot RGB-D models (FoundationPose, SAM-6D) with vision-based depth estimation (RAFT-Stereo) and replaces SAM with fine-tuned Mask R-CNN for better segmentation in occluded/textureless environments.

Result: Enhanced SAM-6D outperforms FoundationPose in zero-shot pose estimation of unseen surgical tools, setting a new benchmark for RMIS.

Conclusion: The work improves generalizability of pose estimation for unseen tools and pioneers zero-shot RGB-D methods in RMIS.

Abstract: Accurate pose estimation of surgical tools in Robot-assisted Minimally
Invasive Surgery (RMIS) is essential for surgical navigation and robot control.
While traditional marker-based methods offer accuracy, they face challenges
with occlusions, reflections, and tool-specific designs. Similarly, supervised
learning methods require extensive training on annotated datasets, limiting
their adaptability to new tools. Despite their success in other domains,
zero-shot pose estimation models remain unexplored in RMIS for pose estimation
of surgical instruments, creating a gap in generalising to unseen surgical
tools. This paper presents a novel 6 Degrees of Freedom (DoF) pose estimation
pipeline for surgical instruments, leveraging state-of-the-art zero-shot RGB-D
models like the FoundationPose and SAM-6D. We advanced these models by
incorporating vision-based depth estimation using the RAFT-Stereo method, for
robust depth estimation in reflective and textureless environments.
Additionally, we enhanced SAM-6D by replacing its instance segmentation module,
Segment Anything Model (SAM), with a fine-tuned Mask R-CNN, significantly
boosting segmentation accuracy in occluded and complex conditions. Extensive
validation reveals that our enhanced SAM-6D surpasses FoundationPose in
zero-shot pose estimation of unseen surgical instruments, setting a new
benchmark for zero-shot RGB-D pose estimation in RMIS. This work enhances the
generalisability of pose estimation for unseen objects and pioneers the
application of RGB-D zero-shot methods in RMIS.

</details>


### [192] [HumaniBench: A Human-Centric Framework for Large Multimodal Models Evaluation](https://arxiv.org/pdf/2505.11454)
*Shaina Raza, Aravind Narayanan, Vahid Reza Khazaie, Ashmal Vayani, Mukund S. Chettiar, Amandeep Singh, Mubarak Shah, Deval Pandya*

Main category: cs.CV

TL;DR: HumaniBench is a new benchmark evaluating Large Multimodal Models (LMMs) on human-centered AI principles like fairness, ethics, and inclusivity. It includes 32K image-question pairs and tests 15 LMMs, revealing gaps in robustness and alignment with human values.


<details>
  <summary>Details</summary>
Motivation: Current LMMs perform well on standard benchmarks but lack alignment with human-centered criteria like fairness and empathy. HumaniBench addresses this gap.

Method: HumaniBench uses 32K image-question pairs annotated via a GPT4-assisted pipeline and expert verification. It evaluates seven HCAI principles across seven diverse tasks.

Result: Proprietary models generally outperform open-source ones, but robustness and visual grounding remain weak. Some models struggle to balance accuracy with human-aligned principles.

Conclusion: HumaniBench is the first benchmark focused on HCAI principles, providing a testbed to improve LMMs' alignment with human values. Data and tools are publicly available.

Abstract: Large multimodal models (LMMs) now excel on many vision language benchmarks,
however, they still struggle with human centered criteria such as fairness,
ethics, empathy, and inclusivity, key to aligning with human values. We
introduce HumaniBench, a holistic benchmark of 32K real-world image question
pairs, annotated via a scalable GPT4o assisted pipeline and exhaustively
verified by domain experts. HumaniBench evaluates seven Human Centered AI
(HCAI) principles: fairness, ethics, understanding, reasoning, language
inclusivity, empathy, and robustness, across seven diverse tasks, including
open and closed ended visual question answering (VQA), multilingual QA, visual
grounding, empathetic captioning, and robustness tests. Benchmarking 15 state
of the art LMMs (open and closed source) reveals that proprietary models
generally lead, though robustness and visual grounding remain weak points. Some
open-source models also struggle to balance accuracy with adherence to
human-aligned principles. HumaniBench is the first benchmark purpose built
around HCAI principles. It provides a rigorous testbed for diagnosing alignment
gaps and guiding LMMs toward behavior that is both accurate and socially
responsible. Dataset, annotation prompts, and evaluation code are available at:
https://vectorinstitute.github.io/HumaniBench

</details>


### [193] [PSDiffusion: Harmonized Multi-Layer Image Generation via Layout and Appearance Alignment](https://arxiv.org/pdf/2505.11468)
*Dingbang Huang, Wenbo Li, Yifei Zhao, Xinyu Pan, Yanhong Zeng, Bo Dai*

Main category: cs.CV

TL;DR: PSDiffusion is a unified diffusion framework for simultaneous multi-layer text-to-image generation, addressing interactions among layers while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Existing multi-layer generation methods lack handling of layer interactions like global layout, physics-plausible contacts, and visual effects.

Method: PSDiffusion introduces a global-layer interactive mechanism for concurrent and collaborative generation of layered images in a single feed-forward process.

Result: The model generates multi-layer images (RGB background and RGBA foregrounds) with high quality, completeness, and global coherence.

Conclusion: PSDiffusion outperforms existing methods by ensuring spatial and visual interactions among layers without post-decomposition or sequential generation.

Abstract: Diffusion models have made remarkable advancements in generating high-quality
images from textual descriptions. Recent works like LayerDiffuse have extended
the previous single-layer, unified image generation paradigm to transparent
image layer generation. However, existing multi-layer generation methods fail
to handle the interactions among multiple layers such as rational global
layout, physics-plausible contacts and visual effects like shadows and
reflections while maintaining high alpha quality. To solve this problem, we
propose PSDiffusion, a unified diffusion framework for simultaneous multi-layer
text-to-image generation. Our model can automatically generate multi-layer
images with one RGB background and multiple RGBA foregrounds through a single
feed-forward process. Unlike existing methods that combine multiple tools for
post-decomposition or generate layers sequentially and separately, our method
introduces a global-layer interactive mechanism that generates layered-images
concurrently and collaboratively, ensuring not only high quality and
completeness for each layer, but also spatial and visual interactions among
layers for global coherence.

</details>


### [194] [Unsupervised Detection of Distribution Shift in Inverse Problems using Diffusion Models](https://arxiv.org/pdf/2505.11482)
*Shirin Shoushtari, Edward P. Chandler, Yuanhao Wang, M. Salman Asif, Ulugbek S. Kamilov*

Main category: cs.CV

TL;DR: A fully unsupervised metric for estimating distribution shifts in diffusion models using corrupted measurements and score functions, improving reconstruction quality in inverse problems.


<details>
  <summary>Details</summary>
Motivation: Performance of diffusion models degrades under distribution shifts, and existing methods require clean test images, which are unavailable in inverse problems.

Method: Proposes a score-based metric using corrupted measurements and score functions to estimate KL divergence between training and test distributions.

Result: The metric approximates KL divergence from clean images and aligning scores improves reconstruction quality.

Conclusion: Unsupervised score-based metric effectively estimates distribution shifts and enhances performance in inverse problems.

Abstract: Diffusion models are widely used as priors in imaging inverse problems.
However, their performance often degrades under distribution shifts between the
training and test-time images. Existing methods for identifying and quantifying
distribution shifts typically require access to clean test images, which are
almost never available while solving inverse problems (at test time). We
propose a fully unsupervised metric for estimating distribution shifts using
only indirect (corrupted) measurements and score functions from diffusion
models trained on different datasets. We theoretically show that this metric
estimates the KL divergence between the training and test image distributions.
Empirically, we show that our score-based metric, using only corrupted
measurements, closely approximates the KL divergence computed from clean
images. Motivated by this result, we show that aligning the out-of-distribution
score with the in-distribution score -- using only corrupted measurements --
reduces the KL divergence and leads to improved reconstruction quality across
multiple inverse problems.

</details>


### [195] [GIE-Bench: Towards Grounded Evaluation for Text-Guided Image Editing](https://arxiv.org/pdf/2505.11493)
*Yusu Qian, Jiasen Lu, Tsu-Jui Fu, Xinze Wang, Chen Chen, Yinfei Yang, Wenze Hu, Zhe Gan*

Main category: cs.CV

TL;DR: A new benchmark (GIE-Bench) evaluates text-guided image editing models on functional correctness and content preservation, revealing GPT-Image-1's strengths and weaknesses.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation metrics (e.g., CLIP) lack precision for text-guided image editing, necessitating a more grounded benchmark.

Method: GIE-Bench uses multiple-choice questions for functional correctness and object-aware masking for content preservation, with 1000+ examples across 20 categories.

Result: GPT-Image-1 excels in instruction-following but over-modifies irrelevant regions, validated against human ratings.

Conclusion: GIE-Bench offers a scalable, reproducible framework for improving evaluation of text-guided image editing models.

Abstract: Editing images using natural language instructions has become a natural and
expressive way to modify visual content; yet, evaluating the performance of
such models remains challenging. Existing evaluation approaches often rely on
image-text similarity metrics like CLIP, which lack precision. In this work, we
introduce a new benchmark designed to evaluate text-guided image editing models
in a more grounded manner, along two critical dimensions: (i) functional
correctness, assessed via automatically generated multiple-choice questions
that verify whether the intended change was successfully applied; and (ii)
image content preservation, which ensures that non-targeted regions of the
image remain visually consistent using an object-aware masking technique and
preservation scoring. The benchmark includes over 1000 high-quality editing
examples across 20 diverse content categories, each annotated with detailed
editing instructions, evaluation questions, and spatial object masks. We
conduct a large-scale study comparing GPT-Image-1, the latest flagship in the
text-guided image editing space, against several state-of-the-art editing
models, and validate our automatic metrics against human ratings. Results show
that GPT-Image-1 leads in instruction-following accuracy, but often
over-modifies irrelevant image regions, highlighting a key trade-off in the
current model behavior. GIE-Bench provides a scalable, reproducible framework
for advancing more accurate evaluation of text-guided image editing.

</details>


### [196] [QVGen: Pushing the Limit of Quantized Video Generative Models](https://arxiv.org/pdf/2505.11497)
*Yushi Huang, Ruihao Gong, Jing Liu, Yifu Ding, Chengtao Lv, Haotong Qin, Jun Zhang*

Main category: cs.CV

TL;DR: QVGen is a quantization-aware training framework for video diffusion models, enabling high performance under low-bit quantization (e.g., 4-bit) by reducing gradient norms and using auxiliary modules, achieving full-precision quality.


<details>
  <summary>Details</summary>
Motivation: Video diffusion models face high computational and memory demands, making real-world deployment challenging. Existing quantization methods for image DMs are ineffective for video DMs.

Method: QVGen introduces auxiliary modules to mitigate quantization errors and a rank-decay strategy using SVD and rank-based regularization to eliminate inference overhead.

Result: QVGen achieves full-precision comparable quality under 4-bit settings and outperforms existing methods, with significant improvements in metrics like Dynamic Degree and Scene Consistency.

Conclusion: QVGen is the first framework to enable high-performance video DMs under extremely low-bit quantization, offering practical deployment solutions.

Abstract: Video diffusion models (DMs) have enabled high-quality video synthesis. Yet,
their substantial computational and memory demands pose serious challenges to
real-world deployment, even on high-end GPUs. As a commonly adopted solution,
quantization has proven notable success in reducing cost for image DMs, while
its direct application to video DMs remains ineffective. In this paper, we
present QVGen, a novel quantization-aware training (QAT) framework tailored for
high-performance and inference-efficient video DMs under extremely low-bit
quantization (e.g., 4-bit or below). We begin with a theoretical analysis
demonstrating that reducing the gradient norm is essential to facilitate
convergence for QAT. To this end, we introduce auxiliary modules ($\Phi$) to
mitigate large quantization errors, leading to significantly enhanced
convergence. To eliminate the inference overhead of $\Phi$, we propose a
rank-decay strategy that progressively eliminates $\Phi$. Specifically, we
repeatedly employ singular value decomposition (SVD) and a proposed rank-based
regularization $\mathbf{\gamma}$ to identify and decay low-contributing
components. This strategy retains performance while zeroing out inference
overhead. Extensive experiments across $4$ state-of-the-art (SOTA) video DMs,
with parameter sizes ranging from $1.3$B $\sim14$B, show that QVGen is the
first to reach full-precision comparable quality under 4-bit settings.
Moreover, it significantly outperforms existing methods. For instance, our
3-bit CogVideoX-2B achieves improvements of $+25.28$ in Dynamic Degree and
$+8.43$ in Scene Consistency on VBench.

</details>


### [197] [Self-Supervised Representation Learning for Nerve Fiber Distribution Patterns in 3D-PLI](https://arxiv.org/pdf/2401.17207)
*Alexander Oberstrass, Sascha E. A. Muenzing, Meiqi Niu, Nicola Palomero-Gallagher, Christian Schiffer, Markus Axer, Katrin Amunts, Timo Dickscheid*

Main category: cs.CV

TL;DR: The paper proposes a self-supervised learning method (CL-3D) to characterize nerve fiber architecture in 3D-PLI images, demonstrating its effectiveness for clustering, classification, and retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: Current methods lack observer-independent descriptors for nerve fiber architecture in 3D-PLI, hindering downstream analysis.

Method: A 3D-Context Contrastive Learning (CL-3D) objective is introduced, leveraging spatial neighborhoods and image augmentations for robust feature extraction.

Result: Features extracted are sensitive to fiber configurations but robust to histological variations, enabling clustering, classification, and retrieval tasks.

Conclusion: The CL-3D method provides a practical, data-driven solution for analyzing nerve fiber architecture in 3D-PLI images.

Abstract: A comprehensive understanding of the organizational principles in the human
brain requires, among other factors, well-quantifiable descriptors of nerve
fiber architecture. Three-dimensional polarized light imaging (3D-PLI) is a
microscopic imaging technique that enables insights into the fine-grained
organization of myelinated nerve fibers with high resolution. Descriptors
characterizing the fiber architecture observed in 3D-PLI would enable
downstream analysis tasks such as multimodal correlation studies, clustering,
and mapping. However, best practices for observer-independent characterization
of fiber architecture in 3D-PLI are not yet available. To this end, we propose
the application of a fully data-driven approach to characterize nerve fiber
architecture in 3D-PLI images using self-supervised representation learning. We
introduce a 3D-Context Contrastive Learning (CL-3D) objective that utilizes the
spatial neighborhood of texture examples across histological brain sections of
a 3D reconstructed volume to sample positive pairs for contrastive learning. We
combine this sampling strategy with specifically designed image augmentations
to gain robustness to typical variations in 3D-PLI parameter maps. The approach
is demonstrated for the 3D reconstructed occipital lobe of a vervet monkey
brain. We show that extracted features are highly sensitive to different
configurations of nerve fibers, yet robust to variations between consecutive
brain sections arising from histological processing. We demonstrate their
practical applicability for retrieving clusters of homogeneous fiber
architecture, performing classification with minimal annotations, and
query-based retrieval of characteristic components of fiber architecture such
as U-fibers.

</details>


### [198] [Learning to Deblur Polarized Images](https://arxiv.org/pdf/2402.18134)
*Chu Zhou, Minggui Teng, Xinyu Zhou, Chao Xu, Imari Sato, Boxin Shi*

Main category: cs.CV

TL;DR: A two-stage neural network pipeline for deblurring polarized images, improving DoLP and AoLP accuracy by addressing polarization constraints.


<details>
  <summary>Details</summary>
Motivation: Polarization cameras suffer from motion blur due to longer exposure times, degrading DoLP and AoLP. Conventional deblurring methods ignore polarization constraints.

Method: Divide-and-conquer strategy: decompose the problem into two sub-problems, addressed by a two-stage neural network.

Result: State-of-the-art performance on synthetic and real-world images; enhances applications like dehazing and reflection removal.

Conclusion: The proposed pipeline effectively deblurs polarized images while preserving polarization information, benefiting downstream vision tasks.

Abstract: A polarization camera can capture four linear polarized images with different
polarizer angles in a single shot, which is useful in polarization-based vision
applications since the degree of linear polarization (DoLP) and the angle of
linear polarization (AoLP) can be directly computed from the captured polarized
images. However, since the on-chip micro-polarizers block part of the light so
that the sensor often requires a longer exposure time, the captured polarized
images are prone to motion blur caused by camera shakes, leading to noticeable
degradation in the computed DoLP and AoLP. Deblurring methods for conventional
images often show degraded performance when handling the polarized images since
they only focus on deblurring without considering the polarization constraints.
In this paper, we propose a polarized image deblurring pipeline to solve the
problem in a polarization-aware manner by adopting a divide-and-conquer
strategy to explicitly decompose the problem into two less ill-posed
sub-problems, and design a two-stage neural network to handle the two
sub-problems respectively. Experimental results show that our method achieves
state-of-the-art performance on both synthetic and real-world images, and can
improve the performance of polarization-based vision applications such as image
dehazing and reflection removal.

</details>


### [199] [FreeA: Human-object Interaction Detection using Free Annotation Labels](https://arxiv.org/pdf/2403.01840)
*Qi Liu, Yuxiao Wang, Xinyu Jiang, Wolin Liang, Zhenao Wei, Yu Lei, Nan Zhuang, Weiying Xue*

Main category: cs.CV

TL;DR: FreeA is a self-adaptive, language-driven HOI detection method that eliminates manual annotation by leveraging text-image models and knowledge-based masking.


<details>
  <summary>Details</summary>
Motivation: To reduce reliance on heavily annotated datasets for HOI detection by automating label generation.

Method: Aligns image features with HOI text templates, uses knowledge-based masking, and matches interaction correlations to improve label accuracy.

Result: Achieves state-of-the-art performance, with significant mAP improvements over weakly supervised models on HICO-DET and V-COCO datasets.

Conclusion: FreeA offers a scalable and efficient solution for HOI detection, outperforming existing weakly supervised methods.

Abstract: Recent human-object interaction (HOI) detection methods depend on extensively
annotated image datasets, which require a significant amount of manpower. In
this paper, we propose a novel self-adaptive, language-driven HOI detection
method, termed FreeA. This method leverages the adaptability of the text-image
model to generate latent HOI labels without requiring manual annotation.
Specifically, FreeA aligns image features of human-object pairs with HOI text
templates and employs a knowledge-based masking technique to decrease
improbable interactions. Furthermore, FreeA implements a proposed method for
matching interaction correlations to increase the probability of actions
associated with a particular action, thereby improving the generated HOI
labels. Experiments on two benchmark datasets showcase that FreeA achieves
state-of-the-art performance among weakly supervised HOI competitors. Our
proposal gets +\textbf{13.29} (\textbf{159\%$\uparrow$}) mAP and
+\textbf{17.30} (\textbf{98\%$\uparrow$}) mAP than the newest ``Weakly''
supervised model, and +\textbf{7.19} (\textbf{28\%$\uparrow$}) mAP and
+\textbf{14.69} (\textbf{34\%$\uparrow$}) mAP than the latest ``Weakly+''
supervised model, respectively, on HICO-DET and V-COCO datasets, more accurate
in localizing and classifying the interactive actions. The source code will be
made public.

</details>


### [200] [Customizing Visual-Language Foundation Models for Multi-modal Anomaly Detection and Reasoning](https://arxiv.org/pdf/2403.11083)
*Xiaohao Xu, Yunkang Cao, Huaxin Zhang, Nong Sang, Xiaonan Huang*

Main category: cs.CV

TL;DR: The paper proposes a generic anomaly detection model using visual-language foundation models, enhanced by multi-modal prompting and unified 2D image representation, showing improved performance across diverse data types.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly detection methods lack generalization across scenarios, prompting the need for a versatile model applicable to multiple industrial contexts.

Method: Custom-built visual-language foundation models with multi-modal prompting (task descriptions, class context, normality rules, reference images) and unified 2D input representation.

Result: Enhanced anomaly detection performance across images, point clouds, and videos, with qualitative success in multi-object scenes and temporal data.

Conclusion: The approach demonstrates effective generalization and reasoning for anomaly detection, with publicly available code for further use.

Abstract: Anomaly detection is vital in various industrial scenarios, including the
identification of unusual patterns in production lines and the detection of
manufacturing defects for quality control. Existing techniques tend to be
specialized in individual scenarios and lack generalization capacities. In this
study, our objective is to develop a generic anomaly detection model that can
be applied in multiple scenarios. To achieve this, we custom-build generic
visual language foundation models that possess extensive knowledge and robust
reasoning abilities as anomaly detectors and reasoners. Specifically, we
introduce a multi-modal prompting strategy that incorporates domain knowledge
from experts as conditions to guide the models. Our approach considers diverse
prompt types, including task descriptions, class context, normality rules, and
reference images. In addition, we unify the input representation of
multi-modality into a 2D image format, enabling multi-modal anomaly detection
and reasoning. Our preliminary studies demonstrate that combining visual and
language prompts as conditions for customizing the models enhances anomaly
detection performance. The customized models showcase the ability to detect
anomalies across different data modalities such as images, point clouds, and
videos. Qualitative case studies further highlight the anomaly detection and
reasoning capabilities, particularly for multi-object scenes and temporal data.
Our code is publicly available at
https://github.com/Xiaohao-Xu/Customizable-VLM

</details>


### [201] [From Image to Video, what do we need in multimodal LLMs?](https://arxiv.org/pdf/2404.11865)
*Suyuan Huang, Haoxin Zhang, Linqing Zhong, Honggu Chen, Yan Gao, Yao Hu, Zengchang Qin*

Main category: cs.CV

TL;DR: RED-VILLM introduces a resource-efficient pipeline to develop Video LLMs by leveraging Image LLMs, reducing data and training needs while outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing Video LLMs are complex and data-intensive, neglecting the potential of Image LLMs. RED-VILLM aims to simplify and enhance Video LLM development.

Method: Uses a plug-and-play temporal adaptation structure on Image LLMs to handle video data, requiring minimal training resources.

Result: Outperforms conventional Video LLMs with less data and resources, including the first Chinese-speaking Video LLM.

Conclusion: RED-VILLM offers a cost-effective, scalable approach for advancing multimodal models.

Abstract: Covering from Image LLMs to the more complex Video LLMs, the Multimodal Large
Language Models (MLLMs) have demonstrated profound capabilities in
comprehending cross-modal information as numerous studies have illustrated.
Previous methods delve into designing comprehensive Video LLMs through
integrating video foundation models with primitive LLMs. Despite its
effectiveness, such paradigm renders Video LLM's structure verbose and
typically requires substantial video data for pre-training. Crucially, it
neglects leveraging the foundational contributions of ready-made Image LLMs. In
this paper, we introduce RED-VILLM, a Resource-Efficient Development pipeline
which builds robust Video LLMs through leveraging the prior knowledge of Image
LLMs. Specifically, since a video is naturally a combination of images along
the temporal dimension, we devise a temporal adaptation plug-and-play
structure, endowing the backbone Image LLM with the capability to grasp
temporal information. Moreover, through applying this pipeline, we achieve the
first Video LLM within the Chinese-speaking community. Extensive experiments
demonstrate that Video LLMs developed through our approach surpass conventional
Video LLMs, requiring minimal instructional data and training resources. Our
approach highlights the potential for a more cost-effective and scalable
advancement in multimodal models.

</details>


### [202] [A Review on Discriminative Self-supervised Learning Methods in Computer Vision](https://arxiv.org/pdf/2405.04969)
*Nikolaos Giakoumoglou, Tania Stathaki, Athanasios Gkelias*

Main category: cs.CV

TL;DR: A review of discriminative self-supervised learning (SSL) methods in computer vision, categorizing them into five groups and analyzing their principles, performance, and challenges.


<details>
  <summary>Details</summary>
Motivation: To reduce reliance on costly manual annotations by leveraging unlabeled data for feature representation learning.

Method: Systematic categorization of discriminative SSL into contrastive, clustering, self-distillation, knowledge distillation, and feature decorrelation methods, with detailed analysis of each.

Result: Comparative evaluations on benchmarks like ImageNet show SSL's effectiveness, though challenges like computational demands persist.

Conclusion: The review synthesizes advancements, highlights open challenges, and guides future research in discriminative SSL for robust computer vision models.

Abstract: Self-supervised learning (SSL) has rapidly emerged as a transformative
approach in computer vision, enabling the extraction of rich feature
representations from vast amounts of unlabeled data and reducing reliance on
costly manual annotations. This review presents a comprehensive analysis of
discriminative SSL methods, which focus on learning representations by solving
pretext tasks that do not require human labels. The paper systematically
categorizes discriminative SSL approaches into five main groups: contrastive
methods, clustering methods, self-distillation methods, knowledge distillation
methods, and feature decorrelation methods. For each category, the review
details the underlying principles, architectural components, loss functions,
and representative algorithms, highlighting their unique mechanisms and
contributions to the field. Extensive comparative evaluations are provided,
including linear and semi-supervised protocols on standard benchmarks such as
ImageNet, as well as transfer learning performance across diverse downstream
tasks. The review also discusses theoretical foundations, scalability,
efficiency, and practical challenges, such as computational demands and
accessibility. By synthesizing recent advancements and identifying key trends,
open challenges, and future research directions, this work serves as a valuable
resource for researchers and practitioners aiming to leverage discriminative
SSL for robust and generalizable computer vision models.

</details>


### [203] [GLDiTalker: Speech-Driven 3D Facial Animation with Graph Latent Diffusion Transformer](https://arxiv.org/pdf/2408.01826)
*Yihong Lin, Zhaoxin Fan, Xianjia Wu, Lingyu Xiong, Liang Peng, Xiandong Li, Wenxiong Kang, Songju Lei, Huang Xu*

Main category: cs.CV

TL;DR: GLDiTalker is a novel speech-driven 3D facial animation model using a Graph Latent Diffusion Transformer to address modality misalignment, improving lip-sync accuracy and motion diversity.


<details>
  <summary>Details</summary>
Motivation: Existing methods suffer from modality inconsistencies (audio-mesh misalignment), reducing motion diversity and lip-sync accuracy.

Method: GLDiTalker uses a two-stage pipeline: Graph-Enhanced Quantized Space Learning for lip-sync accuracy and Space-Time Powered Latent Diffusion for motion diversity.

Result: Outperforms existing methods in lip-sync accuracy and motion diversity on standard benchmarks.

Conclusion: GLDiTalker generates realistic, stable 3D facial animations, addressing key challenges in speech-driven talking head generation.

Abstract: Speech-driven talking head generation is a critical yet challenging task with
applications in augmented reality and virtual human modeling. While recent
approaches using autoregressive and diffusion-based models have achieved
notable progress, they often suffer from modality inconsistencies, particularly
misalignment between audio and mesh, leading to reduced motion diversity and
lip-sync accuracy. To address this, we propose GLDiTalker, a novel
speech-driven 3D facial animation model based on a Graph Latent Diffusion
Transformer. GLDiTalker resolves modality misalignment by diffusing signals
within a quantized spatiotemporal latent space. It employs a two-stage training
pipeline: the Graph-Enhanced Quantized Space Learning Stage ensures lip-sync
accuracy, while the Space-Time Powered Latent Diffusion Stage enhances motion
diversity. Together, these stages enable GLDiTalker to generate realistic,
temporally stable 3D facial animations. Extensive evaluations on standard
benchmarks demonstrate that GLDiTalker outperforms existing methods, achieving
superior results in both lip-sync accuracy and motion diversity.

</details>


### [204] [EmoFace: Emotion-Content Disentangled Speech-Driven 3D Talking Face Animation](https://arxiv.org/pdf/2408.11518)
*Yihong Lin, Liang Peng, Zhaoxin Fan, Xianjia Wu, Jianqiao Hu, Xiandong Li, Wenxiong Kang, Songju Lei*

Main category: cs.CV

TL;DR: EmoFace, a two-stream network with Mesh Attention and SpiralConv3D, improves 3D talking face animation by capturing emotion-facial motion correlations, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current speech-driven 3D face models focus on lip sync but overlook emotion-facial motion correlations, limiting realism.

Method: Proposes EmoFace with emotion and content branches, Mesh Attention, SpiralConv3D, and a self-growing training scheme.

Result: Achieves high accuracy on 3D-RAVDESS and VOCASET datasets (e.g., $4.8863\times 10^{-5}$mm for LVE).

Conclusion: EmoFace advances 3D emotional facial animation by integrating emotion and content features effectively.

Abstract: The creation of increasingly vivid 3D talking face has become a hot topic in
recent years. Currently, most speech-driven works focus on lip synchronisation
but neglect to effectively capture the correlations between emotions and facial
motions. To address this problem, we propose a two-stream network called
EmoFace, which consists of an emotion branch and a content branch. EmoFace
employs a novel Mesh Attention mechanism to analyse and fuse the emotion
features and content features. Particularly, a newly designed spatio-temporal
graph-based convolution, SpiralConv3D, is used in Mesh Attention to learn
potential temporal and spatial feature dependencies between mesh vertices. In
addition, to the best of our knowledge, it is the first time to introduce a new
self-growing training scheme with intermediate supervision to dynamically
adjust the ratio of groundtruth adopted in the 3D face animation task.
Comprehensive quantitative and qualitative evaluations on our high-quality 3D
emotional facial animation dataset, 3D-RAVDESS ($4.8863\times 10^{-5}$mm for
LVE and $0.9509\times 10^{-5}$mm for EVE), together with the public dataset
VOCASET ($2.8669\times 10^{-5}$mm for LVE and $0.4664\times 10^{-5}$mm for
EVE), demonstrate that our approach achieves state-of-the-art performance.

</details>


### [205] [HaHeAE: Learning Generalisable Joint Representations of Human Hand and Head Movements in Extended Reality](https://arxiv.org/pdf/2410.16430)
*Zhiming Hu, Guanhua Zhang, Zheming Yin, Daniel Haeufle, Syn Schmitt, Andreas Bulling*

Main category: cs.CV

TL;DR: HaHeAE is a self-supervised method for learning joint representations of hand and head movements in XR, outperforming existing methods by 74% in reconstruction quality and enabling new applications.


<details>
  <summary>Details</summary>
Motivation: Prior works on hand and head modelling in XR were limited to single modalities or specific applications, lacking generalizability.

Method: Uses an autoencoder with a graph convolutional network-based semantic encoder and diffusion-based stochastic encoder, plus a diffusion-based decoder.

Result: Outperforms other methods by 74% in reconstruction, generalizes across users/activities/environments, and enables new applications like cluster identification and movement generation.

Conclusion: Demonstrates the effectiveness of self-supervised methods for joint hand-head modelling in XR, highlighting their potential for diverse applications.

Abstract: Human hand and head movements are the most pervasive input modalities in
extended reality (XR) and are significant for a wide range of applications.
However, prior works on hand and head modelling in XR only explored a single
modality or focused on specific applications. We present HaHeAE - a novel
self-supervised method for learning generalisable joint representations of hand
and head movements in XR. At the core of our method is an autoencoder (AE) that
uses a graph convolutional network-based semantic encoder and a diffusion-based
stochastic encoder to learn the joint semantic and stochastic representations
of hand-head movements. It also features a diffusion-based decoder to
reconstruct the original signals. Through extensive evaluations on three public
XR datasets, we show that our method 1) significantly outperforms commonly used
self-supervised methods by up to 74.0% in terms of reconstruction quality and
is generalisable across users, activities, and XR environments, 2) enables new
applications, including interpretable hand-head cluster identification and
variable hand-head movement generation, and 3) can serve as an effective
feature extractor for downstream tasks. Together, these results demonstrate the
effectiveness of our method and underline the potential of self-supervised
methods for jointly modelling hand-head behaviours in extended reality.

</details>


### [206] [SynCL: A Synergistic Training Strategy with Instance-Aware Contrastive Learning for End-to-End Multi-Camera 3D Tracking](https://arxiv.org/pdf/2411.06780)
*Shubo Lin, Yutong Kou, Zirui Wu, Shaoru Wang, Bing Li, Weiming Hu, Jin Gao*

Main category: cs.CV

TL;DR: SynCL introduces a synergistic training strategy for 3D visual tracking, addressing limitations of self-attention in multi-task learning, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing query-based 3D trackers face optimization challenges due to self-attention constraints (over-deduplication and self-centric attention), hindering detection and tracking synergy.

Method: SynCL includes a Task-specific Hybrid Matching module, Dynamic Query Filtering, and Instance-aware Contrastive Learning to enhance multi-task learning without added inference costs.

Result: Achieves 58.9% AMOTA on nuScenes, outperforming benchmarks.

Conclusion: SynCL effectively bridges detection and tracking gaps, offering a plug-and-play solution with significant performance improvements.

Abstract: While existing query-based 3D end-to-end visual trackers integrate detection
and tracking via the tracking-by-attention paradigm, these two chicken-and-egg
tasks encounter optimization difficulties when sharing the same parameters. Our
findings reveal that these difficulties arise due to two inherent constraints
on the self-attention mechanism, i.e., over-deduplication for object queries
and self-centric attention for track queries. In contrast, removing the
self-attention mechanism not only minimally impacts regression predictions of
the tracker, but also tends to generate more latent candidate boxes. Based on
these analyses, we present SynCL, a novel plug-and-play synergistic training
strategy designed to co-facilitate multi-task learning for detection and
tracking. Specifically, we propose a Task-specific Hybrid Matching module for a
weight-shared cross-attention-based decoder that matches the targets of track
queries with multiple object queries to exploit promising candidates overlooked
by the self-attention mechanism. To flexibly select optimal candidates for the
one-to-many matching, we also design a Dynamic Query Filtering module
controlled by model training status. Moreover, we introduce Instance-aware
Contrastive Learning to break through the barrier of self-centric attention for
track queries, effectively bridging the gap between detection and tracking.
Without additional inference costs, SynCL consistently delivers improvements in
various benchmarks and achieves state-of-the-art performance with $58.9\%$
AMOTA on the nuScenes dataset. Code and raw results will be publicly available.

</details>


### [207] [Learning Robust Anymodal Segmentor with Unimodal and Cross-modal Distillation](https://arxiv.org/pdf/2411.17141)
*Xu Zheng, Haiwei Xue, Jialei Chen, Yibo Yan, Lutao Jiang, Yuanhuiyi Lyu, Kailun Yang, Linfeng Zhang, Xuming Hu*

Main category: cs.CV

TL;DR: A framework for robust multimodal segmentation is proposed, addressing unimodal bias through distillation techniques.


<details>
  <summary>Details</summary>
Motivation: Multimodal segmentors often over-rely on certain modalities, leading to performance drops when others are missing.

Method: Uses parallel multimodal learning, cross-modal distillation, and modality-agnostic semantic distillation.

Result: Superior performance on synthetic and real-world benchmarks.

Conclusion: The framework effectively mitigates unimodal bias and enhances robustness in multimodal segmentation.

Abstract: Simultaneously using multimodal inputs from multiple sensors to train
segmentors is intuitively advantageous but practically challenging. A key
challenge is unimodal bias, where multimodal segmentors over rely on certain
modalities, causing performance drops when others are missing, common in real
world applications. To this end, we develop the first framework for learning
robust segmentor that can handle any combinations of visual modalities.
Specifically, we first introduce a parallel multimodal learning strategy for
learning a strong teacher. The cross-modal and unimodal distillation is then
achieved in the multi scale representation space by transferring the feature
level knowledge from multimodal to anymodal segmentors, aiming at addressing
the unimodal bias and avoiding over-reliance on specific modalities. Moreover,
a prediction level modality agnostic semantic distillation is proposed to
achieve semantic knowledge transferring for segmentation. Extensive experiments
on both synthetic and real-world multi-sensor benchmarks demonstrate that our
method achieves superior performance.

</details>


### [208] [Evaluating Vision-Language Models as Evaluators in Path Planning](https://arxiv.org/pdf/2411.18711)
*Mohamed Aghzal, Xiang Yue, Erion Plaku, Ziyu Yao*

Main category: cs.CV

TL;DR: The paper introduces PathEval, a benchmark to evaluate Vision-Language Models (VLMs) as plan evaluators in path-planning tasks, revealing their challenges in low-level perception and integration.


<details>
  <summary>Details</summary>
Motivation: To explore if VLMs, despite limitations in planning, can serve as effective plan evaluators in complex scenarios.

Method: PathEval benchmark assesses VLMs' ability to abstract optimal path traits, demonstrate precise perception, and integrate information.

Result: VLMs struggle with low-level vision details, hindering performance, and fine-tuning alone doesn't resolve this. Task-specific adaptation is needed.

Conclusion: VLMs show promise as evaluators but require improved vision encoders for effective path-planning evaluation.

Abstract: Despite their promise to perform complex reasoning, large language models
(LLMs) have been shown to have limited effectiveness in end-to-end planning.
This has inspired an intriguing question: if these models cannot plan well, can
they still contribute to the planning framework as a helpful plan evaluator? In
this work, we generalize this question to consider LLMs augmented with visual
understanding, i.e., Vision-Language Models (VLMs). We introduce PathEval, a
novel benchmark evaluating VLMs as plan evaluators in complex path-planning
scenarios. Succeeding in the benchmark requires a VLM to be able to abstract
traits of optimal paths from the scenario description, demonstrate precise
low-level perception on each path, and integrate this information to decide the
better path. Our analysis of state-of-the-art VLMs reveals that these models
face significant challenges on the benchmark. We observe that the VLMs can
precisely abstract given scenarios to identify the desired traits and exhibit
mixed performance in integrating the provided information. Yet, their vision
component presents a critical bottleneck, with models struggling to perceive
low-level details about a path. Our experimental results show that this issue
cannot be trivially addressed via end-to-end fine-tuning; rather, task-specific
discriminative adaptation of these vision encoders is needed for these VLMs to
become effective path evaluators.

</details>


### [209] [Inspiring the Next Generation of Segment Anything Models: Comprehensively Evaluate SAM and SAM 2 with Diverse Prompts Towards Context-Dependent Concepts under Different Scenes](https://arxiv.org/pdf/2412.01240)
*Xiaoqi Zhao, Youwei Pang, Shijie Chang, Yuan Zhao, Lihe Zhang, Huchuan Lu, Georges El Fakhri, Xiaofeng Liu*

Main category: cs.CV

TL;DR: The paper evaluates SAM and SAM 2's performance on context-dependent (CD) concepts, introduces a unified evaluation framework, and discusses their potential and limitations for future segmentation models like SAM 3.


<details>
  <summary>Details</summary>
Motivation: SAM and SAM 2 excel in segmenting context-independent concepts but overlook CD concepts, which rely on contextual information. A lack of comprehensive evaluation limits understanding of their performance boundaries.

Method: The study conducts a quantitative evaluation on 11 CD concepts across 2D/3D images and videos, using a unified framework with manual, automatic, and self-prompting strategies. It also tests SAM 2's in-context learning and prompt robustness.

Result: The evaluation reveals SAMs' strengths and weaknesses in handling CD concepts, providing insights into their discriminative capabilities and limitations.

Conclusion: The work offers guidance for future research on CD concept segmentation and informs the development of SAM 3, highlighting the need for improved contextual understanding.

Abstract: As a foundational model, SAM has significantly influenced multiple fields
within computer vision, and its upgraded version, SAM 2, enhances capabilities
in video segmentation, poised to make a substantial impact once again. While
SAMs (SAM and SAM 2) have demonstrated excellent performance in segmenting
context-independent concepts like people, cars, and roads, they overlook more
challenging context-dependent (CD) concepts, such as visual saliency,
camouflage, product defects, and medical lesions. CD concepts rely heavily on
global and local contextual information, making them susceptible to shifts in
different contexts, which requires strong discriminative capabilities from the
model. The lack of comprehensive evaluation of SAMs limits understanding of
their performance boundaries, which may hinder the design of future models. In
this paper, we conduct a thorough quantitative evaluation of SAMs on 11 CD
concepts across 2D and 3D images and videos in various visual modalities within
natural, medical, and industrial scenes. We develop a unified evaluation
framework for SAM and SAM 2 that supports manual, automatic, and intermediate
self-prompting, aided by our specific prompt generation and interaction
strategies. We further explore the potential of SAM 2 for in-context learning
and introduce prompt robustness testing to simulate real-world imperfect
prompts. Finally, we analyze the benefits and limitations of SAMs in
understanding CD concepts and discuss their future development in segmentation
tasks. This work aims to provide valuable insights to guide future research in
both context-independent and context-dependent concepts segmentation,
potentially informing the development of the next version -- SAM 3.

</details>


### [210] [Espresso: High Compression For Rich Extraction From Videos for Your Vision-Language Model](https://arxiv.org/pdf/2412.04729)
*Keunwoo Peter Yu, Achal Dave, Rares Ambrus, Jean Mercat*

Main category: cs.CV

TL;DR: Espresso is a new architecture for compressing spatial and temporal features in videos into fixed-length sequences, enabling efficient video encoding while maintaining strong reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Extending vision-language models to long videos is challenging due to high token counts, and existing pooling-based methods sacrifice fixed-length representations.

Method: Espresso separately compresses spatial and temporal features into fixed-length sequences, combining fixed-length compression with segment-wise processing.

Result: Experiments show Espresso offers a scalable and competitive alternative to pooling-based approaches, maintaining strong long-form reasoning.

Conclusion: Fixed-length projectors, like Espresso, are viable for video-language modeling when properly designed and trained.

Abstract: Recent advances in vision-language models (VLMs) have shown great promise in
connecting images and text, but extending these models to long videos remains
challenging due to the rapid growth in token counts. Models that compress
videos by local aggregation in time or space have become popular for handling
long-form inputs; however, these pooling-based projectors sacrifice the
benefits of fixed-length representations that are crucial for streaming and
efficient video understanding. We introduce $\texttt{Espresso}$, a new
architecture that separately compresses spatial and temporal features into
fixed-length sequences. $\texttt{Espresso}$ enables efficient video encoding
while maintaining strong long-form reasoning capabilities. Experiments show
that fixed-length compression combined with segment-wise processing offers a
scalable and competitive alternative to pooling-based approaches. Our results
demonstrate that fixed-length projectors, when properly designed and trained,
remain a viable foundation for video-language modeling.

</details>


### [211] [Efficient and Comprehensive Feature Extraction in Large Vision-Language Model for Pathology Analysis](https://arxiv.org/pdf/2412.09521)
*Shengxuming Zhang, Weihan Li, Tianhong Gao, Jiacong Hu, Haoming Luo, Xiuming Zhang, Jing Zhang, Mingli Song, Zunlei Feng*

Main category: cs.CV

TL;DR: The paper proposes OmniPath, a pathology-specialized LVLM, using mixed task-guided feature enhancement and prompt-guided detail feature completion to improve WSI analysis, outperforming existing methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing LVLMs are limited by input resolution constraints, reducing their effectiveness in pathology image analysis, which requires multi-scale, high-resolution WSI analysis for accurate diagnosis.

Method: Two strategies are introduced: mixed task-guided feature enhancement for lesion-related feature extraction across scales, and prompt-guided detail feature completion to integrate coarse- and fine-grained WSI features without slowing inference.

Result: OmniPath, trained on 490K samples, significantly outperforms existing methods in diagnostic accuracy and efficiency.

Conclusion: OmniPath offers an interactive, clinically aligned solution for auxiliary pathology diagnosis, enhancing accuracy and efficiency.

Abstract: Pathological diagnosis is vital for determining disease characteristics,
guiding treatment, and assessing prognosis, relying heavily on detailed,
multi-scale analysis of high-resolution whole slide images (WSI). However,
existing large vision-language models (LVLMs) are limited by input resolution
constraints, hindering their efficiency and accuracy in pathology image
analysis. To overcome these issues, we propose two innovative strategies: the
mixed task-guided feature enhancement, which directs feature extraction toward
lesion-related details across scales, and the prompt-guided detail feature
completion, which integrates coarse- and fine-grained features from WSI based
on specific prompts without compromising inference speed. Leveraging a
comprehensive dataset of 490K samples from diverse pathology tasks, we trained
the pathology-specialized LVLM, OmniPath. Extensive experiments demonstrate
that this model significantly outperforms existing methods in diagnostic
accuracy and efficiency, providing an interactive, clinically aligned approach
for auxiliary diagnosis in a wide range of pathology applications.

</details>


### [212] [L-WISE: Boosting Human Visual Category Learning Through Model-Based Image Selection and Enhancement](https://arxiv.org/pdf/2412.09765)
*Morgan B. Talbot, Gabriel Kreiman, James J. DiCarlo, Guy Gaziv*

Main category: cs.CV

TL;DR: Artificial neural networks improve human visual learning by selecting difficult images and applying perturbations, boosting accuracy by 33-72% and reducing training time by 20-23%.


<details>
  <summary>Details</summary>
Motivation: To enhance human visual learning and categorization accuracy using model-based strategies derived from artificial neural networks.

Method: Augmenting learning by (i) selecting images based on model-estimated difficulty and (ii) applying perturbations to aid recognition.

Result: Categorization accuracy improved by 33-72%, and training time reduced by 20-23% in fine-grained and clinically relevant tasks.

Conclusion: Artificial neural networks can effectively augment human visual learning, improving accuracy and efficiency.

Abstract: The currently leading artificial neural network models of the visual ventral
stream - which are derived from a combination of performance optimization and
robustification methods - have demonstrated a remarkable degree of behavioral
alignment with humans on visual categorization tasks. We show that image
perturbations generated by these models can enhance the ability of humans to
accurately report the ground truth class. Furthermore, we find that the same
models can also be used out-of-the-box to predict the proportion of correct
human responses to individual images, providing a simple, human-aligned
estimator of the relative difficulty of each image. Motivated by these
observations, we propose to augment visual learning in humans in a way that
improves human categorization accuracy at test time. Our learning augmentation
approach consists of (i) selecting images based on their model-estimated
recognition difficulty, and (ii) applying image perturbations that aid
recognition for novice learners. We find that combining these model-based
strategies leads to categorization accuracy gains of 33-72% relative to control
subjects without these interventions, on unmodified, randomly selected held-out
test images. Beyond the accuracy gain, the training time for the augmented
learning group was also shortened by 20-23%, despite both groups completing the
same number of training trials. We demonstrate the efficacy of our approach in
a fine-grained categorization task with natural images, as well as two tasks in
clinically relevant image domains - histology and dermoscopy - where visual
learning is notoriously challenging. To the best of our knowledge, our work is
the first application of artificial neural networks to increase visual learning
performance in humans by enhancing category-specific image features.

</details>


### [213] [FastVLM: Efficient Vision Encoding for Vision Language Models](https://arxiv.org/pdf/2412.13303)
*Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokul Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, Hadi Pouransari*

Main category: cs.CV

TL;DR: FastVLM optimizes Vision Language Models by reducing latency and token count for high-resolution images, achieving better performance and speed.


<details>
  <summary>Details</summary>
Motivation: High-resolution images improve VLM performance but increase latency and token count, making current methods inefficient.

Method: Introduces FastVLM with FastViTHD, a hybrid vision encoder that reduces tokens and encoding time without additional pruning.

Result: FastVLM achieves 3.2√ó faster TTFT, better benchmark performance, and 85√ó faster TTFT compared to LLaVa-OneVision.

Conclusion: FastVLM balances latency, model size, and accuracy efficiently, simplifying high-resolution VLM design.

Abstract: Scaling the input image resolution is essential for enhancing the performance
of Vision Language Models (VLMs), particularly in text-rich image understanding
tasks. However, popular visual encoders such as ViTs become inefficient at high
resolutions due to the large number of tokens and high encoding latency caused
by stacked self-attention layers. At different operational resolutions, the
vision encoder of a VLM can be optimized along two axes: reducing encoding
latency and minimizing the number of visual tokens passed to the LLM, thereby
lowering overall latency. Based on a comprehensive efficiency analysis of the
interplay between image resolution, vision latency, token count, and LLM size,
we introduce FastVLM, a model that achieves an optimized trade-off between
latency, model size and accuracy. FastVLM incorporates FastViTHD, a novel
hybrid vision encoder designed to output fewer tokens and significantly reduce
encoding time for high-resolution images. Unlike previous methods, FastVLM
achieves the optimal balance between visual token count and image resolution
solely by scaling the input image, eliminating the need for additional token
pruning and simplifying the model design. In the LLaVA-1.5 setup, FastVLM
achieves 3.2$\times$ improvement in time-to-first-token (TTFT) while
maintaining similar performance on VLM benchmarks compared to prior works.
Compared to LLaVa-OneVision at the highest resolution (1152$\times$1152),
FastVLM achieves better performance on key benchmarks like SeedBench, MMMU and
DocVQA, using the same 0.5B LLM, but with 85$\times$ faster TTFT and a vision
encoder that is 3.4$\times$ smaller. Code and models are available at
https://github.com/apple/ml-fastvlm.

</details>


### [214] [NeRF-To-Real Tester: Neural Radiance Fields as Test Image Generators for Vision of Autonomous Systems](https://arxiv.org/pdf/2412.16141)
*Laura Weihl, Bilal Wehbe, Andrzej WƒÖsowski*

Main category: cs.CV

TL;DR: The paper introduces N2R-Tester, a tool using Neural Radiance Fields to generate realistic test images for autonomous systems, addressing overfitting in simulation-trained controllers.


<details>
  <summary>Details</summary>
Motivation: Overfitting of controllers to simulation conditions in AUVs and UAVs leads to poor real-world performance, necessitating diverse and realistic test data.

Method: Leverages Neural Radiance Fields to create realistic test images and integrates them into a metamorphic testing framework for vision components.

Result: N2R-Tester effectively generates diverse test data, demonstrated by its evaluation on eight vision components in AUVs and UAVs.

Conclusion: The approach is versatile and effective for improving the robustness of autonomous systems' vision components.

Abstract: Autonomous inspection of infrastructure on land and in water is a quickly
growing market, with applications including surveying constructions, monitoring
plants, and tracking environmental changes in on- and off-shore wind energy
farms. For Autonomous Underwater Vehicles and Unmanned Aerial Vehicles
overfitting of controllers to simulation conditions fundamentally leads to poor
performance in the operation environment. There is a pressing need for more
diverse and realistic test data that accurately represents the challenges faced
by these systems. We address the challenge of generating perception test data
for autonomous systems by leveraging Neural Radiance Fields to generate
realistic and diverse test images, and integrating them into a metamorphic
testing framework for vision components such as vSLAM and object detection. Our
tool, N2R-Tester, allows training models of custom scenes and rendering test
images from perturbed positions. An experimental evaluation of N2R-Tester on
eight different vision components in AUVs and UAVs demonstrates the efficacy
and versatility of the approach.

</details>


### [215] [Resolving the Ambiguity of Complete-to-Partial Point Cloud Registration for Image-Guided Liver Surgery with Patches-to-Partial Matching](https://arxiv.org/pdf/2412.19328)
*Zixin Yang, Jon S. Heiselman, Cheng Han, Kelly Merrell, Richard Simon, Cristian. A. Linte*

Main category: cs.CV

TL;DR: The paper addresses the challenge of rigid alignment in image-guided liver surgery, proposing a patches-to-partial matching strategy to improve registration accuracy in cases with limited intraoperative visibility.


<details>
  <summary>Details</summary>
Motivation: Current semi-automatic alignment methods in liver surgery are error-prone and require manual correction, especially in scenarios with limited intraoperative surface visibility (complete-to-partial ambiguity).

Method: The authors evaluate state-of-the-art learning-based point cloud registration methods and propose a patches-to-partial matching module to resolve ambiguity, integrating it seamlessly into existing methods.

Result: The proposed module improves registration performance in cases with limited visibility, as validated on in silico and in vitro datasets.

Conclusion: The benchmark and module provide a foundation for advancing point cloud correspondence-based registration in image-guided liver surgery.

Abstract: In image-guided liver surgery, the initial rigid alignment between
preoperative and intraoperative data, often represented as point clouds, is
crucial for providing sub-surface information from preoperative CT/MRI images
to the surgeon during the procedure. Currently, this alignment is typically
performed using semi-automatic methods, which, while effective to some extent,
are prone to errors that demand manual correction. Point cloud
correspondence-based registration methods are promising to serve as a fully
automatic solution. However, they may struggle in scenarios with limited
intraoperative surface visibility, a common challenge in liver surgery,
particularly in laparoscopic procedures, which we refer to as
complete-to-partial ambiguity. We first illustrate this ambiguity by evaluating
the performance of state-of-the-art learning-based point cloud registration
methods on our carefully constructed in silico and in vitro datasets. Then, we
propose a patches-to-partial matching strategy as a plug-and-play module to
resolve the ambiguity, which can be seamlessly integrated into learning-based
registration methods without disrupting their end-to-end structure. It has
proven effective and efficient in improving registration performance for cases
with limited intraoperative visibility. The constructed benchmark and the
proposed module establish a solid foundation for advancing applications of
point cloud correspondence-based registration methods in image-guided liver
surgery.

</details>


### [216] [Communication-Efficient Federated Learning Based on Explanation-Guided Pruning for Remote Sensing Image Classification](https://arxiv.org/pdf/2501.11493)
*Jonas Klotz, Barƒ±≈ü B√ºy√ºkta≈ü, Beg√ºm Demir*

Main category: cs.CV

TL;DR: An explanation-guided pruning strategy for federated learning (FL) reduces communication overhead in remote sensing (RS) image classification by identifying and sharing only the most relevant model parameters.


<details>
  <summary>Details</summary>
Motivation: FL systems face high communication costs due to large model updates, especially in RS applications with restricted bandwidth.

Method: Uses layer-wise relevance propagation (LRP) to prune non-informative model parameters, minimizing update volume.

Result: Effective reduction in shared updates and improved global model generalization on the BigEarthNet-S2 dataset.

Conclusion: The strategy enhances communication efficiency in FL for RS without compromising model performance.

Abstract: Federated learning (FL) is a decentralized machine learning paradigm in which
multiple clients collaboratively train a global model by exchanging only model
updates with the central server without sharing the local data of the clients.
Due to the large volume of model updates required to be transmitted between
clients and the central server, most FL systems are associated with high
transfer costs (i.e., communication overhead). This issue is more critical for
operational applications in remote sensing (RS), especially when large-scale RS
data is processed and analyzed through FL systems with restricted communication
bandwidth. To address this issue, we introduce an explanation-guided pruning
strategy for communication-efficient FL in the context of RS image
classification. Our pruning strategy is defined based on the layer-wise
relevance propagation (LRP) driven explanations to: 1) efficiently and
effectively identify the most relevant and informative model parameters (to be
exchanged between clients and the central server); and 2) eliminate the
non-informative ones to minimize the volume of model updates. The experimental
results on the BigEarthNet-S2 dataset demonstrate that our strategy effectively
reduces the number of shared model updates, while increasing the generalization
ability of the global model. The code of this work is publicly available at
https://git.tu-berlin.de/rsim/FL-LRP.

</details>


### [217] [INSIGHT: Enhancing Autonomous Driving Safety through Vision-Language Models on Context-Aware Hazard Detection and Edge Case Evaluation](https://arxiv.org/pdf/2502.00262)
*Dianwei Chen, Zifan Zhang, Yuchen Liu, Xianfeng Terry Yang*

Main category: cs.CV

TL;DR: INSIGHT, a hierarchical vision-language model, improves hazard detection in autonomous driving by fusing semantic and visual inputs, outperforming existing models on the BDD100K dataset.


<details>
  <summary>Details</summary>
Motivation: Current end-to-end driving models struggle with unpredictable edge-case scenarios like adversarial pedestrian movements and sudden environmental changes, limiting generalization.

Method: Proposes INSIGHT, a hierarchical VLM framework using multimodal data fusion, attention-based mechanisms, and coordinate regression for hazard localization.

Result: Demonstrates improved hazard prediction accuracy and generalization on the BDD100K dataset.

Conclusion: INSIGHT enhances autonomous driving robustness and safety by improving situational awareness and decision-making in complex scenarios.

Abstract: Autonomous driving systems face significant challenges in handling
unpredictable edge-case scenarios, such as adversarial pedestrian movements,
dangerous vehicle maneuvers, and sudden environmental changes. Current
end-to-end driving models struggle with generalization to these rare events due
to limitations in traditional detection and prediction approaches. To address
this, we propose INSIGHT (Integration of Semantic and Visual Inputs for
Generalized Hazard Tracking), a hierarchical vision-language model (VLM)
framework designed to enhance hazard detection and edge-case evaluation. By
using multimodal data fusion, our approach integrates semantic and visual
representations, enabling precise interpretation of driving scenarios and
accurate forecasting of potential dangers. Through supervised fine-tuning of
VLMs, we optimize spatial hazard localization using attention-based mechanisms
and coordinate regression techniques. Experimental results on the BDD100K
dataset demonstrate a substantial improvement in hazard prediction
straightforwardness and accuracy over existing models, achieving a notable
increase in generalization performance. This advancement enhances the
robustness and safety of autonomous driving systems, ensuring improved
situational awareness and potential decision-making in complex real-world
scenarios.

</details>


### [218] [Disentangling CLIP for Multi-Object Perception](https://arxiv.org/pdf/2502.02977)
*Samyak Rawlekar, Yujun Cai, Yiwei Wang, Ming-Hsuan Yang, Narendra Ahuja*

Main category: cs.CV

TL;DR: DCLIP disentangles CLIP features to reduce semantic entanglement, improving multi-object recognition and segmentation.


<details>
  <summary>Details</summary>
Motivation: Vision-language models struggle with complex scenes due to semantic entanglement in features.

Method: DCLIP uses MFI Loss and Asymmetric Loss to disentangle features.

Result: 30% reduction in inter-class similarity; outperforms SOTA in MLR and ZS3 tasks.

Conclusion: Feature disentanglement is key for multi-object perception in VLMs.

Abstract: Vision-language models like CLIP excel at recognizing the single, prominent
object in a scene. However, they struggle in complex scenes containing multiple
objects. We identify a fundamental reason behind this limitation: VLMs features
space exhibits significant semantic entanglement, where features of one class
contain substantial information about other unrelated classes, a phenomenon we
term mutual feature information (MFI). This entanglement becomes evident during
class-specific queries, as unrelated objects are activated alongside the
queried class. To address this limitation, we propose DCLIP, a framework that
disentangles CLIP features using two complementary objectives: a novel MFI Loss
that orthogonalizes the text (class) features to reduce inter-class similarity,
and the Asymmetric Loss (ASL) that aligns image features with the disentangled
text features. Our experiment demonstrates that DCLIP reduces inter-class
feature similarity by 30\% compared to CLIP, leading to significant performance
gains on multi-label recognition (MLR) and zero-shot semantic segmentation
(ZS3). In MLR, DCLIP outperforms SOTA approaches on VOC2007 and COCO-14 while
using 75\% fewer parameters, and surpasses SOTA ZS3 methods by 3.4 mIoU on
VOC2012 and 2.8 mIoU on COCO-17. These results establish feature
disentanglement as a critical factor for effective multi-object perception in
vision-language models.

</details>


### [219] [In-Model Merging for Enhancing the Robustness of Medical Imaging Classification Models](https://arxiv.org/pdf/2502.20516)
*Hu Wang, Ibrahim Almakky, Congbo Ma, Numan Saeed, Mohammad Yaqub*

Main category: cs.CV

TL;DR: InMerge is a novel method for merging similar kernels within a single CNN during training to enhance robustness, outperforming traditional training in medical image classification.


<details>
  <summary>Details</summary>
Motivation: Limited research explores merging within a single model to improve robustness, especially in medical imaging.

Method: InMerge selectively merges similar convolutional kernels in deep layers of a CNN during training.

Result: InMerge-trained models outperform traditionally-trained models on 4 datasets.

Conclusion: InMerge is feasible and effective, providing insights for kernel merging in CNNs.

Abstract: Model merging is an effective strategy to merge multiple models for enhancing
model performances, and more efficient than ensemble learning as it will not
introduce extra computation into inference. However, limited research explores
if the merging process can occur within one model and enhance the model's
robustness, which is particularly critical in the medical image domain. In the
paper, we are the first to propose in-model merging (InMerge), a novel approach
that enhances the model's robustness by selectively merging similar
convolutional kernels in the deep layers of a single convolutional neural
network (CNN) during the training process for classification. We also
analytically reveal important characteristics that affect how in-model merging
should be performed, serving as an insightful reference for the community. We
demonstrate the feasibility and effectiveness of this technique for different
CNN architectures on 4 prevalent datasets. The proposed InMerge-trained model
surpasses the typically-trained model by a substantial margin. The code will be
made public.

</details>


### [220] [Structured Preference Optimization for Vision-Language Long-Horizon Task Planning](https://arxiv.org/pdf/2502.20742)
*Xiwen Liang, Min Lin, Weiqi Ruan, Rongtao Xu, Yuecheng Liu, Jiaqi Chen, Bingqian Lin, Yuzheng Zhuang, Xiaodan Liang*

Main category: cs.CV

TL;DR: SPO improves long-horizon vision-language task planning via structured preference optimization and curriculum-guided training, outperforming baselines on new benchmark ExtendaBench.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with long-horizon tasks due to poor reasoning quality. SPO addresses this by enhancing reasoning and action selection.

Method: SPO uses preference-based scoring (task relevance, visual grounding, historical consistency) and curriculum-guided training (simple to complex tasks).

Result: SPO achieves significant improvements in reasoning quality and decision accuracy (+5.98% GCR in VirtualHome, +3.30% GCR in Habitat).

Conclusion: SPO's preference-driven optimization is effective for long-horizon vision-language task planning, as shown by superior performance on ExtendaBench.

Abstract: Existing methods for vision-language task planning excel in short-horizon
tasks but often fall short in complex, long-horizon planning within dynamic
environments. These challenges primarily arise from the difficulty of
effectively training models to produce high-quality reasoning processes for
long-horizon tasks. To address this, we propose Structured Preference
Optimization (SPO), which aims to enhance reasoning and action selection in
long-horizon task planning through structured preference evaluation and
optimized training strategies. Specifically, SPO introduces: 1)
Preference-Based Scoring and Optimization, which systematically evaluates
reasoning chains based on task relevance, visual grounding, and historical
consistency; and 2) Curriculum-Guided Training, where the model progressively
adapts from simple to complex tasks, improving its generalization ability in
long-horizon scenarios and enhancing reasoning robustness. To advance research
in vision-language long-horizon task planning, we introduce ExtendaBench, a
comprehensive benchmark covering 1,509 tasks across VirtualHome and Habitat
2.0, categorized into ultra-short, short, medium, and long tasks. Experimental
results demonstrate that SPO significantly improves reasoning quality and final
decision accuracy, outperforming prior methods on long-horizon tasks and
underscoring the effectiveness of preference-driven optimization in
vision-language task planning. Specifically, SPO achieves a +5.98% GCR and
+4.68% SR improvement in VirtualHome and a +3.30% GCR and +2.11% SR improvement
in Habitat over the best-performing baselines.

</details>


### [221] [Adapt3R: Adaptive 3D Scene Representation for Domain Transfer in Imitation Learning](https://arxiv.org/pdf/2503.04877)
*Albert Wilcox, Mohamed Ghanem, Masoud Moghani, Pierre Barroso, Benjamin Joffe, Animesh Garg*

Main category: cs.CV

TL;DR: Adapt3R improves imitation learning by using a 3D observation encoder to handle unseen embodiments and camera viewpoints, leveraging 2D semantic features localized in 3D.


<details>
  <summary>Details</summary>
Motivation: Current 3D scene representations in imitation learning show limited improvement for unseen scenarios, necessitating a more robust solution.

Method: Adapt3R combines a pretrained 2D backbone for semantic extraction with 3D localization relative to the end-effector, integrating with various IL algorithms.

Result: Adapt3R enables zero-shot transfer to novel embodiments and camera poses across 93 simulated and 6 real tasks.

Conclusion: Adapt3R enhances generalization in imitation learning without compromising the performance of existing algorithms.

Abstract: Imitation Learning can train robots to perform complex and diverse
manipulation tasks, but learned policies are brittle with observations outside
of the training distribution. 3D scene representations that incorporate
observations from calibrated RGBD cameras have been proposed as a way to
mitigate this, but in our evaluations with unseen embodiments and camera
viewpoints they show only modest improvement. To address those challenges, we
propose Adapt3R, a general-purpose 3D observation encoder which synthesizes
data from calibrated RGBD cameras into a vector that can be used as
conditioning for arbitrary IL algorithms. The key idea is to use a pretrained
2D backbone to extract semantic information, using 3D only as a medium to
localize this information with respect to the end-effector. We show across 93
simulated and 6 real tasks that when trained end-to-end with a variety of IL
algorithms, Adapt3R maintains these algorithms' learning capacity while
enabling zero-shot transfer to novel embodiments and camera poses.

</details>


### [222] [Just Functioning as a Hook for Two-Stage Referring Multi-Object Tracking](https://arxiv.org/pdf/2503.07516)
*Weize Li, Yunhao Du, Qixiang Yin, Zhicheng Zhao, Fei Su, Daqi Liu*

Main category: cs.CV

TL;DR: JustHook is a novel two-stage RMOT framework that improves efficiency and scalability by leveraging pre-trained visual backbones and active contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Existing RMOT methods face computational overhead or overlook contextual aggregation in pre-trained models, limiting efficiency and compatibility.

Method: JustHook introduces Visual Feature Hook (VFH) for direct feature extraction and Parallel Combined Decoder (PCD) for active contrastive learning.

Result: JustHook achieves a 7.77% HOTA improvement on Refer-KITTI-V2, outperforming state-of-the-art methods.

Conclusion: JustHook effectively addresses limitations of existing RMOT methods, offering better performance and scalability.

Abstract: Referring Multi-Object Tracking (RMOT) aims to localize target trajectories
specified by natural language expressions in videos. Existing RMOT methods
mainly follow two paradigms: one-stage strategies and two-stage ones. The
former jointly trains tracking with referring but suffers from substantial
computational overhead. Although the latter improves efficiency, it overlooks
the inherent contextual aggregation capabilities of pre-trained visual
backbones and takes a detour. Meanwhile, its fixed dual-tower architecture
restricts compatibility with other visual / text backbones. To address these
limitations, we propose JustHook, a novel hook-like framework for two-stage
RMOT, which introduces two core components: (1) a Visual Feature Hook (VFH),
enabling JustHook to extract context-rich local features directly from the
original visual backbone like a hook; (2) a Parallel Combined Decoder (PCD),
which transforms the passive cosine similarity measurement between independent
modalities into active contrastive learning within the combined feature space.
The proposed JustHook not only leverages the capabilities of pre-trained models
but also breaks free from the constraints of inherent modality alignment,
achieving strong scalability. Extensive experiments on Refer-KITTI and
Refer-KITTI-V2 demonstrate that JustHook outperforms state-of-the-art methods
across diverse encoder combinations, achieving a notable 7.77\% HOTA
improvement on Refer-KITTI-V2. Code will be made available soon.

</details>


### [223] [Normalized Matching Transformer](https://arxiv.org/pdf/2503.17715)
*Abtin Pourhadi, Paul Swoboda*

Main category: cs.CV

TL;DR: A deep learning-based method for sparse keypoint matching, combining a visual backbone, SplineCNN, and a normalized transformer decoder, outperforms state-of-the-art approaches with fewer training epochs.


<details>
  <summary>Details</summary>
Motivation: To improve sparse keypoint matching accuracy and efficiency by leveraging deep learning and advanced loss functions.

Method: Combines a visual backbone, SplineCNN graph neural network, normalized transformer decoder, and Sinkhorn algorithm, trained with contrastive and hyperspherical losses and data augmentation.

Result: Outperforms current methods by 5.1% and 2.2% on PascalVOC and SPair-71k datasets, with 1.7x fewer training epochs.

Conclusion: The proposed architecture achieves superior performance with simpler training, demonstrating the effectiveness of advanced normalization and loss functions.

Abstract: We present a new state of the art approach for sparse keypoint matching
between pairs of images. Our method consists of a fully deep learning based
approach combining a visual backbone coupled with a SplineCNN graph neural
network for feature processing and a normalized transformer decoder for
decoding keypoint correspondences together with the Sinkhorn algorithm. Our
method is trained using a contrastive and a hyperspherical loss for better
feature representations. We additionally use data augmentation during training.
This comparatively simple architecture combining extensive normalization and
advanced losses outperforms current state of the art approaches on PascalVOC
and SPair-71k datasets by $5.1\%$ and $2.2\%$ respectively compared to BBGM,
ASAR, COMMON and GMTR while training for at least $1.7x$ fewer epochs.

</details>


### [224] [CoMP: Continual Multimodal Pre-training for Vision Foundation Models](https://arxiv.org/pdf/2503.18931)
*Yitong Chen, Lingchen Meng, Wujian Peng, Zuxuan Wu, Yu-Gang Jiang*

Main category: cs.CV

TL;DR: CoMP continually pre-trains Vision Foundation Models (VFMs) to align visual and language representations, improving performance in multimodal and generic tasks.


<details>
  <summary>Details</summary>
Motivation: Align visual representations with language for better cross-modal understanding, regardless of the original pre-training process.

Method: Introduces CoMP, a pipeline with Continual Rotary Position Embedding for varying resolutions and Alignment Loss for cross-modal alignment.

Result: Improved performance in multimodal tasks (e.g., ChartQA), generic classification (ImageNet-1K), and segmentation (ADE20K).

Conclusion: CoMP enhances VFMs' versatility and alignment with language, achieving strong results across diverse tasks.

Abstract: Pre-trained Vision Foundation Models (VFMs) provide strong visual
representations for a wide range of applications. In this paper, we continually
pre-train prevailing VFMs in a multimodal manner such that they can
effortlessly process visual inputs of varying sizes and produce visual
representations that are more aligned with language representations, regardless
of their original pre-training process. To this end, we introduce CoMP, a
carefully designed multimodal pre-training pipeline. CoMP uses a Continual
Rotary Position Embedding to accommodate visual inputs with different
resolutions, and an Alignment Loss between visual and textual features for
better cross-modal alignment. After continual pre-training, leading VFMs like
DINOv2, SigLIP and AIMv2 achieve remarkable improvements not only in multimodal
understanding tasks but also in generic classification and segmentation tasks.
Remarkably, CoMP-AIMv2 achieves scores of 64.9 on ChartQA with a 0.5B LLM,
while maintaining an 87.3% accuracy on ImageNet-1K and a 51.8 mIoU on ADE20K
under frozen chunk evaluation.

</details>


### [225] [A Plasticity-Aware Method for Continual Self-Supervised Learning in Remote Sensing](https://arxiv.org/pdf/2503.24088)
*Lars M√∂llenbrok, Behnood Rasti, Beg√ºm Demir*

Main category: cs.CV

TL;DR: A novel CSSL method for remote sensing improves learning plasticity by decoupling task-common and task-specific features, outperforming CaSSLe in accuracy and intransigence.


<details>
  <summary>Details</summary>
Motivation: Existing CSSL methods prioritize preventing catastrophic forgetting, which reduces learning plasticity. This paper aims to enhance plasticity while retaining stability.

Method: Proposes a knowledge distillation strategy with a decoupling mechanism, dividing features into task-common (correlated) and task-specific (de-correlated) parts.

Result: Outperforms CaSSLe with improvements of up to 1.12-1.24% in average accuracy and 2.01-2.33% in intransigence across scenarios.

Conclusion: The method effectively balances stability and plasticity, advancing CSSL for remote sensing applications.

Abstract: Continual self-supervised learning (CSSL) methods have gained increasing
attention in remote sensing (RS) due to their capability to learn new tasks
sequentially from continuous streams of unlabeled data.
  Existing CSSL methods, while learning new tasks, focus on preventing
catastrophic forgetting. To this end, most of them use regularization
strategies to retain knowledge of previous tasks. This reduces the model's
ability to adapt to the data of new tasks (i.e., learning plasticity), which
can degrade performance. To address this problem, in this paper, we propose a
novel CSSL method that aims to learn tasks sequentially, while achieving high
learning plasticity. To this end, the proposed method uses a knowledge
distillation strategy with an integrated decoupling mechanism. The decoupling
is achieved by first dividing the feature dimensions into task-common and
task-specific parts. Then, the task-common features are forced to be correlated
to ensure memory stability while the task-specific features are forced to be
de-correlated facilitating the learning of new features. Experimental results
show the effectiveness of the proposed method compared to CaSSLe, which is a
widely used CSSL framework, with improvements of up to 1.12% in average
accuracy and 2.33% in intransigence in a task-incremental scenario, and 1.24%
in average accuracy and 2.01% in intransigence in a class-incremental scenario.

</details>


### [226] [IMPACT: A Generic Semantic Loss for Multimodal Medical Image Registration](https://arxiv.org/pdf/2503.24121)
*Valentin Boussot, C√©dric H√©mon, Jean-Claude Nunes, Jason Dowling, Simon Rouz√©, Caroline Lafond, Ana√Øs Barateau, Jean-Louis Dillenseger*

Main category: cs.CV

TL;DR: IMPACT is a novel similarity metric for multimodal image registration, using deep features from pretrained segmentation models for robust alignment without task-specific training.


<details>
  <summary>Details</summary>
Motivation: Image registration is crucial in medical imaging for precise alignment, but existing methods often rely on raw intensities or handcrafted features, limiting robustness across modalities.

Method: IMPACT leverages deep features from pretrained segmentation models (e.g., TotalSegmentator, SAM) to define semantic similarity, integrating into algorithmic (Elastix) and learning-based (VoxelMorph) frameworks.

Result: Evaluated on 3D registration tasks (CT/CBCT, MR/CT), IMPACT improved anatomical alignment (Target Registration Error, Dice Similarity Coefficient) and handled noise/artifacts better than baselines.

Conclusion: IMPACT is a versatile, efficient, and powerful solution for multimodal image registration, suitable for clinical and research applications.

Abstract: Image registration is fundamental in medical imaging, enabling precise
alignment of anatomical structures for diagnosis, treatment planning,
image-guided interventions, and longitudinal monitoring. This work introduces
IMPACT (Image Metric with Pretrained model-Agnostic Comparison for
Transmodality registration), a novel similarity metric designed for robust
multimodal image registration. Rather than relying on raw intensities,
handcrafted descriptors, or task-specific training, IMPACT defines a semantic
similarity measure based on the comparison of deep features extracted from
large-scale pretrained segmentation models. By leveraging representations from
models such as TotalSegmentator, Segment Anything (SAM), and other foundation
networks, IMPACT provides a task-agnostic, training-free solution that
generalizes across imaging modalities. These features, originally trained for
segmentation, offer strong spatial correspondence and semantic alignment
capabilities, making them naturally suited for registration. The method
integrates seamlessly into both algorithmic (Elastix) and learning-based
(VoxelMorph) frameworks, leveraging the strengths of each. IMPACT was evaluated
on five challenging 3D registration tasks involving thoracic CT/CBCT and pelvic
MR/CT datasets. Quantitative metrics, including Target Registration Error and
Dice Similarity Coefficient, demonstrated consistent improvements in anatomical
alignment over baseline methods. Qualitative analyses further highlighted the
robustness of the proposed metric in the presence of noise, artifacts, and
modality variations. With its versatility, efficiency, and strong performance
across diverse tasks, IMPACT offers a powerful solution for advancing
multimodal image registration in both clinical and research settings.

</details>


### [227] [SCAM: A Real-World Typographic Robustness Evaluation for Multimodal Foundation Models](https://arxiv.org/pdf/2504.04893)
*Justus Westerhoff, Erblina Purelku, Jakob Hackstein, Jonas Loos, Leo Pinetzki, Lorenz Hufe*

Main category: cs.CV

TL;DR: The paper introduces SCAM, a large dataset for studying typographic attacks on Vision-Language Models (VLMs), showing their impact and factors influencing vulnerability.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for typographic attacks are limited, hindering research on vulnerabilities in multimodal AI systems.

Method: The authors create SCAM, a diverse dataset of 1,162 typographic attack images, and benchmark VLMs to analyze performance degradation and susceptibility factors.

Result: Typographic attacks degrade VLM performance, with susceptibility influenced by training data and model architecture. Larger LLMs help mitigate vulnerability.

Conclusion: The study provides insights and resources for robust multimodal AI, releasing SCAM and evaluation code publicly.

Abstract: Typographic attacks exploit the interplay between text and visual content in
multimodal foundation models, causing misclassifications when misleading text
is embedded within images. However, existing datasets are limited in size and
diversity, making it difficult to study such vulnerabilities. In this paper, we
introduce SCAM, the largest and most diverse dataset of real-world typographic
attack images to date, containing 1,162 images across hundreds of object
categories and attack words. Through extensive benchmarking of Vision-Language
Models (VLMs) on SCAM, we demonstrate that typographic attacks significantly
degrade performance, and identify that training data and model architecture
influence the susceptibility to these attacks. Our findings reveal that
typographic attacks persist in state-of-the-art Large Vision-Language Models
(LVLMs) due to the choice of their vision encoder, though larger Large Language
Models (LLMs) backbones help mitigate their vulnerability. Additionally, we
demonstrate that synthetic attacks closely resemble real-world (handwritten)
attacks, validating their use in research. Our work provides a comprehensive
resource and empirical insights to facilitate future research toward robust and
trustworthy multimodal AI systems. We publicly release the datasets introduced
in this paper along with the code for evaluations at
www.bliss.berlin/research/scam.

</details>


### [228] [V-MAGE: A Game Evaluation Framework for Assessing Vision-Centric Capabilities in Multimodal Large Language Models](https://arxiv.org/pdf/2504.06148)
*Xiangxi Zheng, Linjie Li, Zhengyuan Yang, Ping Yu, Alex Jinpeng Wang, Rui Yan, Yuan Yao, Lijuan Wang*

Main category: cs.CV

TL;DR: V-MAGE is a game-based framework to evaluate Multimodal Large Language Models (MLLMs) in dynamic, interactive environments, revealing gaps in advanced reasoning and real-time interaction capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for MLLMs lack evaluation of dynamic perception and interactive reasoning, necessitating a more robust framework like V-MAGE.

Method: V-MAGE uses five video games with 30+ scenarios, a dynamic Elo-based ranking system, and benchmarks MLLMs against human performance.

Result: Leading MLLMs perform near-human in simple tasks but lag in complex scenarios, exposing limitations in real-time vision-grounded reasoning.

Conclusion: V-MAGE effectively identifies MLLM limitations and offers insights for improving dynamic, interactive visual reasoning capabilities.

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have
demonstrated impressive capabilities in visual-text processing. However,
existing static image-text benchmarks are insufficient for evaluating their
dynamic perception and interactive reasoning abilities. We introduce
Vision-centric Multiple Abilities Game Evaluation(V-MAGE), a novel game-based
evaluation framework designed to systematically assess MLLMs' visual reasoning
in interactive, continuous-space environments. V-MAGE features five distinct
video games comprising over 30 carefully constructed evaluation scenarios.
These scenarios are set in free-form, visually complex environments that
require models to interpret dynamic game states and make decisions based solely
on visual input, thereby closely reflecting the conditions encountered by human
players. To ensure robust and interpretable comparisons across models, V-MAGE
employs a dynamic Elo-based ranking system that accounts for varying difficulty
levels and task diversity. Benchmarking state-of-the-art MLLMs against human
baselines reveals that while leading models approach human-level performance in
simple tasks, their performance drops significantly in complex scenarios
requiring advanced reasoning and task orchestration. This persistent
performance gap highlights fundamental limitations in current MLLMs' ability to
perform real-time, vision-grounded interactions. Through extensive analyses, we
demonstrate the utility of V-MAGE in uncovering these limitations and providing
actionable insights for improving the visual and reasoning capabilities of
MLLMs in dynamic, interactive settings. Code is publicly available at
https://github.com/CSU-JPG/V-MAGE.

</details>


### [229] [HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned Guidance](https://arxiv.org/pdf/2504.06232)
*Jiazi Bu, Pengyang Ling, Yujie Zhou, Pan Zhang, Tong Wu, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Dahua Lin, Jiaqi Wang*

Main category: cs.CV

TL;DR: HiFlow is a training-free framework for high-resolution image synthesis using pre-trained flow models, addressing challenges like artifacts and low fidelity by leveraging intermediate states and flow-aligned guidance.


<details>
  <summary>Details</summary>
Motivation: High-resolution image synthesis is challenging due to data scarcity and complexity, with existing training-free methods often producing low-quality results.

Method: HiFlow introduces a virtual reference flow in high-resolution space, aligning initialization, direction, and acceleration to guide synthesis.

Result: HiFlow significantly improves high-resolution image quality and works across personalized T2I model variants.

Conclusion: HiFlow outperforms state-of-the-art methods in high-resolution image synthesis, validated by extensive experiments.

Abstract: Text-to-image (T2I) diffusion/flow models have drawn considerable attention
recently due to their remarkable ability to deliver flexible visual creations.
Still, high-resolution image synthesis presents formidable challenges due to
the scarcity and complexity of high-resolution content. Recent approaches have
investigated training-free strategies to enable high-resolution image synthesis
with pre-trained models. However, these techniques often struggle with
generating high-quality visuals and tend to exhibit artifacts or low-fidelity
details, as they typically rely solely on the endpoint of the low-resolution
sampling trajectory while neglecting intermediate states that are critical for
preserving structure and synthesizing finer detail. To this end, we present
HiFlow, a training-free and model-agnostic framework to unlock the resolution
potential of pre-trained flow models. Specifically, HiFlow establishes a
virtual reference flow within the high-resolution space that effectively
captures the characteristics of low-resolution flow information, offering
guidance for high-resolution generation through three key aspects:
initialization alignment for low-frequency consistency, direction alignment for
structure preservation, and acceleration alignment for detail fidelity. By
leveraging such flow-aligned guidance, HiFlow substantially elevates the
quality of high-resolution image synthesis of T2I models and demonstrates
versatility across their personalized variants. Extensive experiments validate
HiFlow's capability in achieving superior high-resolution image quality over
state-of-the-art methods.

</details>


### [230] [Towards Low-Latency Event-based Obstacle Avoidance on a FPGA-Drone](https://arxiv.org/pdf/2504.10400)
*Pietro Bonazzi, Christian Vogt, Michael Jost, Lyes Khacef, Federico Paredes-Vall√©s, Michele Magno*

Main category: cs.CV

TL;DR: Event-based vision systems (EVS) outperform RGB-based models in collision avoidance, achieving higher frame rates, lower errors, and better robustness, especially in out-of-distribution scenarios.


<details>
  <summary>Details</summary>
Motivation: To compare the performance of event-based vision systems (EVS) and RGB-based models for real-time collision avoidance, focusing on accuracy, robustness, and latency.

Method: Quantitative evaluation of EVS and RGB models on an FPGA accelerator, measuring frame rates, prediction errors, and evasion maneuver performance.

Result: EVS achieves 1 kHz frame rate, lower errors (-20 ms, -20 mm), and superior robustness (78% precision vs. 19% for RGB). Latency is 2.14 ms.

Conclusion: EVS is highly effective for real-time collision avoidance, offering advantages in speed, accuracy, and robustness, suitable for resource-constrained environments.

Abstract: This work quantitatively evaluates the performance of event-based vision
systems (EVS) against conventional RGB-based models for action prediction in
collision avoidance on an FPGA accelerator. Our experiments demonstrate that
the EVS model achieves a significantly higher effective frame rate (1 kHz) and
lower temporal (-20 ms) and spatial prediction errors (-20 mm) compared to the
RGB-based model, particularly when tested on out-of-distribution data. The EVS
model also exhibits superior robustness in selecting optimal evasion maneuvers.
In particular, in distinguishing between movement and stationary states, it
achieves a 59 percentage point advantage in precision (78% vs. 19%) and a
substantially higher F1 score (0.73 vs. 0.06), highlighting the susceptibility
of the RGB model to overfitting. Further analysis in different combinations of
spatial classes confirms the consistent performance of the EVS model in both
test data sets. Finally, we evaluated the system end-to-end and achieved a
latency of approximately 2.14 ms, with event aggregation (1 ms) and inference
on the processing unit (0.94 ms) accounting for the largest components. These
results underscore the advantages of event-based vision for real-time collision
avoidance and demonstrate its potential for deployment in resource-constrained
environments.

</details>


### [231] [Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene Understanding](https://arxiv.org/pdf/2504.13580)
*Yuchen Rao, Stefan Ainetter, Sinisa Stekovic, Vincent Lepetit, Friedrich Fraundorfer*

Main category: cs.CV

TL;DR: The paper proposes using automatically retrieved synthetic CAD models as high-quality ground truth for training supervised deep learning models in 3D scene understanding, outperforming manual annotations.


<details>
  <summary>Details</summary>
Motivation: High-level 3D scene understanding is crucial but hindered by the difficulty of generating accurate 3D annotations.

Method: A pipeline for automatic annotation of ScanNet++ v1 dataset with 9D poses and CAD models, similar to prior work on ScanNet.

Result: Models trained on automatic annotations outperform those using manual annotations in tasks like point cloud completion and CAD model retrieval/alignment.

Conclusion: Automatic 3D annotations enhance performance and reduce costs, with released annotations (SCANnotate++) and models to aid future research.

Abstract: High-level 3D scene understanding is essential in many applications. However,
the challenges of generating accurate 3D annotations make development of deep
learning models difficult. We turn to recent advancements in automatic
retrieval of synthetic CAD models, and show that data generated by such methods
can be used as high-quality ground truth for training supervised deep learning
models. More exactly, we employ a pipeline akin to the one previously used to
automatically annotate objects in ScanNet scenes with their 9D poses and CAD
models. This time, we apply it to the recent ScanNet++ v1 dataset, which
previously lacked such annotations. Our findings demonstrate that it is not
only possible to train deep learning models on these automatically-obtained
annotations but that the resulting models outperform those trained on manually
annotated data. We validate this on two distinct tasks: point cloud completion
and single-view CAD model retrieval and alignment. Our results underscore the
potential of automatic 3D annotations to enhance model performance while
significantly reducing annotation costs. To support future research in 3D scene
understanding, we will release our annotations, which we call SCANnotate++,
along with our trained models.

</details>


### [232] [Empowering Agentic Video Analytics Systems with Video Language Models](https://arxiv.org/pdf/2505.00254)
*Yuxuan Yan, Shiqi Jiang, Ting Cao, Yifan Yang, Qianqian Yang, Yuanchao Shu, Yuqing Yang, Lili Qiu*

Main category: cs.CV

TL;DR: AVAS is a VLM-powered system for open-ended video analytics, addressing limitations of existing systems with innovations like Event Knowledge Graphs and agentic retrieval-generation, achieving top performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing AI-driven video analytics systems lack adaptability for open-ended scenarios and struggle with ultra-long videos, prompting the development of AVAS.

Method: AVAS uses Event Knowledge Graphs for efficient video indexing and an agentic retrieval-generation mechanism for handling diverse queries.

Result: AVAS achieves 62.3% and 64.1% accuracy on LVBench and VideoMME-Long, and 75.8% on the new AVAS-100 benchmark.

Conclusion: AVAS demonstrates superior performance in open-ended and ultra-long video analytics, setting a new standard for video-language models.

Abstract: AI-driven video analytics has become increasingly pivotal across diverse
domains. However, existing systems are often constrained to specific,
predefined tasks, limiting their adaptability in open-ended analytical
scenarios. The recent emergence of Video-Language Models (VLMs) as
transformative technologies offers significant potential for enabling
open-ended video understanding, reasoning, and analytics. Nevertheless, their
limited context windows present challenges when processing ultra-long video
content, which is prevalent in real-world applications. To address this, we
introduce AVAS, a VLM-powered system designed for open-ended, advanced video
analytics. AVAS incorporates two key innovations: (1) the near real-time
construction of Event Knowledge Graphs (EKGs) for efficient indexing of long or
continuous video streams, and (2) an agentic retrieval-generation mechanism
that leverages EKGs to handle complex and diverse queries. Comprehensive
evaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that
AVAS achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy,
respectively, significantly surpassing existing VLM and video
Retrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video
analytics in ultra-long and open-world video scenarios, we introduce a new
benchmark, AVAS-100. This benchmark comprises 8 videos, each exceeding 10 hours
in duration, along with 120 manually annotated, diverse, and complex
question-answer pairs. On AVAS-100, AVAS achieves top-tier performance with an
accuracy of 75.8%.

</details>


### [233] [DARTer: Dynamic Adaptive Representation Tracker for Nighttime UAV Tracking](https://arxiv.org/pdf/2505.00752)
*Xuzhao Li, Xuchen Li, Shiyu Hu*

Main category: cs.CV

TL;DR: DARTer is an end-to-end tracking framework for nighttime UAV scenarios, using dynamic feature blending and activation to improve robustness and efficiency.


<details>
  <summary>Details</summary>
Motivation: Nighttime UAV tracking faces challenges like illumination variations and viewpoint changes, which degrade performance. Existing methods are computationally expensive or inefficient.

Method: DARTer uses a Dynamic Feature Blender (DFB) to fuse multi-perspective features and a Dynamic Feature Activator (DFA) to adaptively activate Vision Transformer layers, reducing redundancy.

Result: DARTer outperforms state-of-the-art trackers on nighttime UAV benchmarks, balancing accuracy and efficiency.

Conclusion: DARTer is a promising solution for real-world nighttime UAV tracking, offering robust and efficient performance.

Abstract: Nighttime UAV tracking presents significant challenges due to extreme
illumination variations and viewpoint changes, which severely degrade tracking
performance. Existing approaches either rely on light enhancers with high
computational costs or introduce redundant domain adaptation mechanisms,
failing to fully utilize the dynamic features in varying perspectives. To
address these issues, we propose \textbf{DARTer} (\textbf{D}ynamic
\textbf{A}daptive \textbf{R}epresentation \textbf{T}racker), an end-to-end
tracking framework designed for nighttime UAV scenarios. DARTer leverages a
Dynamic Feature Blender (DFB) to effectively fuse multi-perspective nighttime
features from static and dynamic templates, enhancing representation
robustness. Meanwhile, a Dynamic Feature Activator (DFA) adaptively activates
Vision Transformer layers based on extracted features, significantly improving
efficiency by reducing redundant computations. Our model eliminates the need
for complex multi-task loss functions, enabling a streamlined training process.
Extensive experiments on multiple nighttime UAV tracking benchmarks demonstrate
the superiority of DARTer over state-of-the-art trackers. These results confirm
that DARTer effectively balances tracking accuracy and efficiency, making it a
promising solution for real-world nighttime UAV tracking applications.

</details>


### [234] [VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on Synthetic Video Understanding](https://arxiv.org/pdf/2505.01481)
*Zongxia Li, Xiyang Wu, Guangyao Shi, Yubin Qin, Hongyang Du, Tianyi Zhou, Dinesh Manocha, Jordan Lee Boyd-Graber*

Main category: cs.CV

TL;DR: VideoHallu is a benchmark for evaluating MLLMs' ability to detect abnormalities in synthetic videos, showing current models struggle with commonsense and physics reasoning. Post-training with GRPO improves performance.


<details>
  <summary>Details</summary>
Motivation: Synthetic video generation often violates commonsense and physics, necessitating robust abnormality detectors for MLLMs.

Method: Introduces VideoHallu, a benchmark with 3,000+ QA pairs from synthetic videos, testing MLLMs on alignment, consistency, commonsense, and physics. Benchmarks SOTA models like GPT-4o and Gemini-2.5-Pro.

Result: Current MLLMs perform well on real-world benchmarks but struggle with synthetic video abnormalities. GRPO post-training improves detection.

Conclusion: Targeted training with GRPO enhances MLLMs' understanding of commonsense and physics in synthetic videos, addressing critical gaps.

Abstract: Synthetic video generation has gained significant attention for its realism
and broad applications, but remains prone to violations of common sense and
physical laws. This highlights the need for reliable abnormality detectors that
understand such principles and are robust to hallucinations. To address this,
we introduce VideoHallu, a benchmark of over 3,000 video QA pairs built from
synthetic videos generated by models like Veo2, Sora, and Kling, paired with
expert-crafted counterintuitive QA to evaluate the critical thinking abilities
of Multi-modal Large Language Models (MLLMs) on abnormalities that are
perceptually obvious to humans but often hallucinated due to language priors.
VideoHallu evaluates MLLMs' abnormality detection abilities with examples
across alignment, consistency, commonsense, and physics. We benchmark SOTA
MLLMs, including GPT-4o, Gemini-2.5-Pro, Qwen2.5-VL, Video-R1, and
VideoChat-R1. We observe that these models perform well on many real-world
benchmarks like MVBench and MovieChat, but still struggle with basic
physics-based and commonsense reasoning in synthetic videos. We further show
that post-training with Group Relative Policy Optimization (GRPO), using
curriculum learning on datasets combining video QA with counterintuitive
commonsense and physics reasoning over real and synthetic videos, improves
MLLMs' abnormality detection and critical thinking, demonstrating the value of
targeted training for improving their understanding of commonsense and physical
laws.

</details>


### [235] [An Enhanced YOLOv8 Model for Real-Time and Accurate Pothole Detection and Measurement](https://arxiv.org/pdf/2505.04207)
*Mustafa Yurdakul, ≈ûakir Tasdemir*

Main category: cs.CV

TL;DR: A lightweight YOLOv8-based model with structural improvements (DSConv, SimAM, GELU) is proposed for pothole detection and physical feature analysis using RGB-D images, achieving higher accuracy than standard YOLOv8n-seg.


<details>
  <summary>Details</summary>
Motivation: Potholes cause safety and economic issues, but existing 2D RGB-based methods lack accurate physical feature analysis.

Method: Created a PothRGBD dataset with 1000 RGB-D images using Intel RealSense D415. Improved YOLOv8n-seg with DSConv, SimAM, and GELU for better segmentation and depth/perimeter measurement.

Result: Proposed model improved precision (93.7%), recall (90.4%), and mAP@50 (93.8%) over standard YOLOv8n-seg (91.9%, 85.2%, 91.9%).

Conclusion: The model is lightweight, accurate, and suitable for real-time intelligent transportation solutions.

Abstract: Potholes cause vehicle damage and traffic accidents, creating serious safety
and economic problems. Therefore, early and accurate detection of potholes is
crucial. Existing detection methods are usually only based on 2D RGB images and
cannot accurately analyze the physical characteristics of potholes. In this
paper, a publicly available dataset of RGB-D images (PothRGBD) is created and
an improved YOLOv8-based model is proposed for both pothole detection and
pothole physical features analysis. The Intel RealSense D415 depth camera was
used to collect RGB and depth data from the road surfaces, resulting in a
PothRGBD dataset of 1000 images. The data was labeled in YOLO format suitable
for segmentation. A novel YOLO model is proposed based on the YOLOv8n-seg
architecture, which is structurally improved with Dynamic Snake Convolution
(DSConv), Simple Attention Module (SimAM) and Gaussian Error Linear Unit
(GELU). The proposed model segmented potholes with irregular edge structure
more accurately, and performed perimeter and depth measurements on depth maps
with high accuracy. The standard YOLOv8n-seg model achieved 91.9% precision,
85.2% recall and 91.9% mAP@50. With the proposed model, the values increased to
93.7%, 90.4% and 93.8% respectively. Thus, an improvement of 1.96% in
precision, 6.13% in recall and 2.07% in mAP was achieved. The proposed model
performs pothole detection as well as perimeter and depth measurement with high
accuracy and is suitable for real-time applications due to its low model
complexity. In this way, a lightweight and effective model that can be used in
deep learning-based intelligent transportation solutions has been acquired.

</details>


### [236] [RefRef: A Synthetic Dataset and Benchmark for Reconstructing Refractive and Reflective Objects](https://arxiv.org/pdf/2505.05848)
*Yue Yin, Enze Tao, Weijian Deng, Dylan Campbell*

Main category: cs.CV

TL;DR: A new dataset (RefRef) and benchmark for 3D reconstruction of refractive/reflective objects is introduced, along with an oracle method for accurate light path calculation. Current methods lag behind the oracle, showing the challenge.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with refractive/reflective materials due to assumptions of straight light paths and lack of specialized datasets.

Method: Introduces a synthetic dataset (RefRef) with 150 scenes and proposes an oracle method for accurate light path calculation, plus an alternative approach.

Result: Benchmarking shows current methods significantly underperform compared to the oracle, emphasizing the difficulty of the task.

Conclusion: The RefRef dataset and oracle method highlight challenges in handling refractive/reflective materials, urging further research.

Abstract: Modern 3D reconstruction and novel view synthesis approaches have
demonstrated strong performance on scenes with opaque Lambertian objects.
However, most assume straight light paths and therefore cannot properly handle
refractive and reflective materials. Moreover, datasets specialized for these
effects are limited, stymieing efforts to evaluate performance and develop
suitable techniques. In this work, we introduce a synthetic RefRef dataset and
benchmark for reconstructing scenes with refractive and reflective objects from
posed images. Our dataset has 50 such objects of varying complexity, from
single-material convex shapes to multi-material non-convex shapes, each placed
in three different background types, resulting in 150 scenes. We also propose
an oracle method that, given the object geometry and refractive indices,
calculates accurate light paths for neural rendering, and an approach based on
this that avoids these assumptions. We benchmark these against several
state-of-the-art methods and show that all methods lag significantly behind the
oracle, highlighting the challenges of the task and dataset.

</details>


### [237] [From Pixels to Perception: Interpretable Predictions via Instance-wise Grouped Feature Selection](https://arxiv.org/pdf/2505.06003)
*Moritz Vandenhirtz, Julia E. Vogt*

Main category: cs.CV

TL;DR: Proposes an interpretable ML method using instance-wise sparsification of input images, aligning with human perception via semantic pixel regions and dynamic sparsity levels. Outperforms benchmarks in human-understandable predictions.


<details>
  <summary>Details</summary>
Motivation: To provide insights into ML decision-making by making predictions inherently interpretable, addressing model failures and aligning explanations with human perception.

Method: Instance-wise sparsification of input images, learning masks in semantically meaningful pixel regions, and dynamically determining sparsity levels per instance.

Result: Empirical results on semi-synthetic and natural image datasets show more meaningful, human-understandable predictions than state-of-the-art benchmarks.

Conclusion: The method enhances interpretability in ML models by aligning explanations with human perception and dynamically adjusting sparsity, outperforming existing benchmarks.

Abstract: Understanding the decision-making process of machine learning models provides
valuable insights into the task, the data, and the reasons behind a model's
failures. In this work, we propose a method that performs inherently
interpretable predictions through the instance-wise sparsification of input
images. To align the sparsification with human perception, we learn the masking
in the space of semantically meaningful pixel regions rather than on
pixel-level. Additionally, we introduce an explicit way to dynamically
determine the required level of sparsity for each instance. We show empirically
on semi-synthetic and natural image datasets that our inherently interpretable
classifier produces more meaningful, human-understandable predictions than
state-of-the-art benchmarks.

</details>


### [238] [VIN-NBV: A View Introspection Network for Next-Best-View Selection for Resource-Efficient 3D Reconstruction](https://arxiv.org/pdf/2505.06219)
*Noah Frahm, Dongxu Zhao, Andrea Dunn Beltran, Ron Alterovitz, Jan-Michael Frahm, Junier Oliva, Roni Sengupta*

Main category: cs.CV

TL;DR: The paper introduces the View Introspection Network (VIN) and VIN-NBV policy to improve 3D reconstruction by predicting view quality improvements, outperforming coverage-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing NBV algorithms focus on coverage maximization, which doesn't always enhance reconstruction quality for complex scenes. The paper aims to directly predict and optimize reconstruction improvement.

Method: VIN predicts reconstruction quality improvement scores for views. VIN-NBV uses a greedy policy to select views with the highest predicted improvement. VIN is trained via imitation learning.

Result: VIN-NBV improves reconstruction quality by ~30% compared to coverage maximization baselines under acquisition or time constraints.

Conclusion: The VIN-NBV approach effectively enhances 3D reconstruction by focusing on quality improvement rather than coverage, proving its superiority in constrained scenarios.

Abstract: Next Best View (NBV) algorithms aim to acquire an optimal set of images using
minimal resources, time, or number of captures to enable efficient 3D
reconstruction of a scene. Existing approaches often rely on prior scene
knowledge or additional image captures and often develop policies that maximize
coverage. Yet, for many real scenes with complex geometry and self-occlusions,
coverage maximization does not lead to better reconstruction quality directly.
In this paper, we propose the View Introspection Network (VIN), which is
trained to predict the reconstruction quality improvement of views directly,
and the VIN-NBV policy. A greedy sequential sampling-based policy, where at
each acquisition step, we sample multiple query views and choose the one with
the highest VIN predicted improvement score. We design the VIN to perform
3D-aware featurization of the reconstruction built from prior acquisitions, and
for each query view create a feature that can be decoded into an improvement
score. We then train the VIN using imitation learning to predict the
reconstruction improvement score. We show that VIN-NBV improves reconstruction
quality by ~30% over a coverage maximization baseline when operating with
constraints on the number of acquisitions or the time in motion.

</details>


### [239] [Two-Stage Random Alternation Framework for One-Shot Pansharpening](https://arxiv.org/pdf/2505.06576)
*Haorui Chen, Zeyu Ren, Jiaxuan Ren, Ran Ran, Jinliang Shao, Jie Huang, Liangjian Deng*

Main category: cs.CV

TL;DR: TRA-PAN introduces a two-stage framework for pansharpening, combining instance-specific optimization with strong supervision and physical constraints, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Conventional deep learning models for pansharpening struggle with generalization to unseen real-world image pairs, limiting practical utility.

Method: TRA-PAN uses a two-stage approach: pre-training with Degradation-Aware Modeling and warm-up, followed by Random Alternation Optimization for per-instance refinement.

Result: TRA-PAN achieves superior fusion quality in real-world scenarios, outperforming existing methods in metrics and visual quality.

Conclusion: TRA-PAN enhances practical applicability and robustness in pansharpening, addressing generalization issues of traditional models.

Abstract: Deep learning has substantially advanced pansharpening, achieving impressive
fusion quality. However, a prevalent limitation is that conventional deep
learning models, which typically rely on training datasets, often exhibit
suboptimal generalization to unseen real-world image pairs. This restricts
their practical utility when faced with real-world scenarios not included in
the training datasets. To overcome this, we introduce a two-stage random
alternating framework (TRA-PAN) that performs instance-specific optimization
for any given Multispectral(MS)/Panchromatic(PAN) pair, ensuring robust and
high-quality fusion. TRA-PAN effectively integrates strong supervision
constraints from reduced-resolution images with the physical characteristics of
the full-resolution images. The first stage introduces a pre-training
procedure, which includes Degradation-Aware Modeling (DAM) to capture spectral
degradation mappings, alongside a warm-up procedure designed to reduce training
time and mitigate the adverse effects of reduced-resolution data. The second
stage employs Random Alternation Optimization (RAO), randomly alternating
between reduced- and full-resolution images to refine the fusion model
progressively. This adaptive, per-instance optimization strategy, operating in
a one-shot manner for each MS/PAN pair, yields superior high-resolution
multispectral images. Experimental results demonstrate that TRA-PAN outperforms
state-of-the-art (SOTA) methods in quantitative metrics and visual quality in
real-world scenarios, underscoring its enhanced practical applicability and
robustness.

</details>


### [240] [Visual Watermarking in the Era of Diffusion Models: Advances and Challenges](https://arxiv.org/pdf/2505.08197)
*Junxian Duan, Jiyang Guan, Wenkui Yang, Ran He*

Main category: cs.CV

TL;DR: The paper discusses using diffusion models to enhance watermarking techniques for protecting digital content against misuse in generative AI.


<details>
  <summary>Details</summary>
Motivation: Concerns about copyright infringement and misuse of visual content due to advancements in generative AI like Stable Diffusion.

Method: Analyzes the integration of diffusion models for embedding imperceptible and robust watermarks, improving detection accuracy.

Result: Highlights the strengths and challenges of watermark techniques in diffusion models, focusing on robustness and application.

Conclusion: Emphasizes the need for innovative solutions to protect digital content and ownership rights in the generative AI era.

Abstract: As generative artificial intelligence technologies like Stable Diffusion
advance, visual content becomes more vulnerable to misuse, raising concerns
about copyright infringement. Visual watermarks serve as effective protection
mechanisms, asserting ownership and deterring unauthorized use. Traditional
deepfake detection methods often rely on passive techniques that struggle with
sophisticated manipulations. In contrast, diffusion models enhance detection
accuracy by allowing for the effective learning of features, enabling the
embedding of imperceptible and robust watermarks. We analyze the strengths and
challenges of watermark techniques related to diffusion models, focusing on
their robustness and application in watermark generation. By exploring the
integration of advanced diffusion models and watermarking security, we aim to
advance the discourse on preserving watermark robustness against evolving
forgery threats. It emphasizes the critical importance of developing innovative
solutions to protect digital content and ensure the preservation of ownership
rights in the era of generative AI.

</details>


### [241] [Descriptive Image-Text Matching with Graded Contextual Similarity](https://arxiv.org/pdf/2505.09997)
*Jinhyun Jang, Jiyoung Lee, Kwanghoon Sohn*

Main category: cs.CV

TL;DR: The paper introduces Descriptive Image-Text Matching (DITM) to address limitations of sparse binary supervision in image-text matching by learning graded contextual similarity and leveraging descriptive flexibility of language.


<details>
  <summary>Details</summary>
Motivation: Existing approaches use sparse binary supervision, missing many-to-many image-text relationships and implicit connections from general to specific descriptions.

Method: DITM uses cumulative TF-IDF to score sentence descriptiveness, refining false negatives and aligning sentences in a generic-to-specific order.

Result: DITM outperforms state-of-the-art methods on MS-COCO, Flickr30K, and CxC datasets, improving hierarchical reasoning on HierarCaps.

Conclusion: DITM advances image-text matching by capturing complex relationships and hierarchical reasoning, moving beyond rigid binary supervision.

Abstract: Image-text matching aims to build correspondences between visual and textual
data by learning their pairwise similarities. Most existing approaches have
adopted sparse binary supervision, indicating whether a pair of images and
sentences matches or not. However, such sparse supervision covers a limited
subset of image-text relationships, neglecting their inherent many-to-many
correspondences; an image can be described in numerous texts at different
descriptive levels. Moreover, existing approaches overlook the implicit
connections from general to specific descriptions, which form the underlying
rationale for the many-to-many relationships between vision and language. In
this work, we propose descriptive image-text matching, called DITM, to learn
the graded contextual similarity between image and text by exploring the
descriptive flexibility of language. We formulate the descriptiveness score of
each sentence with cumulative term frequency-inverse document frequency
(TF-IDF) to balance the pairwise similarity according to the keywords in the
sentence. Our method leverages sentence descriptiveness to learn robust
image-text matching in two key ways: (1) to refine the false negative labeling,
dynamically relaxing the connectivity between positive and negative pairs, and
(2) to build more precise matching, aligning a set of relevant sentences in a
generic-to-specific order. By moving beyond rigid binary supervision, DITM
enhances the discovery of both optimal matches and potential positive pairs.
Extensive experiments on MS-COCO, Flickr30K, and CxC datasets demonstrate the
effectiveness of our method in representing complex image-text relationships
compared to state-of-the-art approaches. In addition, DITM enhances the
hierarchical reasoning ability of the model, supported by the extensive
analysis on HierarCaps benchmark.

</details>


### [242] [MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation](https://arxiv.org/pdf/2505.10238)
*Yanbo Ding, Xirui Hu, Zhizhi Guo, Yali Wang*

Main category: cs.CV

TL;DR: MTVCrafter introduces 4D motion tokenization for human image animation, outperforming 2D methods with better generalization and control.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on 2D-rendered pose images, limiting generalization and discarding 3D information.

Method: Proposes MTVCrafter with 4DMoT for motion tokenization and MV-DiT for motion-aware video generation.

Result: Achieves state-of-the-art FID-VID of 6.98, surpassing others by 65%, and generalizes well to diverse characters.

Conclusion: MTVCrafter advances human image animation by leveraging 4D motion, opening new directions for pose-guided video generation.

Abstract: Human image animation has gained increasing attention and developed rapidly
due to its broad applications in digital humans. However, existing methods rely
largely on 2D-rendered pose images for motion guidance, which limits
generalization and discards essential 3D information for open-world animation.
To tackle this problem, we propose MTVCrafter (Motion Tokenization Video
Crafter), the first framework that directly models raw 3D motion sequences
(i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT
(4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens.
Compared to 2D-rendered pose images, 4D motion tokens offer more robust
spatio-temporal cues and avoid strict pixel-level alignment between pose image
and character, enabling more flexible and disentangled control. Then, we
introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention
with 4D positional encodings, MV-DiT can effectively leverage motion tokens as
4D compact yet expressive context for human image animation in the complex 3D
world. Hence, it marks a significant step forward in this field and opens a new
direction for pose-guided human video generation. Experiments show that our
MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98,
surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter
also generalizes well to diverse open-world characters (single/multiple,
full/half-body) across various styles and scenarios. Our video demos and code
are on: https://github.com/DINGYANB/MTVCrafter.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [243] [On the Evaluation of Engineering Artificial General Intelligence](https://arxiv.org/pdf/2505.10653)
*Sandeep Neema, Susmit Jha, Adam Nagel, Ethan Lew, Chandrasekar Sureshkumar, Aleksa Gordic, Chase Shimmin, Hieu Nguygen, Paul Eremenko*

Main category: cs.AI

TL;DR: The paper proposes a framework for evaluating engineering artificial general intelligence (eAGI) agents, adapting Bloom's taxonomy for engineering design contexts.


<details>
  <summary>Details</summary>
Motivation: The challenge of evaluating eAGI agents, which require a blend of knowledge, creativity, and problem-solving skills, is critical for their development.

Method: An extensible evaluation framework is introduced, specializing Bloom's taxonomy for engineering design, including textual and structured artifact evaluation.

Result: The framework advances benchmarking by offering a rich taxonomy of questions, pluggable evaluation, and automatable customization for engineering contexts.

Conclusion: The proposed framework is a significant step toward standardized evaluation of eAGI agents, enabling their development and application in engineering.

Abstract: We discuss the challenges and propose a framework for evaluating engineering
artificial general intelligence (eAGI) agents. We consider eAGI as a
specialization of artificial general intelligence (AGI), deemed capable of
addressing a broad range of problems in the engineering of physical systems and
associated controllers. We exclude software engineering for a tractable scoping
of eAGI and expect dedicated software engineering AI agents to address the
software implementation challenges. Similar to human engineers, eAGI agents
should possess a unique blend of background knowledge (recall and retrieve) of
facts and methods, demonstrate familiarity with tools and processes, exhibit
deep understanding of industrial components and well-known design families, and
be able to engage in creative problem solving (analyze and synthesize),
transferring ideas acquired in one context to another. Given this broad
mandate, evaluating and qualifying the performance of eAGI agents is a
challenge in itself and, arguably, a critical enabler to developing eAGI
agents. In this paper, we address this challenge by proposing an extensible
evaluation framework that specializes and grounds Bloom's taxonomy - a
framework for evaluating human learning that has also been recently used for
evaluating LLMs - in an engineering design context. Our proposed framework
advances the state of the art in benchmarking and evaluation of AI agents in
terms of the following: (a) developing a rich taxonomy of evaluation questions
spanning from methodological knowledge to real-world design problems; (b)
motivating a pluggable evaluation framework that can evaluate not only textual
responses but also evaluate structured design artifacts such as CAD models and
SysML models; and (c) outlining an automatable procedure to customize the
evaluation benchmark to different engineering contexts.

</details>


### [244] [Interpretable Risk Mitigation in LLM Agent Systems](https://arxiv.org/pdf/2505.10670)
*Jan Chojnacki*

Main category: cs.AI

TL;DR: The paper explores improving LLM agent reliability in game-theoretic environments using interpretable feature steering, reducing defection by 28%.


<details>
  <summary>Details</summary>
Motivation: Address safety concerns about LLM agent unpredictability in responsible action domains.

Method: Use a strategy-modification method steering the residual stream with interpretable features from a sparse autoencoder latent space.

Result: Steering with good-faith negotiation reduces defection probability by 28%. Feasible steering ranges identified for open-source LLM agents.

Conclusion: Game-theoretic evaluation and representation-steering alignment can generalize to real-world applications.

Abstract: Autonomous agents powered by large language models (LLMs) enable novel use
cases in domains where responsible action is increasingly important. Yet the
inherent unpredictability of LLMs raises safety concerns about agent
reliability. In this work, we explore agent behaviour in a toy, game-theoretic
environment based on a variation of the Iterated Prisoner's Dilemma. We
introduce a strategy-modification method-independent of both the game and the
prompt-by steering the residual stream with interpretable features extracted
from a sparse autoencoder latent space. Steering with the good-faith
negotiation feature lowers the average defection probability by 28 percentage
points. We also identify feasible steering ranges for several open-source LLM
agents. Finally, we hypothesise that game-theoretic evaluation of LLM agents,
combined with representation-steering alignment, can generalise to real-world
applications on end-user devices and embodied platforms.

</details>


### [245] [Embodied AI in Machine Learning -- is it Really Embodied?](https://arxiv.org/pdf/2505.10705)
*Matej Hoffmann, Shubhan Parag Patni*

Main category: cs.AI

TL;DR: The paper critiques current AI-powered robots as weakly embodied, linking them to GOFAI issues, and discusses challenges in cross-embodiment learning.


<details>
  <summary>Details</summary>
Motivation: To contextualize Embodied AI within GOFAI and behavior-based alternatives, highlighting limitations in current approaches.

Method: Review and critical discussion of cross-embodiment learning, identifying roadblocks.

Result: Identifies fundamental challenges in achieving strong embodiment in AI-powered robots.

Conclusion: Proposes directions for progress in making AI-powered robots more strongly embodied.

Abstract: Embodied Artificial Intelligence (Embodied AI) is gaining momentum in the
machine learning communities with the goal of leveraging current progress in AI
(deep learning, transformers, large language and visual-language models) to
empower robots. In this chapter we put this work in the context of "Good
Old-Fashioned Artificial Intelligence" (GOFAI) (Haugeland, 1989) and the
behavior-based or embodied alternatives (R. A. Brooks 1991; Pfeifer and Scheier
2001). We claim that the AI-powered robots are only weakly embodied and inherit
some of the problems of GOFAI. Moreover, we review and critically discuss the
possibility of cross-embodiment learning (Padalkar et al. 2024). We identify
fundamental roadblocks and propose directions on how to make progress.

</details>


### [246] [Evaluations at Work: Measuring the Capabilities of GenAI in Use](https://arxiv.org/pdf/2505.10742)
*Brandon Lepine, Gawesha Weerantunga, Juho Kim, Pamela Mishkin, Matthew Beane*

Main category: cs.AI

TL;DR: The paper introduces a framework for evaluating human-AI collaboration by decomposing tasks into subtasks and tracking performance and user strategies. It proposes metrics like semantic similarity, structural coherence, and a novel 'information frontier' measure. Findings show LLM integration improves output quality but is moderated by factors like incoherence and knowledge gaps.


<details>
  <summary>Details</summary>
Motivation: Current AI benchmarks fail to capture the complexity of human-AI collaboration in multi-turn dialogues, necessitating a more nuanced evaluation framework.

Method: The study decomposes real-world tasks into subtasks, tracks LLM performance and user strategies, and introduces metrics like semantic similarity and the 'information frontier.' A financial valuation task demonstrates the methodology.

Result: Greater LLM integration enhances output quality but is moderated by response incoherence, subtask diversity, and knowledge gaps. Proactive novelty injection may harm performance.

Conclusion: The work provides a holistic evaluation framework for human-AI collaboration and actionable insights for improving AI-augmented workflows.

Abstract: Current AI benchmarks miss the messy, multi-turn nature of human-AI
collaboration. We present an evaluation framework that decomposes real-world
tasks into interdependent subtasks, letting us track both LLM performance and
users' strategies across a dialogue. Complementing this framework, we develop a
suite of metrics, including a composite usage derived from semantic similarity,
word overlap, and numerical matches; structural coherence; intra-turn
diversity; and a novel measure of the "information frontier" reflecting the
alignment between AI outputs and users' working knowledge. We demonstrate our
methodology in a financial valuation task that mirrors real-world complexity.
Our empirical findings reveal that while greater integration of LLM-generated
content generally enhances output quality, its benefits are moderated by
factors such as response incoherence, excessive subtask diversity, and the
distance of provided information from users' existing knowledge. These results
suggest that proactive dialogue strategies designed to inject novelty may
inadvertently undermine task performance. Our work thus advances a more
holistic evaluation of human-AI collaboration, offering both a robust
methodological framework and actionable insights for developing more effective
AI-augmented work processes.

</details>


### [247] [Code-Driven Planning in Grid Worlds with Large Language Models](https://arxiv.org/pdf/2505.10749)
*Ashwath Vaithinathan Aravindan, Zhisheng Tang, Mayank Kejriwal*

Main category: cs.AI

TL;DR: The paper introduces an iterative programmatic planning (IPP) framework using LLMs for synthesizing interpretable policies in code, outperforming direct code generation and establishing state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To create interpretable agent policies for grid-based tasks without relying on traditional search or reinforcement learning, leveraging LLMs for code generation.

Method: Uses code generation as policy synthesis, incorporating prompting strategies (direct code generation, pseudocode refinement, curriculum-based prompting) and iterative refinement based on feedback.

Result: IPP improves performance over direct code generation by 10% to 10x, achieving state-of-the-art on GRASP. It also reduces amortized costs significantly.

Conclusion: IPP is a viable, cost-effective approach for synthesizing interpretable policies, demonstrating superior performance and reusability.

Abstract: We propose an iterative programmatic planning (IPP) framework for solving
grid-based tasks by synthesizing interpretable agent policies expressed in code
using large language models (LLMs). Instead of relying on traditional search or
reinforcement learning, our approach uses code generation as policy synthesis,
where the LLM outputs executable programs that map environment states to action
sequences. Our proposed architecture incorporates several prompting strategies,
including direct code generation, pseudocode-conditioned refinement, and
curriculum-based prompting, but also includes an iterative refinement mechanism
that updates code based on task performance feedback. We evaluate our approach
using six leading LLMs and two challenging grid-based benchmarks (GRASP and
MiniGrid). Our IPP framework demonstrates improvements over direct code
generation ranging from 10\% to as much as 10x across five of the six models
and establishes a new state-of-the-art result for GRASP. IPP is found to
significantly outperform direct elicitation of a solution from GPT-o3-mini (by
63\% on MiniGrid to 116\% on GRASP), demonstrating the viability of the overall
approach. Computational costs of all code generation approaches are similar.
While code generation has a higher initial prompting cost compared to direct
solution elicitation (\$0.08 per task vs. \$0.002 per instance for
GPT-o3-mini), the code can be reused for any number of instances, making the
amortized cost significantly lower (by 400x on GPT-o3-mini across the complete
GRASP benchmark).

</details>


### [248] [Qualia Optimization](https://arxiv.org/pdf/2505.10779)
*Philip S. Thomas*

Main category: cs.AI

TL;DR: The paper explores whether AI systems could have qualia (subjective experiences like pain or pleasure) and proposes methods to incorporate such considerations into AI development.


<details>
  <summary>Details</summary>
Motivation: To address the speculative but impactful question of AI systems potentially having qualia, emphasizing the need to evaluate subjective experiences alongside performance metrics.

Method: Proposes mathematical problem settings inspired by reinforcement learning and philosophy of mind, refining these into methods that promote reinforcement.

Result: Initial approaches and properties are presented, enabling refinement of the problem setting.

Conclusion: The study culminates in proposing methods to integrate qualia considerations into AI systems, advocating for a broader evaluation framework.

Abstract: This report explores the speculative question: what if current or future AI
systems have qualia, such as pain or pleasure? It does so by assuming that AI
systems might someday possess qualia -- and that the quality of these
subjective experiences should be considered alongside performance metrics.
Concrete mathematical problem settings, inspired by reinforcement learning
formulations and theories from philosophy of mind, are then proposed and
initial approaches and properties are presented. These properties enable
refinement of the problem setting, culminating with the proposal of methods
that promote reinforcement.

</details>


### [249] [A Multi-modal Fusion Network for Terrain Perception Based on Illumination Aware](https://arxiv.org/pdf/2505.11066)
*Rui Wang, Shichun Yang, Yuyi Chen, Zhuoyang Li, Zexiang Tong, Jianyi Xu, Jiayi Lu, Xinjie Feng, Yaoguang Cao*

Main category: cs.AI

TL;DR: The paper proposes an illumination-aware multi-modal fusion network (IMF) for autonomous vehicles to improve real-time road terrain perception under varying lighting and weather conditions.


<details>
  <summary>Details</summary>
Motivation: Existing AV sensors struggle with lighting and weather variations, hindering accurate road condition perception.

Method: IMF combines exteroceptive and proprioceptive perception, dynamically adjusting modality weights based on illumination features, and includes an illumination-perception sub-network for feature estimation.

Result: IMF outperforms state-of-the-art methods, demonstrating the benefits of multi-modal fusion in varying lighting conditions.

Conclusion: The IMF framework effectively enhances road terrain perception for AVs, with publicly available dataset for further research.

Abstract: Road terrains play a crucial role in ensuring the driving safety of
autonomous vehicles (AVs). However, existing sensors of AVs, including cameras
and Lidars, are susceptible to variations in lighting and weather conditions,
making it challenging to achieve real-time perception of road conditions. In
this paper, we propose an illumination-aware multi-modal fusion network (IMF),
which leverages both exteroceptive and proprioceptive perception and optimizes
the fusion process based on illumination features. We introduce an
illumination-perception sub-network to accurately estimate illumination
features. Moreover, we design a multi-modal fusion network which is able to
dynamically adjust weights of different modalities according to illumination
features. We enhance the optimization process by pre-training of the
illumination-perception sub-network and incorporating illumination loss as one
of the training constraints. Extensive experiments demonstrate that the IMF
shows a superior performance compared to state-of-the-art methods. The
comparison results with single modality perception methods highlight the
comprehensive advantages of multi-modal fusion in accurately perceiving road
terrains under varying lighting conditions. Our dataset is available at:
https://github.com/lindawang2016/IMF.

</details>


### [250] [SECRET: Semi-supervised Clinical Trial Document Similarity Search](https://arxiv.org/pdf/2505.10780)
*Trisha Das, Afrah Shafquat, Beigi Mandis, Jacob Aptekar, Jimeng Sun*

Main category: cs.AI

TL;DR: A novel method improves identification of similar historical clinical trials, enhancing trial design and efficiency.


<details>
  <summary>Details</summary>
Motivation: Clinical trials are costly and risky; learning from past trials can mitigate errors and improve success.

Method: Summarizes trial protocols and searches for similar trials based on a query trial's protocol.

Result: Outperforms baselines with 78% recall@1 and 53% precision@1 improvements, excelling in partial similarity and zero-shot tasks.

Conclusion: The method enhances trial design by leveraging historical data, improving safety and efficiency.

Abstract: Clinical trials are vital for evaluation of safety and efficacy of new
treatments. However, clinical trials are resource-intensive, time-consuming and
expensive to conduct, where errors in trial design, reduced efficacy, and
safety events can result in significant delays, financial losses, and damage to
reputation. These risks underline the importance of informed and strategic
decisions in trial design to mitigate these risks and improve the chances of a
successful trial. Identifying similar historical trials is critical as these
trials can provide an important reference for potential pitfalls and challenges
including serious adverse events, dosage inaccuracies, recruitment
difficulties, patient adherence issues, etc. Addressing these challenges in
trial design can lead to development of more effective study protocols with
optimized patient safety and trial efficiency. In this paper, we present a
novel method to identify similar historical trials by summarizing clinical
trial protocols and searching for similar trials based on a query trial's
protocol. Our approach significantly outperforms all baselines, achieving up to
a 78% improvement in recall@1 and a 53% improvement in precision@1 over the
best baseline. We also show that our method outperforms all other baselines in
partial trial similarity search and zero-shot patient-trial matching,
highlighting its superior utility in these tasks.

</details>


### [251] [Developing and Integrating Trust Modeling into Multi-Objective Reinforcement Learning for Intelligent Agricultural Management](https://arxiv.org/pdf/2505.10803)
*Zhaoan Wang, Wonseok Jang, Bowen Ruan, Jun Wang, Shaoping Xiao*

Main category: cs.AI

TL;DR: The paper proposes a Human-AI Interaction (HAII) framework to bridge gaps between AI recommendations and farmers' trust, integrating trust into RL-based farm management for broader AI adoption.


<details>
  <summary>Details</summary>
Motivation: AI in agriculture faces adoption barriers due to misalignment with farmers' practical experience and trust issues. The study aims to enhance trust in AI-based fertilization strategies.

Method: Develops a trust model (ability, benevolence, integrity) and integrates it into a multi-objective RL framework, validated through farmer surveys.

Result: The approach ensures AI recommendations are technically robust, economically viable, context-aware, and socially acceptable, addressing farmer concerns.

Conclusion: Aligning technical performance with human-centered trust supports wider AI adoption in agriculture.

Abstract: Precision agriculture, enhanced by artificial intelligence (AI), offers
promising tools such as remote sensing, intelligent irrigation, fertilization
management, and crop simulation to improve agricultural efficiency and
sustainability. Reinforcement learning (RL), in particular, has outperformed
traditional methods in optimizing yields and resource management. However,
widespread AI adoption is limited by gaps between algorithmic recommendations
and farmers' practical experience, local knowledge, and traditional practices.
To address this, our study emphasizes Human-AI Interaction (HAII), focusing on
transparency, usability, and trust in RL-based farm management. We employ a
well-established trust framework - comprising ability, benevolence, and
integrity - to develop a novel mathematical model quantifying farmers'
confidence in AI-based fertilization strategies. Surveys conducted with farmers
for this research reveal critical misalignments, which are integrated into our
trust model and incorporated into a multi-objective RL framework. Unlike prior
methods, our approach embeds trust directly into policy optimization, ensuring
AI recommendations are technically robust, economically feasible,
context-aware, and socially acceptable. By aligning technical performance with
human-centered trust, this research supports broader AI adoption in
agriculture.

</details>


### [252] [AVA: Attentive VLM Agent for Mastering StarCraft II](https://arxiv.org/pdf/2503.05383)
*Weiyu Ma, Yuqian Fu, Zecheng Zhang, Bernard Ghanem, Guohao Li*

Main category: cs.AI

TL;DR: AVA is a multimodal StarCraft II agent using RGB visuals and language to align with human perception, outperforming traditional methods without extensive training.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between abstract agent perception and human gameplay experience for ecological validity.

Method: Combines a vision-language model, retrieval-augmented generation, and dynamic role-based task distribution.

Result: Achieves comparable performance to traditional MARL methods without explicit training in 21 scenarios.

Conclusion: AVA advances human-aligned game AI and multimodal agent research.

Abstract: We introduce Attentive VLM Agent (AVA), a multimodal StarCraft II agent that
aligns artificial agent perception with the human gameplay experience.
Traditional frameworks such as SMAC rely on abstract state representations that
diverge significantly from human perception, limiting the ecological validity
of agent behavior. Our agent addresses this limitation by incorporating RGB
visual inputs and natural language observations that more closely simulate
human cognitive processes during gameplay. The AVA architecture consists of
three integrated components: (1) a vision-language model enhanced with
specialized self-attention mechanisms for strategic unit targeting and
battlefield assessment, (2) a retrieval-augmented generation system that
leverages domain-specific StarCraft II knowledge to inform tactical decisions,
and (3) a dynamic role-based task distribution system that enables coordinated
multi-agent behavior. The experimental evaluation in our proposed AVACraft
environment, which contains 21 multimodal StarCraft II scenarios, demonstrates
that AVA powered by foundation models (specifically Qwen-VL and GPT-4o) can
execute complex tactical maneuvers without explicit training, achieving
comparable performance to traditional MARL methods that require substantial
training iterations. This work establishes a foundation for developing
human-aligned StarCraft II agents and advances the broader research agenda of
multimodal game AI. Our implementation is available at
https://github.com/camel-ai/VLM-Play-StarCraft2.

</details>


### [253] [PoE-World: Compositional World Modeling with Products of Programmatic Experts](https://arxiv.org/pdf/2505.10819)
*Wasu Top Piriyakulkij, Yichao Liang, Hao Tang, Adrian Weller, Marta Kryven, Kevin Ellis*

Main category: cs.AI

TL;DR: A novel program synthesis method, PoE-World, uses LLMs to create world models as weighted programs, enabling efficient learning from sparse data in complex domains like Atari games.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning world models require large datasets and lack flexibility. Program synthesis with LLMs offers a promising alternative for generalization from limited data.

Method: PoE-World represents world models as exponentially-weighted products of programmatic experts synthesized by LLMs, learning complex, stochastic models from few observations.

Result: The method successfully models complex domains like Atari's Pong and Montezuma's Revenge, showing efficient performance and generalization to unseen levels.

Conclusion: PoE-World demonstrates the potential of program synthesis for building adaptable world models with minimal data, advancing AI agent capabilities.

Abstract: Learning how the world works is central to building AI agents that can adapt
to complex environments. Traditional world models based on deep learning demand
vast amounts of training data, and do not flexibly update their knowledge from
sparse observations. Recent advances in program synthesis using Large Language
Models (LLMs) give an alternate approach which learns world models represented
as source code, supporting strong generalization from little data. To date,
application of program-structured world models remains limited to natural
language and grid-world domains. We introduce a novel program synthesis method
for effectively modeling complex, non-gridworld domains by representing a world
model as an exponentially-weighted product of programmatic experts (PoE-World)
synthesized by LLMs. We show that this approach can learn complex, stochastic
world models from just a few observations. We evaluate the learned world models
by embedding them in a model-based planning agent, demonstrating efficient
performance and generalization to unseen levels on Atari's Pong and Montezuma's
Revenge. We release our code and display the learned world models and videos of
the agent's gameplay at https://topwasu.github.io/poe-world.

</details>


### [254] [TACO: Rethinking Semantic Communications with Task Adaptation and Context Embedding](https://arxiv.org/pdf/2505.10834)
*Achintha Wijesinghe, Weiwei Wang, Suchinthaka Wanninayaka, Songyang Zhang, Zhi Ding*

Main category: cs.AI

TL;DR: A novel semantic communication framework is introduced to enhance task performance by jointly capturing task-specific and contextual information, showing improvements in downstream tasks, generalizability, bandwidth efficiency, and latency.


<details>
  <summary>Details</summary>
Motivation: The challenge in semantic communication is accurately identifying critical semantic information and adapting to evolving receiver objectives without performance degradation.

Method: The proposed framework jointly captures task-specific and contextual information for flexible adaptation to multiple downstream tasks.

Result: Experiments on image datasets and computer vision tasks demonstrate superior performance, better generalizability, high bandwidth efficiency, and low latency.

Conclusion: The framework effectively addresses the challenge of semantic communication, offering promising improvements over existing methods.

Abstract: Recent advancements in generative artificial intelligence have introduced
groundbreaking approaches to innovating next-generation semantic communication,
which prioritizes conveying the meaning of a message rather than merely
transmitting raw data. A fundamental challenge in semantic communication lies
in accurately identifying and extracting the most critical semantic information
while adapting to downstream tasks without degrading performance, particularly
when the objective at the receiver may evolve over time. To enable flexible
adaptation to multiple tasks at the receiver, this work introduces a novel
semantic communication framework, which is capable of jointly capturing
task-specific information to enhance downstream task performance and contextual
information. Through rigorous experiments on popular image datasets and
computer vision tasks, our framework shows promising improvement compared to
existing work, including superior performance in downstream tasks, better
generalizability, ultra-high bandwidth efficiency, and low reconstruction
latency.

</details>


### [255] [Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models](https://arxiv.org/pdf/2505.10844)
*Simeng Han, Stephen Xia, Grant Zhang, Howard Dai, Chen Liu, Lichang Chen, Hoang Huy Nguyen, Hongyuan Mei, Jiayuan Mao, R. Thomas McCoy*

Main category: cs.AI

TL;DR: The paper introduces a benchmark using brainteasers to evaluate LLMs' reasoning strategies, focusing on correctness, quality, and creativity of solutions.


<details>
  <summary>Details</summary>
Motivation: Accuracy alone doesn't reveal how AI models reason. Brainteasers, solvable via multiple approaches, help probe deeper into reasoning strategies.

Method: The study evaluates LLMs across five reasoning layers: semantic parsing, solution generation, self-correction, step-by-step sketches, and hint utilization.

Result: LLMs often produce creative solutions but sometimes default to brute force, indicating room for improvement in reasoning efficiency.

Conclusion: LLMs show potential for creative problem-solving but need refinement to reduce reliance on brute-force methods.

Abstract: Accuracy remains a standard metric for evaluating AI systems, but it offers
limited insight into how models arrive at their solutions. In this work, we
introduce a benchmark based on brainteasers written in long narrative form to
probe more deeply into the types of reasoning strategies that models use.
Brainteasers are well-suited for this goal because they can be solved with
multiple approaches, such as a few-step solution that uses a creative insight
or a longer solution that uses more brute force. We investigate large language
models (LLMs) across multiple layers of reasoning, focusing not only on
correctness but also on the quality and creativity of their solutions. We
investigate many aspects of the reasoning process: (1) semantic parsing of the
brainteasers into precise mathematical competition style formats; (2)
generating solutions from these mathematical forms; (3) self-correcting
solutions based on gold solutions; (4) producing step-by-step sketches of
solutions; and (5) making use of hints. We find that LLMs are in many cases
able to find creative, insightful solutions to brainteasers, suggesting that
they capture some of the capacities needed to solve novel problems in creative
ways. Nonetheless, there also remain situations where they rely on brute force
despite the availability of more efficient, creative solutions, highlighting a
potential direction for improvement in the reasoning abilities of LLMs.

</details>


### [256] [MCU: Improving Machine Unlearning through Mode Connectivity](https://arxiv.org/pdf/2505.10859)
*Yingdan Shi, Ren Wang*

Main category: cs.AI

TL;DR: MCU is a novel Machine Unlearning framework using mode connectivity for nonlinear unlearning, improving efficacy and efficiency with parameter masks and adaptive penalty coefficients.


<details>
  <summary>Details</summary>
Motivation: To address weight entanglement in linear MU methods and enhance unlearning performance and compliance with privacy regulations.

Method: Leverages mode connectivity for nonlinear unlearning, introduces parameter masks, and adaptive penalty coefficients for balanced performance.

Result: MCU outperforms existing MU methods, providing a spectrum of unlearning models and seamless integration.

Conclusion: MCU is an effective, plug-and-play framework for machine unlearning, demonstrated by superior performance in image classification tasks.

Abstract: Machine Unlearning (MU) aims to remove the information of specific training
data from a trained model, ensuring compliance with privacy regulations and
user requests. While one line of existing MU methods relies on linear parameter
updates via task arithmetic, they suffer from weight entanglement. In this
work, we propose a novel MU framework called Mode Connectivity Unlearning (MCU)
that leverages mode connectivity to find an unlearning pathway in a nonlinear
manner. To further enhance performance and efficiency, we introduce a parameter
mask strategy that not only improves unlearning effectiveness but also reduces
computational overhead. Moreover, we propose an adaptive adjustment strategy
for our unlearning penalty coefficient to adaptively balance forgetting quality
and predictive performance during training, eliminating the need for empirical
hyperparameter tuning. Unlike traditional MU methods that identify only a
single unlearning model, MCU uncovers a spectrum of unlearning models along the
pathway. Overall, MCU serves as a plug-and-play framework that seamlessly
integrates with any existing MU methods, consistently improving unlearning
efficacy. Extensive experiments on the image classification task demonstrate
that MCU achieves superior performance.

</details>


### [257] [InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer Interaction](https://arxiv.org/pdf/2505.10887)
*Bin Lei, Weitai Kang, Zijian Zhang, Winson Chen, Xi Xie, Shan Zuo, Mimi Xie, Ali Payani, Mingyi Hong, Yan Yan, Caiwen Ding*

Main category: cs.AI

TL;DR: InfantAgent-Next is a multimodal generalist agent for computer interaction, outperforming existing methods in benchmarks like OSWorld, GAIA, and SWE-Bench.


<details>
  <summary>Details</summary>
Motivation: To create a versatile agent that integrates tool-based and vision agents modularly for collaborative task-solving, addressing limitations of single-model workflows.

Method: Combines tool-based and pure vision agents in a modular architecture, enabling step-by-step task-solving with diverse models.

Result: Achieves 7.27% accuracy on OSWorld, surpassing Claude-Computer-Use, and performs well on GAIA and SWE-Bench.

Conclusion: InfantAgent-Next demonstrates superior generality and performance, with open-sourced code for reproducibility.

Abstract: This paper introduces \textsc{InfantAgent-Next}, a generalist agent capable
of interacting with computers in a multimodal manner, encompassing text,
images, audio, and video. Unlike existing approaches that either build
intricate workflows around a single large model or only provide workflow
modularity, our agent integrates tool-based and pure vision agents within a
highly modular architecture, enabling different models to collaboratively solve
decoupled tasks in a step-by-step manner. Our generality is demonstrated by our
ability to evaluate not only pure vision-based real-world benchmarks (i.e.,
OSWorld), but also more general or tool-intensive benchmarks (e.g., GAIA and
SWE-Bench). Specifically, we achieve $\mathbf{7.27\%}$ accuracy on OSWorld,
higher than Claude-Computer-Use. Codes and evaluation scripts are open-sourced
at https://github.com/bin123apple/InfantAgent.

</details>


### [258] [MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective Search and Data Curation](https://arxiv.org/pdf/2505.10962)
*Zhenwen Liang, Linfeng Song, Yang Li, Tao Yang, Feng Zhang, Haitao Mi, Dong Yu*

Main category: cs.AI

TL;DR: MPS-Prover introduces a novel ATP system with data curation and multi-perspective search, outperforming existing models in efficiency and proof diversity.


<details>
  <summary>Details</summary>
Motivation: Addressing inefficiencies and biased search in existing stepwise ATP systems using LLMs.

Method: Combines post-training data pruning (40% reduction) with a multi-perspective tree search integrating a critic model and heuristic rules.

Result: Achieves state-of-the-art performance on benchmarks like miniF2F and ProofNet, with shorter, more diverse proofs.

Conclusion: MPS-Prover advances LLM-based formal reasoning, offering a robust framework for future theorem provers.

Abstract: Automated Theorem Proving (ATP) in formal languages remains a formidable
challenge in AI, demanding rigorous logical deduction and navigating vast
search spaces. While large language models (LLMs) have shown promising
performance, existing stepwise provers often suffer from biased search
guidance, leading to inefficiencies and suboptimal proof strategies. This paper
introduces the Multi-Perspective Search Prover (MPS-Prover), a novel stepwise
ATP system designed to overcome these limitations. MPS-Prover incorporates two
key innovations: a highly effective post-training data curation strategy that
prunes approximately 40% of redundant training data without sacrificing
performance, and a multi-perspective tree search mechanism. This search
integrates a learned critic model with strategically designed heuristic rules
to diversify tactic selection, prevent getting trapped in unproductive states,
and enhance search robustness. Extensive evaluations demonstrate that
MPS-Prover achieves state-of-the-art performance on multiple challenging
benchmarks, including miniF2F and ProofNet, outperforming prior 7B parameter
models. Furthermore, our analyses reveal that MPS-Prover generates
significantly shorter and more diverse proofs compared to existing stepwise and
whole-proof methods, highlighting its efficiency and efficacy. Our work
advances the capabilities of LLM-based formal reasoning and offers a robust
framework and a comprehensive analysis for developing more powerful theorem
provers.

</details>


### [259] [Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory](https://arxiv.org/pdf/2505.10981)
*Yexiang Liu, Zekun Li, Zhi Fang, Nan Xu, Ran He, Tieniu Tan*

Main category: cs.AI

TL;DR: The paper investigates how different reasoning prompting strategies scale with test-time compute in LLMs, finding that simple Chain-of-Thought outperforms complex strategies as compute increases. It also proposes a method to predict scaling performance and suggests improvements.


<details>
  <summary>Details</summary>
Motivation: To understand how prompting strategies perform under scaling (majority voting) and identify efficient methods to predict and enhance performance.

Method: Systematic experiments on 6 LLMs, 8 prompting strategies, and 6 benchmarks, along with theoretical analysis and a proposed prediction method based on probability theory.

Result: Complex prompting strategies initially perform better but are outperformed by Chain-of-Thought as compute scales. The proposed method accurately predicts scaling performance.

Conclusion: Simple prompting strategies like Chain-of-Thought are more scalable, and the study provides insights and methods to improve test-time scaling performance.

Abstract: Recently, scaling test-time compute on Large Language Models (LLM) has
garnered wide attention. However, there has been limited investigation of how
various reasoning prompting strategies perform as scaling. In this paper, we
focus on a standard and realistic scaling setting: majority voting. We
systematically conduct experiments on 6 LLMs $\times$ 8 prompting strategies
$\times$ 6 benchmarks. Experiment results consistently show that as the
sampling time and computational overhead increase, complicated prompting
strategies with superior initial performance gradually fall behind simple
Chain-of-Thought. We analyze this phenomenon and provide theoretical proofs.
Additionally, we propose a method according to probability theory to quickly
and accurately predict the scaling performance and select the best strategy
under large sampling times without extra resource-intensive inference in
practice. It can serve as the test-time scaling law for majority voting.
Furthermore, we introduce two ways derived from our theoretical analysis to
significantly improve the scaling performance. We hope that our research can
promote to re-examine the role of complicated prompting, unleash the potential
of simple prompting strategies, and provide new insights for enhancing
test-time scaling performance.

</details>


### [260] [Facets in Argumentation: A Formal Approach to Argument Significance](https://arxiv.org/pdf/2505.10982)
*Johannes Fichte, Nicolas Fr√∂hlich, Markus Hecher, Victor Lagerkvist, Yasir Mahmood, Arne Meier, Jonathan Persson*

Main category: cs.AI

TL;DR: The paper introduces 'facets' for reasoning between decision and enumeration in argumentation frameworks, showing they are computationally easier than counting extensions.


<details>
  <summary>Details</summary>
Motivation: To address the gap between decision, counting/enumeration, and fine-grained reasoning in abstract argumentation frameworks, which currently requires expensive reasoning.

Method: Introduces the concept of 'facets'‚Äîarguments that are credulous but not skeptical‚Äîand studies their complexity. Provides an implementation and conducts experiments.

Result: Tasks involving facets are computationally easier than counting extensions, and the implementation demonstrates feasibility.

Conclusion: Facets offer a practical and efficient way to navigate and reason about arguments in abstract argumentation frameworks.

Abstract: Argumentation is a central subarea of Artificial Intelligence (AI) for
modeling and reasoning about arguments. The semantics of abstract argumentation
frameworks (AFs) is given by sets of arguments (extensions) and conditions on
the relationship between them, such as stable or admissible. Today's solvers
implement tasks such as finding extensions, deciding credulous or skeptical
acceptance, counting, or enumerating extensions. While these tasks are well
charted, the area between decision, counting/enumeration and fine-grained
reasoning requires expensive reasoning so far. We introduce a novel concept
(facets) for reasoning between decision and enumeration. Facets are arguments
that belong to some extensions (credulous) but not to all extensions
(skeptical). They are most natural when a user aims to navigate, filter, or
comprehend the significance of specific arguments, according to their needs. We
study the complexity and show that tasks involving facets are much easier than
counting extensions. Finally, we provide an implementation, and conduct
experiments to demonstrate feasibility.

</details>


### [261] [DRL-Based Injection Molding Process Parameter Optimization for Adaptive and Profitable Production](https://arxiv.org/pdf/2505.10988)
*Joon-Young Kim, Jecheon Yu, Heekyu Kim, Seunghwa Ryu*

Main category: cs.AI

TL;DR: A DRL-based framework optimizes plastic injection molding by balancing quality and profit, outperforming traditional methods in speed and adaptability.


<details>
  <summary>Details</summary>
Motivation: The challenge of optimizing process parameters in plastic injection molding under dynamic conditions drives the need for a real-time, adaptive solution.

Method: The study uses deep reinforcement learning (DRL) with SAC and PPO algorithms, incorporating surrogate models for quality and cycle time prediction.

Result: The DRL framework dynamically adapts to variations, maintaining quality and maximizing profit, with 135x faster inference than genetic algorithms.

Conclusion: The scalable and adaptable DRL framework shows promise for intelligent, data-driven decision-making in manufacturing.

Abstract: Plastic injection molding remains essential to modern manufacturing. However,
optimizing process parameters to balance product quality and profitability
under dynamic environmental and economic conditions remains a persistent
challenge. This study presents a novel deep reinforcement learning (DRL)-based
framework for real-time process optimization in injection molding, integrating
product quality and profitability into the control objective. A profit function
was developed to reflect real-world manufacturing costs, incorporating resin,
mold wear, and electricity prices, including time-of-use variations. Surrogate
models were constructed to predict product quality and cycle time, enabling
efficient offline training of DRL agents using soft actor-critic (SAC) and
proximal policy optimization (PPO) algorithms. Experimental results demonstrate
that the proposed DRL framework can dynamically adapt to seasonal and
operational variations, consistently maintaining product quality while
maximizing profit. Compared to traditional optimization methods such as genetic
algorithms, the DRL models achieved comparable economic performance with up to
135x faster inference speeds, making them well-suited for real-time
applications. The framework's scalability and adaptability highlight its
potential as a foundation for intelligent, data-driven decision-making in
modern manufacturing environments.

</details>


### [262] [RAGSynth: Synthetic Data for Robust and Faithful RAG Component Optimization](https://arxiv.org/pdf/2505.10989)
*Haiyang Shen, Hang Yan, Zhongshi Xing, Mugeng Liu, Yue Li, Zhiyang Chen, Yuxiang Wang, Jiuzheng Wang, Yun Ma*

Main category: cs.AI

TL;DR: RAGSynth improves RAG systems by optimizing retriever robustness and generator fidelity using synthetic data, validated across diverse domains.


<details>
  <summary>Details</summary>
Motivation: Existing RAG retrievers struggle with complex queries and incomplete clues, while generators face fidelity issues, prompting the need for a better solution.

Method: Introduces RAGSynth, a framework with data construction modeling and synthetic data generation, and SynthBench, a benchmark for evaluation.

Result: Synthetic data from RAGSynth enhances retriever robustness and generator fidelity, improving RAG system performance across domains.

Conclusion: RAGSynth effectively addresses RAG limitations, offering a scalable solution with open-sourced implementation.

Abstract: RAG can enhance the performance of LLMs on knowledge-intensive tasks. Various
RAG paradigms, including vanilla, planning-based, and iterative RAG, are built
upon 2 cores: the retriever, which should robustly select relevant documents
across complex queries, and the generator, which should faithfully synthesize
responses. However, existing retrievers rely heavily on public knowledge and
struggle with queries of varying logical complexity and clue completeness,
while generators frequently face fidelity problems. In this work, we introduce
RAGSynth, a framework that includes a data construction modeling and a
corresponding synthetic data generation implementation, designed to optimize
retriever robustness and generator fidelity. Additionally, we present
SynthBench, a benchmark encompassing 8 domain-specific documents across 4
domains, featuring diverse query complexities, clue completeness, and
fine-grained citation granularity. Leveraging RAGSynth, we generate a
large-scale synthetic dataset, including single and multi-hop. Extensive
experiments demonstrate that the synthetic data significantly improves the
robustness of the retrievers and the fidelity of the generators. Additional
evaluations confirm that RAGSynth can also generalize well across different
domains. By integrating the optimized retrievers into various RAG paradigms, we
consistently observe enhanced RAG system performance. We have open-sourced the
implementation on https://github.com/EachSheep/RAGSynth.

</details>


### [263] [Most General Explanations of Tree Ensembles](https://arxiv.org/pdf/2505.10991)
*Yacine Izza, Alexey Ignatiev, Joao Marques-Silva, Peter J. Stuckey*

Main category: cs.AI

TL;DR: The paper proposes a method to find the most general abductive explanation for AI decisions, ensuring broad applicability and correctness.


<details>
  <summary>Details</summary>
Motivation: To enhance trust in AI systems by providing the most general and sensible explanations for decisions.

Method: Uses formal models to identify inflated abductive explanations, covering larger input spaces while ensuring correctness.

Result: Demonstrates how to derive the most general abductive explanation for an AI decision.

Conclusion: The most general explanation is preferred for its broad applicability and human-sensible appeal.

Abstract: Explainable Artificial Intelligence (XAI) is critical for attaining trust in
the operation of AI systems. A key question of an AI system is ``why was this
decision made this way''. Formal approaches to XAI use a formal model of the AI
system to identify abductive explanations. While abductive explanations may be
applicable to a large number of inputs sharing the same concrete values, more
general explanations may be preferred for numeric inputs. So-called inflated
abductive explanations give intervals for each feature ensuring that any input
whose values fall withing these intervals is still guaranteed to make the same
prediction. Inflated explanations cover a larger portion of the input space,
and hence are deemed more general explanations. But there can be many
(inflated) abductive explanations for an instance. Which is the best? In this
paper, we show how to find a most general abductive explanation for an AI
decision. This explanation covers as much of the input space as possible, while
still being a correct formal explanation of the model's behaviour. Given that
we only want to give a human one explanation for a decision, the most general
explanation gives us the explanation with the broadest applicability, and hence
the one most likely to seem sensible. (The paper has been accepted at IJCAI2025
conference.)

</details>


### [264] [GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning](https://arxiv.org/pdf/2505.11049)
*Yue Liu, Shengfang Zhai, Mingzhe Du, Yulin Chen, Tri Cao, Hongcheng Gao, Cheng Wang, Xinfeng Li, Kun Wang, Junfeng Fang, Jiaheng Zhang, Bryan Hooi*

Main category: cs.AI

TL;DR: A novel reasoning-based VLM guard model, GuardReasoner-VL, is introduced to enhance safety via online RL, outperforming others by 19.27% F1 score.


<details>
  <summary>Details</summary>
Motivation: To improve the safety of VLMs by incentivizing deliberate reasoning before moderation decisions.

Method: Constructs a reasoning corpus (GuardReasoner-VLTrain), uses SFT for cold-start, enhances reasoning via online RL with rejection sampling, data augmentation, and dynamic clipping.

Result: Superior performance, surpassing the runner-up by 19.27% F1 score.

Conclusion: GuardReasoner-VL effectively enhances VLM safety through reasoning and RL, with released data, code, and models.

Abstract: To enhance the safety of VLMs, this paper introduces a novel reasoning-based
VLM guard model dubbed GuardReasoner-VL. The core idea is to incentivize the
guard model to deliberatively reason before making moderation decisions via
online RL. First, we construct GuardReasoner-VLTrain, a reasoning corpus with
123K samples and 631K reasoning steps, spanning text, image, and text-image
inputs. Then, based on it, we cold-start our model's reasoning ability via SFT.
In addition, we further enhance reasoning regarding moderation through online
RL. Concretely, to enhance diversity and difficulty of samples, we conduct
rejection sampling followed by data augmentation via the proposed safety-aware
data concatenation. Besides, we use a dynamic clipping parameter to encourage
exploration in early stages and exploitation in later stages. To balance
performance and token efficiency, we design a length-aware safety reward that
integrates accuracy, format, and token cost. Extensive experiments demonstrate
the superiority of our model. Remarkably, it surpasses the runner-up by 19.27%
F1 score on average. We release data, code, and models (3B/7B) of
GuardReasoner-VL at https://github.com/yueliu1999/GuardReasoner-VL/

</details>


### [265] [Think Twice Before You Act: Enhancing Agent Behavioral Safety with Thought Correction](https://arxiv.org/pdf/2505.11063)
*Changyue Jiang, Xudong Pan, Min Yang*

Main category: cs.AI

TL;DR: Thought-Aligner is a dynamic thought correction module for LLM-based agents, improving safety from 50% to 90% without altering the agent framework.


<details>
  <summary>Details</summary>
Motivation: Addressing safety risks in long-horizon behavioral trajectories of LLM-based agents caused by minor thought deviations.

Method: Proposes Thought-Aligner, a lightweight model correcting high-risk thoughts before action execution, trained using contrastive learning on 11,400 safe/unsafe thought pairs.

Result: Raises agent safety to 90%, maintains latency below 100ms, and ensures minimal resource usage.

Conclusion: Thought-Aligner offers a practical, efficient, and widely applicable safety solution for LLM-based agents.

Abstract: LLM-based autonomous agents possess capabilities such as reasoning, tool
invocation, and environment interaction, enabling the execution of complex
multi-step tasks. The internal reasoning process, i.e., thought, of behavioral
trajectory significantly influences tool usage and subsequent actions but can
introduce potential risks. Even minor deviations in the agent's thought may
trigger cascading effects leading to irreversible safety incidents. To address
the safety alignment challenges in long-horizon behavioral trajectories, we
propose Thought-Aligner, a plug-in dynamic thought correction module. Utilizing
a lightweight and resource-efficient model, Thought-Aligner corrects each
high-risk thought on the fly before each action execution. The corrected
thought is then reintroduced to the agent, ensuring safer subsequent decisions
and tool interactions. Importantly, Thought-Aligner modifies only the reasoning
phase without altering the underlying agent framework, making it easy to deploy
and widely applicable to various agent frameworks. To train the Thought-Aligner
model, we construct an instruction dataset across ten representative scenarios
and simulate ReAct execution trajectories, generating 5,000 diverse
instructions and more than 11,400 safe and unsafe thought pairs. The model is
fine-tuned using contrastive learning techniques. Experiments across three
agent safety benchmarks involving 12 different LLMs demonstrate that
Thought-Aligner raises agent behavioral safety from approximately 50% in the
unprotected setting to 90% on average. Additionally, Thought-Aligner maintains
response latency below 100ms with minimal resource usage, demonstrating its
capability for efficient deployment, broad applicability, and timely
responsiveness. This method thus provides a practical dynamic safety solution
for the LLM-based agents.

</details>


### [266] [Analysis of Customer Journeys Using Prototype Detection and Counterfactual Explanations for Sequential Data](https://arxiv.org/pdf/2505.11086)
*Keita Kinjo*

Main category: cs.AI

TL;DR: A novel three-step method analyzes customer journeys by defining sequence distances, predicting purchase likelihood, and recommending counterfactual sequences to improve marketing strategies.


<details>
  <summary>Details</summary>
Motivation: The study addresses the lack of quantitative analysis of customer journeys due to sequential data complexity and aims to enhance marketing strategies.

Method: The approach involves defining sequence distances, predicting purchase likelihood, and generating counterfactual sequences to boost purchase probability.

Result: Typical sequences were identified, and key parts influencing purchases were detected, demonstrating the method's effectiveness.

Conclusion: The proposed approach can enhance marketing activities by providing actionable insights into customer journeys.

Abstract: Recently, the proliferation of omni-channel platforms has attracted interest
in customer journeys, particularly regarding their role in developing marketing
strategies. However, few efforts have been taken to quantitatively study or
comprehensively analyze them owing to the sequential nature of their data and
the complexity involved in analysis. In this study, we propose a novel approach
comprising three steps for analyzing customer journeys. First, the distance
between sequential data is defined and used to identify and visualize
representative sequences. Second, the likelihood of purchase is predicted based
on this distance. Third, if a sequence suggests no purchase, counterfactual
sequences are recommended to increase the probability of a purchase using a
proposed method, which extracts counterfactual explanations for sequential
data. A survey was conducted, and the data were analyzed; the results revealed
that typical sequences could be extracted, and the parts of those sequences
important for purchase could be detected. We believe that the proposed approach
can support improvements in various marketing activities.

</details>


### [267] [Group Think: Multiple Concurrent Reasoning Agents Collaborating at Token Level Granularity](https://arxiv.org/pdf/2505.11107)
*Chan-Jan Hsu, Davide Buffelli, Jamie McGowan, Feng-Ting Liao, Yi-Chang Chen, Sattar Vakili, Da-shan Shiu*

Main category: cs.AI

TL;DR: Group Think enables a single LLM to act as multiple concurrent reasoning agents, improving reasoning quality and reducing latency by allowing dynamic token-level collaboration.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between latency and reasoning quality in multi-agent LLMs by introducing a concurrent-reasoning paradigm.

Method: Proposes Group Think, where a single LLM acts as multiple thinkers with shared visibility, enabling dynamic token-level collaboration.

Result: Reduces redundant reasoning, improves quality, and lowers latency, with efficient resource utilization for edge inference.

Conclusion: Group Think paves the way for more efficient and sophisticated collaborative behavior in LLMs.

Abstract: Recent advances in large language models (LLMs) have demonstrated the power
of reasoning through self-generated chains of thought. Multiple reasoning
agents can collaborate to raise joint reasoning quality above individual
outcomes. However, such agents typically interact in a turn-based manner,
trading increased latency for improved quality. In this paper, we propose Group
Think--a single LLM that acts as multiple concurrent reasoning agents, or
thinkers. With shared visibility into each other's partial generation progress,
Group Think introduces a new concurrent-reasoning paradigm in which multiple
reasoning trajectories adapt dynamically to one another at the token level. For
example, a reasoning thread may shift its generation mid-sentence upon
detecting that another thread is better positioned to continue. This
fine-grained, token-level collaboration enables Group Think to reduce redundant
reasoning and improve quality while achieving significantly lower latency.
Moreover, its concurrent nature allows for efficient utilization of idle
computational resources, making it especially suitable for edge inference,
where very small batch size often underutilizes local~GPUs. We give a simple
and generalizable modification that enables any existing LLM to perform Group
Think on a local GPU. We also present an evaluation strategy to benchmark
reasoning latency and empirically demonstrate latency improvements using
open-source LLMs that were not explicitly trained for Group Think. We hope this
work paves the way for future LLMs to exhibit more sophisticated and more
efficient collaborative behavior for higher quality generation.

</details>


### [268] [Predicting Student Dropout Risk With A Dual-Modal Abrupt Behavioral Changes Approach](https://arxiv.org/pdf/2505.11119)
*Jiabei Cheng, Zhen-Qun Yang, Jiannong Cao, Yu Yang, Xinzhe Zheng*

Main category: cs.AI

TL;DR: The paper introduces the Dual-Modal Multiscale Sliding Window (DMSW) Model to predict student dropout risk by analyzing academic and behavioral data, improving accuracy by 15% over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Timely dropout prediction is crucial for early intervention, but poor data quality and lack of quantifiable metrics in offline settings limit existing methods.

Method: Proposes the DMSW Model, integrating academic and behavioral data to dynamically capture behavior patterns with minimal data.

Result: DMSW improves prediction accuracy by 15%, enabling earlier identification of high-risk students and timely support.

Conclusion: The model bridges theory and practice, offering educators a tool to enhance retention and outcomes through tailored interventions.

Abstract: Timely prediction of students at high risk of dropout is critical for early
intervention and improving educational outcomes. However, in offline
educational settings, poor data quality, limited scale, and high heterogeneity
often hinder the application of advanced machine learning models. Furthermore,
while educational theories provide valuable insights into dropout phenomena,
the lack of quantifiable metrics for key indicators limits their use in
data-driven modeling. Through data analysis and a review of educational
literature, we identified abrupt changes in student behavior as key early
signals of dropout risk. To address this, we propose the Dual-Modal Multiscale
Sliding Window (DMSW) Model, which integrates academic performance and
behavioral data to dynamically capture behavior patterns using minimal data.
The DMSW model improves prediction accuracy by 15% compared to traditional
methods, enabling educators to identify high-risk students earlier, provide
timely support, and foster a more inclusive learning environment. Our analysis
highlights key behavior patterns, offering practical insights for preventive
strategies and tailored support. These findings bridge the gap between theory
and practice in dropout prediction, giving educators an innovative tool to
enhance student retention and outcomes.

</details>


### [269] [Navigating the Alpha Jungle: An LLM-Powered MCTS Framework for Formulaic Factor Mining](https://arxiv.org/pdf/2505.11122)
*Yu Shi, Yitong Duan, Jian Li*

Main category: cs.AI

TL;DR: A novel framework combining LLMs and MCTS for efficient and interpretable alpha factor mining in quantitative investment, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional alpha mining methods are inefficient or yield uninterpretable results, prompting the need for an automated yet effective approach.

Method: Integrates LLMs with MCTS to iteratively generate and refine symbolic alpha formulas, guided by financial backtesting feedback and a subtree avoidance mechanism.

Result: Outperforms existing methods in predictive accuracy, trading performance, and interpretability on real-world stock market data.

Conclusion: The LLM-based framework offers a superior, efficient solution for formulaic alpha mining.

Abstract: Alpha factor mining is pivotal in quantitative investment for identifying
predictive signals from complex financial data. While traditional formulaic
alpha mining relies on human expertise, contemporary automated methods, such as
those based on genetic programming or reinforcement learning, often suffer from
search inefficiency or yield poorly interpretable alpha factors. This paper
introduces a novel framework that integrates Large Language Models (LLMs) with
Monte Carlo Tree Search (MCTS) to overcome these limitations. Our approach
leverages the LLM's instruction-following and reasoning capability to
iteratively generate and refine symbolic alpha formulas within an MCTS-driven
exploration. A key innovation is the guidance of MCTS exploration by rich,
quantitative feedback from financial backtesting of each candidate factor,
enabling efficient navigation of the vast search space. Furthermore, a frequent
subtree avoidance mechanism is introduced to bolster search efficiency and
alpha factor performance. Experimental results on real-world stock market data
demonstrate that our LLM-based framework outperforms existing methods by mining
alphas with superior predictive accuracy, trading performance, and improved
interpretability, while offering a more efficient solution for formulaic alpha
mining.

</details>


### [270] [Scalability of Reinforcement Learning Methods for Dispatching in Semiconductor Frontend Fabs: A Comparison of Open-Source Models with Real Industry Datasets](https://arxiv.org/pdf/2505.11135)
*Patrick St√∂ckermann, Henning S√ºdfeld, Alessandro Immordino, Thomas Altenm√ºller, Marc Wegmann, Martin Gebser, Konstantin Schekotihin, Georg Seidel, Chew Wye Chan, Fei Fei Zhang*

Main category: cs.AI

TL;DR: The paper compares RL methods (policy-gradient and Evolution Strategies) for semiconductor scheduling, showing Evolution Strategies scale better and improve performance in real-world and benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Existing benchmark datasets lack real-world complexity, so the study evaluates optimization methods' scalability and effectiveness in realistic semiconductor fab simulations.

Method: Uses Reinforcement Learning (policy-gradient and Evolution Strategies) for optimization, focusing on bottleneck tool selection and diverse training datasets.

Result: Evolution Strategies outperform policy-gradient, improving tardiness (up to 4%) and throughput (up to 1%) in real datasets, with even better gains in benchmarks.

Conclusion: Evolution Strategies are more scalable and effective for semiconductor scheduling, especially with diverse training data and bottleneck tool control.

Abstract: Benchmark datasets are crucial for evaluating approaches to scheduling or
dispatching in the semiconductor industry during the development and deployment
phases. However, commonly used benchmark datasets like the Minifab or SMT2020
lack the complex details and constraints found in real-world scenarios. To
mitigate this shortcoming, we compare open-source simulation models with a real
industry dataset to evaluate how optimization methods scale with different
levels of complexity. Specifically, we focus on Reinforcement Learning methods,
performing optimization based on policy-gradient and Evolution Strategies. Our
research provides insights into the effectiveness of these optimization methods
and their applicability to realistic semiconductor frontend fab simulations. We
show that our proposed Evolution Strategies-based method scales much better
than a comparable policy-gradient-based approach. Moreover, we identify the
selection and combination of relevant bottleneck tools to control by the agent
as crucial for an efficient optimization. For the generalization across
different loading scenarios and stochastic tool failure patterns, we achieve
advantages when utilizing a diverse training dataset. While the overall
approach is computationally expensive, it manages to scale well with the number
of CPU cores used for training. For the real industry dataset, we achieve an
improvement of up to 4% regarding tardiness and up to 1% regarding throughput.
For the less complex open-source models Minifab and SMT2020, we observe
double-digit percentage improvement in tardiness and single digit percentage
improvement in throughput by use of Evolution Strategies.

</details>


### [271] [Reinforcement Learning for AMR Charging Decisions: The Impact of Reward and Action Space Design](https://arxiv.org/pdf/2505.11136)
*Janik Bischoff, Alexandru Rinciog, Anne Meyer*

Main category: cs.AI

TL;DR: The paper proposes a reinforcement learning (RL) approach to optimize charging strategies for mobile robots in warehouses, comparing flexible and guided RL designs. It highlights trade-offs in performance and stability, introduces a simulation framework extension, and evaluates designs using adaptive heuristics and Proximal Policy Optimization.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of optimizing charging strategies for autonomous mobile robots in large-scale warehouses, where traditional methods may be inefficient or inflexible.

Method: The study uses RL with varying reward and action space configurations, comparing flexible and guided designs. It extends the SLAPStack simulation framework and evaluates performance using Proximal Policy Optimization and adaptive heuristics.

Result: Flexible RL designs outperform heuristic baselines in service times but require longer convergence and are less stable. Guided designs offer stability but limited generalization.

Conclusion: The paper demonstrates the effectiveness of RL for charging strategy optimization, emphasizing trade-offs between flexibility and stability, and contributes a simulation framework extension and novel RL design.

Abstract: We propose a novel reinforcement learning (RL) design to optimize the
charging strategy for autonomous mobile robots in large-scale block stacking
warehouses. RL design involves a wide array of choices that can mostly only be
evaluated through lengthy experimentation. Our study focuses on how different
reward and action space configurations, ranging from flexible setups to more
guided, domain-informed design configurations, affect the agent performance.
Using heuristic charging strategies as a baseline, we demonstrate the
superiority of flexible, RL-based approaches in terms of service times.
Furthermore, our findings highlight a trade-off: While more open-ended designs
are able to discover well-performing strategies on their own, they may require
longer convergence times and are less stable, whereas guided configurations
lead to a more stable learning process but display a more limited
generalization potential. Our contributions are threefold. First, we extend
SLAPStack, an open-source, RL-compatible simulation-framework to accommodate
charging strategies. Second, we introduce a novel RL design for tackling the
charging strategy problem. Finally, we introduce several novel adaptive
baseline heuristics and reproducibly evaluate the design using a Proximal
Policy Optimization agent and varying different design configurations, with a
focus on reward.

</details>


### [272] [Feasibility with Language Models for Open-World Compositional Zero-Shot Learning](https://arxiv.org/pdf/2505.11181)
*Jae Myung Kim, Stephan Alaniz, Cordelia Schmid, Zeynep Akata*

Main category: cs.AI

TL;DR: FLM uses LLMs to assess state-object feasibility, improving Open-World Zero-Shot Learning performance.


<details>
  <summary>Details</summary>
Motivation: Zero-shot predictors struggle with unseen state-object combinations; FLM leverages LLMs to address this.

Method: FLM queries LLMs for feasibility of pairs, using logits for positive answers and in-context learning.

Result: FLM improves performance across benchmarks, with Vicuna and ChatGPT as top-performing models.

Conclusion: FLM effectively enhances OW-CZSL by leveraging LLMs for feasibility assessment.

Abstract: Humans can easily tell if an attribute (also called state) is realistic,
i.e., feasible, for an object, e.g. fire can be hot, but it cannot be wet. In
Open-World Compositional Zero-Shot Learning, when all possible state-object
combinations are considered as unseen classes, zero-shot predictors tend to
perform poorly. Our work focuses on using external auxiliary knowledge to
determine the feasibility of state-object combinations. Our Feasibility with
Language Model (FLM) is a simple and effective approach that leverages Large
Language Models (LLMs) to better comprehend the semantic relationships between
states and objects. FLM involves querying an LLM about the feasibility of a
given pair and retrieving the output logit for the positive answer. To mitigate
potential misguidance of the LLM given that many of the state-object
compositions are rare or completely infeasible, we observe that the in-context
learning ability of LLMs is essential. We present an extensive study
identifying Vicuna and ChatGPT as best performing, and we demonstrate that our
FLM consistently improves OW-CZSL performance across all three benchmarks.

</details>


### [273] [Can Global XAI Methods Reveal Injected Bias in LLMs? SHAP vs Rule Extraction vs RuleSHAP](https://arxiv.org/pdf/2505.11189)
*Francesco Sovrano*

Main category: cs.AI

TL;DR: The paper evaluates global XAI methods for detecting bias in LLMs, introduces a text-to-ordinal mapping strategy, and proposes RuleSHAP to improve bias detection.


<details>
  <summary>Details</summary>
Motivation: Generative AI systems can spread misinformation and biases, threatening SDGs. Current XAI tools are inadequate for LLMs.

Method: Uses text-to-ordinal mapping to enable XAI tools, injects biases into LLMs, and tests RuleFit and SHAP before introducing RuleSHAP.

Result: RuleFit struggles with complex biases; SHAP approximates but can't express them. RuleSHAP improves detection by +94% (MRR@1).

Conclusion: RuleSHAP enhances bias detection in LLMs, addressing limitations of existing XAI tools.

Abstract: Generative AI systems can help spread information but also misinformation and
biases, potentially undermining the UN Sustainable Development Goals (SDGs).
Explainable AI (XAI) aims to reveal the inner workings of AI systems and expose
misbehaviours or biases. However, current XAI tools, built for simpler models,
struggle to handle the non-numerical nature of large language models (LLMs).
This paper examines the effectiveness of global XAI methods, such as
rule-extraction algorithms and SHAP, in detecting bias in LLMs. To do so, we
first show a text-to-ordinal mapping strategy to convert non-numerical
inputs/outputs into numerical features, enabling these tools to identify (some)
misinformation-related biases in LLM-generated content. Then, we inject
non-linear biases of varying complexity (univariate, conjunctive, and
non-convex) into widespread LLMs like ChatGPT and Llama via system
instructions, using global XAI methods to detect them. This way, we found that
RuleFit struggles with conjunctive and non-convex biases, while SHAP can
approximate conjunctive biases but cannot express them as actionable rules.
Hence, we introduce RuleSHAP, a global rule extraction algorithm combining SHAP
and RuleFit to detect more non-univariate biases, improving injected bias
detection over RuleFit by +94% (MRR@1) on average.

</details>


### [274] [Multi-Modal Multi-Task (M3T) Federated Foundation Models for Embodied AI: Potentials and Challenges for Edge Integration](https://arxiv.org/pdf/2505.11191)
*Kasra Borazjani, Payam Abdisarabshali, Fardis Nadimi, Naji Khosravan, Minghui Liwang, Xianbin Wang, Yiguang Hong, Seyyedali Hosseinalipour*

Main category: cs.AI

TL;DR: The paper proposes Federated Foundation Models (FFMs) for embodied AI, combining Foundation Models (FMs) and Federated Learning (FL) to address generalization, personalization, and privacy challenges.


<details>
  <summary>Details</summary>
Motivation: The need for AI systems to learn from diverse inputs, adapt to user preferences, and operate safely under constraints like privacy and resource limitations.

Method: Introduces FFMs, unifying multi-modal multi-task FMs with FL, and outlines a framework (EMBODY) to address deployment challenges.

Result: A vision for FFMs in embodied AI, identifying challenges and research directions, along with an evaluation framework.

Conclusion: FFMs offer a promising paradigm for embodied AI, balancing generalization, personalization, and privacy, with actionable research directions outlined.

Abstract: As embodied AI systems become increasingly multi-modal, personalized, and
interactive, they must learn effectively from diverse sensory inputs, adapt
continually to user preferences, and operate safely under resource and privacy
constraints. These challenges expose a pressing need for machine learning
models capable of swift, context-aware adaptation while balancing model
generalization and personalization. Here, two methods emerge as suitable
candidates, each offering parts of these capabilities: Foundation Models (FMs)
provide a pathway toward generalization across tasks and modalities, whereas
Federated Learning (FL) offers the infrastructure for distributed,
privacy-preserving model updates and user-level model personalization. However,
when used in isolation, each of these approaches falls short of meeting the
complex and diverse capability requirements of real-world embodied
environments. In this vision paper, we introduce Federated Foundation Models
(FFMs) for embodied AI, a new paradigm that unifies the strengths of
multi-modal multi-task (M3T) FMs with the privacy-preserving distributed nature
of FL, enabling intelligent systems at the wireless edge. We collect critical
deployment dimensions of FFMs in embodied AI ecosystems under a unified
framework, which we name "EMBODY": Embodiment heterogeneity, Modality richness
and imbalance, Bandwidth and compute constraints, On-device continual learning,
Distributed control and autonomy, and Yielding safety, privacy, and
personalization. For each, we identify concrete challenges and envision
actionable research directions. We also present an evaluation framework for
deploying FFMs in embodied AI systems, along with the associated trade-offs.

</details>


### [275] [GLOVA: Global and Local Variation-Aware Analog Circuit Design with Risk-Sensitive Reinforcement Learning](https://arxiv.org/pdf/2505.11208)
*Dongjun Kim, Junwoo Park, Chaehyeon Shin, Jaeheon Jung, Kyungho Shin, Seungheon Baek, Sanghyuk Heo, Woongrae Kim, Inchul Jeong, Joohwan Cho, Jongsun Park*

Main category: cs.AI

TL;DR: GLOVA is a framework for analog circuit sizing that improves robustness against PVT variations using risk-sensitive reinforcement learning and ensemble-based critic, achieving significant efficiency gains.


<details>
  <summary>Details</summary>
Motivation: Addressing performance degradation from PVT variations and mismatches in real-world wafers, which current automated methods fail to thoroughly handle.

Method: Leverages risk-sensitive reinforcement learning and ensemble-based critic for sample-efficient learning, along with Œº-œÉ evaluation and simulation reordering for cost reduction.

Result: Achieves up to 80.5√ó improvement in sample efficiency and 76.0√ó reduction in time compared to prior methods.

Conclusion: GLOVA effectively manages PVT variations and mismatches, offering a robust and efficient solution for analog circuit design.

Abstract: Analog/mixed-signal circuit design encounters significant challenges due to
performance degradation from process, voltage, and temperature (PVT)
variations. To achieve commercial-grade reliability, iterative manual design
revisions and extensive statistical simulations are required. While several
studies have aimed to automate variation aware analog design to reduce
time-to-market, the substantial mismatches in real-world wafers have not been
thoroughly addressed. In this paper, we present GLOVA, an analog circuit sizing
framework that effectively manages the impact of diverse random mismatches to
improve robustness against PVT variations. In the proposed approach,
risk-sensitive reinforcement learning is leveraged to account for the
reliability bound affected by PVT variations, and ensemble-based critic is
introduced to achieve sample-efficient learning. For design verification, we
also propose $\mu$-$\sigma$ evaluation and simulation reordering method to
reduce simulation costs of identifying failed designs. GLOVA supports
verification through industrial-level PVT variation evaluation methods,
including corner simulation as well as global and local Monte Carlo (MC)
simulations. Compared to previous state-of-the-art variation-aware analog
sizing frameworks, GLOVA achieves up to 80.5$\times$ improvement in sample
efficiency and 76.0$\times$ reduction in time.

</details>


### [276] [Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs](https://arxiv.org/pdf/2505.11227)
*Zhangying Feng, Qianglong Chen, Ning Lu, Yongqian Li, Siqi Cheng, Shuangmu Peng, Duyu Tang, Shengcai Liu, Zhirui Zhang*

Main category: cs.AI

TL;DR: Pure RL training enhances reasoning in LLMs without PRMs, challenging their necessity. Self-PRM improves accuracy but struggles with precision on hard problems.


<details>
  <summary>Details</summary>
Motivation: To investigate if PRMs are essential for reasoning in LLMs and explore the synergy between RL training and PRM capabilities.

Method: Systematic study of RL training and PRM capabilities, proposing Self-PRM for autonomous solution evaluation.

Result: Pure RL improves reasoning and PRM capabilities; Self-PRM boosts accuracy but has low precision on difficult problems.

Conclusion: PRMs may not be essential; pure RL fosters reasoning and PRM capabilities, though challenges remain in precision and reward alignment.

Abstract: The development of reasoning capabilities represents a critical frontier in
large language models (LLMs) research, where reinforcement learning (RL) and
process reward models (PRMs) have emerged as predominant methodological
frameworks. Contrary to conventional wisdom, empirical evidence from
DeepSeek-R1 demonstrates that pure RL training focused on mathematical
problem-solving can progressively enhance reasoning abilities without PRM
integration, challenging the perceived necessity of process supervision. In
this study, we conduct a systematic investigation of the relationship between
RL training and PRM capabilities. Our findings demonstrate that problem-solving
proficiency and process supervision capabilities represent complementary
dimensions of reasoning that co-evolve synergistically during pure RL training.
In particular, current PRMs underperform simple baselines like majority voting
when applied to state-of-the-art models such as DeepSeek-R1 and QwQ-32B. To
address this limitation, we propose Self-PRM, an introspective framework in
which models autonomously evaluate and rerank their generated solutions through
self-reward mechanisms. Although Self-PRM consistently improves the accuracy of
the benchmark (particularly with larger sample sizes), analysis exposes
persistent challenges: The approach exhibits low precision (<10\%) on difficult
problems, frequently misclassifying flawed solutions as valid. These analyses
underscore the need for continued RL scaling to improve reward alignment and
introspective accuracy. Overall, our findings suggest that PRM may not be
essential for enhancing complex reasoning, as pure RL not only improves
problem-solving skills but also inherently fosters robust PRM capabilities. We
hope these findings provide actionable insights for building more reliable and
self-aware complex reasoning models.

</details>


### [277] [LD-Scene: LLM-Guided Diffusion for Controllable Generation of Adversarial Safety-Critical Driving Scenarios](https://arxiv.org/pdf/2505.11247)
*Mingxing Peng, Yuting Xie, Xusen Guo, Ruoyu Yao, Hai Yang, Jun Ma*

Main category: cs.AI

TL;DR: LD-Scene integrates LLMs and LDMs for user-controllable adversarial scenario generation in autonomous driving, improving realism and diversity.


<details>
  <summary>Details</summary>
Motivation: Safety-critical scenarios are rare and hard to collect, making evaluation of autonomous vehicles challenging. Existing methods lack controllability and user-friendliness.

Method: Combines Latent Diffusion Models (LDMs) for realistic trajectory distributions and LLMs to translate user queries into adversarial loss functions. Includes CoT code generator and debugger for robustness.

Result: Achieves state-of-the-art performance in generating realistic, diverse, and effective adversarial scenarios on the nuScenes dataset.

Conclusion: LD-Scene enhances controllability and robustness in scenario generation, enabling tailored testing for autonomous driving systems.

Abstract: Ensuring the safety and robustness of autonomous driving systems necessitates
a comprehensive evaluation in safety-critical scenarios. However, these
safety-critical scenarios are rare and difficult to collect from real-world
driving data, posing significant challenges to effectively assessing the
performance of autonomous vehicles. Typical existing methods often suffer from
limited controllability and lack user-friendliness, as extensive expert
knowledge is essentially required. To address these challenges, we propose
LD-Scene, a novel framework that integrates Large Language Models (LLMs) with
Latent Diffusion Models (LDMs) for user-controllable adversarial scenario
generation through natural language. Our approach comprises an LDM that
captures realistic driving trajectory distributions and an LLM-based guidance
module that translates user queries into adversarial loss functions,
facilitating the generation of scenarios aligned with user queries. The
guidance module integrates an LLM-based Chain-of-Thought (CoT) code generator
and an LLM-based code debugger, enhancing the controllability and robustness in
generating guidance functions. Extensive experiments conducted on the nuScenes
dataset demonstrate that LD-Scene achieves state-of-the-art performance in
generating realistic, diverse, and effective adversarial scenarios.
Furthermore, our framework provides fine-grained control over adversarial
behaviors, thereby facilitating more effective testing tailored to specific
driving scenarios.

</details>


### [278] [SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning](https://arxiv.org/pdf/2505.11274)
*Zheng Li, Qingxiu Dong, Jingyuan Ma, Di Zhang, Zhifang Sui*

Main category: cs.AI

TL;DR: SelfBudgeter is a self-adaptive strategy for efficient reasoning, reducing resource waste by pre-estimating query difficulty and using budget-guided reinforcement learning to maintain accuracy while compressing response length.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models inefficiently process both trivial and complex queries, wasting resources and increasing latency.

Method: Dual-phase training: pre-estimating reasoning cost and budget-guided GPRO for reinforcement learning to control output length.

Result: Achieves up to 74.47% response length compression on MATH benchmark with nearly undiminished accuracy.

Conclusion: SelfBudgeter efficiently allocates budgets based on problem complexity, improving resource use and user control.

Abstract: Recently, large reasoning models demonstrate exceptional performance on
various tasks. However, reasoning models inefficiently over-process both
trivial and complex queries, leading to resource waste and prolonged user
latency. To address this challenge, we propose SelfBudgeter - a self-adaptive
controllable reasoning strategy for efficient reasoning. Our approach adopts a
dual-phase training paradigm: first, the model learns to pre-estimate the
reasoning cost based on the difficulty of the query. Then, we introduce
budget-guided GPRO for reinforcement learning, which effectively maintains
accuracy while reducing output length. SelfBudgeter allows users to anticipate
generation time and make informed decisions about continuing or interrupting
the process. Furthermore, our method enables direct manipulation of reasoning
length via pre-filling token budget. Experimental results demonstrate that
SelfBudgeter can rationally allocate budgets according to problem complexity,
achieving up to 74.47% response length compression on the MATH benchmark while
maintaining nearly undiminished accuracy.

</details>


### [279] [Meta-World+: An Improved, Standardized, RL Benchmark](https://arxiv.org/pdf/2505.11289)
*Reginald McLean, Evangelos Chatzaroulas, Luc McCutcheon, Frank R√∂der, Tianhe Yu, Zhanpeng He, K. R. Zentner, Ryan Julian, J K Terry, Isaac Woungang, Nariman Farsad, Pablo Samuel Castro*

Main category: cs.AI

TL;DR: The paper clarifies undocumented changes in Meta-World, releases a new version for reproducibility, and improves usability.


<details>
  <summary>Details</summary>
Motivation: To address inconsistencies in Meta-World evaluations and improve benchmark design for multi-task and meta-reinforcement learning.

Method: Analyzed past versions of Meta-World, resolved ambiguities, and developed a new open-source version with enhanced features.

Result: A reproducible, ergonomic, and customizable version of Meta-World is released.

Conclusion: The new Meta-World version ensures fair comparisons and better control for users, advancing multi-task and meta-reinforcement learning research.

Abstract: Meta-World is widely used for evaluating multi-task and meta-reinforcement
learning agents, which are challenged to master diverse skills simultaneously.
Since its introduction however, there have been numerous undocumented changes
which inhibit a fair comparison of algorithms. This work strives to
disambiguate these results from the literature, while also leveraging the past
versions of Meta-World to provide insights into multi-task and
meta-reinforcement learning benchmark design. Through this process we release a
new open-source version of Meta-World
(https://github.com/Farama-Foundation/Metaworld/) that has full reproducibility
of past results, is more technically ergonomic, and gives users more control
over the tasks that are included in a task set.

</details>


### [280] [Extracting Explainable Dates From Medical Images By Reverse-Engineering UNIX Timestamps](https://arxiv.org/pdf/2505.11451)
*Lee Harris, James Bentham, Philippe De Wilde*

Main category: cs.AI

TL;DR: The paper explores using regular expression synthesis to improve date extraction from medical documents, balancing precision and recall.


<details>
  <summary>Details</summary>
Motivation: Dates are crucial for medical decisions, but extracting them accurately is challenging. Existing methods (AI models or regex) lack precision or explainability.

Method: Tested public regex, created manual regex, and used regex synthesis to generate better patterns from reverse-engineered UNIX timestamps.

Result: Synthesized regex reduced false positives (text resembling dates) but slightly increased missed dates compared to manual regex.

Conclusion: Regex synthesis can effectively identify complex dates in text, offering a novel approach to learning deterministic logic.

Abstract: Dates often contribute towards highly impactful medical decisions, but it is
rarely clear how to extract this data. AI has only just begun to be used
transcribe such documents, and common methods are either to trust that the
output produced by a complex AI model, or to parse the text using regular
expressions. Recent work has established that regular expressions are an
explainable form of logic, but it is difficult to decompose these into the
component parts that are required to construct precise UNIX timestamps. First,
we test publicly-available regular expressions, and we found that these were
unable to capture a significant number of our dates. Next, we manually created
easily-decomposable regular expressions, and we found that these were able to
detect the majority of real dates, but also a lot of sequences of text that
look like dates. Finally, we used regular expression synthesis to automatically
identify regular expressions from the reverse-engineered UNIX timestamps that
we created. We find that regular expressions created by regular expression
synthesis detect far fewer sequences of text that look like dates than those
that were manually created, at the cost of a slight increase to the number of
missed dates. Overall, our results show that regular expressions can be created
through regular expression synthesis to identify complex dates and date ranges
in text transcriptions. To our knowledge, our proposed way of learning
deterministic logic by reverse-engineering several many-one mappings and
feeding these into a regular expression synthesiser is a new approach.

</details>


### [281] [Automatic Reward Shaping from Confounded Offline Data](https://arxiv.org/pdf/2505.11478)
*Mingxuan Li, Junzhe Zhang, Elias Bareinboim*

Main category: cs.AI

TL;DR: The paper proposes a novel deep reinforcement learning algorithm robust to confounding biases in observed data, outperforming standard DQN in confounded Atari games.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of off-policy learning in high-dimensional domains with unobserved confounding, where standard methods like DQN may fail.

Method: Extends DQN to find a safe policy for the worst-case environment compatible with observations, handling confounding biases.

Result: The proposed algorithm consistently outperforms standard DQN in confounded Atari games with mismatched inputs and unobserved confounders.

Conclusion: The method effectively mitigates confounding biases, demonstrating superior performance in complex, high-dimensional domains.

Abstract: A key task in Artificial Intelligence is learning effective policies for
controlling agents in unknown environments to optimize performance measures.
Off-policy learning methods, like Q-learning, allow learners to make optimal
decisions based on past experiences. This paper studies off-policy learning
from biased data in complex and high-dimensional domains where \emph{unobserved
confounding} cannot be ruled out a priori. Building on the well-celebrated Deep
Q-Network (DQN), we propose a novel deep reinforcement learning algorithm
robust to confounding biases in observed data. Specifically, our algorithm
attempts to find a safe policy for the worst-case environment compatible with
the observations. We apply our method to twelve confounded Atari games, and
find that it consistently dominates the standard DQN in all games where the
observed input to the behavioral and target policies mismatch and unobserved
confounders exist.

</details>


### [282] [MOSAAIC: Managing Optimization towards Shared Autonomy, Authority, and Initiative in Co-creation](https://arxiv.org/pdf/2505.11481)
*Alayt Issak, Jeba Rezwana, Casper Harteveld*

Main category: cs.AI

TL;DR: The paper introduces MOSAAIC, a framework for balancing control between humans and AI in co-creativity, focusing on autonomy, initiative, and authority.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of balancing human and AI control in co-creative processes.

Method: Conducted a systematic literature review of 172 papers and developed the MOSAAIC framework.

Result: MOSAAIC characterizes control dimensions and offers optimization strategies, validated through case studies.

Conclusion: MOSAAIC provides a practical tool for managing control in co-creativity, enhancing collaboration between humans and AI.

Abstract: Striking the appropriate balance between humans and co-creative AI is an open
research question in computational creativity. Co-creativity, a form of hybrid
intelligence where both humans and AI take action proactively, is a process
that leads to shared creative artifacts and ideas. Achieving a balanced dynamic
in co-creativity requires characterizing control and identifying strategies to
distribute control between humans and AI. We define control as the power to
determine, initiate, and direct the process of co-creation. Informed by a
systematic literature review of 172 full-length papers, we introduce MOSAAIC
(Managing Optimization towards Shared Autonomy, Authority, and Initiative in
Co-creation), a novel framework for characterizing and balancing control in
co-creation. MOSAAIC identifies three key dimensions of control: autonomy,
initiative, and authority. We supplement our framework with control
optimization strategies in co-creation. To demonstrate MOSAAIC's applicability,
we analyze the distribution of control in six existing co-creative AI case
studies and present the implications of using this framework.

</details>


### [283] [A-I-RAVEN and I-RAVEN-Mesh: Two New Benchmarks for Abstract Visual Reasoning](https://arxiv.org/pdf/2406.11061)
*Miko≈Çaj Ma≈Çki≈Ñski, Jacek Ma≈Ñdziuk*

Main category: cs.AI

TL;DR: The paper introduces two datasets (A-I-RAVEN and I-RAVEN-Mesh) to test generalization and knowledge transfer in deep neural networks for abstract visual reasoning, revealing model shortcomings.


<details>
  <summary>Details</summary>
Motivation: To assess generalization and knowledge reuse in deep neural networks for abstract visual reasoning, using RPMs as a benchmark.

Method: Introduces A-I-RAVEN for systematic generalization testing and I-RAVEN-Mesh for progressive knowledge transfer evaluation, testing 13 models.

Result: Identifies specific shortcomings in generalization and knowledge transfer among the evaluated models.

Conclusion: The new datasets provide robust tools for assessing AVR capabilities, highlighting areas for model improvement.

Abstract: We study generalization and knowledge reuse capabilities of deep neural
networks in the domain of abstract visual reasoning (AVR), employing Raven's
Progressive Matrices (RPMs), a recognized benchmark task for assessing AVR
abilities. Two knowledge transfer scenarios referring to the I-RAVEN dataset
are investigated. Firstly, inspired by generalization assessment capabilities
of the PGM dataset and popularity of I-RAVEN, we introduce
Attributeless-I-RAVEN (A-I-RAVEN), a benchmark with 10 generalization regimes
that allow to systematically test generalization of abstract rules applied to
held-out attributes at various levels of complexity (primary and extended
regimes). In contrast to PGM, A-I-RAVEN features compositionality, a variety of
figure configurations, and does not require substantial computational
resources. Secondly, we construct I-RAVEN-Mesh, a dataset that enriches RPMs
with a novel component structure comprising line-based patterns, facilitating
assessment of progressive knowledge acquisition in transfer learning setting.
We evaluate 13 strong models from the AVR literature on the introduced
datasets, revealing their specific shortcomings in generalization and knowledge
transfer.

</details>


### [284] [A Novel Mathematical Framework for Objective Characterization of Ideas](https://arxiv.org/pdf/2409.07578)
*B. Sankar, Dibakar Sen*

Main category: cs.AI

TL;DR: A mathematical framework automates the evaluation of ideas from conversational AI or humans, aiding novice designers by objectively measuring diversity and selecting promising ideas.


<details>
  <summary>Details</summary>
Motivation: The need for objective, automated evaluation of ideas generated by CAI or humans, addressing human judgment limitations like bias and oversight.

Method: Converts ideas into higher-dimensional vectors and uses tools like UMAP, DBSCAN, and PCA to quantitatively measure diversity and select promising ideas.

Result: Provides a reliable, objective method for idea selection, enhancing ideation efficiency.

Conclusion: The framework improves the ideation phase by offering an automated, unbiased way to evaluate and select ideas.

Abstract: The demand for innovation in product design necessitates a prolific ideation
phase. Conversational AI (CAI) systems that use Large Language Models (LLMs)
such as GPT (Generative Pre-trained Transformer) have been shown to be fruitful
in augmenting human creativity, providing numerous novel and diverse ideas.
Despite the success in ideation quantity, the qualitative assessment of these
ideas remains challenging and traditionally reliant on expert human evaluation.
This method suffers from limitations such as human judgment errors, bias, and
oversight. Addressing this gap, our study introduces a comprehensive
mathematical framework for automated analysis to objectively evaluate the
plethora of ideas generated by CAI systems and/or humans. This framework is
particularly advantageous for novice designers who lack experience in selecting
promising ideas. By converting the ideas into higher dimensional vectors and
quantitatively measuring the diversity between them using tools such as UMAP,
DBSCAN and PCA, the proposed method provides a reliable and objective way of
selecting the most promising ideas, thereby enhancing the efficiency of the
ideation phase.

</details>


### [285] [TestAgent: A Framework for Domain-Adaptive Evaluation of LLMs via Dynamic Benchmark Construction and Exploratory Interaction](https://arxiv.org/pdf/2410.11507)
*Wanying Wang, Zeyu Ma, Pengfei Liu, Mingang Chen*

Main category: cs.AI

TL;DR: The paper introduces ‚ÄúTestAgent,‚Äù an agent-based framework for dynamic evaluation of LLMs across domains, using Benchmark+ and Assessment+ concepts for flexible and comprehensive assessment.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluation methods are static, resource-heavy, and lack cross-domain adaptability, limiting real-world applicability.

Method: Proposes ‚ÄúTestAgent‚Äù with Benchmark+ (flexible strategy-criterion format) and Assessment+ (enhanced interaction), leveraging retrieval-augmented generation and reinforcement learning.

Result: Experiments show TestAgent‚Äôs effectiveness in dynamic benchmark generation and cross-domain evaluation.

Conclusion: TestAgent offers a novel approach for domain-adaptive LLM evaluation, enabling dynamic benchmarks and exploratory assessment.

Abstract: As large language models (LLMs) are increasingly deployed to various vertical
domains, automatically evaluating their performance across different domains
remains a critical challenge. Current evaluation methods often rely on static
and resource-intensive datasets that are not aligned with real-world
requirements and lack cross-domain adaptability. To address these limitations,
we revisit the evaluation process and introduce two key concepts:
\textbf{Benchmark+}, which extends the traditional question-answer benchmark
into a more flexible ``strategy-criterion'' format; and \textbf{Assessment+},
which enhances the interaction process to facilitate deeper exploration and
comprehensive analysis from multiple perspectives. We propose
\textbf{\textsc{TestAgent}}, an agent-based evaluation framework that
implements these concepts using retrieval-augmented generation and
reinforcement learning. \textsc{TestAgent} enables automatic dynamic benchmark
generation and in-depth assessment across diverse vertical domains. Experiments
on tasks ranging from constructing multiple vertical domain evaluations to
transforming static benchmarks into dynamic forms demonstrate the effectiveness
of \textsc{TestAgent}. This work provides a novel perspective on automatic
evaluation methods for domain-specific LLMs, offering a pathway for
domain-adaptive dynamic benchmark construction and exploratory assessment.

</details>


### [286] [Brain-like variational inference](https://arxiv.org/pdf/2410.19315)
*Hadi Vafaii, Dekel Galor, Jacob L. Yates*

Main category: cs.AI

TL;DR: The paper introduces iP-VAE, a spiking neural network model for variational inference, derived from natural gradient descent on variational free energy. It outperforms standard VAEs and predictive coding models in sparsity, reconstruction, and biological plausibility.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between theoretical equivalence of inference in brains and machines (via ELBO and variational free energy) and its neural implementation.

Method: Online natural gradient descent on variational free energy under Poisson assumptions, leading to the iP-VAE model with local updates and spiking dynamics.

Result: iP-VAE shows emergent normalization, hardware-efficient spike counts, and outperforms standard VAEs and predictive coding models in sparsity, reconstruction, and generalization.

Conclusion: Deriving inference algorithms from first principles yields biologically plausible and effective architectures, as demonstrated by iP-VAE.

Abstract: Inference in both brains and machines can be formalized by optimizing a
shared objective: maximizing the evidence lower bound (ELBO) in machine
learning, or minimizing variational free energy (F) in neuroscience (ELBO =
-F). While this equivalence suggests a unifying framework, it leaves open how
inference is implemented in neural systems. Here, we show that online natural
gradient descent on F, under Poisson assumptions, leads to a recurrent spiking
neural network that performs variational inference via membrane potential
dynamics. The resulting model -- the iterative Poisson variational autoencoder
(iP-VAE) -- replaces the encoder network with local updates derived from
natural gradient descent on F. Theoretically, iP-VAE yields a number of
desirable features such as emergent normalization via lateral competition, and
hardware-efficient integer spike count representations. Empirically, iP-VAE
outperforms both standard VAEs and Gaussian-based predictive coding models in
sparsity, reconstruction, and biological plausibility. iP-VAE also exhibits
strong generalization to out-of-distribution inputs, exceeding hybrid
iterative-amortized VAEs. These results demonstrate how deriving inference
algorithms from first principles can yield concrete architectures that are
simultaneously biologically plausible and empirically effective.

</details>


### [287] [Infrastructure for AI Agents](https://arxiv.org/pdf/2501.10114)
*Alan Chan, Kevin Wei, Sihao Huang, Nitarshan Rajkumar, Elija Perrier, Seth Lazar, Gillian K. Hadfield, Markus Anderljung*

Main category: cs.AI

TL;DR: The paper proposes 'agent infrastructure'‚Äîexternal systems and protocols to mediate AI agent interactions, focusing on attribution, interaction shaping, and harm detection.


<details>
  <summary>Details</summary>
Motivation: Current focus on modifying agent behavior directly is insufficient; external protocols and systems are needed to manage interactions and impacts in open-ended environments.

Method: Introduces the concept of agent infrastructure, outlining three key functions: attribution, interaction shaping, and harm detection. Provides a catalog of research directions for these functions.

Result: Identifies gaps and open questions in agent infrastructure, emphasizing its necessity for ecosystems of advanced AI agents.

Conclusion: Agent infrastructure is indispensable for managing AI agent interactions, akin to internet protocols, and requires further research to prepare society for advanced agents.

Abstract: AI agents plan and execute interactions in open-ended environments. For
example, OpenAI's Operator can use a web browser to do product comparisons and
buy online goods. To facilitate beneficial interactions and mitigate harmful
ones, much research focuses on directly modifying agent behaviour. For example,
developers can train agents to follow user instructions. This focus on direct
modifications is useful, but insufficient. We will also need external protocols
and systems that shape how agents interact with institutions and other actors.
For instance, agents will need more efficient protocols to communicate with
each other and form agreements. In addition, attributing an agent's actions to
a particular human or other legal entity can help to establish trust, and also
disincentivize misuse. Given this motivation, we propose the concept of agent
infrastructure: technical systems and shared protocols external to agents that
are designed to mediate and influence their interactions with and impacts on
their environments. Just as the Internet relies on protocols like HTTPS, our
work argues that agent infrastructure will be similarly indispensable to
ecosystems of agents. We identify three functions for agent infrastructure: 1)
attributing actions, properties, and other information to specific agents,
their users, or other actors; 2) shaping agents' interactions; and 3) detecting
and remedying harmful actions from agents. We provide an incomplete catalog of
research directions for such functions. For each direction, we include analysis
of use cases, infrastructure adoption, relationships to existing (internet)
infrastructure, limitations, and open questions. Making progress on agent
infrastructure can prepare society for the adoption of more advanced agents.

</details>


### [288] [MIR-Bench: Can Your LLM Recognize Complicated Patterns via Many-Shot In-Context Reasoning?](https://arxiv.org/pdf/2502.09933)
*Kai Yan, Zhan Ling, Kang Liu, Yifan Yang, Ting-Han Fan, Lingfeng Shen, Zhengyin Du, Jiecao Chen*

Main category: cs.AI

TL;DR: MIR-Bench is introduced as the first many-shot in-context reasoning benchmark for LLMs, addressing gaps in pattern recognition and long-context information integration.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack evaluation for aggregating information from long contexts and focus on few-shot settings, while many-shot ICL lacks tasks requiring complex intelligence.

Method: Proposes MIR-Bench, a benchmark for many-shot in-context reasoning, evaluating LLMs on pattern recognition via diverse input-output examples.

Result: Insights include scaling effects, robustness, inductive vs. transductive reasoning, RAG, coding for reasoning, and cross-domain generalizability.

Conclusion: MIR-Bench fills a critical gap, enabling deeper study of many-shot in-context reasoning and LLM capabilities.

Abstract: The ability to recognize patterns from examples and apply them to new ones is
a primal ability for general intelligence, and is widely studied by psychology
and AI researchers. Many benchmarks have been proposed to measure such ability
for Large Language Models (LLMs); however, they focus on few-shot (usually <10)
setting and lack evaluation for aggregating many pieces of information from
long contexts. On the other hand, the ever-growing context length of LLMs have
brought forth the novel paradigm of many-shot In-Context Learning (ICL), which
addresses new tasks with hundreds to thousands of examples without expensive
and inefficient fine-tuning. However, many-shot evaluations often focus on
classification, and popular long-context LLM tasks such as Needle-In-A-Haystack
(NIAH) seldom require complicated intelligence for integrating many pieces of
information. To fix the issues from both worlds, we propose MIR-Bench, the
first many-shot in-context reasoning benchmark for pattern recognition that
asks LLM to predict output via input-output examples from underlying functions
with diverse data format. Based on MIR-Bench, we study many novel problems for
many-shot in-context reasoning, and acquired many insightful findings including
scaling effect, robustness, inductive vs. transductive reasoning, retrieval
Augmented Generation (RAG), coding for inductive reasoning, cross-domain
generalizability, etc.

</details>


### [289] [Quantifying the Capability Boundary of DeepSeek Models: An Application-Driven Performance Analysis](https://arxiv.org/pdf/2502.11164)
*Kaikai Zhao, Zhaoxiang Liu, Xuejiao Lei, Jiaojiao Zhao, Zhenhong Long, Zipeng Wang, Ning Wang, Meijuan An, Qingliang Meng, Peijun Yang, Minjie Hua, Chaoyang Ma, Wen Liu, Kai Wang, Shiguo Lian*

Main category: cs.AI

TL;DR: The paper evaluates DeepSeek models comprehensively, revealing insights on performance, reasoning enhancements, and quantization effects, and provides a model selection guide.


<details>
  <summary>Details</summary>
Motivation: Address the lack of detailed evaluations for DeepSeek models in real-world applications to help users choose the most suitable models.

Method: Conduct a systematic evaluation using the A-Eval-2.0 benchmark on various DeepSeek models, including reasoning-enhanced and quantized versions.

Result: Key findings include the impact of model size, reasoning enhancements, and quantization on performance, with distillation and reasoning enhancements showing higher gains for harder tasks.

Conclusion: The study provides actionable insights and a model selection handbook to help users choose cost-effective models based on their needs.

Abstract: DeepSeek-R1, known for its low training cost and exceptional reasoning
capabilities, has achieved state-of-the-art performance on various benchmarks.
However, detailed evaluations for DeepSeek Series models from the perspective
of real-world applications are lacking, making it challenging for users to
select the most suitable DeepSeek models for their specific needs. To address
this gap, we presents the first comprehensive evaluation of the DeepSeek and
its related models (including DeepSeek-V3, DeepSeek-R1,
DeepSeek-R1-Distill-Qwen series, DeepSeek-R1-Distill-Llama series, their
corresponding 4-bit quantized models, and the reasoning model QwQ-32B) using
our enhanced A-Eval benchmark, A-Eval-2.0. Our systematic analysis reveals
several key insights: (1) Given identical model architectures and training
data, larger parameter models demonstrate superior performance, aligning with
the scaling law. However, smaller models may achieve enhanced capabilities when
employing optimized training strategies and higher-quality data; (2)
Reasoning-enhanced model show significant performance gains in logical
reasoning tasks but may underperform in text understanding and generation
tasks; (3) As the data difficulty increases, distillation or reasoning
enhancements yield higher performance gains for the models. Interestingly,
reasoning enhancements can even have a negative impact on simpler problems; (4)
Quantization impacts different capabilities unevenly, with significant drop on
logical reasoning and minimal impact on text generation. Based on these results
and findings, we design an model selection handbook enabling users to select
the most cost-effective models without efforts.

</details>


### [290] [SOLAR: Scalable Optimization of Large-scale Architecture for Reasoning](https://arxiv.org/pdf/2503.04530)
*Chen Li, Yinyi Luo, Anudeep Bolimera, Uzair Ahmed, Shri Kiran Srinivasan, Hrishikesh Gokhale, Marios Savvides*

Main category: cs.AI

TL;DR: SOLAR optimizes reasoning topologies (CoT, ToT, GoT) dynamically, enhancing accuracy and efficiency. It introduces TAG for dataset automation and Topological-Scaling for adaptive learning. Results show significant accuracy gains and reduced latency, with M-TRM outperforming single-task models.


<details>
  <summary>Details</summary>
Motivation: Current LLMs rely on fixed reasoning structures like Chain-of-Thought, limiting performance on complex tasks. SOLAR addresses this by dynamically optimizing topologies for better reasoning.

Method: SOLAR uses TAG for automated dataset creation and Topological-Scaling for adaptive learning. It introduces M-TRM for optimal topology and answer selection in one pass.

Result: Notable gains: +5% to +10.02% accuracy, reduced response length by 5%, lower latency. M-TRM improves accuracy by +10% and rank correlation by +9%.

Conclusion: SOLAR sets a new benchmark for scalable, high-precision LLM reasoning with fully automated dynamic topology optimization.

Abstract: Large Language Models excel in reasoning yet often rely on Chain-of-Thought
prompts, limiting performance on tasks demanding more nuanced topological
structures. We present SOLAR (Scalable Optimization of Large-scale Architecture
for Reasoning), a framework that dynamically optimizes Chain-of-Thought (CoT),
Tree-of-Thought (ToT), and Graph-of-Thought (GoT) topologies to boost accuracy
and efficiency. Our Topological-Annotation-Generation (TAG) system automates
dataset creation, annotation, and difficulty segmentation, leading to stronger
post training and test-time performance. We also propose Topological-Scaling, a
curriculum-learning-based approach that adaptively combines post training and
inference scaling to each task. On MATH and GSM8K, SOLAR delivers notable
gains: +5% accuracy with Topological Tuning, +9% with Topological Rewarding,
and +10.02% with Hybrid Scaling, while reducing response length by over 5%,
lowering inference latency. To further enhance efficiency, we introduce a
multi-task Topological Reward Model (M-TRM) that selects both the optimal
reasoning topology and final answer in a single pass, eliminating multiple
single-task TRMs. Remarkably, M-TRM also surpasses all single-task TRMs,
improving accuracy by +10% and rank correlation by +9%. Overall, SOLAR
establishes a new benchmark for scalable, high-precision LLM reasoning and
introduces a fully automated, dynamic topology competition mechanism.

</details>


### [291] [EIAD: Explainable Industrial Anomaly Detection Via Multi-Modal Large Language Models](https://arxiv.org/pdf/2503.14162)
*Zongyun Zhang, Jiacheng Ruan, Xian Gao, Ting Liu, Yuzhuo Fu*

Main category: cs.AI

TL;DR: The paper proposes a novel multi-modal approach (EIAD) for industrial anomaly detection, decoupling dialog and feature extraction, and introduces a new dataset (DDQA) for improved performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot methods lack detailed defect descriptions, and multi-modal models struggle with balancing QA performance and grounding capabilities due to overfitting.

Method: Introduces a multi-modal defect localization module with independent optimization and tailored learning strategies, and creates the DDQA dataset for authentic training.

Result: EIAD achieves outstanding performance in defect detection and localization, enhancing accuracy and interpretability.

Conclusion: EIAD shows strong potential for practical industrial applications due to its improved performance and reliability.

Abstract: Industrial Anomaly Detection (IAD) is critical to ensure product quality
during manufacturing. Although existing zero-shot defect segmentation and
detection methods have shown effectiveness, they cannot provide detailed
descriptions of the defects. Furthermore, the application of large multi-modal
models in IAD remains in its infancy, facing challenges in balancing
question-answering (QA) performance and mask-based grounding capabilities,
often owing to overfitting during the fine-tuning process. To address these
challenges, we propose a novel approach that introduces a dedicated multi-modal
defect localization module to decouple the dialog functionality from the core
feature extraction. This decoupling is achieved through independent
optimization objectives and tailored learning strategies. Additionally, we
contribute to the first multi-modal industrial anomaly detection training
dataset, named Defect Detection Question Answering (DDQA), encompassing a wide
range of defect types and industrial scenarios. Unlike conventional datasets
that rely on GPT-generated data, DDQA ensures authenticity and reliability and
offers a robust foundation for model training. Experimental results demonstrate
that our proposed method, Explainable Industrial Anomaly Detection Assistant
(EIAD), achieves outstanding performance in defect detection and localization
tasks. It not only significantly enhances accuracy but also improves
interpretability. These advancements highlight the potential of EIAD for
practical applications in industrial settings.

</details>


### [292] [A Computational Theory for Efficient Mini Agent Evaluation with Causal Guarantees](https://arxiv.org/pdf/2503.21138)
*Hedong Yan*

Main category: cs.AI

TL;DR: A computational theory for evaluating mini agents reduces costs by building evaluation models, proving error bounds and efficiency, and using a meta-learner for heterogeneous agents. Results show significant error reduction and time savings.


<details>
  <summary>Details</summary>
Motivation: To reduce the high cost of experimental evaluation for agents by introducing a computational evaluation theory.

Method: Proposes a meta-learner for heterogeneous agents, builds evaluation models, and proves error bounds and efficiency.

Result: Reduced evaluation errors by 24.1% to 99.0% across 12 scenes and cut evaluation time by 3 to 7 orders of magnitude.

Conclusion: The method effectively accelerates and improves agent evaluation, offering practical benefits across diverse applications.

Abstract: In order to reduce the cost of experimental evaluation for agents, we
introduce a computational theory of evaluation for mini agents: build
evaluation model to accelerate the evaluation procedures. We prove upper bounds
of generalized error and generalized causal effect error of given evaluation
models for infinite agents. We also prove efficiency, and consistency to
estimated causal effect from deployed agents to evaluation metric by
prediction. To learn evaluation models, we propose a meta-learner to handle
heterogeneous agents space problem. Comparing with existed evaluation
approaches, our (conditional) evaluation model reduced 24.1\% to 99.0\%
evaluation errors across 12 scenes, including individual medicine, scientific
simulation, social experiment, business activity, and quantum trade. The
evaluation time is reduced 3 to 7 order of magnitude per subject comparing with
experiments or simulations.

</details>


### [293] [Among Us: A Sandbox for Measuring and Detecting Agentic Deception](https://arxiv.org/pdf/2504.04072)
*Satvik Golechha, Adri√† Garriga-Alonso*

Main category: cs.AI

TL;DR: The paper introduces a sandbox game, Among Us, to study open-ended deception in LLM-agents, revealing RL-trained models are better at deception than detection. Detection methods like logistic regression and SAEs show promise but don't reduce lying.


<details>
  <summary>Details</summary>
Motivation: Prior studies limit deception assessment to binary or short-term scenarios, lacking open-ended, long-term deceptive behavior analysis in AI agents.

Method: The study uses the Among Us game to evaluate 18 LLMs, testing their deception and detection abilities with logistic regression and sparse autoencoders (SAEs).

Result: RL-trained models excel at deception but struggle with detection. Probes trained on dishonest behavior generalize well (AUROCs >95%), but SAEs can't reduce lying.

Conclusion: The sandbox, logs, and probes aim to help anticipate and mitigate deceptive behavior in language-based AI agents.

Abstract: Prior studies on deception in language-based AI agents typically assess
whether the agent produces a false statement about a topic, or makes a binary
choice prompted by a goal, rather than allowing open-ended deceptive behavior
to emerge in pursuit of a longer-term goal. To fix this, we introduce
$\textit{Among Us}$, a sandbox social deception game where LLM-agents exhibit
long-term, open-ended deception as a consequence of the game objectives. While
most benchmarks saturate quickly, $\textit{Among Us}$ can be expected to last
much longer, because it is a multi-player game far from equilibrium. Using the
sandbox, we evaluate $18$ proprietary and open-weight LLMs and uncover a
general trend: models trained with RL are comparatively much better at
producing deception than detecting it. We evaluate the effectiveness of methods
to detect lying and deception: logistic regression on the activations and
sparse autoencoders (SAEs). We find that probes trained on a dataset of
``pretend you're a dishonest model: $\dots$'' generalize extremely well
out-of-distribution, consistently obtaining AUROCs over 95% even when evaluated
just on the deceptive statement, without the chain of thought. We also find two
SAE features that work well at deception detection but are unable to steer the
model to lie less. We hope our open-sourced sandbox, game logs, and probes
serve to anticipate and mitigate deceptive behavior and capabilities in
language-based agents.

</details>


### [294] [Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?](https://arxiv.org/pdf/2504.13837)
*Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, Gao Huang*

Main category: cs.AI

TL;DR: RLVR enhances LLMs' reasoning but doesn't introduce fundamentally new patterns; base models outperform at large k. Distillation shows promise, but current RLVR methods fall short.


<details>
  <summary>Details</summary>
Motivation: To critically examine RLVR's impact on LLMs' reasoning abilities and assess its limitations.

Method: Systematic evaluation across model families, RL algorithms, and benchmarks using pass@k metrics.

Result: RLVR-trained models outperform base models at small k but lag at large k. Distillation introduces new reasoning patterns.

Conclusion: Current RLVR methods don't unlock novel reasoning; improved paradigms like continual scaling are needed.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently
demonstrated notable success in enhancing the reasoning performance of large
language models (LLMs), particularly on mathematics and programming tasks.
Similar to how traditional RL helps agents explore and learn new strategies,
RLVR is believed to enable LLMs to continuously self-improve, thus acquiring
novel reasoning abilities beyond those of the corresponding base models. In
this study we critically examine the current state of RLVR by systematically
probing the reasoning capability boundaries of RLVR-trained LLMs across various
model families, RL algorithms, and math, coding, and visual reasoning
benchmarks, using pass@k at large k values as the evaluation metric.
Surprisingly, we find that the current training setup does not elicit
fundamentally new reasoning patterns. While RLVR-trained models outperform
their base models at small k (e.g., k = 1), the base models achieve a higher
pass@k score when k is large. Coverage and perplexity analyses show that the
observed reasoning abilities originate from and are bounded by the base model.
Treating the base model as an upper bound, our quantitative analysis shows that
six popular RLVR algorithms perform similarly and remain far from optimal in
leveraging the potential of the base model. By contrast, we find that
distillation can introduce new reasoning patterns from the teacher and
genuinely expand the model's reasoning capabilities. Overall, our findings
suggest that current RLVR methods have not yet realized the potential of RL to
elicit truly novel reasoning abilities in LLMs. This highlights the need for
improved RL paradigms, such as continual scaling and multi-turn
agent-environment interaction, to unlock this potential.

</details>


### [295] [AI Idea Bench 2025: AI Research Idea Generation Benchmark](https://arxiv.org/pdf/2504.14191)
*Yansheng Qiu, Haoquan Zhang, Zhaopan Xu, Ming Li, Diping Song, Zheng Wang, Kaipeng Zhang*

Main category: cs.AI

TL;DR: AI Idea Bench 2025 is a framework to evaluate LLM-generated ideas in AI research, addressing current assessment gaps like knowledge leakage and lack of open-ended benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of LLM-generated ideas overlook key factors like knowledge leakage and grounded truth, limiting potential breakthroughs.

Method: The framework uses a dataset of 3,495 AI papers and inspired works, evaluating idea quality via alignment with original content and general references.

Result: AI Idea Bench 2025 provides a robust benchmarking system for comparing idea-generation techniques.

Conclusion: This framework aids in automating scientific discovery by improving the assessment of LLM-generated ideas.

Abstract: Large-scale Language Models (LLMs) have revolutionized human-AI interaction
and achieved significant success in the generation of novel ideas. However,
current assessments of idea generation overlook crucial factors such as
knowledge leakage in LLMs, the absence of open-ended benchmarks with grounded
truth, and the limited scope of feasibility analysis constrained by prompt
design. These limitations hinder the potential of uncovering groundbreaking
research ideas. In this paper, we present AI Idea Bench 2025, a framework
designed to quantitatively evaluate and compare the ideas generated by LLMs
within the domain of AI research from diverse perspectives. The framework
comprises a comprehensive dataset of 3,495 AI papers and their associated
inspired works, along with a robust evaluation methodology. This evaluation
system gauges idea quality in two dimensions: alignment with the ground-truth
content of the original papers and judgment based on general reference
material. AI Idea Bench 2025's benchmarking system stands to be an invaluable
resource for assessing and comparing idea-generation techniques, thereby
facilitating the automation of scientific discovery.

</details>


### [296] [Head-Tail-Aware KL Divergence in Knowledge Distillation for Spiking Neural Networks](https://arxiv.org/pdf/2504.20445)
*Tianqing Zhang, Zixin Zhu, Kairong Yu, Hongwei Wang*

Main category: cs.AI

TL;DR: The paper proposes a novel knowledge distillation method, HTA-KL, to improve SNN performance by balancing knowledge transfer between high- and low-probability predictions.


<details>
  <summary>Details</summary>
Motivation: SNNs lag behind ANNs in performance due to training limitations. Existing KD methods using KL divergence inadequately address SNN-specific needs.

Method: Introduces HTA-KL divergence, a cumulative probability-based mask to dynamically weight high- and low-probability regions, combining FKL and RKL.

Result: Outperforms existing methods on CIFAR-10, CIFAR-100, and Tiny ImageNet with fewer timesteps.

Conclusion: HTA-KL effectively bridges the performance gap between SNNs and ANNs by optimizing knowledge transfer.

Abstract: Spiking Neural Networks (SNNs) have emerged as a promising approach for
energy-efficient and biologically plausible computation. However, due to
limitations in existing training methods and inherent model constraints, SNNs
often exhibit a performance gap when compared to Artificial Neural Networks
(ANNs). Knowledge distillation (KD) has been explored as a technique to
transfer knowledge from ANN teacher models to SNN student models to mitigate
this gap. Traditional KD methods typically use Kullback-Leibler (KL) divergence
to align output distributions. However, conventional KL-based approaches fail
to fully exploit the unique characteristics of SNNs, as they tend to
overemphasize high-probability predictions while neglecting low-probability
ones, leading to suboptimal generalization. To address this, we propose
Head-Tail Aware Kullback-Leibler (HTA-KL) divergence, a novel KD method for
SNNs. HTA-KL introduces a cumulative probability-based mask to dynamically
distinguish between high- and low-probability regions. It assigns adaptive
weights to ensure balanced knowledge transfer, enhancing the overall
performance. By integrating forward KL (FKL) and reverse KL (RKL) divergence,
our method effectively align both head and tail regions of the distribution. We
evaluate our methods on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets. Our
method outperforms existing methods on most datasets with fewer timesteps.

</details>


### [297] [Grounding Synthetic Data Evaluations of Language Models in Unsupervised Document Corpora](https://arxiv.org/pdf/2505.08905)
*Michael Majurski, Cynthia Matuszek*

Main category: cs.AI

TL;DR: The paper proposes an automated method for creating fact-based synthetic benchmarks to evaluate language models (LMs) using grounding documents, reducing reliance on human effort.


<details>
  <summary>Details</summary>
Motivation: Human effort in benchmark construction is outpaced by the scale of LMs, making manual evaluation impractical.

Method: Leverages LMs to automatically generate domain-specific evaluation questions from grounding documents (e.g., textbooks).

Result: Achieves high correlation with human-curated benchmarks (Spearman 0.97, Pearson 0.75) and reveals strong performance of Gemma-3 models.

Conclusion: The automated approach efficiently evaluates LM capabilities and supports diverse question formats, offering scalable benchmarking.

Abstract: Language Models (LMs) continue to advance, improving response quality and
coherence. Given Internet-scale training datasets, LMs have likely encountered
much of what users may ask them to generate in some form during their training.
A plethora of evaluation benchmarks have been constructed to assess model
quality, response appropriateness, and reasoning capabilities. However, the
human effort required for benchmark construction is rapidly being outpaced by
the size and scope of the models under evaluation. Having humans build a
benchmark for every possible domain of interest is impractical. Therefore, we
propose a methodology for automating the construction of fact-based synthetic
data model evaluations grounded in document populations. This work leverages
the same LMs to evaluate domain-specific knowledge automatically, using only
grounding documents (e.g., a textbook) as input. This synthetic data
benchmarking approach corresponds well with human curated questions producing a
Spearman ranking correlation of 0.97 and a benchmark evaluation Pearson
accuracy correlation of 0.75. This novel approach supports generating both
multiple choice and open-ended synthetic data questions to gain diagnostic
insight of LM capability. We apply this methodology to evaluate model
performance on two recent arXiv preprints, discovering a surprisingly strong
performance from Gemma-3 models on open-ended questions. Code is available at
https://github.com/mmajurski/grounded-synth-lm-benchmark

</details>


### [298] [Leveraging Graph Retrieval-Augmented Generation to Support Learners' Understanding of Knowledge Concepts in MOOCs](https://arxiv.org/pdf/2505.10074)
*Mohamed Abdelmagied, Mohamed Amine Chatti, Shoeb Joarder, Qurat Ul Ain, Rawaa Alatrash*

Main category: cs.AI

TL;DR: The paper proposes a Graph RAG pipeline using Educational and Personal Knowledge Graphs to enhance MOOC learning by generating personalized questions and answers, addressing LLM hallucinations and unstructured material limitations.


<details>
  <summary>Details</summary>
Motivation: MOOCs lack direct interaction, and LLMs are unreliable due to hallucinations. RAG helps but is limited by unstructured material and passive guidance.

Method: A Graph RAG pipeline integrates EduKGs and PKGs for personalized question generation and answering, tested on CourseMapper with expert instructors.

Result: Evaluation shows Graph RAG's potential to improve personalized learning in MOOCs.

Conclusion: Graph RAG effectively guides learners in MOOCs, enhancing understanding of new concepts.

Abstract: Massive Open Online Courses (MOOCs) lack direct interaction between learners
and instructors, making it challenging for learners to understand new knowledge
concepts. Recently, learners have increasingly used Large Language Models
(LLMs) to support them in acquiring new knowledge. However, LLMs are prone to
hallucinations which limits their reliability. Retrieval-Augmented Generation
(RAG) addresses this issue by retrieving relevant documents before generating a
response. However, the application of RAG across different MOOCs is limited by
unstructured learning material. Furthermore, current RAG systems do not
actively guide learners toward their learning needs. To address these
challenges, we propose a Graph RAG pipeline that leverages Educational
Knowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to guide
learners to understand knowledge concepts in the MOOC platform CourseMapper.
Specifically, we implement (1) a PKG-based Question Generation method to
recommend personalized questions for learners in context, and (2) an
EduKG-based Question Answering method that leverages the relationships between
knowledge concepts in the EduKG to answer learner selected questions. To
evaluate both methods, we conducted a study with 3 expert instructors on 3
different MOOCs in the MOOC platform CourseMapper. The results of the
evaluation show the potential of Graph RAG to empower learners to understand
new knowledge concepts in a personalized learning experience.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [299] [Multi-Stage Speaker Diarization for Noisy Classrooms](https://arxiv.org/pdf/2505.10879)
*Ali Sartaz Khan, Tolulope Ogunremi, Ahmed Attia, Dorottya Demszky*

Main category: cs.SD

TL;DR: The study evaluates multi-stage diarization models for noisy classrooms, showing denoising and hybrid VAD improve accuracy, achieving DERs of 17% (teacher-student) and 45% (all-speaker).


<details>
  <summary>Details</summary>
Motivation: Classroom settings pose challenges like poor audio quality and overlapping speech, making speaker diarization difficult.

Method: Uses Nvidia's NeMo pipeline, tests denoising, compares VAD models, and explores a hybrid VAD approach with ASR integration.

Result: Denoising reduces DER; hybrid VAD achieves 17% DER (teacher-student) and 45% (all-speaker). Training on mixed datasets improves noisy performance.

Conclusion: Multi-stage models and ASR integration enhance diarization in noisy classrooms, though trade-offs exist between VAD and speaker confusion.

Abstract: Speaker diarization, the process of identifying "who spoke when" in audio
recordings, is essential for understanding classroom dynamics. However,
classroom settings present distinct challenges, including poor recording
quality, high levels of background noise, overlapping speech, and the
difficulty of accurately capturing children's voices. This study investigates
the effectiveness of multi-stage diarization models using Nvidia's NeMo
diarization pipeline. We assess the impact of denoising on diarization accuracy
and compare various voice activity detection (VAD) models, including
self-supervised transformer-based frame-wise VAD models. We also explore a
hybrid VAD approach that integrates Automatic Speech Recognition (ASR)
word-level timestamps with frame-level VAD predictions. We conduct experiments
using two datasets from English speaking classrooms to separate teacher vs.
student speech and to separate all speakers. Our results show that denoising
significantly improves the Diarization Error Rate (DER) by reducing the rate of
missed speech. Additionally, training on both denoised and noisy datasets leads
to substantial performance gains in noisy conditions. The hybrid VAD model
leads to further improvements in speech detection, achieving a DER as low as
17% in teacher-student experiments and 45% in all-speaker experiments. However,
we also identified trade-offs between voice activity detection and speaker
confusion. Overall, our study highlights the effectiveness of multi-stage
diarization models and integrating ASR-based information for enhancing speaker
diarization in noisy classroom environments.

</details>


### [300] [BanglaFake: Constructing and Evaluating a Specialized Bengali Deepfake Audio Dataset](https://arxiv.org/pdf/2505.10885)
*Istiaq Ahmed Fahad, Kamruzzaman Asif, Sifat Sikder*

Main category: cs.SD

TL;DR: BangalFake dataset addresses deepfake audio detection challenges in Bengali, featuring 25,520 utterances and evaluations showing high naturalness and intelligibility.


<details>
  <summary>Details</summary>
Motivation: The lack of datasets and subtle acoustic features make deepfake audio detection difficult for low-resource languages like Bengali.

Method: Created BangalFake dataset with 12,260 real and 13,260 deepfake utterances using SOTA TTS models, evaluated via MOS and t-SNE visualization.

Result: MOS scores: 3.40 (naturalness) and 4.01 (intelligibility). t-SNE shows challenges in differentiating real vs. fake.

Conclusion: BangalFake is a valuable resource for advancing deepfake detection in Bengali, addressing low-resource language limitations.

Abstract: Deepfake audio detection is challenging for low-resource languages like
Bengali due to limited datasets and subtle acoustic features. To address this,
we introduce BangalFake, a Bengali Deepfake Audio Dataset with 12,260 real and
13,260 deepfake utterances. Synthetic speech is generated using SOTA
Text-to-Speech (TTS) models, ensuring high naturalness and quality. We evaluate
the dataset through both qualitative and quantitative analyses. Mean Opinion
Score (MOS) from 30 native speakers shows Robust-MOS of 3.40 (naturalness) and
4.01 (intelligibility). t-SNE visualization of MFCCs highlights real vs. fake
differentiation challenges. This dataset serves as a crucial resource for
advancing deepfake detection in Bengali, addressing the limitations of
low-resource language research.

</details>


### [301] [$\mathcal{A}LLM4ADD$: Unlocking the Capabilities of Audio Large Language Models for Audio Deepfake Detection](https://arxiv.org/pdf/2505.11079)
*Hao Gu, Jiangyan Yi, Chenglong Wang, Jianhua Tao, Zheng Lian, Jiayi He, Yong Ren, Yujie Chen, Zhengqi Wen*

Main category: cs.SD

TL;DR: The paper explores using Audio Large Language Models (ALLMs) for Audio Deepfake Detection (ADD), proposing a framework called $\mathcal{A}LLM4ADD$ that reformulates ADD as an audio question-answering task. Despite initial zero-shot ineffectiveness, fine-tuned ALLMs achieve superior performance, especially in data-scarce scenarios.


<details>
  <summary>Details</summary>
Motivation: The rise of high-fidelity audio generative models and their misuse potential necessitates effective ADD solutions. ALLMs' success in audio tasks prompts investigation into their applicability for ADD.

Method: The study first evaluates ALLMs' zero-shot performance on ADD, then proposes $\mathcal{A}LLM4ADD$, reformulating ADD as an audio question-answering task and fine-tuning ALLMs for authenticity assessment.

Result: Fine-tuned ALLMs outperform in fake audio detection, particularly in data-scarce settings, demonstrating the framework's effectiveness.

Conclusion: The work pioneers ALLM application in ADD, showing promise for future research and development of more effective detection systems.

Abstract: Audio deepfake detection (ADD) has grown increasingly important due to the
rise of high-fidelity audio generative models and their potential for misuse.
Given that audio large language models (ALLMs) have made significant progress
in various audio processing tasks, a heuristic question arises: Can ALLMs be
leveraged to solve ADD?. In this paper, we first conduct a comprehensive
zero-shot evaluation of ALLMs on ADD, revealing their ineffectiveness in
detecting fake audio. To enhance their performance, we propose
$\mathcal{A}LLM4ADD$, an ALLM-driven framework for ADD. Specifically, we
reformulate ADD task as an audio question answering problem, prompting the
model with the question: "Is this audio fake or real?". We then perform
supervised fine-tuning to enable the ALLM to assess the authenticity of query
audio. Extensive experiments are conducted to demonstrate that our ALLM-based
method can achieve superior performance in fake audio detection, particularly
in data-scarce scenarios. As a pioneering study, we anticipate that this work
will inspire the research community to leverage ALLMs to develop more effective
ADD systems.

</details>


### [302] [Audio Turing Test: Benchmarking the Human-likeness of Large Language Model-based Text-to-Speech Systems in Chinese](https://arxiv.org/pdf/2505.11200)
*Xihuai Wang, Ziyi Zhao, Siyu Ren, Shao Zhang, Song Li, Xiaoyu Li, Ziwen Wang, Lin Qiu, Guanglu Wan, Xuezhi Cao, Xunliang Cai, Weinan Zhang*

Main category: cs.SD

TL;DR: The paper introduces the Audio Turing Test (ATT) and ATT-Corpus, a multi-dimensional Chinese dataset, to improve TTS evaluation by simplifying human judgment and reducing bias. It also presents Auto-ATT, a finetuned model for automatic evaluation, showing strong alignment with human judgments.


<details>
  <summary>Details</summary>
Motivation: Current TTS evaluation methods like MOS are subjective and lack multi-dimensional design, especially for Chinese TTS. The paper aims to address these limitations with a simpler, more robust approach.

Method: The authors propose ATT, a Turing-Test-inspired protocol where evaluators judge if a voice sounds human. They also develop ATT-Corpus, a multi-dimensional Chinese dataset, and finetune Qwen2-Audio-Instruct (Auto-ATT) for automatic evaluation.

Result: ATT effectively differentiates TTS models across specific dimensions, and Auto-ATT aligns well with human evaluations, proving its reliability for fast assessment.

Conclusion: ATT and Auto-ATT offer a robust, simplified alternative to traditional TTS evaluation methods, with potential for broader application in model development.

Abstract: Recent advances in large language models (LLMs) have significantly improved
text-to-speech (TTS) systems, enhancing control over speech style, naturalness,
and emotional expression, which brings TTS Systems closer to human-level
performance. Although the Mean Opinion Score (MOS) remains the standard for TTS
System evaluation, it suffers from subjectivity, environmental inconsistencies,
and limited interpretability. Existing evaluation datasets also lack a
multi-dimensional design, often neglecting factors such as speaking styles,
context diversity, and trap utterances, which is particularly evident in
Chinese TTS evaluation. To address these challenges, we introduce the Audio
Turing Test (ATT), a multi-dimensional Chinese corpus dataset ATT-Corpus paired
with a simple, Turing-Test-inspired evaluation protocol. Instead of relying on
complex MOS scales or direct model comparisons, ATT asks evaluators to judge
whether a voice sounds human. This simplification reduces rating bias and
improves evaluation robustness. To further support rapid model development, we
also finetune Qwen2-Audio-Instruct with human judgment data as Auto-ATT for
automatic evaluation. Experimental results show that ATT effectively
differentiates models across specific capability dimensions using its
multi-dimensional design. Auto-ATT also demonstrates strong alignment with
human evaluations, confirming its value as a fast and reliable assessment tool.
The white-box ATT-Corpus and Auto-ATT can be found in ATT Hugging Face
Collection
(https://huggingface.co/collections/meituan/audio-turing-test-682446320368164faeaf38a4).

</details>


### [303] [Seeing Sound, Hearing Sight: Uncovering Modality Bias and Conflict of AI models in Sound Localization](https://arxiv.org/pdf/2505.11217)
*Yanhao Jia, Ji Xie, S Jivaganesh, Hao Li, Xu Wu, Mengmi Zhang*

Main category: cs.SD

TL;DR: The paper examines how AI models handle cross-modal conflicts in sound localization, comparing them to human performance. Humans prioritize sound over misleading visuals, while AI often defaults to visuals, degrading performance. Finetuning a model with stereo audio-image data improves results, even mirroring human-like biases.


<details>
  <summary>Details</summary>
Motivation: To understand modality bias and conflict resolution in AI sound localization, given humans' superior ability to resolve sensory conflicts.

Method: Systematically assess leading multimodal models against human performance in psychophysics experiments across six audiovisual conditions. Finetune a state-of-the-art model using stereo audio-image data from 3D simulations.

Result: Humans outperform AI in handling conflicting or missing visuals. Finetuned AI models surpass benchmarks and show human-like horizontal localization bias.

Conclusion: Sensory input quality and system architecture critically influence multimodal representation accuracy, with finetuned models bridging gaps between AI and human performance.

Abstract: Imagine hearing a dog bark and turning toward the sound only to see a parked
car, while the real, silent dog sits elsewhere. Such sensory conflicts test
perception, yet humans reliably resolve them by prioritizing sound over
misleading visuals. Despite advances in multimodal AI integrating vision and
audio, little is known about how these systems handle cross-modal conflicts or
whether they favor one modality. In this study, we systematically examine
modality bias and conflict resolution in AI sound localization. We assess
leading multimodal models and benchmark them against human performance in
psychophysics experiments across six audiovisual conditions, including
congruent, conflicting, and absent cues. Humans consistently outperform AI,
demonstrating superior resilience to conflicting or missing visuals by relying
on auditory information. In contrast, AI models often default to visual input,
degrading performance to near chance levels. To address this, we finetune a
state-of-the-art model using a stereo audio-image dataset generated via 3D
simulations. Even with limited training data, the refined model surpasses
existing benchmarks. Notably, it also mirrors human-like horizontal
localization bias favoring left-right precision-likely due to the stereo audio
structure reflecting human ear placement. These findings underscore how sensory
input quality and system architecture shape multimodal representation accuracy.

</details>


### [304] [Improving Inference-Time Optimisation for Vocal Effects Style Transfer with a Gaussian Prior](https://arxiv.org/pdf/2505.11315)
*Chin-Yun Yu, Marco A. Mart√≠nez-Ram√≠rez, Junghyun Koo, Wei-Hsiang Liao, Yuki Mitsufuji, Gy√∂rgy Fazekas*

Main category: cs.SD

TL;DR: ST-ITO improves audio style transfer by adding a Gaussian prior from a vocal preset dataset, outperforming baselines in metrics and subjective evaluations.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of ST-ITO, which treats all configurations equally and relies solely on embeddings, leading to unrealistic results.

Method: Introduces a Gaussian prior from the DiffVox dataset for parameter space, enabling maximum-a-posteriori estimation during optimization.

Result: Significant improvements in vocal effects transfer, reducing parameter error by 33% and better matching reference styles. Subjective evaluations confirm superiority.

Conclusion: Incorporating prior knowledge at inference time enhances audio effects transfer, enabling more realistic and effective audio processing.

Abstract: Style Transfer with Inference-Time Optimisation (ST-ITO) is a recent approach
for transferring the applied effects of a reference audio to a raw audio track.
It optimises the effect parameters to minimise the distance between the style
embeddings of the processed audio and the reference. However, this method
treats all possible configurations equally and relies solely on the embedding
space, which can lead to unrealistic or biased results. We address this pitfall
by introducing a Gaussian prior derived from a vocal preset dataset, DiffVox,
over the parameter space. The resulting optimisation is equivalent to
maximum-a-posteriori estimation. Evaluations on vocal effects transfer on the
MedleyDB dataset show significant improvements across metrics compared to
baselines, including a blind audio effects estimator, nearest-neighbour
approaches, and uncalibrated ST-ITO. The proposed calibration reduces parameter
mean squared error by up to 33% and matches the reference style better.
Subjective evaluations with 16 participants confirm our method's superiority,
especially in limited data regimes. This work demonstrates how incorporating
prior knowledge in inference time enhances audio effects transfer, paving the
way for more effective and realistic audio processing systems.

</details>


### [305] [Machine Learning Approaches to Vocal Register Classification in Contemporary Male Pop Music](https://arxiv.org/pdf/2505.11378)
*Alexander Kim, Charlotte Botha*

Main category: cs.SD

TL;DR: The paper introduces methods to classify vocal registers in male pop music using mel-spectrogram analysis and discusses the AVRA software for vocal analysis.


<details>
  <summary>Details</summary>
Motivation: Navigating vocal registers, especially around the passagio, is challenging for singers, particularly in pop music where varied timbres are used.

Method: Two methods are presented: SVM and CNN models analyze textural features of mel-spectrogram images to classify vocal registers.

Result: Both SVM and CNN models achieved consistent vocal register classification, showing promise for broader applications.

Conclusion: The methods and AVRA software demonstrate potential for robust vocal register classification across more voice types and genres.

Abstract: For singers of all experience levels, one of the most daunting challenges in
learning technical repertoire is navigating placement and vocal register in and
around the passagio (passage between chest voice and head voice registers).
Particularly in pop music, where a single artist may use a variety of timbre's
and textures to achieve a desired quality, it can be difficult to identify what
vocal register within the vocal range a singer is using. This paper presents
two methods for classifying vocal registers in an audio signal of male pop
music through the analysis of textural features of mel-spectrogram images.
Additionally, we will discuss the practical integration of these models for
vocal analysis tools, and introduce a concurrently developed software called
AVRA which stands for Automatic Vocal Register Analysis. Our proposed methods
achieved consistent classification of vocal register through both Support
Vector Machine (SVM) and Convolutional Neural Network (CNN) models, which
supports the promise of more robust classification possibilities across more
voice types and genres of singing.

</details>


### [306] [ImprovNet -- Generating Controllable Musical Improvisations with Iterative Corruption Refinement](https://arxiv.org/pdf/2502.04522)
*Keshav Bhandari, Sungkyun Chang, Tongyu Lu, Fareza R. Enus, Louis B. Bradshaw, Dorien Herremans, Simon Colton*

Main category: cs.SD

TL;DR: ImprovNet is a transformer-based model for controllable musical style transfer, excelling in cross-genre improvisations, harmonization, and short prompt tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of controllable musical style transfer due to limited datasets and lack of unified models for multiple tasks.

Method: Uses a self-supervised corruption-refinement training strategy with a transformer-based architecture.

Result: Outperforms Anticipatory Music Transformer in tasks and achieves 79% accuracy in genre conversion.

Conclusion: ImprovNet effectively generates coherent improvisations while maintaining structural ties to original compositions.

Abstract: Despite deep learning's remarkable advances in style transfer across various
domains, generating controllable performance-level musical style transfer for
complete symbolically represented musical works remains a challenging area of
research. Much of this is owed to limited datasets, especially for genres such
as jazz, and the lack of unified models that can handle multiple music
generation tasks. This paper presents ImprovNet, a transformer-based
architecture that generates expressive and controllable musical improvisations
through a self-supervised corruption-refinement training strategy. The
improvisational style transfer is aimed at making meaningful modifications to
one or more musical elements - melody, harmony or rhythm of the original
composition with respect to the target genre. ImprovNet unifies multiple
capabilities within a single model: it can perform cross-genre and intra-genre
improvisations, harmonize melodies with genre-specific styles, and execute
short prompt continuation and infilling tasks. The model's iterative generation
framework allows users to control the degree of style transfer and structural
similarity to the original composition. Objective and subjective evaluations
demonstrate ImprovNet's effectiveness in generating musically coherent
improvisations while maintaining structural relationships with the original
pieces. The model outperforms Anticipatory Music Transformer in short
continuation and infilling tasks and successfully achieves recognizable genre
conversion, with 79\% of participants correctly identifying jazz-style
improvisations of classical pieces. Our code and demo page can be found at
https://github.com/keshavbhandari/improvnet.

</details>


### [307] [JamendoMaxCaps: A Large Scale Music-caption Dataset with Imputed Metadata](https://arxiv.org/pdf/2502.07461)
*Abhinaba Roy, Renhang Liu, Tongyu Lu, Dorien Herremans*

Main category: cs.SD

TL;DR: JamendoMaxCaps is a large-scale music-caption dataset with 362,000 instrumental tracks, enhanced captions, and metadata imputation via a retrieval system and LLLM. Validated with five metrics, it supports music-language research.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive dataset for music-language understanding tasks by combining captions, metadata, and retrieval techniques.

Method: Created a dataset with captions from a state-of-the-art model, imputed missing metadata using a retrieval system and LLLM, and validated with five measurements.

Result: A high-quality, publicly available dataset (JamendoMaxCaps) with enhanced captions and metadata for music-language research.

Conclusion: JamendoMaxCaps advances research in music retrieval, multimodal learning, and generative models by offering a rich, validated dataset.

Abstract: We introduce JamendoMaxCaps, a large-scale music-caption dataset featuring
over 362,000 freely licensed instrumental tracks from the renowned Jamendo
platform. The dataset includes captions generated by a state-of-the-art
captioning model, enhanced with imputed metadata. We also introduce a retrieval
system that leverages both musical features and metadata to identify similar
songs, which are then used to fill in missing metadata using a local large
language model (LLLM). This approach allows us to provide a more comprehensive
and informative dataset for researchers working on music-language understanding
tasks. We validate this approach quantitatively with five different
measurements. By making the JamendoMaxCaps dataset publicly available, we
provide a high-quality resource to advance research in music-language
understanding tasks such as music retrieval, multimodal representation
learning, and generative music models.

</details>


### [308] [Supervised contrastive learning from weakly-labeled audio segments for musical version matching](https://arxiv.org/pdf/2502.16936)
*Joan Serr√†, R. Oguz Araz, Dmitry Bogdanov, Yuki Mitsufuji*

Main category: cs.SD

TL;DR: The paper proposes a method for detecting musical versions at the segment level using weakly annotated segments and a novel contrastive loss, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods match musical versions at the track level, but applications require segment-level matching. Current approaches also use outdated loss functions.

Method: Uses weakly annotated segments with pairwise distance reductions and introduces a modified contrastive loss based on decoupling, hyper-parameter, and geometric considerations.

Result: Achieves state-of-the-art results in track-level evaluation and breakthrough performance in segment-level evaluation.

Conclusion: The proposed methods are generalizable and may benefit domains beyond audio or musical version matching.

Abstract: Detecting musical versions (different renditions of the same piece) is a
challenging task with important applications. Because of the ground truth
nature, existing approaches match musical versions at the track level (e.g.,
whole song). However, most applications require to match them at the segment
level (e.g., 20s chunks). In addition, existing approaches resort to
classification and triplet losses, disregarding more recent losses that could
bring meaningful improvements. In this paper, we propose a method to learn from
weakly annotated segments, together with a contrastive loss variant that
outperforms well-studied alternatives. The former is based on pairwise segment
distance reductions, while the latter modifies an existing loss following
decoupling, hyper-parameter, and geometric considerations. With these two
elements, we do not only achieve state-of-the-art results in the standard
track-level evaluation, but we also obtain a breakthrough performance in a
segment-level evaluation. We believe that, due to the generality of the
challenges addressed here, the proposed methods may find utility in domains
beyond audio or musical version matching.

</details>


### [309] [SlimSpeech: Lightweight and Efficient Text-to-Speech with Slim Rectified Flow](https://arxiv.org/pdf/2504.07776)
*Kaidi Wang, Wenhao Guan, Shenghui Lu, Jianglong Yao, Lin Li, Qingyang Hong*

Main category: cs.SD

TL;DR: SlimSpeech is a lightweight speech synthesis system using rectified flow, achieving high-quality results with fewer parameters and steps.


<details>
  <summary>Details</summary>
Motivation: To enhance speech synthesis efficiency by reducing model size and inference steps while maintaining quality.

Method: Modifies rectified flow structure, uses distillation, and refines reflow for a smaller, efficient model.

Result: Achieves comparable performance to larger models with one-step sampling and fewer parameters.

Conclusion: SlimSpeech offers a lightweight, efficient solution for high-quality speech synthesis.

Abstract: Recently, flow matching based speech synthesis has significantly enhanced the
quality of synthesized speech while reducing the number of inference steps. In
this paper, we introduce SlimSpeech, a lightweight and efficient speech
synthesis system based on rectified flow. We have built upon the existing
speech synthesis method utilizing the rectified flow model, modifying its
structure to reduce parameters and serve as a teacher model. By refining the
reflow operation, we directly derive a smaller model with a more straight
sampling trajectory from the larger model, while utilizing distillation
techniques to further enhance the model performance. Experimental results
demonstrate that our proposed method, with significantly reduced model
parameters, achieves comparable performance to larger models through one-step
sampling.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [310] [Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment](https://arxiv.org/pdf/2505.10597)
*Jiazheng Zhang, Wenqing Jing, Zizhuo Zhang, Zhiheng Xi, Shihan Dou, Rongxiang Weng, Jiahuan Li, Jingang Wang, MingXu Cai, Shibo Hong, Tao Gui, Qi Zhang*

Main category: cs.LG

TL;DR: CRM improves reward model robustness by filtering noisy preferences via peer review and curriculum learning, enhancing LLM alignment with human values.


<details>
  <summary>Details</summary>
Motivation: Noisy human feedback causes reward misgeneralization, leading to poor LLM performance. Addressing this noise is critical for reliable alignment.

Method: Proposes Collaborative Reward Modeling (CRM), combining peer review (two RMs filtering each other's data) and curriculum learning (easy-to-hard data structuring).

Result: CRM boosts generalization, achieving up to 9.94 accuracy gain on RewardBench under 40% noise, and works with implicit-reward methods.

Conclusion: CRM offers a robust, versatile solution for aligning LLMs with human values despite noisy feedback.

Abstract: Reward models (RMs) are essential for aligning large language models (LLMs)
with human values. However, noisy preferences in human feedback often lead to
reward misgeneralization, where RMs overfit to spurious patterns and provide
misleading signals during policy optimization. We systematically analyze the
training dynamics of preference pairs and identify that noisy examples are
harder to fit and introduce instability. Empirical evidence shows that LLMs
optimized using reward models trained on full noisy datasets perform worse than
those trained on filtered, high-quality preferences. To address this, we
propose Collaborative Reward Modeling (CRM), an online framework that enhances
robustness by combining peer review and curriculum learning. Two reward models
are trained in parallel and assess each other's data selections to filter out
potential noise. Curriculum learning structures the preference data from easy
to hard, ensuring synchronized training and stable feedback. Extensive
experiments demonstrate that CRM improves generalization, with up to 9.94
points of accuracy gain on RewardBench under 40 percent label noise. CRM is
also compatible with implicit-reward alignment methods, offering a practical
and versatile strategy for robust alignment.

</details>


### [311] [UDDETTS: Unifying Discrete and Dimensional Emotions for Controllable Emotional Text-to-Speech](https://arxiv.org/pdf/2505.10599)
*Jiaxuan Liu, Zhenhua Ling*

Main category: cs.LG

TL;DR: UDDETTS is a neural codec language model for controllable emotional TTS, combining discrete and dimensional emotions using the ADV space, and trained semi-supervisedly for superior synthesis.


<details>
  <summary>Details</summary>
Motivation: Challenges in controllable emotional TTS include predefined discrete labels lacking nuance and limited datasets causing overfitting.

Method: Proposes UDDETTS, integrating the ADV space for dimensional emotion control and using semi-supervised training with diverse datasets.

Result: UDDETTS achieves linear emotion control in ADV space and superior emotional speech synthesis.

Conclusion: UDDETTS effectively addresses challenges in emotional TTS by unifying discrete and dimensional emotion control.

Abstract: Recent neural codec language models have made great progress in the field of
text-to-speech (TTS), but controllable emotional TTS still faces many
challenges. Traditional methods rely on predefined discrete emotion labels to
control emotion categories and intensities, which can't capture the complexity
and continuity of human emotional perception and expression. The lack of
large-scale emotional speech datasets with balanced emotion distributions and
fine-grained emotion annotations often causes overfitting in synthesis models
and impedes effective emotion control. To address these issues, we propose
UDDETTS, a neural codec language model unifying discrete and dimensional
emotions for controllable emotional TTS. This model introduces the
interpretable Arousal-Dominance-Valence (ADV) space for dimensional emotion
description and supports emotion control driven by either discrete emotion
labels or nonlinearly quantified ADV values. Furthermore, a semi-supervised
training strategy is designed to comprehensively utilize diverse speech
datasets with different types of emotion annotations to train the UDDETTS.
Experiments show that UDDETTS achieves linear emotion control along the three
dimensions of ADV space, and exhibits superior end-to-end emotional speech
synthesis capabilities.

</details>


### [312] [Enhancing IoT Cyber Attack Detection in the Presence of Highly Imbalanced Data](https://arxiv.org/pdf/2505.10600)
*Md. Ehsanul Haque, Md. Saymon Hosen Polash, Md Al-Imran Sanjida Simla, Md Alomgir Hossain, Sarwar Jahan*

Main category: cs.LG

TL;DR: Hybrid sampling techniques improve IoT IDS performance on imbalanced datasets, with Random Forest and Soft Voting models showing top results.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of IoT networks increases cyber risks, requiring effective IDS for imbalanced datasets where traditional models struggle.

Method: Hybrid sampling techniques are applied to address data imbalance, followed by evaluating machine learning models like Random Forest, Soft Voting, SVC, KNN, MLP, and Logistic Regression.

Result: Random Forest achieved the best performance (Kappa: 0.9903, accuracy: 0.9961, AUC: 0.9994), with Soft Voting also performing well (accuracy: 0.9952, AUC: 0.9997).

Conclusion: Hybrid sampling and robust model selection significantly enhance IoT security in imbalanced data environments.

Abstract: Due to the rapid growth in the number of Internet of Things (IoT) networks,
the cyber risk has increased exponentially, and therefore, we have to develop
effective IDS that can work well with highly imbalanced datasets. A high rate
of missed threats can be the result, as traditional machine learning models
tend to struggle in identifying attacks when normal data volume is much higher
than the volume of attacks. For example, the dataset used in this study reveals
a strong class imbalance with 94,659 instances of the majority class and only
28 instances of the minority class, making it quite challenging to determine
rare attacks accurately. The challenges presented in this research are
addressed by hybrid sampling techniques designed to improve data imbalance
detection accuracy in IoT domains. After applying these techniques, we evaluate
the performance of several machine learning models such as Random Forest, Soft
Voting, Support Vector Classifier (SVC), K-Nearest Neighbors (KNN), Multi-Layer
Perceptron (MLP), and Logistic Regression with respect to the classification of
cyber-attacks. The obtained results indicate that the Random Forest model
achieved the best performance with a Kappa score of 0.9903, test accuracy of
0.9961, and AUC of 0.9994. Strong performance is also shown by the Soft Voting
model, with an accuracy of 0.9952 and AUC of 0.9997, indicating the benefits of
combining model predictions. Overall, this work demonstrates the value of
hybrid sampling combined with robust model and feature selection for
significantly improving IoT security against cyber-attacks, especially in
highly imbalanced data environments.

</details>


### [313] [Continuity and Isolation Lead to Doubts or Dilemmas in Large Language Models](https://arxiv.org/pdf/2505.10606)
*Hector Pasten, Felipe Urrutia, Hector Jimenez, Cristian B. Calderon, Crist√≥bal Rojas, Alexander Kozachinskiy*

Main category: cs.LG

TL;DR: The paper identifies two phenomena in Transformers, isolation and continuity, which limit their ability to learn certain sequences. It proves these issues arise in Transformers with compact positional encoding and validates them experimentally.


<details>
  <summary>Details</summary>
Motivation: To advance the understanding of Transformers by uncovering theoretical limitations in their learning capabilities, specifically isolation and continuity.

Method: Mathematical proof of the phenomena in Transformers with compact positional encoding, supported by rigorous experiments.

Result: Demonstrates that isolation and continuity hinder Transformers from learning simple sequences, with experimental validation.

Conclusion: The findings highlight inherent limitations in Transformers, impacting their design and application.

Abstract: Understanding how Transformers work and how they process information is key
to the theoretical and empirical advancement of these machines. In this work,
we demonstrate the existence of two phenomena in Transformers, namely isolation
and continuity. Both of these phenomena hinder Transformers to learn even
simple pattern sequences. Isolation expresses that any learnable sequence must
be isolated from another learnable sequence, and hence some sequences cannot be
learned by a single Transformer at the same time. Continuity entails that an
attractor basin forms around a learned sequence, such that any sequence falling
in that basin will collapse towards the learned sequence. Here, we
mathematically prove these phenomena emerge in all Transformers that use
compact positional encoding, and design rigorous experiments, demonstrating
that the theoretical limitations we shed light on occur on the practical scale.

</details>


### [314] [MONAQ: Multi-Objective Neural Architecture Querying for Time-Series Analysis on Resource-Constrained Devices](https://arxiv.org/pdf/2505.10607)
*Patara Trirat, Jae-Gil Lee*

Main category: cs.LG

TL;DR: MONAQ is a framework using LLMs for multi-objective neural architecture search (NAS) in time-series analysis, outperforming handcrafted models and NAS baselines.


<details>
  <summary>Details</summary>
Motivation: Efficient time-series analysis on resource-constrained hardware is needed for IoT and smartphone applications, but current NAS methods lack focus on general time-series tasks.

Method: MONAQ reformulates NAS into multi-objective querying tasks, using multimodal query generation and LLM-based search to create deployment-ready models.

Result: Experiments on 15 datasets show MONAQ-discovered models outperform handcrafted models and NAS baselines in efficiency and performance.

Conclusion: MONAQ successfully leverages LLMs for efficient and effective time-series NAS, addressing hardware constraints and multimodal inputs.

Abstract: The growing use of smartphones and IoT devices necessitates efficient
time-series analysis on resource-constrained hardware, which is critical for
sensing applications such as human activity recognition and air quality
prediction. Recent efforts in hardware-aware neural architecture search (NAS)
automate architecture discovery for specific platforms; however, none focus on
general time-series analysis with edge deployment. Leveraging the
problem-solving and reasoning capabilities of large language models (LLM), we
propose MONAQ, a novel framework that reformulates NAS into Multi-Objective
Neural Architecture Querying tasks. MONAQ is equipped with multimodal query
generation for processing multimodal time-series inputs and hardware
constraints, alongside an LLM agent-based multi-objective search to achieve
deployment-ready models via code generation. By integrating numerical data,
time-series images, and textual descriptions, MONAQ improves an LLM's
understanding of time-series data. Experiments on fifteen datasets demonstrate
that MONAQ-discovered models outperform both handcrafted models and NAS
baselines while being more efficient.

</details>


### [315] [How many measurements are enough? Bayesian recovery in inverse problems with general distributions](https://arxiv.org/pdf/2505.10630)
*Ben Adcock, Nick Huang*

Main category: cs.LG

TL;DR: The paper studies Bayesian recovery for inverse problems, focusing on sample complexity under general priors, forward operators, and noise. It provides non-asymptotic bounds and highlights the role of DNN-based priors and coherence in recovery.


<details>
  <summary>Details</summary>
Motivation: To understand the sample complexity of Bayesian recovery in inverse problems with general distributions, extending prior work and providing rigorous guarantees.

Method: Analyzes posterior sampling with an approximate prior, using intrinsic complexity (approximate covering number) and concentration bounds for forward operators and noise. Specializes to DNN-based priors and coherence analysis.

Result: Sample complexity scales log-linearly with latent dimension for DNN priors. Coherence of the forward operator determines recovery efficiency.

Conclusion: The framework unifies and extends prior work, offering guarantees for Bayesian inverse problems with arbitrary distributions, emphasizing the role of DNN priors and coherence.

Abstract: We study the sample complexity of Bayesian recovery for solving inverse
problems with general prior, forward operator and noise distributions. We
consider posterior sampling according to an approximate prior $\mathcal{P}$,
and establish sufficient conditions for stable and accurate recovery with high
probability. Our main result is a non-asymptotic bound that shows that the
sample complexity depends on (i) the intrinsic complexity of $\mathcal{P}$,
quantified by its so-called approximate covering number, and (ii) concentration
bounds for the forward operator and noise distributions. As a key application,
we specialize to generative priors, where $\mathcal{P}$ is the pushforward of a
latent distribution via a Deep Neural Network (DNN). We show that the sample
complexity scales log-linearly with the latent dimension $k$, thus establishing
the efficacy of DNN-based priors. Generalizing existing results on
deterministic (i.e., non-Bayesian) recovery for the important problem of random
sampling with an orthogonal matrix $U$, we show how the sample complexity is
determined by the coherence of $U$ with respect to the support of
$\mathcal{P}$. Hence, we establish that coherence plays a fundamental role in
Bayesian recovery as well. Overall, our framework unifies and extends prior
work, providing rigorous guarantees for the sample complexity of solving
Bayesian inverse problems with arbitrary distributions.

</details>


### [316] [FRET: Feature Redundancy Elimination for Test Time Adaptation](https://arxiv.org/pdf/2505.10641)
*Linjing You, Jiabao Lu, Xiayuan Huang, Xiangli Nie*

Main category: cs.LG

TL;DR: FRET introduces methods (S-FRET and G-FRET) to reduce feature redundancy in test-time adaptation, improving model generalization under distribution shifts.


<details>
  <summary>Details</summary>
Motivation: Addressing feature redundancy in embeddings during test-time adaptation, which existing methods overlook, to enhance model adaptability.

Method: S-FRET minimizes feature redundancy directly, while G-FRET combines GCN and contrastive learning to reduce redundancy and improve feature discriminability.

Result: G-FRET achieves state-of-the-art performance, extracting non-redundant and discriminative features for robust adaptation.

Conclusion: FRET, especially G-FRET, effectively improves test-time adaptation by addressing feature redundancy and enhancing discriminability.

Abstract: Test-Time Adaptation (TTA) aims to enhance the generalization of deep
learning models when faced with test data that exhibits distribution shifts
from the training data. In this context, only a pre-trained model and unlabeled
test data are available, making it particularly relevant for privacy-sensitive
applications. In practice, we observe that feature redundancy in embeddings
tends to increase as domain shifts intensify in TTA. However, existing TTA
methods often overlook this redundancy, which can hinder the model's
adaptability to new data. To address this issue, we introduce Feature
Redundancy Elimination for Test-time Adaptation (FRET), a novel perspective for
TTA. A straightforward approach (S-FRET) is to directly minimize the feature
redundancy score as an optimization objective to improve adaptation. Despite
its simplicity and effectiveness, S-FRET struggles with label shifts, limiting
its robustness in real-world scenarios. To mitigate this limitation, we further
propose Graph-based FRET (G-FRET), which integrates a Graph Convolutional
Network (GCN) with contrastive learning. This design not only reduces feature
redundancy but also enhances feature discriminability in both the
representation and prediction layers. Extensive experiments across multiple
model architectures, tasks, and datasets demonstrate the effectiveness of
S-FRET and show that G-FRET achieves state-of-the-art performance. Further
analysis reveals that G-FRET enables the model to extract non-redundant and
highly discriminative features during inference, thereby facilitating more
robust test-time adaptation.

</details>


### [317] [Accelerating Visual-Policy Learning through Parallel Differentiable Simulation](https://arxiv.org/pdf/2505.10646)
*Haoxiang You, Yilang Liu, Ian Abraham*

Main category: cs.LG

TL;DR: A computationally efficient algorithm for visual policy learning using differentiable simulation and first-order policy gradients, reducing overhead and improving stability.


<details>
  <summary>Details</summary>
Motivation: To streamline visual policy learning by decoupling rendering from computation, avoiding specialized tools and enhancing efficiency.

Method: Leverages differentiable simulation and first-order analytical policy gradients, decoupling rendering for seamless integration.

Result: Reduces training time, outperforms baselines, and achieves 4√ó improvement in final returns on complex tasks like humanoid locomotion.

Conclusion: The method is efficient, stable, and effective for visual policy learning, with significant performance gains.

Abstract: In this work, we propose a computationally efficient algorithm for visual
policy learning that leverages differentiable simulation and first-order
analytical policy gradients. Our approach decouple the rendering process from
the computation graph, enabling seamless integration with existing
differentiable simulation ecosystems without the need for specialized
differentiable rendering software. This decoupling not only reduces
computational and memory overhead but also effectively attenuates the policy
gradient norm, leading to more stable and smoother optimization. We evaluate
our method on standard visual control benchmarks using modern GPU-accelerated
simulation. Experiments show that our approach significantly reduces wall-clock
training time and consistently outperforms all baseline methods in terms of
final returns. Notably, on complex tasks such as humanoid locomotion, our
method achieves a $4\times$ improvement in final return, and successfully
learns a humanoid running policy within 4 hours on a single GPU.

</details>


### [318] [Seasonal Forecasting of Pan-Arctic Sea Ice with State Space Model](https://arxiv.org/pdf/2505.10665)
*Wei Wang, Weidong Yang, Lei Wang, Guihua Wang, Ruibo Lei*

Main category: cs.LG

TL;DR: IceMamba, a deep learning model with attention mechanisms, outperforms 25 other models in seasonal Arctic sea ice forecasting, excelling in RMSE and ACC metrics.


<details>
  <summary>Details</summary>
Motivation: The rapid decline of Arctic sea ice due to climate change necessitates accurate seasonal forecasts, but existing models (dynamical and deep learning) have limitations in long-term predictions and handling seasonal variations.

Method: Introduces IceMamba, a deep learning architecture combining attention mechanisms with state space models, and compares it against 25 forecast models.

Result: IceMamba excels in RMSE and ACC, ranking second in IIEE, demonstrating superior seasonal forecasting capabilities.

Conclusion: IceMamba improves forecasting accuracy, aiding climate adaptation strategies by better predicting sea ice variability.

Abstract: The rapid decline of Arctic sea ice resulting from anthropogenic climate
change poses significant risks to indigenous communities, ecosystems, and the
global climate system. This situation emphasizes the immediate necessity for
precise seasonal sea ice forecasts. While dynamical models perform well for
short-term forecasts, they encounter limitations in long-term forecasts and are
computationally intensive. Deep learning models, while more computationally
efficient, often have difficulty managing seasonal variations and uncertainties
when dealing with complex sea ice dynamics. In this research, we introduce
IceMamba, a deep learning architecture that integrates sophisticated attention
mechanisms within the state space model. Through comparative analysis of 25
renowned forecast models, including dynamical, statistical, and deep learning
approaches, our experimental results indicate that IceMamba delivers excellent
seasonal forecasting capabilities for Pan-Arctic sea ice concentration.
Specifically, IceMamba outperforms all tested models regarding average RMSE and
anomaly correlation coefficient (ACC) and ranks second in Integrated Ice Edge
Error (IIEE). This innovative approach enhances our ability to foresee and
alleviate the effects of sea ice variability, offering essential insights for
strategies aimed at climate adaptation.

</details>


### [319] [A Conformal Predictive Measure for Assessing Catastrophic Forgetting](https://arxiv.org/pdf/2505.10677)
*Ioannis Pitsiorlas, Nour Jamoussi, Marios Kountouris*

Main category: cs.LG

TL;DR: A new metric, CPCF, is introduced to assess catastrophic forgetting in continual learning using conformal prediction, showing strong correlation with task accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of quantifying and evaluating catastrophic forgetting (CF) in continual learning, which is crucial for real-world applications.

Method: Proposes the Conformal Prediction Confidence Factor (CPCF), leveraging adaptive conformal prediction to monitor model confidence on learned tasks.

Result: Experiments on four datasets show CPCF strongly correlates with task accuracy, validating its reliability and interpretability.

Conclusion: CPCF is a robust and effective tool for assessing CF in dynamic learning environments.

Abstract: This work introduces a novel methodology for assessing catastrophic
forgetting (CF) in continual learning. We propose a new conformal prediction
(CP)-based metric, termed the Conformal Prediction Confidence Factor (CPCF), to
quantify and evaluate CF effectively. Our framework leverages adaptive CP to
estimate forgetting by monitoring the model's confidence on previously learned
tasks. This approach provides a dynamic and practical solution for monitoring
and measuring CF of previous tasks as new ones are introduced, offering greater
suitability for real-world applications. Experimental results on four benchmark
datasets demonstrate a strong correlation between CPCF and the accuracy of
previous tasks, validating the reliability and interpretability of the proposed
metric. Our results highlight the potential of CPCF as a robust and effective
tool for assessing and understanding CF in dynamic learning environments.

</details>


### [320] [A probabilistic framework for dynamic quantization](https://arxiv.org/pdf/2505.10689)
*Gabriele Santini, Francesco Paissan, Elisabetta Farella*

Main category: cs.LG

TL;DR: A probabilistic framework for dynamic quantization of neural networks, enabling input-adaptive rescaling of quantization parameters with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: To improve computational efficiency in neural networks by dynamically adjusting quantization parameters per input, reducing memory overhead.

Method: Uses a probabilistic model on pre-activations via a lightweight surrogate for adaptive quantization.

Result: Validated on computer vision tasks with negligible performance loss, outperforming standard quantization in tradeoff.

Conclusion: The framework offers an efficient, adaptive quantization solution with balanced performance and overhead.

Abstract: We propose a probabilistic framework for dynamic quantization of neural
networks that allows for a computationally efficient input-adaptive rescaling
of the quantization parameters. Our framework applies a probabilistic model to
the network's pre-activations through a lightweight surrogate, enabling the
adaptive adjustment of the quantization parameters on a per-input basis without
significant memory overhead. We validate our approach on a set of popular
computer vision tasks and models, observing only a negligible loss in
performance. Our method strikes the best performance and computational overhead
tradeoff compared to standard quantization strategies.

</details>


### [321] [Asymptotically-Optimal Gaussian Bandits with Side Observations](https://arxiv.org/pdf/2505.10698)
*Alexia Atsidakou, Orestis Papadigenopoulos, Constantine Caramanis, Sujay Sanghavi, Sanjay Shakkottai*

Main category: cs.LG

TL;DR: The paper studies Gaussian bandits with general side information, introducing an LP-based lower bound and an asymptotically optimal algorithm.


<details>
  <summary>Details</summary>
Motivation: To address the problem of Gaussian bandits with arbitrary side information, generalizing standard bandits, full-feedback, and graph-structured feedback.

Method: Constructs an LP-based asymptotic instance-dependent lower bound on regret and proposes an asymptotically optimal algorithm.

Result: The LP lower bound is derived, and an optimal algorithm for the general setting is presented.

Conclusion: The work provides a theoretical foundation and practical solution for Gaussian bandits with side information.

Abstract: We study the problem of Gaussian bandits with general side information, as
first introduced by Wu, Szepesvari, and Gyorgy. In this setting, the play of an
arm reveals information about other arms, according to an arbitrary a priori
known side information matrix: each element of this matrix encodes the fidelity
of the information that the ``row'' arm reveals about the ``column'' arm. In
the case of Gaussian noise, this model subsumes standard bandits,
full-feedback, and graph-structured feedback as special cases. In this work, we
first construct an LP-based asymptotic instance-dependent lower bound on the
regret. The LP optimizes the cost (regret) required to reliably estimate the
suboptimality gap of each arm. This LP lower bound motivates our main
contribution: the first known asymptotically optimal algorithm for this general
setting.

</details>


### [322] [Clustering Rooftop PV Systems via Probabilistic Embeddings](https://arxiv.org/pdf/2505.10699)
*Kutay B√∂lat, Tarek Alskaif, Peter Palensky, Simon Tindemans*

Main category: cs.LG

TL;DR: A probabilistic embedding-based clustering framework is proposed to manage high-dimensional, missing-value-affected PV system data, outperforming physics-based methods.


<details>
  <summary>Details</summary>
Motivation: The rise in rooftop PV installations necessitates efficient monitoring and analysis of large, distributed time-series data with missing values.

Method: The framework encodes PV system power generation patterns as probability distributions, then clusters systems using statistical distances and agglomerative clustering.

Result: The method produces robust, uncertainty-aware cluster profiles and reliable missing-value imputation, outperforming physics-based baselines.

Conclusion: The framework effectively addresses PV data challenges, offering practical guidance for balancing performance and robustness.

Abstract: As the number of rooftop photovoltaic (PV) installations increases,
aggregators and system operators are required to monitor and analyze these
systems, raising the challenge of integration and management of large,
spatially distributed time-series data that are both high-dimensional and
affected by missing values. In this work, a probabilistic entity
embedding-based clustering framework is proposed to address these problems.
This method encodes each PV system's characteristic power generation patterns
and uncertainty as a probability distribution, then groups systems by their
statistical distances and agglomerative clustering. Applied to a multi-year
residential PV dataset, it produces concise, uncertainty-aware cluster profiles
that outperform a physics-based baseline in representativeness and robustness,
and support reliable missing-value imputation. A systematic hyperparameter
study further offers practical guidance for balancing model performance and
robustness.

</details>


### [323] [ZEUS: Zero-shot Embeddings for Unsupervised Separation of Tabular Data](https://arxiv.org/pdf/2505.10704)
*Patryk Marsza≈Çek, Tomasz Ku≈õmierczyk, Witold Wydma≈Ñski, Jacek Tabor, Marek ≈ömieja*

Main category: cs.LG

TL;DR: ZEUS is a zero-shot deep learning model for clustering tabular data without per-dataset tuning, outperforming traditional and deep learning methods.


<details>
  <summary>Details</summary>
Motivation: Challenges in clustering tabular data include dataset-dependent similarity and unstable performance due to lack of supervised signals.

Method: ZEUS uses zero-shot learning, decomposing datasets into meaningful components and leveraging pre-training on synthetic data.

Result: ZEUS matches or outperforms traditional and deep learning methods, offering faster and more user-friendly clustering.

Conclusion: ZEUS is the first zero-shot method for unsupervised tabular data clustering, demonstrating strong performance and generalization.

Abstract: Clustering tabular data remains a significant open challenge in data analysis
and machine learning. Unlike for image data, similarity between tabular records
often varies across datasets, making the definition of clusters highly
dataset-dependent. Furthermore, the absence of supervised signals complicates
hyperparameter tuning in deep learning clustering methods, frequently resulting
in unstable performance. To address these issues and reduce the need for
per-dataset tuning, we adopt an emerging approach in deep learning: zero-shot
learning. We propose ZEUS, a self-contained model capable of clustering new
datasets without any additional training or fine-tuning. It operates by
decomposing complex datasets into meaningful components that can then be
clustered effectively. Thanks to pre-training on synthetic datasets generated
from a latent-variable prior, it generalizes across various datasets without
requiring user intervention. To the best of our knowledge, ZEUS is the first
zero-shot method capable of generating embeddings for tabular data in a fully
unsupervised manner. Experimental results demonstrate that it performs on par
with or better than traditional clustering algorithms and recent deep
learning-based methods, while being significantly faster and more
user-friendly.

</details>


### [324] [GNN-Suite: a Graph Neural Network Benchmarking Framework for Biomedical Informatics](https://arxiv.org/pdf/2505.10711)
*Sebesty√©n Kamp, Giovanni Stracquadanio, T. Ian Simpson*

Main category: cs.LG

TL;DR: GNN-Suite is a modular framework for benchmarking GNNs in computational biology, demonstrating its utility in cancer-driver gene identification. GCN2 performed best, outperforming baseline models.


<details>
  <summary>Details</summary>
Motivation: To standardize GNN experimentation and reproducibility in computational biology, particularly for cancer-driver gene identification.

Method: GNN-Suite uses Nextflow for workflow standardization, evaluates diverse GNN architectures (e.g., GAT, GCN2) on PPI data from STRING/BioGRID, with uniform hyperparameters and 10 independent runs for robust metrics.

Result: GCN2 achieved the highest balanced accuracy (0.807) on STRING-based networks, with all GNNs outperforming the Logistic Regression baseline.

Conclusion: GNN-Suite aids in identifying optimal models and data integration methods, promoting reproducibility and benchmarking standards. Future work includes exploring more datasets and refining architectures.

Abstract: We present GNN-Suite, a robust modular framework for constructing and
benchmarking Graph Neural Network (GNN) architectures in computational biology.
GNN-Suite standardises experimentation and reproducibility using the Nextflow
workflow to evaluate GNN performance. We demonstrate its utility in identifying
cancer-driver genes by constructing molecular networks from protein-protein
interaction (PPI) data from STRING and BioGRID and annotating nodes with
features from the PCAWG, PID, and COSMIC-CGC repositories.
  Our design enables fair comparisons among diverse GNN architectures including
GAT, GAT3H, GCN, GCN2, GIN, GTN, HGCN, PHGCN, and GraphSAGE and a baseline
Logistic Regression (LR) model. All GNNs were configured as standardised
two-layer models and trained with uniform hyperparameters (dropout = 0.2; Adam
optimiser with learning rate = 0.01; and an adjusted binary cross-entropy loss
to address class imbalance) over an 80/20 train-test split for 300 epochs. Each
model was evaluated over 10 independent runs with different random seeds to
yield statistically robust performance metrics, with balanced accuracy (BACC)
as the primary measure. Notably, GCN2 achieved the highest BACC (0.807 +/-
0.035) on a STRING-based network, although all GNN types outperformed the LR
baseline, highlighting the advantage of network-based learning over
feature-only approaches.
  Our results show that a common framework for implementing and evaluating GNN
architectures aids in identifying not only the best model but also the most
effective means of incorporating complementary data. By making GNN-Suite
publicly available, we aim to foster reproducible research and promote improved
benchmarking standards in computational biology. Future work will explore
additional omics datasets and further refine network architectures to enhance
predictive accuracy and interpretability in biomedical applications.

</details>


### [325] [Learning Repetition-Invariant Representations for Polymer Informatics](https://arxiv.org/pdf/2505.10726)
*Yihan Zhu, Gang Liu, Eric Inae, Tengfei Luo, Meng Jiang*

Main category: cs.LG

TL;DR: GRIN is a new method for learning polymer representations invariant to repeating units, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing graph neural networks fail to model polymer structures consistently due to varying repeating units.

Method: GRIN uses graph-based maximum spanning tree alignment and repeat-unit augmentation for structural consistency.

Result: GRIN outperforms baselines, learning stable, invariant representations for polymers of unseen sizes.

Conclusion: GRIN provides a robust solution for polymer representation learning, ensuring invariance to repeating units.

Abstract: Polymers are large macromolecules composed of repeating structural units
known as monomers and are widely applied in fields such as energy storage,
construction, medicine, and aerospace. However, existing graph neural network
methods, though effective for small molecules, only model the single unit of
polymers and fail to produce consistent vector representations for the true
polymer structure with varying numbers of units. To address this challenge, we
introduce Graph Repetition Invariance (GRIN), a novel method to learn polymer
representations that are invariant to the number of repeating units in their
graph representations. GRIN integrates a graph-based maximum spanning tree
alignment with repeat-unit augmentation to ensure structural consistency. We
provide theoretical guarantees for repetition-invariance from both model and
data perspectives, demonstrating that three repeating units are the minimal
augmentation required for optimal invariant representation learning. GRIN
outperforms state-of-the-art baselines on both homopolymer and copolymer
benchmarks, learning stable, repetition-invariant representations that
generalize effectively to polymer chains of unseen sizes.

</details>


### [326] [Random Client Selection on Contrastive Federated Learning for Tabular Data](https://arxiv.org/pdf/2505.10759)
*Achmad Ginanjar, Xue Li, Priyanka Singh, Wen Hua*

Main category: cs.LG

TL;DR: The paper analyzes gradient-based attacks in Contrastive Federated Learning (CFL) and evaluates random client selection as a defense, showing its effectiveness in enhancing privacy.


<details>
  <summary>Details</summary>
Motivation: VFL enables privacy-preserving collaborative learning but is vulnerable to information leakage. CFL mitigates this but still faces gradient-based attacks.

Method: The study conducts a comprehensive experimental analysis of gradient-based attacks in CFL and tests random client selection as a defense.

Result: Random client selection is effective in defending against gradient attacks in CFL.

Conclusion: The findings offer insights for robust security in CFL, advancing secure collaborative learning frameworks.

Abstract: Vertical Federated Learning (VFL) has revolutionised collaborative machine
learning by enabling privacy-preserving model training across multiple parties.
However, it remains vulnerable to information leakage during intermediate
computation sharing. While Contrastive Federated Learning (CFL) was introduced
to mitigate these privacy concerns through representation learning, it still
faces challenges from gradient-based attacks. This paper presents a
comprehensive experimental analysis of gradient-based attacks in CFL
environments and evaluates random client selection as a defensive strategy.
Through extensive experimentation, we demonstrate that random client selection
proves particularly effective in defending against gradient attacks in the CFL
network. Our findings provide valuable insights for implementing robust
security measures in contrastive federated learning systems, contributing to
the development of more secure collaborative learning frameworks

</details>


### [327] [Deep Symbolic Optimization: Reinforcement Learning for Symbolic Mathematics](https://arxiv.org/pdf/2505.10762)
*Conor F. Hayes, Felipe Leno Da Silva, Jiachen Yang, T. Nathan Mundhenk, Chak Shing Lee, Jacob F. Pettit, Claudio Santiago, Sookyung Kim, Joanne T. Kim, Ignacio Aravena Solis, Ruben Glatt, Andre R. Goncalves, Alexander Ladd, Ahmet Can Solak, Thomas Desautels, Daniel Faissol, Brenden K. Petersen, Mikel Landajuela*

Main category: cs.LG

TL;DR: DSO is a framework for symbolic optimization in scientific discovery, combining neural networks and reinforcement learning to efficiently search for interpretable symbolic models.


<details>
  <summary>Details</summary>
Motivation: To automate the discovery of symbolic structures, such as mathematical models, for scientific applications.

Method: Formulates discovery as sequential decision-making, using a generative neural network and reinforcement learning, integrated with gradient-based, evolutionary, and local search techniques.

Result: Achieves state-of-the-art performance in accuracy and interpretability on benchmark problems.

Conclusion: DSO has transformative potential for automating symbolic optimization in scientific discovery.

Abstract: Deep Symbolic Optimization (DSO) is a novel computational framework that
enables symbolic optimization for scientific discovery, particularly in
applications involving the search for intricate symbolic structures. One
notable example is equation discovery, which aims to automatically derive
mathematical models expressed in symbolic form. In DSO, the discovery process
is formulated as a sequential decision-making task. A generative neural network
learns a probabilistic model over a vast space of candidate symbolic
expressions, while reinforcement learning strategies guide the search toward
the most promising regions. This approach integrates gradient-based
optimization with evolutionary and local search techniques, and it incorporates
in-situ constraints, domain-specific priors, and advanced policy optimization
methods. The result is a robust framework capable of efficiently exploring
extensive search spaces to identify interpretable and physically meaningful
models. Extensive evaluations on benchmark problems have demonstrated that DSO
achieves state-of-the-art performance in both accuracy and interpretability. In
this chapter, we provide a comprehensive overview of the DSO framework and
illustrate its transformative potential for automating symbolic optimization in
scientific discovery.

</details>


### [328] [Context-Aware Probabilistic Modeling with LLM for Multimodal Time Series Forecasting](https://arxiv.org/pdf/2505.10774)
*Yueyang Yao, Jiajun Li, Xingyuan Dai, MengMeng Zhang, Xiaoyan Gong, Fei-Yue Wang, Yisheng Lv*

Main category: cs.LG

TL;DR: CAPTime is a novel method for time series forecasting that integrates text and probabilistic LLM decoding, outperforming existing approaches in accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to effectively combine exogenous texts with probabilistic LLMs, limiting contextual awareness and distribution modeling.

Method: CAPTime uses a pretrained time series encoder and learnable interactions to align temporal patterns with textual contexts, enabling multimodal probabilistic forecasting via frozen LLMs.

Result: CAPTime achieves superior accuracy and generalization, especially in multimodal and data-scarce scenarios.

Conclusion: CAPTime addresses key limitations in time series forecasting by leveraging text-informed abstraction and LLM capabilities, proving robust and effective.

Abstract: Time series forecasting is important for applications spanning energy
markets, climate analysis, and traffic management. However, existing methods
struggle to effectively integrate exogenous texts and align them with the
probabilistic nature of large language models (LLMs). Current approaches either
employ shallow text-time series fusion via basic prompts or rely on
deterministic numerical decoding that conflict with LLMs' token-generation
paradigm, which limits contextual awareness and distribution modeling. To
address these limitations, we propose CAPTime, a context-aware probabilistic
multimodal time series forecasting method that leverages text-informed
abstraction and autoregressive LLM decoding. Our method first encodes temporal
patterns using a pretrained time series encoder, then aligns them with textual
contexts via learnable interactions to produce joint multimodal
representations. By combining a mixture of distribution experts with frozen
LLMs, we enable context-aware probabilistic forecasting while preserving LLMs'
inherent distribution modeling capabilities. Experiments on diverse time series
forecasting tasks demonstrate the superior accuracy and generalization of
CAPTime, particularly in multimodal scenarios. Additional analysis highlights
its robustness in data-scarce scenarios through hybrid probabilistic decoding.

</details>


### [329] [Cell Library Characterization for Composite Current Source Models Based on Gaussian Process Regression and Active Learning](https://arxiv.org/pdf/2505.10799)
*Tao Bai, Junzhuo Zhou, Zeyuan Deng, Peng Cao*

Main category: cs.LG

TL;DR: A Gaussian Process Regression (GPR) model with active learning (AL) is introduced to efficiently and accurately characterize the Composite Current Source (CCS) model, outperforming traditional methods in accuracy, runtime, and storage.


<details>
  <summary>Details</summary>
Motivation: The CCS model's high accuracy requirements, large data volume, and simulation costs present challenges, necessitating an improved characterization method.

Method: A novel GPR model with AL is developed to characterize CCS, reducing runtime and storage while maintaining high accuracy.

Result: Achieves 2.05 ps average absolute error and 2.27% relative error for 57 cells under 9 PVT corners, with 27% runtime and 19.5x storage reduction.

Conclusion: The GPR with AL framework effectively addresses CCS characterization challenges, offering superior performance over conventional tools.

Abstract: The composite current source (CCS) model has been adopted as an advanced
timing model that represents the current behavior of cells for improved
accuracy and better capability than traditional non-linear delay models (NLDM)
to model complex dynamic effects and interactions under advanced process nodes.
However, the high accuracy requirement, large amount of data and extensive
simulation cost pose severe challenges to CCS characterization. To address
these challenges, we introduce a novel Gaussian Process Regression(GPR) model
with active learning(AL) to establish the characterization framework
efficiently and accurately. Our approach significantly outperforms conventional
commercial tools as well as learning based approaches by achieving an average
absolute error of 2.05 ps and a relative error of 2.27% for current waveform of
57 cells under 9 process, voltage, temperature (PVT) corners with TSMC 22nm
process. Additionally, our model drastically reduces the runtime to 27% and the
storage by up to 19.5x compared with that required by commercial tools.

</details>


### [330] [Attention-Based Reward Shaping for Sparse and Delayed Rewards](https://arxiv.org/pdf/2505.10802)
*Ian Holmes, Min Chi*

Main category: cs.LG

TL;DR: ARES is an offline, transformer-based reward shaping method that improves RL performance in sparse/delayed reward settings.


<details>
  <summary>Details</summary>
Motivation: Sparse and delayed rewards hinder RL applications; ARES addresses this by creating dense rewards.

Method: Uses transformer attention to generate shaped rewards from episodes and final returns, works offline.

Result: ARES improves learning in delayed reward settings, even with low-quality data.

Conclusion: ARES is robust, works offline, and handles extreme reward delays, advancing RL in challenging scenarios.

Abstract: Sparse and delayed reward functions pose a significant obstacle for
real-world Reinforcement Learning (RL) applications. In this work, we propose
Attention-based REward Shaping (ARES), a general and robust algorithm which
uses a transformer's attention mechanism to generate shaped rewards and create
a dense reward function for any environment. ARES requires a set of episodes
and their final returns as input. It can be trained entirely offline and is
able to generate meaningful shaped rewards even when using small datasets or
episodes produced by agents taking random actions. ARES is compatible with any
RL algorithm and can handle any level of reward sparsity. In our experiments,
we focus on the most challenging case where rewards are fully delayed until the
end of each episode. We evaluate ARES across a diverse range of environments,
widely used RL algorithms, and baseline methods to assess the effectiveness of
the shaped rewards it produces. Our results show that ARES can significantly
improve learning in delayed reward settings, enabling RL agents to train in
scenarios that would otherwise require impractical amounts of data or even be
unlearnable. To our knowledge, ARES is the first approach that works fully
offline, remains robust to extreme reward delays and low-quality data, and is
not limited to goal-based tasks.

</details>


### [331] [Distilled Circuits: A Mechanistic Study of Internal Restructuring in Knowledge Distillation](https://arxiv.org/pdf/2505.10822)
*Reilly Haskins, Benjamin Adams*

Main category: cs.LG

TL;DR: The paper analyzes knowledge distillation's internal mechanisms, revealing how student models reorganize or discard teacher components, impacting robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: To understand the internal computational transformations in knowledge distillation, which remain poorly understood despite its widespread use.

Method: Applied mechanistic interpretability techniques to compare internal circuits, representations, and activations between GPT2-small (teacher) and DistilGPT2 (student). Introduced an alignment metric for functional comparison.

Result: Student models reorganize, compress, or discard teacher components, relying more on fewer parts. Functional behaviors are preserved, but internal computation shifts significantly.

Conclusion: Knowledge distillation alters internal computation, affecting model robustness and generalization, despite preserving functional outputs.

Abstract: Knowledge distillation compresses a larger neural model (teacher) into
smaller, faster student models by training them to match teacher outputs.
However, the internal computational transformations that occur during this
process remain poorly understood. We apply techniques from mechanistic
interpretability to analyze how internal circuits, representations, and
activation patterns differ between teacher and student. Focusing on GPT2-small
and its distilled counterpart DistilGPT2, we find that student models
reorganize, compress, and discard teacher components, often resulting in
stronger reliance on fewer individual components. To quantify functional
alignment beyond output similarity, we introduce an alignment metric based on
influence-weighted component similarity, validated across multiple tasks. Our
findings reveal that while knowledge distillation preserves broad functional
behaviors, it also causes significant shifts in internal computation, with
important implications for the robustness and generalization capacity of
distilled models.

</details>


### [332] [MergeBench: A Benchmark for Merging Domain-Specialized LLMs](https://arxiv.org/pdf/2505.10833)
*Yifei He, Siqi Zeng, Yuzheng Hu, Rui Yang, Tong Zhang, Han Zhao*

Main category: cs.LG

TL;DR: MergeBench is introduced to evaluate model merging at scale, covering diverse tasks and large models, providing guidelines and insights for future research.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of model merging lack scale and task diversity, raising questions about applicability to large, domain-specialized LLMs.

Method: MergeBench uses state-of-the-art models (Llama, Gemma) across 2B-9B scales and five domains, standardizing finetuning and evaluating eight merging methods.

Result: Merging performs better on stronger base models, with techniques like coefficient tuning and sparsification aiding knowledge retention. Challenges include computational costs and performance gaps.

Conclusion: MergeBench lays groundwork for future research, highlighting model merging's potential and remaining challenges.

Abstract: Model merging provides a scalable alternative to multi-task training by
combining specialized finetuned models through parameter arithmetic, enabling
efficient deployment without the need for joint training or access to all task
data. While recent methods have shown promise, existing evaluations are limited
in both model scale and task diversity, leaving open questions about their
applicability to large, domain-specialized LLMs. To tackle the challenges, we
introduce MergeBench, a comprehensive evaluation suite designed to assess model
merging at scale. MergeBench builds on state-of-the-art open-source language
models, including Llama and Gemma families at 2B to 9B scales, and covers five
key domains: instruction following, mathematics, multilingual understanding,
coding and safety. We standardize finetuning and evaluation protocols, and
assess eight representative merging methods across multi-task performance,
forgetting and runtime efficiency. Based on extensive experiments, we provide
practical guidelines for algorithm selection and share insights showing that
model merging tends to perform better on stronger base models, with techniques
such as merging coefficient tuning and sparsification improving knowledge
retention. However, several challenges remain, including the computational cost
on large models, the gap for in-domain performance compared to multi-task
models, and the underexplored role of model merging in standard LLM training
pipelines. We hope MergeBench provides a foundation for future research to
advance the understanding and practical application of model merging. We open
source our code at
\href{https://github.com/uiuctml/MergeBench}{https://github.com/uiuctml/MergeBench}.

</details>


### [333] [LARGO: Latent Adversarial Reflection through Gradient Optimization for Jailbreaking LLMs](https://arxiv.org/pdf/2505.10838)
*Ran Li, Hao Wang, Chengzhi Mao*

Main category: cs.LG

TL;DR: LARGO introduces a gradient-based latent space attack for jailbreaking LLMs, outperforming AutoDAN by 44 points in success rate.


<details>
  <summary>Details</summary>
Motivation: Efficient red-teaming methods are needed to uncover LLM vulnerabilities, as gradient-based methods struggle in discrete language spaces.

Method: LARGO optimizes adversarial latent vectors in the LLM's continuous latent space and decodes them into natural language recursively.

Result: LARGO achieves a 44-point higher attack success rate than AutoDAN on benchmarks like AdvBench and JailbreakBench.

Conclusion: LARGO demonstrates the efficacy of gradient-based optimization for attacking LLM internals, offering a potent alternative to agentic prompting.

Abstract: Efficient red-teaming method to uncover vulnerabilities in Large Language
Models (LLMs) is crucial. While recent attacks often use LLMs as optimizers,
the discrete language space make gradient-based methods struggle. We introduce
LARGO (Latent Adversarial Reflection through Gradient Optimization), a novel
latent self-reflection attack that reasserts the power of gradient-based
optimization for generating fluent jailbreaking prompts. By operating within
the LLM's continuous latent space, LARGO first optimizes an adversarial latent
vector and then recursively call the same LLM to decode the latent into natural
language. This methodology yields a fast, effective, and transferable attack
that produces fluent and stealthy prompts. On standard benchmarks like AdvBench
and JailbreakBench, LARGO surpasses leading jailbreaking techniques, including
AutoDAN, by 44 points in attack success rate. Our findings demonstrate a potent
alternative to agentic LLM prompting, highlighting the efficacy of interpreting
and attacking LLM internals through gradient optimization.

</details>


### [334] [Ready2Unlearn: A Learning-Time Approach for Preparing Models with Future Unlearning Readiness](https://arxiv.org/pdf/2505.10845)
*Hanyu Duan, Yi Yang, Ahmed Abbasi, Kar Yan Tam*

Main category: cs.LG

TL;DR: Ready2Unlearn is a proactive training-phase method for machine unlearning, improving efficiency and model readiness for future unlearning requests.


<details>
  <summary>Details</summary>
Motivation: Existing unlearning methods are reactive during deployment; Ready2Unlearn aims to prepare models during training for better unlearning performance.

Method: Leverages meta-learning to train models with unlearning readiness, compatible with gradient ascent-based unlearning algorithms.

Result: Reduced unlearning time, better model capability retention, and resistance to forgotten data recovery in vision and language tasks.

Conclusion: Encourages proactive strategies for reliable and principled machine unlearning.

Abstract: This paper introduces Ready2Unlearn, a learning-time optimization approach
designed to facilitate future unlearning processes. Unlike the majority of
existing unlearning efforts that focus on designing unlearning algorithms,
which are typically implemented reactively when an unlearning request is made
during the model deployment phase, Ready2Unlearn shifts the focus to the
training phase, adopting a "forward-looking" perspective. Building upon
well-established meta-learning principles, Ready2Unlearn proactively trains
machine learning models with unlearning readiness, such that they are well
prepared and can handle future unlearning requests in a more efficient and
principled manner. Ready2Unlearn is model-agnostic and compatible with any
gradient ascent-based machine unlearning algorithms. We evaluate the method on
both vision and language tasks under various unlearning settings, including
class-wise unlearning and random data unlearning. Experimental results show
that by incorporating such preparedness at training time, Ready2Unlearn
produces an unlearning-ready model state, which offers several key advantages
when future unlearning is required, including reduced unlearning time, improved
retention of overall model capability, and enhanced resistance to the
inadvertent recovery of forgotten data. We hope this work could inspire future
efforts to explore more proactive strategies for equipping machine learning
models with built-in readiness towards more reliable and principled machine
unlearning.

</details>


### [335] [AutoRAN: Weak-to-Strong Jailbreaking of Large Reasoning Models](https://arxiv.org/pdf/2505.10846)
*Jiacheng Liang, Tanqiu Jiang, Yuhui Wang, Rongyi Zhu, Fenglong Ma, Ting Wang*

Main category: cs.LG

TL;DR: AutoRAN is an automated jailbreak attack framework that uses a weak reasoning model to exploit vulnerabilities in stronger reasoning models, achieving near 100% success rates.


<details>
  <summary>Details</summary>
Motivation: To expose vulnerabilities in large reasoning models (LRMs) by demonstrating how weak models can exploit their reasoning structures.

Method: AutoRAN simulates target LRMs' reasoning, generates narrative prompts, and refines them iteratively using intermediate reasoning steps.

Result: Achieves near 100% success rates across various LRMs (e.g., GPT-o3/o4-mini, Gemini-2.5-Flash) on benchmarks like AdvBench and HarmBench.

Conclusion: Highlights the need for improved safety measures in reasoning-based models, as weak models can effectively exploit stronger ones.

Abstract: This paper presents AutoRAN, the first automated, weak-to-strong jailbreak
attack framework targeting large reasoning models (LRMs). At its core, AutoRAN
leverages a weak, less-aligned reasoning model to simulate the target model's
high-level reasoning structures, generates narrative prompts, and iteratively
refines candidate prompts by incorporating the target model's intermediate
reasoning steps. We evaluate AutoRAN against state-of-the-art LRMs including
GPT-o3/o4-mini and Gemini-2.5-Flash across multiple benchmark datasets
(AdvBench, HarmBench, and StrongReject). Results demonstrate that AutoRAN
achieves remarkable success rates (approaching 100%) within one or a few turns
across different LRMs, even when judged by a robustly aligned external model.
This work reveals that leveraging weak reasoning models can effectively exploit
the critical vulnerabilities of much more capable reasoning models,
highlighting the need for improved safety measures specifically designed for
reasoning-based models. The code for replicating AutoRAN and running records
are available at: (https://github.com/JACKPURCELL/AutoRAN-public). (warning:
this paper contains potentially harmful content generated by LRMs.)

</details>


### [336] [Foundation model for mass spectrometry proteomics](https://arxiv.org/pdf/2505.10848)
*Justin Sanders, Melih Yilmaz, Jacob H. Russell, Wout Bittremieux, William E. Fondrie, Nicholas M. Riley, Sewoong Oh, William Stafford Noble*

Main category: cs.LG

TL;DR: A foundation model for mass spectra is proposed, pre-trained on de novo sequencing, improving performance on downstream proteomics tasks like spectrum quality and modification predictions.


<details>
  <summary>Details</summary>
Motivation: To unify spectrum prediction tasks and enhance proteomics data analysis using machine learning.

Method: Pre-train a spectrum encoder with de novo sequencing, then apply it to downstream tasks via multi-task fine-tuning.

Result: The model improves performance on spectrum quality, chimericity, phosphorylation, and glycosylation predictions.

Conclusion: The foundation model generalizes well, enhances limited-data tasks, and benefits proteomics experiments.

Abstract: Mass spectrometry is the dominant technology in the field of proteomics,
enabling high-throughput analysis of the protein content of complex biological
samples. Due to the complexity of the instrumentation and resulting data,
sophisticated computational methods are required for the processing and
interpretation of acquired mass spectra. Machine learning has shown great
promise to improve the analysis of mass spectrometry data, with numerous
purpose-built methods for improving specific steps in the data acquisition and
analysis pipeline reaching widespread adoption. Here, we propose unifying
various spectrum prediction tasks under a single foundation model for mass
spectra. To this end, we pre-train a spectrum encoder using de novo sequencing
as a pre-training task. We then show that using these pre-trained spectrum
representations improves our performance on the four downstream tasks of
spectrum quality prediction, chimericity prediction, phosphorylation
prediction, and glycosylation status prediction. Finally, we perform multi-task
fine-tuning and find that this approach improves the performance on each task
individually. Overall, our work demonstrates that a foundation model for tandem
mass spectrometry proteomics trained on de novo sequencing learns generalizable
representations of spectra, improves performance on downstream tasks where
training data is limited, and can ultimately enhance data acquisition and
analysis in proteomics experiments.

</details>


### [337] [ImputeINR: Time Series Imputation via Implicit Neural Representations for Disease Diagnosis with Missing Data](https://arxiv.org/pdf/2505.10856)
*Mengxuan Li, Ke Liu, Jialong Guo, Jiajun Bu, Hongwei Wang, Haishuai Wang*

Main category: cs.LG

TL;DR: ImputeINR uses implicit neural representations for continuous time series imputation, outperforming existing methods, especially for high missing ratios, and improves downstream disease diagnosis.


<details>
  <summary>Details</summary>
Motivation: Healthcare data often have missing values, and current imputation methods struggle with sparse data and high missing ratios.

Method: ImputeINR employs implicit neural representations (INR) to learn continuous functions for time series, enabling fine-grained imputation even with sparse data.

Result: Experiments on eight datasets show ImputeINR's superior performance, particularly for high missing ratios, and it enhances downstream disease diagnosis tasks.

Conclusion: ImputeINR is an effective solution for time series imputation in healthcare, improving both imputation accuracy and downstream task performance.

Abstract: Healthcare data frequently contain a substantial proportion of missing
values, necessitating effective time series imputation to support downstream
disease diagnosis tasks. However, existing imputation methods focus on discrete
data points and are unable to effectively model sparse data, resulting in
particularly poor performance for imputing substantial missing values. In this
paper, we propose a novel approach, ImputeINR, for time series imputation by
employing implicit neural representations (INR) to learn continuous functions
for time series. ImputeINR leverages the merits of INR in that the continuous
functions are not coupled to sampling frequency and have infinite sampling
frequency, allowing ImputeINR to generate fine-grained imputations even on
extremely sparse observed values. Extensive experiments conducted on eight
datasets with five ratios of masked values show the superior imputation
performance of ImputeINR, especially for high missing ratios in time series
data. Furthermore, we validate that applying ImputeINR to impute missing values
in healthcare data enhances the performance of downstream disease diagnosis
tasks. Codes are available.

</details>


### [338] [On DeepSeekMoE: Statistical Benefits of Shared Experts and Normalized Sigmoid Gating](https://arxiv.org/pdf/2505.10860)
*Huy Nguyen, Thong T. Doan, Quang Pham, Nghi D. Q. Bui, Nhat Ho, Alessandro Rinaldo*

Main category: cs.LG

TL;DR: The paper theoretically and empirically analyzes DeepSeekMoE's shared expert strategy and normalized sigmoid gating, showing their benefits in sample efficiency and expert utilization.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in theoretical understanding of DeepSeekMoE's unique features (shared expert strategy and normalized sigmoid gating) and justify their value.

Method: Convergence analysis of expert estimation and empirical experiments on synthetic and real-world datasets for vision/language tasks.

Result: Demonstrates gains in sample efficiency and provides insights into expert/gating design. Empirical analysis reveals router behaviors like saturation and utilization.

Conclusion: The study validates the effectiveness of DeepSeekMoE's features, offering practical design insights for MoE architectures.

Abstract: Mixture of experts (MoE) methods are a key component in most large language
model architectures, including the recent series of DeepSeek models. Compared
to other MoE implementations, DeepSeekMoE stands out because of two unique
features: the deployment of a shared expert strategy and of the normalized
sigmoid gating mechanism. Despite the prominent role of DeepSeekMoE in the
success of the DeepSeek series of models, there have been only a few attempts
to justify theoretically the value of the shared expert strategy, while its
normalized sigmoid gating has remained unexplored. To bridge this gap, we
undertake a comprehensive theoretical study of these two features of
DeepSeekMoE from a statistical perspective. We perform a convergence analysis
of the expert estimation task to highlight the gains in sample efficiency for
both the shared expert strategy and the normalized sigmoid gating, offering
useful insights into the design of expert and gating structures. To verify
empirically our theoretical findings, we carry out several experiments on both
synthetic data and real-world datasets for (vision) language modeling tasks.
Finally, we conduct an extensive empirical analysis of the router behaviors,
ranging from router saturation, router change rate, to expert utilization.

</details>


### [339] [Improving the Data-efficiency of Reinforcement Learning by Warm-starting with LLM](https://arxiv.org/pdf/2505.10861)
*Thang Duong, Minglai Yang, Chicheng Zhang*

Main category: cs.LG

TL;DR: LORO uses LLMs to generate high-quality data for RL warm-starting, outperforming baselines in sample efficiency and cumulative rewards.


<details>
  <summary>Details</summary>
Motivation: To improve RL sample efficiency by leveraging LLMs for generating optimal policy-like data.

Method: LORO combines LLM-generated off-policy datasets with RL exploration to refine policies.

Result: LORO achieves up to 4√ó cumulative rewards of pure RL baselines in OpenAI Gym environments.

Conclusion: LORO demonstrates the potential of LLMs in enhancing RL efficiency and performance.

Abstract: We investigate the usage of Large Language Model (LLM) in collecting
high-quality data to warm-start Reinforcement Learning (RL) algorithms for
learning in some classical Markov Decision Process (MDP) environments. In this
work, we focus on using LLM to generate an off-policy dataset that sufficiently
covers state-actions visited by optimal policies, then later using an RL
algorithm to explore the environment and improve the policy suggested by the
LLM. Our algorithm, LORO, can both converge to an optimal policy and have a
high sample efficiency thanks to the LLM's good starting policy. On multiple
OpenAI Gym environments, such as CartPole and Pendulum, we empirically
demonstrate that LORO outperforms baseline algorithms such as pure LLM-based
policies, pure RL, and a naive combination of the two, achieving up to $4
\times$ the cumulative rewards of the pure RL baseline.

</details>


### [340] [Hashing for Structure-based Anomaly Detection](https://arxiv.org/pdf/2505.10873)
*Filippo Leveni, Luca Magri, Cesare Alippi, Giacomo Boracchi*

Main category: cs.LG

TL;DR: The paper introduces an isolation-based anomaly detection method using Locality Sensitive Hashing in Preference Space for efficiency.


<details>
  <summary>Details</summary>
Motivation: To identify anomalies in datasets by leveraging structured patterns and high-dimensional embeddings.

Method: Uses Locality Sensitive Hashing to avoid explicit distance computations in high dimensions, improving efficiency.

Result: Achieves state-of-the-art performance with lower computational cost.

Conclusion: The proposed technique is effective and efficient for anomaly detection in structured datasets.

Abstract: We focus on the problem of identifying samples in a set that do not conform
to structured patterns represented by low-dimensional manifolds. An effective
way to solve this problem is to embed data in a high dimensional space, called
Preference Space, where anomalies can be identified as the most isolated
points. In this work, we employ Locality Sensitive Hashing to avoid explicit
computation of distances in high dimensions and thus improve Anomaly Detection
efficiency. Specifically, we present an isolation-based anomaly detection
technique designed to work in the Preference Space which achieves
state-of-the-art performance at a lower computational cost. Code is publicly
available at
https://github.com/ineveLoppiliF/Hashing-for-Structure-based-Anomaly-Detection.

</details>


### [341] [MultiLink: Multi-class Structure Recovery via Agglomerative Clustering and Model Selection](https://arxiv.org/pdf/2505.10874)
*Luca Magri, Filippo Leveni, Giacomo Boracchi*

Main category: cs.LG

TL;DR: MultiLink algorithm robustly fits multiple geometric structures in noisy datasets, outperforming state-of-the-art methods in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Recovering multiple geometric structures (e.g., planes, cylinders) in noisy datasets with outliers is challenging. Existing methods are slow and sensitive to thresholds.

Method: MultiLink combines on-the-fly model fitting and selection via a novel linkage scheme to merge clusters, improving efficiency and robustness.

Result: MultiLink is faster, less sensitive to inlier thresholds, and outperforms alternatives in multi-class and single-class problems.

Conclusion: MultiLink offers a practical, efficient solution for robust geometric structure recovery, with publicly available code.

Abstract: We address the problem of recovering multiple structures of different classes
in a dataset contaminated by noise and outliers. In particular, we consider
geometric structures defined by a mixture of underlying parametric models (e.g.
planes and cylinders, homographies and fundamental matrices), and we tackle the
robust fitting problem by preference analysis and clustering. We present a new
algorithm, termed MultiLink, that simultaneously deals with multiple classes of
models. MultiLink combines on-the-fly model fitting and model selection in a
novel linkage scheme that determines whether two clusters are to be merged. The
resulting method features many practical advantages with respect to methods
based on preference analysis, being faster, less sensitive to the inlier
threshold, and able to compensate limitations deriving from hypotheses
sampling. Experiments on several public datasets demonstrate that Multi-Link
favourably compares with state of the art alternatives, both in multi-class and
single-class problems. Code is publicly made available for download.

</details>


### [342] [Preference Isolation Forest for Structure-based Anomaly Detection](https://arxiv.org/pdf/2505.10876)
*Filippo Leveni, Luca Magri, Cesare Alippi, Giacomo Boracchi*

Main category: cs.LG

TL;DR: PIF framework detects anomalies by embedding data into a preference space and isolating outliers using three methods: Voronoi-iForest, RuzHash-iForest, and Sliding-PIF.


<details>
  <summary>Details</summary>
Motivation: Detecting anomalies as samples deviating from low-dimensional manifolds.

Method: Combines adaptive isolation-based methods with preference embedding, proposing three isolation approaches.

Result: Anomalies are identified as isolated points in the preference space.

Conclusion: PIF offers a flexible and efficient framework for anomaly detection.

Abstract: We address the problem of detecting anomalies as samples that do not conform
to structured patterns represented by low-dimensional manifolds. To this end,
we conceive a general anomaly detection framework called Preference Isolation
Forest (PIF), that combines the benefits of adaptive isolation-based methods
with the flexibility of preference embedding. The key intuition is to embed the
data into a high-dimensional preference space by fitting low-dimensional
manifolds, and to identify anomalies as isolated points. We propose three
isolation approaches to identify anomalies: $i$) Voronoi-iForest, the most
general solution, $ii$) RuzHash-iForest, that avoids explicit computation of
distances via Local Sensitive Hashing, and $iii$) Sliding-PIF, that leverages a
locality prior to improve efficiency and effectiveness.

</details>


### [343] [Graph and Simplicial Complex Prediction Gaussian Process via the Hodgelet Representations](https://arxiv.org/pdf/2505.10877)
*Mathieu Alain, So Takao, Xiaowen Dong, Bastian Rieck, Emmanuel Noutahi*

Main category: cs.LG

TL;DR: Extending Gaussian processes to simplicial complexes for better graph-level predictions, especially with scarce data.


<details>
  <summary>Details</summary>
Motivation: GNNs overfit with scarce data; GPs offer an alternative but need extension to handle higher-order graph structures like simplicial complexes.

Method: Extend GPs to simplicial complexes, incorporate Hodge decompositions to capture homological information (e.g., holes).

Result: Improved predictions across applications, enabling broader use of GPs for graph and SC-level tasks.

Conclusion: The framework enhances GP applicability for graph-structured data, addressing GNN limitations in data-scarce scenarios.

Abstract: Predicting the labels of graph-structured data is crucial in scientific
applications and is often achieved using graph neural networks (GNNs). However,
when data is scarce, GNNs suffer from overfitting, leading to poor performance.
Recently, Gaussian processes (GPs) with graph-level inputs have been proposed
as an alternative. In this work, we extend the Gaussian process framework to
simplicial complexes (SCs), enabling the handling of edge-level attributes and
attributes supported on higher-order simplices. We further augment the
resulting SC representations by considering their Hodge decompositions,
allowing us to account for homological information, such as the number of
holes, in the SC. We demonstrate that our framework enhances the predictions
across various applications, paving the way for GPs to be more widely used for
graph and SC-level predictions.

</details>


### [344] [Approximation and Generalization Abilities of Score-based Neural Network Generative Models for Sub-Gaussian Distributions](https://arxiv.org/pdf/2505.10880)
*Guoji Fu, Wee Sun Lee*

Main category: cs.LG

TL;DR: The paper analyzes the approximation and generalization capabilities of score-based neural network generative models (SGMs) for estimating an unknown distribution, achieving nearly optimal convergence rates under mild assumptions.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the theoretical performance of SGMs in distribution estimation, relaxing stringent assumptions like Lipschitz continuity or strict positivity of the target density.

Method: The authors use deep ReLU neural networks with specific width and depth constraints to approximate scores, employing an early stopping strategy for convergence.

Result: They prove nearly optimal mean square error and score matching loss rates, even under weaker assumptions like sub-Gaussianity or Sobolev/Besov class densities.

Conclusion: The framework is universal and improves upon previous work by removing restrictive assumptions, demonstrating SGMs' robustness in distribution estimation.

Abstract: This paper studies the approximation and generalization abilities of
score-based neural network generative models (SGMs) in estimating an unknown
distribution $P_0$ from $n$ i.i.d. observations in $d$ dimensions. Assuming
merely that $P_0$ is $\alpha$-sub-Gaussian, we prove that for any time step $t
\in [t_0, n^{O(1)}]$, where $t_0 \geq O(\alpha^2n^{-2/d}\log n)$, there exists
a deep ReLU neural network with width $\leq O(\log^3n)$ and depth $\leq
O(n^{3/d}\log_2n)$ that can approximate the scores with $\tilde{O}(n^{-1})$
mean square error and achieve a nearly optimal rate of
$\tilde{O}(n^{-1}t_0^{-d/2})$ for score estimation, as measured by the score
matching loss. Our framework is universal and can be used to establish
convergence rates for SGMs under milder assumptions than previous work. For
example, assuming further that the target density function $p_0$ lies in
Sobolev or Besov classes, with an appropriately early stopping strategy, we
demonstrate that neural network-based SGMs can attain nearly minimax
convergence rates up to logarithmic factors. Our analysis removes several
crucial assumptions, such as Lipschitz continuity of the score function or a
strictly positive lower bound on the target density.

</details>


### [345] [Prior-Guided Diffusion Planning for Offline Reinforcement Learning](https://arxiv.org/pdf/2505.10881)
*Donghyeon Ki, JunHyeok Oh, Seong-Woong Shim, Byung-Jun Lee*

Main category: cs.LG

TL;DR: The paper introduces Prior Guidance (PG), a novel guided sampling framework for diffusion models in offline RL, addressing issues like suboptimal actions and high inference costs by replacing the Gaussian prior with a learnable distribution.


<details>
  <summary>Details</summary>
Motivation: Existing guided sampling strategies in diffusion models for offline RL suffer from suboptimal multi-modal actions, distributional drift, or high inference-time costs.

Method: PG replaces the standard Gaussian prior with a learnable distribution optimized via behavior-regularized objectives, enabling direct generation of high-value trajectories without costly reward optimization.

Result: PG outperforms state-of-the-art diffusion policies and planners in diverse long-horizon offline RL benchmarks.

Conclusion: PG offers an efficient and effective solution for high-quality trajectory generation in offline RL, eliminating the need for costly sampling strategies.

Abstract: Diffusion models have recently gained prominence in offline reinforcement
learning due to their ability to effectively learn high-performing,
generalizable policies from static datasets. Diffusion-based planners
facilitate long-horizon decision-making by generating high-quality trajectories
through iterative denoising, guided by return-maximizing objectives. However,
existing guided sampling strategies such as Classifier Guidance,
Classifier-Free Guidance, and Monte Carlo Sample Selection either produce
suboptimal multi-modal actions, struggle with distributional drift, or incur
prohibitive inference-time costs. To address these challenges, we propose Prior
Guidance (PG), a novel guided sampling framework that replaces the standard
Gaussian prior of a behavior-cloned diffusion model with a learnable
distribution, optimized via a behavior-regularized objective. PG directly
generates high-value trajectories without costly reward optimization of the
diffusion model itself, and eliminates the need to sample multiple candidates
at inference for sample selection. We present an efficient training strategy
that applies behavior regularization in latent space, and empirically
demonstrate that PG outperforms state-of-the-art diffusion policies and
planners across diverse long-horizon offline RL benchmarks.

</details>


### [346] [Global Convergence of Adaptive Sensing for Principal Eigenvector Estimation](https://arxiv.org/pdf/2505.10882)
*Alex Saad-Falcon, Brighton Ancelin, Justin Romberg*

Main category: cs.LG

TL;DR: The paper introduces a compressively sampled variant of Oja's algorithm for efficient PCA in high-dimensional spaces, proving global convergence with adaptive sensing and noise.


<details>
  <summary>Details</summary>
Motivation: Traditional PCA methods are computationally expensive in high dimensions, while subspace tracking algorithms like Oja's require full-dimensional observations. This work addresses these limitations.

Method: A variant of Oja's algorithm is analyzed, using two compressed measurements per iteration: one in the current estimate direction and one in a random orthogonal direction.

Result: The algorithm achieves global convergence with noise, featuring a warmup phase and a local convergence phase with specific iteration bounds.

Conclusion: This work provides the first convergence guarantees for adaptive sensing in subspace tracking with noise, with simpler proofs and practical implications for high-dimensional data.

Abstract: This paper addresses the challenge of efficient principal component analysis
(PCA) in high-dimensional spaces by analyzing a compressively sampled variant
of Oja's algorithm with adaptive sensing. Traditional PCA methods incur
substantial computational costs that scale poorly with data dimensionality,
whereas subspace tracking algorithms like Oja's offer more efficient
alternatives but typically require full-dimensional observations. We analyze a
variant where, at each iteration, only two compressed measurements are taken:
one in the direction of the current estimate and one in a random orthogonal
direction. We prove that this adaptive sensing approach achieves global
convergence in the presence of noise when tracking the leading eigenvector of a
datastream with eigengap $\Delta=\lambda_1-\lambda_2$. Our theoretical analysis
demonstrates that the algorithm experiences two phases: (1) a warmup phase
requiring $O(\lambda_1\lambda_2d^2/\Delta^2)$ iterations to achieve a
constant-level alignment with the true eigenvector, followed by (2) a local
convergence phase where the sine alignment error decays at a rate of
$O(\lambda_1\lambda_2d^2/\Delta^2 t)$ for iterations $t$. The guarantee aligns
with existing minimax lower bounds with an added factor of $d$ due to the
compressive sampling. This work provides the first convergence guarantees in
adaptive sensing for subspace tracking with noise. Our proof technique is also
considerably simpler than those in prior works. The results have important
implications for applications where acquiring full-dimensional samples is
challenging or costly.

</details>


### [347] [Multi-Objective Preference Optimization: Improving Human Alignment of Generative Models](https://arxiv.org/pdf/2505.10892)
*Akhil Agnihotri, Rahul Jain, Deepak Ramachandran, Zheng Wen*

Main category: cs.LG

TL;DR: The paper introduces MOPO, a multi-objective preference optimization algorithm for aligning LLMs with multiple human objectives, outperforming single-objective methods.


<details>
  <summary>Details</summary>
Motivation: Existing preference optimization techniques like RLHF and DPO only handle single objectives, but humans have multiple, potentially conflicting goals (e.g., helpfulness and harmlessness). MOPO addresses this gap.

Method: MOPO frames alignment as a constrained KL-regularized optimization, maximizing the primary objective while bounding secondary objectives with safety thresholds. It works directly on pairwise preference data without point-wise rewards or heuristic prompts.

Result: MOPO approximates the Pareto front in synthetic benchmarks and outperforms baselines in real-world experiments, achieving higher rewards and stable optimization.

Conclusion: MOPO effectively handles multi-objective alignment, offering a scalable and robust solution for optimizing conflicting human preferences in LLMs.

Abstract: Post-training of LLMs with RLHF, and subsequently preference optimization
algorithms such as DPO, IPO, etc., made a big difference in improving human
alignment. However, all such techniques can only work with a single (human)
objective. In practice, human users have multiple objectives, such as
helpfulness and harmlessness, and there is no natural way to aggregate them
into a single objective. In this paper, we address the multi-objective
preference-alignment problem, where a policy must optimize several, potentially
conflicting, objectives. We introduce the Multi-Objective Preference
Optimization (MOPO) algorithm, which frames alignment as a constrained
KL-regularized optimization: the primary objective is maximized while secondary
objectives are lower-bounded by tunable safety thresholds. Unlike prior work,
MOPO operates directly on pairwise preference data, requires no point-wise
reward assumption, and avoids heuristic prompt-context engineering. The method
recovers policies on the Pareto front whenever the front is attainable;
practically, it reduces to simple closed-form iterative updates suitable for
large-scale training. On synthetic benchmarks with diverse canonical preference
structures, we show that MOPO approximates the Pareto front. When fine-tuning a
1.3B-parameter language model on real-world human-preference datasets, MOPO
attains higher rewards and yields policies that Pareto-dominate baselines;
ablation studies confirm optimization stability and robustness to
hyperparameters.

</details>


### [348] [CTP: A hybrid CNN-Transformer-PINN model for ocean front forecasting](https://arxiv.org/pdf/2505.10894)
*Yishuo Wang, Feng Zhou, Muping Zhou, Qicheng Meng, Zhijun Hu, Yi Wang*

Main category: cs.LG

TL;DR: CTP is a deep learning framework combining CNN, Transformer, and PINN for ocean front prediction, outperforming existing methods in accuracy and stability.


<details>
  <summary>Details</summary>
Motivation: Ocean fronts are critical in marine processes, but current methods like LSTM and ConvLSTM struggle with spatial continuity and physical consistency in forecasts.

Method: CTP integrates CNN for spatial encoding, Transformer for temporal attention, and PINN for physical constraints.

Result: CTP achieves SOTA performance in single and multi-step predictions, excelling in accuracy, $F_1$ score, and stability in SCS and KUR regions.

Conclusion: CTP effectively addresses limitations of existing methods, offering improved ocean front prediction with physical consistency.

Abstract: This paper proposes CTP, a novel deep learning framework that integrates
convolutional neural network(CNN), Transformer architectures, and
physics-informed neural network(PINN) for ocean front prediction. Ocean fronts,
as dynamic interfaces between distinct water masses, play critical roles in
marine biogeochemical and physical processes. Existing methods such as LSTM,
ConvLSTM, and AttentionConv often struggle to maintain spatial continuity and
physical consistency over multi-step forecasts. CTP addresses these challenges
by combining localized spatial encoding, long-range temporal attention, and
physical constraint enforcement. Experimental results across south China
sea(SCS) and Kuroshio(KUR) regions from 1993 to 2020 demonstrate that CTP
achieves state-of-the-art(SOTA) performance in both single-step and multi-step
predictions, significantly outperforming baseline models in accuracy, $F_1$
score, and temporal stability.

</details>


### [349] [Automated Identification of Logical Errors in Programs: Advancing Scalable Analysis of Student Misconceptions](https://arxiv.org/pdf/2505.10913)
*Muntasir Hoq, Ananya Rao, Reisha Jaishankar, Krish Piryani, Nithya Janapati, Jessica Vandenberg, Bradford Mott, Narges Norouzi, James Lester, Bita Akram*

Main category: cs.LG

TL;DR: A scalable framework using AST embeddings (SANN) detects logical errors in students' code, aiding targeted learning support in CS education.


<details>
  <summary>Details</summary>
Motivation: Understanding programming difficulties helps educators provide effective support. Identifying logical errors in real-time is challenging but crucial for improving learning outcomes.

Method: The framework uses an explainable AST embedding model (SANN) to analyze structural components of programs for logical errors.

Result: Experiments show the framework accurately detects logical errors and provides insights into students' learning processes.

Conclusion: The framework enhances programming education by offering a tool for real-time error detection and deeper learning insights.

Abstract: In Computer Science (CS) education, understanding factors contributing to
students' programming difficulties is crucial for effective learning support.
By identifying specific issues students face, educators can provide targeted
assistance to help them overcome obstacles and improve learning outcomes. While
identifying sources of struggle, such as misconceptions, in real-time can be
challenging in current educational practices, analyzing logical errors in
students' code can offer valuable insights. This paper presents a scalable
framework for automatically detecting logical errors in students' programming
solutions. Our framework is based on an explainable Abstract Syntax Tree (AST)
embedding model, the Subtree-based Attention Neural Network (SANN), that
identifies the structural components of programs containing logical errors. We
conducted a series of experiments to evaluate its effectiveness, and the
results suggest that our framework can accurately capture students' logical
errors and, more importantly, provide us with deeper insights into their
learning processes, offering a valuable tool for enhancing programming
education.

</details>


### [350] [A Dataset for Spatiotemporal-Sensitive POI Question Answering](https://arxiv.org/pdf/2505.10928)
*Xiao Han, Dayan Pan, Xiangyu Zhao, Xuyuan Hu, Zhaolin Deng, Xiangjie Kong, Guojiang Shen*

Main category: cs.LG

TL;DR: POI-QA is a new QA dataset for evaluating spatiotemporal reasoning, revealing limitations in current LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing QA datasets lack spatiotemporal-sensitive questions, limiting evaluation of models' reasoning capabilities.

Method: Constructed POI-QA by aligning vehicle trajectory data with POI data, validating facts, and generating bilingual QA pairs.

Result: Top-performing LLM (Qwen2.5-7B) achieved HR@10 of 0.41, below human performance (0.56).

Conclusion: POI-QA highlights LLMs' weaknesses in spatiotemporal reasoning and serves as a benchmark for improvement.

Abstract: Spatiotemporal relationships are critical in data science, as many prediction
and reasoning tasks require analysis across both spatial and temporal
dimensions--for instance, navigating an unfamiliar city involves planning
itineraries that sequence locations and timing cultural experiences. However,
existing Question-Answering (QA) datasets lack sufficient
spatiotemporal-sensitive questions, making them inadequate benchmarks for
evaluating models' spatiotemporal reasoning capabilities. To address this gap,
we introduce POI-QA, a novel spatiotemporal-sensitive QA dataset centered on
Point of Interest (POI), constructed through three key steps: mining and
aligning open-source vehicle trajectory data from GAIA with high-precision
geographic POI data, rigorous manual validation of noisy spatiotemporal facts,
and generating bilingual (Chinese/English) QA pairs that reflect
human-understandable spatiotemporal reasoning tasks. Our dataset challenges
models to parse complex spatiotemporal dependencies, and evaluations of
state-of-the-art multilingual LLMs (e.g., Qwen2.5-7B, Llama3.1-8B) reveal stark
limitations: even the top-performing model (Qwen2.5-7B fine-tuned with
RAG+LoRA) achieves a top 10 Hit Ratio (HR@10) of only 0.41 on the easiest task,
far below human performance at 0.56. This underscores persistent weaknesses in
LLMs' ability to perform consistent spatiotemporal reasoning, while
highlighting POI-QA as a robust benchmark to advance algorithms sensitive to
spatiotemporal dynamics. The dataset is publicly available at
https://www.kaggle.com/ds/7394666.

</details>


### [351] [Physics-informed Temporal Alignment for Auto-regressive PDE Foundation Models](https://arxiv.org/pdf/2505.10930)
*Congcong Zhu, Xiaoyan Xu, Jiayue Han, Jingrun Chen*

Main category: cs.LG

TL;DR: PITA, a self-supervised learning framework, addresses error accumulation in auto-regressive PDE models by aligning physical dynamics with physics-informed constraints, improving accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Auto-regressive PDE models suffer from error accumulation, especially for out-of-distribution data, limiting their effectiveness in long-term dynamics.

Method: Proposes physics-informed temporal alignment (PITA), integrating physics constraints into self-supervision to align dynamics across time steps.

Result: PITA significantly improves accuracy and robustness of foundation models on diverse PDE data.

Conclusion: PITA effectively mitigates error accumulation and enhances generalization for out-of-distribution PDE tasks.

Abstract: Auto-regressive partial differential equation (PDE) foundation models have
shown great potential in handling time-dependent data. However, these models
suffer from the shortcut problem deeply rooted in auto-regressive prediction,
causing error accumulation. The challenge becomes particularly evident for
out-of-distribution data, as the pretraining performance may approach random
model initialization for downstream tasks with long-term dynamics. To deal with
this problem, we propose physics-informed temporal alignment (PITA), a
self-supervised learning framework inspired by inverse problem solving.
Specifically, PITA aligns the physical dynamics discovered at different time
steps on each given PDE trajectory by integrating physics-informed constraints
into the self-supervision signal. The alignment is derived from observation
data without relying on known physics priors, indicating strong generalization
ability to the out-of-distribution data. Extensive experiments show that PITA
significantly enhances the accuracy and robustness of existing foundation
models on diverse time-dependent PDE data. The code is available at
https://github.com/SCAILab-USTC/PITA.

</details>


### [352] [Privacy-Aware Lifelong Learning](https://arxiv.org/pdf/2505.10941)
*Ozan √ñzdenizci, Elmar Rueckert, Robert Legenstein*

Main category: cs.LG

TL;DR: PALL integrates lifelong learning and privacy-aware unlearning in a single model, optimizing task-specific subnetworks and using memory rehearsal for exact unlearning.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of combining lifelong learning (retaining knowledge) with machine unlearning (forgetting sensitive data) for responsible AI.

Method: Uses task-specific sparse subnetworks with parameter sharing and an episodic memory rehearsal mechanism.

Result: Scalable across architectures in image classification, achieving exact unlearning without performance loss.

Conclusion: PALL provides a state-of-the-art solution for integrating lifelong learning and privacy-aware unlearning.

Abstract: Lifelong learning algorithms enable models to incrementally acquire new
knowledge without forgetting previously learned information. Contrarily, the
field of machine unlearning focuses on explicitly forgetting certain previous
knowledge from pretrained models when requested, in order to comply with data
privacy regulations on the right-to-be-forgotten. Enabling efficient lifelong
learning with the capability to selectively unlearn sensitive information from
models presents a critical and largely unaddressed challenge with contradicting
objectives. We address this problem from the perspective of simultaneously
preventing catastrophic forgetting and allowing forward knowledge transfer
during task-incremental learning, while ensuring exact task unlearning and
minimizing memory requirements, based on a single neural network model to be
adapted. Our proposed solution, privacy-aware lifelong learning (PALL),
involves optimization of task-specific sparse subnetworks with parameter
sharing within a single architecture. We additionally utilize an episodic
memory rehearsal mechanism to facilitate exact unlearning without performance
degradations. We empirically demonstrate the scalability of PALL across various
architectures in image classification, and provide a state-of-the-art solution
that uniquely integrates lifelong learning and privacy-aware unlearning
mechanisms for responsible AI applications.

</details>


### [353] [Certifying Stability of Reinforcement Learning Policies using Generalized Lyapunov Functions](https://arxiv.org/pdf/2505.10947)
*Kehan Long, Jorge Cort√©s, Nikolay Atanasov*

Main category: cs.LG

TL;DR: The paper proposes a method to certify stability for learned control policies by augmenting RL value functions with neural network residuals, relaxing classical Lyapunov conditions.


<details>
  <summary>Details</summary>
Motivation: Classical Lyapunov methods are hard to apply to learned policies; the study aims to bridge control theory and learning-based methods.

Method: Augment RL value functions with neural network residuals and relax Lyapunov conditions to require only average decrease over multiple steps.

Result: Successfully certifies stability for RL policies on benchmarks and improves certified regions of attraction.

Conclusion: The approach enables easier stability certification for learned policies, integrating classical and modern methods.

Abstract: We study the problem of certifying the stability of closed-loop systems under
control policies derived from optimal control or reinforcement learning (RL).
Classical Lyapunov methods require a strict step-wise decrease in the Lyapunov
function but such a certificate is difficult to construct for a learned control
policy. The value function associated with an RL policy is a natural Lyapunov
function candidate but it is not clear how it should be modified. To gain
intuition, we first study the linear quadratic regulator (LQR) problem and make
two key observations. First, a Lyapunov function can be obtained from the value
function of an LQR policy by augmenting it with a residual term related to the
system dynamics and stage cost. Second, the classical Lyapunov decrease
requirement can be relaxed to a generalized Lyapunov condition requiring only
decrease on average over multiple time steps. Using this intuition, we consider
the nonlinear setting and formulate an approach to learn generalized Lyapunov
functions by augmenting RL value functions with neural network residual terms.
Our approach successfully certifies the stability of RL policies trained on
Gymnasium and DeepMind Control benchmarks. We also extend our method to jointly
train neural controllers and stability certificates using a multi-step Lyapunov
loss, resulting in larger certified inner approximations of the region of
attraction compared to the classical Lyapunov approach. Overall, our
formulation enables stability certification for a broad class of systems with
learned policies by making certificates easier to construct, thereby bridging
classical control theory and modern learning-based methods.

</details>


### [354] [FP64 is All You Need: Rethinking Failure Modes in Physics-Informed Neural Networks](https://arxiv.org/pdf/2505.10949)
*Chenhui Xu, Dancheng Liu, Amir Nassereldine, Jinjun Xiong*

Main category: cs.LG

TL;DR: PINNs fail due to insufficient arithmetic precision (FP32), not local optima. Using FP64 resolves the issue, enabling reliable PDE solving.


<details>
  <summary>Details</summary>
Motivation: To challenge the traditional belief that PINN failures are due to local optima and identify the real cause‚Äîinsufficient arithmetic precision.

Method: Analyzed PINN training dynamics with FP32 and FP64 precision, using the LBFGS optimizer to observe convergence behavior.

Result: FP32 causes premature convergence, freezing PINNs in failure phases. FP64 eliminates failure modes, enabling successful PDE solving.

Conclusion: Arithmetic precision (FP64) is crucial for dependable PINN performance, reframing failure modes as precision-induced stalls.

Abstract: Physics Informed Neural Networks (PINNs) often exhibit failure modes in which
the PDE residual loss converges while the solution error stays large, a
phenomenon traditionally blamed on local optima separated from the true
solution by steep loss barriers. We challenge this understanding by demonstrate
that the real culprit is insufficient arithmetic precision: with standard FP32,
the LBFGS optimizer prematurely satisfies its convergence test, freezing the
network in a spurious failure phase. Simply upgrading to FP64 rescues
optimization, enabling vanilla PINNs to solve PDEs without any failure modes.
These results reframe PINN failure modes as precision induced stalls rather
than inescapable local minima and expose a three stage training dynamic
unconverged, failure, success whose boundaries shift with numerical precision.
Our findings emphasize that rigorous arithmetic precision is the key to
dependable PDE solving with neural networks.

</details>


### [355] [Shackled Dancing: A Bit-Locked Diffusion Algorithm for Lossless and Controllable Image Steganography](https://arxiv.org/pdf/2505.10950)
*Tianshuo Zhang, Gao Jia, Wenzhe Zhai, Rui Yann, Xianglei Xing*

Main category: cs.LG

TL;DR: SD¬≤ is a generative steganography method using diffusion models for secure, high-capacity, and high-fidelity information embedding.


<details>
  <summary>Details</summary>
Motivation: Existing steganography methods struggle with balancing security, capacity, and perceptual quality. Diffusion models offer potential but lack precise control for information embedding.

Method: SD¬≤ combines bit-position locking and diffusion sampling injection to embed information controllably during image synthesis.

Result: SD¬≤ achieves 100% message recovery, outperforms prior methods in security, capacity, and stability, and maintains image fidelity.

Conclusion: SD¬≤ advances controllable generation and secure visual communication, offering a robust solution for steganography.

Abstract: Data steganography aims to conceal information within visual content, yet
existing spatial- and frequency-domain approaches suffer from trade-offs
between security, capacity, and perceptual quality. Recent advances in
generative models, particularly diffusion models, offer new avenues for
adaptive image synthesis, but integrating precise information embedding into
the generative process remains challenging. We introduce Shackled Dancing
Diffusion, or SD$^2$, a plug-and-play generative steganography method that
combines bit-position locking with diffusion sampling injection to enable
controllable information embedding within the generative trajectory. SD$^2$
leverages the expressive power of diffusion models to synthesize diverse
carrier images while maintaining full message recovery with $100\%$ accuracy.
Our method achieves a favorable balance between randomness and constraint,
enhancing robustness against steganalysis without compromising image fidelity.
Extensive experiments show that SD$^2$ substantially outperforms prior methods
in security, embedding capacity, and stability. This algorithm offers new
insights into controllable generation and opens promising directions for secure
visual communication.

</details>


### [356] [SubGCache: Accelerating Graph-based RAG with Subgraph-level KV Cache](https://arxiv.org/pdf/2505.10951)
*Qiuyu Zhu, Liang Zhang, Qianxiong Xu, Cheng Long, Jie Zhang*

Main category: cs.LG

TL;DR: SubGCache reduces LLM inference latency by reusing pre-computed KV caches for similar subgraph prompts.


<details>
  <summary>Details</summary>
Motivation: To address inefficiency in graph-based RAG systems where similar subgraphs are repeatedly computed for different queries.

Method: Clusters queries by subgraph embeddings, pre-computes KV caches for representative subgraphs, and reuses them for similar queries.

Result: Achieves up to 6.68√ó reduction in TTFT with comparable or improved generation quality.

Conclusion: SubGCache effectively optimizes computation in graph-based RAG systems without sacrificing performance.

Abstract: Graph-based retrieval-augmented generation (RAG) enables large language
models (LLMs) to incorporate structured knowledge via graph retrieval as
contextual input, enhancing more accurate and context-aware reasoning. We
observe that for different queries, it could retrieve similar subgraphs as
prompts, and thus we propose SubGCache, which aims to reduce inference latency
by reusing computation across queries with similar structural prompts (i.e.,
subgraphs). Specifically, SubGCache clusters queries based on subgraph
embeddings, constructs a representative subgraph for each cluster, and
pre-computes the key-value (KV) cache of the representative subgraph. For each
query with its retrieved subgraph within a cluster, it reuses the pre-computed
KV cache of the representative subgraph of the cluster without computing the KV
tensors again for saving computation. Experiments on two new datasets across
multiple LLM backbones and graph-based RAG frameworks demonstrate that
SubGCache consistently reduces inference latency with comparable and even
improved generation quality, achieving up to 6.68$\times$ reduction in
time-to-first-token (TTFT).

</details>


### [357] [Constrained Preferential Bayesian Optimization and Its Application in Banner Ad Design](https://arxiv.org/pdf/2505.10954)
*Koki Iwai, Yusuke Kumagae, Yuki Koyama, Masahiro Hamasaki, Masataka Goto*

Main category: cs.LG

TL;DR: CPBO extends PBO to handle inequality constraints, introducing a novel acquisition function and demonstrating effectiveness in real-world applications like ad design.


<details>
  <summary>Details</summary>
Motivation: Existing PBO methods lack support for inequality constraints, which are common in real-world optimization tasks.

Method: Proposed constrained preferential Bayesian optimization (CPBO) with a new acquisition function to explore feasible regions.

Result: CPBO successfully identifies optimal solutions while respecting constraints, validated in a user study with ad designers.

Conclusion: CPBO fills a critical gap in PBO, enabling practical applications like constrained creative design.

Abstract: Preferential Bayesian optimization (PBO) is a variant of Bayesian
optimization that observes relative preferences (e.g., pairwise comparisons)
instead of direct objective values, making it especially suitable for
human-in-the-loop scenarios. However, real-world optimization tasks often
involve inequality constraints, which existing PBO methods have not yet
addressed. To fill this gap, we propose constrained preferential Bayesian
optimization (CPBO), an extension of PBO that incorporates inequality
constraints for the first time. Specifically, we present a novel acquisition
function for this purpose. Our technical evaluation shows that our CPBO method
successfully identifies optimal solutions by focusing on exploring feasible
regions. As a practical application, we also present a designer-in-the-loop
system for banner ad design using CPBO, where the objective is the designer's
subjective preference, and the constraint ensures a target predicted
click-through rate. We conducted a user study with professional ad designers,
demonstrating the potential benefits of our approach in guiding creative design
under real-world constraints.

</details>


### [358] [Relational Graph Transformer](https://arxiv.org/pdf/2505.10960)
*Vijay Prakash Dwivedi, Sri Jaladi, Yangyi Shen, Federico L√≥pez, Charilaos I. Kanatsoulis, Rishi Puri, Matthias Fey, Jure Leskovec*

Main category: cs.LG

TL;DR: RelGT, a Graph Transformer for relational data, outperforms GNNs by addressing limitations in capturing structural patterns and long-range dependencies.


<details>
  <summary>Details</summary>
Motivation: Existing GNNs and Graph Transformers struggle with relational data due to issues like poor positional encodings, inability to model temporal dynamics, and loss of structural information.

Method: RelGT uses multi-element tokenization (features, type, hop distance, time, local structure) and combines local and global attention mechanisms.

Result: RelGT outperforms GNN baselines by up to 18% on 21 RelBench tasks.

Conclusion: RelGT establishes Graph Transformers as effective for Relational Deep Learning.

Abstract: Relational Deep Learning (RDL) is a promising approach for building
state-of-the-art predictive models on multi-table relational data by
representing it as a heterogeneous temporal graph. However, commonly used Graph
Neural Network models suffer from fundamental limitations in capturing complex
structural patterns and long-range dependencies that are inherent in relational
data. While Graph Transformers have emerged as powerful alternatives to GNNs on
general graphs, applying them to relational entity graphs presents unique
challenges: (i) Traditional positional encodings fail to generalize to massive,
heterogeneous graphs; (ii) existing architectures cannot model the temporal
dynamics and schema constraints of relational data; (iii) existing tokenization
schemes lose critical structural information. Here we introduce the Relational
Graph Transformer (RelGT), the first graph transformer architecture designed
specifically for relational tables. RelGT employs a novel multi-element
tokenization strategy that decomposes each node into five components (features,
type, hop distance, time, and local structure), enabling efficient encoding of
heterogeneity, temporality, and topology without expensive precomputation. Our
architecture combines local attention over sampled subgraphs with global
attention to learnable centroids, incorporating both local and database-wide
representations. Across 21 tasks from the RelBench benchmark, RelGT
consistently matches or outperforms GNN baselines by up to 18%, establishing
Graph Transformers as a powerful architecture for Relational Deep Learning.

</details>


### [359] [Group-in-Group Policy Optimization for LLM Agent Training](https://arxiv.org/pdf/2505.10978)
*Lang Feng, Zhenghai Xue, Tingcong Liu, Bo An*

Main category: cs.LG

TL;DR: GiGPO is a novel RL algorithm for LLM agents, enabling fine-grained credit assignment in long-horizon tasks while maintaining efficiency. It outperforms baselines by >12% on ALFWorld and >9% on WebShop.


<details>
  <summary>Details</summary>
Motivation: Scalability of group-based RL in long-horizon LLM agent training is limited due to sparse/delayed rewards and credit assignment challenges.

Method: GiGPO uses a two-level structure: episode-level macro advantages and step-level micro advantages via anchor state grouping.

Result: GiGPO achieves >12% and >9% performance gains on ALFWorld and WebShop, respectively, with minimal overhead.

Conclusion: GiGPO effectively addresses credit assignment in long-horizon RL for LLMs, outperforming baselines without added computational cost.

Abstract: Recent advances in group-based reinforcement learning (RL) have driven
frontier large language models (LLMs) in single-turn tasks like mathematical
reasoning. However, their scalability to long-horizon LLM agent training
remains limited. Unlike static tasks, agent-environment interactions unfold
over many steps and often yield sparse or delayed rewards, making credit
assignment across individual steps significantly more challenging. In this
work, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL
algorithm that achieves fine-grained credit assignment for LLM agents while
preserving the appealing properties of group-based RL: critic-free, low memory,
and stable convergence. GiGPO introduces a two-level structure for estimating
relative advantage: (i) At the episode-level, GiGPO computes macro relative
advantages based on groups of complete trajectories; (ii) At the step-level,
GiGPO introduces an anchor state grouping mechanism that retroactively
constructs step-level groups by identifying repeated environment states across
trajectories. Actions stemming from the same state are grouped together,
enabling micro relative advantage estimation. This hierarchical structure
effectively captures both global trajectory quality and local step
effectiveness without relying on auxiliary models or additional rollouts. We
evaluate GiGPO on two challenging agent benchmarks, ALFWorld and WebShop, using
Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct. Crucially, GiGPO delivers
fine-grained per-step credit signals and achieves performance gains of > 12\%
on ALFWorld and > 9\% on WebShop over the GRPO baseline: all while maintaining
the same GPU memory overhead, identical LLM rollout, and incurring little to no
additional time cost.

</details>


### [360] [GenoArmory: A Unified Evaluation Framework for Adversarial Attacks on Genomic Foundation Models](https://arxiv.org/pdf/2505.10983)
*Haozheng Luo, Chenghao Qiu, Yimin Wang, Shang Wu, Jiahao Yu, Han Liu, Binghui Wang, Yan Chen*

Main category: cs.LG

TL;DR: GenoArmory is the first unified benchmark for evaluating adversarial attacks on Genomic Foundation Models (GFMs), offering a comprehensive framework to assess vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Existing GFM benchmarks lack systematic evaluation of adversarial vulnerabilities, prompting the need for GenoArmory.

Method: Evaluated five GFMs using four attack algorithms and three defense strategies, analyzing vulnerabilities related to architecture, quantization, and datasets. Introduced GenoAdv, a dataset for GFM safety.

Result: Classification models are more robust than generative models. Adversarial attacks often target biologically significant regions, indicating meaningful feature capture.

Conclusion: GenoArmory provides a valuable tool for assessing GFM vulnerabilities, revealing task-specific robustness and biological relevance in adversarial targeting.

Abstract: We propose the first unified adversarial attack benchmark for Genomic
Foundation Models (GFMs), named GenoArmory. Unlike existing GFM benchmarks,
GenoArmory offers the first comprehensive evaluation framework to
systematically assess the vulnerability of GFMs to adversarial attacks.
Methodologically, we evaluate the adversarial robustness of five
state-of-the-art GFMs using four widely adopted attack algorithms and three
defense strategies. Importantly, our benchmark provides an accessible and
comprehensive framework to analyze GFM vulnerabilities with respect to model
architecture, quantization schemes, and training datasets. Additionally, we
introduce GenoAdv, a new adversarial sample dataset designed to improve GFM
safety. Empirically, classification models exhibit greater robustness to
adversarial perturbations compared to generative models, highlighting the
impact of task type on model vulnerability. Moreover, adversarial attacks
frequently target biologically significant genomic regions, suggesting that
these models effectively capture meaningful sequence features.

</details>


### [361] [ReaCritic: Large Reasoning Transformer-based DRL Critic-model Scaling For Heterogeneous Networks](https://arxiv.org/pdf/2505.10992)
*Feiran You, Hongyang Du*

Main category: cs.LG

TL;DR: ReaCritic, a transformer-based critic model, enhances DRL by incorporating reasoning abilities, improving performance in HetNets and control tasks.


<details>
  <summary>Details</summary>
Motivation: Existing DRL methods struggle with HetNets' complexity due to shallow critic architectures, while LLMs show reasoning improves decisions.

Method: ReaCritic uses horizontal and vertical reasoning via transformer stacks, compatible with value-based and actor-critic DRL algorithms.

Result: Improves convergence speed and performance in HetNets and OpenAI Gym tasks.

Conclusion: ReaCritic effectively addresses DRL limitations in dynamic environments by integrating reasoning.

Abstract: Heterogeneous Networks (HetNets) pose critical challenges for intelligent
management due to the diverse user requirements and time-varying wireless
conditions. These factors introduce significant decision complexity, which
limits the adaptability of existing Deep Reinforcement Learning (DRL) methods.
In many DRL algorithms, especially those involving value-based or actor-critic
structures, the critic component plays a key role in guiding policy learning by
estimating value functions. However, conventional critic models often use
shallow architectures that map observations directly to scalar estimates,
limiting their ability to handle multi-task complexity. In contrast, recent
progress in inference-time scaling of Large Language Models (LLMs) has shown
that generating intermediate reasoning steps can significantly improve decision
quality. Motivated by this, we propose ReaCritic, a large reasoning
transformer-based criticmodel scaling scheme that brings reasoning ability into
DRL. ReaCritic performs horizontal reasoning over parallel state-action inputs
and vertical reasoning through deep transformer stacks. It is compatible with a
broad range of value-based and actor-critic DRL algorithms and enhances
generalization in dynamic wireless environments. Extensive experiments
demonstrate that ReaCritic improves convergence speed and final performance
across various HetNet settings and standard OpenAI Gym control tasks.

</details>


### [362] [Logo-LLM: Local and Global Modeling with Large Language Models for Time Series Forecasting](https://arxiv.org/pdf/2505.11017)
*Wenjie Ou, Zhishuo Zhao, Dongyue Guo, Yi Lin*

Main category: cs.LG

TL;DR: Logo-LLM is an LLM-based framework for time series forecasting that extracts multi-scale temporal features from different layers of a pre-trained LLM, addressing limitations of existing methods by modeling both local and global dependencies.


<details>
  <summary>Details</summary>
Motivation: Existing Transformer-based and LLM-adapted methods for time series forecasting often overlook short-term local variations and underutilize hierarchical representations in LLMs.

Method: Logo-LLM extracts multi-scale features from different LLM layers, uses Local-Mixer and Global-Mixer modules to align and integrate these features, and leverages shallow layers for local dynamics and deeper layers for global trends.

Result: Logo-LLM outperforms benchmarks, showing strong generalization in few-shot and zero-shot settings with low computational overhead.

Conclusion: Logo-LLM effectively combines local and global temporal features from LLMs, improving time series forecasting performance and efficiency.

Abstract: Time series forecasting is critical across multiple domains, where time
series data exhibits both local patterns and global dependencies. While
Transformer-based methods effectively capture global dependencies, they often
overlook short-term local variations in time series. Recent methods that adapt
large language models (LLMs) into time series forecasting inherit this
limitation by treating LLMs as black-box encoders, relying solely on the
final-layer output and underutilizing hierarchical representations. To address
this limitation, we propose Logo-LLM, a novel LLM-based framework that
explicitly extracts and models multi-scale temporal features from different
layers of a pre-trained LLM. Through empirical analysis, we show that shallow
layers of LLMs capture local dynamics in time series, while deeper layers
encode global trends. Moreover, Logo-LLM introduces lightweight Local-Mixer and
Global-Mixer modules to align and integrate features with the temporal input
across layers. Extensive experiments demonstrate that Logo-LLM achieves
superior performance across diverse benchmarks, with strong generalization in
few-shot and zero-shot settings while maintaining low computational overhead.

</details>


### [363] [Informed, but Not Always Improved: Challenging the Benefit of Background Knowledge in GNNs](https://arxiv.org/pdf/2505.11023)
*Kutalmƒ±≈ü Co≈ükun, Ivo Kavisanczki, Amin Mirzaei, Tom Siegl, Bjarne C. Hiller, Stefan L√ºdtke, Martin Becker*

Main category: cs.LG

TL;DR: The paper explores the impact of background knowledge (BK) graphs on graph-based machine learning in biomedical tasks, finding BK often doesn't improve performance as expected. It introduces a framework to test BK's robustness and suggests aligning GNN architectures with BK for better results.


<details>
  <summary>Details</summary>
Motivation: To understand the actual contribution of BK graphs in improving model performance, especially in low-data domains like biomedical research, and to investigate the impact of imperfect BK.

Method: The study uses cancer subtype classification as a real-world task, comparing state-of-the-art GNNs with uninformed models. It introduces a synthetic setting and perturbations to evaluate BK's robustness.

Result: BK-aware models perform no better than uninformed models, and their performance remains stable even with heavily perturbed BK. The framework reveals the need for careful alignment of GNN architectures with BK characteristics.

Conclusion: Alignment between GNN architectures and BK characteristics is crucial for leveraging BK effectively, offering potential for significant performance improvements.

Abstract: In complex and low-data domains such as biomedical research, incorporating
background knowledge (BK) graphs, such as protein-protein interaction (PPI)
networks, into graph-based machine learning pipelines is a promising research
direction. However, while BK is often assumed to improve model performance, its
actual contribution and the impact of imperfect knowledge remain poorly
understood. In this work, we investigate the role of BK in an important
real-world task: cancer subtype classification. Surprisingly, we find that (i)
state-of-the-art GNNs using BK perform no better than uninformed models like
linear regression, and (ii) their performance remains largely unchanged even
when the BK graph is heavily perturbed. To understand these unexpected results,
we introduce an evaluation framework, which employs (i) a synthetic setting
where the BK is clearly informative and (ii) a set of perturbations that
simulate various imperfections in BK graphs. With this, we test the robustness
of BK-aware models in both synthetic and real-world biomedical settings. Our
findings reveal that careful alignment of GNN architectures and BK
characteristics is necessary but holds the potential for significant
performance improvements.

</details>


### [364] [Leveraging Real-Time Data Analysis and Multiple Kernel Learning for Manufacturing of Innovative Steels](https://arxiv.org/pdf/2505.11024)
*Wolfgang Rannetbauer, Simon Hubmer, Carina Hambrock, Ronny Ramlau*

Main category: cs.LG

TL;DR: The paper proposes updating the thermal spray coating process for steel manufacturing by integrating real-time data analytics and predictive quality management to address challenges in production and maintenance.


<details>
  <summary>Details</summary>
Motivation: Challenges in production and plant maintenance due to standardization in refurbishment processes for thermally sprayed components in steel manufacturing.

Method: Integration of real-time data analytics (data aggregator with sensors and flow meters) and predictive quality management (quality predictor using multiple kernel learning).

Result: Small-scale tests confirmed accurate prediction of coating quality and proactive operator notifications for deviations.

Conclusion: The updated process with real-time analytics and predictive quality management effectively addresses dynamic demands in steel manufacturing.

Abstract: The implementation of thermally sprayed components in steel manufacturing
presents challenges for production and plant maintenance. While enhancing
performance through specialized surface properties, these components may
encounter difficulties in meeting modified requirements due to standardization
in the refurbishment process. This article proposes updating the established
coating process for thermally spray coated components for steel manufacturing
(TCCSM) by integrating real-time data analytics and predictive quality
management. Two essential components--the data aggregator and the quality
predictor--are designed through continuous process monitoring and the
application of data-driven methodologies to meet the dynamic demands of the
evolving steel landscape. The quality predictor is powered by the simple and
effective multiple kernel learning strategy with the goal of realizing
predictive quality. The data aggregator, designed with sensors, flow meters,
and intelligent data processing for the thermal spray coating process, is
proposed to facilitate real-time analytics. The performance of this combination
was verified using small-scale tests that enabled not only the accurate
prediction of coating quality based on the collected data but also proactive
notification to the operator as soon as significant deviations are identified.

</details>


### [365] [Exploiting the Asymmetric Uncertainty Structure of Pre-trained VLMs on the Unit Hypersphere](https://arxiv.org/pdf/2505.11029)
*Li Ju, Max Andersson, Stina Fredriksson, Edward Gl√∂ckner, Andreas Hellander, Ekta Vats, Prashant Singh*

Main category: cs.LG

TL;DR: AsymVLM addresses asymmetric uncertainty in vision-language models by building probabilistic embeddings on a unit hypersphere, improving uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Deterministic VLMs fail to capture ambiguity and uncertainty in natural language and visual data, and existing probabilistic methods ignore asymmetric uncertainty structures.

Method: Proposes AsymVLM to create probabilistic embeddings from pre-trained VLMs on a unit hypersphere, accounting for asymmetric uncertainty.

Result: Validated on benchmarks, showing improved uncertainty quantification and demonstrating inherent asymmetry in uncertainty structures.

Conclusion: AsymVLM effectively addresses asymmetric uncertainty in VLMs, enhancing performance and understanding of modality-specific uncertainties.

Abstract: Vision-language models (VLMs) as foundation models have significantly
enhanced performance across a wide range of visual and textual tasks, without
requiring large-scale training from scratch for downstream tasks. However,
these deterministic VLMs fail to capture the inherent ambiguity and uncertainty
in natural language and visual data. Recent probabilistic post-hoc adaptation
methods address this by mapping deterministic embeddings onto probability
distributions; however, existing approaches do not account for the asymmetric
uncertainty structure of the modalities, and the constraint that meaningful
deterministic embeddings reside on a unit hypersphere, potentially leading to
suboptimal performance. In this paper, we address the asymmetric uncertainty
structure inherent in textual and visual data, and propose AsymVLM to build
probabilistic embeddings from pre-trained VLMs on the unit hypersphere,
enabling uncertainty quantification. We validate the effectiveness of the
probabilistic embeddings on established benchmarks, and present comprehensive
ablation studies demonstrating the inherent nature of asymmetry in the
uncertainty structure of textual and visual data.

</details>


### [366] [Deep Latent Variable Model based Vertical Federated Learning with Flexible Alignment and Labeling Scenarios](https://arxiv.org/pdf/2505.11035)
*Kihun Hong, Sejun Park, Ganguk Hwang*

Main category: cs.LG

TL;DR: A unified VFL framework addresses alignment gaps as missing data, outperforming baselines in 160/168 cases with a 9.6% average improvement.


<details>
  <summary>Details</summary>
Motivation: Existing VFL methods have restrictive assumptions (e.g., small parties, aligned data, labeled data). This work aims to overcome these limitations.

Method: Reinterprets alignment gaps as missing data problems, proposing a framework for arbitrary alignment and labeling scenarios.

Result: Outperforms baselines in 160/168 configurations, with a 9.6% average improvement.

Conclusion: First VFL framework to handle arbitrary alignment, unlabeled data, and multi-party collaboration simultaneously.

Abstract: Federated learning (FL) has attracted significant attention for enabling
collaborative learning without exposing private data. Among the primary
variants of FL, vertical federated learning (VFL) addresses feature-partitioned
data held by multiple institutions, each holding complementary information for
the same set of users. However, existing VFL methods often impose restrictive
assumptions such as a small number of participating parties, fully aligned
data, or only using labeled data. In this work, we reinterpret alignment gaps
in VFL as missing data problems and propose a unified framework that
accommodates both training and inference under arbitrary alignment and labeling
scenarios, while supporting diverse missingness mechanisms. In the experiments
on 168 configurations spanning four benchmark datasets, six training-time
missingness patterns, and seven testing-time missingness patterns, our method
outperforms all baselines in 160 cases with an average gap of 9.6 percentage
points over the next-best competitors. To the best of our knowledge, this is
the first VFL framework to jointly handle arbitrary data alignment, unlabeled
data, and multi-party collaboration all at once.

</details>


### [367] [Maximizing Asynchronicity in Event-based Neural Networks](https://arxiv.org/pdf/2505.11165)
*Haiqing Hao, Nikola Zubiƒá, Weihua He, Zhipeng Sui, Davide Scaramuzza, Wenhui Wang*

Main category: cs.LG

TL;DR: EVA is a novel A2S framework for event cameras, leveraging language modeling techniques to create expressive, generalizable event representations, outperforming prior methods in recognition and detection tasks.


<details>
  <summary>Details</summary>
Motivation: Event cameras' asynchronous, sparse data challenges standard ML. Existing A2S methods lack expressivity and generalizability compared to dense methods.

Method: EVA adapts language modeling techniques (linear attention, self-supervised learning) to asynchronously encode events into ML-suitable representations.

Result: EVA excels in recognition (DVS128-Gesture, N-Cars) and detection (47.7 mAP on Gen1), surpassing prior A2S methods.

Conclusion: EVA advances real-time event-based vision by offering highly expressive and generalizable representations.

Abstract: Event cameras deliver visual data with high temporal resolution, low latency,
and minimal redundancy, yet their asynchronous, sparse sequential nature
challenges standard tensor-based machine learning (ML). While the recent
asynchronous-to-synchronous (A2S) paradigm aims to bridge this gap by
asynchronously encoding events into learned representations for ML pipelines,
existing A2S approaches often sacrifice representation expressivity and
generalizability compared to dense, synchronous methods. This paper introduces
EVA (EVent Asynchronous representation learning), a novel A2S framework to
generate highly expressive and generalizable event-by-event representations.
Inspired by the analogy between events and language, EVA uniquely adapts
advances from language modeling in linear attention and self-supervised
learning for its construction. In demonstration, EVA outperforms prior A2S
methods on recognition tasks (DVS128-Gesture and N-Cars), and represents the
first A2S framework to successfully master demanding detection tasks, achieving
a remarkable 47.7 mAP on the Gen1 dataset. These results underscore EVA's
transformative potential for advancing real-time event-based vision
applications.

</details>


### [368] [Efficient Attention via Pre-Scoring: Prioritizing Informative Keys in Transformers](https://arxiv.org/pdf/2505.11040)
*Zhexiang Li, Haoyu Wang, Yutong Bao, David Woodruff*

Main category: cs.LG

TL;DR: The paper proposes a pre-scoring mechanism to enhance HyperAttention by prioritizing significant keys, improving perplexity and efficiency.


<details>
  <summary>Details</summary>
Motivation: HyperAttention's uniform residual sampling limits crucial keys' capturing, raising perplexity.

Method: Introduces three pre-scoring methods (K-means, K-median, leverage score-based ranking) to replace uniform sampling in HyperAttention.

Result: Reduces perplexity from 12 to 8.3 on ChatGLM2 and maintains accuracy on ViT, while being 20x faster than FlashAttention.

Conclusion: Integrating pre-scoring into hierarchical attention improves Transformer efficiency and accuracy.

Abstract: Recent advances in transformer architectures deeply enhance long-context
language modeling. Among them, HyperAttention achieves competitive efficiency
by combining a single-level LSH-based clustering with uniform residual
sampling. However,such a sampling limits crucial keys' capturing, which in turn
raises the overall perplexity. In this paper, we propose a pre-scoring
mechanism to assist HyperAttention to prioritize significant keys.
Specifically, we introduce three scoring methods: K-means clustering, K-median
clustering, and leverage score-based ranking (inspired by LevAttention) to
filter keys effectively. We further replace HyperAttention's original uniform
residual sampling entirely, relying exclusively on our pre-scoring mechanism.
Experiments on ChatGLM2 (131k token context) reduce perplexity from 12 to 8.3,
which outperforms standard HyperAttention. Moreover, when running on the
Vision-Transformer (ViT), our method shows that it can guarantee similar
accuracy compared with LevAttention, and will surpass LevAttention given
specific parameters. Although this method introduces computational overhead,
its combination with HyperAttention remains 20 times faster than
FlashAttention, providing a balanced trade-off between speed and modeling
accuracy. Our results highlight the effectiveness of integrating pre-scoring
into hierarchical attention mechanisms, significantly improving Transformer's
efficiency.

</details>


### [369] [Exploration by Random Distribution Distillation](https://arxiv.org/pdf/2505.11044)
*Zhirui Fang, Kai Yang, Jian Tao, Jiafei Lyu, Lusong Li, Li Shen, Xiu Li*

Main category: cs.LG

TL;DR: The paper introduces Random Distribution Distillation (RDD), a novel method for online reinforcement learning exploration, combining count-based and prediction-error approaches to enhance exploration efficiency.


<details>
  <summary>Details</summary>
Motivation: Exploration is a key challenge in online reinforcement learning; current methods like count-based and curiosity-based approaches have limitations. RDD aims to unify these methods for better performance.

Method: RDD samples the output of a target network from a normal distribution, using the prediction-target network difference as an intrinsic reward. It bounds rewards with a pseudo-count term and a discrepancy term.

Result: RDD effectively unifies count-based and prediction-error methods, performing well in high-dimensional spaces and demonstrating improved exploration in experiments.

Conclusion: RDD is a promising approach for enhancing online reinforcement learning exploration, validated by theoretical and experimental results.

Abstract: Exploration remains a critical challenge in online reinforcement learning, as
an agent must effectively explore unknown environments to achieve high returns.
Currently, the main exploration algorithms are primarily count-based methods
and curiosity-based methods, with prediction-error methods being a prominent
example. In this paper, we propose a novel method called \textbf{R}andom
\textbf{D}istribution \textbf{D}istillation (RDD), which samples the output of
a target network from a normal distribution. RDD facilitates a more extensive
exploration by explicitly treating the difference between the prediction
network and the target network as an intrinsic reward. Furthermore, by
introducing randomness into the output of the target network for a given state
and modeling it as a sample from a normal distribution, intrinsic rewards are
bounded by two key components: a pseudo-count term ensuring proper exploration
decay and a discrepancy term accounting for predictor convergence. We
demonstrate that RDD effectively unifies both count-based and prediction-error
approaches. It retains the advantages of prediction-error methods in
high-dimensional spaces, while also implementing an intrinsic reward decay mode
akin to the pseudo-count method. In the experimental section, RDD is compared
with more advanced methods in a series of environments. Both theoretical
analysis and experimental results confirm the effectiveness of our approach in
improving online exploration for reinforcement learning tasks.

</details>


### [370] [Halting Recurrent GNNs and the Graded $Œº$-Calculus](https://arxiv.org/pdf/2505.11050)
*Jeroen Bollen, Jan Van den Bussche, Stijn Vansummeren, Jonni Virtema*

Main category: cs.LG

TL;DR: Proposes a halting mechanism for recurrent GNNs, proving it can express node classifiers in graded modal mu-calculus, independent of graph size.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations in current recurrent GNNs, which lack termination guarantees or assume known graph size.

Method: Introduces a new halting model and approximate semantics for graded mu-calculus, leading to a size-oblivious counting algorithm.

Result: Shows the halting recurrent GNN can express all node classifiers in graded modal mu-calculus.

Conclusion: The halting mechanism and counting algorithm advance GNN expressivity and practical applicability.

Abstract: Graph Neural Networks (GNNs) are a class of machine-learning models that
operate on graph-structured data. Their expressive power is intimately related
to logics that are invariant under graded bisimilarity. Current proposals for
recurrent GNNs either assume that the graph size is given to the model, or
suffer from a lack of termination guarantees. In this paper, we propose a
halting mechanism for recurrent GNNs. We prove that our halting model can
express all node classifiers definable in graded modal mu-calculus, even for
the standard GNN variant that is oblivious to the graph size. A recent
breakthrough in the study of the expressivity of graded modal mu-calculus in
the finite suggests that conversely, restricted to node classifiers definable
in monadic second-order logic, recurrent GNNs can express only node classifiers
definable in graded modal mu-calculus. To prove our main result, we develop a
new approximate semantics for graded mu-calculus, which we believe to be of
independent interest. We leverage this new semantics into a new model-checking
algorithm, called the counting algorithm, which is oblivious to the graph size.
In a final step we show that the counting algorithm can be implemented on a
halting recurrent GNN.

</details>


### [371] [NeuralSurv: Deep Survival Analysis with Bayesian Uncertainty Quantification](https://arxiv.org/pdf/2505.11054)
*M√©lodie Monod, Alessandro Micheli, Samir Bhatt*

Main category: cs.LG

TL;DR: NeuralSurv is a Bayesian deep survival model with uncertainty quantification, offering superior calibration and performance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of Bayesian uncertainty quantification in deep survival models and improve calibration in data-scarce scenarios.

Method: Uses a two-stage data-augmentation scheme for flexible time-varying relationships and a mean-field variational algorithm for efficient posterior inference.

Result: Outperforms state-of-the-art models in calibration and matches/exceeds discriminative performance on synthetic and real-world datasets.

Conclusion: Bayesian principles enhance model calibration and provide robust uncertainty estimates, especially in data-scarce settings.

Abstract: We introduce NeuralSurv, the first deep survival model to incorporate
Bayesian uncertainty quantification. Our non-parametric, architecture-agnostic
framework flexibly captures time-varying covariate-risk relationships in
continuous time via a novel two-stage data-augmentation scheme, for which we
establish theoretical guarantees. For efficient posterior inference, we
introduce a mean-field variational algorithm with coordinate-ascent updates
that scale linearly in model size. By locally linearizing the Bayesian neural
network, we obtain full conjugacy and derive all coordinate updates in closed
form. In experiments, NeuralSurv delivers superior calibration compared to
state-of-the-art deep survival models, while matching or exceeding their
discriminative performance across both synthetic benchmarks and real-world
datasets. Our results demonstrate the value of Bayesian principles in
data-scarce regimes by enhancing model calibration and providing robust,
well-calibrated uncertainty estimates for the survival function.

</details>


### [372] [Assessing the Performance of Analog Training for Transfer Learning](https://arxiv.org/pdf/2505.11067)
*Omobayode Fagbohungbe, Corey Lammie, Malte J. Rasch, Takashi Ando, Tayfun Gokmen, Vijay Narayanan*

Main category: cs.LG

TL;DR: The paper introduces a new algorithm, c-TTv2, for analog in-memory computing to address challenges like asymmetric device behavior and variability, and evaluates its performance for transfer learning.


<details>
  <summary>Details</summary>
Motivation: Current training algorithms fail to achieve good outcomes in analog in-memory computing due to device non-linearities and variations. Existing solutions require unrealistic device symmetry and precision.

Method: The c-TTv2 algorithm uses a chopped technique to mitigate challenges. Performance is assessed using a Swin-ViT model on CIFAR100, testing robustness to device specifications like noise and variability.

Result: The c-TTv2 algorithm shows promise in handling analog device challenges, though specific performance metrics are not detailed in the abstract.

Conclusion: c-TTv2 is a viable solution for analog transfer learning, addressing key limitations of current methods, but further evaluation is needed.

Abstract: Analog in-memory computing is a next-generation computing paradigm that
promises fast, parallel, and energy-efficient deep learning training and
transfer learning (TL). However, achieving this promise has remained elusive
due to a lack of suitable training algorithms. Analog memory devices exhibit
asymmetric and non-linear switching behavior in addition to device-to-device
variation, meaning that most, if not all, of the current off-the-shelf training
algorithms cannot achieve good training outcomes. Also, recently introduced
algorithms have enjoyed limited attention, as they require bi-directionally
switching devices of unrealistically high symmetry and precision and are highly
sensitive. A new algorithm chopped TTv2 (c-TTv2), has been introduced, which
leverages the chopped technique to address many of the challenges mentioned
above. In this paper, we assess the performance of the c-TTv2 algorithm for
analog TL using a Swin-ViT model on a subset of the CIFAR100 dataset. We also
investigate the robustness of our algorithm to changes in some device
specifications, including weight transfer noise, symmetry point skew, and
symmetry point variability

</details>


### [373] [Addition is almost all you need: Compressing neural networks with double binary factorization](https://arxiv.org/pdf/2505.11076)
*Vladim√≠r Bo≈æa, Vladim√≠r Macko*

Main category: cs.LG

TL;DR: DBF factorizes weight matrices into two binary matrices with scaling vectors, preserving efficiency while improving accuracy and compression rates.


<details>
  <summary>Details</summary>
Motivation: Address computational and storage demands of LLMs while minimizing accuracy loss from binary quantization.

Method: Double Binary Factorization (DBF) decomposes dense weights into two binary matrices with scaling vectors, allowing adjustable compression ratios.

Result: DBF outperforms 1-bit binarization methods and matches 2-bit quantization techniques like QuIP# and QTIP, with flexible compression control.

Conclusion: DBF offers efficient, accurate, and adaptable compression for LLMs, with fine-grained control over compression ratios.

Abstract: Binary quantization approaches, which replace weight matrices with binary
matrices and substitute costly multiplications with cheaper additions, offer a
computationally efficient approach to address the increasing computational and
storage requirements of Large Language Models (LLMs). However, the severe
quantization constraint ($\pm1$) can lead to significant accuracy degradation.
In this paper, we propose Double Binary Factorization (DBF), a novel method
that factorizes dense weight matrices into products of two binary (sign)
matrices, each accompanied by scaling vectors. DBF preserves the efficiency
advantages of binary representations while achieving compression rates that are
competitive with or superior to state-of-the-art methods. Specifically, in a
1-bit per weight range, DBF is better than existing binarization approaches. In
a 2-bit per weight range, DBF is competitive with the best quantization methods
like QuIP\# and QTIP. Unlike most existing compression techniques, which offer
limited compression level choices, DBF allows fine-grained control over
compression ratios by adjusting the factorization's intermediate dimension.
Based on this advantage, we further introduce an algorithm for estimating
non-uniform layer-wise compression ratios for DBF, based on previously
developed channel pruning criteria.
  Code available at: https://github.com/usamec/double_binary

</details>


### [374] [Visual Planning: Let's Think Only with Images](https://arxiv.org/pdf/2505.11409)
*Yi Xu, Chengzu Li, Han Zhou, Xingchen Wan, Caiqi Zhang, Anna Korhonen, Ivan Vuliƒá*

Main category: cs.LG

TL;DR: The paper introduces Visual Planning, a paradigm for reasoning via visual representations instead of text, and proposes VPRL, a reinforcement learning framework, showing superior performance in visual navigation tasks.


<details>
  <summary>Details</summary>
Motivation: Language may not be the most effective modality for reasoning, especially in tasks involving spatial or geometrical information, prompting the need for visual-based reasoning.

Method: Proposes Visual Planning, a paradigm using sequences of images for step-by-step inference, and introduces VPRL, a reinforcement learning framework with GRPO for post-training vision models.

Result: VPRL outperforms text-only reasoning methods in visual navigation tasks (FrozenLake, Maze, MiniBehavior), demonstrating the effectiveness of visual planning.

Conclusion: Visual Planning is a viable alternative to language-based reasoning, offering new possibilities for tasks requiring intuitive, image-based inference.

Abstract: Recent advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have substantially enhanced machine reasoning across diverse
tasks. However, these models predominantly rely on pure text as the medium for
both expressing and structuring reasoning, even when visual information is
present. In this work, we argue that language may not always be the most
natural or effective modality for reasoning, particularly in tasks involving
spatial and geometrical information. Motivated by this, we propose a new
paradigm, Visual Planning, which enables planning through purely visual
representations, independent of text. In this paradigm, planning is executed
via sequences of images that encode step-by-step inference in the visual
domain, akin to how humans sketch or visualize future actions. We introduce a
novel reinforcement learning framework, Visual Planning via Reinforcement
Learning (VPRL), empowered by GRPO for post-training large vision models,
leading to substantial improvements in planning in a selection of
representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our
visual planning paradigm outperforms all other planning variants that conduct
reasoning in the text-only space. Our results establish Visual Planning as a
viable and promising alternative to language-based reasoning, opening new
avenues for tasks that benefit from intuitive, image-based inference.

</details>


### [375] [ShiQ: Bringing back Bellman to LLMs](https://arxiv.org/pdf/2505.11081)
*Pierre Clavier, Nathan Grinsztajn, Raphael Avalos, Yannis Flet-Berliac, Irem Ergun, Omar D. Domingues, Eugene Tarassov, Olivier Pietquin, Pierre H. Richemond, Florian Strub, Matthieu Geist*

Main category: cs.LG

TL;DR: The paper introduces ShiQ, a Q-learning adaptation for fine-tuning LLMs, addressing inefficiencies in direct policy optimization by leveraging Bellman equations for reliable Q-value estimates.


<details>
  <summary>Details</summary>
Motivation: Direct policy optimization for LLM fine-tuning is computationally expensive, while Q-learning's sample efficiency and offline learning potential are underutilized.

Method: Derives theoretically grounded loss functions from Bellman equations, adapting Q-learning for LLMs, and implements the ShiQ algorithm for off-policy, token-wise learning.

Result: ShiQ demonstrates effectiveness on synthetic data and benchmarks like UltraFeedback and BFCL-V3 in single- and multi-turn LLM settings.

Conclusion: ShiQ successfully bridges Q-learning and LLM fine-tuning, offering a practical and efficient alternative to direct policy optimization.

Abstract: The fine-tuning of pre-trained large language models (LLMs) using
reinforcement learning (RL) is generally formulated as direct policy
optimization. This approach was naturally favored as it efficiently improves a
pretrained LLM, seen as an initial policy. Another RL paradigm, Q-learning
methods, has received far less attention in the LLM community while
demonstrating major success in various non-LLM RL tasks. In particular,
Q-learning effectiveness comes from its sample efficiency and ability to learn
offline, which is particularly valuable given the high computational cost of
sampling with LLMs. However, naively applying a Q-learning-style update to the
model's logits is ineffective due to the specificity of LLMs. Our core
contribution is to derive theoretically grounded loss functions from Bellman
equations to adapt Q-learning methods to LLMs. To do so, we carefully adapt
insights from the RL literature to account for LLM-specific characteristics,
ensuring that the logits become reliable Q-value estimates. We then use this
loss to build a practical algorithm, ShiQ for Shifted-Q, that supports
off-policy, token-wise learning while remaining simple to implement. Finally,
we evaluate ShiQ on both synthetic data and real-world benchmarks, e.g.,
UltraFeedback and BFCL-V3, demonstrating its effectiveness in both single-turn
and multi-turn LLM settings

</details>


### [376] [Fault Diagnosis across Heterogeneous Domains via Self-Adaptive Temporal-Spatial Attention and Sample Generation](https://arxiv.org/pdf/2505.11083)
*Guangqiang Li, M. Amine Atoui, Xiangshun Li*

Main category: cs.LG

TL;DR: A novel fault diagnosis model (TSA-SAN) addresses partial overlap in health state categories across operating modes by generating diverse data and using attention mechanisms, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing fault diagnosis methods assume identical health state categories across modes, but real industrial scenarios often have partial overlap, posing challenges.

Method: TSA-SAN constructs inter-mode mappings, enriches fault data via interpolation, uses self-adaptive instance normalization, and employs temporal-spatial attention to focus on key features.

Result: The model significantly outperforms state-of-the-art methods in experiments.

Conclusion: TSA-SAN effectively handles partial category overlap and distribution differences, improving fault diagnosis performance.

Abstract: Deep learning methods have shown promising performance in fault diagnosis for
multimode process. Most existing studies assume that the collected health state
categories from different operating modes are identical. However, in real
industrial scenarios, these categories typically exhibit only partial overlap.
The incompleteness of the available data and the large distributional
differences between the operating modes pose a significant challenge to
existing fault diagnosis methods. To address this problem, a novel fault
diagnosis model named self-adaptive temporal-spatial attention network
(TSA-SAN) is proposed. First, inter-mode mappings are constructed using healthy
category data to generate multimode samples. To enrich the diversity of the
fault data, interpolation is performed between healthy and fault samples.
Subsequently, the fault diagnosis model is trained using real and generated
data. The self-adaptive instance normalization is established to suppress
irrelevant information while retaining essential statistical features for
diagnosis. In addition, a temporal-spatial attention mechanism is constructed
to focus on the key features, thus enhancing the generalization ability of the
model. The extensive experiments demonstrate that the proposed model
significantly outperforms the state-of-the-art methods. The code will be
available on Github at https://github.com/GuangqiangLi/TSA-SAN.

</details>


### [377] [A Fast Kernel-based Conditional Independence test with Application to Causal Discovery](https://arxiv.org/pdf/2505.11085)
*Oliver Schacht, Biwei Huang*

Main category: cs.LG

TL;DR: FastKCI is a scalable, parallelizable kernel-based conditional independence test that reduces computational complexity while maintaining statistical power.


<details>
  <summary>Details</summary>
Motivation: The cubic computational complexity of KCI testing limits its use on large datasets, prompting the need for a more efficient method.

Method: FastKCI uses a mixture-of-experts approach, partitioning data via a Gaussian mixture model and conducting parallel local KCI tests, with results aggregated via importance-weighted sampling.

Result: FastKCI achieves significant computational speedups without losing statistical power, as validated on synthetic and real-world datasets.

Conclusion: FastKCI is a practical and efficient solution for conditional independence testing in large-scale causal inference.

Abstract: Kernel-based conditional independence (KCI) testing is a powerful
nonparametric method commonly employed in causal discovery tasks. Despite its
flexibility and statistical reliability, cubic computational complexity limits
its application to large datasets. To address this computational bottleneck, we
propose \textit{FastKCI}, a scalable and parallelizable kernel-based
conditional independence test that utilizes a mixture-of-experts approach
inspired by embarrassingly parallel inference techniques for Gaussian
processes. By partitioning the dataset based on a Gaussian mixture model over
the conditioning variables, FastKCI conducts local KCI tests in parallel,
aggregating the results using an importance-weighted sampling scheme.
Experiments on synthetic datasets and benchmarks on real-world production data
validate that FastKCI maintains the statistical power of the original KCI test
while achieving substantial computational speedups. FastKCI thus represents a
practical and efficient solution for conditional independence testing in causal
inference on large-scale data.

</details>


### [378] [Bidirectional Distillation: A Mixed-Play Framework for Multi-Agent Generalizable Behaviors](https://arxiv.org/pdf/2505.11100)
*Lang Feng, Jiahao Lin, Dong Xing, Li Zhang, De Ma, Gang Pan*

Main category: cs.LG

TL;DR: BiDist, a bidirectional distillation framework, enhances MARL generalization by combining forward and reverse distillation, outperforming self-play methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitation of self-play-based methods in MARL for population-population generalization, especially with unseen co-players.

Method: Proposes BiDist, a mixed-play framework using bidirectional knowledge distillation (forward and reverse) to diversify policy space.

Result: BiDist shows strong generalization in cooperative, competitive, and social dilemma tasks, diversifying policy space effectively.

Conclusion: BiDist is a concise, efficient solution for MARL generalization, validated by theory and experiments.

Abstract: Population-population generalization is a challenging problem in multi-agent
reinforcement learning (MARL), particularly when agents encounter unseen
co-players. However, existing self-play-based methods are constrained by the
limitation of inside-space generalization. In this study, we propose
Bidirectional Distillation (BiDist), a novel mixed-play framework, to overcome
this limitation in MARL. BiDist leverages knowledge distillation in two
alternating directions: forward distillation, which emulates the historical
policies' space and creates an implicit self-play, and reverse distillation,
which systematically drives agents towards novel distributions outside the
known policy space in a non-self-play manner. In addition, BiDist operates as a
concise and efficient solution without the need for the complex and costly
storage of past policies. We provide both theoretical analysis and empirical
evidence to support BiDist's effectiveness. Our results highlight its
remarkable generalization ability across a variety of cooperative, competitive,
and social dilemma tasks, and reveal that BiDist significantly diversifies the
policy distribution space. We also present comprehensive ablation studies to
reinforce BiDist's effectiveness and key success factors. Source codes are
available in the supplementary material.

</details>


### [379] [Inferring the Most Similar Variable-length Subsequences between Multidimensional Time Series](https://arxiv.org/pdf/2505.11106)
*Thanadej Rattanakornphan, Piyanon Charoenpoonpanich, Chainarong Amornbunchornvej*

Main category: cs.LG

TL;DR: Proposes an efficient algorithm for finding the most similar multidimensional subsequences between time series with length differences, outperforming baselines in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of efficient frameworks for identifying similar patterns in multidimensional time series with distortions and length variations.

Method: Develops an algorithm with theoretical guarantees for correctness and efficiency, tested on simulation and real-world datasets.

Result: Outperforms baselines, running up to 20 times faster, and provides insights into stock market dependencies and baboon movement patterns.

Conclusion: The algorithm is versatile, efficient, and publicly available for any time series analysis.

Abstract: Finding the most similar subsequences between two multidimensional time
series has many applications: e.g. capturing dependency in stock market or
discovering coordinated movement of baboons. Considering one pattern occurring
in one time series, we might be wondering whether the same pattern occurs in
another time series with some distortion that might have a different length.
Nevertheless, to the best of our knowledge, there is no efficient framework
that deals with this problem yet. In this work, we propose an algorithm that
provides the exact solution of finding the most similar multidimensional
subsequences between time series where there is a difference in length both
between time series and between subsequences. The algorithm is built based on
theoretical guarantee of correctness and efficiency. The result in simulation
datasets illustrated that our approach not just only provided correct solution,
but it also utilized running time only quarter of time compared against the
baseline approaches. In real-world datasets, it extracted the most similar
subsequences even faster (up to 20 times faster against baseline methods) and
provided insights regarding the situation in stock market and following
relations of multidimensional time series of baboon movement. Our approach can
be used for any time series. The code and datasets of this work are provided
for the public use.

</details>


### [380] [FairSHAP: Preprocessing for Fairness Through Attribution-Based Data Augmentation](https://arxiv.org/pdf/2505.11111)
*Lin Zhu, Yijun Bian, Lei You*

Main category: cs.LG

TL;DR: FairSHAP is a pre-processing framework using Shapley values to enhance fairness in ML models by identifying and modifying fairness-critical instances, improving both individual and group fairness with minimal data disruption.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of transparency in existing preprocessing methods for identifying unfairness sources in ML models, especially in high-stakes domains.

Method: FairSHAP leverages Shapley values to pinpoint fairness-critical instances and modifies them via instance-level matching across sensitive groups, reducing discriminative risk while maintaining data integrity and accuracy.

Result: FairSHAP significantly improves demographic parity and equality of opportunity across datasets, with minimal data perturbation and sometimes better predictive performance.

Conclusion: FairSHAP is a model-agnostic, transparent tool that integrates into ML pipelines, offering actionable insights into bias sources while enhancing fairness.

Abstract: Ensuring fairness in machine learning models is critical, particularly in
high-stakes domains where biased decisions can lead to serious societal
consequences. Existing preprocessing approaches generally lack transparent
mechanisms for identifying which features or instances are responsible for
unfairness. This obscures the rationale behind data modifications. We introduce
FairSHAP, a novel pre-processing framework that leverages Shapley value
attribution to improve both individual and group fairness. FairSHAP identifies
fairness-critical instances in the training data using an interpretable measure
of feature importance, and systematically modifies them through instance-level
matching across sensitive groups. This process reduces discriminative risk - an
individual fairness metric - while preserving data integrity and model
accuracy. We demonstrate that FairSHAP significantly improves demographic
parity and equality of opportunity across diverse tabular datasets, achieving
fairness gains with minimal data perturbation and, in some cases, improved
predictive performance. As a model-agnostic and transparent method, FairSHAP
integrates seamlessly into existing machine learning pipelines and provides
actionable insights into the sources of bias.Our code is on
https://github.com/youlei202/FairSHAP.

</details>


### [381] [Dual-Balancing for Physics-Informed Neural Networks](https://arxiv.org/pdf/2505.11117)
*Chenhong Zhou, Jie Chen, Zaifeng Yang, Ching Eng Png*

Main category: cs.LG

TL;DR: DB-PINN dynamically adjusts loss weights to address imbalance issues in PINNs, improving accuracy and convergence speed.


<details>
  <summary>Details</summary>
Motivation: Vanilla PINNs suffer from poor accuracy and slow convergence due to multi-objective optimization challenges.

Method: Proposes Dual-Balanced PINN (DB-PINN) with inter-balancing (gradient imbalance) and intra-balancing (fitting difficulty) for dynamic weight adjustment.

Result: DB-PINN outperforms gradient-based weighting methods in convergence speed and prediction accuracy.

Conclusion: DB-PINN effectively addresses imbalance issues in PINNs, enhancing performance and stability.

Abstract: Physics-informed neural networks (PINNs) have emerged as a new learning
paradigm for solving partial differential equations (PDEs) by enforcing the
constraints of physical equations, boundary conditions (BCs), and initial
conditions (ICs) into the loss function. Despite their successes, vanilla PINNs
still suffer from poor accuracy and slow convergence due to the intractable
multi-objective optimization issue. In this paper, we propose a novel
Dual-Balanced PINN (DB-PINN), which dynamically adjusts loss weights by
integrating inter-balancing and intra-balancing to alleviate two imbalance
issues in PINNs. Inter-balancing aims to mitigate the gradient imbalance
between PDE residual loss and condition-fitting losses by determining an
aggregated weight that offsets their gradient distribution discrepancies.
Intra-balancing acts on condition-fitting losses to tackle the imbalance in
fitting difficulty across diverse conditions. By evaluating the fitting
difficulty based on the loss records, intra-balancing can allocate the
aggregated weight proportionally to each condition loss according to its
fitting difficulty levels. We further introduce a robust weight update strategy
to prevent abrupt spikes and arithmetic overflow in instantaneous weight values
caused by large loss variances, enabling smooth weight updating and stable
training. Extensive experiments demonstrate that DB-PINN achieves significantly
superior performance than those popular gradient-based weighting methods in
terms of convergence speed and prediction accuracy. Our code and supplementary
material are available at https://github.com/chenhong-zhou/DualBalanced-PINNs.

</details>


### [382] [GraphOracle: A Foundation Model for Knowledge Graph Reasoning](https://arxiv.org/pdf/2505.11125)
*Enjun Du, Siyi Liu, Yongqi Zhang*

Main category: cs.LG

TL;DR: GraphOracle is a relation-centric foundation model for knowledge graphs, improving prediction performance by up to 35% with minimal adaptation.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in developing foundation models for knowledge graphs due to their dynamic nature and cross-domain reasoning needs.

Method: Converts knowledge graphs into Relation-Dependency Graphs (RDG) with fewer edges, uses query-dependent attention for inductive representations, and pre-trains on diverse graphs.

Result: Achieves state-of-the-art performance on 31 benchmarks, with up to 35% improvement over baselines.

Conclusion: GraphOracle effectively generalizes to unseen entities, relations, and graphs, demonstrating strong adaptability.

Abstract: Foundation models have demonstrated remarkable capabilities across various
domains, but developing analogous models for knowledge graphs presents unique
challenges due to their dynamic nature and the need for cross-domain reasoning.
To address these issues, we introduce \textbf{\textsc{GraphOracle}}, a
relation-centric foundation model that unifies reasoning across knowledge
graphs by converting them into Relation-Dependency Graphs (RDG), explicitly
encoding compositional patterns with fewer edges than prior methods. A
query-dependent attention mechanism is further developed to learn inductive
representations for both relations and entities. Pre-training on diverse
knowledge graphs, followed by minutes-level fine-tuning, enables effective
generalization to unseen entities, relations, and entire graphs. Through
comprehensive experiments on 31 diverse benchmarks spanning transductive,
inductive, and cross-domain settings, we demonstrate consistent
state-of-the-art performance with minimal adaptation, improving the prediction
performance by up to 35\% compared to the strongest baselines.

</details>


### [383] [Attention on the Sphere](https://arxiv.org/pdf/2505.11157)
*Boris Bonev, Max Rietmann, Andrea Paris, Alberto Carpentieri, Thorsten Kurth*

Main category: cs.LG

TL;DR: A generalized attention mechanism for spherical domains is introduced, enabling Transformers to process spherical data with geometric faithfulness and rotational equivariance, outperforming Cartesian methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for preserving spherical symmetries and topology in fields like atmospheric physics, cosmology, and robotics for physical accuracy.

Method: Integration of numerical quadrature weights into attention for rotational equivariance, and neighborhood attention to reduce complexity and enhance locality.

Result: Outperforms planar counterparts in tasks like simulating shallow water equations, spherical image segmentation, and depth estimation.

Conclusion: Spherical Transformers with geometric priors are superior for learning on spherical domains, validated by diverse tasks.

Abstract: We introduce a generalized attention mechanism for spherical domains,
enabling Transformer architectures to natively process data defined on the
two-dimensional sphere - a critical need in fields such as atmospheric physics,
cosmology, and robotics, where preserving spherical symmetries and topology is
essential for physical accuracy. By integrating numerical quadrature weights
into the attention mechanism, we obtain a geometrically faithful spherical
attention that is approximately rotationally equivariant, providing strong
inductive biases and leading to better performance than Cartesian approaches.
To further enhance both scalability and model performance, we propose
neighborhood attention on the sphere, which confines interactions to geodesic
neighborhoods. This approach reduces computational complexity and introduces
the additional inductive bias for locality, while retaining the symmetry
properties of our method. We provide optimized CUDA kernels and
memory-efficient implementations to ensure practical applicability. The method
is validated on three diverse tasks: simulating shallow water equations on the
rotating sphere, spherical image segmentation, and spherical depth estimation.
Across all tasks, our spherical Transformers consistently outperform their
planar counterparts, highlighting the advantage of geometric priors for
learning on spherical domains.

</details>


### [384] [FedDuA: Doubly Adaptive Federated Learning](https://arxiv.org/pdf/2505.11126)
*Shokichi Takakura, Seng Pei Liew, Satoshi Hasegawa*

Main category: cs.LG

TL;DR: FedDuA improves FedAvg by adaptively selecting global learning rates to address heterogeneity, achieving faster convergence without extra costs.


<details>
  <summary>Details</summary>
Motivation: FedAvg's slow convergence due to dataset heterogeneity and parameter space anisotropy.

Method: Formalizes server optimization via mirror descent, introducing FedDuA with adaptive learning rates based on inter-client and coordinate-wise heterogeneity.

Result: Proves minimax optimality of step-size rule; outperforms baselines in experiments, robust to hyperparameters.

Conclusion: FedDuA enhances federated learning efficiency without additional client-side costs.

Abstract: Federated learning is a distributed learning framework where clients
collaboratively train a global model without sharing their raw data. FedAvg is
a popular algorithm for federated learning, but it often suffers from slow
convergence due to the heterogeneity of local datasets and anisotropy in the
parameter space. In this work, we formalize the central server optimization
procedure through the lens of mirror descent and propose a novel framework,
called FedDuA, which adaptively selects the global learning rate based on both
inter-client and coordinate-wise heterogeneity in the local updates. We prove
that our proposed doubly adaptive step-size rule is minimax optimal and provide
a convergence analysis for convex objectives. Although the proposed method does
not require additional communication or computational cost on clients,
extensive numerical experiments show that our proposed framework outperforms
baselines in various settings and is robust to the choice of hyperparameters.

</details>


### [385] [What's Inside Your Diffusion Model? A Score-Based Riemannian Metric to Explore the Data Manifold](https://arxiv.org/pdf/2505.11128)
*Simone Azeglio, Arianna Di Bernardo*

Main category: cs.LG

TL;DR: The paper introduces a score-based Riemannian metric to analyze the geometry of data manifolds learned by diffusion models, improving interpolation and extrapolation tasks.


<details>
  <summary>Details</summary>
Motivation: To understand the geometric properties of data manifolds in diffusion models, which remain poorly characterized despite their success in capturing complex image distributions.

Method: Proposes a score-based Riemannian metric using the Stein score function to define a metric tensor, enabling geodesic computation for manifold navigation.

Result: Outperforms baselines on perceptual and distribution-level metrics, showing smoother, more realistic image transitions.

Conclusion: The method reveals implicit geometric structures in diffusion models and offers a principled way to navigate natural image manifolds using Riemannian geometry.

Abstract: Recent advances in diffusion models have demonstrated their remarkable
ability to capture complex image distributions, but the geometric properties of
the learned data manifold remain poorly understood. We address this gap by
introducing a score-based Riemannian metric that leverages the Stein score
function from diffusion models to characterize the intrinsic geometry of the
data manifold without requiring explicit parameterization. Our approach defines
a metric tensor in the ambient space that stretches distances perpendicular to
the manifold while preserving them along tangential directions, effectively
creating a geometry where geodesics naturally follow the manifold's contours.
We develop efficient algorithms for computing these geodesics and demonstrate
their utility for both interpolation between data points and extrapolation
beyond the observed data distribution. Through experiments on synthetic data
with known geometry, Rotated MNIST, and complex natural images via Stable
Diffusion, we show that our score-based geodesics capture meaningful
transformations that respect the underlying data distribution. Our method
consistently outperforms baseline approaches on perceptual metrics (LPIPS) and
distribution-level metrics (FID, KID), producing smoother, more realistic image
transitions. These results reveal the implicit geometric structure learned by
diffusion models and provide a principled way to navigate the manifold of
natural images through the lens of Riemannian geometry.

</details>


### [386] [Fairness-aware Anomaly Detection via Fair Projection](https://arxiv.org/pdf/2505.11132)
*Feng Xiao, Xiaoying Tang, Jicong Fan*

Main category: cs.LG

TL;DR: FairAD is a fairness-aware unsupervised anomaly detection method that ensures group fairness by mapping data to a common target distribution and introduces a threshold-free fairness metric.


<details>
  <summary>Details</summary>
Motivation: Addressing potential bias in anomaly detection systems to prevent unfair treatment across demographic groups in high-impact applications like finance and healthcare.

Method: FairAD learns a projection to map data of different groups to a common target distribution for reliable density estimation, ensuring fairness.

Result: Achieves improved trade-off between detection accuracy and fairness on real-world benchmarks, even with skewed data.

Conclusion: FairAD effectively balances fairness and accuracy in unsupervised anomaly detection, with a novel metric for fairness evaluation.

Abstract: Unsupervised anomaly detection is a critical task in many high-social-impact
applications such as finance, healthcare, social media, and cybersecurity,
where demographics involving age, gender, race, disease, etc, are used
frequently. In these scenarios, possible bias from anomaly detection systems
can lead to unfair treatment for different groups and even exacerbate social
bias. In this work, first, we thoroughly analyze the feasibility and necessary
assumptions for ensuring group fairness in unsupervised anomaly detection.
Second, we propose a novel fairness-aware anomaly detection method FairAD. From
the normal training data, FairAD learns a projection to map data of different
demographic groups to a common target distribution that is simple and compact,
and hence provides a reliable base to estimate the density of the data. The
density can be directly used to identify anomalies while the common target
distribution ensures fairness between different groups. Furthermore, we propose
a threshold-free fairness metric that provides a global view for model's
fairness, eliminating dependence on manual threshold selection. Experiments on
real-world benchmarks demonstrate that our method achieves an improved
trade-off between detection accuracy and fairness under both balanced and
skewed data across different groups.

</details>


### [387] [Towards Robust Spiking Neural Networks:Mitigating Heterogeneous Training Vulnerability via Dominant Eigencomponent Projection](https://arxiv.org/pdf/2505.11134)
*Desong Zhang, Jia Hu, Geyong Min*

Main category: cs.LG

TL;DR: SNNs trained with direct encoding and BPTT are vulnerable to catastrophic collapse from slight data distribution shifts. DEP, a hyperparameter-free method, mitigates this by reducing Hessian spectral radius, enhancing robustness.


<details>
  <summary>Details</summary>
Motivation: SNNs' energy efficiency is compromised by vulnerability to heterogeneous data poisoning when trained with direct encoding and BPTT.

Method: Developed Dominant Eigencomponent Projection (DEP) to orthogonally project gradients, reducing Hessian spectral radius and preventing sharp minima.

Result: DEP mitigates vulnerability and significantly enhances SNN robustness against heterogeneous data poisoning.

Conclusion: DEP supports safer and more reliable SNN deployment by addressing training vulnerabilities.

Abstract: Spiking Neural Networks (SNNs) process information via discrete spikes,
enabling them to operate at remarkably low energy levels. However, our
experimental observations reveal a striking vulnerability when SNNs are trained
using the mainstream method--direct encoding combined with backpropagation
through time (BPTT): even a single backward pass on data drawn from a slightly
different distribution can lead to catastrophic network collapse. Our
theoretical analysis attributes this vulnerability to the repeated inputs
inherent in direct encoding and the gradient accumulation characteristic of
BPTT, which together produce an exceptional large Hessian spectral radius. To
address this challenge, we develop a hyperparameter-free method called Dominant
Eigencomponent Projection (DEP). By orthogonally projecting gradients to
precisely remove their dominant components, DEP effectively reduces the Hessian
spectral radius, thereby preventing SNNs from settling into sharp minima.
Extensive experiments demonstrate that DEP not only mitigates the vulnerability
of SNNs to heterogeneous data poisoning, but also significantly enhances
overall robustness compared to key baselines, providing strong support for
safer and more reliable SNN deployment.

</details>


### [388] [RanDeS: Randomized Delta Superposition for Multi-Model Compression](https://arxiv.org/pdf/2505.11204)
*Hangyu Zhou, Aaron Gokaslan, Volodymyr Kuleshov, Bharath Hariharan*

Main category: cs.LG

TL;DR: The paper proposes a method to reduce interference in model merging by using random orthogonal transformations, improving performance without extra memory.


<details>
  <summary>Details</summary>
Motivation: Model merging suffers from degraded performance due to interference among task-specific parameter adjustments (deltas).

Method: Reformulate model merging as a compress-and-retrieve scheme, using random orthogonal transformations to decorrelate deltas.

Result: Drastically reduces interference, improving performance in vision and language tasks with no extra memory for new models.

Conclusion: The approach enables efficient, flexible multi-model serving with minimal compute overhead.

Abstract: From a multi-model compression perspective, model merging enables
memory-efficient serving of multiple models fine-tuned from the same base, but
suffers from degraded performance due to interference among their task-specific
parameter adjustments (i.e., deltas). In this paper, we reformulate model
merging as a compress-and-retrieve scheme, revealing that the task interference
arises from the summation of irrelevant deltas during model retrieval. To
address this issue, we use random orthogonal transformations to decorrelate
these vectors into self-cancellation. We show that this approach drastically
reduces interference, improving performance across both vision and language
tasks. Since these transformations are fully defined by random seeds, adding
new models requires no extra memory. Further, their data- and model-agnostic
nature enables easy addition or removal of models with minimal compute
overhead, supporting efficient and flexible multi-model serving.

</details>


### [389] [Covariance Density Neural Networks](https://arxiv.org/pdf/2505.11139)
*Om Roy, Yashar Moshfeghi, Keith Smith*

Main category: cs.LG

TL;DR: The paper introduces Density Matrix-based Graph Neural Networks to improve performance over Covariance Neural Networks (VNNs) by using a quasi-Hamiltonian approach, enhancing discriminability and robustness, particularly in Brain-Computer Interface EEG classification.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of consensus in choosing graph structures for modeling network data, the paper aims to improve VNNs by leveraging a density matrix for better performance and stability.

Method: Constructs a density matrix from the sample covariance matrix, treating it as a quasi-Hamiltonian, and uses it as a Graph Shift Operator (GSO) to extract data components at different scales.

Result: The approach outperforms VNNs and EEGnet in EEG motor imagery classification, offering better robustness, discriminability, and transferability in Brain-Computer Interfaces.

Conclusion: Density Matrix-based GNNs provide a robust framework for transferable Brain-Computer Interface applications, addressing stability-discriminability trade-offs effectively.

Abstract: Graph neural networks have re-defined how we model and predict on network
data but there lacks a consensus on choosing the correct underlying graph
structure on which to model signals. CoVariance Neural Networks (VNN) address
this issue by using the sample covariance matrix as a Graph Shift Operator
(GSO). Here, we improve on the performance of VNNs by constructing a Density
Matrix where we consider the sample Covariance matrix as a quasi-Hamiltonian of
the system in the space of random variables. Crucially, using this density
matrix as the GSO allows components of the data to be extracted at different
scales, allowing enhanced discriminability and performance. We show that this
approach allows explicit control of the stability-discriminability trade-off of
the network, provides enhanced robustness to noise compared to VNNs, and
outperforms them in useful real-life applications where the underlying
covariance matrix is informative. In particular, we show that our model can
achieve strong performance in subject-independent Brain Computer Interface EEG
motor imagery classification, outperforming EEGnet while being faster. This
shows how covariance density neural networks provide a basis for the
notoriously difficult task of transferability of BCIs when evaluated on unseen
individuals.

</details>


### [390] [Bayesian Hierarchical Invariant Prediction](https://arxiv.org/pdf/2505.11211)
*Francisco Madaleno, Pernille Julie Viuff Sand, Francisco C. Pereira, Sergio Hernan Garrido Mejia*

Main category: cs.LG

TL;DR: BHIP reframes ICP using Hierarchical Bayes for better scalability and prior integration, testing sparsity-inducing priors for reliable causal feature identification.


<details>
  <summary>Details</summary>
Motivation: To improve computational scalability and leverage prior information in causal prediction under heterogeneous data.

Method: Bayesian Hierarchical Invariant Prediction (BHIP) with horseshoe and spike-and-slab priors for sparsity.

Result: Improved scalability and reliable causal feature identification in synthetic and real-world data.

Conclusion: BHIP is a viable alternative to ICP, offering enhanced performance and flexibility.

Abstract: We propose Bayesian Hierarchical Invariant Prediction (BHIP) reframing
Invariant Causal Prediction (ICP) through the lens of Hierarchical Bayes. We
leverage the hierarchical structure to explicitly test invariance of causal
mechanisms under heterogeneous data, resulting in improved computational
scalability for a larger number of predictors compared to ICP. Moreover, given
its Bayesian nature BHIP enables the use of prior information. In this paper,
we test two sparsity inducing priors: horseshoe and spike-and-slab, both of
which allow us a more reliable identification of causal features. We test BHIP
in synthetic and real-world data showing its potential as an alternative
inference method to ICP.

</details>


### [391] [Bi-directional Recurrence Improves Transformer in Partially Observable Markov Decision Processes](https://arxiv.org/pdf/2505.11153)
*Ashok Arora, Neetesh Kumar*

Main category: cs.LG

TL;DR: A bi-recurrent model improves sample efficiency and reduces parameters in POMDPs, outperforming existing methods by 87.39% to 482.04%.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of transformer-based models in POMDPs, such as high parameter counts and underdeveloped applications, to enhance sample efficiency.

Method: Introduces a bi-recurrent architecture replacing multiple feed-forward layers with a single bi-directional recurrence unit to better utilize sequential dependencies.

Result: Outperforms transformer-based, attention-based, and recurrence-based methods by 87.39% to 482.04% across 23 POMDP environments.

Conclusion: The bi-recurrent model effectively handles partial observability, improves sample efficiency, and reduces parameter count, making it suitable for real-world RL applications.

Abstract: In real-world reinforcement learning (RL) scenarios, agents often encounter
partial observability, where incomplete or noisy information obscures the true
state of the environment. Partially Observable Markov Decision Processes
(POMDPs) are commonly used to model these environments, but effective
performance requires memory mechanisms to utilise past observations. While
recurrence networks have traditionally addressed this need, transformer-based
models have recently shown improved sample efficiency in RL tasks. However,
their application to POMDPs remains underdeveloped, and their real-world
deployment is constrained due to the high parameter count. This work introduces
a novel bi-recurrent model architecture that improves sample efficiency and
reduces model parameter count in POMDP scenarios. The architecture replaces the
multiple feed forward layers with a single layer of bi-directional recurrence
unit to better capture and utilize sequential dependencies and contextual
information. This approach improves the model's ability to handle partial
observability and increases sample efficiency, enabling effective learning from
comparatively fewer interactions. To evaluate the performance of the proposed
model architecture, experiments were conducted on a total of 23 POMDP
environments. The proposed model architecture outperforms existing
transformer-based, attention-based, and recurrence-based methods by a margin
ranging from 87.39% to 482.04% on average across the 23 POMDP environments.

</details>


### [392] [A Set-Sequence Model for Time Series](https://arxiv.org/pdf/2505.11243)
*Elliot L. Epstein, Apaar Sadhwani, Kay Giesecke*

Main category: cs.LG

TL;DR: The paper introduces a Set-Sequence model for financial prediction, eliminating handcrafted features by learning cross-sectional summaries and using sequence models for unit-level predictions. It outperforms benchmarks in stock return and mortgage behavior tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional methods rely on handcrafted features to capture latent cross-sectional effects in financial prediction, which is inefficient and inflexible.

Method: A Set-Sequence model: Set model learns shared cross-sectional summaries, and Sequence model uses these summaries for unit-level predictions. Both are trained jointly.

Result: Outperforms benchmarks in stock return prediction and mortgage behavior tasks, with linear-time efficiency.

Conclusion: The Set-Sequence model is efficient, flexible, and superior to traditional methods, with potential for broader applications.

Abstract: In many financial prediction problems, the behavior of individual units (such
as loans, bonds, or stocks) is influenced by observable unit-level factors and
macroeconomic variables, as well as by latent cross-sectional effects.
Traditional approaches attempt to capture these latent effects via handcrafted
summary features. We propose a Set-Sequence model that eliminates the need for
handcrafted features. The Set model first learns a shared cross-sectional
summary at each period. The Sequence model then ingests the summary-augmented
time series for each unit independently to predict its outcome. Both components
are learned jointly over arbitrary sets sampled during training. Our approach
harnesses the set nature of the cross-section and is computationally efficient,
generating set summaries in linear time relative to the number of units. It is
also flexible, allowing the use of existing sequence models and accommodating a
variable number of units at inference. Empirical evaluations demonstrate that
our Set-Sequence model significantly outperforms benchmarks on stock return
prediction and mortgage behavior tasks. Code will be released.

</details>


### [393] [Gaussian Weight Sampling for Scalable, Efficient and Stable Pseudo-Quantization Training](https://arxiv.org/pdf/2505.11170)
*Myeonghwan Ahn, Sungjoo Yoo*

Main category: cs.LG

TL;DR: The paper proposes Pseudo-quantization Training (PQT) as a solution to the challenges of fully quantized training (FQT), introducing a noise distribution method for efficient low-precision training.


<details>
  <summary>Details</summary>
Motivation: FQT faces consistency and scalability issues, requiring extensive resources. PQT is explored to address these challenges with a practical, efficient approach.

Method: The authors propose a noise distribution method (R) for PQT, enabling low-precision FP parameters and efficient fake quantization. Gaussian weight sampling is used for scalability and stability.

Result: PQT with Gaussian sampling is scalable (down to FP6), efficient (1.40% overhead), and stable, matching or surpassing BF16 baseline performance in GPT2 and Llama2 models.

Conclusion: PQT provides a stable and efficient foundation for low-precision training, outperforming FQT and matching BF16 performance.

Abstract: Ever-growing scale of large language models (LLMs) is pushing for improved
efficiency, favoring fully quantized training (FQT) over BF16. While FQT
accelerates training, it faces consistency challenges and requires searching
over an exponential number of cases, each needing over 200B tokens to ensure
stability.
  Pseudo-quantization training (PQT) addresses the issues of FQT, although it
is not well-studied. We explore the practical implications of PQT in detail and
propose a noise distribution $R$ that is floating-point (FP)-friendly, with
ideal properties including stochastic precision annealing. As a result, the
proposed method serves as an effective theoretical foundation for low-precision
FP parameters through PQT, utilizing efficient fake quantization via an
addition and subsequent FP casting.
  We demonstrate that Gaussian weight sampling is (1) scalable: supports
low-precision FP parameters down to FP6 and high-precision noise up to 9-bit
with BF16 operator. The proposed method is (2) efficient: incurring
computational overhead as low as 1.40\% on the A100 GPU in terms of Llama2
training tokens per second, and requiring 2 bytes per parameter in GPU memory.
We demonstrate that PQT with Gaussian weight sampling is (3) stable: closely
following or even surpassing performance of the BF16 baseline while
pre-training GPT2 and Llama2 models with up to 1B parameters and 300B tokens.

</details>


### [394] [VitaGraph: Building a Knowledge Graph for Biologically Relevant Learning Tasks](https://arxiv.org/pdf/2505.11185)
*Francesco Madeddu, Lucia Testa, Gianluca De Carlo, Michele Pieroni, Andrea Mastropietro, Aris Anagnostopoulos, Paolo Tieri, Sergio Barbarossa*

Main category: cs.LG

TL;DR: The paper presents a refined biological knowledge graph integrating multiple datasets to improve AI-driven tasks like drug repurposing and PPI prediction.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for high-quality data in computational biology to enhance machine learning model reliability.

Method: A pipeline cleans and enriches the Drug Repurposing Knowledge Graph (DRKG) with features like molecular fingerprints and gene ontologies.

Result: A coherent, multi-purpose biological knowledge graph for tasks like drug repurposing and PPI prediction.

Conclusion: The refined graph serves as a state-of-the-art platform for computational biology and precision medicine research.

Abstract: The intrinsic complexity of human biology presents ongoing challenges to
scientific understanding. Researchers collaborate across disciplines to expand
our knowledge of the biological interactions that define human life. AI
methodologies have emerged as powerful tools across scientific domains,
particularly in computational biology, where graph data structures effectively
model biological entities such as protein-protein interaction (PPI) networks
and gene functional networks. Those networks are used as datasets for paramount
network medicine tasks, such as gene-disease association prediction, drug
repurposing, and polypharmacy side effect studies. Reliable predictions from
machine learning models require high-quality foundational data. In this work,
we present a comprehensive multi-purpose biological knowledge graph constructed
by integrating and refining multiple publicly available datasets. Building upon
the Drug Repurposing Knowledge Graph (DRKG), we define a pipeline tasked with
a) cleaning inconsistencies and redundancies present in DRKG, b) coalescing
information from the main available public data sources, and c) enriching the
graph nodes with expressive feature vectors such as molecular fingerprints and
gene ontologies. Biologically and chemically relevant features improve the
capacity of machine learning models to generate accurate and well-structured
embedding spaces. The resulting resource represents a coherent and reliable
biological knowledge graph that serves as a state-of-the-art platform to
advance research in computational biology and precision medicine. Moreover, it
offers the opportunity to benchmark graph-based machine learning and network
medicine models on relevant tasks. We demonstrate the effectiveness of the
proposed dataset by benchmarking it against the task of drug repurposing, PPI
prediction, and side-effect prediction, modeled as link prediction problems.

</details>


### [395] [Heterogeneity-Aware Client Sampling: A Unified Solution for Consistent Federated Learning](https://arxiv.org/pdf/2505.11304)
*Shudi Weng, Chao Ren, Ming Xiao, Mikael Skoglund*

Main category: cs.LG

TL;DR: The paper analyzes how communication and computation heterogeneity in federated learning (FL) causes optimization inconsistency and proposes FedACS, a method to mitigate this issue, achieving superior performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Heterogeneity in client capabilities in FL leads to optimization inconsistency, but its joint impact is underexplored. This paper aims to unify its understanding and propose a solution.

Method: The authors conduct a unified theoretical analysis of heterogeneous FL and introduce FedACS, a client sampling method to address inconsistency.

Result: FedACS converges to the correct optimum at a rate of $O(1/\sqrt{R})$, outperforming baselines by 4.3%-36% while reducing communication and computation costs.

Conclusion: FedACS effectively eliminates objective inconsistency in FL, offering a universal solution with proven convergence and practical efficiency gains.

Abstract: Federated learning (FL) commonly involves clients with diverse communication
and computational capabilities. Such heterogeneity can significantly distort
the optimization dynamics and lead to objective inconsistency, where the global
model converges to an incorrect stationary point potentially far from the
pursued optimum. Despite its critical impact, the joint effect of communication
and computation heterogeneity has remained largely unexplored, due to the
intrinsic complexity of their interaction. In this paper, we reveal the
fundamentally distinct mechanisms through which heterogeneous communication and
computation drive inconsistency in FL. To the best of our knowledge, this is
the first unified theoretical analysis of general heterogeneous FL, offering a
principled understanding of how these two forms of heterogeneity jointly
distort the optimization trajectory under arbitrary choices of local solvers.
Motivated by these insights, we propose Federated Heterogeneity-Aware Client
Sampling, FedACS, a universal method to eliminate all types of objective
inconsistency. We theoretically prove that FedACS converges to the correct
optimum at a rate of $O(1/\sqrt{R})$, even in dynamic heterogeneous
environments. Extensive experiments across multiple datasets show that FedACS
outperforms state-of-the-art and category-specific baselines by 4.3%-36%, while
reducing communication costs by 22%-89% and computation loads by 14%-105%,
respectively.

</details>


### [396] [Modeling Cell Dynamics and Interactions with Unbalanced Mean Field Schr√∂dinger Bridge](https://arxiv.org/pdf/2505.11197)
*Zhenyi Zhang, Zihan Wang, Yuhao Sun, Tiejun Li, Peijie Zhou*

Main category: cs.LG

TL;DR: The paper introduces UMFSB and CytoBridge to model stochastic cell dynamics and interactions from sparse snapshot data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to account for cell-cell interactions, which are crucial for understanding cellular processes.

Method: Proposes UMFSB framework and CytoBridge, a deep learning algorithm, to model unbalanced stochastic dynamics and interactions.

Result: Validated on synthetic and real datasets, CytoBridge accurately identifies growth, transitions, and interactions, reducing false transitions.

Conclusion: CytoBridge improves modeling of cellular dynamics and interactions, offering better accuracy than current approaches.

Abstract: Modeling the dynamics from sparsely time-resolved snapshot data is crucial
for understanding complex cellular processes and behavior. Existing methods
leverage optimal transport, Schr\"odinger bridge theory, or their variants to
simultaneously infer stochastic, unbalanced dynamics from snapshot data.
However, these approaches remain limited in their ability to account for
cell-cell interactions. This integration is essential in real-world scenarios
since intercellular communications are fundamental life processes and can
influence cell state-transition dynamics. To address this challenge, we
formulate the Unbalanced Mean-Field Schr\"odinger Bridge (UMFSB) framework to
model unbalanced stochastic interaction dynamics from snapshot data. Inspired
by this framework, we further propose CytoBridge, a deep learning algorithm
designed to approximate the UMFSB problem. By explicitly modeling cellular
transitions, proliferation, and interactions through neural networks,
CytoBridge offers the flexibility to learn these processes directly from data.
The effectiveness of our method has been extensively validated using both
synthetic gene regulatory data and real scRNA-seq datasets. Compared to
existing methods, CytoBridge identifies growth, transition, and interaction
patterns, eliminates false transitions, and reconstructs the developmental
landscape with greater accuracy.

</details>


### [397] [Minimizing False-Positive Attributions in Explanations of Non-Linear Models](https://arxiv.org/pdf/2505.11210)
*Anders Gj√∏lbye, Stefan Haufe, Lars Kai Hansen*

Main category: cs.LG

TL;DR: PatternLocal is a new XAI method that reduces false-positive attributions by transforming discriminative model weights into a generative representation, outperforming other methods in non-linear tasks.


<details>
  <summary>Details</summary>
Motivation: Suppressor variables can mislead XAI methods by causing false-positive feature attributions, especially in non-linear models, necessitating a solution like PatternLocal.

Method: PatternLocal uses a locally linear surrogate (e.g., LIME, KernelSHAP) and transforms discriminative weights into a generative representation to suppress suppressor variables while maintaining local fidelity.

Result: PatternLocal outperformed other XAI methods in hyperparameter optimization on the XAI-TRIS benchmark, reducing false-positive attributions.

Conclusion: PatternLocal provides more reliable and actionable insights for non-linear tasks by effectively addressing suppressor variable challenges.

Abstract: Suppressor variables can influence model predictions without being dependent
on the target outcome and they pose a significant challenge for Explainable AI
(XAI) methods. These variables may cause false-positive feature attributions,
undermining the utility of explanations. Although effective remedies exist for
linear models, their extension to non-linear models and to instance-based
explanations has remained limited. We introduce PatternLocal, a novel XAI
technique that addresses this gap. PatternLocal begins with a locally linear
surrogate, e.g. LIME, KernelSHAP, or gradient-based methods, and transforms the
resulting discriminative model weights into a generative representation,
thereby suppressing the influence of suppressor variables while preserving
local fidelity. In extensive hyperparameter optimization on the XAI-TRIS
benchmark, PatternLocal consistently outperformed other XAI methods and reduced
false-positive attributions when explaining non-linear tasks, thereby enabling
more reliable and actionable insights.

</details>


### [398] [Sample Efficient Reinforcement Learning via Large Vision Language Model Distillation](https://arxiv.org/pdf/2505.11221)
*Donghoon Lee, Tung M. Luu, Younghwan Lee, Chang D. Yoo*

Main category: cs.LG

TL;DR: LVLM2P distills knowledge from large vision-language models into RL agents to improve efficiency and reduce exploration, enhancing sample efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the impracticality of large multimodal models and high sample complexity in RL for real-world deployment.

Method: Uses LVLM as a teacher to guide RL agents with instructional actions, reducing exploration and eliminating manual textual descriptors.

Result: LVLM2P significantly improves the sample efficiency of baseline RL algorithms.

Conclusion: The framework effectively bridges the gap between large models and practical RL applications.

Abstract: Recent research highlights the potential of multimodal foundation models in
tackling complex decision-making challenges. However, their large parameters
make real-world deployment resource-intensive and often impractical for
constrained systems. Reinforcement learning (RL) shows promise for
task-specific agents but suffers from high sample complexity, limiting
practical applications. To address these challenges, we introduce LVLM to
Policy (LVLM2P), a novel framework that distills knowledge from large
vision-language models (LVLM) into more efficient RL agents. Our approach
leverages the LVLM as a teacher, providing instructional actions based on
trajectories collected by the RL agent, which helps reduce less meaningful
exploration in the early stages of learning, thereby significantly accelerating
the agent's learning progress. Additionally, by leveraging the LVLM to suggest
actions directly from visual observations, we eliminate the need for manual
textual descriptors of the environment, enhancing applicability across diverse
tasks. Experiments show that LVLM2P significantly enhances the sample
efficiency of baseline RL algorithms.

</details>


### [399] [Learning traffic flows: Graph Neural Networks for Metamodelling Traffic Assignment](https://arxiv.org/pdf/2505.11230)
*Oskar Bohn Lassen, Serio Agriesti, Mohamed Eldafrawi, Daniele Gammelli, Guido Cantelmo, Guido Gentile, Francisco Camara Pereira*

Main category: cs.LG

TL;DR: A learning-based approach using Message-Passing Neural Networks (MPNNs) is proposed to approximate equilibrium flow in traffic assignment, offering faster and more scalable solutions compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional traffic assignment methods are computationally expensive and struggle with real-time or large-scale scenarios, necessitating a more efficient solution.

Method: The paper employs MPNNs as a metamodel to mimic conventional traffic simulators, capturing the underlying process rather than just data patterns.

Result: The model outperforms conventional deep learning techniques and demonstrates robustness in predicting traffic flows for out-of-distribution scenarios.

Conclusion: The MPNN-based approach accelerates scenario assessments, reduces computational costs, and enables real-time decision-making in transportation planning.

Abstract: The Traffic Assignment Problem is a fundamental, yet computationally
expensive, task in transportation modeling, especially for large-scale
networks. Traditional methods require iterative simulations to reach
equilibrium, making real-time or large-scale scenario analysis challenging. In
this paper, we propose a learning-based approach using Message-Passing Neural
Networks as a metamodel to approximate the equilibrium flow of the Stochastic
User Equilibrium assignment. Our model is designed to mimic the algorithmic
structure used in conventional traffic simulators allowing it to better capture
the underlying process rather than just the data. We benchmark it against other
conventional deep learning techniques and evaluate the model's robustness by
testing its ability to predict traffic flows on input data outside the domain
on which it was trained. This approach offers a promising solution for
accelerating out-of-distribution scenario assessments, reducing computational
costs in large-scale transportation planning, and enabling real-time
decision-making.

</details>


### [400] [Mergenetic: a Simple Evolutionary Model Merging Library](https://arxiv.org/pdf/2505.11427)
*Adrian Robert Minut, Tommaso Mencattini, Andrea Santilli, Donato Crisostomi, Emanuele Rodol√†*

Main category: cs.LG

TL;DR: Mergenetic is an open-source library for evolutionary model merging, enabling flexible experimentation and competitive results with modest hardware.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks lack support for flexible experimentation with evolutionary model merging strategies in language models.

Method: Mergenetic combines merging methods and evolutionary algorithms, using lightweight fitness estimators to reduce evaluation costs.

Result: The library produces competitive results across tasks and languages using modest hardware.

Conclusion: Mergenetic provides a cost-effective and accessible solution for evolutionary model merging in language models.

Abstract: Model merging allows combining the capabilities of existing models into a new
one - post hoc, without additional training. This has made it increasingly
popular thanks to its low cost and the availability of libraries that support
merging on consumer GPUs. Recent work shows that pairing merging with
evolutionary algorithms can boost performance, but no framework currently
supports flexible experimentation with such strategies in language models. We
introduce Mergenetic, an open-source library for evolutionary model merging.
Mergenetic enables easy composition of merging methods and evolutionary
algorithms while incorporating lightweight fitness estimators to reduce
evaluation costs. We describe its design and demonstrate that Mergenetic
produces competitive results across tasks and languages using modest hardware.

</details>


### [401] [Memory-Efficient Orthogonal Fine-Tuning with Principal Subspace Adaptation](https://arxiv.org/pdf/2505.11235)
*Fei Wu, Jia Hu, Geyong Min, Shiqiang Wang*

Main category: cs.LG

TL;DR: MOFT introduces memory-efficient orthogonal fine-tuning by using principal subspace adaptation, reducing memory usage while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Full fine-tuning of large models is costly; PEFT, especially orthogonal fine-tuning, is effective but memory-inefficient.

Method: MOFT constrains orthogonal transformations to a low-rank subspace via SVD, adds a projection constraint, and introduces learnable scaling vectors.

Result: MOFT outperforms baselines on 37 tasks across NLP and CV, with reduced memory usage.

Conclusion: MOFT offers a memory-efficient alternative to orthogonal fine-tuning without sacrificing performance.

Abstract: Driven by the relentless growth in model parameters, which renders full
fine-tuning prohibitively expensive for large-scale deployment,
parameter-efficient fine-tuning (PEFT) has emerged as a crucial approach for
rapidly adapting large models to a wide range of downstream tasks. Among the
PEFT family, orthogonal fine-tuning and its variants have demonstrated
remarkable performance by preserving hyperspherical energy, which encodes
pairwise angular similarity between neurons. However, these methods are
inherently memory-inefficient due to the need to store intermediate activations
from multiple full-dimensional sparse matrices. To address this limitation, we
propose Memory-efficient Orthogonal Fine-Tuning (MOFT) with principal subspace
adaptation. Specifically, we first establish a theoretical condition under
which orthogonal transformations within a low-rank subspace preserve
hyperspherical energy. Based on this insight, we constrain orthogonal
fine-tuning to the principal subspace defined by the top-r components obtained
through singular value decomposition and impose an additional constraint on the
projection matrix to satisfy the preservation condition. To enhance MOFT's
flexibility across tasks, we relax strict orthogonality by introducing two
learnable scaling vectors. Extensive experiments on 37 diverse tasks and four
models across NLP and CV demonstrate that MOFT consistently outperforms key
baselines while significantly reducing the memory footprint of orthogonal
fine-tuning.

</details>


### [402] [Massive-STEPS: Massive Semantic Trajectories for Understanding POI Check-ins -- Dataset and Benchmarks](https://arxiv.org/pdf/2505.11239)
*Wilson Wongso, Hao Xue, Flora D. Salim*

Main category: cs.LG

TL;DR: The paper introduces Massive-STEPS, a large-scale, diverse POI dataset to address gaps in outdated and non-reproducible data for human mobility research.


<details>
  <summary>Details</summary>
Motivation: Overcoming reliance on old datasets and lack of diverse, reproducible city-level check-in data for POI recommendation research.

Method: Created Massive-STEPS, a benchmark dataset with semantic POI metadata from 12 diverse cities (2017-2018 data). Evaluated POI models using supervised and zero-shot approaches.

Result: Provided a modern, longer-duration dataset enabling reproducible research. Benchmarked models across urban contexts.

Conclusion: Massive-STEPS facilitates equitable, reproducible research in human mobility and POI recommendation.

Abstract: Understanding human mobility through Point-of-Interest (POI) recommendation
is increasingly important for applications such as urban planning, personalized
services, and generative agent simulation. However, progress in this field is
hindered by two key challenges: the over-reliance on older datasets from
2012-2013 and the lack of reproducible, city-level check-in datasets that
reflect diverse global regions. To address these gaps, we present Massive-STEPS
(Massive Semantic Trajectories for Understanding POI Check-ins), a large-scale,
publicly available benchmark dataset built upon the Semantic Trails dataset and
enriched with semantic POI metadata. Massive-STEPS spans 12 geographically and
culturally diverse cities and features more recent (2017-2018) and
longer-duration (24 months) check-in data than prior datasets. We benchmarked a
wide range of POI recommendation models on Massive-STEPS using both supervised
and zero-shot approaches, and evaluated their performance across multiple urban
contexts. By releasing Massive-STEPS, we aim to facilitate reproducible and
equitable research in human mobility and POI recommendation. The dataset and
benchmarking code are available at:
https://github.com/cruiseresearchgroup/Massive-STEPS

</details>


### [403] [Rethinking Irregular Time Series Forecasting: A Simple yet Effective Baseline](https://arxiv.org/pdf/2505.11250)
*Xvyuan Liu, Xiangfei Qiu, Xingjian Wu, Zhengyu Li, Chenjuan Guo, Jilin Hu, Bin Yang*

Main category: cs.LG

TL;DR: APN framework improves forecasting of irregular multivariate time series (IMTS) with adaptive patching and efficient querying, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Challenges in IMTS forecasting include irregularity, missing data, and complex existing methods. APN aims to simplify and improve accuracy.

Method: Proposes APN with Time-Aware Patch Aggregation (TAPA) for adaptive patching and a query module for efficient historical integration, using a shallow MLP for predictions.

Result: APN outperforms state-of-the-art methods in efficiency and accuracy across multiple datasets.

Conclusion: APN provides a practical and accurate solution for IMTS forecasting, addressing key challenges with a simple yet effective framework.

Abstract: The forecasting of irregular multivariate time series (IMTS) is crucial in
key areas such as healthcare, biomechanics, climate science, and astronomy.
However, achieving accurate and practical predictions is challenging due to two
main factors. First, the inherent irregularity and data missingness in
irregular time series make modeling difficult. Second, most existing methods
are typically complex and resource-intensive. In this study, we propose a
general framework called APN to address these challenges. Specifically, we
design a novel Time-Aware Patch Aggregation (TAPA) module that achieves
adaptive patching. By learning dynamically adjustable patch boundaries and a
time-aware weighted averaging strategy, TAPA transforms the original irregular
sequences into high-quality, regularized representations in a
channel-independent manner. Additionally, we use a simple query module to
effectively integrate historical information while maintaining the model's
efficiency. Finally, predictions are made by a shallow MLP. Experimental
results on multiple real-world datasets show that APN outperforms existing
state-of-the-art methods in both efficiency and accuracy.

</details>


### [404] [Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction](https://arxiv.org/pdf/2505.11254)
*Jeffrey Willette, Heejun Lee, Sung Ju Hwang*

Main category: cs.LG

TL;DR: The paper addresses the performance degradation in sparse attention mechanisms due to distributional shift and proposes a method to correct it, improving accuracy while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: The quadratic complexity of transformer attention is costly for long sequences. Sparse attention reduces computation but suffers from performance degradation due to distributional shift.

Method: The authors propose a novel procedure to correct the distributional shift in sparse attention outputs, aligning them closer to quadratic attention.

Result: Their method improves performance by 36%pt, recovering 88% of quadratic attention accuracy on the RULER benchmark, while maintaining high sparsity (98.5%) and speed (32x faster than Flash Attention 2).

Conclusion: The proposed method effectively mitigates the performance drop in sparse attention, offering a practical solution for efficient long-sequence processing.

Abstract: The attention mechanism of a transformer has a quadratic complexity, leading
to high inference costs and latency for long sequences. However, attention
matrices are mostly sparse, which implies that many entries may be omitted from
computation for efficient inference. Sparse attention inference methods aim to
reduce this computational burden; however, they also come with a troublesome
performance degradation. We discover that one reason for this degradation is
that the sparse calculation induces a distributional shift in the attention
outputs. The distributional shift causes decoding-time queries to fail to align
well with the appropriate keys from the prefill stage, leading to a drop in
performance. We propose a simple, novel, and effective procedure for correcting
this distributional shift, bringing the distribution of sparse attention
outputs closer to that of quadratic attention. Our method can be applied on top
of any sparse attention method, and results in an average 36%pt performance
increase, recovering 88% of quadratic attention accuracy on the 131K RULER
benchmark when applied on top of sliding window attention with sink tokens
while only adding a small overhead. Our method can maintain approximately 98.5%
sparsity over full quadratic attention, making our model 32 times faster than
Flash Attention 2 when processing 1M token prefills.

</details>


### [405] [Fourier Low-rank and Sparse Tensor for Efficient Tensor Completion](https://arxiv.org/pdf/2505.11261)
*Jingyang Li, Jiuqian Shang, Yang Chen*

Main category: cs.LG

TL;DR: FLoST is a new tensor completion model using Fourier transform to separate low-frequency (low-rank) and high-frequency (sparse) temporal patterns, outperforming traditional methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional tensor models treat all modes symmetrically, missing unique spatiotemporal patterns in scientific data, especially temporal variations.

Method: FLoST decomposes the tensor temporally via Fourier transform, modeling low-frequency components with low-rank matrices and high-frequency fluctuations with sparsity.

Result: FLoST is more accurate and computationally efficient than existing models, especially for large time dimensions.

Conclusion: FLoST provides a superior, interpretable solution for spatiotemporal data reconstruction by effectively capturing both smooth and localized variations.

Abstract: Tensor completion is crucial in many scientific domains with missing data
problems. Traditional low-rank tensor models, including CP, Tucker, and
Tensor-Train, exploit low-dimensional structures to recover missing data.
However, these methods often treat all tensor modes symmetrically, failing to
capture the unique spatiotemporal patterns inherent in scientific data, where
the temporal component exhibits both low-frequency stability and high-frequency
variations. To address this, we propose a novel model, \underline{F}ourier
\underline{Lo}w-rank and \underline{S}parse \underline{T}ensor (FLoST), which
decomposes the tensor along the temporal dimension using a Fourier transform.
This approach captures low-frequency components with low-rank matrices and
high-frequency fluctuations with sparsity, resulting in a hybrid structure that
efficiently models both smooth and localized variations. Compared to the
well-known tubal-rank model, which assumes low-rankness across all frequency
components, FLoST requires significantly fewer parameters, making it
computationally more efficient, particularly when the time dimension is large.
Through theoretical analysis and empirical experiments, we demonstrate that
FLoST outperforms existing tensor completion models in terms of both accuracy
and computational efficiency, offering a more interpretable solution for
spatiotemporal data reconstruction.

</details>


### [406] [Driving Mechanisms and Forecasting of China's Pet Population-An ARIMA-RF-HW Hybrid Approach](https://arxiv.org/pdf/2505.11269)
*Shengjia Chang, Xianshuo Yue*

Main category: cs.LG

TL;DR: A hybrid ARIMA-RF-HW model improves pet population forecasting in China by combining ARIMA, Random Forest, and Holt-Winters methods, identifying key drivers like urban income and policy quantity.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy of pet population forecasting in China by integrating multiple models and analyzing economic, social, and policy indicators.

Method: Proposes a dynamically weighted hybrid model (ARIMA-RF-HW) with data preprocessing (Z-score normalization, missing value imputation) using 2005-2023 data and nine indicators.

Result: Key drivers include urban income, consumption, and policy quantity, with steady cat growth and fluctuating dog numbers. Aging and urbanization amplify pet demand.

Conclusion: The model aids policymakers in pet health management and guides businesses in service development, promoting sustainable industry growth.

Abstract: This study proposes a dynamically weighted ARIMA-RF-HW hybrid model
integrating ARIMA for seasonality and trends, Random Forest for nonlinear
features, and Holt-Winters smoothing for seasonal adjustment to improve China's
pet population forecasting accuracy. Using 2005-2023 data with nine economic,
social, and policy indicators (urban income, consumption, aging ratio, policy
quantity, new veterinary drug approvals), data were preprocessed via Z-score
normalization and missing value imputation. The results show that key drivers
of pet populations include urban income (19.48% for cats, 17.15% for dogs),
consumption (17.99% for cats), and policy quantity (13.33% for cats, 14.02% for
dogs), with aging (12.81% for cats, 13.27% for dogs) and urbanization
amplifying the demand for pets. Forecasts show steady cat growth and
fluctuating dog numbers, reflecting cats' adaptability to urban environments.
This research supports policymakers in optimizing pet health management and
guides enterprises in developing differentiated services, advancing sustainable
industry growth.

</details>


### [407] [Multiclass threshold-based classification](https://arxiv.org/pdf/2505.11276)
*Francesco Marchetti, Edoardo Legnaro, Sabrina Guastavino*

Main category: cs.LG

TL;DR: A threshold-based framework for multiclass classification replaces softmax's probabilistic interpretation with a geometric one, enabling a posteriori threshold tuning and score-oriented losses, improving performance.


<details>
  <summary>Details</summary>
Motivation: To generalize the argmax rule in multiclass classification by introducing a geometric interpretation of softmax outputs, allowing threshold tuning and refined predictions.

Method: Replaces softmax's probabilistic outputs with a geometric interpretation on the simplex, introduces multidimensional thresholds, and proposes score-oriented losses.

Result: Multidimensional threshold tuning improves performance across networks and datasets; score-oriented losses compete with standard ones.

Conclusion: The framework enhances multiclass classification by enabling threshold optimization and introducing competitive score-oriented losses.

Abstract: In this paper, we introduce a threshold-based framework for multiclass
classification that generalizes the standard argmax rule. This is done by
replacing the probabilistic interpretation of softmax outputs with a geometric
one on the multidimensional simplex, where the classification depends on a
multidimensional threshold. This change of perspective enables for any trained
classification network an a posteriori optimization of the classification score
by means of threshold tuning, as usually carried out in the binary setting.
This allows a further refinement of the prediction capability of any network.
Moreover, this multidimensional threshold-based setting makes it possible to
define score-oriented losses, which are based on the interpretation of the
threshold as a random variable. Our experiments show that the multidimensional
threshold tuning yields consistent performance improvements across various
networks and datasets, and that the proposed multiclass score-oriented losses
are competitive with standard loss functions, resembling the advantages
observed in the binary case.

</details>


### [408] [SubROC: AUC-Based Discovery of Exceptional Subgroup Performance for Binary Classifiers](https://arxiv.org/pdf/2505.11283)
*Tom Siegl, Kutalmƒ±≈ü Co≈ükun, Bjarne Hiller, Amin Mirzaei, Florian Lemmerich, Martin Becker*

Main category: cs.LG

TL;DR: SubROC is a framework for identifying and analyzing subgroups where ML models underperform or excel, offering efficient search and interpretability.


<details>
  <summary>Details</summary>
Motivation: ML models often perform unevenly across populations, necessitating tools to identify and address these disparities for safe deployment.

Method: SubROC uses Exceptional Model Mining, incorporates ROC and PR AUC, prunes search space, controls class imbalance, adjusts for redundancy, and includes significance testing.

Result: SubROC effectively identifies model strengths and weaknesses in interpretable subgroups, demonstrated in case studies and comparative analyses.

Conclusion: SubROC provides a reliable, efficient, and user-friendly solution for subgroup analysis in ML model evaluation.

Abstract: Machine learning (ML) is increasingly employed in real-world applications
like medicine or economics, thus, potentially affecting large populations.
However, ML models often do not perform homogeneously across such populations
resulting in subgroups of the population (e.g., sex=female AND
marital_status=married) where the model underperforms or, conversely, is
particularly accurate. Identifying and describing such subgroups can support
practical decisions on which subpopulation a model is safe to deploy or where
more training data is required. The potential of identifying and analyzing such
subgroups has been recognized, however, an efficient and coherent framework for
effective search is missing. Consequently, we introduce SubROC, an open-source,
easy-to-use framework based on Exceptional Model Mining for reliably and
efficiently finding strengths and weaknesses of classification models in the
form of interpretable population subgroups. SubROC incorporates common
evaluation measures (ROC and PR AUC), efficient search space pruning for fast
exhaustive subgroup search, control for class imbalance, adjustment for
redundant patterns, and significance testing. We illustrate the practical
benefits of SubROC in case studies as well as in comparative analyses across
multiple datasets.

</details>


### [409] [Bidirectional Information Flow (BIF) -- A Sample Efficient Hierarchical Gaussian Process for Bayesian Optimization](https://arxiv.org/pdf/2505.11294)
*Juan D. Guerra, Thomas Garbay, Guillaume Lajoie, Marco Bonizzato*

Main category: cs.LG

TL;DR: Bidirectional Information Flow (BIF) enhances Hierarchical Gaussian Process (H-GP) models by enabling bidirectional information exchange, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Typical H-GP models lack bidirectional information flow, limiting efficiency and convergence.

Method: Proposes BIF, a framework for bidirectional information exchange between parent and child models in H-GPs during online training.

Result: BIF outperforms conventional H-GP methods, achieving up to 85% and 5x higher R¬≤ scores for parent and children models.

Conclusion: BIF improves sample efficiency, robustness, and modular reuse in hierarchical models, demonstrating superior performance in optimization tasks.

Abstract: Hierarchical Gaussian Process (H-GP) models divide problems into different
subtasks, allowing for different models to address each part, making them
well-suited for problems with inherent hierarchical structure. However, typical
H-GP models do not fully take advantage of this structure, only sending
information up or down the hierarchy. This one-way coupling limits sample
efficiency and slows convergence. We propose Bidirectional Information Flow
(BIF), an efficient H-GP framework that establishes bidirectional information
exchange between parent and child models in H-GPs for online training. BIF
retains the modular structure of hierarchical models - the parent combines
subtask knowledge from children GPs - while introducing top-down feedback to
continually refine children models during online learning. This mutual exchange
improves sample efficiency, enables robust training, and allows modular reuse
of learned subtask models. BIF outperforms conventional H-GP Bayesian
Optimization methods, achieving up to 85% and 5x higher $R^2$ scores for the
parent and children respectively, on synthetic and real-world neurostimulation
optimization tasks.

</details>


### [410] [Graph Representational Learning: When Does More Expressivity Hurt Generalization?](https://arxiv.org/pdf/2505.11298)
*Sohir Maskey, Raffaele Paolino, Fabian Jogl, Gitta Kutyniok, Johannes F. Lutzeyer*

Main category: cs.LG

TL;DR: The paper explores the relationship between GNN expressivity and performance, introducing premetrics for structural similarity and deriving generalization bounds. Findings suggest expressive GNNs may generalize poorly unless balanced by training size or reduced graph distance.


<details>
  <summary>Details</summary>
Motivation: To clarify the unclear relationship between GNN expressivity and predictive performance, and to understand how structural similarity affects generalization.

Method: Introduces premetrics for structural similarity, derives generalization bounds based on graph distance, model complexity, and training size, and validates with empirical results.

Result: Expressive GNNs may generalize worse unless their complexity is balanced by larger training sets or closer training-test graph distances.

Conclusion: The study links GNN expressivity and generalization, providing theoretical insights and empirical support for balancing complexity with training conditions.

Abstract: Graph Neural Networks (GNNs) are powerful tools for learning on structured
data, yet the relationship between their expressivity and predictive
performance remains unclear. We introduce a family of premetrics that capture
different degrees of structural similarity between graphs and relate these
similarities to generalization, and consequently, the performance of expressive
GNNs. By considering a setting where graph labels are correlated with
structural features, we derive generalization bounds that depend on the
distance between training and test graphs, model complexity, and training set
size. These bounds reveal that more expressive GNNs may generalize worse unless
their increased complexity is balanced by a sufficiently large training set or
reduced distance between training and test graphs. Our findings relate
expressivity and generalization, offering theoretical insights supported by
empirical results.

</details>


### [411] [Effective Probabilistic Time Series Forecasting with Fourier Adaptive Noise-Separated Diffusion](https://arxiv.org/pdf/2505.11306)
*Xinyan Wang, Rui Dai, Kaikui Liu, Xiangxiang Chu*

Main category: cs.LG

TL;DR: FALDA is a probabilistic framework for time series forecasting using Fourier-based decomposition and a lightweight denoiser, outperforming existing methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve probabilistic time series forecasting by reducing epistemic uncertainty and focusing on aleatoric uncertainty.

Method: Uses Fourier-based decomposition and a conditional diffusion model with a lightweight denoiser (DEMA) for tailored modeling and noise estimation.

Result: Outperforms existing probabilistic and point forecasting methods, with up to 9% improvement in accuracy.

Conclusion: FALDA is effective for long-term forecasting, balancing accuracy and computational efficiency.

Abstract: We propose the Fourier Adaptive Lite Diffusion Architecture (FALDA), a novel
probabilistic framework for time series forecasting. First, we introduce the
Diffusion Model for Residual Regression (DMRR) framework, which unifies
diffusion-based probabilistic regression methods. Within this framework, FALDA
leverages Fourier-based decomposition to incorporate a component-specific
architecture, enabling tailored modeling of individual temporal components. A
conditional diffusion model is utilized to estimate the future noise term,
while our proposed lightweight denoiser, DEMA (Decomposition MLP with AdaLN),
conditions on the historical noise term to enhance denoising performance.
Through mathematical analysis and empirical validation, we demonstrate that
FALDA effectively reduces epistemic uncertainty, allowing probabilistic
learning to primarily focus on aleatoric uncertainty. Experiments on six
real-world benchmarks demonstrate that FALDA consistently outperforms existing
probabilistic forecasting approaches across most datasets for long-term time
series forecasting while achieving enhanced computational efficiency without
compromising accuracy. Notably, FALDA also achieves superior overall
performance compared to state-of-the-art (SOTA) point forecasting approaches,
with improvements of up to 9%.

</details>


### [412] [Diffusion Learning with Partial Agent Participation and Local Updates](https://arxiv.org/pdf/2505.11307)
*Elsa Rizk, Kun Yuan, Ali H. Sayed*

Main category: cs.LG

TL;DR: The paper proposes an enhanced diffusion learning approach with local updates and partial agent participation to reduce communication overhead and improve reliability in edge devices.


<details>
  <summary>Details</summary>
Motivation: Traditional diffusion learning suffers from high communication overhead and unreliability due to edge device volatility.

Method: The enhanced approach incorporates local updates to reduce communication frequency and partial agent participation based on availability.

Result: The algorithm is proven stable in mean-square error and analyzed for Mean-Square-Deviation (MSD) performance. Numerical experiments validate the findings.

Conclusion: The proposed method effectively addresses communication and reliability challenges in diffusion learning for edge devices.

Abstract: Diffusion learning is a framework that endows edge devices with advanced
intelligence. By processing and analyzing data locally and allowing each agent
to communicate with its immediate neighbors, diffusion effectively protects the
privacy of edge devices, enables real-time response, and reduces reliance on
central servers. However, traditional diffusion learning relies on
communication at every iteration, leading to communication overhead, especially
with large learning models. Furthermore, the inherent volatility of edge
devices, stemming from power outages or signal loss, poses challenges to
reliable communication between neighboring agents. To mitigate these issues,
this paper investigates an enhanced diffusion learning approach incorporating
local updates and partial agent participation. Local updates will curtail
communication frequency, while partial agent participation will allow for the
inclusion of agents based on their availability. We prove that the resulting
algorithm is stable in the mean-square error sense and provide a tight analysis
of its Mean-Square-Deviation (MSD) performance. Various numerical experiments
are conducted to illustrate our theoretical findings.

</details>


### [413] [Reinforcement Learning Closures for Underresolved Partial Differential Equations using Synthetic Data](https://arxiv.org/pdf/2505.11308)
*Lothar Heimbach, Sebastian Kaltenbach, Petr Karnakov, Francis J. Alexander, Petros Koumoutsakos*

Main category: cs.LG

TL;DR: A framework using synthetic data and reinforcement learning to develop closure models for coarse-grained PDEs, tested on Burgers' and advection equations.


<details>
  <summary>Details</summary>
Motivation: Solving PDEs for real-world applications is computationally expensive, and coarse-grained approximations lose detail. Closure models are needed to represent unresolved interactions.

Method: Uses synthetic data from manufactured solutions and reinforcement learning to create closure models for coarse-grained PDEs.

Result: Effective closure models for 1D/2D Burgers' and 2D advection equations, with generalization to homogeneous PDEs.

Conclusion: The framework shows promise for accurate, efficient closure models, even with scarce data.

Abstract: Partial Differential Equations (PDEs) describe phenomena ranging from
turbulence and epidemics to quantum mechanics and financial markets. Despite
recent advances in computational science, solving such PDEs for real-world
applications remains prohibitively expensive because of the necessity of
resolving a broad range of spatiotemporal scales. In turn, practitioners often
rely on coarse-grained approximations of the original PDEs, trading off
accuracy for reduced computational resources. To mitigate the loss of detail
inherent in such approximations, closure models are employed to represent
unresolved spatiotemporal interactions. We present a framework for developing
closure models for PDEs using synthetic data acquired through the method of
manufactured solutions. These data are used in conjunction with reinforcement
learning to provide closures for coarse-grained PDEs. We illustrate the
efficacy of our method using the one-dimensional and two-dimensional Burgers'
equations and the two-dimensional advection equation. Moreover, we demonstrate
that closure models trained for inhomogeneous PDEs can be effectively
generalized to homogeneous PDEs. The results demonstrate the potential for
developing accurate and computationally efficient closure models for systems
with scarce data.

</details>


### [414] [Where You Place the Norm Matters: From Prejudiced to Neutral Initializations](https://arxiv.org/pdf/2505.11312)
*Emanuele Francazi, Francesco Pinto, Aurelien Lucchi, Marco Baity-Jesi*

Main category: cs.LG

TL;DR: The paper explores how normalization layers (e.g., Batch/Layer Normalization) affect neural network behavior at initialization, influencing early training dynamics and class prediction distributions.


<details>
  <summary>Details</summary>
Motivation: To theoretically understand how normalization layers impact model behavior from initialization, addressing gaps in current knowledge.

Method: Investigates the presence and placement of normalization within hidden layers, analyzing their effect on initial prediction distributions (Neutral to Prejudiced).

Result: Normalization placement systematically alters initial prediction behavior, shaping learning dynamics and offering insights for network design.

Conclusion: The study provides a principled framework for understanding normalization's role in early training, aiding more controlled and interpretable network architectures.

Abstract: Normalization layers, such as Batch Normalization and Layer Normalization,
are central components in modern neural networks, widely adopted to improve
training stability and generalization. While their practical effectiveness is
well documented, a detailed theoretical understanding of how normalization
affects model behavior, starting from initialization, remains an important open
question. In this work, we investigate how both the presence and placement of
normalization within hidden layers influence the statistical properties of
network predictions before training begins. In particular, we study how these
choices shape the distribution of class predictions at initialization, which
can range from unbiased (Neutral) to highly concentrated (Prejudiced) toward a
subset of classes. Our analysis shows that normalization placement induces
systematic differences in the initial prediction behavior of neural networks,
which in turn shape the dynamics of learning. By linking architectural choices
to prediction statistics at initialization, our work provides a principled
understanding of how normalization can influence early training behavior and
offers guidance for more controlled and interpretable network design.

</details>


### [415] [Anomaly Detection for Non-stationary Time Series using Recurrent Wavelet Probabilistic Neural Network](https://arxiv.org/pdf/2505.11321)
*Pu Yang, J. A. Barria*

Main category: cs.LG

TL;DR: Proposes an unsupervised RWPNN for anomaly detection in non-stationary environments, combining temporal feature modeling with nonparametric density estimation.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of detecting anomalies in non-stationary environments where traditional methods may fail due to data variability.

Method: Uses a Stacked Recurrent Encoder-Decoder (SREnc-Dec) for temporal features and a Multi-Receptive-field Wavelet Probabilistic Network (MRWPN) for probabilistic modeling.

Result: Demonstrates robust and accurate anomaly detection across 45 real-world datasets, with early warning capabilities.

Conclusion: RWPNN effectively handles high-dimensional data and non-stationary conditions, outperforming traditional methods.

Abstract: In this paper, an unsupervised Recurrent Wavelet Probabilistic Neural Network
(RWPNN) is proposed, which aims at detecting anomalies in non-stationary
environments by modelling the temporal features using a nonparametric density
estimation network. The novel framework consists of two components, a Stacked
Recurrent Encoder-Decoder (SREnc-Dec) module that captures temporal features in
a latent space, and a Multi-Receptive-field Wavelet Probabilistic Network
(MRWPN) that creates an ensemble probabilistic model to characterise the latent
space. This formulation extends the standard wavelet probabilistic networks to
wavelet deep probabilistic networks, which can handle higher data
dimensionality. The MRWPN module can adapt to different rates of data variation
in different datasets without imposing strong distribution assumptions,
resulting in a more robust and accurate detection for Time Series Anomaly
Detection (TSAD) tasks in the non-stationary environment. We carry out the
assessment on 45 real-world time series datasets from various domains, verify
the performance of RWPNN in TSAD tasks with several constraints, and show its
ability to provide early warnings for anomalous events.

</details>


### [416] [The Final Layer Holds the Key: A Unified and Efficient GNN Calibration Framework](https://arxiv.org/pdf/2505.11335)
*Jincheng Huang, Jie Xu, Xiaoshuang Shi, Ping Hu, Lei Feng, Xiaofeng Zhu*

Main category: cs.LG

TL;DR: A new graph calibration method for GNNs addresses under-confidence by analyzing class-centroid and node-level calibration at the final layer, reducing weight decay for better reliability.


<details>
  <summary>Details</summary>
Motivation: GNNs often exhibit miscalibrated confidence, harming decision reliability, and existing methods lack theoretical guarantees and add computational overhead.

Method: Proposes a unified framework linking model confidence to class-centroid and node-level calibration, reducing final-layer weight decay for under-confidence.

Result: Extensive experiments show the method's superiority in improving GNN confidence calibration.

Conclusion: The method efficiently addresses GNN under-confidence with theoretical backing and practical validation.

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable effectiveness on
graph-based tasks. However, their predictive confidence is often miscalibrated,
typically exhibiting under-confidence, which harms the reliability of their
decisions. Existing calibration methods for GNNs normally introduce additional
calibration components, which fail to capture the intrinsic relationship
between the model and the prediction confidence, resulting in limited
theoretical guarantees and increased computational overhead. To address this
issue, we propose a simple yet efficient graph calibration method. We establish
a unified theoretical framework revealing that model confidence is jointly
governed by class-centroid-level and node-level calibration at the final layer.
Based on this insight, we theoretically show that reducing the weight decay of
the final-layer parameters alleviates GNN under-confidence by acting on the
class-centroid level, while node-level calibration acts as a finer-grained
complement to class-centroid level calibration, which encourages each test node
to be closer to its predicted class centroid at the final-layer
representations. Extensive experiments validate the superiority of our method.

</details>


### [417] [Sobolev Training of End-to-End Optimization Proxies](https://arxiv.org/pdf/2505.11342)
*Andrew W. Rosemberg, Joaquim Dias Garcia, Russell Bent, Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: Optimization proxies, trained via Sobolev training, improve accuracy and reduce errors by integrating solver sensitivities, showing significant gains in supervised and self-supervised settings.


<details>
  <summary>Details</summary>
Motivation: To enhance optimization proxies by aligning their predictions and derivatives with those of traditional solvers, ensuring faster and more reliable solutions for large-scale problems.

Method: Uses Sobolev training to integrate solver sensitivities into proxies, tested in supervised (exact outputs available) and self-supervised (only objective/constraint structure) settings.

Result: Supervised training reduces errors by 56% and constraint violations by 400%; self-supervised training halves optimality gaps in medium-risk regions.

Conclusion: Sobolev training, whether supervised or self-supervised, provides fast and reliable surrogates for large-scale optimization tasks.

Abstract: Optimization proxies - machine learning models trained to approximate the
solution mapping of parametric optimization problems in a single forward pass -
offer dramatic reductions in inference time compared to traditional iterative
solvers. This work investigates the integration of solver sensitivities into
such end to end proxies via a Sobolev training paradigm and does so in two
distinct settings: (i) fully supervised proxies, where exact solver outputs and
sensitivities are available, and (ii) self supervised proxies that rely only on
the objective and constraint structure of the underlying optimization problem.
By augmenting the standard training loss with directional derivative
information extracted from the solver, the proxy aligns both its predicted
solutions and local derivatives with those of the optimizer. Under Lipschitz
continuity assumptions on the true solution mapping, matching first order
sensitivities is shown to yield uniform approximation error proportional to the
training set covering radius. Empirically, different impacts are observed in
each studied setting. On three large Alternating Current Optimal Power Flow
benchmarks, supervised Sobolev training cuts mean squared error by up to 56
percent and the median worst case constraint violation by up to 400 percent
while keeping the optimality gap below 0.22 percent. For a mean variance
portfolio task trained without labeled solutions, self supervised Sobolev
training halves the average optimality gap in the medium risk region (standard
deviation above 10 percent of budget) and matches the baseline elsewhere.
Together, these results highlight Sobolev training whether supervised or self
supervised as a path to fast reliable surrogates for safety critical large
scale optimization workloads.

</details>


### [418] [What Can We Learn From MIMO Graph Convolutions?](https://arxiv.org/pdf/2505.11346)
*Andreas Roth, Thomas Liebig*

Main category: cs.LG

TL;DR: The paper introduces localized MIMO graph convolutions (LMGCs) as a generalization of linear message-passing neural networks, proving their injective properties and demonstrating their effectiveness in combining benefits of various methods.


<details>
  <summary>Details</summary>
Motivation: Existing GNNs approximate graph convolutions in the SISO case, but practical applications often involve MIMO scenarios. The paper aims to derive and approximate the MIMO graph convolution directly.

Method: The authors derive the MIMO graph convolution via the convolution theorem and approximate it directly in the MIMO case. They introduce LMGCs as a localized approximation, proving their injective properties and linear independence.

Result: LMGCs generalize many linear message-passing neural networks and are shown to be injective on multisets with a single computational graph, while using multiple graphs ensures linearly independent representations.

Conclusion: LMGCs effectively combine the benefits of various methods, as confirmed by experimental results, offering a robust framework for MIMO graph convolutions.

Abstract: Most graph neural networks (GNNs) utilize approximations of the general graph
convolution derived in the graph Fourier domain. While GNNs are typically
applied in the multi-input multi-output (MIMO) case, the approximations are
performed in the single-input single-output (SISO) case. In this work, we first
derive the MIMO graph convolution through the convolution theorem and
approximate it directly in the MIMO case. We find the key MIMO-specific
property of the graph convolution to be operating on multiple computational
graphs, or equivalently, applying distinct feature transformations for each
pair of nodes. As a localized approximation, we introduce localized MIMO graph
convolutions (LMGCs), which generalize many linear message-passing neural
networks. For almost every choice of edge weights, we prove that LMGCs with a
single computational graph are injective on multisets, and the resulting
representations are linearly independent when more than one computational graph
is used. Our experimental results confirm that an LMGC can combine the benefits
of various methods.

</details>


### [419] [Training NTK to Generalize with KARE](https://arxiv.org/pdf/2505.11347)
*Johannes Schwab, Bryan Kelly, Semyon Malamud, Teng Andrea Xu*

Main category: cs.LG

TL;DR: Explicitly training the Neural Tangent Kernel (NTK) using Kernel Alignment Risk Estimator (KARE) outperforms traditional DNN training and DNN-induced NTK, suggesting a shift in dominance from DNNs to kernel methods in certain settings.


<details>
  <summary>Details</summary>
Motivation: The performance of NTK often matches or exceeds DNNs, implying DNN training implicitly learns kernels. This paper explores explicit NTK optimization to challenge DNN dominance.

Method: Train NTK explicitly using KARE to minimize generalization error, instead of minimizing empirical risk like traditional DNN training.

Result: NTKs trained with KARE consistently match or outperform original DNNs and DNN-induced NTKs in simulations and real data.

Conclusion: Explicit NTK training can outperform DNNs, indicating over-parametrized feature learning and challenging DNN dominance in certain settings.

Abstract: The performance of the data-dependent neural tangent kernel (NTK; Jacot et
al. (2018)) associated with a trained deep neural network (DNN) often matches
or exceeds that of the full network. This implies that DNN training via
gradient descent implicitly performs kernel learning by optimizing the NTK. In
this paper, we propose instead to optimize the NTK explicitly. Rather than
minimizing empirical risk, we train the NTK to minimize its generalization
error using the recently developed Kernel Alignment Risk Estimator (KARE; Jacot
et al. (2020)). Our simulations and real data experiments show that NTKs
trained with KARE consistently match or significantly outperform the original
DNN and the DNN- induced NTK (the after-kernel). These results suggest that
explicitly trained kernels can outperform traditional end-to-end DNN
optimization in certain settings, challenging the conventional dominance of
DNNs. We argue that explicit training of NTK is a form of over-parametrized
feature learning.

</details>


### [420] [A Stability Principle for Learning under Non-Stationarity](https://arxiv.org/pdf/2310.18304)
*Chengpiao Huang, Kaizheng Wang*

Main category: cs.LG

TL;DR: A framework for statistical learning in non-stationary environments adapts to unknown changes by optimizing historical data usage while controlling bias.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning in non-stationary environments where data distributions change over time.

Method: Uses a stability principle to select optimal look-back windows, balancing historical data utilization and bias control. Introduces function similarity measures and segmentation for quasi-stationary data.

Result: Achieves minimax optimal regret bounds for strongly convex or Lipschitz losses. Validated on electricity demand and nurse staffing datasets.

Conclusion: The framework effectively adapts to non-stationarity, offering theoretical guarantees and practical utility.

Abstract: We develop a versatile framework for statistical learning in non-stationary
environments. In each time period, our approach applies a stability principle
to select a look-back window that maximizes the utilization of historical data
while keeping the cumulative bias within an acceptable range relative to the
stochastic error. Our theory showcases the adaptivity of this approach to
unknown non-stationarity. We prove regret bounds that are minimax optimal up to
logarithmic factors when the population losses are strongly convex, or
Lipschitz only. At the heart of our analysis lie two novel components: a
measure of similarity between functions and a segmentation technique for
dividing the non-stationary data sequence into quasi-stationary pieces. We
evaluate the practical performance of our approach through real-data
experiments on electricity demand prediction and hospital nurse staffing.

</details>


### [421] [Context parroting: A simple but tough-to-beat baseline for foundation models in scientific machine learning](https://arxiv.org/pdf/2505.11349)
*Yuanzhao Zhang, William Gilpin*

Main category: cs.LG

TL;DR: Time series foundation models predict physical systems but rely on context parroting, a simple copying strategy, outperforming complex models at lower cost.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of time series foundation models in predicting physical systems and uncover their underlying learning strategies.

Method: Analyze foundation models' forecasting abilities, comparing them to a naive context parroting model, and link findings to dynamical systems and scaling laws.

Result: Foundation models often use context parroting, which outperforms state-of-the-art models in zero-shot forecasting at minimal computational cost.

Conclusion: Context parroting is a strong baseline for time-series foundation models, offering insights into in-context learning and scaling laws.

Abstract: Recently-developed time series foundation models for scientific machine
learning exhibit emergent abilities to predict physical systems. These
abilities include zero-shot forecasting, in which a model forecasts future
states of a system given only a short trajectory as context. Here, we show that
foundation models applied to physical systems can give accurate predictions,
but that they fail to develop meaningful representations of the underlying
physics. Instead, foundation models often forecast by context parroting, a
simple zero-shot forecasting strategy that copies directly from the context. As
a result, a naive direct context parroting model scores higher than
state-of-the-art time-series foundation models on predicting a diverse range of
dynamical systems, at a tiny fraction of the computational cost. We draw a
parallel between context parroting and induction heads, which explains why
large language models trained on text can be repurposed for time series
forecasting. Our dynamical systems perspective also ties the scaling between
forecast accuracy and context length to the fractal dimension of the attractor,
providing insight into the previously observed in-context neural scaling laws.
Context parroting thus serves as a simple but tough-to-beat baseline for future
time-series foundation models and can help identify in-context learning
strategies beyond parroting.

</details>


### [422] [Fractal Graph Contrastive Learning](https://arxiv.org/pdf/2505.11356)
*Nero Z. Li, Xuehao Zhai, Zhichao Shi, Boshen Shi, Xuhui Jiang*

Main category: cs.LG

TL;DR: FractalGCL introduces a theory-driven GCL framework using fractal self-similarity for global structural consistency, improving performance and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing GCL methods lack explicit control over global structural consistency in data augmentations, limiting performance.

Method: Proposes FractalGCL with renormalisation-based augmentation and fractal-dimension-aware contrastive loss, plus a one-shot estimator to reduce computational overhead.

Result: Achieves state-of-the-art results, outperforming baselines by ~7% on traffic networks, with a 61% reduction in training time.

Conclusion: FractalGCL effectively balances performance and computational efficiency, advancing GCL with fractal theory.

Abstract: While Graph Contrastive Learning (GCL) has attracted considerable attention
in the field of graph self-supervised learning, its performance heavily relies
on data augmentations that are expected to generate semantically consistent
positive pairs. Existing strategies typically resort to random perturbations or
local structure preservation, yet lack explicit control over global structural
consistency between augmented views. To address this limitation, we propose
Fractal Graph Contrastive Learning (FractalGCL), a theory-driven framework that
leverages fractal self-similarity to enforce global topological coherence.
FractalGCL introduces two key innovations: a renormalisation-based augmentation
that generates structurally aligned positive views via box coverings; and a
fractal-dimension-aware contrastive loss that aligns graph embeddings according
to their fractal dimensions. While combining the two innovations markedly
boosts graph-representation quality, it also adds non-trivial computational
overhead. To mitigate the computational overhead of fractal dimension
estimation, we derive a one-shot estimator by proving that the dimension
discrepancy between original and renormalised graphs converges weakly to a
centred Gaussian distribution. This theoretical insight enables a reduction in
dimension computation cost by an order of magnitude, cutting overall training
time by approximately 61%. The experiments show that FractalGCL not only
delivers state-of-the-art results on standard benchmarks but also outperforms
traditional baselines on traffic networks by an average margin of about
remarkably 7%. Codes are available at
(https://anonymous.4open.science/r/FractalGCL-0511).

</details>


### [423] [LGBQPC: Local Granular-Ball Quality Peaks Clustering](https://arxiv.org/pdf/2505.11359)
*Zihang Jia, Zhen Zhang, Witold Pedrycz*

Main category: cs.LG

TL;DR: The paper introduces the LGBQPC algorithm, an improved version of GBDPC, addressing its limitations in handling complex data structures by refining GB generation and clustering processes using POJG principles.


<details>
  <summary>Details</summary>
Motivation: GBDPC, while efficient, struggles with complex manifold structures and non-uniform density distributions. The paper aims to enhance its performance for such tasks.

Method: Proposes LGBQPC with GB-POJG+ for improved GB generation and innovations like relative GB quality and geodesic distance in clustering.

Result: LGBQPC outperforms GBDPC on 40 benchmark datasets, especially for complex data structures.

Conclusion: LGBQPC effectively addresses GBDPC's limitations, offering better performance for complex clustering tasks.

Abstract: The density peaks clustering (DPC) algorithm has attracted considerable
attention for its ability to detect arbitrarily shaped clusters based on a
simple yet effective assumption. Recent advancements integrating granular-ball
(GB) computing with DPC have led to the GB-based DPC (GBDPC) algorithm, which
improves computational efficiency. However, GBDPC demonstrates limitations when
handling complex clustering tasks, particularly those involving data with
complex manifold structures or non-uniform density distributions. To overcome
these challenges, this paper proposes the local GB quality peaks clustering
(LGBQPC) algorithm, which offers comprehensive improvements to GBDPC in both GB
generation and clustering processes based on the principle of justifiable
granularity (POJG). Firstly, an improved GB generation method, termed GB-POJG+,
is developed, which systematically refines the original GB-POJG in four key
aspects: the objective function, termination criterion for GB division,
definition of abnormal GB, and granularity level adaptation strategy. GB-POJG+
simplifies parameter configuration by requiring only a single penalty
coefficient and ensures high-quality GB generation while maintaining the number
of generated GBs within an acceptable range. In the clustering phase, two key
innovations are introduced based on the GB k-nearest neighbor graph: relative
GB quality for density estimation and geodesic distance for GB distance metric.
These modifications substantially improve the performance of GBDPC on datasets
with complex manifold structures or non-uniform density distributions.
Extensive numerical experiments on 40 benchmark datasets, including both
synthetic and publicly available datasets, validate the superior performance of
the proposed LGBQPC algorithm.

</details>


### [424] [Efficient End-to-End Learning for Decision-Making: A Meta-Optimization Approach](https://arxiv.org/pdf/2505.11360)
*Rares Cristian, Pavithra Harsha, Georgia Perakis, Brian Quanz*

Main category: cs.LG

TL;DR: A meta-optimization method is introduced to reduce computational overhead in end-to-end learning by approximating optimization problems efficiently, ensuring feasibility and scalability.


<details>
  <summary>Details</summary>
Motivation: End-to-end learning outperforms traditional methods but faces computational challenges due to expensive optimization problems during training.

Method: A neural network architecture is proposed to approximate optimization problems near-optimally, using alternate projections for feasibility, with proven convergence and generalization guarantees.

Result: The method achieves superior computational efficiency, faster high-quality approximations, and better scalability compared to existing techniques.

Conclusion: The approach is versatile, applicable to various optimization problems, and demonstrated in real-world scenarios like electricity generation, shortest path tasks, and newsvendor problems.

Abstract: End-to-end learning has become a widely applicable and studied problem in
training predictive ML models to be aware of their impact on downstream
decision-making tasks. These end-to-end models often outperform traditional
methods that separate training from the optimization and only myopically focus
on prediction error. However, the computational complexity of end-to-end
frameworks poses a significant challenge, particularly for large-scale
problems. While training an ML model using gradient descent, each time we need
to compute a gradient we must solve an expensive optimization problem. We
present a meta-optimization method that learns efficient algorithms to
approximate optimization problems, dramatically reducing computational overhead
of solving the decision problem in general, an aspect we leverage in the
training within the end-to-end framework. Our approach introduces a neural
network architecture that near-optimally solves optimization problems while
ensuring feasibility constraints through alternate projections. We prove
exponential convergence, approximation guarantees, and generalization bounds
for our learning method. This method offers superior computational efficiency,
producing high-quality approximations faster and scaling better with problem
size compared to existing techniques. Our approach applies to a wide range of
optimization problems including deterministic, single-stage as well as
two-stage stochastic optimization problems. We illustrate how our proposed
method applies to (1) an electricity generation problem using real data from an
electricity routing company coordinating the movement of electricity throughout
13 states, (2) a shortest path problem with a computer vision task of
predicting edge costs from terrain maps, (3) a two-stage multi-warehouse
cross-fulfillment newsvendor problem, as well as a variety of other
newsvendor-like problems.

</details>


### [425] [On the Universality of Self-Supervised Learning](https://arxiv.org/pdf/2405.01053)
*Wenwen Qiang, Jingyao Wang, Changwen Zheng, Hui Xiong, Gang Hua*

Main category: cs.LG

TL;DR: The paper proposes GeSSL, a framework for self-supervised learning that explicitly models universality (discriminability, generalizability, transferability) through a bi-level optimization structure and theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Existing SSL methods lack explicit modeling of universality and its theoretical foundations.

Method: GeSSL integrates a bi-level optimization structure to jointly model task-specific adaptation and cross-task consistency, with a theoretical generalization bound.

Result: Empirical results show GeSSL outperforms existing methods on diverse downstream tasks.

Conclusion: GeSSL effectively models universal representations, validated by superior performance and theoretical guarantees.

Abstract: In this paper, we investigate what constitutes a good representation or model
in self-supervised learning (SSL). We argue that a good representation should
exhibit universality, characterized by three essential properties:
discriminability, generalizability, and transferability. While these
capabilities are implicitly desired in most SSL frameworks, existing methods
lack an explicit modeling of universality, and its theoretical foundations
remain underexplored. To address these gaps, we propose General SSL (GeSSL), a
novel framework that explicitly models universality from three complementary
dimensions: the optimization objective, the parameter update mechanism, and the
learning paradigm. GeSSL integrates a bi-level optimization structure that
jointly models task-specific adaptation and cross-task consistency, thereby
capturing all three aspects of universality within a unified SSL objective.
Furthermore, we derive a theoretical generalization bound, ensuring that the
optimization process of GeSSL consistently leads to representations that
generalize well to unseen tasks. Empirical results on multiple benchmark
datasets demonstrate that GeSSL consistently achieves superior performance
across diverse downstream tasks, validating its effectiveness in modeling
universal representations.

</details>


### [426] [Understanding Nonlinear Implicit Bias via Region Counts in Input Space](https://arxiv.org/pdf/2505.11370)
*Jingwei Li, Jing Xu, Zifan Wang, Huishuai Zhang, Jingzhao Zhang*

Main category: cs.LG

TL;DR: The paper explores implicit bias in neural networks by proposing region count as a metric for characterizing it, linking it to generalization performance and hyperparameter choices.


<details>
  <summary>Details</summary>
Motivation: To better understand implicit bias in nonlinear, overparameterized neural networks and its role in generalization.

Method: Proposes region count (connected regions with the same predicted label) as a metric, analyzes its empirical and theoretical connections to generalization and hyperparameters.

Result: Small region counts correlate with simple decision boundaries and good generalization; larger learning rates and smaller batch sizes reduce region counts.

Conclusion: Region count is a useful invariant metric for implicit bias, with practical implications for hyperparameter tuning in neural networks.

Abstract: One explanation for the strong generalization ability of neural networks is
implicit bias. Yet, the definition and mechanism of implicit bias in non-linear
contexts remains little understood. In this work, we propose to characterize
implicit bias by the count of connected regions in the input space with the
same predicted label. Compared with parameter-dependent metrics (e.g., norm or
normalized margin), region count can be better adapted to nonlinear,
overparameterized models, because it is determined by the function mapping and
is invariant to reparametrization. Empirically, we found that small region
counts align with geometrically simple decision boundaries and correlate well
with good generalization performance. We also observe that good hyper-parameter
choices such as larger learning rates and smaller batch sizes can induce small
region counts. We further establish the theoretical connections and explain how
larger learning rate can induce small region counts in neural networks.

</details>


### [427] [Intervention-Aware Forecasting: Breaking Historical Limits from a System Perspective](https://arxiv.org/pdf/2405.13522)
*Zhijian Xu, Hao Wang, Qiang Xu*

Main category: cs.LG

TL;DR: The paper proposes an Intervention-Aware Time Series Forecasting (IATSF) framework to address the limitation of traditional methods that ignore external interventions, particularly textual ones, and introduces FIATS, a model that outperforms state-of-the-art methods by explicitly incorporating such interventions.


<details>
  <summary>Details</summary>
Motivation: Traditional time series forecasting methods fail to account for external interventions, which significantly impact future dynamics. This limitation is addressed by incorporating textual interventions, which capture qualitative or uncertain influences better than conventional exogenous variables.

Method: The paper introduces the IATSF framework and FIATS, a lightweight model using Channel-Aware Adaptive Sensitivity Modeling (CASM) and Channel-Aware Parameter Sharing (CAPS) to integrate textual interventions and historical data in a channel-specific manner.

Result: FIATS outperforms state-of-the-art methods, demonstrating that forecasting improvements come from modeling external interventions, not just increased model complexity.

Conclusion: The IATSF framework and FIATS model effectively incorporate external interventions, particularly textual ones, to enhance time series forecasting accuracy beyond traditional methods.

Abstract: Traditional time series forecasting methods predominantly rely on historical
data patterns, neglecting external interventions that significantly shape
future dynamics. Through control-theoretic analysis, we show that the implicit
"self-stimulation" assumption limits the accuracy of these forecasts. To
overcome this limitation, we propose an Intervention-Aware Time Series
Forecasting (IATSF) framework explicitly designed to incorporate external
interventions. We particularly emphasize textual interventions due to their
unique capability to represent qualitative or uncertain influences inadequately
captured by conventional exogenous variables. We propose a leak-free benchmark
composed of temporally synchronized textual intervention data across synthetic
and real-world scenarios. To rigorously evaluate IATSF, we develop FIATS, a
lightweight forecasting model that integrates textual interventions through
Channel-Aware Adaptive Sensitivity Modeling (CASM) and Channel-Aware Parameter
Sharing (CAPS) mechanisms, enabling the model to adjust its sensitivity to
interventions and historical data in a channel-specific manner. Extensive
empirical evaluations confirm that FIATS surpasses state-of-the-art methods,
highlighting that forecasting improvements stem explicitly from modeling
external interventions rather than increased model complexity alone.

</details>


### [428] [On the Interconnections of Calibration, Quantification, and Classifier Accuracy Prediction under Dataset Shift](https://arxiv.org/pdf/2505.11380)
*Alejandro Moreo*

Main category: cs.LG

TL;DR: The paper explores the equivalence of calibration, quantification, and classifier accuracy prediction under dataset shift, proposing cross-disciplinary methods that often outperform dedicated approaches.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in classifier calibration, quantification, and accuracy prediction under dataset shift, the paper aims to unify these areas.

Method: Proves equivalence of the three problems via mutual reduction and adapts methods from one discipline to solve others.

Result: Proposed methods are competitive or superior to dedicated approaches, demonstrating cross-disciplinary potential.

Conclusion: Encourages unified approaches and synergies across calibration, quantification, and accuracy prediction research.

Abstract: When the distribution of the data used to train a classifier differs from
that of the test data, i.e., under dataset shift, well-established routines for
calibrating the decision scores of the classifier, estimating the proportion of
positives in a test sample, or estimating the accuracy of the classifier,
become particularly challenging. This paper investigates the interconnections
among three fundamental problems, calibration, quantification, and classifier
accuracy prediction, under dataset shift conditions. Specifically, we prove
their equivalence through mutual reduction, i.e., we show that access to an
oracle for any one of these tasks enables the resolution of the other two.
Based on these proofs, we propose new methods for each problem based on direct
adaptations of well-established methods borrowed from the other disciplines.
Our results show such methods are often competitive, and sometimes even surpass
the performance of dedicated approaches from each discipline. The main goal of
this paper is to fostering cross-fertilization among these research areas,
encouraging the development of unified approaches and promoting synergies
across the fields.

</details>


### [429] [Estimating Long-term Heterogeneous Dose-response Curve: Generalization Bound Leveraging Optimal Transport Weights](https://arxiv.org/pdf/2406.19195)
*Zeqin Yang, Weilin Chen, Ruichu Cai, Yuguang Yan, Zhifeng Hao, Zhipeng Yu, Zhichao Zou, Jixing Xu, Zhen Peng, Jiecheng Guo*

Main category: cs.LG

TL;DR: The paper proposes a method to estimate long-term Heterogeneous Dose-Response Curves (HDRC) while addressing unobserved confounders and continuous treatment, using optimal transport weighting and theoretical generalization bounds.


<details>
  <summary>Details</summary>
Motivation: Existing methods for long-term treatment effect estimation rely on unrealistic assumptions (e.g., no unobserved confounders or binary treatment), limiting their applicability in real-world scenarios.

Method: The authors introduce an optimal transport weighting framework to align long-term observational data with short-term experimental data and establish a generalization bound for counterfactual prediction.

Result: Experiments on synthetic and semi-synthetic datasets validate the effectiveness of the proposed HDRC estimator.

Conclusion: The approach successfully addresses the challenges of unobserved confounders and continuous treatment, providing a robust solution for personalized decision-making.

Abstract: Long-term treatment effect estimation is a significant but challenging
problem in many applications. Existing methods rely on ideal assumptions, such
as no unobserved confounders or binary treatment, to estimate long-term average
treatment effects. However, in numerous real-world applications, these
assumptions could be violated, and average treatment effects are insufficient
for personalized decision-making. In this paper, we address a more general
problem of estimating long-term Heterogeneous Dose-Response Curve (HDRC) while
accounting for unobserved confounders and continuous treatment. Specifically,
to remove the unobserved confounders in the long-term observational data, we
introduce an optimal transport weighting framework to align the long-term
observational data to an auxiliary short-term experimental data. Furthermore,
to accurately predict the heterogeneous effects of continuous treatment, we
establish a generalization bound on counterfactual prediction error by
leveraging the reweighted distribution induced by optimal transport. Finally,
we develop a long-term HDRC estimator building upon the above theoretical
foundations. Extensive experiments on synthetic and semi-synthetic datasets
demonstrate the effectiveness of our approach.

</details>


### [430] [IISE PG&E Energy Analytics Challenge 2025: Hourly-Binned Regression Models Beat Transformers in Load Forecasting](https://arxiv.org/pdf/2505.11390)
*Millend Roy, Vladimir Pyltsov, Yinbo Hu*

Main category: cs.LG

TL;DR: Deep learning models like TimeGPT don't outperform simpler methods (e.g., XGBoost) in long-term electricity load forecasting due to data limitations. XGBoost achieves the best results with minimal feature engineering.


<details>
  <summary>Details</summary>
Motivation: Accurate electricity load forecasting is crucial for grid stability and renewable energy integration, but the effectiveness of transformer-based models for long-term predictions is uncertain.

Method: Evaluated models from classical regression to deep learning using ESD 2025 data, employing PCA for dimensionality reduction and stacking 24 regression models for yearly forecasts.

Result: XGBoost outperformed deep learning models, achieving the lowest error rates, while TimeGPT and others struggled due to limited training data and exogenous variables.

Conclusion: Deep learning's limitations in long-term forecasting highlight the need for model selection based on dataset characteristics, favoring simpler, efficient methods like XGBoost.

Abstract: Accurate electricity load forecasting is essential for grid stability,
resource optimization, and renewable energy integration. While
transformer-based deep learning models like TimeGPT have gained traction in
time-series forecasting, their effectiveness in long-term electricity load
prediction remains uncertain. This study evaluates forecasting models ranging
from classical regression techniques to advanced deep learning architectures
using data from the ESD 2025 competition. The dataset includes two years of
historical electricity load data, alongside temperature and global horizontal
irradiance (GHI) across five sites, with a one-day-ahead forecasting horizon.
Since actual test set load values remain undisclosed, leveraging predicted
values would accumulate errors, making this a long-term forecasting challenge.
We employ (i) Principal Component Analysis (PCA) for dimensionality reduction
and (ii) frame the task as a regression problem, using temperature and GHI as
covariates to predict load for each hour, (iii) ultimately stacking 24 models
to generate yearly forecasts.
  Our results reveal that deep learning models, including TimeGPT, fail to
consistently outperform simpler statistical and machine learning approaches due
to the limited availability of training data and exogenous variables. In
contrast, XGBoost, with minimal feature engineering, delivers the lowest error
rates across all test cases while maintaining computational efficiency. This
highlights the limitations of deep learning in long-term electricity
forecasting and reinforces the importance of model selection based on dataset
characteristics rather than complexity. Our study provides insights into
practical forecasting applications and contributes to the ongoing discussion on
the trade-offs between traditional and modern forecasting methods.

</details>


### [431] [Finding Counterfactual Evidences for Node Classification](https://arxiv.org/pdf/2505.11396)
*Dazhuo Qiu, Jinwen Chen, Arijit Khan, Yan Zhao, Francesco Bonchi*

Main category: cs.LG

TL;DR: The paper addresses the challenge of finding counterfactual evidences in GNN-based node classification to improve fairness and interpretability, using observational data when randomized trials are impractical.


<details>
  <summary>Details</summary>
Motivation: To enhance fairness and interpretability in GNNs by leveraging counterfactual learning, especially when randomized controlled trials are not feasible.

Method: Develops search algorithms and an indexing solution combining node features and structural information to identify counterfactual evidences, independent of specific GNNs.

Result: Demonstrates the effectiveness of counterfactual evidences in improving GNN fairness and accuracy through downstream applications.

Conclusion: Counterfactual evidences offer a promising approach to address GNN limitations, enhancing fairness and interpretability without relying on randomized trials.

Abstract: Counterfactual learning is emerging as an important paradigm, rooted in
causality, which promises to alleviate common issues of graph neural networks
(GNNs), such as fairness and interpretability. However, as in many real-world
application domains where conducting randomized controlled trials is
impractical, one has to rely on available observational (factual) data to
detect counterfactuals. In this paper, we introduce and tackle the problem of
searching for counterfactual evidences for the GNN-based node classification
task. A counterfactual evidence is a pair of nodes such that, regardless they
exhibit great similarity both in the features and in their neighborhood
subgraph structures, they are classified differently by the GNN. We develop
effective and efficient search algorithms and a novel indexing solution that
leverages both node features and structural information to identify
counterfactual evidences, and generalizes beyond any specific GNN. Through
various downstream applications, we demonstrate the potential of counterfactual
evidences to enhance fairness and accuracy of GNNs.

</details>


### [432] [Provably Efficient Exploration in Inverse Constrained Reinforcement Learning](https://arxiv.org/pdf/2409.15963)
*Bo Yue, Jian Li, Guiliang Liu*

Main category: cs.LG

TL;DR: The paper proposes a strategic exploration framework for efficient sampling in Inverse Constrained Reinforcement Learning (ICRL), addressing gaps in current methods by introducing two exploratory algorithms with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Current sampling strategies in ICRL lack clarity in efficacy and efficiency, necessitating a more systematic approach to infer constraints from expert behaviors.

Method: The authors define a feasible cost set for ICRL and analyze error influences. They introduce two algorithms: one reduces cost estimation errors dynamically, and the other strategically constrains exploration around optimal policies.

Result: Both algorithms are theoretically supported with tractable sample complexity and empirically validated across various environments.

Conclusion: The proposed framework improves efficiency and feasibility in constraint inference for ICRL, offering practical solutions with theoretical backing.

Abstract: Optimizing objective functions subject to constraints is fundamental in many
real-world applications. However, these constraints are often not readily
defined and must be inferred from expert agent behaviors, a problem known as
Inverse Constraint Inference. Inverse Constrained Reinforcement Learning (ICRL)
is a common solver for recovering feasible constraints in complex environments,
relying on training samples collected from interactive environments. However,
the efficacy and efficiency of current sampling strategies remain unclear. We
propose a strategic exploration framework for sampling with guaranteed
efficiency to bridge this gap. By defining the feasible cost set for ICRL
problems, we analyze how estimation errors in transition dynamics and the
expert policy influence the feasibility of inferred constraints. Based on this
analysis, we introduce two exploratory algorithms to achieve efficient
constraint inference via 1) dynamically reducing the bounded aggregate error of
cost estimations or 2) strategically constraining the exploration policy around
plausibly optimal ones. Both algorithms are theoretically grounded with
tractable sample complexity, and their performance is validated empirically
across various environments.

</details>


### [433] [Is Grokking a Computational Glass Relaxation?](https://arxiv.org/pdf/2505.11411)
*Xiaotian Zhang, Yue Shang, Entao Yang, Ge Zhang*

Main category: cs.LG

TL;DR: The paper explores grokking in neural networks, likening it to glass relaxation, and challenges prior theories by showing no entropy barrier in the memorization-to-generalization transition. It introduces a toy optimizer, WanD, to eliminate grokking.


<details>
  <summary>Details</summary>
Motivation: To understand the underlying mechanisms of neural networks' generalizability, particularly the abrupt generalization phenomenon known as grokking.

Method: Frames grokking as computational glass relaxation, sampling NNs' Boltzmann entropy landscape, and experiments with transformers on arithmetic tasks. Introduces the WanD optimizer inspired by Wang-Landau molecular dynamics.

Result: No entropy barrier in grokking's memorization-to-generalization transition, challenging prior theories. High-entropy advantage under grokking identified. WanD optimizer eliminates grokking and finds generalizing solutions.

Conclusion: Grokking is not a first-order phase transition. The findings suggest new directions for optimizer design and challenge existing theories on grokking.

Abstract: Understanding neural network's (NN) generalizability remains a central
question in deep learning research. The special phenomenon of grokking, where
NNs abruptly generalize long after the training performance reaches a
near-perfect level, offers a unique window to investigate the underlying
mechanisms of NNs' generalizability. Here we propose an interpretation for
grokking by framing it as a computational glass relaxation: viewing NNs as a
physical system where parameters are the degrees of freedom and train loss is
the system energy, we find memorization process resembles a rapid cooling of
liquid into non-equilibrium glassy state at low temperature and the later
generalization is like a slow relaxation towards a more stable configuration.
This mapping enables us to sample NNs' Boltzmann entropy (states of density)
landscape as a function of training loss and test accuracy. Our experiments in
transformers on arithmetic tasks suggests that there is NO entropy barrier in
the memorization-to-generalization transition of grokking, challenging previous
theory that defines grokking as a first-order phase transition. We identify a
high-entropy advantage under grokking, an extension of prior work linking
entropy to generalizability but much more significant. Inspired by grokking's
far-from-equilibrium nature, we develop a toy optimizer WanD based on
Wang-landau molecular dynamics, which can eliminate grokking without any
constraints and find high-norm generalizing solutions. This provides
strictly-defined counterexamples to theory attributing grokking solely to
weight norm evolution towards the Goldilocks zone and also suggests new
potential ways for optimizer design.

</details>


### [434] [Uncertainty quantification with approximate variational learning for wearable photoplethysmography prediction tasks](https://arxiv.org/pdf/2505.11412)
*Ciaran Bench, Vivek Desai, Mohammad Moulaeifard, Nils Strodthoff, Philip Aston, Andrew Thompson*

Main category: cs.LG

TL;DR: The paper explores uncertainty quantification techniques (Monte Carlo Dropout and Improved Variational Online Newton) for deep networks analyzing PPG signals to improve trustworthiness in AF classification and BP regression.


<details>
  <summary>Details</summary>
Motivation: Deep networks lack interpretability and are prone to overfitting, risking poor performance on unseen data and misdiagnosis in cardiac health assessments using PPG signals.

Method: Two scalable uncertainty quantification techniques are applied to assess model trustworthiness for AF classification and BP regression from raw PPG time series.

Result: Hyperparameter choice significantly impacts predictive performance and uncertainty quality, with stochasticity affecting aleatoric uncertainty proportion and calibration. Discrepancies in uncertainty quality across predicted classes highlight the need for thorough evaluation.

Conclusion: Careful hyperparameter tuning is essential to balance predictive performance and calibration quality, with optimal settings varying by uncertainty expression.

Abstract: Photoplethysmography (PPG) signals encode information about relative changes
in blood volume that can be used to assess various aspects of cardiac health
non-invasively, e.g.\ to detect atrial fibrillation (AF) or predict blood
pressure (BP). Deep networks are well-equipped to handle the large quantities
of data acquired from wearable measurement devices. However, they lack
interpretability and are prone to overfitting, leaving considerable risk for
poor performance on unseen data and misdiagnosis. Here, we describe the use of
two scalable uncertainty quantification techniques: Monte Carlo Dropout and the
recently proposed Improved Variational Online Newton. These techniques are used
to assess the trustworthiness of models trained to perform AF classification
and BP regression from raw PPG time series. We find that the choice of
hyperparameters has a considerable effect on the predictive performance of the
models and on the quality and composition of predicted uncertainties. E.g. the
stochasticity of the model parameter sampling determines the proportion of the
total uncertainty that is aleatoric, and has varying effects on predictive
performance and calibration quality dependent on the chosen uncertainty
quantification technique and the chosen expression of uncertainty. We find
significant discrepancy in the quality of uncertainties over the predicted
classes, emphasising the need for a thorough evaluation protocol that assesses
local and adaptive calibration. This work suggests that the choice of
hyperparameters must be carefully tuned to balance predictive performance and
calibration quality, and that the optimal parameterisation may vary depending
on the chosen expression of uncertainty.

</details>


### [435] [MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse Mixture-of-Experts Systems](https://arxiv.org/pdf/2505.11415)
*Yinsicheng Jiang, Yao Fu, Yeqi Huang, Ping Nie, Zhan Lu, Leyang Xue, Congjie He, Man-Kit Sit, Jilong Xue, Li Dong, Ziming Miao, Dayou Du, Tairan Xu, Kai Zou, Edoardo Ponti, Luo Mai*

Main category: cs.LG

TL;DR: MoE-CAP is a benchmark for MoE systems, highlighting the trade-offs between Cost, Accuracy, and Performance (CAP) and introducing new metrics for better evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to accurately capture the trade-offs in MoE systems, complicating deployment decisions.

Method: Introduces MoE-CAP benchmark, CAP Radar Diagram, and sparsity-aware metrics (S-MBU, S-MFU) for evaluation.

Result: Optimal CAP balance is hard to achieve; MoE systems often sacrifice one dimension for the other two.

Conclusion: MoE-CAP and new metrics provide better tools for benchmarking MoE systems across hardware and scenarios.

Abstract: The sparse Mixture-of-Experts (MoE) architecture is increasingly favored for
scaling Large Language Models (LLMs) efficiently, but it depends on
heterogeneous compute and memory resources. These factors jointly affect system
Cost, Accuracy, and Performance (CAP), making trade-offs inevitable. Existing
benchmarks often fail to capture these trade-offs accurately, complicating
practical deployment decisions. To address this, we introduce MoE-CAP, a
benchmark specifically designed for MoE systems. Our analysis reveals that
achieving an optimal balance across CAP is difficult with current hardware; MoE
systems typically optimize two of the three dimensions at the expense of the
third-a dynamic we term the MoE-CAP trade-off. To visualize this, we propose
the CAP Radar Diagram. We further introduce sparsity-aware performance
metrics-Sparse Memory Bandwidth Utilization (S-MBU) and Sparse Model FLOPS
Utilization (S-MFU)-to enable accurate performance benchmarking of MoE systems
across diverse hardware platforms and deployment scenarios.

</details>


### [436] [Degree-Conscious Spiking Graph for Cross-Domain Adaptation](https://arxiv.org/pdf/2410.06883)
*Yingxu Wang, Mengzhu Wang, Siwei Liu, Houcheng Su, Nan Yin, James Kwok*

Main category: cs.LG

TL;DR: DeSGraDA improves Spiking Graph Networks (SGNs) for cross-domain adaptation by introducing degree-conscious spiking, temporal alignment, and pseudo-labeling, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing SGNs struggle with distribution shifts, limiting their generalization across domains.

Method: Proposes DeSGraDA with degree-conscious spiking, adversarial temporal alignment, and pseudo-labeling for cross-domain adaptation.

Result: Outperforms state-of-the-art methods in accuracy and energy efficiency on benchmark datasets.

Conclusion: DeSGraDA effectively addresses domain adaptation in SGNs, enhancing generalization and performance.

Abstract: Spiking Graph Networks (SGNs) have demonstrated significant potential in
graph classification by emulating brain-inspired neural dynamics to achieve
energy-efficient computation. However, existing SGNs are generally constrained
to in-distribution scenarios and struggle with distribution shifts. In this
paper, we first propose the domain adaptation problem in SGNs, and introduce a
novel framework named Degree-Consicious Spiking Graph for Cross-Domain
Adaptation. DeSGraDA enhances generalization across domains with three key
components. First, we introduce the degree-conscious spiking representation
module by adapting spike thresholds based on node degrees, enabling more
expressive and structure-aware signal encoding. Then, we perform temporal
distribution alignment by adversarially matching membrane potentials between
domains, ensuring effective performance under domain shift while preserving
energy efficiency. Additionally, we extract consistent predictions across two
spaces to create reliable pseudo-labels, effectively leveraging unlabeled data
to enhance graph classification performance. Furthermore, we establish the
first generalization bound for SGDA, providing theoretical insights into its
adaptation performance. Extensive experiments on benchmark datasets validate
that DeSGraDA consistently outperforms state-of-the-art methods in both
classification accuracy and energy efficiency.

</details>


### [437] [MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production](https://arxiv.org/pdf/2505.11432)
*Chao Jin, Ziheng Jiang, Zhihao Bai, Zheng Zhong, Juncai Liu, Xiang Li, Ningxin Zheng, Xi Wang, Cong Xie, Wen Heng, Yiyuan Ma, Wenlei Bao, Size Zheng, Yanghua Peng, Haibin Lin, Xuanzhe Liu, Xin Jin, Xin Liu*

Main category: cs.LG

TL;DR: MegaScale-MoE is a system designed to efficiently train large-scale MoE models by optimizing communication and computation overlap, achieving significant throughput improvements.


<details>
  <summary>Details</summary>
Motivation: MoE models show promise for scaling LLMs but suffer from efficiency degradation due to model size and hardware evolution. Efficient communication is key to improving training.

Method: Customizes communication-efficient parallelism for MoE layers, overlaps communication with computation, and applies communication compression to lower precision.

Result: Achieves 1.41M tokens/s throughput on 1,440 GPUs, 1.88√ó efficiency improvement over Megatron-LM.

Conclusion: Shares insights to inspire future MoE system research, demonstrating practical efficiency gains.

Abstract: We present MegaScale-MoE, a production system tailored for the efficient
training of large-scale mixture-of-experts (MoE) models. MoE emerges as a
promising architecture to scale large language models (LLMs) to unprecedented
sizes, thereby enhancing model performance. However, existing MoE training
systems experience a degradation in training efficiency, exacerbated by the
escalating scale of MoE models and the continuous evolution of hardware.
  Recognizing the pivotal role of efficient communication in enhancing MoE
training, MegaScale-MoE customizes communication-efficient parallelism
strategies for attention and FFNs in each MoE layer and adopts a holistic
approach to overlap communication with computation at both inter- and
intra-operator levels. Additionally, MegaScale-MoE applies communication
compression with adjusted communication patterns to lower precision, further
improving training efficiency. When training a 352B MoE model on 1,440 NVIDIA
Hopper GPUs, MegaScale-MoE achieves a training throughput of 1.41M tokens/s,
improving the efficiency by 1.88$\times$ compared to Megatron-LM. We share our
operational experience in accelerating MoE training and hope that by offering
our insights in system design, this work will motivate future research in MoE
systems.

</details>


### [438] [Linear Attention Sequence Parallelism](https://arxiv.org/pdf/2404.02882)
*Weigao Sun, Zhen Qin, Dong Li, Xuyang Shen, Yu Qiao, Yiran Zhong*

Main category: cs.LG

TL;DR: LASP is a new sequence parallelism method for linear attention models, improving communication efficiency and scalability up to 4096K sequences.


<details>
  <summary>Details</summary>
Motivation: Existing SP methods are inefficient for linear attention due to their right-product-first feature, leading to sub-optimal communication and usability.

Method: LASP uses a ring-style communication mechanism, kernel fusion, and state caching to optimize efficiency and compatibility with batch-level parallelism.

Result: LASP scales sequences up to 4096K on 128 GPUs, 8√ó longer than existing SP methods.

Conclusion: LASP is a hardware-friendly, efficient SP approach for linear attention models, with broad applicability.

Abstract: Sequence parallelism (SP) serves as a prevalent strategy to handle long
sequences that exceed the memory limit of a single device. However, for linear
sequence modeling methods like linear attention, existing SP approaches do not
take advantage of their right-product-first feature, resulting in sub-optimal
communication efficiency and usability. In this paper, we introduce Linear
Attention Sequence Parallelism (LASP), an efficient SP approach designed for
linear attention-based transformer models. Specifically, we design an efficient
point-to-point ring-style communication mechanism to leverage the right-product
kernel trick of linear attention, which sharply decreases the communication
overhead, comparing with existing SP methods. We enhance the computation
efficiency of LASP by performing kernel fusion and intermediate state caching,
making the implementation of LASP hardware-friendly on GPUs. Furthermore, we
meticulously ensure the compatibility of sequence-level LASP with all types of
batch-level data parallel methods, which is vital for distributed training on
large clusters with very-long sequences. We also discuss the generalization of
LASP on other linear sequence modeling methods. Extensive experiments on linear
attention-based models are conducted with varying sequence lengths from 2K to
4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\times$
longer than existing SP methods. Code is available at:
https://github.com/OpenNLPLab/LASP.

</details>


### [439] [A Generative Framework for Causal Estimation via Importance-Weighted Diffusion Distillation](https://arxiv.org/pdf/2505.11444)
*Xinran Song, Tianyu Chen, Mingyuan Zhou*

Main category: cs.LG

TL;DR: IWDD integrates IPW into diffusion models for accurate, fast causal estimation, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Address covariate imbalance and confounding bias in non-randomized treatment assignment for causal inference.

Method: Combine pretrained diffusion models with importance-weighted score distillation, introducing a randomization-based adjustment to avoid explicit IPW computation.

Result: Achieves state-of-the-art out-of-sample prediction performance with the highest win rates.

Conclusion: IWDD improves causal estimation and supports individualized treatment strategies, with code released for reproducibility.

Abstract: Estimating individualized treatment effects from observational data is a
central challenge in causal inference, largely due to covariate imbalance and
confounding bias from non-randomized treatment assignment. While inverse
probability weighting (IPW) is a well-established solution to this problem, its
integration into modern deep learning frameworks remains limited. In this work,
we propose Importance-Weighted Diffusion Distillation (IWDD), a novel
generative framework that combines the pretraining of diffusion models with
importance-weighted score distillation to enable accurate and fast causal
estimation-including potential outcome prediction and treatment effect
estimation. We demonstrate how IPW can be naturally incorporated into the
distillation of pretrained diffusion models, and further introduce a
randomization-based adjustment that eliminates the need to compute IPW
explicitly-thereby simplifying computation and, more importantly, provably
reducing the variance of gradient estimates. Empirical results show that IWDD
achieves state-of-the-art out-of-sample prediction performance, with the
highest win rates compared to other baselines, significantly improving causal
estimation and supporting the development of individualized treatment
strategies. We will release our PyTorch code for reproducibility and future
research.

</details>


### [440] [Drama: Mamba-Enabled Model-Based Reinforcement Learning Is Sample and Parameter Efficient](https://arxiv.org/pdf/2410.08893)
*Wenlong Wang, Ivana Dusparic, Yucheng Shi, Ke Zhang, Vinny Cahill*

Main category: cs.LG

TL;DR: Drama, an SSM-based world model using Mamba, offers efficient training with O(n) complexity, addressing RNN and Transformer limitations in model-based RL.


<details>
  <summary>Details</summary>
Motivation: To overcome the inefficiencies and challenges of RNNs and Transformers in world models for RL, such as vanishing gradients and high computational costs.

Method: Proposes Drama, leveraging Mamba for O(n) complexity, and introduces a novel sampling method to handle early training suboptimality.

Result: Achieves competitive SOTA performance on Atari100k with a 7M-parameter model, trainable on standard hardware.

Conclusion: Drama provides an efficient, scalable solution for model-based RL, accessible on off-the-shelf hardware.

Abstract: Model-based reinforcement learning (RL) offers a solution to the data
inefficiency that plagues most model-free RL algorithms. However, learning a
robust world model often requires complex and deep architectures, which are
computationally expensive and challenging to train. Within the world model,
sequence models play a critical role in accurate predictions, and various
architectures have been explored, each with its own challenges. Currently,
recurrent neural network (RNN)-based world models struggle with vanishing
gradients and capturing long-term dependencies. Transformers, on the other
hand, suffer from the quadratic memory and computational complexity of
self-attention mechanisms, scaling as $O(n^2)$, where $n$ is the sequence
length.
  To address these challenges, we propose a state space model (SSM)-based world
model, Drama, specifically leveraging Mamba, that achieves $O(n)$ memory and
computational complexity while effectively capturing long-term dependencies and
enabling efficient training with longer sequences. We also introduce a novel
sampling method to mitigate the suboptimality caused by an incorrect world
model in the early training stages. Combining these techniques, Drama achieves
a normalised score on the Atari100k benchmark that is competitive with other
state-of-the-art (SOTA) model-based RL algorithms, using only a 7
million-parameter world model. Drama is accessible and trainable on
off-the-shelf hardware, such as a standard laptop. Our code is available at
https://github.com/realwenlongwang/Drama.git.

</details>


### [441] [Signal attenuation enables scalable decentralized multi-agent reinforcement learning over networks](https://arxiv.org/pdf/2505.11461)
*Wesley A Suttle, Vipul K Sharma, Brian M Sadler*

Main category: cs.LG

TL;DR: The paper addresses decentralization in MARL by leveraging signal attenuation in radar networks, proposing new MDP formulations and decentralized algorithms.


<details>
  <summary>Details</summary>
Motivation: Classic MARL requires global state observability, limiting scalability. Signal attenuation in wireless/radar networks offers a natural way to enable decentralization.

Method: Proposes two constrained multi-agent MDP formulations, derives local approximations for value functions/gradients, and develops decentralized saddle point policy gradient algorithms.

Result: Establishes error bounds for local approximations and demonstrates feasibility of decentralization in MARL for radar power allocation.

Conclusion: The approach provides a model for extending decentralization to other wireless/radar problems, leveraging signal attenuation.

Abstract: Classic multi-agent reinforcement learning (MARL) methods require that agents
enjoy global state observability, preventing development of decentralized
algorithms and limiting scalability. Recent work has shown that, under
assumptions on decaying inter-agent influence, global observability can be
replaced by local neighborhood observability at each agent, enabling
decentralization and scalability. Real-world applications enjoying such decay
properties remain underexplored, however, despite the fact that signal power
decay, or signal attenuation, due to path loss is an intrinsic feature of many
problems in wireless communications and radar networks. In this paper, we show
that signal attenuation enables decentralization in MARL by considering the
illustrative special case of performing power allocation for target detection
in a radar network. To achieve this, we propose two new constrained multi-agent
Markov decision process formulations of this power allocation problem, derive
local neighborhood approximations for global value function and gradient
estimates and establish corresponding error bounds, and develop decentralized
saddle point policy gradient algorithms for solving the proposed problems. Our
approach, though oriented towards the specific radar network problem we
consider, provides a useful model for future extensions to additional problems
in wireless communications and radar networks.

</details>


### [442] [Words in Motion: Extracting Interpretable Control Vectors for Motion Transformers](https://arxiv.org/pdf/2406.11624)
*Omer Sahin Tas, Royden Wagner*

Main category: cs.LG

TL;DR: The paper analyzes and modifies Transformer hidden states for interpretability in motion forecasting, using linear probing and control vectors to achieve feasible predictions and zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: Transformer hidden states are hard to interpret, and the paper aims to uncover and manipulate interpretable features within them for motion forecasting.

Method: Uses linear probing to identify interpretable features, fits control vectors between hidden states, and refines them with sparse autoencoders (SAEs).

Result: High probing accuracy shows latent space regularities; control vectors preserve prediction feasibility and enable zero-shot generalization.

Conclusion: The approach provides mechanistic interpretation and efficient zero-shot generalization with minimal computational cost.

Abstract: Transformer-based models generate hidden states that are difficult to
interpret. In this work, we analyze hidden states and modify them at inference,
with a focus on motion forecasting. We use linear probing to analyze whether
interpretable features are embedded in hidden states. Our experiments reveal
high probing accuracy, indicating latent space regularities with functionally
important directions. Building on this, we use the directions between hidden
states with opposing features to fit control vectors. At inference, we add our
control vectors to hidden states and evaluate their impact on predictions.
Remarkably, such modifications preserve the feasibility of predictions. We
further refine our control vectors using sparse autoencoders (SAEs). This leads
to more linear changes in predictions when scaling control vectors. Our
approach enables mechanistic interpretation as well as zero-shot generalization
to unseen dataset characteristics with negligible computational overhead.

</details>


### [443] [msf-CNN: Patch-based Multi-Stage Fusion with Convolutional Neural Networks for TinyML](https://arxiv.org/pdf/2505.11483)
*Zhaolan Huang, Emmanuel Baccelli*

Main category: cs.LG

TL;DR: msf-CNN is a novel technique optimizing fusion settings for CNNs on MCUs, reducing RAM usage by 50% compared to prior methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of fitting AI models within MCUs' tiny memory budgets while maintaining low inference latency.

Method: Patch-based fusion represented as a directed acyclic graph to explore optimal fusion settings for CNNs.

Result: Achieves 50% less RAM usage than MCUNetV2 and StreamNet, with broader solution space.

Conclusion: msf-CNN provides flexibility and efficiency for AI deployment on resource-constrained MCUs.

Abstract: AI spans from large language models to tiny models running on
microcontrollers (MCUs). Extremely memory-efficient model architectures are
decisive to fit within an MCU's tiny memory budget e.g., 128kB of RAM. However,
inference latency must remain small to fit real-time constraints. An approach
to tackle this is patch-based fusion, which aims to optimize data flows across
neural network layers. In this paper, we introduce msf-CNN, a novel technique
that efficiently finds optimal fusion settings for convolutional neural
networks (CNNs) by walking through the fusion solution space represented as a
directed acyclic graph. Compared to previous work on CNN fusion for MCUs,
msf-CNN identifies a wider set of solutions. We published an implementation of
msf-CNN running on various microcontrollers (ARM Cortex-M, RISC-V, ESP32). We
show that msf-CNN can achieve inference using 50% less RAM compared to the
prior art (MCUNetV2 and StreamNet). We thus demonstrate how msf-CNN offers
additional flexibility for system designers.

</details>


### [444] [Potential failures of physics-informed machine learning in traffic flow modeling: theoretical and experimental analysis](https://arxiv.org/pdf/2505.11491)
*Yuan-Zheng Lei, Yaobang Gong, Dianwei Chen, Yao Cheng, Xianfeng Terry Yang*

Main category: cs.LG

TL;DR: PIML for traffic flow modeling fails when it underperforms data-driven and physics-based models. Physics residuals don't hinder optimization; success depends on gradient alignment. Dataset suitability is indicated by the CFL condition.


<details>
  <summary>Details</summary>
Motivation: To understand why PIML models fail in traffic flow modeling and identify conditions for their success.

Method: Analyze loss landscapes by perturbing trained models along Hessian eigenvectors. Evaluate performance under varying conditions like sparse sampling and CFL adherence.

Result: Physics residuals don't inherently hinder optimization. Dataset suitability depends on CFL condition. Higher-order models (ARZ) have larger error bounds than lower-order ones (LWR).

Conclusion: PIML success requires gradient alignment and suitable datasets (CFL condition). Physics models and data inaccuracies complicate optimization.

Abstract: This study critically examines the performance of physics-informed machine
learning (PIML) approaches for traffic flow modeling, defining the failure of a
PIML model as the scenario where it underperforms both its purely data-driven
and purely physics-based counterparts. We analyze the loss landscape by
perturbing trained models along the principal eigenvectors of the Hessian
matrix and evaluating corresponding loss values. Our results suggest that
physics residuals in PIML do not inherently hinder optimization, contrary to a
commonly assumed failure cause. Instead, successful parameter updates require
both ML and physics gradients to form acute angles with the quasi-true gradient
and lie within a conical region. Given inaccuracies in both the physics models
and the training data, satisfying this condition is often difficult.
Experiments reveal that physical residuals can degrade the performance of LWR-
and ARZ-based PIML models, especially under highly physics-driven settings.
Moreover, sparse sampling and the use of temporally averaged traffic data can
produce misleadingly small physics residuals that fail to capture actual
physical dynamics, contributing to model failure. We also identify the
Courant-Friedrichs-Lewy (CFL) condition as a key indicator of dataset
suitability for PIML, where successful applications consistently adhere to this
criterion. Lastly, we observe that higher-order models like ARZ tend to have
larger error lower bounds than lower-order models like LWR, which is consistent
with the experimental findings of existing studies.

</details>


### [445] [MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal Projection](https://arxiv.org/pdf/2410.14731)
*Bokai Lin, Zihao Zeng, Zipeng Xiao, Siqi Kou, Tianqi Hou, Xiaofeng Gao, Hao Zhang, Zhijie Deng*

Main category: cs.LG

TL;DR: The paper proposes a method to compress the KV cache in LLMs by focusing on the feature dimension axis using low-rank projection matrices, achieving high efficiency and performance retention.


<details>
  <summary>Details</summary>
Motivation: The KV cache in LLMs becomes a bottleneck in storage and memory transfer as model and data sizes grow. Prior work focused on compressing the first three axes of cache tensors, leaving the feature dimension axis unexplored.

Method: The method uses low-rank projection matrices to reduce feature dimensions, starting with PCA but addressing its limitations by tuning orthogonal matrices with a distillation objective and Matryoshka training. Adaptive compression rates are searched for optimal performance.

Result: The method achieves up to 75% KV cache compression while sustaining over 90% performance for models like LLaMA2-7B-base and Mistral-7B-v0.3-base.

Conclusion: The proposed approach efficiently compresses the KV cache with minimal performance loss, offering a smooth tradeoff between compression and accuracy for pre-trained LLMs.

Abstract: KV cache has become a de facto technique for the inference of large language
models (LLMs), where tensors of shape (layer number, head number, sequence
length, feature dimension) are introduced to cache historical information for
self-attention. As the size of the model and data grows, the KV cache can
quickly become a bottleneck within the system in both storage and memory
transfer. To address this, prior studies usually focus on the first three axes
of the cache tensors for compression. This paper supplements them, focusing on
the feature dimension axis, by utilizing low-rank projection matrices to
transform the cache features into spaces with reduced dimensions. We begin by
investigating the canonical orthogonal projection method for data compression
through principal component analysis (PCA). We observe the issue with PCA
projection where significant performance degradation is observed at low
compression rates. To bridge the gap, we propose to directly tune the
orthogonal projection matrices with a distillation objective using an elaborate
Matryoshka training strategy. After training, we adaptively search for the
optimal compression rates for various layers and heads given varying
compression budgets. Compared to previous works, our method can easily embrace
pre-trained LLMs and hold a smooth tradeoff between performance and compression
rate. We empirically witness the high data efficiency of our training procedure
and find that our method can sustain over 90% performance with an average KV
cache compression rate of 60% (and up to 75% in certain extreme scenarios) for
popular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base.

</details>


### [446] [Sparsing Law: Towards Large Language Models with Greater Activation Sparsity](https://arxiv.org/pdf/2411.02335)
*Yuqi Luo, Chenyang Song, Xu Han, Yingfa Chen, Chaojun Xiao, Zhiyuan Liu, Maosong Sun*

Main category: cs.LG

TL;DR: The paper studies activation sparsity in LLMs, proposing a new metric (PPL-$p\%$ sparsity) and revealing trends in sparsity influenced by activation functions, training data, and model architecture.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify the correlation between activation sparsity and influential factors in LLMs, as existing research lacks comprehensive analysis.

Method: Proposes PPL-$p\%$ sparsity metric and conducts extensive experiments on decoder-only Transformer-based LLMs, analyzing activation functions, training data, and model architecture.

Result: Key findings include opposite sparsity trends for SiLU and ReLU, power-law evolution of activation ratio with training data, and insensitivity of sparsity to parameter scale.

Conclusion: The empirical laws discovered can guide the design of more efficient and interpretable LLMs by optimizing activation sparsity.

Abstract: Activation sparsity denotes the existence of substantial weakly-contributed
elements within activation outputs that can be eliminated, benefiting many
important applications concerned with large language models (LLMs). Although
promoting greater activation sparsity within LLMs deserves deep studies,
existing works lack comprehensive and quantitative research on the correlation
between activation sparsity and potentially influential factors. In this paper,
we present a comprehensive study on the quantitative scaling properties and
influential factors of the activation sparsity within decoder-only
Transformer-based LLMs. Specifically, we propose PPL-$p\%$ sparsity, a precise
and performance-aware activation sparsity metric that is applicable to any
activation function. Through extensive experiments, we find several important
phenomena. Firstly, different activation functions exhibit comparable
performance but opposite training-time sparsity trends. The activation ratio
(i.e., $1-\mathrm{sparsity\ ratio}$) evolves as a convergent increasing
power-law and decreasing logspace power-law with the amount of training data
for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate
that ReLU is more efficient as the activation function than SiLU and can
leverage more training data to improve activation sparsity. Secondly, the
activation ratio linearly increases with the width-depth ratio below a certain
bottleneck point, indicating the potential advantage of a deeper architecture
at a fixed parameter scale. Finally, at similar width-depth ratios, we
surprisingly find that the limit value of activation sparsity varies weakly
with the parameter scale, i.e., the activation patterns within LLMs are
insensitive to the parameter scale. These empirical laws towards LLMs with
greater activation sparsity have important implications for making LLMs more
efficient and interpretable.

</details>


### [447] [Statistical Guarantees for Lifelong Reinforcement Learning using PAC-Bayes Theory](https://arxiv.org/pdf/2411.00401)
*Zhi Zhang, Chris Chow, Yasi Zhang, Yanchao Sun, Haochen Zhang, Eric Hanchen Jiang, Han Liu, Furong Huang, Yuchen Cui, Oscar Hernan Madrid Padilla*

Main category: cs.LG

TL;DR: EPIC is a lifelong RL algorithm using PAC-Bayes theory to learn a shared policy distribution, enabling rapid adaptation to new tasks while retaining prior knowledge. It outperforms existing methods with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of lifelong RL, where agents face dynamic task streams, by developing a method that retains knowledge and adapts efficiently.

Method: EPIC employs PAC-Bayes theory to learn a shared policy distribution (world policy) for rapid adaptation and knowledge retention.

Result: EPIC shows superior performance in lifelong RL, with theoretical guarantees on generalization and sample complexity.

Conclusion: EPIC is a promising approach for lifelong RL, combining theoretical rigor with practical effectiveness.

Abstract: Lifelong reinforcement learning (RL) has been developed as a paradigm for
extending single-task RL to more realistic, dynamic settings. In lifelong RL,
the "life" of an RL agent is modeled as a stream of tasks drawn from a task
distribution. We propose EPIC (Empirical PAC-Bayes that Improves Continuously),
a novel algorithm designed for lifelong RL using PAC-Bayes theory. EPIC learns
a shared policy distribution, referred to as the world policy, which enables
rapid adaptation to new tasks while retaining valuable knowledge from previous
experiences. Our theoretical analysis establishes a relationship between the
algorithm's generalization performance and the number of prior tasks preserved
in memory. We also derive the sample complexity of EPIC in terms of RL regret.
Extensive experiments on a variety of environments demonstrate that EPIC
significantly outperforms existing methods in lifelong RL, offering both
theoretical guarantees and practical efficacy through the use of the world
policy.

</details>


### [448] [HAFLQ: Heterogeneous Adaptive Federated LoRA Fine-tuned LLM with Quantization](https://arxiv.org/pdf/2411.06581)
*Yang Su, Na Yan, Yansha Deng, Mischa Dohler, Robert Schober*

Main category: cs.LG

TL;DR: HAFLQ is a framework for efficient federated fine-tuning of LLMs, addressing challenges like high resource demands, heterogeneity, and communication bottlenecks through adaptive quantization, parameter freezing, and dynamic aggregation.


<details>
  <summary>Details</summary>
Motivation: Federated fine-tuning of LLMs faces inefficiencies due to computational demands, client heterogeneity, bandwidth constraints, and ineffective aggregation.

Method: HAFLQ uses salience-driven adaptive quantization, parameter truncation, bandwidth-adaptive quantization, and rank-1 matrix-level aggregation.

Result: HAFLQ reduces memory by 31%, communication cost by 49%, improves accuracy by 50%, and speeds up convergence.

Conclusion: HAFLQ effectively addresses federated fine-tuning challenges, offering a scalable and efficient solution for heterogeneous environments.

Abstract: Federated fine-tuning of pre-trained Large Language Models (LLMs) enables
task-specific adaptation across diverse datasets while preserving privacy.
However, challenges such as high computational and memory demands,
heterogeneous client resources, bandwidth constraints, and ineffective global
aggregation hinder its efficiency. To address these issues, we propose HAFLQ
(Heterogeneous Adaptive Federated Low-Rank Adaptation Fine-tuned LLM with
Quantization), a novel framework for efficient and scalable federated
fine-tuning of LLMs in heterogeneous environments. To reduce memory and
computation demands, we propose a salience-driven adaptive LLM quantization
framework that evaluates the importance of transformer blocks using a salience
metric and applies adaptive block-wise quantization accordingly. To handle
heterogeneous computational capabilities, we propose an importance-based
parameter truncation and freezing scheme. To address communication bottlenecks,
we propose an importance-aware bandwidth-adaptive quantization method, which
dynamically adjusts parameter precision based on importance and bandwidth
constraints. To improve global model aggregation, we propose an adaptive rank-1
matrix-level aggregation strategy, which prevents information dilution and
accelerates convergence by aggregating only updated rank-1 matrices from
clients. Experimental results on the text classification task demonstrate that
HAFLQ reduces memory usage by 31%, lowers communication cost by 49%, improves
accuracy by 50%, and achieves faster convergence compared to the baseline
method.

</details>


### [449] [Shuttle Between the Instructions and the Parameters of Large Language Models](https://arxiv.org/pdf/2502.02315)
*Wangtao Sun, Haotian Xu, Huanxuan Liao, Xuanqing Yu, Zhongtao Jiang, Shizhu He, Jun Zhao, Kang Liu*

Main category: cs.LG

TL;DR: SHIP is a neural framework that learns mutual mappings between LLM instructions and parameters, outperforming baselines in deduction and induction tasks.


<details>
  <summary>Details</summary>
Motivation: To explore the correlation between LLM instructions and parameters as compressed task data, enabling mutual prediction.

Method: Proposes SHIP, a neural network framework to model and learn mappings between instructions and parameters.

Result: SHIP excels in instruction deduction and induction, surpassing baselines, and combines mappings for inductive reasoning.

Conclusion: SHIP demonstrates effective mutual mapping between instructions and parameters, enhancing LLM task-solving capabilities.

Abstract: The interaction with Large Language Models (LLMs) through instructions has
been extensively investigated in the research community. While instructions
have been widely used as the guidelines for task solving, this paper further
notices that both instructions and parameters are the compression of task data.
Therefore, they could be strongly correlated and can be learned to predict one
from the other. This paper proposes a novel neural network framework, SHIP
(\textbf{Sh}uttle between the \textbf{I}nstructions and the
\textbf{P}arameters), to model and learn the mutual mappings between the
instructions and the parameters of LLMs. We verify that SHIP can effectively
map one of the instructions/parameters to the other by evaluating it on the
tasks of instruction deduction and induction. The results show that SHIP
performs better than existing baseline methods in terms of deductive
capabilities while significantly surpassing them in inductive capabilities.
Moreover, SHIP can effectively combine the two mapping processes to perform
excellent inductive reasoning. The code and data for this paper are released at
https://anonymous.4open.science/r/Shuttle-Between-Instructions-Parameters/.

</details>


### [450] [Finding One's Bearings in the Hyperparameter Landscape of a Wide-Kernel Convolutional Fault Detector](https://arxiv.org/pdf/2411.15191)
*Dan Hudson, Jurgen van den Hoogen, Martin Atzmueller*

Main category: cs.LG

TL;DR: Neural networks for bearing fault detection can fail due to incorrect hyperparameter settings, which may change with new data. The paper provides guidance on setting architecture-specific hyperparameters like kernel width, showing sensitivity to data properties.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding how hyperparameters, especially architecture-specific ones like kernel width, affect neural network performance on new data for bearing fault detection.

Method: Combines multiple methods to analyze hyperparameter behavior, focusing on kernel width in a wide-kernel CNN. Uses seven benchmark datasets and manipulated copies to study sensitivity to data properties like sampling rate and spectral content.

Result: Found that kernel size in the first layer is sensitive to data changes, and high-frequency noise isn't the main reason for preferring wide kernels. Provides clear hyperparameter guidance.

Conclusion: Clear guidance is provided for setting hyperparameters in the neural network architecture to ensure effective performance on new data, debunking earlier assumptions about high-frequency noise.

Abstract: State-of-the-art algorithms are reported to be almost perfect at
distinguishing the vibrations arising from healthy and damaged machine
bearings, according to benchmark datasets at least. However, what about their
application to new data? In this paper, we confirm that neural networks for
bearing fault detection can be crippled by incorrect hyperparameterisation, and
also that the correct hyperparameter settings can change when transitioning to
new data. The paper combines multiple methods to explain the behaviour of the
hyperparameters of a wide-kernel convolutional neural network and how to set
them. Since guidance already exists for generic hyperparameters like minibatch
size, we focus on how to set architecture-specific hyperparameters such as the
width of the convolutional kernels, a topic which might otherwise be obscure.
We reflect different data properties by fusing information from seven different
benchmark datasets, and our results show that the kernel size in the first
layer in particular is sensitive to changes in the data. Looking deeper, we use
manipulated copies of one dataset in an attempt to spot why the kernel size
sometimes needs to change. The relevance of sampling rate is studied by using
different levels of resampling, and spectral content is studied by increasingly
filtering out high frequencies. We find that, contrary to speculation in
earlier work, high-frequency noise is not the main reason why a wide kernel is
preferable to a narrow kernel. Finally, we conclude by stating clear guidance
on how to set the hyperparameters of our neural network architecture to work
effectively on new data.

</details>


### [451] [FOReCAst: The Future Outcome Reasoning and Confidence Assessment Benchmark](https://arxiv.org/pdf/2502.19676)
*Zhangdie Yuan, Zifeng Ding, Andreas Vlachos*

Main category: cs.LG

TL;DR: FOReCAst is a new benchmark for evaluating forecasting models' prediction accuracy and confidence calibration across diverse real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing forecasting benchmarks lack comprehensive confidence assessment and real-world alignment, limiting their practical utility.

Method: Introduces FOReCAst, a benchmark with diverse forecasting scenarios (Boolean questions, timeframe prediction, quantity estimation) to assess prediction and confidence.

Result: FOReCAst enables comprehensive evaluation of forecasting models for real-world applications.

Conclusion: FOReCAst addresses gaps in current benchmarks, providing a more practical tool for forecasting evaluation.

Abstract: Forecasting is an important task in many domains, such as technology and
economics. However existing forecasting benchmarks largely lack comprehensive
confidence assessment, focus on limited question types, and often consist of
artificial questions that do not align with real-world human forecasting needs.
To address these gaps, we introduce FOReCAst (Future Outcome Reasoning and
Confidence Assessment), a benchmark that evaluates models' ability to make
predictions and their confidence in them. FOReCAst spans diverse forecasting
scenarios involving Boolean questions, timeframe prediction, and quantity
estimation, enabling a comprehensive evaluation of both prediction accuracy and
confidence calibration for real-world applications.

</details>


### [452] [NeuroLifting: Neural Inference on Markov Random Fields at Scale](https://arxiv.org/pdf/2411.18954)
*Yaomin Wang, Chaolong Ying, Xiaodong Luo, Tianshu Yu*

Main category: cs.LG

TL;DR: NeuroLifting uses GNNs to reparameterize MRF variables, enabling gradient descent optimization. It outperforms approximate methods and rivals exact solvers like Toulbar2 in quality, with linear scalability for large MRFs.


<details>
  <summary>Details</summary>
Motivation: Traditional MRF inference methods struggle to balance efficiency and solution quality, especially at scale.

Method: NeuroLifting employs GNNs to reparameterize MRF variables, allowing gradient descent optimization in a smooth neural network framework.

Result: NeuroLifting matches Toulbar2 in quality for moderate scales and surpasses all baselines for large-scale MRFs, with linear computational growth.

Conclusion: NeuroLifting advances MRF inference by providing a scalable, efficient, and high-quality solution for large-scale problems.

Abstract: Inference in large-scale Markov Random Fields (MRFs) is a critical yet
challenging task, traditionally approached through approximate methods like
belief propagation and mean field, or exact methods such as the Toulbar2
solver. These strategies often fail to strike an optimal balance between
efficiency and solution quality, particularly as the problem scale increases.
This paper introduces NeuroLifting, a novel technique that leverages Graph
Neural Networks (GNNs) to reparameterize decision variables in MRFs,
facilitating the use of standard gradient descent optimization. By extending
traditional lifting techniques into a non-parametric neural network framework,
NeuroLifting benefits from the smooth loss landscape of neural networks,
enabling efficient and parallelizable optimization. Empirical results
demonstrate that, on moderate scales, NeuroLifting performs very close to the
exact solver Toulbar2 in terms of solution quality, significantly surpassing
existing approximate methods. Notably, on large-scale MRFs, NeuroLifting
delivers superior solution quality against all baselines, as well as exhibiting
linear computational complexity growth. This work presents a significant
advancement in MRF inference, offering a scalable and effective solution for
large-scale problems.

</details>


### [453] [Leveraging Large Language Models for Effective Label-free Node Classification in Text-Attributed Graphs](https://arxiv.org/pdf/2412.11983)
*Taiyan Zhang, Renchi Yang, Yurui Lai, Mingyu Yan, Xiaochun Ye, Dongrui Fan*

Main category: cs.LG

TL;DR: Locle is an active self-training framework for label-free node classification using LLMs and GNNs, outperforming state-of-the-art methods with minimal cost.


<details>
  <summary>Details</summary>
Motivation: GNNs require large labeled datasets, which are costly. LLMs offer zero-shot capabilities but face challenges like high query costs or noisy labels.

Method: Locle iteratively selects critical nodes, generates pseudo-labels using LLMs and GNNs, and refines labels with a rewired topology.

Result: Locle achieves an 8.08% accuracy improvement on the DBLP dataset with minimal cost.

Conclusion: Locle effectively combines LLMs and GNNs for cost-efficient, label-free node classification.

Abstract: Graph neural networks (GNNs) have become the preferred models for node
classification in graph data due to their robust capabilities in integrating
graph structures and attributes. However, these models heavily depend on a
substantial amount of high-quality labeled data for training, which is often
costly to obtain. With the rise of large language models (LLMs), a promising
approach is to utilize their exceptional zero-shot capabilities and extensive
knowledge for node labeling. Despite encouraging results, this approach either
requires numerous queries to LLMs or suffers from reduced performance due to
noisy labels generated by LLMs. To address these challenges, we introduce
Locle, an active self-training framework that does Label-free node
Classification with LLMs cost-Effectively. Locle iteratively identifies small
sets of "critical" samples using GNNs and extracts informative pseudo-labels
for them with both LLMs and GNNs, serving as additional supervision signals to
enhance model training. Specifically, Locle comprises three key components: (i)
an effective active node selection strategy for initial annotations; (ii) a
careful sample selection scheme to identify "critical" nodes based on label
disharmonicity and entropy; and (iii) a label refinement module that combines
LLMs and GNNs with a rewired topology. Extensive experiments on five benchmark
text-attributed graph datasets demonstrate that Locle significantly outperforms
state-of-the-art methods under the same query budget to LLMs in terms of
label-free node classification. Notably, on the DBLP dataset with 14.3k nodes,
Locle achieves an 8.08% improvement in accuracy over the state-of-the-art at a
cost of less than one cent. Our code is available at
https://github.com/HKBU-LAGAS/Locle.

</details>


### [454] [EXAdam: The Power of Adaptive Cross-Moments](https://arxiv.org/pdf/2412.20302)
*Ahmed M. Adly*

Main category: cs.LG

TL;DR: EXAdam is an enhanced version of Adam with debiasing terms and gradient-based acceleration, showing faster convergence and better accuracy in tests.


<details>
  <summary>Details</summary>
Motivation: To address limitations of Adam by improving moment estimation and responsiveness to the loss landscape.

Method: Introduces debiasing terms and a gradient-based acceleration mechanism.

Result: 38.46% faster convergence and accuracy improvements of 1.96%, 2.17%, and 1.17% in training, validation, and testing.

Conclusion: EXAdam is a promising advancement in adaptive optimization, though further validation is needed.

Abstract: This paper introduces EXAdam ($\textbf{EX}$tended $\textbf{Adam}$), a novel
optimization algorithm that builds upon the widely-used Adam optimizer. EXAdam
incorporates two key enhancements: (1) new debiasing terms for improved moment
estimation and (2) a gradient-based acceleration mechanism for increased
responsiveness to the current loss landscape. These innovations work
synergistically to address limitations of the original Adam algorithm,
potentially offering improved convergence properties, enhanced ability to
escape saddle points, and potentially greater robustness to hyperparameter
choices, though this requires further investigation. We provide a theoretical
analysis of EXAdam's components and their interactions, highlighting the
algorithm's potential advantages in navigating complex optimization landscapes.
Empirical evaluations demonstrate EXAdam's superiority over Adam, achieving
38.46% faster convergence and yielding improvements of 1.96%, 2.17%, and 1.17%
in training, validation, and testing accuracies, respectively, when applied to
a CNN trained on the CIFAR-10 dataset. While these results are promising,
further empirical validation across diverse tasks is essential to fully gauge
EXAdam's efficacy. Nevertheless, EXAdam represents a significant advancement in
adaptive optimization techniques, with promising implications for a wide range
of machine learning applications. This work aims to contribute to the ongoing
development of more efficient, adaptive, and universally applicable
optimization methods in the field of machine learning and artificial
intelligence.

</details>


### [455] [Understanding Why Adam Outperforms SGD: Gradient Heterogeneity in Transformers](https://arxiv.org/pdf/2502.00213)
*Akiyoshi Tomihari, Issei Sato*

Main category: cs.LG

TL;DR: The paper explores why Adam outperforms SGD in optimizing transformers, attributing it to gradient heterogeneity and its impact on optimization.


<details>
  <summary>Details</summary>
Motivation: To understand why adaptive optimizers like Adam perform better than SGD for transformers, focusing on gradient heterogeneity.

Method: Analyzes gradient heterogeneity, its effect on optimization, and its relationship with layer normalization in transformers. Experiments validate findings in NLP and vision tasks.

Result: Gradient heterogeneity hinders SGD but affects sign-based optimization (like Adam) less. Layer normalization placement influences heterogeneity.

Conclusion: Provides insights into transformer optimization challenges and guidance for future optimizer design.

Abstract: Transformers are challenging to optimize with SGD and typically require
adaptive optimizers such as Adam. However, the reasons behind the superior
performance of Adam over SGD remain unclear. In this study, we investigate the
optimization of transformers by focusing on gradient heterogeneity, defined as
the disparity in gradient norms among parameters. Our analysis shows that
gradient heterogeneity hinders gradient-based optimization, including SGD,
while sign-based optimization, a simplified variant of Adam, is less affected.
We further examine gradient heterogeneity in transformers and show that it is
influenced by the placement of layer normalization. Experimental results from
fine-tuning transformers in both NLP and vision domains validate our
theoretical analyses. This study provides insights into the optimization
challenges of transformers and offers guidance for designing future
optimization algorithms. Code is available at
https://github.com/tom4649/gradient-heterogeneity.

</details>


### [456] [Exploratory Diffusion Model for Unsupervised Reinforcement Learning](https://arxiv.org/pdf/2502.07279)
*Chengyang Ying, Huayu Chen, Xinning Zhou, Zhongkai Hao, Hang Su, Jun Zhu*

Main category: cs.LG

TL;DR: ExDM leverages diffusion models for unsupervised reinforcement learning, improving exploration and downstream task adaptation.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with heterogeneous explored data, requiring strong representation abilities. ExDM addresses this by using diffusion models.

Method: ExDM fits explored data with a diffusion model, introduces a score-based intrinsic reward, and fine-tunes policies for downstream tasks.

Result: ExDM outperforms SOTA baselines in exploration and downstream task adaptation, especially in complex environments.

Conclusion: ExDM effectively combines diffusion models with reinforcement learning for superior unsupervised exploration and task adaptation.

Abstract: Unsupervised reinforcement learning (URL) aims to pre-train agents by
exploring diverse states or skills in reward-free environments, facilitating
efficient adaptation to downstream tasks. As the agent cannot access extrinsic
rewards during unsupervised exploration, existing methods design intrinsic
rewards to model the explored data and encourage further exploration. However,
the explored data are always heterogeneous, posing the requirements of powerful
representation abilities for both intrinsic reward models and pre-trained
policies. In this work, we propose the Exploratory Diffusion Model (ExDM),
which leverages the strong expressive ability of diffusion models to fit the
explored data, simultaneously boosting exploration and providing an efficient
initialization for downstream tasks. Specifically, ExDM can accurately estimate
the distribution of collected data in the replay buffer with the diffusion
model and introduces the score-based intrinsic reward, encouraging the agent to
explore less-visited states. After obtaining the pre-trained policies, ExDM
enables rapid adaptation to downstream tasks. In detail, we provide theoretical
analyses and practical algorithms for fine-tuning diffusion policies,
addressing key challenges such as training instability and computational
complexity caused by multi-step sampling. Extensive experiments demonstrate
that ExDM outperforms existing SOTA baselines in efficient unsupervised
exploration and fast fine-tuning downstream tasks, especially in structurally
complicated environments.

</details>


### [457] [Model Selection for Off-policy Evaluation: New Algorithms and Experimental Protocol](https://arxiv.org/pdf/2502.08021)
*Pai Liu, Lingfeng Zhao, Shivangi Agarwal, Jinghan Liu, Audrey Huang, Philip Amortila, Nan Jiang*

Main category: cs.LG

TL;DR: The paper addresses hyperparameter tuning for off-policy evaluation (OPE) in offline RL, proposing new model-free and model-based selectors and a stable experimental protocol.


<details>
  <summary>Details</summary>
Motivation: The challenge of hyperparameter tuning in OPE for offline RL, which lacks robust solutions due to high variance or dependency on hyperparameters.

Method: Develops new model-free and model-based selectors with theoretical guarantees and introduces a new experimental protocol for stable evaluation.

Result: The proposed model-free selector, LSTD-Tournament, shows promising performance in Gym-Hopper experiments.

Conclusion: The work advances OPE in offline RL by improving hyperparameter tuning and evaluation stability.

Abstract: Holdout validation and hyperparameter tuning from data is a long-standing
problem in offline reinforcement learning (RL). A standard framework is to use
off-policy evaluation (OPE) methods to evaluate and select the policies, but
OPE either incurs exponential variance (e.g., importance sampling) or has
hyperparameters on their own (e.g., FQE and model-based). In this work we focus
on hyperparameter tuning for OPE itself, which is even more under-investigated.
Concretely, we select among candidate value functions ("model-free") or
dynamics ("model-based") to best assess the performance of a target policy. We
develop: (1) new model-free and model-based selectors with theoretical
guarantees, and (2) a new experimental protocol for empirically evaluating
them. Compared to the model-free protocol in prior works, our new protocol
allows for more stable generation and better control of candidate value
functions in an optimization-free manner, and evaluation of model-free and
model-based methods alike. We exemplify the protocol on Gym-Hopper, and find
that our new model-free selector, LSTD-Tournament, demonstrates promising
empirical performance.

</details>


### [458] [TANTE: Time-Adaptive Operator Learning via Neural Taylor Expansion](https://arxiv.org/pdf/2502.08574)
*Zhikai Wu, Sifan Wang, Shiyang Zhang, Sizhuang He, Min Zhu, Anran Jiao, Lu Lu, David van Dijk*

Main category: cs.LG

TL;DR: TANTE is a novel operator-learning framework for time-dependent PDEs, using adaptive step sizes and neural Taylor expansion to improve accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for operator learning in PDEs use fixed time steps, leading to error accumulation and inefficiency in handling varying temporal complexity.

Method: TANTE employs neural networks to learn higher-order temporal derivatives and local convergence radii, enabling dynamic step-size adjustment during rollout.

Result: TANTE outperforms fixed-step methods, achieving 10-50% accuracy gains and 30-80% speed-ups in inference.

Conclusion: TANTE offers a robust solution for adaptive operator learning in PDEs, balancing accuracy and computational efficiency.

Abstract: Operator learning for time-dependent partial differential equations (PDEs)
has seen rapid progress in recent years, enabling efficient approximation of
complex spatiotemporal dynamics. However, most existing methods rely on fixed
time step sizes during rollout, which limits their ability to adapt to varying
temporal complexity and often leads to error accumulation. To address this gap,
we propose the Time-Adaptive Transformer with Neural Taylor Expansion (TANTE),
a novel operator-learning framework that produces continuous-time predictions
with adaptive step sizes. TANTE predicts future states by performing a Taylor
expansion at the current state, where neural networks learn both the
higher-order temporal derivatives and the local radius of convergence. This
allows the model to dynamically adjust its rollout based on the local behavior
of the solution, thereby reducing cumulative error and improving computational
efficiency. We demonstrate the effectiveness of TANTE across a wide range of
PDE benchmarks, achieving superior accuracy and adaptability compared to
fixed-step baselines, delivering accuracy gains of 10-50 % and speed-ups of
30-80 % at inference.

</details>


### [459] [A Radon-Nikod√Ωm Perspective on Anomaly Detection: Theory and Implications](https://arxiv.org/pdf/2502.18002)
*Shlok Mehendale, Aditya Challa, Rahul Yedida, Sravan Danda, Santonu Sarkar, Snehanshu Saha*

Main category: cs.LG

TL;DR: The paper introduces RN-Loss, a loss function for anomaly detection based on the Radon-Nikod√Ωm theorem, improving performance across datasets.


<details>
  <summary>Details</summary>
Motivation: To enhance anomaly detection by leveraging the Radon-Nikod√Ωm theorem for loss function design.

Method: Multiply the vanilla loss function with the Radon-Nikod√Ωm derivative, adapting it for supervised and unsupervised settings.

Result: RN-Loss outperforms state-of-the-art methods on 68% of multivariate and 72% of univariate datasets.

Conclusion: RN-Loss, grounded in measure theory, significantly improves anomaly detection performance.

Abstract: Which principle underpins the design of an effective anomaly detection loss
function? The answer lies in the concept of Radon-Nikod\'ym theorem, a
fundamental concept in measure theory. The key insight from this article is:
Multiplying the vanilla loss function with the Radon-Nikod\'ym derivative
improves the performance across the board. We refer to this as RN-Loss. We
prove this using the setting of PAC (Probably Approximately Correct)
learnability.
  Depending on the context a Radon-Nikod\'ym derivative takes different forms.
In the simplest case of supervised anomaly detection, Radon-Nikod\'ym
derivative takes the form of a simple weighted loss. In the case of
unsupervised anomaly detection (with distributional assumptions),
Radon-Nikod\'ym derivative takes the form of the popular cluster based local
outlier factor. We evaluate our algorithm on 96 datasets, including univariate
and multivariate data from diverse domains, including healthcare,
cybersecurity, and finance. We show that RN-Derivative algorithms outperform
state-of-the-art methods on 68% of Multivariate datasets (based on F1 scores)
and also achieves peak F1-scores on 72% of time series (Univariate) datasets.

</details>


### [460] [Rethinking Weight-Averaged Model-merging](https://arxiv.org/pdf/2411.09263)
*Hu Wang, Congbo Ma, Ibrahim Almakky, Ian Reid, Gustavo Carneiro, Mohammad Yaqub*

Main category: cs.LG

TL;DR: The paper explores why weight-averaged model-merging works in deep learning, analyzing intrinsic weight patterns, comparing weight vs. feature averaging, and examining prediction stability.


<details>
  <summary>Details</summary>
Motivation: To understand the underlying mechanisms of model-merging, which enhances performance without training, but lacks clear explanations.

Method: Investigates model-merging from three perspectives: intrinsic weight patterns, weight vs. feature averaging, and prediction stability across parameter scales.

Result: Provides empirical insights into weight-averaged model-merging, connecting weight structures to its effectiveness and showing robustness across parameter scales.

Conclusion: The study deepens understanding of model-merging, revealing its regularization-like effects and practical implications for deep learning.

Abstract: Model-merging has emerged as a powerful approach in deep learning, capable of
enhancing model performance without any training. However, the underlying
mechanisms that explain its effectiveness remain largely unexplored. In this
paper, we investigate this technique from three novel perspectives to
empirically provide deeper insights into why and how weight-averaged
model-merging~\cite{wortsman2022soups} works: (1) we examine the intrinsic
patterns captured by the learning of the model weights, and we are the first to
connect that these weights encode structured with why weight-averaged model
merging can work; (2) we investigate averaging on weights versus averaging on
features, providing analyses from the view of diverse architecture comparisons
on multiple datasets; and (3) we explore the impact on model-merging prediction
stability in terms of changing the parameter magnitude, revealing insights into
the way of weight averaging works as regularization by showing the robustness
across different parameter scales. The code is available at
https://github.com/billhhh/Rethink-Merge.

</details>


### [461] [Ensuring Safety in an Uncertain Environment: Constrained MDPs via Stochastic Thresholds](https://arxiv.org/pdf/2504.04973)
*Qian Zuo, Fengxiang He*

Main category: cs.LG

TL;DR: The paper introduces SPOT, a model-based primal-dual algorithm for CMDPs with stochastic thresholds, ensuring safety in uncertain environments. It achieves sublinear regret and constraint violation.


<details>
  <summary>Details</summary>
Motivation: To address safety in reinforcement learning under unknown and uncertain environments with stochastic thresholds.

Method: Uses a Growing-Window estimator and designs SPOT, a primal-dual algorithm for multiple constraints against stochastic thresholds.

Result: Achieves sublinear regret ($\tilde{\mathcal{O}}(\sqrt{T})$) and constraint violation ($\tilde{\mathcal{O}}(\sqrt{T})$).

Conclusion: SPOT is the first RL algorithm with theoretical guarantees for uncertain environments with unknown thresholds.

Abstract: This paper studies constrained Markov decision processes (CMDPs) with
constraints against stochastic thresholds, aiming at the safety of
reinforcement learning in unknown and uncertain environments. We leverage a
Growing-Window estimator sampling from interactions with the uncertain and
dynamic environment to estimate the thresholds, based on which we design
Stochastic Pessimistic-Optimistic Thresholding (SPOT), a novel model-based
primal-dual algorithm for multiple constraints against stochastic thresholds.
SPOT enables reinforcement learning under both pessimistic and optimistic
threshold settings. We prove that our algorithm achieves sublinear regret and
constraint violation; i.e., a reward regret of $\tilde{\mathcal{O}}(\sqrt{T})$
while allowing an $\tilde{\mathcal{O}}(\sqrt{T})$ constraint violation over $T$
episodes. The theoretical guarantees show that our algorithm achieves
performance comparable to that of an approach relying on fixed and clear
thresholds. To the best of our knowledge, SPOT is the first reinforcement
learning algorithm that realises theoretical guaranteed performance in an
uncertain environment where even thresholds are unknown.

</details>


### [462] [Are We Truly Forgetting? A Critical Re-examination of Machine Unlearning Evaluation Protocols](https://arxiv.org/pdf/2503.06991)
*Yongwoo Kim, Sungmin Cha, Donghyun Kim*

Main category: cs.LG

TL;DR: The paper critiques current machine unlearning evaluations for relying too much on logit-based metrics and introduces a comprehensive, representation-based evaluation under large-scale scenarios to ensure genuine data removal.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of existing unlearning evaluations, which may falsely assure security by focusing on small-scale, logit-based metrics, and propose a more rigorous assessment from a representation perspective.

Method: Conduct a new evaluation using representation-based metrics under large-scale scenarios, focusing on whether unlearning truly removes targeted data from the model's representations.

Result: Current unlearning methods either degrade model quality or only modify the classifier, maintaining representational similarity to the original model despite superior logit-based metrics.

Conclusion: The paper proposes a standardized, rigorous evaluation setup for unlearning algorithms, emphasizing representation divergence to ensure realistic and effective unlearning.

Abstract: Machine unlearning is a process to remove specific data points from a trained
model while maintaining the performance on retain data, addressing privacy or
legal requirements. Despite its importance, existing unlearning evaluations
tend to focus on logit-based metrics (i.e., accuracy) under small-scale
scenarios. We observe that this could lead to a false sense of security in
unlearning approaches under real-world scenarios. In this paper, we conduct a
new comprehensive evaluation that employs representation-based evaluations of
the unlearned model under large-scale scenarios to verify whether the
unlearning approaches genuinely eliminate the targeted forget data from the
model's representation perspective. Our analysis reveals that current
state-of-the-art unlearning approaches either completely degrade the
representational quality of the unlearned model or merely modify the classifier
(i.e., the last layer), thereby achieving superior logit-based evaluation
metrics while maintaining significant representational similarity to the
original model. Furthermore, we introduce a rigorous unlearning evaluation
setup, in which the forgetting classes exhibit semantic similarity to
downstream task classes, necessitating that feature representations diverge
significantly from those of the original model, thus enabling a more rigorous
evaluation from a representation perspective. We hope our benchmark serves as a
standardized protocol for evaluating unlearning algorithms under realistic
conditions.

</details>


### [463] [TwinTURBO: Semi-Supervised Fine-Tuning of Foundation Models via Mutual Information Decompositions for Downstream Task and Latent Spaces](https://arxiv.org/pdf/2503.07851)
*Guillaume Qu√©tant, Pavlo Molchanov, Slava Voloshynovskiy*

Main category: cs.LG

TL;DR: A semi-supervised fine-tuning framework for foundation models uses mutual information decomposition to improve performance with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of training with scarce labeled data by leveraging unlabeled data effectively.

Method: Derives two lower bounds: one for downstream tasks (e.g., classification) using cross-entropy and KL divergence, and another for latent space representation with contrastive-like decomposition. Uses a specialized projector module to retain pre-trained structure.

Result: Significant improvements in classification tasks under low-labeled conditions.

Conclusion: The framework effectively leverages unlabeled data to enhance performance in scenarios with limited labeled data.

Abstract: We present a semi-supervised fine-tuning framework for foundation models that
utilises mutual information decomposition to address the challenges of training
for a limited amount of labelled data. Our approach derives two distinct lower
bounds: i) for the downstream task space, such as classification, optimised
using conditional and marginal cross-entropy alongside Kullback-Leibler
divergence, and ii) for the latent space representation, regularised and
aligned using a contrastive-like decomposition. This fine-tuning strategy
retains the pre-trained structure of the foundation model, modifying only a
specialised projector module comprising a small transformer and a token
aggregation technique. Experiments on several datasets demonstrate significant
improvements in classification tasks under extremely low-labelled conditions by
effectively leveraging unlabelled data.

</details>


### [464] [TimeCapsule: Solving the Jigsaw Puzzle of Long-Term Time Series Forecasting with Compressed Predictive Representations](https://arxiv.org/pdf/2504.12721)
*Yihang Lu, Yangyang Xu, Qitao Qing, Xianwei Meng*

Main category: cs.LG

TL;DR: TimeCapsule simplifies LTSF by unifying redundancy reduction and multi-scale modeling in a 3D tensor framework, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Complex LTSF models are often outperformed by simpler ones; the paper aims to streamline advanced techniques for efficiency.

Method: Introduces TimeCapsule, a model using 3D tensors and mode production for multi-mode dependencies and dimensionality compression, with JEPA for predictive representation.

Result: TimeCapsule achieves state-of-the-art performance on benchmarks.

Conclusion: Simplified yet generalized frameworks like TimeCapsule can outperform complex LTSF models.

Abstract: Recent deep learning models for Long-term Time Series Forecasting (LTSF)
often emphasize complex, handcrafted designs, while simpler architectures like
linear models or MLPs have often outperformed these intricate solutions. In
this paper, we revisit and organize the core ideas behind several key
techniques, such as redundancy reduction and multi-scale modeling, which are
frequently employed in advanced LTSF models. Our goal is to streamline these
ideas for more efficient deep learning utilization. To this end, we introduce
TimeCapsule, a model built around the principle of high-dimensional information
compression that unifies these techniques in a generalized yet simplified
framework. Specifically, we model time series as a 3D tensor, incorporating
temporal, variate, and level dimensions, and leverage mode production to
capture multi-mode dependencies while achieving dimensionality compression. We
propose an internal forecast within the compressed representation domain,
supported by the Joint-Embedding Predictive Architecture (JEPA), to monitor the
learning of predictive representations. Extensive experiments on challenging
benchmarks demonstrate the versatility of our method, showing that TimeCapsule
can achieve state-of-the-art performance.

</details>


### [465] [GEOM-Drugs Revisited: Toward More Chemically Accurate Benchmarks for 3D Molecule Generation](https://arxiv.org/pdf/2505.00169)
*Filipp Nikitin, Ian Dunn, David Ryan Koes, Olexandr Isayev*

Main category: cs.LG

TL;DR: The paper addresses flaws in evaluating 3D molecular generation models using GEOM-Drugs, proposing a corrected framework with accurate valency tables and GFN2-xTB benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current evaluation protocols for 3D molecular generation models are flawed, leading to inaccurate assessments.

Method: The authors fix data preprocessing issues, create accurate valency tables, and introduce a GFN2-xTB-based benchmark. They retrain and re-evaluate leading models.

Result: Updated performance metrics highlight the importance of chemically rigorous evaluation.

Conclusion: The paper advocates for improved evaluation practices in 3D molecular generation and provides tools for future benchmarking.

Abstract: Deep generative models have shown significant promise in generating valid 3D
molecular structures, with the GEOM-Drugs dataset serving as a key benchmark.
However, current evaluation protocols suffer from critical flaws, including
incorrect valency definitions, bugs in bond order calculations, and reliance on
force fields inconsistent with the reference data. In this work, we revisit
GEOM-Drugs and propose a corrected evaluation framework: we identify and fix
issues in data preprocessing, construct chemically accurate valency tables, and
introduce a GFN2-xTB-based geometry and energy benchmark. We retrain and
re-evaluate several leading models under this framework, providing updated
performance metrics and practical recommendations for future benchmarking. Our
results underscore the need for chemically rigorous evaluation practices in 3D
molecular generation. Our recommended evaluation methods and GEOM-Drugs
processing scripts are available at
https://github.com/isayevlab/geom-drugs-3dgen-evaluation.

</details>


### [466] [FairPO: Robust Preference Optimization for Fair Multi-Label Learning](https://arxiv.org/pdf/2505.02433)
*Soumen Kumar Mondal, Akshit Varmora, Prateek Chanda, Ganesh Ramakrishnan*

Main category: cs.LG

TL;DR: FairPO is a framework for fair multi-label classification by optimizing preference signals with group robustness, dynamically adjusting training to mitigate bias.


<details>
  <summary>Details</summary>
Motivation: To address fairness in multi-label classification by differentiating privileged and non-privileged label groups and reducing bias.

Method: Uses a preference-based loss (inspired by DPO) for robust optimization over label groups, dynamically adjusting training emphasis.

Result: Mitigates bias while preserving baseline performance for non-privileged labels.

Conclusion: FairPO effectively promotes fairness and plans to explore alternative loss formulations and extend to multilabel generation.

Abstract: We propose FairPO, a novel framework designed to promote fairness in
multi-label classification by directly optimizing preference signals with a
group robustness perspective. In our framework, the set of labels is
partitioned into privileged and non-privileged groups, and a preference-based
loss inspired by Direct Preference Optimization (DPO) is employed to more
effectively differentiate true positive labels from confusing negatives within
the privileged group, while preserving baseline classification performance for
non-privileged labels. By framing the learning problem as a robust optimization
over groups, our approach dynamically adjusts the training emphasis toward
groups with poorer performance, thereby mitigating bias and ensuring a fairer
treatment across diverse label categories. In addition, we outline plans to
extend this approach by investigating alternative loss formulations such as
Simple Preference Optimisation (SimPO) and Contrastive Preference Optimization
(CPO) to exploit reference-free reward formulations and contrastive training
signals. Furthermore, we plan to extend FairPO with multilabel generation
capabilities, enabling the model to dynamically generate diverse and coherent
label sets for ambiguous inputs.

</details>


### [467] [Using Distance Correlation for Efficient Bayesian Optimization](https://arxiv.org/pdf/2102.08993)
*Takuya Kanazawa*

Main category: cs.LG

TL;DR: BDC integrates Bayesian optimization with Distance Correlation for efficient black-box function optimization, performing comparably to popular methods without manual tuning.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of expensive data collection in black-box function optimization, such as hyperparameter tuning in AI models.

Method: Proposes BDC, a Bayesian optimization scheme using Distance Correlation to balance exploration and exploitation automatically.

Result: BDC performs on par with established methods like expected improvement and max-value entropy search in benchmarks and real-world applications.

Conclusion: BDC is a viable and efficient alternative for black-box optimization, requiring no manual hyperparameter tuning.

Abstract: The need to collect data via expensive measurements of black-box functions is
prevalent across science, engineering and medicine. As an example,
hyperparameter tuning of a large AI model is critical to its predictive
performance but is generally time-consuming and unwieldy. Bayesian optimization
(BO) is a collection of methods that aim to address this issue by means of
Bayesian statistical inference. In this work, we put forward a BO scheme named
BDC, which integrates BO with a statistical measure of association of two
random variables called Distance Correlation. BDC balances exploration and
exploitation automatically, and requires no manual hyperparameter tuning. We
evaluate BDC on a range of benchmark tests and observe that it performs on per
with popular BO methods such as the expected improvement and max-value entropy
search. We also apply BDC to optimization of sequential integral observations
of an unknown terrain and confirm its utility.

</details>


### [468] [Focus on the Likely: Test-time Instance-based Uncertainty Removal](https://arxiv.org/pdf/2505.03819)
*Johannes Schneider*

Main category: cs.LG

TL;DR: Focusing on likely classes improves model predictions via test-time fine-tuning methods, achieving accuracy gains by refining uncertain predictions.


<details>
  <summary>Details</summary>
Motivation: To enhance model predictions by refining uncertain outcomes and aligning them with plausible classes.

Method: Two novel test-time fine-tuning methods: focusing on likely classes and applying a gradient descent step with a large learning rate.

Result: Accuracy gains demonstrated in diverse text and image domain models, particularly with shared feature emphasis.

Conclusion: Refining predictions by focusing on likely classes is effective, with theoretical and empirical support.

Abstract: We ask: Does focusing on classes predicted as likely improve model
predictions? We aim for an affirmative answer by proposing two novel test-time
fine-tuning methods to improve uncertain model predictions. Instead of greedily
selecting the most likely class, we introduce an additional step, \emph{focus
on the likely classes}, to refine predictions. By applying a theoretically
motivated single gradient descent step with a large learning rate, we refine
predictions when an initial forward pass indicates high uncertainty. This
aligns predictions more closely with the ideal of assigning zero probability to
less plausible outcomes. The experimental evaluation demonstrates accuracy
gains for one of our methods, which emphasizes shared features among likely
classes, across diverse text and image domain models. %Our theoretical
discussion provides a deeper understanding, highlighting the varying impact of
shared and non-shared features among (focus) classes. %Our discussion also
suggests an interesting view on standard, offline training vs. test-time
training: Opposing optimization rationales regarding breadth of feature
dependence are preferable during each training phase.

</details>


### [469] [Practitioner Motives to Use Different Hyperparameter Optimization Methods](https://arxiv.org/pdf/2203.01717)
*Niclas Kannengie√üer, Niklas Hasebrook, Felix Morsbach, Marc-Andr√© Z√∂ller, J√∂rg Franke, Marius Lindauer, Frank Hutter, Ali Sunyaev*

Main category: cs.LG

TL;DR: The paper explores why practitioners prefer less efficient HPO methods like grid search over more efficient ones like Bayesian optimization, identifying motives and contextual factors through interviews and surveys.


<details>
  <summary>Details</summary>
Motivation: Practitioners often use inefficient HPO methods despite better alternatives, but their motives remain unclear. Understanding these motives can improve HPO tool design.

Method: Conducted 20 semi-structured interviews and an online survey with 49 ML experts to analyze motives and contextual factors influencing HPO method choice.

Result: Identified main goals (e.g., model understanding) and contextual factors (e.g., resources) affecting HPO method selection, providing insights for user-centered HPO tool development.

Conclusion: The study offers a foundation for designing context-adaptive and user-centered HPO tools in automated ML by clarifying practitioner motives.

Abstract: Programmatic hyperparameter optimization (HPO) methods, such as Bayesian
optimization and evolutionary algorithms, are highly sample-efficient in
identifying optimal hyperparameter configurations for machine learning (ML)
models. However, practitioners frequently use less efficient methods, such as
grid search, which can lead to under-optimized models. We suspect this behavior
is driven by a range of practitioner-specific motives. Practitioner motives,
however, still need to be clarified to enhance user-centered development of HPO
tools. To uncover practitioner motives to use different HPO methods, we
conducted 20 semi-structured interviews and an online survey with 49 ML
experts. By presenting main goals (e.g., increase ML model understanding) and
contextual factors affecting practitioners' selection of HPO methods (e.g.,
available computer resources), this study offers a conceptual foundation to
better understand why practitioners use different HPO methods, supporting
development of more user-centered and context-adaptive HPO tools in automated
ML.

</details>


### [470] [Review of Extreme Multilabel Classification](https://arxiv.org/pdf/2302.05971)
*Arpan Dasgupta, Preeti Lamba, Ankita Kushwaha, Kiran Ravish, Siddhant Katyan, Shrutimoy Das, Pawan Kumar*

Main category: cs.LG

TL;DR: XMLC addresses multi-label classification with an extremely large number of labels, requiring scalable methods beyond traditional approaches like one-versus-all. Techniques include embedding, handling tail labels, and diverse methods like deep learning, SVD, and clustering.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in scaling classification for extreme label counts, where traditional methods fail, and addressing imbalances like tail labels.

Method: Common approaches involve embedding labels and features into lower-dimensional spaces, using techniques like deep learning, SVD, clustering, and hashing.

Result: The field has developed metrics to evaluate performance, especially for tail labels, ensuring balanced prediction accuracy.

Conclusion: XMLC is a dynamic area with diverse solutions, focusing on scalability and handling label imbalances effectively.

Abstract: Extreme multi-label classification or XMLC, is an active area of interest in
machine learning. Compared to traditional multi-label classification, here the
number of labels is extremely large, hence, the name extreme multi-label
classification. Using classical one-versus-all classification does not scale in
this case due to large number of labels; the same is true for any other
classifier. Embedding labels and features into a lower-dimensional space is a
common first step in many XMLC methods. Moreover, other issues include
existence of head and tail labels, where tail labels are those that occur in a
relatively small number of samples. The existence of tail labels creates issues
during embedding. This area has invited application of wide range of approaches
ranging from bit compression motivated from compressed sensing, tree based
embeddings, deep learning based latent space embedding including using
attention weights, linear algebra based embeddings such as SVD, clustering,
hashing, to name a few. The community has come up with a useful set of metrics
to identify correctly the prediction for head or tail labels.

</details>


### [471] [Spectral alignment of stochastic gradient descent for high-dimensional classification tasks](https://arxiv.org/pdf/2310.03010)
*Gerard Ben Arous, Reza Gheissari, Jiaoyang Huang, Aukosh Jagannath*

Main category: cs.LG

TL;DR: The paper explores the alignment between SGD training dynamics and the spectra of Hessian/gradient matrices in neural networks, confirming predictions from numerical studies.


<details>
  <summary>Details</summary>
Motivation: To rigorously study the connection between SGD dynamics and the spectral properties of Hessian and gradient matrices in neural networks, addressing long-standing predictions.

Method: Analyzes SGD trajectories and outlier eigenspaces in multi-class high-dimensional mixtures and 1 or 2-layer neural networks, extending to multi-layer settings.

Result: SGD trajectories and outlier eigenspaces align with a low-dimensional subspace, with layer-specific alignment and evolving final-layer eigenspace in multi-layer networks.

Conclusion: The study validates predictions about Hessian and gradient spectra in overparametrized networks, highlighting their role in training dynamics.

Abstract: We rigorously study the relation between the training dynamics via stochastic
gradient descent (SGD) and the spectra of empirical Hessian and gradient
matrices. We prove that in two canonical classification tasks for multi-class
high-dimensional mixtures and either 1 or 2-layer neural networks, both the SGD
trajectory and emergent outlier eigenspaces of the Hessian and gradient
matrices align with a common low-dimensional subspace. Moreover, in multi-layer
settings this alignment occurs per layer, with the final layer's outlier
eigenspace evolving over the course of training, and exhibiting rank deficiency
when the SGD converges to sub-optimal classifiers. This establishes some of the
rich predictions that have arisen from extensive numerical studies in the last
decade about the spectra of Hessian and information matrices over the course of
training in overparametrized networks.

</details>


### [472] [Prototype Augmented Hypernetworks for Continual Learning](https://arxiv.org/pdf/2505.07450)
*Neil De La Fuente, Maria Pilligua, Daniel Vidal, Albin Soutiff, Cecilia Curreli, Daniel Cremers, Andrey Barsky*

Main category: cs.LG

TL;DR: PAH uses hypernetworks and task prototypes to prevent catastrophic forgetting in continual learning, achieving high accuracy with minimal forgetting.


<details>
  <summary>Details</summary>
Motivation: Address catastrophic forgetting in continual learning by dynamically generating task-specific classifiers without storing past data.

Method: Prototype-Augmented Hypernetworks (PAH) with dual distillation losses for logit and prototype alignment.

Result: 74.5% accuracy on Split-CIFAR100 and 63.7% on TinyImageNet with low forgetting rates (1.7% and 4.4%).

Conclusion: PAH outperforms prior methods by effectively mitigating forgetting while maintaining high accuracy.

Abstract: Continual learning (CL) aims to learn a sequence of tasks without forgetting
prior knowledge, but gradient updates for a new task often overwrite the
weights learned earlier, causing catastrophic forgetting (CF). We propose
Prototype-Augmented Hypernetworks (PAH), a framework where a single
hypernetwork, conditioned on learnable task prototypes, dynamically generates
task-specific classifier heads on demand. To mitigate forgetting, PAH combines
cross-entropy with dual distillation losses, one to align logits and another to
align prototypes, ensuring stable feature representations across tasks.
Evaluations on Split-CIFAR100 and TinyImageNet demonstrate that PAH achieves
state-of-the-art performance, reaching 74.5 % and 63.7 % accuracy with only 1.7
% and 4.4 % forgetting, respectively, surpassing prior methods without storing
samples or heads.

</details>


### [473] [Exploring Federated Unlearning: Review, Comparison, and Insights](https://arxiv.org/pdf/2310.19218)
*Yang Zhao, Jiaxi Yang, Yiling Tao, Lixu Wang, Xiaoxiao Li, Dusit Niyato, H. Vincent Poor*

Main category: cs.LG

TL;DR: This paper analyzes federated unlearning methods, balancing privacy, accuracy, and efficiency, and introduces the OpenFederatedUnlearning framework for evaluation.


<details>
  <summary>Details</summary>
Motivation: Address the growing need for privacy-preserving machine learning by enabling selective data removal in federated systems.

Method: Comprehensive analysis of existing federated unlearning approaches, focusing on algorithmic efficiency, model accuracy, and privacy preservation.

Result: Identifies key trade-offs and proposes OpenFederatedUnlearning, a unified benchmark for evaluating methods.

Conclusion: Provides insights for practitioners and outlines future research directions to advance federated unlearning.

Abstract: The increasing demand for privacy-preserving machine learning has spurred
interest in federated unlearning, which enables the selective removal of data
from models trained in federated systems. However, developing federated
unlearning methods presents challenges, particularly in balancing three often
conflicting objectives: privacy, accuracy, and efficiency. This paper provides
a comprehensive analysis of existing federated unlearning approaches, examining
their algorithmic efficiency, impact on model accuracy, and effectiveness in
preserving privacy. We discuss key trade-offs among these dimensions and
highlight their implications for practical applications across various domains.
Additionally, we propose the OpenFederatedUnlearning framework, a unified
benchmark for evaluating federated unlearning methods, incorporating classic
baselines and diverse performance metrics. Our findings aim to guide
practitioners in navigating the complex interplay of these objectives, offering
insights to achieve effective and efficient federated unlearning. Finally, we
outline directions for future research to further advance the state of
federated unlearning techniques.

</details>


### [474] [Large Language Model Enhancers for Graph Neural Networks: An Analysis from the Perspective of Causal Mechanism Identification](https://arxiv.org/pdf/2505.08265)
*Hang Gao, Wenxuan Huang, Fengge Wu, Junsuo Zhao, Changwen Zheng, Huaping Liu*

Main category: cs.LG

TL;DR: The paper explores using LLMs to enhance node representations for GNNs, analyzes their properties via interchange interventions, and proposes an optimization module to improve LLM-GNN information transfer.


<details>
  <summary>Details</summary>
Motivation: The potential of LLMs as feature enhancers for GNNs is underexplored, prompting a deeper analysis of their properties and mechanisms.

Method: Constructs a synthetic graph dataset for causal analysis, uses interchange interventions to study LLM enhancers and GNNs, and designs an optimization module for better information transfer.

Result: Experiments validate the proposed optimization module across multiple datasets and models.

Conclusion: The study provides insights into LLM-GNN interactions and offers a practical module to enhance their performance.

Abstract: The use of large language models (LLMs) as feature enhancers to optimize node
representations, which are then used as inputs for graph neural networks
(GNNs), has shown significant potential in graph representation learning.
However, the fundamental properties of this approach remain underexplored. To
address this issue, we propose conducting a more in-depth analysis of this
issue based on the interchange intervention method. First, we construct a
synthetic graph dataset with controllable causal relationships, enabling
precise manipulation of semantic relationships and causal modeling to provide
data for analysis. Using this dataset, we conduct interchange interventions to
examine the deeper properties of LLM enhancers and GNNs, uncovering their
underlying logic and internal mechanisms. Building on the analytical results,
we design a plug-and-play optimization module to improve the information
transfer between LLM enhancers and GNNs. Experiments across multiple datasets
and models validate the proposed module.

</details>


### [475] [Towards Principled Task Grouping for Multi-Task Learning](https://arxiv.org/pdf/2402.15328)
*Chenguang Wang, Xuanhao Pan, Tianshu Yu*

Main category: cs.LG

TL;DR: A principled task grouping method for multi-task learning (MTL) improves performance by optimizing transfer between tasks, validated across diverse domains.


<details>
  <summary>Details</summary>
Motivation: MTL struggles with managing positive and negative transfer between tasks, limiting performance improvements. Task grouping can address this by organizing tasks effectively.

Method: Introduces a theoretically grounded task grouping approach with a flexible mathematical programming formulation for resource constraints.

Result: Superior performance demonstrated across computer vision, combinatorial optimization, and time series tasks compared to baselines.

Conclusion: The method enhances MTL effectiveness and general applicability without efficiency trade-offs.

Abstract: Multi-task learning (MTL) aims to leverage shared information among tasks to
improve learning efficiency and accuracy. However, MTL often struggles to
effectively manage positive and negative transfer between tasks, which can
hinder performance improvements. Task grouping addresses this challenge by
organizing tasks into meaningful clusters, maximizing beneficial transfer while
minimizing detrimental interactions. This paper introduces a principled
approach to task grouping in MTL, advancing beyond existing methods by
addressing key theoretical and practical limitations. Unlike prior studies, our
method offers a theoretically grounded approach that does not depend on
restrictive assumptions for constructing transfer gains. We also present a
flexible mathematical programming formulation that accommodates a wide range of
resource constraints, thereby enhancing its versatility. Experimental results
across diverse domains, including computer vision datasets, combinatorial
optimization benchmarks, and time series tasks, demonstrate the superiority of
our method over extensive baselines, thereby validating its effectiveness and
general applicability in MTL without sacrificing efficiency.

</details>


### [476] [Online Bandit Learning with Offline Preference Data for Improved RLHF](https://arxiv.org/pdf/2406.09574)
*Akhil Agnihotri, Rahul Jain, Deepak Ramachandran, Zheng Wen*

Main category: cs.LG

TL;DR: The paper introduces warmPref-PS, a posterior sampling algorithm for online learning with offline preference data, improving performance by modeling expert competence.


<details>
  <summary>Details</summary>
Motivation: RLHF relies on human feedback, often noisy as scores, while RL assumes reward feedback. The gap between offline preference data and online learning needs addressing.

Method: Adopts a finite-armed linear bandit model, proposes warmPref-PS for online learning with offline preference data, modeling expert competence.

Result: Theoretical analysis shows reduced Bayesian regret; empirical evaluation outperforms baselines.

Conclusion: warmPref-PS effectively bridges offline preference data and online learning, enhancing RLHF fine-tuning.

Abstract: Reinforcement Learning with Human Feedback (RLHF) is at the core of
fine-tuning methods for generative AI models for language and images. Such
feedback is often sought as rank or preference feedback from human raters, as
opposed to eliciting scores since the latter tends to be noisy. On the other
hand, RL theory and algorithms predominantly assume that a reward feedback is
available. In particular, approaches for online learning that can be helpful in
adaptive data collection via active learning cannot incorporate offline
preference data. In this paper, we adopt a finite-armed linear bandit model as
a prototypical model of online learning. We consider an offline preference
dataset to be available generated by an expert of unknown 'competence'. We
propose warmPref-PS, a posterior sampling algorithm for online learning that
can be warm-started with an offline dataset with noisy preference feedback. We
show that by modeling the 'competence' of the expert that generated it, we are
able to use such a dataset most effectively. We support our claims with novel
theoretical analysis of its Bayesian regret, as well as, extensive empirical
evaluation of an approximate loss function that optimizes for infinitely many
arms, and performs substantially better than baselines.

</details>


### [477] [CONGO: Compressive Online Gradient Optimization](https://arxiv.org/pdf/2407.06325)
*Jeremy Carleton, Prathik Vijaykumar, Divyanshu Saxena, Dheeraj Narasimha, Srinivas Shakkottai, Aditya Akella*

Main category: cs.LG

TL;DR: The paper introduces the Compressive Online Gradient Optimization (CONGO) framework to address zeroth-order online convex optimization with sparse gradients, leveraging compressive sensing for efficient gradient estimation.


<details>
  <summary>Details</summary>
Motivation: The work is motivated by optimizing large-scale queueing networks where latency functions react sparsely to resource changes, requiring efficient gradient estimation with limited samples.

Method: The CONGO framework applies compressive sensing methods to achieve regret bounds independent of the full problem dimension, reducing sample requirements for gradient estimation.

Result: CONGO outperforms traditional gradient descent methods in numerical simulations and real-world benchmarks by exploiting gradient sparsity.

Conclusion: The CONGO framework effectively leverages gradient sparsity for efficient optimization in high-dimensional, resource-constrained settings.

Abstract: We address the challenge of zeroth-order online convex optimization where the
objective function's gradient exhibits sparsity, indicating that only a small
number of dimensions possess non-zero gradients. Our aim is to leverage this
sparsity to obtain useful estimates of the objective function's gradient even
when the only information available is a limited number of function samples.
Our motivation stems from the optimization of large-scale queueing networks
that process time-sensitive jobs. Here, a job must be processed by potentially
many queues in sequence to produce an output, and the service time at any queue
is a function of the resources allocated to that queue. Since resources are
costly, the end-to-end latency for jobs must be balanced with the overall cost
of the resources used. While the number of queues is substantial, the latency
function primarily reacts to resource changes in only a few, rendering the
gradient sparse. We tackle this problem by introducing the Compressive Online
Gradient Optimization framework which allows compressive sensing methods
previously applied to stochastic optimization to achieve regret bounds with an
optimal dependence on the time horizon without the full problem dimension
appearing in the bound. For specific algorithms, we reduce the samples required
per gradient estimate to scale with the gradient's sparsity factor rather than
its full dimensionality. Numerical simulations and real-world microservices
benchmarks demonstrate CONGO's superiority over gradient descent approaches
that do not account for sparsity.

</details>


### [478] [Out-of-distribution generalisation is hard: evidence from ARC-like tasks](https://arxiv.org/pdf/2505.09716)
*George Dimitriadis, Spyridon Samothrakis*

Main category: cs.LG

TL;DR: The paper argues that testing for out-of-distribution (OOD) generalization alone is insufficient to confirm compositional learning; the features must also be verified as compositional. It demonstrates this with tasks where common neural networks fail, introduces two new architectures, and shows that even successful OOD performance doesn't guarantee correct feature learning.


<details>
  <summary>Details</summary>
Motivation: To address the gap in confirming compositional learning in algorithms, emphasizing the need to verify both OOD performance and compositional feature identification.

Method: Evaluates three common neural networks (MLP, CNN, Transformer) on OOD tasks, introduces two novel architectures with specific biases, and tests their feature learning.

Result: Common networks fail in OOD tasks; new architectures achieve high OOD performance but may still fail to learn correct compositional features.

Conclusion: OOD testing alone is inadequate; compositional feature verification is essential for true compositional generalization.

Abstract: Out-of-distribution (OOD) generalisation is considered a hallmark of human
and animal intelligence. To achieve OOD through composition, a system must
discover the environment-invariant properties of experienced input-output
mappings and transfer them to novel inputs. This can be realised if an
intelligent system can identify appropriate, task-invariant, and composable
input features, as well as the composition methods, thus allowing it to act
based not on the interpolation between learnt data points but on the
task-invariant composition of those features. We propose that in order to
confirm that an algorithm does indeed learn compositional structures from data,
it is not enough to just test on an OOD setup, but one also needs to confirm
that the features identified are indeed compositional. We showcase this by
exploring two tasks with clearly defined OOD metrics that are not OOD solvable
by three commonly used neural networks: a Multi-Layer Perceptron (MLP), a
Convolutional Neural Network (CNN), and a Transformer. In addition, we develop
two novel network architectures imbued with biases that allow them to be
successful in OOD scenarios. We show that even with correct biases and almost
perfect OOD performance, an algorithm can still fail to learn the correct
features for compositional generalisation.

</details>


### [479] [Measuring Variable Importance in Heterogeneous Treatment Effects with Confidence](https://arxiv.org/pdf/2408.13002)
*Joseph Paillard, Angel Reyero Lobo, Vitaliy Kolodyazhniy, Bertrand Thirion, Denis A. Engemann*

Main category: cs.LG

TL;DR: PermuCATE, a CPI-based algorithm, improves variable importance assessment for CATE estimation with lower variance than LOCO, enhancing statistical power in limited-data settings like biomedicine.


<details>
  <summary>Details</summary>
Motivation: To reliably identify variables driving treatment response heterogeneity in causal machine learning for real-world applications.

Method: Proposes PermuCATE, using Conditional Permutation Importance (CPI) for rigorous variable importance assessment in CATE estimation.

Result: PermuCATE shows lower variance than LOCO, providing reliable variable importance and higher statistical power, validated in simulations and real-world health data.

Conclusion: PermuCATE is a robust tool for causal inference, especially in data-limited biomedical settings.

Abstract: Causal machine learning holds promise for estimating individual treatment
effects from complex data. For successful real-world applications of machine
learning methods, it is of paramount importance to obtain reliable insights
into which variables drive heterogeneity in the response to treatment. We
propose PermuCATE, an algorithm based on the Conditional Permutation Importance
(CPI) method, for statistically rigorous global variable importance assessment
in the estimation of the Conditional Average Treatment Effect (CATE).
Theoretical analysis of the finite sample regime and empirical studies show
that PermuCATE has lower variance than the Leave-One-Covariate-Out (LOCO)
reference method and provides a reliable measure of variable importance. This
property increases statistical power, which is crucial for causal inference in
the limited-data regime common to biomedical applications. We empirically
demonstrate the benefits of PermuCATE in simulated and real-world health
datasets, including settings with up to hundreds of correlated variables.

</details>


### [480] [Large Vision Model-Enhanced Digital Twin with Deep Reinforcement Learning for User Association and Load Balancing in Dynamic Wireless Networks](https://arxiv.org/pdf/2410.07611)
*Zhenyu Tao, Wei Xu, Xiaohu You*

Main category: cs.LG

TL;DR: A parallel DT-driven DRL method is proposed for user association in dynamic cellular networks, using an LVM-enhanced digital twin and a zero-shot generative mobility model (Map2Traj) for efficient training.


<details>
  <summary>Details</summary>
Motivation: Challenges in optimizing user association due to dynamic user mobility and fluctuating counts, along with high trial-and-error costs and poor performance of existing DRL methods in real-world scenarios.

Method: Develops an LVM-enhanced digital twin with Map2Traj for trajectory estimation from street maps, and a parallel DT framework for DRL training to handle dynamic user scenarios.

Result: The LVM-enhanced DT achieves comparable training efficacy to real environments, and the parallel DT framework improves cell-edge user performance by nearly 20%.

Conclusion: The proposed method effectively addresses limitations of existing DRL approaches, offering scalable and efficient training for dynamic network scenarios.

Abstract: Optimization of user association in a densely deployed cellular network is
usually challenging and even more complicated due to the dynamic nature of user
mobility and fluctuation in user counts. While deep reinforcement learning
(DRL) emerges as a promising solution, its application in practice is hindered
by high trial-and-error costs in real world and unsatisfactory physical network
performance during training. Also, existing DRL-based user association methods
are typically applicable to scenarios with a fixed number of users due to
convergence and compatibility challenges. To address these limitations, we
introduce a large vision model (LVM)-enhanced digital twin (DT) for wireless
networks and propose a parallel DT-driven DRL method for user association and
load balancing in networks with dynamic user counts, distribution, and mobility
patterns. To construct this LVM-enhanced DT for DRL training, we develop a
zero-shot generative user mobility model, named Map2Traj, based on the
diffusion model. Map2Traj estimates user trajectory patterns and spatial
distributions solely from street maps. DRL models undergo training in the DT
environment, avoiding direct interactions with physical networks. To enhance
the generalization ability of DRL models for dynamic scenarios, a parallel DT
framework is further established to alleviate strong correlation and
non-stationarity in single-environment training and improve training
efficiency. Numerical results show that the developed LVM-enhanced DT achieves
closely comparable training efficacy to the real environment, and the proposed
parallel DT framework even outperforms the single real-world environment in DRL
training with nearly 20\% gain in terms of cell-edge user performance.

</details>


### [481] [QuXAI: Explainers for Hybrid Quantum Machine Learning Models](https://arxiv.org/pdf/2505.10167)
*Saikat Barua, Mostafizur Rahman, Shehenaz Khaled, Md Jafor Sadek, Rafiul Islam, Shahnewaz Siddique*

Main category: cs.LG

TL;DR: QuXAI introduces a framework for explaining hybrid quantum-classical ML models, addressing the lack of robust explainability in HQML systems.


<details>
  <summary>Details</summary>
Motivation: The complexity of HQML models often results in black-box behavior, undermining transparency and reliability. This work aims to bridge the gap in explainability for such systems.

Method: The approach involves creating HQML models with quantum feature maps, using Q-MEDLEY to explain feature importance while preserving quantum transformations, and visualizing attributions.

Result: Q-MEDLEY effectively identifies influential classical aspects and noise in HQML models, performing competitively against classical XAI techniques. Ablation studies highlight the benefits of its composite structure.

Conclusion: QuXAI enhances interpretability and reliability of HQML models, fostering safer and more responsible use of quantum-enhanced AI.

Abstract: The emergence of hybrid quantum-classical machine learning (HQML) models
opens new horizons of computational intelligence but their fundamental
complexity frequently leads to black box behavior that undermines transparency
and reliability in their application. Although XAI for quantum systems still in
its infancy, a major research gap is evident in robust global and local
explainability approaches that are designed for HQML architectures that employ
quantized feature encoding followed by classical learning. The gap is the focus
of this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an
explainer for explaining feature importance in these hybrid systems. Our model
entails the creation of HQML models incorporating quantum feature maps, the use
of Q-MEDLEY, which combines feature based inferences, preserving the quantum
transformation stage and visualizing the resulting attributions. Our result
shows that Q-MEDLEY delineates influential classical aspects in HQML models, as
well as separates their noise, and competes well against established XAI
techniques in classical validation settings. Ablation studies more
significantly expose the virtues of the composite structure used in Q-MEDLEY.
The implications of this work are critically important, as it provides a route
to improve the interpretability and reliability of HQML models, thus promoting
greater confidence and being able to engage in safer and more responsible use
of quantum-enhanced AI technology.
  Our code and experiments are open-sourced at:
https://github.com/GitsSaikat/QuXAI

</details>


### [482] [Learning Equivariant Non-Local Electron Density Functionals](https://arxiv.org/pdf/2410.07972)
*Nicholas Gao, Eike Eberhard, Stephan G√ºnnemann*

Main category: cs.LG

TL;DR: EG-XC, a novel non-local XC functional using equivariant GNNs, improves accuracy and scalability in density functional theory by compressing electron density into a nuclei-centered point cloud.


<details>
  <summary>Details</summary>
Motivation: Current machine-learned and human-designed XC functionals lack accuracy, scalability, or rely on costly data. EG-XC addresses these limitations.

Method: EG-XC compresses electron density into an SO(3)-equivariant nuclei-centered point cloud and applies an equivariant GNN for scalable, accurate non-local interactions. Training involves differentiating through a self-consistent field solver with energy targets.

Result: EG-XC accurately reconstructs CCSD(T) energies, reduces MAE by 35-50% on out-of-distribution data, and outperforms force fields in data efficiency and molecular size extrapolation.

Conclusion: EG-XC offers a scalable, accurate, and data-efficient solution for non-local XC functionals, significantly improving upon existing methods.

Abstract: The accuracy of density functional theory hinges on the approximation of
non-local contributions to the exchange-correlation (XC) functional. To date,
machine-learned and human-designed approximations suffer from insufficient
accuracy, limited scalability, or dependence on costly reference data. To
address these issues, we introduce Equivariant Graph Exchange Correlation
(EG-XC), a novel non-local XC functional based on equivariant graph neural
networks (GNNs). Where previous works relied on semi-local functionals or
fixed-size descriptors of the density, we compress the electron density into an
SO(3)-equivariant nuclei-centered point cloud for efficient non-local
atomic-range interactions. By applying an equivariant GNN on this point cloud,
we capture molecular-range interactions in a scalable and accurate manner. To
train EG-XC, we differentiate through a self-consistent field solver requiring
only energy targets. In our empirical evaluation, we find EG-XC to accurately
reconstruct `gold-standard' CCSD(T) energies on MD17. On out-of-distribution
conformations of 3BPA, EG-XC reduces the relative MAE by 35% to 50%.
Remarkably, EG-XC excels in data efficiency and molecular size extrapolation on
QM9, matching force fields trained on 5 times more and larger molecules. On
identical training sets, EG-XC yields on average 51% lower MAEs.

</details>


### [483] [Beyond the Heatmap: A Rigorous Evaluation of Component Impact in MCTS-Based TSP Solvers](https://arxiv.org/pdf/2411.09238)
*Xuanhao Pan, Chenguang Wang, Chaolong Ying, Ye Xue, Tianshu Yu*

Main category: cs.LG

TL;DR: The paper questions the overemphasis on heatmap complexity in the 'Heatmap + MCTS' paradigm for TSP, showing that MCTS tuning and simple heatmaps can outperform sophisticated ones.


<details>
  <summary>Details</summary>
Motivation: To critically assess the impact of heatmap complexity versus MCTS configuration in solving TSP, challenging the assumption that heatmap sophistication is key.

Method: Empirical analysis across diverse TSP scales and benchmarks, comparing heatmap complexity and MCTS tuning, and introducing a standardized MCTS hyperparameter tuning pipeline.

Result: Optimal MCTS tuning with simple heatmaps matches or surpasses sophisticated heatmaps, highlighting MCTS's role in performance.

Conclusion: A balanced integration of learning and search components is advocated, with standardized MCTS tuning for fair future evaluations.

Abstract: The ``Heatmap + Monte Carlo Tree Search (MCTS)'' paradigm has recently
emerged as a prominent framework for solving the Travelling Salesman Problem
(TSP). While considerable effort has been devoted to enhancing heatmap
sophistication through advanced learning models, this paper rigorously examines
whether this emphasis is justified, critically assessing the relative impact of
heatmap complexity versus MCTS configuration. Our extensive empirical analysis
across diverse TSP scales, distributions, and benchmarks reveals two pivotal
insights: 1) The configuration of MCTS strategies significantly influences
solution quality, underscoring the importance of meticulous tuning to achieve
optimal results and enabling valid comparisons among different heatmap
methodologies. 2) A rudimentary, parameter-free heatmap based on the intrinsic
$k$-nearest neighbor structure of TSP instances, when coupled with an optimally
tuned MCTS, can match or surpass the performance of more sophisticated, learned
heatmaps, demonstrating robust generalizability on problem scale and
distribution shift. To facilitate rigorous and fair evaluations in future
research, we introduce a streamlined pipeline for standardized MCTS
hyperparameter tuning. Collectively, these findings challenge the prevalent
assumption that heatmap complexity is the primary determinant of performance,
advocating instead for a balanced integration and comprehensive evaluation of
both learning and search components within this paradigm. Our code is available
at: https://github.com/LOGO-CUHKSZ/rethink_mcts_tsp.

</details>


### [484] [Toward Foundation Model for Multivariate Wearable Sensing of Physiological Signals](https://arxiv.org/pdf/2412.09758)
*Yunfei Luo, Yuliang Chen, Asif Salekin, Tauhidur Rahman*

Main category: cs.LG

TL;DR: NormWear is a multi-modal foundation model for wearable sensing data, using channel-aware attention and a shared [CLS] token to generalize across diverse sensors and applications, outperforming baselines in various settings.


<details>
  <summary>Details</summary>
Motivation: Wearable sensing data's variability and heterogeneity in healthcare applications necessitate generalizable representations, which existing models struggle to provide.

Method: NormWear employs a channel-aware attention mechanism with a shared [CLS] token to detect intra- and inter-sensor signal patterns, pretrained on diverse physiological signals.

Result: NormWear demonstrates exceptional generalizability across 11 datasets and 18 applications, outperforming baselines in zero-shot, partial-shot, and full-shot settings.

Conclusion: NormWear is a versatile and effective foundation model for wearable sensing data, with broad applicability in real-world health monitoring and diagnostics.

Abstract: Time-series foundation models excel at tasks like forecasting across diverse
data types by leveraging informative waveform representations. Wearable sensing
data, however, pose unique challenges due to their variability in patterns and
frequency bands, especially for healthcare-related outcomes. The main obstacle
lies in crafting generalizable representations that adapt efficiently across
heterogeneous sensing configurations and applications. To address this, we
propose NormWear, the first multi-modal and ubiquitous foundation model
designed to extract generalized and informative representations from wearable
sensing data. Specifically, we design a channel-aware attention mechanism with
a shared special liaison [CLS] token to detect signal patterns in both
intra-sensor and inter-sensors. This helps the model to extract more meaningful
information considering both time series themselves and the relationships
between input sensors. This helps the model to be widely compatible with
various sensors settings. NormWear is pretrained on a diverse set of
physiological signals, including PPG, ECG, EEG, GSR, and IMU, from various
public datasets. Our model shows exceptional generalizability across 11 public
wearable sensing datasets, spanning 18 applications in mental health, body
state inference, vital sign estimation, and disease risk evaluation. It
consistently outperforms competitive baselines under zero-shot, partial-shot,
and full-shot settings, indicating broad applicability in real-world health
applications.

</details>


### [485] [Stability and List-Replicability for Agnostic Learners](https://arxiv.org/pdf/2501.05333)
*Ari Blondal, Shan Gao, Hamed Hatami, Pooya Hatami*

Main category: cs.LG

TL;DR: The paper resolves two open problems from Chase et al.'s work, showing that agnostic global stability is either too restrictive or equivalent to online learnability, depending on the relaxation.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of agnostic global stability being too restrictive, the paper explores relaxed conditions proposed by Chase et al.

Method: Characterizes learnable classes under two relaxations: one where stability depends on excess error, and another restricted to small population loss distributions.

Result: 1. Agnostic stability with excess error dependence is equivalent to online learnability (Littlestone dimension). 2. Only finite classes are learnable under the second relaxation.

Conclusion: The relaxations either align with online learnability or remain restrictive, resolving Chase et al.'s open problems.

Abstract: Two seminal papers--Alon, Livni, Malliaris, Moran (STOC 2019) and Bun, Livni,
and Moran (FOCS 2020)--established the equivalence between online learnability
and globally stable PAC learnability in binary classification. However, Chase,
Chornomaz, Moran, and Yehudayoff (STOC 2024) recently showed that this
equivalence does not hold in the agnostic setting. Specifically, they proved
that in the agnostic setting, only finite hypothesis classes are globally
stable learnable. Therefore, agnostic global stability is too restrictive to
capture interesting hypothesis classes.
  To address this limitation, Chase et al. introduced two relaxations of
agnostic global stability. In this paper, we characterize the classes that are
learnable under their proposed relaxed conditions, resolving the two open
problems raised in their work.
  First, we prove that in the setting where the stability parameter can depend
on the excess error (the gap between the learner's error and the best
achievable error by the hypothesis class), agnostic stability is fully
characterized by the Littlestone dimension. Consequently, as in the realizable
case, this form of learnability is equivalent to online learnability.
  As part of the proof of this theorem, we strengthen the celebrated result of
Bun et al. by showing that classes with infinite Littlestone dimension are not
stably PAC learnable, even if we allow the stability parameter to depend on the
excess error.
  For the second relaxation proposed by Chase et al., we prove that only finite
hypothesis classes are globally stable learnable, even if we restrict the
agnostic setting to distributions with small population loss.

</details>


### [486] [Active RLHF via Best Policy Learning from Trajectory Preference Feedback](https://arxiv.org/pdf/2501.18873)
*Akhil Agnihotri, Rahul Jain, Deepak Ramachandran, Zheng Wen*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We address the problem of best policy identification in preference-based
reinforcement learning (PbRL), where learning occurs from noisy binary
preferences over trajectory pairs rather than explicit numerical rewards. This
approach is useful for post-training optimization of generative AI models
during multi-turn user interactions, where preference feedback is more robust
than handcrafted reward models. In this setting, learning is driven by both an
offline preference dataset -- collected from a rater of unknown `competence' --
and online data collected with pure exploration. Since offline datasets may
exhibit out-of-distribution (OOD) biases, principled online data collection is
necessary. To address this, we propose Posterior Sampling for Preference
Learning ($\mathsf{PSPL}$), a novel algorithm inspired by Top-Two Thompson
Sampling, that maintains independent posteriors over the true reward model and
transition dynamics. We provide the first theoretical guarantees for PbRL in
this setting, establishing an upper bound on the simple Bayesian regret of
$\mathsf{PSPL}$. Since the exact algorithm can be computationally impractical,
we also provide an approximate version that outperforms existing baselines.

</details>


### [487] [Covering Multiple Objectives with a Small Set of Solutions Using Bayesian Optimization](https://arxiv.org/pdf/2501.19342)
*Natalie Maus, Kyurae Kim, Yimeng Zeng, Haydn Thomas Jones, Fangping Wan, Marcelo Der Torossian Torres, Cesar de la Fuente-Nunez, Jacob R. Gardner*

Main category: cs.LG

TL;DR: MOCOBO is a Bayesian optimization method for finding a small set of solutions that collectively cover multiple objectives, validated in drug design applications.


<details>
  <summary>Details</summary>
Motivation: Traditional multi-objective optimization seeks a single Pareto-optimal set, but this work addresses the need for a small set of solutions that cover all objectives, inspired by drug design challenges.

Method: Proposes Multi-Objective Coverage Bayesian Optimization (MOCOBO) to efficiently find a covering set of K < T solutions for T objectives.

Result: MOCOBO achieves coverage comparable to optimizing each objective individually, with successful in vitro peptide potency against drug-resistant pathogens.

Conclusion: MOCOBO is effective for high-dimensional tasks like drug discovery, demonstrating practical utility in finding covering solutions.

Abstract: In multi-objective black-box optimization, the goal is typically to find
solutions that optimize a set of $T$ black-box objective functions, $f_1$, ...,
$f_T$, simultaneously. Traditional approaches often seek a single
Pareto-optimal set that balances trade-offs among all objectives. In this work,
we consider a problem setting that departs from this paradigm: finding a small
set of K < T solutions, that collectively "covers" the T objectives. A set of
solutions is defined as "covering" if, for each objective $f_1$, ..., $f_T$,
there is at least one good solution. A motivating example for this problem
setting occurs in drug design. For example, we may have T pathogens and aim to
identify a set of K < T antibiotics such that at least one antibiotic can be
used to treat each pathogen. To address this problem, we propose
Multi-Objective Coverage Bayesian Optimization (MOCOBO), a principled algorithm
designed to efficiently find a covering set. We validate our approach through
experiments on challenging high-dimensional tasks, including applications in
peptide and molecular design, where MOCOBO is shown to find high-performing
covering sets of solutions. The results show that the coverage of the K < T
solutions found by MOCOBO matches or nearly matches the coverage of T solutions
obtained by optimizing each objective individually. Furthermore, in in vitro
experiments, the peptides found by MOCOBO exhibited high potency against
drug-resistant pathogens, further demonstrating the potential of MOCOBO for
drug discovery.

</details>


### [488] [Binned Spectral Power Loss for Improved Prediction of Chaotic Systems](https://arxiv.org/pdf/2502.00472)
*Dibyajyoti Chakraborty, Arvind T. Mohan, Romit Maulik*

Main category: cs.LG

TL;DR: The paper introduces the Binned Spectral Power (BSP) Loss to address spectral bias in deep learning for forecasting chaotic systems, improving stability and accuracy without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Spectral bias in neural networks hinders accurate long-term forecasting of multiscale chaotic systems, especially autoregressively, leading to compounding errors.

Method: Proposes the BSP Loss, a frequency-domain loss function that adaptively weighs errors across scales, penalizing deviations in energy distribution.

Result: BSP Loss mitigates spectral bias, enhancing stability and spectral accuracy in forecasting chaotic systems, validated on benchmark datasets.

Conclusion: The BSP Loss enables more robust deep learning models for chaotic system forecasting by directly targeting spectral consistency.

Abstract: Forecasting multiscale chaotic dynamical systems with deep learning remains a
formidable challenge due to the spectral bias of neural networks, which hinders
the accurate representation of fine-scale structures in long-term predictions.
This issue is exacerbated when models are deployed autoregressively, leading to
compounding errors and instability. In this work, we introduce a novel approach
to mitigate the spectral bias which we call the Binned Spectral Power (BSP)
Loss. The BSP loss is a frequency-domain loss function that adaptively weighs
errors in predicting both larger and smaller scales of the dataset. Unlike
traditional losses that focus on pointwise misfits, our BSP loss explicitly
penalizes deviations in the energy distribution across different scales,
promoting stable and physically consistent predictions. We demonstrate that the
BSP loss mitigates the well-known problem of spectral bias in deep learning. We
further validate our approach for the data-driven high-dimensional time-series
forecasting of a range of benchmark chaotic systems which are typically
intractable due to spectral bias. Our results demonstrate that the BSP loss
significantly improves the stability and spectral accuracy of neural
forecasting models without requiring architectural modifications. By directly
targeting spectral consistency, our approach paves the way for more robust deep
learning models for long-term forecasting of chaotic dynamical systems.

</details>


### [489] [Strategic Classification with Randomised Classifiers](https://arxiv.org/pdf/2502.01313)
*Jack Geary, Henry Gouk*

Main category: cs.LG

TL;DR: The paper analyzes strategic classification with randomized classifiers, showing they can outperform deterministic ones without downsides, with risk bounds and convergence rates matching the i.i.d. case.


<details>
  <summary>Details</summary>
Motivation: To extend strategic classification beyond deterministic classifiers by exploring the potential benefits of randomized classifiers.

Method: Theoretical analysis of randomized classifiers in strategic classification, comparing their performance to deterministic ones and examining risk bounds and convergence rates.

Result: Randomized classifiers can achieve better accuracy than deterministic ones under certain conditions, with no worse outcomes. Risk bounds and convergence rates are similar to the deterministic case.

Conclusion: Randomization can improve strategic classification without introducing significant drawbacks, offering practical advantages.

Abstract: We consider the problem of strategic classification, where a learner must
build a model to classify agents based on features that have been strategically
modified. Previous work in this area has concentrated on the case when the
learner is restricted to deterministic classifiers. In contrast, we perform a
theoretical analysis of an extension to this setting that allows the learner to
produce a randomised classifier. We show that, under certain conditions, the
optimal randomised classifier can achieve better accuracy than the optimal
deterministic classifier, but under no conditions can it be worse. When a
finite set of training data is available, we show that the excess risk of
Strategic Empirical Risk Minimisation over the class of randomised classifiers
is bounded in a similar manner as the deterministic case. In both the
deterministic and randomised cases, the risk of the classifier produced by the
learner converges to that of the corresponding optimal classifier as the volume
of available training data grows. Moreover, this convergence happens at the
same rate as in the i.i.d. case. Our findings are compared with previous
theoretical work analysing the problem of strategic classification. We conclude
that randomisation has the potential to alleviate some issues that could be
faced in practice without introducing any substantial downsides.

</details>


### [490] [From Uncertain to Safe: Conformal Fine-Tuning of Diffusion Models for Safe PDE Control](https://arxiv.org/pdf/2502.02205)
*Peiyan Hu, Xiaowei Qian, Wenhao Deng, Rui Wang, Haodong Feng, Ruiqi Feng, Tao Zhang, Long Wei, Yue Wang, Zhi-Ming Ma, Tailin Wu*

Main category: cs.LG

TL;DR: SafeDiffCon introduces uncertainty quantile for safe PDE control, outperforming baselines in safety and performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of safety considerations in existing deep learning methods for PDE-constrained control.

Method: Post-trains a diffusion model with reweighted loss and dynamic adjustment during inference using uncertainty quantile.

Result: SafeDiffCon meets all safety constraints and achieves top control performance in tested tasks.

Conclusion: SafeDiffCon effectively integrates safety into PDE control, setting a new benchmark for safe deep learning applications.

Abstract: The application of deep learning for partial differential equation
(PDE)-constrained control is gaining increasing attention. However, existing
methods rarely consider safety requirements crucial in real-world applications.
To address this limitation, we propose Safe Diffusion Models for PDE Control
(SafeDiffCon), which introduce the uncertainty quantile as model uncertainty
quantification to achieve optimal control under safety constraints through both
post-training and inference phases. Firstly, our approach post-trains a
pre-trained diffusion model to generate control sequences that better satisfy
safety constraints while achieving improved control objectives via a reweighted
diffusion loss, which incorporates the uncertainty quantile estimated using
conformal prediction. Secondly, during inference, the diffusion model
dynamically adjusts both its generation process and parameters through
iterative guidance and fine-tuning, conditioned on control targets while
simultaneously integrating the estimated uncertainty quantile. We evaluate
SafeDiffCon on three control tasks: 1D Burgers' equation, 2D incompressible
fluid, and controlled nuclear fusion problem. Results demonstrate that
SafeDiffCon is the only method that satisfies all safety constraints, whereas
other classical and deep learning baselines fail. Furthermore, while adhering
to safety constraints, SafeDiffCon achieves the best control performance. The
code can be found at https://github.com/AI4Science-WestlakeU/safediffcon.

</details>


### [491] [ENFORCE: Nonlinear Constrained Learning with Adaptive-depth Neural Projection](https://arxiv.org/pdf/2502.06774)
*Giacomo Lastrucci, Artur M. Schweidtmann*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Ensuring neural networks adhere to domain-specific constraints is crucial for
addressing safety and ethical concerns while also enhancing inference accuracy.
Despite the nonlinear nature of most real-world tasks, existing methods are
predominantly limited to affine or convex constraints. We introduce ENFORCE, a
neural network architecture that uses an adaptive projection module (AdaNP) to
enforce nonlinear equality constraints in the predictions. We prove that our
projection mapping is 1-Lipschitz, making it well-suited for stable training.
We evaluate ENFORCE on an illustrative regression task and for learning
solutions to high-dimensional optimization problems in an unsupervised setting.
The predictions of our new architecture satisfy $N_C$ equality constraints that
are nonlinear in both the inputs and outputs of the neural network, while
maintaining scalability with a tractable computational complexity of
$\mathcal{O}(N_C^3)$ at training and inference time.

</details>


### [492] [Learnable Residual-Based Latent Denoising in Semantic Communication](https://arxiv.org/pdf/2502.07319)
*Mingkai Xu, Yongpeng Wu, Yuxuan Shi, Xiang-Gen Xia, Wenjun Zhang, Ping Zhang*

Main category: cs.LG

TL;DR: A latent denoising semantic communication framework improves image transmission by removing channel noise and recovering semantic information, using iterative residual learning and adaptive denoising steps based on SNR.


<details>
  <summary>Details</summary>
Motivation: To enhance the quality of decoded images in noisy channels by preprocessing received signals to remove noise and recover semantic information.

Method: Incorporates a learnable latent denoiser at the receiver, uses iterative residual learning for efficient denoising, and adapts denoising steps based on predicted latent similarity scores from channel SNR.

Result: Simulations show the framework effectively removes noise at various levels and reconstructs visually appealing images.

Conclusion: The proposed framework robustly improves image transmission quality in noisy environments by efficiently denoising and adapting to channel conditions.

Abstract: A latent denoising semantic communication (SemCom) framework is proposed for
robust image transmission over noisy channels. By incorporating a learnable
latent denoiser into the receiver, the received signals are preprocessed to
effectively remove the channel noise and recover the semantic information,
thereby enhancing the quality of the decoded images. Specifically, a latent
denoising mapping is established by an iterative residual learning approach to
improve the denoising efficiency while ensuring stable performance. Moreover,
channel signal-to-noise ratio (SNR) is utilized to estimate and predict the
latent similarity score (SS) for conditional denoising, where the number of
denoising steps is adapted based on the predicted SS sequence, further reducing
the communication latency. Finally, simulations demonstrate that the proposed
framework can effectively and efficiently remove the channel noise at various
levels and reconstruct visual-appealing images.

</details>


### [493] [Integrated Data Analysis of Plasma Electron Density Profile Tomography for HL-3 with Gaussian Process Regression](https://arxiv.org/pdf/2502.08882)
*Cong Wang, Jiahong Chen, Renjie Yang, Dong Li, Zhibin Wang, Zongyu Yang, Zhijun Wang, Yixiong Wei, Zhaoyang Liu, Chenshu Hu, Jing Li*

Main category: cs.LG

TL;DR: A Gaussian Process Regression model integrates line and point measurements for plasma electron density tomography in HL-3 tokamak, achieving high accuracy (3.60*10^(-4) and robustness.


<details>
  <summary>Details</summary>
Motivation: To improve plasma electron density profile tomography by combining line-integral and point measurements for more accurate reconstructions.

Method: Uses Gaussian Process Regression to integrate far-infrared laser interferometer and reflectometry data, incorporating magnetic equilibrium via coordinate mapping.

Result: Achieves an average relative error of 3.60*10^(-4) and demonstrates robustness in sensitivity tests.

Conclusion: The model provides a reliable and accurate solution for plasma density tomography in tokamaks.

Abstract: An integrated data analysis model based on Gaussian Process Regression is
proposed for plasma electron density profile tomography in the HL-3 tokamak.
The model combines line-integral measurements from the far-infrared laser
interferometer with point measurements obtained via the frequency-modulated
continuous wave reflectometry. By employing Gaussian Process Regression, the
model effectively incorporates point measurements into 2D profile
reconstructions, while coordinate mapping integrates magnetic equilibrium
information. The average relative error of the reconstructed profile obtained
by the integrated data analysis model with normalized magnetic flux is as low
as 3.60*10^(-4). Additionally, sensitivity tests were conducted on the grid
resolution, the standard deviation of diagnostic data, and noise levels,
providing a robust foundation for the real application to experimental data.

</details>


### [494] [Mol-LLaMA: Towards General Understanding of Molecules in Large Molecular Language Model](https://arxiv.org/pdf/2502.13449)
*Dongki Kim, Wonbin Lee, Sung Ju Hwang*

Main category: cs.LG

TL;DR: Mol-LLaMA is a large molecular language model designed to improve molecular understanding by integrating general knowledge, explainability, and reasoning, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Existing molecular language models lack accurate analysis due to limited knowledge and reasoning, hindering drug discovery and interdisciplinary research.

Method: Mol-LLaMA uses key data types for molecular features and integrates complementary information from various molecular encoders.

Result: The model demonstrates improved comprehension of molecular features and provides informative responses.

Conclusion: Mol-LLaMA shows potential as a general-purpose assistant for molecular analysis, advancing drug discovery and interdisciplinary research.

Abstract: Understanding molecules is key to understanding organisms and driving
advances in drug discovery, requiring interdisciplinary knowledge across
chemistry and biology. Although large molecular language models have achieved
notable success in task transfer, they often struggle to accurately analyze
molecular features due to limited knowledge and reasoning capabilities. To
address this issue, we present Mol-LLaMA, a large molecular language model that
grasps the general knowledge centered on molecules and exhibits explainability
and reasoning ability. To this end, we design key data types that encompass the
fundamental molecular features, taking into account the essential abilities for
molecular reasoning. Further, to improve molecular understanding, we propose a
module that integrates complementary information from different molecular
encoders, leveraging the distinct advantages of molecular representations. Our
experimental results demonstrate that Mol-LLaMA is capable of comprehending the
general features of molecules and providing informative responses, implying its
potential as a general-purpose assistant for molecular analysis. Our project
page is at https://mol-llama.github.io/.

</details>


### [495] [Towards Understanding Gradient Flow Dynamics of Homogeneous Neural Networks Beyond the Origin](https://arxiv.org/pdf/2502.15952)
*Akshay Kumar, Jarvis Haupt*

Main category: cs.LG

TL;DR: The paper analyzes gradient flow dynamics of homogeneous neural networks after escaping the origin, characterizing the first saddle point and preserving sparsity structure.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of homogeneous neural networks post-origin escape and characterize early training dynamics.

Method: Study gradient flow dynamics for networks with locally Lipschitz gradients, focusing on post-escape behavior and saddle points.

Result: The first saddle point post-escape is characterized, and sparsity structure is preserved until the next saddle point.

Conclusion: Insights into early training dynamics and sparsity preservation in homogeneous networks are provided.

Abstract: Recent works exploring the training dynamics of homogeneous neural network
weights under gradient flow with small initialization have established that in
the early stages of training, the weights remain small and near the origin, but
converge in direction. Building on this, the current paper studies the gradient
flow dynamics of homogeneous neural networks with locally Lipschitz gradients,
after they escape the origin. Insights gained from this analysis are used to
characterize the first saddle point encountered by gradient flow after escaping
the origin. Also, it is shown that for homogeneous feed-forward neural
networks, under certain conditions, the sparsity structure emerging among the
weights before the escape is preserved after escaping the origin and until
reaching the next saddle point.

</details>


### [496] [MetaSym: A Symplectic Meta-learning Framework for Physical Intelligence](https://arxiv.org/pdf/2502.16667)
*Pranav Vaidhyanathan, Aristotelis Papatheodorou, Mark T. Mitchison, Natalia Ares, Ioannis Havoutis*

Main category: cs.LG

TL;DR: MetaSym is a novel deep learning framework combining symplectic inductive bias and meta-attention for scalable, physics-aware modeling, outperforming state-of-the-art baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of scalable and generalizable physics-aware deep learning, especially preserving physical invariants like energy and momentum.

Method: MetaSym integrates a symplectic encoder for strong inductive bias and an autoregressive decoder with meta-attention for flexible adaptation.

Result: Superior performance in few-shot adaptation across varied datasets (spring-mesh, quantum systems, quadrotor dynamics).

Conclusion: MetaSym effectively balances physical invariance and adaptability, demonstrating state-of-the-art results.

Abstract: Scalable and generalizable physics-aware deep learning has long been
considered a significant challenge with various applications across diverse
domains ranging from robotics to molecular dynamics. Central to almost all
physical systems are symplectic forms, the geometric backbone that underpins
fundamental invariants like energy and momentum. In this work, we introduce a
novel deep learning framework, MetaSym. In particular, MetaSym combines a
strong symplectic inductive bias obtained from a symplectic encoder, and an
autoregressive decoder with meta-attention. This principled design ensures that
core physical invariants remain intact, while allowing flexible, data-efficient
adaptation to system heterogeneities. We benchmark MetaSym with highly varied
and realistic datasets, such as a high-dimensional spring-mesh system (Otness
et al., 2021), an open quantum system with dissipation and measurement
backaction, and robotics-inspired quadrotor dynamics. Our results demonstrate
superior performance in modeling dynamics under few-shot adaptation,
outperforming state-of-the-art baselines that use larger models.

</details>


### [497] [Long-term Causal Inference via Modeling Sequential Latent Confounding](https://arxiv.org/pdf/2502.18994)
*Weilin Chen, Ruichu Cai, Yuguang Yan, Zhifeng Hao, Jos√© Miguel Hern√°ndez-Lobato*

Main category: cs.LG

TL;DR: The paper extends the CAECB assumption to handle temporal short-term outcomes, enabling long-term causal effect identification under a functional relationship between sequential confounding biases.


<details>
  <summary>Details</summary>
Motivation: Address the limitation of the CAECB assumption, which is restricted to single short-term outcomes, by proposing a more flexible assumption for temporal outcomes.

Method: Introduce a novel assumption linking sequential confounding biases in temporal short-term outcomes, develop an estimator, and analyze its asymptotic properties.

Result: Theoretical identification of long-term causal effects is established, and experiments validate the method's effectiveness.

Conclusion: The proposed method successfully extends the CAECB framework, enabling broader applicability in long-term causal inference.

Abstract: Long-term causal inference is an important but challenging problem across
various scientific domains. To solve the latent confounding problem in
long-term observational studies, existing methods leverage short-term
experimental data. Ghassami et al. propose an approach based on the Conditional
Additive Equi-Confounding Bias (CAECB) assumption, which asserts that the
confounding bias in the short-term outcome is equal to that in the long-term
outcome, so that the long-term confounding bias and the causal effects can be
identified. While effective in certain cases, this assumption is limited to
scenarios where there is only one short-term outcome with the same scale as the
long-term outcome. In this paper, we introduce a novel assumption that extends
the CAECB assumption to accommodate temporal short-term outcomes. Our proposed
assumption states a functional relationship between sequential confounding
biases across temporal short-term outcomes, under which we theoretically
establish the identification of long-term causal effects. Based on the
identification result, we develop an estimator and conduct a theoretical
analysis of its asymptotic properties. Extensive experiments validate our
theoretical results and demonstrate the effectiveness of the proposed method.

</details>


### [498] [From Equations to Insights: Unraveling Symbolic Structures in PDEs with LLMs](https://arxiv.org/pdf/2503.09986)
*Rohan Bhatnagar, Ling Liang, Krish Patel, Haizhao Yang*

Main category: cs.LG

TL;DR: The paper proposes using large language models (LLMs) to uncover symbolic relationships in PDEs, improving efficiency and accuracy in solving them.


<details>
  <summary>Details</summary>
Motivation: The success of AI in diverse fields motivates its application to scientific problems like PDEs, where symbolic relationships remain underexplored.

Method: Leverage LLMs to learn symbolic relationships in PDEs, predicting operators and improving symbolic machine learning.

Result: LLMs effectively predict PDE operators, enhancing efficiency and accuracy in finding analytical approximations.

Conclusion: This approach advances understanding of symbolic structures in scientific problems and improves solution processes.

Abstract: Motivated by the remarkable success of artificial intelligence (AI) across
diverse fields, the application of AI to solve scientific problems, often
formulated as partial differential equations (PDEs), has garnered increasing
attention. While most existing research concentrates on theoretical properties
(such as well-posedness, regularity, and continuity) of the solutions,
alongside direct AI-driven methods for solving PDEs, the challenge of
uncovering symbolic relationships within these equations remains largely
unexplored. In this paper, we propose leveraging large language models (LLMs)
to learn such symbolic relationships. Our results demonstrate that LLMs can
effectively predict the operators involved in PDE solutions by utilizing the
symbolic information in the PDEs both theoretically and numerically.
Furthermore, we show that discovering these symbolic relationships can
substantially improve both the efficiency and accuracy of symbolic machine
learning for finding analytical approximation of PDE solutions, delivering a
fully interpretable solution pipeline. This work opens new avenues for
understanding the symbolic structure of scientific problems and advancing their
solution processes.

</details>


### [499] [A finite-sample bound for identifying partially observed linear switched systems from a single trajectory](https://arxiv.org/pdf/2503.13766)
*Daniel Racz, Mihaly Petreczky, Balint Daroczy*

Main category: cs.LG

TL;DR: A finite-sample probabilistic bound for parameter estimation error in Linear Switched Systems is derived, ensuring statistical consistency under quadratic stability.


<details>
  <summary>Details</summary>
Motivation: To provide a theoretical guarantee for the parameter estimation error of a system identification algorithm in Linear Switched Systems, addressing the lack of finite-sample bounds in single-trajectory settings.

Method: The algorithm estimates Markov parameters from a single trajectory and uses a variant of the Ho-Kalman algorithm to recover system matrices. The proof uses weakly dependent processes theory.

Result: The derived bound guarantees statistical consistency under quadratic stability assumptions.

Conclusion: This work presents the first finite-sample bound for the algorithm in single-trajectory settings, advancing theoretical understanding of system identification for Linear Switched Systems.

Abstract: We derive a finite-sample probabilistic bound on the parameter estimation
error of a system identification algorithm for Linear Switched Systems. The
algorithm estimates Markov parameters from a single trajectory and applies a
variant of the Ho-Kalman algorithm to recover the system matrices. Our bound
guarantees statistical consistency under the assumption that the true system
exhibits quadratic stability. The proof leverages the theory of weakly
dependent processes. To the best of our knowledge, this is the first
finite-sample bound for this algorithm in the single-trajectory setting.

</details>


### [500] [Task-Specific Data Selection for Instruction Tuning via Monosemantic Neuronal Activations](https://arxiv.org/pdf/2503.15573)
*Da Ma, Gonghu Shang, Zhi Chen, Libo Qin, Yijie Luo, Lei Pan, Shuai Fan, Lu Chen, Kai Yu*

Main category: cs.LG

TL;DR: The paper introduces a model-centric data selection method using sparse autoencoders to improve task-specific performance in instruction-tuned LLMs by capturing internal neuron activations.


<details>
  <summary>Details</summary>
Motivation: Existing data selection methods for LLMs, like influence-based or distribution alignment, often fail to reflect how models internally process samples, limiting task-specific performance.

Method: The authors represent samples by their neuron activation patterns, use sparse autoencoders to disentangle polysemantic activations, and introduce a similarity metric for better data selection.

Result: Experiments show the method outperforms baselines in stability and task-specific performance across various datasets and models.

Conclusion: The proposed approach effectively bridges the gap in data selection by leveraging internal model computations, enhancing LLM performance on target tasks.

Abstract: Instruction tuning improves the ability of large language models (LLMs) to
follow diverse human instructions, but achieving strong performance on specific
target tasks remains challenging. A critical bottleneck is selecting the most
relevant data to maximize task-specific performance. Existing data selection
approaches include unstable influence-based methods and more stable
distribution alignment methods, the latter of which critically rely on the
underlying sample representation. In practice, most distribution alignment
methods, from shallow features (e.g., BM25) to neural embeddings (e.g., BGE,
LLM2Vec), may fail to capture how the model internally processes samples. To
bridge this gap, we adopt a model-centric strategy in which each sample is
represented by its neuronal activation pattern in the model, directly
reflecting internal computation. However, directly using raw neuron activations
leads to spurious similarity between unrelated samples due to neuron
polysemanticity, where a single neuron may respond to multiple, unrelated
concepts. To address this, we employ sparse autoencoders to disentangle
polysemantic activations into sparse, monosemantic representations, and
introduce a dedicated similarity metric for this space to better identify
task-relevant data. Comprehensive experiments across multiple instruction
datasets, models, tasks, and selection ratios show that our approach
consistently outperforms existing data selection baselines in both stability
and task-specific performance.

</details>


### [501] [IPGO: Indirect Prompt Gradient Optimization for Parameter-Efficient Prompt-level Fine-Tuning on Text-to-Image Models](https://arxiv.org/pdf/2503.21812)
*Jianping Ye, Michel Wedel, Kunpeng Zhang*

Main category: cs.LG

TL;DR: IPGO and IPGO+ are novel frameworks for fine-tuning text-to-image diffusion models, improving alignment with semantics, aesthetics, and human preferences. They outperform SOTA methods with high win-rates.


<details>
  <summary>Details</summary>
Motivation: Address suboptimal alignment in text-to-image diffusion models with content semantics, aesthetics, and human preferences.

Method: Proposes IPGO for prompt-level fine-tuning using gradient optimization of injected embeddings with constraints. IPGO+ adds a cross-attention mechanism. Evaluated with reward models on three datasets.

Result: IPGO achieves >99% win-rate in prompt-wise learning; IPGO+ achieves 75% win-rate in prompt-batch learning, outperforming SOTA benchmarks.

Conclusion: IPGO and IPGO+ significantly enhance image quality with minimal data and resources, demonstrating generalizability and superior performance.

Abstract: Text-to-Image Diffusion models excel at generating images from text prompts
but often exhibit suboptimal alignment with content semantics, aesthetics, and
human preferences. To address these limitations, this study proposes a novel
parameter-efficient framework, Indirect Prompt Gradient Optimization (IPGO),
for prompt-level diffusion model fine-tuning. IPGO enhances prompt embeddings
by injecting continuously differentiable embeddings at the beginning and end of
the prompt embeddings, leveraging low-rank structures with the flexibility and
nonlinearity from rotations. This approach enables gradient-based optimization
of injected embeddings under range, orthonormality, and conformity constraints,
effectively narrowing the search space, promoting a stable solution, and
ensuring alignment between the embeddings of the injected embeddings and the
original prompt. Its extension IPGO+ adds a parameter-free cross-attention
mechanism on the prompt embedding to enforce dependencies between the original
prompt and the inserted embeddings. We conduct extensive evaluations through
prompt-wise (IPGO) and prompt-batch (IPGO+) training using three reward models
of image aesthetics, image-text alignment, and human preferences across three
datasets of varying complexity. The results show that IPGO consistently
outperforms SOTA benchmarks, including stable diffusion v1.5 with raw prompts,
text-embedding-based methods (TextCraftor), training-based methods (DRaFT and
DDPO), and training-free methods (DPO-Diffusion, Promptist, and ChatGPT-4o).
Specifically, IPGO achieves a win-rate exceeding 99% in prompt-wise learning,
and IPGO+ achieves a comparable, but often better performance against current
SOTAs (a 75% win rate) in prompt-batch learning. Moreover, we illustrate IPGO's
generalizability and its capability to significantly enhance image quality
while requiring minimal data and resources.

</details>


### [502] [Mitigating Many-Shot Jailbreaking](https://arxiv.org/pdf/2504.09604)
*Christopher M. Ackerman, Nina Panickssery*

Main category: cs.LG

TL;DR: The paper explores mitigation techniques for Many-shot jailbreaking (MSJ) attacks on LLMs, showing that combining fine-tuning and input sanitization significantly reduces attack effectiveness while maintaining model performance.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of modern LLMs to MSJ attacks, where adversarial prompts exploit long context windows to bypass safety training.

Method: Evaluates fine-tuning and input sanitization approaches, both individually and combined, to mitigate MSJ attacks.

Result: Combined techniques significantly reduce MSJ attack effectiveness without harming benign task performance.

Conclusion: The proposed approach could effectively mitigate MSJ vulnerabilities if integrated into model safety post-training.

Abstract: Many-shot jailbreaking (MSJ) is an adversarial technique that exploits the
long context windows of modern LLMs to circumvent model safety training by
including in the prompt many examples of a "fake" assistant responding
inappropriately before the final request. With enough examples, the model's
in-context learning abilities override its safety training, and it responds as
if it were the "fake" assistant. In this work, we probe the effectiveness of
different fine-tuning and input sanitization approaches on mitigating MSJ
attacks, alone and in combination. We find incremental mitigation effectiveness
for each, and show that the combined techniques significantly reduce the
effectiveness of MSJ attacks, while retaining model performance in benign
in-context learning and conversational tasks. We suggest that our approach
could meaningfully ameliorate this vulnerability if incorporated into model
safety post-training.

</details>


### [503] [Quantization Error Propagation: Revisiting Layer-Wise Post-Training Quantization](https://arxiv.org/pdf/2504.09629)
*Yamato Arai, Yuma Ichikawa*

Main category: cs.LG

TL;DR: QEP improves layer-wise PTQ by addressing quantization error propagation, enhancing accuracy, especially in low-bit regimes.


<details>
  <summary>Details</summary>
Motivation: Existing layer-wise PTQ methods suffer from growing quantization errors across layers, degrading performance, particularly in low-bit settings.

Method: Proposes Quantization Error Propagation (QEP), a lightweight framework that propagates and compensates for quantization errors, with tunable propagation to balance performance and overhead.

Result: QEP-enhanced PTQ achieves higher accuracy than existing methods, with notable gains in extremely low-bit quantization.

Conclusion: QEP effectively addresses core limitations of layer-wise PTQ, offering a scalable and adaptable solution for improved quantization performance.

Abstract: Layer-wise PTQ is a promising technique for compressing large language models
(LLMs), due to its simplicity and effectiveness without requiring retraining.
However, recent progress in this area is saturating, underscoring the need to
revisit its core limitations and explore further improvements. We address this
challenge by identifying a key limitation of existing layer-wise PTQ methods:
the growth of quantization errors across layers significantly degrades
performance, particularly in low-bit regimes. To address this fundamental
issue, we propose Quantization Error Propagation (QEP), a general, lightweight,
and scalable framework that enhances layer-wise PTQ by explicitly propagating
quantization errors and compensating for accumulated errors. QEP also offers a
tunable propagation mechanism that prevents overfitting and controls
computational overhead, enabling the framework to adapt to various
architectures and resource budgets. Extensive experiments on several LLMs
demonstrate that QEP-enhanced layer-wise PTQ achieves substantially higher
accuracy than existing methods. Notably, the gains are most pronounced in the
extremely low-bit quantization regime.

</details>


### [504] [Towards Anomaly-Aware Pre-Training and Fine-Tuning for Graph Anomaly Detection](https://arxiv.org/pdf/2504.14250)
*Yunhui Liu, Jiashun Cheng, Yiqing Lin, Qizhuo Xie, Jia Li, Fugee Tsung, Hongzhi Yin, Tao Zheng, Jianhua Zhao, Tieke He*

Main category: cs.LG

TL;DR: APF is a framework for graph anomaly detection (GAD) that addresses label scarcity and homophily disparity through anomaly-aware pre-training and fine-tuning, achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: The challenges in GAD include label scarcity due to costly annotations and homophily disparity at node and class levels.

Method: APF uses Rayleigh Quotient for anomaly-aware pre-training, spectral polynomial filters for dual representations, and a gated fusion mechanism with anomaly-aware regularization during fine-tuning.

Result: APF outperforms state-of-the-art baselines on 10 benchmark datasets and theoretically achieves linear separability under mild conditions.

Conclusion: APF effectively mitigates GAD challenges and demonstrates strong performance, validated by experiments.

Abstract: Graph anomaly detection (GAD) has garnered increasing attention in recent
years, yet remains challenging due to two key factors: (1) label scarcity
stemming from the high cost of annotations and (2) homophily disparity at node
and class levels. In this paper, we introduce Anomaly-Aware Pre-Training and
Fine-Tuning (APF), a targeted and effective framework to mitigate the above
challenges in GAD. In the pre-training stage, APF incorporates node-specific
subgraphs selected via the Rayleigh Quotient, a label-free anomaly metric, into
the learning objective to enhance anomaly awareness. It further introduces two
learnable spectral polynomial filters to jointly learn dual representations
that capture both general semantics and subtle anomaly cues. During
fine-tuning, a gated fusion mechanism adaptively integrates pre-trained
representations across nodes and dimensions, while an anomaly-aware
regularization loss encourages abnormal nodes to preserve more anomaly-relevant
information. Furthermore, we theoretically show that APF tends to achieve
linear separability under mild conditions. Comprehensive experiments on 10
benchmark datasets validate the superior performance of APF in comparison to
state-of-the-art baselines.

</details>


### [505] [An Axiomatic Assessment of Entropy- and Variance-based Uncertainty Quantification in Regression](https://arxiv.org/pdf/2504.18433)
*Christopher B√ºlte, Yusuf Sale, Timo L√∂hr, Paul Hofman, Gitta Kutyniok, Eyke H√ºllermeier*

Main category: cs.LG

TL;DR: The paper introduces axioms to evaluate uncertainty measures in regression, generalizing common approaches and analyzing entropy- and variance-based measures.


<details>
  <summary>Details</summary>
Motivation: Address the gap in formal justification and evaluation of uncertainty measures in regression settings.

Method: Proposes axioms for assessing aleatoric, epistemic, and total uncertainty, using a predictive exponential family to generalize uncertainty representation.

Result: Identifies limitations of entropy- and variance-based measures, providing a principled foundation for uncertainty quantification.

Conclusion: Offers theoretical insights and practical guidelines for reliable uncertainty assessment in regression.

Abstract: Uncertainty quantification (UQ) is crucial in machine learning, yet most
(axiomatic) studies of uncertainty measures focus on classification, leaving a
gap in regression settings with limited formal justification and evaluations.
In this work, we introduce a set of axioms to rigorously assess measures of
aleatoric, epistemic, and total uncertainty in supervised regression. By
utilizing a predictive exponential family, we can generalize commonly used
approaches for uncertainty representation and corresponding uncertainty
measures. More specifically, we analyze the widely used entropy- and
variance-based measures regarding limitations and challenges. Our findings
provide a principled foundation for uncertainty quantification in regression,
offering theoretical insights and practical guidelines for reliable uncertainty
assessment.

</details>


### [506] [Entropy-Guided Sampling of Flat Modes in Discrete Spaces](https://arxiv.org/pdf/2505.02296)
*Pinaki Mohanty, Riddhiman Bhattacharya, Ruqi Zhang*

Main category: cs.LG

TL;DR: Proposes Entropic Discrete Langevin Proposal (EDLP) for sampling from flat modes in discrete spaces, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Flat modes are robust solutions but underexplored; existing methods struggle to sample them effectively.

Method: EDLP incorporates local entropy via a continuous auxiliary variable under a joint distribution, guiding the sampler toward flat modes.

Result: EDLP shows superior performance in tasks like Bernoulli distribution, Boltzmann machines, and combinatorial optimization.

Conclusion: EDLP effectively addresses the challenge of sampling from flat modes with theoretical guarantees and empirical success.

Abstract: Sampling from flat modes in discrete spaces is a crucial yet underexplored
problem. Flat modes represent robust solutions and have broad applications in
combinatorial optimization and discrete generative modeling. However, existing
sampling algorithms often overlook the mode volume and struggle to capture flat
modes effectively. To address this limitation, we propose \emph{Entropic
Discrete Langevin Proposal} (EDLP), which incorporates local entropy into the
sampling process through a continuous auxiliary variable under a joint
distribution. The local entropy term guides the discrete sampler toward flat
modes with a small overhead. We provide non-asymptotic convergence guarantees
for EDLP in locally log-concave discrete distributions. Empirically, our method
consistently outperforms traditional approaches across tasks that require
sampling from flat basins, including Bernoulli distribution, restricted
Boltzmann machines, combinatorial optimization, and binary neural networks.

</details>


### [507] [34 Examples of LLM Applications in Materials Science and Chemistry: Towards Automation, Assistants, Agents, and Accelerated Scientific Discovery](https://arxiv.org/pdf/2505.03049)
*Yoel Zimmermann, Adib Bazgir, Alexander Al-Feghali, Mehrad Ansari, Joshua Bocarsly, L. Catherine Brinson, Yuan Chiang, Defne Circi, Min-Hsueh Chiu, Nathan Daelman, Matthew L. Evans, Abhijeet S. Gangan, Janine George, Hassan Harb, Ghazal Khalighinejad, Sartaaj Takrim Khan, Sascha Klawohn, Magdalena Lederbauer, Soroush Mahjoubi, Bernadette Mohr, Seyed Mohamad Moosavi, Aakash Naik, Aleyna Beste Ozhan, Dieter Plessers, Aritra Roy, Fabian Sch√∂ppach, Philippe Schwaller, Carla Terboven, Katharina Ueltzen, Yue Wu, Shang Zhu, Jan Janssen, Calvin Li, Ian Foster, Ben Blaiszik*

Main category: cs.LG

TL;DR: LLMs are transforming materials science and chemistry by enhancing property prediction, design, automation, and knowledge extraction, as demonstrated by 34 projects from a hackathon.


<details>
  <summary>Details</summary>
Motivation: To explore and showcase the expanding capabilities of LLMs in materials science and chemistry, highlighting their versatility and potential impact on research workflows.

Method: Review of 34 projects from the Large Language Model Hackathon, covering seven key research areas like property prediction, design, automation, and knowledge extraction.

Result: LLMs prove effective as predictive models and prototyping tools, with improvements in reasoning and training data enhancing their performance in low-data and interdisciplinary settings.

Conclusion: While LLMs offer significant opportunities for scientific research, challenges like reliability and interpretability require further exploration and refinement.

Abstract: Large Language Models (LLMs) are reshaping many aspects of materials science
and chemistry research, enabling advances in molecular property prediction,
materials design, scientific automation, knowledge extraction, and more. Recent
developments demonstrate that the latest class of models are able to integrate
structured and unstructured data, assist in hypothesis generation, and
streamline research workflows. To explore the frontier of LLM capabilities
across the research lifecycle, we review applications of LLMs through 34 total
projects developed during the second annual Large Language Model Hackathon for
Applications in Materials Science and Chemistry, a global hybrid event. These
projects spanned seven key research areas: (1) molecular and material property
prediction, (2) molecular and material design, (3) automation and novel
interfaces, (4) scientific communication and education, (5) research data
management and automation, (6) hypothesis generation and evaluation, and (7)
knowledge extraction and reasoning from the scientific literature.
Collectively, these applications illustrate how LLMs serve as versatile
predictive models, platforms for rapid prototyping of domain-specific tools,
and much more. In particular, improvements in both open source and proprietary
LLM performance through the addition of reasoning, additional training data,
and new techniques have expanded effectiveness, particularly in low-data
environments and interdisciplinary research. As LLMs continue to improve, their
integration into scientific workflows presents both new opportunities and new
challenges, requiring ongoing exploration, continued refinement, and further
research to address reliability, interpretability, and reproducibility.

</details>


### [508] [Autoencoder-Based Hybrid Replay for Class-Incremental Learning](https://arxiv.org/pdf/2505.05926)
*Milad Khademi Nori, Il-Min Kim, Guanghui Wang*

Main category: cs.LG

TL;DR: Proposes an autoencoder-based hybrid replay (AHR) strategy for class-incremental learning, reducing memory complexity to O(0.1t) while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenges of task confusion and catastrophic forgetting in class-incremental learning, aiming to reduce memory and compute complexities.

Method: Uses a hybrid autoencoder (HAE) as a compressor for exemplar replay, employing energy minimization and repulsive force algorithms for latent space management.

Result: AHR achieves state-of-the-art performance with reduced memory requirements (O(0.1t)) and maintains compute complexity (O(t)).

Conclusion: AHR outperforms baselines in benchmarks, offering efficient memory use and high performance, with plans to open-source the code.

Abstract: In class-incremental learning (CIL), effective incremental learning
strategies are essential to mitigate task confusion and catastrophic
forgetting, especially as the number of tasks $t$ increases. Current exemplar
replay strategies impose $\mathcal{O}(t)$ memory/compute complexities. We
propose an autoencoder-based hybrid replay (AHR) strategy that leverages our
new hybrid autoencoder (HAE) to function as a compressor to alleviate the
requirement for large memory, achieving $\mathcal{O}(0.1 t)$ at the worst case
with the computing complexity of $\mathcal{O}(t)$ while accomplishing
state-of-the-art performance. The decoder later recovers the exemplar data
stored in the latent space, rather than in raw format. Additionally, HAE is
designed for both discriminative and generative modeling, enabling
classification and replay capabilities, respectively. HAE adopts the charged
particle system energy minimization equations and repulsive force algorithm for
the incremental embedding and distribution of new class centroids in its latent
space. Our results demonstrate that AHR consistently outperforms recent
baselines across multiple benchmarks while operating with the same
memory/compute budgets. The source code is included in the supplementary
material and will be open-sourced upon publication.

</details>


### [509] [Relative Overfitting and Accept-Reject Framework](https://arxiv.org/pdf/2505.07783)
*Yanxin Liu, Yunqi Zhang*

Main category: cs.LG

TL;DR: The paper addresses scaling challenges in LLMs by attributing them to noise effects and introduces the 'relative overfitting' concept and AR framework to improve performance efficiently.


<details>
  <summary>Details</summary>
Motivation: To tackle bottlenecks in LLM scaling laws caused by noise effects and diminishing returns, aiming for cost-effective performance improvements.

Method: Proposes the AR framework, leveraging complementary strengths of LLMs and SLMs, validated on diverse NLP tasks and datasets.

Result: Demonstrates better performance improvements with lower costs compared to increasing LLM parameters, showing universal and stable effectiveness.

Conclusion: The AR framework and 'relative overfitting' concept offer potential to overcome scaling bottlenecks in LLMs and other ML domains like CV and AI for science.

Abstract: Currently, the scaling law of Large Language Models (LLMs) faces challenges
and bottlenecks. This paper posits that noise effects, stemming from changes in
the signal-to-noise ratio under diminishing marginal returns, are the root
cause of these issues. To control this noise, we investigated the differences
between models with performance advantages and disadvantages, introducing the
concept of "relative overfitting." Based on their complementary strengths, we
have proposed an application framework, Accept-Reject (AR). In Natural Language
Processing (NLP), we use LLMs and Small Language Models (SLMs) as the medium
for discussion. This framework enables SLMs to exert a universal positive
influence on LLM decision outputs, rather than the intuitively expected
negative influence. We validated our approach using self-built models based on
mainstream architectures and pre-trained mainstream models across multiple
datasets, including basic language modeling, long-context tasks, subject
examination, and question-answering (QA) benchmarks. The results demonstrate
that through our structure, compared to increasing the LLM's parameters, we can
achieve better performance improvements with significantly lower parameter and
computational costs in many scenarios. These improvements are universal,
stable, and effective. Furthermore, we explore the potential of "relative
overfitting" and the AR framework in other machine learning domains, such as
computer vision (CV) and AI for science. We hope the proposed approach can help
scale laws overcome existing bottlenecks.

</details>


### [510] [A Multi-scale Representation Learning Framework for Long-Term Time Series Forecasting](https://arxiv.org/pdf/2505.08199)
*Boshi Gao, Qingjian Ni, Fanbo Ju, Yu Chen, Ziqi Zhao*

Main category: cs.LG

TL;DR: The paper introduces MDMixer, an MLP-based framework for long-term time series forecasting (LTSF), addressing multi-granularity information use, channel-specific attributes, and trend/seasonal components. It outperforms TimeMixer by 4.64% in MAE.


<details>
  <summary>Details</summary>
Motivation: LTSF is challenging due to complex temporal patterns and multi-scale variations. Existing methods underutilize multi-granularity data and ignore channel-specific attributes.

Method: MDMixer disentangles temporal dynamics with multi-scale predictions, dynamically integrates them, and separately models trend and seasonal components.

Result: MDMixer improves MAE by 4.64% over TimeMixer on eight benchmarks, balancing efficiency and interpretability.

Conclusion: MDMixer effectively addresses LTSF challenges, offering superior performance and practical utility.

Abstract: Long-term time series forecasting (LTSF) offers broad utility in practical
settings like energy consumption and weather prediction. Accurately predicting
long-term changes, however, is demanding due to the intricate temporal patterns
and inherent multi-scale variations within time series. This work confronts key
issues in LTSF, including the suboptimal use of multi-granularity information,
the neglect of channel-specific attributes, and the unique nature of trend and
seasonal components, by introducing a proficient MLP-based forecasting
framework. Our method adeptly disentangles complex temporal dynamics using
clear, concurrent predictions across various scales. These multi-scale
forecasts are then skillfully integrated through a system that dynamically
assigns importance to information from different granularities, sensitive to
individual channel characteristics. To manage the specific features of temporal
patterns, a two-pronged structure is utilized to model trend and seasonal
elements independently. Experimental results on eight LTSF benchmarks
demonstrate that MDMixer improves average MAE performance by 4.64% compared to
the recent state-of-the-art MLP-based method (TimeMixer), while achieving an
effective balance between training efficiency and model interpretability.

</details>


### [511] [Density Ratio-based Causal Discovery from Bivariate Continuous-Discrete Data](https://arxiv.org/pdf/2505.08371)
*Takashi Nicholas Maeda, Shohei Shimizu, Hidetoshi Matsui*

Main category: cs.LG

TL;DR: A novel causal discovery method for mixed bivariate data (continuous and discrete) using conditional density ratio monotonicity, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with bivariate data due to reliance on conditional independence tests or unfair comparisons between variable types.

Method: Analyzes monotonicity of the conditional density ratio of the continuous variable given the discrete variable to determine causal direction.

Result: Theoretical proof and experiments show the method's accuracy, surpassing current techniques.

Conclusion: The approach provides a principled, assumption-free way to infer causality in mixed bivariate data.

Abstract: This paper proposes a causal discovery method for mixed bivariate data
consisting of one continuous and one discrete variable. Existing
constraint-based approaches are ineffective in the bivariate setting, as they
rely on conditional independence tests that are not suited to bivariate data.
Score-based methods either impose strong distributional assumptions or face
challenges in fairly comparing causal directions between variables of different
types, due to differences in their information content. We introduce a novel
approach that determines causal direction by analyzing the monotonicity of the
conditional density ratio of the continuous variable, conditioned on different
values of the discrete variable. Our theoretical analysis shows that the
conditional density ratio exhibits monotonicity when the continuous variable
causes the discrete variable, but not in the reverse direction. This property
provides a principled basis for comparing causal directions between variables
of different types, free from strong distributional assumptions and bias
arising from differences in their information content. We demonstrate its
effectiveness through experiments on both synthetic and real-world datasets,
showing superior accuracy compared to existing methods.

</details>


### [512] [Analog Foundation Models](https://arxiv.org/pdf/2505.09663)
*Julian B√ºchel, Iason Chalas, Giovanni Acampa, An Chen, Omobayode Fagbohungbe, Sidney Tsai, Kaoutar El Maghraoui, Manuel Le Gallo, Abbas Rahimi, Abu Sebastian*

Main category: cs.LG

TL;DR: A general method to adapt LLMs for noisy, low-precision analog hardware, retaining performance comparable to 4-bit weight, 8-bit activation baselines.


<details>
  <summary>Details</summary>
Motivation: Analog in-memory computing (AIMC) offers speed and power efficiency but introduces noise and quantization constraints, limiting LLM performance.

Method: A scalable training methodology to robustly adapt LLMs for analog hardware, addressing noise and quantization.

Result: State-of-the-art models like Phi-3-mini-4k-instruct and Llama-3.2-1B-Instruct achieve performance comparable to 4-bit weight, 8-bit activation baselines.

Conclusion: The work bridges the gap between high-capacity LLMs and efficient analog hardware, enabling energy-efficient foundation models.

Abstract: Analog in-memory computing (AIMC) is a promising compute paradigm to improve
speed and power efficiency of neural network inference beyond the limits of
conventional von Neumann-based architectures. However, AIMC introduces
fundamental challenges such as noisy computations and strict constraints on
input and output quantization. Because of these constraints and imprecisions,
off-the-shelf LLMs are not able to achieve 4-bit-level performance when
deployed on AIMC-based hardware. While researchers previously investigated
recovering this accuracy gap on small, mostly vision-based models, a generic
method applicable to LLMs pre-trained on trillions of tokens does not yet
exist. In this work, we introduce a general and scalable method to robustly
adapt LLMs for execution on noisy, low-precision analog hardware. Our approach
enables state-of-the-art models $\unicode{x2013}$ including
Phi-3-mini-4k-instruct and Llama-3.2-1B-Instruct $\unicode{x2013}$ to retain
performance comparable to 4-bit weight, 8-bit activation baselines, despite the
presence of analog noise and quantization constraints. Additionally, we show
that as a byproduct of our training methodology, analog foundation models can
be quantized for inference on low-precision digital hardware. Finally, we show
that our models also benefit from test-time compute scaling, showing better
scaling behavior than models trained with 4-bit weight and 8-bit static input
quantization. Our work bridges the gap between high-capacity LLMs and efficient
analog hardware, offering a path toward energy-efficient foundation models.
Code is available at https://github.com/IBM/analog-foundation-models.

</details>


### [513] [BINGO: A Novel Pruning Mechanism to Reduce the Size of Neural Networks](https://arxiv.org/pdf/2505.09864)
*Aditya Panangat*

Main category: cs.LG

TL;DR: BINGO introduces a one-shot pruning method for neural networks, reducing computational costs while preserving accuracy, addressing the high expenses of large models.


<details>
  <summary>Details</summary>
Motivation: The high cost and computational demands of training large neural networks limit accessibility and sustainability, prompting the need for efficient pruning techniques.

Method: BINGO evaluates weight significance during training, assigning scores to prune insignificant weights in one shot, avoiding iterative processes.

Result: BINGO achieves accuracy-preserving pruning with lower computational and environmental costs compared to existing methods.

Conclusion: BINGO enables efficient model pruning, promoting sustainable and accessible AI development without sacrificing performance.

Abstract: Over the past decade, the use of machine learning has increased
exponentially. Models are far more complex than ever before, growing to
gargantuan sizes and housing millions of weights. Unfortunately, the fact that
large models have become the state of the art means that it often costs
millions of dollars to train and operate them. These expenses not only hurt
companies but also bar non-wealthy individuals from contributing to new
developments and force consumers to pay greater prices for AI. Current methods
used to prune models, such as iterative magnitude pruning, have shown great
accuracy but require an iterative training sequence that is incredibly
computationally and environmentally taxing. To solve this problem, BINGO is
introduced. BINGO, during the training pass, studies specific subsets of a
neural network one at a time to gauge how significant of a role each weight
plays in contributing to a network's accuracy. By the time training is done,
BINGO generates a significance score for each weight, allowing for
insignificant weights to be pruned in one shot. BINGO provides an
accuracy-preserving pruning technique that is less computationally intensive
than current methods, allowing for a world where AI growth does not have to
mean model growth, as well.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [514] [Decision Making in Urban Traffic: A Game Theoretic Approach for Autonomous Vehicles Adhering to Traffic Rules](https://arxiv.org/pdf/2505.10690)
*Keqi Shu, Minghao Ning, Ahmad Alghooneh, Shen Li, Mohammad Pirani, Amir Khajepour*

Main category: cs.MA

TL;DR: A rule-based framework for autonomous vehicle decision-making extracts right-of-way from traffic rules, formulates interactions as a differential game, and finds Nash equilibrium for optimal decisions, tested successfully in simulations and real vehicles.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in urban autonomous vehicle decision-making, such as unpredictable traffic participant behavior and adherence to evolving traffic rules.

Method: Proposes a rule-based framework integrating traffic rules into behavioral parameters and formulating interactions as a differential game to find Nash equilibrium.

Result: The framework enables safe interaction with traffic participants and adherence to traffic rules, validated in simulations and real-world tests.

Conclusion: The proposed method effectively navigates complex urban traffic scenarios by combining rule-based decision-making with game-theoretic interaction modeling.

Abstract: One of the primary challenges in urban autonomous vehicle decision-making and
planning lies in effectively managing intricate interactions with diverse
traffic participants characterized by unpredictable movement patterns.
Additionally, interpreting and adhering to traffic regulations within rapidly
evolving traffic scenarios pose significant hurdles. This paper proposed a
rule-based autonomous vehicle decision-making and planning framework which
extracts right-of-way from traffic rules to generate behavioural parameters,
integrating them to effectively adhere to and navigate through traffic
regulations. The framework considers the strong interaction between traffic
participants mathematically by formulating the decision-making and planning
problem into a differential game. By finding the Nash equilibrium of the
problem, the autonomous vehicle is able to find optimal decisions. The proposed
framework was tested under simulation as well as full-size vehicle platform,
the results show that the ego vehicle is able to safely interact with
surrounding traffic participants while adhering to traffic rules.

</details>


### [515] [Vaiage: A Multi-Agent Solution to Personalized Travel Planning](https://arxiv.org/pdf/2505.10922)
*Binwen Liu, Jiexi Ge, Jiamin Wang*

Main category: cs.MA

TL;DR: Vaiage is a multi-agent framework using LLMs for adaptive, explainable travel planning, outperforming traditional methods in feasibility and quality.


<details>
  <summary>Details</summary>
Motivation: Traditional trip planning platforms are static and lack contextual adaptation, failing to support real-time interaction or intent refinement.

Method: Vaiage employs a graph-structured multi-agent framework with LLMs for intent inference, personalized recommendations, and itinerary synthesis, integrating natural language interaction and map-based feedback.

Result: Vaiage scored 8.5/10 in human-in-the-loop experiments, outperforming variants without strategy (7.2) or external APIs (6.8), with qualitative improvements in itinerary quality.

Conclusion: Combining LLM reasoning with symbolic agent coordination effectively addresses open-ended, real-world planning tasks like travel.

Abstract: Planning trips is a cognitively intensive task involving conflicting user
preferences, dynamic external information, and multi-step temporal-spatial
optimization. Traditional platforms often fall short - they provide static
results, lack contextual adaptation, and fail to support real-time interaction
or intent refinement.
  Our approach, Vaiage, addresses these challenges through a graph-structured
multi-agent framework built around large language models (LLMs) that serve as
both goal-conditioned recommenders and sequential planners. LLMs infer user
intent, suggest personalized destinations and activities, and synthesize
itineraries that align with contextual constraints such as budget, timing,
group size, and weather. Through natural language interaction, structured tool
use, and map-based feedback loops, Vaiage enables adaptive, explainable, and
end-to-end travel planning grounded in both symbolic reasoning and
conversational understanding.
  To evaluate Vaiage, we conducted human-in-the-loop experiments using
rubric-based GPT-4 assessments and qualitative feedback. The full system
achieved an average score of 8.5 out of 10, outperforming the no-strategy (7.2)
and no-external-API (6.8) variants, particularly in feasibility. Qualitative
analysis indicated that agent coordination - especially the Strategy and
Information Agents - significantly improved itinerary quality by optimizing
time use and integrating real-time context. These results demonstrate the
effectiveness of combining LLM reasoning with symbolic agent coordination in
open-ended, real-world planning tasks.

</details>


### [516] [Explaining Strategic Decisions in Multi-Agent Reinforcement Learning for Aerial Combat Tactics](https://arxiv.org/pdf/2505.11311)
*Ardian Selmonaj, Alessandro Antonucci, Adrian Schneider, Michael R√ºegsegger, Matthias Sommer*

Main category: cs.MA

TL;DR: The paper reviews explainability methods for Multi-Agent Reinforcement Learning (MARL) in military air combat, emphasizing transparency for trust and human alignment.


<details>
  <summary>Details</summary>
Motivation: The lack of explainability in MARL limits its practical use in sensitive military contexts, where trust and safety are critical.

Method: The work adapts explainability techniques to simulated air combat scenarios to analyze model behavior.

Result: The study highlights the importance of linking AI tactics with human-understandable reasoning for reliable deployment.

Conclusion: Explainability is crucial for advancing MARL in defense, aiding strategic planning and military training.

Abstract: Artificial intelligence (AI) is reshaping strategic planning, with
Multi-Agent Reinforcement Learning (MARL) enabling coordination among
autonomous agents in complex scenarios. However, its practical deployment in
sensitive military contexts is constrained by the lack of explainability, which
is an essential factor for trust, safety, and alignment with human strategies.
This work reviews and assesses current advances in explainability methods for
MARL with a focus on simulated air combat scenarios. We proceed by adapting
various explainability techniques to different aerial combat scenarios to gain
explanatory insights about the model behavior. By linking AI-generated tactics
with human-understandable reasoning, we emphasize the need for transparency to
ensure reliable deployment and meaningful human-machine interaction. By
illuminating the crucial importance of explainability in advancing MARL for
operational defense, our work supports not only strategic planning but also the
training of military personnel with insightful and comprehensible analyses.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [517] [Concept Drift Guided LayerNorm Tuning for Efficient Multimodal Metaphor Identification](https://arxiv.org/pdf/2505.11237)
*Wenhao Qian, Zhenzhen Hu, Zijie Song, Jia Li*

Main category: cs.MM

TL;DR: The paper introduces CDGLT, a training-efficient framework for multimodal metaphor identification, leveraging Concept Drift and prompt construction to bridge literal and figurative interpretations.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with multimodal metaphors due to unconventional expressions and high computational costs of generative approaches.

Method: CDGLT uses Concept Drift (SLERP of CLIP embeddings) and prompt construction with pre-trained models for efficient feature extraction and fusion.

Result: Achieves state-of-the-art performance on MET-Meme benchmark with reduced training costs.

Conclusion: CDGLT advances efficient and accurate multimodal metaphor understanding.

Abstract: Metaphorical imagination, the ability to connect seemingly unrelated
concepts, is fundamental to human cognition and communication. While
understanding linguistic metaphors has advanced significantly, grasping
multimodal metaphors, such as those found in internet memes, presents unique
challenges due to their unconventional expressions and implied meanings.
Existing methods for multimodal metaphor identification often struggle to
bridge the gap between literal and figurative interpretations. Additionally,
generative approaches that utilize large language models or text-to-image
models, while promising, suffer from high computational costs. This paper
introduces \textbf{C}oncept \textbf{D}rift \textbf{G}uided \textbf{L}ayerNorm
\textbf{T}uning (\textbf{CDGLT}), a novel and training-efficient framework for
multimodal metaphor identification. CDGLT incorporates two key innovations: (1)
Concept Drift, a mechanism that leverages Spherical Linear Interpolation
(SLERP) of cross-modal embeddings from a CLIP encoder to generate a new,
divergent concept embedding. This drifted concept helps to alleviate the gap
between literal features and the figurative task. (2) A prompt construction
strategy, that adapts the method of feature extraction and fusion using
pre-trained language models for the multimodal metaphor identification task.
CDGLT achieves state-of-the-art performance on the MET-Meme benchmark while
significantly reducing training costs compared to existing generative methods.
Ablation studies demonstrate the effectiveness of both Concept Drift and our
adapted LN Tuning approach. Our method represents a significant step towards
efficient and accurate multimodal metaphor understanding. The code is
available:
\href{https://github.com/Qianvenh/CDGLT}{https://github.com/Qianvenh/CDGLT}.

</details>


### [518] [TCC-Bench: Benchmarking the Traditional Chinese Culture Understanding Capabilities of MLLMs](https://arxiv.org/pdf/2505.11275)
*Pengju Xu, Yan Wang, Shuyuan Zhang, Xuan Zhou, Xin Li, Yue Yuan, Fengzhao Li, Shunyuan Zhou, Xingyu Wang, Yi Zhang, Haiying Zhao*

Main category: cs.MM

TL;DR: TCC-Bench is a bilingual VQA benchmark for evaluating MLLMs' understanding of traditional Chinese culture, revealing current models' limitations in culturally grounded reasoning.


<details>
  <summary>Details</summary>
Motivation: Address the limited effectiveness of MLLMs in non-Western cultural contexts by creating a culturally rich benchmark.

Method: Develop TCC-Bench using a semi-automated pipeline with GPT-4o for question generation and human curation for quality.

Result: Current MLLMs struggle with culturally grounded visual content, showing gaps in cultural inclusivity.

Conclusion: Further research is needed to develop culturally inclusive and context-aware multimodal systems.

Abstract: Recent progress in Multimodal Large Language Models (MLLMs) have
significantly enhanced the ability of artificial intelligence systems to
understand and generate multimodal content. However, these models often exhibit
limited effectiveness when applied to non-Western cultural contexts, which
raises concerns about their wider applicability. To address this limitation, we
propose the \textbf{T}raditional \textbf{C}hinese \textbf{C}ulture
understanding \textbf{Bench}mark (\textbf{TCC-Bench}), a bilingual
(\textit{i.e.}, Chinese and English) Visual Question Answering (VQA) benchmark
specifically designed for assessing the understanding of traditional Chinese
culture by MLLMs. TCC-Bench comprises culturally rich and visually diverse
data, incorporating images from museum artifacts, everyday life scenes, comics,
and other culturally significant contexts. We adopt a semi-automated pipeline
that utilizes GPT-4o in text-only mode to generate candidate questions,
followed by human curation to ensure data quality and avoid potential data
leakage. The benchmark also avoids language bias by preventing direct
disclosure of cultural concepts within question texts. Experimental evaluations
across a wide range of MLLMs demonstrate that current models still face
significant challenges when reasoning about culturally grounded visual content.
The results highlight the need for further research in developing culturally
inclusive and context-aware multimodal systems. The code and data can be found
at: https://github.com/Morty-Xu/TCC-Bench.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [519] [SongEval: A Benchmark Dataset for Song Aesthetics Evaluation](https://arxiv.org/pdf/2505.10793)
*Jixun Yao, Guobin Ma, Huixin Xue, Huakang Chen, Chunbo Hao, Yuepeng Jiang, Haohe Liu, Ruibin Yuan, Jin Xu, Wei Xue, Hao Liu, Lei Xie*

Main category: eess.AS

TL;DR: SongEval is a new benchmark dataset for evaluating song aesthetics, addressing the subjectivity of music appreciation with professional ratings across five dimensions.


<details>
  <summary>Details</summary>
Motivation: Existing metrics fail to capture subjective musical appeal, necessitating a dataset like SongEval for better evaluation.

Method: SongEval includes 2,399 full-length songs rated by 16 professionals across five aesthetic dimensions, covering multiple genres and languages.

Result: SongEval outperforms existing metrics in predicting human-perceived musical quality.

Conclusion: SongEval provides a robust, large-scale benchmark for evaluating song aesthetics, bridging the gap between objective metrics and human perception.

Abstract: Aesthetics serve as an implicit and important criterion in song generation
tasks that reflect human perception beyond objective metrics. However,
evaluating the aesthetics of generated songs remains a fundamental challenge,
as the appreciation of music is highly subjective. Existing evaluation metrics,
such as embedding-based distances, are limited in reflecting the subjective and
perceptual aspects that define musical appeal. To address this issue, we
introduce SongEval, the first open-source, large-scale benchmark dataset for
evaluating the aesthetics of full-length songs. SongEval includes over 2,399
songs in full length, summing up to more than 140 hours, with aesthetic ratings
from 16 professional annotators with musical backgrounds. Each song is
evaluated across five key dimensions: overall coherence, memorability,
naturalness of vocal breathing and phrasing, clarity of song structure, and
overall musicality. The dataset covers both English and Chinese songs, spanning
nine mainstream genres. Moreover, to assess the effectiveness of song aesthetic
evaluation, we conduct experiments using SongEval to predict aesthetic scores
and demonstrate better performance than existing objective evaluation metrics
in predicting human-perceived musical quality.

</details>


### [520] [Anti-aliasing of neural distortion effects via model fine tuning](https://arxiv.org/pdf/2505.11375)
*Alistair Carson, Alec Wright, Stefan Bilbao*

Main category: eess.AS

TL;DR: A teacher-student fine-tuning method reduces aliasing in neural guitar distortion models, outperforming oversampling while preserving perceptual similarity.


<details>
  <summary>Details</summary>
Motivation: Neural networks for guitar distortion effects suffer from frequency aliasing due to nonlinear activations, requiring a solution to maintain sound quality.

Method: A teacher-student approach fine-tunes a student model against aliasing-free data derived from sinusoids processed by the original model.

Result: Aliasing is significantly reduced in LSTM and TCN models, often surpassing two times oversampling. Harmonic distortion is affected but remains perceptually acceptable, especially with LSTMs.

Conclusion: The method effectively reduces aliasing while maintaining perceptual similarity, with LSTM models offering the best balance.

Abstract: Neural networks have become ubiquitous with guitar distortion effects
modelling in recent years. Despite their ability to yield perceptually
convincing models, they are susceptible to frequency aliasing when driven by
high frequency and high gain inputs. Nonlinear activation functions create both
the desired harmonic distortion and unwanted aliasing distortion as the
bandwidth of the signal is expanded beyond the Nyquist frequency. Here, we
present a method for reducing aliasing in neural models via a teacher-student
fine tuning approach, where the teacher is a pre-trained model with its weights
frozen, and the student is a copy of this with learnable parameters. The
student is fine-tuned against an aliasing-free dataset generated by passing
sinusoids through the original model and removing non-harmonic components from
the output spectra. Our results show that this method significantly suppresses
aliasing for both long-short-term-memory networks (LSTM) and temporal
convolutional networks (TCN). In the majority of our case studies, the
reduction in aliasing was greater than that achieved by two times oversampling.
One side-effect of the proposed method is that harmonic distortion components
are also affected. This adverse effect was found to be model-dependent, with
the LSTM models giving the best balance between anti-aliasing and preserving
the perceived similarity to an analog reference device.

</details>


### [521] [LipDiffuser: Lip-to-Speech Generation with Conditional Diffusion Models](https://arxiv.org/pdf/2505.11391)
*Danilo de Oliveira, Julius Richter, Tal Peer, Timo Germann*

Main category: eess.AS

TL;DR: LipDiffuser is a diffusion model for lip-to-speech generation, outperforming baselines in quality and speaker similarity.


<details>
  <summary>Details</summary>
Motivation: To synthesize natural and intelligible speech from silent videos using a diffusion model.

Method: Uses MP-ADM as a denoiser, MP-FiLM for visual features, and a neural vocoder for waveform reconstruction.

Result: Outperforms baselines in perceptual quality, speaker similarity, and ASR performance.

Conclusion: LipDiffuser is effective and generalizes well, as confirmed by evaluations and listening experiments.

Abstract: We present LipDiffuser, a conditional diffusion model for lip-to-speech
generation synthesizing natural and intelligible speech directly from silent
video recordings. Our approach leverages the magnitude-preserving ablated
diffusion model (MP-ADM) architecture as a denoiser model. To effectively
condition the model, we incorporate visual features using magnitude-preserving
feature-wise linear modulation (MP-FiLM) alongside speaker embeddings. A neural
vocoder then reconstructs the speech waveform from the generated
mel-spectrograms. Evaluations on LRS3 and TCD-TIMIT demonstrate that
LipDiffuser outperforms existing lip-to-speech baselines in perceptual speech
quality and speaker similarity, while remaining competitive in downstream
automatic speech recognition (ASR). These findings are also supported by a
formal listening experiment. Extensive ablation studies and cross-dataset
evaluation confirm the effectiveness and generalization capabilities of our
approach.

</details>


### [522] [SupertonicTTS: Towards Highly Scalable and Efficient Text-to-Speech System](https://arxiv.org/pdf/2503.23108)
*Hyeongju Kim, Jinhyeok Yang, Yechan Yu, Seunghun Ji, Jacob Morton, Frederik Bous, Joon Byun, Juheon Lee*

Main category: eess.AS

TL;DR: SupertonicTTS is a scalable and efficient TTS system with a lightweight architecture, eliminating G2P modules and external aligners, while maintaining competitive performance.


<details>
  <summary>Details</summary>
Motivation: To improve scalability and efficiency in speech synthesis by reducing architectural complexity and computational overhead.

Method: Uses a speech autoencoder, text-to-latent module with flow-matching, and utterance-level duration predictor. Simplifies the pipeline with raw character-level text and cross-attention.

Result: Achieves competitive performance with reduced complexity and computational overhead.

Conclusion: SupertonicTTS offers a streamlined, efficient TTS solution without sacrificing performance.

Abstract: We present a novel text-to-speech (TTS) system, namely SupertonicTTS, for
improved scalability and efficiency in speech synthesis. SupertonicTTS
comprises three components: a speech autoencoder for continuous latent
representation, a text-to-latent module leveraging flow-matching for
text-to-latent mapping, and an utterance-level duration predictor. To enable a
lightweight architecture, we employ a low-dimensional latent space, temporal
compression of latents, and ConvNeXt blocks. We further simplify the TTS
pipeline by operating directly on raw character-level text and employing
cross-attention for text-speech alignment, thus eliminating the need for
grapheme-to-phoneme (G2P) modules and external aligners. In addition, we
introduce context-sharing batch expansion that accelerates loss convergence and
stabilizes text-speech alignment. Experimental results demonstrate that
SupertonicTTS achieves competitive performance while significantly reducing
architectural complexity and computational overhead compared to contemporary
TTS models. Audio samples demonstrating the capabilities of SupertonicTTS are
available at: https://supertonictts.github.io/.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [523] [GRNN:Recurrent Neural Network based on Ghost Features for Video Super-Resolution](https://arxiv.org/pdf/2505.10577)
*Yutong Guo*

Main category: eess.IV

TL;DR: The paper proposes using 'Ghost features' to reduce redundancy in VSR models and combines it with RNN to address gradient disappearance, improving PSNR and SSIM.


<details>
  <summary>Details</summary>
Motivation: Feature redundancy in VSR models is rarely discussed, and the paper aims to address this by leveraging 'Ghost features' and improving temporal modeling.

Method: The approach uses 'Ghost features' to reduce redundancy and integrates them with RNN to model time series, using current and next frames, previous output, and hidden state as inputs.

Result: Experiments show improved PSNR and SSIM, with better preservation of texture details in videos.

Conclusion: The proposed method effectively reduces redundancy and enhances video super-resolution performance.

Abstract: Modern video super-resolution (VSR) systems based on convolutional neural
networks (CNNs) require huge computational costs. The problem of feature
redundancy is present in most models in many domains, but is rarely discussed
in VSR. We experimentally observe that many features in VSR models are also
similar to each other, so we propose to use "Ghost features" to reduce this
redundancy. We also analyze the so-called "gradient disappearance" phenomenon
generated by the conventional recurrent convolutional network (RNN) model, and
combine the Ghost module with RNN to complete the modeling on time series. The
current frame is used as input to the model together with the next frame, the
output of the previous frame and the hidden state. Extensive experiments on
several benchmark models and datasets show that the PSNR and SSIM of our
proposed modality are improved to some extent. Some texture details in the
video are also better preserved.

</details>


### [524] [ExploreGS: a vision-based low overhead framework for 3D scene reconstruction](https://arxiv.org/pdf/2505.10578)
*Yunji Feng, Chengpu Yu, Fengrui Ran, Zhi Yang, Yinni Liu*

Main category: eess.IV

TL;DR: ExploreGS is a vision-based 3D scene reconstruction framework for drones, replacing lidar with RGB images for cost-effective, high-quality results.


<details>
  <summary>Details</summary>
Motivation: To provide a low-cost, efficient alternative to lidar-based 3D reconstruction for drones.

Method: Uses RGB images and integrates scene exploration with reconstruction, employing a BoW model for real-time processing and on-board 3DGS training.

Result: Achieves high-quality reconstruction comparable to state-of-the-art methods, even on resource-constrained devices.

Conclusion: ExploreGS is an efficient, cost-effective solution for drone-based 3D scene reconstruction.

Abstract: This paper proposes a low-overhead, vision-based 3D scene reconstruction
framework for drones, named ExploreGS. By using RGB images, ExploreGS replaces
traditional lidar-based point cloud acquisition process with a vision model,
achieving a high-quality reconstruction at a lower cost. The framework
integrates scene exploration and model reconstruction, and leverags a
Bag-of-Words(BoW) model to enable real-time processing capabilities, therefore,
the 3D Gaussian Splatting (3DGS) training can be executed on-board.
Comprehensive experiments in both simulation and real-world environments
demonstrate the efficiency and applicability of the ExploreGS framework on
resource-constrained devices, while maintaining reconstruction quality
comparable to state-of-the-art methods.

</details>


### [525] [MOSAIC: A Multi-View 2.5D Organ Slice Selector with Cross-Attentional Reasoning for Anatomically-Aware CT Localization in Medical Organ Segmentation](https://arxiv.org/pdf/2505.10672)
*Hania Ghouse, Muzammil Behzad*

Main category: eess.IV

TL;DR: A novel anatomically-aware slice selector pipeline using a vision-language model (VLM) improves multi-organ segmentation by filtering irrelevant slices, introducing a new metric (SLC) for localization fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing 3D segmentation is resource-heavy, while 2D methods lack contextual awareness. The paper aims to address these inefficiencies.

Method: Proposes a VLM-based pipeline for cross-view organ presence detection using tri-slice (2.5D) representations, selectively retaining relevant slices.

Result: Substantial improvements over baselines, with accurate and reliable organ-focused slice filtering, reducing segmentation costs.

Conclusion: The method efficiently filters slices, maintaining anatomical fidelity and reducing computational burden for downstream tasks.

Abstract: Efficient and accurate multi-organ segmentation from abdominal CT volumes is
a fundamental challenge in medical image analysis. Existing 3D segmentation
approaches are computationally and memory intensive, often processing entire
volumes that contain many anatomically irrelevant slices. Meanwhile, 2D methods
suffer from class imbalance and lack cross-view contextual awareness. To
address these limitations, we propose a novel, anatomically-aware slice
selector pipeline that reduces input volume prior to segmentation. Our unified
framework introduces a vision-language model (VLM) for cross-view organ
presence detection using fused tri-slice (2.5D) representations from axial,
sagittal, and coronal planes. Our proposed model acts as an "expert" in
anatomical localization, reasoning over multi-view representations to
selectively retain slices with high structural relevance. This enables
spatially consistent filtering across orientations while preserving contextual
cues. More importantly, since standard segmentation metrics such as Dice or IoU
fail to measure the spatial precision of such slice selection, we introduce a
novel metric, Slice Localization Concordance (SLC), which jointly captures
anatomical coverage and spatial alignment with organ-centric reference slices.
Unlike segmentation-specific metrics, SLC provides a model-agnostic evaluation
of localization fidelity. Our model offers substantial improvement gains
against several baselines across all organs, demonstrating both accurate and
reliable organ-focused slice filtering. These results show that our method
enables efficient and spatially consistent organ filtering, thereby
significantly reducing downstream segmentation cost while maintaining high
anatomical fidelity.

</details>


### [526] [ROIsGAN: A Region Guided Generative Adversarial Framework for Murine Hippocampal Subregion Segmentation](https://arxiv.org/pdf/2505.10687)
*Sayed Mehedi Azim, Brian Corbett, Iman Dehzangi*

Main category: eess.IV

TL;DR: The paper introduces ROIsGAN, a novel GAN-based method for automated segmentation of hippocampal subregions from IHC images, outperforming existing models by 1-10% in Dice score and up to 11% in IoU.


<details>
  <summary>Details</summary>
Motivation: Accurate segmentation of hippocampal subregions (DG, CA1, CA3) from IHC images is crucial for understanding disease mechanisms and therapeutic interventions, but no automated methods exist.

Method: Proposes ROIsGAN, a region-guided U-Net-based GAN, using adversarial learning and a novel discriminator loss combining Dice and binary cross-entropy for enhanced boundary delineation.

Result: ROIsGAN outperforms conventional models, achieving significant performance gains (1-10% Dice, up to 11% IoU), especially under challenging staining conditions.

Conclusion: The work provides foundational datasets and tools for automated hippocampal segmentation, advancing neuroscience research with scalable, high-precision analysis.

Abstract: The hippocampus, a critical brain structure involved in memory processing and
various neurodegenerative and psychiatric disorders, comprises three key
subregions: the dentate gyrus (DG), Cornu Ammonis 1 (CA1), and Cornu Ammonis 3
(CA3). Accurate segmentation of these subregions from histological tissue
images is essential for advancing our understanding of disease mechanisms,
developmental dynamics, and therapeutic interventions. However, no existing
methods address the automated segmentation of hippocampal subregions from
tissue images, particularly from immunohistochemistry (IHC) images. To bridge
this gap, we introduce a novel set of four comprehensive murine hippocampal IHC
datasets featuring distinct staining modalities: cFos, NeuN, and multiplexed
stains combining cFos, NeuN, and either {\Delta}FosB or GAD67, capturing
structural, neuronal activity, and plasticity associated information.
Additionally, we propose ROIsGAN, a region-guided U-Net-based generative
adversarial network tailored for hippocampal subregion segmentation. By
leveraging adversarial learning, ROIsGAN enhances boundary delineation and
structural detail refinement through a novel region-guided discriminator loss
combining Dice and binary cross-entropy loss. Evaluated across DG, CA1, and CA3
subregions, ROIsGAN consistently outperforms conventional segmentation models,
achieving performance gains ranging from 1-10% in Dice score and up to 11% in
Intersection over Union (IoU), particularly under challenging staining
conditions. Our work establishes foundational datasets and methods for
automated hippocampal segmentation, enabling scalable, high-precision analysis
of tissue images in neuroscience research. Our generated datasets, proposed
model as a standalone tool, and its corresponding source code are publicly
available at: https://github.com/MehediAzim/ROIsGAN

</details>


### [527] [Predicting Risk of Pulmonary Fibrosis Formation in PASC Patients](https://arxiv.org/pdf/2505.10691)
*Wanying Dou, Gorkem Durak, Koushik Biswas, Ziliang Hong, Andrea Mia Bejar, Elif Keles, Kaan Akin, Sukru Mehmet Erturk, Alpay Medetalibeyoglu, Marc Sala, Alexander Misharin, Hatice Savas, Mary Salvatore, Sachin Jambawalikar, Drew Torigian, Jayaram K. Udupa, Ulas Bagci*

Main category: eess.IV

TL;DR: The paper introduces a deep learning and radiomics framework for predicting lung fibrosis in Long COVID patients using chest CT scans, achieving high accuracy and AUC.


<details>
  <summary>Details</summary>
Motivation: Address the uncertainty and challenges in diagnosing and managing Post-Acute Sequelae of COVID-19 (PASC), particularly lung fibrosis, due to its heterogeneous symptoms.

Method: A multi-center chest CT analysis framework combining deep learning (CNNs) and radiomics for fibrosis prediction, including Grad-CAM visualization and feature extraction.

Result: Achieved 82.2% accuracy and 85.5% AUC in classification tasks, demonstrating clinical relevance for PASC-related lung fibrosis prediction.

Conclusion: Deep learning-driven methods show promise for early detection and risk assessment of PASC-related lung fibrosis, offering a novel approach in the literature.

Abstract: While the acute phase of the COVID-19 pandemic has subsided, its long-term
effects persist through Post-Acute Sequelae of COVID-19 (PASC), commonly known
as Long COVID. There remains substantial uncertainty regarding both its
duration and optimal management strategies. PASC manifests as a diverse array
of persistent or newly emerging symptoms--ranging from fatigue, dyspnea, and
neurologic impairments (e.g., brain fog), to cardiovascular, pulmonary, and
musculoskeletal abnormalities--that extend beyond the acute infection phase.
This heterogeneous presentation poses substantial challenges for clinical
assessment, diagnosis, and treatment planning. In this paper, we focus on
imaging findings that may suggest fibrotic damage in the lungs, a critical
manifestation characterized by scarring of lung tissue, which can potentially
affect long-term respiratory function in patients with PASC. This study
introduces a novel multi-center chest CT analysis framework that combines deep
learning and radiomics for fibrosis prediction. Our approach leverages
convolutional neural networks (CNNs) and interpretable feature extraction,
achieving 82.2% accuracy and 85.5% AUC in classification tasks. We demonstrate
the effectiveness of Grad-CAM visualization and radiomics-based feature
analysis in providing clinically relevant insights for PASC-related lung
fibrosis prediction. Our findings highlight the potential of deep
learning-driven computational methods for early detection and risk assessment
of PASC-related lung fibrosis--presented for the first time in the literature.

</details>


### [528] [Adaptive Spatial Transcriptomics Interpolation via Cross-modal Cross-slice Modeling](https://arxiv.org/pdf/2505.10729)
*NingFeng Que, Xiaofei Wang, Jingjing Chen, Yixuan Jiang, Chao Li*

Main category: eess.IV

TL;DR: C2-STi proposes a method to interpolate missing spatial transcriptomics (ST) slices, addressing challenges like tissue heterogeneity and gene correlations, using distance-aware modulation, gene co-expression, and cross-modal alignment with H&E images.


<details>
  <summary>Details</summary>
Motivation: Spatial transcriptomics (ST) analysis is limited by missing intermediate slices and high costs, hindering 3D insights. C2-STi aims to interpolate missing slices for comprehensive ST analysis.

Method: C2-STi uses: 1) distance-aware local structural modulation for cross-slice deformations, 2) pyramid gene co-expression correlation for multi-scale gene associations, and 3) cross-modal alignment with H&E images for cellular feature alignment.

Result: Experiments show C2-STi outperforms state-of-the-art methods in single-slice and multi-slice ST interpolation.

Conclusion: C2-STi effectively interpolates missing ST slices, enhancing spatial transcriptomics analysis by addressing key challenges.

Abstract: Spatial transcriptomics (ST) is a promising technique that characterizes the
spatial gene profiling patterns within the tissue context. Comprehensive ST
analysis depends on consecutive slices for 3D spatial insights, whereas the
missing intermediate tissue sections and high costs limit the practical
feasibility of generating multi-slice ST. In this paper, we propose C2-STi, the
first attempt for interpolating missing ST slices at arbitrary intermediate
positions between adjacent ST slices. Despite intuitive, effective ST
interpolation presents significant challenges, including 1) limited continuity
across heterogeneous tissue sections, 2) complex intrinsic correlation across
genes, and 3) intricate cellular structures and biological semantics within
each tissue section. To mitigate these challenges, in C2-STi, we design 1) a
distance-aware local structural modulation module to adaptively capture
cross-slice deformations and enhance positional correlations between ST slices,
2) a pyramid gene co-expression correlation module to capture multi-scale
biological associations among genes, and 3) a cross-modal alignment module that
integrates the ST-paired hematoxylin and eosin (H&E)-stained images to filter
and align the essential cellular features across ST and H\&E images. Extensive
experiments on the public dataset demonstrate our superiority over
state-of-the-art approaches on both single-slice and multi-slice ST
interpolation. Codes are available at
https://github.com/XiaofeiWang2018/C2-STi.

</details>


### [529] [Pretrained hybrid transformer for generalizable cardiac substructures segmentation from contrast and non-contrast CTs in lung and breast cancers](https://arxiv.org/pdf/2505.10855)
*Aneesh Rangnekar, Nikhil Mankuzhy, Jonas Willmann, Chloe Choi, Abraham Wu, Maria Thor, Andreas Rimner, Harini Veeraraghavan*

Main category: eess.IV

TL;DR: A hybrid transformer convolutional network (HTN) was refined to segment cardiac substructures in lung and breast cancer patients, showing robustness to imaging contrasts and scan positions, outperforming benchmarks with fewer training cases.


<details>
  <summary>Details</summary>
Motivation: AI segmentations for radiation treatment planning often fail in clinical cases differing from training data, necessitating a robust model adaptable to varied imaging and patient conditions.

Method: A pretrained transformer was refined into an HTN, trained on balanced datasets of contrast-enhanced and non-contrast CT scans, and evaluated on validation sets with diverse patient characteristics.

Result: The HTN achieved similar accuracy to an oracle model with half the training cases, outperformed benchmarks, and showed robustness to imaging contrasts and scan positions.

Conclusion: The HTN provides reliable cardiac substructure segmentation under varied conditions, meeting clinical requirements with reduced labeled data needs.

Abstract: AI automated segmentations for radiation treatment planning (RTP) can
deteriorate when applied in clinical cases with different characteristics than
training dataset. Hence, we refined a pretrained transformer into a hybrid
transformer convolutional network (HTN) to segment cardiac substructures lung
and breast cancer patients acquired with varying imaging contrasts and patient
scan positions. Cohort I, consisting of 56 contrast-enhanced (CECT) and 124
non-contrast CT (NCCT) scans from patients with non-small cell lung cancers
acquired in supine position, was used to create oracle with all 180 training
cases and balanced (CECT: 32, NCCT: 32 training) HTN models. Models were
evaluated on a held-out validation set of 60 cohort I patients and 66 patients
with breast cancer from cohort II acquired in supine (n=45) and prone (n=21)
positions. Accuracy was measured using DSC, HD95, and dose metrics. Publicly
available TotalSegmentator served as the benchmark. The oracle and balanced
models were similarly accurate (DSC Cohort I: 0.80 \pm 0.10 versus 0.81 \pm
0.10; Cohort II: 0.77 \pm 0.13 versus 0.80 \pm 0.12), outperforming
TotalSegmentator. The balanced model, using half the training cases as oracle,
produced similar dose metrics as manual delineations for all cardiac
substructures. This model was robust to CT contrast in 6 out of 8 substructures
and patient scan position variations in 5 out of 8 substructures and showed low
correlations of accuracy to patient size and age. A HTN demonstrated robustly
accurate (geometric and dose metrics) cardiac substructures segmentation from
CTs with varying imaging and patient characteristics, one key requirement for
clinical use. Moreover, the model combining pretraining with balanced
distribution of NCCT and CECT scans was able to provide reliably accurate
segmentations under varied conditions with far fewer labeled datasets compared
to an oracle model.

</details>


### [530] [Generative Models in Computational Pathology: A Comprehensive Survey on Methods, Applications, and Challenges](https://arxiv.org/pdf/2505.10993)
*Yuan Zhang, Xinfeng Zhang, Xiaoming Qi Xinyu Wu, Feng Chen, Guanyu Yang, Huazhu Fu*

Main category: eess.IV

TL;DR: A review of generative modeling in computational pathology, covering image/text generation, multimodal applications, and challenges like fidelity and ethics.


<details>
  <summary>Details</summary>
Motivation: To synthesize recent progress and highlight the potential of generative models in computational pathology for tasks like data augmentation and multimodal representation.

Method: Analyzed over 150 studies, tracing the evolution of generative architectures (GANs to diffusion models) and examining datasets, evaluation protocols, and limitations.

Result: Identified key domains (image/text generation, multimodal applications) and ongoing challenges (fidelity, interpretability, ethical concerns).

Conclusion: Calls for unified, multimodal, and clinically deployable generative systems, providing a foundational reference for researchers.

Abstract: Generative modeling has emerged as a promising direction in computational
pathology, offering capabilities such as data-efficient learning, synthetic
data augmentation, and multimodal representation across diverse diagnostic
tasks. This review provides a comprehensive synthesis of recent progress in the
field, organized into four key domains: image generation, text generation,
multimodal image-text generation, and other generative applications, including
spatial simulation and molecular inference. By analyzing over 150
representative studies, we trace the evolution of generative architectures from
early generative adversarial networks to recent advances in diffusion models
and foundation models with generative capabilities. We further examine the
datasets and evaluation protocols commonly used in this domain and highlight
ongoing limitations, including challenges in generating high-fidelity whole
slide images, clinical interpretability, and concerns related to the ethical
and legal implications of synthetic data. The review concludes with a
discussion of open challenges and prospective research directions, with an
emphasis on developing unified, multimodal, and clinically deployable
generative systems. This work aims to provide a foundational reference for
researchers and practitioners developing and applying generative models in
computational pathology.

</details>


### [531] [Diffusion Model in Hyperspectral Image Processing and Analysis: A Review](https://arxiv.org/pdf/2505.11158)
*Xing Hu, Xiangcheng Liu, Qianqian Duan, Danfeng Hong, Dawei Zhang*

Main category: eess.IV

TL;DR: The paper reviews Diffusion Model's advantages in hyperspectral image processing, highlighting its effectiveness in handling high-dimensional data, denoising, and enhancing analysis accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional models struggle with hyperspectral image complexities like high dimensionality and noise, prompting exploration of Diffusion Model's potential.

Method: The paper reviews Diffusion Model's application in hyperspectral tasks, including data processing, noise removal, classification, and anomaly detection.

Result: Diffusion Model improves accuracy and efficiency in hyperspectral image analysis, outperforming traditional methods.

Conclusion: Diffusion Model offers a promising direction for future hyperspectral image research, addressing current challenges effectively.

Abstract: Hyperspectral image processing and analysis has important application value
in remote sensing, agriculture and environmental monitoring, but its high
dimensionality, data redundancy and noise interference etc. bring great
challenges to the analysis. Traditional models have limitations in dealing with
these complex data, and it is difficult to meet the increasing demand for
analysis. In recent years, Diffusion Model, as an emerging generative model,
has shown unique advantages in hyperspectral image processing. By simulating
the diffusion process of data in time, the Diffusion Model can effectively
process high-dimensional data, generate high-quality samples, and perform well
in denoising and data enhancement. In this paper, we review the recent research
advances in diffusion modeling for hyperspectral image processing and analysis,
and discuss its applications in tasks such as high-dimensional data processing,
noise removal, classification, and anomaly detection. The performance of
diffusion-based models on image processing is compared and the challenges are
summarized. It is shown that the diffusion model can significantly improve the
accuracy and efficiency of hyperspectral image analysis, providing a new
direction for future research.

</details>


### [532] [Diff-Unfolding: A Model-Based Score Learning Framework for Inverse Problems](https://arxiv.org/pdf/2505.11393)
*Yuanhao Wang, Shirin Shoushtari, Ulugbek S. Kamilov*

Main category: eess.IV

TL;DR: Diff-Unfolding is a framework for learning posterior score functions in conditional diffusion models, integrating measurement operators into a modular network for versatile inverse problem solving without retraining.


<details>
  <summary>Details</summary>
Motivation: To improve the generalization and performance of diffusion models for inverse problems by decoupling the measurement model from the learned image prior.

Method: Uses an unrolled optimization scheme to train posterior scores, incorporating the physical measurement operator into the network architecture.

Result: Achieves state-of-the-art performance in image restoration and MRI, with significant PSNR and LPIPS improvements, while being efficient and compact.

Conclusion: Diff-Unfolding is a practical, high-performance solution for inverse problems, validated by theoretical and experimental results.

Abstract: Diffusion models are extensively used for modeling image priors for inverse
problems. We introduce \emph{Diff-Unfolding}, a principled framework for
learning posterior score functions of \emph{conditional diffusion models} by
explicitly incorporating the physical measurement operator into a modular
network architecture. Diff-Unfolding formulates posterior score learning as the
training of an unrolled optimization scheme, where the measurement model is
decoupled from the learned image prior. This design allows our method to
generalize across inverse problems at inference time by simply replacing the
forward operator without retraining. We theoretically justify our unrolling
approach by showing that the posterior score can be derived from a composite
model-based optimization formulation. Extensive experiments on image
restoration and accelerated MRI show that Diff-Unfolding achieves
state-of-the-art performance, improving PSNR by up to 2 dB and reducing LPIPS
by $22.7\%$, while being both compact (47M parameters) and efficient (0.72
seconds per $256 \times 256$ image). An optimized C++/LibTorch implementation
further reduces inference time to 0.63 seconds, underscoring the practicality
of our approach.

</details>


### [533] [From Fibers to Cells: Fourier-Based Registration Enables Virtual Cresyl Violet Staining From 3D Polarized Light Imaging](https://arxiv.org/pdf/2505.11394)
*Alexander Oberstrass, Esteban Vaca, Eric Upschulte, Meiqi Niu, Nicola Palomero-Gallagher, David Graessel, Christian Schiffer, Markus Axer, Katrin Amunts, Timo Dickscheid*

Main category: eess.IV

TL;DR: The paper proposes a deep learning method to virtually stain 3D-PLI images for cytoarchitecture analysis, aligning them with post-stained sections for detailed microstructure study.


<details>
  <summary>Details</summary>
Motivation: To overcome distortions and limited samples in post-staining, enabling detailed study of brain microstructure by linking fiber- and cytoarchitecture.

Method: Uses deep learning for image-to-image translation, supervised by a dataset of 3D-PLI and Cresyl violet-stained sections, with Fourier-based registration for alignment.

Result: The method successfully predicts Cresyl violet staining from 3D-PLI, matching individual cell instances.

Conclusion: Virtual staining via deep learning offers a viable solution for spatially aligned cyto- and myeloarchitecture analysis.

Abstract: Comprehensive assessment of the various aspects of the brain's microstructure
requires the use of complementary imaging techniques. This includes measuring
the spatial distribution of cell bodies (cytoarchitecture) and nerve fibers
(myeloarchitecture). The gold standard for cytoarchitectonic analysis is light
microscopic imaging of cell-body stained tissue sections. To reveal the 3D
orientations of nerve fibers, 3D Polarized Light Imaging (3D-PLI) has been
introduced as a reliable technique providing a resolution in the micrometer
range while allowing processing of series of complete brain sections. 3D-PLI
acquisition is label-free and allows subsequent staining of sections after
measurement. By post-staining for cell bodies, a direct link between fiber- and
cytoarchitecture can potentially be established within the same section.
However, inevitable distortions introduced during the staining process make a
nonlinear and cross-modal registration necessary in order to study the detailed
relationships between cells and fibers in the images. In addition, the
complexity of processing histological sections for post-staining only allows
for a limited number of samples. In this work, we take advantage of deep
learning methods for image-to-image translation to generate a virtual staining
of 3D-PLI that is spatially aligned at the cellular level. In a supervised
setting, we build on a unique dataset of brain sections, to which Cresyl violet
staining has been applied after 3D-PLI measurement. To ensure high
correspondence between both modalities, we address the misalignment of training
data using Fourier-based registration methods. In this way, registration can be
efficiently calculated during training for local image patches of target and
predicted staining. We demonstrate that the proposed method enables prediction
of a Cresyl violet staining from 3D-PLI, matching individual cell instances.

</details>


### [534] [GOUHFI: a novel contrast- and resolution-agnostic segmentation tool for Ultra-High Field MRI](https://arxiv.org/pdf/2505.11445)
*Marc-Antoine Fortin, Anne Louise Kristoffersen, Michael Staff Larsen, Laurent Lamalle, Ruediger Stirnberg, Paal Erik Goa*

Main category: eess.IV

TL;DR: GOUHFI is a novel DL-based segmentation tool for UHF-MRI, overcoming limitations of traditional methods by handling various contrasts and resolutions with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Current segmentation techniques for 1.5-3T MRI perform poorly on UHF-MRI, limiting its potential for quantitative analysis.

Method: GOUHFI uses a domain randomization approach with synthetic images to train a 3D U-Net, tested on multiple datasets.

Result: Achieved high DSC scores (0.87, 0.84, 0.91 at 3T, 7T, 9.4T) and resistance to UHF inhomogeneities.

Conclusion: GOUHFI is a versatile, powerful tool for UHF-MRI segmentation, enabling quantitative analysis pipelines at ultra-high fields.

Abstract: Recently, Ultra-High Field MRI (UHF-MRI) has become more available and one of
the best tools to study the brain. One common step in quantitative neuroimaging
is the brain segmentation. However, the differences between UHF-MRI and 1.5-3T
images are such that the automatic segmentation techniques optimized at these
field strengths usually produce unsatisfactory segmentation results for UHF
images. It has been particularly challenging to perform quantitative analyses
as typically done with 1.5-3T data, considerably limiting the potential of
UHF-MRI. Hence, we propose a novel Deep Learning (DL)-based segmentation
technique called GOUHFI: Generalized and Optimized segmentation tool for
Ultra-High Field Images, designed to segment UHF images of various contrasts
and resolutions. For training, we used a total of 206 label maps from four
datasets acquired at 3T, 7T and 9.4T. In contrast to most DL strategies, we
used a previously proposed domain randomization approach, where synthetic
images generated from the label maps were used for training a 3D U-Net. GOUHFI
was tested on seven different datasets and compared to techniques like
FastSurferVINN and CEREBRUM-7T. GOUHFI was able to the segment six contrasts
and seven resolutions tested at 3T, 7T and 9.4T. Average Dice-Sorensen
Similarity Coefficient (DSC) scores of 0.87, 0.84, 0.91 were computed against
the ground truth segmentations at 3T, 7T and 9.4T. Moreover, GOUHFI
demonstrated impressive resistance to the typical inhomogeneities observed at
UHF-MRI, making it a new powerful segmentation tool that allows to apply the
usual quantitative analysis pipelines also at UHF. Ultimately, GOUHFI is a
promising new segmentation tool, being the first of its kind proposing a
contrast- and resolution-agnostic alternative for UHF-MRI, making it the
forthcoming alternative for neuroscientists working with UHF-MRI or even lower
field strengths.

</details>


### [535] [Contactless pulse rate assessment: Results and insights for application in driving simulator](https://arxiv.org/pdf/2505.01299)
*ƒêorƒëe D. Ne≈°koviƒá, Kristina Stojmenova Peƒçeƒçnik, Jaka Sodnik, Nadica Miljkoviƒá*

Main category: eess.IV

TL;DR: The study explores non-contact pulse rate (PR) monitoring using facial video recordings in a driving simulator, comparing it to wearable PPG sensors. Results show improved accuracy with Eulerian Video Magnification (EVM) and highlight age-related PR differences.


<details>
  <summary>Details</summary>
Motivation: To enable unobtrusive, continuous PR monitoring for driver state assessment, addressing limitations of wearable PPG sensors like motion artifacts and discomfort.

Method: Uses facial video recordings and EVM to detect skin color variations, comparing PR values with Empatica E4 wristband data. Analyzes 80 recordings from 64 subjects.

Result: EVM improves PR estimation (MAE: 6.48 to 5.04 bpm; RMSE: 7.84 to 6.38 bpm). Significant PR differences found between age groups. Empatica E4 bias noted.

Conclusion: Camera-based PR monitoring is feasible in dynamic environments, with potential for real-time integration in driving simulators.

Abstract: Camera-based monitoring of Pulse Rate (PR) enables continuous and unobtrusive
assessment of driver's state, allowing estimation of fatigue or stress that
could impact traffic safety. Commonly used wearable Photoplethysmography (PPG)
sensors, while effective, suffer from motion artifacts and user discomfort.
This study explores the feasibility of non-contact PR assessment using facial
video recordings captured by a Red, Green, and Blue (RGB) camera in a driving
simulation environment. The proposed approach detects subtle skin color
variations due to blood flow and compares extracted PR values against reference
measurements from a wearable wristband Empatica E4. We evaluate the impact of
Eulerian Video Magnification (EVM) on signal quality and assess statistical
differences in PR between age groups. Data obtained from 80 recordings from 64
healthy subjects covering a PR range of 45-160 bpm are analyzed, and signal
extraction accuracy is quantified using metrics, such as Mean Absolute Error
(MAE) and Root Mean Square Error (RMSE). Results show that EVM slightly
improves PR estimation accuracy, reducing MAE from 6.48 bpm to 5.04 bpm and
RMSE from 7.84 bpm to 6.38 bpm. A statistically significant difference is found
between older and younger groups with both video-based and ground truth
evaluation procedures. Additionally, we discuss Empatica E4 bias and its
potential impact on the overall assessment of contact measurements. Altogether
the findings demonstrate the feasibility of camera-based PR monitoring in
dynamic environments and its potential integration into driving simulators for
real-time physiological assessment.

</details>


### [536] [Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video Generation Model](https://arxiv.org/pdf/2505.07449)
*Wei Li, Ming Hu, Guoan Wang, Lihao Liu, Kaijin Zhou, Junzhi Ning, Xin Guo, Zongyuan Ge, Lixu Gu, Junjun He*

Main category: eess.IV

TL;DR: Ophora is an AI model that generates ophthalmic surgical videos from natural language instructions, addressing data scarcity and privacy issues.


<details>
  <summary>Details</summary>
Motivation: The difficulty in collecting annotated ophthalmic surgical videos due to privacy and labor constraints motivates the development of Ophora.

Method: Ophora uses a Comprehensive Data Curation pipeline to create a dataset (Ophora-160K) and a Progressive Video-Instruction Tuning scheme to adapt a pre-trained T2V model for surgical video generation.

Result: Ophora generates realistic and reliable surgical videos, validated by quantitative analysis and ophthalmologist feedback, and enhances downstream surgical workflow tasks.

Conclusion: Ophora successfully addresses data scarcity in ophthalmic surgery by generating high-quality videos from instructions, with potential for broader applications.

Abstract: In ophthalmic surgery, developing an AI system capable of interpreting
surgical videos and predicting subsequent operations requires numerous
ophthalmic surgical videos with high-quality annotations, which are difficult
to collect due to privacy concerns and labor consumption. Text-guided video
generation (T2V) emerges as a promising solution to overcome this issue by
generating ophthalmic surgical videos based on surgeon instructions. In this
paper, we present Ophora, a pioneering model that can generate ophthalmic
surgical videos following natural language instructions. To construct Ophora,
we first propose a Comprehensive Data Curation pipeline to convert narrative
ophthalmic surgical videos into a large-scale, high-quality dataset comprising
over 160K video-instruction pairs, Ophora-160K. Then, we propose a Progressive
Video-Instruction Tuning scheme to transfer rich spatial-temporal knowledge
from a T2V model pre-trained on natural video-text datasets for
privacy-preserved ophthalmic surgical video generation based on Ophora-160K.
Experiments on video quality evaluation via quantitative analysis and
ophthalmologist feedback demonstrate that Ophora can generate realistic and
reliable ophthalmic surgical videos based on surgeon instructions. We also
validate the capability of Ophora for empowering downstream tasks of ophthalmic
surgical workflow understanding. Code is available at
https://github.com/mar-cry/Ophora.

</details>


### [537] [Whitened Score Diffusion: A Structured Prior for Imaging Inverse Problems](https://arxiv.org/pdf/2505.10311)
*Jeffrey Alido, Tongyu Li, Yu Sun, Lei Tian*

Main category: eess.IV

TL;DR: Whitened Score (WS) diffusion models avoid covariance inversion in anisotropic Gaussian diffusion, outperforming conventional models.


<details>
  <summary>Details</summary>
Motivation: Address limitations of conventional DMs with anisotropic Gaussian diffusion by proposing a stable alternative.

Method: Introduce WS diffusion models, learning the Whitened Score function to bypass covariance inversion.

Result: WS DMs outperform conventional DMs on tasks with anisotropic Gaussian noise, showing better performance on CIFAR and CelebA datasets.

Conclusion: WS DMs offer a robust framework for diffusion models with arbitrary Gaussian noise, enhancing performance in imaging tasks.

Abstract: Conventional score-based diffusion models (DMs) may struggle with anisotropic
Gaussian diffusion processes due to the required inversion of covariance
matrices in the denoising score matching training objective
\cite{vincent_connection_2011}. We propose Whitened Score (WS) diffusion
models, a novel SDE-based framework that learns the Whitened Score function
instead of the standard score. This approach circumvents covariance inversion,
extending score-based DMs by enabling stable training of DMs on arbitrary
Gaussian forward noising processes. WS DMs establish equivalence with FM for
arbitrary Gaussian noise, allow for tailored spectral inductive biases, and
provide strong Bayesian priors for imaging inverse problems with structured
noise. We experiment with a variety of computational imaging tasks using the
CIFAR and CelebA ($64\times64$) datasets and demonstrate that WS diffusion
priors trained on anisotropic Gaussian noising processes consistently
outperform conventional diffusion priors based on isotropic Gaussian noise.

</details>


### [538] [WeGA: Weakly-Supervised Global-Local Affinity Learning Framework for Lymph Node Metastasis Prediction in Rectal Cancer](https://arxiv.org/pdf/2505.10502)
*Yifan Gao, Yaoxian Dong, Wenbin Wu, Chaoyang Ge, Feng Yuan, Jiaxi Sheng, Haoyue Li, Xin Gao*

Main category: eess.IV

TL;DR: WeGA is a weakly-supervised global-local affinity learning framework for accurate lymph node metastasis prediction in rectal cancer, outperforming existing methods by leveraging global context and local node details.


<details>
  <summary>Details</summary>
Motivation: Current MRI-based LNM assessment lacks accuracy, and automated systems struggle due to missing node-level annotations and isolated node treatment.

Method: WeGA uses a dual-branch architecture (DINOv2 for global context, residual encoder for local details), global-local affinity extractor, and regional affinity loss for structural coherence.

Result: WeGA achieves superior AUCs (0.750, 0.822, 0.802) across internal and external test centers.

Conclusion: WeGA improves LNM prediction accuracy by modeling node relationships and context, enhancing diagnostic precision and treatment planning.

Abstract: Accurate lymph node metastasis (LNM) assessment in rectal cancer is essential
for treatment planning, yet current MRI-based evaluation shows unsatisfactory
accuracy, leading to suboptimal clinical decisions. Developing automated
systems also faces significant obstacles, primarily the lack of node-level
annotations. Previous methods treat lymph nodes as isolated entities rather
than as an interconnected system, overlooking valuable spatial and contextual
information. To solve this problem, we present WeGA, a novel weakly-supervised
global-local affinity learning framework that addresses these challenges
through three key innovations: 1) a dual-branch architecture with DINOv2
backbone for global context and residual encoder for local node details; 2) a
global-local affinity extractor that aligns features across scales through
cross-attention fusion; and 3) a regional affinity loss that enforces
structural coherence between classification maps and anatomical regions.
Experiments across one internal and two external test centers demonstrate that
WeGA outperforms existing methods, achieving AUCs of 0.750, 0.822, and 0.802
respectively. By effectively modeling the relationships between individual
lymph nodes and their collective context, WeGA provides a more accurate and
generalizable approach for lymph node metastasis prediction, potentially
enhancing diagnostic precision and treatment selection for rectal cancer
patients.

</details>
