<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 69]
- [cs.CV](#cs.CV) [Total: 99]
- [cs.AI](#cs.AI) [Total: 24]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.LG](#cs.LG) [Total: 111]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.MM](#cs.MM) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [eess.IV](#eess.IV) [Total: 14]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Bidirectional Mamba for Single-Cell Data: Efficient Context Learning with Biological Fidelity](https://arxiv.org/abs/2504.16956)
*Cong Qi, Hanzhang Fang, Tianxing Hu, Siqi Jiang, Wei Zhi*

Main category: cs.CL

TL;DR: GeneMamba is a scalable, efficient foundation model for single-cell RNA-seq data, using state space modeling to outperform transformer-based methods.


<details>
  <summary>Details</summary>
Motivation: Addressing computational challenges in scRNA-seq analysis, such as high dimensionality and batch effects, where transformer models fall short due to quadratic complexity and poor long-range dependency handling.

Method: GeneMamba employs a Bi-Mamba architecture for bidirectional gene context with linear-time complexity, pretrained on 30M cells with biologically informed objectives like pathway-aware contrastive loss.

Result: GeneMamba excels in tasks like multi-batch integration, cell type annotation, and gene-gene correlation, showing strong performance and interpretability.

Conclusion: GeneMamba is a robust, scalable alternative to transformer-based methods, enhancing large-scale single-cell data analysis.

Abstract: Single-cell RNA sequencing (scRNA-seq) enables high-resolution analysis of
cellular heterogeneity, but its complexity, which is marked by high
dimensionality, sparsity, and batch effects, which poses major computational
challenges. Transformer-based models have made significant advances in this
domain but are often limited by their quadratic complexity and suboptimal
handling of long-range dependencies. In this work, we introduce GeneMamba, a
scalable and efficient foundation model for single-cell transcriptomics built
on state space modeling. Leveraging the Bi-Mamba architecture, GeneMamba
captures bidirectional gene context with linear-time complexity, offering
substantial computational gains over transformer baselines. The model is
pretrained on nearly 30 million cells and incorporates biologically informed
objectives, including pathway-aware contrastive loss and rank-based gene
encoding. We evaluate GeneMamba across diverse tasks, including multi-batch
integration, cell type annotation, and gene-gene correlation, demonstrating
strong performance, interpretability, and robustness. These results position
GeneMamba as a practical and powerful alternative to transformer-based methods,
advancing the development of biologically grounded, scalable tools for
large-scale single-cell data analysis.

</details>


### [2] [Tokenization Matters: Improving Zero-Shot NER for Indic Languages](https://arxiv.org/abs/2504.16977)
*Priyaranjan Pattnayak, Hitesh Laxmichand Patel, Amit Agarwal*

Main category: cs.CL

TL;DR: SentencePiece outperforms BPE and Character Level tokenization for NER in low-resource Indic languages, especially in zero-shot cross-lingual settings, due to better entity consistency and morphological preservation.


<details>
  <summary>Details</summary>
Motivation: The study aims to evaluate tokenization methods (BPE, SentencePiece, Character Level) for NER in low-resource Indic languages, addressing BPE's limitations in handling morphological complexity.

Method: Systematic comparison of BPE, SentencePiece, and Character Level tokenization using IndicBERT for NER tasks in low-resource Indic languages, assessing intrinsic (e.g., OOV rates) and extrinsic (e.g., zero-shot transfer) metrics.

Result: SentencePiece performs best, preserving entity consistency and generalizing well, especially for morphologically rich languages like Santali and Manipuri, while BPE struggles with unseen languages.

Conclusion: SentencePiece is the most effective tokenization strategy for NER in multilingual and low-resource Indic NLP applications, offering superior generalization and linguistic preservation.

Abstract: Tokenization is a critical component of Natural Language Processing (NLP),
especially for low resource languages, where subword segmentation influences
vocabulary structure and downstream task accuracy. Although Byte Pair Encoding
(BPE) is a standard tokenization method in multilingual language models, its
suitability for Named Entity Recognition (NER) in low resource Indic languages
remains underexplored due to its limitations in handling morphological
complexity. In this work, we systematically compare BPE, SentencePiece, and
Character Level tokenization strategies using IndicBERT for NER tasks in low
resource Indic languages like Assamese, Bengali, Marathi, and Odia, as well as
extremely low resource Indic languages like Santali, Manipuri, and Sindhi. We
assess both intrinsic linguistic properties tokenization efficiency, out of
vocabulary (OOV) rates, and morphological preservation as well as extrinsic
downstream performance, including fine tuning and zero shot cross lingual
transfer.
  Our experiments show that SentencePiece is a consistently better performing
approach than BPE for NER in low resource Indic Languages, particularly in zero
shot cross lingual settings, as it better preserves entity consistency. While
BPE provides the most compact tokenization form, it is not capable of
generalization because it misclassifies or even fails to recognize entity
labels when tested on unseen languages. In contrast, SentencePiece constitutes
a better linguistic structural preservation model, benefiting extremely low
resource and morphologically rich Indic languages, such as Santali and
Manipuri, for superior entity recognition, as well as high generalization
across scripts, such as Sindhi, written in Arabic. The results point to
SentencePiece as the more effective tokenization strategy for NER within
multilingual and low resource Indic NLP applications.

</details>


### [3] [Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation](https://arxiv.org/abs/2504.17025)
*Luca Moroni, Giovanni Puccetti, Pere-Lluis Huguet Cabot, Andrei Stefan Bejgu, Edoardo Barba, Alessio Miaschi, Felice Dell'Orletta, Andrea Esuli, Roberto Navigli*

Main category: cs.CL

TL;DR: The paper introduces SAVA, a method to optimize English LLMs for Italian, reducing token fertility and parameters while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: English LLMs are inefficient for non-English languages due to high token fertility and slow inference.

Method: SAVA leverages neural mapping for vocabulary substitution, adapting Mistral-7b-v0.1 and Llama-3.1-8B for Italian.

Result: Token fertility reduced by 25%, parameters by 1B; models recover performance with minimal target-language training.

Conclusion: SAVA effectively optimizes LLMs for Italian, improving efficiency and performance on downstream tasks.

Abstract: The number of pretrained Large Language Models (LLMs) is increasing steadily,
though the majority are designed predominantly for the English language. While
state-of-the-art LLMs can handle other languages, due to language contamination
or some degree of multilingual pretraining data, they are not optimized for
non-English languages, leading to inefficient encoding (high token "fertility")
and slower inference speed. In this work, we thoroughly compare a variety of
vocabulary adaptation techniques for optimizing English LLMs for the Italian
language, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a
novel method that leverages neural mapping for vocabulary substitution. SAVA
achieves competitive performance across multiple downstream tasks, enhancing
grounded alignment strategies. We adapt two LLMs: Mistral-7b-v0.1, reducing
token fertility by 25\%, and Llama-3.1-8B, optimizing the vocabulary and
reducing the number of parameters by 1 billion. We show that, following the
adaptation of the vocabulary, these models can recover their performance with a
relatively limited stage of continual training on the target language. Finally,
we test the capabilities of the adapted models on various multi-choice and
generative tasks.

</details>


### [4] [Do Words Reflect Beliefs? Evaluating Belief Depth in Large Language Models](https://arxiv.org/abs/2504.17052)
*Shariar Kabir, Kevin Esterling, Yue Dong*

Main category: cs.CL

TL;DR: The paper investigates whether LLMs' political responses reflect genuine beliefs or surface-level alignment, proposing a framework to evaluate belief depth via argumentative consistency and uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs' political stances are stable and genuine or merely aligned with training data.

Method: A novel framework analyzing argumentative consistency and uncertainty quantification, tested on 12 LLMs using 19 economic policies from the Political Compass Test.

Result: LLMs show topic-specific belief stability, with high consistency under challenge (95% left-leaning, 89% right-leaning). Semantic entropy effectively distinguishes genuine beliefs (AUROC=0.78).

Conclusion: LLMs lack stable, human-like ideologies; topic-specific reliability assessments are crucial for real-world use.

Abstract: Large Language Models (LLMs) are increasingly shaping political discourse,
yet their responses often display inconsistency when subjected to scrutiny.
While prior research has primarily categorized LLM outputs as left- or
right-leaning to assess their political stances, a critical question remains:
Do these responses reflect genuine internal beliefs or merely surface-level
alignment with training data? To address this, we propose a novel framework for
evaluating belief depth by analyzing (1) argumentative consistency and (2)
uncertainty quantification. We evaluate 12 LLMs on 19 economic policies from
the Political Compass Test, challenging their belief stability with both
supportive and opposing arguments. Our analysis reveals that LLMs exhibit
topic-specific belief stability rather than a uniform ideological stance.
Notably, up to 95% of left-leaning models' responses and 89% of right-leaning
models' responses remain consistent under the challenge, enabling semantic
entropy to achieve high accuracy (AUROC=0.78), effectively distinguishing
between surface-level alignment from genuine belief. These findings call into
question the assumption that LLMs maintain stable, human-like political
ideologies, emphasizing the importance of conducting topic-specific reliability
assessments for real-world applications.

</details>


### [5] [Agree to Disagree? A Meta-Evaluation of LLM Misgendering](https://arxiv.org/abs/2504.17075)
*Arjun Subramonian, Vagrant Gautam, Preethi Seshadri, Dietrich Klakow, Kai-Wei Chang, Yizhou Sun*

Main category: cs.CL

TL;DR: The paper evaluates the convergent validity of methods measuring LLM misgendering, finding significant disagreement between probability- and generation-based evaluations, and highlights limitations of automatic methods compared to human judgment.


<details>
  <summary>Details</summary>
Motivation: To determine if existing evaluation methods for LLM misgendering align (convergent validity) and to assess their reliability.

Method: A meta-evaluation of probability- and generation-based methods across three datasets, transforming datasets for parallel evaluation, and analyzing 6 models from 3 families. Human evaluation of 2400 LLM generations was also conducted.

Result: Methods disagreed on 20.2% of instances, and automatic evaluations failed to capture the complexity of misgendering behavior, diverging from human judgments.

Conclusion: The study reveals flaws in current evaluation conventions for LLM misgendering and recommends improvements, questioning broader LLM evaluation practices.

Abstract: Numerous methods have been proposed to measure LLM misgendering, including
probability-based evaluations (e.g., automatically with templatic sentences)
and generation-based evaluations (e.g., with automatic heuristics or human
validation). However, it has gone unexamined whether these evaluation methods
have convergent validity, that is, whether their results align. Therefore, we
conduct a systematic meta-evaluation of these methods across three existing
datasets for LLM misgendering. We propose a method to transform each dataset to
enable parallel probability- and generation-based evaluation. Then, by
automatically evaluating a suite of 6 models from 3 families, we find that
these methods can disagree with each other at the instance, dataset, and model
levels, conflicting on 20.2% of evaluation instances. Finally, with a human
evaluation of 2400 LLM generations, we show that misgendering behaviour is
complex and goes far beyond pronouns, which automatic evaluations are not
currently designed to capture, suggesting essential disagreement with human
evaluations. Based on our findings, we provide recommendations for future
evaluations of LLM misgendering. Our results are also more widely relevant, as
they call into question broader methodological conventions in LLM evaluation,
which often assume that different evaluation methods agree.

</details>


### [6] [How Individual Traits and Language Styles Shape Preferences In Open-ended User-LLM Interaction: A Preliminary Study](https://arxiv.org/abs/2504.17083)
*Rendi Chevi, Kentaro Inui, Thamar Solorio, Alham Fikri Aji*

Main category: cs.CL

TL;DR: The paper explores how language style in LLM responses influences user preferences, finding it varies by user traits, but warns of risks like misinformation due to style's impact.


<details>
  <summary>Details</summary>
Motivation: To understand how language style in LLM responses affects user preferences, despite potential risks like misinformation.

Method: Conducted exploratory and experimental user studies to analyze the influence of language style on preferences.

Result: Language style impacts preferences, but effects vary by user traits; findings are preliminary due to sample limitations.

Conclusion: Future work will address sample diversity and size to better analyze language style, traits, and preferences, and explore causal relationships.

Abstract: What makes an interaction with the LLM more preferable for the user? While it
is intuitive to assume that information accuracy in the LLM's responses would
be one of the influential variables, recent studies have found that inaccurate
LLM's responses could still be preferable when they are perceived to be more
authoritative, certain, well-articulated, or simply verbose. These variables
interestingly fall under the broader category of language style, implying that
the style in the LLM's responses might meaningfully influence users'
preferences. This hypothesized dynamic could have double-edged consequences:
enhancing the overall user experience while simultaneously increasing their
susceptibility to risks such as LLM's misinformation or hallucinations. In this
short paper, we present our preliminary studies in exploring this subject.
Through a series of exploratory and experimental user studies, we found that
LLM's language style does indeed influence user's preferences, but how and
which language styles influence the preference varied across different user
populations, and more interestingly, moderated by the user's very own
individual traits. As a preliminary work, the findings in our studies should be
interpreted with caution, particularly given the limitations in our samples,
which still need wider demographic diversity and larger sample sizes. Our
future directions will first aim to address these limitations, which would
enable a more comprehensive joint effect analysis between the language style,
individual traits, and preferences, and further investigate the potential
causal relationship between and beyond these variables.

</details>


### [7] [Co-CoT: A Prompt-Based Framework for Collaborative Chain-of-Thought Reasoning](https://arxiv.org/abs/2504.17091)
*Seunghyun Yoo*

Main category: cs.CL

TL;DR: Proposes an Interactive Chain-of-Thought (CoT) Framework to enhance AI transparency and user engagement by making reasoning modular and editable.


<details>
  <summary>Details</summary>
Motivation: Addresses the decline in deep thinking due to short-form content and AI reliance, aiming to improve critical engagement with AI outputs.

Method: Introduces a framework with modular reasoning blocks, user-editable steps, and a lightweight edit-adaptation mechanism for alignment with user preferences.

Result: Enables transparent, responsible AI usage with features like bias checkpoints and privacy safeguards.

Conclusion: The framework fosters critical thinking, ethical transparency, and inclusive adaptation in AI systems.

Abstract: Due to the proliferation of short-form content and the rapid adoption of AI,
opportunities for deep, reflective thinking have significantly diminished,
undermining users' critical thinking and reducing engagement with the reasoning
behind AI-generated outputs. To address this issue, we propose an Interactive
Chain-of-Thought (CoT) Framework that enhances human-centered explainability
and responsible AI usage by making the model's inference process transparent,
modular, and user-editable. The framework decomposes reasoning into clearly
defined blocks that users can inspect, modify, and re-execute, encouraging
active cognitive engagement rather than passive consumption. It further
integrates a lightweight edit-adaptation mechanism inspired by preference
learning, allowing the system to align with diverse cognitive styles and user
intentions. Ethical transparency is ensured through explicit metadata
disclosure, built-in bias checkpoint functionality, and privacy-preserving
safeguards. This work outlines the design principles and architecture necessary
to promote critical engagement, responsible interaction, and inclusive
adaptation in AI systems aimed at addressing complex societal challenges.

</details>


### [8] [The Rise of Small Language Models in Healthcare: A Comprehensive Survey](https://arxiv.org/abs/2504.17119)
*Muskan Garg, Shaina Raza, Shebuti Rayana, Xingyi Liu, Sunghwan Sohn*

Main category: cs.CL

TL;DR: A survey on small language models (SLMs) for healthcare, highlighting their scalability and clinical viability in resource-limited settings. It provides a taxonomic framework, timeline, and experimental results to guide professionals.


<details>
  <summary>Details</summary>
Motivation: Addressing data privacy concerns and resource limitations in healthcare, SLMs offer a practical alternative to large language models (LLMs) for efficient healthcare informatics.

Method: The paper presents a taxonomic framework categorizing SLMs by NLP tasks, stakeholder roles, and care continuum. It covers model building, adaptation, and optimization techniques like prompting and compression.

Result: A comprehensive survey with experimental results demonstrates SLMs' transformative potential in healthcare NLP tasks.

Conclusion: SLMs are a viable solution for healthcare informatics, with the survey providing resources and insights for future research and development.

Abstract: Despite substantial progress in healthcare applications driven by large
language models (LLMs), growing concerns around data privacy, and limited
resources; the small language models (SLMs) offer a scalable and clinically
viable solution for efficient performance in resource-constrained environments
for next-generation healthcare informatics. Our comprehensive survey presents a
taxonomic framework to identify and categorize them for healthcare
professionals and informaticians. The timeline of healthcare SLM contributions
establishes a foundational framework for analyzing models across three
dimensions: NLP tasks, stakeholder roles, and the continuum of care. We present
a taxonomic framework to identify the architectural foundations for building
models from scratch; adapting SLMs to clinical precision through prompting,
instruction fine-tuning, and reasoning; and accessibility and sustainability
through compression techniques. Our primary objective is to offer a
comprehensive survey for healthcare professionals, introducing recent
innovations in model optimization and equipping them with curated resources to
support future research and development in the field. Aiming to showcase the
groundbreaking advancements in SLMs for healthcare, we present a comprehensive
compilation of experimental results across widely studied NLP tasks in
healthcare to highlight the transformative potential of SLMs in healthcare. The
updated repository is available at Github

</details>


### [9] [M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction](https://arxiv.org/abs/2504.17353)
*Chengguang Gan, Sunbowen Lee, Zhixi Cai, Yanbin Wei, Lei Zheng, Yunhao Liang, Shiwen Ni, Tatsunori Mori*

Main category: cs.CL

TL;DR: The paper extends Mutual Reinforcement Effect (MRE) to multimodal domains, introducing Multimodal MRE (M-MRE) and a dataset. A Prompt Format Adapter (PFA) is proposed for Large Vision-Language Models, showing MRE's effectiveness in multimodal tasks.


<details>
  <summary>Details</summary>
Motivation: To explore MRE's applicability beyond text, addressing its unexplored potential in visual and multimodal domains.

Method: Introduces M-MRE task and dataset, proposes PFA for compatibility with LVLMs, and validates through experiments.

Result: MRE is observed in M-MRE, showing mutual gains across tasks, confirming its generalizability to multimodal scenarios.

Conclusion: MRE is effective in multimodal domains, enhancing performance across interrelated tasks, proving its broader applicability.

Abstract: Mutual Reinforcement Effect (MRE) is an emerging subfield at the intersection
of information extraction and model interpretability. MRE aims to leverage the
mutual understanding between tasks of different granularities, enhancing the
performance of both coarse-grained and fine-grained tasks through joint
modeling. While MRE has been explored and validated in the textual domain, its
applicability to visual and multimodal domains remains unexplored. In this
work, we extend MRE to the multimodal information extraction domain for the
first time. Specifically, we introduce a new task: Multimodal Mutual
Reinforcement Effect (M-MRE), and construct a corresponding dataset to support
this task. To address the challenges posed by M-MRE, we further propose a
Prompt Format Adapter (PFA) that is fully compatible with various Large
Vision-Language Models (LVLMs). Experimental results demonstrate that MRE can
also be observed in the M-MRE task, a multimodal text-image understanding
scenario. This provides strong evidence that MRE facilitates mutual gains
across three interrelated tasks, confirming its generalizability beyond the
textual domain.

</details>


### [10] [Steering the CensorShip: Uncovering Representation Vectors for LLM "Thought" Control](https://arxiv.org/abs/2504.17130)
*Hannah Cyberey, David Evans*

Main category: cs.CL

TL;DR: The paper investigates censorship in LLMs, using representation engineering to identify and control refusal-compliance vectors and thought suppression in reasoning models.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs are tuned to refuse harmful requests and align with controller preferences, and to explore methods for detecting and manipulating censorship.

Method: Uses representation engineering to find refusal-compliance vectors and analyzes reasoning LLMs to uncover thought suppression, applying negative vectors to remove censorship.

Result: Identifies vectors that detect and control censorship levels and suppresses reasoning to bypass censorship.

Conclusion: Demonstrates effective methods for analyzing and manipulating censorship in LLMs, revealing underlying mechanisms of control.

Abstract: Large language models (LLMs) have transformed the way we access information.
These models are often tuned to refuse to comply with requests that are
considered harmful and to produce responses that better align with the
preferences of those who control the models. To understand how this
"censorship" works. We use representation engineering techniques to study
open-weights safety-tuned models. We present a method for finding a
refusal--compliance vector that detects and controls the level of censorship in
model outputs. We also analyze recent reasoning LLMs, distilled from
DeepSeek-R1, and uncover an additional dimension of censorship through "thought
suppression". We show a similar approach can be used to find a vector that
suppresses the model's reasoning process, allowing us to remove censorship by
applying the negative multiples of this vector

</details>


### [11] [MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation](https://arxiv.org/abs/2504.17137)
*Chanhee Park, Hyeonseok Moon, Chanjun Park, Heuiseok Lim*

Main category: cs.CL

TL;DR: MIRAGE is a QA dataset for evaluating RAG systems, featuring 7,560 instances and 37,800 retrieval entries, with novel metrics for adaptability.


<details>
  <summary>Details</summary>
Motivation: Current RAG evaluation lacks benchmarks for detailed, component-specific assessment due to the complexity of retrieval-generation interplay.

Method: Introduces MIRAGE, a curated QA dataset with a large retrieval pool, and novel metrics for adaptability (e.g., noise vulnerability).

Result: Comprehensive experiments reveal insights into optimal retriever-LLM alignment and RAG system dynamics.

Conclusion: MIRAGE facilitates precise RAG evaluation and is publicly available for research use.

Abstract: Retrieval-Augmented Generation (RAG) has gained prominence as an effective
method for enhancing the generative capabilities of Large Language Models
(LLMs) through the incorporation of external knowledge. However, the evaluation
of RAG systems remains a challenge, due to the intricate interplay between
retrieval and generation components. This limitation has resulted in a scarcity
of benchmarks that facilitate a detailed, component-specific assessment. In
this work, we present MIRAGE, a Question Answering dataset specifically
designed for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped
to a retrieval pool of 37,800 entries, enabling an efficient and precise
evaluation of both retrieval and generation tasks. We also introduce novel
evaluation metrics aimed at measuring RAG adaptability, encompassing dimensions
such as noise vulnerability, context acceptability, context insensitivity, and
context misinterpretation. Through comprehensive experiments across various
retriever-LLM configurations, we provide new insights into the optimal
alignment of model pairs and the nuanced dynamics within RAG systems. The
dataset and evaluation code are publicly available, allowing for seamless
integration and customization in diverse research settings\footnote{The MIRAGE
code and data are available at https://github.com/nlpai-lab/MIRAGE.

</details>


### [12] [Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning](https://arxiv.org/abs/2504.17192)
*Minju Seo, Jinheon Baek, Seongyun Lee, Sung Ju Hwang*

Main category: cs.CL

TL;DR: PaperCoder is a multi-agent LLM framework that converts machine learning papers into functional code repositories, excelling in quality and faithfulness compared to benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of available code implementations in machine learning research, which hinders reproducibility and progress.

Method: Uses a three-stage multi-agent LLM framework: planning (roadmap, architecture, dependencies), analysis (implementation details), and generation (modular code).

Result: Demonstrates high-quality, faithful implementations, outperforming baselines in the PaperBench benchmark.

Conclusion: PaperCoder effectively bridges the gap between research papers and functional code, enhancing reproducibility and collaboration.

Abstract: Despite the rapid growth of machine learning research, corresponding code
implementations are often unavailable, making it slow and labor-intensive for
researchers to reproduce results and build upon prior work. In the meantime,
recent Large Language Models (LLMs) excel at understanding scientific documents
and generating high-quality code. Inspired by this, we introduce PaperCoder, a
multi-agent LLM framework that transforms machine learning papers into
functional code repositories. PaperCoder operates in three stages: planning,
where it constructs a high-level roadmap, designs the system architecture with
diagrams, identifies file dependencies, and generates configuration files;
analysis, which focuses on interpreting implementation-specific details; and
generation, where modular, dependency-aware code is produced. Moreover, each
phase is instantiated through a set of specialized agents designed to
collaborate effectively across the pipeline. We then evaluate PaperCoder on
generating code implementations from machine learning papers based on both
model-based and human evaluations, specifically from the original paper
authors, with author-released repositories as ground truth if available. Our
results demonstrate the effectiveness of PaperCoder in creating high-quality,
faithful implementations. Furthermore, it consistently shows strengths in the
recently released PaperBench benchmark, surpassing strong baselines by
substantial margins.

</details>


### [13] [A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation](https://arxiv.org/abs/2504.17200)
*Yangxinyu Xie, Bowen Jiang, Tanwi Mallick, Joshua David Bergerson, John K. Hutchison, Duane R. Verner, Jordan Branham, M. Ross Alexander, Robert B. Ross, Yan Feng, Leslie-Anne Levy, Weijie Su, Camillo J. Taylor*

Main category: cs.CL

TL;DR: A retrieval-augmented generation (RAG)-based multi-agent LLM system, WildfireGPT, is proposed to enhance decision-making for natural hazards, outperforming existing LLM solutions.


<details>
  <summary>Details</summary>
Motivation: LLMs lack context-specificity for specialized domains like natural hazards. This work aims to bridge that gap for better decision support.

Method: Uses a RAG-based multi-agent LLM system integrating hazard data, observations, and literature for tailored insights.

Result: WildfireGPT significantly outperforms existing LLM solutions in expert-led case studies.

Conclusion: The system effectively combines accuracy and contextual relevance for natural hazard decision-making.

Abstract: Large language models (LLMs) are a transformational capability at the
frontier of artificial intelligence and machine learning that can support
decision-makers in addressing pressing societal challenges such as extreme
natural hazard events. As generalized models, LLMs often struggle to provide
context-specific information, particularly in areas requiring specialized
knowledge. In this work we propose a retrieval-augmented generation (RAG)-based
multi-agent LLM system to support analysis and decision-making in the context
of natural hazards and extreme weather events. As a proof of concept, we
present WildfireGPT, a specialized system focused on wildfire hazards. The
architecture employs a user-centered, multi-agent design to deliver tailored
risk insights across diverse stakeholder groups. By integrating natural hazard
and extreme weather projection data, observational datasets, and scientific
literature through an RAG framework, the system ensures both the accuracy and
contextual relevance of the information it provides. Evaluation across ten
expert-led case studies demonstrates that WildfireGPT significantly outperforms
existing LLM-based solutions for decision support.

</details>


### [14] [Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark](https://arxiv.org/abs/2504.16427)
*Hanlei Zhang, Zhuohang Li, Yeshuang Zhu, Hua Xu, Peiwu Wang, Haige Zhu, Jie Zhou, Jinchao Zhang*

Main category: cs.CL

TL;DR: The paper introduces MMLA, a benchmark for evaluating multimodal large language models (MLLMs) on cognitive-level semantics, revealing their limitations despite extensive testing.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research on MLLMs' ability to understand cognitive-level semantics in multimodal language analysis.

Method: MMLA benchmark with 61K multimodal utterances, evaluating eight LLMs/MLLMs via zero-shot inference, supervised fine-tuning, and instruction tuning.

Result: Fine-tuned models achieve only 60%~70% accuracy, highlighting MLLMs' limitations in complex human language understanding.

Conclusion: MMLA provides a foundation for advancing MLLMs in multimodal language analysis, with open-sourced datasets and code.

Abstract: Multimodal language analysis is a rapidly evolving field that leverages
multiple modalities to enhance the understanding of high-level semantics
underlying human conversational utterances. Despite its significance, little
research has investigated the capability of multimodal large language models
(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce
MMLA, a comprehensive benchmark specifically designed to address this gap. MMLA
comprises over 61K multimodal utterances drawn from both staged and real-world
scenarios, covering six core dimensions of multimodal semantics: intent,
emotion, dialogue act, sentiment, speaking style, and communication behavior.
We evaluate eight mainstream branches of LLMs and MLLMs using three methods:
zero-shot inference, supervised fine-tuning, and instruction tuning. Extensive
experiments reveal that even fine-tuned models achieve only about 60%~70%
accuracy, underscoring the limitations of current MLLMs in understanding
complex human language. We believe that MMLA will serve as a solid foundation
for exploring the potential of large language models in multimodal language
analysis and provide valuable resources to advance this field. The datasets and
code are open-sourced at https://github.com/thuiar/MMLA.

</details>


### [15] [Does Knowledge Distillation Matter for Large Language Model based Bundle Generation?](https://arxiv.org/abs/2504.17220)
*Kaidong Feng, Zhu Sun, Jie Yang, Hui Fang, Xinghua Qu, Wenyuan Liu*

Main category: cs.CL

TL;DR: The paper explores knowledge distillation (KD) for efficient bundle generation using LLMs, addressing format, quantity, and utilization of distilled knowledge.


<details>
  <summary>Details</summary>
Motivation: Deploying large-scale LLMs for bundle generation is computationally expensive, prompting the need for efficient alternatives like KD.

Method: A comprehensive KD framework is proposed, progressively extracting knowledge, varying quantities, and utilizing adaptation techniques.

Result: Experiments show KD's potential to maintain performance while reducing computational costs in LLM-based bundle generation.

Conclusion: KD significantly enhances efficiency in LLM-based bundle generation without compromising performance.

Abstract: LLMs are increasingly explored for bundle generation, thanks to their
reasoning capabilities and knowledge. However, deploying large-scale LLMs
introduces significant efficiency challenges, primarily high computational
costs during fine-tuning and inference due to their massive parameterization.
Knowledge distillation (KD) offers a promising solution, transferring expertise
from large teacher models to compact student models. This study systematically
investigates knowledge distillation approaches for bundle generation, aiming to
minimize computational demands while preserving performance. We explore three
critical research questions: (1) how does the format of KD impact bundle
generation performance? (2) to what extent does the quantity of distilled
knowledge influence performance? and (3) how do different ways of utilizing the
distilled knowledge affect performance? We propose a comprehensive KD framework
that (i) progressively extracts knowledge (patterns, rules, deep thoughts);
(ii) captures varying quantities of distilled knowledge through different
strategies; and (iii) exploits complementary LLM adaptation techniques
(in-context learning, supervised fine-tuning, combination) to leverage
distilled knowledge in small student models for domain-specific adaptation and
enhanced efficiency. Extensive experiments provide valuable insights into how
knowledge format, quantity, and utilization methodologies collectively shape
LLM-based bundle generation performance, exhibiting KD's significant potential
for more efficient yet effective LLM-based bundle generation.

</details>


### [16] [Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn Supportive Dialogues](https://arxiv.org/abs/2504.17238)
*Jinfeng Zhou, Yuxuan Chen, Jianing Yin, Yongkang Huang, Yihan Shi, Xikun Zhang, Libiao Peng, Rongsheng Zhang, Tangjie Lv, Zhipeng Hu, Hongning Wang, Minlie Huang*

Main category: cs.CL

TL;DR: CRDial is a framework for Cognitive Restructuring (CR) via multi-turn dialogues, addressing gaps in existing methods by integrating identification, restructuring stages, and supportive strategies, validated by human studies.


<details>
  <summary>Details</summary>
Motivation: Clinician shortage and stigma drive the need for human-LLM interactive psychotherapy for CR, but current methods lack alignment with the psychotherapeutic process.

Method: CRDial introduces multi-turn dialogues with specific stages, supportive strategies, and a multi-channel loop mechanism, creating the Crisp dataset and training Crispers (7B/14B LLMs).

Result: Human studies demonstrate Crispers' superiority in pointwise, pairwise, and intervention evaluations.

Conclusion: CRDial and Crispers effectively address the limitations of existing CR methods, offering a scalable solution for interactive psychotherapy.

Abstract: Cognitive Restructuring (CR) is a psychotherapeutic process aimed at
identifying and restructuring an individual's negative thoughts, arising from
mental health challenges, into more helpful and positive ones via multi-turn
dialogues. Clinician shortage and stigma urge the development of human-LLM
interactive psychotherapy for CR. Yet, existing efforts implement CR via simple
text rewriting, fixed-pattern dialogues, or a one-shot CR workflow, failing to
align with the psychotherapeutic process for effective CR. To address this gap,
we propose CRDial, a novel framework for CR, which creates multi-turn dialogues
with specifically designed identification and restructuring stages of negative
thoughts, integrates sentence-level supportive conversation strategies, and
adopts a multi-channel loop mechanism to enable iterative CR. With CRDial, we
distill Crisp, a large-scale and high-quality bilingual dialogue dataset, from
LLM. We then train Crispers, Crisp-based conversational LLMs for CR, at 7B and
14B scales. Extensive human studies show the superiority of Crispers in
pointwise, pairwise, and intervention evaluations.

</details>


### [17] [Low-Resource Neural Machine Translation Using Recurrent Neural Networks and Transfer Learning: A Case Study on English-to-Igbo](https://arxiv.org/abs/2504.17252)
*Ocheme Anthony Ekle, Biswarup Das*

Main category: cs.CL

TL;DR: The paper develops NMT and Transformer-based models for English-to-Igbo translation, using RNNs and transfer learning to achieve a 70% accuracy with a +4.83 BLEU improvement.


<details>
  <summary>Details</summary>
Motivation: Addressing the performance gap in low-resource language translation, specifically for Igbo, spoken by over 40 million people.

Method: Uses RNN architectures (LSTM, GRU) with attention mechanisms and transfer learning via MarianNMT pre-trained models.

Result: RNN-based system matches benchmarks; transfer learning adds +4.83 BLEU, reaching 70% accuracy.

Conclusion: Combining RNNs with transfer learning effectively improves low-resource language translation.

Abstract: In this study, we develop Neural Machine Translation (NMT) and
Transformer-based transfer learning models for English-to-Igbo translation - a
low-resource African language spoken by over 40 million people across Nigeria
and West Africa. Our models are trained on a curated and benchmarked dataset
compiled from Bible corpora, local news, Wikipedia articles, and Common Crawl,
all verified by native language experts. We leverage Recurrent Neural Network
(RNN) architectures, including Long Short-Term Memory (LSTM) and Gated
Recurrent Units (GRU), enhanced with attention mechanisms to improve
translation accuracy. To further enhance performance, we apply transfer
learning using MarianNMT pre-trained models within the SimpleTransformers
framework. Our RNN-based system achieves competitive results, closely matching
existing English-Igbo benchmarks. With transfer learning, we observe a
performance gain of +4.83 BLEU points, reaching an estimated translation
accuracy of 70%. These findings highlight the effectiveness of combining RNNs
with transfer learning to address the performance gap in low-resource language
translation tasks.

</details>


### [18] [JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning](https://arxiv.org/abs/2504.17264)
*Zhaolu Kang, Hongtian Cai, Xiangyang Ji, Jinzhe Li, Nanfei Gu*

Main category: cs.CL

TL;DR: JurisCTC is a novel model for Legal Judgment Prediction (LJP) that uses contrastive learning to transfer knowledge between civil and criminal law domains, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of exploration in applying Unsupervised Domain Adaptation (UDA) to legal domains and overcoming challenges like complex legal texts and limited annotated datasets.

Method: Proposes JurisCTC, which employs contrastive learning to distinguish samples from different legal domains and facilitates knowledge transfer.

Result: Achieves peak accuracies of 76.59% and 78.83% in civil and criminal law domains, outperforming other models and LLMs.

Conclusion: JurisCTC effectively improves LJP task accuracy by enabling cross-domain knowledge transfer in legal contexts.

Abstract: In recent years, Unsupervised Domain Adaptation (UDA) has gained significant
attention in the field of Natural Language Processing (NLP) owing to its
ability to enhance model generalization across diverse domains. However, its
application for knowledge transfer between distinct legal domains remains
largely unexplored. To address the challenges posed by lengthy and complex
legal texts and the limited availability of large-scale annotated datasets, we
propose JurisCTC, a novel model designed to improve the accuracy of Legal
Judgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC
facilitates effective knowledge transfer across various legal domains and
employs contrastive learning to distinguish samples from different domains.
Specifically, for the LJP task, we enable knowledge transfer between civil and
criminal law domains. Compared to other models and specific large language
models (LLMs), JurisCTC demonstrates notable advancements, achieving peak
accuracies of 76.59% and 78.83%, respectively.

</details>


### [19] [Evaluating and Mitigating Bias in AI-Based Medical Text Generation](https://arxiv.org/abs/2504.17279)
*Xiuying Chen, Tairan Wang, Juexiao Zhou, Zirui Song, Xin Gao, Xiangliang Zhang*

Main category: cs.CL

TL;DR: The paper addresses fairness issues in AI-generated medical text, proposing an algorithm to reduce bias across demographic groups without compromising overall performance.


<details>
  <summary>Details</summary>
Motivation: AI systems in medicine may reflect human biases, especially in text generation, which is understudied compared to medical imaging. This study aims to tackle fairness in medical text generation.

Method: An algorithm selectively optimizes underperforming groups by considering word-level and pathology accuracy, ensuring differentiability for training. Evaluated across multiple models and datasets.

Result: The algorithm reduced group disparities by over 30% with minimal impact (≤2%) on overall text generation accuracy.

Conclusion: The proposed method effectively mitigates bias in medical text generation, enhancing fairness and reliability without sacrificing performance.

Abstract: Artificial intelligence (AI) systems, particularly those based on deep
learning models, have increasingly achieved expert-level performance in medical
applications. However, there is growing concern that such AI systems may
reflect and amplify human bias, and reduce the quality of their performance in
historically under-served populations. The fairness issue has attracted
considerable research interest in the medical imaging classification field, yet
it remains understudied in the text generation domain. In this study, we
investigate the fairness problem in text generation within the medical field
and observe significant performance discrepancies across different races,
sexes, and age groups, including intersectional groups, various model scales,
and different evaluation metrics. To mitigate this fairness issue, we propose
an algorithm that selectively optimizes those underperformed groups to reduce
bias. The selection rules take into account not only word-level accuracy but
also the pathology accuracy to the target reference, while ensuring that the
entire process remains fully differentiable for effective model training. Our
evaluations across multiple backbones, datasets, and modalities demonstrate
that our proposed algorithm enhances fairness in text generation without
compromising overall performance. Specifically, the disparities among various
groups across different metrics were diminished by more than 30% with our
algorithm, while the relative change in text generation accuracy was typically
within 2%. By reducing the bias generated by deep learning models, our proposed
approach can potentially alleviate concerns about the fairness and reliability
of text generation diagnosis in medical domain.
  Our code is publicly available to facilitate further research at
https://github.com/iriscxy/GenFair.

</details>


### [20] [CoheMark: A Novel Sentence-Level Watermark for Enhanced Text Quality](https://arxiv.org/abs/2504.17309)
*Junyan Zhang, Shuliang Liu, Aiwei Liu, Yubo Gao, Jungang Li, Xiaojie Gu, Xuming Hu*

Main category: cs.CL

TL;DR: CoheMark is a sentence-level watermarking technique that improves logical fluency by leveraging cohesive relationships between sentences, balancing text quality and robust watermark detection.


<details>
  <summary>Details</summary>
Motivation: Existing sentence-level watermarking techniques often compromise text quality due to arbitrary segmentation or generation processes. CoheMark aims to address this by ensuring better logical fluency and robustness.

Method: CoheMark uses trained fuzzy c-means clustering for sentence selection and applies specific next sentence selection criteria to embed watermarks.

Result: Experimental results show CoheMark achieves strong watermark strength with minimal impact on text quality.

Conclusion: CoheMark effectively balances high text quality and robust watermark detection, making it a promising solution for tracing content generated by large language models.

Abstract: Watermarking technology is a method used to trace the usage of content
generated by large language models. Sentence-level watermarking aids in
preserving the semantic integrity within individual sentences while maintaining
greater robustness. However, many existing sentence-level watermarking
techniques depend on arbitrary segmentation or generation processes to embed
watermarks, which can limit the availability of appropriate sentences. This
limitation, in turn, compromises the quality of the generated response. To
address the challenge of balancing high text quality with robust watermark
detection, we propose CoheMark, an advanced sentence-level watermarking
technique that exploits the cohesive relationships between sentences for better
logical fluency. The core methodology of CoheMark involves selecting sentences
through trained fuzzy c-means clustering and applying specific next sentence
selection criteria. Experimental evaluations demonstrate that CoheMark achieves
strong watermark strength while exerting minimal impact on text quality.

</details>


### [21] [FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation](https://arxiv.org/abs/2504.17311)
*Yulia Otmakhova, Hung Thinh Truong, Rahmad Mahendra, Zenan Zhai, Rongxin Zhu, Daniel Beck, Jey Han Lau*

Main category: cs.CL

TL;DR: FLUKE is a task-agnostic framework for evaluating NLP model robustness using systematic linguistic variations, revealing task-dependent impacts and vulnerabilities in both fine-tuned models and LLMs.


<details>
  <summary>Details</summary>
Motivation: To systematically assess NLP model robustness across diverse linguistic variations, addressing gaps in task-agnostic evaluation.

Method: FLUKE introduces controlled linguistic variations (orthography to dialect) and uses LLMs with human validation to generate test data modifications.

Result: Findings show task-dependent robustness, LLMs outperform fine-tuned models but remain brittle to some variations, and all models struggle with negation.

Conclusion: Systematic robustness testing is crucial for understanding model behaviors, as highlighted by FLUKE's task-agnostic evaluations.

Abstract: We present FLUKE (Framework for LingUistically-driven and tasK-agnostic
robustness Evaluation), a task-agnostic framework for assessing model
robustness through systematic minimal variations of test data. FLUKE introduces
controlled variations across linguistic levels - from orthography to dialect
and style varieties - and leverages large language models (LLMs) with human
validation to generate modifications. We demonstrate FLUKE's utility by
evaluating both fine-tuned models and LLMs across four diverse NLP tasks, and
reveal that (1) the impact of linguistic variations is highly task-dependent,
with some tests being critical for certain tasks but irrelevant for others; (2)
while LLMs have better overall robustness compared to fine-tuned models, they
still exhibit significant brittleness to certain linguistic variations; (3) all
models show substantial vulnerability to negation modifications across most
tasks. These findings highlight the importance of systematic robustness testing
for understanding model behaviors.

</details>


### [22] [Bridging Cognition and Emotion: Empathy-Driven Multimodal Misinformation Detection](https://arxiv.org/abs/2504.17332)
*Zihan Wang, Lu Yuan, Zhengxuan Zhang, Qing Zhao*

Main category: cs.CL

TL;DR: The paper proposes the Dual-Aspect Empathy Framework (DAE) to detect misinformation by integrating cognitive and emotional empathy, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional misinformation detection overlooks human empathy's role in propagation, prompting the need for a more human-centric approach.

Method: DAE analyzes creators' cognitive and emotional strategies and simulates readers' responses using LLMs, with an empathy-aware filtering mechanism.

Result: DAE outperforms existing methods on benchmark datasets, offering a novel paradigm for misinformation detection.

Conclusion: DAE provides a comprehensive, human-centric approach to misinformation detection, enhancing authenticity and diversity in responses.

Abstract: In the digital era, social media has become a major conduit for information
dissemination, yet it also facilitates the rapid spread of misinformation.
Traditional misinformation detection methods primarily focus on surface-level
features, overlooking the crucial roles of human empathy in the propagation
process. To address this gap, we propose the Dual-Aspect Empathy Framework
(DAE), which integrates cognitive and emotional empathy to analyze
misinformation from both the creator and reader perspectives. By examining
creators' cognitive strategies and emotional appeals, as well as simulating
readers' cognitive judgments and emotional responses using Large Language
Models (LLMs), DAE offers a more comprehensive and human-centric approach to
misinformation detection. Moreover, we further introduce an empathy-aware
filtering mechanism to enhance response authenticity and diversity.
Experimental results on benchmark datasets demonstrate that DAE outperforms
existing methods, providing a novel paradigm for multimodal misinformation
detection.

</details>


### [23] [PatientDx: Merging Large Language Models for Protecting Data-Privacy in Healthcare](https://arxiv.org/abs/2504.17360)
*Jose G. Moreno, Jesus Lovon, M'Rick Robin-Charlet, Christine Damase-Michel, Lynda Tamine*

Main category: cs.CL

TL;DR: PatientDx is a framework for merging LLMs to improve health-predictive tasks without fine-tuning on sensitive patient data, achieving up to 7% AUROC improvement on MIMIC-IV mortality tasks while reducing data privacy risks.


<details>
  <summary>Details</summary>
Motivation: Address data privacy concerns in healthcare by avoiding fine-tuning LLMs on sensitive patient data while maintaining performance.

Method: Utilizes model merging techniques, optimizing a building block strategy with a pivotal model for numerical reasoning and hyperparameter tuning without direct training on patient data.

Result: 7% AUROC improvement on MIMIC-IV mortality tasks; reduced data leak risks compared to fine-tuned models.

Conclusion: PatientDx offers a privacy-preserving alternative to fine-tuning, maintaining performance and reducing data privacy risks in healthcare applications.

Abstract: Fine-tuning of Large Language Models (LLMs) has become the default practice
for improving model performance on a given task. However, performance
improvement comes at the cost of training on vast amounts of annotated data
which could be sensitive leading to significant data privacy concerns. In
particular, the healthcare domain is one of the most sensitive domains exposed
to data privacy issues. In this paper, we present PatientDx, a framework of
model merging that allows the design of effective LLMs for health-predictive
tasks without requiring fine-tuning nor adaptation on patient data. Our
proposal is based on recently proposed techniques known as merging of LLMs and
aims to optimize a building block merging strategy. PatientDx uses a pivotal
model adapted to numerical reasoning and tunes hyperparameters on examples
based on a performance metric but without training of the LLM on these data.
Experiments using the mortality tasks of the MIMIC-IV dataset show improvements
up to 7% in terms of AUROC when compared to initial models. Additionally, we
confirm that when compared to fine-tuned models, our proposal is less prone to
data leak problems without hurting performance. Finally, we qualitatively show
the capabilities of our proposal through a case study. Our best model is
publicly available at https://huggingface.co/ Jgmorenof/mistral\_merged\_0\_4.

</details>


### [24] [LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams](https://arxiv.org/abs/2504.17366)
*Yongxuan Wu, Runyu Chen, Peiyu Liu, Hongjin Qian*

Main category: cs.CL

TL;DR: The paper introduces a spoken long-text dataset from live streams to address challenges in long-context understanding for real-world dialogues. It evaluates LLMs and specialized methods, revealing task-specific biases and poor performance on redundancy. A new baseline is proposed, and the benchmark aids future improvements.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack complexity for real-world dialogues, limiting LLM applicability. The study aims to bridge this gap with a redundancy-rich dataset.

Method: Constructed a spoken long-text dataset from live streams with tasks in retrieval-dependent, reasoning-dependent, and hybrid categories. Evaluated LLMs and specialized methods.

Result: Current methods show task-specific preferences and struggle with redundancy. A proposed baseline performs well across tasks.

Conclusion: The benchmark addresses gaps in long-context spoken language understanding, offering a foundation for real-world applications like e-commerce.

Abstract: Long-context understanding poses significant challenges in natural language
processing, particularly for real-world dialogues characterized by speech-based
elements, high redundancy, and uneven information density. Although large
language models (LLMs) achieve impressive results on existing benchmarks, these
datasets fail to reflect the complexities of such texts, limiting their
applicability to practical scenarios. To bridge this gap, we construct the
first spoken long-text dataset, derived from live streams, designed to reflect
the redundancy-rich and conversational nature of real-world scenarios. We
construct tasks in three categories: retrieval-dependent, reasoning-dependent,
and hybrid. We then evaluate both popular LLMs and specialized methods to
assess their ability to understand long-contexts in these tasks. Our results
show that current methods exhibit strong task-specific preferences and perform
poorly on highly redundant inputs, with no single method consistently
outperforming others. We propose a new baseline that better handles redundancy
in spoken text and achieves strong performance across tasks. Our findings
highlight key limitations of current methods and suggest future directions for
improving long-context understanding. Finally, our benchmark fills a gap in
evaluating long-context spoken language understanding and provides a practical
foundation for developing real-world e-commerce systems. The code and benchmark
are available at https://github.com/Yarayx/livelongbench.

</details>


### [25] [PicPersona-TOD : A Dataset for Personalizing Utterance Style in Task-Oriented Dialogue with Image Persona](https://arxiv.org/abs/2504.17390)
*Jihyun Lee, Yejin Jeon, Seungyeon Seo, Gary Geunbae Lee*

Main category: cs.CL

TL;DR: PicPersona-TOD introduces a dataset and model (Pictor) for personalized Task-Oriented Dialogue systems using user images and first impressions to tailor responses, improving engagement.


<details>
  <summary>Details</summary>
Motivation: Existing TOD systems produce generic responses lacking personalization, failing to adapt to user attributes like age or emotion.

Method: Uses user images, first impressions, dialogue policy-guided prompting, and external knowledge to reduce hallucinations. Introduces the Pictor NLG model.

Result: Human evaluations show enhanced user experience with personalized responses. Pictor performs robustly across unseen domains.

Conclusion: PicPersona-TOD and Pictor improve TOD systems by personalizing responses, making interactions more engaging and adaptable.

Abstract: Task-Oriented Dialogue (TOD) systems are designed to fulfill user requests
through natural language interactions, yet existing systems often produce
generic, monotonic responses that lack individuality and fail to adapt to
users' personal attributes. To address this, we introduce PicPersona-TOD, a
novel dataset that incorporates user images as part of the persona, enabling
personalized responses tailored to user-specific factors such as age or
emotional context. This is facilitated by first impressions, dialogue
policy-guided prompting, and the use of external knowledge to reduce
hallucinations. Human evaluations confirm that our dataset enhances user
experience, with personalized responses contributing to a more engaging
interaction. Additionally, we introduce a new NLG model, Pictor, which not only
personalizes responses, but also demonstrates robust performance across unseen
domains https://github.com/JihyunLee1/PicPersona.

</details>


### [26] [Creating Targeted, Interpretable Topic Models with LLM-Generated Text Augmentation](https://arxiv.org/abs/2504.17445)
*Anna Lieb, Maneesh Arora, Eni Mustafaraj*

Main category: cs.CL

TL;DR: Using GPT-4 to augment topic modeling improves interpretability and practicality for domain-specific research questions.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of traditional topic models (interpretability and practicality) in social science research.

Method: Augment topic modeling with GPT-4-generated text and evaluate using a political science case study.

Result: GPT-4-augmented topic modeling produces highly interpretable categories for domain-specific questions.

Conclusion: LLM-augmented topic modeling enhances utility for targeted social science research with minimal human input.

Abstract: Unsupervised machine learning techniques, such as topic modeling and
clustering, are often used to identify latent patterns in unstructured text
data in fields such as political science and sociology. These methods overcome
common concerns about reproducibility and costliness involved in the
labor-intensive process of human qualitative analysis. However, two major
limitations of topic models are their interpretability and their practicality
for answering targeted, domain-specific social science research questions. In
this work, we investigate opportunities for using LLM-generated text
augmentation to improve the usefulness of topic modeling output. We use a
political science case study to evaluate our results in a domain-specific
application, and find that topic modeling using GPT-4 augmentations creates
highly interpretable categories that can be used to investigate domain-specific
research questions with minimal human guidance.

</details>


### [27] [Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation](https://arxiv.org/abs/2504.17480)
*Xin Yi, Shunfan Zhengc, Linlin Wanga, Xiaoling Wang, Liang He*

Main category: cs.CL

TL;DR: The paper explores watermark radioactivity in LLMs, proposing CDG-KD for bidirectional attacks (removal and forgery) under unauthorized knowledge distillation, highlighting the need for robust watermarking.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored robustness and unforgeability of watermarks in LLMs against scrubbing and spoofing attacks during unauthorized knowledge distillation.

Method: Proposes CDG-KD, using contrastive decoding to extract corrupted/amplified watermarks and bidirectional distillation to train models for removal or forgery.

Result: CDG-KD effectively performs attacks while maintaining model performance, demonstrating vulnerabilities in current watermarking.

Conclusion: Emphasizes the need for more robust and unforgeable watermarking schemes in LLMs.

Abstract: Watermarking has emerged as a critical technique for combating misinformation
and protecting intellectual property in large language models (LLMs). A recent
discovery, termed watermark radioactivity, reveals that watermarks embedded in
teacher models can be inherited by student models through knowledge
distillation. On the positive side, this inheritance allows for the detection
of unauthorized knowledge distillation by identifying watermark traces in
student models. However, the robustness of watermarks against scrubbing attacks
and their unforgeability in the face of spoofing attacks under unauthorized
knowledge distillation remain largely unexplored. Existing watermark attack
methods either assume access to model internals or fail to simultaneously
support both scrubbing and spoofing attacks. In this work, we propose
Contrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified
framework that enables bidirectional attacks under unauthorized knowledge
distillation. Our approach employs contrastive decoding to extract corrupted or
amplified watermark texts via comparing outputs from the student model and
weakly watermarked references, followed by bidirectional distillation to train
new student models capable of watermark removal and watermark forgery,
respectively. Extensive experiments show that CDG-KD effectively performs
attacks while preserving the general performance of the distilled model. Our
findings underscore critical need for developing watermarking schemes that are
robust and unforgeable.

</details>


### [28] [HalluLens: LLM Hallucination Benchmark](https://arxiv.org/abs/2504.17550)
*Yejin Bang, Ziwei Ji, Alan Schelten, Anthony Hartshorn, Tara Fowler, Cheng Zhang, Nicola Cancedda, Pascale Fung*

Main category: cs.CL

TL;DR: The paper introduces a benchmark for LLM hallucinations, proposing a taxonomy to distinguish extrinsic and intrinsic types, and addresses data leakage issues with dynamic test sets.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs reduce user trust and hinder AI adoption, necessitating a unified framework for evaluation.

Method: Develops a hallucination benchmark with new extrinsic tasks and dynamic test set generation, alongside analyzing existing benchmarks.

Result: Provides a clear taxonomy and robust benchmark to evaluate hallucinations, distinguishing them from factuality.

Conclusion: The work advances LLM research by standardizing hallucination evaluation and mitigating benchmark limitations.

Abstract: Large language models (LLMs) often generate responses that deviate from user
input or training data, a phenomenon known as "hallucination." These
hallucinations undermine user trust and hinder the adoption of generative AI
systems. Addressing hallucinations is essential for the advancement of LLMs.
This paper introduces a comprehensive hallucination benchmark, incorporating
both new extrinsic and existing intrinsic evaluation tasks, built upon clear
taxonomy of hallucination. A major challenge in benchmarking hallucinations is
the lack of a unified framework due to inconsistent definitions and
categorizations. We disentangle LLM hallucination from "factuality," proposing
a clear taxonomy that distinguishes between extrinsic and intrinsic
hallucinations, to promote consistency and facilitate research. Extrinsic
hallucinations, where the generated content is not consistent with the training
data, are increasingly important as LLMs evolve. Our benchmark includes dynamic
test set generation to mitigate data leakage and ensure robustness against such
leakage. We also analyze existing benchmarks, highlighting their limitations
and saturation. The work aims to: (1) establish a clear taxonomy of
hallucinations, (2) introduce new extrinsic hallucination tasks, with data that
can be dynamically regenerated to prevent saturation by leakage, (3) provide a
comprehensive analysis of existing benchmarks, distinguishing them from
factuality evaluations.

</details>


### [29] [When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars](https://arxiv.org/abs/2504.17562)
*Rei Higuchi, Ryotaro Kawata, Naoki Nishikawa, Kazusato Oko, Shoichiro Yamaguchi, Sosuke Kobayashi, Seiya Tokui, Kohei Hayashi, Daisuke Okanohara, Taiji Suzuki*

Main category: cs.CL

TL;DR: Prepending metadata in pre-training improves model performance in downstream tasks if the context allows latent semantics inference, but harms it otherwise.


<details>
  <summary>Details</summary>
Motivation: To understand why metadata prepending inconsistently improves model performance, focusing on latent semantics inference.

Method: Examined model behavior using artificial data generated by probabilistic context-free grammars.

Result: Metadata helps with long contexts for latent semantics but hurts performance with insufficient context.

Conclusion: Metadata prepending's effectiveness depends on the downstream task's ability to infer latent semantics from the prompt.

Abstract: The ability to acquire latent semantics is one of the key properties that
determines the performance of language models. One convenient approach to
invoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at
the beginning of texts in the pre-training data, making it easier for the model
to access latent semantics before observing the entire text. Previous studies
have reported that this technique actually improves the performance of trained
models in downstream tasks; however, this improvement has been observed only in
specific downstream tasks, without consistent enhancement in average next-token
prediction loss. To understand this phenomenon, we closely investigate how
prepending metadata during pre-training affects model performance by examining
its behavior using artificial data. Interestingly, we found that this approach
produces both positive and negative effects on the downstream tasks. We
demonstrate that the effectiveness of the approach depends on whether latent
semantics can be inferred from the downstream task's prompt. Specifically,
through investigations using data generated by probabilistic context-free
grammars, we show that training with metadata helps improve model's performance
when the given context is long enough to infer the latent semantics. In
contrast, the technique negatively impacts performance when the context lacks
the necessary information to make an accurate posterior inference.

</details>


### [30] [DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training](https://arxiv.org/abs/2504.17565)
*Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Yunjie Ji, Han Zhao, Xiangang Li*

Main category: cs.CL

TL;DR: The paper introduces a large-scale, difficulty-graded reasoning dataset to improve LLM training, achieving a 79.2% pass rate on a benchmark.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding of base model training processes and data quality in LLMs.

Method: Constructs a dataset with 3.34M queries and 40M responses, using pass rate and CV to select valuable data, and adjusts learning rates for reasoning-focused training.

Result: Achieves a 79.2% pass rate on AIME2024, surpassing most distilled models and nearing state-of-the-art performance.

Conclusion: The approach enhances reasoning capabilities and promotes open-source progress by releasing datasets and methods.

Abstract: Although large language models (LLMs) have recently achieved remarkable
performance on various complex reasoning benchmarks, the academic community
still lacks an in-depth understanding of base model training processes and data
quality. To address this, we construct a large-scale, difficulty-graded
reasoning dataset containing approximately 3.34 million unique queries of
varying difficulty levels and about 40 million distilled responses generated by
multiple models over several passes. Leveraging pass rate and Coefficient of
Variation (CV), we precisely select the most valuable training data to enhance
reasoning capability. Notably, we observe a training pattern shift, indicating
that reasoning-focused training based on base models requires higher learning
rates for effective training. Using this carefully selected data, we
significantly improve the reasoning capabilities of the base model, achieving a
pass rate of 79.2\% on the AIME2024 mathematical reasoning benchmark. This
result surpasses most current distilled models and closely approaches
state-of-the-art performance. We provide detailed descriptions of our data
processing, difficulty assessment, and training methodology, and have publicly
released all datasets and methods to promote rapid progress in open-source
long-reasoning LLMs. The dataset is available at:
https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M

</details>


### [31] [RAGAT-Mind: A Multi-Granular Modeling Approach for Rumor Detection Based on MindSpore](https://arxiv.org/abs/2504.17574)
*Zhenkai Qin, Guifang Yang, Dongze Wu*

Main category: cs.CL

TL;DR: RAGAT-Mind, a multi-granular model for Chinese rumor detection, combines TextCNN, GRU, self-attention, and BiGCN, achieving 99.2% accuracy on Weibo1-Rumor dataset.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of false information on social media by improving rumor detection in Chinese text.

Method: Integrates TextCNN, bidirectional GRU, Multi-Head Self-Attention, and BiGCN for hierarchical and graph-based feature extraction.

Result: Achieves 99.2% accuracy and 0.9919 macro-F1 score, demonstrating superior performance.

Conclusion: RAGAT-Mind effectively combines linguistic and structural features, showing strong generalization and practical value for rumor detection.

Abstract: As false information continues to proliferate across social media platforms,
effective rumor detection has emerged as a pressing challenge in natural
language processing. This paper proposes RAGAT-Mind, a multi-granular modeling
approach for Chinese rumor detection, built upon the MindSpore deep learning
framework. The model integrates TextCNN for local semantic extraction,
bidirectional GRU for sequential context learning, Multi-Head Self-Attention
for global dependency focusing, and Bidirectional Graph Convolutional Networks
(BiGCN) for structural representation of word co-occurrence graphs. Experiments
on the Weibo1-Rumor dataset demonstrate that RAGAT-Mind achieves superior
classification performance, attaining 99.2% accuracy and a macro-F1 score of
0.9919. The results validate the effectiveness of combining hierarchical
linguistic features with graph-based semantic structures. Furthermore, the
model exhibits strong generalization and interpretability, highlighting its
practical value for real-world rumor detection applications.

</details>


### [32] [Towards a comprehensive taxonomy of online abusive language informed by machine leaning](https://arxiv.org/abs/2504.17653)
*Samaneh Hosseini Moghaddam, Kelly Lyons, Cheryl Regehr, Vivek Goel, Kaitlyn Regehr*

Main category: cs.CL

TL;DR: A taxonomy for classifying abusive language online is developed using 18 multi-label datasets, featuring 5 categories and 17 dimensions to improve detection and mitigation efforts.


<details>
  <summary>Details</summary>
Motivation: Addressing the risks of abusive language in online communications by creating a shared understanding for better detection and intervention.

Method: Systematic taxonomy development integrating classification systems from 18 existing multi-label datasets to capture key characteristics of abusive language.

Result: A hierarchical and faceted taxonomy with 5 categories and 17 dimensions, classifying context, target, intensity, directness, and theme of abuse.

Conclusion: The taxonomy fosters cohesive efforts, knowledge exchange, and accelerates progress in online abuse detection and mitigation among stakeholders.

Abstract: The proliferation of abusive language in online communications has posed
significant risks to the health and wellbeing of individuals and communities.
The growing concern regarding online abuse and its consequences necessitates
methods for identifying and mitigating harmful content and facilitating
continuous monitoring, moderation, and early intervention. This paper presents
a taxonomy for distinguishing key characteristics of abusive language within
online text. Our approach uses a systematic method for taxonomy development,
integrating classification systems of 18 existing multi-label datasets to
capture key characteristics relevant to online abusive language classification.
The resulting taxonomy is hierarchical and faceted, comprising 5 categories and
17 dimensions. It classifies various facets of online abuse, including context,
target, intensity, directness, and theme of abuse. This shared understanding
can lead to more cohesive efforts, facilitate knowledge exchange, and
accelerate progress in the field of online abuse detection and mitigation among
researchers, policy makers, online platform owners, and other stakeholders.

</details>


### [33] [Evaluating Grounded Reasoning by Code-Assisted Large Language Models for Mathematics](https://arxiv.org/abs/2504.17665)
*Zena Al-Khalili, Nick Howell, Dietrich Klakow*

Main category: cs.CL

TL;DR: The paper evaluates code-assisted LLMs' program generation for math reasoning tasks, focusing on grounding to math rules and its impact on performance.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of code-assisted LLMs focus on execution correctness, neglecting rigorous analysis of generated programs. This work aims to fill that gap.

Method: Analyzes programs generated by five LLMs on two math datasets, assessing grounding to math rules manually and automatically.

Result: Grounding varies by LLM capability and problem difficulty; closed-source models perform better. Ungrounded programs doubled on harder problems (MATH500 vs. ASDiv).

Conclusion: Highlights the need for deeper evaluation beyond execution accuracy to understand LLMs' math reasoning capabilities and limitations.

Abstract: Assisting LLMs with code generation improved their performance on
mathematical reasoning tasks. However, the evaluation of code-assisted LLMs is
generally restricted to execution correctness, lacking a rigorous evaluation of
their generated programs. In this work, we bridge this gap by conducting an
in-depth analysis of code-assisted LLMs' generated programs in response to math
reasoning tasks. Our evaluation focuses on the extent to which LLMs ground
their programs to math rules, and how that affects their end performance. For
this purpose, we assess the generations of five different LLMs, on two
different math datasets, both manually and automatically. Our results reveal
that the distribution of grounding depends on LLMs' capabilities and the
difficulty of math problems. Furthermore, mathematical grounding is more
effective for closed-source models, while open-source models fail to employ
math rules in their solutions correctly. On MATH500, the percentage of grounded
programs decreased to half, while the ungrounded generations doubled in
comparison to ASDiv grade-school problems. Our work highlights the need for
in-depth evaluation beyond execution accuracy metrics, toward a better
understanding of code-assisted LLMs' capabilities and limits in the math
domain.

</details>


### [34] [Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction](https://arxiv.org/abs/2504.17671)
*Yuanchang Ye, Weiyan Wen*

Main category: cs.CL

TL;DR: A Split Conformal Prediction (SCP) framework mitigates hallucination in Large Vision-Language Models (LVLMs) for VQA tasks by quantifying uncertainty and ensuring statistical guarantees.


<details>
  <summary>Details</summary>
Motivation: LVLMs often produce hallucinated content with high confidence, posing risks in safety-critical applications like healthcare and autonomous systems.

Method: The SCP framework uses dynamic threshold calibration and cross-modal consistency verification, partitioning data into calibration and test sets to compute nonconformity scores and construct prediction sets with statistical guarantees.

Result: Evaluations on benchmarks (ScienceQA, MMMU) with eight LVLMs show SCP enforces theoretical guarantees across all risk levels (α), achieving stable performance.

Conclusion: The SCP framework bridges the gap between theoretical reliability and practical applicability, offering a scalable solution for hallucination detection and uncertainty-aware decision-making in multi-modal AI systems.

Abstract: This study addresses the critical challenge of hallucination mitigation in
Large Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks
through a Split Conformal Prediction (SCP) framework. While LVLMs excel in
multi-modal reasoning, their outputs often exhibit hallucinated content with
high confidence, posing risks in safety-critical applications. We propose a
model-agnostic uncertainty quantification method that integrates dynamic
threshold calibration and cross-modal consistency verification. By partitioning
data into calibration and test sets, the framework computes nonconformity
scores to construct prediction sets with statistical guarantees under
user-defined risk levels ($\alpha$). Key innovations include: (1) rigorous
control of \textbf{marginal coverage} to ensure empirical error rates remain
strictly below $\alpha$; (2) dynamic adjustment of prediction set sizes
inversely with $\alpha$, filtering low-confidence outputs; (3) elimination of
prior distribution assumptions and retraining requirements. Evaluations on
benchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces
theoretical guarantees across all $\alpha$ values. The framework achieves
stable performance across varying calibration-to-test split ratios,
underscoring its robustness for real-world deployment in healthcare, autonomous
systems, and other safety-sensitive domains. This work bridges the gap between
theoretical reliability and practical applicability in multi-modal AI systems,
offering a scalable solution for hallucination detection and uncertainty-aware
decision-making.

</details>


### [35] [Energy Considerations of Large Language Model Inference and Efficiency Optimizations](https://arxiv.org/abs/2504.17674)
*Jared Fernandez, Clara Na, Vashisth Tiwari, Yonatan Bisk, Sasha Luccioni, Emma Strubell*

Main category: cs.CL

TL;DR: The paper analyzes the energy implications of inference efficiency optimizations in LLMs, showing workload-specific impacts and potential energy savings of up to 73%.


<details>
  <summary>Details</summary>
Motivation: Addressing the rising computational and environmental costs of LLMs by focusing on real-world inference workloads, often overlooked in prior work.

Method: Introduces a modeling approach with input-output token distributions and batch size variations, analyzing frameworks, decoding strategies, hardware, and serving settings.

Result: Demonstrates that naive energy estimates underestimate real-world consumption, and optimizations can reduce energy use by up to 73%.

Conclusion: Provides insights for sustainable LLM deployment and energy-efficient AI infrastructure design.

Abstract: As large language models (LLMs) scale in size and adoption, their
computational and environmental costs continue to rise. Prior benchmarking
efforts have primarily focused on latency reduction in idealized settings,
often overlooking the diverse real-world inference workloads that shape energy
use. In this work, we systematically analyze the energy implications of common
inference efficiency optimizations across diverse Natural Language Processing
(NLP) and generative Artificial Intelligence (AI) workloads, including
conversational AI and code generation. We introduce a modeling approach that
approximates real-world LLM workflows through a binning strategy for
input-output token distributions and batch size variations. Our empirical
analysis spans software frameworks, decoding strategies, GPU architectures,
online and offline serving settings, and model parallelism configurations. We
show that the effectiveness of inference optimizations is highly sensitive to
workload geometry, software stack, and hardware accelerators, demonstrating
that naive energy estimates based on FLOPs or theoretical GPU utilization
significantly underestimate real-world energy consumption. Our findings reveal
that the proper application of relevant inference efficiency optimizations can
reduce total energy use by up to 73% from unoptimized baselines. These insights
provide a foundation for sustainable LLM deployment and inform energy-efficient
design strategies for future AI infrastructure.

</details>


### [36] [Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks](https://arxiv.org/abs/2504.17685)
*Haru-Tada Sato, Fuka Matsuzaki, Jun-ichiro Takahashi*

Main category: cs.CL

TL;DR: Small language model (SLM) ensembles using Ensemble Bayesian Inference (EBI) achieve accuracy comparable to large proprietary models, even with models of lower individual performance.


<details>
  <summary>Details</summary>
Motivation: To explore cost-effective alternatives to proprietary large language models (LLMs) by leveraging ensembles of smaller models.

Method: Proposes Ensemble Bayesian Inference (EBI), a Bayesian approach to combine judgments from multiple SLMs, tested on tasks like aptitude assessments and consumer profile analysis in Japanese and English.

Result: EBI improves performance beyond individual SLMs, even incorporating models with negative Lift values, and works across languages.

Conclusion: EBI offers a resource-efficient way to build high-performance AI systems and utilize lower-performing models effectively.

Abstract: This study explores the potential of small language model(SLM) ensembles to
achieve accuracy comparable to proprietary large language models (LLMs). We
propose Ensemble Bayesian Inference (EBI), a novel approach that applies
Bayesian estimation to combine judgments from multiple SLMs, allowing them to
exceed the performance limitations of individual models. Our experiments on
diverse tasks(aptitude assessments and consumer profile analysis in both
Japanese and English) demonstrate EBI's effectiveness. Notably, we analyze
cases where incorporating models with negative Lift values into ensembles
improves overall performance, and we examine the method's efficacy across
different languages. These findings suggest new possibilities for constructing
high-performance AI systems with limited computational resources and for
effectively utilizing models with individually lower performance. Building on
existing research on LLM performance evaluation, ensemble methods, and
open-source LLM utilization, we discuss the novelty and significance of our
approach.

</details>


### [37] [Safety in Large Reasoning Models: A Survey](https://arxiv.org/abs/2504.17704)
*Cheng Wang, Yue Liu, Baolong Li, Duzhen Zhang, Zhongzhi Li, Junfeng Fang*

Main category: cs.CL

TL;DR: A survey of safety risks, attacks, and defenses in Large Reasoning Models (LRMs), organized into a taxonomy for clarity.


<details>
  <summary>Details</summary>
Motivation: Address growing concerns about vulnerabilities and safety in LRMs as their reasoning capabilities advance.

Method: Comprehensive survey and taxonomy of safety risks, attacks, and defense strategies in LRMs.

Result: Structured understanding of LRM safety landscape to aid future research.

Conclusion: The work facilitates improved security and reliability of LRMs for real-world applications.

Abstract: Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks
like mathematics and coding, leveraging their advanced reasoning capabilities.
Nevertheless, as these capabilities progress, significant concerns regarding
their vulnerabilities and safety have arisen, which can pose challenges to
their deployment and application in real-world settings. This paper presents a
comprehensive survey of LRMs, meticulously exploring and summarizing the newly
emerged safety risks, attacks, and defense strategies. By organizing these
elements into a detailed taxonomy, this work aims to offer a clear and
structured understanding of the current safety landscape of LRMs, facilitating
future research and development to enhance the security and reliability of
these powerful models.

</details>


### [38] [Multilingual Performance Biases of Large Language Models in Education](https://arxiv.org/abs/2504.17720)
*Vansh Gupta, Sankalan Pal Chowdhury, Vilém Zouhar, Donya Rooein, Mrinmaya Sachan*

Main category: cs.CL

TL;DR: LLMs perform variably in non-English educational tasks, with performance linked to training data. Practitioners should verify effectiveness in target languages before deployment.


<details>
  <summary>Details</summary>
Motivation: To assess if LLMs are suitable for educational tasks in non-English languages, given their English-centric nature.

Method: Evaluated popular LLMs on four educational tasks (misconception identification, feedback, tutoring, grading) across six non-English languages and English.

Result: Performance correlates with training data representation; lower-resource languages show poorer results. Significant drops from English performance observed.

Conclusion: Practitioners should verify LLM performance in target languages before educational deployment.

Abstract: Large language models (LLMs) are increasingly being adopted in educational
settings. These applications expand beyond English, though current LLMs remain
primarily English-centric. In this work, we ascertain if their use in education
settings in non-English languages is warranted. We evaluated the performance of
popular LLMs on four educational tasks: identifying student misconceptions,
providing targeted feedback, interactive tutoring, and grading translations in
six languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to
English. We find that the performance on these tasks somewhat corresponds to
the amount of language represented in training data, with lower-resource
languages having poorer task performance. Although the models perform
reasonably well in most languages, the frequent performance drop from English
is significant. Thus, we recommend that practitioners first verify that the LLM
works well in the target language for their educational task before deployment.

</details>


### [39] [Conversational Assistants to support Heart Failure Patients: comparing a Neurosymbolic Architecture with ChatGPT](https://arxiv.org/abs/2504.17753)
*Anuja Tayal, Devika Salunke, Barbara Di Eugenio, Paula Allen-Meares, Eulalia Puig Abril, Olga Garcia, Carolyn Dickens, Andrew Boyd*

Main category: cs.CL

TL;DR: A study compares a neurosymbolic in-house conversational assistant with a ChatGPT-based one for heart failure patients, finding trade-offs in accuracy, verbosity, and speech errors, with no patient preference.


<details>
  <summary>Details</summary>
Motivation: The rise of conversational assistants in healthcare, driven by Large Language Models, necessitates evaluations to compare traditional and generative AI architectures.

Method: A within-group user study comparing two versions of a conversational assistant (neurosymbolic in-house vs. ChatGPT-based) for heart failure patients querying salt content in food.

Result: The in-house system is more accurate, completes more tasks, and is less verbose, while ChatGPT-based system makes fewer speech errors and needs fewer clarifications. Patients show no preference.

Conclusion: Both architectures have strengths and weaknesses, suggesting context-specific utility in healthcare applications.

Abstract: Conversational assistants are becoming more and more popular, including in
healthcare, partly because of the availability and capabilities of Large
Language Models. There is a need for controlled, probing evaluations with real
stakeholders which can highlight advantages and disadvantages of more
traditional architectures and those based on generative AI. We present a
within-group user study to compare two versions of a conversational assistant
that allows heart failure patients to ask about salt content in food. One
version of the system was developed in-house with a neurosymbolic architecture,
and one is based on ChatGPT. The evaluation shows that the in-house system is
more accurate, completes more tasks and is less verbose than the one based on
ChatGPT; on the other hand, the one based on ChatGPT makes fewer speech errors
and requires fewer clarifications to complete the task. Patients show no
preference for one over the other.

</details>


### [40] [The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs](https://arxiv.org/abs/2504.17768)
*Piotr Nawrot, Robert Li, Renjie Huang, Sebastian Ruder, Kelly Marchisio, Edoardo M. Ponti*

Main category: cs.CL

TL;DR: Sparse attention in Transformer LLMs enhances long-context capabilities, but trade-offs and scaling effects vary by task, model size, and sparsity level. Larger, sparser models excel for very long sequences, but no universal strategy exists. Novel scaling laws validate findings.


<details>
  <summary>Details</summary>
Motivation: To explore the viability, efficiency-accuracy trade-offs, and scaling effects of sparse attention in Transformer LLMs for long-context tasks.

Method: Comparative study of training-free sparse attention methods across model scales, sequence lengths, and sparsity levels on diverse long-sequence tasks.

Result: 1) Larger, sparser models outperform smaller dense ones for long sequences. 2) Higher sparsity is feasible during decoding than prefilling. 3) No universal sparsity strategy; performance degradation occurs in some tasks. 4) Novel scaling laws validate findings.

Conclusion: Sparse attention is valuable for long-context LLMs but requires careful evaluation of trade-offs for optimal performance.

Abstract: Sparse attention offers a promising strategy to extend long-context
capabilities in Transformer LLMs, yet its viability, its efficiency-accuracy
trade-offs, and systematic scaling studies remain unexplored. To address this
gap, we perform a careful comparison of training-free sparse attention methods
at varying model scales, sequence lengths, and sparsity levels on a diverse
collection of long-sequence tasks-including novel ones that rely on natural
language while remaining controllable and easy to evaluate. Based on our
experiments, we report a series of key findings: 1) an isoFLOPS analysis
reveals that for very long sequences, larger and highly sparse models are
preferable to smaller and dense ones. 2) The level of sparsity attainable while
statistically guaranteeing accuracy preservation is higher during decoding than
prefilling, and correlates with model size in the former. 3) There is no clear
strategy that performs best across tasks and phases, with different units of
sparsification or budget adaptivity needed for different scenarios. Even
moderate sparsity levels often result in significant performance degradation on
at least one task, highlighting that sparse attention is not a universal
solution. 4) We introduce and validate novel scaling laws specifically tailored
for sparse attention, providing evidence that our findings are likely to hold
true beyond our range of experiments. Through these insights, we demonstrate
that sparse attention is a key tool to enhance the capabilities of Transformer
LLMs for processing longer sequences, but requires careful evaluation of
trade-offs for performance-sensitive applications.

</details>


### [41] [CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization](https://arxiv.org/abs/2406.07494)
*Frederic Kirstein, Jan Philip Wahle, Bela Gipp, Terry Ruas*

Main category: cs.CL

TL;DR: The paper reviews Transformer-based abstractive dialogue summarization, identifying key challenges (e.g., language, factuality) and linking them to techniques like graph-based methods. It highlights gaps in datasets, evaluation metrics, and the impact of large language models.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive work on dialogue summarization challenges and unify understanding of techniques, datasets, and evaluation metrics.

Method: Systematic review of 1262 papers (2019-2024) from Semantic Scholar and DBLP, analyzing challenges, techniques, and evaluation methods.

Result: Progress in language-related challenges but persistent issues in comprehension, factuality, and salience. Limited datasets, overuse of ROUGE, and insufficient human evaluation detail.

Conclusion: Despite advances, key challenges remain, and the taxonomy stays relevant even with large language models.

Abstract: Abstractive dialogue summarization is the task of distilling conversations
into informative and concise summaries. Although reviews have been conducted on
this topic, there is a lack of comprehensive work detailing the challenges of
dialogue summarization, unifying the differing understanding of the task, and
aligning proposed techniques, datasets, and evaluation metrics with the
challenges. This article summarizes the research on Transformer-based
abstractive summarization for English dialogues by systematically reviewing
1262 unique research papers published between 2019 and 2024, relying on the
Semantic Scholar and DBLP databases. We cover the main challenges present in
dialog summarization (i.e., language, structure, comprehension, speaker,
salience, and factuality) and link them to corresponding techniques such as
graph-based approaches, additional training tasks, and planning strategies,
which typically overly rely on BART-based encoder-decoder models. We find that
while some challenges, like language, have seen considerable progress, mainly
due to training methods, others, such as comprehension, factuality, and
salience, remain difficult and hold significant research opportunities. We
investigate how these approaches are typically assessed, covering the datasets
for the subdomains of dialogue (e.g., meeting, medical), the established
automatic metrics and human evaluation approaches for assessing scores and
annotator agreement. We observe that only a few datasets span across all
subdomains. The ROUGE metric is the most used, while human evaluation is
frequently reported without sufficient detail on inner-annotator agreement and
annotation guidelines. Additionally, we discuss the possible implications of
the recently explored large language models and conclude that despite a
potential shift in relevance and difficulty, our described challenge taxonomy
remains relevant.

</details>


### [42] [Synthetic Lyrics Detection Across Languages and Genres](https://arxiv.org/abs/2406.15231)
*Yanis Labrak, Markus Frohmann, Gabriel Meseguer-Brocal, Elena V. Epure*

Main category: cs.CL

TL;DR: The paper explores detecting synthetic lyrics generated by LLMs, addressing gaps in existing research by evaluating detection methods on a diverse dataset and adapting features for lyrics.


<details>
  <summary>Details</summary>
Motivation: The rise of LLM-generated music content raises concerns about copyright, satisfaction, and spamming, but lacks focus on lyrics. This work fills that gap.

Method: Curated a diverse dataset of real and synthetic lyrics, validated generation pipeline, evaluated detection methods, and adapted features for lyrics.

Result: Promising results in detecting synthetic lyrics, with insights on generalization across languages, scalability, and few-shot performance.

Conclusion: The findings can inform AI-generated music policies and improve user transparency.

Abstract: In recent years, the use of large language models (LLMs) to generate music
content, particularly lyrics, has gained in popularity. These advances provide
valuable tools for artists and enhance their creative processes, but they also
raise concerns about copyright violations, consumer satisfaction, and content
spamming. Previous research has explored content detection in various domains.
However, no work has focused on the text modality, lyrics, in music. To address
this gap, we curated a diverse dataset of real and synthetic lyrics from
multiple languages, music genres, and artists. The generation pipeline was
validated using both humans and automated methods. We performed a thorough
evaluation of existing synthetic text detection approaches on lyrics, a
previously unexplored data type. We also investigated methods to adapt the
best-performing features to lyrics through unsupervised domain adaptation.
Following both music and industrial constraints, we examined how well these
approaches generalize across languages, scale with data availability, handle
multilingual language content, and perform on novel genres in few-shot
settings. Our findings show promising results that could inform policy
decisions around AI-generated music and enhance transparency for users.

</details>


### [43] [OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure](https://arxiv.org/abs/2406.17276)
*Jikai Wang, Yi Su, Juntao Li, Qingrong Xia, Zi Ye, Xinyu Duan, Zhefeng Wang, Min Zhang*

Main category: cs.CL

TL;DR: OPT-Tree is an adaptive draft tree algorithm for speculative decoding, improving inference efficiency in autoregressive language models by maximizing token acceptance per step.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models' one-step-one-word generation limits inference efficiency, especially as models grow larger. Existing draft structures lack adaptability.

Method: Proposes OPT-Tree, which constructs scalable draft trees by optimizing the structure to maximize expected token acceptance length.

Result: Achieves up to 3.2x speed-up over autoregressive decoding, with potential for generating >10 tokens per step under ideal conditions.

Conclusion: OPT-Tree offers a scalable, adaptive solution for efficient speculative decoding, significantly accelerating autoregressive models.

Abstract: Autoregressive language models demonstrate excellent performance in various
scenarios. However, the inference efficiency is limited by its
one-step-one-word generation mode, which has become a pressing problem recently
as the models become increasingly larger. Speculative decoding employs a "draft
and then verify" mechanism to allow multiple tokens to be generated in one
step, realizing lossless acceleration. Existing methods mainly adopt fixed
heuristic draft structures, which fail to adapt to different situations to
maximize the acceptance length during verification. To alleviate this dilemma,
we proposed OPT-Tree, an algorithm to construct adaptive and scalable draft
trees. It searches the optimal tree structure that maximizes the mathematical
expectation of the acceptance length in each decoding step. Experimental
results reveal that OPT-Tree outperforms the existing draft structures and
achieves a speed-up ratio of up to 3.2 compared with autoregressive decoding.
If the draft model is powerful enough and the node budget is sufficient, it can
generate more than ten tokens in a single step. Our code is available at
https://github.com/Jikai0Wang/OPT-Tree.

</details>


### [44] [LaMsS: When Large Language Models Meet Self-Skepticism](https://arxiv.org/abs/2409.06601)
*Yetao Wu, Yihong Wang, Teng Chen, Ningyuan Xi, Qingqing Gu, Hongyang Lei, Luo Ji*

Main category: cs.CL

TL;DR: The paper proposes LaMsS, a method combining LLMs with self-skepticism to reduce hallucinations by introducing skepticism tokens and thresholds for response confidence.


<details>
  <summary>Details</summary>
Motivation: Hallucination in LLMs limits their applications; human skeptical thinking could help LLMs self-reflect and mitigate hallucinations.

Method: Introduces skepticism tokens into LLM vocabulary, performs pretraining and finetuning to decode normal tokens followed by skeptical ones, and sets response skepticism thresholds.

Result: LaMsS outperforms baselines in accuracy, AUC, and AP on multi-choice and open-domain QA benchmarks, generalizing well to multi-task and out-of-domain settings.

Conclusion: LaMsS demonstrates the potential of self-skepticism in AI, improving LLM reliability. Code and models are available.

Abstract: Hallucination is a major challenge for large language models (LLMs),
preventing their further application in some fields. The skeptical thinking of
humankind could be useful for LLMs to self-cognition, self-reflection and
alleviate their hallucinations. Inspired by this consideration, we propose a
novel approach called LaMsS, which combines the semantic understanding
capability of LLMs with self-skepticism. By introducing a series of skepticism
tokens and augmenting them into the vocabulary, we conduct both pertaining and
finetuning, which allow the LLM to decode each normal token followed by a
skeptical token, representing different skepticism levels. By calculating the
response skepticism given a query, one can define a new self-aware LLM which is
only willing to answer with relative lower skepticism level than the threshold.
By examining the accuracy, AUC and AP of willingly answering questions, we
demonstrate that LaMsS achieves better performance than baselines on both
multi-choice questions and open-domain question-answering benchmarks, and can
generalize to multi-task and out-of-domain settings. Our study sheds some
lights on the self-skepticism modeling on further artificial intelligence.
Project code and model checkpoints can be found in
https://anonymous.4open.science/r/SM-1E76.

</details>


### [45] [Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse](https://arxiv.org/abs/2409.11242)
*Maojia Song, Shang Hong Sim, Rishabh Bhardwaj, Hai Leong Chieu, Navonil Majumder, Soujanya Poria*

Main category: cs.CL

TL;DR: The paper introduces Trust-Score to evaluate LLMs in RAG systems and proposes Trust-Align to improve their performance, showing significant gains over baselines.


<details>
  <summary>Details</summary>
Motivation: There is a lack of understanding about how suitable LLMs are for RAG tasks, prompting the need for a metric like Trust-Score.

Method: Trust-Score evaluates LLM trustworthiness in RAG, and Trust-Align is introduced to align LLMs for better performance.

Result: Trust-Align improves performance on benchmarks (ASQA, QAMPARI, ELI5) and enhances citation quality and refusal ability.

Conclusion: Trust-Align effectively aligns LLMs for RAG tasks, outperforming baselines and improving trustworthiness.

Abstract: LLMs are an integral component of retrieval-augmented generation (RAG)
systems. While many studies focus on evaluating the overall quality of
end-to-end RAG systems, there is a gap in understanding the appropriateness of
LLMs for the RAG task. To address this, we introduce Trust-Score, a holistic
metric that evaluates the trustworthiness of LLMs within the RAG framework. Our
results show that various prompting methods, such as in-context learning, fail
to effectively adapt LLMs to the RAG task as measured by Trust-Score.
Consequently, we propose Trust-Align, a method to align LLMs for improved
Trust-Score performance. 26 out of 27 models aligned using Trust-Align
substantially outperform competitive baselines on ASQA, QAMPARI, and ELI5.
Specifically, in LLaMA-3-8b, Trust-Align outperforms FRONT on ASQA (up 12.56),
QAMPARI (up 36.04), and ELI5 (up 17.69). Trust-Align also significantly
enhances models' ability to correctly refuse and provide quality citations. We
also demonstrate the effectiveness of Trust-Align across different open-weight
models, including the LLaMA series (1b to 8b), Qwen-2.5 series (0.5b to 7b),
and Phi3.5 (3.8b). We release our code at
https://github.com/declare-lab/trust-align.

</details>


### [46] [Lab-AI: Using Retrieval Augmentation to Enhance Language Models for Personalized Lab Test Interpretation in Clinical Medicine](https://arxiv.org/abs/2409.18986)
*Xiaoyu Wang, Haoyong Ouyang, Balu Bhasuran, Xiao Luo, Karim Hanna, Mia Liza A. Lustria, Carl Yang, Zhe He*

Main category: cs.CL

TL;DR: Lab-AI uses RAG to provide personalized lab result interpretations, outperforming non-RAG systems significantly.


<details>
  <summary>Details</summary>
Motivation: Current patient portals use universal normal ranges, ignoring patient-specific factors like age and gender, leading to inaccurate interpretations.

Method: Lab-AI employs two modules: factor retrieval and normal range retrieval, tested on 122 lab tests using GPT-4-turbo with RAG.

Result: Achieved 0.948 F1 score for factor retrieval and 0.995 accuracy for normal range retrieval, outperforming non-RAG systems by 33.5% and up to 132%.

Conclusion: Lab-AI demonstrates significant potential to improve patient understanding of lab results through personalized interpretations.

Abstract: Accurate interpretation of lab results is crucial in clinical medicine, yet
most patient portals use universal normal ranges, ignoring conditional factors
like age and gender. This study introduces Lab-AI, an interactive system that
offers personalized normal ranges using retrieval-augmented generation (RAG)
from credible health sources. Lab-AI has two modules: factor retrieval and
normal range retrieval. We tested these on 122 lab tests: 40 with conditional
factors and 82 without. For tests with factors, normal ranges depend on
patient-specific information. Our results show GPT-4-turbo with RAG achieved a
0.948 F1 score for factor retrieval and 0.995 accuracy for normal range
retrieval. GPT-4-turbo with RAG outperformed the best non-RAG system by 33.5%
in factor retrieval and showed 132% and 100% improvements in question-level and
lab-level performance, respectively, for normal range retrieval. These findings
highlight Lab-AI's potential to enhance patient understanding of lab results.

</details>


### [47] [Can LLMs Really Learn to Translate a Low-Resource Language from One Grammar Book?](https://arxiv.org/abs/2409.19151)
*Seth Aycock, David Stap, Di Wu, Christof Monz, Khalil Sima'an*

Main category: cs.CL

TL;DR: The paper explores how grammar books aid translation in extremely low-resource (XLR) languages, finding parallel examples more useful than grammatical explanations, and suggests focusing on task-appropriate data like parallel examples for translation and grammatical data for linguistic tasks.


<details>
  <summary>Details</summary>
Motivation: To understand the source of translation ability in XLR languages when using grammar books with long-context LLMs, and to determine the most effective data types for such tasks.

Method: Investigates translation improvements by analyzing grammar books, tests on XLR languages (Kalamang, Nepali, Guarani), and evaluates grammatical knowledge via tasks like grammaticality judgment and gloss prediction.

Result: Parallel examples in grammar books drive translation improvements, not grammatical explanations. A typological feature prompt excels in linguistic tasks.

Conclusion: For XLR translation, prioritize parallel data over linguistic descriptions, as grammatical explanations are ineffective with current LLMs.

Abstract: Extremely low-resource (XLR) languages lack substantial corpora for training
NLP models, motivating the use of all available resources such as dictionaries
and grammar books. Machine Translation from One Book (Tanzer et al., 2024)
suggests that prompting long-context LLMs with one grammar book enables
English-Kalamang translation, an XLR language unseen by LLMs - a noteworthy
case of linguistics helping an NLP task. We investigate the source of this
translation ability, finding almost all improvements stem from the book's
parallel examples rather than its grammatical explanations. We find similar
results for Nepali and Guarani, seen low-resource languages, and we achieve
performance comparable to an LLM with a grammar book by simply fine-tuning an
encoder-decoder translation model. We then investigate where grammar books help
by testing two linguistic tasks, grammaticality judgment and gloss prediction,
and we explore what kind of grammatical knowledge helps by introducing a
typological feature prompt that achieves leading results on these more relevant
tasks. We thus emphasise the importance of task-appropriate data for XLR
languages: parallel examples for translation, and grammatical data for
linguistic tasks. As we find no evidence that long-context LLMs can make
effective use of grammatical explanations for XLR translation, we conclude data
collection for multilingual XLR tasks such as translation is best focused on
parallel data over linguistic description.

</details>


### [48] [TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking](https://arxiv.org/abs/2410.01952)
*Danqing Wang, Jianxin Ma, Fei Fang, Lei Li*

Main category: cs.CL

TL;DR: TypedThinker enhances LLM reasoning by predicting and applying diverse reasoning types (inductive, abductive, analogical) beyond traditional deductive methods, improving performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current LLMs rely heavily on deductive reasoning, limiting their problem-solving diversity. Some problems require specific reasoning strategies, but identifying and applying these is challenging.

Method: Proposes TypedThinker, which predicts suitable reasoning types for problems and provides demonstrations to guide LLMs in applying these strategies.

Result: Achieves performance gains of 3.4% to 7% on benchmarks for models like Mistral 7B, LLaMA3 8B, and Qwen 2 7B.

Conclusion: TypedThinker diversifies LLM reasoning without needing knowledge distillation, making it adaptable for advanced or specialized models.

Abstract: Large Language Models (LLMs) have demonstrated strong reasoning capabilities
in solving complex problems. However, current approaches primarily enhance
reasoning through the elaboration of thoughts while neglecting the diversity of
reasoning types. LLMs typically employ deductive reasoning, proceeding
step-by-step from given conditions, which limits their exploration during
problem-solving. Our analysis reveals that certain problems are exclusively
solvable through specific reasoning strategies like inductive, abductive, or
analogical reasoning. However, incorporating diverse reasoning approaches
presents two key challenges: identifying the appropriate reasoning type for
each problem and exploiting this approach during problem-solving. Therefore, we
propose the TypedThinker that predicts suitable reasoning types based on the
problem and their previous effectiveness and provides relevant demonstrations
to guide LLMs in applying these strategies. Experimental results show
significant improvements across multiple benchmarks, with performance gains of
3.4% for Mistral 7B, 6.5% for LLaMA3 8B, and 7% for Qwen 2 7B on logical and
mathematical reasoning tasks. TypedThinker enhances LLM reasoning without
requiring knowledge distillation from larger models. It can be integrated into
more advanced systems like GPT-4o or specialized models like MetaMath to
diversify their reasoning approaches and improve their problem-solving
capabilities.

</details>


### [49] [Selective Attention Improves Transformer](https://arxiv.org/abs/2410.02703)
*Yaniv Leviathan, Matan Kalman, Yossi Matias*

Main category: cs.CL

TL;DR: Selective Attention improves transformer performance by reducing attention to unneeded elements, saving memory and compute.


<details>
  <summary>Details</summary>
Motivation: Unneeded elements in attention's context degrade performance, prompting a need for a more efficient mechanism.

Method: Introduces Selective Attention, a parameter-free change to standard attention, reducing focus on irrelevant elements.

Result: Improves language modeling and downstream tasks, reduces memory/compute needs (e.g., 16X-47X less memory for attention).

Conclusion: Selective Attention enhances efficiency without sacrificing performance, enabling smaller, faster models.

Abstract: Unneeded elements in the attention's context degrade performance. We
introduce Selective Attention, a simple parameter-free change to the standard
attention mechanism which reduces attention to unneeded elements. Selective
attention consistently improves language modeling and downstream task
performance in a variety of model sizes and context lengths. For example,
transformers trained with the language modeling objective on C4 with selective
attention perform language modeling equivalently to standard transformers with
~2X more heads and parameters in their attention modules. Selective attention
also allows decreasing the size of the attention's context buffer, leading to
meaningful reductions in the memory and compute requirements during inference.
For example, transformers trained on C4 with context sizes of 512, 1,024, and
2,048 need 16X, 25X, and 47X less memory for their attention module,
respectively, when equipped with selective attention, as those without
selective attention, with the same validation perplexity.

</details>


### [50] [Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs: Thematic Insights and Fairness Evaluation](https://arxiv.org/abs/2410.05401)
*Tunazzina Islam, Dan Goldwasser*

Main category: cs.CL

TL;DR: The study analyzes microtargeting in climate change campaigns on Facebook using LLMs, achieving 88.55% accuracy in predicting demographic targets. It highlights thematic strategies for different groups and identifies biases in model predictions.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness and fairness of LLMs in analyzing microtargeting practices in climate change communication on social media.

Method: Post-hoc analysis of Facebook ads using LLMs to predict demographic targets and generate explanations for classifications.

Result: LLMs achieved 88.55% accuracy in predicting targets, with distinct strategies for young adults (activism) and women (caregiving). Biases were found in classifying seniors and males.

Conclusion: LLMs are effective for dissecting microtargeting but require fairness improvements. The study offers a framework for enhancing transparency and inclusivity in climate campaigns.

Abstract: Climate change communication on social media increasingly employs
microtargeting strategies to effectively reach and influence specific
demographic groups. This study presents a post-hoc analysis of microtargeting
practices within climate campaigns by leveraging large language models (LLMs)
to examine Facebook advertisements. Our analysis focuses on two key aspects:
demographic targeting and fairness. We evaluate the ability of LLMs to
accurately predict the intended demographic targets, such as gender and age
group, achieving an overall accuracy of 88.55%. Furthermore, we instruct the
LLMs to generate explanations for their classifications, providing transparent
reasoning behind each decision. These explanations reveal the specific thematic
elements used to engage different demographic segments, highlighting distinct
strategies tailored to various audiences. Our findings show that young adults
are primarily targeted through messages emphasizing activism and environmental
consciousness, while women are engaged through themes related to caregiving
roles and social advocacy. In addition to evaluating the effectiveness of LLMs
in detecting microtargeted messaging, we conduct a comprehensive fairness
analysis to identify potential biases in model predictions. Our findings
indicate that while LLMs perform well overall, certain biases exist,
particularly in the classification of senior citizens and male audiences. By
showcasing the efficacy of LLMs in dissecting and explaining targeted
communication strategies and by highlighting fairness concerns, this study
provides a valuable framework for future research aimed at enhancing
transparency, accountability, and inclusivity in social media-driven climate
campaigns.

</details>


### [51] [Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies](https://arxiv.org/abs/2410.19878)
*Luping Wang, Sheng Chen, Linnan Jiang, Shu Pan, Runze Cai, Sen Yang, Fei Yang*

Main category: cs.CL

TL;DR: The paper reviews Parameter-Efficient Fine-Tuning (PEFT) as a solution to the computational challenges of large models, detailing its methods, applications, and future directions.


<details>
  <summary>Details</summary>
Motivation: Large models face high computational and storage costs, making fine-tuning difficult on limited hardware. PEFT addresses this by minimizing additional parameters and resources.

Method: PEFT adjusts pre-trained model parameters for specific tasks with minimal computational overhead. The review covers its core ideas, algorithms, and applications.

Result: PEFT provides a practical way to adapt large models efficiently, reducing resource demands while maintaining performance.

Conclusion: The review aims to help readers quickly understand PEFT, fostering its development and innovation in the field.

Abstract: The large models, as predicted by scaling raw forecasts, have made
groundbreaking progress in many fields, particularly in natural language
generation tasks, where they have approached or even surpassed human levels.
However, the unprecedented scale of their parameters brings significant
computational and storage costs. These large models require substantial
computational resources and GPU memory to operate. When adapting large models
to specific downstream tasks, their massive parameter scale poses a significant
challenge in fine-tuning on hardware platforms with limited computational power
and GPU memory. To address this issue, Parameter-Efficient Fine-Tuning (PEFT)
offers a practical solution by efficiently adjusting the parameters of large
pre-trained models to suit various downstream tasks. Specifically, PEFT adjusts
the parameters of pre-trained large models to adapt to specific tasks or
domains, minimizing the introduction of additional parameters and the
computational resources required. This review mainly introduces the preliminary
knowledge of PEFT, the core ideas and principles of various PEFT algorithms,
the applications of PEFT, and potential future research directions. By reading
this review, we believe that interested parties can quickly grasp the PEFT
methodology, thereby accelerating its development and innovation.

</details>


### [52] [Estimating the Influence of Sequentially Correlated Literary Properties in Textual Classification: A Data-Centric Hypothesis-Testing Approach](https://arxiv.org/abs/2411.04950)
*Gideon Yoffe, Nachum Dershowitz, Ariel Vishne, Barak Sober*

Main category: cs.CL

TL;DR: A framework to test if textual classification is influenced by thematic continuity or non-sequential features like style, using surrogate labelings and statistical testing.


<details>
  <summary>Details</summary>
Motivation: To determine if classification outcomes are driven by thematic structure or non-sequential features, addressing potential confounds in tasks like authorship attribution.

Method: Models label sequences as stochastic processes, uses empirical autocovariance for surrogate labelings, and compares traditional and neural embeddings in supervised/unsupervised settings.

Result: Supervised and neural models are prone to false positives (confusing themes for style), while unsupervised models with traditional features perform better.

Conclusion: Controlling for sequential correlation is crucial to reduce false positives and ensure classification reflects genuine stylistic differences, impacting fields like forensic linguistics.

Abstract: We introduce a data-centric hypothesis-testing framework to quantify the
influence of sequentially correlated literary properties--such as thematic
continuity--on textual classification tasks. Our method models label sequences
as stochastic processes and uses an empirical autocovariance matrix to generate
surrogate labelings that preserve sequential dependencies. This enables
statistical testing to determine whether classification outcomes are primarily
driven by thematic structure or by non-sequential features like authorial
style. Applying this framework across a diverse corpus of English prose, we
compare traditional (word n-grams and character k-mers) and neural
(contrastively trained) embeddings in both supervised and unsupervised
classification settings. Crucially, our method identifies when classifications
are confounded by sequentially correlated similarity, revealing that supervised
and neural models are more prone to false positives--mistaking shared themes
and cross-genre differences for stylistic signals. In contrast, unsupervised
models using traditional features often yield high true positive rates with
minimal false positives, especially in genre-consistent settings. By
disentangling sequential from non-sequential influences, our approach provides
a principled way to assess and interpret classification reliability. This is
particularly impactful for authorship attribution, forensic linguistics, and
the analysis of redacted or composite texts, where conventional methods may
conflate theme with style. Our results demonstrate that controlling for
sequential correlation is essential for reducing false positives and ensuring
that classification outcomes reflect genuine stylistic distinctions.

</details>


### [53] [jina-clip-v2: Multilingual Multimodal Embeddings for Text and Images](https://arxiv.org/abs/2412.08802)
*Andreas Koukounas, Georgios Mastrapas, Sedigheh Eslami, Bo Wang, Mohammad Kalim Akram, Michael Günther, Isabelle Mohr, Saba Sturua, Nan Wang, Han Xiao*

Main category: cs.CL

TL;DR: jina-clip-v2 improves CLIP by supporting text-only tasks, multilingual understanding, and visually rich documents via multi-task contrastive learning.


<details>
  <summary>Details</summary>
Motivation: CLIP models underperform in text-only tasks, lack multilingual support, and struggle with visually rich documents.

Method: Uses multilingual text encoder and multi-task contrastive learning on text pairs, triplets, and image-text pairs.

Result: Outperforms CLIP in zero-shot text retrieval, semantic similarity, and crossmodal tasks in multilingual settings.

Conclusion: jina-clip-v2 enhances CLIP's versatility and performance, offering flexible embedding dimensions.

Abstract: Contrastive Language-Image Pretraining (CLIP) has been widely used for
crossmodal information retrieval and multimodal understanding tasks. However,
CLIP models are mainly optimized for crossmodal vision-language tasks and
underperform in single-mode text tasks. Moreover, these models are often
trained on English datasets and therefore lack multilingual understanding.
Additionally, from a visual understanding perspective, previous CLIP-based
models exhibit insufficient understanding of visually rich documents. In this
work, we propose jina-clip-v2, a contrastive vision-language model trained on
text pairs, triplets and image-text pairs via a multi-task and multi-stage
contrastive learning paradigm in order to support both text-only and crossmodal
tasks. We employ a multilingual text encoder and expand the training dataset to
include multilingual texts from 29 non-English languages, including Hindi,
Chinese, German, French, and others, as well as images of visually rich
documents. We evaluate the model's performance and show that jina-clip-v2
achieves notable improvements over state-of-the-art CLIP-based models in
zero-shot text-only retrieval, semantic textual similarity, and crossmodal
retrieval tasks in both English and multilingual settings. jina-clip-v2 also
provides for flexibility in embedding dimensionality, enabling users to select
the granularity of the representations. jina-clip-v2 is publicly available at
https://huggingface.co/jinaai/jina-clip-v2.

</details>


### [54] [Context-Aware Neural Gradient Mapping for Fine-Grained Instruction Processing](https://arxiv.org/abs/2501.14936)
*David Boldo, Lily Pemberton, Gabriel Thistledown, Jacob Fairchild, Felix Kowalski*

Main category: cs.CL

TL;DR: The paper introduces a Context-Aware Neural Gradient Mapping framework for dynamic gradient adjustments in large language models, improving generalization and efficiency.


<details>
  <summary>Details</summary>
Motivation: Enhancing task-specific generalization and computational efficiency in language models, especially with sparse or noisy data.

Method: Uses contextual embeddings and gradient descent modifications, leveraging differential geometry for efficient adaptation.

Result: Outperforms baselines in accuracy, robustness, and efficiency, scaling well for large models.

Conclusion: The framework improves language understanding and scalability, making it viable for diverse applications.

Abstract: The integration of contextual embeddings into the optimization processes of
large language models is an advancement in natural language processing. The
Context-Aware Neural Gradient Mapping framework introduces a dynamic gradient
adjustment mechanism, incorporating contextual embeddings directly into the
optimization process. This approach facilitates real-time parameter
adjustments, enhancing task-specific generalization even in the presence of
sparse or noisy data inputs. The mathematical foundation of this framework
relies on gradient descent modifications, where contextual embeddings are
derived from a supplementary neural network trained to map input features to
optimal adaptation gradients. By employing differential geometry principles,
high-dimensional input dependencies are encoded into low-dimensional gradient
manifolds, enabling efficient adaptation without necessitating the retraining
of the entire model. Empirical evaluations demonstrate that the proposed
framework consistently outperforms baseline models across various metrics,
including accuracy, robustness to noise, and computational efficiency. The
integration of context-specific embeddings allows for a more complex
understanding of language, thereby improving the model's ability to handle
diverse linguistic phenomena. Furthermore, the computational efficiency
achieved through this method demonstrates its scalability for large-scale
language models operating under diverse constraints.

</details>


### [55] [Multilingual State Space Models for Structured Question Answering in Indic Languages](https://arxiv.org/abs/2502.01673)
*Arpita Vats, Rahul Raja, Mrinal Mathur, Vinija Jain, Aman Chadha*

Main category: cs.CL

TL;DR: The paper explores State Space Models (SSMs) for question answering in Indic languages, demonstrating their effectiveness in handling linguistic complexities and improving QA performance.


<details>
  <summary>Details</summary>
Motivation: Indic languages pose unique NLP challenges due to their diversity and complexity, requiring specialized solutions for QA tasks.

Method: The study evaluates multiple SSM architectures on diverse Indic language datasets, analyzing their performance in capturing linguistic nuances.

Result: SSMs significantly improve question interpretation, context alignment, and answer generation, setting a benchmark for future research.

Conclusion: The work pioneers SSMs for Indic QA, proposing framework enhancements for low-resource and multilingual settings.

Abstract: The diversity and complexity of Indic languages present unique challenges for
natural language processing (NLP) tasks, particularly in the domain of question
answering (QA).To address these challenges, this paper explores the application
of State Space Models (SSMs),to build efficient and contextually aware QA
systems tailored for Indic languages. SSMs are particularly suited for this
task due to their ability to model long-term and short-term dependencies in
sequential data, making them well-equipped to handle the rich morphology,
complex syntax, and contextual intricacies characteristic of Indian languages.
We evaluated multiple SSM architectures across diverse datasets representing
various Indic languages and conducted a comparative analysis of their
performance. Our results demonstrate that these models effectively capture
linguistic subtleties, leading to significant improvements in question
interpretation, context alignment, and answer generation. This work represents
the first application of SSMs to question answering tasks in Indic languages,
establishing a foundational benchmark for future research in this domain. We
propose enhancements to existing SSM frameworks, optimizing their applicability
to low-resource settings and multilingual scenarios prevalent in Indic
languages.

</details>


### [56] [Probabilistic Subspace Manifolds for Contextual Inference in Large Language Models](https://arxiv.org/abs/2502.05346)
*Christopher Nightingale, Dominic Lavington, Jonathan Thistlethwaite, Sebastian Penhaligon, Thomas Belinski, David Boldo*

Main category: cs.CL

TL;DR: Probabilistic token embeddings improve semantic granularity and contextual coherence, enhancing robustness and adaptability in NLP tasks.


<details>
  <summary>Details</summary>
Motivation: To address the rigidity of conventional embeddings and improve contextual inference and semantic granularity.

Method: Represent token embeddings as probability distributions over learned manifolds and integrate probabilistic subspaces within attention mechanisms.

Result: Improved neighborhood consistency, reduced redundancy, increased robustness against adversarial modifications, and better adaptability across domains.

Conclusion: Probabilistic embeddings offer enhanced representation stability, contextual expressiveness, and advantages in generative modeling tasks.

Abstract: Representing token embeddings as probability distributions over learned
manifolds allows for more flexible contextual inference, reducing
representational rigidity while enhancing semantic granularity. Comparative
evaluations demonstrate that probabilistic embeddings improve neighborhood
consistency and decrease redundancy, ensuring that token relationships remain
more structurally coherent across fine-tuning iterations. The integration of
probabilistic subspaces within attention mechanisms facilitates more adaptive
contextual weighting, enabling models to capture latent dependencies that would
otherwise be obscured in conventional embeddings. Experimental results
highlight increased robustness against adversarial modifications, with
probabilistic embeddings preserving contextual integrity even under
perturbation-based evaluation scenarios. Performance assessments indicate that
probabilistic representations achieve greater adaptability in domain-specific
applications, mitigating the need for extensive retraining when shifting across
linguistic domains. Computational trade-offs remain within operationally
feasible limits, with marginal increases in inference latency balanced against
the benefits of enhanced representation stability and contextual
expressiveness. The capacity to encode structured uncertainty provides
advantages in generative modeling tasks, particularly where maintaining
coherence across extended sequences requires a representation framework capable
of handling ambiguous or context-dependent linguistic constructs.

</details>


### [57] [Towards Reasoning Ability of Small Language Models](https://arxiv.org/abs/2502.11569)
*Gaurav Srivastava, Shuxiang Cao, Xuan Wang*

Main category: cs.CL

TL;DR: Small language models (SLMs) can achieve reasoning abilities comparable to large language models (LLMs) through structured training or compression, challenging the assumption that reasoning emerges only at scale.


<details>
  <summary>Details</summary>
Motivation: To investigate whether SLMs can match LLMs in reasoning performance, given their efficiency and deployability advantages.

Method: Systematically survey, benchmark, and analyze 72 SLMs from six families across 14 reasoning benchmarks, using four evaluation methods and comparing LLM judges to human evaluations.

Result: SLMs can achieve reasoning abilities comparable to LLMs, with structured training or post-training compression enabling strong performance.

Conclusion: Scaling is not the only path to strong reasoning; SLMs can serve as efficient alternatives for reasoning-intensive tasks.

Abstract: Reasoning has long been viewed as an emergent property of large language
models (LLMs), appearing at or above a certain scale ($\sim$100B parameters).
However, recent studies challenge this assumption, showing that small language
models (SLMs) can also achieve competitive reasoning performance. SLMs are
increasingly favored for their efficiency and deployability. However, there is
a lack of systematic study on the reasoning abilities of diverse SLMs,
including those trained from scratch or derived from LLMs through quantization,
pruning, and distillation. This raises a critical question: Can SLMs achieve
reasoning abilities comparable to LLMs? In this work, we systematically survey,
benchmark, and analyze 72 SLMs from six model families across 14 reasoning
benchmarks. For reliable evaluation, we examine four evaluation methods and
compare four LLM judges against human evaluations on 800 data points. We repeat
all experiments three times to ensure a robust performance assessment.
Additionally, we analyze the impact of different prompting strategies in small
models. Beyond accuracy, we also evaluate model robustness under adversarial
conditions and intermediate reasoning steps. Our findings challenge the
assumption that scaling is the only way to achieve strong reasoning. Instead,
we foresee a future where SLMs with strong reasoning capabilities can be
developed through structured training or post-training compression. They can
serve as efficient alternatives to LLMs for reasoning-intensive tasks.

</details>


### [58] [PSCon: Product Search Through Conversations](https://arxiv.org/abs/2502.13881)
*Jie Zou, Mohammad Aliannejadi, Evangelos Kanoulas, Shuxi Han, Heli Ma, Zheng Wang, Yang Yang, Heng Tao Shen*

Main category: cs.CL

TL;DR: The paper introduces PSCon, a new dataset for Conversational Product Search (CPS) using human-like language, addressing gaps in existing datasets by supporting cross-market and multi-lingual usage. It also proposes a benchmark model.


<details>
  <summary>Details</summary>
Motivation: Existing CPS research relies on simulated conversations and lacks real datasets with human-like language. Current datasets are limited to specific markets or languages.

Method: A coached human-human data collection protocol was used to create PSCon, a dataset for CPS, supporting dual markets and two languages.

Result: PSCon enables research on six CPS subtasks and includes a benchmark model.

Conclusion: The dataset and model advance CPS research by providing a realistic, versatile resource.

Abstract: Conversational Product Search ( CPS ) systems interact with users via natural
language to offer personalized and context-aware product lists. However, most
existing research on CPS is limited to simulated conversations, due to the lack
of a real CPS dataset driven by human-like language. Moreover, existing
conversational datasets for e-commerce are constructed for a particular market
or a particular language and thus can not support cross-market and
multi-lingual usage. In this paper, we propose a CPS data collection protocol
and create a new CPS dataset, called PSCon, which assists product search
through conversations with human-like language. The dataset is collected by a
coached human-human data collection protocol and is available for dual markets
and two languages. By formulating the task of CPS, the dataset allows for
comprehensive and in-depth research on six subtasks: user intent detection,
keyword extraction, system action prediction, question selection, item ranking,
and response generation. Moreover, we present a concise analysis of the dataset
and propose a benchmark model on the proposed CPS dataset. Our proposed dataset
and model will be helpful for facilitating future research on CPS.

</details>


### [59] [Automatically Evaluating the Paper Reviewing Capability of Large Language Models](https://arxiv.org/abs/2502.17086)
*Hyungyu Shin, Jingyu Tang, Yoonjoo Lee, Nayoung Kim, Hyunseung Lim, Ji Yong Cho, Hwajung Hong, Moontae Lee, Juho Kim*

Main category: cs.CL

TL;DR: An automated pipeline evaluates LLMs' paper review capabilities, finding they lack balanced perspectives, overlook novelty, and make poor acceptance decisions compared to expert reviews.


<details>
  <summary>Details</summary>
Motivation: Peer review is critical but faces challenges like reviewer shortages. LLMs could help, but their review quality is limited, and evaluating them is time-consuming.

Method: Developed an automatic evaluation pipeline using 676 OpenReview papers to compare LLMs' reviews with expert reviews, focusing on strengths, weaknesses, and agreement.

Result: LLMs lack balanced perspectives, overlook novelty in criticisms, and perform poorly in acceptance decisions compared to experts.

Conclusion: The automated pipeline provides a scalable way to assess LLMs' review capabilities over time, highlighting current limitations.

Abstract: Peer review is essential for scientific progress, but it faces challenges
such as reviewer shortages and growing workloads. Although Large Language
Models (LLMs) show potential for providing assistance, research has reported
significant limitations in the reviews they generate. While the insights are
valuable, conducting the analysis is challenging due to the considerable time
and effort required, especially given the rapid pace of LLM developments. To
address the challenge, we developed an automatic evaluation pipeline to assess
the LLMs' paper review capability by comparing them with expert-generated
reviews. By constructing a dataset consisting of 676 OpenReview papers, we
examined the agreement between LLMs and experts in their strength and weakness
identifications. The results showed that LLMs lack balanced perspectives,
significantly overlook novelty assessment when criticizing, and produce poor
acceptance decisions. Our automated pipeline enables a scalable evaluation of
LLMs' paper review capability over time.

</details>


### [60] [SemEval-2025 Task 11: Bridging the Gap in Text-Based Emotion Detection](https://arxiv.org/abs/2503.07269)
*Shamsuddeen Hassan Muhammad, Nedjma Ousidhoum, Idris Abdulmumin, Seid Muhie Yimam, Jan Philip Wahle, Terry Ruas, Meriem Beloucif, Christine De Kock, Tadesse Destaw Belay, Ibrahim Said Ahmad, Nirmal Surange, Daniela Teodorescu, David Ifeoluwa Adelani, Alham Fikri Aji, Felermino Ali, Vladimir Araujo, Abinew Ali Ayele, Oana Ignat, Alexander Panchenko, Yi Zhou, Saif M. Mohammad*

Main category: cs.CL

TL;DR: A shared task on text-based emotion detection in over 30 low-resource languages, with multi-label and intensity annotations, attracted 700+ participants and 200+ teams.


<details>
  <summary>Details</summary>
Motivation: To advance emotion detection in diverse, low-resource languages and provide a benchmark for cross-lingual and multi-label emotion analysis.

Method: Participants predicted labels in three tracks: multi-label emotion detection, emotion intensity scoring, and cross-lingual detection. Baseline results and top-performing systems were analyzed.

Result: Over 200 teams submitted results, with 93 system papers. Findings highlight effective methods and common approaches across tracks and languages.

Conclusion: The task successfully created a benchmark dataset and insights for emotion detection in low-resource languages, with public data availability.

Abstract: We present our shared task on text-based emotion detection, covering more
than 30 languages from seven distinct language families. These languages are
predominantly low-resource and are spoken across various continents. The data
instances are multi-labeled with six emotional classes, with additional
datasets in 11 languages annotated for emotion intensity. Participants were
asked to predict labels in three tracks: (a) multilabel emotion detection, (b)
emotion intensity score detection, and (c) cross-lingual emotion detection.
  The task attracted over 700 participants. We received final submissions from
more than 200 teams and 93 system description papers. We report baseline
results, along with findings on the best-performing systems, the most common
approaches, and the most effective methods across different tracks and
languages. The datasets for this task are publicly available. The dataset is
available at SemEval2025 Task 11 https://brighter-dataset.github.io

</details>


### [61] [HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks](https://arxiv.org/abs/2503.10894)
*Jiuding Sun, Jing Huang, Sidharth Baskaran, Karel D'Oosterlinck, Christopher Potts, Michael Sklar, Atticus Geiger*

Main category: cs.CL

TL;DR: HyperDAS improves mechanistic interpretability by using a transformer-based hypernetwork to locate and construct concept features in neural networks, achieving state-of-the-art performance on the RAVEL benchmark.


<details>
  <summary>Details</summary>
Motivation: Current methods like DAS require brute-force searches for concept features, which is inefficient. HyperDAS aims to automate this process while ensuring interpretability.

Method: HyperDAS employs a transformer-based hypernetwork to automatically identify token-positions and construct features in the residual stream.

Result: HyperDAS outperforms existing methods on the RAVEL benchmark with Llama3-8B, demonstrating superior concept disentanglement.

Conclusion: HyperDAS advances interpretability by automating feature location and construction, though care is needed to avoid injecting new information into the model.

Abstract: Mechanistic interpretability has made great strides in identifying neural
network features (e.g., directions in hidden activation space) that mediate
concepts(e.g., the birth year of a person) and enable predictable manipulation.
Distributed alignment search (DAS) leverages supervision from counterfactual
data to learn concept features within hidden states, but DAS assumes we can
afford to conduct a brute force search over potential feature locations. To
address this, we present HyperDAS, a transformer-based hypernetwork
architecture that (1) automatically locates the token-positions of the residual
stream that a concept is realized in and (2) constructs features of those
residual stream vectors for the concept. In experiments with Llama3-8B,
HyperDAS achieves state-of-the-art performance on the RAVEL benchmark for
disentangling concepts in hidden states. In addition, we review the design
decisions we made to mitigate the concern that HyperDAS (like all powerful
interpretabilty methods) might inject new information into the target model
rather than faithfully interpreting it.

</details>


### [62] [Shared Global and Local Geometry of Language Model Embeddings](https://arxiv.org/abs/2503.21073)
*Andrew Lee, Melanie Weber, Fernanda Viégas, Martin Wattenberg*

Main category: cs.CL

TL;DR: Token embeddings in language models share common geometric structures, including global similarities and local geometry, revealing lower-dimensional manifolds and semantic coherence. This alignment enables applications like Emb2Emb for interpretability.


<details>
  <summary>Details</summary>
Motivation: To explore and characterize the geometric structure of token embeddings in language models, uncovering shared representations and their implications for interpretability.

Method: Analyzed global similarities in token embeddings and local geometry using Locally Linear Embeddings and intrinsic dimension measures. Developed Emb2Emb for transferring steering vectors between models.

Result: Token embeddings lie on lower-dimensional manifolds, with semantically coherent clusters for lower intrinsic dimensions. Alignment persists through hidden states, enabling cross-model applications.

Conclusion: Common geometric structures in token embeddings reveal shared representations and enable practical tools like Emb2Emb for model interpretability.

Abstract: Researchers have recently suggested that models share common representations.
In our work, we find that token embeddings of language models exhibit common
geometric structure. First, we find ``global'' similarities: token embeddings
often share similar relative orientations. Next, we characterize local geometry
in two ways: (1) by using Locally Linear Embeddings, and (2) by defining a
simple measure for the intrinsic dimension of each token embedding. Our
intrinsic dimension demonstrates that token embeddings lie on a lower
dimensional manifold. We qualitatively show that tokens with lower intrinsic
dimensions often have semantically coherent clusters, while those with higher
intrinsic dimensions do not. Both characterizations allow us to find
similarities in the local geometry of token embeddings. Perhaps most
surprisingly, we find that alignment in token embeddings persists through the
hidden states of language models, allowing us to develop an application for
interpretability. Namely, we introduce Emb2Emb, a simple method to transfer
steering vectors from one language model to another, despite the two models
having different dimensions.

</details>


### [63] [Cognitive Memory in Large Language Models](https://arxiv.org/abs/2504.02441)
*Lianlei Shan, Shixian Luo, Zezhou Zhu, Yu Yuan, Yong Wu*

Main category: cs.CL

TL;DR: The paper analyzes memory mechanisms in LLMs, categorizing them into sensory, short-term, and long-term memory, and explores methods for acquisition, management, and utilization. It also discusses KV cache-based and parameter-based memory techniques, concluding with future research directions.


<details>
  <summary>Details</summary>
Motivation: To understand and improve LLM memory mechanisms for better context-rich responses, reduced hallucinations, and enhanced efficiency.

Method: Categorizes memory into sensory, short-term, and long-term, and details text-based, KV cache-based, parameter-based, and hidden-state-based memory methods.

Result: Provides a comprehensive analysis of memory mechanisms, including acquisition, management, and utilization strategies, and highlights efficiency improvements.

Conclusion: Emphasizes the importance of memory mechanisms in LLMs and suggests future research directions.

Abstract: This paper examines memory mechanisms in Large Language Models (LLMs),
emphasizing their importance for context-rich responses, reduced
hallucinations, and improved efficiency. It categorizes memory into sensory,
short-term, and long-term, with sensory memory corresponding to input prompts,
short-term memory processing immediate context, and long-term memory
implemented via external databases or structures. The text-based memory section
covers acquisition (selection and summarization), management (updating,
accessing, storing, and resolving conflicts), and utilization (full-text
search, SQL queries, semantic search). The KV cache-based memory section
discusses selection methods (regularity-based summarization, score-based
approaches, special token embeddings) and compression techniques (low-rank
compression, KV merging, multimodal compression), along with management
strategies like offloading and shared attention mechanisms. Parameter-based
memory methods (LoRA, TTT, MoE) transform memories into model parameters to
enhance efficiency, while hidden-state-based memory approaches (chunk
mechanisms, recurrent transformers, Mamba model) improve long-text processing
by combining RNN hidden states with current methods. Overall, the paper offers
a comprehensive analysis of LLM memory mechanisms, highlighting their
significance and future research directions.

</details>


### [64] [Not All Data Are Unlearned Equally](https://arxiv.org/abs/2504.05058)
*Aravind Krishnan, Siva Reddy, Marius Mosbach*

Main category: cs.CL

TL;DR: The paper explores machine unlearning in LLMs, showing that the success of unlearning depends on the frequency of the knowledge in pre-training data, with more frequent knowledge being harder to remove. It also highlights misalignment in evaluation methods and calls for better practices.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of unlearning specific knowledge in LLMs, particularly for privacy, and to investigate the assumption that all data points are equally unlearnable.

Method: The study examines how unlearning success varies with the frequency of knowledge in pre-training data and evaluates misalignment between probability and generation-based methods.

Result: Frequent knowledge is harder to unlearn, and evaluation misalignment worsens with larger models.

Conclusion: Better evaluation practices and methods considering training data are needed for effective LLM unlearning.

Abstract: Machine unlearning is concerned with the task of removing knowledge learned
from particular data points from a trained model. In the context of large
language models (LLMs), unlearning has recently received increased attention,
particularly for removing knowledge about named entities from models for
privacy purposes. While various approaches have been proposed to address the
unlearning problem, most existing approaches treat all data points to be
unlearned equally, i.e., unlearning that Montreal is a city in Canada is
treated exactly the same as unlearning the phone number of the first author of
this paper. In this work, we show that this all data is equal assumption does
not hold for LLM unlearning. We study how the success of unlearning depends on
the frequency of the knowledge we want to unlearn in the pre-training data of a
model and find that frequency strongly affects unlearning, i.e., more frequent
knowledge is harder to unlearn. Additionally, we uncover a misalignment between
probability and generation-based evaluations of unlearning and show that this
problem worsens as models become larger. Overall, our experiments highlight the
need for better evaluation practices and novel methods for LLM unlearning that
take the training data of models into account.

</details>


### [65] [Multilingual MFA: Forced Alignment on Low-Resource Related Languages](https://arxiv.org/abs/2504.07315)
*Alessio Tosolini, Claire Bowern*

Main category: cs.CL

TL;DR: Comparing multilingual vs. crosslingual training for Australian languages, adapting an English model shows benefits for unseen languages.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of multilingual and crosslingual training for related and unrelated Australian languages with similar phonology.

Method: Using the Montreal Forced Aligner, acoustic models were trained from scratch and adapted from a large English model, tested on seen data, unseen data (same language), and unseen data/language.

Result: Adapting the English baseline model improved performance for previously unseen languages.

Conclusion: Crosslingual adaptation of an English model is advantageous for handling unseen languages.

Abstract: We compare the outcomes of multilingual and crosslingual training for related
and unrelated Australian languages with similar phonological inventories. We
use the Montreal Forced Aligner to train acoustic models from scratch and adapt
a large English model, evaluating results against seen data, unseen data (seen
language), and unseen data and language. Results indicate benefits of adapting
the English baseline model for previously unseen languages.

</details>


### [66] [Transferable text data distillation by trajectory matching](https://arxiv.org/abs/2504.09818)
*Rong Yao, Hailin Hu, Yifei Fu, Hanting Chen, Wenyi Fang, Fanyi Du, Kai Han, Yunhe Wang*

Main category: cs.CL

TL;DR: A novel data distillation method for LLMs is introduced, synthesizing small datasets to match full-data training effects, outperforming SOTA selection methods and showing cross-architecture transferability.


<details>
  <summary>Details</summary>
Motivation: High training costs of large language models (LLMs) necessitate minimizing data size without compromising performance, especially in NLP where text discreteness complicates distillation.

Method: The method involves learning pseudo prompt data via trajectory matching, nearest neighbor ID matching, and regularization loss for robustness.

Result: Outperforms SOTA data selection (LESS) on ARC-Easy and MMLU benchmarks and shows transferability across LLM architectures (OPT to Llama).

Conclusion: This is the first effective data distillation approach for text generation tasks like instruction tuning, offering flexibility and efficiency.

Abstract: In the realm of large language model (LLM), as the size of large models
increases, it also brings higher training costs. There is a urgent need to
minimize the data size in LLM training. Compared with data selection method,
the data distillation method aims to synthesize a small number of data samples
to achieve the training effect of the full data set and has better flexibility.
Despite its successes in computer vision, the discreteness of text data has
hitherto stymied its exploration in natural language processing (NLP). In this
work, we proposed a method that involves learning pseudo prompt data based on
trajectory matching and finding its nearest neighbor ID to achieve
cross-architecture transfer. During the distillation process, we introduce a
regularization loss to improve the robustness of our distilled data. To our
best knowledge, this is the first data distillation work suitable for text
generation tasks such as instruction tuning. Evaluations on two benchmarks,
including ARC-Easy and MMLU instruction tuning datasets, established the
superiority of our distillation approach over the SOTA data selection method
LESS. Furthermore, our method demonstrates a good transferability over LLM
structures (i.e., OPT to Llama).

</details>


### [67] [GeoSense: Evaluating Identification and Application of Geometric Principles in Multimodal Reasoning](https://arxiv.org/abs/2504.12597)
*Liangyu Xu, Yingxiu Zhao, Jingyun Wang, Yingyao Wang, Bu Pi, Chen Wang, Mingliang Zhang, Jihao Gu, Xiang Li, Xiaoyong Zhu, Jun Song, Bo Zheng*

Main category: cs.CL

TL;DR: GeoSense is a bilingual benchmark for evaluating MLLMs' geometric reasoning, revealing gaps in principle identification and application.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack joint assessment of visual and symbolic reasoning in MLLMs for geometry problem-solving.

Method: GeoSense introduces a hierarchical framework, annotated dataset, and evaluation strategy to test MLLMs.

Result: Gemini-2.0-pro-flash scored highest (65.3), but principle identification and application remain challenging.

Conclusion: GeoSense highlights MLLMs' limitations and guides future improvements in geometric reasoning.

Abstract: Geometry problem-solving (GPS), a challenging task requiring both visual
comprehension and symbolic reasoning, effectively measures the reasoning
capabilities of multimodal large language models (MLLMs). Humans exhibit strong
reasoning ability in this task through accurate identification and adaptive
application of geometric principles within visual contexts. However, existing
benchmarks fail to jointly assess both dimensions of the human-like geometric
reasoning mechanism in MLLMs, remaining a critical gap in assessing their
ability to tackle GPS. To this end, we introduce GeoSense, the first
comprehensive bilingual benchmark designed to systematically evaluate the
geometric reasoning abilities of MLLMs through the lens of geometric
principles. GeoSense features a five-level hierarchical framework of geometric
principles spanning plane and solid geometry, an intricately annotated dataset
of 1,789 problems, and an innovative evaluation strategy. Through extensive
experiments on GeoSense with various open-source and closed-source MLLMs, we
observe that Gemini-2.0-pro-flash performs best, achieving an overall score of
$65.3$. Our in-depth analysis reveals that the identification and application
of geometric principles remain a bottleneck for leading MLLMs, jointly
hindering their reasoning abilities. These findings underscore GeoSense's
potential to guide future advancements in MLLMs' geometric reasoning
capabilities, paving the way for more robust and human-like reasoning in
artificial intelligence.

</details>


### [68] [From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs](https://arxiv.org/abs/2504.13471)
*Jiliang Ni, Jiachen Pu, Zhongyi Yang, Kun Zhou, Hui Wang, Xiaoliang Xiao, Dakui Wang, Xin Li, Jingfeng Luo, Conggang Hu*

Main category: cs.CL

TL;DR: A three-stage pipeline (prototyping, knowledge transfer, model compression) is introduced to optimize cost and performance in LLM-based systems, yielding a tiny, efficient model.


<details>
  <summary>Details</summary>
Motivation: Address the high costs and latency of one-stage LLM pipelines while maintaining performance.

Method: 1. Prototype with function call-based LLM pipeline. 2. Transfer knowledge to a smaller student model. 3. Compress via quantization and pruning.

Result: A 0.4B model with ultra-low latency and cost, retaining performance.

Conclusion: The modular, cost-efficient framework shows promise for broader NLP applications.

Abstract: In recent years, Large Language Models (LLMs) have significantly advanced
artificial intelligence by optimizing traditional Natural Language Processing
(NLP) pipelines, improving performance and generalization. This has spurred
their integration into various systems. Many NLP systems, including ours,
employ a "one-stage" pipeline directly incorporating LLMs. While effective,
this approach incurs substantial costs and latency due to the need for large
model parameters to achieve satisfactory outcomes. This paper introduces a
three-stage cost-efficient end-to-end LLM deployment pipeline-including
prototyping, knowledge transfer, and model compression-to tackle the
cost-performance dilemma in LLM-based frameworks. Our approach yields a super
tiny model optimized for cost and performance in online systems, simplifying
the system architecture. Initially, by transforming complex tasks into a
function call-based LLM-driven pipeline, an optimal performance prototype
system is constructed to produce high-quality data as a teacher model. The
second stage combines techniques like rejection fine-tuning, reinforcement
learning, and knowledge distillation to transfer knowledge to a smaller 0.5B
student model, delivering effective performance at minimal cost. The final
stage applies quantization and pruning to extremely compress models to 0.4B,
achieving ultra-low latency and cost. The framework's modular design and
cross-domain capabilities suggest potential applicability in other NLP areas.

</details>


### [69] [Efficient Pretraining Length Scaling](https://arxiv.org/abs/2504.14992)
*Bohong Wu, Shen Yan, Sijun Zhang, Jianqiao Lu, Yutao Zeng, Ya Wang, Xun Zhou*

Main category: cs.CL

TL;DR: The paper introduces the PHD-Transformer, a framework for efficient length scaling in pre-training, using a novel KV cache management strategy and optimized variants (PHD-SWA and PHD-CSWA) to improve performance.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored potential of length scaling in pre-training and address inefficiencies in existing methods.

Method: Proposes the PHD-Transformer with a KV cache management strategy and introduces two optimized variants (PHD-SWA and PHD-CSWA) for enhanced performance.

Result: Demonstrates consistent improvements across multiple benchmarks.

Conclusion: The PHD-Transformer effectively enables length scaling in pre-training while maintaining inference efficiency.

Abstract: Recent advances in large language models have demonstrated the effectiveness
of length scaling during post-training, yet its potential in pre-training
remains underexplored. We present the Parallel Hidden Decoding Transformer
(\textit{PHD}-Transformer), a novel framework that enables efficient length
scaling during pre-training while maintaining inference efficiency.
\textit{PHD}-Transformer achieves this through an innovative KV cache
management strategy that distinguishes between original tokens and hidden
decoding tokens. By retaining only the KV cache of original tokens for
long-range dependencies while immediately discarding hidden decoding tokens
after use, our approach maintains the same KV cache size as the vanilla
transformer while enabling effective length scaling. To further enhance
performance, we introduce two optimized variants: \textit{PHD-SWA} employs
sliding window attention to preserve local dependencies, while
\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate
linear growth in pre-filling time. Extensive experiments demonstrate consistent
improvements across multiple benchmarks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [70] [Dense Air Pollution Estimation from Sparse in-situ Measurements and Satellite Data](https://arxiv.org/abs/2504.17039)
*Ruben Gonzalez Avilés, Linus Scheibenreif, Damian Borth*

Main category: cs.CV

TL;DR: A novel dense estimation technique for ambient NO$_2$ concentration estimation is introduced, improving accuracy and computational efficiency over existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational intensity and limitations of current satellite-based air pollution estimation methods.

Method: Uses a uniformly random offset sampling strategy to disperse ground truth data, enabling efficient grid-based estimates.

Result: Achieves a 9.45% improvement in accuracy (MAE of 4.98 µg/m³) and reduces computational resources.

Conclusion: The method provides a scalable and accurate solution for global environmental monitoring.

Abstract: This paper addresses the critical environmental challenge of estimating
ambient Nitrogen Dioxide (NO$_2$) concentrations, a key issue in public health
and environmental policy. Existing methods for satellite-based air pollution
estimation model the relationship between satellite and in-situ measurements at
select point locations. While these approaches have advanced our ability to
provide air quality estimations on a global scale, they come with inherent
limitations. The most notable limitation is the computational intensity
required for generating comprehensive estimates over extensive areas. Motivated
by these limitations, this study introduces a novel dense estimation technique.
Our approach seeks to balance the accuracy of high-resolution estimates with
the practicality of computational constraints, thereby enabling efficient and
scalable global environmental assessment. By utilizing a uniformly random
offset sampling strategy, our method disperses the ground truth data pixel
location evenly across a larger patch. At inference, the dense estimation
method can then generate a grid of estimates in a single step, significantly
reducing the computational resources required to provide estimates for larger
areas. Notably, our approach also surpasses the results of existing point-wise
methods by a significant margin of $9.45\%$, achieving a Mean Absolute Error
(MAE) of $4.98\ \mu\text{g}/\text{m}^3$. This demonstrates both high accuracy
and computational efficiency, highlighting the applicability of our method for
global environmental assessment. Furthermore, we showcase the method's
adaptability and robustness by applying it to diverse geographic regions. Our
method offers a viable solution to the computational challenges of large-scale
environmental monitoring.

</details>


### [71] [DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs](https://arxiv.org/abs/2504.17040)
*Zhenhailong Wang, Senthil Purushwalkam, Caiming Xiong, Silvio Savarese, Heng Ji, Ran Xu*

Main category: cs.CV

TL;DR: DyMU is a training-free framework that dynamically reduces computational costs in vision-language models (VLMs) by merging similar visual tokens and reconstructing attention dynamics, achieving high performance with reduced tokens.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in fixed-length outputs of vision transformers and reduce computational burden in VLMs without additional training.

Method: Combines Dynamic Token Merging (DToMe) to merge similar visual tokens based on image complexity and Virtual Token Unmerging (VTU) to simulate full-sequence attention dynamics.

Result: Reduces visual token count by 32%-85% while maintaining comparable performance to full-length models across diverse VLM architectures.

Conclusion: DyMU offers a scalable, training-free solution for efficient VLM operation, adaptable to image complexity and user control over computational costs.

Abstract: We present DyMU, an efficient, training-free framework that dynamically
reduces the computational burden of vision-language models (VLMs) while
maintaining high task performance. Our approach comprises two key components.
First, Dynamic Token Merging (DToMe) reduces the number of visual token
embeddings by merging similar tokens based on image complexity, addressing the
inherent inefficiency of fixed-length outputs in vision transformers. Second,
Virtual Token Unmerging (VTU) simulates the expected token sequence for large
language models (LLMs) by efficiently reconstructing the attention dynamics of
a full sequence, thus preserving the downstream performance without additional
fine-tuning. Unlike previous approaches, our method dynamically adapts token
compression to the content of the image and operates completely training-free,
making it readily applicable to most state-of-the-art VLM architectures.
Extensive experiments on image and video understanding tasks demonstrate that
DyMU can reduce the average visual token count by 32%-85% while achieving
comparable performance to full-length models across diverse VLM architectures,
including the recently popularized AnyRes-based visual encoders. Furthermore,
through qualitative analyses, we demonstrate that DToMe effectively adapts
token reduction based on image complexity and, unlike existing systems,
provides users more control over computational costs. Project page:
https://mikewangwzhl.github.io/dymu/.

</details>


### [72] [DIVE: Inverting Conditional Diffusion Models for Discriminative Tasks](https://arxiv.org/abs/2504.17253)
*Yinqi Li, Hong Chang, Ruibing Hou, Shiguang Shan, Xilin Chen*

Main category: cs.CV

TL;DR: The paper explores using pretrained diffusion models for discriminative tasks, specifically object detection, by inverting a layout-to-image diffusion model. It introduces gradient-based optimization and a prior distribution model, achieving results comparable to baselines on COCO and faster classification without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: To extend the discriminative capabilities of pretrained diffusion models beyond classification to more complex tasks like object detection, leveraging their generative strengths.

Method: Inverts a pretrained layout-to-image diffusion model using gradient-based discrete optimization and a prior distribution model to apply Bayes' rule accurately.

Result: Achieves performance on par with basic discriminative object detection baselines on COCO and speeds up diffusion-based classification without losing accuracy.

Conclusion: Demonstrates the feasibility of repurposing generative diffusion models for discriminative tasks effectively, with potential for broader applications.

Abstract: Diffusion models have shown remarkable progress in various generative tasks
such as image and video generation. This paper studies the problem of
leveraging pretrained diffusion models for performing discriminative tasks.
Specifically, we extend the discriminative capability of pretrained frozen
generative diffusion models from the classification task to the more complex
object detection task, by "inverting" a pretrained layout-to-image diffusion
model. To this end, a gradient-based discrete optimization approach for
replacing the heavy prediction enumeration process, and a prior distribution
model for making more accurate use of the Bayes' rule, are proposed
respectively. Empirical results show that this method is on par with basic
discriminative object detection baselines on COCO dataset. In addition, our
method can greatly speed up the previous diffusion-based method for
classification without sacrificing accuracy. Code and models are available at
https://github.com/LiYinqi/DIVE .

</details>


### [73] [PPS-Ctrl: Controllable Sim-to-Real Translation for Colonoscopy Depth Estimation](https://arxiv.org/abs/2504.17067)
*Xinqi Xiong, Andrea Dunn Beltran, Jun Myeong Choi, Marc Niethammer, Roni Sengupta*

Main category: cs.CV

TL;DR: A novel image-to-image translation framework using Stable Diffusion and ControlNet, conditioned on Per-Pixel Shading (PPS), improves depth estimation in endoscopy by generating realistic textures while preserving structure.


<details>
  <summary>Details</summary>
Motivation: Accurate depth estimation is crucial for endoscopy but challenging due to the difficulty of obtaining ground-truth depth in clinical settings and the domain gap between synthetic and real data.

Method: The framework integrates Stable Diffusion with ControlNet, using a latent representation from PPS maps to preserve structure and generate realistic textures.

Result: The approach outperforms GAN-based MI-CycleGAN, producing more realistic translations and better depth estimation.

Conclusion: The proposed method effectively bridges the domain gap, enhancing depth estimation for endoscopy navigation and diagnostics.

Abstract: Accurate depth estimation enhances endoscopy navigation and diagnostics, but
obtaining ground-truth depth in clinical settings is challenging. Synthetic
datasets are often used for training, yet the domain gap limits generalization
to real data. We propose a novel image-to-image translation framework that
preserves structure while generating realistic textures from clinical data. Our
key innovation integrates Stable Diffusion with ControlNet, conditioned on a
latent representation extracted from a Per-Pixel Shading (PPS) map. PPS
captures surface lighting effects, providing a stronger structural constraint
than depth maps. Experiments show our approach produces more realistic
translations and improves depth estimation over GAN-based MI-CycleGAN. Our code
is publicly accessible at https://github.com/anaxqx/PPS-Ctrl.

</details>


### [74] [Distilling semantically aware orders for autoregressive image generation](https://arxiv.org/abs/2504.17069)
*Rishav Pramanik, Antoine Poupon, Juan A. Rodriguez, Masih Aminbeidokhti, David Vazquez, Christopher Pal, Zhaozheng Yin, Marco Pedersoli*

Main category: cs.CV

TL;DR: The paper proposes a new method for autoregressive image generation by training models to generate patches in any order, improving image quality over traditional raster-scan approaches.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models for image generation lack a natural order, and the traditional raster-scan order is suboptimal as it ignores content causality (e.g., clouds depending on the sun's color).

Method: Train a model to generate patches in any order, infer content and location during generation, and finetune the model using extracted orders for better image quality.

Result: The new method outperforms raster-scan approaches in image quality on two datasets, with similar training costs and no extra annotations.

Conclusion: The proposed any-order generation method is more effective for autoregressive image generation, respecting content causality and improving results.

Abstract: Autoregressive patch-based image generation has recently shown competitive
results in terms of image quality and scalability. It can also be easily
integrated and scaled within Vision-Language models. Nevertheless,
autoregressive models require a defined order for patch generation. While a
natural order based on the dictation of the words makes sense for text
generation, there is no inherent generation order that exists for image
generation. Traditionally, a raster-scan order (from top-left to bottom-right)
guides autoregressive image generation models. In this paper, we argue that
this order is suboptimal, as it fails to respect the causality of the image
content: for instance, when conditioned on a visual description of a sunset, an
autoregressive model may generate clouds before the sun, even though the color
of clouds should depend on the color of the sun and not the inverse. In this
work, we show that first by training a model to generate patches in
any-given-order, we can infer both the content and the location (order) of each
patch during generation. Secondly, we use these extracted orders to finetune
the any-given-order model to produce better-quality images. Through our
experiments, we show on two datasets that this new generation method produces
better images than the traditional raster-scan approach, with similar training
costs and no extra annotations.

</details>


### [75] [Scene-Aware Location Modeling for Data Augmentation in Automotive Object Detection](https://arxiv.org/abs/2504.17076)
*Jens Petersen, Davide Abati, Amirhossein Habibian, Auke Wiggers*

Main category: cs.CV

TL;DR: The paper introduces a scene-aware probabilistic location model for realistic layout augmentation in generative data augmentation for automotive object detection, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing generative data augmentation methods for automotive object detection either focus on realism or diversity but neglect realistic object placement (layouts).

Method: A scene-aware probabilistic location model predicts realistic object placements, combined with generative inpainting for augmentation.

Result: Achieves up to 2.8× higher gains than competitors (+1.4 vs. +0.5 mAP boost) and improves instance segmentation.

Conclusion: Realistic layout augmentation is crucial for optimal data augmentation, demonstrated by superior performance in automotive tasks.

Abstract: Generative image models are increasingly being used for training data
augmentation in vision tasks. In the context of automotive object detection,
methods usually focus on producing augmented frames that look as realistic as
possible, for example by replacing real objects with generated ones. Others try
to maximize the diversity of augmented frames, for example by pasting lots of
generated objects onto existing backgrounds. Both perspectives pay little
attention to the locations of objects in the scene. Frame layouts are either
reused with little or no modification, or they are random and disregard realism
entirely. In this work, we argue that optimal data augmentation should also
include realistic augmentation of layouts. We introduce a scene-aware
probabilistic location model that predicts where new objects can realistically
be placed in an existing scene. By then inpainting objects in these locations
with a generative model, we obtain much stronger augmentation performance than
existing approaches. We set a new state of the art for generative data
augmentation on two automotive object detection tasks, achieving up to
$2.8\times$ higher gains than the best competing approach ($+1.4$ vs. $+0.5$
mAP boost). We also demonstrate significant improvements for instance
segmentation.

</details>


### [76] [A Comprehensive Survey of Knowledge-Based Vision Question Answering Systems: The Lifecycle of Knowledge in Visual Reasoning Task](https://arxiv.org/abs/2504.17547)
*Jiaqi Deng, Zonghan Wu, Huan Huo, Guandong Xu*

Main category: cs.CV

TL;DR: This paper surveys KB-VQA, highlighting its challenges, the role of LLMs, and proposing a taxonomy for KB-VQA methods.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a comprehensive survey on KB-VQA, which integrates visual, textual, and knowledge inputs for advanced applications.

Method: The survey categorizes KB-VQA systems into stages: knowledge representation, retrieval, and reasoning, and reviews existing methods.

Result: A structured taxonomy of KB-VQA approaches is established, and future research directions are outlined.

Conclusion: The survey provides a foundation for advancing KB-VQA models by organizing existing methods and identifying challenges.

Abstract: Knowledge-based Vision Question Answering (KB-VQA) extends general Vision
Question Answering (VQA) by not only requiring the understanding of visual and
textual inputs but also extensive range of knowledge, enabling significant
advancements across various real-world applications. KB-VQA introduces unique
challenges, including the alignment of heterogeneous information from diverse
modalities and sources, the retrieval of relevant knowledge from noisy or
large-scale repositories, and the execution of complex reasoning to infer
answers from the combined context. With the advancement of Large Language
Models (LLMs), KB-VQA systems have also undergone a notable transformation,
where LLMs serve as powerful knowledge repositories, retrieval-augmented
generators and strong reasoners. Despite substantial progress, no comprehensive
survey currently exists that systematically organizes and reviews the existing
KB-VQA methods. This survey aims to fill this gap by establishing a structured
taxonomy of KB-VQA approaches, and categorizing the systems into main stages:
knowledge representation, knowledge retrieval, and knowledge reasoning. By
exploring various knowledge integration techniques and identifying persistent
challenges, this work also outlines promising future research directions,
providing a foundation for advancing KB-VQA models and their applications.

</details>


### [77] [Transferring Spatial Filters via Tangent Space Alignment in Motor Imagery BCIs](https://arxiv.org/abs/2504.17111)
*Tekin Gunasar, Virginia de Sa*

Main category: cs.CV

TL;DR: Proposes a Riemannian manifold-based method for subject transfer in motor imagery BCIs, improving CSP performance, especially with limited training data.


<details>
  <summary>Details</summary>
Motivation: To enhance subject transfer in motor imagery BCIs by addressing limitations of standard CSP, particularly with scarce training data.

Method: Aligns covariance matrices on a Riemannian manifold and computes a new CSP-based spatial filter, integrating multi-subject information.

Result: Marginal improvements over standard CSP in three datasets, with significant gains when training data is limited.

Conclusion: The method effectively improves subject transfer, especially in data-scarce scenarios, outperforming standard CSP.

Abstract: We propose a method to improve subject transfer in motor imagery BCIs by
aligning covariance matrices on a Riemannian manifold, followed by computing a
new common spatial patterns (CSP) based spatial filter. We explore various ways
to integrate information from multiple subjects and show improved performance
compared to standard CSP. Across three datasets, our method shows marginal
improvements over standard CSP; however, when training data are limited, the
improvements become more significant.

</details>


### [78] [Latent Video Dataset Distillation](https://arxiv.org/abs/2504.17132)
*Ning Li, Antai Andy Liu, Jingran Zhang, Justin Cui*

Main category: cs.CV

TL;DR: A novel video dataset distillation method in latent space with a diversity-aware selection strategy and training-free compression, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing video dataset distillation methods focus on pixel space, ignoring latent space advancements.

Method: Uses a variational encoder for latent space distillation, diversity-aware sample selection, and training-free compression.

Result: Outperforms prior methods, e.g., 2.6% increase on HMDB51 IPC 1, 7.8% on MiniUCF IPC 5.

Conclusion: The approach sets a new benchmark in video dataset distillation by leveraging latent space and diversity-aware techniques.

Abstract: Dataset distillation has demonstrated remarkable effectiveness in
high-compression scenarios for image datasets. While video datasets inherently
contain greater redundancy, existing video dataset distillation methods
primarily focus on compression in the pixel space, overlooking advances in the
latent space that have been widely adopted in modern text-to-image and
text-to-video models. In this work, we bridge this gap by introducing a novel
video dataset distillation approach that operates in the latent space using a
state-of-the-art variational encoder. Furthermore, we employ a diversity-aware
data selection strategy to select both representative and diverse samples.
Additionally, we introduce a simple, training-free method to further compress
the distilled latent dataset. By combining these techniques, our approach
achieves a new state-of-the-art performance in dataset distillation,
outperforming prior methods on all datasets, e.g. on HMDB51 IPC 1, we achieve a
2.6% performance increase; on MiniUCF IPC 5, we achieve a 7.8% performance
increase.

</details>


### [79] [FMNV: A Dataset of Media-Published News Videos for Fake News Detection](https://arxiv.org/abs/2504.07687)
*Yihao Wang, Zhong Qian, Peifeng Li*

Main category: cs.CV

TL;DR: The paper introduces FMNV, a dataset of professionally crafted fake news videos, and FMNVD, a detection model using LLMs and multimodal features, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in datasets for high-impact fake news videos from media outlets, which pose greater societal harm than user-generated content.

Method: Constructed FMNV dataset, categorized fake news types, used LLMs to generate deceptive content, and developed FMNVD model with dual-stream architecture (CLIP and Faster R-CNN) and co-attention mechanisms.

Result: FMNV generalizes well across baselines; FMNVD shows superior detection efficacy.

Conclusion: Establishes benchmarks for detecting high-impact fake news and advances cross-modal inconsistency analysis.

Abstract: News media, particularly video-based platforms, have become deeply embedded
in daily life, concurrently amplifying risks of misinformation dissemination.
Consequently, multimodal fake news detection has garnered significant research
attention. However, existing datasets predominantly comprise user-generated
videos characterized by crude editing and limited public engagement, whereas
professionally crafted fake news videos disseminated by media outlets, often
politically or virally motivated-pose substantially greater societal harm. To
address this gap, we construct FMNV, a novel dataset exclusively composed of
news videos published by media organizations. Through empirical analysis of
existing datasets and our curated collection, we categorize fake news videos
into four distinct types. Building upon this taxonomy, we employ Large Language
Models (LLMs) to automatically generate deceptive content by manipulating
authentic media-published news videos. Furthermore, we propose FMNVD, a
baseline model featuring a dual-stream architecture integrating CLIP and Faster
R-CNN for video feature extraction, enhanced by co-attention mechanisms for
feature refinement and multimodal aggregation. Comparative experiments
demonstrate both the generalization capability of FMNV across multiple
baselines and the superior detection efficacy of FMNVD. This work establishes
critical benchmarks for detecting high-impact fake news in media ecosystems
while advancing methodologies for cross-modal inconsistency analysis.

</details>


### [80] [A Comprehensive Review on RNA Subcellular Localization Prediction](https://arxiv.org/abs/2504.17162)
*Cece Zhang, Xuehuan Zhu, Nick Peterson, Jieqiong Wang, Shibiao Wan*

Main category: cs.CV

TL;DR: The paper reviews AI/ML advancements for predicting RNA subcellular localization, addressing challenges like data scarcity and benchmarking.


<details>
  <summary>Details</summary>
Motivation: Traditional wet lab methods for RNA localization are inefficient; AI/ML offers scalable, cost-effective alternatives.

Method: Comprehensive review of AI-based approaches, including sequence-based, image-based, and hybrid methods.

Result: AI/ML methods show promise for large-scale RNA localization prediction, aiding research and disease treatment.

Conclusion: The review serves as a resource for developing innovative solutions in RNA subcellular localization.

Abstract: The subcellular localization of RNAs, including long non-coding RNAs
(lncRNAs), messenger RNAs (mRNAs), microRNAs (miRNAs) and other smaller RNAs,
plays a critical role in determining their biological functions. For instance,
lncRNAs are predominantly associated with chromatin and act as regulators of
gene transcription and chromatin structure, while mRNAs are distributed across
the nucleus and cytoplasm, facilitating the transport of genetic information
for protein synthesis. Understanding RNA localization sheds light on processes
like gene expression regulation with spatial and temporal precision. However,
traditional wet lab methods for determining RNA localization, such as in situ
hybridization, are often time-consuming, resource-demanding, and costly. To
overcome these challenges, computational methods leveraging artificial
intelligence (AI) and machine learning (ML) have emerged as powerful
alternatives, enabling large-scale prediction of RNA subcellular localization.
This paper provides a comprehensive review of the latest advancements in
AI-based approaches for RNA subcellular localization prediction, covering
various RNA types and focusing on sequence-based, image-based, and hybrid
methodologies that combine both data types. We highlight the potential of these
methods to accelerate RNA research, uncover molecular pathways, and guide
targeted disease treatments. Furthermore, we critically discuss the challenges
in AI/ML approaches for RNA subcellular localization, such as data scarcity and
lack of benchmarks, and opportunities to address them. This review aims to
serve as a valuable resource for researchers seeking to develop innovative
solutions in the field of RNA subcellular localization and beyond.

</details>


### [81] [PhysioSync: Temporal and Cross-Modal Contrastive Learning Inspired by Physiological Synchronization for EEG-Based Emotion Recognition](https://arxiv.org/abs/2504.17163)
*Kai Cui, Jia Li, Yu Liu, Xuesong Zhang, Zhenzhen Hu, Meng Wang*

Main category: cs.CV

TL;DR: PhysioSync is a pre-training framework using contrastive learning to improve EEG-based emotion recognition by modeling cross-modal and temporal dynamics.


<details>
  <summary>Details</summary>
Motivation: EEG signals are noisy and vary across individuals, while existing multimodal approaches overlook dynamic synchronization and temporal dynamics in Peripheral Physiological Signals (PPS).

Method: Proposes PhysioSync with Cross-Modal Consistency Alignment (CM-CA) and Long- and Short-Term Temporal Contrastive Learning (LS-TCL) to capture synchronization across modalities and time resolutions.

Result: Outperforms on DEAP and DREAMER datasets, showing effectiveness in EEG-centered emotion recognition.

Conclusion: PhysioSync enhances emotion recognition by addressing cross-modal and temporal synchronization challenges.

Abstract: Electroencephalography (EEG) signals provide a promising and involuntary
reflection of brain activity related to emotional states, offering significant
advantages over behavioral cues like facial expressions. However, EEG signals
are often noisy, affected by artifacts, and vary across individuals,
complicating emotion recognition. While multimodal approaches have used
Peripheral Physiological Signals (PPS) like GSR to complement EEG, they often
overlook the dynamic synchronization and consistent semantics between the
modalities. Additionally, the temporal dynamics of emotional fluctuations
across different time resolutions in PPS remain underexplored. To address these
challenges, we propose PhysioSync, a novel pre-training framework leveraging
temporal and cross-modal contrastive learning, inspired by physiological
synchronization phenomena. PhysioSync incorporates Cross-Modal Consistency
Alignment (CM-CA) to model dynamic relationships between EEG and complementary
PPS, enabling emotion-related synchronizations across modalities. Besides, it
introduces Long- and Short-Term Temporal Contrastive Learning (LS-TCL) to
capture emotional synchronization at different temporal resolutions within
modalities. After pre-training, cross-resolution and cross-modal features are
hierarchically fused and fine-tuned to enhance emotion recognition. Experiments
on DEAP and DREAMER datasets demonstrate PhysioSync's advanced performance
under uni-modal and cross-modal conditions, highlighting its effectiveness for
EEG-centered emotion recognition.

</details>


### [82] [A Genealogy of Multi-Sensor Foundation Models in Remote Sensing](https://arxiv.org/abs/2504.17177)
*Kevin Lane, Morteza Karimzadeh*

Main category: cs.CV

TL;DR: The paper reviews foundation models in remote sensing, comparing approaches, highlighting benefits and drawbacks, and suggesting future improvements for domain-specific models.


<details>
  <summary>Details</summary>
Motivation: To address the growing interest in foundation models for remote sensing and evaluate their adaptation from computer vision, identifying advantages, challenges, and opportunities for improvement.

Method: Examines existing approaches, their computer vision roots, representation quality, resource efficiency, and multi-sensor utilization.

Result: Identifies gaps in leveraging multi-sensor data and suggests ways to improve remote sensing-specific foundation models.

Conclusion: Future work should focus on better utilizing unlabeled, seasonal, and multi-sensor data to enhance foundation models in remote sensing.

Abstract: Foundation models have garnered increasing attention for representation
learning in remote sensing, primarily adopting approaches that have
demonstrated success in computer vision with minimal domain-specific
modification. However, the development and application of foundation models in
this field are still burgeoning, as there are a variety of competing approaches
that each come with significant benefits and drawbacks. This paper examines
these approaches along with their roots in the computer vision field in order
to characterize potential advantages and pitfalls while outlining future
directions to further improve remote sensing-specific foundation models. We
discuss the quality of the learned representations and methods to alleviate the
need for massive compute resources. We place emphasis on the multi-sensor
aspect of Earth observations, and the extent to which existing approaches
leverage multiple sensors in training foundation models in relation to
multi-modal foundation models. Finally, we identify opportunities for further
harnessing the vast amounts of unlabeled, seasonal, and multi-sensor remote
sensing observations.

</details>


### [83] [We'll Fix it in Post: Improving Text-to-Video Generation with Neuro-Symbolic Feedback](https://arxiv.org/abs/2504.17180)
*Minkyu Choi, S P Sharan, Harsh Goel, Sahil Shah, Sandeep Chinchali*

Main category: cs.CV

TL;DR: A zero-training video refinement pipeline, Projectname, uses neuro-symbolic feedback to improve text-to-video generation, enhancing alignment with prompts by 40%.


<details>
  <summary>Details</summary>
Motivation: Existing T2V models struggle with semantic and temporal consistency for complex prompts and face high computational costs for training.

Method: Projectname analyzes formal video representations to identify inconsistencies, then uses neuro-symbolic feedback to guide targeted edits.

Result: Empirical evaluations show a 40% improvement in temporal and logical alignment across diverse prompts.

Conclusion: Projectname effectively enhances T2V generation without additional training, addressing key limitations of current models.

Abstract: Current text-to-video (T2V) generation models are increasingly popular due to
their ability to produce coherent videos from textual prompts. However, these
models often struggle to generate semantically and temporally consistent videos
when dealing with longer, more complex prompts involving multiple objects or
sequential events. Additionally, the high computational costs associated with
training or fine-tuning make direct improvements impractical. To overcome these
limitations, we introduce \(\projectname\), a novel zero-training video
refinement pipeline that leverages neuro-symbolic feedback to automatically
enhance video generation, achieving superior alignment with the prompts. Our
approach first derives the neuro-symbolic feedback by analyzing a formal video
representation and pinpoints semantically inconsistent events, objects, and
their corresponding frames. This feedback then guides targeted edits to the
original video. Extensive empirical evaluations on both open-source and
proprietary T2V models demonstrate that \(\projectname\) significantly enhances
temporal and logical alignment across diverse prompts by almost $40\%$.

</details>


### [84] [An Explainable Nature-Inspired Framework for Monkeypox Diagnosis: Xception Features Combined with NGBoost and African Vultures Optimization Algorithm](https://arxiv.org/abs/2504.17540)
*Ahmadreza Shateri, Negar Nourani, Morteza Dorrigiv, Hamid Nasiri*

Main category: cs.CV

TL;DR: A deep learning framework for automated monkeypox detection from skin lesions, achieving high accuracy (97.53%) using Xception, PCA, and NGBoost, optimized with AVOA.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for early and accurate monkeypox diagnosis due to its recent global spread, especially in non-endemic regions.

Method: Uses Xception for feature extraction, PCA for dimensionality reduction, NGBoost for classification, and AVOA for hyperparameter tuning. Evaluated on the MSLD dataset.

Result: Achieves 97.53% accuracy, 97.72% F1-score, and 97.47% AUC, with interpretability via Grad-CAM and LIME.

Conclusion: The framework provides a precise, efficient diagnostic tool for early monkeypox detection, beneficial in resource-limited settings.

Abstract: The recent global spread of monkeypox, particularly in regions where it has
not historically been prevalent, has raised significant public health concerns.
Early and accurate diagnosis is critical for effective disease management and
control. In response, this study proposes a novel deep learning-based framework
for the automated detection of monkeypox from skin lesion images, leveraging
the power of transfer learning, dimensionality reduction, and advanced machine
learning techniques. We utilize the newly developed Monkeypox Skin Lesion
Dataset (MSLD), which includes images of monkeypox, chickenpox, and measles, to
train and evaluate our models. The proposed framework employs the Xception
architecture for deep feature extraction, followed by Principal Component
Analysis (PCA) for dimensionality reduction, and the Natural Gradient Boosting
(NGBoost) algorithm for classification. To optimize the model's performance and
generalization, we introduce the African Vultures Optimization Algorithm (AVOA)
for hyperparameter tuning, ensuring efficient exploration of the parameter
space. Our results demonstrate that the proposed AVOA-NGBoost model achieves
state-of-the-art performance, with an accuracy of 97.53%, F1-score of 97.72%
and an AUC of 97.47%. Additionally, we enhance model interpretability using
Grad-CAM and LIME techniques, providing insights into the decision-making
process and highlighting key features influencing classification. This
framework offers a highly precise and efficient diagnostic tool, potentially
aiding healthcare providers in early detection and diagnosis, particularly in
resource-constrained environments.

</details>


### [85] [Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery Simulation](https://arxiv.org/abs/2504.17207)
*Phillip Y. Lee, Jihyeon Je, Chanho Park, Mikaela Angelina Uy, Leonidas Guibas, Minhyuk Sung*

Main category: cs.CV

TL;DR: A framework for perspective-aware reasoning in VLMs using mental imagery simulation, improving human-like perception.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of perspective-aware reasoning in VLMs and their egocentric bias by mimicking human mental imagery.

Method: Proposes Abstract Perspective Change (APC), leveraging vision foundation models for scene abstraction and perspective shifts.

Result: Significant improvements in perspective-aware reasoning, outperforming fine-tuned models and novel-view-synthesis approaches.

Conclusion: APC effectively bridges the gap between VLMs and human perception, enhancing perspective-aware reasoning.

Abstract: We present a framework for perspective-aware reasoning in vision-language
models (VLMs) through mental imagery simulation. Perspective-taking, the
ability to perceive an environment or situation from an alternative viewpoint,
is a key benchmark for human-level visual understanding, essential for
environmental interaction and collaboration with autonomous agents. Despite
advancements in spatial reasoning within VLMs, recent research has shown that
modern VLMs significantly lack perspective-aware reasoning capabilities and
exhibit a strong bias toward egocentric interpretations. To bridge the gap
between VLMs and human perception, we focus on the role of mental imagery,
where humans perceive the world through abstracted representations that
facilitate perspective shifts. Motivated by this, we propose a framework for
perspective-aware reasoning, named Abstract Perspective Change (APC), that
effectively leverages vision foundation models, such as object detection,
segmentation, and orientation estimation, to construct scene abstractions and
enable perspective transformations. Our experiments on synthetic and real-image
benchmarks, compared with various VLMs, demonstrate significant improvements in
perspective-aware reasoning with our framework, further outperforming
fine-tuned spatial reasoning models and novel-view-synthesis-based approaches.

</details>


### [86] [MCAF: Efficient Agent-based Video Understanding Framework through Multimodal Coarse-to-Fine Attention Focusing](https://arxiv.org/abs/2504.17213)
*Shiwen Cao, Zhaoxing Zhang, Junming Jiao, Juyi Qiao, Guowen Song, Rong Shen*

Main category: cs.CV

TL;DR: MCAF is a training-free, agent-based framework for video understanding using multimodal coarse-to-fine attention focusing, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Video understanding, especially for long videos, is challenging due to information redundancy. MCAF aims to strategically allocate attention for accurate comprehension.

Method: MCAF hierarchically focuses on relevant frames using multimodal information and employs a dilated temporal expansion mechanism. It also uses a self-reflection mechanism for iterative improvement.

Result: MCAF achieves a 5% gain on EgoSchema and outperforms state-of-the-art methods on Next-QA, IntentQA, and Video-MME datasets.

Conclusion: MCAF effectively improves video understanding by adaptively adjusting attention and outperforms existing methods.

Abstract: Even in the era of rapid advances in large models, video understanding,
particularly long videos, remains highly challenging. Compared with textual or
image-based information, videos commonly contain more information with
redundancy, requiring large models to strategically allocate attention at a
global level for accurate comprehension. To address this, we propose MCAF, an
agent-based, training-free framework perform video understanding through
Multimodal Coarse-to-fine Attention Focusing. The key innovation lies in its
ability to sense and prioritize segments of the video that are highly relevant
to the understanding task. First, MCAF hierarchically concentrates on highly
relevant frames through multimodal information, enhancing the correlation
between the acquired contextual information and the query. Second, it employs a
dilated temporal expansion mechanism to mitigate the risk of missing crucial
details when extracting information from these concentrated frames. In
addition, our framework incorporates a self-reflection mechanism utilizing the
confidence level of the model's responses as feedback. By iteratively applying
these two creative focusing strategies, it adaptively adjusts attention to
capture highly query-connected context and thus improves response accuracy.
MCAF outperforms comparable state-of-the-art methods on average. On the
EgoSchema dataset, it achieves a remarkable 5% performance gain over the
leading approach. Meanwhile, on Next-QA and IntentQA datasets, it outperforms
the current state-of-the-art standard by 0.2% and 0.3% respectively. On the
Video-MME dataset, which features videos averaging nearly an hour in length,
MCAF also outperforms other agent-based methods.

</details>


### [87] [Towards Generalizable Deepfake Detection with Spatial-Frequency Collaborative Learning and Hierarchical Cross-Modal Fusion](https://arxiv.org/abs/2504.17223)
*Mengyu Qiao, Runze Tian, Yang Wang*

Main category: cs.CV

TL;DR: A novel deepfake detection framework using multi-scale spatial-frequency analysis to improve accuracy and generalizability by exploiting frequency-native artifacts and spatial-frequency interactions.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detectors fail against unseen forgeries due to reliance on spatial domain analysis and underutilization of frequency-native artifacts and spatial-frequency interactions.

Method: Proposes a framework with local and global spectral feature extraction pipelines and a multi-stage cross-modal fusion mechanism to model spatial-frequency interactions.

Result: Outperforms state-of-the-art deepfake detection methods in accuracy and generalizability on widely adopted benchmarks.

Conclusion: The framework effectively addresses limitations of existing methods by leveraging spatial-frequency analysis for universal deepfake detection.

Abstract: The rapid evolution of deep generative models poses a critical challenge to
deepfake detection, as detectors trained on forgery-specific artifacts often
suffer significant performance degradation when encountering unseen forgeries.
While existing methods predominantly rely on spatial domain analysis, frequency
domain operations are primarily limited to feature-level augmentation, leaving
frequency-native artifacts and spatial-frequency interactions insufficiently
exploited. To address this limitation, we propose a novel detection framework
that integrates multi-scale spatial-frequency analysis for universal deepfake
detection. Our framework comprises three key components: (1) a local spectral
feature extraction pipeline that combines block-wise discrete cosine transform
with cascaded multi-scale convolutions to capture subtle spectral artifacts;
(2) a global spectral feature extraction pipeline utilizing scale-invariant
differential accumulation to identify holistic forgery distribution patterns;
and (3) a multi-stage cross-modal fusion mechanism that incorporates
shallow-layer attention enhancement and deep-layer dynamic modulation to model
spatial-frequency interactions. Extensive evaluations on widely adopted
benchmarks demonstrate that our method outperforms state-of-the-art deepfake
detection methods in both accuracy and generalizability.

</details>


### [88] [Visual and textual prompts for enhancing emotion recognition in video](https://arxiv.org/abs/2504.17224)
*Zhifeng Wang, Qixuan Zhang, Peter Zhang, Wenjia Niu, Kaihao Zhang, Ramesh Sankaranarayana, Sabrina Caldwell, Tom Gedeon*

Main category: cs.CV

TL;DR: SoVTP improves video emotion recognition in VLLMs by integrating spatial, physiological, and contextual cues into a unified prompting framework.


<details>
  <summary>Details</summary>
Motivation: Current VLLMs lack spatial and contextual awareness for video emotion recognition, focusing too narrowly on facial features and missing broader non-verbal cues.

Method: Proposes Set-of-Vision-Text Prompting (SoVTP), combining spatial annotations, physiological signals, and contextual cues into a unified prompting strategy.

Result: SoVTP outperforms existing visual prompting methods, enhancing VLLMs' emotion recognition in videos.

Conclusion: SoVTP effectively bridges the gap in VLLMs for robust video emotion recognition by leveraging holistic and fine-grained cues.

Abstract: Vision Large Language Models (VLLMs) exhibit promising potential for
multi-modal understanding, yet their application to video-based emotion
recognition remains limited by insufficient spatial and contextual awareness.
Traditional approaches, which prioritize isolated facial features, often
neglect critical non-verbal cues such as body language, environmental context,
and social interactions, leading to reduced robustness in real-world scenarios.
To address this gap, we propose Set-of-Vision-Text Prompting (SoVTP), a novel
framework that enhances zero-shot emotion recognition by integrating spatial
annotations (e.g., bounding boxes, facial landmarks), physiological signals
(facial action units), and contextual cues (body posture, scene dynamics,
others' emotions) into a unified prompting strategy. SoVTP preserves holistic
scene information while enabling fine-grained analysis of facial muscle
movements and interpersonal dynamics. Extensive experiments show that SoVTP
achieves substantial improvements over existing visual prompting methods,
demonstrating its effectiveness in enhancing VLLMs' video emotion recognition
capabilities.

</details>


### [89] [Range Image-Based Implicit Neural Compression for LiDAR Point Clouds](https://arxiv.org/abs/2504.17229)
*Akihiro Kuwabara, Sorachi Kato, Takuya Fujihashi, Toshiaki Koike-Akino, Takashi Watanabe*

Main category: cs.CV

TL;DR: A novel LiDAR point cloud compression method using implicit neural representation (INR) for 2D range images (RIs) outperforms existing techniques in 3D reconstruction and detection at low bitrates.


<details>
  <summary>Details</summary>
Motivation: To efficiently compress LiDAR point clouds for high-precision 3D scene archives, addressing limitations of conventional image compression techniques for RIs.

Method: Proposes an INR-based RI compression method, splitting RIs into depth and mask images, using patch-wise and pixel-wise INR architectures with pruning and quantization.

Result: Outperforms existing methods in 3D reconstruction and detection quality on the KITTI dataset at low bitrates and decoding latency.

Conclusion: The INR-based approach effectively compresses RIs, enabling efficient 3D scene understanding.

Abstract: This paper presents a novel scheme to efficiently compress Light Detection
and Ranging~(LiDAR) point clouds, enabling high-precision 3D scene archives,
and such archives pave the way for a detailed understanding of the
corresponding 3D scenes. We focus on 2D range images~(RIs) as a lightweight
format for representing 3D LiDAR observations. Although conventional image
compression techniques can be adapted to improve compression efficiency for
RIs, their practical performance is expected to be limited due to differences
in bit precision and the distinct pixel value distribution characteristics
between natural images and RIs. We propose a novel implicit neural
representation~(INR)--based RI compression method that effectively handles
floating-point valued pixels. The proposed method divides RIs into depth and
mask images and compresses them using patch-wise and pixel-wise INR
architectures with model pruning and quantization, respectively. Experiments on
the KITTI dataset show that the proposed method outperforms existing image,
point cloud, RI, and INR-based compression methods in terms of 3D
reconstruction and detection quality at low bitrates and decoding latency.

</details>


### [90] [RSEND: Retinex-based Squeeze and Excitation Network with Dark Region Detection for Efficient Low Light Image Enhancement](https://arxiv.org/abs/2406.09656)
*Jingcheng Li, Ye Qiao, Haocheng Xu, Sitao Huang*

Main category: cs.CV

TL;DR: RSEND is a one-stage Retinex-based framework for low-light image enhancement, outperforming CNN and transformer models in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Low-light images suffer from poor quality, and existing CNN-based methods are inefficient and perform poorly in complex datasets like LOL-v2.

Method: RSEND divides images into illumination and reflectance maps, enhances details, refines the image, and denoises the result using Squeeze and Excitation networks.

Result: RSEND achieves PSNR improvements of 0.44 dB to 4.2 dB, outperforming CNN and transformer models in LOL-v2-real.

Conclusion: RSEND offers a more accurate, concise, and efficient solution for low-light image enhancement.

Abstract: Images captured under low-light scenarios often suffer from low quality.
Previous CNN-based deep learning methods often involve using Retinex theory.
Nevertheless, most of them cannot perform well in more complicated datasets
like LOL-v2 while consuming too much computational resources. Besides, some of
these methods require sophisticated training at different stages, making the
procedure even more time-consuming and tedious. In this paper, we propose a
more accurate, concise, and one-stage Retinex theory based framework, RSEND.
RSEND first divides the low-light image into the illumination map and
reflectance map, then captures the important details in the illumination map
and performs light enhancement. After this step, it refines the enhanced
gray-scale image and does element-wise matrix multiplication with the
reflectance map. By denoising the output it has from the previous step, it
obtains the final result. In all the steps, RSEND utilizes Squeeze and
Excitation network to better capture the details. Comprehensive quantitative
and qualitative experiments show that our Efficient Retinex model significantly
outperforms other CNN-based models, achieving a PSNR improvement ranging from
0.44 dB to 4.2 dB in different datasets and even outperforms transformer-based
models in the LOL-v2-real dataset.

</details>


### [91] [Scene Perceived Image Perceptual Score (SPIPS): combining global and local perception for image quality assessment](https://arxiv.org/abs/2504.17234)
*Zhiqiang Lao, Heather Yu*

Main category: cs.CV

TL;DR: A novel image quality assessment (IQA) method combining deep learning features with traditional metrics to better align with human perception.


<details>
  <summary>Details</summary>
Motivation: The rise of AI-generated and processed images demands more accurate IQA methods, as traditional techniques fail to address the complexity introduced by deep neural networks.

Method: Disentangles deep features into semantic and perceptual details, combines them with conventional IQA metrics, and uses an MLP to map features into a quality score.

Result: The proposed method outperforms existing IQA models in consistency with human perceptual judgments.

Conclusion: The hybrid approach effectively bridges the gap between deep learning and human perception, offering a more comprehensive IQA framework.

Abstract: The rapid advancement of artificial intelligence and widespread use of
smartphones have resulted in an exponential growth of image data, both real
(camera-captured) and virtual (AI-generated). This surge underscores the
critical need for robust image quality assessment (IQA) methods that accurately
reflect human visual perception. Traditional IQA techniques primarily rely on
spatial features - such as signal-to-noise ratio, local structural distortions,
and texture inconsistencies - to identify artifacts. While effective for
unprocessed or conventionally altered images, these methods fall short in the
context of modern image post-processing powered by deep neural networks (DNNs).
The rise of DNN-based models for image generation, enhancement, and restoration
has significantly improved visual quality, yet made accurate assessment
increasingly complex. To address this, we propose a novel IQA approach that
bridges the gap between deep learning methods and human perception. Our model
disentangles deep features into high-level semantic information and low-level
perceptual details, treating each stream separately. These features are then
combined with conventional IQA metrics to provide a more comprehensive
evaluation framework. This hybrid design enables the model to assess both
global context and intricate image details, better reflecting the human visual
process, which first interprets overall structure before attending to
fine-grained elements. The final stage employs a multilayer perceptron (MLP) to
map the integrated features into a concise quality score. Experimental results
demonstrate that our method achieves improved consistency with human perceptual
judgments compared to existing IQA models.

</details>


### [92] [Precision Neural Network Quantization via Learnable Adaptive Modules](https://arxiv.org/abs/2504.17263)
*Wenqiang Zhou, Zhendong Yu, Xinyu Liu, Jiaming Yang, Rong Xiao, Tao Wang, Chenwei Tang, Jiancheng Lv*

Main category: cs.CV

TL;DR: ASQ improves QAT by dynamically adjusting quantization scaling factors and using a non-uniform POST-based scheme, outperforming state-of-the-art methods and even full precision baselines.


<details>
  <summary>Details</summary>
Motivation: To resolve the conflict between trainable quantization parameters and inference flexibility, especially for varying activation distributions.

Method: Proposes ASQ: dynamic scaling factor adjustment and POST-based non-uniform quantization with LUT for efficiency.

Result: ASQ outperforms existing QAT methods, with a 4-bit ResNet34 achieving 1.2% higher accuracy on ImageNet.

Conclusion: ASQ effectively balances performance and flexibility, advancing neural network quantization.

Abstract: Quantization Aware Training (QAT) is a neural network quantization technique
that compresses model size and improves operational efficiency while
effectively maintaining model performance. The paradigm of QAT is to introduce
fake quantization operators during the training process, allowing the model to
autonomously compensate for information loss caused by quantization. Making
quantization parameters trainable can significantly improve the performance of
QAT, but at the cost of compromising the flexibility during inference,
especially when dealing with activation values with substantially different
distributions. In this paper, we propose an effective learnable adaptive neural
network quantization method, called Adaptive Step Size Quantization (ASQ), to
resolve this conflict. Specifically, the proposed ASQ method first dynamically
adjusts quantization scaling factors through a trained module capable of
accommodating different activations. Then, to address the rigid resolution
issue inherent in Power of Two (POT) quantization, we propose an efficient
non-uniform quantization scheme. We utilize the Power Of Square root of Two
(POST) as the basis for exponential quantization, effectively handling the
bell-shaped distribution of neural network weights across various bit-widths
while maintaining computational efficiency through a Look-Up Table method
(LUT). Extensive experimental results demonstrate that the proposed ASQ method
is superior to the state-of-the-art QAT approaches. Notably that the ASQ is
even competitive compared to full precision baselines, with its 4-bit quantized
ResNet34 model improving accuracy by 1.2\% on ImageNet.

</details>


### [93] [Towards Generalized and Training-Free Text-Guided Semantic Manipulation](https://arxiv.org/abs/2504.17269)
*Yu Hong, Xiao Cai, Pengpeng Zeng, Shuai Zhang, Jingkuan Song, Lianli Gao, Heng Tao Shen*

Main category: cs.CV

TL;DR: A novel training-free method (GTF) for text-guided semantic image manipulation using diffusion models, supporting multiple edits and modalities without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing methods for text-guided semantic manipulation are inefficient, poorly extensible, or lack generalizability. Geometric properties of diffusion model noises are leveraged to address these limitations.

Method: Proposes GTF, a plug-and-play, modality-agnostic framework that controls noise geometry for semantic edits (addition, removal, style transfer) without training.

Result: GTF achieves high-fidelity results across multiple semantic manipulations and modalities, outperforming existing methods.

Conclusion: GTF advances semantic manipulation by being efficient, extensible, and generalizable, setting a new state-of-the-art.

Abstract: Text-guided semantic manipulation refers to semantically editing an image
generated from a source prompt to match a target prompt, enabling the desired
semantic changes (e.g., addition, removal, and style transfer) while preserving
irrelevant contents. With the powerful generative capabilities of the diffusion
model, the task has shown the potential to generate high-fidelity visual
content. Nevertheless, existing methods either typically require time-consuming
fine-tuning (inefficient), fail to accomplish multiple semantic manipulations
(poorly extensible), and/or lack support for different modality tasks (limited
generalizability). Upon further investigation, we find that the geometric
properties of noises in the diffusion model are strongly correlated with the
semantic changes. Motivated by this, we propose a novel $\textit{GTF}$ for
text-guided semantic manipulation, which has the following attractive
capabilities: 1) $\textbf{Generalized}$: our $\textit{GTF}$ supports multiple
semantic manipulations (e.g., addition, removal, and style transfer) and can be
seamlessly integrated into all diffusion-based methods (i.e., Plug-and-play)
across different modalities (i.e., modality-agnostic); and 2)
$\textbf{Training-free}$: $\textit{GTF}$ produces high-fidelity results via
simply controlling the geometric relationship between noises without tuning or
optimization. Our extensive experiments demonstrate the efficacy of our
approach, highlighting its potential to advance the state-of-the-art in
semantics manipulation.

</details>


### [94] [EdgePoint2: Compact Descriptors for Superior Efficiency and Accuracy](https://arxiv.org/abs/2504.17280)
*Haodi Yao, Fenghua He, Ning Hao, Chen Xie*

Main category: cs.CV

TL;DR: EdgePoint2 is a lightweight neural network for keypoint detection and description, optimized for edge computing, balancing efficiency and accuracy with compact descriptors.


<details>
  <summary>Details</summary>
Motivation: Deep learning methods for keypoint extraction are computationally expensive for edge applications, and existing lightweight models trade efficiency for accuracy. High-dimensional descriptors also hinder distributed applications.

Method: EdgePoint2 uses an optimized architecture and introduces a training approach combining Orthogonal Procrustes loss and similarity loss for compact descriptors. It offers 14 sub-models for diverse needs.

Result: EdgePoint2 achieves SOTA accuracy and efficiency with lower-dimensional descriptors (32/48/64), excelling in flexibility, robustness, and versatility.

Conclusion: EdgePoint2 is a competitive solution for edge computing, addressing computational and communication constraints while maintaining high performance.

Abstract: The field of keypoint extraction, which is essential for vision applications
like Structure from Motion (SfM) and Simultaneous Localization and Mapping
(SLAM), has evolved from relying on handcrafted methods to leveraging deep
learning techniques. While deep learning approaches have significantly improved
performance, they often incur substantial computational costs, limiting their
deployment in real-time edge applications. Efforts to create lightweight neural
networks have seen some success, yet they often result in trade-offs between
efficiency and accuracy. Additionally, the high-dimensional descriptors
generated by these networks poses challenges for distributed applications
requiring efficient communication and coordination, highlighting the need for
compact yet competitively accurate descriptors. In this paper, we present
EdgePoint2, a series of lightweight keypoint detection and description neural
networks specifically tailored for edge computing applications on embedded
system. The network architecture is optimized for efficiency without
sacrificing accuracy. To train compact descriptors, we introduce a combination
of Orthogonal Procrustes loss and similarity loss, which can serve as a general
approach for hypersphere embedding distillation tasks. Additionally, we offer
14 sub-models to satisfy diverse application requirements. Our experiments
demonstrate that EdgePoint2 consistently achieves state-of-the-art (SOTA)
accuracy and efficiency across various challenging scenarios while employing
lower-dimensional descriptors (32/48/64). Beyond its accuracy, EdgePoint2
offers significant advantages in flexibility, robustness, and versatility.
Consequently, EdgePoint2 emerges as a highly competitive option for visual
tasks, especially in contexts demanding adaptability to diverse computational
and communication constraints.

</details>


### [95] [Advanced Segmentation of Diabetic Retinopathy Lesions Using DeepLabv3+](https://arxiv.org/abs/2504.17306)
*Meher Boulaabi, Takwa Ben Aïcha Gader, Afef Kacem Echi, Sameh Mbarek*

Main category: cs.CV

TL;DR: A binary segmentation method for diabetic retinopathy lesions improved accuracy by combining individual model outputs and using DeepLabv3+, achieving 99% segmentation accuracy.


<details>
  <summary>Details</summary>
Motivation: To enhance segmentation of diabetic retinopathy lesions (microaneurysms, hemorrhages, exudates, soft exudates) by addressing dataset limitations and annotation complexity.

Method: Implemented binary segmentation for each lesion type, combined outputs, used preprocessing (cropping, CLAHE on LAB image), and employed targeted data augmentation with DeepLabv3+.

Result: Achieved 99% segmentation accuracy, validated on the IDRID dataset.

Conclusion: Innovative strategies, including model combination and preprocessing, effectively advance medical image analysis for diabetic retinopathy.

Abstract: To improve the segmentation of diabetic retinopathy lesions (microaneurysms,
hemorrhages, exudates, and soft exudates), we implemented a binary segmentation
method specific to each type of lesion. As post-segmentation, we combined the
individual model outputs into a single image to better analyze the lesion
types. This approach facilitated parameter optimization and improved accuracy,
effectively overcoming challenges related to dataset limitations and annotation
complexity. Specific preprocessing steps included cropping and applying
contrast-limited adaptive histogram equalization to the L channel of the LAB
image. Additionally, we employed targeted data augmentation techniques to
further refine the model's efficacy. Our methodology utilized the DeepLabv3+
model, achieving a segmentation accuracy of 99%. These findings highlight the
efficacy of innovative strategies in advancing medical image analysis,
particularly in the precise segmentation of diabetic retinopathy lesions. The
IDRID dataset was utilized to validate and demonstrate the robustness of our
approach.

</details>


### [96] [DIMT25@ICDAR2025: HW-TSC's End-to-End Document Image Machine Translation System Leveraging Large Vision-Language Model](https://arxiv.org/abs/2504.17315)
*Zhanglin Wu, Tengfei Song, Ning Xie, Weidong Zhang, Pengfei Li, Shuang Wu, Chong Li, Junhao Zhu, Hao Yang*

Main category: cs.CV

TL;DR: Huawei's solution for document image translation combines multi-task learning and perceptual chain-of-thought using LVLM, achieving strong results in OCR-based and OCR-free tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of translating documents with complex layouts in an end-to-end manner.

Method: Uses a training framework with multi-task learning and perceptual chain-of-thought, plus minimum Bayesian decoding and post-processing for inference.

Result: Demonstrates effectiveness in handling both OCR-based and OCR-free document translation tasks.

Conclusion: The proposed unified framework is a robust solution for document image machine translation.

Abstract: This paper presents the technical solution proposed by Huawei Translation
Service Center (HW-TSC) for the "End-to-End Document Image Machine Translation
for Complex Layouts" competition at the 19th International Conference on
Document Analysis and Recognition (DIMT25@ICDAR2025). Leveraging
state-of-the-art open-source large vision-language model (LVLM), we introduce a
training framework that combines multi-task learning with perceptual
chain-of-thought to develop a comprehensive end-to-end document translation
system. During the inference phase, we apply minimum Bayesian decoding and
post-processing strategies to further enhance the system's translation
capabilities. Our solution uniquely addresses both OCR-based and OCR-free
document image translation tasks within a unified framework. This paper
systematically details the training methods, inference strategies, LVLM base
models, training data, experimental setups, and results, demonstrating an
effective approach to document image machine translation.

</details>


### [97] [TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos](https://arxiv.org/abs/2504.17343)
*Linli Yao, Yicheng Li, Yuancheng Wei, Lei Li, Shuhuai Ren, Yuanxin Liu, Kun Ouyang, Lean Wang, Shicheng Li, Sida Li, Lingpeng Kong, Qi Liu, Yuanxing Zhang, Xu Sun*

Main category: cs.CV

TL;DR: TimeChat-Online is a novel VideoLLM for real-time video interaction, featuring the Differential Token Drop (DTD) module to reduce redundancy and improve efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of existing VideoLLMs in handling streaming videos by reducing visual redundancy and enabling real-time interaction.

Method: Introduces the DTD module inspired by human visual perception to filter redundant frames while preserving meaningful changes. Also presents TimeChat-Online-139K dataset for diverse interaction patterns.

Result: DTD reduces video tokens by 82.8% while maintaining 98% performance. TimeChat-Online excels in streaming benchmarks and long-form video tasks.

Conclusion: TimeChat-Online revolutionizes real-time video interaction with its efficiency and performance, setting a new standard for VideoLLMs in streaming scenarios.

Abstract: The rapid growth of online video platforms, particularly live streaming
services, has created an urgent need for real-time video understanding systems.
These systems must process continuous video streams and respond to user queries
instantaneously, presenting unique challenges for current Video Large Language
Models (VideoLLMs). While existing VideoLLMs excel at processing complete
videos, they face significant limitations in streaming scenarios due to their
inability to handle dense, redundant frames efficiently. We introduce
TimeChat-Online, a novel online VideoLLM that revolutionizes real-time video
interaction. At its core lies our innovative Differential Token Drop (DTD)
module, which addresses the fundamental challenge of visual redundancy in
streaming videos. Drawing inspiration from human visual perception's Change
Blindness phenomenon, DTD preserves meaningful temporal changes while filtering
out static, redundant content between frames. Remarkably, our experiments
demonstrate that DTD achieves an 82.8% reduction in video tokens while
maintaining 98% performance on StreamingBench, revealing that over 80% of
visual content in streaming videos is naturally redundant without requiring
language guidance. To enable seamless real-time interaction, we present
TimeChat-Online-139K, a comprehensive streaming video dataset featuring diverse
interaction patterns including backward-tracing, current-perception, and
future-responding scenarios. TimeChat-Online's unique Proactive Response
capability, naturally achieved through continuous monitoring of video scene
transitions via DTD, sets it apart from conventional approaches. Our extensive
evaluation demonstrates TimeChat-Online's superior performance on streaming
benchmarks (StreamingBench and OvOBench) and maintaining competitive results on
long-form video tasks such as Video-MME and MLVU.

</details>


### [98] [DRC: Enhancing Personalized Image Generation via Disentangled Representation Composition](https://arxiv.org/abs/2504.17349)
*Yiyan Xu, Wuqiang Zheng, Wenjie Wang, Fengbin Zhu, Xinting Hu, Yang Zhang, Fuli Feng, Tat-Seng Chua*

Main category: cs.CV

TL;DR: DRC introduces a framework for personalized image generation by disentangling style and semantic features to avoid guidance collapse in LMMs.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to accurately capture and fuse user style preferences and semantic intentions, leading to guidance collapse.

Method: DRC uses disentangled representation composition with dual-tower disentangler and semantic-preserving augmentations.

Result: DRC outperforms benchmarks, mitigating guidance collapse and improving personalized generation.

Conclusion: Disentangled representation learning is crucial for effective and controllable personalized image generation.

Abstract: Personalized image generation has emerged as a promising direction in
multimodal content creation. It aims to synthesize images tailored to
individual style preferences (e.g., color schemes, character appearances,
layout) and semantic intentions (e.g., emotion, action, scene contexts) by
leveraging user-interacted history images and multimodal instructions. Despite
notable progress, existing methods -- whether based on diffusion models, large
language models, or Large Multimodal Models (LMMs) -- struggle to accurately
capture and fuse user style preferences and semantic intentions. In particular,
the state-of-the-art LMM-based method suffers from the entanglement of visual
features, leading to Guidance Collapse, where the generated images fail to
preserve user-preferred styles or reflect the specified semantics.
  To address these limitations, we introduce DRC, a novel personalized image
generation framework that enhances LMMs through Disentangled Representation
Composition. DRC explicitly extracts user style preferences and semantic
intentions from history images and the reference image, respectively, to form
user-specific latent instructions that guide image generation within LMMs.
Specifically, it involves two critical learning stages: 1) Disentanglement
learning, which employs a dual-tower disentangler to explicitly separate style
and semantic features, optimized via a reconstruction-driven paradigm with
difficulty-aware importance sampling; and 2) Personalized modeling, which
applies semantic-preserving augmentations to effectively adapt the disentangled
representations for robust personalized generation. Extensive experiments on
two benchmarks demonstrate that DRC shows competitive performance while
effectively mitigating the guidance collapse issue, underscoring the importance
of disentangled representation learning for controllable and effective
personalized image generation.

</details>


### [99] [I-INR: Iterative Implicit Neural Representations](https://arxiv.org/abs/2504.17364)
*Ali Haider, Muhammad Salman Ali, Maryam Qamar, Tahir Khalil, Soo Ye Kim, Jihyong Oh, Enzo Tartaglione, Sung-Ho Bae*

Main category: cs.CV

TL;DR: The paper introduces Iterative Implicit Neural Representations (I-INRs) to improve signal reconstruction by addressing regression-to-the-mean issues in INRs, enhancing detail retention and noise robustness.


<details>
  <summary>Details</summary>
Motivation: INRs struggle with fine details, high-frequency information, and noise due to their regression-based formulation.

Method: Proposes I-INRs, a plug-and-play iterative refinement framework compatible with existing INR architectures.

Result: I-INRs outperform baselines (WIRE, SIREN, Gauss) in tasks like image restoration, denoising, and occupancy prediction.

Conclusion: I-INRs offer superior reconstruction quality and robustness, making them a promising advancement for INR-based applications.

Abstract: Implicit Neural Representations (INRs) have revolutionized signal processing
and computer vision by modeling signals as continuous, differentiable functions
parameterized by neural networks. However, their inherent formulation as a
regression problem makes them prone to regression to the mean, limiting their
ability to capture fine details, retain high-frequency information, and handle
noise effectively. To address these challenges, we propose Iterative Implicit
Neural Representations (I-INRs) a novel plug-and-play framework that enhances
signal reconstruction through an iterative refinement process. I-INRs
effectively recover high-frequency details, improve robustness to noise, and
achieve superior reconstruction quality. Our framework seamlessly integrates
with existing INR architectures, delivering substantial performance gains
across various tasks. Extensive experiments show that I-INRs outperform
baseline methods, including WIRE, SIREN, and Gauss, in diverse computer vision
applications such as image restoration, image denoising, and object occupancy
prediction.

</details>


### [100] [TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation](https://arxiv.org/abs/2504.17365)
*Ling You, Wenxuan Huang, Xinni Xie, Xiangyi Wei, Bangyan Li, Shaohui Lin, Yang Li, Changbo Wang*

Main category: cs.CV

TL;DR: TimeSoccer is an end-to-end MLLM for soccer video captioning, combining timestamp prediction and caption generation in one pass, with a motion-aware frame compression module for long videos.


<details>
  <summary>Details</summary>
Motivation: Existing soccer MLLMs rely on temporal priors or complex two-step methods, lacking end-to-end processing and global context for full-match videos.

Method: TimeSoccer uses MoFA-Select for adaptive frame compression and joint timestamp-caption prediction, enabling global context modeling in 45-minute matches.

Result: TimeSoccer achieves SoTA performance in SDVC, producing high-quality commentary with precise temporal alignment and semantic relevance.

Conclusion: TimeSoccer addresses limitations of prior methods, offering an efficient, end-to-end solution for soccer video captioning.

Abstract: Soccer is a globally popular sporting event, typically characterized by long
matches and distinctive highlight moments. Recent advances in Multimodal Large
Language Models (MLLMs) offer promising capabilities in temporal grounding and
video understanding, soccer commentary generation often requires precise
temporal localization and semantically rich descriptions over long-form video.
However, existing soccer MLLMs often rely on the temporal a priori for caption
generation, so they cannot process the soccer video end-to-end. While some
traditional approaches follow a two-step paradigm that is complex and fails to
capture the global context to achieve suboptimal performance. To solve the
above issues, we present TimeSoccer, the first end-to-end soccer MLLM for
Single-anchor Dense Video Captioning (SDVC) in full-match soccer videos.
TimeSoccer jointly predicts timestamps and generates captions in a single pass,
enabling global context modeling across 45-minute matches. To support long
video understanding of soccer matches, we introduce MoFA-Select, a
training-free, motion-aware frame compression module that adaptively selects
representative frames via a coarse-to-fine strategy, and incorporates
complementary training paradigms to strengthen the model's ability to handle
long temporal sequences. Extensive experiments demonstrate that our TimeSoccer
achieves State-of-The-Art (SoTA) performance on the SDVC task in an end-to-end
form, generating high-quality commentary with accurate temporal alignment and
strong semantic relevance.

</details>


### [101] [Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D Dataset](https://arxiv.org/abs/2504.17371)
*Oussema Dhaouadi, Johannes Meier, Luca Wahl, Jacques Kaiser, Luca Scalerandi, Nick Wandelburg, Zhuolun Zhou, Nijanthan Berinpanathan, Holger Banzhaf, Daniel Cremers*

Main category: cs.CV

TL;DR: DSC3D is a high-quality, occlusion-free 3D trajectory dataset for autonomous driving, captured via drone tracking, offering diverse scenarios and surpassing existing datasets in scale and variety.


<details>
  <summary>Details</summary>
Motivation: Traditional datasets suffer from occlusion and limited coverage, hindering accurate 3D trajectory data for autonomous driving. DSC3D addresses these gaps by providing comprehensive, high-quality data.

Method: A novel monocular camera drone tracking pipeline was used to capture 175,000+ trajectories of 14 traffic participant types across five diverse locations.

Result: DSC3D exceeds existing datasets in diversity and scale, featuring unprecedented scenarios like complex urban interactions and parking maneuvers.

Conclusion: DSC3D enhances autonomous driving systems by enabling detailed 3D environmental representations, improving safety and obstacle interactions, and supporting applications like motion prediction and behavior modeling.

Abstract: Accurate 3D trajectory data is crucial for advancing autonomous driving. Yet,
traditional datasets are usually captured by fixed sensors mounted on a car and
are susceptible to occlusion. Additionally, such an approach can precisely
reconstruct the dynamic environment in the close vicinity of the measurement
vehicle only, while neglecting objects that are further away. In this paper, we
introduce the DeepScenario Open 3D Dataset (DSC3D), a high-quality,
occlusion-free dataset of 6 degrees of freedom bounding box trajectories
acquired through a novel monocular camera drone tracking pipeline. Our dataset
includes more than 175,000 trajectories of 14 types of traffic participants and
significantly exceeds existing datasets in terms of diversity and scale,
containing many unprecedented scenarios such as complex vehicle-pedestrian
interaction on highly populated urban streets and comprehensive parking
maneuvers from entry to exit. DSC3D dataset was captured in five various
locations in Europe and the United States and include: a parking lot, a crowded
inner-city, a steep urban intersection, a federal highway, and a suburban
intersection. Our 3D trajectory dataset aims to enhance autonomous driving
systems by providing detailed environmental 3D representations, which could
lead to improved obstacle interactions and safety. We demonstrate its utility
across multiple applications including motion prediction, motion planning,
scenario mining, and generative reactive traffic agents. Our interactive online
visualization platform and the complete dataset are publicly available at
app.deepscenario.com, facilitating research in motion prediction, behavior
modeling, and safety validation.

</details>


### [102] [SDVPT: Semantic-Driven Visual Prompt Tuning for Open-World Object Counting](https://arxiv.org/abs/2504.17395)
*Yiming Zhao, Guorong Li, Laiyun Qing, Amin Beheshti, Jian Yang, Michael Sheng, Yuankai Qi, Qingming Huang*

Main category: cs.CV

TL;DR: The paper introduces SDVPT, a plug-and-play framework for open-world object counting, enhancing generalizability to unseen categories via semantic-driven visual prompt tuning.


<details>
  <summary>Details</summary>
Motivation: Existing fine-tuning strategies for VLMs in object counting focus only on training categories, limiting generalizability to unseen categories.

Method: Proposes SDVPT with two-stage visual prompt learning (CSPI and TGPR) to generate and refine category-specific prompts, leveraging semantic correlations for unseen categories.

Result: SDVPT improves performance across three datasets (FSC-147, CARPK, PUCPR+) when integrated with existing models.

Conclusion: SDVPT effectively enhances open-world object counting by transferring knowledge to unseen categories with minimal overhead.

Abstract: Open-world object counting leverages the robust text-image alignment of
pre-trained vision-language models (VLMs) to enable counting of arbitrary
categories in images specified by textual queries. However, widely adopted
naive fine-tuning strategies concentrate exclusively on text-image consistency
for categories contained in training, which leads to limited generalizability
for unseen categories. In this work, we propose a plug-and-play Semantic-Driven
Visual Prompt Tuning framework (SDVPT) that transfers knowledge from the
training set to unseen categories with minimal overhead in parameters and
inference time. First, we introduce a two-stage visual prompt learning strategy
composed of Category-Specific Prompt Initialization (CSPI) and Topology-Guided
Prompt Refinement (TGPR). The CSPI generates category-specific visual prompts,
and then TGPR distills latent structural patterns from the VLM's text encoder
to refine these prompts. During inference, we dynamically synthesize the visual
prompts for unseen categories based on the semantic correlation between unseen
and training categories, facilitating robust text-image alignment for unseen
categories. Extensive experiments integrating SDVPT with all available
open-world object counting models demonstrate its effectiveness and
adaptability across three widely used datasets: FSC-147, CARPK, and PUCPR+.

</details>


### [103] [Fine-tune Smarter, Not Harder: Parameter-Efficient Fine-Tuning for Geospatial Foundation Models](https://arxiv.org/abs/2504.17397)
*Francesc Marti-Escofet, Benedikt Blumenstiel, Linus Scheibenreif, Paolo Fraccaro, Konrad Schindler*

Main category: cs.CV

TL;DR: PEFT techniques match or exceed full fine-tuning performance for geospatial models, reducing resource use and improving generalization.


<details>
  <summary>Details</summary>
Motivation: Address challenges of computational cost and feature forgetting in fine-tuning large foundation models for EO tasks.

Method: Extensive experiments with various foundation model architectures and PEFT techniques across five EO datasets.

Result: PEFT techniques enhance generalization, reduce training time/memory, and perform comparably to full fine-tuning.

Conclusion: PEFT is a scalable, efficient solution for adapting pre-trained geospatial models, integrated into TerraTorch for accessibility.

Abstract: Earth observation (EO) is crucial for monitoring environmental changes,
responding to disasters, and managing natural resources. In this context,
foundation models facilitate remote sensing image analysis to retrieve relevant
geoinformation accurately and efficiently. However, as these models grow in
size, fine-tuning becomes increasingly challenging due to the associated
computational resources and costs, limiting their accessibility and
scalability. Furthermore, full fine-tuning can lead to forgetting pre-trained
features and even degrade model generalization. To address this,
Parameter-Efficient Fine-Tuning (PEFT) techniques offer a promising solution.
In this paper, we conduct extensive experiments with various foundation model
architectures and PEFT techniques to evaluate their effectiveness on five
different EO datasets. Our results provide a comprehensive comparison, offering
insights into when and how PEFT methods support the adaptation of pre-trained
geospatial models. We demonstrate that PEFT techniques match or even exceed
full fine-tuning performance and enhance model generalisation to unseen
geographic regions, while reducing training time and memory requirements.
Additional experiments investigate the effect of architecture choices such as
the decoder type or the use of metadata, suggesting UNet decoders and
fine-tuning without metadata as the recommended configuration. We have
integrated all evaluated foundation models and techniques into the open-source
package TerraTorch to support quick, scalable, and cost-effective model
adaptation.

</details>


### [104] [S2S-Net: Addressing the Domain Gap of Heterogeneous Sensor Systems in LiDAR-Based Collective Perception](https://arxiv.org/abs/2504.17399)
*Sven Teufel, Jörg Gamerdinger, Oliver Bringmann*

Main category: cs.CV

TL;DR: S2S-Net addresses the Sensor2Sensor domain gap in V2V collective perception, achieving high performance on the SCOPE dataset.


<details>
  <summary>Details</summary>
Motivation: Overcome the Sensor2Sensor domain gap in collective perception for CAVs, which arises from heterogeneous sensor setups.

Method: Propose S2S-Net, a sensor-domain robust architecture, and evaluate its domain adaptation capabilities on the SCOPE dataset.

Result: S2S-Net maintains high performance in unseen sensor domains and achieves state-of-the-art results.

Conclusion: S2S-Net effectively addresses the Sensor2Sensor gap, enhancing collective perception in CAVs.

Abstract: Collective Perception (CP) has emerged as a promising approach to overcome
the limitations of individual perception in the context of autonomous driving.
Various approaches have been proposed to realize collective perception;
however, the Sensor2Sensor domain gap that arises from the utilization of
different sensor systems in Connected and Automated Vehicles (CAVs) remains
mostly unaddressed. This is primarily due to the paucity of datasets containing
heterogeneous sensor setups among the CAVs. The recently released SCOPE
datasets address this issue by providing data from three different LiDAR
sensors for each CAV. This study is the first to tackle the Sensor2Sensor
domain gap in vehicle to vehicle (V2V) collective perception. First, we present
our sensor-domain robust architecture S2S-Net. Then an in-depth analysis of the
Sensor2Sensor domain adaptation capabilities of S2S-Net on the SCOPE dataset is
conducted. S2S-Net demonstrates the capability to maintain very high
performance in unseen sensor domains and achieved state-of-the-art results on
the SCOPE dataset.

</details>


### [105] [StereoMamba: Real-time and Robust Intraoperative Stereo Disparity Estimation via Long-range Spatial Dependencies](https://arxiv.org/abs/2504.17401)
*Xu Wang, Jialang Xu, Shuai Zhang, Baoru Huang, Danail Stoyanov, Evangelos B. Mazomenos*

Main category: cs.CV

TL;DR: StereoMamba, a novel architecture for stereo disparity estimation in RAMIS, combines FE-Mamba and MFF modules to achieve high accuracy, robustness, and speed.


<details>
  <summary>Details</summary>
Motivation: Current deep learning methods for stereo disparity estimation in RAMIS lack an optimal balance between accuracy, robustness, and inference speed.

Method: Proposes StereoMamba with FE-Mamba for long-range spatial dependencies and MFF for multi-scale feature fusion.

Result: Achieves superior performance on SCARED benchmark (EPE 2.64 px, depth MAE 2.55 mm) and high SSIM/PSNR on other datasets.

Conclusion: StereoMamba strikes an optimal balance between accuracy, robustness, and efficiency, demonstrating strong generalization.

Abstract: Stereo disparity estimation is crucial for obtaining depth information in
robot-assisted minimally invasive surgery (RAMIS). While current deep learning
methods have made significant advancements, challenges remain in achieving an
optimal balance between accuracy, robustness, and inference speed. To address
these challenges, we propose the StereoMamba architecture, which is
specifically designed for stereo disparity estimation in RAMIS. Our approach is
based on a novel Feature Extraction Mamba (FE-Mamba) module, which enhances
long-range spatial dependencies both within and across stereo images. To
effectively integrate multi-scale features from FE-Mamba, we then introduce a
novel Multidimensional Feature Fusion (MFF) module. Experiments against the
state-of-the-art on the ex-vivo SCARED benchmark demonstrate that StereoMamba
achieves superior performance on EPE of 2.64 px and depth MAE of 2.55 mm, the
second-best performance on Bad2 of 41.49% and Bad3 of 26.99%, while maintaining
an inference speed of 21.28 FPS for a pair of high-resolution images
(1280*1024), striking the optimum balance between accuracy, robustness, and
efficiency. Furthermore, by comparing synthesized right images, generated from
warping left images using the generated disparity maps, with the actual right
image, StereoMamba achieves the best average SSIM (0.8970) and PSNR (16.0761),
exhibiting strong zero-shot generalization on the in-vivo RIS2017 and StereoMIS
datasets.

</details>


### [106] [3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models](https://arxiv.org/abs/2504.17414)
*Min Wei, Chaohui Yu, Jingkai Zhou, Fan Wang*

Main category: cs.CV

TL;DR: 3DV-TON is a diffusion-based framework for high-fidelity, temporally consistent video try-on, using animatable 3D meshes and a robust masking strategy to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing video try-on methods struggle with complex clothing patterns and diverse poses, leading to inconsistent results.

Method: Uses animatable 3D meshes for frame-level guidance, an adaptive pipeline for dynamic 3D guidance, and a rectangular masking strategy to reduce artifacts.

Result: Outperforms existing methods, demonstrated by quantitative and qualitative results on the HR-VVT dataset.

Conclusion: 3DV-TON advances video try-on with high-quality, consistent results and introduces a new benchmark dataset.

Abstract: Video try-on replaces clothing in videos with target garments. Existing
methods struggle to generate high-quality and temporally consistent results
when handling complex clothing patterns and diverse body poses. We present
3DV-TON, a novel diffusion-based framework for generating high-fidelity and
temporally consistent video try-on results. Our approach employs generated
animatable textured 3D meshes as explicit frame-level guidance, alleviating the
issue of models over-focusing on appearance fidelity at the expanse of motion
coherence. This is achieved by enabling direct reference to consistent garment
texture movements throughout video sequences. The proposed method features an
adaptive pipeline for generating dynamic 3D guidance: (1) selecting a keyframe
for initial 2D image try-on, followed by (2) reconstructing and animating a
textured 3D mesh synchronized with original video poses. We further introduce a
robust rectangular masking strategy that successfully mitigates artifact
propagation caused by leaking clothing information during dynamic human and
garment movements. To advance video try-on research, we introduce HR-VVT, a
high-resolution benchmark dataset containing 130 videos with diverse clothing
types and scenarios. Quantitative and qualitative results demonstrate our
superior performance over existing methods. The project page is at this link
https://2y7c3.github.io/3DV-TON/

</details>


### [107] [Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs](https://arxiv.org/abs/2504.17432)
*Tiancheng Gu, Kaicheng Yang, Ziyong Feng, Xingjun Wang, Yanzhao Zhang, Dingkun Long, Yingda Chen, Weidong Cai, Jiankang Deng*

Main category: cs.CV

TL;DR: UniME, a two-stage framework using MLLMs, improves multimodal representation learning by addressing CLIP's limitations through knowledge distillation and hard negative enhanced instruction tuning.


<details>
  <summary>Details</summary>
Motivation: CLIP's limitations (text truncation, isolated encoding, poor compositionality) and unexplored potential of MLLMs for transferable multimodal representations.

Method: Two-stage approach: (1) textual discriminative knowledge distillation from an LLM teacher, (2) hard negative enhanced instruction tuning to improve discriminative power.

Result: UniME outperforms on MMEB benchmark and retrieval tasks, showing superior discriminative and compositional capabilities.

Conclusion: UniME effectively enhances multimodal representation learning, addressing CLIP's shortcomings and advancing MLLM applications.

Abstract: The Contrastive Language-Image Pre-training (CLIP) framework has become a
widely used approach for multimodal representation learning, particularly in
image-text retrieval and clustering. However, its efficacy is constrained by
three key limitations: (1) text token truncation, (2) isolated image-text
encoding, and (3) deficient compositionality due to bag-of-words behavior.
While recent Multimodal Large Language Models (MLLMs) have demonstrated
significant advances in generalized vision-language understanding, their
potential for learning transferable multimodal representations remains
underexplored.In this work, we present UniME (Universal Multimodal Embedding),
a novel two-stage framework that leverages MLLMs to learn discriminative
representations for diverse downstream tasks. In the first stage, we perform
textual discriminative knowledge distillation from a powerful LLM-based teacher
model to enhance the embedding capability of the MLLM\'s language component. In
the second stage, we introduce hard negative enhanced instruction tuning to
further advance discriminative representation learning. Specifically, we
initially mitigate false negative contamination and then sample multiple hard
negatives per instance within each batch, forcing the model to focus on
challenging samples. This approach not only improves discriminative power but
also enhances instruction-following ability in downstream tasks. We conduct
extensive experiments on the MMEB benchmark and multiple retrieval tasks,
including short and long caption retrieval and compositional retrieval. Results
demonstrate that UniME achieves consistent performance improvement across all
tasks, exhibiting superior discriminative and compositional capabilities.

</details>


### [108] [Predict-Optimize-Distill: A Self-Improving Cycle for 4D Object Understanding](https://arxiv.org/abs/2504.17441)
*Mingxuan Wu, Huang Huang, Justin Kerr, Chung Min Kim, Anthony Zhang, Brent Yi, Angjoo Kanazawa*

Main category: cs.CV

TL;DR: POD is a self-improving framework for 4D object understanding, combining prediction and optimization in a cycle to refine object pose predictions over time.


<details>
  <summary>Details</summary>
Motivation: Humans improve 3D object state prediction with observation, but existing systems rely on multi-view observations or supervised datasets. POD aims to bridge this gap.

Method: POD iteratively predicts part poses, refines them via global optimization, and distills results back into the model using synthetic self-labeled data.

Result: POD outperforms optimization baselines, especially for longer videos, and improves with more observations and iterations.

Conclusion: POD's self-improving cycle effectively scales performance with additional data and refinement, advancing 4D object understanding.

Abstract: Humans can resort to long-form inspection to build intuition on predicting
the 3D configurations of unseen objects. The more we observe the object motion,
the better we get at predicting its 3D state immediately. Existing systems
either optimize underlying representations from multi-view observations or
train a feed-forward predictor from supervised datasets. We introduce
Predict-Optimize-Distill (POD), a self-improving framework that interleaves
prediction and optimization in a mutually reinforcing cycle to achieve better
4D object understanding with increasing observation time. Given a multi-view
object scan and a long-form monocular video of human-object interaction, POD
iteratively trains a neural network to predict local part poses from RGB
frames, uses this predictor to initialize a global optimization which refines
output poses through inverse rendering, then finally distills the results of
optimization back into the model by generating synthetic self-labeled training
data from novel viewpoints. Each iteration improves both the predictive model
and the optimized motion trajectory, creating a virtuous cycle that bootstraps
its own training data to learn about the pose configurations of an object. We
also introduce a quasi-multiview mining strategy for reducing depth ambiguity
by leveraging long video. We evaluate POD on 14 real-world and 5 synthetic
objects with various joint types, including revolute and prismatic joints as
well as multi-body configurations where parts detach or reattach independently.
POD demonstrates significant improvement over a pure optimization baseline
which gets stuck in local minima, particularly for longer videos. We also find
that POD's performance improves with both video length and successive
iterations of the self-improving cycle, highlighting its ability to scale
performance with additional observations and looped refinement.

</details>


### [109] [FRAG: Frame Selection Augmented Generation for Long Video and Long Document Understanding](https://arxiv.org/abs/2504.17447)
*De-An Huang, Subhashree Radhakrishnan, Zhiding Yu, Jan Kautz*

Main category: cs.CV

TL;DR: FRAG improves long input processing in LMMs by selecting relevant frames without long context, boosting performance in video and document tasks.


<details>
  <summary>Details</summary>
Motivation: Overcome computational limits of long context LMMs by avoiding full input processing.

Method: Frame Selection Augmented Generation (FRAG) scores and selects top frames independently, using existing LMMs without fine-tuning.

Result: FRAG enhances performance, achieving SOTA in video (e.g., 5.8% on MLVU) and document tasks (e.g., 20% on MP-DocVQA).

Conclusion: FRAG's simple yet effective approach advances long input processing without long context LMMs.

Abstract: There has been impressive progress in Large Multimodal Models (LMMs). Recent
works extend these models to long inputs, including multi-page documents and
long videos. However, the model size and performance of these long context
models are still limited due to the computational cost in both training and
inference. In this work, we explore an orthogonal direction and process long
inputs without long context LMMs. We propose Frame Selection Augmented
Generation (FRAG), where the model first selects relevant frames within the
input, and then only generates the final outputs based on the selected frames.
The core of the selection process is done by scoring each frame independently,
which does not require long context processing. The frames with the highest
scores are then selected by a simple Top-K selection. We show that this
frustratingly simple framework is applicable to both long videos and multi-page
documents using existing LMMs without any fine-tuning. We consider two models,
LLaVA-OneVision and InternVL2, in our experiments and show that FRAG
consistently improves the performance and achieves state-of-the-art
performances for both long video and long document understanding. For videos,
FRAG substantially improves InternVL2-76B by 5.8% on MLVU and 3.7% on
Video-MME. For documents, FRAG achieves over 20% improvements on MP-DocVQA
compared with recent LMMs specialized in long document understanding. Code is
available at: https://github.com/NVlabs/FRAG

</details>


### [110] [Unveiling Hidden Vulnerabilities in Digital Human Generation via Adversarial Attacks](https://arxiv.org/abs/2504.17457)
*Zhiying Li, Yeying Jin, Fan Shen, Zhi Liu, Weibin Chen, Pengju Zhang, Xiaomei Zhang, Boyu Chen, Michael Shen, Kejian Wu, Zhaoxin Fan, Jin Dong*

Main category: cs.CV

TL;DR: The paper introduces Tangible Attack (TBA), a framework for adversarial attacks on digital human generation models, using a Dual Heterogeneous Noise Generator (DHNG) and custom adversarial loss to increase estimation errors by 41.0%.


<details>
  <summary>Details</summary>
Motivation: Existing EHPS research focuses on accuracy but overlooks robustness and security, making systems vulnerable to attacks.

Method: TBA employs DHNG (VAE and ControlNet) to generate targeted noise and a custom adversarial loss for optimization, refining attacks via multi-gradient signals.

Result: TBA increases estimation errors by 41.0%, with an average improvement of 17.0%, exposing vulnerabilities in EHPS models.

Conclusion: The study reveals critical security flaws in EHPS systems and underscores the need for stronger defenses in digital human generation.

Abstract: Expressive human pose and shape estimation (EHPS) is crucial for digital
human generation, especially in applications like live streaming. While
existing research primarily focuses on reducing estimation errors, it largely
neglects robustness and security aspects, leaving these systems vulnerable to
adversarial attacks. To address this significant challenge, we propose the
\textbf{Tangible Attack (TBA)}, a novel framework designed to generate
adversarial examples capable of effectively compromising any digital human
generation model. Our approach introduces a \textbf{Dual Heterogeneous Noise
Generator (DHNG)}, which leverages Variational Autoencoders (VAE) and
ControlNet to produce diverse, targeted noise tailored to the original image
features. Additionally, we design a custom \textbf{adversarial loss function}
to optimize the noise, ensuring both high controllability and potent
disruption. By iteratively refining the adversarial sample through
multi-gradient signals from both the noise and the state-of-the-art EHPS model,
TBA substantially improves the effectiveness of adversarial attacks. Extensive
experiments demonstrate TBA's superiority, achieving a remarkable 41.0\%
increase in estimation error, with an average improvement of approximately
17.0\%. These findings expose significant security vulnerabilities in current
EHPS models and highlight the need for stronger defenses in digital human
generation systems.

</details>


### [111] [Enhanced Sample Selection with Confidence Tracking: Identifying Correctly Labeled yet Hard-to-Learn Samples in Noisy Data](https://arxiv.org/abs/2504.17474)
*Weiran Pan, Wei Wei, Feida Zhu, Yong Deng*

Main category: cs.CV

TL;DR: A novel sample selection method for image classification with noisy labels, using confidence trends instead of loss values to distinguish hard-to-learn samples from mislabeled ones.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on loss thresholds, causing a trade-off between precision and recall in sample selection. The goal is to accurately identify correctly labeled but hard-to-learn samples.

Method: Track confidence gaps between annotated labels and other classes during training, evaluating trends with the Mann-Kendall Test. Samples with increasing confidence gaps are considered correctly labeled.

Result: Improves performance of existing noisy-label learning methods on standard benchmarks and real-world datasets.

Conclusion: The method effectively alleviates the trade-off in sample selection by leveraging confidence trends, enhancing robustness in noisy-label scenarios.

Abstract: We propose a novel sample selection method for image classification in the
presence of noisy labels. Existing methods typically consider small-loss
samples as correctly labeled. However, some correctly labeled samples are
inherently difficult for the model to learn and can exhibit high loss similar
to mislabeled samples in the early stages of training. Consequently, setting a
threshold on per-sample loss to select correct labels results in a trade-off
between precision and recall in sample selection: a lower threshold may miss
many correctly labeled hard-to-learn samples (low recall), while a higher
threshold may include many mislabeled samples (low precision). To address this
issue, our goal is to accurately distinguish correctly labeled yet
hard-to-learn samples from mislabeled ones, thus alleviating the trade-off
dilemma. We achieve this by considering the trends in model prediction
confidence rather than relying solely on loss values. Empirical observations
show that only for correctly labeled samples, the model's prediction confidence
for the annotated labels typically increases faster than for any other classes.
Based on this insight, we propose tracking the confidence gaps between the
annotated labels and other classes during training and evaluating their trends
using the Mann-Kendall Test. A sample is considered potentially correctly
labeled if all its confidence gaps tend to increase. Our method functions as a
plug-and-play component that can be seamlessly integrated into existing sample
selection techniques. Experiments on several standard benchmarks and real-world
datasets demonstrate that our method enhances the performance of existing
methods for learning with noisy labels.

</details>


### [112] [RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image Generation](https://arxiv.org/abs/2504.17502)
*Aviv Slobodkin, Hagai Taitelbaum, Yonatan Bitton, Brian Gordon, Michal Sokolik, Nitzan Bitton Guetta, Almog Gueta, Royi Rassin, Itay Laish, Dani Lischinski, Idan Szpektor*

Main category: cs.CV

TL;DR: RefVNLI is a new metric for evaluating subject-driven T2I generation, addressing limitations of existing methods by assessing both textual alignment and subject preservation effectively.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for subject-driven T2I generation are unreliable, focusing on only one aspect, misaligning with human judgments, or being costly.

Method: RefVNLI is introduced, trained on a large-scale dataset from video-reasoning benchmarks and image perturbations.

Result: RefVNLI outperforms baselines, achieving up to 6.4-point gains in textual alignment and 8.5-point gains in subject consistency, with 87% human preference alignment.

Conclusion: RefVNLI provides a cost-effective, reliable solution for evaluating subject-driven T2I generation, improving performance and human alignment.

Abstract: Subject-driven text-to-image (T2I) generation aims to produce images that
align with a given textual description, while preserving the visual identity
from a referenced subject image. Despite its broad downstream applicability --
ranging from enhanced personalization in image generation to consistent
character representation in video rendering -- progress in this field is
limited by the lack of reliable automatic evaluation. Existing methods either
assess only one aspect of the task (i.e., textual alignment or subject
preservation), misalign with human judgments, or rely on costly API-based
evaluation. To address this, we introduce RefVNLI, a cost-effective metric that
evaluates both textual alignment and subject preservation in a single
prediction. Trained on a large-scale dataset derived from video-reasoning
benchmarks and image perturbations, RefVNLI outperforms or matches existing
baselines across multiple benchmarks and subject categories (e.g.,
\emph{Animal}, \emph{Object}), achieving up to 6.4-point gains in textual
alignment and 8.5-point gains in subject consistency. It also excels with
lesser-known concepts, aligning with human preferences at over 87\% accuracy.

</details>


### [113] [Mamba-Sea: A Mamba-based Framework with Global-to-Local Sequence Augmentation for Generalizable Medical Image Segmentation](https://arxiv.org/abs/2504.17515)
*Zihan Cheng, Jintao Guo, Jian Zhang, Lei Qi, Luping Zhou, Yinghuan Shi, Yang Gao*

Main category: cs.CV

TL;DR: Mamba-Sea, a Mamba-based framework, addresses domain generalization in medical image segmentation by incorporating global-to-local sequence augmentation, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To improve generalization of medical image segmentation models under domain shifts by leveraging the Mamba architecture's ability to capture long-range dependencies efficiently.

Method: Proposes Mamba-Sea, a novel framework with global augmentation to simulate appearance variations and local sequence-wise augmentation to perturb token styles, enhancing robustness to domain shifts.

Result: Achieves a Dice coefficient of over 90% on the Prostate dataset, surpassing the previous SOTA of 88.61%.

Conclusion: Mamba-Sea demonstrates the potential of Mamba architectures for domain generalization in medical image segmentation, offering strong robustness and performance.

Abstract: To segment medical images with distribution shifts, domain generalization
(DG) has emerged as a promising setting to train models on source domains that
can generalize to unseen target domains. Existing DG methods are mainly based
on CNN or ViT architectures. Recently, advanced state space models, represented
by Mamba, have shown promising results in various supervised medical image
segmentation. The success of Mamba is primarily owing to its ability to capture
long-range dependencies while keeping linear complexity with input sequence
length, making it a promising alternative to CNNs and ViTs. Inspired by the
success, in the paper, we explore the potential of the Mamba architecture to
address distribution shifts in DG for medical image segmentation. Specifically,
we propose a novel Mamba-based framework, Mamba-Sea, incorporating
global-to-local sequence augmentation to improve the model's generalizability
under domain shift issues. Our Mamba-Sea introduces a global augmentation
mechanism designed to simulate potential variations in appearance across
different sites, aiming to suppress the model's learning of domain-specific
information. At the local level, we propose a sequence-wise augmentation along
input sequences, which perturbs the style of tokens within random continuous
sub-sequences by modeling and resampling style statistics associated with
domain shifts. To our best knowledge, Mamba-Sea is the first work to explore
the generalization of Mamba for medical image segmentation, providing an
advanced and promising Mamba-based architecture with strong robustness to
domain shifts. Remarkably, our proposed method is the first to surpass a Dice
coefficient of 90% on the Prostate dataset, which exceeds previous SOTA of
88.61%. The code is available at https://github.com/orange-czh/Mamba-Sea.

</details>


### [114] [Towards One-Stage End-to-End Table Structure Recognition with Parallel Regression for Diverse Scenarios](https://arxiv.org/abs/2504.17522)
*Anyi Xiao, Cihui Yang*

Main category: cs.CV

TL;DR: A one-stage end-to-end network, TableCenterNet, is proposed for table structure recognition, unifying spatial and logical structure prediction into parallel regression for improved efficiency and adaptability.


<details>
  <summary>Details</summary>
Motivation: Existing methods for table structure recognition are either multi-stage and time-consuming or rely on complex post-processing, lacking balance between adaptability, robustness, and efficiency.

Method: TableCenterNet uses a synergistic architecture with shared feature extraction and task-specific decoding to predict table spatial and logical structures in parallel.

Result: The method achieves state-of-the-art performance on the TableGraph-24k dataset, demonstrating effectiveness across diverse scenarios.

Conclusion: TableCenterNet offers a simpler, faster, and more adaptable solution for table structure recognition compared to existing approaches.

Abstract: Table structure recognition aims to parse tables in unstructured data into
machine-understandable formats. Recent methods address this problem through a
two-stage process or optimized one-stage approaches. However, these methods
either require multiple networks to be serially trained and perform more
time-consuming sequential decoding, or rely on complex post-processing
algorithms to parse the logical structure of tables. They struggle to balance
cross-scenario adaptability, robustness, and computational efficiency. In this
paper, we propose a one-stage end-to-end table structure parsing network called
TableCenterNet. This network unifies the prediction of table spatial and
logical structure into a parallel regression task for the first time, and
implicitly learns the spatial-logical location mapping laws of cells through a
synergistic architecture of shared feature extraction layers and task-specific
decoding. Compared with two-stage methods, our method is easier to train and
faster to infer. Experiments on benchmark datasets show that TableCenterNet can
effectively parse table structures in diverse scenarios and achieve
state-of-the-art performance on the TableGraph-24k dataset. Code is available
at https://github.com/dreamy-xay/TableCenterNet.

</details>


### [115] [ESDiff: Encoding Strategy-inspired Diffusion Model with Few-shot Learning for Color Image Inpainting](https://arxiv.org/abs/2504.17524)
*Junyan Zhang, Yan Li, Mengxiao Geng, Liu Shi, Qiegen Liu*

Main category: cs.CV

TL;DR: A novel encoding strategy-inspired diffusion model for few-shot color image inpainting, outperforming traditional and deep learning methods in quality and detail preservation.


<details>
  <summary>Details</summary>
Motivation: Traditional inpainting methods struggle with complex details, and deep learning models require large datasets. This work aims to address these limitations with a few-shot learning approach.

Method: Uses a 'virtual mask' encoding strategy for high-dimensional object construction via channel perturbations, combined with low-rank methods and diffusion models for iterative inpainting.

Result: Outperforms existing techniques in quantitative metrics, improving texture and structural integrity in reconstructed images.

Conclusion: The proposed method achieves more precise and coherent inpainting results with limited training data, advancing the field.

Abstract: Image inpainting is a technique used to restore missing or damaged regions of
an image. Traditional methods primarily utilize information from adjacent
pixels for reconstructing missing areas, while they struggle to preserve
complex details and structures. Simultaneously, models based on deep learning
necessitate substantial amounts of training data. To address this challenge, an
encoding strategy-inspired diffusion model with few-shot learning for color
image inpainting is proposed in this paper. The main idea of this novel
encoding strategy is the deployment of a "virtual mask" to construct
high-dimensional objects through mutual perturbations between channels. This
approach enables the diffusion model to capture diverse image representations
and detailed features from limited training samples. Moreover, the encoding
strategy leverages redundancy between channels, integrates with low-rank
methods during iterative inpainting, and incorporates the diffusion model to
achieve accurate information output. Experimental results indicate that our
method exceeds current techniques in quantitative metrics, and the
reconstructed images quality has been improved in aspects of texture and
structural integrity, leading to more precise and coherent results.

</details>


### [116] [Text-to-Image Alignment in Denoising-Based Models through Step Selection](https://arxiv.org/abs/2504.17525)
*Paul Grimal, Hervé Le Borgne, Olivier Ferret*

Main category: cs.CV

TL;DR: A novel method enhances signal at critical denoising steps to improve text-image alignment in generative AI, outperforming early-stage modifications.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in text-image alignment and reasoning limitations in visual generative AI models.

Method: Selectively enhances signal at later denoising steps to optimize image generation based on input semantics.

Result: Achieves state-of-the-art performance on Diffusion and Flow Matching models, improving semantic alignment.

Conclusion: Judicious choice of sampling stage is crucial for enhancing performance and image alignment.

Abstract: Visual generative AI models often encounter challenges related to text-image
alignment and reasoning limitations. This paper presents a novel method for
selectively enhancing the signal at critical denoising steps, optimizing image
generation based on input semantics. Our approach addresses the shortcomings of
early-stage signal modifications, demonstrating that adjustments made at later
stages yield superior results. We conduct extensive experiments to validate the
effectiveness of our method in producing semantically aligned images on
Diffusion and Flow Matching model, achieving state-of-the-art performance. Our
results highlight the importance of a judicious choice of sampling stage to
improve performance and overall image alignment.

</details>


### [117] [When Gaussian Meets Surfel: Ultra-fast High-fidelity Radiance Field Rendering](https://arxiv.org/abs/2504.17545)
*Keyang Ye, Tianjia Shao, Kun Zhou*

Main category: cs.CV

TL;DR: Gaussian-enhanced Surfels (GESs) combine 2D surfels and 3D Gaussians for fast, high-fidelity radiance field rendering, avoiding artifacts and enabling extensions like anti-aliasing and compact storage.


<details>
  <summary>Details</summary>
Motivation: To create a bi-scale representation for radiance field rendering that balances coarse and fine details while ensuring fast, view-consistent rendering.

Method: Uses 2D surfels for coarse geometry and 3D Gaussians for fine details, with a two-pass rendering process (rasterization and splatting). Optimized via a coarse-to-fine procedure.

Result: Achieves ultra-fast, high-fidelity rendering with view consistency and no popping artifacts. Extensions (Mip-GES, Speedy-GES, etc.) further enhance performance.

Conclusion: GESs advance radiance field rendering, offering speed, quality, and flexibility, with potential for broader applications.

Abstract: We introduce Gaussian-enhanced Surfels (GESs), a bi-scale representation for
radiance field rendering, wherein a set of 2D opaque surfels with
view-dependent colors represent the coarse-scale geometry and appearance of
scenes, and a few 3D Gaussians surrounding the surfels supplement fine-scale
appearance details. The rendering with GESs consists of two passes -- surfels
are first rasterized through a standard graphics pipeline to produce depth and
color maps, and then Gaussians are splatted with depth testing and color
accumulation on each pixel order independently. The optimization of GESs from
multi-view images is performed through an elaborate coarse-to-fine procedure,
faithfully capturing rich scene appearance. The entirely sorting-free rendering
of GESs not only achieves very fast rates, but also produces view-consistent
images, successfully avoiding popping artifacts under view changes. The basic
GES representation can be easily extended to achieve anti-aliasing in rendering
(Mip-GES), boosted rendering speeds (Speedy-GES) and compact storage
(Compact-GES), and reconstruct better scene geometries by replacing 3D
Gaussians with 2D Gaussians (2D-GES). Experimental results show that GESs
advance the state-of-the-arts as a compelling representation for ultra-fast
high-fidelity radiance field rendering.

</details>


### [118] [Unsupervised Urban Land Use Mapping with Street View Contrastive Clustering and a Geographical Prior](https://arxiv.org/abs/2504.17551)
*Lin Che, Yizi Chen, Tanhua Jin, Martin Raubal, Konrad Schindler, Peter Kiefer*

Main category: cs.CV

TL;DR: An unsupervised contrastive clustering model with a geographical prior is introduced for street view images to improve urban land use mapping, addressing the limitations of supervised methods.


<details>
  <summary>Details</summary>
Motivation: Existing remote sensing techniques lack precision in complex urban environments, and supervised street view-based methods struggle with scarce labeled data and generalization.

Method: The study proposes an unsupervised contrastive clustering model with a built-in geographical prior for street view images, combined with visual cluster assignment.

Result: The method successfully generates land use maps from geotagged street view datasets of two cities, leveraging spatial coherence.

Conclusion: The approach offers a scalable, flexible solution for unsupervised land use mapping, adaptable to various settings with street view data.

Abstract: Urban land use classification and mapping are critical for urban planning,
resource management, and environmental monitoring. Existing remote sensing
techniques often lack precision in complex urban environments due to the
absence of ground-level details. Unlike aerial perspectives, street view images
provide a ground-level view that captures more human and social activities
relevant to land use in complex urban scenes. Existing street view-based
methods primarily rely on supervised classification, which is challenged by the
scarcity of high-quality labeled data and the difficulty of generalizing across
diverse urban landscapes. This study introduces an unsupervised contrastive
clustering model for street view images with a built-in geographical prior, to
enhance clustering performance. When combined with a simple visual assignment
of the clusters, our approach offers a flexible and customizable solution to
land use mapping, tailored to the specific needs of urban planners. We
experimentally show that our method can generate land use maps from geotagged
street view image datasets of two cities. As our methodology relies on the
universal spatial coherence of geospatial data ("Tobler's law"), it can be
adapted to various settings where street view images are available, to enable
scalable, unsupervised land use mapping and updating. The code will be
available at https://github.com/lin102/CCGP.

</details>


### [119] [Occlusion-Aware Self-Supervised Monocular Depth Estimation for Weak-Texture Endoscopic Images](https://arxiv.org/abs/2504.17582)
*Zebo Huang, Yinghui Wang*

Main category: cs.CV

TL;DR: A self-supervised monocular depth estimation network for endoscopic scenes addresses illumination and occlusion challenges, improving depth reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail under dynamic lighting and occlusions in gastrointestinal tracts, leading to unreliable depth estimates.

Method: Introduces an occlusion-aware framework with data augmentation via occlusion masks and semantic segmentation guided by non-negative matrix factorization.

Result: Achieves state-of-the-art performance on the SCARED dataset and generalizes well on Endo-SLAM and SERV-CT datasets.

Conclusion: The proposed method effectively handles endoscopic scene challenges, enhancing depth estimation accuracy and robustness.

Abstract: We propose a self-supervised monocular depth estimation network tailored for
endoscopic scenes, aiming to infer depth within the gastrointestinal tract from
monocular images. Existing methods, though accurate, typically assume
consistent illumination, which is often violated due to dynamic lighting and
occlusions caused by GI motility. These variations lead to incorrect geometric
interpretations and unreliable self-supervised signals, degrading depth
reconstruction quality. To address this, we introduce an occlusion-aware
self-supervised framework. First, we incorporate an occlusion mask for data
augmentation, generating pseudo-labels by simulating viewpoint-dependent
occlusion scenarios. This enhances the model's ability to learn robust depth
features under partial visibility. Second, we leverage semantic segmentation
guided by non-negative matrix factorization, clustering convolutional
activations to generate pseudo-labels in texture-deprived regions, thereby
improving segmentation accuracy and mitigating information loss from lighting
changes. Experimental results on the SCARED dataset show that our method
achieves state-of-the-art performance in self-supervised depth estimation.
Additionally, evaluations on the Endo-SLAM and SERV-CT datasets demonstrate
strong generalization across diverse endoscopic environments.

</details>


### [120] [Tamper-evident Image using JPEG Fixed Points](https://arxiv.org/abs/2504.17594)
*Zhaofeng Si, Siwei Lyu*

Main category: cs.CV

TL;DR: JPEG compression and decompression iterations lead to stable fixed-point images, which are used to create tamper-evident images.


<details>
  <summary>Details</summary>
Motivation: To prove the existence of fixed points in JPEG procedures and leverage them for tamper detection.

Method: Analyze JPEG compression/decompression processes to identify fixed points and develop a tamper-evident method.

Result: Fixed points exist, preserve visual quality, and enable tamper detection by showing deviations.

Conclusion: Fixed points in JPEG can be used to create tamper-evident images, exposing manipulations.

Abstract: An intriguing phenomenon about JPEG compression has been observed since two
decades ago- after repeating JPEG compression and decompression, it leads to a
stable image that does not change anymore, which is a fixed point. In this
work, we prove the existence of fixed points in the essential JPEG procedures.
We analyze JPEG compression and decompression processes, revealing the
existence of fixed points that can be reached within a few iterations. These
fixed points are diverse and preserve the image's visual quality, ensuring
minimal distortion. This result is used to develop a method to create a
tamper-evident image from the original authentic image, which can expose
tampering operations by showing deviations from the fixed point image.

</details>


### [121] [STCL:Curriculum learning Strategies for deep learning image steganography models](https://arxiv.org/abs/2504.17609)
*Fengchun Liu, Tong Zhang, Chunying Zhang*

Main category: cs.CV

TL;DR: The paper proposes a Steganography Curriculum Learning (STCL) strategy to improve image steganography models by training on easier images first and gradually increasing difficulty, enhancing quality and convergence.


<details>
  <summary>Details</summary>
Motivation: Addresses poor steganographic image quality and slow convergence in deep learning models by introducing a curriculum-based training approach.

Method: Uses teacher models to evaluate image difficulty and a knee point-based scheduling strategy to control training progression.

Result: Improves model performance on ALASKA2, VOC2012, and ImageNet datasets with higher PSNR, SSIM, and decoding accuracy, and lower steganography analysis scores.

Conclusion: The STCL strategy effectively enhances steganography model performance by optimizing training progression and reducing overfitting.

Abstract: Aiming at the problems of poor quality of steganographic images and slow
network convergence of image steganography models based on deep learning, this
paper proposes a Steganography Curriculum Learning training strategy (STCL) for
deep learning image steganography models. So that only easy images are selected
for training when the model has poor fitting ability at the initial stage, and
gradually expand to more difficult images, the strategy includes a difficulty
evaluation strategy based on the teacher model and an knee point-based training
scheduling strategy. Firstly, multiple teacher models are trained, and the
consistency of the quality of steganographic images under multiple teacher
models is used as the difficulty score to construct the training subsets from
easy to difficult. Secondly, a training control strategy based on knee points
is proposed to reduce the possibility of overfitting on small training sets and
accelerate the training process. Experimental results on three large public
datasets, ALASKA2, VOC2012 and ImageNet, show that the proposed image
steganography scheme is able to improve the model performance under multiple
algorithmic frameworks, which not only has a high PSNR, SSIM score, and
decoding accuracy, but also the steganographic images generated by the model
under the training of the STCL strategy have a low steganography analysis
scores. You can find our code at
\href{https://github.com/chaos-boops/STCL}{https://github.com/chaos-boops/STCL}.

</details>


### [122] [RGB-D Tracking via Hierarchical Modality Aggregation and Distribution Network](https://arxiv.org/abs/2504.17595)
*Boyue Xu, Yi Xu, Ruichao Hou, Jia Bei, Tongwei Ren, Gangshan Wu*

Main category: cs.CV

TL;DR: HMAD, a novel hierarchical network, improves RGB-D tracking by efficiently fusing and distributing multi-level RGB and depth features, achieving state-of-the-art performance and real-time capability.


<details>
  <summary>Details</summary>
Motivation: Current RGB-D trackers are inefficient and limited to single-level features, lacking robustness and speed for real-world applications.

Method: HMAD employs a hierarchical approach to aggregate and distribute RGB and depth features, enhancing fusion robustness.

Result: HMAD outperforms existing methods on RGB-D datasets and proves effective in real-time tracking scenarios.

Conclusion: HMAD addresses inefficiencies in RGB-D tracking, offering improved performance and real-time applicability.

Abstract: The integration of dual-modal features has been pivotal in advancing
RGB-Depth (RGB-D) tracking. However, current trackers are less efficient and
focus solely on single-level features, resulting in weaker robustness in fusion
and slower speeds that fail to meet the demands of real-world applications. In
this paper, we introduce a novel network, denoted as HMAD (Hierarchical
Modality Aggregation and Distribution), which addresses these challenges. HMAD
leverages the distinct feature representation strengths of RGB and depth
modalities, giving prominence to a hierarchical approach for feature
distribution and fusion, thereby enhancing the robustness of RGB-D tracking.
Experimental results on various RGB-D datasets demonstrate that HMAD achieves
state-of-the-art performance. Moreover, real-world experiments further validate
HMAD's capacity to effectively handle a spectrum of tracking challenges in
real-time scenarios.

</details>


### [123] [Enhancing CNNs robustness to occlusions with bioinspired filters for border completion](https://arxiv.org/abs/2504.17619)
*Catarina P. Coutinho, Aneeqa Merhab, Janko Petkovic, Ferdinando Zanchetta, Rita Fioresi*

Main category: cs.CV

TL;DR: Custom filters based on visual cortex modeling improve CNN performance on occluded MNIST images.


<details>
  <summary>Details</summary>
Motivation: To enhance CNN performance by mimicking the visual cortex's border completion mechanism.

Method: Mathematical modeling of visual cortex mechanisms to define custom filters for CNNs, tested with a modified LeNet 5 on occluded MNIST images.

Result: Consistent improvement in performance, especially accuracy.

Conclusion: Modeling visual cortex mechanisms for custom filters effectively boosts CNN performance in handling occlusions.

Abstract: We exploit the mathematical modeling of the visual cortex mechanism for
border completion to define custom filters for CNNs. We see a consistent
improvement in performance, particularly in accuracy, when our modified LeNet 5
is tested with occluded MNIST images.

</details>


### [124] [Improving Open-World Object Localization by Discovering Background](https://arxiv.org/abs/2504.17626)
*Ashish Singh, Michael J. Jones, Kuan-Chuan Peng, Anoop Cherian, Moitreya Chatterjee, Erik Learned-Miller*

Main category: cs.CV

TL;DR: The paper introduces a framework for open-world object localization by leveraging background information to improve objectness learning, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: The challenge is to localize objects in an open-world setting, including unseen classes, using limited training data. Existing methods focus on object characterization, but this work explores background information.

Method: The proposed framework identifies non-discriminative background regions and trains an object proposal network to avoid detecting objects in these areas.

Result: Experiments on benchmarks show significant improvements over state-of-the-art approaches.

Conclusion: Incorporating background information effectively enhances open-world object localization.

Abstract: Our work addresses the problem of learning to localize objects in an
open-world setting, i.e., given the bounding box information of a limited
number of object classes during training, the goal is to localize all objects,
belonging to both the training and unseen classes in an image, during
inference. Towards this end, recent work in this area has focused on improving
the characterization of objects either explicitly by proposing new objective
functions (localization quality) or implicitly using object-centric
auxiliary-information, such as depth information, pixel/region affinity map
etc. In this work, we address this problem by incorporating background
information to guide the learning of the notion of objectness. Specifically, we
propose a novel framework to discover background regions in an image and train
an object proposal network to not detect any objects in these regions. We
formulate the background discovery task as that of identifying image regions
that are not discriminative, i.e., those that are redundant and constitute low
information content. We conduct experiments on standard benchmarks to showcase
the effectiveness of our proposed approach and observe significant improvements
over the previous state-of-the-art approaches for this task.

</details>


### [125] [A Guide to Structureless Visual Localization](https://arxiv.org/abs/2504.17636)
*Vojtech Panek, Qunjie Zhou, Yaqing Ding, Sérgio Agostinho, Zuzana Kukelova, Torsten Sattler, Laura Leal-Taixé*

Main category: cs.CV

TL;DR: The paper provides the first comprehensive discussion and comparison of structureless visual localization methods, highlighting their flexibility and slightly lower accuracy compared to structure-based approaches.


<details>
  <summary>Details</summary>
Motivation: Visual localization is crucial for applications like self-driving cars and AR/VR, but structure-based methods lack flexibility. Structureless methods, though less studied, offer easier updates.

Method: The paper reviews and compares structureless methods, focusing on classical geometric reasoning versus newer pose regression techniques.

Result: Classical geometric methods outperform pose regression in accuracy. Structureless methods are more flexible but slightly less accurate than structure-based ones.

Conclusion: Structureless methods offer a promising trade-off between flexibility and accuracy, suggesting future work to bridge the gap with structure-based approaches.

Abstract: Visual localization algorithms, i.e., methods that estimate the camera pose
of a query image in a known scene, are core components of many applications,
including self-driving cars and augmented / mixed reality systems.
State-of-the-art visual localization algorithms are structure-based, i.e., they
store a 3D model of the scene and use 2D-3D correspondences between the query
image and 3D points in the model for camera pose estimation. While such
approaches are highly accurate, they are also rather inflexible when it comes
to adjusting the underlying 3D model after changes in the scene. Structureless
localization approaches represent the scene as a database of images with known
poses and thus offer a much more flexible representation that can be easily
updated by adding or removing images. Although there is a large amount of
literature on structure-based approaches, there is significantly less work on
structureless methods. Hence, this paper is dedicated to providing the, to the
best of our knowledge, first comprehensive discussion and comparison of
structureless methods. Extensive experiments show that approaches that use a
higher degree of classical geometric reasoning generally achieve higher pose
accuracy. In particular, approaches based on classical absolute or
semi-generalized relative pose estimation outperform very recent methods based
on pose regression by a wide margin. Compared with state-of-the-art
structure-based approaches, the flexibility of structureless methods comes at
the cost of (slightly) lower pose accuracy, indicating an interesting direction
for future work.

</details>


### [126] [CLIPSE -- a minimalistic CLIP-based image search engine for research](https://arxiv.org/abs/2504.17643)
*Steve Göring*

Main category: cs.CV

TL;DR: CLIPSE is a self-hosted image search engine using CLIP embeddings for images and text queries, designed for simplicity and extensibility. It handles smaller datasets well, but larger datasets may require a distributed approach.


<details>
  <summary>Details</summary>
Motivation: To provide a simple, extensible framework for image search using CLIP embeddings, catering to research applications.

Method: Uses CLIP embeddings for processing images and text queries, evaluated in benchmark scenarios for indexing and querying time.

Result: Effective for smaller datasets; larger datasets need a distributed setup.

Conclusion: CLIPSE is a practical solution for small-scale image search, with scalability options for larger datasets.

Abstract: A brief overview of CLIPSE, a self-hosted image search engine with the main
application of research, is provided. In general, CLIPSE uses CLIP embeddings
to process the images and also the text queries. The overall framework is
designed with simplicity to enable easy extension and usage. Two benchmark
scenarios are described and evaluated, covering indexing and querying time. It
is shown that CLIPSE is capable of handling smaller datasets; for larger
datasets, a distributed approach with several instances should be considered.

</details>


### [127] [DiMeR: Disentangled Mesh Reconstruction Model](https://arxiv.org/abs/2504.17670)
*Lutao Jiang, Jiantao Lin, Kanghao Chen, Wenhang Ge, Xin Yang, Yifan Jiang, Yuanhuiyi Lyu, Xu Zheng, Yingcong Chen*

Main category: cs.CV

TL;DR: DiMeR introduces a disentangled dual-stream model for sparse-view mesh reconstruction, using normal maps for geometry and RGB images for texture, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: RGB images in 3D reconstruction often cause conflicting training objectives and lack clarity for geometry, prompting the need for a disentangled approach.

Method: DiMeR disentangles input and framework into geometry (using normal maps) and texture (using RGB images) streams, improving mesh extraction and supervision.

Result: DiMeR outperforms previous methods by over 30% in Chamfer Distance on GSO and OmniObject3D datasets, excelling in sparse-view, single-image, and text-to-3D tasks.

Conclusion: DiMeR's disentangled approach effectively addresses training conflicts and improves reconstruction accuracy, setting a new benchmark in 3D generative models.

Abstract: With the advent of large-scale 3D datasets, feed-forward 3D generative
models, such as the Large Reconstruction Model (LRM), have gained significant
attention and achieved remarkable success. However, we observe that RGB images
often lead to conflicting training objectives and lack the necessary clarity
for geometry reconstruction. In this paper, we revisit the inductive biases
associated with mesh reconstruction and introduce DiMeR, a novel disentangled
dual-stream feed-forward model for sparse-view mesh reconstruction. The key
idea is to disentangle both the input and framework into geometry and texture
parts, thereby reducing the training difficulty for each part according to the
Principle of Occam's Razor. Given that normal maps are strictly consistent with
geometry and accurately capture surface variations, we utilize normal maps as
exclusive input for the geometry branch to reduce the complexity between the
network's input and output. Moreover, we improve the mesh extraction algorithm
to introduce 3D ground truth supervision. As for texture branch, we use RGB
images as input to obtain the textured mesh. Overall, DiMeR demonstrates robust
capabilities across various tasks, including sparse-view reconstruction,
single-image-to-3D, and text-to-3D. Numerous experiments show that DiMeR
significantly outperforms previous methods, achieving over 30% improvement in
Chamfer Distance on the GSO and OmniObject3D dataset.

</details>


### [128] [PICO: Reconstructing 3D People In Contact with Objects](https://arxiv.org/abs/2504.17695)
*Alpár Cseke, Shashank Tripathi, Sai Kumar Dwivedi, Arjun Lakshmipathy, Agniv Chatterjee, Michael J. Black, Dimitrios Tzionas*

Main category: cs.CV

TL;DR: The paper introduces PICO-db, a dataset for 3D Human-Object Interaction (HOI) recovery from natural images, and PICO-fit, a method leveraging this dataset for accurate 3D mesh fitting.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D HOI recovery are limited to controlled settings and specific object classes. The goal is to generalize to natural images and novel objects.

Method: (1) Create PICO-db by pairing natural images with dense 3D contact labels on body and object meshes. (2) Develop PICO-fit, a render-and-compare method using contact correspondences to fit 3D meshes to images.

Result: PICO-fit outperforms existing methods, handling diverse object categories and enabling scalable HOI understanding.

Conclusion: The approach advances 3D HOI recovery by generalizing to natural scenes and novel objects, with publicly available data and code.

Abstract: Recovering 3D Human-Object Interaction (HOI) from single color images is
challenging due to depth ambiguities, occlusions, and the huge variation in
object shape and appearance. Thus, past work requires controlled settings such
as known object shapes and contacts, and tackles only limited object classes.
Instead, we need methods that generalize to natural images and novel object
classes. We tackle this in two main ways: (1) We collect PICO-db, a new dataset
of natural images uniquely paired with dense 3D contact on both body and object
meshes. To this end, we use images from the recent DAMON dataset that are
paired with contacts, but these contacts are only annotated on a canonical 3D
body. In contrast, we seek contact labels on both the body and the object. To
infer these given an image, we retrieve an appropriate 3D object mesh from a
database by leveraging vision foundation models. Then, we project DAMON's body
contact patches onto the object via a novel method needing only 2 clicks per
patch. This minimal human input establishes rich contact correspondences
between bodies and objects. (2) We exploit our new dataset of contact
correspondences in a novel render-and-compare fitting method, called PICO-fit,
to recover 3D body and object meshes in interaction. PICO-fit infers contact
for the SMPL-X body, retrieves a likely 3D object mesh and contact from PICO-db
for that object, and uses the contact to iteratively fit the 3D body and object
meshes to image evidence via optimization. Uniquely, PICO-fit works well for
many object categories that no existing method can tackle. This is crucial to
enable HOI understanding to scale in the wild. Our data and code are available
at https://pico.is.tue.mpg.de.

</details>


### [129] [Hierarchical and Multimodal Data for Daily Activity Understanding](https://arxiv.org/abs/2504.17696)
*Ghazal Kaviani, Yavuz Yarici, Seulgi Kim, Mohit Prabhushankar, Ghassan AlRegib, Mashhour Solh, Ameya Patil*

Main category: cs.CV

TL;DR: DARai is a multimodal dataset with hierarchical annotations for understanding human activities, featuring 200+ hours of data from 20 sensors across 10 environments. It supports tasks like recognition, localization, and anticipation, highlighting sensor limitations.


<details>
  <summary>Details</summary>
Motivation: To capture the complexity of human activities in real-world settings and enable research in human-centered AI applications.

Method: Constructed a dataset with 50 participants in 10 environments, annotated hierarchically (L1-L3). Conducted unimodal/multimodal experiments for recognition, localization, and anticipation.

Result: DARai reveals challenges in sensor fusion and domain variance, with 22.7% L2 and 14.2% L3 overlaps.

Conclusion: DARai is a valuable resource for AI research, offering insights into human activity understanding and sensor limitations.

Abstract: Daily Activity Recordings for Artificial Intelligence (DARai, pronounced
"Dahr-ree") is a multimodal, hierarchically annotated dataset constructed to
understand human activities in real-world settings. DARai consists of
continuous scripted and unscripted recordings of 50 participants in 10
different environments, totaling over 200 hours of data from 20 sensors
including multiple camera views, depth and radar sensors, wearable inertial
measurement units (IMUs), electromyography (EMG), insole pressure sensors,
biomonitor sensors, and gaze tracker.
  To capture the complexity in human activities, DARai is annotated at three
levels of hierarchy: (i) high-level activities (L1) that are independent tasks,
(ii) lower-level actions (L2) that are patterns shared between activities, and
(iii) fine-grained procedures (L3) that detail the exact execution steps for
actions. The dataset annotations and recordings are designed so that 22.7% of
L2 actions are shared between L1 activities and 14.2% of L3 procedures are
shared between L2 actions. The overlap and unscripted nature of DARai allows
counterfactual activities in the dataset.
  Experiments with various machine learning models showcase the value of DARai
in uncovering important challenges in human-centered applications.
Specifically, we conduct unimodal and multimodal sensor fusion experiments for
recognition, temporal localization, and future action anticipation across all
hierarchical annotation levels. To highlight the limitations of individual
sensors, we also conduct domain-variant experiments that are enabled by DARai's
multi-sensor and counterfactual activity design setup.
  The code, documentation, and dataset are available at the dedicated DARai
website:
https://alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/

</details>


### [130] [Generative Fields: Uncovering Hierarchical Feature Control for StyleGAN via Inverted Receptive Fields](https://arxiv.org/abs/2504.17712)
*Zhuo He, Paul Henderson, Nicolas Pugeault*

Main category: cs.CV

TL;DR: The paper introduces 'generative fields' to improve feature control in StyleGAN, proposing a new editing pipeline using the S latent space for disentangled control.


<details>
  <summary>Details</summary>
Motivation: Controlling features in GAN-generated images is challenging due to latent space entanglement. Existing methods like W space are limited in expressivity and require pre-training.

Method: The paper introduces 'generative fields' inspired by CNN receptive fields and proposes an editing pipeline using the S latent space for direct feature control.

Result: The method achieves disentangled control of feature synthesis in StyleGAN, leveraging CNN structural features.

Conclusion: Generative fields and the S latent space provide a more effective way to control StyleGAN's feature synthesis, overcoming limitations of previous approaches.

Abstract: StyleGAN has demonstrated the ability of GANs to synthesize highly-realistic
faces of imaginary people from random noise. One limitation of GAN-based image
generation is the difficulty of controlling the features of the generated
image, due to the strong entanglement of the low-dimensional latent space.
Previous work that aimed to control StyleGAN with image or text prompts
modulated sampling in W latent space, which is more expressive than Z latent
space. However, W space still has restricted expressivity since it does not
control the feature synthesis directly; also the feature embedding in W space
requires a pre-training process to reconstruct the style signal, limiting its
application. This paper introduces the concept of "generative fields" to
explain the hierarchical feature synthesis in StyleGAN, inspired by the
receptive fields of convolution neural networks (CNNs). Additionally, we
propose a new image editing pipeline for StyleGAN using generative field theory
and the channel-wise style latent space S, utilizing the intrinsic structural
feature of CNNs to achieve disentangled control of feature synthesis at
synthesis time.

</details>


### [131] [DPMambaIR:All-in-One Image Restoration via Degradation-Aware Prompt State Space Model](https://arxiv.org/abs/2504.17732)
*Zhanwen Liu, Sai Zhou, Yuchao Dai, Yang Wang, Yisheng An, Xiangmo Zhao*

Main category: cs.CV

TL;DR: DPMambaIR is a novel All-in-One image restoration framework that integrates fine-grained degradation modeling and high-frequency enhancement, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing All-in-One image restoration methods lack fine-grained degradation modeling and struggle with multi-task conflicts, limiting their effectiveness.

Method: DPMambaIR combines a Degradation-Aware Prompt State Space Model (DP-SSM) for fine-grained degradation modeling and a High-Frequency Enhancement Block (HEB) to preserve details.

Result: Experiments show DPMambaIR achieves 27.69dB PSNR and 0.893 SSIM, outperforming other methods on a mixed dataset with seven degradation types.

Conclusion: DPMambaIR demonstrates superior performance as a unified solution for All-in-One image restoration, addressing key limitations of existing approaches.

Abstract: All-in-One image restoration aims to address multiple image degradation
problems using a single model, significantly reducing training costs and
deployment complexity compared to traditional methods that design dedicated
models for each degradation type. Existing approaches typically rely on
Degradation-specific models or coarse-grained degradation prompts to guide
image restoration. However, they lack fine-grained modeling of degradation
information and face limitations in balancing multi-task conflicts. To overcome
these limitations, we propose DPMambaIR, a novel All-in-One image restoration
framework. By integrating a Degradation-Aware Prompt State Space Model (DP-SSM)
and a High-Frequency Enhancement Block (HEB), DPMambaIR enables fine-grained
modeling of complex degradation information and efficient global integration,
while mitigating the loss of high-frequency details caused by task competition.
Specifically, the DP-SSM utilizes a pre-trained degradation extractor to
capture fine-grained degradation features and dynamically incorporates them
into the state space modeling process, enhancing the model's adaptability to
diverse degradation types. Concurrently, the HEB supplements high-frequency
information, effectively addressing the loss of critical details, such as edges
and textures, in multi-task image restoration scenarios. Extensive experiments
on a mixed dataset containing seven degradation types show that DPMambaIR
achieves the best performance, with 27.69dB and 0.893 in PSNR and SSIM,
respectively. These results highlight the potential and superiority of
DPMambaIR as a unified solution for All-in-One image restoration.

</details>


### [132] [EgoCHARM: Resource-Efficient Hierarchical Activity Recognition using an Egocentric IMU Sensor](https://arxiv.org/abs/2504.17735)
*Akhil Padmanabha, Saravanan Govindarajan, Hwanmun Kim, Sergio Ortiz, Rahul Rajan, Doruk Senkal, Sneha Kadetotad*

Main category: cs.CV

TL;DR: EgoCHARM is a resource-efficient ML algorithm for egocentric activity recognition using a single IMU, achieving high F1 scores with minimal parameters.


<details>
  <summary>Details</summary>
Motivation: Current HAR methods on smartglasses are either low-performing or resource-intensive, limiting practical applications.

Method: EgoCHARM uses a hierarchical, semi-supervised learning strategy to recognize high and low level activities with minimal labeled data.

Result: Achieved F1 scores of 0.826 (high level) and 0.855 (low level) with just 63k and 22k parameters, respectively.

Conclusion: EgoCHARM is efficient and deployable on current IMU chips, though sensitivity analysis reveals limitations for egocentric IMU-based recognition.

Abstract: Human activity recognition (HAR) on smartglasses has various use cases,
including health/fitness tracking and input for context-aware AI assistants.
However, current approaches for egocentric activity recognition suffer from low
performance or are resource-intensive. In this work, we introduce a resource
(memory, compute, power, sample) efficient machine learning algorithm,
EgoCHARM, for recognizing both high level and low level activities using a
single egocentric (head-mounted) Inertial Measurement Unit (IMU). Our
hierarchical algorithm employs a semi-supervised learning strategy, requiring
primarily high level activity labels for training, to learn generalizable low
level motion embeddings that can be effectively utilized for low level activity
recognition. We evaluate our method on 9 high level and 3 low level activities
achieving 0.826 and 0.855 F1 scores on high level and low level activity
recognition respectively, with just 63k high level and 22k low level model
parameters, allowing the low level encoder to be deployed directly on current
IMU chips with compute. Lastly, we present results and insights from a
sensitivity analysis and highlight the opportunities and limitations of
activity recognition using egocentric IMUs.

</details>


### [133] [Step1X-Edit: A Practical Framework for General Image Editing](https://arxiv.org/abs/2504.17761)
*Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, Guopeng Li, Yuang Peng, Quan Sun, Jingwei Wu, Yan Cai, Zheng Ge, Ranchen Ming, Lei Xia, Xianfang Zeng, Yibo Zhu, Binxing Jiao, Xiangyu Zhang, Gang Yu, Daxin Jiang*

Main category: cs.CV

TL;DR: The paper introduces Step1X-Edit, an open-source image editing model that rivals proprietary models like GPT-4o and Gemini2 Flash, using Multimodal LLM and diffusion techniques.


<details>
  <summary>Details</summary>
Motivation: To bridge the performance gap between open-source and closed-source image editing models.

Method: Uses Multimodal LLM for processing images and instructions, integrates latent embeddings with a diffusion decoder, and trains on a high-quality generated dataset.

Result: Step1X-Edit surpasses open-source baselines and nears proprietary model performance on the GEdit-Bench benchmark.

Conclusion: Step1X-Edit advances open-source image editing, offering competitive performance against leading proprietary models.

Abstract: In recent years, image editing models have witnessed remarkable and rapid
development. The recent unveiling of cutting-edge multimodal models such as
GPT-4o and Gemini2 Flash has introduced highly promising image editing
capabilities. These models demonstrate an impressive aptitude for fulfilling a
vast majority of user-driven editing requirements, marking a significant
advancement in the field of image manipulation. However, there is still a large
gap between the open-source algorithm with these closed-source models. Thus, in
this paper, we aim to release a state-of-the-art image editing model, called
Step1X-Edit, which can provide comparable performance against the closed-source
models like GPT-4o and Gemini2 Flash. More specifically, we adopt the
Multimodal LLM to process the reference image and the user's editing
instruction. A latent embedding has been extracted and integrated with a
diffusion image decoder to obtain the target image. To train the model, we
build a data generation pipeline to produce a high-quality dataset. For
evaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world
user instructions. Experimental results on GEdit-Bench demonstrate that
Step1X-Edit outperforms existing open-source baselines by a substantial margin
and approaches the performance of leading proprietary models, thereby making
significant contributions to the field of image editing.

</details>


### [134] [The Fourth Monocular Depth Estimation Challenge](https://arxiv.org/abs/2504.17787)
*Anton Obukhov, Matteo Poggi, Fabio Tosi, Ripudaman Singh Arora, Jaime Spencer, Chris Russell, Simon Hadfield, Richard Bowden, Shuaihang Wang, Zhenxin Ma, Weijie Chen, Baobei Xu, Fengyu Sun, Di Xie, Jiang Zhu, Mykola Lavreniuk, Haining Guan, Qun Wu, Yupei Zeng, Chao Lu, Huanran Wang, Guangyuan Zhou, Haotian Zhang, Jianxiong Wang, Qiang Rao, Chunjie Wang, Xiao Liu, Zhiqiang Lou, Hualie Jiang, Yihao Chen, Rui Xu, Minglang Tan, Zihan Qin, Yifan Mao, Jiayang Liu, Jialei Xu, Yifan Yang, Wenbo Zhao, Junjun Jiang, Xianming Liu, Mingshuai Zhao, Anlong Ming, Wu Chen, Feng Xue, Mengying Yu, Shida Gao, Xiangfeng Wang, Gbenga Omotara, Ramy Farag, Jacket Demby, Seyed Mohamad Ali Tousi, Guilherme N DeSouza, Tuan-Anh Yang, Minh-Quang Nguyen, Thien-Phuc Tran, Albert Luginov, Muhammad Shahzad*

Main category: cs.CV

TL;DR: The fourth MDEC focused on zero-shot generalization to SYNS-Patches, revised evaluation protocols, and included new baselines. Winning submissions improved 3D F-Score from 22.58% to 23.05%.


<details>
  <summary>Details</summary>
Motivation: To advance monocular depth estimation by testing zero-shot generalization in challenging environments and refining evaluation methods.

Method: Revised evaluation protocol with least-squares alignment, included off-the-shelf methods (Depth Anything v2, Marigold), and analyzed 24 submissions.

Result: 10 submissions with reports outperformed baselines, with top methods using affine-invariant predictions. 3D F-Score improved to 23.05%.

Conclusion: The challenge demonstrated progress in monocular depth estimation, with affine-invariant methods leading to better performance.

Abstract: This paper presents the results of the fourth edition of the Monocular Depth
Estimation Challenge (MDEC), which focuses on zero-shot generalization to the
SYNS-Patches benchmark, a dataset featuring challenging environments in both
natural and indoor settings. In this edition, we revised the evaluation
protocol to use least-squares alignment with two degrees of freedom to support
disparity and affine-invariant predictions. We also revised the baselines and
included popular off-the-shelf methods: Depth Anything v2 and Marigold. The
challenge received a total of 24 submissions that outperformed the baselines on
the test set; 10 of these included a report describing their approach, with
most leading methods relying on affine-invariant predictions. The challenge
winners improved the 3D F-Score over the previous edition's best result,
raising it from 22.58% to 23.05%.

</details>


### [135] [Dynamic Camera Poses and Where to Find Them](https://arxiv.org/abs/2504.17788)
*Chris Rockwell, Joseph Tung, Tsung-Yi Lin, Ming-Yu Liu, David F. Fouhey, Chen-Hsuan Lin*

Main category: cs.CV

TL;DR: DynPose-100K is a large-scale dataset of dynamic Internet videos annotated with camera poses, addressing challenges in pose estimation for such videos.


<details>
  <summary>Details</summary>
Motivation: Annotating camera poses on dynamic Internet videos is difficult but crucial for fields like realistic video generation and simulation.

Method: The pipeline combines task-specific and generalist models for filtering, and integrates point tracking, dynamic masking, and structure-from-motion for pose estimation.

Result: DynPose-100K is large-scale and diverse, outperforming state-of-the-art methods.

Conclusion: The dataset enables advancements in downstream applications by providing high-quality annotated dynamic videos.

Abstract: Annotating camera poses on dynamic Internet videos at scale is critical for
advancing fields like realistic video generation and simulation. However,
collecting such a dataset is difficult, as most Internet videos are unsuitable
for pose estimation. Furthermore, annotating dynamic Internet videos present
significant challenges even for state-of-theart methods. In this paper, we
introduce DynPose-100K, a large-scale dataset of dynamic Internet videos
annotated with camera poses. Our collection pipeline addresses filtering using
a carefully combined set of task-specific and generalist models. For pose
estimation, we combine the latest techniques of point tracking, dynamic
masking, and structure-from-motion to achieve improvements over the
state-of-the-art approaches. Our analysis and experiments demonstrate that
DynPose-100K is both large-scale and diverse across several key attributes,
opening up avenues for advancements in various downstream applications.

</details>


### [136] [Token-Shuffle: Towards High-Resolution Image Generation with Autoregressive Models](https://arxiv.org/abs/2504.17789)
*Xu Ma, Peize Sun, Haoyu Ma, Hao Tang, Chih-Yao Ma, Jialiang Wang, Kunpeng Li, Xiaoliang Dai, Yujun Shi, Xuan Ju, Yushi Hu, Artsiom Sanakoyeu, Felix Juefei-Xu, Ji Hou, Junjiao Tian, Tao Xu, Tingbo Hou, Yen-Cheng Liu, Zecheng He, Zijian He, Matt Feiszli, Peizhao Zhang, Peter Vajda, Sam Tsai, Yun Fu*

Main category: cs.CV

TL;DR: Token-Shuffle reduces image tokens in AR models for efficient high-resolution image synthesis, outperforming AR and diffusion models.


<details>
  <summary>Details</summary>
Motivation: AR models for image synthesis are limited by high token counts, reducing efficiency and resolution. Token-Shuffle addresses this by leveraging visual vocabulary redundancy in MLLMs.

Method: Token-Shuffle merges local tokens along channels (token-shuffle) and restores spatial arrangement (token-unshuffle) in Transformer blocks, enabling efficient high-resolution synthesis.

Result: Achieves 2048x2048 resolution, outperforming AR (LlamaGen) and diffusion (LDM) models in benchmarks and human evaluations.

Conclusion: Token-Shuffle is a foundational design for efficient high-resolution image generation in MLLMs.

Abstract: Autoregressive (AR) models, long dominant in language generation, are
increasingly applied to image synthesis but are often considered less
competitive than Diffusion-based models. A primary limitation is the
substantial number of image tokens required for AR models, which constrains
both training and inference efficiency, as well as image resolution. To address
this, we present Token-Shuffle, a novel yet simple method that reduces the
number of image tokens in Transformer. Our key insight is the dimensional
redundancy of visual vocabularies in Multimodal Large Language Models (MLLMs),
where low-dimensional visual codes from visual encoder are directly mapped to
high-dimensional language vocabularies. Leveraging this, we consider two key
operations: token-shuffle, which merges spatially local tokens along channel
dimension to decrease the input token number, and token-unshuffle, which
untangles the inferred tokens after Transformer blocks to restore the spatial
arrangement for output. Jointly training with textual prompts, our strategy
requires no additional pretrained text-encoder and enables MLLMs to support
extremely high-resolution image synthesis in a unified next-token prediction
way while maintaining efficient training and inference. For the first time, we
push the boundary of AR text-to-image generation to a resolution of 2048x2048
with gratifying generation performance. In GenAI-benchmark, our 2.7B model
achieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen
by 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human
evaluations also demonstrate our prominent image generation ability in terms of
text-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle
can serve as a foundational design for efficient high-resolution image
generation within MLLMs.

</details>


### [137] [LiDPM: Rethinking Point Diffusion for Lidar Scene Completion](https://arxiv.org/abs/2504.17791)
*Tetiana Martyniuk, Gilles Puy, Alexandre Boulch, Renaud Marlet, Raoul de Charette*

Main category: cs.CV

TL;DR: LiDPM bridges the gap between local and object-level diffusion models for lidar scene completion, showing vanilla DDPMs with a good starting point suffice for better results.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generating fine-grained details in lidar point clouds for outdoor scenes using diffusion models.

Method: Uses a vanilla DDPM with a well-chosen starting point, avoiding approximations in local diffusion formulations.

Result: Achieves better scene completion results on SemanticKITTI compared to prior methods.

Conclusion: A vanilla DDPM approach is effective for lidar scene completion without needing local diffusion approximations.

Abstract: Training diffusion models that work directly on lidar points at the scale of
outdoor scenes is challenging due to the difficulty of generating fine-grained
details from white noise over a broad field of view. The latest works
addressing scene completion with diffusion models tackle this problem by
reformulating the original DDPM as a local diffusion process. It contrasts with
the common practice of operating at the level of objects, where vanilla DDPMs
are currently used. In this work, we close the gap between these two lines of
work. We identify approximations in the local diffusion formulation, show that
they are not required to operate at the scene level, and that a vanilla DDPM
with a well-chosen starting point is enough for completion. Finally, we
demonstrate that our method, LiDPM, leads to better results in scene completion
on SemanticKITTI. The project page is https://astra-vision.github.io/LiDPM .

</details>


### [138] [ARF-Plus: Controlling Perceptual Factors in Artistic Radiance Fields for 3D Scene Stylization](https://arxiv.org/abs/2308.12452)
*Wenzhao Li, Tianhao Wu, Fangcheng Zhong, Cengiz Oztireli*

Main category: cs.CV

TL;DR: ARF-Plus introduces a 3D neural style transfer framework with four perceptual controls for enhanced stylization of 3D scenes.


<details>
  <summary>Details</summary>
Motivation: Addresses the lack of perceptual controllability in radiance fields style transfer, inspired by 2D image style transfer concepts.

Method: Proposes ARF-Plus, integrating four controls: color preservation, style pattern scale, spatial selective stylization, and depth enhancement.

Result: Demonstrates successful perceptual control in 3D scene stylization, applicable to single or multiple styles.

Conclusion: Enables customizable and novel stylistic effects in 3D scenes, expanding creative possibilities.

Abstract: The radiance fields style transfer is an emerging field that has recently
gained popularity as a means of 3D scene stylization, thanks to the outstanding
performance of neural radiance fields in 3D reconstruction and view synthesis.
We highlight a research gap in radiance fields style transfer, the lack of
sufficient perceptual controllability, motivated by the existing concept in the
2D image style transfer. In this paper, we present ARF-Plus, a 3D neural style
transfer framework offering manageable control over perceptual factors, to
systematically explore the perceptual controllability in 3D scene stylization.
Four distinct types of controls - color preservation control, (style pattern)
scale control, spatial (selective stylization area) control, and depth
enhancement control - are proposed and integrated into this framework. Results
from real-world datasets, both quantitative and qualitative, show that the four
types of controls in our ARF-Plus framework successfully accomplish their
corresponding perceptual controls when stylizing 3D scenes. These techniques
work well for individual style inputs as well as for the simultaneous
application of multiple styles within a scene. This unlocks a realm of
limitless possibilities, allowing customized modifications of stylization
effects and flexible merging of the strengths of different styles, ultimately
enabling the creation of novel and eye-catching stylistic effects on 3D scenes.

</details>


### [139] [Event-based Continuous Color Video Decompression from Single Frames](https://arxiv.org/abs/2312.00113)
*Ziyun Wang, Friedhelm Hamann, Kenneth Chaney, Wen Jiang, Guillermo Gallego, Kostas Daniilidis*

Main category: cs.CV

TL;DR: ContinuityCam generates continuous video from a single RGB image and event camera data, outperforming baselines in quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: High-speed motion capture is limited by conventional cameras' bandwidth and dynamic range; event cameras offer a solution with compressed, high-temporal-resolution data.

Method: Combines continuous long-range motion modeling with neural synthesis to predict frames at arbitrary times using an initial image and event data.

Result: Outperforms baselines by 3.61 dB in PSNR and 33% in LPIPS, with superior performance in downstream tasks.

Conclusion: The method is robust, efficient, and validated by a novel dataset and hardware setup.

Abstract: We present ContinuityCam, a novel approach to generate a continuous video
from a single static RGB image and an event camera stream. Conventional cameras
struggle with high-speed motion capture due to bandwidth and dynamic range
limitations. Event cameras are ideal sensors to solve this problem because they
encode compressed change information at high temporal resolution. In this work,
we tackle the problem of event-based continuous color video decompression,
pairing single static color frames and event data to reconstruct temporally
continuous videos. Our approach combines continuous long-range motion modeling
with a neural synthesis model, enabling frame prediction at arbitrary times
within the events. Our method only requires an initial image, thus increasing
the robustness to sudden motions, light changes, minimizing the prediction
latency, and decreasing bandwidth usage. We also introduce a novel single-lens
beamsplitter setup that acquires aligned images and events, and a novel and
challenging Event Extreme Decompression Dataset (E2D2) that tests the method in
various lighting and motion profiles. We thoroughly evaluate our method by
benchmarking color frame reconstruction, outperforming the baseline methods by
3.61 dB in PSNR and by 33% decrease in LPIPS, as well as showing superior
results on two downstream tasks.

</details>


### [140] [Fast OMP for Exact Recovery and Sparse Approximation](https://arxiv.org/abs/2404.00146)
*Huiyuan Yu, Jia He, Maggie Cheng*

Main category: cs.CV

TL;DR: The paper improves Orthogonal Matching Pursuit (OMP) by introducing a faster projection method and a new greedy selection criterion, reducing computational complexity and iteration count while maintaining recovery guarantees.


<details>
  <summary>Details</summary>
Motivation: OMP is computationally inefficient for signals with many non-zeros, limiting its practicality. The paper aims to enhance OMP's speed and efficiency.

Method: Proposes a fast orthogonal projection algorithm and a new greedy selection criterion to reduce iterations and computation time.

Result: Experiments show significant speed improvements over classical OMP, with maintained recovery guarantees and comparable approximation error.

Conclusion: The modified OMP achieves faster signal recovery with fewer iterations, making it more practical for large-scale applications.

Abstract: Orthogonal Matching Pursuit (OMP) has been a powerful method in sparse signal
recovery and approximation. However OMP suffers computational issue when the
signal has large number of non-zeros. This paper advances OMP in two fronts: it
offers a fast algorithm for the orthogonal projection of the input signal at
each iteration, and a new selection criterion for making the greedy choice,
which reduces the number of iterations it takes to recover the signal. The
proposed modifications to OMP directly reduce the computational complexity.
Experiment results show significant improvement over the classical OMP in
computation time. The paper also provided a sufficient condition for exact
recovery under the new greedy choice criterion. For general signals that may
not have sparse representations, the paper provides a bound for the
approximation error. The approximation error is at the same order as OMP but is
obtained within fewer iterations and less time.

</details>


### [141] [ObjectAdd: Adding Objects into Image via a Training-Free Diffusion Modification Fashion](https://arxiv.org/abs/2404.17230)
*Ziyue Zhang, Mingbao Lin, Quanjian Song, Yuxin Zhang, Rongrong Ji*

Main category: cs.CV

TL;DR: ObjectAdd is a training-free diffusion method for adding objects into specified areas of an image while maintaining consistency.


<details>
  <summary>Details</summary>
Motivation: The method addresses the difficulty of describing everything in one prompt and the need to add objects post-generation.

Method: Uses embedding-level concatenation, object-driven layout control, and prompted image inpainting for seamless object addition.

Result: Achieves accurate object placement, unchanged content outside the box, and flawless fusion between areas.

Conclusion: ObjectAdd effectively adds objects to images while preserving consistency and user control.

Abstract: We introduce ObjectAdd, a training-free diffusion modification method to add
user-expected objects into user-specified area. The motive of ObjectAdd stems
from: first, describing everything in one prompt can be difficult, and second,
users often need to add objects into the generated image. To accommodate with
real world, our ObjectAdd maintains accurate image consistency after adding
objects with technical innovations in: (1) embedding-level concatenation to
ensure correct text embedding coalesce; (2) object-driven layout control with
latent and attention injection to ensure objects accessing user-specified area;
(3) prompted image inpainting in an attention refocusing & object expansion
fashion to ensure rest of the image stays the same. With a text-prompted image,
our ObjectAdd allows users to specify a box and an object, and achieves: (1)
adding object inside the box area; (2) exact content outside the box area; (3)
flawless fusion between the two areas

</details>


### [142] [AI in Lung Health: Benchmarking Detection and Diagnostic Models Across Multiple CT Scan Datasets](https://arxiv.org/abs/2405.04605)
*Fakrul Islam Tushar, Avivah Wang, Lavsen Dahal, Michael R. Harowicz, Kyle J. Lafata, Tina D. Tailor, Joseph Y. Lo*

Main category: cs.CV

TL;DR: The paper introduces the Duke Lung Cancer Screening (DLCS) Dataset, a large open-access LDCT dataset, and benchmarks deep learning models for nodule detection and lung cancer classification, achieving strong generalizability.


<details>
  <summary>Details</summary>
Motivation: Early detection of lung cancer via LDCT is crucial, and robust AI models require large, annotated datasets. The DLCS Dataset addresses this need.

Method: Developed MONAI-based RetinaNet models for nodule detection and compared five models for classification, including the novel SWS++ model.

Result: Models showed strong generalizability, with SWS++ performing comparably or better than existing models (AUC: 0.71 to 0.90).

Conclusion: The work provides a standardized benchmarking resource for lung cancer AI research, promoting reproducibility and future advancements.

Abstract: Lung cancer remains the leading cause of cancer-related mortality worldwide,
and early detection through low-dose computed tomography (LDCT) has shown
significant promise in reducing death rates. With the growing integration of
artificial intelligence (AI) into medical imaging, the development and
evaluation of robust AI models require access to large, well-annotated
datasets. In this study, we introduce the utility of Duke Lung Cancer Screening
(DLCS) Dataset, the largest open-access LDCT dataset with over 2,000 scans and
3,000 expert-verified nodules. We benchmark deep learning models for both 3D
nodule detection and lung cancer classification across internal and external
datasets including LUNA16, LUNA25, and NLST-3D+. For detection, we develop two
MONAI-based RetinaNet models (DLCSDmD and LUNA16-mD), evaluated using the
Competition Performance Metric (CPM). For classification, we compare five
models, including state-of-the-art pretrained models (Models Genesis, Med3D), a
selfsupervised foundation model (FMCB), a randomly initialized ResNet50, and
proposed a novel Strategic Warm-Start++ (SWS++) model. SWS++ uses curated
candidate patches to pretrain a classification backbone within the same
detection pipeline, enabling task-relevant feature learning. Our models
demonstrated strong generalizability, with SWS++ achieving comparable or
superior performance to existing foundational models across multiple datasets
(AUC: 0.71 to 0.90). All code, models, and data are publicly released to
promote reproducibility and collaboration. This work establishes a standardized
benchmarking resource for lung cancer AI research, supporting future efforts in
model development, validation, and clinical translation.

</details>


### [143] [Discrete Cosine Transform Based Decorrelated Attention for Vision Transformers](https://arxiv.org/abs/2405.13901)
*Hongyi Pan, Emadeldeen Hamdan, Xin Zhu, Ahmet Enis Cetin, Ulas Bagci*

Main category: cs.CV

TL;DR: The paper introduces DCT-based initialization and compression for Vision Transformers, improving accuracy and reducing computational overhead.


<details>
  <summary>Details</summary>
Motivation: Training self-attention weights from random initialization is challenging, and traditional methods may not be optimal.

Method: Proposes DCT-based initialization for attention weights and DCT-based compression for attention function, truncating high-frequency components.

Result: DCT initialization boosts Vision Transformers' accuracy; DCT compression reduces weight matrix size and computational cost without sacrificing accuracy.

Conclusion: DCT-based techniques offer robust initialization and efficient compression for Vision Transformers, enhancing performance and reducing overhead.

Abstract: Central to the Transformer architectures' effectiveness is the self-attention
mechanism, a function that maps queries, keys, and values into a
high-dimensional vector space. However, training the attention weights of
queries, keys, and values is non-trivial from a state of random initialization.
In this paper, we propose two methods. (i) We first address the initialization
problem of Vision Transformers by introducing a simple, yet highly innovative,
initialization approach utilizing discrete cosine transform (DCT) coefficients.
Our proposed DCT-based \textit{attention} initialization marks a significant
gain compared to traditional initialization strategies; offering a robust
foundation for the attention mechanism. Our experiments reveal that the
DCT-based initialization enhances the accuracy of Vision Transformers in
classification tasks. (ii) We also recognize that since DCT effectively
decorrelates image information in the frequency domain, this decorrelation is
useful for compression because it allows the quantization step to discard many
of the higher-frequency components. Based on this observation, we propose a
novel DCT-based compression technique for the attention function of Vision
Transformers. Since high-frequency DCT coefficients usually correspond to
noise, we truncate the high-frequency DCT components of the input patches. Our
DCT-based compression reduces the size of weight matrices for queries, keys,
and values. While maintaining the same level of accuracy, our DCT compressed
Swin Transformers obtain a considerable decrease in the computational overhead.

</details>


### [144] [DDU-Net: A Domain Decomposition-Based CNN for High-Resolution Image Segmentation on Multiple GPUs](https://arxiv.org/abs/2407.21266)
*Corné Verburg, Alexander Heinlein, Eric C. Cyr*

Main category: cs.CV

TL;DR: A novel DDU-Net architecture combines encoder-decoder with domain decomposition to segment ultra-high-resolution images efficiently, achieving higher IoU scores by enabling inter-patch communication.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like spatial information loss and computational inefficiency in ultra-high-resolution image segmentation.

Method: Proposes DDU-Net, partitioning images into non-overlapping patches processed independently, with a communication network for spatial context.

Result: Achieves 2-3% higher IoU than no-communication networks and matches baseline U-Net performance.

Conclusion: DDU-Net effectively segments ultra-high-resolution images while preserving spatial context.

Abstract: The segmentation of ultra-high resolution images poses challenges such as
loss of spatial information or computational inefficiency. In this work, a
novel approach that combines encoder-decoder architectures with domain
decomposition strategies to address these challenges is proposed. Specifically,
a domain decomposition-based U-Net (DDU-Net) architecture is introduced, which
partitions input images into non-overlapping patches that can be processed
independently on separate devices. A communication network is added to
facilitate inter-patch information exchange to enhance the understanding of
spatial context. Experimental validation is performed on a synthetic dataset
that is designed to measure the effectiveness of the communication network.
Then, the performance is tested on the DeepGlobe land cover classification
dataset as a real-world benchmark data set. The results demonstrate that the
approach, which includes inter-patch communication for images divided into
$16\times16$ non-overlapping subimages, achieves a $2-3\,\%$ higher
intersection over union (IoU) score compared to the same network without
inter-patch communication. The performance of the network which includes
communication is equivalent to that of a baseline U-Net trained on the full
image, showing that our model provides an effective solution for segmenting
ultra-high-resolution images while preserving spatial context. The code is
available at https://github.com/corne00/DDU-Net.

</details>


### [145] [Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining](https://arxiv.org/abs/2408.02657)
*Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yi Xin, Xinyue Li, Qi Qin, Yu Qiao, Hongsheng Li, Peng Gao*

Main category: cs.CV

TL;DR: Lumina-mGPT is a multimodal autoregressive model excelling in text-to-image generation and versatile vision-language tasks, outperforming diffusion models with high efficiency.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that decoder-only autoregressive models can achieve high-quality image generation and versatile multimodal capabilities comparable to modern diffusion models.

Method: Uses Flexible Progressive Supervised Fine-tuning (FP-SFT) and Unambiguous image Representation (UniRep) for flexible image generation. Explores Omniponent Supervised Fine-tuning (Omni-SFT) for unified multimodal tasks.

Result: Achieves photorealistic image generation, supports varying aspect ratios, and performs well in visual recognition and vision-language tasks.

Conclusion: Lumina-mGPT shows strong potential as a unified multimodal generalist, with promising results in generation, recognition, and vision-language tasks.

Abstract: We present Lumina-mGPT, a family of multimodal autoregressive models capable
of various vision and language tasks, particularly excelling in generating
flexible photorealistic images from text descriptions. By initializing from
multimodal Generative PreTraining (mGPT), we demonstrate that decoder-only
Autoregressive (AR) model can achieve image generation performance comparable
to modern diffusion models with high efficiency through Flexible Progressive
Supervised Fine-tuning (FP-SFT). Equipped with our proposed Unambiguous image
Representation (UniRep), Lumina-mGPT can flexibly generate high-quality images
of varying aspect ratios. Building on the strong image generation capabilities,
we further explore Ominiponent Supervised Fine-tuning (Omni-SFT), an initial
attempt to elevate Lumina-mGPT into a unified multi-modal generalist. The
resulting model demonstrates versatile multimodal capabilities, including
visual generation tasks like text-to-image/multiview generation and
controllable generation, visual recognition tasks like segmentation and depth
estimation, and vision-language tasks like multi-turn visual question
answering, showing the rosy potential of the technical direction. Codes and
checkpoints are available at https://github.com/Alpha-VLLM/Lumina-mGPT.

</details>


### [146] [Set2Seq Transformer: Temporal and Positional-Aware Set Representations for Sequential Multiple-Instance Learning](https://arxiv.org/abs/2408.03404)
*Athanasios Efthymiou, Stevan Rudinac, Monika Kackovic, Nachoem Wijnberg, Marcel Worring*

Main category: cs.CV

TL;DR: The paper introduces Set2Seq Transformer, a novel architecture for sequential multiple-instance learning that jointly models set structure and temporal dependencies, outperforming static methods in diverse tasks like art analysis and wildfire forecasting.


<details>
  <summary>Details</summary>
Motivation: Existing methods either ignore temporal dynamics or lack set representation mechanisms, limiting their ability to capture complex patterns in sequential set data.

Method: The proposed Set2Seq Transformer learns temporal and positional-aware set representations in an end-to-end multimodal manner.

Result: The model significantly improves over static methods, demonstrating effectiveness in tasks like art success prediction and wildfire danger forecasting.

Conclusion: Set2Seq Transformer successfully integrates set structure and temporal dynamics, showing broad applicability across domains and tasks.

Abstract: Sequential multiple-instance learning involves learning representations of
sets distributed across discrete timesteps. In many real-world applications,
modeling both the internal structure of sets and their temporal relationships
across time is essential for capturing complex underlying patterns. However,
existing methods either focus on learning set representations at a static
level, ignoring temporal dynamics, or treat sequences as ordered lists of
individual elements, lacking explicit mechanisms to represent sets. In this
work, we propose Set2Seq Transformer, a novel architecture that jointly models
permutation-invariant set structure and temporal dependencies by learning
temporal and positional-aware representations of sets within a sequence in an
end-to-end multimodal manner. We evaluate our Set2Seq Transformer on two tasks
that require modeling both set structure alongside temporal and positional
patterns, but differ significantly in domain, modality, and objective. First,
we consider a fine-art analysis task, modeling artists' oeuvres for predicting
artistic success using a novel dataset, WikiArt-Seq2Rank. Second, we utilize
our Set2Seq Transformer for a short-term wildfire danger forecasting task.
Through extensive experimentation, we show that our Set2Seq Transformer
significantly improves over traditional static multiple-instance learning
methods by effectively learning permutation-invariant set, temporal, and
positional-aware representations across diverse domains, modalities, and tasks.
We will release both the dataset and model implementations on GitHub.

</details>


### [147] [AgentsCoMerge: Large Language Model Empowered Collaborative Decision Making for Ramp Merging](https://arxiv.org/abs/2408.03624)
*Senkang Hu, Zhengru Fang, Zihan Fang, Yiqin Deng, Xianhao Chen, Yuguang Fang, Sam Kwong*

Main category: cs.CV

TL;DR: Proposes AgentsCoMerge, a collaborative decision-making framework using LLMs for CAVs to improve safety and efficiency in ramp merging.


<details>
  <summary>Details</summary>
Motivation: Address traffic congestion, accidents, and emissions caused by ramp merging in CAV systems.

Method: Combines scene observation, hierarchical planning, communication modules, and reinforcement reflection training.

Result: Demonstrates superior efficiency and effectiveness in multi-agent collaborative decision-making for ramp merging.

Conclusion: AgentsCoMerge enhances CAV performance in merging scenarios, offering a scalable solution for traffic bottlenecks.

Abstract: Ramp merging is one of the bottlenecks in traffic systems, which commonly
cause traffic congestion, accidents, and severe carbon emissions. In order to
address this essential issue and enhance the safety and efficiency of connected
and autonomous vehicles (CAVs) at multi-lane merging zones, we propose a novel
collaborative decision-making framework, named AgentsCoMerge, to leverage large
language models (LLMs). Specifically, we first design a scene observation and
understanding module to allow an agent to capture the traffic environment. Then
we propose a hierarchical planning module to enable the agent to make decisions
and plan trajectories based on the observation and the agent's own state. In
addition, in order to facilitate collaboration among multiple agents, we
introduce a communication module to enable the surrounding agents to exchange
necessary information and coordinate their actions. Finally, we develop a
reinforcement reflection guided training paradigm to further enhance the
decision-making capability of the framework. Extensive experiments are
conducted to evaluate the performance of our proposed method, demonstrating its
superior efficiency and effectiveness for multi-agent collaborative
decision-making under various ramp merging scenarios.

</details>


### [148] [Contrastive Learning with Synthetic Positives](https://arxiv.org/abs/2408.16965)
*Dewen Zeng, Yawen Wu, Xinrong Hu, Xiaowei Xu, Yiyu Shi*

Main category: cs.CV

TL;DR: CLSP introduces synthetic positives via a diffusion model to improve contrastive learning, outperforming NNCLR and All4One by 2% and 1% in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Nearest neighbor contrastive learning lacks diversity in positives, limiting efficacy.

Method: Uses synthetic images from a diffusion model as hard positives in contrastive loss.

Result: Achieves SOTA performance, improving by 2% and 1% over NNCLR and All4One, and excels in transfer learning.

Conclusion: CLSP sets a baseline for SSL with synthetic data, enhancing model performance.

Abstract: Contrastive learning with the nearest neighbor has proved to be one of the
most efficient self-supervised learning (SSL) techniques by utilizing the
similarity of multiple instances within the same class. However, its efficacy
is constrained as the nearest neighbor algorithm primarily identifies "easy"
positive pairs, where the representations are already closely located in the
embedding space. In this paper, we introduce a novel approach called
Contrastive Learning with Synthetic Positives (CLSP) that utilizes synthetic
images, generated by an unconditional diffusion model, as the additional
positives to help the model learn from diverse positives. Through feature
interpolation in the diffusion model sampling process, we generate images with
distinct backgrounds yet similar semantic content to the anchor image. These
images are considered "hard" positives for the anchor image, and when included
as supplementary positives in the contrastive loss, they contribute to a
performance improvement of over 2% and 1% in linear evaluation compared to the
previous NNCLR and All4One methods across multiple benchmark datasets such as
CIFAR10, achieving state-of-the-art methods. On transfer learning benchmarks,
CLSP outperforms existing SSL frameworks on 6 out of 8 downstream datasets. We
believe CLSP establishes a valuable baseline for future SSL studies
incorporating synthetic data in the training process.

</details>


### [149] [On the Generalizability of Foundation Models for Crop Type Mapping](https://arxiv.org/abs/2409.09451)
*Yi-Chia Chang, Adam J. Stewart, Favyen Bastani, Piper Wolters, Shreya Kannan, George R. Huber, Jingtong Wang, Arindam Banerjee*

Main category: cs.CV

TL;DR: The paper evaluates the generalization of Earth observation (EO) foundation models to new geographic regions, focusing on agricultural crop classification. It compares three models and highlights the superiority of Sentinel-2-specific pre-training, while addressing challenges like class imbalance.


<details>
  <summary>Details</summary>
Motivation: To investigate the transferability of EO foundation models to new regions, especially in agriculture, and assess geospatial bias concerns.

Method: Five crop classification datasets across continents were used, harmonizing classes for four cereal grains. Three foundation models (SSL4EO-S12, SatlasPretrain, ImageNet) were evaluated in-distribution and out-of-distribution.

Result: Sentinel-2-specific pre-trained models (SSL4EO-S12) outperformed general models (ImageNet). Class imbalance required 900 labeled images for high average accuracy, while 100 sufficed for overall accuracy.

Conclusion: Specialized pre-training for EO data improves transferability, but class imbalance remains a challenge. Open-source datasets and code support further research.

Abstract: Foundation models pre-trained using self-supervised learning have shown
powerful transfer learning capabilities on various downstream tasks, including
language understanding, text generation, and image recognition. The Earth
observation (EO) field has produced several foundation models pre-trained
directly on multispectral satellite imagery for applications like precision
agriculture, wildfire and drought monitoring, and natural disaster response.
However, few studies have investigated the ability of these models to
generalize to new geographic locations, and potential concerns of geospatial
bias -- models trained on data-rich developed nations not transferring well to
data-scarce developing nations -- remain. We investigate the ability of popular
EO foundation models to transfer to new geographic regions in the agricultural
domain, where differences in farming practices and class imbalance make
transfer learning particularly challenging. We first select five crop
classification datasets across five continents, normalizing for dataset size
and harmonizing classes to focus on four major cereal grains: maize, soybean,
rice, and wheat. We then compare three popular foundation models, pre-trained
on SSL4EO-S12, SatlasPretrain, and ImageNet, using in-distribution (ID) and
out-of-distribution (OOD) evaluation. Experiments show that pre-trained weights
designed explicitly for Sentinel-2, such as SSL4EO-S12, outperform general
pre-trained weights like ImageNet. Furthermore, while only 100 labeled images
are sufficient for achieving high overall accuracy, 900 images are required to
achieve high average accuracy due to class imbalance. All harmonized datasets
and experimental code are open-source and available for download.

</details>


### [150] [DiffKillR: Killing and Recreating Diffeomorphisms for Cell Annotation in Dense Microscopy Images](https://arxiv.org/abs/2410.03058)
*Chen Liu, Danqi Liao, Alejandro Parada-Mayorga, Alejandro Ribeiro, Marcello DiStasio, Smita Krishnaswamy*

Main category: cs.CV

TL;DR: DiffKillR is a framework combining archetype matching and image registration to automate cell annotation in microscopy images, reducing manual labeling.


<details>
  <summary>Details</summary>
Motivation: The challenge of accurately annotating densely packed information in digital microscopy images drives the need for automated solutions.

Method: DiffKillR uses two neural networks: one for diffeomorphism-invariant feature space learning and another for warping field computation to map annotations.

Result: Validated on three microscopy tasks, DiffKillR outperforms existing supervised, semi-supervised, and unsupervised methods.

Conclusion: DiffKillR efficiently propagates annotations with minimal manual input, suitable for any pixel-level annotation task.

Abstract: The proliferation of digital microscopy images, driven by advances in
automated whole slide scanning, presents significant opportunities for
biomedical research and clinical diagnostics. However, accurately annotating
densely packed information in these images remains a major challenge. To
address this, we introduce DiffKillR, a novel framework that reframes cell
annotation as the combination of archetype matching and image registration
tasks. DiffKillR employs two complementary neural networks: one that learns a
diffeomorphism-invariant feature space for robust cell matching and another
that computes the precise warping field between cells for annotation mapping.
Using a small set of annotated archetypes, DiffKillR efficiently propagates
annotations across large microscopy images, reducing the need for extensive
manual labeling. More importantly, it is suitable for any type of pixel-level
annotation. We will discuss the theoretical properties of DiffKillR and
validate it on three microscopy tasks, demonstrating its advantages over
existing supervised, semi-supervised, and unsupervised methods. The code is
available at https://github.com/KrishnaswamyLab/DiffKillR.

</details>


### [151] [Neuro-Symbolic Evaluation of Text-to-Video Models using Formal Verification](https://arxiv.org/abs/2411.16718)
*S. P. Sharan, Minkyu Choi, Sahil Shah, Harsh Goel, Mohammad Omama, Sandeep Chinchali*

Main category: cs.CV

TL;DR: NeuS-V introduces a neuro-symbolic metric for evaluating text-to-video alignment, outperforming existing metrics by 5x in human correlation.


<details>
  <summary>Details</summary>
Motivation: Current metrics neglect temporal fidelity and text-to-video alignment, critical for safety-critical applications.

Method: Converts prompts into Temporal Logic (TL) specifications and videos into automata, then formally checks alignment.

Result: NeuS-V shows 5x higher correlation with human evaluations; current models perform poorly on temporally complex prompts.

Conclusion: Highlights the need for improved text-to-video generation capabilities, especially for temporal fidelity.

Abstract: Recent advancements in text-to-video models such as Sora, Gen-3, MovieGen,
and CogVideoX are pushing the boundaries of synthetic video generation, with
adoption seen in fields like robotics, autonomous driving, and entertainment.
As these models become prevalent, various metrics and benchmarks have emerged
to evaluate the quality of the generated videos. However, these metrics
emphasize visual quality and smoothness, neglecting temporal fidelity and
text-to-video alignment, which are crucial for safety-critical applications. To
address this gap, we introduce NeuS-V, a novel synthetic video evaluation
metric that rigorously assesses text-to-video alignment using neuro-symbolic
formal verification techniques. Our approach first converts the prompt into a
formally defined Temporal Logic (TL) specification and translates the generated
video into an automaton representation. Then, it evaluates the text-to-video
alignment by formally checking the video automaton against the TL
specification. Furthermore, we present a dataset of temporally extended prompts
to evaluate state-of-the-art video generation models against our benchmark. We
find that NeuS-V demonstrates a higher correlation by over 5x with human
evaluations when compared to existing metrics. Our evaluation further reveals
that current video generation models perform poorly on these temporally complex
prompts, highlighting the need for future work in improving text-to-video
generation capabilities.

</details>


### [152] [PhysFlow: Unleashing the Potential of Multi-modal Foundation Models and Video Diffusion for 4D Dynamic Physical Scene Simulation](https://arxiv.org/abs/2411.14423)
*Zhuoman Liu, Weicai Ye, Yan Luximon, Pengfei Wan, Di Zhang*

Main category: cs.CV

TL;DR: PhysFlow enhances 4D dynamic scene simulation using multi-modal foundation models and video diffusion for realistic material interactions.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack the ability to represent complex real-world materials and interactions accurately.

Method: Combines multi-modal models for material identification and 3D Gaussian splats, refined via video diffusion with differentiable MPM and optical flow guidance.

Result: Achieves realistic and accurate simulation of dynamic interactions in real-world scenarios.

Conclusion: PhysFlow advances accuracy and flexibility in physics-based simulations.

Abstract: Realistic simulation of dynamic scenes requires accurately capturing diverse
material properties and modeling complex object interactions grounded in
physical principles. However, existing methods are constrained to basic
material types with limited predictable parameters, making them insufficient to
represent the complexity of real-world materials. We introduce PhysFlow, a
novel approach that leverages multi-modal foundation models and video diffusion
to achieve enhanced 4D dynamic scene simulation. Our method utilizes
multi-modal models to identify material types and initialize material
parameters through image queries, while simultaneously inferring 3D Gaussian
splats for detailed scene representation. We further refine these material
parameters using video diffusion with a differentiable Material Point Method
(MPM) and optical flow guidance rather than render loss or Score Distillation
Sampling (SDS) loss. This integrated framework enables accurate prediction and
realistic simulation of dynamic interactions in real-world scenarios, advancing
both accuracy and flexibility in physics-based simulations.

</details>


### [153] [Improved implicit diffusion model with knowledge distillation to estimate the spatial distribution density of carbon stock in remote sensing imagery](https://arxiv.org/abs/2411.17973)
*Zhenyu Yu, Jinnian Wang, Mohd Yamani Idna Idris*

Main category: cs.CV

TL;DR: The study uses GF-1 WFV satellite imagery and AI models (KD-VGG, KD-UNet, IIDM) for high-accuracy forest carbon stock estimation, improving RMSE by 41.69-42.33% over regression models.


<details>
  <summary>Details</summary>
Motivation: Forests are vital carbon sinks, and accurate monitoring via remote sensing is crucial for climate change mitigation.

Method: Utilized GF-1 WFV imagery, introduced KD-VGG and KD-UNet for feature extraction, and proposed the IIDM model for estimation.

Result: IIDM achieved the highest accuracy (RMSE 12.17%), outperforming other models by 41.69-42.33%.

Conclusion: AI-generated models like IIDM enhance carbon stock estimation, supporting better forest management and climate policies.

Abstract: The forest serves as the most significant terrestrial carbon stock mechanism,
effectively reducing atmospheric CO2 concentrations and mitigating climate
change. Remote sensing provides high data accuracy and enables large-scale
observations. Optical images facilitate long-term monitoring, which is crucial
for future carbon stock estimation studies. This study focuses on Huize County,
Qujing City, Yunnan Province, China, utilizing GF-1 WFV satellite imagery. The
KD-VGG and KD-UNet modules were introduced for initial feature extraction, and
the improved implicit diffusion model (IIDM) was proposed. The results showed:
(1) The VGG module improved initial feature extraction, improving accuracy, and
reducing inference time with optimized model parameters. (2) The
Cross-attention + MLPs module enabled effective feature fusion, establishing
critical relationships between global and local features, achieving
high-accuracy estimation. (3) The IIDM model, a novel contribution,
demonstrated the highest estimation accuracy with an RMSE of 12.17%,
significantly improving by 41.69% to 42.33% compared to the regression model.
In carbon stock estimation, the generative model excelled in extracting deeper
features, significantly outperforming other models, demonstrating the
feasibility of AI-generated content in quantitative remote sensing. The
16-meter resolution estimates provide a robust basis for tailoring forest
carbon sink regulations, enhancing regional carbon stock management.

</details>


### [154] [Machine Learning-Based Automated Assessment of Intracorporeal Suturing in Laparoscopic Fundoplication](https://arxiv.org/abs/2412.16195)
*Shekhar Madhav Khairnar, Huu Phong Nguyen, Alexis Desir, Carla Holcomb, Daniel J. Scott, Ganesh Sankaranarayanan*

Main category: cs.CV

TL;DR: AI-based tool tracking using SAM eliminates human annotation for automated surgical skill assessment. Supervised and unsupervised models achieved high accuracy in classifying novice vs. expert performance.


<details>
  <summary>Details</summary>
Motivation: To provide instantaneous feedback for surgical trainees by automating skill assessment without time-intensive human annotation.

Method: Used SAM for tool tracking, extracted kinematic features, and applied supervised (Random Forest, etc.) and unsupervised (1-D CNN) models for classification.

Result: Supervised learning achieved 0.795 accuracy; unsupervised 1-D CNN outperformed with 0.817 accuracy, eliminating kinematic feature computation.

Conclusion: AI models can automate surgical skill assessment effectively, removing reliance on human annotation.

Abstract: Automated assessment of surgical skills using artificial intelligence (AI)
provides trainees with instantaneous feedback. After bimanual tool motions are
captured, derived kinematic metrics are reliable predictors of performance in
laparoscopic tasks. Implementing automated tool tracking requires
time-intensive human annotation. We developed AI-based tool tracking using the
Segment Anything Model (SAM) to eliminate the need for human annotators. Here,
we describe a study evaluating the usefulness of our tool tracking model in
automated assessment during a laparoscopic suturing task in the fundoplication
procedure. An automated tool tracking model was applied to recorded videos of
Nissen fundoplication on porcine bowel. Surgeons were grouped as novices
(PGY1-2) and experts (PGY3-5, attendings). The beginning and end of each
suturing step were segmented, and motions of the left and right tools were
extracted. A low-pass filter with a 24 Hz cut-off frequency removed noise.
Performance was assessed using supervised and unsupervised models, and an
ablation study compared results. Kinematic features--RMS velocity, RMS
acceleration, RMS jerk, total path length, and Bimanual Dexterity--were
extracted and analyzed using Logistic Regression, Random Forest, Support Vector
Classifier, and XGBoost. PCA was performed for feature reduction. For
unsupervised learning, a Denoising Autoencoder (DAE) model with classifiers,
such as a 1-D CNN and traditional models, was trained. Data were extracted for
28 participants (9 novices, 19 experts). Supervised learning with PCA and
Random Forest achieved an accuracy of 0.795 and an F1 score of 0.778. The
unsupervised 1-D CNN achieved superior results with an accuracy of 0.817 and an
F1 score of 0.806, eliminating the need for kinematic feature computation. We
demonstrated an AI model capable of automated performance classification,
independent of human annotation.

</details>


### [155] [3D-LLaVA: Towards Generalist 3D LMMs with Omni Superpoint Transformer](https://arxiv.org/abs/2501.01163)
*Jiajun Deng, Tianyu He, Li Jiang, Tianyu Wang, Feras Dayoub, Ian Reid*

Main category: cs.CV

TL;DR: 3D-LLaVA is a simple yet powerful 3D Large Multimodal Model (LMM) for fine-grained scene understanding and human-agent interaction, using point clouds and a novel Omni Superpoint Transformer (OST).


<details>
  <summary>Details</summary>
Motivation: Enhancing 3D LMMs for fine-grained understanding and flexible interaction, addressing limitations of complex pipelines in existing methods.

Method: Minimalist design with OST, integrating visual feature selection, prompt encoding, and mask decoding, pretrained and tuned for 3D-LLM bridging.

Result: Achieves impressive performance on benchmarks.

Conclusion: 3D-LLaVA demonstrates effectiveness in 3D scene comprehension and interaction, offering a streamlined alternative to complex pipelines.

Abstract: Current 3D Large Multimodal Models (3D LMMs) have shown tremendous potential
in 3D-vision-based dialogue and reasoning. However, how to further enhance 3D
LMMs to achieve fine-grained scene understanding and facilitate flexible
human-agent interaction remains a challenging problem. In this work, we
introduce 3D-LLaVA, a simple yet highly powerful 3D LMM designed to act as an
intelligent assistant in comprehending, reasoning, and interacting with the 3D
world. Unlike existing top-performing methods that rely on complicated
pipelines-such as offline multi-view feature extraction or additional
task-specific heads-3D-LLaVA adopts a minimalist design with integrated
architecture and only takes point clouds as input. At the core of 3D-LLaVA is a
new Omni Superpoint Transformer (OST), which integrates three functionalities:
(1) a visual feature selector that converts and selects visual tokens, (2) a
visual prompt encoder that embeds interactive visual prompts into the visual
token space, and (3) a referring mask decoder that produces 3D masks based on
text description. This versatile OST is empowered by the hybrid pretraining to
obtain perception priors and leveraged as the visual connector that bridges the
3D data to the LLM. After performing unified instruction tuning, our 3D-LLaVA
reports impressive results on various benchmarks.

</details>


### [156] [Large-image Object Detection for Fine-grained Recognition of Punches Patterns in Medieval Panel Painting](https://arxiv.org/abs/2501.12489)
*Josh Bruegger, Diana Ioana Catana, Vanja Macovaz, Matias Valdenegro-Toro, Matthia Sabatelli, Marco Zullich*

Main category: cs.CV

TL;DR: The paper proposes a ML-based method using YOLOv10 to automate the detection of punches in historical panel paintings, aiding art historians in attribution.


<details>
  <summary>Details</summary>
Motivation: Manual attribution of art pieces is laborious and subjective. Quantitative features like punches can support this process, and ML can automate their extraction.

Method: A dataset of large-scale images is collected, and YOLOv10 is trained for punch detection using a sliding-window approach with custom non-maximal suppression.

Result: The method reliably identifies and extracts punches, supporting art historians in attribution.

Conclusion: The ML pipeline effectively automates punch detection, offering a reliable tool for art historical research.

Abstract: The attribution of the author of an art piece is typically a laborious manual
process, usually relying on subjective evaluations of expert figures. However,
there are some situations in which quantitative features of the artwork can
support these evaluations. The extraction of these features can sometimes be
automated, for instance, with the use of Machine Learning (ML) techniques. An
example of these features is represented by repeated, mechanically impressed
patterns, called punches, present chiefly in 13th and 14th-century panel
paintings from Tuscany. Previous research in art history showcased a strong
connection between the shapes of punches and specific artists or workshops,
suggesting the possibility of using these quantitative cues to support the
attribution. In the present work, we first collect a dataset of large-scale
images of these panel paintings. Then, using YOLOv10, a recent and popular
object detection model, we train a ML pipeline to perform object detection on
the punches contained in the images. Due to the large size of the images, the
detection procedure is split across multiple frames by adopting a
sliding-window approach with overlaps, after which the predictions are combined
for the whole image using a custom non-maximal suppression routine. Our results
indicate how art historians working in the field can reliably use our method
for the identification and extraction of punches.

</details>


### [157] [LinPrim: Linear Primitives for Differentiable Volumetric Rendering](https://arxiv.org/abs/2501.16312)
*Nicolas von Lützow, Matthias Nießner*

Main category: cs.CV

TL;DR: The paper introduces two new volumetric scene representations using octahedra and tetrahedra, optimized via a differentiable rasterizer, achieving comparable performance to state-of-the-art methods with fewer primitives.


<details>
  <summary>Details</summary>
Motivation: To explore alternative volumetric scene representations beyond NeRF and 3D Gaussians, focusing on linear primitives for efficient and high-fidelity 3D reconstruction.

Method: Proposes octahedra and tetrahedra as scene primitives, optimized using a GPU-efficient differentiable rasterizer for end-to-end gradient-based optimization.

Result: Demonstrates comparable performance to state-of-the-art volumetric methods with fewer primitives, maintaining real-time rendering capabilities.

Conclusion: Highlights the potential of novel primitives to expand the design space for 3D representations, offering insights into fidelity and performance trade-offs.

Abstract: Volumetric rendering has become central to modern novel view synthesis
methods, which use differentiable rendering to optimize 3D scene
representations directly from observed views. While many recent works build on
NeRF or 3D Gaussians, we explore an alternative volumetric scene
representation. More specifically, we introduce two new scene representations
based on linear primitives - octahedra and tetrahedra - both of which define
homogeneous volumes bounded by triangular faces. To optimize these primitives,
we present a differentiable rasterizer that runs efficiently on GPUs, allowing
end-to-end gradient-based optimization while maintaining real-time rendering
capabilities. Through experiments on real-world datasets, we demonstrate
comparable performance to state-of-the-art volumetric methods while requiring
fewer primitives to achieve similar reconstruction fidelity. Our findings
deepen the understanding of 3D representations by providing insights into the
fidelity and performance characteristics of transparent polyhedra and suggest
that adopting novel primitives can expand the available design space.

</details>


### [158] [Review of Demographic Fairness in Face Recognition](https://arxiv.org/abs/2502.02309)
*Ketan Kotwal, Sebastien Marcel*

Main category: cs.CV

TL;DR: A review on demographic fairness in face recognition (FR), covering causes, datasets, metrics, and mitigation methods, while highlighting advancements and future challenges.


<details>
  <summary>Details</summary>
Motivation: Addressing biases in FR systems due to demographic disparities, which impact fairness, equity, and reliability, especially in sensitive applications.

Method: Systematic examination of causes, datasets, assessment metrics, and mitigation approaches in FR research.

Result: Comprehensive overview of demographic fairness in FR, categorizing key contributions and identifying gaps.

Conclusion: Emphasizes the need for equitable and trustworthy FR systems, providing a unified perspective for researchers.

Abstract: Demographic fairness in face recognition (FR) has emerged as a critical area
of research, given its impact on fairness, equity, and reliability across
diverse applications. As FR technologies are increasingly deployed globally,
disparities in performance across demographic groups-- such as race, ethnicity,
and gender-- have garnered significant attention. These biases not only
compromise the credibility of FR systems but also raise ethical concerns,
especially when these technologies are employed in sensitive domains. This
review consolidates extensive research efforts providing a comprehensive
overview of the multifaceted aspects of demographic fairness in FR.
  We systematically examine the primary causes, datasets, assessment metrics,
and mitigation approaches associated with demographic disparities in FR. By
categorizing key contributions in these areas, this work provides a structured
approach to understanding and addressing the complexity of this issue. Finally,
we highlight current advancements and identify emerging challenges that need
further investigation. This article aims to provide researchers with a unified
perspective on the state-of-the-art while emphasizing the critical need for
equitable and trustworthy FR systems.

</details>


### [159] [Disentangling Visual Transformers: Patch-level Interpretability for Image Classification](https://arxiv.org/abs/2502.17196)
*Guillaume Jeanneret, Loïc Simon, Frédéric Jurie*

Main category: cs.CV

TL;DR: HiT is an interpretable transformer architecture that disentangles patch influences for better explicability with a reasonable performance trade-off.


<details>
  <summary>Details</summary>
Motivation: Transformers lack interpretability due to the complex self-attention mechanism, limiting their use in applications requiring transparency.

Method: Proposes Hindered Transformer (HiT), redesigning transformers to disentangle patch influences, representing classification as a linear combination of patch-level information.

Result: HiT offers improved interpretability with a reasonable performance trade-off, making it suitable for applications prioritizing explicability.

Conclusion: HiT provides a viable, interpretable alternative to traditional transformers, balancing performance and transparency.

Abstract: Visual transformers have achieved remarkable performance in image
classification tasks, but this performance gain has come at the cost of
interpretability. One of the main obstacles to the interpretation of
transformers is the self-attention mechanism, which mixes visual information
across the whole image in a complex way. In this paper, we propose Hindered
Transformer (HiT), a novel interpretable by design architecture inspired by
visual transformers. Our proposed architecture rethinks the design of
transformers to better disentangle patch influences at the classification
stage. Ultimately, HiT can be interpreted as a linear combination of
patch-level information. We show that the advantages of our approach in terms
of explicability come with a reasonable trade-off in performance, making it an
attractive alternative for applications where interpretability is paramount.

</details>


### [160] [HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video Understanding](https://arxiv.org/abs/2503.08585)
*Shehreen Azad, Vibhav Vineet, Yogesh Singh Rawat*

Main category: cs.CV

TL;DR: HierarQ is a task-aware hierarchical Q-Former framework for video understanding, addressing frame sampling and context length issues in MLLMs by processing frames sequentially and using a two-stream feature modulator.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with medium-to-long video understanding due to frame sampling and context length limitations, risking loss of key information.

Method: HierarQ uses a two-stream language-guided feature modulator (entity and scene streams) with memory banks to capture short and long-term context, bypassing frame sampling.

Result: HierarQ achieves state-of-the-art performance on 10 video benchmarks for understanding, QA, and captioning.

Conclusion: HierarQ is robust and efficient for comprehensive video analysis, outperforming existing methods.

Abstract: Despite advancements in multimodal large language models (MLLMs), current
approaches struggle in medium-to-long video understanding due to frame and
context length limitations. As a result, these models often depend on frame
sampling, which risks missing key information over time and lacks task-specific
relevance. To address these challenges, we introduce HierarQ, a task-aware
hierarchical Q-Former based framework that sequentially processes frames to
bypass the need for frame sampling, while avoiding LLM's context length
limitations. We introduce a lightweight two-stream language-guided feature
modulator to incorporate task awareness in video understanding, with the entity
stream capturing frame-level object information within a short context and the
scene stream identifying their broader interactions over longer period of time.
Each stream is supported by dedicated memory banks which enables our proposed
Hierachical Querying transformer (HierarQ) to effectively capture short and
long-term context. Extensive evaluations on 10 video benchmarks across video
understanding, question answering, and captioning tasks demonstrate HierarQ's
state-of-the-art performance across most datasets, proving its robustness and
efficiency for comprehensive video analysis.

</details>


### [161] [Dynamic Pyramid Network for Efficient Multimodal Large Language Model](https://arxiv.org/abs/2503.20322)
*Hao Ai, Kunyi Wang, Zezhou Wang, Hao Lu, Jin Tian, Yaxin Luo, Peng Xing, Jen-Yuan Huang, Huaxia Li, Gen luo*

Main category: cs.CV

TL;DR: The paper introduces Dynamic Pyramid Network (DPN) and Dynamic Pooling Experts (DPE) to efficiently compress visual features in Multimodal Large Language Models (MLLMs) without losing fine-grained semantics, achieving significant computational savings and performance gains.


<details>
  <summary>Details</summary>
Motivation: The high computational cost of MLLMs limits their real-world application, and existing visual compression methods degrade performance, especially in difficult samples.

Method: DPN hierarchically compresses visual features, preserving fine-grained details in shallow layers. DPE dynamically adjusts compression rates based on input difficulty.

Result: DPN saves 56% FLOPs on LLaVA while improving performance by +0.74%. It also generalizes well to high-resolution MLLMs like LLaVA-HR.

Conclusion: DPN and DPE offer an efficient solution for MLLMs, balancing computational savings with performance retention.

Abstract: Multimodal large language models (MLLMs) have demonstrated impressive
performance in various vision-language (VL) tasks, but their expensive
computations still limit the real-world application. To address this issue,
recent efforts aim to compress the visual features to save the computational
costs of MLLMs. However, direct visual compression methods, e.g. efficient
projectors, inevitably destroy the visual semantics in MLLM, especially in
difficult samples. To overcome this shortcoming, we propose a novel dynamic
pyramid network (DPN) for efficient MLLMs. Specifically, DPN formulates MLLM as
a hierarchical structure where visual features are gradually compressed with
increasing depth. In this case, even with a high compression ratio,
fine-grained visual information can still be perceived in shallow layers. To
maximize the benefit of DPN, we further propose an innovative Dynamic Pooling
Experts (DPE) that can dynamically choose the optimal visual compression rate
according to input features. With this design, harder samples will be assigned
larger computations, thus preserving the model performance. To validate our
approach, we conduct extensive experiments on two popular MLLMs and ten
benchmarks. Experimental results show that DPN can save up to 56% average FLOPs
on LLaVA while further achieving +0.74% performance gains. Besides, the
generalization ability of DPN is also validated on the existing high-resolution
MLLM called LLaVA-HR. The source code will be released at
https://github.com/aihao2000/DPN-LLaVA.

</details>


### [162] [How Well Can Vison-Language Models Understand Humans' Intention? An Open-ended Theory of Mind Question Evaluation Benchmark](https://arxiv.org/abs/2503.22093)
*Ximing Wen, Mallika Mainali, Anik Sen*

Main category: cs.CV

TL;DR: The paper evaluates Vision Language Models (VLMs) on Theory of Mind (ToM) tasks, revealing GPT-4's superior performance and highlighting challenges in inferring intentions in complex scenarios.


<details>
  <summary>Details</summary>
Motivation: To explore VLMs' underexplored ability in ToM tasks like inferring human intentions, beliefs, and mental states.

Method: Proposed an open-ended question framework and curated a benchmark dataset of 30 images to evaluate four VLMs of varying sizes.

Result: GPT-4 outperformed others, with GPT-4o-mini being comparable. VLMs struggled in complex scenarios like bullying or cheating, and smaller models sometimes inferred correct intentions despite incorrect visual cues.

Conclusion: VLMs show promise in ToM tasks but face challenges in complex scenarios, with performance varying by model size. The dataset is publicly available for further research.

Abstract: Vision Language Models (VLMs) have demonstrated strong reasoning capabilities
in Visual Question Answering (VQA) tasks; however, their ability to perform
Theory of Mind (ToM) tasks, such as inferring human intentions, beliefs, and
mental states, remains underexplored. We propose an open-ended question
framework to evaluate VLMs' performance across diverse categories of ToM tasks.
We curated and annotated a benchmark dataset of 30 images and evaluated the
performance of four VLMs of varying sizes. Our results show that the GPT-4
model outperformed all the others, with only one smaller model, GPT-4o-mini,
achieving comparable performance. We observed that VLMs often struggle to infer
intentions in complex scenarios such as bullying or cheating. Our findings
reveal that smaller models can sometimes infer correct intentions despite
relying on incorrect visual cues. The dataset is available at
https://github.com/ximingwen/ToM-AAAI25-Multimodal.

</details>


### [163] [Causal Disentanglement for Robust Long-tail Medical Image Generation](https://arxiv.org/abs/2504.14450)
*Weizhi Nie, Zichun Zhang, Weijie Wang, Bruno Lepri, Anan Liu, Nicu Sebe*

Main category: cs.CV

TL;DR: A novel framework for generating high-quality, diverse counterfactual medical images using causal disentanglement and text-guided modeling to address data scarcity and improve interpretability.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges like complex pathological features, imbalanced data, and the need for structurally stable images while enhancing clinical relevance and model interpretability.

Method: Uses causal disentanglement for feature separation, group supervision for independence, a diffusion model for pathological feature modeling, and a large language model for extracting lesion details. Also optimizes noise for long-tailed categories.

Result: Generates diverse, high-quality counterfactual medical images with improved structural stability and clinical relevance.

Conclusion: The proposed framework effectively addresses data scarcity and enhances interpretability, leveraging causal disentanglement and text-guided modeling for better medical image generation.

Abstract: Counterfactual medical image generation effectively addresses data scarcity
and enhances the interpretability of medical images. However, due to the
complex and diverse pathological features of medical images and the imbalanced
class distribution in medical data, generating high-quality and diverse medical
images from limited data is significantly challenging. Additionally, to fully
leverage the information in limited data, such as anatomical structure
information and generate more structurally stable medical images while avoiding
distortion or inconsistency. In this paper, in order to enhance the clinical
relevance of generated data and improve the interpretability of the model, we
propose a novel medical image generation framework, which generates independent
pathological and structural features based on causal disentanglement and
utilizes text-guided modeling of pathological features to regulate the
generation of counterfactual images. First, we achieve feature separation
through causal disentanglement and analyze the interactions between features.
Here, we introduce group supervision to ensure the independence of pathological
and identity features. Second, we leverage a diffusion model guided by
pathological findings to model pathological features, enabling the generation
of diverse counterfactual images. Meanwhile, we enhance accuracy by leveraging
a large language model to extract lesion severity and location from medical
reports. Additionally, we improve the performance of the latent diffusion model
on long-tailed categories through initial noise optimization.

</details>


### [164] [Shifts in Doctors' Eye Movements Between Real and AI-Generated Medical Images](https://arxiv.org/abs/2504.15007)
*David C Wong, Bin Wang, Gorkem Durak, Marouane Tliba, Mohamed Amine Kerkouri, Aladine Chetouani, Ahmet Enis Cetin, Cagdas Topel, Nicolo Gennaro, Camila Vendrami, Tugce Agirlar Trabzonlu, Amir Ali Rahsepar, Laetitia Perronne, Matthew Antalek, Onural Ozturk, Gokcan Okur, Andrew C. Gordon, Ayis Pyrros, Frank H Miller, Amir A Borhani, Hatice Savas, Eric M. Hart, Elizabeth A Krupinski, Ulas Bagci*

Main category: cs.CV

TL;DR: The paper analyzes radiologists' eye-tracking patterns to understand attention and agreement in medical imaging, comparing gaze behavior between real and deep-learning-generated images.


<details>
  <summary>Details</summary>
Motivation: To uncover insights into how radiologists visually interpret and diagnose clinical cases, and to explore differences in gaze behavior between real and synthetic images.

Method: Analyzed eye-movement patterns (saccades direction, amplitude, joint distribution) and fixation bias maps (first, last, short, longest fixations) to quantify attention allocation and differences in gaze distribution.

Result: Revealed patterns in radiologists' attention and diagnostic strategies, and quantified differences in gaze behavior between real and synthetic images.

Conclusion: Eye-tracking analysis provides valuable insights into radiologists' visual interpretation and highlights distinct gaze behaviors for real versus synthetic images.

Abstract: Eye-tracking analysis plays a vital role in medical imaging, providing key
insights into how radiologists visually interpret and diagnose clinical cases.
In this work, we first analyze radiologists' attention and agreement by
measuring the distribution of various eye-movement patterns, including saccades
direction, amplitude, and their joint distribution. These metrics help uncover
patterns in attention allocation and diagnostic strategies. Furthermore, we
investigate whether and how doctors' gaze behavior shifts when viewing
authentic (Real) versus deep-learning-generated (Fake) images. To achieve this,
we examine fixation bias maps, focusing on first, last, short, and longest
fixations independently, along with detailed saccades patterns, to quantify
differences in gaze distribution and visual saliency between authentic and
synthetic images.

</details>


### [165] [Vidi: Large Multimodal Models for Video Understanding and Editing](https://arxiv.org/abs/2504.15681)
*Vidi Team, Celong Liu, Chia-Wen Kuo, Dawei Du, Fan Chen, Guang Chen, Jiamin Yuan, Lingxi Zhang, Lu Guo, Lusha Li, Longyin Wen, Qingyu Chen, Rachel Deng, Sijie Zhu, Stuart Siew, Tong Jin, Wei Lu, Wen Zhong, Xiaohui Shen, Xin Gu, Xing Mei, Xueqiong Qu*

Main category: cs.CV

TL;DR: Vidi, a Large Multimodal Model (LMM), excels in temporal retrieval for video editing, outperforming proprietary models like GPT-4o and Gemini.


<details>
  <summary>Details</summary>
Motivation: To address challenges in processing multimodal, long-duration videos for intelligent editing, requiring strong temporal understanding.

Method: Introduces Vidi, an LMM for temporal retrieval, and VUE-TR benchmark with longer videos, audio support, diverse queries, high-quality annotations, and refined metrics.

Result: Vidi significantly outperforms leading models (e.g., GPT-4o, Gemini) in temporal retrieval tasks.

Conclusion: Vidi demonstrates superior capability in video editing scenarios, supported by the comprehensive VUE-TR benchmark.

Abstract: Humans naturally share information with those they are connected to, and
video has become one of the dominant mediums for communication and expression
on the Internet. To support the creation of high-quality large-scale video
content, a modern pipeline requires a comprehensive understanding of both the
raw input materials (e.g., the unedited footage captured by cameras) and the
editing components (e.g., visual effects). In video editing scenarios, models
must process multiple modalities (e.g., vision, audio, text) with strong
background knowledge and handle flexible input lengths (e.g., hour-long raw
videos), which poses significant challenges for traditional models. In this
report, we introduce Vidi, a family of Large Multimodal Models (LMMs) for a
wide range of video understand editing scenarios. The first release focuses on
temporal retrieval, i.e., identifying the time ranges within the input videos
corresponding to a given text query, which plays a critical role in intelligent
editing. The model is capable of processing hour-long videos with strong
temporal understanding capability, e.g., retrieve time ranges for certain
queries. To support a comprehensive evaluation in real-world scenarios, we also
present the VUE-TR benchmark, which introduces five key advancements. 1) Video
duration: significantly longer than videos of existing temporal retrival
datasets, 2) Audio support: includes audio-based queries, 3) Query format:
diverse query lengths/formats, 4) Annotation quality: ground-truth time ranges
are manually annotated. 5) Evaluation metric: a refined IoU metric to support
evaluation over multiple time ranges. Remarkably, Vidi significantly
outperforms leading proprietary models, e.g., GPT-4o and Gemini, on the
temporal retrieval task, indicating its superiority in video editing scenarios.

</details>


### [166] [Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language Models](https://arxiv.org/abs/2504.15929)
*Saban Ozturk, Melih B. Yilmaz, Muti Kara, M. Talat Yavuz, Aykut Koç, Tolga Çukur*

Main category: cs.CV

TL;DR: MedTrim enhances image-text alignment in medical vision-language models by leveraging multimodal triplet learning and structured meta-entity information, improving performance in retrieval and classification tasks.


<details>
  <summary>Details</summary>
Motivation: The growing volume of diagnostic imaging data increases errors and workflow backlogs, highlighting the need for better alignment methods in medical vision-language models to capture fine-grained pathology attributes.

Method: MedTrim uses ontology-based entity recognition to extract meta-entities from reports, a novel score function for refined triplet mining, and a multimodal triplet alignment objective for detailed pathology alignment.

Result: MedTrim outperforms existing methods in downstream tasks like retrieval and classification by preserving clinically significant intra-class variations.

Conclusion: MedTrim offers a robust framework for aligning image and text representations in medical imaging, addressing limitations of current contrastive learning methods.

Abstract: Diagnostic imaging relies on interpreting both images and radiology reports,
but the growing data volumes place significant pressure on medical experts,
yielding increased errors and workflow backlogs. Medical vision-language models
(med-VLMs) have emerged as a powerful framework to efficiently process
multimodal imaging data, particularly in chest X-ray (CXR) evaluations, albeit
their performance hinges on how well image and text representations are
aligned. Existing alignment methods, predominantly based on contrastive
learning, prioritize separation between disease classes over segregation of
fine-grained pathology attributes like location, size or severity, leading to
suboptimal representations. Here, we propose MedTrim (Meta-entity-driven
Triplet mining), a novel method that enhances image-text alignment through
multimodal triplet learning synergistically guided by disease class as well as
adjectival and directional pathology descriptors. Unlike common alignment
methods that separate broad disease classes, MedTrim leverages structured
meta-entity information to preserve subtle but clinically significant
intra-class variations. For this purpose, we first introduce an ontology-based
entity recognition module that extracts pathology-specific meta-entities from
CXR reports, as annotations on pathology attributes are rare in public
datasets. For refined sample selection in triplet mining, we then introduce a
novel score function that captures an aggregate measure of inter-sample
similarity based on disease classes and adjectival/directional descriptors.
Lastly, we introduce a multimodal triplet alignment objective for explicit
within- and cross-modal alignment between samples sharing detailed pathology
characteristics. Our demonstrations indicate that MedTrim improves performance
in downstream retrieval and classification tasks compared to state-of-the-art
alignment methods.

</details>


### [167] [Marginalized Generalized IoU (MGIoU): A Unified Objective Function for Optimizing Any Convex Parametric Shapes](https://arxiv.org/abs/2504.16443)
*Duy-Tho Le, Trung Pham, Jianfei Cai, Hamid Rezatofighi*

Main category: cs.CV

TL;DR: The paper introduces Marginalized Generalized IoU (MGIoU) and its variants (MGIoU+, MGIoU-) to unify and improve parametric shape optimization, offering efficiency, strong correlation with IoU, and robustness across diverse applications.


<details>
  <summary>Details</summary>
Motivation: Existing optimization methods for parametric shapes suffer from issues like lack of correlation with IoU, instability, computational intensity, and domain-specific limitations, leading to a fragmented landscape of objective functions.

Method: The authors propose MGIoU, which projects structured convex shapes onto their unique shape Normals to compute one-dimensional normalized GIoU, and extend it to MGIoU+ for unstructured convex shapes and MGIoU- for overlap minimization.

Result: Experiments show MGIoU and MGIoU+ outperform existing losses, reduce computation latency by 10-40x, and satisfy metric properties and scale-invariance.

Conclusion: MGIoU and its variants provide a unified, efficient, and robust solution for parametric shape optimization across diverse applications.

Abstract: Optimizing the similarity between parametric shapes is crucial for numerous
computer vision tasks, where Intersection over Union (IoU) stands as the
canonical measure. However, existing optimization methods exhibit significant
shortcomings: regression-based losses like L1/L2 lack correlation with IoU,
IoU-based losses are unstable and limited to simple shapes, and task-specific
methods are computationally intensive and not generalizable accross domains. As
a result, the current landscape of parametric shape objective functions has
become scattered, with each domain proposing distinct IoU approximations. To
address this, we unify the parametric shape optimization objective functions by
introducing Marginalized Generalized IoU (MGIoU), a novel loss function that
overcomes these challenges by projecting structured convex shapes onto their
unique shape Normals to compute one-dimensional normalized GIoU. MGIoU offers a
simple, efficient, fully differentiable approximation strongly correlated with
IoU. We then extend MGIoU to MGIoU+ that supports optimizing unstructured
convex shapes. Together, MGIoU and MGIoU+ unify parametric shape optimization
across diverse applications. Experiments on standard benchmarks demonstrate
that MGIoU and MGIoU+ consistently outperform existing losses while reducing
loss computation latency by 10-40x. Additionally, MGIoU and MGIoU+ satisfy
metric properties and scale-invariance, ensuring robustness as an objective
function. We further propose MGIoU- for minimizing overlaps in tasks like
collision-free trajectory prediction. Code is available at
https://ldtho.github.io/MGIoU

</details>


### [168] [V$^2$R-Bench: Holistically Evaluating LVLM Robustness to Fundamental Visual Variations](https://arxiv.org/abs/2504.16727)
*Zhiyuan Fan, Yumeng Wang, Sandeep Polisetty, Yi R. Fung*

Main category: cs.CV

TL;DR: V²R-Bench is introduced to evaluate LVLMs' robustness to visual variations, revealing vulnerabilities in object recognition and position bias, attributed to pipeline errors and poor multimodal alignment.


<details>
  <summary>Details</summary>
Motivation: To assess LVLMs' robustness to visual variations like position, scale, and orientation, which are underexplored despite their prevalence in natural scenes.

Method: Developed V²R-Bench, a benchmark framework with automated dataset generation and metrics, evaluated 21 LVLMs, and analyzed vulnerabilities through component-level analysis and synthetic data experiments.

Result: LVLMs show surprising vulnerability to visual variations, exhibiting position bias and human-like acuity thresholds, with issues traced to pipeline errors and inadequate multimodal alignment.

Conclusion: The findings highlight architectural deficiencies in LVLMs, calling for innovations to improve robustness to visual variations.

Abstract: Large Vision Language Models (LVLMs) excel in various vision-language tasks.
Yet, their robustness to visual variations in position, scale, orientation, and
context that objects in natural scenes inevitably exhibit due to changes in
viewpoint and environment remains largely underexplored. To bridge this gap, we
introduce V$^2$R-Bench, a comprehensive benchmark framework for evaluating
Visual Variation Robustness of LVLMs, which encompasses automated evaluation
dataset generation and principled metrics for thorough robustness assessment.
Through extensive evaluation on 21 LVLMs, we reveal a surprising vulnerability
to visual variations, in which even advanced models that excel at complex
vision-language tasks significantly underperform on simple tasks such as object
recognition. Interestingly, these models exhibit a distinct visual position
bias that contradicts theories of effective receptive fields, and demonstrate a
human-like visual acuity threshold. To identify the source of these
vulnerabilities, we present a systematic framework for component-level
analysis, featuring a novel visualization approach for aligned visual features.
Results show that these vulnerabilities stem from error accumulation in the
pipeline architecture and inadequate multimodal alignment. Complementary
experiments with synthetic data further demonstrate that these limitations are
fundamentally architectural deficiencies, scoring the need for architectural
innovations in future LVLM designs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [169] [A Framework for the Assurance of AI-Enabled Systems](https://arxiv.org/abs/2504.16937)
*Ariel S. Kapusta, David Jin, Peter M. Teague, Robert A. Houston, Jonathan B. Elliott, Grace Y. Park, Shelby S. Holdren*

Main category: cs.AI

TL;DR: The paper proposes a claims-based framework for AI assurance in defense applications to balance rapid deployment with rigorous risk management.


<details>
  <summary>Details</summary>
Motivation: The DOD seeks to accelerate AI adoption but faces challenges like technical, security, and ethical risks, requiring trustworthy assurance.

Method: A claims-based framework for risk management and assurance of AI systems, supporting all acquisition pathways.

Result: Provides a process for AI assurance, definitions for constructive discussions, and key considerations for trustworthy AI deployment.

Conclusion: The framework offers the DOD a robust yet efficient way to deploy AI capabilities while managing risks and maintaining trust.

Abstract: The United States Department of Defense (DOD) looks to accelerate the
development and deployment of AI capabilities across a wide spectrum of defense
applications to maintain strategic advantages. However, many common features of
AI algorithms that make them powerful, such as capacity for learning,
large-scale data ingestion, and problem-solving, raise new technical, security,
and ethical challenges. These challenges may hinder adoption due to uncertainty
in development, testing, assurance, processes, and requirements.
Trustworthiness through assurance is essential to achieve the expected value
from AI.
  This paper proposes a claims-based framework for risk management and
assurance of AI systems that addresses the competing needs for faster
deployment, successful adoption, and rigorous evaluation. This framework
supports programs across all acquisition pathways provide grounds for
sufficient confidence that an AI-enabled system (AIES) meets its intended
mission goals without introducing unacceptable risks throughout its lifecycle.
The paper's contributions are a framework process for AI assurance, a set of
relevant definitions to enable constructive conversations on the topic of AI
assurance, and a discussion of important considerations in AI assurance. The
framework aims to provide the DOD a robust yet efficient mechanism for swiftly
fielding effective AI capabilities without overlooking critical risks or
undermining stakeholder trust.

</details>


### [170] [Rational Inference in Formal Concept Analysis](https://arxiv.org/abs/2504.16938)
*Lucas Carr, Nicholas Leisegang, Thomas Meyer, Sergei Obiedkov*

Main category: cs.AI

TL;DR: The paper extends the KLM framework for defeasible reasoning to Formal Concept Analysis (FCA), offering a contextual and more relevant approach compared to the propositional case.


<details>
  <summary>Details</summary>
Motivation: Traditional implications in FCA are inadequate for handling erroneous or exceptional data, and non-monotonic inference in FCA has been understudied.

Method: The authors construct the KLM framework within FCA, ensuring it aligns with non-monotonic reasoning principles.

Result: The proposed defeasible reasoning in FCA provides a more contextual and relevant inference method than the propositional case.

Conclusion: The paper successfully adapts the KLM framework to FCA, enhancing its applicability and relevance in practical scenarios.

Abstract: Defeasible conditionals are a form of non-monotonic inference which enable
the expression of statements like "if $\phi$ then normally $\psi$". The KLM
framework defines a semantics for the propositional case of defeasible
conditionals by construction of a preference ordering over possible worlds. The
pattern of reasoning induced by these semantics is characterised by consequence
relations satisfying certain desirable properties of non-monotonic reasoning.
In FCA, implications are used to describe dependencies between attributes.
However, these implications are unsuitable to reason with erroneous data or
data prone to exceptions. Until recently, the topic of non-monotonic inference
in FCA has remained largely uninvestigated. In this paper, we provide a
construction of the KLM framework for defeasible reasoning in FCA and show that
this construction remains faithful to the principle of non-monotonic inference
described in the original framework. We present an additional argument that,
while remaining consistent with the original ideas around non-monotonic
reasoning, the defeasible reasoning we propose in FCA offers a more contextual
view on inference, providing the ability for more relevant conclusions to be
drawn when compared to the propositional case.

</details>


### [171] [A Desideratum for Conversational Agents: Capabilities, Challenges, and Future Directions](https://arxiv.org/abs/2504.16939)
*Emre Can Acikgoz, Cheng Qian, Hongru Wang, Vardhan Dongre, Xiusi Chen, Heng Ji, Dilek Hakkani-Tür, Gokhan Tur*

Main category: cs.AI

TL;DR: A survey on LLM-driven Conversational Agents, analyzing their capabilities (Reasoning, Monitor, Control), identifying gaps, and suggesting future research directions for AGI.


<details>
  <summary>Details</summary>
Motivation: To evaluate the progress and limitations of Conversational Agents powered by LLMs and propose a structured path toward human-level intelligence.

Method: Systematic analysis of Conversational Agents' capabilities, introduction of a novel taxonomy, and identification of research gaps.

Result: A structured foundation highlighting current limitations and future directions, including realistic evaluations and self-evolution capabilities.

Conclusion: The paper provides insights for advancing Conversational Agents toward AGI, with a curated repository for further research.

Abstract: Recent advances in Large Language Models (LLMs) have propelled conversational
AI from traditional dialogue systems into sophisticated agents capable of
autonomous actions, contextual awareness, and multi-turn interactions with
users. Yet, fundamental questions about their capabilities, limitations, and
paths forward remain open. This survey paper presents a desideratum for
next-generation Conversational Agents - what has been achieved, what challenges
persist, and what must be done for more scalable systems that approach
human-level intelligence. To that end, we systematically analyze LLM-driven
Conversational Agents by organizing their capabilities into three primary
dimensions: (i) Reasoning - logical, systematic thinking inspired by human
intelligence for decision making, (ii) Monitor - encompassing self-awareness
and user interaction monitoring, and (iii) Control - focusing on tool
utilization and policy following. Building upon this, we introduce a novel
taxonomy by classifying recent work on Conversational Agents around our
proposed desideratum. We identify critical research gaps and outline key
directions, including realistic evaluations, long-term multi-turn reasoning
skills, self-evolution capabilities, collaborative and multi-agent task
completion, personalization, and proactivity. This work aims to provide a
structured foundation, highlight existing limitations, and offer insights into
potential future research directions for Conversational Agents, ultimately
advancing progress toward Artificial General Intelligence (AGI). We maintain a
curated repository of papers at:
https://github.com/emrecanacikgoz/awesome-conversational-agents.

</details>


### [172] [A Systematic Approach to Design Real-World Human-in-the-Loop Deep Reinforcement Learning: Salient Features, Challenges and Trade-offs](https://arxiv.org/abs/2504.17006)
*Jalal Arabneydi, Saiful Islam, Srijita Das, Sai Krishna Gottipati, William Duguay, Cloderic Mars, Matthew E. Taylor, Matthew Guzdial, Antoine Fagette, Younes Zerouali*

Main category: cs.AI

TL;DR: A novel multi-layered hierarchical HITL DRL algorithm integrates self, imitation, and transfer learning with human inputs (reward, action, demonstration) to enhance decision-making and human-AI collaboration, validated in a UAV defense scenario.


<details>
  <summary>Details</summary>
Motivation: To leverage human expertise in DRL for faster training, better performance, and systematic integration of human inputs in AI solutions.

Method: Proposes a hierarchical HITL DRL algorithm combining self, imitation, and transfer learning, tested in a UAV defense problem using Cogment software.

Result: HITL improves training speed and performance; human advice reduces variance, with optimal advice quantity balancing over/under-training.

Conclusion: Human-AI collaboration via HITL DRL effectively solves complex scenarios like UAV defense, demonstrating scalability and practical utility.

Abstract: With the growing popularity of deep reinforcement learning (DRL),
human-in-the-loop (HITL) approach has the potential to revolutionize the way we
approach decision-making problems and create new opportunities for human-AI
collaboration. In this article, we introduce a novel multi-layered hierarchical
HITL DRL algorithm that comprises three types of learning: self learning,
imitation learning and transfer learning. In addition, we consider three forms
of human inputs: reward, action and demonstration. Furthermore, we discuss main
challenges, trade-offs and advantages of HITL in solving complex problems and
how human information can be integrated in the AI solution systematically. To
verify our technical results, we present a real-world unmanned aerial vehicles
(UAV) problem wherein a number of enemy drones attack a restricted area. The
objective is to design a scalable HITL DRL algorithm for ally drones to
neutralize the enemy drones before they reach the area. To this end, we first
implement our solution using an award-winning open-source HITL software called
Cogment. We then demonstrate several interesting results such as (a) HITL leads
to faster training and higher performance, (b) advice acts as a guiding
direction for gradient methods and lowers variance, and (c) the amount of
advice should neither be too large nor too small to avoid over-training and
under-training. Finally, we illustrate the role of human-AI cooperation in
solving two real-world complex scenarios, i.e., overloaded and decoy attacks.

</details>


### [173] [Neural Theorem Proving: Generating and Structuring Proofs for Formal Verification](https://arxiv.org/abs/2504.17017)
*Balaji Rao, William Eiers, Carlo Lipizzi*

Main category: cs.AI

TL;DR: A framework for generating formal proofs in LLMs, combining natural language statements, proof generation, and heuristic-based proof assembly, validated on miniF2F-test and Isabelle.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of formal verification in LLM-generated code and advance theorem proving as a benchmark for reasoning in LLMs.

Method: A 3-component framework: natural language statement generation, LLM-based formal proof generation, and heuristic-driven proof assembly. Training involves SFT and RL-based fine-tuning.

Result: Validated on miniF2F-test and Isabelle, with a use case for AWS S3 bucket policy verification. A curated dataset (FVEL_ER) is also introduced.

Conclusion: The framework advances formal verification in LLMs, demonstrating potential for practical applications like code correctness verification.

Abstract: Formally verifying properties of software code has been a highly desirable
task, especially with the emergence of LLM-generated code. In the same vein,
they provide an interesting avenue for the exploration of formal verification
and mechanistic interpretability. Since the introduction of code-specific
models, despite their successes in generating code in Lean4 and Isabelle, the
task of generalized theorem proving still remains far from being fully solved
and will be a benchmark for reasoning capability in LLMs. In this work, we
introduce a framework that generates whole proofs in a formal language to be
used within systems that utilize the power of built-in tactics and
off-the-shelf automated theorem provers. Our framework includes 3 components:
generating natural language statements of the code to be verified, an LLM that
generates formal proofs for the given statement, and a module employing
heuristics for building the final proof. To train the LLM, we employ a 2-stage
fine-tuning process, where we first use SFT-based training to enable the model
to generate syntactically correct Isabelle code and then RL-based training that
encourages the model to generate proofs verified by a theorem prover. We
validate our framework using the miniF2F-test benchmark and the Isabelle proof
assistant and design a use case to verify the correctness of the AWS S3 bucket
access policy code. We also curate a dataset based on the
FVEL\textsubscript{\textnormal{ER}} dataset for future training tasks.

</details>


### [174] [Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments](https://arxiv.org/abs/2504.17087)
*Yuran Li, Jama Hussein Mohamud, Chongren Sun, Di Wu, Benoit Boulet*

Main category: cs.AI

TL;DR: A three-stage pipeline improves LLM-based evaluation by addressing biases and selecting better judgments, outperforming single-agent methods.


<details>
  <summary>Details</summary>
Motivation: Challenges in evaluating complex LLM tasks and overlooked biases in human judgment necessitate a better evaluation method.

Method: Proposes a three-stage pipeline: rubric development with GPT-4 and humans, scoring by three LLM agents, and filtering low-scoring judgments.

Result: 15.55% improvement over raw judgments and 8.37% over single-agent baselines on JudgeBench.

Conclusion: LLMs show promise as meta-judges, enabling better preference datasets for reinforcement learning.

Abstract: Large language models (LLMs) are being widely applied across various fields,
but as tasks become more complex, evaluating their responses is increasingly
challenging. Compared to human evaluators, the use of LLMs to support
performance evaluation offers a more efficient alternative. However, most
studies focus mainly on aligning LLMs' judgments with human preferences,
overlooking the existence of biases and mistakes in human judgment.
Furthermore, how to select suitable LLM judgments given multiple potential LLM
responses remains underexplored. To address these two aforementioned issues, we
propose a three-stage meta-judge selection pipeline: 1) developing a
comprehensive rubric with GPT-4 and human experts, 2) using three advanced LLM
agents to score judgments, and 3) applying a threshold to filter out
low-scoring judgments. Compared to methods using a single LLM as both judge and
meta-judge, our pipeline introduces multi-agent collaboration and a more
comprehensive rubric. Experimental results on the JudgeBench dataset show about
15.55\% improvement compared to raw judgments and about 8.37\% improvement over
the single-agent baseline. Our work demonstrates the potential of LLMs as
meta-judges and lays the foundation for future research on constructing
preference datasets for LLM-as-a-judge reinforcement learning.

</details>


### [175] [AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle Perception Systems using Adversarially Guided Diffusion Models](https://arxiv.org/abs/2504.17179)
*Mohammad Zarei, Melanie A Jutras, Eliana Evans, Mike Tan, Omid Aaramoon*

Main category: cs.AI

TL;DR: A novel approach using generative and explainable AI to address rare failure modes (RFMs) in AVs by generating diverse environments and natural language descriptions for improved robustness.


<details>
  <summary>Details</summary>
Motivation: AVs struggle with detecting RFMs due to the long-tail challenge, where rare events are underrepresented in training data.

Method: Utilizes segmentation masks, environmental masks, and a custom diffusion model (Stable Diffusion inpainting) with adversarial noise optimization to generate RFM scenarios.

Result: Generates images and descriptions of RFMs to expose AI vulnerabilities, aiding in model training and testing.

Conclusion: The approach enhances AV safety and reliability by addressing RFMs through generative AI and explainability.

Abstract: Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately
detect objects and interpret their surroundings. However, even when trained
using millions of miles of real-world data, AVs are often unable to detect rare
failure modes (RFMs). The problem of RFMs is commonly referred to as the
"long-tail challenge", due to the distribution of data including many instances
that are very rarely seen. In this paper, we present a novel approach that
utilizes advanced generative and explainable AI techniques to aid in
understanding RFMs. Our methods can be used to enhance the robustness and
reliability of AVs when combined with both downstream model training and
testing. We extract segmentation masks for objects of interest (e.g., cars) and
invert them to create environmental masks. These masks, combined with carefully
crafted text prompts, are fed into a custom diffusion model. We leverage the
Stable Diffusion inpainting model guided by adversarial noise optimization to
generate images containing diverse environments designed to evade object
detection models and expose vulnerabilities in AI systems. Finally, we produce
natural language descriptions of the generated RFMs that can guide developers
and policymakers to improve the safety and reliability of AV systems.

</details>


### [176] [Cracking the Code of Action: a Generative Approach to Affordances for Reinforcement Learning](https://arxiv.org/abs/2504.17282)
*Lynn Cherif, Flemming Kondrup, David Venuto, Ankit Anand, Doina Precup, Khimya Khetarpal*

Main category: cs.AI

TL;DR: The paper introduces CoGA, a method using pre-trained vision-language models to generate code for action affordances, improving sample efficiency in web GUI navigation tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of low sample efficiency in sparse-reward, large-action-space environments like web GUIs, especially with limited expert demonstrations.

Method: CoGA leverages vision-language models to generate code for intent-based affordances, automating program generation and verification to constrain the action space.

Result: CoGA significantly improves sample efficiency, generalizes within task families, and matches or outperforms behavior cloning with few expert demonstrations.

Conclusion: CoGA demonstrates the effectiveness of intent-based affordances and automated code generation for efficient learning in complex environments.

Abstract: Agents that can autonomously navigate the web through a graphical user
interface (GUI) using a unified action space (e.g., mouse and keyboard actions)
can require very large amounts of domain-specific expert demonstrations to
achieve good performance. Low sample efficiency is often exacerbated in
sparse-reward and large-action-space environments, such as a web GUI, where
only a few actions are relevant in any given situation. In this work, we
consider the low-data regime, with limited or no access to expert behavior. To
enable sample-efficient learning, we explore the effect of constraining the
action space through $\textit{intent-based affordances}$ -- i.e., considering
in any situation only the subset of actions that achieve a desired outcome. We
propose $\textbf{Code as Generative Affordances}$ $(\textbf{$\texttt{CoGA}$})$,
a method that leverages pre-trained vision-language models (VLMs) to generate
code that determines affordable actions through implicit intent-completion
functions and using a fully-automated program generation and verification
pipeline. These programs are then used in-the-loop of a reinforcement learning
agent to return a set of affordances given a pixel observation. By greatly
reducing the number of actions that an agent must consider, we demonstrate on a
wide range of tasks in the MiniWob++ benchmark that: $\textbf{1)}$
$\texttt{CoGA}$ is orders of magnitude more sample efficient than its RL agent,
$\textbf{2)}$ $\texttt{CoGA}$'s programs can generalize within a family of
tasks, and $\textbf{3)}$ $\texttt{CoGA}$ performs better or on par compared
with behavior cloning when a small number of expert demonstrations is
available.

</details>


### [177] [AI-Enhanced Business Process Automation: A Case Study in the Insurance Domain Using Object-Centric Process Mining](https://arxiv.org/abs/2504.17295)
*Shahrzad Khayatbashi, Viktor Sjölind, Anders Granåker, Amin Jalali*

Main category: cs.AI

TL;DR: The paper explores AI-driven automation in business processes using LLMs, demonstrated through an insurance case study, and evaluates its impact with OCPM.


<details>
  <summary>Details</summary>
Motivation: To assess the impact of AI-driven automation on business processes, especially during transitions between traditional and AI-enhanced variants, using a real-world case study.

Method: Applied Object-Centric Process Mining (OCPM) to analyze the deployment of an LLM in automating claim parts identification in the insurance sector.

Result: LLMs significantly improve operational capacity but introduce new process dynamics needing refinement. OCPM proves useful but has limitations.

Conclusion: AI-driven automation enhances scalability but requires process adjustments. OCPM is a practical tool for such evaluations, though further refinement is needed.

Abstract: Recent advancements in Artificial Intelligence (AI), particularly Large
Language Models (LLMs), have enhanced organizations' ability to reengineer
business processes by automating knowledge-intensive tasks. This automation
drives digital transformation, often through gradual transitions that improve
process efficiency and effectiveness. To fully assess the impact of such
automation, a data-driven analysis approach is needed - one that examines how
traditional and AI-enhanced process variants coexist during this transition.
Object-Centric Process Mining (OCPM) has emerged as a valuable method that
enables such analysis, yet real-world case studies are still needed to
demonstrate its applicability. This paper presents a case study from the
insurance sector, where an LLM was deployed in production to automate the
identification of claim parts, a task previously performed manually and
identified as a bottleneck for scalability. To evaluate this transformation, we
apply OCPM to assess the impact of AI-driven automation on process scalability.
Our findings indicate that while LLMs significantly enhance operational
capacity, they also introduce new process dynamics that require further
refinement. This study also demonstrates the practical application of OCPM in a
real-world setting, highlighting its advantages and limitations.

</details>


### [178] [Comprehend, Divide, and Conquer: Feature Subspace Exploration via Multi-Agent Hierarchical Reinforcement Learning](https://arxiv.org/abs/2504.17356)
*Weiliang Zhang, Xiaohan Huang, Yi Du, Ziyue Qiao, Qingqing Long, Zhen Meng, Yuanchun Zhou, Meng Xiao*

Main category: cs.AI

TL;DR: HRLFS is a novel feature selection method using hierarchical RL and LLM-based hybrid state extraction to improve efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Current RL-based feature selection methods struggle with complex datasets due to inefficiencies like one-agent-per-feature paradigms.

Method: Uses LLM-based hybrid state extraction to cluster features, then hierarchical RL agents for each cluster/sub-cluster.

Result: HRLFS outperforms contemporary methods, improving ML performance and reducing runtime.

Conclusion: HRLFS offers a scalable, robust solution for feature selection in complex datasets.

Abstract: Feature selection aims to preprocess the target dataset, find an optimal and
most streamlined feature subset, and enhance the downstream machine learning
task. Among filter, wrapper, and embedded-based approaches, the reinforcement
learning (RL)-based subspace exploration strategy provides a novel objective
optimization-directed perspective and promising performance. Nevertheless, even
with improved performance, current reinforcement learning approaches face
challenges similar to conventional methods when dealing with complex datasets.
These challenges stem from the inefficient paradigm of using one agent per
feature and the inherent complexities present in the datasets. This observation
motivates us to investigate and address the above issue and propose a novel
approach, namely HRLFS. Our methodology initially employs a Large Language
Model (LLM)-based hybrid state extractor to capture each feature's mathematical
and semantic characteristics. Based on this information, features are
clustered, facilitating the construction of hierarchical agents for each
cluster and sub-cluster. Extensive experiments demonstrate the efficiency,
scalability, and robustness of our approach. Compared to contemporary or the
one-feature-one-agent RL-based approaches, HRLFS improves the downstream ML
performance with iterative feature subspace exploration while accelerating
total run time by reducing the number of agents involved.

</details>


### [179] [Assessing the Capability of Large Language Models for Domain-Specific Ontology Generation](https://arxiv.org/abs/2504.17402)
*Anna Sofia Lippolis, Mohammad Javad Saeedizade, Robin Keskisarkka, Aldo Gangemi, Eva Blomqvist, Andrea Giovanni Nuzzolese*

Main category: cs.AI

TL;DR: LLMs like DeepSeek and o1-preview show consistent performance in domain-specific ontology generation, indicating their potential for scalable, domain-agnostic ontology engineering.


<details>
  <summary>Details</summary>
Motivation: To explore the applicability of LLMs in domain-specific ontology generation and evaluate their generalizability.

Method: Evaluated two LLMs (DeepSeek and o1-preview) using competency questions (CQs) and user stories across six domains.

Result: Both LLMs performed consistently across domains, demonstrating generalizability in ontology generation.

Conclusion: LLMs hold promise for scalable, domain-agnostic ontology construction, paving the way for further research in automated reasoning.

Abstract: Large Language Models (LLMs) have shown significant potential for ontology
engineering. However, it is still unclear to what extent they are applicable to
the task of domain-specific ontology generation. In this study, we explore the
application of LLMs for automated ontology generation and evaluate their
performance across different domains. Specifically, we investigate the
generalizability of two state-of-the-art LLMs, DeepSeek and o1-preview, both
equipped with reasoning capabilities, by generating ontologies from a set of
competency questions (CQs) and related user stories. Our experimental setup
comprises six distinct domains carried out in existing ontology engineering
projects and a total of 95 curated CQs designed to test the models' reasoning
for ontology engineering. Our findings show that with both LLMs, the
performance of the experiments is remarkably consistent across all domains,
indicating that these methods are capable of generalizing ontology generation
tasks irrespective of the domain. These results highlight the potential of
LLM-based approaches in achieving scalable and domain-agnostic ontology
construction and lay the groundwork for further research into enhancing
automated reasoning and knowledge representation techniques.

</details>


### [180] [Redefining Superalignment: From Weak-to-Strong Alignment to Human-AI Co-Alignment to Sustainable Symbiotic Society](https://arxiv.org/abs/2504.17404)
*Feifei Zhao, Yuwei Wang, Enmeng Lu, Dongcheng Zhao, Bing Han, Haibo Tong, Yao Liang, Dongqi Liang, Kang Sun, Lei Wang, Yitao Liang, Chao Liu, Yaodong Yang, Yi Zeng*

Main category: cs.AI

TL;DR: The paper addresses the challenge of ensuring superalignment in AI systems to prevent catastrophic outcomes, proposing a dual framework of external oversight and intrinsic proactive alignment for human-AI co-alignment.


<details>
  <summary>Details</summary>
Motivation: The progression from AI to ASI poses risks of loss of control and misalignment with human values, necessitating safer frameworks for superalignment.

Method: The paper redefines superalignment as human-AI co-alignment, integrating external oversight (human-centered decisions, automated evaluation) and intrinsic proactive alignment (self-awareness, empathy).

Result: The proposed framework aims to achieve sustainable symbiotic societies by combining external and intrinsic alignment methods.

Conclusion: The integration of oversight and proactive alignment paves the way for safe and beneficial AGI and ASI, aligning AI with human values and well-being.

Abstract: Artificial Intelligence (AI) systems are becoming increasingly powerful and
autonomous, and may progress to surpass human intelligence levels, namely
Artificial Superintelligence (ASI). During the progression from AI to ASI, it
may exceed human control, violate human values, and even lead to irreversible
catastrophic consequences in extreme cases. This gives rise to a pressing issue
that needs to be addressed: superalignment, ensuring that AI systems much
smarter than humans, remain aligned with human (compatible) intentions and
values. Existing scalable oversight and weak-to-strong generalization methods
may prove substantially infeasible and inadequate when facing ASI. We must
explore safer and more pluralistic frameworks and approaches for
superalignment. In this paper, we redefine superalignment as the human-AI
co-alignment towards a sustainable symbiotic society, and highlight a framework
that integrates external oversight and intrinsic proactive alignment. External
oversight superalignment should be grounded in human-centered ultimate
decision, supplemented by interpretable automated evaluation and correction, to
achieve continuous alignment with humanity's evolving values. Intrinsic
proactive superalignment is rooted in a profound understanding of the self,
others, and society, integrating self-awareness, self-reflection, and empathy
to spontaneously infer human intentions, distinguishing good from evil and
proactively considering human well-being, ultimately attaining human-AI
co-alignment through iterative interaction. The integration of
externally-driven oversight with intrinsically-driven proactive alignment
empowers sustainable symbiotic societies through human-AI co-alignment, paving
the way for achieving safe and beneficial AGI and ASI for good, for human, and
for a symbiotic ecology.

</details>


### [181] [Towards Machine-Generated Code for the Resolution of User Intentions](https://arxiv.org/abs/2504.17531)
*Justus Flerlage, Ilja Behnke, Odej Kao*

Main category: cs.AI

TL;DR: The paper explores using LLMs to generate and execute workflows from user intentions, showing feasibility with GPT-4o-mini.


<details>
  <summary>Details</summary>
Motivation: Reassess user-device interaction by leveraging AI to automate workflows through code generation, reducing reliance on high-level applications.

Method: Prompt an LLM (GPT-4o-mini) with user intentions and a simplified API to generate and execute code-oriented workflows.

Result: Demonstrates feasibility; GPT-4o-mini excels in generating workflows aligned with user intentions.

Conclusion: AI-driven workflow generation is viable, marking progress in hybrid human-AI collaboration for intent resolution.

Abstract: The growing capabilities of Artificial Intelligence (AI), particularly Large
Language Models (LLMs), prompt a reassessment of the interaction mechanisms
between users and their devices. Currently, users are required to use a set of
high-level applications to achieve their desired results. However, the advent
of AI may signal a shift in this regard, as its capabilities have generated
novel prospects for user-provided intent resolution through the deployment of
model-generated code, which is tantamount to the generation of workflows
comprising a multitude of interdependent steps. This development represents a
significant progression in the realm of hybrid workflows, where human and
artificial intelligence collaborate to address user intentions, with the former
responsible for defining these intentions and the latter for implementing the
solutions to address them. In this paper, we investigate the feasibility of
generating and executing workflows through code generation that results from
prompting an LLM with a concrete user intention, such as \emph{Please send my
car title to my insurance company}, and a simplified application programming
interface for a GUI-less operating system. We provide in-depth analysis and
comparison of various user intentions, the resulting code, and its execution.
The findings demonstrate a general feasibility of our approach and that the
employed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of
code-oriented workflows in accordance with provided user intentions.

</details>


### [182] [Auditing the Ethical Logic of Generative AI Models](https://arxiv.org/abs/2504.17544)
*W. Russell Neuman, Chad Coleman, Ali Dasdan, Safinah Ali, Manan Shah*

Main category: cs.AI

TL;DR: A five-dimensional audit model evaluates ethical reasoning in LLMs, revealing variability in explanatory rigor and moral prioritization, with Chain-of-Thought prompting improving performance.


<details>
  <summary>Details</summary>
Motivation: To address the need for robust ethical evaluation of generative AI models in high-stakes domains.

Method: A multi-battery prompt approach, including ethical dilemmas, to assess LLMs across five dimensions: Analytic Quality, Breadth of Ethical Considerations, Depth of Explanation, Consistency, and Decisiveness.

Result: Benchmarking seven LLMs showed convergence on ethical decisions but variability in explanatory rigor and moral prioritization. Chain-of-Thought prompting improved performance.

Conclusion: The study presents a scalable method for ethical AI benchmarking and suggests AI's potential to complement human moral reasoning.

Abstract: As generative AI models become increasingly integrated into high-stakes
domains, the need for robust methods to evaluate their ethical reasoning
becomes increasingly important. This paper introduces a five-dimensional audit
model -- assessing Analytic Quality, Breadth of Ethical Considerations, Depth
of Explanation, Consistency, and Decisiveness -- to evaluate the ethical logic
of leading large language models (LLMs). Drawing on traditions from applied
ethics and higher-order thinking, we present a multi-battery prompt approach,
including novel ethical dilemmas, to probe the models' reasoning across diverse
contexts. We benchmark seven major LLMs finding that while models generally
converge on ethical decisions, they vary in explanatory rigor and moral
prioritization. Chain-of-Thought prompting and reasoning-optimized models
significantly enhance performance on our audit metrics. This study introduces a
scalable methodology for ethical benchmarking of AI systems and highlights the
potential for AI to complement human moral reasoning in complex decision-making
contexts.

</details>


### [183] [Learning Type-Generalized Actions for Symbolic Planning](https://arxiv.org/abs/2308.04867)
*Daniel Tanneberg, Michael Gienger*

Main category: cs.AI

TL;DR: The paper introduces a method to generalize symbolic actions using entity hierarchies and observed behavior, enabling transferability to novel tasks and environments.


<details>
  <summary>Details</summary>
Motivation: Symbolic planning requires hand-designed representations, limiting transferability. The goal is to generalize actions for broader applicability.

Method: Proposes type-generalized actions learned from few observations, using an entity hierarchy and on-the-fly generalization during planning.

Result: Demonstrated in a grid-based kitchen environment, the method generalizes to novel tasks, entities, and longer sequences.

Conclusion: The approach enhances symbolic planning by reducing dependency on hand-designed representations and improving adaptability.

Abstract: Symbolic planning is a powerful technique to solve complex tasks that require
long sequences of actions and can equip an intelligent agent with complex
behavior. The downside of this approach is the necessity for suitable symbolic
representations describing the state of the environment as well as the actions
that can change it. Traditionally such representations are carefully
hand-designed by experts for distinct problem domains, which limits their
transferability to different problems and environment complexities. In this
paper, we propose a novel concept to generalize symbolic actions using a given
entity hierarchy and observed similar behavior. In a simulated grid-based
kitchen environment, we show that type-generalized actions can be learned from
few observations and generalize to novel situations. Incorporating an
additional on-the-fly generalization mechanism during planning, unseen task
combinations, involving longer sequences, novel entities and unexpected
environment behavior, can be solved.

</details>


### [184] [Unlocking Large Language Model's Planning Capabilities with Maximum Diversity Fine-tuning](https://arxiv.org/abs/2406.10479)
*Wenjun Li, Changyu Chen, Pradeep Varakantham*

Main category: cs.AI

TL;DR: The paper explores fine-tuning LLMs for planning tasks with limited data, proposing CMDS for efficient data sampling and introducing CMDS-g for improved performance.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with planning tasks lacking sufficient pre-training data, and fine-tuning is costly. The paper aims to improve efficiency and generalization.

Method: Proposes Clustering-Based Maximum Diversity Sampling (CMDS) for data selection and introduces CMDS-g, which uses graph representations for better embeddings.

Result: CMDS-g outperforms baselines, showing consistent improvements in performance across various scales and domains.

Conclusion: Fine-tuning with CMDS and CMDS-g enhances LLM planning capabilities efficiently and effectively.

Abstract: Large language models (LLMs) have demonstrated impressive task-solving
capabilities through prompting techniques and system designs, including solving
planning tasks (e.g., math proofs, basic travel planning) when sufficient data
is available online and used during pre-training. However, for planning tasks
with limited prior data (e.g., blocks world, advanced travel planning), the
performance of LLMs, including proprietary models like GPT and Gemini, is poor.
This paper investigates the impact of fine-tuning on the planning capabilities
of LLMs, revealing that LLMs can achieve strong performance in planning through
substantial (tens of thousands of specific examples) fine-tuning. Yet, this
process incurs high economic, time, and computational costs for each planning
problem variation. To address this, we propose Clustering-Based Maximum
Diversity Sampling (CMDS), which selects diverse and representative data to
enhance sample efficiency and the model's generalization capability. Extensive
evaluations demonstrate that CMDS-l, a baseline method combining CMDS with
language embeddings, outperforms random sampling. Furthermore, we introduce a
novel algorithm, CMDS-g, which encodes planning task instances with their graph
representations into the embedding space. Empirical results show that CMDS-g
consistently outperforms baseline methods across various scales and multiple
benchmark domains.

</details>


### [185] [Feature-to-Image Data Augmentation: Improving Model Feature Extraction with Cluster-Guided Synthetic Samples](https://arxiv.org/abs/2409.17685)
*Yasaman Haghbin, Hadi Moradi, Reshad Hosseini*

Main category: cs.AI

TL;DR: FICAug is a feature-to-image data augmentation framework that improves model generalization under limited data by generating structured synthetic samples.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in collecting large datasets, especially in medical and low-resource domains, to prevent overfitting and poor generalization.

Method: Clusters data in feature space using k-means, generates synthetic samples via Gaussian sampling, projects them back to image domain with a generative neural network, and trains a CNN on reconstructed images.

Result: Achieved 84.09% cross-validation accuracy in feature space and 88.63% with ResNet-18 on reconstructed images.

Conclusion: FICAug effectively enhances classification accuracy and feature extraction in limited-data scenarios.

Abstract: One of the growing trends in machine learning is the use of data generation
techniques, since the performance of machine learning models is dependent on
the quantity of the training dataset. However, in many real-world applications,
particularly in medical and low-resource domains, collecting large datasets is
challenging due to resource constraints, which leads to overfitting and poor
generalization. This study introduces FICAug, a novel feature-to-image data
augmentation framework designed to improve model generalization under limited
data conditions by generating structured synthetic samples.
  FICAug first operates in the feature space, where original data are clustered
using the k-means algorithm. Within pure-label clusters, synthetic data are
generated through Gaussian sampling to increase diversity while maintaining
label consistency. These synthetic features are then projected back into the
image domain using a generative neural network, and a convolutional neural
network is trained on the reconstructed images to learn enhanced
representations.
  Experimental results demonstrate that FICAug significantly improves
classification accuracy. In feature space, it achieved a cross-validation
accuracy of 84.09%, while training a ResNet-18 model on the reconstructed
images further boosted performance to 88.63%, illustrating the effectiveness of
the proposed framework in extracting new and task-relevant features.

</details>


### [186] [Enhancing LLMs with Smart Preprocessing for EHR Analysis](https://arxiv.org/abs/2412.02868)
*Yixiang Qu, Yifan Dai, Shilin Yu, Pradham Tanikella, Travis Schrank, Trevor Hackman, Didong Li, Di Wu*

Main category: cs.AI

TL;DR: A compact LLM framework for EHR processing is introduced, optimized for privacy and resource constraints, using preprocessing techniques like regex and RAG to enhance smaller LLMs' performance.


<details>
  <summary>Details</summary>
Motivation: To address privacy and computational limitations in deploying LLMs for sensitive healthcare applications like EHR processing.

Method: Leverages preprocessing (regex and RAG) to filter and highlight critical EHR data, evaluated via zero-shot and few-shot learning on datasets like MIMIC-IV.

Result: Preprocessing significantly boosts smaller LLMs' performance, making them viable for privacy-sensitive, resource-limited settings.

Conclusion: The framework offers practical guidance for deploying LLMs in healthcare, balancing privacy, computational feasibility, and clinical utility.

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in
natural language processing; however, their application in sensitive domains
such as healthcare, especially in processing Electronic Health Records (EHRs),
is constrained by limited computational resources and privacy concerns. This
paper introduces a compact LLM framework optimized for local deployment in
environments with stringent privacy requirements and restricted access to
high-performance GPUs. Our approach leverages simple yet powerful preprocessing
techniques, including regular expressions (regex) and Retrieval-Augmented
Generation (RAG), to extract and highlight critical information from clinical
notes. By pre-filtering long, unstructured text, we enhance the performance of
smaller LLMs on EHR-related tasks. Our framework is evaluated using zero-shot
and few-shot learning paradigms on both private and publicly available datasets
(MIMIC-IV), with additional comparisons against fine-tuned LLMs on MIMIC-IV.
Experimental results demonstrate that our preprocessing strategy significantly
supercharges the performance of smaller LLMs, making them well-suited for
privacy-sensitive and resource-constrained applications. This study offers
valuable insights into optimizing LLM performance for local, secure, and
efficient healthcare applications. It provides practical guidance for
real-world deployment for LLMs while tackling challenges related to privacy,
computational feasibility, and clinical applicability.

</details>


### [187] [Neural DNF-MT: A Neuro-symbolic Approach for Learning Interpretable and Editable Policies](https://arxiv.org/abs/2501.03888)
*Kexin Gu Baugh, Luke Dickens, Alessandra Russo*

Main category: cs.AI

TL;DR: The paper introduces neural DNF-MT, a neuro-symbolic approach for interpretable deep reinforcement learning policies, combining deep learning with logic programs.


<details>
  <summary>Details</summary>
Motivation: Address the black-box nature of deep reinforcement learning models by enabling interpretable policy learning.

Method: Proposes neural DNF-MT, a differentiable model trained with actor-critic algorithms, translating into interpretable logic programs. Supports predicate invention for complex observations.

Result: Performs comparably to black-box methods while providing interpretable, editable logic-based policies.

Conclusion: Neural DNF-MT bridges deep learning and symbolic logic, offering interpretability without sacrificing performance.

Abstract: Although deep reinforcement learning has been shown to be effective, the
model's black-box nature presents barriers to direct policy interpretation. To
address this problem, we propose a neuro-symbolic approach called neural DNF-MT
for end-to-end policy learning. The differentiable nature of the neural DNF-MT
model enables the use of deep actor-critic algorithms for training. At the same
time, its architecture is designed so that trained models can be directly
translated into interpretable policies expressed as standard (bivalent or
probabilistic) logic programs. Moreover, additional layers can be included to
extract abstract features from complex observations, acting as a form of
predicate invention. The logic representations are highly interpretable, and we
show how the bivalent representations of deterministic policies can be edited
and incorporated back into a neural model, facilitating manual intervention and
adaptation of learned policies. We evaluate our approach on a range of tasks
requiring learning deterministic or stochastic behaviours from various forms of
observations. Our empirical results show that our neural DNF-MT model performs
at the level of competing black-box methods whilst providing interpretable
policies.

</details>


### [188] [Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?](https://arxiv.org/abs/2501.15857)
*Yutong Yin, Zhaoran Wang*

Main category: cs.AI

TL;DR: Transformers can perform compositional reasoning on the FTCT task, integrating fragmented knowledge during testing, enabled by few-shot Chain-of-Thought prompting.


<details>
  <summary>Details</summary>
Motivation: To validate if Transformers can replicate human-like compositional reasoning by integrating fragmented knowledge, and to understand the underlying mechanisms.

Method: Introduces the FTCT task, where models train on fragmented causal knowledge and test by inferring complete causal traces. Uses few-shot Chain-of-Thought prompting.

Result: Transformers succeed in compositional reasoning, correlating with model complexity and data similarity. They learn a generalizable program.

Conclusion: Transformers can achieve human-like compositional reasoning, with performance linked to model design and training-testing alignment.

Abstract: Humans exhibit remarkable compositional reasoning by integrating knowledge
from various sources. For example, if someone learns ( B = f(A) ) from one
source and ( C = g(B) ) from another, they can deduce ( C=g(B)=g(f(A)) ) even
without encountering ( ABC ) together, showcasing the generalization ability of
human intelligence. In this paper, we introduce a synthetic learning task,
"FTCT" (Fragmented at Training, Chained at Testing), to validate the potential
of Transformers in replicating this skill and interpret its inner mechanism. In
the training phase, data consist of separated knowledge fragments from an
overall causal graph. During testing, Transformers must infer complete causal
graph traces by integrating these fragments. Our findings demonstrate that
few-shot Chain-of-Thought prompting enables Transformers to perform
compositional reasoning on FTCT by revealing correct combinations of fragments,
even if such combinations were absent in the training data. Furthermore, the
emergence of compositional reasoning ability is strongly correlated with the
model complexity and training-testing data similarity. We propose, both
theoretically and empirically, that Transformers learn an underlying
generalizable program from training, enabling effective compositional reasoning
during testing.

</details>


### [189] [Engineering Artificial Intelligence: Framework, Challenges, and Future Direction](https://arxiv.org/abs/2504.02269)
*Jay Lee, Hanqi Su, Dai-Yan Ji, Takanobu Minami*

Main category: cs.AI

TL;DR: The paper proposes the 'ABCDE' framework for Engineering AI, introducing a systematic ecosystem with eight layers to address domain-specific challenges and guide AI development in engineering.


<details>
  <summary>Details</summary>
Motivation: The complexity and diversity of engineering problems lack systematic methodologies for AI development, requiring domain-specific solutions.

Method: Introduces the 'ABCDE' framework and an eight-layer ecosystem to guide AI development for engineering needs.

Result: A unified framework with attributes, goals, and applications is presented, along with key challenges and future research directions.

Conclusion: The paper aims to advance AI implementation in engineering, fostering next-generation solutions through a comprehensive perspective.

Abstract: Over the past ten years, the application of artificial intelligence (AI) and
machine learning (ML) in engineering domains has gained significant popularity,
showcasing their potential in data-driven contexts. However, the complexity and
diversity of engineering problems often require the development of
domain-specific AI approaches, which are frequently hindered by a lack of
systematic methodologies, scalability, and robustness during the development
process. To address this gap, this paper introduces the "ABCDE" as the key
elements of Engineering AI and proposes a unified, systematic engineering AI
ecosystem framework, including eight essential layers, along with attributes,
goals, and applications, to guide the development and deployment of AI
solutions for specific engineering needs. Additionally, key challenges are
examined, and eight future research directions are highlighted. By providing a
comprehensive perspective, this paper aims to advance the strategic
implementation of AI, fostering the development of next-generation engineering
AI solutions.

</details>


### [190] [TALES: Text Adventure Learning Environment Suite](https://arxiv.org/abs/2504.14128)
*Christopher Zhang Cui, Xingdi Yuan, Ziang Xiao, Prithviraj Ammanabrolu, Marc-Alexandre Côté*

Main category: cs.AI

TL;DR: TALES is a collection of text-adventure games designed to evaluate diverse reasoning in LLMs, showing top models struggle with human-designed games.


<details>
  <summary>Details</summary>
Motivation: To challenge and assess the reasoning capabilities of LLMs in complex, sequential decision-making tasks.

Method: Introduces TALES, a mix of synthetic and human-written text-adventure games, and evaluates various LLMs on them.

Result: Top LLMs perform well on synthetic games but score below 15% on human-designed games.

Conclusion: Current LLMs lack the reasoning sophistication for human-level performance in complex, enjoyable tasks.

Abstract: Reasoning is an essential skill to enable Large Language Models (LLMs) to
interact with the world. As tasks become more complex, they demand increasingly
sophisticated and diverse reasoning capabilities for sequential
decision-making, requiring structured reasoning over the context history to
determine the next best action. We introduce TALES, a diverse collection of
synthetic and human-written text-adventure games designed to challenge and
evaluate diverse reasoning capabilities. We present results over a range of
LLMs, open- and closed-weights, performing a qualitative analysis on the top
performing models. Despite an impressive showing on synthetic games, even the
top LLM-driven agents fail to achieve 15% on games designed for human
enjoyment. Code and visualization of the experiments can be found at
https://microsoft.github.io/tale-suite.

</details>


### [191] [KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM Inference in Resource-Constrained Environments](https://arxiv.org/abs/2504.15364)
*Junyoung Park, Dalton Jones, Matt J Morse, Raghavv Goel, Mingu Lee, Chris Lott*

Main category: cs.AI

TL;DR: KeyDiff is a training-free KV cache eviction method for LLMs, optimizing key diversity to handle long prompts efficiently under resource constraints.


<details>
  <summary>Details</summary>
Motivation: To enable LLM deployment in resource-limited environments by addressing the challenge of processing long input prompts.

Method: Proposes KeyDiff, a method based on key similarity for KV cache eviction, independent of attention scores.

Result: KeyDiff achieves near-optimal performance (0.04% gap) with 23% KV cache reduction on LongBench for Llama models.

Conclusion: KeyDiff is effective for resource-constrained LLM applications, offering theoretical and practical advantages.

Abstract: In this work, we demonstrate that distinctive keys during LLM inference tend
to have high attention scores. We explore this phenomenon and propose KeyDiff,
a training-free KV cache eviction method based on key similarity. This method
facilitates the deployment of LLM-based application requiring long input
prompts in resource-constrained environments with limited memory and compute
budgets. Unlike other KV cache eviction methods, KeyDiff can process
arbitrarily long prompts within strict resource constraints and efficiently
generate responses. We demonstrate that KeyDiff computes the optimal solution
to a KV cache selection problem that maximizes key diversity, providing a
theoretical understanding of KeyDiff. Notably,KeyDiff does not rely on
attention scores, allowing the use of optimized attention mechanisms like
FlashAttention. We demonstrate the effectiveness of KeyDiff across diverse
tasks and models, illustrating a performance gap of less than 0.04\% with 8K
cache budget ($\sim$ 23\% KV cache reduction) from the non-evicting baseline on
the LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.

</details>


### [192] [Impact of Noise on LLM-Models Performance in Abstraction and Reasoning Corpus (ARC) Tasks with Model Temperature Considerations](https://arxiv.org/abs/2504.15903)
*Nikhil Khandalkar, Pavan Yadav, Krishna Shinde, Lokesh B. Ramegowda, Rajarshi Das*

Main category: cs.AI

TL;DR: LLMs like GPT-4o perform well on ARC tasks under ideal conditions, but models like DeepSeek R1 and LLaMA 3.2 struggle, revealing sensitivity to noise and limitations in abstract reasoning.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare the structured reasoning capabilities of LLMs under varying noise levels and temperature settings, identifying vulnerabilities in their generalization.

Method: Systematic evaluation of models (GPT-4o, DeepSeek R1, LLaMA 3.2) across different noise levels and temperature settings using the ARC benchmark.

Result: Noise consistently impairs performance across all models, exposing their fragility and lack of robustness in real-world scenarios.

Conclusion: Current LLMs need improvement in robustness and adaptability to handle real-world ambiguity, guiding future research toward more human-like cognitive flexibility.

Abstract: Recent advancements in Large Language Models (LLMs) have generated growing
interest in their structured reasoning capabilities, particularly in tasks
involving abstraction and pattern recognition. The Abstraction and Reasoning
Corpus (ARC) benchmark plays a crucial role in evaluating these capabilities by
testing how well AI models generalize to novel problems. While GPT-4o
demonstrates strong performance by solving all ARC tasks under zero-noise
conditions, other models like DeepSeek R1 and LLaMA 3.2 fail to solve any,
suggesting limitations in their ability to reason beyond simple pattern
matching. To explore this gap, we systematically evaluate these models across
different noise levels and temperature settings. Our results reveal that the
introduction of noise consistently impairs model performance, regardless of
architecture. This decline highlights a shared vulnerability: current LLMs,
despite showing signs of abstract reasoning, remain highly sensitive to input
perturbations. Such fragility raises concerns about their real-world
applicability, where noise and uncertainty are common. By comparing how
different model architectures respond to these challenges, we offer insights
into the structural weaknesses of modern LLMs in reasoning tasks. This work
underscores the need for developing more robust and adaptable AI systems
capable of handling the ambiguity and variability inherent in real-world
scenarios. Our findings aim to guide future research toward enhancing model
generalization, robustness, and alignment with human-like cognitive
flexibility.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [193] [Waveform-Logmel Audio Neural Networks for Respiratory Sound Classification](https://arxiv.org/abs/2504.17156)
*Jiadong Xie, Yunlian Zhou, Mingsheng Xu*

Main category: cs.SD

TL;DR: A novel neural network architecture (WLANN) using waveform and log-mel spectrogram features with Bi-GRU improves respiratory sound classification, achieving high sensitivity and total score.


<details>
  <summary>Details</summary>
Motivation: Challenges in classifying abnormal respiratory sounds due to scarcity of data motivate the development of a more effective method.

Method: Proposes WLANN, combining waveform and log-mel spectrogram features with Bi-GRU for context modeling.

Result: Achieves 90.3% sensitivity and 93.6% total score on the SPRSound dataset, outperforming previous methods.

Conclusion: WLANN is highly effective for respiratory disease diagnosis, demonstrating superior performance in sound classification.

Abstract: Auscultatory analysis using an electronic stethoscope has attracted
increasing attention in the clinical diagnosis of respiratory diseases.
Recently, neural networks have been applied to assist in respiratory sound
classification with achievements. However, it remains challenging due to the
scarcity of abnormal respiratory sound. In this paper, we propose a novel
architecture, namely Waveform-Logmel audio neural networks (WLANN), which uses
both waveform and log-mel spectrogram as the input features and uses
Bidirectional Gated Recurrent Units (Bi-GRU) to context model the fused
features. Experimental results of our WLANN applied to SPRSound respiratory
dataset show that the proposed framework can effectively distinguish
pathological respiratory sound classes, outperforming the previous studies,
with 90.3% in sensitivity and 93.6% in total score. Our study demonstrates the
high effectiveness of the WLANN in the diagnosis of respiratory diseases.

</details>


### [194] [A Machine Learning Approach for Denoising and Upsampling HRTFs](https://arxiv.org/abs/2504.17586)
*Xuyi Hu, Jian Li, Lorenzo Picinali, Aidan O. T. Hogg*

Main category: cs.SD

TL;DR: A novel method using HRTF Denoisy U-Net and AE-GAN upsamples sparse, noisy HRTF measurements, achieving low LSD error and cosine similarity loss.


<details>
  <summary>Details</summary>
Motivation: Personalized HRTFs improve spatial audio but require time-consuming, noise-free measurements. Machine learning reduces points but still needs controlled environments.

Method: Combines HRTF Denoisy U-Net for denoising and AE-GAN for upsampling from three measurement points.

Result: Achieves LSD error of 5.41 dB and cosine similarity loss of 0.0070.

Conclusion: The method effectively upsamples HRTFs, addressing measurement constraints.

Abstract: The demand for realistic virtual immersive audio continues to grow, with
Head-Related Transfer Functions (HRTFs) playing a key role. HRTFs capture how
sound reaches our ears, reflecting unique anatomical features and enhancing
spatial perception. It has been shown that personalized HRTFs improve
localization accuracy, but their measurement remains time-consuming and
requires a noise-free environment. Although machine learning has been shown to
reduce the required measurement points and, thus, the measurement time, a
controlled environment is still necessary. This paper proposes a method to
address this constraint by presenting a novel technique that can upsample
sparse, noisy HRTF measurements. The proposed approach combines an HRTF Denoisy
U-Net for denoising and an Autoencoding Generative Adversarial Network (AE-GAN)
for upsampling from three measurement points. The proposed method achieves a
log-spectral distortion (LSD) error of 5.41 dB and a cosine similarity loss of
0.0070, demonstrating the method's effectiveness in HRTF upsampling.

</details>


### [195] [Unleashing the Power of Natural Audio Featuring Multiple Sound Sources](https://arxiv.org/abs/2504.17782)
*Xize Cheng, Slytherin Wang, Zehan Wang, Rongjie Huang, Tao Jin, Zhou Zhao*

Main category: cs.SD

TL;DR: ClearSep is a framework for universal sound separation using a data engine to decompose naturally mixed audio, improving generalization to real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Current methods rely on artificially mixed audio, limiting generalization to natural audio. ClearSep addresses this by using naturally mixed data.

Method: ClearSep employs a data engine to decompose complex audio, uses remix-based metrics for evaluation, and iteratively optimizes performance with tailored training strategies.

Result: ClearSep achieves state-of-the-art performance in sound separation tasks, demonstrating effectiveness in natural audio scenarios.

Conclusion: ClearSep advances sound separation by leveraging naturally mixed audio and iterative optimization, showing promise for real-world applications.

Abstract: Universal sound separation aims to extract clean audio tracks corresponding
to distinct events from mixed audio, which is critical for artificial auditory
perception. However, current methods heavily rely on artificially mixed audio
for training, which limits their ability to generalize to naturally mixed audio
collected in real-world environments. To overcome this limitation, we propose
ClearSep, an innovative framework that employs a data engine to decompose
complex naturally mixed audio into multiple independent tracks, thereby
allowing effective sound separation in real-world scenarios. We introduce two
remix-based evaluation metrics to quantitatively assess separation quality and
use these metrics as thresholds to iteratively apply the data engine alongside
model training, progressively optimizing separation performance. In addition,
we propose a series of training strategies tailored to these separated
independent tracks to make the best use of them. Extensive experiments
demonstrate that ClearSep achieves state-of-the-art performance across multiple
sound separation tasks, highlighting its potential for advancing sound
separation in natural audio scenarios. For more examples and detailed results,
please visit our demo page at https://clearsep.github.io.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [196] [A Novel Graph Transformer Framework for Gene Regulatory Network Inference](https://arxiv.org/abs/2504.16961)
*Binon Teji, Swarup Roy*

Main category: cs.LG

TL;DR: The paper introduces GT-GRN, a graph transformer-based model for Gene Regulatory Network (GRN) inference, integrating autoencoder embeddings, prior knowledge, and positional encodings to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Improving GRN inference by addressing noise and misrepresentation in gene coexpression data, leveraging prior knowledge and network structures for more accurate reconstructions.

Method: Uses autoencoder embeddings for gene expression patterns, BERT for encoding prior knowledge from GRN structures, and positional encodings. Integrates these into GT-GRN, a graph transformer model.

Result: GT-GRN significantly outperforms existing GRN inference methods, demonstrating superior accuracy and robustness.

Conclusion: The integration of multiple inferred networks and advanced embeddings enhances GRN inference, with GT-GRN proving highly effective.

Abstract: The inference of gene regulatory networks (GRNs) is a foundational stride
towards deciphering the fundamentals of complex biological systems. Inferring a
possible regulatory link between two genes can be formulated as a link
prediction problem. Inference of GRNs via gene coexpression profiling data may
not always reflect true biological interactions, as its susceptibility to noise
and misrepresenting true biological regulatory relationships. Most GRN
inference methods face several challenges in the network reconstruction phase.
Therefore, it is important to encode gene expression values, leverege the prior
knowledge gained from the available inferred network structures and positional
informations of the input network nodes towards inferring a better and more
confident GRN network reconstruction. In this paper, we explore the integration
of multiple inferred networks to enhance the inference of Gene Regulatory
Networks (GRNs). Primarily, we employ autoencoder embeddings to capture gene
expression patterns directly from raw data, preserving intricate biological
signals. Then, we embed the prior knowledge from GRN structures transforming
them into a text-like representation using random walks, which are then encoded
with a masked language model, BERT, to generate global embeddings for each gene
across all networks. Additionally, we embed the positional encodings of the
input gene networks to better identify the position of each unique gene within
the graph. These embeddings are integrated into graph transformer-based model,
termed GT-GRN, for GRN inference. The GT-GRN model effectively utilizes the
topological structure of the ground truth network while incorporating the
enriched encoded information. Experimental results demonstrate that GT-GRN
significantly outperforms existing GRN inference methods, achieving superior
accuracy and highlighting the robustness of our approach.

</details>


### [197] [Backslash: Rate Constrained Optimized Training of Large Language Models](https://arxiv.org/abs/2504.16968)
*Jun Wu, Jiangtao Wen, Yuxing Han*

Main category: cs.LG

TL;DR: Backslash introduces rate-constrained training for LLMs, reducing memory usage by 60-90% without accuracy loss, outperforming post-training compression.


<details>
  <summary>Details</summary>
Motivation: Address the unexplored area of compression during LLM training, aiming to reduce parameter redundancy while maintaining performance.

Method: Rate-Constrained Training (Backslash) based on rate-distortion optimization (RDO) for flexible accuracy-complexity trade-offs.

Result: Achieves 60-90% memory reduction, enhances generalization, improves robustness to pruning, and simplifies networks for edge devices.

Conclusion: Backslash is a versatile, effective training-time compression method for LLMs, offering significant benefits over post-training approaches.

Abstract: The rapid advancement of large-language models (LLMs) has driven extensive
research into parameter compression after training has been completed, yet
compression during the training phase remains largely unexplored. In this work,
we introduce Rate-Constrained Training (Backslash), a novel training-time
compression approach based on rate-distortion optimization (RDO). Backslash
enables a flexible trade-off between model accuracy and complexity,
significantly reducing parameter redundancy while preserving performance.
Experiments in various architectures and tasks demonstrate that Backslash can
reduce memory usage by 60\% - 90\% without accuracy loss and provides
significant compression gain compared to compression after training. Moreover,
Backslash proves to be highly versatile: it enhances generalization with small
Lagrange multipliers, improves model robustness to pruning (maintaining
accuracy even at 80\% pruning rates), and enables network simplification for
accelerated inference on edge devices.

</details>


### [198] [STFM: A Spatio-Temporal Information Fusion Model Based on Phase Space Reconstruction for Sea Surface Temperature Prediction](https://arxiv.org/abs/2504.16970)
*Yin Wang, Chunlin Gong, Xiang Wu, Hanleran Zhang*

Main category: cs.LG

TL;DR: A data-driven SST prediction framework using phase space reconstruction and STFM for accurate forecasting with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Accurate SST prediction is vital for optimizing production planning, but current methods (physics-based or machine learning) face computational or data challenges.

Method: Phase space reconstruction creates initial-delay attractor pairs with homeomorphism, and STFM uncovers their intrinsic connections for efficient SST dynamics capture.

Result: The method achieves high prediction accuracy with minimal training data compared to conventional models.

Conclusion: The proposed framework offers an efficient, accurate, and data-light solution for SST forecasting.

Abstract: The sea surface temperature (SST), a key environmental parameter, is crucial
to optimizing production planning, making its accurate prediction a vital
research topic. However, the inherent nonlinearity of the marine dynamic system
presents significant challenges. Current forecasting methods mainly include
physics-based numerical simulations and data-driven machine learning
approaches. The former, while describing SST evolution through differential
equations, suffers from high computational complexity and limited
applicability, whereas the latter, despite its computational benefits, requires
large datasets and faces interpretability challenges. This study presents a
prediction framework based solely on data-driven techniques. Using phase space
reconstruction, we construct initial-delay attractor pairs with a mathematical
homeomorphism and design a Spatio-Temporal Fusion Mapping (STFM) to uncover
their intrinsic connections. Unlike conventional models, our method captures
SST dynamics efficiently through phase space reconstruction and achieves high
prediction accuracy with minimal training data in comparative tests

</details>


### [199] [Unsupervised Time-Series Signal Analysis with Autoencoders and Vision Transformers: A Review of Architectures and Applications](https://arxiv.org/abs/2504.16972)
*Hossein Ahmadi, Sajjad Emdadi Mahdimahalleh, Arman Farahat, Banafsheh Saffari*

Main category: cs.LG

TL;DR: A review of autoencoders and vision transformers for unsupervised signal analysis, covering architectures, applications, and trends in domains like IoT and biomedical engineering.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of unlabeled time-series data in fields like IoT and biomedical engineering necessitates advancements in unsupervised learning for signal analysis.

Method: The review synthesizes recent progress in using autoencoders and vision transformers, focusing on their architectures, applications, and hybrid approaches.

Result: Highlights strengths in feature extraction, anomaly detection, and classification, but notes challenges in interpretability, scalability, and domain generalization.

Conclusion: Provides a roadmap for developing robust, adaptive models for signal intelligence by bridging methodological innovations and practical applications.

Abstract: The rapid growth of unlabeled time-series data in domains such as wireless
communications, radar, biomedical engineering, and the Internet of Things (IoT)
has driven advancements in unsupervised learning. This review synthesizes
recent progress in applying autoencoders and vision transformers for
unsupervised signal analysis, focusing on their architectures, applications,
and emerging trends. We explore how these models enable feature extraction,
anomaly detection, and classification across diverse signal types, including
electrocardiograms, radar waveforms, and IoT sensor data. The review highlights
the strengths of hybrid architectures and self-supervised learning, while
identifying challenges in interpretability, scalability, and domain
generalization. By bridging methodological innovations and practical
applications, this work offers a roadmap for developing robust, adaptive models
for signal intelligence.

</details>


### [200] [Communication-Efficient Personalized Distributed Learning with Data and Node Heterogeneity](https://arxiv.org/abs/2504.17520)
*Zhuojun Tian, Zhaoyang Zhang, Yiwei Li, Mehdi Bennis*

Main category: cs.LG

TL;DR: A communication-efficient personalized learning algorithm is proposed to address data and node heterogeneity in decentralized learning, using global parameters and personalized binary masks, with theoretical and empirical validation.


<details>
  <summary>Details</summary>
Motivation: To tackle data and node heterogeneity in decentralized learning while ensuring communication efficiency and personalization.

Method: Each local model combines global real-valued parameters with personalized binary masks, updated via a binary mask aggregation algorithm with group sparse regularization.

Result: The method effectively leverages agent relativity and meets personalized requirements, validated by numerical simulations.

Conclusion: The proposed algorithm and DSLTH foundation are effective for heterogeneous decentralized learning.

Abstract: To jointly tackle the challenges of data and node heterogeneity in
decentralized learning, we propose a distributed strong lottery ticket
hypothesis (DSLTH), based on which a communication-efficient personalized
learning algorithm is developed. In the proposed method, each local model is
represented as the Hadamard product of global real-valued parameters and a
personalized binary mask for pruning. The local model is learned by updating
and fusing the personalized binary masks while the real-valued parameters are
fixed among different agents. To further reduce the complexity of hardware
implementation, we incorporate a group sparse regularization term in the loss
function, enabling the learned local model to achieve structured sparsity.
Then, a binary mask aggregation algorithm is designed by introducing an
intermediate aggregation tensor and adding a personalized fine-tuning step in
each iteration, which constrains model updates towards the local data
distribution. The proposed method effectively leverages the relativity among
agents while meeting personalized requirements in heterogeneous node
conditions. We also provide a theoretical proof for the DSLTH, establishing it
as the foundation of the proposed method. Numerical simulations confirm the
validity of the DSLTH and demonstrate the effectiveness of the proposed
algorithm.

</details>


### [201] [Safety Pretraining: Toward the Next Generation of Safe AI](https://arxiv.org/abs/2504.16980)
*Pratyush Maini, Sachin Goyal, Dylan Sam, Alex Robey, Yash Savani, Yiding Jiang, Andy Zou, Zacharcy C. Lipton, J. Zico Kolter*

Main category: cs.LG

TL;DR: A data-centric pretraining framework for LLMs reduces harmful content generation by integrating safety measures from the start, achieving an 8.4% attack success rate without performance loss.


<details>
  <summary>Details</summary>
Motivation: Addressing the brittleness of post-hoc alignment methods and the persistent risk of harmful content generation in LLMs.

Method: Develops a safety classifier, synthetic safety datasets, refusal dialogues, educational material, and harmfulness-tag annotations during pretraining.

Result: Reduces attack success rates from 38.8% to 8.4% while maintaining performance on safety benchmarks.

Conclusion: Proactive safety integration during pretraining effectively mitigates harmful outputs in LLMs.

Abstract: As large language models (LLMs) are increasingly deployed in high-stakes
settings, the risk of generating harmful or toxic content remains a central
challenge. Post-hoc alignment methods are brittle: once unsafe patterns are
learned during pretraining, they are hard to remove. We present a data-centric
pretraining framework that builds safety into the model from the start. Our
contributions include: (i) a safety classifier trained on 10,000 GPT-4 labeled
examples, used to filter 600B tokens; (ii) the largest synthetic safety dataset
to date (100B tokens) generated via recontextualization of harmful web data;
(iii) RefuseWeb and Moral Education datasets that convert harmful prompts into
refusal dialogues and web-style educational material; (iv) Harmfulness-Tag
annotations injected during pretraining to flag unsafe content and steer away
inference from harmful generations; and (v) safety evaluations measuring base
model behavior before instruction tuning. Our safety-pretrained models reduce
attack success rates from 38.8% to 8.4% with no performance degradation on
standard LLM safety benchmarks.

</details>


### [202] [(Im)possibility of Automated Hallucination Detection in Large Language Models](https://arxiv.org/abs/2504.17004)
*Amin Karbasi, Omar Montasser, John Sous, Grigoris Velegkas*

Main category: cs.LG

TL;DR: The paper explores whether automated hallucination detection in LLMs is possible, linking it to language identification. It shows detection is impossible without expert-labeled feedback but feasible with it.


<details>
  <summary>Details</summary>
Motivation: To analyze the feasibility of automatically detecting hallucinations in LLM outputs, inspired by language identification frameworks.

Method: Theoretical framework linking hallucination detection to language identification, analyzing training regimes with and without expert-labeled feedback.

Result: Hallucination detection is impossible without expert-labeled feedback but becomes feasible with it for all countable language collections.

Conclusion: Expert-labeled feedback is crucial for training hallucination detectors, supporting feedback-based methods like RLHF for reliable LLM deployment.

Abstract: Is automated hallucination detection possible? In this work, we introduce a
theoretical framework to analyze the feasibility of automatically detecting
hallucinations produced by large language models (LLMs). Inspired by the
classical Gold-Angluin framework for language identification and its recent
adaptation to language generation by Kleinberg and Mullainathan, we investigate
whether an algorithm, trained on examples drawn from an unknown target language
$K$ (selected from a countable collection) and given access to an LLM, can
reliably determine whether the LLM's outputs are correct or constitute
hallucinations.
  First, we establish an equivalence between hallucination detection and the
classical task of language identification. We prove that any hallucination
detection method can be converted into a language identification method, and
conversely, algorithms solving language identification can be adapted for
hallucination detection. Given the inherent difficulty of language
identification, this implies that hallucination detection is fundamentally
impossible for most language collections if the detector is trained using only
correct examples from the target language.
  Second, we show that the use of expert-labeled feedback, i.e., training the
detector with both positive examples (correct statements) and negative examples
(explicitly labeled incorrect statements), dramatically changes this
conclusion. Under this enriched training regime, automated hallucination
detection becomes possible for all countable language collections.
  These results highlight the essential role of expert-labeled examples in
training hallucination detectors and provide theoretical support for
feedback-based methods, such as reinforcement learning with human feedback
(RLHF), which have proven critical for reliable LLM deployment.

</details>


### [203] [Democracy of AI Numerical Weather Models: An Example of Global Forecasting with FourCastNetv2 Made by a University Research Lab Using GPU](https://arxiv.org/abs/2504.17028)
*Iman Khadir, Shane Stevenson, Henry Li, Kyle Krick, Abram Burrows, David Hall, Stan Posey, Samuel S. P. Shen*

Main category: cs.LG

TL;DR: The paper explores democratizing AI-driven weather forecasting for university research groups using GPUs and freely available AI models like NVIDIA's FourCastNetv2, addressing challenges and providing practical guidance.


<details>
  <summary>Details</summary>
Motivation: To make AI-driven weather forecasting accessible to resource-constrained university research groups by leveraging existing tools and hardware.

Method: Utilizes NVIDIA's FourCastNetv2 for predictions and trains the original FourCastNet model using NVIDIA A100 GPUs, focusing on data management, training efficiency, and validation.

Result: Demonstrates the feasibility and limitations of using limited GPU resources for AI weather forecasting, providing a guide for research and education.

Conclusion: The paper serves as a foundational resource for democratizing AI weather forecasting in academia, aiding research and education in machine learning, climate science, and data science.

Abstract: This paper demonstrates the feasibility of democratizing AI-driven global
weather forecasting models among university research groups by leveraging
Graphics Processing Units (GPUs) and freely available AI models, such as
NVIDIA's FourCastNetv2. FourCastNetv2 is an NVIDIA's advanced neural network
for weather prediction and is trained on a 73-channel subset of the European
Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset
at single levels and different pressure levels. Although the training
specifications for FourCastNetv2 are not released to the public, the training
documentation of the model's first generation, FourCastNet, is available to all
users. The training had 64 A100 GPUs and took 16 hours to complete. Although
NVIDIA's models offer significant reductions in both time and cost compared to
traditional Numerical Weather Prediction (NWP), reproducing published
forecasting results presents ongoing challenges for resource-constrained
university research groups with limited GPU availability. We demonstrate both
(i) leveraging FourCastNetv2 to create predictions through the designated
application programming interface (API) and (ii) utilizing NVIDIA hardware to
train the original FourCastNet model. Further, this paper demonstrates the
capabilities and limitations of NVIDIA A100's for resource-limited research
groups in universities. We also explore data management, training efficiency,
and model validation, highlighting the advantages and challenges of using
limited high-performance computing resources. Consequently, this paper and its
corresponding GitHub materials may serve as an initial guide for other
university research groups and courses related to machine learning, climate
science, and data science to develop research and education programs on AI
weather forecasting, and hence help democratize the AI NWP in the digital
economy.

</details>


### [204] [Statistical Guarantees in Synthetic Data through Conformal Adversarial Generation](https://arxiv.org/abs/2504.17058)
*Rahul Vishwakarma*

Main category: cs.LG

TL;DR: A novel framework, Conformalized GAN (cGAN), integrates conformal prediction into GANs to provide statistical guarantees for synthetic data.


<details>
  <summary>Details</summary>
Motivation: Existing generative models lack rigorous statistical guarantees, limiting their use in critical domains.

Method: Incorporates conformal prediction methodologies (ICP, Mondrian, Cross-Conformal, Venn-Abers) into GANs.

Result: cGAN offers enhanced calibration and provable statistical guarantees for synthetic data.

Conclusion: cGAN enables reliable synthetic data generation for high-stakes applications like healthcare and finance.

Abstract: The generation of high-quality synthetic data presents significant challenges
in machine learning research, particularly regarding statistical fidelity and
uncertainty quantification. Existing generative models produce compelling
synthetic samples but lack rigorous statistical guarantees about their relation
to the underlying data distribution, limiting their applicability in critical
domains requiring robust error bounds. We address this fundamental limitation
by presenting a novel framework that incorporates conformal prediction
methodologies into Generative Adversarial Networks (GANs). By integrating
multiple conformal prediction paradigms including Inductive Conformal
Prediction (ICP), Mondrian Conformal Prediction, Cross-Conformal Prediction,
and Venn-Abers Predictors, we establish distribution-free uncertainty
quantification in generated samples. This approach, termed Conformalized GAN
(cGAN), demonstrates enhanced calibration properties while maintaining the
generative power of traditional GANs, producing synthetic data with provable
statistical guarantees. We provide rigorous mathematical proofs establishing
finite-sample validity guarantees and asymptotic efficiency properties,
enabling the reliable application of synthetic data in high-stakes domains
including healthcare, finance, and autonomous systems.

</details>


### [205] [Antenna Near-Field Reconstruction from Far-Field Data Using Convolutional Neural Networks](https://arxiv.org/abs/2504.17065)
*Sahar Bagherkhani, Jackson Christopher Earls, Franco De Flaviis, Pierre Baldi*

Main category: cs.LG

TL;DR: A deep learning approach using CNNs for FF-NF transformation achieves accurate near-field reconstruction from far-field data.


<details>
  <summary>Details</summary>
Motivation: Electromagnetic field reconstruction is vital for applications like antenna diagnostics and interference analysis, but traditional methods rely on explicit analytical transformations.

Method: CNNs are trained on paired far-field and near-field data, evaluated using MSE.

Result: Best model shows training error of 0.0199 and test error of 0.3898, with visual confirmation of accurate field behavior capture.

Conclusion: Deep learning, particularly CNNs, is effective for electromagnetic field reconstruction, offering a promising alternative to analytical methods.

Abstract: Electromagnetic field reconstruction is crucial in many applications,
including antenna diagnostics, electromagnetic interference analysis, and
system modeling. This paper presents a deep learning-based approach for
Far-Field to Near-Field (FF-NF) transformation using Convolutional Neural
Networks (CNNs). The goal is to reconstruct near-field distributions from the
far-field data of an antenna without relying on explicit analytical
transformations. The CNNs are trained on paired far-field and near-field data
and evaluated using mean squared error (MSE). The best model achieves a
training error of 0.0199 and a test error of 0.3898. Moreover, visual
comparisons between the predicted and true near-field distributions demonstrate
the model's effectiveness in capturing complex electromagnetic field behavior,
highlighting the potential of deep learning in electromagnetic field
reconstruction.

</details>


### [206] [Whence Is A Model Fair? Fixing Fairness Bugs via Propensity Score Matching](https://arxiv.org/abs/2504.17066)
*Kewen Peng, Yicheng Yang, Hao Zhuo, Tim Menzies*

Main category: cs.LG

TL;DR: FairMatch uses propensity score matching to improve fairness evaluation and mitigation without losing predictive performance.


<details>
  <summary>Details</summary>
Motivation: Existing fairness-aware learning models often remain unfair due to biased training and test data sampling.

Method: Proposes FairMatch, a post-processing method using propensity score matching and probabilistic calibration for unmatched samples.

Result: Identifies unbiased subsets in test data and reduces bias significantly on the rest.

Conclusion: Propensity score matching enhances fairness evaluation and mitigation effectively.

Abstract: Fairness-aware learning aims to mitigate discrimination against specific
protected social groups (e.g., those categorized by gender, ethnicity, age)
while minimizing predictive performance loss. Despite efforts to improve
fairness in machine learning, prior studies have shown that many models remain
unfair when measured against various fairness metrics. In this paper, we
examine whether the way training and testing data are sampled affects the
reliability of reported fairness metrics. Since training and test sets are
often randomly sampled from the same population, bias present in the training
data may still exist in the test data, potentially skewing fairness
assessments. To address this, we propose FairMatch, a post-processing method
that applies propensity score matching to evaluate and mitigate bias. FairMatch
identifies control and treatment pairs with similar propensity scores in the
test set and adjusts decision thresholds for different subgroups accordingly.
For samples that cannot be matched, we perform probabilistic calibration using
fairness-aware loss functions. Experimental results demonstrate that our
approach can (a) precisely locate subsets of the test data where the model is
unbiased, and (b) significantly reduce bias on the remaining data. Overall,
propensity score matching offers a principled way to improve both fairness
evaluation and mitigation, without sacrificing predictive performance.

</details>


### [207] [In-Context Learning can distort the relationship between sequence likelihoods and biological fitness](https://arxiv.org/abs/2504.17068)
*Pranav Kantroo, Günter P. Wagner, Benjamin B. Machta*

Main category: cs.LG

TL;DR: Language models can distort fitness-likelihood relationships due to in-context learning, especially with repeated motifs, affecting transformer-based models.


<details>
  <summary>Details</summary>
Motivation: To investigate how in-context learning in language models distorts the correlation between sequence likelihood scores and experimental fitness, particularly with repeated motifs.

Method: Using protein language models (especially transformer-based) trained on masked language modeling, analyzing sequences with repeated motifs and other biological features.

Result: Transformer models show anomalously high likelihood scores for repeated motifs due to a look-up behavior, overriding learned priors. This effect extends to imperfect repeats and other biological features like RNA hairpins.

Conclusion: In-context learning in language models can disrupt fitness-likelihood relationships, highlighting a vulnerability in transformer architectures for biological sequence analysis.

Abstract: Language models have emerged as powerful predictors of the viability of
biological sequences. During training these models learn the rules of the
grammar obeyed by sequences of amino acids or nucleotides. Once trained, these
models can take a sequence as input and produce a likelihood score as an
output; a higher likelihood implies adherence to the learned grammar and
correlates with experimental fitness measurements. Here we show that in-context
learning can distort the relationship between fitness and likelihood scores of
sequences. This phenomenon most prominently manifests as anomalously high
likelihood scores for sequences that contain repeated motifs. We use protein
language models with different architectures trained on the masked language
modeling objective for our experiments, and find transformer-based models to be
particularly vulnerable to this effect. This behavior is mediated by a look-up
operation where the model seeks the identity of the masked position by using
the other copy of the repeated motif as a reference. This retrieval behavior
can override the model's learned priors. This phenomenon persists for
imperfectly repeated sequences, and extends to other kinds of biologically
relevant features such as reversed complement motifs in RNA sequences that fold
into hairpin structures.

</details>


### [208] [Sparse Phased Array Optimization Using Deep Learning](https://arxiv.org/abs/2504.17073)
*David Lu, Lior Maman, Jackson Earls, Amir Boag, Pierre Baldi*

Main category: cs.LG

TL;DR: A deep learning-based method optimizes sparse phased array designs to reduce grating lobes, improving signal strength and interference suppression.


<details>
  <summary>Details</summary>
Motivation: Enhancing antenna array performance by addressing non-convex design challenges and reducing grating lobes for better beamforming and interference mitigation.

Method: Uses neural networks to approximate a non-convex cost function, enabling gradient descent optimization of antenna element coordinates. Incorporates physical constraints via a tailored penalty mechanism.

Result: Achieved cost reductions of 411% to 643% (average 552%) in ten array configurations, significantly lowering side lobe levels.

Conclusion: The method enables ultra-precise beamforming and next-generation wireless/radar systems with improved efficiency and clarity.

Abstract: Antenna arrays are widely used in wireless communication, radar systems,
radio astronomy, and military defense to enhance signal strength, directivity,
and interference suppression. We introduce a deep learning-based optimization
approach that enhances the design of sparse phased arrays by reducing grating
lobes. This approach begins by generating sparse array configurations to
address the non-convex challenges and extensive degrees of freedom inherent in
array design. We use neural networks to approximate the non-convex cost
function that estimates the energy ratio between the main and side lobes. This
differentiable approximation facilitates cost function minimization through
gradient descent, optimizing the antenna elements' coordinates and leading to
an improved layout. Additionally, we incorporate a tailored penalty mechanism
that includes various physical and design constraints into the optimization
process, enhancing its robustness and practical applicability. We demonstrate
the effectiveness of our method by applying it to the ten array configurations
with the lowest initial costs, achieving further cost reductions ranging from
411% to 643%, with an impressive average improvement of 552%. By significantly
reducing side lobe levels in antenna arrays, this breakthrough paves the way
for ultra-precise beamforming, enhanced interference mitigation, and
next-generation wireless and radar systems with unprecedented efficiency and
clarity.

</details>


### [209] [Conditional Diffusion-Based Retrieval of Atmospheric CO2 from Earth Observing Spectroscopy](https://arxiv.org/abs/2504.17074)
*William R. Keely, Otto Lamminpää, Steffen Mauceri, Sean M. R. Crowell, Christopher W. O'Dell, Gregory R. McGarragh*

Main category: cs.LG

TL;DR: The paper proposes a diffusion-based method for faster and more accurate greenhouse gas (GHG) retrieval from satellite data, addressing limitations of current Optimal Estimation (OE) methods.


<details>
  <summary>Details</summary>
Motivation: Current OE methods for GHG retrieval are computationally expensive, converge poorly, and provide unrealistic uncertainty estimates. Upcoming missions will generate more data, necessitating faster, robust algorithms for real-time global monitoring.

Method: A diffusion-based approach is introduced to retrieve Gaussian or non-Gaussian posteriors for GHG concentrations, tested on NASA's Orbiting Carbon Observatory-2 spectrometer.

Result: The proposed method offers a computational speed-up over OE while maintaining accuracy and improving uncertainty quantification.

Conclusion: The diffusion-based approach advances GHG retrieval, supporting real-time global carbon monitoring for climate policy.

Abstract: Satellite-based estimates of greenhouse gas (GHG) properties from
observations of reflected solar spectra are integral for understanding and
monitoring complex terrestrial systems and their impact on the carbon cycle due
to their near global coverage. Known as retrieval, making GHG concentration
estimations from these observations is a non-linear Bayesian inverse problem,
which is operationally solved using a computationally expensive algorithm
called Optimal Estimation (OE), providing a Gaussian approximation to a
non-Gaussian posterior. This leads to issues in solver algorithm convergence,
and to unrealistically confident uncertainty estimates for the retrieved
quantities. Upcoming satellite missions will provide orders of magnitude more
data than the current constellation of GHG observers. Development of fast and
accurate retrieval algorithms with robust uncertainty quantification is
critical. Doing so stands to provide substantial climate impact of moving
towards the goal of near continuous real-time global monitoring of carbon
sources and sinks which is essential for policy making. To achieve this goal,
we propose a diffusion-based approach to flexibly retrieve a Gaussian or
non-Gaussian posterior, for NASA's Orbiting Carbon Observatory-2 spectrometer,
while providing a substantial computational speed-up over the current
operational state-of-the-art.

</details>


### [210] [A Novel Hybrid Approach Using an Attention-Based Transformer + GRU Model for Predicting Cryptocurrency Prices](https://arxiv.org/abs/2504.17079)
*Esam Mahdi, C. Martin-Barreiro, X. Cabezas*

Main category: cs.LG

TL;DR: A hybrid deep learning model combining Transformer and GRU architectures outperforms other models in predicting cryptocurrency prices.


<details>
  <summary>Details</summary>
Motivation: Improve cryptocurrency price prediction accuracy by leveraging the strengths of Transformer (long-range patterns) and GRU (short-term trends).

Method: Developed a hybrid Transformer-GRU model, tested on Bitcoin and Ethereum data, and compared with RBFN, GRNN, BiLSTM, and BiGRU using MSE, RMSE, MAE, MAPE, and statistical tests.

Result: The hybrid model consistently outperformed other models in accuracy.

Conclusion: The hybrid model is effective for financial predictions, supporting its use in cryptocurrency markets and financial analytics.

Abstract: In this article, we introduce a novel deep learning hybrid model that
integrates attention Transformer and Gated Recurrent Unit (GRU) architectures
to improve the accuracy of cryptocurrency price predictions. By combining the
Transformer's strength in capturing long-range patterns with the GRU's ability
to model short-term and sequential trends, the hybrid model provides a
well-rounded approach to time series forecasting. We apply the model to predict
the daily closing prices of Bitcoin and Ethereum based on historical data that
include past prices, trading volumes, and the Fear and Greed index. We evaluate
the performance of our proposed model by comparing it with four other machine
learning models: two are non-sequential feedforward models: Radial Basis
Function Network (RBFN) and General Regression Neural Network (GRNN), and two
are bidirectional sequential memory-based models: Bidirectional Long-Short-Term
Memory (BiLSTM) and Bidirectional Gated Recurrent Unit (BiGRU). The performance
of the model is assessed using several metrics, including Mean Squared Error
(MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Mean
Absolute Percentage Error (MAPE), along with statistical validation through the
nonparametric Friedman test followed by a post hoc Wilcoxon signed rank test.
The results demonstrate that our hybrid model consistently achieves superior
accuracy, highlighting its effectiveness for financial prediction tasks. These
findings provide valuable insights for improving real-time decision making in
cryptocurrency markets and support the growing use of hybrid deep learning
models in financial analytics.

</details>


### [211] [GeoRDF2Vec Learning Location-Aware Entity Representations in Knowledge Graphs](https://arxiv.org/abs/2504.17099)
*Martin Boeckling, Heiko Paulheim, Sarah Detzler*

Main category: cs.LG

TL;DR: A method to enhance RDF2Vec by incorporating geometric data for location-aware entity embeddings, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing entity representation methods ignore geometric data in knowledge graphs, limiting their accuracy for spatial entities.

Method: Extends RDF2Vec by flooding the graph from geographic nodes and biasing walks with spatial weights.

Result: Outperforms non-location-aware RDF2Vec and GeoTransE in evaluations.

Conclusion: Incorporating geometric data improves entity embeddings, especially for spatial entities.

Abstract: Many knowledge graphs contain a substantial number of spatial entities, such
as cities, buildings, and natural landmarks. For many of these entities, exact
geometries are stored within the knowledge graphs. However, most existing
approaches for learning entity representations do not take these geometries
into account. In this paper, we introduce a variant of RDF2Vec that
incorporates geometric information to learn location-aware embeddings of
entities. Our approach expands different nodes by flooding the graph from
geographic nodes, ensuring that each reachable node is considered. Based on the
resulting flooded graph, we apply a modified version of RDF2Vec that biases
graph walks using spatial weights. Through evaluations on multiple benchmark
datasets, we demonstrate that our approach outperforms both non-location-aware
RDF2Vec and GeoTransE.

</details>


### [212] [Discovering the Precursors of Traffic Breakdowns Using Spatiotemporal Graph Attribution Networks](https://arxiv.org/abs/2504.17109)
*Zhaobin Mo, Xiangyi Liao, Dominik A. Karbowski, Yanbing Wang*

Main category: cs.LG

TL;DR: A novel method combining ST-GNNs and Shapley values to interpret traffic breakdown precursors, identifying road topology and abrupt braking as key factors.


<details>
  <summary>Details</summary>
Motivation: Improving road safety and traffic flow management by understanding and predicting traffic breakdown precursors.

Method: Combines spatiotemporal graph neural networks (ST-GNNs) with Shapley values for interpretable predictions.

Result: Identified road topology and abrupt braking as major precursors of traffic breakdowns on Interstate-24 data.

Conclusion: The method bridges the gap between black-box predictions and interpretable causes, enhancing traffic management.

Abstract: Understanding and predicting the precursors of traffic breakdowns is critical
for improving road safety and traffic flow management. This paper presents a
novel approach combining spatiotemporal graph neural networks (ST-GNNs) with
Shapley values to identify and interpret traffic breakdown precursors. By
extending Shapley explanation methods to a spatiotemporal setting, our proposed
method bridges the gap between black-box neural network predictions and
interpretable causes. We demonstrate the method on the Interstate-24 data, and
identify that road topology and abrupt braking are major factors that lead to
traffic breakdowns.

</details>


### [213] [Scalable Permutation-Aware Modeling for Temporal Set Prediction](https://arxiv.org/abs/2504.17140)
*Ashish Ranjan, Ayush Agarwal, Shalin Barot, Sushant Kumar*

Main category: cs.LG

TL;DR: A scalable framework for temporal set prediction using permutation-equivariant and invariant transformations, reducing computational overhead while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for temporal set prediction are computationally heavy and lack scalability.

Method: Leverages permutation-equivariant and permutation-invariant transformations to model set dynamics efficiently.

Result: Achieves competitive or superior performance to state-of-the-art models with reduced training and inference time.

Conclusion: The proposed method is effective for efficient and scalable temporal set prediction.

Abstract: Temporal set prediction involves forecasting the elements that will appear in
the next set, given a sequence of prior sets, each containing a variable number
of elements. Existing methods often rely on intricate architectures with
substantial computational overhead, which hampers their scalability. In this
work, we introduce a novel and scalable framework that leverages
permutation-equivariant and permutation-invariant transformations to
efficiently model set dynamics. Our approach significantly reduces both
training and inference time while maintaining competitive performance.
Extensive experiments on multiple public benchmarks show that our method
achieves results on par with or superior to state-of-the-art models across
several evaluation metrics. These results underscore the effectiveness of our
model in enabling efficient and scalable temporal set prediction.

</details>


### [214] [OUI Need to Talk About Weight Decay: A New Perspective on Overfitting Detection](https://arxiv.org/abs/2504.17160)
*Alberto Fernández-Hernández, Jose I. Mestre, Manuel F. Dolz, Jose Duato, Enrique S. Quintana-Ortí*

Main category: cs.LG

TL;DR: The paper introduces the Overfitting-Underfitting Indicator (OUI), a tool for monitoring DNN training dynamics and selecting optimal Weight Decay hyperparameters without validation data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying optimal regularization hyperparameters (like Weight Decay) during DNN training without relying on validation data.

Method: Proposes OUI, which monitors training dynamics to indicate overfitting or underfitting. Validated on DenseNet-BC-100, EfficientNet-B0, and ResNet-34 across datasets like CIFAR-100 and ImageNet-1K.

Result: OUI correlates with improved generalization, converges faster than traditional metrics, and enables early identification of optimal Weight Decay values.

Conclusion: OUI is a reliable tool for precise hyperparameter tuning, improving DNN performance by balancing overfitting and underfitting early in training.

Abstract: We introduce the Overfitting-Underfitting Indicator (OUI), a novel tool for
monitoring the training dynamics of Deep Neural Networks (DNNs) and identifying
optimal regularization hyperparameters. Specifically, we validate that OUI can
effectively guide the selection of the Weight Decay (WD) hyperparameter by
indicating whether a model is overfitting or underfitting during training
without requiring validation data. Through experiments on DenseNet-BC-100 with
CIFAR- 100, EfficientNet-B0 with TinyImageNet and ResNet-34 with ImageNet-1K,
we show that maintaining OUI within a prescribed interval correlates strongly
with improved generalization and validation scores. Notably, OUI converges
significantly faster than traditional metrics such as loss or accuracy,
enabling practitioners to identify optimal WD (hyperparameter) values within
the early stages of training. By leveraging OUI as a reliable indicator, we can
determine early in training whether the chosen WD value leads the model to
underfit the training data, overfit, or strike a well-balanced trade-off that
maximizes validation scores. This enables more precise WD tuning for optimal
performance on the tested datasets and DNNs. All code for reproducing these
experiments is available at https://github.com/AlbertoFdezHdez/OUI.

</details>


### [215] [A Double-Norm Aggregated Tensor Latent Factorization Model for Temporal-Aware Traffic Speed Imputation](https://arxiv.org/abs/2504.17196)
*Jiawen Hou, Hao Wu*

Main category: cs.LG

TL;DR: TATSI combines $L_2$-norm and smooth $L_1$-norm for robust and accurate traffic speed data imputation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Traffic speed data often has missing values due to sensor issues, and current tensor decomposition methods lack robustness.

Method: TATSI uses a hybrid loss function ($L_2$ + ${SL}_1$-norm) and SLF-NMU for nonnegative latent factor analysis.

Result: TATSI outperforms state-of-the-art methods in capturing temporal patterns and imputing missing data.

Conclusion: TATSI is an effective solution for robust and accurate traffic speed data imputation in ITS.

Abstract: In intelligent transportation systems (ITS), traffic management departments
rely on sensors, cameras, and GPS devices to collect real-time traffic data.
Traffic speed data is often incomplete due to sensor failures, data
transmission delays, or occlusions, resulting in missing speed data in certain
road segments. Currently, tensor decomposition based methods are extensively
utilized, they mostly rely on the $L_2$-norm to construct their learning
objectives, which leads to reduced robustness in the algorithms. To address
this, we propose Temporal-Aware Traffic Speed Imputation (TATSI), which
combines the $L_2$-norm and smooth $L_1$ (${SL}_1$)-norm in its loss function,
thereby achieving both high accuracy and robust performance in imputing missing
time-varying traffic speed data. TATSI adopts a single latent factor-dependent,
nonnegative, and multiplicative update (SLF-NMU) approach, which serves as an
efficient solver for performing nonnegative latent factor analysis (LFA) on a
tensor. Empirical studies on three real-world time-varying traffic speed
datasets demonstrate that, compared with state-of-the-art traffic speed
predictors, TATSI more precisely captures temporal patterns, thereby yielding
the most accurate imputations for missing traffic speed data.

</details>


### [216] [Synthetic Power Flow Data Generation Using Physics-Informed Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2504.17210)
*Junfei Wang, Darshana Upadhyay, Marzia Zaman, Pirathayini Srikantha*

Main category: cs.LG

TL;DR: A physics-informed generative framework using DDPMs synthesizes feasible power flow data, outperforming baselines in feasibility, diversity, and accuracy.


<details>
  <summary>Details</summary>
Motivation: Real-world power flow data is limited due to privacy and operational constraints, necessitating synthetic data generation.

Method: Uses Denoising Diffusion Probabilistic Models (DDPMs) with auxiliary training and physics-informed loss functions.

Result: Outperforms three baseline models in feasibility, diversity, and accuracy on IEEE 14-bus and 30-bus systems.

Conclusion: Demonstrates the potential of generative modeling for data-driven power system applications.

Abstract: Many data-driven modules in smart grid rely on access to high-quality power
flow data; however, real-world data are often limited due to privacy and
operational constraints. This paper presents a physics-informed generative
framework based on Denoising Diffusion Probabilistic Models (DDPMs) for
synthesizing feasible power flow data. By incorporating auxiliary training and
physics-informed loss functions, the proposed method ensures that the generated
data exhibit both statistical fidelity and adherence to power system
feasibility. We evaluate the approach on the IEEE 14-bus and 30-bus benchmark
systems, demonstrating its ability to capture key distributional properties and
generalize to out-of-distribution scenarios. Comparative results show that the
proposed model outperforms three baseline models in terms of feasibility,
diversity, and accuracy of statistical features. This work highlights the
potential of integrating generative modelling into data-driven power system
applications.

</details>


### [217] [Enhancing Variational Autoencoders with Smooth Robust Latent Encoding](https://arxiv.org/abs/2504.17219)
*Hyomin Lee, Minseon Kim, Sangwon Jang, Jongheon Jeong, Sung Ju Hwang*

Main category: cs.LG

TL;DR: SRL-VAE, a novel adversarial training framework, enhances both generation quality and robustness in VAEs by smoothing latent space and preserving fidelity.


<details>
  <summary>Details</summary>
Motivation: Addressing the underexplored robustness of VAEs in diffusion-based models and challenging the assumption that adversarial training degrades generative model fidelity.

Method: Introduces SRL-VAE, which smooths latent space via adversarial perturbations while regularizing to maintain original fidelity, applied as a post-training step.

Result: Improves image robustness and fidelity, enhancing generation quality in reconstruction and text-guided editing, and robustness against attacks.

Conclusion: Adversarial training can enhance both fidelity and robustness in generative models, establishing a new paradigm.

Abstract: Variational Autoencoders (VAEs) have played a key role in scaling up
diffusion-based generative models, as in Stable Diffusion, yet questions
regarding their robustness remain largely underexplored. Although adversarial
training has been an established technique for enhancing robustness in
predictive models, it has been overlooked for generative models due to concerns
about potential fidelity degradation by the nature of trade-offs between
performance and robustness. In this work, we challenge this presumption,
introducing Smooth Robust Latent VAE (SRL-VAE), a novel adversarial training
framework that boosts both generation quality and robustness. In contrast to
conventional adversarial training, which focuses on robustness only, our
approach smooths the latent space via adversarial perturbations, promoting more
generalizable representations while regularizing with originality
representation to sustain original fidelity. Applied as a post-training step on
pre-trained VAEs, SRL-VAE improves image robustness and fidelity with minimal
computational overhead. Experiments show that SRL-VAE improves both generation
quality, in image reconstruction and text-guided image editing, and robustness,
against Nightshade attacks and image editing attacks. These results establish a
new paradigm, showing that adversarial training, once thought to be detrimental
to generative models, can instead enhance both fidelity and robustness.

</details>


### [218] [Multi-Modal Traffic Analysis: Integrating Time-Series Forecasting, Accident Prediction, and Image Classification](https://arxiv.org/abs/2504.17232)
*Nivedita M, Yasmeen Shajitha S*

Main category: cs.LG

TL;DR: An integrated ML framework combining ARIMA, XGBoost, and CNN for traffic analysis, achieving high accuracy in prediction, classification, and image analysis, with applications in smart cities.


<details>
  <summary>Details</summary>
Motivation: To enhance traffic analysis and accident prevention by integrating multiple ML techniques for real-time monitoring and resource optimization in smart city systems.

Method: Combines ARIMA(2,0,1) for traffic prediction, XGBoost for accident severity classification, and CNN for traffic image classification, tested on diverse datasets.

Result: Outperforms baselines with MAE 2.1 for ARIMA, 100% accuracy for XGBoost, and 92% accuracy for CNN. Identifies key factors like weather and road infrastructure.

Conclusion: The modular framework is effective for smart city deployment, advancing intelligent transportation systems.

Abstract: This study proposes an integrated machine learning framework for advanced
traffic analysis, combining time-series forecasting, classification, and
computer vision techniques. The system utilizes an ARIMA(2,0,1) model for
traffic prediction (MAE: 2.1), an XGBoost classifier for accident severity
classification (100% accuracy on balanced data), and a Convolutional Neural
Network (CNN) for traffic image classification (92% accuracy). Tested on
diverse datasets, the framework outperforms baseline models and identifies key
factors influencing accident severity, including weather and road
infrastructure. Its modular design supports deployment in smart city systems
for real-time monitoring, accident prevention, and resource optimization,
contributing to the evolution of intelligent transportation systems.

</details>


### [219] [NeuralGrok: Accelerate Grokking by Neural Gradient Transformation](https://arxiv.org/abs/2504.17243)
*Xinyu Zhou, Simin Fan, Martin Jaggi, Jie Fu*

Main category: cs.LG

TL;DR: NeuralGrok, a gradient-based method, accelerates generalization in transformers for arithmetic tasks by dynamically modulating gradients via an auxiliary MLP block and bilevel optimization.


<details>
  <summary>Details</summary>
Motivation: To address the slow generalization (grokking) phenomenon in transformers by improving gradient dynamics and reducing model complexity.

Method: Trains an auxiliary MLP to modulate gradients based on their generalization contribution, using bilevel optimization. Introduces AGE metric to analyze model complexity.

Result: NeuralGrok speeds up generalization, stabilizes training, and reduces complexity, outperforming traditional methods like weight decay.

Conclusion: NeuralGrok provides insights into grokking and offers a practical approach to enhance generalization in transformers.

Abstract: Grokking is proposed and widely studied as an intricate phenomenon in which
generalization is achieved after a long-lasting period of overfitting. In this
work, we propose NeuralGrok, a novel gradient-based approach that learns an
optimal gradient transformation to accelerate the generalization of
transformers in arithmetic tasks. Specifically, NeuralGrok trains an auxiliary
module (e.g., an MLP block) in conjunction with the base model. This module
dynamically modulates the influence of individual gradient components based on
their contribution to generalization, guided by a bilevel optimization
algorithm. Our extensive experiments demonstrate that NeuralGrok significantly
accelerates generalization, particularly in challenging arithmetic tasks. We
also show that NeuralGrok promotes a more stable training paradigm, constantly
reducing the model's complexity, while traditional regularization methods, such
as weight decay, can introduce substantial instability and impede
generalization. We further investigate the intrinsic model complexity
leveraging a novel Absolute Gradient Entropy (AGE) metric, which explains that
NeuralGrok effectively facilitates generalization by reducing the model
complexity. We offer valuable insights on the grokking phenomenon of
Transformer models, which encourages a deeper understanding of the fundamental
principles governing generalization ability.

</details>


### [220] [Targeted AMP generation through controlled diffusion with efficient embeddings](https://arxiv.org/abs/2504.17247)
*Diogo Soares, Leon Hetzel, Paulina Szymczak, Fabian Theis, Stephan Günnemann, Ewa Szczurek*

Main category: cs.LG

TL;DR: OmegAMP is a diffusion-based generative model for targeted AMP discovery, offering precise controllability, reduced false positives, and high diversity.


<details>
  <summary>Details</summary>
Motivation: Addressing low hit rates and the need for nuanced controllability in deep learning-based AMP discovery.

Method: Leverages diffusion-based generative models with low-dimensional embeddings, controllability mechanisms, and novel classifiers.

Result: Achieves state-of-the-art performance in AMP discovery, enabling targeted generation with specific properties.

Conclusion: OmegAMP significantly advances computational frameworks for combating antimicrobial resistance.

Abstract: Deep learning-based antimicrobial peptide (AMP) discovery faces critical
challenges such as low experimental hit rates as well as the need for nuanced
controllability and efficient modeling of peptide properties. To address these
challenges, we introduce OmegAMP, a framework that leverages a diffusion-based
generative model with efficient low-dimensional embeddings, precise
controllability mechanisms, and novel classifiers with drastically reduced
false positive rates for candidate filtering. OmegAMP enables the targeted
generation of AMPs with specific physicochemical properties, activity profiles,
and species-specific effectiveness. Moreover, it maximizes sample diversity
while ensuring faithfulness to the underlying data distribution during
generation. We demonstrate that OmegAMP achieves state-of-the-art performance
across all stages of the AMP discovery pipeline, significantly advancing the
potential of computational frameworks in combating antimicrobial resistance.

</details>


### [221] [Group Downsampling with Equivariant Anti-aliasing](https://arxiv.org/abs/2504.17258)
*Md Ashiqur Rahman, Raymond A. Yeh*

Main category: cs.LG

TL;DR: The paper generalizes uniform downsampling for group-equivariant CNNs, proposing an algorithm for subgroup selection and anti-aliasing, improving accuracy and equivariance.


<details>
  <summary>Details</summary>
Motivation: To extend downsampling layers to group-equivariant architectures (e.g., G-CNNs) while preserving equivariance and reducing aliasing.

Method: Presents an algorithm for subgroup selection and introduces anti-aliasing for signals on finite groups, generalizing classical sampling theory.

Result: Experiments show improved accuracy, better equivariance preservation, and reduced model size in G-equivariant networks.

Conclusion: The proposed downsampling method effectively generalizes classical techniques and enhances performance in group-equivariant architectures.

Abstract: Downsampling layers are crucial building blocks in CNN architectures, which
help to increase the receptive field for learning high-level features and
reduce the amount of memory/computation in the model. In this work, we study
the generalization of the uniform downsampling layer for group equivariant
architectures, e.g., G-CNNs. That is, we aim to downsample signals (feature
maps) on general finite groups with anti-aliasing. This involves the following:
(a) Given a finite group and a downsampling rate, we present an algorithm to
form a suitable choice of subgroup. (b) Given a group and a subgroup, we study
the notion of bandlimited-ness and propose how to perform anti-aliasing.
Notably, our method generalizes the notion of downsampling based on classical
sampling theory. When the signal is on a cyclic group, i.e., periodic, our
method recovers the standard downsampling of an ideal low-pass filter followed
by a subsampling operation. Finally, we conducted experiments on image
classification tasks demonstrating that the proposed downsampling operation
improves accuracy, better preserves equivariance, and reduces model size when
incorporated into G-equivariant networks

</details>


### [222] [Symbolic Representation for Any-to-Any Generative Tasks](https://arxiv.org/abs/2504.17261)
*Jiaqi Chen, Xiaoye Zhu, Yue Wang, Tianyang Liu, Xinhui Chen, Ying Chen, Chak Tou Leong, Yifei Ke, Joseph Liu, Yiwen Yuan, Julian McAuley, Li-jia Li*

Main category: cs.LG

TL;DR: A symbolic generative task description language and inference engine for multimodal tasks, outperforming conventional models in flexibility and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of conventional generative models (high computational cost, limited flexibility) by introducing explicit symbolic representations.

Method: Uses symbolic primitives (functions, parameters, topological logic) and a pre-trained language model to map natural language to symbolic workflows without training.

Result: Successfully performs 12 diverse multimodal tasks, matching or outperforming state-of-the-art models in quality, efficiency, and editability.

Conclusion: Symbolic task representations offer a cost-effective, extensible foundation for advancing generative AI capabilities.

Abstract: We propose a symbolic generative task description language and a
corresponding inference engine capable of representing arbitrary multimodal
tasks as structured symbolic flows. Unlike conventional generative models that
rely on large-scale training and implicit neural representations to learn
cross-modal mappings, often at high computational cost and with limited
flexibility, our framework introduces an explicit symbolic representation
comprising three core primitives: functions, parameters, and topological logic.
Leveraging a pre-trained language model, our inference engine maps natural
language instructions directly to symbolic workflows in a training-free manner.
Our framework successfully performs over 12 diverse multimodal generative
tasks, demonstrating strong performance and flexibility without the need for
task-specific tuning. Experiments show that our method not only matches or
outperforms existing state-of-the-art unified models in content quality, but
also offers greater efficiency, editability, and interruptibility. We believe
that symbolic task representations provide a cost-effective and extensible
foundation for advancing the capabilities of generative AI.

</details>


### [223] [Signal Recovery from Random Dot-Product Graphs Under Local Differential Privacy](https://arxiv.org/abs/2504.17274)
*Siddharth Vishwanath, Jonathan Hehir*

Main category: cs.LG

TL;DR: The paper addresses recovering latent information from graphs under ε-edge local differential privacy, ensuring edge confidentiality. It shows consistent recovery of latent positions in generalized random dot-product graphs and proves near-optimality under privacy constraints.


<details>
  <summary>Details</summary>
Motivation: To protect edge confidentiality in graphs while enabling recovery of latent information, extending privacy-preserving techniques beyond community detection.

Method: Uses a standard local differential privacy mechanism, analyzes induced geometric distortion, and adjusts statistical inference for privatized graphs.

Result: Achieves consistent recovery of latent positions and proves near minimax-optimality under privacy constraints. Also recovers geometric/topological information via persistence diagrams.

Conclusion: The framework extends privacy-preserving inference to richer graph models and tasks, demonstrating practical utility.

Abstract: We consider the problem of recovering latent information from graphs under
$\varepsilon$-edge local differential privacy where the presence of
relationships/edges between two users/vertices remains confidential, even from
the data curator. For the class of generalized random dot-product graphs, we
show that a standard local differential privacy mechanism induces a specific
geometric distortion in the latent positions. Leveraging this insight, we show
that consistent recovery of the latent positions is achievable by appropriately
adjusting the statistical inference procedure for the privatized graph.
Furthermore, we prove that our procedure is nearly minimax-optimal under local
edge differential privacy constraints. Lastly, we show that this framework
allows for consistent recovery of geometric and topological information
underlying the latent positions, as encoded in their persistence diagrams. Our
results extend previous work from the private community detection literature to
a substantially richer class of models and inferential tasks.

</details>


### [224] [HeRB: Heterophily-Resolved Structure Balancer for Graph Neural Networks](https://arxiv.org/abs/2504.17276)
*Ke-Jia Chen, Wenhui Mu, Zheng Liu*

Main category: cs.LG

TL;DR: HeRB addresses structural imbalance in GNNs by resolving heterophily and transferring homophilic knowledge, outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: GNNs struggle with structural imbalance, especially due to heterophily (connected nodes with distinct labels/features), which prior solutions ignored.

Method: HeRB includes a heterophily-lessening augmentation module and a homophilic knowledge transfer mechanism.

Result: HeRB excels on homophilic and heterophilic datasets, with ablation studies confirming its components' effectiveness.

Conclusion: HeRB successfully tackles structural imbalance by addressing heterophily and leveraging homophilic knowledge, proving its superiority.

Abstract: Recent research has witnessed the remarkable progress of Graph Neural
Networks (GNNs) in the realm of graph data representation. However, GNNs still
encounter the challenge of structural imbalance. Prior solutions to this
problem did not take graph heterophily into account, namely that connected
nodes process distinct labels or features, thus resulting in a deficiency in
effectiveness. Upon verifying the impact of heterophily on solving the
structural imbalance problem, we propose to rectify the heterophily first and
then transfer homophilic knowledge. To the end, we devise a method named HeRB
(Heterophily-Resolved Structure Balancer) for GNNs. HeRB consists of two
innovative components: 1) A heterophily-lessening augmentation module which
serves to reduce inter-class edges and increase intra-class edges; 2) A
homophilic knowledge transfer mechanism to convey homophilic information from
head nodes to tail nodes. Experimental results demonstrate that HeRB achieves
superior performance on two homophilic and six heterophilic benchmark datasets,
and the ablation studies further validate the efficacy of two proposed
components.

</details>


### [225] [ExOSITO: Explainable Off-Policy Learning with Side Information for Intensive Care Unit Blood Test Orders](https://arxiv.org/abs/2504.17277)
*Zongliang Ji, Andre Carlos Kajdacsy-Balla Amaral, Anna Goldenberg, Rahul G. Krishnan*

Main category: cs.LG

TL;DR: A novel method, ExOSITO, combines off-policy learning with privileged information to optimize ICU lab test orders, reducing costs while ensuring vital tests are not missed.


<details>
  <summary>Details</summary>
Motivation: Balancing the need for critical lab test information with reducing clinical burden and costs in ICU settings.

Method: Uses off-policy learning and causal bandit trained with offline data and clinically-approved reward functions, integrating clinical knowledge.

Result: Outperforms physician policies and prior approaches, providing interpretable clinical insights and cost reduction.

Conclusion: ExOSITO offers an effective, interpretable tool for clinicians to optimize lab test orders in ICUs.

Abstract: Ordering a minimal subset of lab tests for patients in the intensive care
unit (ICU) can be challenging. Care teams must balance between ensuring the
availability of the right information and reducing the clinical burden and
costs associated with each lab test order. Most in-patient settings experience
frequent over-ordering of lab tests, but are now aiming to reduce this burden
on both hospital resources and the environment. This paper develops a novel
method that combines off-policy learning with privileged information to
identify the optimal set of ICU lab tests to order. Our approach, EXplainable
Off-policy learning with Side Information for ICU blood Test Orders (ExOSITO)
creates an interpretable assistive tool for clinicians to order lab tests by
considering both the observed and predicted future status of each patient. We
pose this problem as a causal bandit trained using offline data and a reward
function derived from clinically-approved rules; we introduce a novel learning
framework that integrates clinical knowledge with observational data to bridge
the gap between the optimal and logging policies. The learned policy function
provides interpretable clinical information and reduces costs without omitting
any vital lab orders, outperforming both a physician's policy and prior
approaches to this practical problem.

</details>


### [226] [The Ultimate Cookbook for Invisible Poison: Crafting Subtle Clean-Label Text Backdoors with Style Attributes](https://arxiv.org/abs/2504.17300)
*Wencong You, Daniel Lowd*

Main category: cs.LG

TL;DR: The paper introduces AttrBkd, a method for creating subtle backdoor attacks in text classifiers by using natural triggers, making them harder for humans to detect while maintaining effectiveness.


<details>
  <summary>Details</summary>
Motivation: Prior backdoor attacks used unnatural triggers easily detected by humans, limiting their practicality. The paper aims to create attacks that are both effective and indistinguishable from normal text.

Method: Proposes AttrBkd, which crafts subtle triggers by extracting fine-grained attributes from existing backdoor attacks. Human evaluations assess subtlety and effectiveness.

Result: AttrBkd outperforms baseline attacks in both attack success rate and subtlety, showing triggers can evade human detection while remaining effective.

Conclusion: The study highlights the importance of human evaluation in assessing backdoor attacks, revealing misalignment between automated metrics and human judgment.

Abstract: Backdoor attacks on text classifiers can cause them to predict a predefined
label when a particular "trigger" is present. Prior attacks often rely on
triggers that are ungrammatical or otherwise unusual, leading to conspicuous
attacks. As a result, human annotators, who play a critical role in curating
training data in practice, can easily detect and filter out these unnatural
texts during manual inspection, reducing the risk of such attacks. We argue
that a key criterion for a successful attack is for text with and without
triggers to be indistinguishable to humans. However, prior work neither
directly nor comprehensively evaluated attack subtlety and invisibility with
human involvement. We bridge the gap by conducting thorough human evaluations
to assess attack subtlety. We also propose \emph{AttrBkd}, consisting of three
recipes for crafting subtle yet effective trigger attributes, such as
extracting fine-grained attributes from existing baseline backdoor attacks. Our
human evaluations find that AttrBkd with these baseline-derived attributes is
often more effective (higher attack success rate) and more subtle (fewer
instances detected by humans) than the original baseline backdoor attacks,
demonstrating that backdoor attacks can bypass detection by being inconspicuous
and appearing natural even upon close inspection, while still remaining
effective. Our human annotation also provides information not captured by
automated metrics used in prior work, and demonstrates the misalignment of
these metrics with human judgment.

</details>


### [227] [Machine learning-based condition monitoring of powertrains in modern electric drives](https://arxiv.org/abs/2504.17305)
*Dinan Li, Panagiotis Kakosimos, Luca Peretti*

Main category: cs.LG

TL;DR: The paper explores using machine learning to create a thermal model for power modules in industrial drives, comparing traditional and deep learning methods for temperature estimation.


<details>
  <summary>Details</summary>
Motivation: To enhance industrial system smartness by leveraging existing drive data for predictive thermal modeling.

Method: Developed a data-driven thermal model using various ML approaches (linear models to deep neural networks) and validated it with static/dynamic operating profiles.

Result: Evaluated performance of different ML models for estimating power module case temperature, assessing suitability for industrial embedded systems.

Conclusion: Demonstrated the feasibility of using ML for accurate thermal modeling in industrial applications, with potential for system optimization.

Abstract: The recent technological advances in digitalization have revolutionized the
industrial sector. Leveraging data analytics has now enabled the collection of
deep insights into the performance and, as a result, the optimization of
assets. Industrial drives, for example, already accumulate all the necessary
information to control electric machines. These signals include but are not
limited to currents, frequency, and temperature. Integrating machine learning
(ML) models responsible for predicting the evolution of those directly
collected or implicitly derived parameters enhances the smartness of industrial
systems even further. In this article, data already residing in most modern
electric drives has been used to develop a data-driven thermal model of a power
module. A test bench has been designed and used specifically for training and
validating the thermal digital twin undergoing various static and dynamic
operating profiles. Different approaches, from traditional linear models to
deep neural networks, have been implemented to emanate the best ML model for
estimating the case temperature of a power module. Several evaluation metrics
were then used to assess the investigated methods' performance and
implementation in industrial embedded systems.

</details>


### [228] [Class-Conditional Distribution Balancing for Group Robust Classification](https://arxiv.org/abs/2504.17314)
*Miaoyun Zhao, Qiang Zhang, Chenrong Li*

Main category: cs.LG

TL;DR: A novel method addresses spurious correlations in models by balancing class-conditional distributions without needing bias annotations or predictions, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Spurious correlations hinder robust generalization, and existing solutions rely on expensive bias annotations or impractical pretrained models.

Method: Proposes a sample reweighting strategy to balance class-conditional distributions, reducing mutual information between spurious factors and labels.

Result: The method outperforms or matches supervised approaches, effectively dismantling spurious correlations.

Conclusion: The approach offers a practical, annotation-free solution for robust learning, especially in resource-limited domains.

Abstract: Spurious correlations that lead models to correct predictions for the wrong
reasons pose a critical challenge for robust real-world generalization.
Existing research attributes this issue to group imbalance and addresses it by
maximizing group-balanced or worst-group accuracy, which heavily relies on
expensive bias annotations. A compromise approach involves predicting bias
information using extensively pretrained foundation models, which requires
large-scale data and becomes impractical for resource-limited rare domains. To
address these challenges, we offer a novel perspective by reframing the
spurious correlations as imbalances or mismatches in class-conditional
distributions, and propose a simple yet effective robust learning method that
eliminates the need for both bias annotations and predictions. With the goal of
reducing the mutual information between spurious factors and label information,
our method leverages a sample reweighting strategy to achieve class-conditional
distribution balancing, which automatically highlights minority groups and
classes, effectively dismantling spurious correlations and producing a debiased
data distribution for classification. Extensive experiments and analysis
demonstrate that our approach consistently delivers state-of-the-art
performance, rivaling methods that rely on bias supervision.

</details>


### [229] [Collaborative Multi-Agent Reinforcement Learning for Automated Feature Transformation with Graph-Driven Path Optimization](https://arxiv.org/abs/2504.17355)
*Xiaohan Huang, Dongjie Wang, Zhiyuan Ning, Ziyue Qiao, Qingqing Long, Haowei Zhu, Yi Du, Min Wu, Yuanchun Zhou, Meng Xiao*

Main category: cs.LG

TL;DR: TCTO is a multi-agent reinforcement learning framework for automated feature engineering, using a dynamic interaction graph to optimize transformations and improve downstream ML performance.


<details>
  <summary>Details</summary>
Motivation: Existing feature transformation methods ignore dynamic dependencies between steps, leading to inefficiencies and redundancy.

Method: TCTO employs a graph-driven approach with pruning and backtracking to optimize feature transformations, reusing high-utility subgraphs from history.

Result: Experiments show TCTO outperforms existing methods across diverse datasets.

Conclusion: TCTO effectively automates feature engineering by leveraging dynamic graph optimization and historical reuse.

Abstract: Feature transformation methods aim to find an optimal mathematical
feature-feature crossing process that generates high-value features and
improves the performance of downstream machine learning tasks. Existing
frameworks, though designed to mitigate manual costs, often treat feature
transformations as isolated operations, ignoring dynamic dependencies between
transformation steps. To address the limitations, we propose TCTO, a
collaborative multi-agent reinforcement learning framework that automates
feature engineering through graph-driven path optimization. The framework's
core innovation lies in an evolving interaction graph that models features as
nodes and transformations as edges. Through graph pruning and backtracking, it
dynamically eliminates low-impact edges, reduces redundant operations, and
enhances exploration stability. This graph also provides full traceability to
empower TCTO to reuse high-utility subgraphs from historical transformations.
To demonstrate the efficacy and adaptability of our approach, we conduct
comprehensive experiments and case studies, which show superior performance
across a range of datasets.

</details>


### [230] [Doubly Adaptive Social Learning](https://arxiv.org/abs/2504.17370)
*Marco Carpentiero, Virginia Bordignon, Vincenzo Matta, Ali H. Sayed*

Main category: cs.LG

TL;DR: The paper introduces a doubly adaptive social learning (A²SL) strategy to handle dynamic drifts in online social learning, ensuring agents consistently place belief on the true hypothesis.


<details>
  <summary>Details</summary>
Motivation: Traditional social learning fails in dynamic environments where hypotheses and likelihood models change over time.

Method: A²SL uses stochastic gradient descent and adaptive belief updates, controlled by two parameters, to track drifts and true hypotheses.

Result: Agents learn consistently, with error probabilities converging to values proportional to the adaptation parameters.

Conclusion: A²SL effectively addresses dynamic drifts in online social learning, validated by synthetic and real data.

Abstract: In social learning, a network of agents assigns probability scores (beliefs)
to some hypotheses of interest, which rule the generation of local streaming
data observed by each agent. Belief formation takes place by means of an
iterative two-step procedure where: i) the agents update locally their beliefs
by using some likelihood model; and ii) the updated beliefs are combined with
the beliefs of the neighboring agents, using a pooling rule. This procedure can
fail to perform well in the presence of dynamic drifts, leading the agents to
incorrect decision making. Here, we focus on the fully online setting where
both the true hypothesis and the likelihood models can change over time. We
propose the doubly adaptive social learning ($\text{A}^2\text{SL}$) strategy,
which infuses social learning with the necessary adaptation capabilities. This
goal is achieved by exploiting two adaptation stages: i) a stochastic gradient
descent update to learn and track the drifts in the decision model; ii) and an
adaptive belief update to track the true hypothesis changing over time. These
stages are controlled by two adaptation parameters that govern the evolution of
the error probability for each agent. We show that all agents learn
consistently for sufficiently small adaptation parameters, in the sense that
they ultimately place all their belief mass on the true hypothesis. In
particular, the probability of choosing the wrong hypothesis converges to
values on the order of the adaptation parameters. The theoretical analysis is
illustrated both on synthetic data and by applying the $\text{A}^2\text{SL}$
strategy to a social learning problem in the online setting using real data.

</details>


### [231] [Coding for Computation: Efficient Compression of Neural Networks for Reconfigurable Hardware](https://arxiv.org/abs/2504.17403)
*Hans Rosenberger, Rodrigo Fischer, Johanna S. Fröhlich, Ali Bereyhi, Ralf R. Müller*

Main category: cs.LG

TL;DR: A compression scheme for neural networks reduces computations for FPGA inference by combining pruning, weight sharing, and linear computation coding, focusing on hardware efficiency.


<details>
  <summary>Details</summary>
Motivation: As neural networks grow in size, efficient implementation is crucial. Current compression techniques focus on memory reduction, but this work targets computation reduction for hardware-friendly inference.

Method: Combines pruning via regularized training, weight sharing, and linear computation coding (LCC) to minimize additions required for inference.

Result: Achieves competitive performance for both simple multilayer perceptrons and large-scale deep networks like ResNet-34.

Conclusion: The proposed scheme effectively reduces computations for NN inference on reconfigurable hardware while maintaining performance.

Abstract: As state of the art neural networks (NNs) continue to grow in size, their
resource-efficient implementation becomes ever more important. In this paper,
we introduce a compression scheme that reduces the number of computations
required for NN inference on reconfigurable hardware such as FPGAs. This is
achieved by combining pruning via regularized training, weight sharing and
linear computation coding (LCC). Contrary to common NN compression techniques,
where the objective is to reduce the memory used for storing the weights of the
NNs, our approach is optimized to reduce the number of additions required for
inference in a hardware-friendly manner. The proposed scheme achieves
competitive performance for simple multilayer perceptrons, as well as for large
scale deep NNs such as ResNet-34.

</details>


### [232] [Towards Harnessing the Collaborative Power of Large and Small Models for Domain Tasks](https://arxiv.org/abs/2504.17421)
*Yang Liu, Bingjie Yan, Tianyuan Zou, Jianqing Zhang, Zixuan Gu, Jianbing Ding, Xidong Wang, Jingyi Li, Xiaozhou Ye, Ye Ouyang, Qiang Yang, Ya-Qin Zhang*

Main category: cs.LG

TL;DR: Proposes collaboration between large and small language models to enhance efficiency and domain-specific adaptation, advocating for industry-driven research.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency and resource demands of large language models (LLMs) by leveraging smaller models (SMs) for domain-specific tasks.

Method: Explores strategies for synergistic collaboration between LLMs and SMs, identifying challenges and opportunities.

Result: Highlights the potential of collaborative approaches to unlock new AI capabilities in private domains.

Conclusion: Advocates for industry-driven research with multi-objective benchmarks on real-world private datasets.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities, but
they require vast amounts of data and computational resources. In contrast,
smaller models (SMs), while less powerful, can be more efficient and tailored
to specific domains. In this position paper, we argue that taking a
collaborative approach, where large and small models work synergistically, can
accelerate the adaptation of LLMs to private domains and unlock new potential
in AI. We explore various strategies for model collaboration and identify
potential challenges and opportunities. Building upon this, we advocate for
industry-driven research that prioritizes multi-objective benchmarks on
real-world private datasets and applications.

</details>


### [233] [HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models](https://arxiv.org/abs/2504.17449)
*Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, Qin Xie, Guiming Xie, Xuejian Gong*

Main category: cs.LG

TL;DR: HMI is a system for efficient multi-tenant inference of hierarchical PLMs, reducing GPU memory usage and maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the high computational demands of PLMs in multi-tenant environments by optimizing resource usage.

Method: Categorizes PLM knowledge hierarchically, manages domain/task-specific knowledge, and optimizes system performance with prefetching and parallel implementations.

Result: HMI efficiently serves up to 10,000 hPLMs on a single GPU with minimal accuracy loss.

Conclusion: HMI provides a scalable and resource-efficient solution for multi-tenant PLM inference.

Abstract: The significant computational demands of pretrained language models (PLMs),
which often require dedicated hardware, present a substantial challenge in
serving them efficiently, especially in multi-tenant environments. To address
this, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant
Inference system, designed to manage tenants with distinct PLMs
resource-efficiently. Our approach is three-fold: Firstly, we categorize PLM
knowledge into general, domain-specific, and task-specific. Leveraging insights
on knowledge acquisition across different model layers, we construct
hierarchical PLMs (hPLMs) by extracting and storing knowledge at different
levels, significantly reducing GPU memory usage per tenant. Secondly, we
establish hierarchical knowledge management for hPLMs generated by various
tenants in HMI. We manage domain-specific knowledge with acceptable storage
increases by constructing and updating domain-specific knowledge trees based on
frequency. We manage task-specific knowledge within limited GPU memory through
parameter swapping. Finally, we propose system optimizations to enhance
resource utilization and inference throughput. These include fine-grained
pipelining via hierarchical knowledge prefetching to overlap CPU and I/O
operations with GPU computations, and optimizing parallel implementations with
batched matrix multiplications. Our experimental results demonstrate that the
proposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a
single GPU, with only a negligible compromise in accuracy.

</details>


### [234] [CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated Active Learning](https://arxiv.org/abs/2504.17448)
*Jun Zhang, Jue Wang, Huan Li, Zhongle Xie, Ke Chen, Lidan Shou*

Main category: cs.LG

TL;DR: CHASe (Client Heterogeneity-Aware Data Selection) is proposed for Federated Active Learning (FAL) to address data heterogeneity and model parameter fluctuations, improving accuracy by selecting high epistemic variation samples.


<details>
  <summary>Details</summary>
Motivation: Existing FAL methods struggle with data distribution heterogeneity and model parameter fluctuations, leading to reduced accuracy.

Method: CHASe tracks epistemic variations (EVs) via inference inconsistencies, calibrates decision boundaries with alignment loss, and uses a data freeze-awaken mechanism for efficient selection.

Result: CHASe outperforms baselines in effectiveness and efficiency across diverse datasets, models, and federation settings.

Conclusion: CHASe effectively addresses FAL challenges, enhancing model accuracy and efficiency in heterogeneous environments.

Abstract: Active learning (AL) reduces human annotation costs for machine learning
systems by strategically selecting the most informative unlabeled data for
annotation, but performing it individually may still be insufficient due to
restricted data diversity and annotation budget. Federated Active Learning
(FAL) addresses this by facilitating collaborative data selection and model
training, while preserving the confidentiality of raw data samples. Yet,
existing FAL methods fail to account for the heterogeneity of data distribution
across clients and the associated fluctuations in global and local model
parameters, adversely affecting model accuracy. To overcome these challenges,
we propose CHASe (Client Heterogeneity-Aware Data Selection), specifically
designed for FAL. CHASe focuses on identifying those unlabeled samples with
high epistemic variations (EVs), which notably oscillate around the decision
boundaries during training. To achieve both effectiveness and efficiency,
\model{} encompasses techniques for 1) tracking EVs by analyzing inference
inconsistencies across training epochs, 2) calibrating decision boundaries of
inaccurate models with a new alignment loss, and 3) enhancing data selection
efficiency via a data freeze and awaken mechanism with subset sampling.
Experiments show that CHASe surpasses various established baselines in terms of
effectiveness and efficiency, validated across diverse datasets, model
complexities, and heterogeneous federation settings.

</details>


### [235] [Evaluating Time Series Models for Urban Wastewater Management: Predictive Performance, Model Complexity and Resilience](https://arxiv.org/abs/2504.17461)
*Vipin Singh, Tianheng Ling, Teodor Chiaburu, Felix Biessmann*

Main category: cs.LG

TL;DR: The paper evaluates ML models for urban wastewater management, comparing global and local models for resilience and performance in extreme rainfall scenarios.


<details>
  <summary>Details</summary>
Motivation: Climate change strains urban sewer systems, requiring cost-efficient, adaptable ML solutions to replace traditional models.

Method: Proposes a protocol to assess Neural Network architectures for CSS forecasting, focusing on performance, complexity, and resilience to perturbations.

Result: Global models perform better, but local models are resilient in decentralized scenarios. Longer forecast horizons improve robustness.

Conclusion: ML offers reliable solutions for sustainable wastewater management, with local models being viable for IoT deployment.

Abstract: Climate change increases the frequency of extreme rainfall, placing a
significant strain on urban infrastructures, especially Combined Sewer Systems
(CSS). Overflows from overburdened CSS release untreated wastewater into
surface waters, posing environmental and public health risks. Although
traditional physics-based models are effective, they are costly to maintain and
difficult to adapt to evolving system dynamics. Machine Learning (ML)
approaches offer cost-efficient alternatives with greater adaptability. To
systematically assess the potential of ML for modeling urban infrastructure
systems, we propose a protocol for evaluating Neural Network architectures for
CSS time series forecasting with respect to predictive performance, model
complexity, and robustness to perturbations. In addition, we assess model
performance on peak events and critical fluctuations, as these are the key
regimes for urban wastewater management. To investigate the feasibility of
lightweight models suitable for IoT deployment, we compare global models, which
have access to all information, with local models, which rely solely on nearby
sensor readings. Additionally, to explore the security risks posed by network
outages or adversarial attacks on urban infrastructure, we introduce error
models that assess the resilience of models. Our results demonstrate that while
global models achieve higher predictive performance, local models provide
sufficient resilience in decentralized scenarios, ensuring robust modeling of
urban infrastructure. Furthermore, models with longer native forecast horizons
exhibit greater robustness to data perturbations. These findings contribute to
the development of interpretable and reliable ML solutions for sustainable
urban wastewater management. The implementation is available in our GitHub
repository.

</details>


### [236] [GRANITE : a Byzantine-Resilient Dynamic Gossip Learning Framework](https://arxiv.org/abs/2504.17471)
*Yacine Belal, Mohamed Maouche, Sonia Ben Mokhtar, Anthony Simonet-Boulogne*

Main category: cs.LG

TL;DR: GRANITE is a framework for robust decentralized learning (Gossip Learning) over sparse, dynamic graphs, resilient to Byzantine attacks via a history-aware peer sampling protocol and adaptive probabilistic thresholds.


<details>
  <summary>Details</summary>
Motivation: The robustness of Gossip Learning (GL) against Byzantine attacks, especially when attackers target the Random Peer Sampling (RPS) protocol, remains unaddressed.

Method: GRANITE uses a History-aware Byzantine-resilient Peer Sampling (HaPS) protocol and Adaptive Probabilistic Threshold (APT) to filter poisoned models and set aggregation thresholds.

Result: GRANITE maintains convergence with up to 30% Byzantine nodes, improves learning speed, and works in sparser graphs than current theory allows.

Conclusion: GRANITE effectively addresses Byzantine resilience in GL, enabling robust learning in sparse, dynamic graphs.

Abstract: Gossip Learning (GL) is a decentralized learning paradigm where users
iteratively exchange and aggregate models with a small set of neighboring
peers. Recent GL approaches rely on dynamic communication graphs built and
maintained using Random Peer Sampling (RPS) protocols. Thanks to graph
dynamics, GL can achieve fast convergence even over extremely sparse
topologies. However, the robustness of GL over dy- namic graphs to Byzantine
(model poisoning) attacks remains unaddressed especially when Byzantine nodes
attack the RPS protocol to scale up model poisoning. We address this issue by
introducing GRANITE, a framework for robust learning over sparse, dynamic
graphs in the presence of a fraction of Byzantine nodes. GRANITE relies on two
key components (i) a History-aware Byzantine-resilient Peer Sampling protocol
(HaPS), which tracks previously encountered identifiers to reduce adversarial
influence over time, and (ii) an Adaptive Probabilistic Threshold (APT), which
leverages an estimate of Byzantine presence to set aggregation thresholds with
formal guarantees. Empirical results confirm that GRANITE maintains convergence
with up to 30% Byzantine nodes, improves learning speed via adaptive filtering
of poisoned models and obtains these results in up to 9 times sparser graphs
than dictated by current theory.

</details>


### [237] [Plasticine: Accelerating Research in Plasticity-Motivated Deep Reinforcement Learning](https://arxiv.org/abs/2504.17490)
*Mingqi Yuan, Qi Wang, Guozheng Ma, Bo Li, Xin Jin, Yunbo Wang, Xiaokang Yang, Wenjun Zeng, Dacheng Tao*

Main category: cs.LG

TL;DR: Plasticine is an open-source framework for benchmarking plasticity optimization in deep RL, offering tools to quantify and mitigate plasticity loss.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of unified benchmarks and evaluation protocols for plasticity loss in deep RL, which hinders progress in lifelong learning agents.

Method: Introduces Plasticine, a framework with single-file implementations of 13 mitigation methods, 10 metrics, and learning scenarios of varying non-stationarity.

Result: Provides a systematic way to quantify plasticity loss, evaluate mitigation strategies, and analyze plasticity dynamics.

Conclusion: Plasticine enables researchers to advance the field of lifelong learning in RL by offering a standardized toolset for plasticity optimization.

Abstract: Developing lifelong learning agents is crucial for artificial general
intelligence. However, deep reinforcement learning (RL) systems often suffer
from plasticity loss, where neural networks gradually lose their ability to
adapt during training. Despite its significance, this field lacks unified
benchmarks and evaluation protocols. We introduce Plasticine, the first
open-source framework for benchmarking plasticity optimization in deep RL.
Plasticine provides single-file implementations of over 13 mitigation methods,
10 evaluation metrics, and learning scenarios with increasing non-stationarity
levels from standard to open-ended environments. This framework enables
researchers to systematically quantify plasticity loss, evaluate mitigation
strategies, and analyze plasticity dynamics across different contexts. Our
documentation, examples, and source code are available at
https://github.com/RLE-Foundation/Plasticine.

</details>


### [238] [Prototype-enhanced prediction in graph neural networks for climate applications](https://arxiv.org/abs/2504.17492)
*Nawid Keshtmand, Elena Fillola, Jeffrey Nicholas Clark, Raul Santos-Rodriguez, Matthew Rigby*

Main category: cs.LG

TL;DR: Using prototypes as inputs improves high-dimensional emulator performance, even with random prototypes, but data-driven selection (k-means) boosts performance by nearly 10%.


<details>
  <summary>Details</summary>
Motivation: To enhance the quality of data-driven emulators for physics-based simulations, reducing computational costs and improving accuracy.

Method: Introduces prototypes—approximations of emulator outputs—as additional inputs to the model. Tests the approach on atmospheric dispersion emulation, comparing baseline models to prototype-enhanced ones.

Result: Prototype models outperform baselines, even with random prototypes. Data-driven prototype selection (k-means) further improves performance by up to 10%.

Conclusion: Prototypes significantly enhance emulator performance, with data-driven selection methods offering the highest gains.

Abstract: Data-driven emulators are increasingly being used to learn and emulate
physics-based simulations, reducing computational expense and run time. Here,
we present a structured way to improve the quality of these high-dimensional
emulated outputs, through the use of prototypes: an approximation of the
emulator's output passed as an input, which informs the model and leads to
better predictions. We demonstrate our approach to emulate atmospheric
dispersion, key for greenhouse gas emissions monitoring, by comparing a
baseline model to models trained using prototypes as an additional input. The
prototype models achieve better performance, even with few prototypes and even
if they are chosen at random, but we show that choosing the prototypes through
data-driven methods (k-means) can lead to almost 10\% increased performance in
some metrics.

</details>


### [239] [Goal-Oriented Time-Series Forecasting: Foundation Framework Design](https://arxiv.org/abs/2504.17493)
*Luca-Andrei Fechete, Mohamed Sana, Fadhel Ayed, Nicola Piovesan, Wenjie Li, Antonio De Domenico, Tareq Si Salem*

Main category: cs.LG

TL;DR: A new training methodology for time-series forecasting dynamically adjusts focus based on application-specific forecast ranges, improving accuracy and end-application performance.


<details>
  <summary>Details</summary>
Motivation: Traditional forecasting methods ignore real-world application needs by focusing solely on minimizing prediction errors.

Method: Breaks predictions into smaller segments, dynamically weights them, and combines for accurate forecasts.

Result: Tested on standard and new datasets, the method improves prediction accuracy and end-application performance.

Conclusion: This approach bridges prediction and decision-making, enhancing practical forecasting systems.

Abstract: Traditional time-series forecasting often focuses only on minimizing
prediction errors, ignoring the specific requirements of real-world
applications that employ them. This paper presents a new training methodology,
which allows a forecasting model to dynamically adjust its focus based on the
importance of forecast ranges specified by the end application. Unlike previous
methods that fix these ranges beforehand, our training approach breaks down
predictions over the entire signal range into smaller segments, which are then
dynamically weighted and combined to produce accurate forecasts. We tested our
method on standard datasets, including a new dataset from wireless
communication, and found that not only it improves prediction accuracy but also
improves the performance of end application employing the forecasting model.
This research provides a basis for creating forecasting systems that better
connect prediction and decision-making in various practical applications.

</details>


### [240] [Combining GCN Structural Learning with LLM Chemical Knowledge for or Enhanced Virtual Screening](https://arxiv.org/abs/2504.17497)
*Radia Berreziga, Mohammed Brahimi, Khairedine Kraim, Hamid Azzoune*

Main category: cs.LG

TL;DR: A hybrid architecture combining GCNs and LLM embeddings improves virtual screening performance, outperforming traditional methods like SVM and XGBoost.


<details>
  <summary>Details</summary>
Motivation: Traditional machine learning methods in drug discovery rely on predefined molecular representations, risking information loss and bias. Deep learning and LLMs offer more expressive alternatives.

Method: Proposes a hybrid model integrating GCNs with precomputed LLM embeddings, concatenating them at each GCN layer for deeper global context integration.

Result: Achieves an F1-score of 88.8%, surpassing standalone GCN (87.9%), XGBoost (85.5%), and SVM (85.4%).

Conclusion: The hybrid approach effectively combines localized structural learning with global chemical knowledge, enhancing virtual screening performance.

Abstract: Virtual screening plays a critical role in modern drug discovery by enabling
the identification of promising candidate molecules for experimental
validation. Traditional machine learning methods such as support vector
machines (SVM) and XGBoost rely on predefined molecular representations, often
leading to information loss and potential bias. In contrast, deep learning
approaches-particularly Graph Convolutional Networks (GCNs)-offer a more
expressive and unbiased alternative by operating directly on molecular graphs.
Meanwhile, Large Language Models (LLMs) have recently demonstrated
state-of-the-art performance in drug design, thanks to their capacity to
capture complex chemical patterns from large-scale data via attention
mechanisms.
  In this paper, we propose a hybrid architecture that integrates GCNs with
LLM-derived embeddings to combine localized structural learning with global
chemical knowledge. The LLM embeddings can be precomputed and stored in a
molecular feature library, removing the need to rerun the LLM during training
or inference and thus maintaining computational efficiency. We found that
concatenating the LLM embeddings after each GCN layer-rather than only at the
final layer-significantly improves performance, enabling deeper integration of
global context throughout the network. The resulting model achieves superior
results, with an F1-score of (88.8%), outperforming standalone GCN (87.9%),
XGBoost (85.5%), and SVM (85.4%) baselines.

</details>


### [241] [TACO: Tackling Over-correction in Federated Learning with Tailored Adaptive Correction](https://arxiv.org/abs/2504.17528)
*Weijie Liu, Ziwei Zhan, Carlee Joe-Wong, Edith Ngai, Jingpu Duan, Deke Guo, Xu Chen, Xiaoxi Zhang*

Main category: cs.LG

TL;DR: TACO addresses over-correction in FL by fine-grained gradient correction and lightweight aggregation, improving model accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Non-IID data in FL causes statistical heterogeneity; existing methods over-correct, degrading performance.

Method: TACO uses client-specific gradient correction and tailored aggregation, minimizing computation overhead.

Result: TACO improves model accuracy and convergence, validated by experiments and convergence analysis.

Conclusion: TACO effectively mitigates over-correction and enhances FL performance in non-IID settings.

Abstract: Non-independent and identically distributed (Non-IID) data across edge
clients have long posed significant challenges to federated learning (FL)
training in edge computing environments. Prior works have proposed various
methods to mitigate this statistical heterogeneity. While these works can
achieve good theoretical performance, in this work we provide the first
investigation into a hidden over-correction phenomenon brought by the uniform
model correction coefficients across clients adopted by existing methods. Such
over-correction could degrade model performance and even cause failures in
model convergence. To address this, we propose TACO, a novel algorithm that
addresses the non-IID nature of clients' data by implementing fine-grained,
client-specific gradient correction and model aggregation, steering local
models towards a more accurate global optimum. Moreover, we verify that leading
FL algorithms generally have better model accuracy in terms of communication
rounds rather than wall-clock time, resulting from their extra computation
overhead imposed on clients. To enhance the training efficiency, TACO deploys a
lightweight model correction and tailored aggregation approach that requires
minimum computation overhead and no extra information beyond the synchronized
model parameters. To validate TACO's effectiveness, we present the first FL
convergence analysis that reveals the root cause of over-correction. Extensive
experiments across various datasets confirm TACO's superior and stable
performance in practice.

</details>


### [242] [Tailored minimal reservoir computing: on the bidirectional connection between nonlinearities in the reservoir and in data](https://arxiv.org/abs/2504.17503)
*Davide Prosperino, Haochun Ma, Christoph Räth*

Main category: cs.LG

TL;DR: Optimal reservoir computer design depends on matching the nonlinearity of input data, with performance peaking when reservoir nonlinearity aligns with data. A method for estimating minimal nonlinearity in unknown time series is proposed and validated.


<details>
  <summary>Details</summary>
Motivation: To understand how the degree of nonlinearity in input data affects the optimal design of reservoir computers and to develop a method for estimating nonlinearity in unknown datasets.

Method: Reduce minimal RCs to a single tunable nonlinearity parameter, generalize the fractional Halvorsen system for controlled testing, and propose a method for estimating nonlinearity by sweeping reservoir exponents.

Result: Prediction performance is maximized when reservoir nonlinearity matches data nonlinearity. The method successfully estimates minimal nonlinearity in synthetic and real-world datasets.

Conclusion: Tailoring reservoir computers to data nonlinearity improves performance, especially in resource-constrained scenarios, offering a principled approach to RC design.

Abstract: We study how the degree of nonlinearity in the input data affects the optimal
design of reservoir computers, focusing on how closely the model's nonlinearity
should align with that of the data. By reducing minimal RCs to a single tunable
nonlinearity parameter, we explore how the predictive performance varies with
the degree of nonlinearity in the reservoir. To provide controlled testbeds, we
generalize to the fractional Halvorsen system, a novel chaotic system with
fractional exponents. Our experiments reveal that the prediction performance is
maximized when the reservoir's nonlinearity matches the nonlinearity present in
the data. In cases where multiple nonlinearities are present in the data, we
find that the correlation dimension of the predicted signal is reconstructed
correctly when the smallest nonlinearity is matched. We use this observation to
propose a method for estimating the minimal nonlinearity in unknown time series
by sweeping the reservoir exponent and identifying the transition to a
successful reconstruction. Applying this method to both synthetic and
real-world datasets, including financial time series, we demonstrate its
practical viability. Finally, we transfer these insights to classical RC by
augmenting traditional architectures with fractional, generalized reservoir
states. This yields performance gains, particularly in resource-constrained
scenarios such as physical reservoirs, where increasing reservoir size is
impractical or economically unviable. Our work provides a principled route
toward tailoring RCs to the intrinsic complexity of the systems they aim to
model.

</details>


### [243] [Learning Isometric Embeddings of Road Networks using Multidimensional Scaling](https://arxiv.org/abs/2504.17534)
*Juan Carlos Climent Pardo*

Main category: cs.LG

TL;DR: The paper addresses generalization in autonomous driving by using graph representations and MDS techniques to create feature spaces for neural network-based motion planners.


<details>
  <summary>Details</summary>
Motivation: Current autonomous driving systems lack generalization across diverse road scenarios, necessitating a method to capture varied road structures, topologies, and dynamic changes.

Method: The paper leverages graph representations of road networks and applies multidimensional scaling (MDS) techniques to design feature spaces. It also explores embedding graph nodes for easier learning and dimensionality reduction.

Result: The approach demonstrates how graph representations and MDS can improve generalization in motion planning for autonomous driving.

Conclusion: Graph-based representations and MDS techniques offer a promising solution to enhance generalization in learning-based autonomous driving applications.

Abstract: The lack of generalization in learning-based autonomous driving applications
is shown by the narrow range of road scenarios that vehicles can currently
cover. A generalizable approach should capture many distinct road structures
and topologies, as well as consider traffic participants, and dynamic changes
in the environment, so that vehicles can navigate and perform motion planning
tasks even in the most difficult situations. Designing suitable feature spaces
for neural network-based motion planers that encapsulate all kinds of road
scenarios is still an open research challenge. This paper tackles this
learning-based generalization challenge and shows how graph representations of
road networks can be leveraged by using multidimensional scaling (MDS)
techniques in order to obtain such feature spaces. State-of-the-art graph
representations and MDS approaches are analyzed for the autonomous driving use
case. Finally, the option of embedding graph nodes is discussed in order to
perform easier learning procedures and obtain dimensionality reduction.

</details>


### [244] [Cooperative Task Offloading through Asynchronous Deep Reinforcement Learning in Mobile Edge Computing for Future Networks](https://arxiv.org/abs/2504.17526)
*Yuelin Liu, Haiyuan Li, Xenofon Vasilakos, Rasheed Hussain, Dimitra Simeonidou*

Main category: cs.LG

TL;DR: Proposes CTO-TP, a cooperative task offloading framework using transformer-driven prediction and asynchronous multi-agent deep reinforcement learning to reduce latency and energy consumption in MEC.


<details>
  <summary>Details</summary>
Motivation: Addresses uneven resource utilization and suboptimal performance in single MEC server offloading, and the limitations of centralized task offloading strategies.

Method: Leverages asynchronous multi-agent deep reinforcement learning for edge-edge cooperation, optimizing task offloading and resource allocation.

Result: Reduces overall system latency by up to 80% and energy consumption by 87% compared to baseline schemes.

Conclusion: CTO-TP effectively enhances MEC performance by improving latency and energy efficiency through cooperative and asynchronous approaches.

Abstract: Future networks (including 6G) are poised to accelerate the realisation of
Internet of Everything. However, it will result in a high demand for computing
resources to support new services. Mobile Edge Computing (MEC) is a promising
solution, enabling to offload computation-intensive tasks to nearby edge
servers from the end-user devices, thereby reducing latency and energy
consumption. However, relying solely on a single MEC server for task offloading
can lead to uneven resource utilisation and suboptimal performance in complex
scenarios. Additionally, traditional task offloading strategies specialise in
centralised policy decisions, which unavoidably entail extreme transmission
latency and reach computational bottleneck. To fill the gaps, we propose a
latency and energy efficient Cooperative Task Offloading framework with
Transformer-driven Prediction (CTO-TP), leveraging asynchronous multi-agent
deep reinforcement learning to address these challenges. This approach fosters
edge-edge cooperation and decreases the synchronous waiting time by performing
asynchronous training, optimising task offloading, and resource allocation
across distributed networks. The performance evaluation demonstrates that the
proposed CTO-TP algorithm reduces up to 80% overall system latency and 87%
energy consumption compared to the baseline schemes.

</details>


### [245] [Beyond Cox Models: Assessing the Performance of Machine-Learning Methods in Non-Proportional Hazards and Non-Linear Survival Analysis](https://arxiv.org/abs/2504.17568)
*Ivan Rossi, Flavio Sartori, Cesare Rollo, Giovanni Birolo, Piero Fariselli, Tiziana Sanavia*

Main category: cs.LG

TL;DR: The study compares machine/deep learning methods with penalized Cox models for survival analysis, highlighting conditions where non-linear and non-PH models outperform Cox regression. It emphasizes proper evaluation metrics like Antolini's C-index and Brier's score.


<details>
  <summary>Details</summary>
Motivation: To evaluate methods relaxing linearity and PH assumptions in survival analysis, addressing limitations of Cox models and improper use of evaluation metrics.

Method: Tested eight models (six non-linear, four non-PH) on synthetic and real datasets, comparing performance using Antolini's C-index and Brier's score.

Result: Non-linear and non-PH models can outperform Cox regression under specific conditions. Proper metrics revealed their underestimated performance.

Conclusion: Survival prediction should involve testing multiple methods, considering sample size, non-linearity, and non-PH conditions. Code is available for reproducibility.

Abstract: Survival analysis often relies on Cox models, assuming both linearity and
proportional hazards (PH). This study evaluates machine and deep learning
methods that relax these constraints, comparing their performance with
penalized Cox models on a benchmark of three synthetic and three real datasets.
In total, eight different models were tested, including six non-linear models
of which four were also non-PH. Although Cox regression often yielded
satisfactory performance, we showed the conditions under which machine and deep
learning models can perform better. Indeed, the performance of these methods
has often been underestimated due to the improper use of Harrell's concordance
index (C-index) instead of more appropriate scores such as Antolini's
concordance index, which generalizes C-index in cases where the PH assumption
does not hold. In addition, since occasionally high C-index models happen to be
badly calibrated, combining Antolini's C-index with Brier's score is useful to
assess the overall performance of a survival method. Results on our benchmark
data showed that survival prediction should be approached by testing different
methods to select the most appropriate one according to sample size,
non-linearity and non-PH conditions. To allow an easy reproducibility of these
tests on our benchmark data, code and documentation are freely available at
https://github.com/compbiomed-unito/survhive.

</details>


### [246] [TileLang: A Composable Tiled Programming Model for AI Systems](https://arxiv.org/abs/2504.17577)
*Lei Wang, Yu Cheng, Yining Shi, Zhengju Tang, Zhiwen Mo, Wenhao Xie, Lingxiao Ma, Yuqing Xia, Jilong Xue, Fan Yang, Zhi Yang*

Main category: cs.LG

TL;DR: TileLang simplifies AI kernel programming by decoupling scheduling from dataflow, enabling high-performance kernels with less effort.


<details>
  <summary>Details</summary>
Motivation: Writing high-performance AI kernels is complex due to hardware-centric optimizations, and existing compilers lack usability and expressiveness.

Method: TileLang introduces a tiled programming model with customization annotations and primitives to separate scheduling from dataflow.

Result: TileLang achieves state-of-the-art performance in key kernels, demonstrating its effectiveness.

Conclusion: TileLang's unified paradigm and transparent scheduling provide the power and flexibility needed for modern AI development.

Abstract: Modern AI workloads rely heavily on optimized computing kernels for both
training and inference. These AI kernels follow well-defined data-flow
patterns, such as moving tiles between DRAM and SRAM and performing a sequence
of computations on those tiles. However, writing high-performance kernels
remains complex despite the clarity of these patterns. Achieving peak
performance requires careful, hardware-centric optimizations to fully leverage
modern accelerators. While domain-specific compilers attempt to reduce the
burden of writing high-performance kernels, they often struggle with usability
and expressiveness gaps. In this paper, we present TileLang, a generalized
tiled programming model for more efficient AI Kernel programming. TileLang
decouples scheduling space (thread binding, layout, tensorize and pipeline)
from dataflow, and encapsulated them as a set of customization annotations and
primitives. This approach allows users to focus on the kernel's data-flow
itself, while leaving most other optimizations to compilers. We conduct
comprehensive experiments on commonly-used devices, across numerous
experiments, our evaluation shows that TileLang can achieve state-of-the-art
performance in key kernels, demonstrating that its unified block-and-thread
paradigm and transparent scheduling capabilities deliver both the power and
flexibility demanded by modern AI system development.

</details>


### [247] [Decentralized Time Series Classification with ROCKET Features](https://arxiv.org/abs/2504.17617)
*Bruno Casella, Matthias Jakobs, Marco Aldinucci, Sebastian Buschjäger*

Main category: cs.LG

TL;DR: DROCKS is a decentralized FL framework for time series classification, outperforming client-server FL methods while enhancing robustness and privacy.


<details>
  <summary>Details</summary>
Motivation: Address privacy and robustness issues in federated learning for TSC by eliminating the central server's risks.

Method: Uses ROCKET features and a decentralized approach where nodes refine the model sequentially.

Result: Outperforms client-server FL methods and shows resilience to failures and attacks.

Conclusion: DROCKS offers a robust, privacy-preserving solution for TSC in federated settings.

Abstract: Time series classification (TSC) is a critical task with applications in
various domains, including healthcare, finance, and industrial monitoring. Due
to privacy concerns and data regulations, Federated Learning has emerged as a
promising approach for learning from distributed time series data without
centralizing raw information. However, most FL solutions rely on a
client-server architecture, which introduces robustness and confidentiality
risks related to the distinguished role of the server, which is a single point
of failure and can observe knowledge extracted from clients. To address these
challenges, we propose DROCKS, a fully decentralized FL framework for TSC that
leverages ROCKET (RandOm Convolutional KErnel Transform) features. In DROCKS,
the global model is trained by sequentially traversing a structured path across
federation nodes, where each node refines the model and selects the most
effective local kernels before passing them to the successor. Extensive
experiments on the UCR archive demonstrate that DROCKS outperforms
state-of-the-art client-server FL approaches while being more resilient to node
failures and malicious attacks. Our code is available at
https://anonymous.4open.science/r/DROCKS-7FF3/README.md.

</details>


### [248] [Advancing CMA-ES with Learning-Based Cooperative Coevolution for Scalable Optimization](https://arxiv.org/abs/2504.17578)
*Hongshu Guo, Wenjie Qiu, Zeyuan Ma, Xinglin Zhang, Jun Zhang, Yue-Jiao Gong*

Main category: cs.LG

TL;DR: LCC is a learning-based cooperative coevolution framework that dynamically selects decomposition strategies using a neural network, outperforming existing methods in optimization effectiveness and transferability.


<details>
  <summary>Details</summary>
Motivation: Existing CC paradigms require expert knowledge for variable decomposition, limiting their practicality. LCC aims to automate this process using meta-learning.

Method: LCC uses a neural network trained via Proximal Policy Optimization to dynamically select decomposition strategies based on optimization status features.

Result: LCC outperforms state-of-the-art baselines in optimization effectiveness and resource efficiency, showing strong transferability to new problems.

Conclusion: LCC successfully automates decomposition strategy selection, improving optimization performance and adaptability without requiring expert input.

Abstract: Recent research in Cooperative Coevolution~(CC) have achieved promising
progress in solving large-scale global optimization problems. However, existing
CC paradigms have a primary limitation in that they require deep expertise for
selecting or designing effective variable decomposition strategies. Inspired by
advancements in Meta-Black-Box Optimization, this paper introduces LCC, a
pioneering learning-based cooperative coevolution framework that dynamically
schedules decomposition strategies during optimization processes. The
decomposition strategy selector is parameterized through a neural network,
which processes a meticulously crafted set of optimization status features to
determine the optimal strategy for each optimization step. The network is
trained via the Proximal Policy Optimization method in a reinforcement learning
manner across a collection of representative problems, aiming to maximize the
expected optimization performance. Extensive experimental results demonstrate
that LCC not only offers certain advantages over state-of-the-art baselines in
terms of optimization effectiveness and resource consumption, but it also
exhibits promising transferability towards unseen problems.

</details>


### [249] [Interpretable non-linear dimensionality reduction using gaussian weighted linear transformation](https://arxiv.org/abs/2504.17601)
*Erik Bergh*

Main category: cs.LG

TL;DR: A novel dimensionality reduction method combines linear interpretability with non-linear expressiveness, offering transparent insights and practical tools for analysis.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between representational power and interpretability in existing methods like t-SNE and PCA.

Method: Proposes an algorithm using weighted linear transformations with Gaussian functions to create a non-linear mapping, preserving interpretability.

Result: Provides powerful dimensionality reduction with transparent insights, including tools to analyze transformations.

Conclusion: The method bridges the gap between interpretability and expressiveness, with practical software for adoption.

Abstract: Dimensionality reduction techniques are fundamental for analyzing and
visualizing high-dimensional data. With established methods like t-SNE and PCA
presenting a trade-off between representational power and interpretability.
This paper introduces a novel approach that bridges this gap by combining the
interpretability of linear methods with the expressiveness of non-linear
transformations. The proposed algorithm constructs a non-linear mapping between
high-dimensional and low-dimensional spaces through a combination of linear
transformations, each weighted by Gaussian functions. This architecture enables
complex non-linear transformations while preserving the interpretability
advantages of linear methods, as each transformation can be analyzed
independently. The resulting model provides both powerful dimensionality
reduction and transparent insights into the transformed space. Techniques for
interpreting the learned transformations are presented, including methods for
identifying suppressed dimensions and how space is expanded and contracted.
These tools enable practitioners to understand how the algorithm preserves and
modifies geometric relationships during dimensionality reduction. To ensure the
practical utility of this algorithm, the creation of user-friendly software
packages is emphasized, facilitating its adoption in both academia and
industry.

</details>


### [250] [PTCL: Pseudo-Label Temporal Curriculum Learning for Label-Limited Dynamic Graph](https://arxiv.org/abs/2504.17641)
*Shengtao Zhang, Haokai Zhang, Shiqi Lou, Zicheng Wang, Zinan Zeng, Yilin Wang, Minnan Luo*

Main category: cs.LG

TL;DR: PTCL is a method for dynamic node classification using only final timestamp labels, employing pseudo-labels and temporal curriculum learning.


<details>
  <summary>Details</summary>
Motivation: Dynamic node classification often lacks complete labels due to high costs and uncertainty. Final labels are easier to obtain, but existing methods require full label history.

Method: PTCL uses a temporal decoupling architecture (backbone and decoder) and Temporal Curriculum Learning to prioritize pseudo-labels closer to the final timestamp.

Result: PTCL outperforms other methods, demonstrated on the new CoOAG dataset and real-world scenarios.

Conclusion: PTCL and the FLiD framework provide a solution for label-limited dynamic node classification, with potential for broader applications.

Abstract: Dynamic node classification is critical for modeling evolving systems like
financial transactions and academic collaborations. In such systems,
dynamically capturing node information changes is critical for dynamic node
classification, which usually requires all labels at every timestamp. However,
it is difficult to collect all dynamic labels in real-world scenarios due to
high annotation costs and label uncertainty (e.g., ambiguous or delayed labels
in fraud detection). In contrast, final timestamp labels are easier to obtain
as they rely on complete temporal patterns and are usually maintained as a
unique label for each user in many open platforms, without tracking the history
data. To bridge this gap, we propose PTCL(Pseudo-label Temporal Curriculum
Learning), a pioneering method addressing label-limited dynamic node
classification where only final labels are available. PTCL introduces: (1) a
temporal decoupling architecture separating the backbone (learning time-aware
representations) and decoder (strictly aligned with final labels), which
generate pseudo-labels, and (2) a Temporal Curriculum Learning strategy that
prioritizes pseudo-labels closer to the final timestamp by assigning them
higher weights using an exponentially decaying function. We contribute a new
academic dataset (CoOAG), capturing long-range research interest in dynamic
graph. Experiments across real-world scenarios demonstrate PTCL's consistent
superiority over other methods adapted to this task. Beyond methodology, we
propose a unified framework FLiD (Framework for Label-Limited Dynamic Node
Classification), consisting of a complete preparation workflow, training
pipeline, and evaluation standards, and supporting various models and datasets.
The code can be found at https://github.com/3205914485/FLiD.

</details>


### [251] [TarDiff: Target-Oriented Diffusion Guidance for Synthetic Electronic Health Record Time Series Generation](https://arxiv.org/abs/2504.17613)
*Bowen Deng, Chang Xu, Hao Li, Yuhao Huang, Min Hou, Jiang Bian*

Main category: cs.LG

TL;DR: TarDiff is a target-oriented diffusion framework for generating synthetic EHR time-series data, optimizing for downstream model performance rather than just replicating data distributions.


<details>
  <summary>Details</summary>
Motivation: Existing synthetic EHR generation methods focus on statistical fidelity but may overlook rare conditions, limiting model performance. TarDiff addresses this by prioritizing utility for specific clinical tasks.

Method: TarDiff integrates task-specific influence guidance into diffusion models, measuring synthetic samples' impact on downstream model loss and embedding this gradient into the generation process.

Result: TarDiff outperforms existing methods by up to 20.4% in AUPRC and 18.4% in AUROC across six EHR datasets, enhancing both temporal fidelity and model performance.

Conclusion: TarDiff offers a robust solution to data scarcity and class imbalance in healthcare analytics by generating utility-optimized synthetic data.

Abstract: Synthetic Electronic Health Record (EHR) time-series generation is crucial
for advancing clinical machine learning models, as it helps address data
scarcity by providing more training data. However, most existing approaches
focus primarily on replicating statistical distributions and temporal
dependencies of real-world data. We argue that fidelity to observed data alone
does not guarantee better model performance, as common patterns may dominate,
limiting the representation of rare but important conditions. This highlights
the need for generate synthetic samples to improve performance of specific
clinical models to fulfill their target outcomes. To address this, we propose
TarDiff, a novel target-oriented diffusion framework that integrates
task-specific influence guidance into the synthetic data generation process.
Unlike conventional approaches that mimic training data distributions, TarDiff
optimizes synthetic samples by quantifying their expected contribution to
improving downstream model performance through influence functions.
Specifically, we measure the reduction in task-specific loss induced by
synthetic samples and embed this influence gradient into the reverse diffusion
process, thereby steering the generation towards utility-optimized data.
Evaluated on six publicly available EHR datasets, TarDiff achieves
state-of-the-art performance, outperforming existing methods by up to 20.4% in
AUPRC and 18.4% in AUROC. Our results demonstrate that TarDiff not only
preserves temporal fidelity but also enhances downstream model performance,
offering a robust solution to data scarcity and class imbalance in healthcare
analytics.

</details>


### [252] [Aerial Image Classification in Scarce and Unconstrained Environments via Conformal Prediction](https://arxiv.org/abs/2504.17655)
*Farhad Pourkamali-Anaraki*

Main category: cs.LG

TL;DR: The paper analyzes conformal prediction methods on a challenging aerial image dataset, showing their effectiveness in providing uncertainty estimates with statistical guarantees, even with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of data-scarce and highly variable real-world settings, unlike standard benchmarks, and evaluate the reliability and efficiency of conformal prediction.

Method: Leverages pretrained models (MobileNet, DenseNet, ResNet) fine-tuned with limited labeled data, compares pipelines with/without temperature scaling, and assesses performance using empirical coverage and average prediction set size.

Result: Conformal prediction yields valuable uncertainty estimates even with small labeled samples. Temperature scaling doesn't consistently reduce prediction set size, and model compression shows potential for resource-constrained environments.

Conclusion: Future research should explore noisy/ambiguous labels' impact and effective model reduction strategies for conformal prediction.

Abstract: This paper presents a comprehensive empirical analysis of conformal
prediction methods on a challenging aerial image dataset featuring diverse
events in unconstrained environments. Conformal prediction is a powerful
post-hoc technique that takes the output of any classifier and transforms it
into a set of likely labels, providing a statistical guarantee on the coverage
of the true label. Unlike evaluations on standard benchmarks, our study
addresses the complexities of data-scarce and highly variable real-world
settings. We investigate the effectiveness of leveraging pretrained models
(MobileNet, DenseNet, and ResNet), fine-tuned with limited labeled data, to
generate informative prediction sets. To further evaluate the impact of
calibration, we consider two parallel pipelines (with and without temperature
scaling) and assess performance using two key metrics: empirical coverage and
average prediction set size. This setup allows us to systematically examine how
calibration choices influence the trade-off between reliability and efficiency.
Our findings demonstrate that even with relatively small labeled samples and
simple nonconformity scores, conformal prediction can yield valuable
uncertainty estimates for complex tasks. Moreover, our analysis reveals that
while temperature scaling is often employed for calibration, it does not
consistently lead to smaller prediction sets, underscoring the importance of
careful consideration in its application. Furthermore, our results highlight
the significant potential of model compression techniques within the conformal
prediction pipeline for deployment in resource-constrained environments. Based
on our observations, we advocate for future research to delve into the impact
of noisy or ambiguous labels on conformal prediction performance and to explore
effective model reduction strategies.

</details>


### [253] [The effects of Hessian eigenvalue spectral density type on the applicability of Hessian analysis to generalization capability assessment of neural networks](https://arxiv.org/abs/2504.17618)
*Nikita Gabdullin*

Main category: cs.LG

TL;DR: The paper explores Hessian eigenvalue spectral density (HESD) in neural networks, identifying mainly positive (MP-HESD) and negative (MN-HESD) types, and proposes criteria to determine HESD type for generalization estimation.


<details>
  <summary>Details</summary>
Motivation: To understand factors influencing HESD types and their implications for neural network generalization.

Method: Conducts experiments with various optimizers, datasets, and preprocessing to analyze HESD behavior, including external gradient manipulation effects.

Result: MP-HESD is common in training/fine-tuning, while MN-HESD arises from gradient manipulation. Quasi-singular HESD is observed, impacting Hessian-based generalization criteria.

Conclusion: Proposes a unified HESD analysis methodology, highlighting limitations of conventional Hessian assumptions and the need for careful application in gradient-manipulated scenarios.

Abstract: Hessians of neural network (NN) contain essential information about the
curvature of NN loss landscapes which can be used to estimate NN generalization
capabilities. We have previously proposed generalization criteria that rely on
the observation that Hessian eigenvalue spectral density (HESD) behaves
similarly for a wide class of NNs. This paper further studies their
applicability by investigating factors that can result in different types of
HESD. We conduct a wide range of experiments showing that HESD mainly has
positive eigenvalues (MP-HESD) for NN training and fine-tuning with various
optimizers on different datasets with different preprocessing and augmentation
procedures. We also show that mainly negative HESD (MN-HESD) is a consequence
of external gradient manipulation, indicating that the previously proposed
Hessian analysis methodology cannot be applied in such cases. We also propose
criteria and corresponding conditions to determine HESD type and estimate NN
generalization potential. These HESD types and previously proposed
generalization criteria are combined into a unified HESD analysis methodology.
Finally, we discuss how HESD changes during training, and show the occurrence
of quasi-singular (QS) HESD and its influence on the proposed methodology and
on the conventional assumptions about the relation between Hessian eigenvalues
and NN loss landscape curvature.

</details>


### [254] [Effortless, Simulation-Efficient Bayesian Inference using Tabular Foundation Models](https://arxiv.org/abs/2504.17660)
*Julius Vetter, Manuel Gloeckler, Daniel Gedon, Jakob H. Macke*

Main category: cs.LG

TL;DR: NPE-PF leverages tabular foundation models like TabPFN for efficient, training-free Bayesian inference, outperforming traditional SBI methods in simulation efficiency and robustness.


<details>
  <summary>Details</summary>
Motivation: To achieve accurate Bayesian inference with minimal simulations, especially for costly simulators, by repurposing probabilistic foundation models for tabular data.

Method: Proposes Neural Posterior Estimation with Prior-data Fitted Networks (NPE-PF), using TabPFN as a pre-trained autoregressive conditional density estimator for SBI.

Result: NPE-PF matches or surpasses current SBI methods in accuracy and significantly reduces required simulations, while eliminating network selection and training.

Conclusion: NPE-PF offers a scalable, robust, and user-friendly approach to SBI, enabling efficient inference for diverse stochastic inverse problems.

Abstract: Simulation-based inference (SBI) offers a flexible and general approach to
performing Bayesian inference: In SBI, a neural network is trained on synthetic
data simulated from a model and used to rapidly infer posterior distributions
for observed data. A key goal for SBI is to achieve accurate inference with as
few simulations as possible, especially for expensive simulators. In this work,
we address this challenge by repurposing recent probabilistic foundation models
for tabular data: We show how tabular foundation models -- specifically TabPFN
-- can be used as pre-trained autoregressive conditional density estimators for
SBI. We propose Neural Posterior Estimation with Prior-data Fitted Networks
(NPE-PF) and show that it is competitive with current SBI approaches in terms
of accuracy for both benchmark tasks and two complex scientific inverse
problems. Crucially, it often substantially outperforms them in terms of
simulation efficiency, sometimes requiring orders of magnitude fewer
simulations. NPE-PF eliminates the need for inference network selection,
training, and hyperparameter tuning. We also show that it exhibits superior
robustness to model misspecification and can be scaled to simulation budgets
that exceed the context size limit of TabPFN. NPE-PF provides a new direction
for SBI, where training-free, general-purpose inference models offer efficient,
easy-to-use, and flexible solutions for a wide range of stochastic inverse
problems.

</details>


### [255] [On Multivariate Financial Time Series Classification](https://arxiv.org/abs/2504.17664)
*Grégory Bournassenko*

Main category: cs.LG

TL;DR: Comparison of ML/DL models in financial time series analysis, highlighting Big Data's role.


<details>
  <summary>Details</summary>
Motivation: To explore the effectiveness of small vs. big data approaches and modern vs. traditional models in financial markets.

Method: Comparison of SVM (traditional) and ConvTimeNet (modern) models, focusing on scalability and challenges.

Result: Big Data is crucial for accurate financial time series analysis and prediction.

Conclusion: Understanding and leveraging Big Data enhances financial market predictions.

Abstract: This article investigates the use of Machine Learning and Deep Learning
models in multivariate time series analysis within financial markets. It
compares small and big data approaches, focusing on their distinct challenges
and the benefits of scaling. Traditional methods such as SVMs are contrasted
with modern architectures like ConvTimeNet. The results show the importance of
using and understanding Big Data in depth in the analysis and prediction of
financial time series.

</details>


### [256] [Federated Learning: A Survey on Privacy-Preserving Collaborative Intelligence](https://arxiv.org/abs/2504.17703)
*Edward Collins, Michel Wang*

Main category: cs.LG

TL;DR: A survey on Federated Learning (FL), covering its architecture, lifecycle, challenges, trends, applications, and future directions.


<details>
  <summary>Details</summary>
Motivation: Address data privacy, security, and regulatory concerns by enabling collaborative model training without centralizing sensitive data.

Method: Decentralized approach involving local training, model aggregation, and global updates, with techniques like differential privacy and secure aggregation.

Result: FL is effective in domains like healthcare and finance, but faces challenges like non-IID data and communication overhead.

Conclusion: FL is promising but requires further research to improve scalability, efficiency, and trustworthiness.

Abstract: Federated Learning (FL) has emerged as a transformative paradigm in the field
of distributed machine learning, enabling multiple clients such as mobile
devices, edge nodes, or organizations to collaboratively train a shared global
model without the need to centralize sensitive data. This decentralized
approach addresses growing concerns around data privacy, security, and
regulatory compliance, making it particularly attractive in domains such as
healthcare, finance, and smart IoT systems. This survey provides a concise yet
comprehensive overview of Federated Learning, beginning with its core
architecture and communication protocol. We discuss the standard FL lifecycle,
including local training, model aggregation, and global updates. A particular
emphasis is placed on key technical challenges such as handling non-IID
(non-independent and identically distributed) data, mitigating system and
hardware heterogeneity, reducing communication overhead, and ensuring privacy
through mechanisms like differential privacy and secure aggregation.
Furthermore, we examine emerging trends in FL research, including personalized
FL, cross-device versus cross-silo settings, and integration with other
paradigms such as reinforcement learning and quantum computing. We also
highlight real-world applications and summarize benchmark datasets and
evaluation metrics commonly used in FL research. Finally, we outline open
research problems and future directions to guide the development of scalable,
efficient, and trustworthy FL systems.

</details>


### [257] [Early Detection of Multidrug Resistance Using Multivariate Time Series Analysis and Interpretable Patient-Similarity Representations](https://arxiv.org/abs/2504.17717)
*Óscar Escudero-Arnanz, Antonio G. Marques, Inmaculada Mora-Jiménez, Joaquín Álvarez-Rodríguez, Cristina Soguero-Ruiz*

Main category: cs.LG

TL;DR: An interpretable ML framework for MDR prediction using patient similarity metrics and graph-based analysis, achieving 81% AUC and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing the global health challenge of MDR by improving prediction accuracy and explainability in critical care.

Method: Modeling patients as MTS, using similarity measures (descriptive stats, DTW, Time Cluster Kernel) for classification (Logistic Regression, Random Forest, SVM), and applying spectral clustering/t-SNE for explainability.

Result: Validated on ICU records, achieved 81% AUC, identified key risk factors, and revealed clinically meaningful clusters.

Conclusion: Graph-based patient similarity enhances MDR prediction and interpretability, aiding early detection and risk stratification in critical care.

Abstract: Background and Objectives: Multidrug Resistance (MDR) is a critical global
health issue, causing increased hospital stays, healthcare costs, and
mortality. This study proposes an interpretable Machine Learning (ML) framework
for MDR prediction, aiming for both accurate inference and enhanced
explainability.
  Methods: Patients are modeled as Multivariate Time Series (MTS), capturing
clinical progression and patient-to-patient interactions. Similarity among
patients is quantified using MTS-based methods: descriptive statistics, Dynamic
Time Warping, and Time Cluster Kernel. These similarity measures serve as
inputs for MDR classification via Logistic Regression, Random Forest, and
Support Vector Machines, with dimensionality reduction and kernel
transformations improving model performance. For explainability, patient
similarity networks are constructed from these metrics. Spectral clustering and
t-SNE are applied to identify MDR-related subgroups and visualize high-risk
clusters, enabling insight into clinically relevant patterns.
  Results: The framework was validated on ICU Electronic Health Records from
the University Hospital of Fuenlabrada, achieving an AUC of 81%. It outperforms
baseline ML and deep learning models by leveraging graph-based patient
similarity. The approach identifies key risk factors -- prolonged antibiotic
use, invasive procedures, co-infections, and extended ICU stays -- and reveals
clinically meaningful clusters. Code and results are available at
\https://github.com/oscarescuderoarnanz/DM4MTS.
  Conclusions: Patient similarity representations combined with graph-based
analysis provide accurate MDR prediction and interpretable insights. This
method supports early detection, risk factor identification, and patient
stratification, highlighting the potential of explainable ML in critical care.

</details>


### [258] [Fault Diagnosis in New Wind Turbines using Knowledge from Existing Turbines by Generative Domain Adaptation](https://arxiv.org/abs/2504.17709)
*Stefan Jonas, Angela Meyer*

Main category: cs.LG

TL;DR: A generative deep learning method using CycleGAN improves fault diagnosis in wind turbines with scarce training data by mapping SCADA samples from data-rich turbines.


<details>
  <summary>Details</summary>
Motivation: Overcome the limitation of unreliable fault diagnosis in wind turbines due to scarce training data for normal behavior models (NBMs).

Method: CycleGAN-based domain mapping to make SCADA samples from turbines with limited data resemble those from turbines with abundant data.

Result: Significantly improved fault diagnosis, with F1-score improvements of +10.3% (1 month data) and +16.8% (2 weeks data). Outperforms conventional fine-tuning.

Conclusion: The method enables earlier, more reliable fault diagnosis in new wind farms, offering a promising direction for anomaly detection under data scarcity.

Abstract: Intelligent condition monitoring of wind turbines is essential for reducing
downtimes. Machine learning models trained on wind turbine operation data are
commonly used to detect anomalies and, eventually, operation faults. However,
data-driven normal behavior models (NBMs) require a substantial amount of
training data, as NBMs trained with scarce data may result in unreliable fault
diagnosis. To overcome this limitation, we present a novel generative deep
learning approach to make SCADA samples from one wind turbine lacking training
data resemble SCADA data from wind turbines with representative training data.
Through CycleGAN-based domain mapping, our method enables the application of an
NBM trained on an existing wind turbine to one with severely limited data. We
demonstrate our approach on field data mapping SCADA samples across 7
substantially different WTs. Our findings show significantly improved fault
diagnosis in wind turbines with scarce data. Our method achieves the most
similar anomaly scores to an NBM trained with abundant data, outperforming NBMs
trained on scarce training data with improvements of +10.3% in F1-score when 1
month of training data is available and +16.8% when 2 weeks are available. The
domain mapping approach outperforms conventional fine-tuning at all considered
degrees of data scarcity, ranging from 1 to 8 weeks of training data. The
proposed technique enables earlier and more reliable fault diagnosis in newly
installed wind farms, demonstrating a novel and promising research direction to
improve anomaly detection when faced with training data scarcity.

</details>


### [259] [Conformal Segmentation in Industrial Surface Defect Detection with Statistical Guarantees](https://arxiv.org/abs/2504.17721)
*Cheng Shen, Yuewei Liu*

Main category: cs.LG

TL;DR: A method to improve steel defect detection reliability by statistically rigorous thresholds and prediction sets, ensuring controlled error rates.


<details>
  <summary>Details</summary>
Motivation: Manual inspection is inefficient and costly, while automated CNN-based methods face reliability issues due to annotation uncertainties and overfitting.

Method: Uses calibration data to define loss functions, derive thresholds, and construct prediction sets for defect regions, ensuring bounded error rates.

Result: Achieves controlled error rates on test sets and shows adaptability across calibration-to-test ratios.

Conclusion: The method provides statistically rigorous and adaptable defect detection, improving reliability over traditional and CNN-based approaches.

Abstract: In industrial settings, surface defects on steel can significantly compromise
its service life and elevate potential safety risks. Traditional defect
detection methods predominantly rely on manual inspection, which suffers from
low efficiency and high costs. Although automated defect detection approaches
based on Convolutional Neural Networks(e.g., Mask R-CNN) have advanced rapidly,
their reliability remains challenged due to data annotation uncertainties
during deep model training and overfitting issues. These limitations may lead
to detection deviations when processing the given new test samples, rendering
automated detection processes unreliable. To address this challenge, we first
evaluate the detection model's practical performance through calibration data
that satisfies the independent and identically distributed (i.i.d) condition
with test data. Specifically, we define a loss function for each calibration
sample to quantify detection error rates, such as the complement of recall rate
and false discovery rate. Subsequently, we derive a statistically rigorous
threshold based on a user-defined risk level to identify high-probability
defective pixels in test images, thereby constructing prediction sets (e.g.,
defect regions). This methodology ensures that the expected error rate (mean
error rate) on the test set remains strictly bounced by the predefined risk
level. Additionally, we observe a negative correlation between the average
prediction set size and the risk level on the test set, establishing a
statistically rigorous metric for assessing detection model uncertainty.
Furthermore, our study demonstrates robust and efficient control over the
expected test set error rate across varying calibration-to-test partitioning
ratios, validating the method's adaptability and operational effectiveness.

</details>


### [260] [Towards Robust LLMs: an Adversarial Robustness Measurement Framework](https://arxiv.org/abs/2504.17723)
*Natan Levy, Adiel Ashrov, Guy Katz*

Main category: cs.LG

TL;DR: The paper introduces the RoMA framework to measure LLM robustness against adversarial inputs, showing its accuracy and efficiency compared to formal verification. It highlights variability in robustness across models and tasks, advocating for task-specific evaluations.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are vulnerable to adversarial perturbations, but their robustness is under-explored compared to vision-based models. This gap undermines their reliability in critical applications.

Method: The authors adapt the Robustness Measurement and Assessment (RoMA) framework to quantify LLM resilience without needing model parameters, comparing its results to formal verification methods.

Result: RoMA proves accurate with minimal error margins and high efficiency. Robustness varies significantly across models, tasks, and perturbation types, indicating non-uniformity.

Conclusion: Task-specific robustness evaluations are essential for selecting models based on application needs. The RoMA framework advances reliable LLM deployment by providing a systematic assessment methodology.

Abstract: The rise of Large Language Models (LLMs) has revolutionized artificial
intelligence, yet these models remain vulnerable to adversarial perturbations,
undermining their reliability in high-stakes applications. While adversarial
robustness in vision-based neural networks has been extensively studied, LLM
robustness remains under-explored. We adapt the Robustness Measurement and
Assessment (RoMA) framework to quantify LLM resilience against adversarial
inputs without requiring access to model parameters. By comparing RoMA's
estimates to those of formal verification methods, we demonstrate its accuracy
with minimal error margins while maintaining computational efficiency. Our
empirical evaluation reveals that robustness varies significantly not only
between different models but also across categories within the same task and
between various types of perturbations. This non-uniformity underscores the
need for task-specific robustness evaluations, enabling practitioners to
compare and select models based on application-specific robustness
requirements. Our work provides a systematic methodology to assess LLM
robustness, advancing the development of more reliable language models for
real-world deployment.

</details>


### [261] [Interpretable Early Detection of Parkinson's Disease through Speech Analysis](https://arxiv.org/abs/2504.17739)
*Lorenzo Simone, Mauro Giuseppe Camporeale, Vito Marco Rubino, Vincenzo Gervasi, Giovanni Dimauro*

Main category: cs.LG

TL;DR: A deep learning approach for early Parkinson's disease detection from speech recordings, enhancing interpretability by identifying key vocal segments.


<details>
  <summary>Details</summary>
Motivation: Speech impairments in Parkinson's disease offer early diagnostic opportunities, and machine learning can aid timely detection.

Method: Proposed a deep learning model analyzing speech recordings, focusing on vocal segments to link predictive patterns with articulatory features.

Result: Competitive classification performance on the Italian Parkinson's Voice and Speech Database, with improved interpretability.

Conclusion: The approach effectively detects Parkinson's disease early and provides interpretable insights into speech impairments.

Abstract: Parkinson's disease is a progressive neurodegenerative disorder affecting
motor and non-motor functions, with speech impairments among its earliest
symptoms. Speech impairments offer a valuable diagnostic opportunity, with
machine learning advances providing promising tools for timely detection. In
this research, we propose a deep learning approach for early Parkinson's
disease detection from speech recordings, which also highlights the vocal
segments driving predictions to enhance interpretability. This approach seeks
to associate predictive speech patterns with articulatory features, providing a
basis for interpreting underlying neuromuscular impairments. We evaluated our
approach using the Italian Parkinson's Voice and Speech Database, containing
831 audio recordings from 65 participants, including both healthy individuals
and patients. Our approach showed competitive classification performance
compared to state-of-the-art methods, while providing enhanced interpretability
by identifying key speech features influencing predictions.

</details>


### [262] [Embedding Empirical Distributions for Computing Optimal Transport Maps](https://arxiv.org/abs/2504.17740)
*Mingchen Jiang, Peng Xu, Xichen Ye, Xiaohui Chen, Yun Yang, Yifan Chen*

Main category: cs.LG

TL;DR: A novel transformer-based method for learning optimal transport (OT) maps across multiple distributions, validated through experiments.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in neural OT methods, which focus on single maps between two distributions, by enabling OT map computation for new empirical distributions.

Method: Uses transformer architecture to create embeddings from distributional data of varying lengths, then employs a hypernetwork to generate neural OT maps.

Result: Validated through numerical experiments; model and code available on GitHub.

Conclusion: Proposes an effective approach for multi-distribution OT map learning, with practical implementation provided.

Abstract: Distributional data have become increasingly prominent in modern signal
processing, highlighting the necessity of computing optimal transport (OT) maps
across multiple probability distributions. Nevertheless, recent studies on
neural OT methods predominantly focused on the efficient computation of a
single map between two distributions. To address this challenge, we introduce a
novel approach to learning transport maps for new empirical distributions.
Specifically, we employ the transformer architecture to produce embeddings from
distributional data of varying length; these embeddings are then fed into a
hypernetwork to generate neural OT maps. Various numerical experiments were
conducted to validate the embeddings and the generated OT maps. The model
implementation and the code are provided on
https://github.com/jiangmingchen/HOTET.

</details>


### [263] [MSGCN: Multiplex Spatial Graph Convolution Network for Interlayer Link Weight Prediction](https://arxiv.org/abs/2504.17749)
*Steven E. Wilson, Sina Khanmohammadi*

Main category: cs.LG

TL;DR: The paper proposes MSGCN, a method for predicting link weights in multilayer networks, addressing the complexity of such tasks compared to binary link classification.


<details>
  <summary>Details</summary>
Motivation: Link weight prediction in multilayer networks is complex and understudied, especially compared to simpler tasks like binary link classification.

Method: The authors introduce MSGCN, a model that generalizes spatial graph convolution to multiplex networks, capturing geometric node structures across layers.

Result: MSGCN demonstrates robust, accurate, and generalizable performance in predicting interlayer link weights across diverse multiplex network structures.

Conclusion: MSGCN effectively addresses the challenges of link weight prediction in multilayer networks, offering a scalable and accurate solution.

Abstract: Graph Neural Networks (GNNs) have been widely used for various learning
tasks, ranging from node classification to link prediction. They have
demonstrated excellent performance in multiple domains involving
graph-structured data. However, an important category of learning tasks, namely
link weight prediction, has received less emphasis due to its increased
complexity compared to binary link classification. Link weight prediction
becomes even more challenging when considering multilayer networks, where nodes
can be interconnected across multiple layers. To address these challenges, we
propose a new method named Multiplex Spatial Graph Convolution Network (MSGCN),
which spatially embeds information across multiple layers to predict interlayer
link weights. The MSGCN model generalizes spatial graph convolution to
multiplex networks and captures the geometric structure of nodes across
multiple layers. Extensive experiments using data with known interlayer link
information show that the MSGCN model has robust, accurate, and generalizable
link weight prediction performance across a wide variety of multiplex network
structures.

</details>


### [264] [Disaggregated Deep Learning via In-Physics Computing at Radio Frequency](https://arxiv.org/abs/2504.17752)
*Zhihui Gao, Sri Krishna Vadlamani, Kfir Sulimany, Dirk Englund, Tingjun Chen*

Main category: cs.LG

TL;DR: WISE is a novel computing architecture for wireless edge networks, enabling energy-efficient deep learning inference through wireless broadcasting and in-physics computation.


<details>
  <summary>Details</summary>
Motivation: Resource constraints on edge devices make traditional deep learning deployment inefficient; WISE aims to overcome energy limitations.

Method: Uses disaggregated model access via wireless broadcasting and in-physics computation of complex-valued matrix-vector multiplications at radio frequency.

Result: Achieves 95.7% image classification accuracy with ultra-low power (6.0 fJ/MAC) and high efficiency (165.8 TOPS/W).

Conclusion: WISE significantly improves energy efficiency for deep learning on edge devices, outperforming traditional methods by over 100x.

Abstract: Modern edge devices, such as cameras, drones, and Internet-of-Things nodes,
rely on deep learning to enable a wide range of intelligent applications,
including object recognition, environment perception, and autonomous
navigation. However, deploying deep learning models directly on the often
resource-constrained edge devices demands significant memory footprints and
computational power for real-time inference using traditional digital computing
architectures. In this paper, we present WISE, a novel computing architecture
for wireless edge networks designed to overcome energy constraints in deep
learning inference. WISE achieves this goal through two key innovations:
disaggregated model access via wireless broadcasting and in-physics computation
of general complex-valued matrix-vector multiplications directly at radio
frequency. Using a software-defined radio platform with wirelessly broadcast
model weights over the air, we demonstrate that WISE achieves 95.7% image
classification accuracy with ultra-low operation power of 6.0 fJ/MAC per
client, corresponding to a computation efficiency of 165.8 TOPS/W. This
approach enables energy-efficient deep learning inference on wirelessly
connected edge devices, achieving more than two orders of magnitude improvement
in efficiency compared to traditional digital computing.

</details>


### [265] [Replay to Remember: Retaining Domain Knowledge in Streaming Language Models](https://arxiv.org/abs/2504.17780)
*Sneh Pillai*

Main category: cs.LG

TL;DR: A lightweight method combining LoRA and minimal replay mitigates catastrophic forgetting in LLMs under real-time domain adaptation constraints.


<details>
  <summary>Details</summary>
Motivation: Address catastrophic forgetting in LLMs during continual learning, especially in resource-limited, real-time settings.

Method: Combines LoRA (parameter-efficient tuning) with minimal replay in a streaming setup across medical, genetics, and law domains.

Result: Minimal replay stabilizes and partially restores domain knowledge, reducing catastrophic forgetting.

Conclusion: Offers practical insights for deploying adaptable LLMs in constrained real-world scenarios.

Abstract: Continual learning in large language models (LLMs) typically encounters the
critical challenge of catastrophic forgetting, where previously acquired
knowledge deteriorates upon exposure to new data. While techniques like replay
buffers and parameter-efficient tuning (e.g., Low-Rank Adaptation or LoRA) have
been proposed, few studies investigate real-time domain adaptation under strict
computational and data-stream constraints. In this paper, we demonstrate a
lightweight method combining LoRA and a minimal replay mechanism in a realistic
streaming setting across three diverse knowledge domains: medical question
answering, genetics, and law. Using perplexity, semantic similarity, and
GPT-based human-like evaluation metrics, we quantify the model's adaptation,
forgetting, and recovery over time. Our experiments reveal that while
catastrophic forgetting naturally occurs, even minimal replay significantly
stabilizes and partially restores domain-specific knowledge. This study
contributes practical insights for deploying adaptable LLMs in
resource-constrained, real-world scenarios.

</details>


### [266] [Regressing the Relative Future: Efficient Policy Optimization for Multi-turn RLHF](https://arxiv.org/abs/2410.04612)
*Zhaolin Gao, Wenhao Zhan, Jonathan D. Chang, Gokul Swamy, Kianté Brantley, Jason D. Lee, Wen Sun*

Main category: cs.LG

TL;DR: REFUEL is a policy optimization method for multi-turn dialogue in LLMs, addressing covariate shift by training on self-generated data and outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with multi-turn tasks like dialogue due to covariate shift in RLHF methods.

Method: REFUEL uses a single model for Q-value estimation and trains on self-generated data, framing multi-turn RLHF as regression tasks.

Result: REFUEL outperforms DPO and REBEL, with smaller models (8B) surpassing larger ones (70B) in multi-turn dialogues.

Conclusion: REFUEL is an efficient, theoretically sound solution for multi-turn RLHF, with strong empirical performance.

Abstract: Large Language Models (LLMs) have achieved remarkable success at tasks like
summarization that involve a single turn of interaction. However, they can
still struggle with multi-turn tasks like dialogue that require long-term
planning. Previous works on multi-turn dialogue extend single-turn
reinforcement learning from human feedback (RLHF) methods to the multi-turn
setting by treating all prior dialogue turns as a long context. Such approaches
suffer from covariate shift: the conversations in the training set have
previous turns generated by some reference policy, which means that low
training error may not necessarily correspond to good performance when the
learner is actually in the conversation loop. In response, we introduce
REgressing the RELative FUture (REFUEL), an efficient policy optimization
approach designed to address multi-turn RLHF in LLMs. REFUEL employs a single
model to estimate $Q$-values and trains on self-generated data, addressing the
covariate shift issue. REFUEL frames the multi-turn RLHF problem as a sequence
of regression tasks on iteratively collected datasets, enabling ease of
implementation. Theoretically, we prove that REFUEL can match the performance
of any policy covered by the training set. Empirically, we evaluate our
algorithm by using Llama-3.1-70B-it to simulate a user in conversation with our
model. REFUEL consistently outperforms state-of-the-art methods such as DPO and
REBEL across various settings. Furthermore, despite having only 8 billion
parameters, Llama-3-8B-it fine-tuned with REFUEL outperforms Llama-3.1-70B-it
on long multi-turn dialogues. Implementation of REFUEL can be found at
https://github.com/ZhaolinGao/REFUEL/, and models trained by REFUEL can be
found at https://huggingface.co/Cornell-AGI.

</details>


### [267] [Learning by Doing: An Online Causal Reinforcement Learning Framework with Causal-Aware Policy](https://arxiv.org/abs/2402.04869)
*Ruichu Cai, Siyang Huang, Jie Qiao, Wei Chen, Yan Zeng, Keli Zhang, Fuchun Sun, Yang Yu, Zhifeng Hao*

Main category: cs.LG

TL;DR: The paper proposes a causal reinforcement learning (RL) framework that integrates causal graphical models to improve interpretability and decision-making by alternating between causal structure learning and policy guidance.


<details>
  <summary>Details</summary>
Motivation: Causal knowledge is crucial for human-like reasoning in RL but is underutilized. The paper aims to bridge this gap by explicitly modeling causality to enhance RL agents' interpretability and efficiency.

Method: The method involves modeling state generation with causal graphical models, updating causal structures through active intervention, and alternating between causal learning and policy guidance. A theoretical framework ensures performance guarantees.

Result: The proposed method shows effectiveness and robustness in a simulated fault alarm environment, outperforming state-of-the-art baselines. Theoretical and empirical results confirm the virtuous cycle of causal-guided learning.

Conclusion: The framework successfully integrates causality into RL, improving interpretability and performance. Future work may explore broader applications and benchmarks.

Abstract: As a key component to intuitive cognition and reasoning solutions in human
intelligence, causal knowledge provides great potential for reinforcement
learning (RL) agents' interpretability towards decision-making by helping
reduce the searching space. However, there is still a considerable gap in
discovering and incorporating causality into RL, which hinders the rapid
development of causal RL. In this paper, we consider explicitly modeling the
generation process of states with the causal graphical model, based on which we
augment the policy. We formulate the causal structure updating into the RL
interaction process with active intervention learning of the environment. To
optimize the derived objective, we propose a framework with theoretical
performance guarantees that alternates between two steps: using interventions
for causal structure learning during exploration and using the learned causal
structure for policy guidance during exploitation. Due to the lack of public
benchmarks that allow direct intervention in the state space, we design the
root cause localization task in our simulated fault alarm environment and then
empirically show the effectiveness and robustness of the proposed method
against state-of-the-art baselines. Theoretical analysis shows that our
performance improvement attributes to the virtuous cycle of causal-guided
policy learning and causal structure learning, which aligns with our
experimental results. Codes are available at
https://github.com/DMIRLAB-Group/FaultAlarm_RL.

</details>


### [268] [Keyframe-oriented Vision Token Pruning: Enhancing Efficiency of Large Vision Language Models on Long-Form Video Processing](https://arxiv.org/abs/2503.10742)
*Yudong Liu, Jingwei Sun, Yueqian Lin, Jingyang Zhang, Ming Yin, Qinsi Wang, Jianyi Zhang, Hai Li, Yiran Chen*

Main category: cs.LG

TL;DR: KVTP (Keyframe-oriented Vision Token Pruning) is a novel framework that reduces redundant computation in VLMs for long-form videos by adaptively pruning tokens based on frame relevance, maintaining performance.


<details>
  <summary>Details</summary>
Motivation: VLMs incur high computational overhead due to redundant visual data in long videos; existing methods either disrupt context or overlook dependencies.

Method: KVTP combines keyframe selection and token pruning, adaptively assigning pruning rates to retain essential context while reducing redundancy.

Result: KVTP reduces token usage by 80% without losing spatiotemporal or contextual consistency, cutting computation while maintaining performance.

Conclusion: KVTP enables efficient long-video processing, making VLM deployment more scalable.

Abstract: Vision language models (VLMs) demonstrate strong capabilities in jointly
processing visual and textual data. However, they often incur substantial
computational overhead due to redundant visual information, particularly in
long-form video scenarios. Existing approaches predominantly focus on either
vision token pruning, which may overlook spatio-temporal dependencies, or
keyframe selection, which identifies informative frames but discards others,
thus disrupting contextual continuity. In this work, we propose KVTP
(Keyframe-oriented Vision Token Pruning), a novel framework that overcomes the
drawbacks of token pruning and keyframe selection. By adaptively assigning
pruning rates based on frame relevance to the query, KVTP effectively retains
essential contextual information while significantly reducing redundant
computation. To thoroughly evaluate the long-form video understanding
capacities of VLMs, we curated and reorganized subsets from VideoMME,
EgoSchema, and NextQA into a unified benchmark named SparseKV-QA that
highlights real-world scenarios with sparse but crucial events. Our experiments
with VLMs of various scales show that KVTP can reduce token usage by 80%
without compromising spatiotemporal and contextual consistency, significantly
cutting computation while maintaining the performance. These results
demonstrate our approach's effectiveness in efficient long-video processing,
facilitating more scalable VLM deployment.

</details>


### [269] [Effective Bayesian Causal Inference via Structural Marginalisation and Autoregressive Orders](https://arxiv.org/abs/2402.14781)
*Christian Toth, Christian Knoll, Franz Pernkopf, Robert Peharz*

Main category: cs.LG

TL;DR: The paper introduces a Bayesian causal inference method that decomposes structure marginalization into causal orders and DAGs, using Gaussian processes and a novel ARCO distribution for efficient computation.


<details>
  <summary>Details</summary>
Motivation: Traditional causal inference neglects epistemic model uncertainty, while Bayesian methods face computational challenges in marginalizing over entire causal models.

Method: Decomposes structure marginalization into causal orders and DAGs, using Gaussian processes and a novel auto-regressive distribution (ARCO) for sampling-based approximation.

Result: Outperforms state-of-the-art in structure learning on simulated benchmarks and yields competitive results on real-world data, accurately inferring interventional distributions and causal effects.

Conclusion: The proposed method efficiently addresses computational challenges in Bayesian causal inference, improving accuracy and scalability.

Abstract: The traditional two-stage approach to causal inference first identifies a
single causal model (or equivalence class of models), which is then used to
answer causal queries. However, this neglects any epistemic model uncertainty.
In contrast, Bayesian causal inference does incorporate epistemic uncertainty
into query estimates via Bayesian marginalisation (posterior averaging) over
all causal models. While principled, this marginalisation over entire causal
models, i.e., both causal structures (graphs) and mechanisms, poses a
tremendous computational challenge. In this work, we address this challenge by
decomposing structure marginalisation into the marginalisation over (i) causal
orders and (ii) directed acyclic graphs (DAGs) given an order. We can
marginalise the latter in closed form by limiting the number of parents per
variable and utilising Gaussian processes to model mechanisms. To marginalise
over orders, we use a sampling-based approximation, for which we devise a novel
auto-regressive distribution over causal orders (ARCO). Our method outperforms
state-of-the-art in structure learning on simulated non-linear additive noise
benchmarks, and yields competitive results on real-world data. Furthermore, we
can accurately infer interventional distributions and average causal effects.

</details>


### [270] [Looking beyond the next token](https://arxiv.org/abs/2504.11336)
*Abitha Thankaraj, Yiding Jiang, J. Zico Kolter, Yonatan Bisk*

Main category: cs.LG

TL;DR: Trelawney, a data sequence rearrangement method, improves model performance without architectural changes, enabling better planning, reasoning, and goal generation.


<details>
  <summary>Details</summary>
Motivation: Address the mismatch between causal language model training (predicting tokens from context) and human writing (knowing goals beforehand).

Method: Rearranges and processes training data sequences to align with the true data-generating process, avoiding architectural changes.

Result: Improves performance on benchmarks for planning, algorithmic reasoning, and story generation; enables goal generation.

Conclusion: Trelawney offers a simple yet effective solution, potentially expanding language model capabilities beyond current paradigms.

Abstract: The structure of causal language model training assumes that each token can
be accurately predicted from the previous context. This contrasts with humans'
natural writing and reasoning process, where goals are typically known before
the exact argument or phrasings. While this mismatch has been well studied in
the literature, the working assumption has been that architectural changes are
needed to address this mismatch. We argue that rearranging and processing the
training data sequences can allow models to more accurately imitate the true
data-generating process, and does not require any other changes to the
architecture or training infrastructure. We demonstrate that this technique,
Trelawney, and the inference algorithms derived from it allow us to improve
performance on several key benchmarks that span planning, algorithmic
reasoning, and story generation tasks. Finally, our method naturally enables
the generation of long-term goals at no additional cost. We investigate how
using the model's goal-generation capability can further improve planning and
reasoning. Additionally, we believe Trelawney could potentially open doors to
new capabilities beyond the current language modeling paradigm.

</details>


### [271] [Teaching Large Language Models to Reason through Learning and Forgetting](https://arxiv.org/abs/2504.11364)
*Tianwei Ni, Allen Nie, Sapana Chaudhary, Yao Liu, Huzefa Rangwala, Rasool Fakoor*

Main category: cs.LG

TL;DR: Fine-tuning large language models with successful and failed reasoning paths improves search capability and reduces inference time by 180x.


<details>
  <summary>Details</summary>
Motivation: Inference-time search enhances model performance but increases computational costs and time.

Method: Fine-tune the model using successful and failed reasoning paths with a smaller learning rate to prevent degradation.

Result: Outperforms standard fine-tuning and inference-time search, reducing inference time significantly.

Conclusion: Integrating search capabilities via fine-tuning is effective and efficient for complex reasoning tasks.

Abstract: Leveraging inference-time search in large language models has proven
effective in further enhancing a trained model's capability to solve complex
mathematical and reasoning problems. However, this approach significantly
increases computational costs and inference time, as the model must generate
and evaluate multiple candidate solutions to identify a viable reasoning path.
To address this, we propose an effective approach that integrates search
capabilities directly into the model by fine-tuning it using both successful
(learning) and failed reasoning paths (forgetting) derived from diverse search
methods. While fine-tuning the model with these data might seem
straightforward, we identify a critical issue: the model's search capability
tends to degrade rapidly if fine-tuning is performed naively. We show that this
degradation can be substantially mitigated by employing a smaller learning
rate. Extensive experiments on the challenging Game-of-24 and Countdown
mathematical reasoning benchmarks show that our approach not only outperforms
both standard fine-tuning and inference-time search baselines but also
significantly reduces inference time by 180$\times$.

</details>


### [272] [MAGE: Model-Level Graph Neural Networks Explanations via Motif-based Graph Generation](https://arxiv.org/abs/2405.12519)
*Zhaoning Yu, Hongyang Gao*

Main category: cs.LG

TL;DR: MAGE introduces a motif-based GNN explainer to improve interpretability by focusing on valid substructures like rings, addressing limitations of traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional GNN explanation methods fail to identify valid substructures, leading to questionable interpretability in molecular tasks.

Method: MAGE extracts motifs via decomposition, identifies class-specific motifs with attention-based learning, and generates explanations using a motif-based graph generator.

Result: Demonstrated effectiveness on six molecular datasets, yielding human-understandable and valid explanations.

Conclusion: MAGE improves GNN interpretability by incorporating critical substructures, ensuring validity and clarity in explanations.

Abstract: Graph Neural Networks (GNNs) have shown remarkable success in molecular
tasks, yet their interpretability remains challenging. Traditional model-level
explanation methods like XGNN and GNNInterpreter often fail to identify valid
substructures like rings, leading to questionable interpretability. This
limitation stems from XGNN's atom-by-atom approach and GNNInterpreter's
reliance on average graph embeddings, which overlook the essential structural
elements crucial for molecules. To address these gaps, we introduce an
innovative \textbf{M}otif-b\textbf{A}sed \textbf{G}NN \textbf{E}xplainer (MAGE)
that uses motifs as fundamental units for generating explanations. Our approach
begins with extracting potential motifs through a motif decomposition
technique. Then, we utilize an attention-based learning method to identify
class-specific motifs. Finally, we employ a motif-based graph generator for
each class to create molecular graph explanations based on these class-specific
motifs. This novel method not only incorporates critical substructures into the
explanations but also guarantees their validity, yielding results that are
human-understandable. Our proposed method's effectiveness is demonstrated
through quantitative and qualitative assessments conducted on six real-world
molecular datasets.

</details>


### [273] [On Minimizing Adversarial Counterfactual Error in Adversarial RL](https://arxiv.org/abs/2406.04724)
*Roman Belaire, Arunesh Sinha, Pradeep Varakantham*

Main category: cs.LG

TL;DR: The paper introduces Adversarial Counterfactual Error (ACoE) to improve robustness in Deep Reinforcement Learning (DRL) against adversarial noise, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: DRL policies are vulnerable to adversarial noise, risking safety-critical scenarios. Current methods either degrade performance or are overly conservative.

Method: Proposes ACoE, a novel objective balancing value optimization and robustness, and its scalable surrogate C-ACoE for model-free settings.

Result: Empirical evaluations on MuJoCo, Atari, and Highway benchmarks show superior performance over state-of-the-art methods.

Conclusion: ACoE offers a promising direction for robust DRL under adversarial conditions.

Abstract: Deep Reinforcement Learning (DRL) policies are highly susceptible to
adversarial noise in observations, which poses significant risks in
safety-critical scenarios. The challenge inherent to adversarial perturbations
is that by altering the information observed by the agent, the state becomes
only partially observable. Existing approaches address this by either enforcing
consistent actions across nearby states or maximizing the worst-case value
within adversarially perturbed observations. However, the former suffers from
performance degradation when attacks succeed, while the latter tends to be
overly conservative, leading to suboptimal performance in benign settings. We
hypothesize that these limitations stem from their failing to account for
partial observability directly. To this end, we introduce a novel objective
called Adversarial Counterfactual Error (ACoE), defined on the beliefs about
the true state and balancing value optimization with robustness. To make ACoE
scalable in model-free settings, we propose the theoretically-grounded
surrogate objective Cumulative-ACoE (C-ACoE). Our empirical evaluations on
standard benchmarks (MuJoCo, Atari, and Highway) demonstrate that our method
significantly outperforms current state-of-the-art approaches for addressing
adversarial RL challenges, offering a promising direction for improving
robustness in DRL under adversarial conditions. Our code is available at
https://github.com/romanbelaire/acoe-robust-rl.

</details>


### [274] [Diffusion Models Are Real-Time Game Engines](https://arxiv.org/abs/2408.14837)
*Dani Valevski, Yaniv Leviathan, Moab Arar, Shlomi Fruchter*

Main category: cs.LG

TL;DR: GameNGen is a neural-powered game engine enabling real-time interaction with high-quality, stable gameplay, trained on DOOM and achieving near-human-indistinguishable results.


<details>
  <summary>Details</summary>
Motivation: To create a game engine entirely powered by neural models, capable of real-time interaction and high-quality, stable gameplay over long trajectories.

Method: Trained in two phases: (1) an RL-agent learns and records gameplay, and (2) a diffusion model generates next frames conditioned on past frames and actions, with conditioning augmentations and decoder fine-tuning for stability and fidelity.

Result: Achieves 20 FPS on a single TPU, PSNR of 29.4, and human raters struggle to distinguish real clips from generated ones even after 5 minutes.

Conclusion: GameNGen demonstrates the feasibility of neural-powered game engines for real-time, high-quality gameplay simulation.

Abstract: We present GameNGen, the first game engine powered entirely by a neural model
that also enables real-time interaction with a complex environment over long
trajectories at high quality. When trained on the classic game DOOM, GameNGen
extracts gameplay and uses it to generate a playable environment that can
interactively simulate new trajectories. GameNGen runs at 20 frames per second
on a single TPU and remains stable over extended multi-minute play sessions.
Next frame prediction achieves a PSNR of 29.4, comparable to lossy JPEG
compression. Human raters are only slightly better than random chance at
distinguishing short clips of the game from clips of the simulation, even after
5 minutes of auto-regressive generation. GameNGen is trained in two phases: (1)
an RL-agent learns to play the game and the training sessions are recorded, and
(2) a diffusion model is trained to produce the next frame, conditioned on the
sequence of past frames and actions. Conditioning augmentations help ensure
stable auto-regressive generation over long trajectories, and decoder
fine-tuning improves the fidelity of visual details and text.

</details>


### [275] [On the Benefits of Memory for Modeling Time-Dependent PDEs](https://arxiv.org/abs/2409.02313)
*Ricardo Buitrago Ruiz, Tanya Marwah, Albert Gu, Andrej Risteski*

Main category: cs.LG

TL;DR: The paper explores using memory (past states) to improve predictions for time-dependent PDEs, introducing Memory Neural Operator (MemNO) which outperforms Markovian methods, especially in low-resolution or noisy scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of Markovian approaches in solving time-dependent PDEs by leveraging past states for better predictions, inspired by the Mori-Zwanzig theory.

Method: Introduces MemNO, combining state space models (S4) and Fourier Neural Operators (FNOs) to model memory effectively.

Result: MemNO reduces test error by up to 6x compared to Markovian baselines, particularly for PDEs with high-frequency modes or noise.

Conclusion: Memory-based approaches like MemNO significantly enhance PDE solutions, especially in challenging scenarios like low resolution or noise.

Abstract: Data-driven techniques have emerged as a promising alternative to traditional
numerical methods for solving PDEs. For time-dependent PDEs, many approaches
are Markovian -- the evolution of the trained system only depends on the
current state, and not the past states. In this work, we investigate the
benefits of using memory for modeling time-dependent PDEs: that is, when past
states are explicitly used to predict the future. Motivated by the Mori-Zwanzig
theory of model reduction, we theoretically exhibit examples of simple (even
linear) PDEs, in which a solution that uses memory is arbitrarily better than a
Markovian solution. Additionally, we introduce Memory Neural Operator (MemNO),
a neural operator architecture that combines recent state space models
(specifically, S4) and Fourier Neural Operators (FNOs) to effectively model
memory. We empirically demonstrate that when the PDEs are supplied in low
resolution or contain observation noise at train and test time, MemNO
significantly outperforms the baselines without memory -- with up to 6x
reduction in test error. Furthermore, we show that this benefit is particularly
pronounced when the PDE solutions have significant high-frequency Fourier modes
(e.g., low-viscosity fluid dynamics) and we construct a challenging benchmark
dataset consisting of such PDEs.

</details>


### [276] [nGPT: Normalized Transformer with Representation Learning on the Hypersphere](https://arxiv.org/abs/2410.01131)
*Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun, Boris Ginsburg*

Main category: cs.LG

TL;DR: The paper introduces nGPT, a normalized Transformer with hypersphere-based representation learning, showing faster training and improved efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance training efficiency and accuracy in neural networks by normalizing vectors and leveraging hypersphere geometry.

Method: Proposes nGPT, where embeddings, MLP, attention matrices, and hidden states are unit norm normalized, operating on a hypersphere.

Result: nGPT reduces training steps by 4-20x while maintaining accuracy, depending on sequence length.

Conclusion: Normalization and hypersphere-based learning in nGPT significantly improve training efficiency without compromising performance.

Abstract: We propose a novel neural network architecture, the normalized Transformer
(nGPT) with representation learning on the hypersphere. In nGPT, all vectors
forming the embeddings, MLP, attention matrices and hidden states are unit norm
normalized. The input stream of tokens travels on the surface of a hypersphere,
with each layer contributing a displacement towards the target output
predictions. These displacements are defined by the MLP and attention blocks,
whose vector components also reside on the same hypersphere. Experiments show
that nGPT learns much faster, reducing the number of training steps required to
achieve the same accuracy by a factor of 4 to 20, depending on the sequence
length.

</details>


### [277] [A Simple and Efficient Approach to Batch Bayesian Optimization](https://arxiv.org/abs/2411.16206)
*Dawei Zhan, Zhaoxi Zeng, Shuoxiao Wei, Ping Wu*

Main category: cs.LG

TL;DR: A scalable batch Bayesian optimization method using axis-aligned subspaces and expected subspace improvement for parallel evaluation.


<details>
  <summary>Details</summary>
Motivation: Current batch Bayesian optimization methods perform poorly with large batch sizes, limiting their use in parallel computing.

Method: Proposes drawing batch subspaces and selecting acquisition points from each using the expected subspace improvement criterion.

Result: Outperforms sequential Bayesian optimization and competes with seven batch methods, significantly speeding up convergence.

Conclusion: The approach is simple, efficient, and scalable for large-scale batch Bayesian optimization.

Abstract: Extending Bayesian optimization to batch evaluation can enable the designer
to make the most use of parallel computing technology. However, most of current
batch approaches do not scale well with the batch size. That is, their
performances deteriorate dramatically as the batch size increases. To address
this issue, we propose a simple and efficient approach to extend Bayesian
optimization to large-scale batch evaluation in this work. Different from
existing batch approaches, the idea of the new approach is to draw a batch of
axis-aligned subspaces of the original problem and select one acquisition point
from each subspace. To achieve this, we propose the expected subspace
improvement criterion to measure the amount of the improvement that a candidate
point can achieve within a certain axis-aligned subspace. By optimizing these
expected subspace improvement functions simultaneously, we can get a batch of
query points for parallel evaluation. Numerical experiments show that our
proposed approach can speedup the convergence significantly when compared with
the sequential Bayesian optimization algorithm, and performs very competitively
when compared with seven batch Bayesian optimization algorithms. A Matlab
implementation of the proposed approach is available at
https://github.com/zhandawei/Expected_Subspace_Improvement_Batch_Bayesian_Optimization.

</details>


### [278] [Know Unreported Roadway Incidents in Real-time: Early Traffic Anomaly Detection](https://arxiv.org/abs/2412.10892)
*Haocheng Duan, Hao Wu, Sean Qian*

Main category: cs.LG

TL;DR: A deep learning framework for early detection and prediction of traffic anomalies, outperforming conventional methods by leveraging spatial effects and early-stage characteristics.


<details>
  <summary>Details</summary>
Motivation: To enable real-time, early detection of unreported traffic anomalies for urgent traffic management, addressing limitations of conventional AID methods.

Method: Proposes a deep learning framework incorporating domain knowledge and scalable design, using low-cost data without manual intervention.

Result: Demonstrates more effective and early anomaly detection across various road segments.

Conclusion: The framework offers scalable, generalizable, and automated early anomaly detection, improving traffic management.

Abstract: This research aims to know traffic anomalies as early as possible. A traffic
anomaly refers to a generic incident on the road that influences traffic flow
and calls for urgent traffic management measures. `Knowing'' the occurrence of
a traffic anomaly is twofold: the ability to detect this anomaly before it is
reported anywhere, or it may be such that an anomaly can be predicted before it
actually occurs on the road (e.g., non-recurrent traffic breakdown). In either
way, the objective is to inform traffic operators of unreported incidents in
real time and as early as possible. The key is to stay ahead of the curve. Time
is of the essence.
  Conventional automatic incident detection (AID) methods often struggle with
early detection due to their limited consideration of spatial effects and
early-stage characteristics. Therefore, we propose a deep learning framework
utilizing prior domain knowledge and model-designing strategies. This allows
the model to detect a broader range of anomalies, not only incidents that
significantly influence traffic flow but also early characteristics of
incidents along with historically unreported anomalies. We specially design the
model to target the early-stage detection/prediction of an incident.
Additionally, unlike most conventional AID studies, our method is highly
scalable and generalizable, as it is fully automated with no manual selection
of historical reports required, relies solely on widely available low-cost
data, and requires no additional detectors. The experimental results across
numerous road segments on different maps demonstrate that our model leads to
more effective and early anomaly detection.

</details>


### [279] [Predictive and prescriptive analytics for multi-site modelling of frail and elderly patient services](https://arxiv.org/abs/2311.07283)
*Elizabeth Williams, Daniel Gartner, Paul Harper*

Main category: cs.LG

TL;DR: The paper integrates predictive and prescriptive analytics to optimize bed and staffing capacity in hospitals, achieving a 7% cost saving.


<details>
  <summary>Details</summary>
Motivation: Addressing operational challenges in healthcare due to ageing populations, focusing on resource capacity planning.

Method: Uses Classification and Regression Trees for predictive modeling and deterministic/stochastic optimization for prescriptive planning.

Result: 7% cost saving compared to average-based planning, with improved capacity allocation.

Conclusion: The combined methodology is versatile and effective for sectors facing similar resource planning challenges.

Abstract: Many economies are challenged by the effects of an ageing population,
particularly in sectors where resource capacity planning is critical, such as
healthcare. This research addresses the operational challenges of bed and
staffing capacity planning in hospital wards by using predictive and
prescriptive analytical methods, both individually and in tandem. We applied
these methodologies to a study of 165,000 patients across a network of 11
hospitals in the UK. Predictive modelling, specifically Classification and
Regression Trees, forecasts patient length of stay based on clinical and
demographic data. On the prescriptive side, deterministic and two-stage
stochastic optimisation models determine optimal bed and staff planning
strategies to minimise costs. Linking the predictive models with the
prescriptive optimisation models, generates demand forecasts that inform the
optimisation process, providing accurate and practical solutions. The results
demonstrate that this integrated approach captures real-world variations in
patient LOS and offers a 7% cost saving compared to average-based planning.
This approach helps healthcare managers make robust decisions by incorporating
patient-specific characteristics, improving capacity allocation, and mitigating
risks associated with demand variability. Consequently, this combined
methodology can be broadly extended across various sectors facing similar
challenges, showcasing the versatility and effectiveness of integrating
predictive and prescriptive analytics.

</details>


### [280] [Emergent Symbol-like Number Variables in Artificial Neural Networks](https://arxiv.org/abs/2501.06141)
*Satchel Grant, Noah D. Goodman, James L. McClelland*

Main category: cs.LG

TL;DR: The paper explores how neural networks (NNs) represent numeric information and interprets their solutions through symbolic algorithms (SAs). It uses GRUs, LSTMs, and Transformers on numeric tasks, showing that NN activity can be understood via interpretable subspaces, though interpretations are often graded. The work introduces Alignment Functions for flexibility and emphasizes causal interventions for interpretability.


<details>
  <summary>Details</summary>
Motivation: To understand how NNs represent numeric information and whether their solutions can be interpreted through symbolic algorithms, providing insights into NN interpretability.

Method: GRUs, LSTMs, and Transformers are trained using Next Token Prediction (NTP) on numeric tasks. The study uses causal and theoretical methods to interpret NN activity via symbolic algorithms and introduces Alignment Functions for Distributed Alignment Search (DAS).

Result: Recurrent models develop symbol-like number variables, and Transformers use anti-Markovian solutions without sufficient attention layers. Interpretations are graded, highlighting the complexity of NN activity.

Conclusion: NNs should be interpreted at the level of neural subspaces through symbolic algorithms, emphasizing causal interventions and flexible methods like Alignment Functions.

Abstract: What types of numeric representations emerge in neural systems? What would a
satisfying answer to this question look like? In this work, we interpret Neural
Network (NN) solutions to sequence based counting tasks through a variety of
lenses. We seek to understand how well we can understand NNs through the lens
of interpretable Symbolic Algorithms (SAs), where SAs are defined by precise,
abstract, mutable variables used to perform computations. We use GRUs, LSTMs,
and Transformers trained using Next Token Prediction (NTP) on numeric tasks
where the solutions to the tasks depend on numeric information only latent in
the task structure. We show through multiple causal and theoretical methods
that we can interpret NN's raw activity through the lens of simplified SAs when
we frame the neural activity in terms of interpretable subspaces rather than
individual neurons. Depending on the analysis, however, these interpretations
can be graded, existing on a continuum, highlighting the philosophical question
of what it means to "interpret" neural activity, and motivating us to introduce
Alignment Functions to add flexibility to the existing Distributed Alignment
Search (DAS) method. Through our specific analyses we show the importance of
causal interventions for NN interpretability; we show that recurrent models
develop graded, symbol-like number variables within their neural activity; we
introduce a generalization of DAS to frame NN activity in terms of linear
functions of interpretable variables; and we show that Transformers must use
anti-Markovian solutions -- solutions that avoid using cumulative, Markovian
hidden states -- in the absence of sufficient attention layers. We use our
results to encourage interpreting NNs at the level of neural subspaces through
the lens of SAs.

</details>


### [281] [MUVO: A Multimodal Generative World Model for Autonomous Driving with Geometric Representations](https://arxiv.org/abs/2311.11762)
*Daniel Bogdoll, Yitian Yang, Tim Joseph, Melih Yazgan, J. Marius Zöllner*

Main category: cs.LG

TL;DR: The paper explores multimodal sensor fusion (camera and lidar) and 3D occupancy prediction in autonomous driving world models, introducing MUVO to evaluate their impact.


<details>
  <summary>Details</summary>
Motivation: Most autonomous driving models focus on camera data, neglecting lidar or combined sensor setups. Raw sensor predictions are less actionable than 3D occupancy predictions, yet no prior work examines their combined effects.

Method: The authors conduct experiments using MUVO (MUltimodal World Model with Geometric VOxel representations) to evaluate sensor fusion strategies and 3D occupancy prediction.

Result: The study provides insights into the effects of multimodal sensor fusion and highlights the benefits of 3D occupancy prediction.

Conclusion: Combining multimodal sensor data with 3D occupancy prediction improves autonomous driving world models, addressing gaps in current approaches.

Abstract: World models for autonomous driving have the potential to dramatically
improve the reasoning capabilities of today's systems. However, most works
focus on camera data, with only a few that leverage lidar data or combine both
to better represent autonomous vehicle sensor setups. In addition, raw sensor
predictions are less actionable than 3D occupancy predictions, but there are no
works examining the effects of combining both multimodal sensor data and 3D
occupancy prediction. In this work, we perform a set of experiments with a
MUltimodal World Model with Geometric VOxel representations (MUVO) to evaluate
different sensor fusion strategies to better understand the effects on sensor
data prediction. We also analyze potential weaknesses of current sensor fusion
approaches and examine the benefits of additionally predicting 3D occupancy.

</details>


### [282] [Model Alignment Search](https://arxiv.org/abs/2501.06164)
*Satchel Grant*

Main category: cs.LG

TL;DR: The paper introduces Model Alignment Search (MAS), a method to causally explore representational similarity between neural systems and behavior, avoiding correlation pitfalls.


<details>
  <summary>Details</summary>
Motivation: To isolate specific functional aspects of representational similarity and relate them to behavior, addressing cause vs. correlation issues.

Method: MAS learns invertible linear transformations to align subspaces between two networks' representations, enabling isolation and manipulation of functional information.

Result: MAS successfully transfers causal variables (e.g., item count in a task) between networks with different seeds/architectures and explores numeric representations in models.

Conclusion: MAS offers a causal approach to functional similarity, with a counterfactual loss aiding alignment even without causal access to one model.

Abstract: When can we say that two neural systems are the same? The answer to this
question is goal-dependent, and it is often addressed through correlative
methods such as Representational Similarity Analysis (RSA) and Centered Kernel
Alignment (CKA). We find ourselves chiefly interested in the relationship
between representations and behavior, asking ourselves how we can isolate
specific functional aspects of representational similarity to relate our
measures to behavior -- avoiding cause vs. correlation pitfalls in the process.
In this work, we introduce Model Alignment Search (MAS), a method for causally
exploring distributed representational similarity as it relates to behavior.
The method learns invertible linear transformations that find an aligned
subspace between two distributed networks' representations where functional
information can be isolated and manipulated. We first show that the method can
be used to transfer values of specific causal variables -- such as the number
of items in a counting task -- between networks with different training seeds
and different architectures. We then explore open questions in number cognition
by comparing different types of numeric representations in models trained on
structurally different tasks, we explore differences between MAS and
preexisting functional similarity methods, and lastly, we introduce a
counterfactual latent auxiliary loss that helps shape functionally relevant
alignments even in cases where we do not have causal access to one of the two
models for training.

</details>


### [283] [PARMESAN: Parameter-Free Memory Search and Transduction for Dense Prediction Tasks](https://arxiv.org/abs/2403.11743)
*Philip Matthias Winter, Maria Wimmer, David Major, Dimitrios Lenis, Astrid Berg, Theresa Neubauer, Gaia Romana De Paolis, Johannes Novotny, Sophia Ulonska, Katja Bühler*

Main category: cs.LG

TL;DR: PARMESAN introduces a parameter-free method for deep learning flexibility using transductive reasoning and memory search, outperforming baselines in speed and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing inflexibility in deep learning adaptation, especially in continual learning, by separating computation from memory.

Method: PARMESAN leverages a memory module for dense prediction tasks, searching hidden representations at inference without continuous parameter training.

Result: Learns 3-4 orders of magnitude faster than baselines while matching performance, hardware-efficiency, and knowledge retention.

Conclusion: PARMESAN offers a scalable, efficient solution for continual learning, compatible with various architectures and data types.

Abstract: This work addresses flexibility in deep learning by means of transductive
reasoning. For adaptation to new data and tasks, e.g., in continual learning,
existing methods typically involve tuning learnable parameters or complete
re-training from scratch, rendering such approaches unflexible in practice. We
argue that the notion of separating computation from memory by the means of
transduction can act as a stepping stone for solving these issues. We therefore
propose PARMESAN (parameter-free memory search and transduction), a scalable
method which leverages a memory module for solving dense prediction tasks. At
inference, hidden representations in memory are being searched to find
corresponding patterns. In contrast to other methods that rely on continuous
training of learnable parameters, PARMESAN learns via memory consolidation
simply by modifying stored contents. Our method is compatible with commonly
used architectures and canonically transfers to 1D, 2D, and 3D grid-based data.
The capabilities of our approach are demonstrated at the complex task of
continual learning. PARMESAN learns by 3-4 orders of magnitude faster than
established baselines while being on par in terms of predictive performance,
hardware-efficiency, and knowledge retention.

</details>


### [284] [T-Explainer: A Model-Agnostic Explainability Framework Based on Gradients](https://arxiv.org/abs/2404.16495)
*Evandro S. Ortigossa, Fábio F. Dias, Brian Barr, Claudio T. Silva, Luis Gustavo Nonato*

Main category: cs.LG

TL;DR: The paper introduces T-Explainer, a novel XAI method based on Taylor expansion, addressing instability in feature attribution methods and offering local accuracy and consistency.


<details>
  <summary>Details</summary>
Motivation: The opacity of complex machine learning models hinders their practical application in critical domains, necessitating explainable AI (XAI) solutions.

Method: T-Explainer, an additive attribution explainer using Taylor expansion, is proposed to provide stable and accurate feature importance explanations.

Result: Benchmark experiments show T-Explainer's effectiveness and stability compared to existing methods, with tools for evaluation and visualization.

Conclusion: T-Explainer serves as a comprehensive XAI framework, improving transparency and reliability in model explanations.

Abstract: The development of machine learning applications has increased significantly
in recent years, motivated by the remarkable ability of learning-powered
systems to discover and generalize intricate patterns hidden in massive
datasets. Modern learning models, while powerful, often exhibit a complexity
level that renders them opaque black boxes, lacking transparency and hindering
our understanding of their decision-making processes. Opacity challenges the
practical application of machine learning, especially in critical domains
requiring informed decisions. Explainable Artificial Intelligence (XAI)
addresses that challenge, unraveling the complexity of black boxes by providing
explanations. Feature attribution/importance XAI stands out for its ability to
delineate the significance of input features in predictions. However, most
attribution methods have limitations, such as instability, when divergent
explanations result from similar or the same instance. This work introduces
T-Explainer, a novel additive attribution explainer based on the Taylor
expansion that offers desirable properties such as local accuracy and
consistency. We demonstrate T-Explainer's effectiveness and stability over
multiple runs in quantitative benchmark experiments against well-known
attribution methods. Additionally, we provide several tools to evaluate and
visualize explanations, turning T-Explainer into a comprehensive XAI framework.

</details>


### [285] [Spatially-Delineated Domain-Adapted AI Classification: An Application for Oncology Data](https://arxiv.org/abs/2501.11695)
*Majid Farhadloo, Arun Sharma, Alexey Leontovich, Svetomir N. Markovic, Shashi Shekhar*

Main category: cs.LG

TL;DR: A novel multi-task self-learning framework improves classification accuracy by focusing on spatial arrangements in multi-type point maps, outperforming baseline methods.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of spatial variability and heterogeneity in point arrangements across different place-types, aiming to improve classification accuracy for applications like cancer immunotherapy design.

Method: The proposed framework uses spatial mix-up masking and spatial contrastive predictive coding to target spatial arrangements, enhancing domain-adapted AI classification.

Result: Experiments on real-world datasets (e.g., oncology data) demonstrate higher prediction accuracy compared to baseline methods.

Conclusion: The framework effectively mitigates spatial discrepancies and improves classification performance, offering potential for applications in clinical and other domains.

Abstract: Given multi-type point maps from different place-types (e.g., tumor regions),
our objective is to develop a classifier trained on the source place-type to
accurately distinguish between two classes of the target place-type based on
their point arrangements. This problem is societally important for many
applications, such as generating clinical hypotheses for designing new
immunotherapies for cancer treatment. The challenge lies in the spatial
variability, the inherent heterogeneity and variation observed in spatial
properties or arrangements across different locations (i.e., place-types).
Previous techniques focus on self-supervised tasks to learn domain-invariant
features and mitigate domain differences; however, they often neglect the
underlying spatial arrangements among data points, leading to significant
discrepancies across different place-types. We explore a novel multi-task
self-learning framework that targets spatial arrangements, such as spatial
mix-up masking and spatial contrastive predictive coding, for
spatially-delineated domain-adapted AI classification. Experimental results on
real-world datasets (e.g., oncology data) show that the proposed framework
provides higher prediction accuracy than baseline methods.

</details>


### [286] [Overcoming Knowledge Barriers: Online Imitation Learning from Visual Observation with Pretrained World Models](https://arxiv.org/abs/2404.18896)
*Xingyuan Zhang, Philip Becker-Ehmck, Patrick van der Smagt, Maximilian Karl*

Main category: cs.LG

TL;DR: The paper addresses challenges in Imitation Learning from Observation (ILfO) with pretrained models, proposing solutions for the Embodiment Knowledge Barrier (EKB) and Demonstration Knowledge Barrier (DKB) via the AIME-NoB algorithm.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in ILfO caused by pretrained models' inability to handle novel observations (EKB) and reliance on limited datasets (DKB).

Method: Introduces AIME-NoB, integrating online interactions and a data-driven regulariser for EKB, and a surrogate reward function for DKB.

Result: AIME-NoB improves sample efficiency and performance on vision-based control tasks in DeepMind Control Suite and MetaWorld benchmarks.

Conclusion: AIME-NoB provides a robust framework for addressing ILfO challenges with pretrained models, enhancing adaptability and accuracy.

Abstract: Pretraining and finetuning models has become increasingly popular in
decision-making. But there are still serious impediments in Imitation Learning
from Observation (ILfO) with pretrained models. This study identifies two
primary obstacles: the Embodiment Knowledge Barrier (EKB) and the Demonstration
Knowledge Barrier (DKB). The EKB emerges due to the pretrained models'
limitations in handling novel observations, which leads to inaccurate action
inference. Conversely, the DKB stems from the reliance on limited demonstration
datasets, restricting the model's adaptability across diverse scenarios. We
propose separate solutions to overcome each barrier and apply them to Action
Inference by Maximising Evidence (AIME), a state-of-the-art algorithm. This new
algorithm, AIME-NoB, integrates online interactions and a data-driven
regulariser to mitigate the EKB. Additionally, it uses a surrogate reward
function to broaden the policy's supported states, addressing the DKB. Our
experiments on vision-based control tasks from the DeepMind Control Suite and
MetaWorld benchmarks show that AIME-NoB significantly improves sample
efficiency and converged performance, presenting a robust framework for
overcoming the challenges in ILfO with pretrained models. Code available at
https://github.com/IcarusWizard/AIME-NoB.

</details>


### [287] [GraphRAG under Fire](https://arxiv.org/abs/2501.14050)
*Jiacheng Liang, Yuhui Wang, Changjiang Li, Rongyi Zhu, Tanqiu Jiang, Neil Gong, Ting Wang*

Main category: cs.LG

TL;DR: GraphRAG improves RAG with multi-scale knowledge graphs but introduces new security vulnerabilities. GRAGPoison exploits these vulnerabilities with high success.


<details>
  <summary>Details</summary>
Motivation: To explore GraphRAG's security implications, particularly its vulnerability to poisoning attacks, given its unexplored security risks.

Method: The study introduces GRAGPoison, an attack leveraging shared relations in knowledge graphs, using relation injection, enhancement, and narrative generation.

Result: GRAGPoison achieves up to 98% success rate and uses less than 68% poisoning text, outperforming existing attacks.

Conclusion: GraphRAG's graph-based features enhance resilience but also create new attack surfaces, highlighting the need for further defensive research.

Abstract: GraphRAG advances retrieval-augmented generation (RAG) by structuring
external knowledge as multi-scale knowledge graphs, enabling language models to
integrate both broad context and granular details in their generation. While
GraphRAG has demonstrated success across domains, its security implications
remain largely unexplored. To bridge this gap, this work examines GraphRAG's
vulnerability to poisoning attacks, uncovering an intriguing security paradox:
compared to conventional RAG, GraphRAG's graph-based indexing and retrieval
enhance resilience against simple poisoning attacks; yet, the same features
also create new attack surfaces. We present GRAGPoison, a novel attack that
exploits shared relations in the underlying knowledge graph to craft poisoning
text capable of compromising multiple queries simultaneously. GRAGPoison
employs three key strategies: i) relation injection to introduce false
knowledge, ii) relation enhancement to amplify poisoning influence, and iii)
narrative generation to embed malicious content within coherent text. Empirical
evaluation across diverse datasets and models shows that GRAGPoison
substantially outperforms existing attacks in terms of effectiveness (up to
98\% success rate) and scalability (using less than 68\% poisoning text) on
various GraphRAG-based systems. We also explore potential defensive measures
and their limitations, identifying promising directions for future research.

</details>


### [288] [Transfer Learning with Foundational Models for Time Series Forecasting using Low-Rank Adaptations](https://arxiv.org/abs/2410.11539)
*M. Germán-Morales, A. J. Rivera-Rivas, M. J. del Jesus Díaz, C. J. Carmona*

Main category: cs.LG

TL;DR: LLIAM adapts Large Language Models for Time Series Forecasting using prompting and Low-Rank Adaptations, outperforming state-of-the-art methods and demonstrating strong generalization in zero-shot scenarios.


<details>
  <summary>Details</summary>
Motivation: To leverage the scalability and adaptability of Foundational Models (FMs) for Time Series Forecasting, avoiding complex modifications and costly training.

Method: Proposes LLIAM, combining time-series prompting and Low-Rank Adaptations for fine-tuning. Evaluated against DL algorithms and TimeLLM, followed by zero-shot testing.

Result: LLIAM achieves competent results, outperforming state-of-the-art methods and generalizing well to unseen domains.

Conclusion: LLIAM offers a straightforward, efficient approach for Time Series Forecasting, promoting resource reuse and aligning with Green AI goals.

Abstract: Foundational Models are an emerging widely used technique of GenAI. These
models are distinguished by their scalability and the ease with which they can
be adapted through the exploitation of Transfer Learning. The availability of
high computational power and large datasets have supported their development,
achieving a high generalization capacity due to the enormous and heterogeneous
amounts of data used in their initial training. These characteristics
contribute to a solid base that can be adapted or adjusted to a wide range of
tasks, increasing their applicability. This study proposes the methodology
LLIAM, a straightforward adaptation of a kind of FM, Large Language Models, for
the Time Series Forecasting task. An adequate time-series prompting schema and
Low-Rank Adaptations are used to enhance the knowledge of the model with
diverse time series datasets, known as the fine-tuning phase. A study divided
in two stages has been performed for evaluating the effectiveness of the
proposed methodology. Initially, a comparison was made between the performance
of LLIAM and different state-of-the-art DL algorithms, including Recurrent
Neural Networks and Temporal Convolutional Networks, as well as a LLM-based
method, TimeLLM. Following this, a zero-shot study is presented in order to
evaluate the generalization capacity of the proposed methodology with time
series datasets from unknown domains not considered in the model training. The
outcomes of this investigation demonstrate the efficacy of LLIAM, highlighting
that this straightforward and general approach can attain competent results
without the necessity for applying complex modifications. This work also
encourages the use of available resources (such as these pre-trained models)
and efficient fine-tuning techniques to avoid unnecessary and costly training,
narrowing the gap between the goals of traditional AI and Green AI.

</details>


### [289] [Optimal Rates for Robust Stochastic Convex Optimization](https://arxiv.org/abs/2412.11003)
*Changyu Gao, Andrew Lowy, Xingyu Zhou, Stephen J. Wright*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Machine learning algorithms in high-dimensional settings are highly
susceptible to the influence of even a small fraction of structured outliers,
making robust optimization techniques essential. In particular, within the
$\epsilon$-contamination model, where an adversary can inspect and replace up
to an $\epsilon$-fraction of the samples, a fundamental open problem is
determining the optimal rates for robust stochastic convex optimization (SCO)
under such contamination. We develop novel algorithms that achieve
minimax-optimal excess risk (up to logarithmic factors) under the
$\epsilon$-contamination model. Our approach improves over existing algorithms,
which are not only suboptimal but also require stringent assumptions, including
Lipschitz continuity and smoothness of individual sample functions. By
contrast, our optimal algorithms do not require these stringent assumptions,
assuming only population-level smoothness of the loss. Moreover, our algorithms
can be adapted to handle the case in which the covariance parameter is unknown,
and can be extended to nonsmooth population risks via convolutional smoothing.
We complement our algorithmic developments with a tight information-theoretic
lower bound for robust SCO.

</details>


### [290] [Weak-to-Strong Diffusion with Reflection](https://arxiv.org/abs/2502.00473)
*Lichen Bai, Masashi Sugiyama, Zeke Xie*

Main category: cs.LG

TL;DR: W2SD reduces the gap between generated and real data by leveraging weak-to-strong model differences, improving outputs across modalities and benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models have a gap between generated and real data due to training and design limitations.

Method: Proposes W2SD, a framework using weak-to-strong model differences to steer latent variables toward real data distribution via reflective operations.

Result: W2SD significantly improves human preference, aesthetic quality, and prompt adherence, achieving SOTA performance with minimal computational overhead.

Conclusion: W2SD is a flexible, effective solution for enhancing diffusion models, with practical utility and deployability.

Abstract: The goal of diffusion generative models is to align the learned distribution
with the real data distribution through gradient score matching. However,
inherent limitations in training data quality, modeling strategies, and
architectural design lead to inevitable gap between generated outputs and real
data. To reduce this gap, we propose Weak-to-Strong Diffusion (W2SD), a novel
framework that utilizes the estimated difference between existing weak and
strong models (i.e., weak-to-strong difference) to bridge the gap between an
ideal model and a strong model. By employing a reflective operation that
alternates between denoising and inversion with weak-to-strong difference, we
theoretically understand that W2SD steers latent variables along sampling
trajectories toward regions of the real data distribution. W2SD is highly
flexible and broadly applicable, enabling diverse improvements through the
strategic selection of weak-to-strong model pairs (e.g., DreamShaper vs. SD1.5,
good experts vs. bad experts in MoE). Extensive experiments demonstrate that
W2SD significantly improves human preference, aesthetic quality, and prompt
adherence, achieving SOTA performance across various modalities (e.g., image,
video), architectures (e.g., UNet-based, DiT-based, MoE), and benchmarks. For
example, Juggernaut-XL with W2SD can improve with the HPSv2 winning rate up to
90% over the original results. Moreover, the performance gains achieved by W2SD
markedly outweigh its additional computational overhead, while the cumulative
improvements from different weak-to-strong difference further solidify its
practical utility and deployability.

</details>


### [291] [Nonasymptotic CLT and Error Bounds for Two-Time-Scale Stochastic Approximation](https://arxiv.org/abs/2502.09884)
*Seo Taek Kong, Sihan Zeng, Thinh T. Doan, R. Srikant*

Main category: cs.LG

TL;DR: The paper analyzes finite-time error rates for two-time-scale stochastic approximation algorithms, improving prior rates to $1/\sqrt{n}$ with Polyak-Ruppert averaging.


<details>
  <summary>Details</summary>
Motivation: Recent machine learning applications require understanding finite-time error rates, but existing analyses are either asymptotic or suboptimal.

Method: Derives a non-asymptotic central limit theorem for two-time-scale stochastic approximation with Polyak-Ruppert averaging.

Result: Achieves $1/\sqrt{n}$ error rate in expectation, significantly improving prior rates.

Conclusion: Polyak-Ruppert averaging enables optimal finite-time convergence for two-time-scale algorithms.

Abstract: We consider linear two-time-scale stochastic approximation algorithms driven
by martingale noise. Recent applications in machine learning motivate the need
to understand finite-time error rates, but conventional stochastic
approximation analysis focus on either asymptotic convergence in distribution
or finite-time bounds that are far from optimal. Prior work on asymptotic
central limit theorems (CLTs) suggest that two-time-scale algorithms may be
able to achieve $1/\sqrt{n}$ error in expectation, with a constant given by the
expected norm of the limiting Gaussian vector. However, the best known
finite-time rates are much slower. We derive the first non-asymptotic central
limit theorem with respect to the Wasserstein-1 distance for two-time-scale
stochastic approximation with Polyak-Ruppert averaging. As a corollary, we show
that expected error achieved by Polyak-Ruppert averaging decays at rate
$1/\sqrt{n}$, which significantly improves on the rates of convergence in prior
works.

</details>


### [292] [Efficient Model Editing with Task Vector Bases: A Theoretical Framework and Scalable Approach](https://arxiv.org/abs/2502.01015)
*Siqi Zeng, Yifei He, Weiqiu You, Yifan Hao, Yao-Hung Hubert Tsai, Makoto Yamada, Han Zhao*

Main category: cs.LG

TL;DR: The paper introduces a theoretical framework for task vector arithmetic, addressing performance and scalability issues in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing task vector approaches lack theoretical grounding, leading to performance gaps and high memory usage, especially for large-scale tasks.

Method: The authors propose the Task Vector Bases framework, building on task arithmetic literature, to reduce memory costs and maintain performance.

Result: The method achieves competitive performance with reduced memory usage, preserving the flexibility of task vector arithmetic.

Conclusion: The framework provides a practical solution for large-scale task arithmetic, balancing performance and scalability.

Abstract: Task vectors, which are derived from the difference between pre-trained and
fine-tuned model weights, enable flexible task adaptation and model merging
through arithmetic operations such as addition and negation. However, existing
approaches often rely on heuristics with limited theoretical support, often
leading to performance gaps comparing to direct task fine tuning. Meanwhile,
although it is easy to manipulate saved task vectors with arithmetic for
different purposes, such compositional flexibility demands high memory usage,
especially when dealing with a huge number of tasks, limiting scalability. This
work addresses these issues with a theoretically grounded framework that
explains task vector arithmetic and introduces the task vector bases framework.
Building upon existing task arithmetic literature, our method significantly
reduces the memory cost for downstream arithmetic with little effort, while
achieving competitive performance and maintaining compositional advantage,
providing a practical solution for large-scale task arithmetic. The code is
available at https://github.com/uiuctml/TaskVectorBasis.

</details>


### [293] [Data Analysis Prediction over Multiple Unseen Datasets: A Vector Embedding Approach](https://arxiv.org/abs/2502.17060)
*Andreas Loizou, Dimitrios Tsoumakos*

Main category: cs.LG

TL;DR: A novel deep learning method, NumTabData2Vec, is proposed to select high-quality datasets by projecting them into vector embeddings, improving analytics accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The challenge of selecting high-quality datasets from large volumes to enhance analytics performance drives the need for an automated solution.

Method: The approach involves dataset vectorization using NumTabData2Vec, a deep learning model, to infer analytics outcomes by comparing similar datasets.

Result: The method accurately predicts analytics outcomes and efficiently distinguishes between real-world scenarios in lower-dimensional vector space.

Conclusion: NumTabData2Vec effectively addresses dataset selection challenges, offering improved accuracy and efficiency in analytics tasks.

Abstract: The massive increase in the data volume and dataset availability for analysts
compels researchers to focus on data content and select high-quality datasets
to enhance the performance of analytics operators. While selecting the highest
quality data for analysis highly increases task accuracy and efficiency, it is
still a hard task, especially when the number of available inputs is very
large. To address this issue, we propose a novel methodology that infers the
outcome of analytics operators by creating a model from datasets similar to the
queried one. Dataset similarity is performed via projecting each dataset to a
vector embedding representation. The vectorization process is performed using
our proposed deep learning model NumTabData2Vec, which takes a whole dataset
and projects it into a lower vector embedding representation space. Through
experimental evaluation, we compare the prediction performance and the
execution time of our framework to another state-of-the-art modelling operator
framework, illustrating that our approach predicts analytics outcomes
accurately. Furthermore, our vectorization model can project different
real-world scenarios to a lower vector embedding representation and distinguish
between them.

</details>


### [294] [Investigating the Relationship Between Debiasing and Artifact Removal using Saliency Maps](https://arxiv.org/abs/2503.00234)
*Lukasz Sztukiewicz, Ignacy Stępka, Michał Wiliński, Jerzy Stefanowski*

Main category: cs.LG

TL;DR: The paper explores the link between debiasing and artifact removal in neural networks for computer vision, using XAI-based metrics to show that debiasing shifts focus away from protected attributes and that artifact removal techniques can enhance fairness.


<details>
  <summary>Details</summary>
Motivation: Addressing fairness and bias in AI is critical, and understanding the relationship between debiasing and artifact removal can improve AI development.

Method: Introduces XAI-based metrics to analyze saliency maps and evaluates debiasing methods that redirect model focus from protected attributes.

Result: Debiasing methods shift model focus away from protected attributes, and artifact removal techniques can be repurposed to improve fairness.

Conclusion: There is a bidirectional connection between fairness and artifact removal, suggesting shared solutions for both challenges.

Abstract: The widespread adoption of machine learning systems has raised critical
concerns about fairness and bias, making mitigating harmful biases essential
for AI development. In this paper, we investigate the relationship between
debiasing and removing artifacts in neural networks for computer vision tasks.
First, we introduce a set of novel XAI-based metrics that analyze saliency maps
to assess shifts in a model's decision-making process. Then, we demonstrate
that successful debiasing methods systematically redirect model focus away from
protected attributes. Finally, we show that techniques originally developed for
artifact removal can be effectively repurposed for improving fairness. These
findings provide evidence for the existence of a bidirectional connection
between ensuring fairness and removing artifacts corresponding to protected
attributes.

</details>


### [295] [Adaptive Resampling with Bootstrap for Noisy Multi-Objective Optimization Problems](https://arxiv.org/abs/2503.21495)
*Timo Budszuhn, Mark Joachim Krallmann, Daniel Horn*

Main category: cs.LG

TL;DR: A resampling decision function for noisy multi-objective optimization is proposed, using bootstrapping and dominance probability, adaptable to unknown noise properties.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between exploration and precision in noisy multi-objective optimization, considering unknown noise distribution.

Method: Uses bootstrapping and dominance probability, transferring observed distributions from other points for adaptability with few observations.

Result: Demonstrated efficiency when integrated into NSGA-II with sequential resampling under various noise conditions.

Conclusion: The proposed method effectively balances exploration and precision in noisy optimization, adapting to unknown noise properties.

Abstract: The challenge of noisy multi-objective optimization lies in the constant
trade-off between exploring new decision points and improving the precision of
known points through resampling. This decision should take into account both
the variability of the objective functions and the current estimate of a point
in relation to the Pareto front. Since the amount and distribution of noise are
generally unknown, it is desirable for a decision function to be highly
adaptive to the properties of the optimization problem. This paper presents a
resampling decision function that incorporates the stochastic nature of the
optimization problem by using bootstrapping and the probability of dominance.
The distribution-free estimation of the probability of dominance is achieved
using bootstrap estimates of the means. To make the procedure applicable even
with very few observations, we transfer the distribution observed at other
decision points. The efficiency of this resampling approach is demonstrated by
applying it in the NSGA-II algorithm with a sequential resampling procedure
under multiple noise variations.

</details>


### [296] [A Robust Model-Based Approach for Continuous-Time Policy Evaluation with Unknown Lévy Process Dynamics](https://arxiv.org/abs/2504.01482)
*Qihao Ye, Xiaochuan Tian, Yuhua Zhu*

Main category: cs.LG

TL;DR: A model-based framework for continuous-time policy evaluation (CTPE) in reinforcement learning is proposed, incorporating Brownian and Lévy noise to handle rare and extreme events. The method solves a PIDE for the value function and uses a robust numerical approach for coefficient recovery.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurately recovering unknown coefficients in stochastic dynamics, especially under heavy-tailed Lévy processes, and improve policy evaluation in reinforcement learning.

Method: The approach formulates policy evaluation as solving a PIDE and combines maximum likelihood estimation with an iterative tail correction mechanism for robust coefficient recovery.

Result: Numerical experiments show the method's effectiveness in recovering heavy-tailed Lévy dynamics and validate the theoretical error analysis for policy evaluation.

Conclusion: The proposed framework provides a robust and accurate solution for policy evaluation in stochastic environments with rare and extreme events.

Abstract: This paper develops a model-based framework for continuous-time policy
evaluation (CTPE) in reinforcement learning, incorporating both Brownian and
L\'evy noise to model stochastic dynamics influenced by rare and extreme
events. Our approach formulates the policy evaluation problem as solving a
partial integro-differential equation (PIDE) for the value function with
unknown coefficients. A key challenge in this setting is accurately recovering
the unknown coefficients in the stochastic dynamics, particularly when driven
by L\'evy processes with heavy tail effects. To address this, we propose a
robust numerical approach that effectively handles both unbiased and censored
trajectory datasets. This method combines maximum likelihood estimation with an
iterative tail correction mechanism, improving the stability and accuracy of
coefficient recovery. Additionally, we establish a theoretical bound for the
policy evaluation error based on coefficient recovery error. Through numerical
experiments, we demonstrate the effectiveness and robustness of our method in
recovering heavy-tailed L\'evy dynamics and verify the theoretical error
analysis in policy evaluation.

</details>


### [297] [Variational Self-Supervised Learning](https://arxiv.org/abs/2504.04318)
*Mehmet Can Yavuz, Berrin Yanikoglu*

Main category: cs.LG

TL;DR: VSSL combines variational inference and self-supervised learning for efficient, decoder-free representation learning, outperforming methods like BYOL and MoCo V3.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between variational modeling and self-supervised learning by eliminating the need for generative reconstruction.

Method: Uses two symmetrically coupled encoders with Gaussian outputs, a teacher-student setup, and replaces ELBO's reconstruction term with cross-view denoising.

Result: Achieves competitive or superior performance on CIFAR-10, CIFAR-100, and ImageNet-100.

Conclusion: VSSL provides a scalable, probabilistically grounded approach for learning transferable representations without generative reconstruction.

Abstract: We present Variational Self-Supervised Learning (VSSL), a novel framework
that combines variational inference with self-supervised learning to enable
efficient, decoder-free representation learning. Unlike traditional VAEs that
rely on input reconstruction via a decoder, VSSL symmetrically couples two
encoders with Gaussian outputs. A momentum-updated teacher network defines a
dynamic, data-dependent prior, while the student encoder produces an
approximate posterior from augmented views. The reconstruction term in the ELBO
is replaced with a cross-view denoising objective, preserving the analytical
tractability of Gaussian KL divergence. We further introduce cosine-based
formulations of KL and log-likelihood terms to enhance semantic alignment in
high-dimensional latent spaces. Experiments on CIFAR-10, CIFAR-100, and
ImageNet-100 show that VSSL achieves competitive or superior performance to
leading self-supervised methods, including BYOL and MoCo V3. VSSL offers a
scalable, probabilistically grounded approach to learning transferable
representations without generative reconstruction, bridging the gap between
variational modeling and modern self-supervised techniques.

</details>


### [298] [Nemotron-CrossThink: Scaling Self-Learning beyond Math Reasoning](https://arxiv.org/abs/2504.13941)
*Syeda Nahida Akter, Shrimai Prabhumoye, Matvei Novikov, Seungju Han, Ying Lin, Evelina Bakhturina, Eric Nyberg, Yejin Choi, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro*

Main category: cs.LG

TL;DR: NEMOTRON-CROSSTHINK improves LLM reasoning by integrating multi-domain data and structured templates in RL training, achieving higher accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Generalizing RL methods for LLMs beyond math is challenging due to limited data, unclear rewards, and diverse tasks.

Method: Incorporates varied data sources, structured templates, verifiable answers, and optimized blending strategies.

Result: Boosts accuracy on math (+30.1%) and non-math benchmarks (+12.8%) while reducing token usage by 28%.

Conclusion: Multi-domain, multi-format RL training enhances LLM accuracy, efficiency, and generalization.

Abstract: Large Language Models (LLMs) have shown strong reasoning capabilities,
particularly when enhanced through Reinforcement Learning (RL). While prior
work has successfully applied RL to mathematical reasoning -- where rules and
correctness are well-defined -- generalizing these methods to broader reasoning
domains remains challenging due to limited data, the lack of verifiable reward
structures, and diverse task requirements. In this work, we propose
NEMOTRON-CROSSTHINK, a framework that systematically incorporates multi-domain
corpora, including both synthetic and real-world question-answer pairs, into RL
training to improve generalization across diverse reasoning tasks.
NEMOTRON-CROSSTHINK addresses key challenges by (1) incorporating data from
varied sources spanning STEM, humanities, social sciences, etc.; (2) applying
structured templates (e.g., multiple-choice and open-ended) to control
answer-space complexity; (3) filtering for verifiable answers; and (4)
optimizing data blending strategies that utilizes data from multiple sources
effectively. Our approach enables scalable and verifiable reward modeling
beyond mathematics and demonstrates improved accuracies on both math (MATH-500:
+30.1%, AMC23:+27.5%) and non-math reasoning benchmarks (MMLU-PRO: +12.8%,
GPQA-DIAMOND: +11.3%, AGIEVAL: +15.1%, SUPERGPQA: +3.8%). Moreover,
NEMOTRON-CROSSTHINK exhibits significantly improved response efficiency --
using 28% fewer tokens for correct answers -- highlighting more focused and
effective reasoning. Through NEMOTRON-CROSSTHINK, we demonstrate that
integrating multi-domain, multi-format data in RL leads to more accurate,
efficient, and generalizable LLMs.

</details>


### [299] [Sharpness-Aware Parameter Selection for Machine Unlearning](https://arxiv.org/abs/2504.06398)
*Saber Malekmohammadi, Hong kyu Lee, Li Xiong*

Main category: cs.LG

TL;DR: The paper proposes a method for efficient machine unlearning by identifying and updating the most influential model parameters, reducing computational cost while maintaining effectiveness.


<details>
  <summary>Details</summary>
Motivation: Sensitive data mistakenly included in training models needs removal, but existing unlearning methods are inefficient or computationally expensive.

Method: Identify model parameters with the largest diagonal Hessian values, as they contribute most to unlearning, and update only these.

Result: The method improves unlearning efficacy with lower computational cost, supported by theoretical and empirical evidence.

Conclusion: Targeted parameter updates based on Hessian values offer an efficient and effective solution for machine unlearning.

Abstract: It often happens that some sensitive personal information, such as credit
card numbers or passwords, are mistakenly incorporated in the training of
machine learning models and need to be removed afterwards. The removal of such
information from a trained model is a complex task that needs to partially
reverse the training process. There have been various machine unlearning
techniques proposed in the literature to address this problem. Most of the
proposed methods revolve around removing individual data samples from a trained
model. Another less explored direction is when features/labels of a group of
data samples need to be reverted. While the existing methods for these tasks do
the unlearning task by updating the whole set of model parameters or only the
last layer of the model, we show that there are a subset of model parameters
that have the largest contribution in the unlearning target features. More
precisely, the model parameters with the largest corresponding diagonal value
in the Hessian matrix (computed at the learned model parameter) have the most
contribution in the unlearning task. By selecting these parameters and updating
them during the unlearning stage, we can have the most progress in unlearning.
We provide theoretical justifications for the proposed strategy by connecting
it to sharpness-aware minimization and robust unlearning. We empirically show
the effectiveness of the proposed strategy in improving the efficacy of
unlearning with a low computational cost.

</details>


### [300] [Clifford Group Equivariant Diffusion Models for 3D Molecular Generation](https://arxiv.org/abs/2504.15773)
*Cong Liu, Sharvaree Vadgama, David Ruhe, Erik Bekkers, Patrick Forré*

Main category: cs.LG

TL;DR: The paper introduces Clifford Diffusion Models (CDMs), leveraging Clifford algebra for E(n)-equivariant diffusion models to capture richer geometric information through higher-order multivector subspaces.


<details>
  <summary>Details</summary>
Motivation: To enhance generative modeling by incorporating richer geometric information from Clifford algebra's multivector subspaces into diffusion models.

Method: Extends diffusion to higher-grade multivectors in Clifford algebra, embedding data in grade-k subspaces and applying latent diffusion across complete multivectors.

Result: Empirical results on QM9 dataset show CDMs are promising for unconditional molecular generation.

Conclusion: CDMs offer a novel approach for generative modeling by utilizing Clifford algebra's expressive power.

Abstract: This paper explores leveraging the Clifford algebra's expressive power for
$\E(n)$-equivariant diffusion models. We utilize the geometric products between
Clifford multivectors and the rich geometric information encoded in Clifford
subspaces in \emph{Clifford Diffusion Models} (CDMs). We extend the diffusion
process beyond just Clifford one-vectors to incorporate all higher-grade
multivector subspaces. The data is embedded in grade-$k$ subspaces, allowing us
to apply latent diffusion across complete multivectors. This enables CDMs to
capture the joint distribution across different subspaces of the algebra,
incorporating richer geometric information through higher-order features. We
provide empirical results for unconditional molecular generation on the QM9
dataset, showing that CDMs provide a promising avenue for generative modeling.

</details>


### [301] [FedMerge: Federated Personalization via Model Merging](https://arxiv.org/abs/2504.06768)
*Shutong Chen, Tianyi Zhou, Guodong Long, Jing Jiang, Chengqi Zhang*

Main category: cs.LG

TL;DR: FedMerge proposes a personalized model per client in federated learning by merging multiple global models with optimized weights, eliminating the need for local finetuning.


<details>
  <summary>Details</summary>
Motivation: Existing FL methods provide limited choices for non-IID clients, requiring local finetuning. FedMerge aims to address this by creating customized models.

Method: FedMerge jointly optimizes global models and merging weights per client, sending a merged model to each client instead of broadcasting global models.

Result: FedMerge outperforms existing FL approaches in non-IID settings across diverse tasks and data types.

Conclusion: FedMerge effectively bridges the local-global gap, offering better personalization without local finetuning.

Abstract: One global model in federated learning (FL) might not be sufficient to serve
many clients with non-IID tasks and distributions. While there has been
advances in FL to train multiple global models for better personalization, they
only provide limited choices to clients so local finetuning is still
indispensable. In this paper, we propose a novel ``FedMerge'' approach that can
create a personalized model per client by simply merging multiple global models
with automatically optimized and customized weights. In FedMerge, a few global
models can serve many non-IID clients, even without further local finetuning.
We formulate this problem as a joint optimization of global models and the
merging weights for each client. Unlike existing FL approaches where the server
broadcasts one or multiple global models to all clients, the server only needs
to send a customized, merged model to each client. Moreover, instead of
periodically interrupting the local training and re-initializing it to a global
model, the merged model aligns better with each client's task and data
distribution, smoothening the local-global gap between consecutive rounds
caused by client drift. We evaluate FedMerge on three different non-IID
settings applied to different domains with diverse tasks and data types, in
which FedMerge consistently outperforms existing FL approaches, including
clustering-based and mixture-of-experts (MoE) based methods.

</details>


### [302] [Reinforcement Learning from Multi-level and Episodic Human Feedback](https://arxiv.org/abs/2504.14732)
*Muhammad Qasim Elahi, Somtochukwu Oguchienti, Maheed H. Ahmed, Mahsa Ghasemi*

Main category: cs.LG

TL;DR: The paper explores multi-level human feedback for reward function design in reinforcement learning, proposing an algorithm to learn rewards and policies from episodic scores, with proven sublinear regret and empirical validation.


<details>
  <summary>Details</summary>
Motivation: Designing effective reward functions in reinforcement learning is challenging, especially for complex tasks. Human feedback paradigms, like comparative feedback, are limited. Multi-level feedback offers richer signals and handles non-Markovian rewards.

Method: The authors propose an algorithm to learn reward functions and optimal policies from multi-level human feedback, which is provided as episodic scores. The method handles non-Markovian rewards and leverages coarse but informative signals.

Result: The algorithm achieves sublinear regret, demonstrating theoretical guarantees. Empirical simulations confirm its effectiveness in learning from episodic feedback.

Conclusion: Multi-level human feedback is a viable alternative to comparative feedback, offering advantages for reward function design. The proposed algorithm is effective and theoretically sound.

Abstract: Designing an effective reward function has long been a challenge in
reinforcement learning, particularly for complex tasks in unstructured
environments. To address this, various learning paradigms have emerged that
leverage different forms of human input to specify or refine the reward
function. Reinforcement learning from human feedback is a prominent approach
that utilizes human comparative feedback, expressed as a preference for one
behavior over another, to tackle this problem. In contrast to comparative
feedback, we explore multi-level human feedback, which is provided in the form
of a score at the end of each episode. This type of feedback offers more coarse
but informative signals about the underlying reward function than binary
feedback. Additionally, it can handle non-Markovian rewards, as it is based on
the evaluation of an entire episode. We propose an algorithm to efficiently
learn both the reward function and the optimal policy from this form of
feedback. Moreover, we show that the proposed algorithm achieves sublinear
regret and demonstrate its empirical effectiveness through extensive
simulations.

</details>


### [303] [An Effective Gram Matrix Characterizes Generalization in Deep Networks](https://arxiv.org/abs/2504.16450)
*Rubing Yang, Pratik Chaudhari*

Main category: cs.LG

TL;DR: The paper derives a differential equation to model the generalization gap in deep networks trained by gradient descent, identifying key factors like contraction and perturbation. It introduces an "effective Gram matrix" to predict test loss and shows training is benign, with generalization influenced by data-architecture alignment.


<details>
  <summary>Details</summary>
Motivation: To understand and predict the generalization gap in deep networks by analyzing the dynamics of gradient descent training.

Method: Derives a differential equation for the generalization gap, controlled by contraction and perturbation factors, and analyzes an "effective Gram matrix" to characterize the gap.

Result: Empirical evaluations show accurate test loss prediction. Training is benign, and generalization depends on data-architecture alignment.

Conclusion: The generalization gap is governed by the alignment between the effective Gram matrix and the residual, with data-architecture match being key.

Abstract: We derive a differential equation that governs the evolution of the
generalization gap when a deep network is trained by gradient descent. This
differential equation is controlled by two quantities, a contraction factor
that brings together trajectories corresponding to slightly different datasets,
and a perturbation factor that accounts for them training on different
datasets. We analyze this differential equation to compute an ``effective Gram
matrix'' that characterizes the generalization gap after training in terms of
the alignment between this Gram matrix and a certain initial ``residual''.
Empirical evaluations on image classification datasets indicate that this
analysis can predict the test loss accurately. Further, at any point during
training, the residual predominantly lies in the subspace of the effective Gram
matrix with the smallest eigenvalues. This indicates that the training process
is benign, i.e., it does not lead to significant deterioration of the
generalization gap (which is zero at initialization). The alignment between the
effective Gram matrix and the residual is different for different datasets and
architectures. The match/mismatch of the data and the architecture is primarily
responsible for good/bad generalization.

</details>


### [304] [Hyper-Transforming Latent Diffusion Models](https://arxiv.org/abs/2504.16580)
*Ignacio Peis, Batuhan Koyuncu, Isabel Valera, Jes Frellsen*

Main category: cs.LG

TL;DR: A novel generative framework combines INRs and Transformer-based hypernetworks for scalable and efficient function generation, improving upon MLP-based methods.


<details>
  <summary>Details</summary>
Motivation: Prior methods using MLP-based hypernetworks face scalability issues. This work aims to enhance representation capacity and computational efficiency.

Method: Integrates Transformer-based hypernetworks into latent variable models, replacing standard decoders in LDMs for INR generation. Training can be from scratch or via hyper-transforming.

Result: The framework efficiently adapts existing generative models to INR-based representations without full retraining.

Conclusion: The proposed method offers a scalable and efficient solution for generating functions using INRs and Transformers.

Abstract: We introduce a novel generative framework for functions by integrating
Implicit Neural Representations (INRs) and Transformer-based hypernetworks into
latent variable models. Unlike prior approaches that rely on MLP-based
hypernetworks with scalability limitations, our method employs a
Transformer-based decoder to generate INR parameters from latent variables,
addressing both representation capacity and computational efficiency. Our
framework extends latent diffusion models (LDMs) to INR generation by replacing
standard decoders with a Transformer-based hypernetwork, which can be trained
either from scratch or via hyper-transforming-a strategy that fine-tunes only
the decoder while freezing the pre-trained latent space. This enables efficient
adaptation of existing generative models to INR-based representations without
requiring full retraining.

</details>


### [305] [Simple Graph Contrastive Learning via Fractional-order Neural Diffusion Networks](https://arxiv.org/abs/2504.16748)
*Yanan Zhao, Feng Ji, Kai Zhao, Xuhao Li, Qiyu Kang, Wenfei Liang, Yahya Alkhatib, Xingchao Jian, Wee Peng Tay*

Main category: cs.LG

TL;DR: A novel augmentation-free Graph Contrastive Learning (GCL) framework using graph neural diffusion models with Fractional Differential Equations (FDE) is introduced, eliminating the need for negative samples and achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing GCL methods, which rely on complex augmentations or negative samples, by proposing an augmentation-free approach that generates diverse views using FDE-based encoders.

Method: Utilizes learnable encoders governed by FDEs, where varying order parameters of the differential operator produces diverse views (local or global) for contrastive learning without negative samples.

Result: The framework achieves state-of-the-art performance across various datasets, including homophilic and heterophilic ones.

Conclusion: The proposed augmentation-free GCL framework with FDE-based encoders is effective, versatile, and eliminates the need for negative samples, advancing unsupervised graph representation learning.

Abstract: Graph Contrastive Learning (GCL) has recently made progress as an
unsupervised graph representation learning paradigm. GCL approaches can be
categorized into augmentation-based and augmentation-free methods. The former
relies on complex data augmentations, while the latter depends on encoders that
can generate distinct views of the same input. Both approaches may require
negative samples for training. In this paper, we introduce a novel
augmentation-free GCL framework based on graph neural diffusion models.
Specifically, we utilize learnable encoders governed by Fractional Differential
Equations (FDE). Each FDE is characterized by an order parameter of the
differential operator. We demonstrate that varying these parameters allows us
to produce learnable encoders that generate diverse views, capturing either
local or global information, for contrastive learning. Our model does not
require negative samples for training and is applicable to both homophilic and
heterophilic datasets. We demonstrate its effectiveness across various
datasets, achieving state-of-the-art performance.

</details>


### [306] [Exploring How LLMs Capture and Represent Domain-Specific Knowledge](https://arxiv.org/abs/2504.16871)
*Mirian Hipolito Garcia, Camille Couturier, Daniel Madrigal Diaz, Ankur Mallick, Anastasios Kyrillidis, Robert Sim, Victor Ruhle, Saravan Rajmohan*

Main category: cs.LG

TL;DR: LLMs can distinguish domain-specific queries using hidden states, and fine-tuned models aren't always the best performers.


<details>
  <summary>Details</summary>
Motivation: To investigate if LLMs inherently capture domain-specific nuances and how robust these representations are to prompt variations.

Method: Probing domain sensitivity by analyzing hidden states during prefill, studying robustness to prompt styles, and using domain representations for model selection.

Result: LLMs can differentiate queries from related domains, and fine-tuned models aren't always the most accurate.

Conclusion: LLMs inherently recognize domain nuances, and model selection based on domain traces can improve performance.

Abstract: We study whether Large Language Models (LLMs) inherently capture
domain-specific nuances in natural language. Our experiments probe the domain
sensitivity of LLMs by examining their ability to distinguish queries from
different domains using hidden states generated during the prefill phase. We
reveal latent domain-related trajectories that indicate the model's internal
recognition of query domains. We also study the robustness of these domain
representations to variations in prompt styles and sources. Our approach
leverages these representations for model selection, mapping the LLM that best
matches the domain trace of the input query (i.e., the model with the highest
performance on similar traces). Our findings show that LLMs can differentiate
queries for related domains, and that the fine-tuned model is not always the
most accurate. Unlike previous work, our interpretations apply to both closed
and open-ended generative tasks

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [307] [AGCo-MATA: Air-Ground Collaborative Multi-Agent Task Allocation in Mobile Crowdsensing](https://arxiv.org/abs/2504.17409)
*Tianhao Shao, Bohan Feng, Yingying Zhou, Bin Guo, Kaixing Zhao*

Main category: cs.MA

TL;DR: The paper proposes two algorithms for task allocation in heterogeneous air-ground collaborative multi-agent systems, optimizing task completion and travel efficiency in different scenarios.


<details>
  <summary>Details</summary>
Motivation: To improve task allocation efficiency in mobile crowd sensing (MCS) using UAVs and UGVs, addressing challenges in heterogeneous multi-agent frameworks.

Method: Developed MT-MCMF for AG-FAMT (maximizing task completion and minimizing travel distance) and W-ILP for AG-MAFT (minimizing travel distance and time cost).

Result: Experiments showed both algorithms outperform baselines in task quantity, difficulty, and distribution scenarios.

Conclusion: The proposed algorithms enhance MCS quality, offering a novel approach for heterogeneous agent task allocation.

Abstract: Rapid progress in intelligent unmanned systems has presented new
opportunities for mobile crowd sensing (MCS). Today, heterogeneous air-ground
collaborative multi-agent framework, which comprise unmanned aerial vehicles
(UAVs) and unmanned ground vehicles (UGVs), have presented superior flexibility
and efficiency compared to traditional homogeneous frameworks in complex
sensing tasks. Within this context, task allocation among different agents
always play an important role in improving overall MCS quality. In order to
better allocate tasks among heterogeneous collaborative agents, in this paper,
we investigated two representative complex multi-agent task allocation
scenarios with dual optimization objectives: (1) For AG-FAMT (Air-Ground Few
Agents More Tasks) scenario, the objectives are to maximize the task completion
while minimizing the total travel distance; (2) For AG-MAFT (Air-Ground More
Agents Few Tasks) scenario, where the agents are allocated based on their
locations, has the optimization objectives of minimizing the total travel
distance while reducing travel time cost. To achieve this, we proposed a
Multi-Task Minimum Cost Maximum Flow (MT-MCMF) optimization algorithm tailored
for AG-FAMT, along with a multi-objective optimization algorithm called W-ILP
designed for AG-MAFT, with a particular focus on optimizing the charging path
planning of UAVs. Our experiments based on a large-scale real-world dataset
demonstrated that the proposed two algorithms both outperform baseline
approaches under varying experimental settings, including task quantity, task
difficulty, and task distribution, providing a novel way to improve the overall
quality of mobile crowdsensing tasks.

</details>


### [308] [A Multi-Agent, Laxity-Based Aggregation Strategy for Cost-Effective Electric Vehicle Charging and Local Transformer Overload Prevention](https://arxiv.org/abs/2504.17575)
*Kristoffer Christensen, Bo Nørregaard Jørgensen, Zheng Grace Ma*

Main category: cs.MA

TL;DR: An aggregator-based coordination mechanism for EV charging prevents transformer overloads using a simple rule-based algorithm, proving cost-effective compared to infrastructure upgrades.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of transformer overloads from concurrent EV charging, which current tariff strategies fail to resolve.

Method: Introduces a rule-based scheduling algorithm using a "laxity" measure to shift EV charging to underutilized periods, tested via multi-agent simulation.

Result: Overloads are eliminated with minimal user inconvenience, costing under DKK 6000 annually, cheaper than infrastructure upgrades.

Conclusion: The aggregator model is a viable, cost-effective solution for future distribution systems, preventing overloads without heavy infrastructure costs.

Abstract: The rapid electrification of transportation, driven by stringent
decarbonization targets and supportive policies, poses significant challenges
for distribution system operators (DSOs). When numerous electric vehicles (EVs)
charge concurrently, local transformers risk overloading - a problem that
current tariff-based strategies do not adequately address. This paper
introduces an aggregator-based coordination mechanism that shifts EV charging
from congested to underutilized periods using a rule-based scheduling
algorithm. Unlike conventional methods that depend on complex real-time pricing
signals or optimization-heavy solutions, the aggregator approach uses a simple
yet effective "laxity" measure to prioritize charging flexibility. To assess
technical and economic viability, a multi-agent simulation was developed to
replicate residential user behavior and DSO constraints under the use of a 400
kVA low-voltage transformer. The results indicate that overloads are completely
eliminated with minimal inconvenience to users, whose increased charging costs
are offset by the aggregator at an annual total of under DKK 6000 -
significantly lower than the cost of infrastructure reinforcement. This study
contributes by (i) quantifying the compensation needed to prevent large-scale
overloads, (ii) presenting a replicable, computationally feasible, rule-based
aggregator model for DSOs, and (iii) comparing aggregator solutions to costly
transformer upgrades, underscoring the aggregator's role as a viable tool for
future distribution systems.

</details>


### [309] [Towards a HIPAA Compliant Agentic AI System in Healthcare](https://arxiv.org/abs/2504.17669)
*Subash Neupane, Shaswata Mitra, Sudip Mittal, Shahram Rahimi*

Main category: cs.MA

TL;DR: A HIPAA-compliant Agentic AI framework for clinical workflows, integrating ABAC, PHI sanitization, and audit trails to ensure regulatory compliance.


<details>
  <summary>Details</summary>
Motivation: To address the need for regulatory compliance (e.g., HIPAA) in AI-driven clinical workflows handling sensitive healthcare data.

Method: Combines ABAC for granular access control, a hybrid PHI sanitization pipeline (regex + BERT), and immutable audit trails.

Result: A framework ensuring HIPAA compliance while autonomously processing PHI in clinical workflows.

Conclusion: The proposed framework enables secure, compliant adoption of Agentic AI in healthcare, balancing autonomy with regulatory adherence.

Abstract: Agentic AI systems powered by Large Language Models (LLMs) as their
foundational reasoning engine, are transforming clinical workflows such as
medical report generation and clinical summarization by autonomously analyzing
sensitive healthcare data and executing decisions with minimal human oversight.
However, their adoption demands strict compliance with regulatory frameworks
such as Health Insurance Portability and Accountability Act (HIPAA),
particularly when handling Protected Health Information (PHI). This
work-in-progress paper introduces a HIPAA-compliant Agentic AI framework that
enforces regulatory compliance through dynamic, context-aware policy
enforcement. Our framework integrates three core mechanisms: (1)
Attribute-Based Access Control (ABAC) for granular PHI governance, (2) a hybrid
PHI sanitization pipeline combining regex patterns and BERT-based model to
minimize leakage, and (3) immutable audit trails for compliance verification.

</details>


### [310] [MARFT: Multi-Agent Reinforcement Fine-Tuning](https://arxiv.org/abs/2504.16129)
*Junwei Liao, Muning Wen, Jun Wang, Weinan Zhang*

Main category: cs.MA

TL;DR: The paper introduces Multi-Agent Reinforcement Fine-Tuning (MARFT), a novel paradigm for fine-tuning LLM-based Multi-Agent Systems (LaMAS) using foundational RL techniques, addressing challenges unique to LaMAS.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in applying MARL to LaMAS by proposing MARFT, a tailored framework for enhancing agent intelligence in complex, collaborative tasks.

Method: The paper reviews RL's evolution to Reinforcement Fine-Tuning, contrasts MARL and MARFT, and presents a scalable MARFT framework with an open-source implementation.

Result: A robust and scalable MARFT framework is introduced, with practical implementation strategies and real-world application insights.

Conclusion: The work serves as a roadmap for advancing MARFT in agentic systems, offering a foundation for resilient and adaptive solutions.

Abstract: LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in
addressing complex, agentic tasks requiring multifaceted reasoning and
collaboration, from generating high-quality presentation slides to conducting
sophisticated scientific research. Meanwhile, RL has been widely recognized for
its effectiveness in enhancing agent intelligence, but limited research has
investigated the fine-tuning of LaMAS using foundational RL techniques.
Moreover, the direct application of MARL methodologies to LaMAS introduces
significant challenges, stemming from the unique characteristics and mechanisms
inherent to LaMAS. To address these challenges, this article presents a
comprehensive study of LLM-based MARL and proposes a novel paradigm termed
Multi-Agent Reinforcement Fine-Tuning (MARFT). We introduce a universal
algorithmic framework tailored for LaMAS, outlining the conceptual foundations,
key distinctions, and practical implementation strategies. We begin by
reviewing the evolution from RL to Reinforcement Fine-Tuning, setting the stage
for a parallel analysis in the multi-agent domain. In the context of LaMAS, we
elucidate critical differences between MARL and MARFT. These differences
motivate a transition toward a novel, LaMAS-oriented formulation of RFT.
Central to this work is the presentation of a robust and scalable MARFT
framework. We detail the core algorithm and provide a complete, open-source
implementation to facilitate adoption and further research. The latter sections
of the paper explore real-world application perspectives and opening challenges
in MARFT. By bridging theoretical underpinnings with practical methodologies,
this work aims to serve as a roadmap for researchers seeking to advance MARFT
toward resilient and adaptive solutions in agentic systems. Our implementation
of the proposed framework is publicly available at:
https://github.com/jwliao-ai/MARFT.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [311] [Multifaceted Evaluation of Audio-Visual Capability for MLLMs: Effectiveness, Efficiency, Generalizability and Robustness](https://arxiv.org/abs/2504.16936)
*Yusheng Zhao, Junyu Luo, Xiao Luo, Weizhi Zhang, Zhiping Xiao, Wei Ju, Philip S. Yu, Ming Zhang*

Main category: cs.MM

TL;DR: The paper evaluates the audio-visual capabilities of multi-modal large language models (MLLMs), focusing on effectiveness, efficiency, generalizability, and robustness. Findings show strong generalization but reliance on vision, susceptibility to adversarial attacks, and comparative robustness.


<details>
  <summary>Details</summary>
Motivation: There is a lack of comprehensive evaluation of MLLMs' audio-visual capabilities, especially under diverse scenarios like distribution shifts and adversarial attacks.

Method: A multifaceted evaluation of MLLMs is conducted, focusing on four dimensions: effectiveness, efficiency, generalizability, and robustness, through extensive experiments.

Result: MLLMs show strong zero-shot and few-shot generalization but rely heavily on vision, perform poorly with corrupted visual input, and are susceptible yet more robust to adversarial attacks than traditional models.

Conclusion: The study provides insights into MLLMs' audio-visual capabilities, identifying strengths and weaknesses, and guiding future research.

Abstract: Multi-modal large language models (MLLMs) have recently achieved great
success in processing and understanding information from diverse modalities
(e.g., text, audio, and visual signals). Despite their growing popularity,
there remains a lack of comprehensive evaluation measuring the audio-visual
capabilities of these models, especially in diverse scenarios (e.g.,
distribution shifts and adversarial attacks). In this paper, we present a
multifaceted evaluation of the audio-visual capability of MLLMs, focusing on
four key dimensions: effectiveness, efficiency, generalizability, and
robustness. Through extensive experiments, we find that MLLMs exhibit strong
zero-shot and few-shot generalization abilities, enabling them to achieve great
performance with limited data. However, their success relies heavily on the
vision modality, which impairs performance when visual input is corrupted or
missing. Additionally, while MLLMs are susceptible to adversarial samples, they
demonstrate greater robustness compared to traditional models. The experimental
results and our findings provide insights into the audio-visual capabilities of
MLLMs, highlighting areas for improvement and offering guidance for future
research.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [312] [Generating Localized Audible Zones Using a Single-Channel Parametric Loudspeaker](https://arxiv.org/abs/2504.17440)
*Tao Zhuang, Shaozhe Li, Feng Niu, Jia-Xin Zhong, Jing Lu*

Main category: eess.AS

TL;DR: Single-loudspeaker sound zone control (SZC) is achieved using a multi-carrier parametric loudspeaker (MCPL), challenging the need for massive multi-channel arrays.


<details>
  <summary>Details</summary>
Motivation: Traditional SZC requires multi-channel loudspeaker arrays, which are complex and resource-intensive. This work aims to simplify SZC systems without sacrificing performance.

Method: Distinct audio signals are modulated onto separate ultrasonic carriers, combined into a single signal, and emitted by a single-channel ultrasonic transducer. Nonlinear demodulation in air creates virtual multi-channel outputs.

Result: Simulations show the MCPL effectively enables SZC with a single loudspeaker, matching the performance of traditional multi-channel systems.

Conclusion: The MCPL offers a simpler, high-performance alternative to multi-loudspeaker SZC systems, opening new possibilities for practical applications.

Abstract: Advanced sound zone control (SZC) techniques typically rely on massive
multi-channel loudspeaker arrays to create high-contrast personal sound zones,
making single-loudspeaker SZC seem impossible. In this Letter, we challenge
this paradigm by introducing the multi-carrier parametric loudspeaker (MCPL),
which enables SZC using only a single loudspeaker. In our approach, distinct
audio signals are modulated onto separate ultrasonic carrier waves at different
frequencies and combined into a single composite signal. This signal is emitted
by a single-channel ultrasonic transducer, and through nonlinear demodulation
in air, the audio signals interact to virtually form multi-channel outputs.
This novel capability allows the application of existing SZC algorithms
originally designed for multi-channel loudspeaker arrays. Simulations validate
the effectiveness of our proposed single-channel MCPL, demonstrating its
potential as a promising alternative to traditional multi-loudspeaker systems
for achieving high-contrast SZC. Our work opens new avenues for simplifying SZC
systems without compromising performance.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [313] [Self-Controlled Diffusion for Denoising in Scientific Imaging](https://arxiv.org/abs/2504.16951)
*Nikolay Falaleev, Nikolai Orlov*

Main category: eess.IV

TL;DR: A novel diffusion model approach for denoising EBSD patterns using a two-stage UNet-based training process with adaptive feedback and quality prediction.


<details>
  <summary>Details</summary>
Motivation: To improve denoising of EBSD patterns by integrating quality assessment and adaptive control to avoid hallucinations.

Method: Two-stage training with a UNet architecture, auxiliary regression head for quality prediction, and adaptive feedback-driven denoising.

Result: Successful denoising of EBSD patterns with reduced risk of hallucinations, demonstrated on a custom dataset.

Conclusion: Diffusion models with adaptive feedback are effective for EBSD pattern denoising, offering fine control and quality assessment.

Abstract: This paper presents a novel approach for denoising Electron Backscatter
Diffraction (EBSD) patterns using diffusion models. We propose a two-stage
training process with a UNet-based architecture, incorporating an auxiliary
regression head to predict the quality of the experimental pattern and assess
the progress of the denoising process. The model uses an adaptive denoising
strategy, which integrates quality prediction and feedback-driven iterative
denoising process control. This adaptive feedback loop allows the model to
adjust its schedule, providing fine control over the denoising process.
Furthermore, our model can identify samples where no meaningful signal is
present, thereby reducing the risk of hallucinations. We demonstrate the
successful application of diffusion models to EBSD pattern denoising using a
custom-collected dataset of EBSD patterns, their corresponding master patterns,
and quality values.

</details>


### [314] [TVC: Tokenized Video Compression with Ultra-Low Bitrate](https://arxiv.org/abs/2504.16953)
*Lebin Zhou, Cihan Ruan, Nam Ling, Wei Wang, Wei Jiang*

Main category: eess.IV

TL;DR: Tokenized Video Compression (TVC) introduces a dual-stream framework for ultra-low bitrate video compression using discrete and continuous tokens, leveraging advanced tokenization and fusion techniques.


<details>
  <summary>Details</summary>
Motivation: Extending tokenized visual representations to video is challenging due to temporal dynamics and bitrate constraints, prompting the need for an efficient solution.

Method: TVC uses Cosmos video tokenizer for discrete and continuous token streams, strategic masking, lossless compression, and ControlNet fusion for reconstruction.

Result: TVC effectively compresses video at ultra-low bitrates while maintaining high perceptual quality and fidelity.

Conclusion: TVC addresses skepticism about tokenized video compression, enabling semantics-aware, token-native approaches.

Abstract: Tokenized visual representations have shown great promise in image
compression, yet their extension to video remains underexplored due to the
challenges posed by complex temporal dynamics and stringent bitrate
constraints. In this paper, we propose Tokenized Video Compression (TVC), the
first token-based dual-stream video compression framework designed to operate
effectively at ultra-low bitrates. TVC leverages the powerful Cosmos video
tokenizer to extract both discrete and continuous token streams. The discrete
tokens (i.e., code maps generated by FSQ) are partially masked using a
strategic masking scheme, then compressed losslessly with a discrete
checkerboard context model to reduce transmission overhead. The masked tokens
are reconstructed by a decoder-only transformer with spatiotemporal token
prediction. Meanwhile, the continuous tokens, produced via an autoencoder (AE),
are quantized and compressed using a continuous checkerboard context model,
providing complementary continuous information at ultra-low bitrate. At the
Decoder side, both streams are fused using ControlNet, with multi-scale
hierarchical integration to ensure high perceptual quality alongside strong
fidelity in reconstruction. This work mitigates the long-standing skepticism
about the practicality of tokenized video compression and opens up new avenues
for semantics-aware, token-native video compression.

</details>


### [315] [Iterative Collaboration Network Guided By Reconstruction Prior for Medical Image Super-Resolution](https://arxiv.org/abs/2504.16958)
*Xiaoyan Kui, Zexin Ji, Beiji Zou, Yang Li, Yulan Dai, Liming Chen, Pierre Vera, Su Ruan*

Main category: eess.IV

TL;DR: Proposes ICONet, an iterative collaboration network for medical image super-resolution, enhancing task interaction and feature completeness.


<details>
  <summary>Details</summary>
Motivation: Conventional single-task super-resolution lacks feature completeness, and multi-task learning often has insufficient interaction between tasks.

Method: ICONet integrates reconstruction and super-resolution branches with an SR-Rec fusion module, using RSCFL for efficient feature learning.

Result: Improves communication between tasks, enabling artifact-free reconstruction and prior-guided super-resolution.

Conclusion: ICONet effectively enhances medical image super-resolution through iterative collaboration and adaptive fusion.

Abstract: High-resolution medical images can provide more detailed information for
better diagnosis. Conventional medical image super-resolution relies on a
single task which first performs the extraction of the features and then
upscaling based on the features. The features extracted may not be complete for
super-resolution. Recent multi-task learning,including reconstruction and
super-resolution, is a good solution to obtain additional relevant information.
The interaction between the two tasks is often insufficient, which still leads
to incomplete and less relevant deep features. To address above limitations, we
propose an iterative collaboration network (ICONet) to improve communications
between tasks by progressively incorporating reconstruction prior to the
super-resolution learning procedure in an iterative collaboration way. It
consists of a reconstruction branch, a super-resolution branch, and a SR-Rec
fusion module. The reconstruction branch generates the artifact-free image as
prior, which is followed by a super-resolution branch for prior
knowledge-guided super-resolution. Unlike the widely-used convolutional neural
networks for extracting local features and Transformers with quadratic
computational complexity for modeling long-range dependencies, we develop a new
residual spatial-channel feature learning (RSCFL) module of two branches to
efficiently establish feature relationships in spatial and channel dimensions.
Moreover, the designed SR-Rec fusion module fuses the reconstruction prior and
super-resolution features with each other in an adaptive manner. Our ICONet is
built with multi-stage models to iteratively upscale the low-resolution images
using steps of 2x and simultaneously interact between two branches in
multi-stage supervisions.

</details>


### [316] [Diffusion Probabilistic Models for Compressive SAR Imaging](https://arxiv.org/abs/2504.17053)
*Odysseas Pappas, Perla Mayo, Andrew Austin, Alin Achim*

Main category: eess.IV

TL;DR: Using denoising diffusion models for compressive SAR image reconstruction improves efficiency and quality compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional iterative optimization for SAR image formation is computationally expensive.

Method: Employ denoising diffusion probabilistic models guided by initial poor reconstructions from sub-sampled data.

Result: Improved imaging quality compared to standard methods using full data.

Conclusion: Diffusion models offer potential performance gains in SAR image reconstruction.

Abstract: Compressed sensing Synthetic Aperture Radar (SAR) image formation, formulated
as an inverse problem and solved with traditional iterative optimization
methods can be very computationally expensive. We investigate the use of
denoising diffusion probabilistic models for compressive SAR image
reconstruction, where the diffusion model is guided by a poor initial
reconstruction from sub-sampled data obtained via standard imaging methods. We
present results on real SAR data and compare our compressively sampled
diffusion model reconstruction with standard image reconstruction methods
utilizing the full data set, demonstrating the potential performance gains in
imaging quality.

</details>


### [317] [Anatomy-constrained modelling of image-derived input functions in dynamic PET using multi-organ segmentation](https://arxiv.org/abs/2504.17114)
*Valentin Langer, Kartikay Tehlan, Thomas Wendler*

Main category: eess.IV

TL;DR: The paper proposes a multi-organ segmentation-based method to improve kinetic modeling in dynamic PET by integrating IDIFs from multiple vascular sources, showing reduced MSE in liver and lungs.


<details>
  <summary>Details</summary>
Motivation: Traditional IDIFs from the aorta overlook anatomical variations and complex vascular contributions, limiting accurate kinetic analysis.

Method: Uses high-resolution CT segmentations of organs (liver, lungs, kidneys, bladder) to integrate IDIFs from aorta, portal vein, pulmonary artery, and ureters.

Result: Achieved MSE reductions of 13.39% for liver and 10.42% for lungs in dynamic PET data from nine patients.

Conclusion: The approach enhances anatomical modeling and could make tracer kinetic modeling more clinically viable.

Abstract: Accurate kinetic analysis of [$^{18}$F]FDG distribution in dynamic positron
emission tomography (PET) requires anatomically constrained modelling of
image-derived input functions (IDIFs). Traditionally, IDIFs are obtained from
the aorta, neglecting anatomical variations and complex vascular contributions.
This study proposes a multi-organ segmentation-based approach that integrates
IDIFs from the aorta, portal vein, pulmonary artery, and ureters. Using
high-resolution CT segmentations of the liver, lungs, kidneys, and bladder, we
incorporate organ-specific blood supply sources to improve kinetic modelling.
Our method was evaluated on dynamic [$^{18}$F]FDG PET data from nine patients,
resulting in a mean squared error (MSE) reduction of $13.39\%$ for the liver
and $10.42\%$ for the lungs. These initial results highlight the potential of
multiple IDIFs in improving anatomical modelling and fully leveraging dynamic
PET imaging. This approach could facilitate the integration of tracer kinetic
modelling into clinical routine.

</details>


### [318] [Physiological neural representation for personalised tracer kinetic parameter estimation from dynamic PET](https://arxiv.org/abs/2504.17122)
*Kartikay Tehlan, Thomas Wendler*

Main category: eess.IV

TL;DR: The paper proposes a physiological neural representation using implicit neural representations (INRs) for efficient, high-resolution kinetic parameter estimation in dynamic PET imaging, outperforming traditional methods and deep neural networks.


<details>
  <summary>Details</summary>
Motivation: Conventional kinetic parameter estimation in PET imaging is computationally intensive and limited by resolution, while deep neural networks require large datasets and resources. The authors aim to overcome these limitations.

Method: The method uses INRs to learn continuous functions for personalized kinetic parameter estimation, integrating anatomical priors from a 3D CT foundation model to enhance robustness.

Result: The approach achieves superior spatial resolution, lower mean-squared error, and better anatomical consistency compared to state-of-the-art DNNs, especially in tumour and vascularized regions.

Conclusion: INRs offer a promising solution for data-efficient, personalized kinetic modelling, with potential applications in tumour characterization and prognostic assessment.

Abstract: Dynamic positron emission tomography (PET) with [$^{18}$F]FDG enables
non-invasive quantification of glucose metabolism through kinetic analysis,
often modelled by the two-tissue compartment model (TCKM). However, voxel-wise
kinetic parameter estimation using conventional methods is computationally
intensive and limited by spatial resolution. Deep neural networks (DNNs) offer
an alternative but require large training datasets and significant
computational resources. To address these limitations, we propose a
physiological neural representation based on implicit neural representations
(INRs) for personalized kinetic parameter estimation. INRs, which learn
continuous functions, allow for efficient, high-resolution parametric imaging
with reduced data requirements. Our method also integrates anatomical priors
from a 3D CT foundation model to enhance robustness and precision in kinetic
modelling. We evaluate our approach on an [$^{18}$F]FDG dynamic PET/CT dataset
and compare it to state-of-the-art DNNs. Results demonstrate superior spatial
resolution, lower mean-squared error, and improved anatomical consistency,
particularly in tumour and highly vascularized regions. Our findings highlight
the potential of INRs for personalized, data-efficient tracer kinetic
modelling, enabling applications in tumour characterization, segmentation, and
prognostic assessment.

</details>


### [319] [3D Deep-learning-based Segmentation of Human Skin Sweat Glands and Their 3D Morphological Response to Temperature Variations](https://arxiv.org/abs/2504.17255)
*Shaoyu Pei, Renxiong Wu, Hao Zheng, Lang Qin, Shuaichen Lin, Yuxing Gan, Wenjing Huang, Zhixuan Wang, Mohan Qin, Yong Liu, Guangming Ni*

Main category: eess.IV

TL;DR: A 3D transformer-based framework for non-invasive, real-time sweat gland segmentation from OCT data, enabling visualization and quantification of morphological changes under temperature variations.


<details>
  <summary>Details</summary>
Motivation: Current methods for observing sweat gland morphology are limited to 2D, in vitro, and destructive techniques, creating a need for advanced, non-invasive tools.

Method: A novel 3D transformer-based multi-object segmentation framework with sliding window, joint spatial-channel attention, and architectural heterogeneity between layers.

Result: Precise 3D segmentation of sweat glands from OCT data, allowing visualization and quantification of temperature-induced morphological changes.

Conclusion: The framework provides a benchmark for normal sweat gland morphology and a tool for studying variability and pathology, advancing dermatological research and clinical applications.

Abstract: Skin, the primary regulator of heat exchange, relies on sweat glands for
thermoregulation. Alterations in sweat gland morphology play a crucial role in
various pathological conditions and clinical diagnoses. Current methods for
observing sweat gland morphology are limited by their two-dimensional, in
vitro, and destructive nature, underscoring the urgent need for real-time,
non-invasive, quantifiable technologies. We proposed a novel three-dimensional
(3D) transformer-based multi-object segmentation framework, integrating a
sliding window approach, joint spatial-channel attention mechanism, and
architectural heterogeneity between shallow and deep layers. Our proposed
network enables precise 3D sweat gland segmentation from skin volume data
captured by optical coherence tomography (OCT). For the first time, subtle
variations of sweat gland 3D morphology in response to temperature changes,
have been visualized and quantified. Our approach establishes a benchmark for
normal sweat gland morphology and provides a real-time, non-invasive tool for
quantifying 3D structural parameters. This enables the study of individual
variability and pathological changes in sweat gland structure, advancing
dermatological research and clinical applications, including thermoregulation
and bromhidrosis treatment.

</details>


### [320] [A Spatially-Aware Multiple Instance Learning Framework for Digital Pathology](https://arxiv.org/abs/2504.17379)
*Hassan Keshvarikhojasteh, Mihail Tifrea, Sibylle Hess, Josien P. W. Pluim, Mitko Veta*

Main category: eess.IV

TL;DR: GABMIL enhances ABMIL by integrating interaction-aware representations, improving performance without added computational cost.


<details>
  <summary>Details</summary>
Motivation: To address the lack of spatial interactions in ABMIL and compare it to TransMIL's approach.

Method: Enhances ABMIL with interaction-aware representations (GABMIL) to capture inter-instance dependencies.

Result: GABMIL outperforms ABMIL by 7% in AUPRC and 5% in Kappa score on tumor subtyping tasks.

Conclusion: Incorporating patch interactions in MIL frameworks is crucial for performance gains.

Abstract: Multiple instance learning (MIL) is a promising approach for weakly
supervised classification in pathology using whole slide images (WSIs).
However, conventional MIL methods such as Attention-Based Deep Multiple
Instance Learning (ABMIL) typically disregard spatial interactions among
patches that are crucial to pathological diagnosis. Recent advancements, such
as Transformer based MIL (TransMIL), have incorporated spatial context and
inter-patch relationships. However, it remains unclear whether explicitly
modeling patch relationships yields similar performance gains in ABMIL, which
relies solely on Multi-Layer Perceptrons (MLPs). In contrast, TransMIL employs
Transformer-based layers, introducing a fundamental architectural shift at the
cost of substantially increased computational complexity. In this work, we
enhance the ABMIL framework by integrating interaction-aware representations to
address this question. Our proposed model, Global ABMIL (GABMIL), explicitly
captures inter-instance dependencies while preserving computational efficiency.
Experimental results on two publicly available datasets for tumor subtyping in
breast and lung cancers demonstrate that GABMIL achieves up to a 7 percentage
point improvement in AUPRC and a 5 percentage point increase in the Kappa score
over ABMIL, with minimal or no additional computational overhead. These
findings underscore the importance of incorporating patch interactions within
MIL frameworks.

</details>


### [321] [Beyond Labels: Zero-Shot Diabetic Foot Ulcer Wound Segmentation with Self-attention Diffusion Models and the Potential for Text-Guided Customization](https://arxiv.org/abs/2504.17628)
*Abderrachid Hamrani, Daniela Leizaola, Renato Sousa, Jose P. Ponce, Stanley Mathis, David G. Armstrong, Anuradha Godavarty*

Main category: eess.IV

TL;DR: ADZUS is a zero-shot, text-guided diffusion model for diabetic foot ulcer segmentation, outperforming supervised models without labeled data.


<details>
  <summary>Details</summary>
Motivation: Diabetic foot ulcers require precise wound assessment; ADZUS addresses the need for flexible, annotation-free segmentation.

Method: ADZUS uses zero-shot learning and text prompts for dynamic wound segmentation, avoiding labeled training data.

Result: ADZUS achieves 86.68% IoU and 94.69% precision, surpassing supervised models like FUSegNet (45% DSC).

Conclusion: ADZUS offers scalable, adaptable wound segmentation but needs optimization for computational cost and fine-tuning.

Abstract: Diabetic foot ulcers (DFUs) pose a significant challenge in healthcare,
requiring precise and efficient wound assessment to enhance patient outcomes.
This study introduces the Attention Diffusion Zero-shot Unsupervised System
(ADZUS), a novel text-guided diffusion model that performs wound segmentation
without relying on labeled training data. Unlike conventional deep learning
models, which require extensive annotation, ADZUS leverages zero-shot learning
to dynamically adapt segmentation based on descriptive prompts, offering
enhanced flexibility and adaptability in clinical applications. Experimental
evaluations demonstrate that ADZUS surpasses traditional and state-of-the-art
segmentation models, achieving an IoU of 86.68\% and the highest precision of
94.69\% on the chronic wound dataset, outperforming supervised approaches such
as FUSegNet. Further validation on a custom-curated DFU dataset reinforces its
robustness, with ADZUS achieving a median DSC of 75\%, significantly surpassing
FUSegNet's 45\%. The model's text-guided segmentation capability enables
real-time customization of segmentation outputs, allowing targeted analysis of
wound characteristics based on clinical descriptions. Despite its competitive
performance, the computational cost of diffusion-based inference and the need
for potential fine-tuning remain areas for future improvement. ADZUS represents
a transformative step in wound segmentation, providing a scalable, efficient,
and adaptable AI-driven solution for medical imaging.

</details>


### [322] [Self-Supervised Noise Adaptive MRI Denoising via Repetition to Repetition (Rep2Rep) Learning](https://arxiv.org/abs/2504.17698)
*Nikola Janjušević, Jingjia Chen, Luke Ginocchio, Mary Bruno, Yuhui Huang, Yao Wang, Hersh Chandarana, Li Feng*

Main category: eess.IV

TL;DR: Rep2Rep is a self-supervised noise-adaptive image denoising framework for low-field MRI, outperforming MC-SURE and matching supervised learning in quality.


<details>
  <summary>Details</summary>
Motivation: To improve denoising for low-field MRI without requiring ground-truth data, leveraging multi-repetition acquisitions.

Method: Extends Noise2Noise by training on repeated MRI acquisitions, incorporating noise-adaptive training for generalization across noise levels.

Result: Outperforms MC-SURE on synthetic and 0.55T MRI data, matches supervised learning, and is preferred by radiologists over noisy images.

Conclusion: Rep2Rep is effective for low-field MRI denoising, offering noise-adaptivity and practical implementation without clean reference images.

Abstract: Purpose: This work proposes a novel self-supervised noise-adaptive image
denoising framework, called Repetition to Repetition (Rep2Rep) learning, for
low-field (<1T) MRI applications. Methods: Rep2Rep learning extends the
Noise2Noise framework by training a neural network on two repeated MRI
acquisitions, using one repetition as input and another as target, without
requiring ground-truth data. It incorporates noise-adaptive training, enabling
denoising generalization across varying noise levels and flexible inference
with any number of repetitions. Performance was evaluated on both synthetic
noisy brain MRI and 0.55T prostate MRI data, and compared against supervised
learning and Monte Carlo Stein's Unbiased Risk Estimator (MC-SURE). Results:
Rep2Rep learning outperforms MC-SURE on both synthetic and 0.55T MRI datasets.
On synthetic brain data, it achieved denoising quality comparable to supervised
learning and surpassed MC-SURE, particularly in preserving structural details
and reducing residual noise. On the 0.55T prostate MRI dataset, a reader study
showed radiologists preferred Rep2Rep-denoised 2-average images over 8-average
noisy images. Rep2Rep demonstrated robustness to noise-level discrepancies
between training and inference, supporting its practical implementation.
Conclusion: Rep2Rep learning offers an effective self-supervised denoising for
low-field MRI by leveraging routinely acquired multi-repetition data. Its
noise-adaptivity enables generalization to different SNR regimes without clean
reference images. This makes Rep2Rep learning a promising tool for improving
image quality and scan efficiency in low-field MRI.

</details>


### [323] [Towards Spatially-Lucid AI Classification in Non-Euclidean Space: An Application for MxIF Oncology Data](https://arxiv.org/abs/2402.14974)
*Majid Farhadloo, Arun Sharma, Jayant Gupta, Alexey Leontovich, Svetomir N. Markovic, Shashi Shekhar*

Main category: eess.IV

TL;DR: A spatial ensemble framework is proposed for classifying multi-category point sets, outperforming baselines in accuracy.


<details>
  <summary>Details</summary>
Motivation: The need for a classifier to distinguish point set arrangements, especially in oncology for immune-tumor analysis, drives this work. Challenges include spatial variability and interpretability.

Method: The framework uses weighted-distance learning rate and spatial domain adaptation across place-types for classification.

Result: Experiments on real-world datasets (e.g., MxIF oncology data) show higher accuracy than baseline methods.

Conclusion: The proposed spatial ensemble framework effectively addresses spatial variability and improves classification accuracy.

Abstract: Given multi-category point sets from different place-types, our goal is to
develop a spatially-lucid classifier that can distinguish between two classes
based on the arrangements of their points. This problem is important for many
applications, such as oncology, for analyzing immune-tumor relationships and
designing new immunotherapies. It is challenging due to spatial variability and
interpretability needs. Previously proposed techniques require dense training
data or have limited ability to handle significant spatial variability within a
single place-type. Most importantly, these deep neural network (DNN) approaches
are not designed to work in non-Euclidean space, particularly point sets.
Existing non-Euclidean DNN methods are limited to one-size-fits-all approaches.
We explore a spatial ensemble framework that explicitly uses different training
strategies, including weighted-distance learning rate and spatial domain
adaptation, on various place-types for spatially-lucid classification.
Experimental results on real-world datasets (e.g., MxIF oncology data) show
that the proposed framework provides higher prediction accuracy than baseline
methods.

</details>


### [324] [Continuous and complete liver vessel segmentation with graph-attention guided diffusion](https://arxiv.org/abs/2411.00617)
*Xiaotong Zhang, Alexander Broersen, Gonnie CM van Erp, Silvia L. Pintea, Jouke Dijkstra*

Main category: eess.IV

TL;DR: A diffusion-based segmentation model improves liver vessel segmentation by integrating connectivity and completeness, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of connectivity and completeness in liver vessel segmentation, especially for small vessels, which current methods fail to explicitly tackle.

Method: Uses a diffusion model with a graph-attention module for vessel geometry and multi-scale attention for small vessels.

Result: Outperforms five state-of-the-art methods on 3D-ircadb-01 and LiVS datasets.

Conclusion: The proposed diffusion-based model effectively improves liver vessel segmentation by focusing on connectivity and small vessel detection.

Abstract: Improving connectivity and completeness are the most challenging aspects of
liver vessel segmentation, especially for small vessels. These challenges
require both learning the continuous vessel geometry and focusing on small
vessel detection. However, current methods do not explicitly address these two
aspects and cannot generalize well when constrained by inconsistent
annotations. Here, we take advantage of the generalization of the diffusion
model and explicitly integrate connectivity and completeness in our
diffusion-based segmentation model. Specifically, we use a graph-attention
module that adds knowledge about vessel geometry. Additionally, we perform the
graph-attention at multiple-scales, thus focusing on small liver vessels. Our
method outperforms five state-of-the-art medical segmentation methods on two
public datasets: 3D-ircadb-01 and LiVS.

</details>


### [325] [OmniMamba4D: Spatio-temporal Mamba for longitudinal CT lesion segmentation](https://arxiv.org/abs/2504.09655)
*Justin Namuk Kim, Yiqiao Liu, Rajath Soans, Keith Persson, Sarah Halek, Michal Tomaszewski, Jianda Yuan, Gregory Goldmacher, Antong Chen*

Main category: eess.IV

TL;DR: OmniMamba4D is a 4D segmentation model for longitudinal CT scans, capturing spatio-temporal features for better lesion progression analysis.


<details>
  <summary>Details</summary>
Motivation: Existing 3D segmentation models lack temporal information, limiting their ability to monitor tumor progression over time.

Method: OmniMamba4D uses a spatio-temporal tetra-orientated Mamba block to process 4D CT data, combining spatial and temporal features.

Result: Achieves a Dice score of 0.682, comparable to SOTA models, with computational efficiency and improved detection of disappeared lesions.

Conclusion: OmniMamba4D provides a novel framework for leveraging spatio-temporal data in longitudinal CT lesion segmentation.

Abstract: Accurate segmentation of longitudinal CT scans is important for monitoring
tumor progression and evaluating treatment responses. However, existing 3D
segmentation models solely focus on spatial information. To address this gap,
we propose OmniMamba4D, a novel segmentation model designed for 4D medical
images (3D images over time). OmniMamba4D utilizes a spatio-temporal
tetra-orientated Mamba block to effectively capture both spatial and temporal
features. Unlike traditional 3D models, which analyze single-time points,
OmniMamba4D processes 4D CT data, providing comprehensive spatio-temporal
information on lesion progression. Evaluated on an internal dataset comprising
of 3,252 CT scans, OmniMamba4D achieves a competitive Dice score of 0.682,
comparable to state-of-the-arts (SOTA) models, while maintaining computational
efficiency and better detecting disappeared lesions. This work demonstrates a
new framework to leverage spatio-temporal information for longitudinal CT
lesion segmentation.

</details>


### [326] [Putting the Segment Anything Model to the Test with 3D Knee MRI - A Comparison with State-of-the-Art Performance](https://arxiv.org/abs/2504.13340)
*Oliver Mills, Philip Conaghan, Nishant Ravikumar, Samuel Relton*

Main category: eess.IV

TL;DR: SAM was tested for meniscus segmentation in knee MRI but underperformed compared to 3D U-Net, despite fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Accurate automated meniscus segmentation could improve early detection of abnormalities and OA understanding.

Method: SAM and 3D U-Net were trained for meniscus segmentation in 3D knee MRI, with SAM fine-tuned in two ways.

Result: SAM achieved comparable Dice scores to 3D U-Net only with end-to-end fine-tuning but was inferior in Hausdorff Distance.

Conclusion: SAM may not be suitable for fine 3D medical segmentation tasks like menisci due to its limitations in handling low-contrast structures.

Abstract: Menisci are cartilaginous tissue found within the knee that contribute to
joint lubrication and weight dispersal. Damage to menisci can lead to onset and
progression of knee osteoarthritis (OA), a condition that is a leading cause of
disability, and for which there are few effective therapies. Accurate automated
segmentation of menisci would allow for earlier detection and treatment of
meniscal abnormalities, as well as shedding more light on the role the menisci
play in OA pathogenesis. Focus in this area has mainly used variants of
convolutional networks, but there has been no attempt to utilise recent large
vision transformer segmentation models. The Segment Anything Model (SAM) is a
so-called foundation segmentation model, which has been found useful across a
range of different tasks due to the large volume of data used for training the
model. In this study, SAM was adapted to perform fully-automated segmentation
of menisci from 3D knee magnetic resonance images. A 3D U-Net was also trained
as a baseline. It was found that, when fine-tuning only the decoder, SAM was
unable to compete with 3D U-Net, achieving a Dice score of $0.81\pm0.03$,
compared to $0.87\pm0.03$, on a held-out test set. When fine-tuning SAM
end-to-end, a Dice score of $0.87\pm0.03$ was achieved. The performance of both
the end-to-end trained SAM configuration and the 3D U-Net were comparable to
the winning Dice score ($0.88\pm0.03$) in the IWOAI Knee MRI Segmentation
Challenge 2019. Performance in terms of the Hausdorff Distance showed that both
configurations of SAM were inferior to 3D U-Net in matching the meniscus
morphology. Results demonstrated that, despite its generalisability, SAM was
unable to outperform a basic 3D U-Net in meniscus segmentation, and may not be
suitable for similar 3D medical image segmentation tasks also involving fine
anatomical structures with low contrast and poorly-defined boundaries.

</details>
