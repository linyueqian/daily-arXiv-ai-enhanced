<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 52]
- [cs.CV](#cs.CV) [Total: 97]
- [cs.AI](#cs.AI) [Total: 23]
- [cs.SD](#cs.SD) [Total: 5]
- [cs.LG](#cs.LG) [Total: 95]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.MM](#cs.MM) [Total: 2]
- [eess.AS](#eess.AS) [Total: 2]
- [eess.IV](#eess.IV) [Total: 17]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [KG-HTC: Integrating Knowledge Graphs into LLMs for Effective Zero-shot Hierarchical Text Classification](https://arxiv.org/pdf/2505.05583)
*Qianbo Zang, Christophe Zgrzendek, Igor Tchappi, Afshin Khadangi, Johannes Sedlmeir*

Main category: cs.CL

TL;DR: KG-HTC integrates knowledge graphs with LLMs for zero-shot hierarchical text classification, outperforming baselines by leveraging structured semantic context.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in HTC like lack of annotated data, large label spaces, and long-tail distributions.

Method: Uses Retrieval-Augmented Generation (RAG) to retrieve relevant subgraphs from knowledge graphs, enhancing LLMs for label semantics.

Result: Significantly outperforms baselines in zero-shot settings, especially at deeper hierarchy levels.

Conclusion: Incorporating structured knowledge into LLMs effectively tackles HTC challenges in large label spaces and long-tailed distributions.

Abstract: Hierarchical Text Classification (HTC) involves assigning documents to labels
organized within a taxonomy. Most previous research on HTC has focused on
supervised methods. However, in real-world scenarios, employing supervised HTC
can be challenging due to a lack of annotated data. Moreover, HTC often faces
issues with large label spaces and long-tail distributions. In this work, we
present Knowledge Graphs for zero-shot Hierarchical Text Classification
(KG-HTC), which aims to address these challenges of HTC in applications by
integrating knowledge graphs with Large Language Models (LLMs) to provide
structured semantic context during classification. Our method retrieves
relevant subgraphs from knowledge graphs related to the input text using a
Retrieval-Augmented Generation (RAG) approach. Our KG-HTC can enhance LLMs to
understand label semantics at various hierarchy levels. We evaluate KG-HTC on
three open-source HTC datasets: WoS, DBpedia, and Amazon. Our experimental
results show that KG-HTC significantly outperforms three baselines in the
strict zero-shot setting, particularly achieving substantial improvements at
deeper levels of the hierarchy. This evaluation demonstrates the effectiveness
of incorporating structured knowledge into LLMs to address HTC's challenges in
large label spaces and long-tailed label distributions. Our code is available
at: https://github.com/QianboZang/KG-HTC.

</details>


### [2] [Privacy-Preserving Transformers: SwiftKey's Differential Privacy Implementation](https://arxiv.org/pdf/2505.05648)
*Abdelrahman Abouelenin, Mohamed Abdelrehim, Raffy Fahim, Amr Hendy, Mohamed Afify*

Main category: cs.CL

TL;DR: A transformer model is trained with differential privacy for SwiftKey, balancing size, speed, and accuracy, outperforming GRU in next-word prediction.


<details>
  <summary>Details</summary>
Motivation: To improve language modeling in SwiftKey while ensuring privacy and efficiency.

Method: Scaling down GPT2 architecture, two-stage training (general data seed + DP fine-tuning on typing data), and ONNX integration.

Result: Small, consistent gains in next-word prediction and accuracy with manageable memory and speed trade-offs.

Conclusion: The approach successfully integrates privacy and performance for SwiftKey's language model.

Abstract: In this paper we train a transformer using differential privacy (DP) for
language modeling in SwiftKey. We run multiple experiments to balance the
trade-off between the model size, run-time speed and accuracy. We show that we
get small and consistent gains in the next-word-prediction and accuracy with
graceful increase in memory and speed compared to the production GRU. This is
obtained by scaling down a GPT2 architecture to fit the required size and a two
stage training process that builds a seed model on general data and DP
finetunes it on typing data. The transformer is integrated using ONNX offering
both flexibility and efficiency.

</details>


### [3] [Exploration of COVID-19 Discourse on Twitter: American Politician Edition](https://arxiv.org/pdf/2505.05687)
*Cindy Kim, Daniela Puchall, Jiangyi Liang, Jiwon Kim*

Main category: cs.CL

TL;DR: The paper analyzes partisan differences in COVID-19 responses via tweets from American political figures, using NLP techniques to classify political stances.


<details>
  <summary>Details</summary>
Motivation: To understand how the pandemic polarized political discourse and identify partisan differences in handling the crisis.

Method: Used bag-of-words, bigram, and TF-IDF models to analyze keywords, topics, and sentiments from tweets of Republican and Democratic figures.

Result: Democrats focused on casualties and medical advice, while Republicans emphasized political updates and monitoring the virus.

Conclusion: Proposes a systematic approach to classify tweets' political stances based on COVID-19 terms using classification algorithms.

Abstract: The advent of the COVID-19 pandemic has undoubtedly affected the political
scene worldwide and the introduction of new terminology and public opinions
regarding the virus has further polarized partisan stances. Using a collection
of tweets gathered from leading American political figures online (Republican
and Democratic), we explored the partisan differences in approach, response,
and attitude towards handling the international crisis. Implementation of the
bag-of-words, bigram, and TF-IDF models was used to identify and analyze
keywords, topics, and overall sentiments from each party. Results suggest that
Democrats are more concerned with the casualties of the pandemic, and give more
medical precautions and recommendations to the public whereas Republicans are
more invested in political responsibilities such as keeping the public updated
through media and carefully watching the progress of the virus. We propose a
systematic approach to predict and distinguish a tweet's political stance (left
or right leaning) based on its COVID-19 related terms using different
classification algorithms on different language models.

</details>


### [4] [Assessing Robustness to Spurious Correlations in Post-Training Language Models](https://arxiv.org/pdf/2505.05704)
*Julia Shuieh, Prasann Singhal, Apaar Shanker, John Heyer, George Pu, Samuel Denton*

Main category: cs.CL

TL;DR: The paper evaluates SFT, DPO, and KTO for aligning LLMs under spurious correlations, finding task-dependent performance.


<details>
  <summary>Details</summary>
Motivation: To assess how spurious correlations in training data affect LLM alignment methods and identify robust strategies.

Method: Systematic evaluation of SFT, DPO, and KTO across synthetic tasks with varying spuriousness and artifact types.

Result: Preference-based methods (DPO/KTO) are robust in math tasks, while SFT excels in context-intensive tasks.

Conclusion: No single method universally outperforms; choice depends on task type and spurious correlation nature.

Abstract: Supervised and preference-based fine-tuning techniques have become popular
for aligning large language models (LLMs) with user intent and correctness
criteria. However, real-world training data often exhibits spurious
correlations -- arising from biases, dataset artifacts, or other "shortcut"
features -- that can compromise a model's performance or generalization. In
this paper, we systematically evaluate three post-training algorithms --
Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and KTO
(Kahneman-Tversky Optimization) -- across a diverse set of synthetic tasks and
spuriousness conditions. Our tasks span mathematical reasoning, constrained
instruction-following, and document-grounded question answering. We vary the
degree of spurious correlation (10% vs. 90%) and investigate two forms of
artifacts: "Feature Ambiguity" and "Distributional Narrowness." Our results
show that the models often but not always degrade under higher spuriousness.
The preference-based methods (DPO/KTO) can demonstrate relative robustness in
mathematical reasoning tasks. By contrast, SFT maintains stronger performance
in complex, context-intensive tasks. These findings highlight that no single
post-training strategy universally outperforms in all scenarios; the best
choice depends on the type of target task and the nature of spurious
correlations.

</details>


### [5] [TopicVD: A Topic-Based Dataset of Video-Guided Multimodal Machine Translation for Documentaries](https://arxiv.org/pdf/2505.05714)
*Jinze Lv, Jian Chen, Zi Long, Xianghua Fu, Yin Chen*

Main category: cs.CL

TL;DR: The paper introduces TopicVD, a topic-based dataset for video-supported multimodal machine translation (MMT) of documentaries, addressing the lack of diverse video data in existing datasets. It proposes a cross-modal bidirectional attention MMT model and demonstrates improved translation performance with visual information, though domain adaptation remains a challenge.


<details>
  <summary>Details</summary>
Motivation: Existing MMT datasets lack extensive video data, limiting their applicability to real-world tasks like documentary translation. TopicVD aims to fill this gap by providing a diverse, topic-categorized dataset.

Method: The study collects video-subtitle pairs from documentaries, categorizes them into eight topics, and preserves contextual information. A cross-modal bidirectional attention MMT model is proposed to capture shared semantics between text and video.

Result: Experiments show that visual information improves NMT performance in documentary translation, but the MMT model struggles with out-of-domain scenarios. Global context also enhances translation performance.

Conclusion: TopicVD advances MMT research by addressing dataset limitations and demonstrating the benefits of visual and contextual information. Effective domain adaptation methods are needed to handle out-of-domain challenges.

Abstract: Most existing multimodal machine translation (MMT) datasets are predominantly
composed of static images or short video clips, lacking extensive video data
across diverse domains and topics. As a result, they fail to meet the demands
of real-world MMT tasks, such as documentary translation. In this study, we
developed TopicVD, a topic-based dataset for video-supported multimodal machine
translation of documentaries, aiming to advance research in this field. We
collected video-subtitle pairs from documentaries and categorized them into
eight topics, such as economy and nature, to facilitate research on domain
adaptation in video-guided MMT. Additionally, we preserved their contextual
information to support research on leveraging the global context of
documentaries in video-guided MMT. To better capture the shared semantics
between text and video, we propose an MMT model based on a cross-modal
bidirectional attention module. Extensive experiments on the TopicVD dataset
demonstrate that visual information consistently improves the performance of
the NMT model in documentary translation. However, the MMT model's performance
significantly declines in out-of-domain scenarios, highlighting the need for
effective domain adaptation methods. Additionally, experiments demonstrate that
global context can effectively improve translation performance. % Dataset and
our implementations are available at https://github.com/JinzeLv/TopicVD

</details>


### [6] [Insertion Language Models: Sequence Generation with Arbitrary-Position Insertions](https://arxiv.org/pdf/2505.05755)
*Dhruvesh Patel, Aishwarya Sahoo, Avinash Amballa, Tahira Naseem, Tim G. J. Rudner, Andrew McCallum*

Main category: cs.CL

TL;DR: Insertion Language Models (ILMs) outperform Autoregressive Models (ARMs) and Masked Diffusion Models (MDMs) in planning tasks and offer flexibility in arbitrary-length text infilling.


<details>
  <summary>Details</summary>
Motivation: ARMs and MDMs have limitations in handling sequences with sophisticated constraints or non-sequential dependencies. ILMs aim to address these gaps by allowing token insertion at arbitrary positions.

Method: ILMs insert tokens one at a time at any position, using a tailored network parameterization and a denoising objective for training.

Result: ILMs outperform ARMs and MDMs in planning tasks and match ARMs in text generation while offering more flexibility than MDMs.

Conclusion: ILMs provide a flexible and effective alternative to ARMs and MDMs for sequence generation, especially for tasks requiring non-sequential dependencies.

Abstract: Autoregressive models (ARMs), which predict subsequent tokens one-by-one
``from left to right,'' have achieved significant success across a wide range
of sequence generation tasks. However, they struggle to accurately represent
sequences that require satisfying sophisticated constraints or whose sequential
dependencies are better addressed by out-of-order generation. Masked Diffusion
Models (MDMs) address some of these limitations, but the process of unmasking
multiple tokens simultaneously in MDMs can introduce incoherences, and MDMs
cannot handle arbitrary infilling constraints when the number of tokens to be
filled in is not known in advance. In this work, we introduce Insertion
Language Models (ILMs), which learn to insert tokens at arbitrary positions in
a sequence -- that is, they select jointly both the position and the vocabulary
element to be inserted. By inserting tokens one at a time, ILMs can represent
strong dependencies between tokens, and their ability to generate sequences in
arbitrary order allows them to accurately model sequences where token
dependencies do not follow a left-to-right sequential structure. To train ILMs,
we propose a tailored network parameterization and use a simple denoising
objective. Our empirical evaluation demonstrates that ILMs outperform both ARMs
and MDMs on common planning tasks. Furthermore, we show that ILMs outperform
MDMs and perform on par with ARMs in an unconditional text generation task
while offering greater flexibility than MDMs in arbitrary-length text
infilling.

</details>


### [7] [Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study](https://arxiv.org/pdf/2505.06149)
*Faeze Ghorbanpour, Daryna Dementieva, Alexander Fraser*

Main category: cs.CL

TL;DR: The paper evaluates multilingual LLMs for hate speech detection, finding that zero-shot and few-shot prompting underperform fine-tuned models but generalize better, with prompt design being crucial.


<details>
  <summary>Details</summary>
Motivation: Existing hate speech detection approaches often ignore linguistic diversity, and the effectiveness of multilingual LLMs in this task is underexplored.

Method: The study tests LLM prompting (zero-shot and few-shot) across eight non-English languages, comparing it to fine-tuned encoder models and analyzing prompt design impact.

Result: Zero-shot and few-shot prompting lag behind fine-tuned models in real-world evaluations but generalize better in functional tests. Prompt design significantly affects performance.

Conclusion: Customized prompting techniques per language are essential for optimizing LLM-based hate speech detection, though fine-tuned models remain superior in most real-world cases.

Abstract: Despite growing interest in automated hate speech detection, most existing
approaches overlook the linguistic diversity of online content. Multilingual
instruction-tuned large language models such as LLaMA, Aya, Qwen, and BloomZ
offer promising capabilities across languages, but their effectiveness in
identifying hate speech through zero-shot and few-shot prompting remains
underexplored. This work evaluates LLM prompting-based detection across eight
non-English languages, utilizing several prompting techniques and comparing
them to fine-tuned encoder models. We show that while zero-shot and few-shot
prompting lag behind fine-tuned encoder models on most of the real-world
evaluation sets, they achieve better generalization on functional tests for
hate speech detection. Our study also reveals that prompt design plays a
critical role, with each language often requiring customized prompting
techniques to maximize performance.

</details>


### [8] [Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM](https://arxiv.org/pdf/2505.05772)
*Zehao Fan, Garrett Gagnon, Zhenyu Liu, Liu Liu*

Main category: cs.CL

TL;DR: STARC is a sparsity-optimized data mapping scheme for efficient LLM decoding on PIM architectures, reducing latency and energy while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Transformer-based models face memory bottlenecks due to KV cache sparsity and irregular access patterns, which current PIM designs struggle to handle.

Method: STARC clusters KV pairs by semantic similarity, maps them to contiguous memory regions, and uses precomputed centroids for selective attention.

Result: STARC reduces attention-layer latency by 19%--31% and energy by 19%--27%, with further improvements under KV cache constraints.

Conclusion: STARC enables efficient, hardware-friendly long-context LLM inference on PIM architectures without compromising accuracy.

Abstract: Transformer-based models are the foundation of modern machine learning, but
their execution, particularly during autoregressive decoding in large language
models (LLMs), places significant pressure on memory systems due to frequent
memory accesses and growing key-value (KV) caches. This creates a bottleneck in
memory bandwidth, especially as context lengths increase. Processing-in-memory
(PIM) architectures are a promising solution, offering high internal bandwidth
and compute parallelism near memory. However, current PIM designs are primarily
optimized for dense attention and struggle with the dynamic, irregular access
patterns introduced by modern KV cache sparsity techniques. Consequently, they
suffer from workload imbalance, reducing throughput and resource utilization.
In this work, we propose STARC, a novel sparsity-optimized data mapping scheme
tailored specifically for efficient LLM decoding on PIM architectures. STARC
clusters KV pairs by semantic similarity and maps them to contiguous memory
regions aligned with PIM bank structures. During decoding, queries retrieve
relevant tokens at cluster granularity by matching against precomputed
centroids, enabling selective attention and parallel processing without
frequent reclustering or data movement overhead. Experiments on the HBM-PIM
system show that, compared to common token-wise sparsity methods, STARC reduces
attention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a
KV cache budget of 1024, it achieves up to 54%--74% latency reduction and
45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC
maintains model accuracy comparable to state-of-the-art sparse attention
methods, demonstrating its effectiveness in enabling efficient and
hardware-friendly long-context LLM inference on PIM architectures.

</details>


### [9] [Tell Me Who Your Students Are: GPT Can Generate Valid Multiple-Choice Questions When Students' (Mis)Understanding Is Hinted](https://arxiv.org/pdf/2505.05815)
*Machi Shimmei, Masaki Uto, Yuichiroh Matsubayashi, Kentaro Inui, Aditi Mallavarapu, Noboru Matsuda*

Main category: cs.CL

TL;DR: AnaQuest is a prompting technique for generating MCQs using a pre-trained language model, integrating formative and summative assessments. It outperforms ChatGPT in resembling human-crafted questions.


<details>
  <summary>Details</summary>
Motivation: To develop a method for generating high-quality MCQs that mimic human-crafted items, improving automated assessment tools.

Method: AnaQuest generates MCQs from student responses, using formative (open-ended) and summative (MCQ) phases, and evaluates validity via IRT.

Result: AnaQuest-generated MCQs were rated as valid as human-crafted ones and outperformed ChatGPT in difficulty and discrimination metrics.

Conclusion: AnaQuest is effective for MCQ generation, closely matching human quality, and superior to baseline AI methods.

Abstract: The primary goal of this study is to develop and evaluate an innovative
prompting technique, AnaQuest, for generating multiple-choice questions (MCQs)
using a pre-trained large language model. In AnaQuest, the choice items are
sentence-level assertions about complex concepts. The technique integrates
formative and summative assessments. In the formative phase, students answer
open-ended questions for target concepts in free text. For summative
assessment, AnaQuest analyzes these responses to generate both correct and
incorrect assertions. To evaluate the validity of the generated MCQs, Item
Response Theory (IRT) was applied to compare item characteristics between MCQs
generated by AnaQuest, a baseline ChatGPT prompt, and human-crafted items. An
empirical study found that expert instructors rated MCQs generated by both AI
models to be as valid as those created by human instructors. However, IRT-based
analysis revealed that AnaQuest-generated questions - particularly those with
incorrect assertions (foils) - more closely resembled human-crafted items in
terms of difficulty and discrimination than those produced by ChatGPT.

</details>


### [10] [Symbol-based entity marker highlighting for enhanced text mining in materials science with generative AI](https://arxiv.org/pdf/2505.05864)
*Junhyeong Lee, Jong Min Yuk, Chan-Woo Lee*

Main category: cs.CL

TL;DR: A hybrid text-mining framework combines multi-step and direct methods to convert unstructured scientific text into structured data, improving entity and relation recognition performance.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing methods (multi-step and direct) in extracting structured data from scientific literature, aiming for better performance and data quality.

Method: Proposes a hybrid framework: transforms raw text into entity-recognized text, then into structured form, enhanced by an entity marker technique for better entity recognition.

Result: Outperforms previous methods on benchmark datasets (MatScholar, SOFC, SOFC slot NER) with up to 58% improvement in entity-level F1 and 83% in relation-level F1.

Conclusion: The hybrid framework effectively integrates advantages of existing methods, significantly improving structured data extraction from scientific text.

Abstract: The construction of experimental datasets is essential for expanding the
scope of data-driven scientific discovery. Recent advances in natural language
processing (NLP) have facilitated automatic extraction of structured data from
unstructured scientific literature. While existing approaches-multi-step and
direct methods-offer valuable capabilities, they also come with limitations
when applied independently. Here, we propose a novel hybrid text-mining
framework that integrates the advantages of both methods to convert
unstructured scientific text into structured data. Our approach first
transforms raw text into entity-recognized text, and subsequently into
structured form. Furthermore, beyond the overall data structuring framework, we
also enhance entity recognition performance by introducing an entity marker-a
simple yet effective technique that uses symbolic annotations to highlight
target entities. Specifically, our entity marker-based hybrid approach not only
consistently outperforms previous entity recognition approaches across three
benchmark datasets (MatScholar, SOFC, and SOFC slot NER) but also improve the
quality of final structured data-yielding up to a 58% improvement in
entity-level F1 score and up to 83% improvement in relation-level F1 score
compared to direct approach.

</details>


### [11] [Elastic Weight Consolidation for Full-Parameter Continual Pre-Training of Gemma2](https://arxiv.org/pdf/2505.05946)
*Vytenis Šliogeris, Povilas Daniušis, Artūras Nakvosas*

Main category: cs.CL

TL;DR: The paper explores using Elastic Weight Consolidation (EWC) in autoregressive pre-training of a 2B-parameter LLM (Gemma2) with Lithuanian language data, showing EWC helps mitigate forgetting and may improve new task learning.


<details>
  <summary>Details</summary>
Motivation: To investigate how EWC can address catastrophic forgetting in continual learning for LLMs, particularly with a focus on Lithuanian language data.

Method: Applied EWC to all parameters of Gemma2 during pre-training, evaluated on language understanding and perplexity benchmarks in English and Lithuanian.

Result: EWC not only reduced catastrophic forgetting but also potentially enhanced learning of new tasks.

Conclusion: EWC is effective for continual learning in LLMs, offering benefits beyond just mitigating forgetting.

Abstract: This technical report describes an experiment on autoregressive pre-training
of Gemma2 2 billion parameter large language model (LLM) with 10\% on the
Lithuanian language component of CulturaX from the point of view of continual
learning. We apply elastic weight consolidation (EWC) to the full set of the
model's parameters and investigate language understanding benchmarks,
consisting of Arc, Belebele, Gsm8K, Hellaswag, MMLU, TruthfulQA, and Winogrande
sets (both in English and Lithuanian versions), and perplexity benchmarks. We
empirically demonstrate that EWC regularisation allows us not only to mitigate
catastrophic forgetting effects but also that it is potentially beneficial for
learning of the new task with LLMs.

</details>


### [12] [Summarisation of German Judgments in conjunction with a Class-based Evaluation](https://arxiv.org/pdf/2505.05947)
*Bianca Steffes, Nils Torben Wiedemann, Alexander Gratz, Pamela Hochreither, Jana Elina Meyer, Katharina Luise Schilke*

Main category: cs.CL

TL;DR: Fine-tuning a decoder-based LLM for summarizing German legal judgments, enriched with legal entities, shows promise but lacks practical quality.


<details>
  <summary>Details</summary>
Motivation: To aid legal experts by automating summarization of lengthy legal documents.

Method: Fine-tune a decoder-based large language model, enriching judgments with legal entity information before training.

Result: Legal entities help identify relevant content, but summary quality is insufficient for practical use.

Conclusion: Further improvements are needed to make automated legal summarization viable for real-world applications.

Abstract: The automated summarisation of long legal documents can be a great aid for
legal experts in their daily work. We automatically create summaries (guiding
principles) of German judgments by fine-tuning a decoder-based large language
model. We enrich the judgments with information about legal entities before the
training. For the evaluation of the created summaries, we define a set of
evaluation classes which allows us to measure their language, pertinence,
completeness and correctness. Our results show that employing legal entities
helps the generative model to find the relevant content, but the quality of the
created summaries is not yet sufficient for a use in practice.

</details>


### [13] [NeoQA: Evidence-based Question Answering with Generated News Events](https://arxiv.org/pdf/2505.05949)
*Max Glockner, Xiang Jiang, Leonardo F. R. Ribeiro, Iryna Gurevych, Markus Dreyer*

Main category: cs.CL

TL;DR: NeoQA is a benchmark for evaluating RAG in LLMs using fictional news events to prevent reliance on pretraining knowledge, highlighting challenges in evidence-based reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for RAG in LLMs become stale as newer models incorporate more pretraining knowledge, making it hard to evaluate evidence-based reasoning.

Method: Constructed NeoQA with fictional news events, timelines, and Q&A pairs to ensure no prior pretraining knowledge exists, requiring LLMs to rely solely on retrieved evidence.

Result: LLMs struggle with subtle mismatches between questions and evidence and exhibit shortcut reasoning when key information is missing.

Conclusion: NeoQA provides a controlled platform for evaluating evidence-based QA, revealing limitations in LLMs' evidence-based reasoning.

Abstract: Evaluating Retrieval-Augmented Generation (RAG) in large language models
(LLMs) is challenging because benchmarks can quickly become stale. Questions
initially requiring retrieval may become answerable from pretraining knowledge
as newer models incorporate more recent information during pretraining, making
it difficult to distinguish evidence-based reasoning from recall. We introduce
NeoQA (News Events for Out-of-training Question Answering), a benchmark
designed to address this issue. To construct NeoQA, we generated timelines and
knowledge bases of fictional news events and entities along with news articles
and Q\&A pairs to prevent LLMs from leveraging pretraining knowledge, ensuring
that no prior evidence exists in their training data. We propose our dataset as
a new platform for evaluating evidence-based question answering, as it requires
LLMs to generate responses exclusively from retrieved evidence and only when
sufficient evidence is available. NeoQA enables controlled evaluation across
various evidence scenarios, including cases with missing or misleading details.
Our findings indicate that LLMs struggle to distinguish subtle mismatches
between questions and evidence, and suffer from short-cut reasoning when key
information required to answer a question is missing from the evidence,
underscoring key limitations in evidence-based reasoning.

</details>


### [14] [Reimagining Urban Science: Scaling Causal Inference with Large Language Models](https://arxiv.org/pdf/2504.12345)
*Yutong Xia, Ao Qu, Yunhan Zheng, Yihong Tang, Dingyi Zhuang, Yuxuan Liang, Shenhao Wang, Cathy Wu, Lijun Sun, Roger Zimmermann, Jinhua Zhao*

Main category: cs.CL

TL;DR: The paper proposes AutoUrbanCI, an LLM-driven framework for urban causal research, addressing inefficiencies and biases in hypothesis generation, data complexity, and experimentation. It emphasizes AI-augmented workflows to enhance inclusivity and reproducibility.


<details>
  <summary>Details</summary>
Motivation: Urban causal research faces challenges like inefficiency, bias, and methodological fragility. Advances in LLMs offer a chance to improve these processes.

Method: Introduces AutoUrbanCI, a modular framework with agents for hypothesis generation, data engineering, experiment design, and results interpretation.

Result: Proposes evaluation criteria for rigor and transparency, advocating for AI-augmented workflows to improve urban causal reasoning.

Conclusion: Calls for a research agenda integrating AI to broaden participation and enhance reproducibility in urban causal research, without replacing human expertise.

Abstract: Urban causal research is essential for understanding the complex dynamics of
cities and informing evidence-based policies. However, it is challenged by the
inefficiency and bias of hypothesis generation, barriers to multimodal data
complexity, and the methodological fragility of causal experimentation. Recent
advances in large language models (LLMs) present an opportunity to rethink how
urban causal analysis is conducted. This Perspective examines current urban
causal research by analyzing taxonomies that categorize research topics, data
sources, and methodological approaches to identify structural gaps. We then
introduce an LLM-driven conceptual framework, AutoUrbanCI, composed of four
distinct modular agents responsible for hypothesis generation, data
engineering, experiment design and execution, and results interpretation with
policy recommendations. We propose evaluation criteria for rigor and
transparency and reflect on implications for human-AI collaboration, equity,
and accountability. We call for a new research agenda that embraces
AI-augmented workflows not as replacements for human expertise but as tools to
broaden participation, improve reproducibility, and unlock more inclusive forms
of urban causal reasoning.

</details>


### [15] [Towards Developmentally Plausible Rewards: Communicative Success as a Learning Signal for Interactive Language Models](https://arxiv.org/pdf/2505.05970)
*Lennart Stöpler, Rufat Asadli, Mitja Nikolaus, Ryan Cotterell, Alex Warstadt*

Main category: cs.CL

TL;DR: A method for training language models interactively, inspired by child language acquisition, using a question-answering setting and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To explore how interaction can improve language learning in computational models, inspired by child language acquisition.

Method: Uses a single-turn dialogue setting with rewards for communicative success, operationalized in a language-only QA framework. Reinforcement learning fine-tunes models.

Result: Reward signals indirectly indicate grammaticality, and constraints on communication lead to interpretable speaker behavior, but no linguistic improvements yet.

Conclusion: Future work should modify task design and training to better leverage interaction for language learning benefits.

Abstract: We propose a method for training language models in an interactive setting
inspired by child language acquisition. In our setting, a speaker attempts to
communicate some information to a listener in a single-turn dialogue and
receives a reward if communicative success is achieved. Unlike earlier related
work using image--caption data for interactive reference games, we
operationalize communicative success in a more abstract language-only
question--answering setting. First, we present a feasibility study
demonstrating that our reward provides an indirect signal about grammaticality.
Second, we conduct experiments using reinforcement learning to fine-tune
language models. We observe that cognitively plausible constraints on the
communication channel lead to interpretable changes in speaker behavior.
However, we do not yet see improvements on linguistic evaluations from our
training regime. We outline potential modifications to the task design and
training configuration that could better position future work to use our
methodology to observe the benefits of interaction on language learning in
computational cognitive models.

</details>


### [16] [An Exploratory Analysis on the Explanatory Potential of Embedding-Based Measures of Semantic Transparency for Malay Word Recognition](https://arxiv.org/pdf/2505.05973)
*M. Maziyah Mohamed, R. H. Baayen*

Main category: cs.CL

TL;DR: The paper explores embedding-based measures of semantic transparency in Malay prefixed words, showing their predictive power for lexical decision latencies.


<details>
  <summary>Details</summary>
Motivation: To operationalize and measure semantic transparency computationally and assess its impact on reading.

Method: Used t-SNE clustering on 4,226 Malay prefixed words, derived five measures, and analyzed their predictive power via Linear Discriminant Analyses and Generalized Additive Mixed Models.

Result: All measures predicted decision latencies; the correlation between a word and its centroid provided the best model fit.

Conclusion: Embedding-based measures effectively capture semantic transparency and influence word recognition.

Abstract: Studies of morphological processing have shown that semantic transparency is
crucial for word recognition. Its computational operationalization is still
under discussion. Our primary objectives are to explore embedding-based
measures of semantic transparency, and assess their impact on reading. First,
we explored the geometry of complex words in semantic space. To do so, we
conducted a t-distributed Stochastic Neighbor Embedding clustering analysis on
4,226 Malay prefixed words. Several clusters were observed for complex words
varied by their prefix class. Then, we derived five simple measures, and
investigated whether they were significant predictors of lexical decision
latencies. Two sets of Linear Discriminant Analyses were run in which the
prefix of a word is predicted from either word embeddings or shift vectors
(i.e., a vector subtraction of the base word from the derived word). The
accuracy with which the model predicts the prefix of a word indicates the
degree of transparency of the prefix. Three further measures were obtained by
comparing embeddings between each word and all other words containing the same
prefix (i.e., centroid), between each word and the shift from their base word,
and between each word and the predicted word of the Functional Representations
of Affixes in Compositional Semantic Space model. In a series of Generalized
Additive Mixed Models, all measures predicted decision latencies after
accounting for word frequency, word length, and morphological family size. The
model that included the correlation between each word and their centroid as a
predictor provided the best fit to the data.

</details>


### [17] [Exploring the Feasibility of Multilingual Grammatical Error Correction with a Single LLM up to 9B parameters: A Comparative Study of 17 Models](https://arxiv.org/pdf/2505.06004)
*Dawid Wisniewski, Antoni Solarski, Artur Nowakowski*

Main category: cs.CL

TL;DR: The paper evaluates 17 language models for multilingual grammatical error correction (GEC) in English, German, Italian, and Swedish, identifying Gemma 9B as the top performer.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of popular language models in correcting grammatical errors across multiple languages using a single model.

Method: Analyzed outputs of 17 models for GEC in four languages, focusing on error reduction and minimal text changes.

Result: Six models improved grammatical correctness across all languages, with Gemma 9B being the best performer.

Conclusion: The study highlights model-specific issues and recommends Gemma 9B for multilingual GEC tasks.

Abstract: Recent language models can successfully solve various language-related tasks,
and many understand inputs stated in different languages. In this paper, we
explore the performance of 17 popular models used to correct grammatical issues
in texts stated in English, German, Italian, and Swedish when using a single
model to correct texts in all those languages. We analyze the outputs generated
by these models, focusing on decreasing the number of grammatical errors while
keeping the changes small. The conclusions drawn help us understand what
problems occur among those models and which models can be recommended for
multilingual grammatical error correction tasks. We list six models that
improve grammatical correctness in all four languages and show that Gemma 9B is
currently the best performing one for the languages considered.

</details>


### [18] [Do Not Change Me: On Transferring Entities Without Modification in Neural Machine Translation -- a Multilingual Perspective](https://arxiv.org/pdf/2505.06010)
*Dawid Wisniewski, Mikolaj Pokrywka, Zofia Rostek*

Main category: cs.CL

TL;DR: The paper evaluates NMT models' ability to preserve entities like URLs, IBANs, and emails in translations across four languages, identifies challenges (e.g., emojis), and introduces a synthetic dataset for testing.


<details>
  <summary>Details</summary>
Motivation: To address the issue of NMT models failing to preserve specific entities during translation, the study aims to assess and improve their accuracy.

Method: The study tests popular NMT models (OPUS, Google Translate, MADLAD, EuroLLM) on entity preservation in English, German, Polish, and Ukrainian, analyzing errors and reasons.

Result: Findings reveal challenges, especially with emojis, and propose a 36,000-sentence synthetic dataset for evaluating entity transfer quality.

Conclusion: The study highlights NMT models' limitations in entity preservation and offers a dataset to aid future improvements.

Abstract: Current machine translation models provide us with high-quality outputs in
most scenarios. However, they still face some specific problems, such as
detecting which entities should not be changed during translation. In this
paper, we explore the abilities of popular NMT models, including models from
the OPUS project, Google Translate, MADLAD, and EuroLLM, to preserve entities
such as URL addresses, IBAN numbers, or emails when producing translations
between four languages: English, German, Polish, and Ukrainian. We investigate
the quality of popular NMT models in terms of accuracy, discuss errors made by
the models, and examine the reasons for errors. Our analysis highlights
specific categories, such as emojis, that pose significant challenges for many
models considered. In addition to the analysis, we propose a new multilingual
synthetic dataset of 36,000 sentences that can help assess the quality of
entity transfer across nine categories and four aforementioned languages.

</details>


### [19] [Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation](https://arxiv.org/pdf/2505.06027)
*Stefan Vasilev, Christian Herold, Baohao Liao, Seyyed Hadi Hashemi, Shahram Khadivi, Christof Monz*

Main category: cs.CL

TL;DR: Unilogit is a self-distillation method for machine unlearning in LLMs, dynamically adjusting target logits to selectively forget data while maintaining model utility, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of selective forgetting in LLMs to comply with data privacy regulations like GDPR.

Method: Dynamically adjusts target logits for uniform probability of the target token, using current model outputs for accurate self-distillation.

Result: Outperforms state-of-the-art methods (NPO, UnDIAL) in balancing forget and retain objectives, with robustness across scenarios.

Conclusion: Unilogit is effective and practical for machine unlearning, enhancing compliance and model utility.

Abstract: This paper introduces Unilogit, a novel self-distillation method for machine
unlearning in Large Language Models. Unilogit addresses the challenge of
selectively forgetting specific information while maintaining overall model
utility, a critical task in compliance with data privacy regulations like GDPR.
Unlike prior methods that rely on static hyperparameters or starting model
outputs, Unilogit dynamically adjusts target logits to achieve a uniform
probability for the target token, leveraging the current model's outputs for
more accurate self-distillation targets. This approach not only eliminates the
need for additional hyperparameters but also enhances the model's ability to
approximate the golden targets. Extensive experiments on public benchmarks and
an in-house e-commerce dataset demonstrate Unilogit's superior performance in
balancing forget and retain objectives, outperforming state-of-the-art methods
such as NPO and UnDIAL. Our analysis further reveals Unilogit's robustness
across various scenarios, highlighting its practical applicability and
effectiveness in achieving efficacious machine unlearning.

</details>


### [20] [Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health Information](https://arxiv.org/pdf/2505.06046)
*Joshua Harris, Fan Grayson, Felix Feldman, Timothy Laurence, Toby Nonnenmacher, Oliver Higgins, Leo Loman, Selina Patel, Thomas Finnie, Samuel Collins, Michael Borowitz*

Main category: cs.CL

TL;DR: The paper introduces PubHealthBench, a benchmark for evaluating LLMs' knowledge of UK public health info, finding SOTA models perform well in MCQA but lag in free-form responses.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' accuracy in public health info retrieval, critical for real-world impact, given the lack of current knowledge about their performance in this domain.

Method: Created PubHealthBench with 8000 questions (MCQA and free-form) from UK Government public health docs, tested 24 LLMs.

Result: SOTA LLMs scored >90% in MCQA, outperforming humans, but <75% in free-form responses.

Conclusion: While SOTA LLMs show promise for public health info, safeguards are needed for free-form responses.

Abstract: As Large Language Models (LLMs) become widely accessible, a detailed
understanding of their knowledge within specific domains becomes necessary for
successful real world use. This is particularly critical in public health,
where failure to retrieve relevant, accurate, and current information could
significantly impact UK residents. However, currently little is known about LLM
knowledge of UK Government public health information. To address this issue,
this paper introduces a new benchmark, PubHealthBench, with over 8000 questions
for evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form
responses to public health queries, created via an automated pipeline. We also
release a new dataset of the extracted UK Government public health guidance
documents used as source text for PubHealthBench. Assessing 24 LLMs on
PubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a
high degree of knowledge, achieving >90% in the MCQA setup, and outperform
humans with cursory search engine use. However, in the free form setup we see
lower performance with no model scoring >75%. Therefore, whilst there are
promising signs that state of the art (SOTA) LLMs are an increasingly accurate
source of public health information, additional safeguards or tools may still
be needed when providing free form responses on public health topics.

</details>


### [21] [Attention on Multiword Expressions: A Multilingual Study of BERT-based Models with Regard to Idiomaticity and Microsyntax](https://arxiv.org/pdf/2505.06062)
*Iuliia Zaitova, Vitalii Hirak, Badr M. Abdullah, Dietrich Klakow, Bernd Möbius, Tania Avgustinova*

Main category: cs.CL

TL;DR: Fine-tuned BERT-based models' attention to Multiword Expressions (MWEs) varies by task type: semantic tasks evenly distribute attention to idioms, while syntactic tasks increase attention to microsyntactic units (MSUs) in lower layers.


<details>
  <summary>Details</summary>
Motivation: To understand how fine-tuning BERT-based models on semantic or syntactic tasks affects their attention patterns toward MWEs (idioms and MSUs).

Method: Analyze attention scores in pre-trained and fine-tuned BERT-based models across six Indo-European languages.

Result: Fine-tuning influences attention allocation: semantic tasks distribute attention to idioms evenly; syntactic tasks increase attention to MSUs in lower layers.

Conclusion: Task-specific fine-tuning shapes how BERT-based models attend to MWEs, aligning with semantic or syntactic processing needs.

Abstract: This study analyzes the attention patterns of fine-tuned encoder-only models
based on the BERT architecture (BERT-based models) towards two distinct types
of Multiword Expressions (MWEs): idioms and microsyntactic units (MSUs). Idioms
present challenges in semantic non-compositionality, whereas MSUs demonstrate
unconventional syntactic behavior that does not conform to standard grammatical
categorizations. We aim to understand whether fine-tuning BERT-based models on
specific tasks influences their attention to MWEs, and how this attention
differs between semantic and syntactic tasks. We examine attention scores to
MWEs in both pre-trained and fine-tuned BERT-based models. We utilize
monolingual models and datasets in six Indo-European languages - English,
German, Dutch, Polish, Russian, and Ukrainian. Our results show that
fine-tuning significantly influences how models allocate attention to MWEs.
Specifically, models fine-tuned on semantic tasks tend to distribute attention
to idiomatic expressions more evenly across layers. Models fine-tuned on
syntactic tasks show an increase in attention to MSUs in the lower layers,
corresponding with syntactic processing requirements.

</details>


### [22] [Multimodal Sentiment Analysis on CMU-MOSEI Dataset using Transformer-based Models](https://arxiv.org/pdf/2505.06110)
*Jugal Gajjar, Kaustik Ranaware*

Main category: cs.CL

TL;DR: The paper presents a multimodal sentiment analysis model using early fusion of text, audio, and visual data with transformer-based encoders, achieving high accuracy and F1-score.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the effectiveness of early fusion in capturing cross-modal interactions for sentiment analysis using transformer architectures.

Method: BERT-based encoders for each modality (text, audio, visual) with concatenated embeddings, trained using Adam optimization, dropout, and early stopping.

Result: Achieved 97.87% 7-class accuracy, 0.9682 F1-score, and low MAE (0.1060), showing strong performance.

Conclusion: Transformer-based early fusion is effective for multimodal sentiment analysis; future work could explore other fusion strategies or interpretability.

Abstract: This project performs multimodal sentiment analysis using the CMU-MOSEI
dataset, using transformer-based models with early fusion to integrate text,
audio, and visual modalities. We employ BERT-based encoders for each modality,
extracting embeddings that are concatenated before classification. The model
achieves strong performance, with 97.87\% 7-class accuracy and a 0.9682
F1-score on the test set, demonstrating the effectiveness of early fusion in
capturing cross-modal interactions. The training utilized Adam optimization
(lr=1e-4), dropout (0.3), and early stopping to ensure generalization and
robustness. Results highlight the superiority of transformer architectures in
modeling multimodal sentiment, with a low MAE (0.1060) indicating precise
sentiment intensity prediction. Future work may compare fusion strategies or
enhance interpretability. This approach utilizes multimodal learning by
effectively combining linguistic, acoustic, and visual cues for sentiment
analysis.

</details>


### [23] [LLMs Get Lost In Multi-Turn Conversation](https://arxiv.org/pdf/2505.06120)
*Philippe Laban, Hiroaki Hayashi, Yingbo Zhou, Jennifer Neville*

Main category: cs.CL

TL;DR: LLMs perform worse in multi-turn conversations (39% drop) due to assumptions and premature solutions, unlike single-turn tasks.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLM performance in multi-turn conversations, which is common but understudied compared to single-turn tasks.

Method: Large-scale simulation experiments comparing top LLMs in single- and multi-turn settings, analyzing 200,000+ conversations.

Result: LLMs show a 39% performance drop in multi-turn tasks, caused by minor aptitude loss and significant unreliability.

Conclusion: LLMs struggle in multi-turn conversations due to early assumptions and over-reliance on incorrect solutions, leading to poor recovery.

Abstract: Large Language Models (LLMs) are conversational interfaces. As such, LLMs
have the potential to assist their users not only when they can fully specify
the task at hand, but also to help them define, explore, and refine what they
need through multi-turn conversational exchange. Although analysis of LLM
conversation logs has confirmed that underspecification occurs frequently in
user instructions, LLM evaluation has predominantly focused on the single-turn,
fully-specified instruction setting. In this work, we perform large-scale
simulation experiments to compare LLM performance in single- and multi-turn
settings. Our experiments confirm that all the top open- and closed-weight LLMs
we test exhibit significantly lower performance in multi-turn conversations
than single-turn, with an average drop of 39% across six generation tasks.
Analysis of 200,000+ simulated conversations decomposes the performance
degradation into two components: a minor loss in aptitude and a significant
increase in unreliability. We find that LLMs often make assumptions in early
turns and prematurely attempt to generate final solutions, on which they overly
rely. In simpler terms, we discover that *when LLMs take a wrong turn in a
conversation, they get lost and do not recover*.

</details>


### [24] [Towards Robust Few-Shot Text Classification Using Transformer Architectures and Dual Loss Strategies](https://arxiv.org/pdf/2505.06145)
*Xu Han, Yumeng Sun, Weiqiang Huang, Hongye Zheng, Junliang Du*

Main category: cs.CL

TL;DR: The paper proposes a strategy combining adaptive fine-tuning, contrastive learning, and regularization to enhance few-shot text classification for Transformer models, showing improved performance on the FewRel 2.0 dataset.


<details>
  <summary>Details</summary>
Motivation: Few-shot text classification is valuable in low-resource settings, but challenges like fuzzy semantic boundaries and overfitting hinder performance.

Method: The approach integrates adaptive fine-tuning, contrastive learning, and regularization optimization to improve Transformer models like T5-small, DeBERTa-v3, and RoBERTa-base.

Result: Experiments demonstrate improved accuracy, especially in 5-shot tasks, with contrastive and regularization losses enhancing generalization and reducing overfitting.

Conclusion: Transformer models with stronger self-attention mechanisms, combined with contrastive and regularization techniques, improve few-shot classification stability and accuracy.

Abstract: Few-shot text classification has important application value in low-resource
environments. This paper proposes a strategy that combines adaptive
fine-tuning, contrastive learning, and regularization optimization to improve
the classification performance of Transformer-based models. Experiments on the
FewRel 2.0 dataset show that T5-small, DeBERTa-v3, and RoBERTa-base perform
well in few-shot tasks, especially in the 5-shot setting, which can more
effectively capture text features and improve classification accuracy. The
experiment also found that there are significant differences in the
classification difficulty of different relationship categories. Some categories
have fuzzy semantic boundaries or complex feature distributions, making it
difficult for the standard cross entropy loss to learn the discriminative
information required to distinguish categories. By introducing contrastive loss
and regularization loss, the generalization ability of the model is enhanced,
effectively alleviating the overfitting problem in few-shot environments. In
addition, the research results show that the use of Transformer models or
generative architectures with stronger self-attention mechanisms can help
improve the stability and accuracy of few-shot classification.

</details>


### [25] [A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed Compute Budgets](https://arxiv.org/pdf/2505.06150)
*Ryan Lagasse, Aidan Kiernans, Avijit Ghosh, Shiri Dori-Hacohen*

Main category: cs.CL

TL;DR: The paper introduces a scaling law for fine-tuning LLMs that considers data composition (dataset volume) alongside total tokens, showing its impact on model performance and token efficiency.


<details>
  <summary>Details</summary>
Motivation: Current scaling laws overlook data composition (number of examples and average token length), which is critical for efficient fine-tuning under fixed compute budgets.

Method: The study formulates a new scaling law, validated through experiments on the BRICC and MMLU datasets using subsampling strategies.

Result: Data composition significantly affects token efficiency, demonstrating the need for refined scaling laws.

Conclusion: The findings advocate for incorporating dataset volume into scaling laws for practical LLM fine-tuning in resource-limited scenarios.

Abstract: We introduce a scaling law for fine-tuning large language models (LLMs) under
fixed compute budgets that explicitly accounts for data composition.
Conventional approaches measure training data solely by total tokens, yet the
number of examples and their average token length -- what we term \emph{dataset
volume} -- play a decisive role in model performance. Our formulation is tuned
following established procedures. Experiments on the BRICC dataset
\cite{salavati2024reducing} and subsets of the MMLU dataset
\cite{hendrycks2021measuringmassivemultitasklanguage}, evaluated under multiple
subsampling strategies, reveal that data composition significantly affects
token efficiency. These results motivate refined scaling laws for practical LLM
fine-tuning in resource-constrained settings.

</details>


### [26] [Estimating Quality in Therapeutic Conversations: A Multi-Dimensional Natural Language Processing Framework](https://arxiv.org/pdf/2505.06151)
*Alice Rueda, Argyrios Perivolaris, Niloy Roy, Dylan Weston, Sarmed Shaya, Zachary Cote, Martin Ivanov, Bazen G. Teferra, Yuqi Wu, Sirisha Rambhatla, Divya Sharma, Andrew Greenshaw, Rakesh Jetly, Yanbo Zhang, Bo Cao, Reza Samavi, Sridhar Krishnan, Venkat Bhat*

Main category: cs.CL

TL;DR: A multi-dimensional NLP framework classifies engagement quality in therapy sessions using conversational dynamics, semantic similarity, sentiment, and question detection. Random Forest and SVM showed high accuracy and AUC, especially after data augmentation.


<details>
  <summary>Details</summary>
Motivation: To objectively assess and improve engagement quality in therapy sessions, which is critical for therapeutic success.

Method: Extracted 42 features from 253 transcripts across four domains, trained classifiers (RF, Cat-Boost, SVM) with hyperparameter tuning and cross-validation, and evaluated performance on augmented and non-augmented data.

Result: RF achieved 88.9% accuracy and 94.6% AUC after augmentation. Conversational dynamics and semantic similarity were top contributors.

Conclusion: The framework is scalable and robust, with potential for multimodal extensions, offering real-time feedback to enhance therapy quality.

Abstract: Engagement between client and therapist is a critical determinant of
therapeutic success. We propose a multi-dimensional natural language processing
(NLP) framework that objectively classifies engagement quality in counseling
sessions based on textual transcripts. Using 253 motivational interviewing
transcripts (150 high-quality, 103 low-quality), we extracted 42 features
across four domains: conversational dynamics, semantic similarity as topic
alignment, sentiment classification, and question detection. Classifiers,
including Random Forest (RF), Cat-Boost, and Support Vector Machines (SVM),
were hyperparameter tuned and trained using a stratified 5-fold
cross-validation and evaluated on a holdout test set. On balanced
(non-augmented) data, RF achieved the highest classification accuracy (76.7%),
and SVM achieved the highest AUC (85.4%). After SMOTE-Tomek augmentation,
performance improved significantly: RF achieved up to 88.9% accuracy, 90.0%
F1-score, and 94.6% AUC, while SVM reached 81.1% accuracy, 83.1% F1-score, and
93.6% AUC. The augmented data results reflect the potential of the framework in
future larger-scale applications. Feature contribution revealed conversational
dynamics and semantic similarity between clients and therapists were among the
top contributors, led by words uttered by the client (mean and standard
deviation). The framework was robust across the original and augmented datasets
and demonstrated consistent improvements in F1 scores and recall. While
currently text-based, the framework supports future multimodal extensions
(e.g., vocal tone, facial affect) for more holistic assessments. This work
introduces a scalable, data-driven method for evaluating engagement quality of
the therapy session, offering clinicians real-time feedback to enhance the
quality of both virtual and in-person therapeutic interactions.

</details>


### [27] [Query-driven Document-level Scientific Evidence Extraction from Biomedical Studies](https://arxiv.org/pdf/2505.06186)
*Massimiliano Pronesti, Joao Bettencourt-Silva, Paul Flanagan, Alessandra Pascale, Oisin Redmond, Anya Belz, Yufang Hou*

Main category: cs.CL

TL;DR: The paper introduces CochraneForest, a dataset for document-level scientific evidence extraction, and URCA, a retrieval-augmented framework, which outperforms existing methods by 10.3% in F1 score.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of extracting scientific evidence for clinical questions with conflicting evidence, particularly for synthesizing biomedical evidence.

Method: Creation of the CochraneForest dataset (202 annotated forest plots) and development of URCA, a retrieval-augmented generation framework.

Result: URCA outperforms existing methods by up to 10.3% in F1 score, but the dataset's complexity highlights its challenge.

Conclusion: CochraneForest serves as a challenging testbed for advancing automated evidence synthesis, with URCA showing promising performance.

Abstract: Extracting scientific evidence from biomedical studies for clinical research
questions (e.g., Does stem cell transplantation improve quality of life in
patients with medically refractory Crohn's disease compared to placebo?) is a
crucial step in synthesising biomedical evidence. In this paper, we focus on
the task of document-level scientific evidence extraction for clinical
questions with conflicting evidence. To support this task, we create a dataset
called CochraneForest, leveraging forest plots from Cochrane systematic
reviews. It comprises 202 annotated forest plots, associated clinical research
questions, full texts of studies, and study-specific conclusions. Building on
CochraneForest, we propose URCA (Uniform Retrieval Clustered Augmentation), a
retrieval-augmented generation framework designed to tackle the unique
challenges of evidence extraction. Our experiments show that URCA outperforms
the best existing methods by up to 10.3% in F1 score on this task. However, the
results also underscore the complexity of CochraneForest, establishing it as a
challenging testbed for advancing automated evidence synthesis systems.

</details>


### [28] [PART: Pre-trained Authorship Representation Transformer](https://arxiv.org/pdf/2209.15373)
*Javier Huertas-Tato, Alejandro Martin, David Camacho*

Main category: cs.CL

TL;DR: PART is a contrastively trained model for authorship embeddings, outperforming RoBERTa by 54-56% in zero-shot accuracy on 250 authors.


<details>
  <summary>Details</summary>
Motivation: Existing authorship models rely on hand-crafted features or classification tasks, performing poorly on out-of-domain authors. Stylometric representations are needed but remain a challenge.

Method: PART uses contrastive training to learn authorship embeddings, trained on 1.5M texts from diverse authors (literature, blogs, corporate emails).

Result: Achieves 72.39% zero-shot accuracy on 250 authors, outperforming RoBERTa by 54-56%. Visualizations reveal features like gender, age, and occupation.

Conclusion: PART effectively learns authorship embeddings, addressing limitations of prior methods and demonstrating strong performance and interpretability.

Abstract: Authors writing documents imprint identifying information within their texts:
vocabulary, registry, punctuation, misspellings, or even emoji usage. Previous
works use hand-crafted features or classification tasks to train their
authorship models, leading to poor performance on out-of-domain authors. Using
stylometric representations is more suitable, but this by itself is an open
research challenge. In this paper, we propose PART, a contrastively trained
model fit to learn \textbf{authorship embeddings} instead of semantics. We
train our model on ~1.5M texts belonging to 1162 literature authors, 17287 blog
posters and 135 corporate email accounts; a heterogeneous set with identifiable
writing styles. We evaluate the model on current challenges, achieving
competitive performance. We also evaluate our model on test splits of the
datasets achieving zero-shot 72.39\% accuracy when bounded to 250 authors, a
54\% and 56\% higher than RoBERTa embeddings. We qualitatively assess the
representations with different data visualizations on the available datasets,
observing features such as gender, age, or occupation of the author.

</details>


### [29] [Large Language Models Are Struggle to Cope with Unreasonability in Math Problems](https://arxiv.org/pdf/2403.19346)
*Jingyuan Ma, Damai Dai, Zihang Yuan, Rui li, Weilin Luo, Bin Wang, Qun Liu, Lei Sha, Zhifang Sui*

Main category: cs.CL

TL;DR: A new benchmark, Unreasonable Math Problem (UMP), evaluates LLMs' ability to handle unconventional math problems with inconsistencies. State-of-the-art models like GPT-4o perform poorly (0.6 score), while reasoning models like DeepSeek-R1 struggle with instability.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' capability in addressing math problems with flawed assumptions or inconsistencies, an area not well-studied.

Method: Proposed the UMP benchmark, a curated set of unreasonable math questions, and tested 19 LLMs, including GPT-4o and DeepSeek-R1.

Result: Even top models like GPT-4o scored only 0.6 on UMP; reasoning models showed instability.

Conclusion: The study highlights LLMs' limitations in handling unreasonable math problems and suggests potential improvement strategies.

Abstract: Recent research have demonstrated LLMs' impressive performance in math and
reasoning. However, the capacity of LLMs to address math problems under
unconventional conditions, such as internal inconsistencies and flawed
assumptions, remains largely unexplored. In this paper, we propose a novel
benchmark Unreasonable Math Problem (UMP) designed to assess LLMs' ability to
recognize and respond to unreasonability in math problem. The benchmark
consists of a carefully curated collection of unreasonable math questions
across diverse types. Based on extensive experiments covering 19 LLMs, we
observe that even state-of-the-art models such as GPT-4o achieve only limited
performance of 0.6 in UMP, while reasoning models such as DeepSeek-R1 are prone
to overthinking and unstable. We further explore strategies for improving the
recognition of unreasonable inputs, shedding light on both the possibility and
limitations of LLMs in this challenging setting.

</details>


### [30] [Talking Heads: Understanding Inter-layer Communication in Transformer Language Models](https://arxiv.org/pdf/2406.09519)
*Jack Merullo, Carsten Eickhoff, Ellie Pavlick*

Main category: cs.CL

TL;DR: The paper investigates how transformer LMs represent and route information between layers, revealing low-rank communication channels and a 3D subspace in GPT-2 for positional indexing. It explains model sensitivity to item order and improves task performance by manipulating representations.


<details>
  <summary>Details</summary>
Motivation: To understand how transformer LMs internally represent and route information, particularly why they sometimes fail in simple tasks despite their sophistication.

Method: Analyzes mechanisms in two LMs, focusing on low-rank subspaces in the residual stream and attention head interactions via SVD. Manipulates model representations and weights to improve performance.

Result: Identifies a 3D subspace for positional indexing in GPT-2, explains sensitivity to item order, and improves task accuracy by over 20% through discovered mechanisms.

Conclusion: Reveals intricate interpretable structures in LMs, aiding understanding of their failures and enabling future analysis of complex behaviors.

Abstract: Although it is known that transformer language models (LMs) pass features
from early layers to later layers, it is not well understood how this
information is represented and routed by the model. We analyze a mechanism used
in two LMs to selectively inhibit items in a context in one task, and find that
it underlies a commonly used abstraction across many context-retrieval
behaviors. Specifically, we find that models write into low-rank subspaces of
the residual stream to represent features which are then read out by later
layers, forming low-rank communication channels (Elhage et al., 2021) between
layers. A particular 3D subspace in model activations in GPT-2 can be traversed
to positionally index items in lists, and we show that this mechanism can
explain an otherwise arbitrary-seeming sensitivity of the model to the order of
items in the prompt. That is, the model has trouble copying the correct
information from context when many items ``crowd" this limited space. By
decomposing attention heads with the Singular Value Decomposition (SVD), we
find that previously described interactions between heads separated by one or
more layers can be predicted via analysis of their weight matrices alone. We
show that it is possible to manipulate the internal model representations as
well as edit model weights based on the mechanism we discover in order to
significantly improve performance on our synthetic Laundry List task, which
requires recall from a list, often improving task accuracy by over 20%. Our
analysis reveals a surprisingly intricate interpretable structure learned from
language model pretraining, and helps us understand why sophisticated LMs
sometimes fail in simple domains, facilitating future analysis of more complex
behaviors.

</details>


### [31] [NeedleBench: Can LLMs Do Retrieval and Reasoning in Information-Dense Context?](https://arxiv.org/pdf/2407.11963)
*Mo Li, Songyang Zhang, Taolin Zhang, Haodong Duan, Yunxin Liu, Kai Chen*

Main category: cs.CL

TL;DR: NeedleBench is a synthetic framework for evaluating retrieval and reasoning in bilingual long-context tasks, addressing limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for long-context handling in LLMs are flawed, either relying on real-world texts or artificial fillers, reducing effectiveness.

Method: NeedleBench embeds key data at varying depths, testing models in two scenarios: information-sparse (simple retrieval) and information-dense (complex reasoning).

Result: Models like Deepseek-R1 and OpenAI's o3 struggle with continuous retrieval and reasoning in dense scenarios, showing 'under-thinking' behavior.

Conclusion: NeedleBench offers insights and tools for improving LLMs' long-context capabilities, with resources available on OpenCompass.

Abstract: The capability of large language models to handle long-context information is
crucial across various real-world applications. Existing evaluation methods
often rely either on real-world long texts, making it difficult to exclude the
influence of models' inherent knowledge, or introduce irrelevant filler content
to artificially achieve target lengths, reducing assessment effectiveness. To
address these limitations, we introduce NeedleBench, a synthetic framework for
assessing retrieval and reasoning performance in bilingual long-context tasks
with adaptive context lengths. NeedleBench systematically embeds key data
points at varying depths to rigorously test model capabilities. Tasks are
categorized into two scenarios: information-sparse, featuring minimal relevant
details within extensive irrelevant text to simulate simple retrieval tasks;
and information-dense (the Ancestral Trace Challenge), where relevant
information is continuously distributed throughout the context to simulate
complex reasoning tasks. Our experiments reveal that although recent reasoning
models like Deepseek-R1 and OpenAI's o3 excel in mathematical reasoning, they
struggle with continuous retrieval and reasoning in information-dense
scenarios, even at shorter context lengths. We also characterize a phenomenon
termed 'under-thinking', where models prematurely conclude reasoning despite
available information. NeedleBench thus provides critical insights and targeted
tools essential for evaluating and improving LLMs' long-context capabilities.
All resources are available at OpenCompass:
https://github.com/open-compass/opencompass.

</details>


### [32] [ReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation Extraction on an Academic Budget](https://arxiv.org/pdf/2408.00103)
*Riccardo Orlando, Pere-Lluis Huguet Cabot, Edoardo Barba, Roberto Navigli*

Main category: cs.CL

TL;DR: ReLiK is a Retriever-Reader architecture for Entity Linking (EL) and Relation Extraction (RE), combining both tasks efficiently with a single forward pass and achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency and performance in EL and RE by integrating them into a unified framework, leveraging pre-trained language models.

Method: Uses a Retriever to identify candidates and a Reader to align them with text spans, employing an innovative input representation for single-pass processing.

Result: Achieves state-of-the-art in benchmarks, with 40x faster inference and academic budget training.

Conclusion: ReLiK sets a new standard for EL and RE, and extends seamlessly to combined Information Extraction (cIE).

Abstract: Entity Linking (EL) and Relation Extraction (RE) are fundamental tasks in
Natural Language Processing, serving as critical components in a wide range of
applications. In this paper, we propose ReLiK, a Retriever-Reader architecture
for both EL and RE, where, given an input text, the Retriever module undertakes
the identification of candidate entities or relations that could potentially
appear within the text. Subsequently, the Reader module is tasked to discern
the pertinent retrieved entities or relations and establish their alignment
with the corresponding textual spans. Notably, we put forward an innovative
input representation that incorporates the candidate entities or relations
alongside the text, making it possible to link entities or extract relations in
a single forward pass and to fully leverage pre-trained language models
contextualization capabilities, in contrast with previous
Retriever-Reader-based methods, which require a forward pass for each
candidate. Our formulation of EL and RE achieves state-of-the-art performance
in both in-domain and out-of-domain benchmarks while using academic budget
training and with up to 40x inference speed compared to competitors. Finally,
we show how our architecture can be used seamlessly for Information Extraction
(cIE), i.e. EL + RE, and setting a new state of the art by employing a shared
Reader that simultaneously extracts entities and relations.

</details>


### [33] [Multi-Draft Speculative Sampling: Canonical Decomposition and Theoretical Limits](https://arxiv.org/pdf/2410.18234)
*Ashish Khisti, M. Reza Ebrahimi, Hassan Dbouk, Arash Behboodi, Roland Memisevic, Christos Louizos*

Main category: cs.CL

TL;DR: The paper introduces a two-step optimal token-level draft selection scheme for multi-draft speculative sampling, improving efficiency and token rates.


<details>
  <summary>Details</summary>
Motivation: To enhance the performance of speculative sampling by optimizing token-level draft selection, addressing limitations of prior methods.

Method: Decomposes the optimal scheme into a two-step solution: importance sampling for intermediate token selection, followed by single-draft speculative sampling. Theoretical analysis includes conditions for perfect acceptance and optimal probabilities.

Result: Demonstrates consistent improvements in block efficiency and token rates over baseline schemes in various scenarios.

Conclusion: The proposed two-step scheme and weighted importance sampling-based methods offer superior performance, validated by theoretical and experimental results.

Abstract: We consider multi-draft speculative sampling, where the proposal sequences
are sampled independently from different draft models. At each step, a
token-level draft selection scheme takes a list of valid tokens as input and
produces an output token whose distribution matches that of the target model.
Previous works have demonstrated that the optimal scheme (which maximizes the
probability of accepting one of the input tokens) can be cast as a solution to
a linear program. In this work we show that the optimal scheme can be
decomposed into a two-step solution: in the first step an importance sampling
(IS) type scheme is used to select one intermediate token; in the second step
(single-draft) speculative sampling is applied to generate the output token.
For the case of two identical draft models we further 1) establish a necessary
and sufficient condition on the distributions of the target and draft models
for the acceptance probability to equal one and 2) provide an explicit
expression for the optimal acceptance probability. Our theoretical analysis
also motives a new class of token-level selection schemes based on weighted
importance sampling. Our experimental results demonstrate consistent
improvements in the achievable block efficiency and token rates over baseline
schemes in a number of scenarios.

</details>


### [34] [SRA-MCTS: Self-driven Reasoning Augmentation with Monte Carlo Tree Search for Code Generation](https://arxiv.org/pdf/2411.11053)
*Bin Xu, Yiguan Lin, Yinghao Li, Yang Gao*

Main category: cs.CL

TL;DR: The paper introduces SRA-MCTS, a reasoning-augmented data generation method to improve large language models' performance in complex code generation by autonomously generating high-quality reasoning paths.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with complex problems due to insufficient reasoning and problem decomposition capabilities.

Method: Proposes SRA-MCTS, a self-improving method that generates natural language reasoning paths and translates them into executable code without additional supervision.

Result: Achieves performance improvements across model scales, robustness against CoT degradation, and better diversity metrics like pass@10.

Conclusion: Encourages exploring reasoning processes in training data to enhance language models' ability to solve complex problems.

Abstract: Large language models demonstrate exceptional performance in simple code
generation tasks but still face challenges in tackling complex problems. These
challenges may stem from insufficient reasoning and problem decomposition
capabilities. To address this issue, we propose a reasoning-augmented data
generation process, SRA-MCTS, which guides the model to autonomously generate
high-quality intermediate reasoning paths. This creates a positive feedback
loop, enabling continuous improvement. Our method operates entirely through the
model itself without requiring additional supervision. By synthesizing natural
language reasoning paths and translating them into executable code, the
approach ensures analytical accuracy and enhances the success rate in solving
complex tasks. Experimental results show that, even without additional
supervisory signals, our method achieves performance improvements across
different model scales, demonstrating the significant potential of
self-improvement in small models. Furthermore, the method remains robust when
traditional Chain-of-Thought (CoT) approaches exhibit performance degradation,
with notable improvements observed in diversity metrics such as pass@10. We
encourage further exploration of reasoning processes within training data to
enhance the ability of language models to address complex problems. Our code
and data are public at https://github.com/DIRECT-BIT/SRA-MCTS.

</details>


### [35] [Can open source large language models be used for tumor documentation in Germany? -- An evaluation on urological doctors' notes](https://arxiv.org/pdf/2501.12106)
*Stefan Lenz, Arsenij Ustjanzew, Marco Jeray, Meike Ressing, Torsten Panholzer*

Main category: cs.CL

TL;DR: Open-source LLMs (7-12B parameters) show promise for automating tumor documentation in Germany, with models like Llama 3.1 8B and Mistral 7B performing well. Few-shot prompting and cross-domain examples enhance performance.


<details>
  <summary>Details</summary>
Motivation: Manual tumor documentation is inefficient; LLMs could improve reliability and efficiency.

Method: Evaluated 11 open-source LLMs (1-70B parameters) on tumor diagnosis, ICD-10 coding, and diagnosis date extraction using annotated urology notes. Tested few-shot prompting and cross-domain examples.

Result: 7-12B parameter models (e.g., Llama 3.1 8B, Mistral 7B) performed best. Smaller models lagged; larger models showed no gains. Cross-domain examples improved few-shot results.

Conclusion: Open-source LLMs (7-12B parameters) are viable for tumor documentation with fine-tuning and prompting. The dataset and code are released for further research.

Abstract: Tumor documentation in Germany is largely done manually, requiring reading
patient records and entering data into structured databases. Large language
models (LLMs) could potentially enhance this process by improving efficiency
and reliability. This evaluation tests eleven different open source LLMs with
sizes ranging from 1-70 billion model parameters on three basic tasks of the
tumor documentation process: identifying tumor diagnoses, assigning ICD-10
codes, and extracting the date of first diagnosis. For evaluating the LLMs on
these tasks, a dataset of annotated text snippets based on anonymized doctors'
notes from urology was prepared. Different prompting strategies were used to
investigate the effect of the number of examples in few-shot prompting and to
explore the capabilities of the LLMs in general. The models Llama 3.1 8B,
Mistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks.
Models with less extensive training data or having fewer than 7 billion
parameters showed notably lower performance, while larger models did not
display performance gains. Examples from a different medical domain than
urology could also improve the outcome in few-shot prompting, which
demonstrates the ability of LLMs to handle tasks needed for tumor
documentation. Open source LLMs show a strong potential for automating tumor
documentation. Models from 7-12 billion parameters could offer an optimal
balance between performance and resource efficiency. With tailored fine-tuning
and well-designed prompting, these models might become important tools for
clinical documentation in the future. The code for the evaluation is available
from https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset
as a new valuable resource that addresses the shortage of authentic and easily
accessible benchmarks in German-language medical NLP.

</details>


### [36] [JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models](https://arxiv.org/pdf/2501.14851)
*Michael K. Chen, Xikun Zhang, Dacheng Tao*

Main category: cs.CL

TL;DR: JustLogic is a synthetic deductive reasoning benchmark addressing flaws in existing benchmarks by being complex, prior-knowledge independent, and enabling in-depth error analysis. SOTA reasoning LLMs match human average but fall short of the ceiling, while non-reasoning models lag behind.


<details>
  <summary>Details</summary>
Motivation: Existing deductive reasoning benchmarks for LLMs are flawed due to low complexity, prior knowledge bias, and superficial error analysis. JustLogic aims to provide a rigorous evaluation tool.

Method: JustLogic is a synthetically generated benchmark with diverse linguistic patterns, vocabulary, and argument structures, designed to be prior-knowledge independent.

Result: SOTA reasoning LLMs perform at human average but below human ceiling; non-reasoning models underperform the human average.

Conclusion: JustLogic effectively evaluates LLMs' deductive reasoning, highlighting gaps between models and human performance.

Abstract: Logical reasoning is a critical component of Large Language Models (LLMs),
and substantial research efforts in recent years have aimed to enhance their
deductive reasoning capabilities. However, existing deductive reasoning
benchmarks, which are crucial for evaluating and advancing LLMs, are inadequate
due to their lack of task complexity, presence of prior knowledge as a
confounder, and superficial error analysis. To address these deficiencies, we
introduce JustLogic, a synthetically generated deductive reasoning benchmark
designed for rigorous evaluation of LLMs. JustLogic is (i) highly complex,
capable of generating a diverse range of linguistic patterns, vocabulary, and
argument structures; (ii) prior knowledge independent, eliminating the
advantage of models possessing prior knowledge and ensuring that only deductive
reasoning is used to answer questions; and (iii) capable of in-depth error
analysis on the heterogeneous effects of reasoning depth and argument form on
model accuracy. Our experimental results on JustLogic reveal that (i)
state-of-the-art (SOTA) reasoning LLMs perform on par or better than the human
average but significantly worse than the human ceiling, and (ii) SOTA
non-reasoning models still underperform the human average. All code and data
are available at https://github.com/michaelchen-lab/JustLogic

</details>


### [37] [AdaCoT: Rethinking Cross-Lingual Factual Reasoning through Adaptive Chain-of-Thought](https://arxiv.org/pdf/2501.16154)
*Xin Huang, Tarun Kumar Vangani, Zhengyuan Liu, Bowei Zou, Ai Ti Aw*

Main category: cs.CL

TL;DR: AdaCoT improves multilingual reasoning by dynamically routing thought processes in intermediary languages, enhancing performance, especially in low-resource languages.


<details>
  <summary>Details</summary>
Motivation: Address performance variability in multilingual models due to imbalanced training data and scalability issues in existing methods.

Method: Introduces AdaCoT, a framework with a language-agnostic core and adaptive reward-based mechanism for optimal reasoning pathways.

Result: Substantial improvements in factual reasoning quality and cross-lingual consistency, particularly in low-resource languages.

Conclusion: Adaptive reasoning paths bridge performance gaps between languages while preserving cultural and linguistic nuances.

Abstract: Large language models have shown impressive multilingual capabilities through
pretraining on diverse corpora. While these models show strong reasoning
abilities, their performance varies significantly across languages due to
imbalanced training data distribution. Existing approaches using sample-level
translation for extensive multilingual pretraining and cross-lingual tuning
face scalability challenges and often fail to capture nuanced reasoning
processes across languages. In this paper, we introduce AdaCoT (Adaptive
Chain-of-Thought), a framework that enhances multilingual factual reasoning by
dynamically routing thought processes in intermediary ``thinking languages''
before generating target-language responses. AdaCoT leverages a
language-agnostic core and incorporates an adaptive, reward-based mechanism for
selecting optimal reasoning pathways without requiring additional pretraining.
Our comprehensive evaluation across multiple benchmarks demonstrates
substantial improvements in both factual reasoning quality and cross-lingual
consistency, with particularly strong performance gains in low-resource
language settings. The results suggest that adaptive reasoning paths can
effectively bridge the performance gap between high and low-resource languages
while maintaining cultural and linguistic nuances.

</details>


### [38] [Estimating LLM Uncertainty with Evidence](https://arxiv.org/pdf/2502.00290)
*Huan Ma, Jingdong Chen, Joey Tianyi Zhou, Guangyu Wang, Changqing Zhang*

Main category: cs.CL

TL;DR: LogTokU is a framework for estimating token uncertainty in LLMs, addressing the limitations of probability-based methods by leveraging evidence strength information.


<details>
  <summary>Details</summary>
Motivation: LLMs often generate unreliable responses (hallucinations) due to lack of knowledge. Existing uncertainty estimation methods, especially probability-based ones, fail to accurately identify token reliability.

Method: The paper introduces LogTokU, which decouples token uncertainty by modeling evidence strength accumulated during training, enabling real-time estimation without multiple sampling.

Result: Experiments show LogTokU is effective and promising for improving token reliability estimation in LLMs.

Conclusion: LogTokU provides a practical solution for real-time uncertainty estimation in LLMs, enhancing their reliability in downstream tasks.

Abstract: Over the past few years, Large Language Models (LLMs) have developed rapidly
and are widely applied in various domains. However, LLMs face the issue of
hallucinations, generating responses that may be unreliable when the models
lack relevant knowledge. To be aware of potential hallucinations, uncertainty
estimation methods have been introduced, and most of them have confirmed that
reliability lies in critical tokens. However, probability-based methods perform
poorly in identifying token reliability, limiting their practical utility. In
this paper, we reveal that the probability-based method fails to estimate token
reliability due to the loss of evidence strength information which is
accumulated in the training stage. Therefore, we present Logits-induced token
uncertainty (LogTokU), a framework for estimating decoupled token uncertainty
in LLMs, enabling real-time uncertainty estimation without requiring multiple
sampling processes. We employ evidence modeling to implement LogTokU and use
the estimated uncertainty to guide downstream tasks. The experimental results
demonstrate that LogTokU has significant effectiveness and promise.

</details>


### [39] [Phonetic accommodation and inhibition in a dynamic neural field model](https://arxiv.org/pdf/2502.01210)
*Sam Kirkham, Patrycja Strycharczuk, Rob Davies, Danielle Welburn*

Main category: cs.CL

TL;DR: The paper explores how real-time phonetic input influences speech planning during accommodation, proposing a computational model with dual-layer planning/memory fields. It predicts convergence and divergence patterns, validated by empirical data, and links inhibitory memory dynamics to resistance in accommodation.


<details>
  <summary>Details</summary>
Motivation: To understand how phonetic input from another speaker shapes speech planning representations during accommodation, bridging gaps in dynamic neural field models and empirical observations.

Method: A computational model using dynamic neural field equations for movement planning and memory dynamics, tested against empirical data from a pilot study.

Result: The model predicts convergence and divergence patterns in accommodation, with empirical data suggesting inhibitory memory dynamics may reflect resistance due to phonological/sociolinguistic factors.

Conclusion: The findings highlight the role of inhibitory memory dynamics in phonetic accommodation and its implications for understanding sound change.

Abstract: Short-term phonetic accommodation is a fundamental driver behind accent
change, but how does real-time input from another speaker's voice shape the
speech planning representations of an interlocutor? We advance a computational
model of change in speech planning representations during phonetic
accommodation, grounded in dynamic neural field equations for movement planning
and memory dynamics. A dual-layer planning/memory field predicts that
convergence to a model talker on one trial can trigger divergence on subsequent
trials, due to a delayed inhibitory effect in the more slowly evolving memory
field. The model's predictions are compared with empirical patterns of
accommodation from an experimental pilot study. We show that observed empirical
phenomena may correspond to variation in the magnitude of inhibitory memory
dynamics, which could reflect resistance to accommodation due to phonological
and/or sociolinguistic pressures. We discuss the implications of these results
for the relations between short-term phonetic accommodation and sound change.

</details>


### [40] [The Order Effect: Investigating Prompt Sensitivity to Input Order in LLMs](https://arxiv.org/pdf/2502.04134)
*Bryan Guan, Tanya Roosta, Peyman Passban, Mehdi Rezagholizadeh*

Main category: cs.CL

TL;DR: The paper examines order sensitivity in LLMs, showing that input arrangement affects output consistency and accuracy, with few-shot prompting offering limited mitigation.


<details>
  <summary>Details</summary>
Motivation: To assess the reliability of LLMs under varying input conditions, focusing on order sensitivity as a key issue impacting consistency and bias.

Method: Experiments across tasks like paraphrasing, relevance judgment, and multiple-choice questions, testing shuffled inputs and few-shot prompting.

Result: Input order significantly impacts performance, with shuffled inputs reducing accuracy. Few-shot prompting helps partially but doesn't fully resolve the issue.

Conclusion: Order sensitivity remains a risk, especially in high-stakes applications, calling for more robust LLMs or better input-handling techniques.

Abstract: As large language models (LLMs) become integral to diverse applications,
ensuring their reliability under varying input conditions is crucial. One key
issue affecting this reliability is order sensitivity, wherein slight
variations in the input arrangement can lead to inconsistent or biased outputs.
Although recent advances have reduced this sensitivity, the problem remains
unresolved. This paper investigates the extent of order sensitivity in LLMs
whose internal components are hidden from users (such as closed-source models
or those accessed via API calls). We conduct experiments across multiple tasks,
including paraphrasing, relevance judgment, and multiple-choice questions. Our
results show that input order significantly affects performance across tasks,
with shuffled inputs leading to measurable declines in output accuracy.
Few-shot prompting demonstrates mixed effectiveness and offers partial
mitigation; however, fails to fully resolve the problem. These findings
highlight persistent risks, particularly in high-stakes applications, and point
to the need for more robust LLMs or improved input-handling techniques in
future development.

</details>


### [41] [k-LLMmeans: Scalable, Stable, and Interpretable Text Clustering via LLM-based Centroids](https://arxiv.org/pdf/2502.09667)
*Jairo Diaz-Rodriguez*

Main category: cs.CL

TL;DR: k-LLMmeans is a modified k-means algorithm for text clustering using LLM-generated summaries as centroids, improving semantic interpretability and scalability while reducing LLM usage.


<details>
  <summary>Details</summary>
Motivation: Existing methods for text clustering often miss semantic nuances or face scalability issues with LLMs. k-LLMmeans aims to address these limitations.

Method: The algorithm uses LLM-generated summaries as centroids, preserving k-means optimization while enhancing semantics. A mini-batch variant is introduced for streaming text.

Result: k-LLMmeans outperforms traditional baselines and matches state-of-the-art LLM-based clustering with fewer LLM calls.

Conclusion: The method is efficient, scalable, and interpretable, with potential applications in real-time text-stream clustering.

Abstract: We introduce k-LLMmeans, a novel modification of the k-means algorithm for
text clustering that leverages LLM-generated summaries as cluster centroids,
capturing semantic nuances often missed by purely numerical averages. This
design preserves the core optimization properties of k-means while enhancing
semantic interpretability and avoiding the scalability and instability issues
typical of modern LLM-based clustering. Unlike existing methods, our approach
does not increase LLM usage with dataset size and produces transparent
intermediate outputs. We further extend it with a mini-batch variant for
efficient, real-time clustering of streaming text. Extensive experiments across
multiple datasets, embeddings, and LLMs show that k-LLMmeans consistently
outperforms k-means and other traditional baselines and achieves results
comparable to state-of-the-art LLM-based clustering, with a fraction of the LLM
calls. Finally, we present a case study on sequential text streams and
introduce a new benchmark dataset constructed from StackExchange to evaluate
text-stream clustering methods.

</details>


### [42] [English Please: Evaluating Machine Translation with Large Language Models for Multilingual Bug Reports](https://arxiv.org/pdf/2502.14338)
*Avinash Patil, Siru Tao, Aryan Jadon*

Main category: cs.CL

TL;DR: The study evaluates machine translation (MT) performance on bug reports, comparing tools like DeepL, AWS Translate, and LLMs (ChatGPT, Claude, Gemini, LLaMA, Mistral) using metrics like BLEU, BERTScore, and classification scores. ChatGPT excels in translation quality, while Claude and Mistral lead in F1-score for source language identification. AWS Translate is most accurate in language identification. No single tool dominates all tasks.


<details>
  <summary>Details</summary>
Motivation: Accurate translation of bug reports is essential for global software development collaboration, but MT performance on technical content like bug reports is understudied.

Method: Evaluated MT tools (DeepL, AWS Translate, ChatGPT, Claude, Gemini, LLaMA, Mistral) using bug reports from Visual Studio Code GitHub (english-please tag). Metrics: BLEU, BERTScore, COMET, METEOR, ROUGE for translation; accuracy, precision, recall, F1-score for language identification.

Result: ChatGPT (gpt-4o) best for translation quality. Claude and Mistral top F1-scores (0.7182, 0.7142) for language identification. Gemini has best precision (0.7414). AWS Translate most accurate (0.4717) in language ID.

Conclusion: No single MT tool excels in all tasks; task-specific evaluation is crucial. Domain adaptation is needed for technical content. Insights provided for integrating MT into bug-triaging workflows.

Abstract: Accurate translation of bug reports is critical for efficient collaboration
in global software development. In this study, we conduct the first
comprehensive evaluation of machine translation (MT) performance on bug
reports, analyzing the capabilities of DeepL, AWS Translate, and large language
models such as ChatGPT, Claude, Gemini, LLaMA, and Mistral using data from the
Visual Studio Code GitHub repository, specifically focusing on reports labeled
with the english-please tag. To assess both translation quality and source
language identification accuracy, we employ a range of MT evaluation
metrics-including BLEU, BERTScore, COMET, METEOR, and ROUGE-alongside
classification metrics such as accuracy, precision, recall, and F1-score. Our
findings reveal that while ChatGPT (gpt-4o) excels in semantic and lexical
translation quality, it does not lead in source language identification. Claude
and Mistral achieve the highest F1-scores (0.7182 and 0.7142, respectively),
and Gemini records the best precision (0.7414). AWS Translate shows the highest
accuracy (0.4717) in identifying source languages. These results highlight that
no single system dominates across all tasks, reinforcing the importance of
task-specific evaluations. This study underscores the need for domain
adaptation when translating technical content and provides actionable insights
for integrating MT into bug-triaging workflows. The code and dataset for this
paper are available at GitHub-https://github.com/av9ash/English-Please

</details>


### [43] [Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix Factorization](https://arxiv.org/pdf/2502.20364)
*Ryan C. Barron, Maksim E. Eren, Olga M. Serafimova, Cynthia Matuszek, Boian S. Alexandrov*

Main category: cs.CL

TL;DR: A generative AI system integrating RAG, VS, and KG via NMF enhances legal information retrieval, minimizes hallucinations, and improves reasoning in complex legal datasets.


<details>
  <summary>Details</summary>
Motivation: The legal domain's complex, interrelated data requires advanced AI to extract insights, navigate relationships, and predict trends for justice and efficiency.

Method: The system uses web scraping to collect legal texts, integrates RAG, VS, and KG via NMF, and leverages semantic representations and topic discovery.

Result: Enables scalable, interpretable legal document clustering, summarization, and cross-referencing, improving retrieval and reasoning.

Conclusion: The framework advances computational law and AI by bridging keyword searches with contextual understanding for semi-structured legal data.

Abstract: Agentic Generative AI, powered by Large Language Models (LLMs) with
Retrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores
(VSs), represents a transformative technology applicable to specialized domains
such as legal systems, research, recommender systems, cybersecurity, and global
security, including proliferation research. This technology excels at inferring
relationships within vast unstructured or semi-structured datasets. The legal
domain here comprises complex data characterized by extensive, interrelated,
and semi-structured knowledge systems with complex relations. It comprises
constitutions, statutes, regulations, and case law. Extracting insights and
navigating the intricate networks of legal documents and their relations is
crucial for effective legal research. Here, we introduce a generative AI system
that integrates RAG, VS, and KG, constructed via Non-Negative Matrix
Factorization (NMF), to enhance legal information retrieval and AI reasoning
and minimize hallucinations. In the legal system, these technologies empower AI
agents to identify and analyze complex connections among cases, statutes, and
legal precedents, uncovering hidden relationships and predicting legal
trends-challenging tasks that are essential for ensuring justice and improving
operational efficiency. Our system employs web scraping techniques to
systematically collect legal texts, such as statutes, constitutional
provisions, and case law, from publicly accessible platforms like Justia. It
bridges the gap between traditional keyword-based searches and contextual
understanding by leveraging advanced semantic representations, hierarchical
relationships, and latent topic discovery. This framework supports legal
document clustering, summarization, and cross-referencing, for scalable,
interpretable, and accurate retrieval for semi-structured data while advancing
computational law and AI.

</details>


### [44] [ConvoGen: Enhancing Conversational AI with Synthetic Data: A Multi-Agent Approach](https://arxiv.org/pdf/2503.17460)
*Reem Gody, Mahmoud Goudy, Ahmed Y. Tawfik*

Main category: cs.CL

TL;DR: ConvoGen is a framework for generating synthetic conversational data using multi-agent systems, leveraging few-shot learning and iterative sampling for diverse, realistic scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the need for high-quality synthetic conversational data for training and evaluating conversational AI models.

Method: Uses multi-agent systems, few-shot learning, and iterative sampling from a dynamically updated few-shot hub.

Result: Produces high-quality, diverse synthetic conversational data.

Conclusion: ConvoGen enhances the development and evaluation of conversational AI systems.

Abstract: In this paper, we present ConvoGen: an innovative framework for generating
synthetic conversational data using multi-agent systems. Our method leverages
few-shot learning and introduces iterative sampling from a dynamically updated
few-shot hub to create diverse and realistic conversational scenarios. The
generated data has numerous applications, including training and evaluating
conversational AI models, and augmenting existing datasets for tasks like
conversational intent classification or conversation summarization. Our
experiments demonstrate the effectiveness of this method in producing
high-quality diverse synthetic conversational data, highlighting its potential
to enhance the development and evaluation of conversational AI systems.

</details>


### [45] [Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation](https://arxiv.org/pdf/2504.17480)
*Xin Yi, Yue Li, Shunfan Zheng, Linlin Wang, Xiaoling Wang, Liang He*

Main category: cs.CL

TL;DR: Watermark radioactivity in LLMs allows watermark inheritance in student models, enabling detection of unauthorized distillation. CDG-KD is proposed for bidirectional attacks (removal and forgery) under unauthorized distillation, highlighting the need for robust watermarks.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored robustness and unforgeability of watermarks in LLMs against scrubbing and spoofing attacks during unauthorized knowledge distillation.

Method: Proposes CDG-KD, a framework using contrastive decoding to extract corrupted/amplified watermarks and bidirectional distillation for watermark removal/forgery.

Result: CDG-KD effectively performs attacks while maintaining model performance, demonstrating vulnerabilities in current watermarking schemes.

Conclusion: The study emphasizes the necessity for developing more robust and unforgeable watermarking techniques in LLMs.

Abstract: Watermarking has emerged as a critical technique for combating misinformation
and protecting intellectual property in large language models (LLMs). A recent
discovery, termed watermark radioactivity, reveals that watermarks embedded in
teacher models can be inherited by student models through knowledge
distillation. On the positive side, this inheritance allows for the detection
of unauthorized knowledge distillation by identifying watermark traces in
student models. However, the robustness of watermarks against scrubbing attacks
and their unforgeability in the face of spoofing attacks under unauthorized
knowledge distillation remain largely unexplored. Existing watermark attack
methods either assume access to model internals or fail to simultaneously
support both scrubbing and spoofing attacks. In this work, we propose
Contrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified
framework that enables bidirectional attacks under unauthorized knowledge
distillation. Our approach employs contrastive decoding to extract corrupted or
amplified watermark texts via comparing outputs from the student model and
weakly watermarked references, followed by bidirectional distillation to train
new student models capable of watermark removal and watermark forgery,
respectively. Extensive experiments show that CDG-KD effectively performs
attacks while preserving the general performance of the distilled model. Our
findings underscore critical need for developing watermarking schemes that are
robust and unforgeable.

</details>


### [46] [RWKV-X: A Linear Complexity Hybrid Language Model](https://arxiv.org/pdf/2504.21463)
*Haowen Hou, Zhiyi Huang, Kaifeng Tan, Rongchang Lu, Fei Richard Yu*

Main category: cs.CL

TL;DR: RWKV-X is a hybrid architecture combining RWKV's efficiency for short-range modeling with a sparse attention mechanism for long-range context, achieving linear-time training and constant-time inference.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of full attention layers in hybrid models while capturing long-range context.

Method: Combines RWKV for short-range modeling with a sparse attention mechanism, enabling linear-time training and constant-time inference.

Result: Achieves near-perfect accuracy on 64K-token sequences, outperforms RWKV-7 on long-context tasks, and maintains strong short-context performance.

Conclusion: RWKV-X is a scalable, efficient backbone for general-purpose language modeling, capable of handling sequences up to 1 million tokens.

Abstract: In this paper, we introduce RWKV-X, a novel hybrid architecture that combines
the efficiency of RWKV for short-range modeling with a sparse attention
mechanism designed to capture long-range context. Unlike previous hybrid
approaches that rely on full attention layers and retain quadratic complexity,
RWKV-X achieves linear-time complexity in training and constant-time complexity
in inference decoding. We demonstrate that RWKV-X, when continually pretrained
on 64K-token sequences, achieves near-perfect accuracy on the 64K passkey
retrieval benchmark. It consistently outperforms prior RWKV-7 models on
long-context benchmarks, while maintaining strong performance on short-context
tasks. These results highlight RWKV-X as a scalable and efficient backbone for
general-purpose language modeling, capable of decoding sequences up to 1
million tokens with stable speed and memory usage. To facilitate further
research and analysis, we have made the checkpoints and the associated code
publicly accessible at: https://github.com/howard-hou/RWKV-X.

</details>


### [47] [Steering Large Language Models with Register Analysis for Arbitrary Style Transfer](https://arxiv.org/pdf/2505.00679)
*Xinchen Yang, Marine Carpuat*

Main category: cs.CL

TL;DR: A prompting method using register analysis improves LLM-based style transfer, outperforming existing strategies in preserving meaning and enhancing style.


<details>
  <summary>Details</summary>
Motivation: The challenge of guiding LLMs for high-quality example-based arbitrary style transfer, particularly in describing exemplar styles.

Method: Proposes a prompting method based on register analysis to guide LLMs in style transfer tasks.

Result: Empirical evaluations show the method enhances style transfer strength and preserves meaning better than existing strategies.

Conclusion: The proposed prompting approach effectively addresses the challenge of style transfer in LLMs.

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
rewriting text across various styles. However, effectively leveraging this
ability for example-based arbitrary style transfer, where an input text is
rewritten to match the style of a given exemplar, remains an open challenge. A
key question is how to describe the style of the exemplar to guide LLMs toward
high-quality rewrites. In this work, we propose a prompting method based on
register analysis to guide LLMs to perform this task. Empirical evaluations
across multiple style transfer tasks show that our prompting approach enhances
style transfer strength while preserving meaning more effectively than existing
prompting strategies.

</details>


### [48] [Bielik 11B v2 Technical Report](https://arxiv.org/pdf/2505.02410)
*Krzysztof Ociepa, Łukasz Flis, Krzysztof Wróbel, Adrian Gwoździej, Remigiusz Kinas*

Main category: cs.CL

TL;DR: Bielik 11B v2 is a Polish-optimized language model based on Mistral 7B, scaled to 11B parameters. It introduces Weighted Instruction Cross-Entropy Loss and Adaptive Learning Rate, outperforming larger models and setting new benchmarks for Polish AI.


<details>
  <summary>Details</summary>
Motivation: To advance Polish language AI capabilities and improve performance in Polish text processing while maintaining cross-lingual efficiency.

Method: Built on Mistral 7B v0.2, scaled to 11B parameters using depth up-scaling. Introduces Weighted Instruction Cross-Entropy Loss and Adaptive Learning Rate for optimized training.

Result: Outperforms larger models (2-6x parameters) and surpasses specialized Polish models in linguistic and reasoning tasks. Efficient deployment via quantization.

Conclusion: Bielik 11B v2 sets new benchmarks for Polish language AI and resource-efficient modeling in underrepresented languages.

Abstract: We present Bielik 11B v2, a state-of-the-art language model optimized for
Polish text processing. Built on the Mistral 7B v0.2 architecture and scaled to
11B parameters using depth up-scaling, this model demonstrates exceptional
performance across Polish language benchmarks while maintaining strong
cross-lingual capabilities. We introduce two key technical innovations:
Weighted Instruction Cross-Entropy Loss, which optimizes learning across
diverse instruction types by assigning quality-based weights to training
examples, and Adaptive Learning Rate, which dynamically adjusts based on
context length. Comprehensive evaluation across multiple benchmarks
demonstrates that Bielik 11B v2 outperforms many larger models, including those
with 2-6 times more parameters, and significantly surpasses other specialized
Polish language models on tasks ranging from linguistic understanding to
complex reasoning. The model's parameter efficiency and extensive quantization
options enable deployment across various hardware configurations, advancing
Polish language AI capabilities and establishing new benchmarks for
resource-efficient language modeling in less-represented languages.

</details>


### [49] [ReplaceMe: Network Simplification via Layer Pruning and Linear Transformations](https://arxiv.org/pdf/2505.02819)
*Dmitriy Shopkhoev, Ammar Ali, Magauiya Zhussip, Valentin Malykh, Stamatios Lefkimmiatis, Nikos Komodakis, Sergey Zagoruyko*

Main category: cs.CL

TL;DR: ReplaceMe is a training-free depth pruning method for transformers, replacing blocks with linear operations while maintaining performance. It outperforms other training-free methods and competes with retraining-based approaches.


<details>
  <summary>Details</summary>
Motivation: To simplify pruning by eliminating the need for additional training or fine-tuning, reducing computational overhead.

Method: Uses a small calibration dataset to estimate a linear transformation for approximating pruned blocks, merging it seamlessly with remaining blocks.

Result: Achieves up to 25% pruning with ~90% performance retention on LLMs, outperforming training-free methods and competing with retraining-based ones.

Conclusion: ReplaceMe offers an efficient, training-free pruning solution with minimal overhead, validated on large language models.

Abstract: We introduce ReplaceMe, a generalized training-free depth pruning method that
effectively replaces transformer blocks with a linear operation, while
maintaining high performance for low compression ratios. In contrast to
conventional pruning approaches that require additional training or
fine-tuning, our approach requires only a small calibration dataset that is
used to estimate a linear transformation to approximate the pruned blocks. This
estimated linear mapping can be seamlessly merged with the remaining
transformer blocks, eliminating the need for any additional network parameters.
Our experiments show that ReplaceMe consistently outperforms other
training-free approaches and remains highly competitive with state-of-the-art
pruning methods that involve extensive retraining/fine-tuning and architectural
modifications. Applied to several large language models (LLMs), ReplaceMe
achieves up to 25% pruning while retaining approximately 90% of the original
model's performance on open benchmarks - without any training or healing steps,
resulting in minimal computational overhead (see Fig.1). We provide an
open-source library implementing ReplaceMe alongside several state-of-the-art
depth pruning techniques, available at this repository.

</details>


### [50] [Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models](https://arxiv.org/pdf/2505.02847)
*Bang Zhang, Ruotian Ma, Qingxuan Jiang, Peisong Wang, Jiaqi Chen, Zheng Xie, Xingyu Chen, Yue Wang, Fanghua Ye, Jian Li, Yifan Yang, Zhaopeng Tu, Xiaolong Li*

Main category: cs.CL

TL;DR: SAGE is an automated framework to evaluate LLMs' social cognition by simulating human-like emotions and thoughts, revealing gaps in empathy and social skills not captured by traditional metrics.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of assessing LLMs' human-like understanding beyond text, focusing on social cognition and empathy.

Method: SAGE uses a Sentient Agent to simulate emotional changes and inner thoughts during multi-turn conversations, generating emotion trajectories and interpretable reasoning.

Result: SAGE's emotion scores correlate strongly with psychological metrics (BLRI, empathy), and it reveals significant gaps in social skills among LLMs (up to 4x).

Conclusion: SAGE offers a scalable, interpretable tool for advancing empathetic and socially adept language agents, beyond conventional evaluation methods.

Abstract: Assessing how well a large language model (LLM) understands human, rather
than merely text, remains an open challenge. To bridge the gap, we introduce
Sentient Agent as a Judge (SAGE), an automated evaluation framework that
measures an LLM's higher-order social cognition. SAGE instantiates a Sentient
Agent that simulates human-like emotional changes and inner thoughts during
interaction, providing a more realistic evaluation of the tested model in
multi-turn conversations. At every turn, the agent reasons about (i) how its
emotion changes, (ii) how it feels, and (iii) how it should reply, yielding a
numerical emotion trajectory and interpretable inner thoughts. Experiments on
100 supportive-dialogue scenarios show that the final Sentient emotion score
correlates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings
and utterance-level empathy metrics, validating psychological fidelity. We also
build a public Sentient Leaderboard covering 18 commercial and open-source
models that uncovers substantial gaps (up to 4x) between frontier systems
(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in
conventional leaderboards (e.g., Arena). SAGE thus provides a principled,
scalable and interpretable tool for tracking progress toward genuinely
empathetic and socially adept language agents.

</details>


### [51] [G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness](https://arxiv.org/pdf/2505.05026)
*Jaehyun Jeon, Jang Han Yoon, Min Soo Kim, Sumin Shim, Yejin Choi, Hanbin Kim, Youngjae Yu*

Main category: cs.CL

TL;DR: WiserUI-Bench and G-FOCUS improve UI persuasiveness assessment using VLMs, reducing reliance on costly A/B testing.


<details>
  <summary>Details</summary>
Motivation: Current UI evaluation methods like A/B testing are expensive and time-consuming, while VLMs lack focus on comparative persuasiveness.

Method: Introduces WiserUI-Bench (300 UI pairs with A/B labels) and G-FOCUS, an inference-time strategy to enhance VLM accuracy.

Result: G-FOCUS outperforms existing methods in consistency and accuracy for UI evaluation.

Conclusion: The approach complements A/B testing, advancing scalable UI design optimization.

Abstract: Evaluating user interface (UI) design effectiveness extends beyond aesthetics
to influencing user behavior, a principle central to Design Persuasiveness. A/B
testing is the predominant method for determining which UI variations drive
higher user engagement, but it is costly and time-consuming. While recent
Vision-Language Models (VLMs) can process automated UI analysis, current
approaches focus on isolated design attributes rather than comparative
persuasiveness-the key factor in optimizing user interactions. To address this,
we introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design
Persuasiveness Assessment task, featuring 300 real-world UI image pairs labeled
with A/B test results and expert rationales. Additionally, we propose G-FOCUS,
a novel inference-time reasoning strategy that enhances VLM-based
persuasiveness assessment by reducing position bias and improving evaluation
accuracy. Experimental results show that G-FOCUS surpasses existing inference
strategies in consistency and accuracy for pairwise UI evaluation. Through
promoting VLM-driven evaluation of UI persuasiveness, our work offers an
approach to complement A/B testing, propelling progress in scalable UI
preference modeling and design optimization. Code and data will be released
publicly.

</details>


### [52] [LiTransProQA: an LLM-based Literary Translation evaluation metric with Professional Question Answering](https://arxiv.org/pdf/2505.05423)
*Ran Zhang, Wei Zhao, Lieve Macken, Steffen Eger*

Main category: cs.CL

TL;DR: LiTransProQA is a reference-free, LLM-based framework for evaluating literary translation, outperforming existing metrics by integrating professional translator insights.


<details>
  <summary>Details</summary>
Motivation: Current metrics favor mechanical accuracy over artistic expression, risking translation quality and cultural authenticity.

Method: LiTransProQA uses a question-answering framework, incorporating professional insights on literary devices, cultural understanding, and authorial voice.

Result: LiTransProQA surpasses state-of-the-art metrics, achieving up to 0.07 gain in correlation and 15+ points in adequacy assessments.

Conclusion: LiTransProQA approaches human-level performance, is applicable to open-source models, and serves as a valuable tool for literary evaluation.

Abstract: The impact of Large Language Models (LLMs) has extended into literary
domains. However, existing evaluation metrics prioritize mechanical accuracy
over artistic expression and tend to overrate machine translation (MT) as being
superior to experienced professional human translation. In the long run, this
bias could result in a permanent decline in translation quality and cultural
authenticity. In response to the urgent need for a specialized literary
evaluation metric, we introduce LiTransProQA, a novel, reference-free,
LLM-based question-answering framework designed specifically for literary
translation evaluation. LiTransProQA uniquely integrates insights from
professional literary translators and researchers, focusing on critical
elements in literary quality assessment such as literary devices, cultural
understanding, and authorial voice. Our extensive evaluation shows that while
literary-finetuned XCOMET-XL yields marginal gains, LiTransProQA substantially
outperforms current metrics, achieving up to 0.07 gain in correlation (ACC-EQ
and Kendall's tau) and surpassing the best state-of-the-art metrics by over 15
points in adequacy assessments. Incorporating professional translator insights
as weights further improves performance, highlighting the value of translator
inputs. Notably, LiTransProQA approaches human-level evaluation performance
comparable to trained linguistic annotators. It demonstrates broad
applicability to open-source models such as LLaMA3.3-70b and Qwen2.5-32b,
indicating its potential as an accessible and training-free literary evaluation
metric and a valuable tool for evaluating texts that require local processing
due to copyright or ethical considerations.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [53] [Data extraction and processing methods to aid the study of driving behaviors at intersections in naturalistic driving](https://arxiv.org/pdf/2505.05487)
*Shrinivas Pundlik, Seonggyu Choe, Patrick Baker, Chen-Yuan Lee, Naser Al-Madi, Alex R. Bowers, Gang Luo*

Main category: cs.CV

TL;DR: Automated methods for analyzing driver head scans and intersection data from naturalistic driving studies, achieving high accuracy in detecting signage, maneuvers, and intersection bounds.


<details>
  <summary>Details</summary>
Motivation: To automate the processing of diverse and extensive data from naturalistic driving studies, focusing on driver behavior and intersection dynamics.

Method: Custom tools and AI models (head pose detection, YOLO object detection) were used to process cabin and scene videos, synchronize data, and infer intersection details.

Result: High accuracy in detecting intersection signage (100%) and maneuvers (94%), with precise measurements for vehicle entry and intersection bounds.

Conclusion: The automated system effectively processes naturalistic driving data, providing reliable insights into driver behavior and intersection dynamics.

Abstract: Naturalistic driving studies use devices in participants' own vehicles to
record daily driving over many months. Due to diverse and extensive amounts of
data recorded, automated processing is necessary. This report describes methods
to extract and characterize driver head scans at intersections from data
collected from an in-car recording system that logged vehicle speed, GPS
location, scene videos, and cabin videos. Custom tools were developed to mark
the intersections, synchronize location and video data, and clip the cabin and
scene videos for +/-100 meters from the intersection location. A
custom-developed head pose detection AI model for wide angle head turns was run
on the cabin videos to estimate the driver head pose, from which head scans >20
deg were computed in the horizontal direction. The scene videos were processed
using a YOLO object detection model to detect traffic lights, stop signs,
pedestrians, and other vehicles on the road. Turning maneuvers were
independently detected using vehicle self-motion patterns. Stop lines on the
road surface were detected using changing intensity patterns over time as the
vehicle moved. The information obtained from processing the scene videos, along
with the speed data was used in a rule-based algorithm to infer the
intersection type, maneuver, and bounds. We processed 190 intersections from 3
vehicles driven in cities and suburban areas from Massachusetts and California.
The automated video processing algorithm correctly detected intersection
signage and maneuvers in 100% and 94% of instances, respectively. The median
[IQR] error in detecting vehicle entry into the intersection was 1.1[0.4-4.9]
meters and 0.2[0.1-0.54] seconds. The median overlap between ground truth and
estimated intersection bounds was 0.88[0.82-0.93].

</details>


### [54] [From Events to Enhancement: A Survey on Event-Based Imaging Technologies](https://arxiv.org/pdf/2505.05488)
*Yunfan Lu, Xiaogang Xu, Pengteng Li, Yusheng Wang, Yi Cui, Huizai Yao, Hui Xiong*

Main category: cs.CV

TL;DR: A survey on event cameras, covering their physical model, advancements in imaging tasks, and challenges in the field.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive study of event cameras' benefits and limitations for universal imaging applications.

Method: Introduces a physical model of event sensors, reviews advancements in image/video enhancement, and explores advanced tasks like light field estimation.

Result: Highlights the potential of event cameras in various imaging tasks but notes gaps in broader understanding and challenges.

Conclusion: The survey offers insights into the evolving field of event imaging and identifies open questions for future research.

Abstract: Event cameras offering high dynamic range and low latency have emerged as
disruptive technologies in imaging. Despite growing research on leveraging
these benefits for different imaging tasks, a comprehensive study of recently
advances and challenges are still lacking. This limits the broader
understanding of how to utilize events in universal imaging applications. In
this survey, we first introduce a physical model and the characteristics of
different event sensors as the foundation. Following this, we highlight the
advancement and interaction of image/video enhancement tasks with events.
Additionally, we explore advanced tasks, which capture richer light information
with events, \eg~light field estimation, multi-view generation, and
photometric. Finally, we discuss new challenges and open questions offering a
perspective for this rapidly evolving field. More continuously updated
resources are at this link: https://github.com/yunfanLu/Awesome-Event-Imaging

</details>


### [55] [MDDFNet: Mamba-based Dynamic Dual Fusion Network for Traffic Sign Detection](https://arxiv.org/pdf/2505.05491)
*TianYi Yu*

Main category: cs.CV

TL;DR: MDDFNet, a novel network for traffic sign detection, addresses feature extraction and scale handling challenges with dynamic dual fusion and a Mamba-based backbone, outperforming state-of-the-art models on TT100K datasets.


<details>
  <summary>Details</summary>
Motivation: Challenges in detecting small objects like traffic signs include singular feature extraction and ineffective handling of varying object sizes.

Method: MDDFNet integrates a dynamic dual fusion module for diverse feature extraction and a Mamba-based backbone for adaptive feature fusion.

Result: MDDFNet achieves superior performance on TT100K datasets, maintaining real-time processing.

Conclusion: MDDFNet effectively detects small traffic signs, addressing key challenges in object detection.

Abstract: The Detection of small objects, especially traffic signs, is a critical
sub-task in object detection and autonomous driving. Despite signficant
progress in previous research, two main challenges remain. First, the issue of
feature extraction being too singular. Second, the detection process struggles
to efectively handle objects of varying sizes or scales. These problems are
also prevalent in general object detection tasks. To address these challenges,
we propose a novel object detection network, Mamba-based Dynamic Dual Fusion
Network (MDDFNet), for traffic sign detection. The network integrates a dynamic
dual fusion module and a Mamba-based backbone to simultaneously tackle the
aforementioned issues. Specifically, the dynamic dual fusion module utilizes
multiple branches to consolidate various spatial and semantic information, thus
enhancing feature diversity. The Mamba-based backbone leverages global feature
fusion and local feature interaction, combining features in an adaptive manner
to generate unique classification characteristics. Extensive experiments
conducted on the TT100K (Tsinghua-Tencent 100K) datasets demonstrate that
MDDFNet outperforms other state-of-the-art detectors, maintaining real-time
processing capabilities of single-stage models while achieving superior
performance. This confirms the efectiveness of MDDFNet in detecting small
traffic signs.

</details>


### [56] [DetoxAI: a Python Toolkit for Debiasing Deep Learning Models in Computer Vision](https://arxiv.org/pdf/2505.05492)
*Ignacy Stępka, Lukasz Sztukiewicz, Michał Wiliński, Jerzy Stefanowski*

Main category: cs.CV

TL;DR: DetoxAI is an open-source Python library for improving fairness in deep learning vision classifiers through post-hoc debiasing, offering debiasing algorithms, fairness metrics, and visualization tools.


<details>
  <summary>Details</summary>
Motivation: Existing fairness solutions focus on tabular data and are poorly suited for vision-based tasks, creating a gap DetoxAI aims to bridge.

Method: DetoxAI implements post-hoc debiasing via interventions in internal representations, along with attribution-based visualization and fairness metrics.

Result: The library provides tools to mitigate bias in vision classifiers, demonstrating tangible value for engineers and researchers.

Conclusion: DetoxAI addresses the gap in fairness solutions for vision tasks, offering practical tools for debiasing deep learning models.

Abstract: While machine learning fairness has made significant progress in recent
years, most existing solutions focus on tabular data and are poorly suited for
vision-based classification tasks, which rely heavily on deep learning. To
bridge this gap, we introduce DetoxAI, an open-source Python library for
improving fairness in deep learning vision classifiers through post-hoc
debiasing. DetoxAI implements state-of-the-art debiasing algorithms, fairness
metrics, and visualization tools. It supports debiasing via interventions in
internal representations and includes attribution-based visualization tools and
quantitative algorithmic fairness metrics to show how bias is mitigated. This
paper presents the motivation, design, and use cases of DetoxAI, demonstrating
its tangible value to engineers and researchers.

</details>


### [57] [Learning 3D Persistent Embodied World Models](https://arxiv.org/pdf/2505.05495)
*Siyuan Zhou, Yilun Du, Yuncong Yang, Lei Han, Peihao Chen, Dit-Yan Yeung, Chuang Gan*

Main category: cs.CV

TL;DR: A persistent embodied world model with memory improves long-horizon simulation for intelligent agents by aggregating predicted RGB-D video into a 3D map.


<details>
  <summary>Details</summary>
Motivation: Existing video-based world models lack memory, hindering consistent long-horizon planning in partially observed environments.

Method: Introduces a video diffusion model predicting RGB-D future observations, aggregated into a persistent 3D map for conditioning.

Result: Enables faithful simulation of seen and unseen world parts, improving planning and policy learning.

Conclusion: The persistent world model enhances long-horizon simulation and downstream embodied applications.

Abstract: The ability to simulate the effects of future actions on the world is a
crucial ability of intelligent embodied agents, enabling agents to anticipate
the effects of their actions and make plans accordingly. While a large body of
existing work has explored how to construct such world models using video
models, they are often myopic in nature, without any memory of a scene not
captured by currently observed images, preventing agents from making consistent
long-horizon plans in complex environments where many parts of the scene are
partially observed. We introduce a new persistent embodied world model with an
explicit memory of previously generated content, enabling much more consistent
long-horizon simulation. During generation time, our video diffusion model
predicts RGB-D video of the future observations of the agent. This generation
is then aggregated into a persistent 3D map of the environment. By conditioning
the video model on this 3D spatial map, we illustrate how this enables video
world models to faithfully simulate both seen and unseen parts of the world.
Finally, we illustrate the efficacy of such a world model in downstream
embodied applications, enabling effective planning and policy learning.

</details>


### [58] [Preliminary Explorations with GPT-4o(mni) Native Image Generation](https://arxiv.org/pdf/2505.05501)
*Pu Cao, Feng Zhou, Junyi Ji, Qingye Kong, Zhixiang Lv, Mingjian Zhang, Xuekun Zhao, Siqi Wu, Yinghui Lin, Qing Song, Lu Yang*

Main category: cs.CV

TL;DR: GPT-4o demonstrates strong multimodal generation capabilities but has limitations in spatial reasoning, temporal prediction, and domain-specific tasks.


<details>
  <summary>Details</summary>
Motivation: To explore GPT-4o's capabilities across diverse tasks, assessing its multimodal understanding and generation quality.

Method: Constructed a task taxonomy and curated test samples for qualitative evaluation across six task categories.

Result: GPT-4o excels in general-purpose tasks like text-to-image generation but struggles with precise spatial reasoning, temporal consistency, and domain-specific accuracy.

Conclusion: GPT-4o advances unified multimodal generation but isn't yet reliable for professional or safety-critical applications.

Abstract: Recently, the visual generation ability by GPT-4o(mni) has been unlocked by
OpenAI. It demonstrates a very remarkable generation capability with excellent
multimodal condition understanding and varied task instructions. In this paper,
we aim to explore the capabilities of GPT-4o across various tasks. Inspired by
previous study, we constructed a task taxonomy along with a carefully curated
set of test samples to conduct a comprehensive qualitative test. Benefiting
from GPT-4o's powerful multimodal comprehension, its image-generation process
demonstrates abilities surpassing those of traditional image-generation tasks.
Thus, regarding the dimensions of model capabilities, we evaluate its
performance across six task categories: traditional image generation tasks,
discriminative tasks, knowledge-based generation, commonsense-based generation,
spatially-aware image generation, and temporally-aware image generation. These
tasks not only assess the quality and conditional alignment of the model's
outputs but also probe deeper into GPT-4o's understanding of real-world
concepts. Our results reveal that GPT-4o performs impressively well in
general-purpose synthesis tasks, showing strong capabilities in text-to-image
generation, visual stylization, and low-level image processing. However,
significant limitations remain in its ability to perform precise spatial
reasoning, instruction-grounded generation, and consistent temporal prediction.
Furthermore, when faced with knowledge-intensive or domain-specific scenarios,
such as scientific illustrations or mathematical plots, the model often
exhibits hallucinations, factual errors, or structural inconsistencies. These
findings suggest that while GPT-4o marks a substantial advancement in unified
multimodal generation, there is still a long way to go before it can be
reliably applied to professional or safety-critical domains.

</details>


### [59] [Marker-Based Extrinsic Calibration Method for Accurate Multi-Camera 3D Reconstruction](https://arxiv.org/pdf/2505.02539)
*Nahuel Garcia-D'Urso, Bernabe Sanchez-Sos, Jorge Azorin-Lopez, Andres Fuster-Guillo, Antonio Macia-Lillo, Higinio Mora-Mora*

Main category: cs.CV

TL;DR: An iterative extrinsic calibration method for multi-camera RGB-D systems improves accuracy using 3D marker constraints, validated in controlled and real-world settings.


<details>
  <summary>Details</summary>
Motivation: Precise extrinsic calibration is crucial for accurate 3D reconstruction in multi-camera RGB-D systems.

Method: The approach segments and refines marker planes via clustering, regression, and iterative reassignment for robust geometric correspondence.

Result: Significant reduction in alignment errors, enabling reliable 3D reconstructions.

Conclusion: The method enhances calibration accuracy, validated in practical applications like the Tech4Diet project.

Abstract: Accurate 3D reconstruction using multi-camera RGB-D systems critically
depends on precise extrinsic calibration to achieve proper alignment between
captured views. In this paper, we introduce an iterative extrinsic calibration
method that leverages the geometric constraints provided by a three-dimensional
marker to significantly improve calibration accuracy. Our proposed approach
systematically segments and refines marker planes through clustering,
regression analysis, and iterative reassignment techniques, ensuring robust
geometric correspondence across camera views. We validate our method
comprehensively in both controlled environments and practical real-world
settings within the Tech4Diet project, aimed at modeling the physical
progression of patients undergoing nutritional treatments. Experimental results
demonstrate substantial reductions in alignment errors, facilitating accurate
and reliable 3D reconstructions.

</details>


### [60] [Apply Hierarchical-Chain-of-Generation to Complex Attributes Text-to-3D Generation](https://arxiv.org/pdf/2505.05505)
*Yiming Qin, Zhu Xu, Yang Liu*

Main category: cs.CV

TL;DR: HCoG automates text-to-3D generation by decomposing long descriptions into hierarchical parts, optimizing attribute binding and occlusion handling for coherent results.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-3D models struggle with complex attributes due to limited text encoder comprehension and occlusion challenges.

Method: HCoG uses a large language model to decompose descriptions, orders parts hierarchically, and optimizes attribute binding via Gaussian kernels.

Result: HCoG produces structurally coherent and attribute-faithful 3D objects with complex attributes.

Conclusion: HCoG effectively addresses text-to-3D challenges, offering automated, high-quality generation for complex objects.

Abstract: Recent text-to-3D models can render high-quality assets, yet they still
stumble on objects with complex attributes. The key obstacles are: (1) existing
text-to-3D approaches typically lift text-to-image models to extract semantics
via text encoders, while the text encoder exhibits limited comprehension
ability for long descriptions, leading to deviated cross-attention focus,
subsequently wrong attribute binding in generated results. (2) Occluded object
parts demand a disciplined generation order and explicit part disentanglement.
Though some works introduce manual efforts to alleviate the above issues, their
quality is unstable and highly reliant on manual information. To tackle above
problems, we propose a automated method Hierarchical-Chain-of-Generation
(HCoG). It leverages a large language model to decompose the long description
into blocks representing different object parts, and orders them from inside
out according to occlusions, forming a hierarchical chain. Within each block we
first coarsely create components, then precisely bind attributes via
target-region localization and corresponding 3D Gaussian kernel optimization.
Between blocks, we introduce Gaussian Extension and Label Elimination to
seamlessly generate new parts by extending new Gaussian kernels, re-assigning
semantic labels, and eliminating unnecessary kernels, ensuring that only
relevant parts are added without disrupting previously optimized parts.
Experiments confirm that HCoG yields structurally coherent, attribute-faithful
3D objects with complex attributes. The code is available at
https://github.com/Wakals/GASCOL .

</details>


### [61] [Occupancy World Model for Robots](https://arxiv.org/pdf/2505.05512)
*Zhang Zhang, Qiang Zhang, Wei Cui, Shuai Shi, Yijie Guo, Gang Han, Wen Zhao, Jingkai Sun, Jiahang Cao, Jiaxu Wang, Hao Cheng, Xiaozhu Ju, Zhengping Che, Renjing Xu, Jian Tang*

Main category: cs.CV

TL;DR: RoboOccWorld introduces a framework for predicting 3D occupancy scene evolutions in indoor robotics using a novel occupancy world model with Conditional Causal State Attention and Hybrid Spatio-Temporal Aggregation.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on outdoor scenes, neglecting indoor robotics. This work addresses the gap by forecasting fine-grained 3D occupancy scene evolutions for indoor environments.

Method: Proposes RoboOccWorld, combining spatio-temporal receptive fields and guided autoregressive transformers. Uses Conditional Causal State Attention (CCSA) and Hybrid Spatio-Temporal Aggregation (HSTA) for scene evolution prediction.

Result: Outperforms state-of-the-art methods in indoor 3D occupancy scene evolution prediction, validated on the restructured OccWorld-ScanNet benchmark.

Conclusion: RoboOccWorld effectively predicts indoor scene evolutions, advancing robotics decision-making in dynamic environments.

Abstract: Understanding and forecasting the scene evolutions deeply affect the
exploration and decision of embodied agents. While traditional methods simulate
scene evolutions through trajectory prediction of potential instances, current
works use the occupancy world model as a generative framework for describing
fine-grained overall scene dynamics. However, existing methods cluster on the
outdoor structured road scenes, while ignoring the exploration of forecasting
3D occupancy scene evolutions for robots in indoor scenes. In this work, we
explore a new framework for learning the scene evolutions of observed
fine-grained occupancy and propose an occupancy world model based on the
combined spatio-temporal receptive field and guided autoregressive transformer
to forecast the scene evolutions, called RoboOccWorld. We propose the
Conditional Causal State Attention (CCSA), which utilizes camera poses of next
state as conditions to guide the autoregressive transformer to adapt and
understand the indoor robotics scenarios. In order to effectively exploit the
spatio-temporal cues from historical observations, Hybrid Spatio-Temporal
Aggregation (HSTA) is proposed to obtain the combined spatio-temporal receptive
field based on multi-scale spatio-temporal windows. In addition, we restructure
the OccWorld-ScanNet benchmark based on local annotations to facilitate the
evaluation of the indoor 3D occupancy scene evolution prediction task.
Experimental results demonstrate that our RoboOccWorld outperforms
state-of-the-art methods in indoor 3D occupancy scene evolution prediction
task. The code will be released soon.

</details>


### [62] [Exploring Convolutional Neural Networks for Rice Grain Classification: An Explainable AI Approach](https://arxiv.org/pdf/2505.05513)
*Muhammad Junaid Asif, Hamza Khan, Rabia Tehseen, Syed Tahir Hussain Rizvi, Mujtaba Asad, Shazia Saqib, Rana Fayyaz Ahmad*

Main category: cs.CV

TL;DR: The paper proposes a CNN-based framework for automatic classification of rice grain varieties, achieving high accuracy and explainability using LIME and SHAP.


<details>
  <summary>Details</summary>
Motivation: Manual rice grain quality checks are laborious and error-prone, necessitating an automated solution for efficient classification.

Method: A convolutional neural network (CNN) is used for classification, evaluated via accuracy, recall, precision, F1-Score, and ROC analysis.

Result: The CNN model achieved high accuracy and minimal misclassifications, with explainability techniques (LIME, SHAP) providing insights into feature influence.

Conclusion: The proposed CNN framework is effective for rice grain classification, offering both accuracy and interpretability.

Abstract: Rice is an essential staple food worldwide that is important in promoting
international trade, economic growth, and nutrition. Asian countries such as
China, India, Pakistan, Thailand, Vietnam, and Indonesia are notable for their
significant contribution to the cultivation and utilization of rice. These
nations are also known for cultivating different rice grains, including short
and long grains. These sizes are further classified as basmati, jasmine, kainat
saila, ipsala, arborio, etc., catering to diverse culinary preferences and
cultural traditions. For both local and international trade, inspecting and
maintaining the quality of rice grains to satisfy customers and preserve a
country's reputation is necessary. Manual quality check and classification is
quite a laborious and time-consuming process. It is also highly prone to
mistakes. Therefore, an automatic solution must be proposed for the effective
and efficient classification of different varieties of rice grains. This
research paper presents an automatic framework based on a convolutional neural
network (CNN) for classifying different varieties of rice grains. We evaluated
the proposed model based on performance metrics such as accuracy, recall,
precision, and F1-Score. The CNN model underwent rigorous training and
validation, achieving a remarkable accuracy rate and a perfect area under each
class's Receiver Operating Characteristic (ROC) curve. The confusion matrix
analysis confirmed the model's effectiveness in distinguishing between the
different rice varieties, indicating minimal misclassifications. Additionally,
the integration of explainability techniques such as LIME (Local Interpretable
Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) provided
valuable insights into the model's decision-making process, revealing how
specific features of the rice grains influenced classification outcomes.

</details>


### [63] [Web2Grasp: Learning Functional Grasps from Web Images of Hand-Object Interactions](https://arxiv.org/pdf/2505.05517)
*Hongyi Chen, Yunchao Yao, Yufei Ye, Zhixuan Xu, Homanga Bharadhwaj, Jiashun Wang, Shubham Tulsiani, Zackory Erickson, Jeffrey Ichnowski*

Main category: cs.CV

TL;DR: The paper proposes using web images to extract human grasp data for training functional grasping models in robots, bypassing costly demonstrations. It reconstructs hand-object interactions from RGB images, retargets to robot hands, and uses simulation to expand the dataset, achieving high success rates in both simulation and real-world tests.


<details>
  <summary>Details</summary>
Motivation: Functional grasping is crucial for dexterous robot hands, but prior work relies on power grasping or expensive demonstrations. The authors aim to leverage readily available web images for natural and functional grasp data.

Method: The method involves reconstructing 3D hand-object meshes from RGB images, retargeting human hands to robot hands, and aligning noisy object meshes with accurate 3D shapes. The dataset is expanded using a simulator to generate feasible grasps.

Result: The model achieves 75.8% success on seen objects and 61.8% on all objects in simulation, with improvements over baselines. Simulator-augmented data boosts performance to 83.4%, and real-world transfer achieves 85% success.

Conclusion: Web-sourced grasp data and simulator augmentation effectively train functional grasping models, outperforming baselines and demonstrating successful sim-to-real transfer.

Abstract: Functional grasp is essential for enabling dexterous multi-finger robot hands
to manipulate objects effectively. However, most prior work either focuses on
power grasping, which simply involves holding an object still, or relies on
costly teleoperated robot demonstrations to teach robots how to grasp each
object functionally. Instead, we propose extracting human grasp information
from web images since they depict natural and functional object interactions,
thereby bypassing the need for curated demonstrations. We reconstruct human
hand-object interaction (HOI) 3D meshes from RGB images, retarget the human
hand to multi-finger robot hands, and align the noisy object mesh with its
accurate 3D shape. We show that these relatively low-quality HOI data from
inexpensive web sources can effectively train a functional grasping model. To
further expand the grasp dataset for seen and unseen objects, we use the
initially-trained grasping policy with web data in the IsaacGym simulator to
generate physically feasible grasps while preserving functionality. We train
the grasping model on 10 object categories and evaluate it on 9 unseen objects,
including challenging items such as syringes, pens, spray bottles, and tongs,
which are underrepresented in existing datasets. The model trained on the web
HOI dataset, achieving a 75.8% success rate on seen objects and 61.8% across
all objects in simulation, with a 6.7% improvement in success rate and a 1.8x
increase in functionality ratings over baselines. Simulator-augmented data
further boosts performance from 61.8% to 83.4%. The sim-to-real transfer to the
LEAP Hand achieves a 85% success rate. Project website is at:
https://webgrasp.github.io/.

</details>


### [64] [Real-Time Privacy Preservation for Robot Visual Perception](https://arxiv.org/pdf/2505.05519)
*Minkyu Choi, Yunhao Yang, Neel P. Bhatt, Kushagra Gupta, Sahil Shah, Aditya Rai, David Fridovich-Keil, Ufuk Topcu, Sandeep P. Chinchali*

Main category: cs.CV

TL;DR: PCVS is a method for real-time privacy-preserving video streaming that guarantees concealment of sensitive objects using logical specifications and conformal prediction.


<details>
  <summary>Details</summary>
Motivation: Existing privacy-preserving methods lack guarantees for complete concealment of sensitive objects and are inadequate for real-time video streams.

Method: PCVS uses logical specifications to constrain sensitive objects, detects them in frames, and blurs subsets to satisfy the specification. Conformal prediction provides theoretical bounds on satisfaction.

Result: PCVS achieves over 95% specification satisfaction rate, outperforming other methods, and maintains performance in real-time robot operations.

Conclusion: PCVS effectively guarantees privacy in real-time video streams while maintaining robot functionality.

Abstract: Many robots (e.g., iRobot's Roomba) operate based on visual observations from
live video streams, and such observations may inadvertently include
privacy-sensitive objects, such as personal identifiers. Existing approaches
for preserving privacy rely on deep learning models, differential privacy, or
cryptography. They lack guarantees for the complete concealment of all
sensitive objects. Guaranteeing concealment requires post-processing techniques
and thus is inadequate for real-time video streams. We develop a method for
privacy-constrained video streaming, PCVS, that conceals sensitive objects
within real-time video streams. PCVS takes a logical specification constraining
the existence of privacy-sensitive objects, e.g., never show faces when a
person exists. It uses a detection model to evaluate the existence of these
objects in each incoming frame. Then, it blurs out a subset of objects such
that the existence of the remaining objects satisfies the specification. We
then propose a conformal prediction approach to (i) establish a theoretical
lower bound on the probability of the existence of these objects in a sequence
of frames satisfying the specification and (ii) update the bound with the
arrival of each subsequent frame. Quantitative evaluations show that PCVS
achieves over 95 percent specification satisfaction rate in multiple datasets,
significantly outperforming other methods. The satisfaction rate is
consistently above the theoretical bounds across all datasets, indicating that
the established bounds hold. Additionally, we deploy PCVS on robots in
real-time operation and show that the robots operate normally without being
compromised when PCVS conceals objects.

</details>


### [65] [GaMNet: A Hybrid Network with Gabor Fusion and NMamba for Efficient 3D Glioma Segmentation](https://arxiv.org/pdf/2505.05520)
*Chengwei Ye, Huanzhen Zhang, Yufei Lin, Kangsheng Wang, Linuo Xu, Shuyan Liu*

Main category: cs.CV

TL;DR: GaMNet combines NMamba for global context and multi-scale CNN for local features, using Gabor filters for interpretability, achieving high accuracy with efficiency.


<details>
  <summary>Details</summary>
Motivation: Address limitations of CNN and Transformer models in glioma segmentation, such as lack of context modeling and high computational cost, to enable real-time use on mobile medical devices.

Method: Integrates NMamba module for global modeling, multi-scale CNN for local feature extraction, and Gabor filters for interpretability.

Result: Achieves high segmentation accuracy with fewer parameters and faster computation, reducing false positives and negatives.

Conclusion: GaMNet outperforms existing methods, enhancing clinical diagnosis reliability.

Abstract: Gliomas are aggressive brain tumors that pose serious health risks. Deep
learning aids in lesion segmentation, but CNN and Transformer-based models
often lack context modeling or demand heavy computation, limiting real-time use
on mobile medical devices. We propose GaMNet, integrating the NMamba module for
global modeling and a multi-scale CNN for efficient local feature extraction.
To improve interpretability and mimic the human visual system, we apply Gabor
filters at multiple scales. Our method achieves high segmentation accuracy with
fewer parameters and faster computation. Extensive experiments show GaMNet
outperforms existing methods, notably reducing false positives and negatives,
which enhances the reliability of clinical diagnosis.

</details>


### [66] [X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP](https://arxiv.org/pdf/2505.05528)
*Hanxun Huang, Sarah Erfani, Yige Li, Xingjun Ma, James Bailey*

Main category: cs.CV

TL;DR: X-Transfer is a novel attack method exposing universal adversarial vulnerability in CLIP models, achieving super transferability across tasks, domains, and models via surrogate scaling.


<details>
  <summary>Details</summary>
Motivation: CLIP models' susceptibility to adversarial perturbations poses risks for downstream tasks and VLMs, necessitating robust attack methods.

Method: X-Transfer generates Universal Adversarial Perturbations (UAPs) using surrogate scaling, dynamically selecting surrogates for efficiency.

Result: X-Transfer outperforms existing UAP methods, setting a new benchmark for adversarial transferability in CLIP models.

Conclusion: X-Transfer's super transferability highlights critical vulnerabilities in CLIP, urging improved robustness in vision-language models.

Abstract: As Contrastive Language-Image Pre-training (CLIP) models are increasingly
adopted for diverse downstream tasks and integrated into large vision-language
models (VLMs), their susceptibility to adversarial perturbations has emerged as
a critical concern. In this work, we introduce \textbf{X-Transfer}, a novel
attack method that exposes a universal adversarial vulnerability in CLIP.
X-Transfer generates a Universal Adversarial Perturbation (UAP) capable of
deceiving various CLIP encoders and downstream VLMs across different samples,
tasks, and domains. We refer to this property as \textbf{super
transferability}--a single perturbation achieving cross-data, cross-domain,
cross-model, and cross-task adversarial transferability simultaneously. This is
achieved through \textbf{surrogate scaling}, a key innovation of our approach.
Unlike existing methods that rely on fixed surrogate models, which are
computationally intensive to scale, X-Transfer employs an efficient surrogate
scaling strategy that dynamically selects a small subset of suitable surrogates
from a large search space. Extensive evaluations demonstrate that X-Transfer
significantly outperforms previous state-of-the-art UAP methods, establishing a
new benchmark for adversarial transferability across CLIP models. The code is
publicly available in our
\href{https://github.com/HanxunH/XTransferBench}{GitHub repository}.

</details>


### [67] [OXSeg: Multidimensional attention UNet-based lip segmentation using semi-supervised lip contours](https://arxiv.org/pdf/2505.05531)
*Hanie Moghaddasi, Christina Chambers, Sarah N. Mattson, Jeffrey R. Wozniak, Claire D. Coles, Raja Mukherjee, Michael Suttie*

Main category: cs.CV

TL;DR: A sequential lip segmentation method using attention UNet and multidimensional input improves accuracy, achieving high dice scores and pixel accuracy, and aids in identifying fetal alcohol syndrome (FAS).


<details>
  <summary>Details</summary>
Motivation: Supervised lip segmentation is limited by training data availability and affected by image quality, lighting, and skin tone. The study aims to improve segmentation accuracy and explore FAS-related lip characteristics.

Method: The method integrates attention UNet with multidimensional inputs (using local binary patterns) and a mask generation technique based on anatomical landmarks. It evaluates segmentation and FAS identification using GANs.

Result: Achieved 84.75% mean dice score and 99.77% pixel accuracy in lip segmentation. FAS identification reached 98.55% accuracy in one population.

Conclusion: The proposed method enhances lip segmentation accuracy, particularly around Cupid's bow, and provides insights into FAS-related lip features.

Abstract: Lip segmentation plays a crucial role in various domains, such as lip
synchronization, lipreading, and diagnostics. However, the effectiveness of
supervised lip segmentation is constrained by the availability of lip contour
in the training phase. A further challenge with lip segmentation is its
reliance on image quality , lighting, and skin tone, leading to inaccuracies in
the detected boundaries. To address these challenges, we propose a sequential
lip segmentation method that integrates attention UNet and multidimensional
input. We unravel the micro-patterns in facial images using local binary
patterns to build multidimensional inputs. Subsequently, the multidimensional
inputs are fed into sequential attention UNets, where the lip contour is
reconstructed. We introduce a mask generation method that uses a few anatomical
landmarks and estimates the complete lip contour to improve segmentation
accuracy. This mask has been utilized in the training phase for lip
segmentation. To evaluate the proposed method, we use facial images to segment
the upper lips and subsequently assess lip-related facial anomalies in subjects
with fetal alcohol syndrome (FAS). Using the proposed lip segmentation method,
we achieved a mean dice score of 84.75%, and a mean pixel accuracy of 99.77% in
upper lip segmentation. To further evaluate the method, we implemented
classifiers to identify those with FAS. Using a generative adversarial network
(GAN), we reached an accuracy of 98.55% in identifying FAS in one of the study
populations. This method could be used to improve lip segmentation accuracy,
especially around Cupid's bow, and shed light on distinct lip-related
characteristics of FAS.

</details>


### [68] [The Moon's Many Faces: A Single Unified Transformer for Multimodal Lunar Reconstruction](https://arxiv.org/pdf/2505.05644)
*Tom Sander, Moritz Tenthoff, Kay Wohlfarth, Christian Wöhler*

Main category: cs.CV

TL;DR: A unified transformer model for multimodal learning in planetary science, enabling tasks like 3D reconstruction and reflectance parameter estimation from lunar images.


<details>
  <summary>Details</summary>
Motivation: To address the lack of multimodal learning applications in planetary science, particularly for lunar image analysis.

Method: Proposes a single transformer architecture to learn shared representations across grayscale images, DEMs, surface normals, and albedo maps, supporting flexible modality translation.

Result: The model successfully learns physically plausible relations across modalities, enabling 3D reconstruction and disentangling photometric parameters.

Conclusion: The foundation model shows promise for future tasks like photometric normalization and co-registration by incorporating more input modalities.

Abstract: Multimodal learning is an emerging research topic across multiple disciplines
but has rarely been applied to planetary science. In this contribution, we
identify that reflectance parameter estimation and image-based 3D
reconstruction of lunar images can be formulated as a multimodal learning
problem. We propose a single, unified transformer architecture trained to learn
shared representations between multiple sources like grayscale images, digital
elevation models, surface normals, and albedo maps. The architecture supports
flexible translation from any input modality to any target modality. Predicting
DEMs and albedo maps from grayscale images simultaneously solves the task of 3D
reconstruction of planetary surfaces and disentangles photometric parameters
and height information. Our results demonstrate that our foundation model
learns physically plausible relations across these four modalities. Adding more
input modalities in the future will enable tasks such as photometric
normalization and co-registration.

</details>


### [69] [Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open Ended Action Environments](https://arxiv.org/pdf/2505.05540)
*Pranav Guruprasad, Yangyue Wang, Sudipta Chowdhury, Harshvardhan Sikka*

Main category: cs.CV

TL;DR: MultiNet v0.2 evaluates VLA models' zero-shot generalization in OOD environments, revealing limitations, VLA superiority, and prompt engineering's impact.


<details>
  <summary>Details</summary>
Motivation: To systematically assess the zero-shot generalization of VLA models in OOD settings, addressing gaps in current evaluation methods.

Method: Introduces MultiNet v0.2, a benchmark testing VLA models (e.g., GPT-4o, OpenVLA) on diverse Procgen tasks.

Result: Models struggle with OOD tasks; VLAs outperform others; prompt engineering significantly improves VLM performance.

Conclusion: VLA models show promise but require better generalization; precise constraints enhance performance.

Abstract: Vision-language-action (VLA) models represent an important step toward
general-purpose robotic systems by integrating visual perception, language
understanding, and action execution. However, systematic evaluation of these
models, particularly their zero-shot generalization capabilities in
out-of-distribution (OOD) environments, remains limited. In this paper, we
introduce MultiNet v0.2, a comprehensive benchmark designed to evaluate and
analyze the generalization performance of state-of-the-art VLM and VLA
models-including GPT-4o, GPT-4.1, OpenVLA,Pi0 Base, and Pi0 FAST-on diverse
procedural tasks from the Procgen benchmark. Our analysis reveals several
critical insights: (1) all evaluated models exhibit significant limitations in
zero-shot generalization to OOD tasks, with performance heavily influenced by
factors such as action representation and task complexit; (2) VLAs generally
outperform other models due to their robust architectural design; and (3) VLM
variants demonstrate substantial improvements when constrained appropriately,
highlighting the sensitivity of model performance to precise prompt
engineering.

</details>


### [70] [HyperspectralMAE: The Hyperspectral Imagery Classification Model using Fourier-Encoded Dual-Branch Masked Autoencoder](https://arxiv.org/pdf/2505.05710)
*Wooyoung Jeong, Hyun Jae Park, Seonghun Jeong, Jong Wook Jang, Tae Hoon Lim, Dae Seoung Kim*

Main category: cs.CV

TL;DR: HyperspectralMAE is a Transformer-based model using dual masking (spatial and spectral) for hyperspectral data, achieving state-of-the-art results in land-cover classification.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral imagery's high dimensionality in spatial and spectral domains requires robust representation learning.

Method: The model employs dual masking (50% spatial patches and 50% spectral bands), harmonic Fourier positional embeddings, and a combined MSE-SAM reconstruction objective.

Result: Pre-trained on large hyperspectral corpora, HyperspectralMAE achieves top transfer-learning accuracy on the Indian Pines benchmark.

Conclusion: Dual masking and wavelength-aware embeddings improve hyperspectral image reconstruction and downstream tasks.

Abstract: Hyperspectral imagery provides rich spectral detail but poses unique
challenges because of its high dimensionality in both spatial and spectral
domains. We propose \textit{HyperspectralMAE}, a Transformer-based foundation
model for hyperspectral data that employs a \textit{dual masking} strategy:
during pre-training we randomly occlude 50\% of spatial patches and 50\% of
spectral bands. This forces the model to learn representations capable of
reconstructing missing information across both dimensions. To encode spectral
order, we introduce learnable harmonic Fourier positional embeddings based on
wavelength. The reconstruction objective combines mean-squared error (MSE) with
the spectral angle mapper (SAM) to balance pixel-level accuracy and
spectral-shape fidelity.
  The resulting model contains about $1.8\times10^{8}$ parameters and produces
768-dimensional embeddings, giving it sufficient capacity for transfer
learning. We pre-trained HyperspectralMAE on two large hyperspectral corpora --
NASA EO-1 Hyperion ($\sim$1\,600 scenes, $\sim$$3\times10^{11}$ pixel spectra)
and DLR EnMAP Level-0 ($\sim$1\,300 scenes, $\sim$$3\times10^{11}$ pixel
spectra) -- and fine-tuned it for land-cover classification on the Indian Pines
benchmark. HyperspectralMAE achieves state-of-the-art transfer-learning
accuracy on Indian Pines, confirming that masked dual-dimensional pre-training
yields robust spectral-spatial representations. These results demonstrate that
dual masking and wavelength-aware embeddings advance hyperspectral image
reconstruction and downstream analysis.

</details>


### [71] [Prompt to Polyp: Clinically-Aware Medical Image Synthesis with Diffusion Models](https://arxiv.org/pdf/2505.05573)
*Mikhail Chaichuk, Sushant Gautam, Steven Hicks, Elena Tutubalina*

Main category: cs.CV

TL;DR: The paper compares text-to-image synthesis methods in healthcare, introducing MSDM, an optimized model, and evaluates its performance against large pre-trained models.


<details>
  <summary>Details</summary>
Motivation: Addressing data scarcity in healthcare AI while preserving privacy by generating realistic medical images from text.

Method: Two approaches: fine-tuning large pre-trained models (FLUX, Kandinsky) and training compact domain-specific models (MSDM). MSDM integrates a clinical text encoder, variational autoencoder, and cross-attention mechanisms.

Result: Large models achieve higher fidelity, but MSDM delivers comparable quality with lower computational costs.

Conclusion: MSDM is a viable alternative to large models, balancing quality and efficiency in medical image synthesis.

Abstract: The generation of realistic medical images from text descriptions has
significant potential to address data scarcity challenges in healthcare AI
while preserving patient privacy. This paper presents a comprehensive study of
text-to-image synthesis in the medical domain, comparing two distinct
approaches: (1) fine-tuning large pre-trained latent diffusion models and (2)
training small, domain-specific models. We introduce a novel model named MSDM,
an optimized architecture based on Stable Diffusion that integrates a clinical
text encoder, variational autoencoder, and cross-attention mechanisms to better
align medical text prompts with generated images. Our study compares two
approaches: fine-tuning large pre-trained models (FLUX, Kandinsky) versus
training compact domain-specific models (MSDM). Evaluation across colonoscopy
(MedVQA-GI) and radiology (ROCOv2) datasets reveals that while large models
achieve higher fidelity, our optimized MSDM delivers comparable quality with
lower computational costs. Quantitative metrics and qualitative evaluations by
medical experts reveal strengths and limitations of each approach.

</details>


### [72] [Automating Infrastructure Surveying: A Framework for Geometric Measurements and Compliance Assessment Using Point Cloud Data](https://arxiv.org/pdf/2505.05752)
*Amin Ghafourian, Andrew Lee, Dechen Gao, Tyler Beer, Kin Yen, Iman Soltani*

Main category: cs.CV

TL;DR: The paper presents a framework for automating geometric measurements and compliance assessment using point cloud data, demonstrated through ADA-compliant curb ramp evaluation.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency, accuracy, and scalability in infrastructure surveying and compliance assessment by leveraging automation.

Method: Integrates deep learning-based detection and segmentation with geometric and signal processing techniques, using a newly collected annotated dataset of curb ramps.

Result: Validated accuracy and reliability through comparison with manual field measurements, showing potential to reduce manual effort and improve consistency.

Conclusion: The framework is scalable beyond ADA compliance, promoting wider adoption of point cloud data in infrastructure surveying and automated construction evaluation.

Abstract: Automation can play a prominent role in improving efficiency, accuracy, and
scalability in infrastructure surveying and assessing construction and
compliance standards. This paper presents a framework for automation of
geometric measurements and compliance assessment using point cloud data. The
proposed approach integrates deep learning-based detection and segmentation, in
conjunction with geometric and signal processing techniques, to automate
surveying tasks. As a proof of concept, we apply this framework to
automatically evaluate the compliance of curb ramps with the Americans with
Disabilities Act (ADA), demonstrating the utility of point cloud data in survey
automation. The method leverages a newly collected, large annotated dataset of
curb ramps, made publicly available as part of this work, to facilitate robust
model training and evaluation. Experimental results, including comparison with
manual field measurements of several ramps, validate the accuracy and
reliability of the proposed method, highlighting its potential to significantly
reduce manual effort and improve consistency in infrastructure assessment.
Beyond ADA compliance, the proposed framework lays the groundwork for broader
applications in infrastructure surveying and automated construction evaluation,
promoting wider adoption of point cloud data in these domains. The annotated
database, manual ramp survey data, and developed algorithms are publicly
available on the project's GitHub page:
https://github.com/Soltanilara/SurveyAutomation.

</details>


### [73] [Steepest Descent Density Control for Compact 3D Gaussian Splatting](https://arxiv.org/pdf/2505.05587)
*Peihao Wang, Yuehao Wang, Dilin Wang, Sreyas Mohan, Zhiwen Fan, Lemeng Wu, Ruisi Cai, Yu-Ying Yeh, Zhangyang Wang, Qiang Liu, Rakesh Ranjan*

Main category: cs.CV

TL;DR: SteepGS improves 3D Gaussian Splatting (3DGS) by optimizing density control, reducing redundant points by ~50% without quality loss.


<details>
  <summary>Details</summary>
Motivation: 3DGS's densification algorithm creates redundant points, leading to inefficiencies in memory, performance, and storage, especially on resource-constrained devices.

Method: A theoretical framework analyzes density control, identifies splitting necessity, and derives conditions for densification. SteepGS implements steepest density control to minimize loss while keeping the point cloud compact.

Result: SteepGS reduces Gaussian points by ~50% without compromising rendering quality, improving efficiency and scalability.

Conclusion: SteepGS offers a principled solution to 3DGS's redundancy issues, enhancing its practicality for real-time applications.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for
real-time, high-resolution novel view synthesis. By representing scenes as a
mixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines for
efficient rendering and reconstruction. To optimize scene coverage and capture
fine details, 3DGS employs a densification algorithm to generate additional
points. However, this process often leads to redundant point clouds, resulting
in excessive memory usage, slower performance, and substantial storage demands
- posing significant challenges for deployment on resource-constrained devices.
To address this limitation, we propose a theoretical framework that demystifies
and improves density control in 3DGS. Our analysis reveals that splitting is
crucial for escaping saddle points. Through an optimization-theoretic approach,
we establish the necessary conditions for densification, determine the minimal
number of offspring Gaussians, identify the optimal parameter update direction,
and provide an analytical solution for normalizing off-spring opacity. Building
on these insights, we introduce SteepGS, incorporating steepest density
control, a principled strategy that minimizes loss while maintaining a compact
point cloud. SteepGS achieves a ~50% reduction in Gaussian points without
compromising rendering quality, significantly enhancing both efficiency and
scalability.

</details>


### [74] [ReactDance: Progressive-Granular Representation for Long-Term Coherent Reactive Dance Generation](https://arxiv.org/pdf/2505.05589)
*Jingzhong Lin, Yuanyuan Qi, Xinru Li, Wenxuan Huang, Xiangfeng Xu, Bangyan Li, Xuejiao Wang, Gaoqi He*

Main category: cs.CV

TL;DR: ReactDance is a diffusion-based framework for reactive dance generation, improving fidelity and coherence with multi-scale control via GRFSQ and BLC innovations.


<details>
  <summary>Details</summary>
Motivation: Existing methods for reactive dance generation lack fine-grained spatial interactions and localized temporal context, leading to poor interaction fidelity and temporal consistency.

Method: ReactDance introduces GRFSQ for multi-scale motion representation and BLC for error-free long sequence generation, combined with a diffusion model using LDCFG.

Result: ReactDance outperforms existing methods, achieving state-of-the-art performance in reactive dance generation.

Conclusion: The proposed framework addresses limitations of prior methods, offering high-fidelity, coherent, and controllable reactive dance generation.

Abstract: Reactive dance generation (RDG) produces follower movements conditioned on
guiding dancer and music while ensuring spatial coordination and temporal
coherence. However, existing methods overemphasize global constraints and
optimization, overlooking local information, such as fine-grained spatial
interactions and localized temporal context. Therefore, we present ReactDance,
a novel diffusion-based framework for high-fidelity RDG with long-term
coherence and multi-scale controllability. Unlike existing methods that
struggle with interaction fidelity, synchronization, and temporal consistency
in duet synthesis, our approach introduces two key innovations: 1)Group
Residual Finite Scalar Quantization (GRFSQ), a multi-scale disentangled motion
representation that captures interaction semantics from coarse body rhythms to
fine-grained joint dynamics, and 2)Blockwise Local Context (BLC), a sampling
strategy eliminating error accumulation in long sequence generation via local
block causal masking and periodic positional encoding. Built on the decoupled
multi-scale GRFSQ representation, we implement a diffusion model
withLayer-Decoupled Classifier-free Guidance (LDCFG), allowing granular control
over motion semantics across scales. Extensive experiments on standard
benchmarks demonstrate that ReactDance surpasses existing methods, achieving
state-of-the-art performance.

</details>


### [75] [Accelerating Diffusion Transformer via Increment-Calibrated Caching with Channel-Aware Singular Value Decomposition](https://arxiv.org/pdf/2505.05829)
*Zhiyuan Chen, Keyi Li, Yifan Jia, Le Ye, Yufei Ma*

Main category: cs.CV

TL;DR: A training-free method, increment-calibrated caching, is proposed to accelerate Diffusion Transformer (DiT) models by reducing redundant computations without quality degradation.


<details>
  <summary>Details</summary>
Motivation: The iterative nature of diffusion models leads to high computational complexity, and existing cache-based methods lack correction, risking quality loss.

Method: The method uses low-rank approximation to generate calibration parameters from the pre-trained model and introduces channel-aware SVD for outlier handling.

Result: The method reduces computation by over 45% compared to 35-step DDIM, improves IS by 12, and increases FID by less than 0.06.

Conclusion: The proposed method effectively accelerates DiT models while maintaining quality, outperforming naive caching methods.

Abstract: Diffusion transformer (DiT) models have achieved remarkable success in image
generation, thanks for their exceptional generative capabilities and
scalability. Nonetheless, the iterative nature of diffusion models (DMs)
results in high computation complexity, posing challenges for deployment.
Although existing cache-based acceleration methods try to utilize the inherent
temporal similarity to skip redundant computations of DiT, the lack of
correction may induce potential quality degradation. In this paper, we propose
increment-calibrated caching, a training-free method for DiT acceleration,
where the calibration parameters are generated from the pre-trained model
itself with low-rank approximation. To deal with the possible correction
failure arising from outlier activations, we introduce channel-aware Singular
Value Decomposition (SVD), which further strengthens the calibration effect.
Experimental results show that our method always achieve better performance
than existing naive caching methods with a similar computation resource budget.
When compared with 35-step DDIM, our method eliminates more than 45%
computation and improves IS by 12 at the cost of less than 0.06 FID increase.
Code is available at https://github.com/ccccczzy/icc.

</details>


### [76] [QuickSplat: Fast 3D Surface Reconstruction via Learned Gaussian Initialization](https://arxiv.org/pdf/2505.05591)
*Yueh-Cheng Liu, Lukas Höllein, Matthias Nießner, Angela Dai*

Main category: cs.CV

TL;DR: QuickSplat introduces data-driven priors for 2D Gaussian splatting to accelerate and improve large-scale indoor scene reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing volumetric rendering methods are slow and struggle with under-observed or textureless regions.

Method: QuickSplat uses learned priors for dense initializations and a densifier network to predict new Gaussians based on rendering gradients.

Result: Achieves 8x faster runtime and reduces depth errors by up to 48% compared to state-of-the-art methods.

Conclusion: QuickSplat significantly improves efficiency and accuracy in large-scale indoor scene reconstruction.

Abstract: Surface reconstruction is fundamental to computer vision and graphics,
enabling applications in 3D modeling, mixed reality, robotics, and more.
Existing approaches based on volumetric rendering obtain promising results, but
optimize on a per-scene basis, resulting in a slow optimization that can
struggle to model under-observed or textureless regions. We introduce
QuickSplat, which learns data-driven priors to generate dense initializations
for 2D gaussian splatting optimization of large-scale indoor scenes. This
provides a strong starting point for the reconstruction, which accelerates the
convergence of the optimization and improves the geometry of flat wall
structures. We further learn to jointly estimate the densification and update
of the scene parameters during each iteration; our proposed densifier network
predicts new Gaussians based on the rendering gradients of existing ones,
removing the needs of heuristics for densification. Extensive experiments on
large-scale indoor scene reconstruction demonstrate the superiority of our
data-driven optimization. Concretely, we accelerate runtime by 8x, while
decreasing depth errors by up to 48% in comparison to state of the art methods.

</details>


### [77] [Towards Facial Image Compression with Consistency Preserving Diffusion Prior](https://arxiv.org/pdf/2505.05870)
*Yimin Zhou, Yichong Xia, Bin Chen, Baoyi An, Haoqian Wang, Zhi Wang, Yaowei Wang, Zikun Zhou*

Main category: cs.CV

TL;DR: FaSDiff improves facial image compression by using a stable diffusion prior and frequency enhancement, balancing human visual quality and machine vision accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to preserve high-frequency details at low bit rates, leading to poor reconstructed image quality and downstream performance.

Method: FaSDiff combines a high-frequency-sensitive compressor and a hybrid low-frequency enhancement module to leverage diffusion priors for better image quality.

Result: FaSDiff outperforms state-of-the-art methods in both human visual quality and machine vision accuracy.

Conclusion: FaSDiff effectively balances human and machine vision needs, offering superior performance in facial image compression.

Abstract: With the widespread application of facial image data across various domains,
the efficient storage and transmission of facial images has garnered
significant attention. However, the existing learned face image compression
methods often produce unsatisfactory reconstructed image quality at low bit
rates. Simply adapting diffusion-based compression methods to facial
compression tasks results in reconstructed images that perform poorly in
downstream applications due to insufficient preservation of high-frequency
information. To further explore the diffusion prior in facial image
compression, we propose Facial Image Compression with a Stable Diffusion Prior
(FaSDiff), a method that preserves consistency through frequency enhancement.
FaSDiff employs a high-frequency-sensitive compressor in an end-to-end
framework to capture fine image details and produce robust visual prompts.
Additionally, we introduce a hybrid low-frequency enhancement module that
disentangles low-frequency facial semantics and stably modulates the diffusion
prior alongside visual prompts. The proposed modules allow FaSDiff to leverage
diffusion priors for superior human visual perception while minimizing
performance loss in machine vision due to semantic inconsistency. Extensive
experiments show that FaSDiff outperforms state-of-the-art methods in balancing
human visual quality and machine vision accuracy. The code will be released
after the paper is accepted.

</details>


### [78] [Enhancing Satellite Object Localization with Dilated Convolutions and Attention-aided Spatial Pooling](https://arxiv.org/pdf/2505.05599)
*Seraj Al Mahmud Mostafa, Chenxi Wang, Jia Yue, Yuta Hozumi, Jianwu Wang*

Main category: cs.CV

TL;DR: YOLO-DCAP, an enhanced YOLOv5, improves object localization in satellite imagery using multi-scale and attention mechanisms, outperforming base and state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Challenges in satellite object localization include high variability, low resolution, and noise. Addressing these for GW, Bore, and OE datasets.

Method: Introduces YOLO-DCAP with Multi-scale Dilated Residual Convolution (MDRC) and Attention-aided Spatial Pooling (AaSP) modules.

Result: Achieves 20.95% mAP50 and 32.23% IoU improvement over base model, 7.35% and 9.84% over alternatives.

Conclusion: YOLO-DCAP is robust and generalizable, with open-sourced code for satellite object localization.

Abstract: Object localization in satellite imagery is particularly challenging due to
the high variability of objects, low spatial resolution, and interference from
noise and dominant features such as clouds and city lights. In this research,
we focus on three satellite datasets: upper atmospheric Gravity Waves (GW),
mesospheric Bores (Bore), and Ocean Eddies (OE), each presenting its own unique
challenges. These challenges include the variability in the scale and
appearance of the main object patterns, where the size, shape, and feature
extent of objects of interest can differ significantly. To address these
challenges, we introduce YOLO-DCAP, a novel enhanced version of YOLOv5 designed
to improve object localization in these complex scenarios. YOLO-DCAP
incorporates a Multi-scale Dilated Residual Convolution (MDRC) block to capture
multi-scale features at scale with varying dilation rates, and an
Attention-aided Spatial Pooling (AaSP) module to focus on the global relevant
spatial regions, enhancing feature selection. These structural improvements
help to better localize objects in satellite imagery. Experimental results
demonstrate that YOLO-DCAP significantly outperforms both the YOLO base model
and state-of-the-art approaches, achieving an average improvement of 20.95% in
mAP50 and 32.23% in IoU over the base model, and 7.35% and 9.84% respectively
over state-of-the-art alternatives, consistently across all three satellite
datasets. These consistent gains across all three satellite datasets highlight
the robustness and generalizability of the proposed approach. Our code is open
sourced at
https://github.com/AI-4-atmosphere-remote-sensing/satellite-object-localization.

</details>


### [79] [A Preliminary Study for GPT-4o on Image Restoration](https://arxiv.org/pdf/2505.05621)
*Hao Yang, Yan Yang, Ruikun Zhang, Liyuan Pan*

Main category: cs.CV

TL;DR: GPT-4o shows promise in image restoration but lacks pixel-level fidelity. Its outputs serve as strong visual priors, improving existing dehazing networks.


<details>
  <summary>Details</summary>
Motivation: To explore GPT-4o's impact on image restoration and address its limitations in structural fidelity.

Method: Systematic evaluation of GPT-4o across diverse restoration tasks, focusing on dehazing, deraining, and low-light enhancement.

Result: GPT-4o outputs are visually appealing but lack structural accuracy. They enhance existing dehazing networks as priors.

Conclusion: GPT-4o can advance image restoration by providing visual priors, with guidelines for future integration.

Abstract: OpenAI's GPT-4o model, integrating multi-modal inputs and outputs within an
autoregressive architecture, has demonstrated unprecedented performance in
image generation. In this work, we investigate its potential impact on the
image restoration community. We present the first systematic evaluation of
GPT-4o across diverse restoration tasks. Our experiments reveal that, although
restoration outputs from GPT-4o are visually appealing, they often suffer from
pixel-level structural fidelity when compared to ground-truth images. Common
issues are variations in image proportions, shifts in object positions and
quantities, and changes in viewpoint.To address it, taking image dehazing,
derainning, and low-light enhancement as representative case studies, we show
that GPT-4o's outputs can serve as powerful visual priors, substantially
enhancing the performance of existing dehazing networks. It offers practical
guidelines and a baseline framework to facilitate the integration of GPT-4o
into future image restoration pipelines. We hope the study on GPT-4o image
restoration will accelerate innovation in the broader field of image generation
areas. To support further research, we will release GPT-4o-restored images from
over 10 widely used image restoration datasets.

</details>


### [80] [Looking Beyond Language Priors: Enhancing Visual Comprehension and Attention in Multimodal Models](https://arxiv.org/pdf/2505.05626)
*Aarti Ghatkesar, Uddeshya Upadhyay, Ganesh Venkatesh*

Main category: cs.CV

TL;DR: The paper addresses the challenge of deep alignment between vision and language in MLLMs, proposing techniques to enhance visual understanding and its influence on language generation.


<details>
  <summary>Details</summary>
Motivation: MLLMs often rely too heavily on language priors, failing to fully utilize visual input, which limits their multimodal capabilities.

Method: The approach involves analyzing how MLLMs build visual understanding and introducing techniques to deepen visual comprehension and guide language generation.

Result: The improved model shows better multimodal understanding, evidenced by superior prediction of visually-dependent tokens and a 10-point boost on visually challenging tasks.

Conclusion: The proposed techniques effectively enhance MLLMs' visual understanding and alignment with language, improving performance on multimodal tasks.

Abstract: Achieving deep alignment between vision and language remains a central
challenge for Multimodal Large Language Models (MLLMs). These models often fail
to fully leverage visual input, defaulting to strong language priors. Our
approach first provides insights into how MLLMs internally build visual
understanding of image regions and then introduces techniques to amplify this
capability. Specifically, we explore techniques designed both to deepen the
model's understanding of visual content and to ensure that these visual
insights actively guide language generation. We demonstrate the superior
multimodal understanding of our resultant model through a detailed upstream
analysis quantifying its ability to predict visually-dependent tokens as well
as 10 pt boost on visually challenging tasks.

</details>


### [81] [VR-RAG: Open-vocabulary Species Recognition with RAG-Assisted Large Multi-Modal Models](https://arxiv.org/pdf/2505.05635)
*Faizan Farooq Khan, Jun Chen, Youssef Mohamed, Chun-Mei Feng, Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: The paper addresses open-vocabulary bird species recognition by proposing VR-RAG, a framework integrating textual knowledge and visual re-ranking, improving performance by 15.4% over existing methods.


<details>
  <summary>Details</summary>
Motivation: Open-vocabulary recognition is challenging due to unbounded categories, especially in nature where new species emerge. Current benchmarks are limited to closed-vocabulary settings.

Method: Proposes VR-RAG, a retrieval-augmented generation framework using GPT-4o for textual summaries and visual re-ranking to recognize unseen species.

Result: VR-RAG improves state-of-the-art performance by 15.4% across five benchmarks, outperforming conventional VLM-based approaches.

Conclusion: The work bridges encyclopedic knowledge and visual recognition, offering a scalable solution for biodiversity monitoring and ecological research.

Abstract: Open-vocabulary recognition remains a challenging problem in computer vision,
as it requires identifying objects from an unbounded set of categories. This is
particularly relevant in nature, where new species are discovered every year.
In this work, we focus on open-vocabulary bird species recognition, where the
goal is to classify species based on their descriptions without being
constrained to a predefined set of taxonomic categories. Traditional benchmarks
like CUB-200-2011 and Birdsnap have been evaluated in a closed-vocabulary
paradigm, limiting their applicability to real-world scenarios where novel
species continually emerge. We show that the performance of current systems
when evaluated under settings closely aligned with open-vocabulary drops by a
huge margin. To address this gap, we propose a scalable framework integrating
structured textual knowledge from Wikipedia articles of 11,202 bird species
distilled via GPT-4o into concise, discriminative summaries. We propose Visual
Re-ranking Retrieval-Augmented Generation(VR-RAG), a novel, retrieval-augmented
generation framework that uses visual similarities to rerank the top m
candidates retrieved by a set of multimodal vision language encoders. This
allows for the recognition of unseen taxa. Extensive experiments across five
established classification benchmarks show that our approach is highly
effective. By integrating VR-RAG, we improve the average performance of
state-of-the-art Large Multi-Modal Model QWEN2.5-VL by 15.4% across five
benchmarks. Our approach outperforms conventional VLM-based approaches, which
struggle with unseen species. By bridging the gap between encyclopedic
knowledge and visual recognition, our work advances open-vocabulary
recognition, offering a flexible, scalable solution for biodiversity monitoring
and ecological research.

</details>


### [82] [Semantic Style Transfer for Enhancing Animal Facial Landmark Detection](https://arxiv.org/pdf/2505.05640)
*Anadil Hussein, Anna Zamansky, George Martvel*

Main category: cs.CV

TL;DR: Neural Style Transfer (NST) is adapted to enhance animal facial landmark detector training, improving structural consistency and robustness through Supervised Style Transfer (SST) and dataset augmentation.


<details>
  <summary>Details</summary>
Motivation: To explore NST's potential for improving animal facial landmark detection, addressing challenges like annotation misalignment and structural consistency.

Method: Applied NST to cropped facial images, introduced SST for style selection, and augmented datasets with style-transferred images.

Result: SST retained 98% baseline accuracy; style-augmented datasets outperformed traditional methods.

Conclusion: Semantic style transfer is effective for enhancing facial landmark detection, generalizable to other species and models.

Abstract: Neural Style Transfer (NST) is a technique for applying the visual
characteristics of one image onto another while preserving structural content.
Traditionally used for artistic transformations, NST has recently been adapted,
e.g., for domain adaptation and data augmentation. This study investigates the
use of this technique for enhancing animal facial landmark detectors training.
As a case study, we use a recently introduced Ensemble Landmark Detector for 48
anatomical cat facial landmarks and the CatFLW dataset it was trained on,
making three main contributions. First, we demonstrate that applying style
transfer to cropped facial images rather than full-body images enhances
structural consistency, improving the quality of generated images. Secondly,
replacing training images with style-transferred versions raised challenges of
annotation misalignment, but Supervised Style Transfer (SST) - which selects
style sources based on landmark accuracy - retained up to 98% of baseline
accuracy. Finally, augmenting the dataset with style-transferred images further
improved robustness, outperforming traditional augmentation methods. These
findings establish semantic style transfer as an effective augmentation
strategy for enhancing the performance of facial landmark detection models for
animals and beyond. While this study focuses on cat facial landmarks, the
proposed method can be generalized to other species and landmark detection
models.

</details>


### [83] [Lost in OCR Translation? Vision-Based Approaches to Robust Document Retrieval](https://arxiv.org/pdf/2505.05666)
*Alexander Most, Joseph Winjum, Ayan Biswas, Shawn Jones, Nishath Rajiv Ranasinghe, Dan O'Malley, Manish Bhattarai*

Main category: cs.CV

TL;DR: The paper compares vision-based RAG (ColPali) with OCR-based RAG, showing OCR generalizes better to unseen documents, while vision-based excels on fine-tuned data. Trade-offs between efficiency and accuracy are discussed.


<details>
  <summary>Details</summary>
Motivation: To address OCR errors in degraded documents and explore vision-language alternatives like ColPali for RAG systems.

Method: Systematic comparison of vision-based (ColPali) and OCR-based (Llama 3.2, Nougat OCR) RAG pipelines across document qualities, using semantic answer evaluation.

Result: Vision-based RAG performs well on fine-tuned documents, but OCR-based RAG generalizes better to varying document qualities.

Conclusion: OCR-based RAG is more generalizable, while vision-based RAG is efficient for specific cases. Practical guidance is provided for choosing between them.

Abstract: Retrieval-Augmented Generation (RAG) has become a popular technique for
enhancing the reliability and utility of Large Language Models (LLMs) by
grounding responses in external documents. Traditional RAG systems rely on
Optical Character Recognition (OCR) to first process scanned documents into
text. However, even state-of-the-art OCRs can introduce errors, especially in
degraded or complex documents. Recent vision-language approaches, such as
ColPali, propose direct visual embedding of documents, eliminating the need for
OCR. This study presents a systematic comparison between a vision-based RAG
system (ColPali) and more traditional OCR-based pipelines utilizing Llama 3.2
(90B) and Nougat OCR across varying document qualities. Beyond conventional
retrieval accuracy metrics, we introduce a semantic answer evaluation benchmark
to assess end-to-end question-answering performance. Our findings indicate that
while vision-based RAG performs well on documents it has been fine-tuned on,
OCR-based RAG is better able to generalize to unseen documents of varying
quality. We highlight the key trade-offs between computational efficiency and
semantic accuracy, offering practical guidance for RAG practitioners in
selecting between OCR-dependent and vision-based document retrieval systems in
production environments.

</details>


### [84] [SuperBench: A Super-Resolution Benchmark Dataset for Scientific Machine Learning](https://arxiv.org/pdf/2306.14070)
*Pu Ren, N. Benjamin Erichson, Junyi Guo, Shashank Subramanian, Omer San, Zarija Lukic, Michael W. Mahoney*

Main category: cs.CV

TL;DR: The paper introduces SuperBench, a benchmark dataset for super-resolution (SR) methods in Scientific Machine Learning (SciML), addressing the lack of standardized benchmarks. It evaluates SR performance, robustness, and the need for domain knowledge in ML models.


<details>
  <summary>Details</summary>
Motivation: The lack of standardized benchmark datasets for SR methods in SciML hinders progress and adoption. The paper aims to fill this gap by introducing SuperBench.

Method: The authors introduce SuperBench, a benchmark dataset with high-resolution data from fluid flows, cosmology, and weather. They validate spatial SR performance from data-centric and physics-preserved perspectives.

Result: Deep learning-based SR methods excel in some tasks but struggle to capture fine-scale features and preserve physical properties in scientific data.

Conclusion: SuperBench is expected to advance SR methods in science by providing a standardized benchmark and highlighting the importance of domain knowledge in ML models.

Abstract: Super-resolution (SR) techniques aim to enhance data resolution, enabling the
retrieval of finer details, and improving the overall quality and fidelity of
the data representation. There is growing interest in applying SR methods to
complex spatiotemporal systems within the Scientific Machine Learning (SciML)
community, with the hope of accelerating numerical simulations and/or improving
forecasts in weather, climate, and related areas. However, the lack of
standardized benchmark datasets for comparing and validating SR methods hinders
progress and adoption in SciML. To address this, we introduce SuperBench, the
first benchmark dataset featuring high-resolution datasets, including data from
fluid flows, cosmology, and weather. Here, we focus on validating spatial SR
performance from data-centric and physics-preserved perspectives, as well as
assessing robustness to data degradation tasks. While deep learning-based SR
methods (developed in the computer vision community) excel on certain tasks,
despite relatively limited prior physics information, we identify limitations
of these methods in accurately capturing intricate fine-scale features and
preserving fundamental physical properties and constraints in scientific data.
These shortcomings highlight the importance and subtlety of incorporating
domain knowledge into ML models. We anticipate that SuperBench will help to
advance SR methods for science.

</details>


### [85] [TeGA: Texture Space Gaussian Avatars for High-Resolution Dynamic Head Modeling](https://arxiv.org/pdf/2505.05672)
*Gengyan Li, Paulo Gotardo, Timo Bolkart, Stephan Garbin, Kripasindhu Sarkar, Abhimitra Meka, Alexandros Lattas, Thabo Beeler*

Main category: cs.CV

TL;DR: A novel high-detail 3D head avatar model improves photorealism and motion capture by increasing 3D Gaussian splatting and introducing a deformable Gaussian encoding method.


<details>
  <summary>Details</summary>
Motivation: Current photoreal avatars lose fidelity due to inaccurate motion estimation and memory limitations, reducing detail and quality.

Method: The model uses a mesh-based 3D morphable model for coarse deformation, with 3D Gaussians in UVD tangent space for photoreal appearance and a novel UVD deformation field for subtle motion.

Result: The method enhances detail and quality, enabling 4K resolution rendering and better facial motion capture.

Conclusion: The deformable Gaussian encoding and fitting procedure preserve detail and improve motion representation, advancing photoreal avatar technology.

Abstract: Sparse volumetric reconstruction and rendering via 3D Gaussian splatting have
recently enabled animatable 3D head avatars that are rendered under arbitrary
viewpoints with impressive photorealism. Today, such photoreal avatars are seen
as a key component in emerging applications in telepresence, extended reality,
and entertainment. Building a photoreal avatar requires estimating the complex
non-rigid motion of different facial components as seen in input video images;
due to inaccurate motion estimation, animatable models typically present a loss
of fidelity and detail when compared to their non-animatable counterparts,
built from an individual facial expression. Also, recent state-of-the-art
models are often affected by memory limitations that reduce the number of 3D
Gaussians used for modeling, leading to lower detail and quality. To address
these problems, we present a new high-detail 3D head avatar model that improves
upon the state of the art, largely increasing the number of 3D Gaussians and
modeling quality for rendering at 4K resolution. Our high-quality model is
reconstructed from multiview input video and builds on top of a mesh-based 3D
morphable model, which provides a coarse deformation layer for the head.
Photoreal appearance is modelled by 3D Gaussians embedded within the continuous
UVD tangent space of this mesh, allowing for more effective densification where
most needed. Additionally, these Gaussians are warped by a novel UVD
deformation field to capture subtle, localized motion. Our key contribution is
the novel deformable Gaussian encoding and overall fitting procedure that
allows our head model to preserve appearance detail, while capturing facial
motion and other transient high-frequency features such as skin wrinkling.

</details>


### [86] [InstanceGen: Image Generation with Instance-level Instructions](https://arxiv.org/pdf/2505.05678)
*Etai Sella, Yanir Kleiman, Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: The paper proposes a method to improve text-to-image generation by combining fine-grained structural guidance from image models with LLM-based instance-level instructions, ensuring better adherence to complex prompts.


<details>
  <summary>Details</summary>
Motivation: Pretrained text-to-image models often fail to capture semantics in complex prompts with multiple objects and attributes, prompting the need for better structural guidance.

Method: The technique integrates fine-grained structural initialization from image models and LLM-based instance-level instructions to guide generation.

Result: The approach produces images that accurately reflect object counts, attributes, and spatial relations as specified in the text prompt.

Conclusion: Combining structural guidance with LLM instructions enhances the fidelity of text-to-image generation for complex prompts.

Abstract: Despite rapid advancements in the capabilities of generative models,
pretrained text-to-image models still struggle in capturing the semantics
conveyed by complex prompts that compound multiple objects and instance-level
attributes. Consequently, we are witnessing growing interests in integrating
additional structural constraints, %leveraging additional structural inputs
typically in the form of coarse bounding boxes, to better guide the generation
process in such challenging cases. In this work, we take the idea of structural
guidance a step further by making the observation that contemporary image
generation models can directly provide a plausible \emph{fine-grained}
structural initialization. We propose a technique that couples this image-based
structural guidance with LLM-based instance-level instructions, yielding output
images that adhere to all parts of the text prompt, including object counts,
instance-level attributes, and spatial relations between instances.

</details>


### [87] [Fine-Tuning Video-Text Contrastive Model for Primate Behavior Retrieval from Unlabeled Raw Videos](https://arxiv.org/pdf/2505.05681)
*Giulio Cesare Mastrocinque Santo, Patrícia Izar, Irene Delval, Victor de Napole Gregolin, Nina S. T. Hirata*

Main category: cs.CV

TL;DR: Fine-tuning pre-trained video-text models for capuchin monkey behavior analysis using noisy, unlabeled videos and weak audio descriptions, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: To develop computational models for retrieving useful clips from raw, unlabeled primate videos, leveraging weak audio descriptions.

Method: A two-folded approach: an agentic data treatment pipeline to extract clean video-text pairs and fine-tuning a pre-trained X-CLIP model using LoRA.

Result: 167% uplift in Hits@5 for 16 frames and 114% for 8 frames; improved ranking of behaviors (NDCG@K).

Conclusion: The proposed method effectively addresses noise in video and audio, enhancing retrieval performance for primate behavior studies.

Abstract: Video recordings of nonhuman primates in their natural habitat are a common
source for studying their behavior in the wild. We fine-tune pre-trained
video-text foundational models for the specific domain of capuchin monkeys,
with the goal of developing useful computational models to help researchers to
retrieve useful clips from videos. We focus on the challenging problem of
training a model based solely on raw, unlabeled video footage, using weak audio
descriptions sometimes provided by field collaborators. We leverage recent
advances in Multimodal Large Language Models (MLLMs) and Vision-Language Models
(VLMs) to address the extremely noisy nature of both video and audio content.
Specifically, we propose a two-folded approach: an agentic data treatment
pipeline and a fine-tuning process. The data processing pipeline automatically
extracts clean and semantically aligned video-text pairs from the raw videos,
which are subsequently used to fine-tune a pre-trained Microsoft's X-CLIP model
through Low-Rank Adaptation (LoRA). We obtained an uplift in $Hits@5$ of
$167\%$ for the 16 frames model and an uplift of $114\%$ for the 8 frame model
on our domain data. Moreover, based on $NDCG@K$ results, our model is able to
rank well most of the considered behaviors, while the tested raw pre-trained
models are not able to rank them at all. The code will be made available upon
acceptance.

</details>


### [88] [DiGIT: Multi-Dilated Gated Encoder and Central-Adjacent Region Integrated Decoder for Temporal Action Detection Transformer](https://arxiv.org/pdf/2505.05711)
*Ho-Joong Kim, Yearang Lee, Jung-Ho Hong, Seong-Whan Lee*

Main category: cs.CV

TL;DR: The paper identifies limitations in query-based detectors for temporal action detection (TAD) due to their adaptation from object detection architectures. It proposes DiGIT, a transformer-based model with a multi-dilated gated encoder and central-adjacent region integrated decoder, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing TAD models, adapted from object detection, struggle with redundancy in multi-scale features and insufficient temporal context capture.

Method: DiGIT replaces the standard encoder with a multi-dilated gated encoder to reduce redundancy and enhance temporal context. It also introduces a central-adjacent region integrated decoder for better sampling in deformable cross-attention.

Result: DiGIT outperforms existing models on THUMOS14, ActivityNet v1.3, and HACS-Segment benchmarks.

Conclusion: The proposed DiGIT model effectively addresses TAD challenges, offering improved performance and a novel architecture for future research.

Abstract: In this paper, we examine a key limitation in query-based detectors for
temporal action detection (TAD), which arises from their direct adaptation of
originally designed architectures for object detection. Despite the
effectiveness of the existing models, they struggle to fully address the unique
challenges of TAD, such as the redundancy in multi-scale features and the
limited ability to capture sufficient temporal context. To address these
issues, we propose a multi-dilated gated encoder and central-adjacent region
integrated decoder for temporal action detection transformer (DiGIT). Our
approach replaces the existing encoder that consists of multi-scale deformable
attention and feedforward network with our multi-dilated gated encoder. Our
proposed encoder reduces the redundant information caused by multi-level
features while maintaining the ability to capture fine-grained and long-range
temporal information. Furthermore, we introduce a central-adjacent region
integrated decoder that leverages a more comprehensive sampling strategy for
deformable cross-attention to capture the essential information. Extensive
experiments demonstrate that DiGIT achieves state-of-the-art performance on
THUMOS14, ActivityNet v1.3, and HACS-Segment. Code is available at:
https://github.com/Dotori-HJ/DiGIT

</details>


### [89] [Semantic-Space-Intervened Diffusive Alignment for Visual Classification](https://arxiv.org/pdf/2505.05721)
*Zixuan Li, Lei Meng, Guoqing Chao, Wei Wu, Xiaoshuo Yan, Yimeng Yang, Zhuang Qi, Xiangxu Meng*

Main category: cs.CV

TL;DR: SeDA improves cross-modal alignment by introducing a semantic space and a bi-stage diffusion framework for progressive alignment between visual and textual features.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with one-step mapping due to distribution mismatches between visual and textual features.

Method: SeDA uses a semantic space as a bridge and a bi-stage diffusion framework (Diffusion-Controlled Semantic Learner and Translator) with progressive feature interactions.

Result: SeDA outperforms existing methods in cross-modal alignment across multiple scenarios.

Conclusion: SeDA effectively addresses alignment challenges by leveraging semantic space and progressive diffusion, enhancing visual classification.

Abstract: Cross-modal alignment is an effective approach to improving visual
classification. Existing studies typically enforce a one-step mapping that uses
deep neural networks to project the visual features to mimic the distribution
of textual features. However, they typically face difficulties in finding such
a projection due to the two modalities in both the distribution of class-wise
samples and the range of their feature values. To address this issue, this
paper proposes a novel Semantic-Space-Intervened Diffusive Alignment method,
termed SeDA, models a semantic space as a bridge in the visual-to-textual
projection, considering both types of features share the same class-level
information in classification. More importantly, a bi-stage diffusion framework
is developed to enable the progressive alignment between the two modalities.
Specifically, SeDA first employs a Diffusion-Controlled Semantic Learner to
model the semantic features space of visual features by constraining the
interactive features of the diffusion model and the category centers of visual
features. In the later stage of SeDA, the Diffusion-Controlled Semantic
Translator focuses on learning the distribution of textual features from the
semantic space. Meanwhile, the Progressive Feature Interaction Network
introduces stepwise feature interactions at each alignment step, progressively
integrating textual information into mapped features. Experimental results show
that SeDA achieves stronger cross-modal feature alignment, leading to superior
performance over existing methods across multiple scenarios.

</details>


### [90] [You Are Your Best Teacher: Semi-Supervised Surgical Point Tracking with Cycle-Consistent Self-Distillation](https://arxiv.org/pdf/2505.05722)
*Valay Bundele, Mehran Hosseinzadeh, Hendrik Lensch*

Main category: cs.CV

TL;DR: SurgTracker, a semi-supervised framework, adapts synthetic-trained point trackers to surgical videos using filtered self-distillation, improving performance with minimal labeled data.


<details>
  <summary>Details</summary>
Motivation: Overcoming domain shift and lack of labeled data in surgical videos, which exhibit complex challenges like tissue deformation and lighting variation.

Method: Uses filtered self-distillation with a fixed teacher-student setup, generating pseudo-labels online and filtering them via cycle consistency for geometric stability.

Result: Achieves improved tracking performance on the STIR benchmark using only 80 unlabeled videos.

Conclusion: Demonstrates robust adaptation in high-shift, data-scarce domains like surgery.

Abstract: Synthetic datasets have enabled significant progress in point tracking by
providing large-scale, densely annotated supervision. However, deploying these
models in real-world domains remains challenging due to domain shift and lack
of labeled data-issues that are especially severe in surgical videos, where
scenes exhibit complex tissue deformation, occlusion, and lighting variation.
While recent approaches adapt synthetic-trained trackers to natural videos
using teacher ensembles or augmentation-heavy pseudo-labeling pipelines, their
effectiveness in high-shift domains like surgery remains unexplored. This work
presents SurgTracker, a semi-supervised framework for adapting
synthetic-trained point trackers to surgical video using filtered
self-distillation. Pseudo-labels are generated online by a fixed
teacher-identical in architecture and initialization to the student-and are
filtered using a cycle consistency constraint to discard temporally
inconsistent trajectories. This simple yet effective design enforces geometric
consistency and provides stable supervision throughout training, without the
computational overhead of maintaining multiple teachers. Experiments on the
STIR benchmark show that SurgTracker improves tracking performance using only
80 unlabeled videos, demonstrating its potential for robust adaptation in
high-shift, data-scarce domains.

</details>


### [91] [Dome-DETR: DETR with Density-Oriented Feature-Query Manipulation for Efficient Tiny Object Detection](https://arxiv.org/pdf/2505.05741)
*Zhangchi Hu, Peixi Wu, Jie Chen, Huyue Zhu, Yijun Wang, Yansong Peng, Hebei Li, Xiaoyan Sun*

Main category: cs.CV

TL;DR: Dome-DETR is a novel framework for efficient tiny object detection, addressing feature redundancy and query allocation issues with Density-Oriented Feature-Query Manipulation.


<details>
  <summary>Details</summary>
Motivation: Existing methods for tiny object detection suffer from inefficient feature leverage and high computational costs due to redundant processing and rigid query allocation.

Method: Proposes Density-Focal Extractor (DeFE) for compact foreground masks, Masked Window Attention Sparsification (MWAS) for focused computation, and Progressive Adaptive Query Initialization (PAQI) for adaptive query density.

Result: Achieves state-of-the-art performance (+3.3 AP on AI-TOD-V2, +2.5 AP on VisDrone) with low computational complexity and compact model size.

Conclusion: Dome-DETR effectively improves tiny object detection efficiency and performance, offering a practical solution for applications like drone surveillance and autonomous systems.

Abstract: Tiny object detection plays a vital role in drone surveillance, remote
sensing, and autonomous systems, enabling the identification of small targets
across vast landscapes. However, existing methods suffer from inefficient
feature leverage and high computational costs due to redundant feature
processing and rigid query allocation. To address these challenges, we propose
Dome-DETR, a novel framework with Density-Oriented Feature-Query Manipulation
for Efficient Tiny Object Detection. To reduce feature redundancies, we
introduce a lightweight Density-Focal Extractor (DeFE) to produce clustered
compact foreground masks. Leveraging these masks, we incorporate Masked Window
Attention Sparsification (MWAS) to focus computational resources on the most
informative regions via sparse attention. Besides, we propose Progressive
Adaptive Query Initialization (PAQI), which adaptively modulates query density
across spatial areas for better query allocation. Extensive experiments
demonstrate that Dome-DETR achieves state-of-the-art performance (+3.3 AP on
AI-TOD-V2 and +2.5 AP on VisDrone) while maintaining low computational
complexity and a compact model size. Code will be released upon acceptance.

</details>


### [92] [kFuse: A novel density based agglomerative clustering](https://arxiv.org/pdf/2505.05748)
*Huan Yan, Junjie Hu*

Main category: cs.CV

TL;DR: The paper introduces kFuse, a density-based agglomerative clustering method, addressing parameter dependency and instability in traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing agglomerative clustering methods require parameter tuning and suffer from unstable results due to distance calculation constraints.

Method: kFuse uses natural neighbors for sub-cluster partitioning, boundary connectivity via adjacent samples and shortest distances, density similarity via mean and variance, and merging rules based on connectivity and similarity.

Result: kFuse improves accuracy in merging and requires only the number of clusters at the final stage, validated by synthetic and real-world datasets.

Conclusion: kFuse effectively addresses limitations of traditional agglomerative clustering, enhancing stability and accuracy.

Abstract: Agglomerative clustering has emerged as a vital tool in data analysis due to
its intuitive and flexible characteristics. However, existing agglomerative
clustering methods often involve additional parameters for sub-cluster
partitioning and inter-cluster similarity assessment. This necessitates
different parameter settings across various datasets, which is undoubtedly
challenging in the absence of prior knowledge. Moreover, existing agglomerative
clustering techniques are constrained by the calculation method of connection
distance, leading to unstable clustering results. To address these issues, this
paper introduces a novel density-based agglomerative clustering method, termed
kFuse. kFuse comprises four key components: (1) sub-cluster partitioning based
on natural neighbors; (2) determination of boundary connectivity between
sub-clusters through the computation of adjacent samples and shortest
distances; (3) assessment of density similarity between sub-clusters via the
calculation of mean density and variance; and (4) establishment of merging
rules between sub-clusters based on boundary connectivity and density
similarity. kFuse requires the specification of the number of clusters only at
the final merging stage. Additionally, by comprehensively considering adjacent
samples, distances, and densities among different sub-clusters, kFuse
significantly enhances accuracy during the merging phase, thereby greatly
improving its identification capability. Experimental results on both synthetic
and real-world datasets validate the effectiveness of kFuse.

</details>


### [93] [A review of advancements in low-light image enhancement using deep learning](https://arxiv.org/pdf/2505.05759)
*Fangxue Liu, Lei Fan*

Main category: cs.CV

TL;DR: This review surveys deep-learning-based low-light image enhancement methods since 2020, explaining their mechanisms and evaluating their impact on downstream vision tasks, while suggesting future research directions.


<details>
  <summary>Details</summary>
Motivation: The performance of computer vision algorithms degrades in low-light conditions, and while deep learning has advanced low-light image processing, a systematic review of recent methods and their effectiveness is lacking.

Method: The review examines various deep-learning-based low-light image enhancement approaches, detailing their mechanisms and illustrating their functioning. It also evaluates their impact on downstream tasks like segmentation, detection, and classification.

Result: The review critically analyzes the strengths and limitations of different enhancement techniques and their effects on vision tasks.

Conclusion: The paper serves as a reference for selecting and optimizing low-light image enhancement methods to improve vision task performance, and it outlines future research directions.

Abstract: In low-light environments, the performance of computer vision algorithms
often deteriorates significantly, adversely affecting key vision tasks such as
segmentation, detection, and classification. With the rapid advancement of deep
learning, its application to low-light image processing has attracted
widespread attention and seen significant progress in recent years. However,
there remains a lack of comprehensive surveys that systematically examine how
recent deep-learning-based low-light image enhancement methods function and
evaluate their effectiveness in enhancing downstream vison tasks. To address
this gap, this review provides a detailed elaboration on how various recent
approaches (from 2020) operate and their enhancement mechanisms, supplemented
with clear illustrations. It also investigates the impact of different
enhancement techniques on subsequent vision tasks, critically analyzing their
strengths and limitations. Additionally, it proposes future research
directions. This review serves as a useful reference for determining low-light
image enhancement techniques and optimizing vision task performance in
low-light conditions.

</details>


### [94] [Describe Anything in Medical Images](https://arxiv.org/pdf/2505.05804)
*Xi Xiao, Yunbei Zhang, Thanh-Huy Nguyen, Ba-Thinh Lam, Janet Wang, Jihun Hamm, Tianyang Wang, Xingjian Li, Xiao Wang, Hao Xu, Tianming Liu, Min Xu*

Main category: cs.CV

TL;DR: MedDAM is a new framework for region-specific captioning in medical images, outperforming existing models by focusing on clinical factuality and region-level alignment.


<details>
  <summary>Details</summary>
Motivation: Existing localized captioning models like DAM lack application in specialized domains like medical imaging, where regional findings are critical for diagnosis.

Method: MedDAM uses expert-designed prompts, a custom evaluation benchmark, and attribute-level verification to assess models without ground-truth region-caption pairs.

Result: MedDAM outperforms leading models (e.g., GPT-4o, Claude 3.7) on medical datasets, highlighting its clinical relevance.

Conclusion: MedDAM advances medical image understanding and serves as a foundation for clinical vision-language integration.

Abstract: Localized image captioning has made significant progress with models like the
Describe Anything Model (DAM), which can generate detailed region-specific
descriptions without explicit region-text supervision. However, such
capabilities have yet to be widely applied to specialized domains like medical
imaging, where diagnostic interpretation relies on subtle regional findings
rather than global understanding. To mitigate this gap, we propose MedDAM, the
first comprehensive framework leveraging large vision-language models for
region-specific captioning in medical images. MedDAM employs medical
expert-designed prompts tailored to specific imaging modalities and establishes
a robust evaluation benchmark comprising a customized assessment protocol, data
pre-processing pipeline, and specialized QA template library. This benchmark
evaluates both MedDAM and other adaptable large vision-language models,
focusing on clinical factuality through attribute-level verification tasks,
thereby circumventing the absence of ground-truth region-caption pairs in
medical datasets. Extensive experiments on the VinDr-CXR, LIDC-IDRI, and
SkinCon datasets demonstrate MedDAM's superiority over leading peers (including
GPT-4o, Claude 3.7 Sonnet, LLaMA-3.2 Vision, Qwen2.5-VL, GPT-4Rol, and
OMG-LLaVA) in the task, revealing the importance of region-level semantic
alignment in medical image understanding and establishing MedDAM as a promising
foundation for clinical vision-language integration.

</details>


### [95] [Leveraging Vision-Language Models for Visual Grounding and Analysis of Automotive UI](https://arxiv.org/pdf/2505.05895)
*Benjamin Raphael Ernhofer, Daniil Prokhorov, Jannica Langner, Dominik Bollmann*

Main category: cs.CV

TL;DR: A vision-language framework for automotive infotainment systems is introduced, leveraging a synthetic data pipeline and fine-tuned Molmo-7B model (ELAM) to achieve strong performance and cross-domain generalization.


<details>
  <summary>Details</summary>
Motivation: Address the need for adaptive solutions in automotive infotainment systems due to frequent UI updates and diverse designs.

Method: Develop a synthetic data pipeline, fine-tune Molmo-7B using LoRa, and incorporate reasoning, visual grounding, and evaluation capabilities.

Result: ELAM achieves 80.4% accuracy on ScreenSpot, outperforming baselines and matching specialized models like ShowUI.

Conclusion: The cost-efficient method demonstrates AI-driven progress in automotive UI understanding, deployable on consumer-grade GPUs.

Abstract: Modern automotive infotainment systems require intelligent and adaptive
solutions to handle frequent User Interface (UI) updates and diverse design
variations. We introduce a vision-language framework for understanding and
interacting with automotive infotainment systems, enabling seamless adaptation
across different UI designs. To further support research in this field, we
release AutomotiveUI-Bench-4K, an open-source dataset of 998 images with 4,208
annotations. Additionally, we present a synthetic data pipeline to generate
training data. We fine-tune a Molmo-7B-based model using Low-Rank Adaptation
(LoRa) and incorporating reasoning generated by our pipeline, along with visual
grounding and evaluation capabilities. The fine-tuned Evaluative Large Action
Model (ELAM) achieves strong performance on AutomotiveUI-Bench-4K (model and
dataset are available on Hugging Face) and demonstrating strong cross-domain
generalization, including a +5.2% improvement on ScreenSpot over the baseline
model. Notably, our approach achieves 80.4% average accuracy on ScreenSpot,
closely matching or even surpassing specialized models for desktop, mobile, and
web, such as ShowUI, despite being trained for the infotainment domain. This
research investigates how data collection and subsequent fine-tuning can lead
to AI-driven progress within automotive UI understanding and interaction. The
applied method is cost-efficient and fine-tuned models can be deployed on
consumer-grade GPUs.

</details>


### [96] [Image Segmentation via Variational Model Based Tailored UNet: A Deep Variational Framework](https://arxiv.org/pdf/2505.05806)
*Kaili Qi, Wenli Yang, Ye Li, Zhongyi Huang*

Main category: cs.CV

TL;DR: VM_TUNet combines variational models and UNet for better segmentation, balancing interpretability and feature learning.


<details>
  <summary>Details</summary>
Motivation: Traditional methods lack adaptability, while deep learning lacks interpretability. VM_TUNet aims to merge their strengths.

Method: Integrates a fourth-order modified Cahn-Hilliard equation with UNet, using TFPM for boundary precision and a data-driven operator for parameter tuning.

Result: Outperforms existing methods, especially in fine boundary delineation.

Conclusion: VM_TUNet successfully merges variational and deep learning advantages for superior segmentation.

Abstract: Traditional image segmentation methods, such as variational models based on
partial differential equations (PDEs), offer strong mathematical
interpretability and precise boundary modeling, but often suffer from
sensitivity to parameter settings and high computational costs. In contrast,
deep learning models such as UNet, which are relatively lightweight in
parameters, excel in automatic feature extraction but lack theoretical
interpretability and require extensive labeled data. To harness the
complementary strengths of both paradigms, we propose Variational Model Based
Tailored UNet (VM_TUNet), a novel hybrid framework that integrates the
fourth-order modified Cahn-Hilliard equation with the deep learning backbone of
UNet, which combines the interpretability and edge-preserving properties of
variational methods with the adaptive feature learning of neural networks.
Specifically, a data-driven operator is introduced to replace manual parameter
tuning, and we incorporate the tailored finite point method (TFPM) to enforce
high-precision boundary preservation. Experimental results on benchmark
datasets demonstrate that VM_TUNet achieves superior segmentation performance
compared to existing approaches, especially for fine boundary delineation.

</details>


### [97] [Examining the Source of Defects from a Mechanical Perspective for 3D Anomaly Detection](https://arxiv.org/pdf/2505.05901)
*Hanzhe Liang, Aoran Wang, Jie Zhou, Xin Jin, Can Gao, Jinbao Wang*

Main category: cs.CV

TL;DR: The paper introduces MC4AD, a Mechanics Complementary framework for 3D anomaly detection, focusing on anomaly causes and corrective forces. It includes DA-Gen for anomaly simulation, CFP-Net for corrective force prediction, and achieves state-of-the-art results with minimal parameters and fast inference.


<details>
  <summary>Details</summary>
Motivation: Anomalies are often caused by unpredictable defective forces; the paper aims to improve anomaly detection by addressing these causes through corrective forces.

Method: Proposes MC4AD with DA-Gen for anomaly simulation and CFP-Net for corrective force prediction. Uses a combined loss (symmetric and overall) for training.

Result: Achieves nine state-of-the-art results on six datasets with minimal parameters and fast inference.

Conclusion: MC4AD effectively addresses 3D anomaly detection by simulating anomalies and predicting corrective forces, validated by superior performance and a new dataset.

Abstract: In this paper, we go beyond identifying anomalies only in structural terms
and think about better anomaly detection motivated by anomaly causes. Most
anomalies are regarded as the result of unpredictable defective forces from
internal and external sources, and their opposite forces are sought to correct
the anomalies. We introduced a Mechanics Complementary framework for 3D anomaly
detection (MC4AD) to generate internal and external Corrective forces for each
point. A Diverse Anomaly-Generation (DA-Gen) module is first proposed to
simulate various anomalies. Then, we present a Corrective Force Prediction
Network (CFP-Net) with complementary representations for point-level
representation to simulate the different contributions of internal and external
corrective forces. A combined loss was proposed, including a new symmetric loss
and an overall loss, to constrain the corrective forces properly. As a
highlight, we consider 3D anomaly detection in industry more comprehensively,
creating a hierarchical quality control strategy based on a three-way decision
and contributing a dataset named Anomaly-IntraVariance with intraclass variance
to evaluate the model. On the proposed and existing five datasets, we obtained
nine state-of-the-art performers with the minimum parameters and the fastest
inference speed. The source is available at
https://github.com/hzzzzzhappy/MC4AD

</details>


### [98] [Dual-level Fuzzy Learning with Patch Guidance for Image Ordinal Regression](https://arxiv.org/pdf/2505.05834)
*Chunlai Dong, Haochao Ying, Qibo Qiu, Jinhong Wang, Danny Chen, Jian Wu*

Main category: cs.CV

TL;DR: The paper introduces DFPG, a framework for ordinal regression that leverages patch-level features and fuzzy logic to handle label ambiguity, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current ordinal regression methods lack patch-level feature utilization due to reliance on image-level labels, limiting their discriminative power.

Method: Proposes DFPG with patch-labeling, filtering strategies, and a dual-level fuzzy learning module to capture label ambiguity from patch-wise and channel-wise perspectives.

Result: DFPG outperforms existing methods on various datasets, especially in distinguishing difficult-to-classify categories.

Conclusion: DFPG effectively bridges the gap between patch-level features and image-level labels, enhancing ordinal regression performance.

Abstract: Ordinal regression bridges regression and classification by assigning objects
to ordered classes. While human experts rely on discriminative patch-level
features for decisions, current approaches are limited by the availability of
only image-level ordinal labels, overlooking fine-grained patch-level
characteristics. In this paper, we propose a Dual-level Fuzzy Learning with
Patch Guidance framework, named DFPG that learns precise feature-based grading
boundaries from ambiguous ordinal labels, with patch-level supervision.
Specifically, we propose patch-labeling and filtering strategies to enable the
model to focus on patch-level features exclusively with only image-level
ordinal labels available. We further design a dual-level fuzzy learning module,
which leverages fuzzy logic to quantitatively capture and handle label
ambiguity from both patch-wise and channel-wise perspectives. Extensive
experiments on various image ordinal regression datasets demonstrate the
superiority of our proposed method, further confirming its ability in
distinguishing samples from difficult-to-classify categories. The code is
available at https://github.com/ZJUMAI/DFPG-ord.

</details>


### [99] [Achieving 3D Attention via Triplet Squeeze and Excitation Block](https://arxiv.org/pdf/2505.05943)
*Maan Alhazmi, Abdulrahman Altahhan*

Main category: cs.CV

TL;DR: The paper introduces TripSE, a new attention mechanism combining Triplet attention and Squeeze-and-Excitation, applied to CNN models like ResNet18, DenseNet, and ConvNeXt, achieving state-of-the-art results in FER.


<details>
  <summary>Details</summary>
Motivation: To enhance CNN-based models for vision tasks, particularly facial expression recognition (FER), by integrating advanced attention mechanisms.

Method: Proposes four TripSE variants and integrates them into ResNet18, DenseNet, and ConvNeXt architectures, evaluating performance on CIFAR100, ImageNet, FER2013, and AffectNet datasets.

Result: ConvNeXt with TripSE achieves 78.27% accuracy on FER2013, setting a new benchmark.

Conclusion: TripSE significantly boosts CNN model performance, especially ConvNeXt, demonstrating its effectiveness in FER.

Abstract: The emergence of ConvNeXt and its variants has reaffirmed the conceptual and
structural suitability of CNN-based models for vision tasks, re-establishing
them as key players in image classification in general, and in facial
expression recognition (FER) in particular. In this paper, we propose a new set
of models that build on these advancements by incorporating a new set of
attention mechanisms that combines Triplet attention with
Squeeze-and-Excitation (TripSE) in four different variants. We demonstrate the
effectiveness of these variants by applying them to the ResNet18, DenseNet and
ConvNext architectures to validate their versatility and impact. Our study
shows that incorporating a TripSE block in these CNN models boosts their
performances, particularly for the ConvNeXt architecture, indicating its
utility. We evaluate the proposed mechanisms and associated models across four
datasets, namely CIFAR100, ImageNet, FER2013 and AffectNet datasets, where
ConvNext with TripSE achieves state-of-the-art results with an accuracy of
\textbf{78.27\%} on the popular FER2013 dataset, a new feat for this dataset.

</details>


### [100] [Automated Knot Detection and Pairing for Wood Analysis in the Timber Industry](https://arxiv.org/pdf/2505.05845)
*Guohao Lin, Shidong Pan, Rasul Khanbayov, Changxi Yang, Ani Khaloian-Sarnaghi, Andriy Kovryga*

Main category: cs.CV

TL;DR: A lightweight, automated pipeline for knot detection and pairing in wood using machine learning, achieving high accuracy in detection (mAP@0.5: 0.887) and pairing (0.85).


<details>
  <summary>Details</summary>
Motivation: Traditional manual knot annotation is labor-intensive and inefficient, prompting the need for automation in timber processing.

Method: Uses YOLOv8l for knot detection and a triplet neural network for pairing, leveraging multidimensional feature extraction and clustering.

Result: Achieved mAP@0.5 of 0.887 for detection and 0.85 pairing accuracy. Key features like knot positions and longitudinal coordinates were critical.

Conclusion: The proposed AI solution is effective, showcasing potential for advancing wood science and industry.

Abstract: Knots in wood are critical to both aesthetics and structural integrity,
making their detection and pairing essential in timber processing. However,
traditional manual annotation was labor-intensive and inefficient,
necessitating automation. This paper proposes a lightweight and fully automated
pipeline for knot detection and pairing based on machine learning techniques.
In the detection stage, high-resolution surface images of wooden boards were
collected using industrial-grade cameras, and a large-scale dataset was
manually annotated and preprocessed. After the transfer learning, the YOLOv8l
achieves an mAP@0.5 of 0.887. In the pairing stage, detected knots were
analyzed and paired based on multidimensional feature extraction. A triplet
neural network was used to map the features into a latent space, enabling
clustering algorithms to identify and pair corresponding knots. The triplet
network with learnable weights achieved a pairing accuracy of 0.85. Further
analysis revealed that he distances from the knot's start and end points to the
bottom of the wooden board, and the longitudinal coordinates play crucial roles
in achieving high pairing accuracy. Our experiments validate the effectiveness
of the proposed solution, demonstrating the potential of AI in advancing wood
science and industry.

</details>


### [101] [RefRef: A Synthetic Dataset and Benchmark for Reconstructing Refractive and Reflective Objects](https://arxiv.org/pdf/2505.05848)
*Yue Yin, Enze Tao, Weijian Deng, Dylan Campbell*

Main category: cs.CV

TL;DR: The paper introduces a synthetic dataset (RefRef) for 3D reconstruction of refractive and reflective objects, proposes an oracle method for accurate light path calculation, and benchmarks it against existing methods, showing significant gaps in performance.


<details>
  <summary>Details</summary>
Motivation: Existing 3D reconstruction methods struggle with refractive and reflective materials due to limited datasets and assumptions about light paths.

Method: The authors create a synthetic dataset (RefRef) with 150 scenes of varying complexity and propose an oracle method for accurate light path calculation, along with a practical approach.

Result: Benchmarking shows current methods perform poorly compared to the oracle, underscoring the difficulty of handling refractive and reflective materials.

Conclusion: The RefRef dataset and benchmark highlight challenges in reconstructing scenes with refractive/reflective objects, urging further research in this area.

Abstract: Modern 3D reconstruction and novel view synthesis approaches have
demonstrated strong performance on scenes with opaque Lambertian objects.
However, most assume straight light paths and therefore cannot properly handle
refractive and reflective materials. Moreover, datasets specialized for these
effects are limited, stymieing efforts to evaluate performance and develop
suitable techniques. In this work, we introduce a synthetic RefRef dataset and
benchmark for reconstructing scenes with refractive and reflective objects from
posed images. Our dataset has 50 such objects of varying complexity, from
single-material convex shapes to multi-material non-convex shapes, each placed
in three different background types, resulting in 150 scenes. We also propose
an oracle method that, given the object geometry and refractive indices,
calculates accurate light paths for neural rendering, and an approach based on
this that avoids these assumptions. We benchmark these against several
state-of-the-art methods and show that all methods lag significantly behind the
oracle, highlighting the challenges of the task and dataset.

</details>


### [102] [PICD: Versatile Perceptual Image Compression with Diffusion Rendering](https://arxiv.org/pdf/2505.05853)
*Tongda Xu, Jiahao Li, Bin Li, Yan Wang, Ya-Qin Zhang, Yan Lu*

Main category: cs.CV

TL;DR: PICD is a perceptual image compression method for screen and natural images, using diffusion rendering to address text artifacts.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with text artifacts in screen content compression.

Method: Separate encoding of text and images, rendered using a diffusion model with domain, adaptor, and instance-level conditioning.

Result: PICD outperforms existing codecs in text accuracy and perceptual quality, also works for natural images.

Conclusion: PICD is a versatile solution for high-quality compression of both screen and natural images.

Abstract: Recently, perceptual image compression has achieved significant advancements,
delivering high visual quality at low bitrates for natural images. However, for
screen content, existing methods often produce noticeable artifacts when
compressing text. To tackle this challenge, we propose versatile perceptual
screen image compression with diffusion rendering (PICD), a codec that works
well for both screen and natural images. More specifically, we propose a
compression framework that encodes the text and image separately, and renders
them into one image using diffusion model. For this diffusion rendering, we
integrate conditional information into diffusion models at three distinct
levels: 1). Domain level: We fine-tune the base diffusion model using text
content prompts with screen content. 2). Adaptor level: We develop an efficient
adaptor to control the diffusion model using compressed image and text as
input. 3). Instance level: We apply instance-wise guidance to further enhance
the decoding process. Empirically, our PICD surpasses existing perceptual
codecs in terms of both text accuracy and perceptual quality. Additionally,
without text conditions, our approach serves effectively as a perceptual codec
for natural images.

</details>


### [103] [Decoupling Multi-Contrast Super-Resolution: Pairing Unpaired Synthesis with Implicit Representations](https://arxiv.org/pdf/2505.05855)
*Hongyu Rui, Yinzhe Wu, Fanwen Wang, Jiahao Huang, Liutao Yang, Zi Wang, Guang Yang*

Main category: cs.CV

TL;DR: A novel Modular Multi-Contrast Super-Resolution (MCSR) framework is proposed to enhance MRI quality without requiring paired training data, achieving superior 4x and 8x upscaling with improved fidelity.


<details>
  <summary>Details</summary>
Motivation: MRI's long acquisition times and low signal-to-noise ratios, especially in diffusion and functional MRI, motivate cross-modal enhancement using high-resolution references to boost low-resolution counterparts.

Method: The framework decouples MCSR into two stages: Unpaired Cross-Modal Synthesis (U-CMS) to translate a high-resolution reference into the target contrast, and Unsupervised Super-Resolution (U-SR) using implicit neural representations for scale-agnostic reconstruction.

Result: The method outperforms existing baselines at 4x and 8x upscaling, showing improved fidelity and anatomical consistency.

Conclusion: The framework is scalable, subject-specific, and data-efficient, demonstrating strong potential for real-world clinical MCSR applications.

Abstract: Magnetic Resonance Imaging (MRI) is critical for clinical diagnostics but is
often limited by long acquisition times and low signal-to-noise ratios,
especially in modalities like diffusion and functional MRI. The multi-contrast
nature of MRI presents a valuable opportunity for cross-modal enhancement,
where high-resolution (HR) modalities can serve as references to boost the
quality of their low-resolution (LR) counterparts-motivating the development of
Multi-Contrast Super-Resolution (MCSR) techniques. Prior work has shown that
leveraging complementary contrasts can improve SR performance; however,
effective feature extraction and fusion across modalities with varying
resolutions remains a major challenge. Moreover, existing MCSR methods often
assume fixed resolution settings and all require large, perfectly paired
training datasets-conditions rarely met in real-world clinical environments. To
address these challenges, we propose a novel Modular Multi-Contrast
Super-Resolution (MCSR) framework that eliminates the need for paired training
data and supports arbitrary upscaling. Our method decouples the MCSR task into
two stages: (1) Unpaired Cross-Modal Synthesis (U-CMS), which translates a
high-resolution reference modality into a synthesized version of the target
contrast, and (2) Unsupervised Super-Resolution (U-SR), which reconstructs the
final output using implicit neural representations (INRs) conditioned on
spatial coordinates. This design enables scale-agnostic and anatomically
faithful reconstruction by bridging un-paired cross-modal synthesis with
unsupervised resolution enhancement. Experiments show that our method achieves
superior performance at 4x and 8x upscaling, with improved fidelity and
anatomical consistency over existing baselines. Our framework demonstrates
strong potential for scalable, subject-specific, and data-efficient MCSR in
real-world clinical settings.

</details>


### [104] [Register and CLS tokens yield a decoupling of local and global features in large ViTs](https://arxiv.org/pdf/2505.05892)
*Alexander Lappe, Martin A. Giese*

Main category: cs.CV

TL;DR: DINOv2 model's attention maps have artifacts due to redundant patch tokens storing global info. Register tokens clean maps but disrupt local-global feature integration. CLS tokens cause similar issues, highlighting the need for careful attention map interpretation in large ViTs.


<details>
  <summary>Details</summary>
Motivation: Address artifacts in DINOv2's attention maps that harm interpretability and dense task performance, caused by patch tokens storing global info redundantly.

Method: Introduce register tokens to store global info, analyze their impact on local-global feature relationships, and compare with CLS token behavior.

Result: Register tokens clean attention maps but disrupt local-global integration; CLS tokens exhibit similar issues, revealing broader challenges.

Conclusion: Care is needed when interpreting attention maps in large ViTs; understanding register and CLS token roles can improve model interpretability.

Abstract: Recent work has shown that the attention maps of the widely popular DINOv2
model exhibit artifacts, which hurt both model interpretability and performance
on dense image tasks. These artifacts emerge due to the model repurposing patch
tokens with redundant local information for the storage of global image
information. To address this problem, additional register tokens have been
incorporated in which the model can store such information instead. We
carefully examine the influence of these register tokens on the relationship
between global and local image features, showing that while register tokens
yield cleaner attention maps, these maps do not accurately reflect the
integration of local image information in large models. Instead, global
information is dominated by information extracted from register tokens, leading
to a disconnect between local and global features. Inspired by these findings,
we show that the CLS token itself, which can be interpreted as a register,
leads to a very similar phenomenon in models without explicit register tokens.
Our work shows that care must be taken when interpreting attention maps of
large ViTs. Further, by clearly attributing the faulty behaviour to register
and CLS tokens, we show a path towards more interpretable vision models.

</details>


### [105] [DFEN: Dual Feature Equalization Network for Medical Image Segmentation](https://arxiv.org/pdf/2505.05913)
*Jianjian Yin, Yi Chen, Chengyu Li, Zhichao Zheng, Yanhui Gu, Junsheng Zhou*

Main category: cs.CV

TL;DR: A dual feature equalization network combining Swin Transformer and CNN is proposed to address unequal contextual feature information in medical image segmentation, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current methods ignore unequal contextual feature information at boundaries and low-class-pixel regions, leading to misclassification.

Method: The network uses image-level and class-level feature equalization modules, with Swin Transformer for long-range dependencies.

Result: Achieves top performance on BUSI, ISIC2017, ACDC, and PH$^2$ datasets.

Conclusion: The proposed method effectively enhances pixel feature representations and outperforms existing approaches.

Abstract: Current methods for medical image segmentation primarily focus on extracting
contextual feature information from the perspective of the whole image. While
these methods have shown effective performance, none of them take into account
the fact that pixels at the boundary and regions with a low number of class
pixels capture more contextual feature information from other classes, leading
to misclassification of pixels by unequal contextual feature information. In
this paper, we propose a dual feature equalization network based on the hybrid
architecture of Swin Transformer and Convolutional Neural Network, aiming to
augment the pixel feature representations by image-level equalization feature
information and class-level equalization feature information. Firstly, the
image-level feature equalization module is designed to equalize the contextual
information of pixels within the image. Secondly, we aggregate regions of the
same class to equalize the pixel feature representations of the corresponding
class by class-level feature equalization module. Finally, the pixel feature
representations are enhanced by learning weights for image-level equalization
feature information and class-level equalization feature information. In
addition, Swin Transformer is utilized as both the encoder and decoder, thereby
bolstering the ability of the model to capture long-range dependencies and
spatial correlations. We conducted extensive experiments on Breast Ultrasound
Images (BUSI), International Skin Imaging Collaboration (ISIC2017), Automated
Cardiac Diagnosis Challenge (ACDC) and PH$^2$ datasets. The experimental
results demonstrate that our method have achieved state-of-the-art performance.
Our code is publicly available at https://github.com/JianJianYin/DFEN.

</details>


### [106] [CGTrack: Cascade Gating Network with Hierarchical Feature Aggregation for UAV Tracking](https://arxiv.org/pdf/2505.05936)
*Weihong Li, Xiaoqiong Liu, Heng Fan, Libo Zhang*

Main category: cs.CV

TL;DR: CGTrack, a novel UAV tracker, combines explicit and implicit techniques in a coarse-to-fine framework to enhance network capacity and efficiency, addressing challenges like occlusions and viewing angle changes.


<details>
  <summary>Details</summary>
Motivation: Improving UAV tracking efficiency without sacrificing network capacity, especially under challenges like occlusions and extreme viewing angle changes.

Method: Introduces a Hierarchical Feature Cascade (HFC) module for feature reuse and a Lightweight Gated Center Head (LGCH) for decoupling target coordinates, enhancing feature representation.

Result: Achieves state-of-the-art performance on UAV tracking benchmarks with fast execution.

Conclusion: CGTrack effectively balances efficiency and capacity, making it a robust solution for UAV tracking.

Abstract: Recent advancements in visual object tracking have markedly improved the
capabilities of unmanned aerial vehicle (UAV) tracking, which is a critical
component in real-world robotics applications. While the integration of
hierarchical lightweight networks has become a prevalent strategy for enhancing
efficiency in UAV tracking, it often results in a significant drop in network
capacity, which further exacerbates challenges in UAV scenarios, such as
frequent occlusions and extreme changes in viewing angles. To address these
issues, we introduce a novel family of UAV trackers, termed CGTrack, which
combines explicit and implicit techniques to expand network capacity within a
coarse-to-fine framework. Specifically, we first introduce a Hierarchical
Feature Cascade (HFC) module that leverages the spirit of feature reuse to
increase network capacity by integrating the deep semantic cues with the rich
spatial information, incurring minimal computational costs while enhancing
feature representation. Based on this, we design a novel Lightweight Gated
Center Head (LGCH) that utilizes gating mechanisms to decouple target-oriented
coordinates from previously expanded features, which contain dense local
discriminative information. Extensive experiments on three challenging UAV
tracking benchmarks demonstrate that CGTrack achieves state-of-the-art
performance while running fast. Code will be available at
https://github.com/Nightwatch-Fox11/CGTrack.

</details>


### [107] [Task-Adapter++: Task-specific Adaptation with Order-aware Alignment for Few-shot Action Recognition](https://arxiv.org/pdf/2505.06002)
*Congqi Cao, Peiheng Han, Yueran zhang, Yating Yu, Qinyi Lv, Lingtong Min, Yanning zhang*

Main category: cs.CV

TL;DR: Task-Adapter++ is a parameter-efficient dual adaptation method for few-shot action recognition, addressing issues like generalization loss, insufficient task-specific exploration, overlooked semantic order, and temporal coupling in cross-modal alignment.


<details>
  <summary>Details</summary>
Motivation: Current methods for few-shot action recognition using pre-trained models like CLIP suffer from generalization loss, inadequate task-specific exploration, ignored semantic order, and poor temporal coupling in cross-modal alignment.

Method: Proposes Task-Adapter++ with task-specific adaptation for image encoders, semantic order adapters for text encoders using LLMs, and fine-grained cross-modal alignment.

Result: Achieves state-of-the-art performance on 5 benchmarks, demonstrating effectiveness and superiority.

Conclusion: Task-Adapter++ effectively addresses key challenges in few-shot action recognition, offering a robust and efficient solution.

Abstract: Large-scale pre-trained models have achieved remarkable success in language
and image tasks, leading an increasing number of studies to explore the
application of pre-trained image models, such as CLIP, in the domain of
few-shot action recognition (FSAR). However, current methods generally suffer
from several problems: 1) Direct fine-tuning often undermines the
generalization capability of the pre-trained model; 2) The exploration of
task-specific information is insufficient in the visual tasks; 3) The semantic
order information is typically overlooked during text modeling; 4) Existing
cross-modal alignment techniques ignore the temporal coupling of multimodal
information. To address these, we propose Task-Adapter++, a parameter-efficient
dual adaptation method for both image and text encoders. Specifically, to make
full use of the variations across different few-shot learning tasks, we design
a task-specific adaptation for the image encoder so that the most
discriminative information can be well noticed during feature extraction.
Furthermore, we leverage large language models (LLMs) to generate detailed
sequential sub-action descriptions for each action class, and introduce
semantic order adapters into the text encoder to effectively model the
sequential relationships between these sub-actions. Finally, we develop an
innovative fine-grained cross-modal alignment strategy that actively maps
visual features to reside in the same temporal stage as semantic descriptions.
Extensive experiments fully demonstrate the effectiveness and superiority of
the proposed method, which achieves state-of-the-art performance on 5
benchmarks consistently. The code is open-sourced at
https://github.com/Jaulin-Bage/Task-Adapter-pp.

</details>


### [108] [From Pixels to Perception: Interpretable Predictions via Instance-wise Grouped Feature Selection](https://arxiv.org/pdf/2505.06003)
*Moritz Vandenhirtz, Julia E. Vogt*

Main category: cs.CV

TL;DR: Proposes an interpretable ML method using instance-wise sparsification of images, aligning with human perception via semantic regions and dynamic sparsity.


<details>
  <summary>Details</summary>
Motivation: To provide insights into model decisions and improve interpretability by aligning explanations with human understanding.

Method: Instance-wise sparsification of input images using semantic pixel regions and dynamic sparsity determination.

Result: Produces more meaningful, human-understandable predictions than benchmarks on semi-synthetic and natural datasets.

Conclusion: The method enhances interpretability and aligns model explanations with human perception effectively.

Abstract: Understanding the decision-making process of machine learning models provides
valuable insights into the task, the data, and the reasons behind a model's
failures. In this work, we propose a method that performs inherently
interpretable predictions through the instance-wise sparsification of input
images. To align the sparsification with human perception, we learn the masking
in the space of semantically meaningful pixel regions rather than on
pixel-level. Additionally, we introduce an explicit way to dynamically
determine the required level of sparsity for each instance. We show empirically
on semi-synthetic and natural image datasets that our inherently interpretable
classifier produces more meaningful, human-understandable predictions than
state-of-the-art benchmarks.

</details>


### [109] [Document Image Rectification Bases on Self-Adaptive Multitask Fusion](https://arxiv.org/pdf/2505.06038)
*Heng Li, Xiangping Wu, Qingcai Chen*

Main category: cs.CV

TL;DR: A novel multi-task fusion network (SalmRec) improves document image rectification by leveraging inter-task feature aggregation and a gating mechanism, outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current multi-task methods for document rectification fail to utilize complementary features and interactions between tasks, limiting performance.

Method: Proposes SalmRec, a network with inter-task feature aggregation and a gating mechanism to balance global and local task features.

Result: Significant performance improvements on English (DIR300, DocUNet) and Chinese (DocReal) benchmarks, validated by ablation studies.

Conclusion: SalmRec effectively enhances geometric distortion perception and feature complementarity, advancing document rectification.

Abstract: Deformed document image rectification is essential for real-world document
understanding tasks, such as layout analysis and text recognition. However,
current multi-task methods -- such as background removal, 3D coordinate
prediction, and text line segmentation -- often overlook the complementary
features between tasks and their interactions. To address this gap, we propose
a self-adaptive learnable multi-task fusion rectification network named
SalmRec. This network incorporates an inter-task feature aggregation module
that adaptively improves the perception of geometric distortions, enhances
feature complementarity, and reduces negative interference. We also introduce a
gating mechanism to balance features both within global tasks and between local
tasks effectively. Experimental results on two English benchmarks (DIR300 and
DocUNet) and one Chinese benchmark (DocReal) demonstrate that our method
significantly improves rectification performance. Ablation studies further
highlight the positive impact of different tasks on dewarping and the
effectiveness of our proposed module.

</details>


### [110] [MM-Skin: Enhancing Dermatology Vision-Language Model with an Image-Text Dataset Derived from Textbooks](https://arxiv.org/pdf/2505.06152)
*Wenqi Zeng, Yuqi Sun, Chenxi Ma, Weimin Tan, Bo Yan*

Main category: cs.CV

TL;DR: The paper introduces MM-Skin, a large-scale multimodal dermatology dataset, and SkinVL, a dermatology-specific vision-language model (VLM), to address the lack of specialized dermatology VLMs. SkinVL outperforms general and medical VLMs in various tasks.


<details>
  <summary>Details</summary>
Motivation: Current dermatology VLMs lack specialized text descriptions in datasets, limiting their diagnostic capabilities.

Method: Developed MM-Skin with 10k image-text pairs and 27k VQA samples, then trained SkinVL using this dataset.

Result: SkinVL excels in VQA, supervised fine-tuning, and zero-shot classification across 8 datasets.

Conclusion: MM-Skin and SkinVL significantly advance dermatology VLMs, offering improved diagnostic tools.

Abstract: Medical vision-language models (VLMs) have shown promise as clinical
assistants across various medical fields. However, specialized dermatology VLM
capable of delivering professional and detailed diagnostic analysis remains
underdeveloped, primarily due to less specialized text descriptions in current
dermatology multimodal datasets. To address this issue, we propose MM-Skin, the
first large-scale multimodal dermatology dataset that encompasses 3 imaging
modalities, including clinical, dermoscopic, and pathological and nearly 10k
high-quality image-text pairs collected from professional textbooks. In
addition, we generate over 27k diverse, instruction-following vision question
answering (VQA) samples (9 times the size of current largest dermatology VQA
dataset). Leveraging public datasets and MM-Skin, we developed SkinVL, a
dermatology-specific VLM designed for precise and nuanced skin disease
interpretation. Comprehensive benchmark evaluations of SkinVL on VQA,
supervised fine-tuning (SFT) and zero-shot classification tasks across 8
datasets, reveal its exceptional performance for skin diseases in comparison to
both general and medical VLM models. The introduction of MM-Skin and SkinVL
offers a meaningful contribution to advancing the development of clinical
dermatology VLM assistants. MM-Skin is available at
https://github.com/ZwQ803/MM-Skin

</details>


### [111] [Towards Better Cephalometric Landmark Detection with Diffusion Data Generation](https://arxiv.org/pdf/2505.06055)
*Dongqian Guo, Wencheng Han, Pang Lyu, Yuxi Zhou, Jianbing Shen*

Main category: cs.CV

TL;DR: A method for generating diverse cephalometric X-ray images and annotations without human intervention improves deep learning-based landmark detection.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity of annotated cephalometric datasets, which limits deep learning effectiveness.

Method: Constructs annotations using anatomical priors, uses a diffusion-based generator for realistic images, and employs a prompt dataset for control.

Result: Training with generated data boosts Success Detection Rate (SDR) by 6.5%, achieving 82.2%.

Conclusion: The approach enhances cephalometric landmark detection accuracy by leveraging synthetic data.

Abstract: Cephalometric landmark detection is essential for orthodontic diagnostics and
treatment planning. Nevertheless, the scarcity of samples in data collection
and the extensive effort required for manual annotation have significantly
impeded the availability of diverse datasets. This limitation has restricted
the effectiveness of deep learning-based detection methods, particularly those
based on large-scale vision models. To address these challenges, we have
developed an innovative data generation method capable of producing diverse
cephalometric X-ray images along with corresponding annotations without human
intervention. To achieve this, our approach initiates by constructing new
cephalometric landmark annotations using anatomical priors. Then, we employ a
diffusion-based generator to create realistic X-ray images that correspond
closely with these annotations. To achieve precise control in producing samples
with different attributes, we introduce a novel prompt cephalometric X-ray
image dataset. This dataset includes real cephalometric X-ray images and
detailed medical text prompts describing the images. By leveraging these
detailed prompts, our method improves the generation process to control
different styles and attributes. Facilitated by the large, diverse generated
data, we introduce large-scale vision detection models into the cephalometric
landmark detection task to improve accuracy. Experimental results demonstrate
that training with the generated data substantially enhances the performance.
Compared to methods without using the generated data, our approach improves the
Success Detection Rate (SDR) by 6.5%, attaining a notable 82.2%. All code and
data are available at: https://um-lab.github.io/cepha-generation

</details>


### [112] [Noise-Consistent Siamese-Diffusion for Medical Image Synthesis and Segmentation](https://arxiv.org/pdf/2505.06068)
*Kunpeng Qiu, Zhiqiang Gao, Zhiying Zhou, Mingjie Sun, Yongxin Guo*

Main category: cs.CV

TL;DR: Siamese-Diffusion, a dual-component model, enhances medical image segmentation by improving synthetic image-mask pair generation, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity of annotated datasets in medical image segmentation, which limits deep learning potential.

Method: Introduces Siamese-Diffusion with Mask-Diffusion and Image-Diffusion components, using Noise Consistency Loss for training and Mask-Diffusion for sampling.

Result: Boosts SANet's mDice and mIoU by 3.6% and 4.4% on Polyps, and UNet by 1.52% and 1.64% on ISIC2018.

Conclusion: Siamese-Diffusion effectively improves segmentation model performance by enhancing synthetic data quality.

Abstract: Deep learning has revolutionized medical image segmentation, yet its full
potential remains constrained by the paucity of annotated datasets. While
diffusion models have emerged as a promising approach for generating synthetic
image-mask pairs to augment these datasets, they paradoxically suffer from the
same data scarcity challenges they aim to mitigate. Traditional mask-only
models frequently yield low-fidelity images due to their inability to
adequately capture morphological intricacies, which can critically compromise
the robustness and reliability of segmentation models. To alleviate this
limitation, we introduce Siamese-Diffusion, a novel dual-component model
comprising Mask-Diffusion and Image-Diffusion. During training, a Noise
Consistency Loss is introduced between these components to enhance the
morphological fidelity of Mask-Diffusion in the parameter space. During
sampling, only Mask-Diffusion is used, ensuring diversity and scalability.
Comprehensive experiments demonstrate the superiority of our method.
Siamese-Diffusion boosts SANet's mDice and mIoU by 3.6% and 4.4% on the Polyps,
while UNet improves by 1.52% and 1.64% on the ISIC2018. Code is available at
GitHub.

</details>


### [113] [Camera-Only Bird's Eye View Perception: A Neural Approach to LiDAR-Free Environmental Mapping for Autonomous Vehicles](https://arxiv.org/pdf/2505.06113)
*Anupkumar Bochare*

Main category: cs.CV

TL;DR: A camera-only perception framework achieves LiDAR-like accuracy for autonomous vehicles using BEV maps, YOLOv11, and DepthAnythingV2, with 85-90% detection rates and 1.2m positional errors.


<details>
  <summary>Details</summary>
Motivation: Reduce reliance on expensive LiDAR sensors by leveraging deep learning for accurate camera-only perception.

Method: Extends Lift-Splat-Shoot with YOLOv11 detection and DepthAnythingV2 depth estimation across multi-camera inputs.

Result: 85% road segmentation accuracy, 85-90% vehicle detection rates, and 1.2m positional errors on OpenLane-V2 and NuScenes datasets.

Conclusion: Deep learning enables cost-efficient, accurate autonomous navigation using only cameras.

Abstract: Autonomous vehicle perception systems have traditionally relied on costly
LiDAR sensors to generate precise environmental representations. In this paper,
we propose a camera-only perception framework that produces Bird's Eye View
(BEV) maps by extending the Lift-Splat-Shoot architecture. Our method combines
YOLOv11-based object detection with DepthAnythingV2 monocular depth estimation
across multi-camera inputs to achieve comprehensive 360-degree scene
understanding. We evaluate our approach on the OpenLane-V2 and NuScenes
datasets, achieving up to 85% road segmentation accuracy and 85-90% vehicle
detection rates when compared against LiDAR ground truth, with average
positional errors limited to 1.2 meters. These results highlight the potential
of deep learning to extract rich spatial information using only camera inputs,
enabling cost-efficient autonomous navigation without sacrificing accuracy.

</details>


### [114] [Photovoltaic Defect Image Generator with Boundary Alignment Smoothing Constraint for Domain Shift Mitigation](https://arxiv.org/pdf/2505.06117)
*Dongying Li, Binyi Su, Hua Zhang, Yong Li, Haiyong Chen*

Main category: cs.CV

TL;DR: PDIG, a Photovoltaic Defect Image Generator using Stable Diffusion, improves defect detection by generating high-quality, diverse images with limited data.


<details>
  <summary>Details</summary>
Motivation: Accurate defect detection in PV cells is crucial, but limited defect data hinders model training. Existing methods lack stability and diversity.

Method: PDIG uses Stable Diffusion with a Semantic Concept Embedding module, Lightweight Industrial Style Adaptor, and Text-Image Dual-Space Constraints for enhanced generation.

Result: PDIG outperforms state-of-the-art methods, improving FID by 19.16 points and boosting downstream defect detection performance.

Conclusion: PDIG effectively addresses data scarcity and quality issues in PV defect detection, offering superior realism and diversity.

Abstract: Accurate defect detection of photovoltaic (PV) cells is critical for ensuring
quality and efficiency in intelligent PV manufacturing systems. However, the
scarcity of rich defect data poses substantial challenges for effective model
training. While existing methods have explored generative models to augment
datasets, they often suffer from instability, limited diversity, and domain
shifts. To address these issues, we propose PDIG, a Photovoltaic Defect Image
Generator based on Stable Diffusion (SD). PDIG leverages the strong priors
learned from large-scale datasets to enhance generation quality under limited
data. Specifically, we introduce a Semantic Concept Embedding (SCE) module that
incorporates text-conditioned priors to capture the relational concepts between
defect types and their appearances. To further enrich the domain distribution,
we design a Lightweight Industrial Style Adaptor (LISA), which injects
industrial defect characteristics into the SD model through cross-disentangled
attention. At inference, we propose a Text-Image Dual-Space Constraints (TIDSC)
module, enforcing the quality of generated images via positional consistency
and spatial smoothing alignment. Extensive experiments demonstrate that PDIG
achieves superior realism and diversity compared to state-of-the-art methods.
Specifically, our approach improves Frechet Inception Distance (FID) by 19.16
points over the second-best method and significantly enhances the performance
of downstream defect detection tasks.

</details>


### [115] [BrainSegDMlF: A Dynamic Fusion-enhanced SAM for Brain Lesion Segmentation](https://arxiv.org/pdf/2505.06133)
*Hongming Wang, Yifeng Wu, Huimin Huang, Hongtao Wu, Jia-Xuan Jiang, Xiaodong Zhang, Hao Zheng, Xian Wu, Yefeng Zheng, Jinping Xu, Jing Cheng*

Main category: cs.CV

TL;DR: The paper introduces BrainSegDMLF, a fully automated model for brain lesion segmentation, addressing limitations of existing methods by integrating multi-modal data, improving small lesion detection, and enabling automatic segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing brain lesion segmentation methods lack multi-modal integration, struggle with small lesions, and rely on manual prompts, limiting diagnostic efficiency.

Method: BrainSegDMLF uses a Dynamic Modal Interactive Fusion module for multi-modal data integration, a Layer-by-Layer Upsampling Decoder for feature extraction, and automatic mask generation.

Result: The model achieves comprehensive lesion segmentation, better small lesion detection, and fully automated operation without manual prompts.

Conclusion: BrainSegDMLF overcomes key limitations in brain lesion segmentation, offering improved accuracy and efficiency for medical diagnostics.

Abstract: The segmentation of substantial brain lesions is a significant and
challenging task in the field of medical image segmentation. Substantial brain
lesions in brain imaging exhibit high heterogeneity, with indistinct boundaries
between lesion regions and normal brain tissue. Small lesions in single slices
are difficult to identify, making the accurate and reproducible segmentation of
abnormal regions, as well as their feature description, highly complex.
Existing methods have the following limitations: 1) They rely solely on
single-modal information for learning, neglecting the multi-modal information
commonly used in diagnosis. This hampers the ability to comprehensively acquire
brain lesion information from multiple perspectives and prevents the effective
integration and utilization of multi-modal data inputs, thereby limiting a
holistic understanding of lesions. 2) They are constrained by the amount of
data available, leading to low sensitivity to small lesions and difficulty in
detecting subtle pathological changes. 3) Current SAM-based models rely on
external prompts, which cannot achieve automatic segmentation and, to some
extent, affect diagnostic efficiency.To address these issues, we have developed
a large-scale fully automated segmentation model specifically designed for
brain lesion segmentation, named BrainSegDMLF. This model has the following
features: 1) Dynamic Modal Interactive Fusion (DMIF) module that processes and
integrates multi-modal data during the encoding process, providing the SAM
encoder with more comprehensive modal information. 2) Layer-by-Layer Upsampling
Decoder, enabling the model to extract rich low-level and high-level features
even with limited data, thereby detecting the presence of small lesions. 3)
Automatic segmentation masks, allowing the model to generate lesion masks
automatically without requiring manual prompts.

</details>


### [116] [DiffLocks: Generating 3D Hair from a Single Image using Diffusion Models](https://arxiv.org/pdf/2505.06166)
*Radu Alexandru Rosu, Keyu Wu, Yao Feng, Youyi Zheng, Michael J. Black*

Main category: cs.CV

TL;DR: DiffLocks is a framework for detailed 3D hair reconstruction from a single image, overcoming limitations of previous methods by using a synthetic dataset and diffusion-transformer model.


<details>
  <summary>Details</summary>
Motivation: The diversity of hairstyles and lack of paired image-to-3D hair data make the task challenging. Previous methods fail to reconstruct detailed or curly hair and are limited to few hairstyles.

Method: DiffLocks automates the creation of a large synthetic hair dataset (40K hairstyles) and trains an image-conditioned diffusion-transformer model to generate 3D strands directly from a single image.

Result: The method accurately reconstructs diverse hairstyles, including highly curled hair (e.g., afro), without post-processing, generalizing to in-the-wild images.

Conclusion: DiffLocks advances 3D hair reconstruction by enabling detailed, diverse hairstyle generation from a single image, addressing key limitations of prior work.

Abstract: We address the task of generating 3D hair geometry from a single image, which
is challenging due to the diversity of hairstyles and the lack of paired
image-to-3D hair data. Previous methods are primarily trained on synthetic data
and cope with the limited amount of such data by using low-dimensional
intermediate representations, such as guide strands and scalp-level embeddings,
that require post-processing to decode, upsample, and add realism. These
approaches fail to reconstruct detailed hair, struggle with curly hair, or are
limited to handling only a few hairstyles. To overcome these limitations, we
propose DiffLocks, a novel framework that enables detailed reconstruction of a
wide variety of hairstyles directly from a single image. First, we address the
lack of 3D hair data by automating the creation of the largest synthetic hair
dataset to date, containing 40K hairstyles. Second, we leverage the synthetic
hair dataset to learn an image-conditioned diffusion-transfomer model that
generates accurate 3D strands from a single frontal image. By using a
pretrained image backbone, our method generalizes to in-the-wild images despite
being trained only on synthetic data. Our diffusion model predicts a scalp
texture map in which any point in the map contains the latent code for an
individual hair strand. These codes are directly decoded to 3D strands without
post-processing techniques. Representing individual strands, instead of guide
strands, enables the transformer to model the detailed spatial structure of
complex hairstyles. With this, DiffLocks can recover highly curled hair, like
afro hairstyles, from a single image for the first time. Data and code is
available at https://radualexandru.github.io/difflocks/

</details>


### [117] [Adapting a Segmentation Foundation Model for Medical Image Classification](https://arxiv.org/pdf/2505.06217)
*Pengfei Gu, Haoteng Tang, Islam A. Ebeid, Jose A. Nunez, Fabian Vazquez, Diego Adame, Marcus Zhan, Huimin Li, Bin Fu, Danny Z. Chen*

Main category: cs.CV

TL;DR: A new framework adapts the Segment Anything Model (SAM) for medical image classification by leveraging its encoder for feature extraction and introducing a Spatially Localized Channel Attention (SLCA) mechanism to improve classification performance.


<details>
  <summary>Details</summary>
Motivation: Despite SAM's strong zero-shot segmentation capabilities, adapting it for medical image classification remains underexplored.

Method: Uses SAM's frozen encoder for feature extraction and introduces SLCA to compute spatially localized attention weights for enhanced classification.

Result: Shows effectiveness and data-efficiency on three public medical image datasets.

Conclusion: The proposed framework successfully adapts SAM for medical image classification, improving performance with minimal training overhead.

Abstract: Recent advancements in foundation models, such as the Segment Anything Model
(SAM), have shown strong performance in various vision tasks, particularly
image segmentation, due to their impressive zero-shot segmentation
capabilities. However, effectively adapting such models for medical image
classification is still a less explored topic. In this paper, we introduce a
new framework to adapt SAM for medical image classification. First, we utilize
the SAM image encoder as a feature extractor to capture segmentation-based
features that convey important spatial and contextual details of the image,
while freezing its weights to avoid unnecessary overhead during training. Next,
we propose a novel Spatially Localized Channel Attention (SLCA) mechanism to
compute spatially localized attention weights for the feature maps. The
features extracted from SAM's image encoder are processed through SLCA to
compute attention weights, which are then integrated into deep learning
classification models to enhance their focus on spatially relevant or
meaningful regions of the image, thus improving classification performance.
Experimental results on three public medical image classification datasets
demonstrate the effectiveness and data-efficiency of our approach.

</details>


### [118] [VIN-NBV: A View Introspection Network for Next-Best-View Selection for Resource-Efficient 3D Reconstruction](https://arxiv.org/pdf/2505.06219)
*Noah Frahm, Dongxu Zhao, Andrea Dunn Beltran, Ron Alterovitz, Jan-Michael Frahm, Junier Oliva, Roni Sengupta*

Main category: cs.CV

TL;DR: The paper introduces the View Introspection Network (VIN) and VIN-NBV policy to improve 3D reconstruction by predicting view quality improvements, outperforming coverage-based methods by ~30%.


<details>
  <summary>Details</summary>
Motivation: Existing NBV algorithms focus on coverage maximization, which doesn't always improve reconstruction quality for complex scenes. The paper addresses this gap by directly predicting view improvements.

Method: Proposes VIN, a network predicting reconstruction quality improvement for views, and VIN-NBV, a greedy policy selecting views with the highest predicted improvement. Uses 3D-aware featurization and imitation learning.

Result: VIN-NBV improves reconstruction quality by ~30% over coverage maximization baselines under acquisition or time constraints.

Conclusion: VIN-NBV effectively enhances 3D reconstruction by prioritizing view quality over coverage, validated by significant performance gains.

Abstract: Next Best View (NBV) algorithms aim to acquire an optimal set of images using
minimal resources, time, or number of captures to enable efficient 3D
reconstruction of a scene. Existing approaches often rely on prior scene
knowledge or additional image captures and often develop policies that maximize
coverage. Yet, for many real scenes with complex geometry and self-occlusions,
coverage maximization does not lead to better reconstruction quality directly.
In this paper, we propose the View Introspection Network (VIN), which is
trained to predict the reconstruction quality improvement of views directly,
and the VIN-NBV policy. A greedy sequential sampling-based policy, where at
each acquisition step, we sample multiple query views and choose the one with
the highest VIN predicted improvement score. We design the VIN to perform
3D-aware featurization of the reconstruction built from prior acquisitions, and
for each query view create a feature that can be decoded into an improvement
score. We then train the VIN using imitation learning to predict the
reconstruction improvement score. We show that VIN-NBV improves reconstruction
quality by ~30% over a coverage maximization baseline when operating with
constraints on the number of acquisitions or the time in motion.

</details>


### [119] [R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning](https://arxiv.org/pdf/2505.02835)
*Yi-Fan Zhang, Xingyu Lu, Xiao Hu, Chaoyou Fu, Bin Wen, Tianke Zhang, Changyi Liu, Kaiyu Jiang, Kaibing Chen, Kaiyu Tang, Haojie Ding, Jiankang Chen, Fan Yang, Zhang Zhang, Tingting Gao, Liang Wang*

Main category: cs.CV

TL;DR: The paper explores using Reinforcement Learning (RL) to improve Multimodal Reward Models (MRMs), proposing the StableReinforce algorithm for stable training and better performance.


<details>
  <summary>Details</summary>
Motivation: Limited exploration of long-term reasoning in MRMs and the instability of existing RL methods for reward modeling.

Method: Reformulates reward modeling as a rule-based RL task, introduces StableReinforce with refined training loss, advantage estimation, and reward design, and uses 200K preference data.

Result: R1-Reward, trained with StableReinforce, improves benchmarks by 8.4% (VL Reward-Bench) and 14.3% (Multimodal Reward Bench).

Conclusion: RL, particularly StableReinforce, effectively optimizes MRMs, with potential for further performance gains.

Abstract: Multimodal Reward Models (MRMs) play a crucial role in enhancing the
performance of Multimodal Large Language Models (MLLMs). While recent
advancements have primarily focused on improving the model structure and
training data of MRMs, there has been limited exploration into the
effectiveness of long-term reasoning capabilities for reward modeling and how
to activate these capabilities in MRMs. In this paper, we explore how
Reinforcement Learning (RL) can be used to improve reward modeling.
Specifically, we reformulate the reward modeling problem as a rule-based RL
task. However, we observe that directly applying existing RL algorithms, such
as Reinforce++, to reward modeling often leads to training instability or even
collapse due to the inherent limitations of these algorithms. To address this
issue, we propose the StableReinforce algorithm, which refines the training
loss, advantage estimation strategy, and reward design of existing RL methods.
These refinements result in more stable training dynamics and superior
performance. To facilitate MRM training, we collect 200K preference data from
diverse datasets. Our reward model, R1-Reward, trained using the
StableReinforce algorithm on this dataset, significantly improves performance
on multimodal reward modeling benchmarks. Compared to previous SOTA models,
R1-Reward achieves a $8.4\%$ improvement on the VL Reward-Bench and a $14.3\%$
improvement on the Multimodal Reward Bench. Moreover, with more inference
compute, R1-Reward's performance is further enhanced, highlighting the
potential of RL algorithms in optimizing MRMs.

</details>


### [120] [Enhancing Target-unspecific Tasks through a Features Matrix](https://arxiv.org/pdf/2505.03414)
*Fangming Cui, Yonggang Zhang, Xuan Wang, Xinmei Tian, Jun Yu*

Main category: cs.CV

TL;DR: A novel Features Matrix (FM) regularization method is proposed to enhance large vision-language models for target-unspecific tasks by preserving general knowledge and preventing overfitting.


<details>
  <summary>Details</summary>
Motivation: Current prompt learning methods for vision-language models perform well on target-specific tasks but struggle with generalizable tasks due to overfitting and loss of general knowledge.

Method: The FM approach extracts and leverages general knowledge by capturing diverse input semantics from a deep and fine perspective, forming a Features Matrix to mitigate overfitting.

Result: The FM is compatible with existing frameworks and significantly improves performance on target-unspecific tasks, achieving state-of-the-art results.

Conclusion: The FM regularization method effectively addresses overfitting and enhances generalizability in vision-language models.

Abstract: Recent developments in prompt learning of large vision-language models have
significantly improved performance in target-specific tasks. However, these
prompt optimizing methods often struggle to tackle the target-unspecific or
generalizable tasks effectively. It may be attributed to the fact that
overfitting training causes the model to forget its general knowledge having
strong promotion on target-unspecific tasks. To alleviate this issue, we
propose a novel Features Matrix (FM) regularization approach designed to
enhance these models on target-unspecific tasks. Our method extracts and
leverages general knowledge, shaping a Features Matrix (FM). Specifically, the
FM captures the semantics of diverse inputs from a deep and fine perspective,
preserving essential general knowledge, which mitigates the risk of
overfitting. Representative evaluations demonstrate that: 1) the FM is
compatible with existing frameworks as a generic and flexible module, and 2)
the FM significantly showcases its effectiveness in enhancing target-unspecific
tasks, achieving state-of-the-art performance.

</details>


### [121] [Image space formalism of convolutional neural networks for k-space interpolation](https://arxiv.org/pdf/2402.17410)
*Peter Dawood, Felix Breuer, Istvan Homolya, Maximilian Gram, Peter M. Jakob, Moritz Zaiss, Martin Blaimer*

Main category: cs.CV

TL;DR: The paper introduces an image space formalism for RAKI to analyze noise resilience, linking nonlinear activations in k-space to image reconstruction features and noise propagation.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between noise resilience in RAKI and nonlinear activations in k-space, and to provide a human-readable analysis of these effects.

Method: The image space formalism expresses nonlinear activations as convolutions, enabling analytical noise propagation analysis (g-factor maps) and control of nonlinearity via leaky ReLU parameters.

Result: Analytical g-factor maps match simulations, revealing trade-offs between noise resilience and artifacts (blurring, contrast loss). Adjusting nonlinearity acts like Tikhonov regularization.

Conclusion: The image space formalism enables analytical noise propagation analysis and visualization of nonlinear activation effects, aiding in understanding and optimizing RAKI reconstructions.

Abstract: Purpose: Noise resilience in image reconstructions by scan-specific robust
artificial neural networks for k-space interpolation (RAKI) is linked to
nonlinear activations in k-space. To gain a deeper understanding of this
relationship, an image space formalism of RAKI is introduced for analyzing
noise propagation analytically, identifying and characterizing image
reconstruction features and to describe the role of nonlinear activations in a
human readable manner. Methods: The image space formalism for RAKI inference is
employed by expressing nonlinear activations in k-space as element-wise
multiplications with activation masks, which transform into convolutions in
image space. Jacobians of the de-aliased, coil-combined image relative to the
aliased coil images can be expressed algebraically, and thus, the noise
amplification is quantified analytically (g-factor maps). We analyze the role
of nonlinearity for noise resilience by controlling the degree of nonlinearity
in the reconstruction model via the negative slope parameter in leaky ReLU.
Results: The analytical g-factor maps correspond with those obtained from Monte
Carlo simulations and from an auto differentiation approach for in vivo brain
images. Apparent blurring and contrast loss artifacts are identified as
implications of enhanced noise resilience. These residual artifacts can be
traded against noise resilience by adjusting the degree of nonlinearity in the
model (Tikhonov-like regularization) in case of limited training data. The
inspection of image space activations reveals an autocorrelation pattern
leading to a potential center artifact. Conclusion: The image space formalism
of RAKI provides the means for analytical quantitative noisepropagation
analysis and human-readable visualization of the effects of the nonlinear
activation functions in k-space.

</details>


### [122] [Neural Slot Interpreters: Grounding Object Semantics in Emergent Slot Representations](https://arxiv.org/pdf/2403.07887)
*Bhishma Dedhia, Niraj K. Jha*

Main category: cs.CV

TL;DR: The paper introduces Neural Slot Interpreter (NSI), a method for grounding object semantics in slots, improving visual grounding and data efficiency over traditional approaches.


<details>
  <summary>Details</summary>
Motivation: Human cognition relies on abstract, composable concepts, but modern machines struggle with this. The paper explores whether slot representations can serve as suitable abstractions for grounding and reasoning.

Method: NSI uses a nested schema with syntax rules to organize object semantics into primitives, grounded into slots via structured contrastive learning. It evaluates grounding efficacy through bi-modal tasks.

Result: NSI outperforms traditional methods in visual grounding, data efficiency, and generalizability. It also enhances few-shot classification when used with Vision Transformers.

Conclusion: NSI demonstrates that slot-based representations improve grounding, interpretability, and downstream tasks, offering a promising direction for machine cognition.

Abstract: Several accounts of human cognition posit that our intelligence is rooted in
our ability to form abstract composable concepts, ground them in our
environment, and reason over these grounded entities. This trifecta of human
thought has remained elusive in modern intelligent machines. In this work, we
investigate whether slot representations extracted from visual scenes serve as
appropriate compositional abstractions for grounding and reasoning. We present
the Neural Slot Interpreter (NSI), which learns to ground object semantics in
slots. At the core of NSI is a nested schema that uses simple syntax rules to
organize the object semantics of a scene into object-centric schema primitives.
Then, the NSI metric learns to ground primitives into slots through a
structured contrastive learning objective that reasons over the intermodal
alignment. Experiments with a bi-modal object-property and scene retrieval task
demonstrate the grounding efficacy and interpretability of correspondences
learned by NSI. From a scene representation standpoint, we find that emergent
NSI slots that move beyond the image grid by binding to spatial objects
facilitate improved visual grounding compared to conventional
bounding-box-based approaches. From a data efficiency standpoint, we
empirically validate that NSI learns more generalizable representations from a
fixed amount of annotation data than the traditional approach. We also show
that the grounded slots surpass unsupervised slots in real-world object
discovery and scale with scene complexity. Finally, we investigate the
downstream efficacy of the grounded slots. Vision Transformers trained on
grounding-aware NSI tokenizers using as few as ten tokens outperform
patch-based tokens on challenging few-shot classification tasks.

</details>


### [123] [Enhancing Screen Time Identification in Children with a Multi-View Vision Language Model and Screen Time Tracker](https://arxiv.org/pdf/2410.01966)
*Xinlong Hou, Sen Shen, Xueshen Li, Xinran Gao, Ziyi Huang, Steven J. Holiday, Matthew R. Cribbet, Susan W. White, Edward Sazonov, Yu Gan*

Main category: cs.CV

TL;DR: A novel sensor informatics framework using egocentric images and a multi-view VLM improves accuracy in monitoring children's screen exposure.


<details>
  <summary>Details</summary>
Motivation: Existing methods for monitoring screen exposure in children are inefficient and inaccurate, relying on self-reports or bulky sensors.

Method: Developed a screen time tracker (STT) with a multi-view VLM to dynamically interpret screen exposure from egocentric images.

Result: Outperformed plain VLM and object detection models in accuracy for children's free-living activities.

Conclusion: The framework shows promise for optimizing behavioral research on screen exposure in naturalistic settings.

Abstract: Being able to accurately monitor the screen exposure of young children is
important for research on phenomena linked to screen use such as childhood
obesity, physical activity, and social interaction. Most existing studies rely
upon self-report or manual measures from bulky wearable sensors, thus lacking
efficiency and accuracy in capturing quantitative screen exposure data. In this
work, we developed a novel sensor informatics framework that utilizes
egocentric images from a wearable sensor, termed the screen time tracker (STT),
and a vision language model (VLM). In particular, we devised a multi-view VLM
that takes multiple views from egocentric image sequences and interprets screen
exposure dynamically. We validated our approach by using a dataset of
children's free-living activities, demonstrating significant improvement over
existing methods in plain vision language models and object detection models.
Results supported the promise of this monitoring approach, which could optimize
behavioral research on screen exposure in children's naturalistic settings.

</details>


### [124] [Towards Foundation Models and Few-Shot Parameter-Efficient Fine-Tuning for Volumetric Organ Segmentation](https://arxiv.org/pdf/2303.17051)
*Julio Silva-Rodríguez, Jose Dolz, Ismail Ben Ayed*

Main category: cs.CV

TL;DR: Proposes Few-Shot Efficient Fine-Tuning (FSEFT) for adapting medical image segmentation foundation models with limited data and resources, using parameter-efficient methods and novel adapters.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges of full fine-tuning in medical image segmentation due to scarce labeled data and limited computational resources in clinical settings.

Method: Introduces FSEFT, leveraging Parameter-Efficient Fine-Tuning and black-box Adapters, including Spatial black-box Adapters for dense prediction tasks.

Result: Confirms foundation models' suitability for medical image segmentation and highlights limitations of traditional fine-tuning in few-shot scenarios.

Conclusion: FSEFT offers a practical solution for adapting foundation models in resource-constrained medical settings, improving efficiency and performance.

Abstract: The recent popularity of foundation models and the pre-train-and-adapt
paradigm, where a large-scale model is transferred to downstream tasks, is
gaining attention for volumetric medical image segmentation. However, current
transfer learning strategies devoted to full fine-tuning for transfer learning
may require significant resources and yield sub-optimal results when the
labeled data of the target task is scarce. This makes its applicability in real
clinical settings challenging since these institutions are usually constrained
on data and computational resources to develop proprietary solutions. To
address this challenge, we formalize Few-Shot Efficient Fine-Tuning (FSEFT), a
novel and realistic scenario for adapting medical image segmentation foundation
models. This setting considers the key role of both data- and
parameter-efficiency during adaptation. Building on a foundation model
pre-trained on open-access CT organ segmentation sources, we propose leveraging
Parameter-Efficient Fine-Tuning and black-box Adapters to address such
challenges. Furthermore, novel efficient adaptation methodologies are
introduced in this work, which include Spatial black-box Adapters that are more
appropriate for dense prediction tasks and constrained transductive inference,
leveraging task-specific prior knowledge. Our comprehensive transfer learning
experiments confirm the suitability of foundation models in medical image
segmentation and unveil the limitations of popular fine-tuning strategies in
few-shot scenarios.

</details>


### [125] [How to build the best medical image segmentation algorithm using foundation models: a comprehensive empirical study with Segment Anything Model](https://arxiv.org/pdf/2404.09957)
*Hanxue Gu, Haoyu Dong, Jichen Yang, Maciej A. Mazurowski*

Main category: cs.CV

TL;DR: This paper systematically evaluates fine-tuning strategies for the Segment Anything Model (SAM) in medical image segmentation, identifying best practices and releasing optimized weights.


<details>
  <summary>Details</summary>
Motivation: The lack of systematic guidelines for fine-tuning SAM in medical image segmentation prompted this study.

Method: The authors evaluated 18 fine-tuning combinations across 17 datasets, testing backbone architectures, model components, and algorithms.

Result: Fine-tuning SAM outperforms previous methods, with parameter-efficient learning in encoder and decoder being optimal. Self-supervised learning further improves performance.

Conclusion: The study provides best practices for SAM fine-tuning in medical imaging and releases MRI-specific weights for improved performance.

Abstract: Automated segmentation is a fundamental medical image analysis task, which
enjoys significant advances due to the advent of deep learning. While
foundation models have been useful in natural language processing and some
vision tasks for some time, the foundation model developed with image
segmentation in mind - Segment Anything Model (SAM) - has been developed only
recently and has shown similar promise. However, there are still no systematic
analyses or "best-practice" guidelines for optimal fine-tuning of SAM for
medical image segmentation. This work summarizes existing fine-tuning
strategies with various backbone architectures, model components, and
fine-tuning algorithms across 18 combinations, and evaluates them on 17
datasets covering all common radiology modalities. Our study reveals that (1)
fine-tuning SAM leads to slightly better performance than previous segmentation
methods, (2) fine-tuning strategies that use parameter-efficient learning in
both the encoder and decoder are superior to other strategies, (3) network
architecture has a small impact on final performance, (4) further training SAM
with self-supervised learning can improve final model performance. We also
demonstrate the ineffectiveness of some methods popular in the literature and
further expand our experiments into few-shot and prompt-based settings. Lastly,
we released our code and MRI-specific fine-tuned weights, which consistently
obtained superior performance over the original SAM, at
https://github.com/mazurowski-lab/finetune-SAM.

</details>


### [126] [Mixed Text Recognition with Efficient Parameter Fine-Tuning and Transformer](https://arxiv.org/pdf/2404.12734)
*Da Chang, Yu Li*

Main category: cs.CV

TL;DR: DLoRA-TrOCR is a parameter-efficient hybrid text spotting method using DoRA and LoRA modules for fine-tuning, achieving high accuracy and generalization with minimal trainable parameters.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of deep learning models in mixed-scene text recognition, such as lack of generality, stability, and high computational demands.

Method: Embeds weight-decomposed DoRA in the image encoder and LoRA in the text decoder for efficient fine-tuning on downstream tasks.

Result: Achieves CER of 4.02 on IAM, F1 of 94.29 on SROIE, and WAR of 86.70 on STR Benchmark, outperforming other methods.

Conclusion: DLoRA-TrOCR offers a scalable, efficient solution for mixed-scene text recognition with state-of-the-art performance.

Abstract: With the rapid development of OCR technology, mixed-scene text recognition
has become a key technical challenge. Although deep learning models have
achieved significant results in specific scenarios, their generality and
stability still need improvement, and the high demand for computing resources
affects flexibility. To address these issues, this paper proposes DLoRA-TrOCR,
a parameter-efficient hybrid text spotting method based on a pre-trained OCR
Transformer. By embedding a weight-decomposed DoRA module in the image encoder
and a LoRA module in the text decoder, this method can be efficiently
fine-tuned on various downstream tasks. Our method requires no more than 0.7\%
trainable parameters, not only accelerating the training efficiency but also
significantly improving the recognition accuracy and cross-dataset
generalization performance of the OCR system in mixed text scenes. Experiments
show that our proposed DLoRA-TrOCR outperforms other parameter-efficient
fine-tuning methods in recognizing complex scenes with mixed handwritten,
printed, and street text, achieving a CER of 4.02 on the IAM dataset, a F1
score of 94.29 on the SROIE dataset, and a WAR of 86.70 on the STR Benchmark,
reaching state-of-the-art performance.

</details>


### [127] [VladVA: Discriminative Fine-tuning of LVLMs](https://arxiv.org/pdf/2412.04378)
*Yassine Ouali, Adrian Bulat, Alexandros Xenos, Anestis Zaganidis, Ioannis Maniadis Metaxas, Brais Martinez, Georgios Tzimiropoulos*

Main category: cs.CV

TL;DR: The paper proposes a method to fine-tune Large Vision-Language Models (LVLMs) for discriminative tasks, combining their reasoning strengths with discriminative capabilities, outperforming CLIP-like models.


<details>
  <summary>Details</summary>
Motivation: VLMs like CLIP lack deep language understanding, while LVLMs excel in reasoning but are less suited for discriminative tasks. The goal is to merge these strengths.

Method: The approach involves discriminative fine-tuning of LVLMs using a framework with contrastive and next-token prediction losses, supported by parameter-efficient adaptation techniques like soft prompting and LoRA.

Result: The method achieves significant improvements over CLIP-like models in image-text retrieval and compositionality benchmarks.

Conclusion: The work successfully bridges the gap between generative and discriminative vision-language models, enhancing both capabilities.

Abstract: Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the
de facto approach for discriminative vision-language representation learning.
However, these models have limited language understanding, often exhibiting a
"bag of words" behavior. At the same time, Large Vision-Language Models
(LVLMs), which combine vision encoders with LLMs, have been shown to be capable
of detailed vision-language reasoning, yet their autoregressive nature renders
them less suitable for discriminative tasks.
  In this work, we propose to combine "the best of both worlds": a new training
approach for discriminative fine-tuning of LVLMs that results in strong
discriminative and compositional capabilities. Essentially, our approach
converts a generative LVLM into a discriminative one, unlocking its capability
for powerful image-text discrimination combined with enhanced language
understanding.
  Our contributions include (1) a carefully designed training/optimization
framework that utilizes image-text pairs of variable length and granularity for
training the model with both contrastive and next-token prediction losses. This
is accompanied by ablation studies that justify the necessity of our
framework's components; (2) a parameter-efficient adaptation method using a
combination of soft prompting and LoRA adapters; (3) significant improvements
over state-of-the-art CLIP-like models of similar size, including standard
image-text retrieval benchmarks and notable gains in compositionality.

</details>


### [128] [NeurCross: A Neural Approach to Computing Cross Fields for Quad Mesh Generation](https://arxiv.org/pdf/2405.13745)
*Qiujie Dong, Huibiao Wen, Rui Xu, Shuangmin Chen, Jiaran Zhou, Shiqing Xin, Changhe Tu, Taku Komura, Wenping Wang*

Main category: cs.CV

TL;DR: NeurCross is a framework for quadrilateral mesh generation that jointly optimizes a cross field and a neural SDF to improve alignment with principal curvatures and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with balancing cross field smoothness and alignment to principal curvatures, which are sensitive to noise and ill-defined in certain regions.

Method: NeurCross simultaneously optimizes a cross field and a neural SDF, guided by surface approximation, curvature alignment, and cross field smoothness.

Result: The neural SDF provides a more regular base surface for cross field alignment and leverages its Hessian matrix for implicit curvature alignment.

Conclusion: NeurCross addresses key challenges in quad mesh generation by integrating neural SDF optimization with cross field alignment.

Abstract: Quadrilateral mesh generation plays a crucial role in numerical simulations
within Computer-Aided Design and Engineering (CAD/E). Producing high-quality
quadrangulation typically requires satisfying four key criteria. First, the
quadrilateral mesh should closely align with principal curvature directions.
Second, singular points should be strategically placed and effectively
minimized. Third, the mesh should accurately conform to sharp feature edges.
Lastly, quadrangulation results should exhibit robustness against noise and
minor geometric variations. Existing methods generally involve first computing
a regular cross field to represent quad element orientations across the
surface, followed by extracting a quadrilateral mesh aligned closely with this
cross field. A primary challenge with this approach is balancing the smoothness
of the cross field with its alignment to pre-computed principal curvature
directions, which are sensitive to small surface perturbations and often
ill-defined in spherical or planar regions.
  To tackle this challenge, we propose NeurCross, a novel framework that
simultaneously optimizes a cross field and a neural signed distance function
(SDF), whose zero-level set serves as a proxy of the input shape. Our joint
optimization is guided by three factors: faithful approximation of the
optimized SDF surface to the input surface, alignment between the cross field
and the principal curvature field derived from the SDF surface, and smoothness
of the cross field. Acting as an intermediary, the neural SDF contributes in
two essential ways. First, it provides an alternative, optimizable base surface
exhibiting more regular principal curvature directions for guiding the cross
field. Second, we leverage the Hessian matrix of the neural SDF to implicitly
enforce cross field alignment with principal curvature directions...

</details>


### [129] [CrowdMoGen: Zero-Shot Text-Driven Collective Motion Generation](https://arxiv.org/pdf/2407.06188)
*Yukang Cao, Xinying Guo, Mingyuan Zhang, Haozhe Xie, Chenyang Gu, Ziwei Liu*

Main category: cs.CV

TL;DR: CrowdMoGen is a zero-shot framework for generating collective motion from text prompts, leveraging LLMs for scene planning and SMPL-based joint priors for motion generation, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-motion methods assume single-unit groups, failing to handle larger crowds or event-specific responses due to scene planning and motion coordination complexities.

Method: CrowdMoGen uses LLMs for high-level group organization and SMPL-based joint priors for context-appropriate motion. A transformer-based generator integrates activities spatially.

Result: CrowdMoGen outperforms prior methods, producing realistic, event-aligned, and spatially coherent motion sequences.

Conclusion: CrowdMoGen advances collective motion generation, with potential applications in urban simulation and crowd planning.

Abstract: While recent advances in text-to-motion generation have shown promising
results, they typically assume all individuals are grouped as a single unit.
Scaling these methods to handle larger crowds and ensuring that individuals
respond appropriately to specific events remains a significant challenge. This
is primarily due to the complexities of scene planning, which involves
organizing groups, planning their activities, and coordinating interactions,
and controllable motion generation. In this paper, we present CrowdMoGen, the
first zero-shot framework for collective motion generation, which effectively
groups individuals and generates event-aligned motion sequences from text
prompts. 1) Being limited by the available datasets for training an effective
scene planning module in a supervised manner, we instead propose a crowd scene
planner that leverages pre-trained large language models (LLMs) to organize
individuals into distinct groups. While LLMs offer high-level guidance for
group divisions, they lack the low-level understanding of human motion. To
address this, we further propose integrating an SMPL-based joint prior to
generate context-appropriate activities, which consists of both joint
trajectories and textual descriptions. 2) Secondly, to incorporate the assigned
activities into the generative network, we introduce a collective motion
generator that integrates the activities into a transformer-based network in a
joint-wise manner, maintaining the spatial constraints during the multi-step
denoising process. Extensive experiments demonstrate that CrowdMoGen
significantly outperforms previous approaches, delivering realistic,
event-driven motion sequences that are spatially coherent. As the first
framework of collective motion generation, CrowdMoGen has the potential to
advance applications in urban simulation, crowd planning, and other large-scale
interactive environments.

</details>


### [130] [Representation Learning and Identity Adversarial Training for Facial Behavior Understanding](https://arxiv.org/pdf/2407.11243)
*Mang Ning, Albert Ali Salah, Itir Onal Ertugrul*

Main category: cs.CV

TL;DR: The paper introduces Face9M, a diverse dataset, and proposes Facial Masked Autoencoder (FMAE) and Identity Adversarial Training (IAT) for improved AU detection, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the lack of diverse, large-scale data and unexplored identity regularization in AU detection, leveraging foundation models and adversarial training.

Method: Pretrain a masked autoencoder on Face9M and apply IAT to learn identity-invariant features, avoiding shortcut learning.

Result: FMAE-IAT achieves new state-of-the-art F1 scores on BP4D, BP4D+, and DISFA datasets.

Conclusion: FMAE and IAT are simple, generic, and effective methods for AU detection, with released code and models for reproducibility.

Abstract: Facial Action Unit (AU) detection has gained significant attention as it
enables the breakdown of complex facial expressions into individual muscle
movements. In this paper, we revisit two fundamental factors in AU detection:
diverse and large-scale data and subject identity regularization. Motivated by
recent advances in foundation models, we highlight the importance of data and
introduce Face9M, a diverse dataset comprising 9 million facial images from
multiple public sources. Pretraining a masked autoencoder on Face9M yields
strong performance in AU detection and facial expression tasks. More
importantly, we emphasize that the Identity Adversarial Training (IAT) has not
been well explored in AU tasks. To fill this gap, we first show that subject
identity in AU datasets creates shortcut learning for the model and leads to
sub-optimal solutions to AU predictions. Secondly, we demonstrate that strong
IAT regularization is necessary to learn identity-invariant features. Finally,
we elucidate the design space of IAT and empirically show that IAT circumvents
the identity-based shortcut learning and results in a better solution. Our
proposed methods, Facial Masked Autoencoder (FMAE) and IAT, are simple, generic
and effective. Remarkably, the proposed FMAE-IAT approach achieves new
state-of-the-art F1 scores on BP4D (67.1\%), BP4D+ (66.8\%), and DISFA (70.1\%)
databases, significantly outperforming previous work. We release the code and
model at https://github.com/forever208/FMAE-IAT.

</details>


### [131] [Generalized Class Discovery in Instance Segmentation](https://arxiv.org/pdf/2502.08149)
*Cuong Manh Hoang, Yeejin Lee, Byeongkeun Kang*

Main category: cs.CV

TL;DR: Proposes an instance-wise temperature assignment (ITA) method and class-wise reliability criteria for generalized class discovery (GCD) in instance segmentation, addressing imbalanced distributions and outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Real-world objects have long-tailed distributions, making instance segmentation challenging for both known and novel classes. The goal is to improve GCD by handling imbalanced data.

Method: Introduces ITA for contrastive learning, class-wise reliability criteria for pseudo-labels, and a soft attention module. Dynamically adjusts criteria to balance diversity and reliability.

Result: Outperforms previous state-of-the-art methods on COCO$_{half}$ + LVIS and LVIS + Visual Genome datasets.

Conclusion: The proposed ITA and reliability criteria effectively address imbalanced distributions, enhancing GCD performance in instance segmentation.

Abstract: This work addresses the task of generalized class discovery (GCD) in instance
segmentation. The goal is to discover novel classes and obtain a model capable
of segmenting instances of both known and novel categories, given labeled and
unlabeled data. Since the real world contains numerous objects with long-tailed
distributions, the instance distribution for each class is inherently
imbalanced. To address the imbalanced distributions, we propose an
instance-wise temperature assignment (ITA) method for contrastive learning and
class-wise reliability criteria for pseudo-labels. The ITA method relaxes
instance discrimination for samples belonging to head classes to enhance GCD.
The reliability criteria are to avoid excluding most pseudo-labels for tail
classes when training an instance segmentation network using pseudo-labels from
GCD. Additionally, we propose dynamically adjusting the criteria to leverage
diverse samples in the early stages while relying only on reliable
pseudo-labels in the later stages. We also introduce an efficient soft
attention module to encode object-specific representations for GCD. Finally, we
evaluate our proposed method by conducting experiments on two settings:
COCO$_{half}$ + LVIS and LVIS + Visual Genome. The experimental results
demonstrate that the proposed method outperforms previous state-of-the-art
methods.

</details>


### [132] [TAPTRv2: Attention-based Position Update Improves Tracking Any Point](https://arxiv.org/pdf/2407.16291)
*Hongyang Li, Hao Zhang, Shilong Liu, Zhaoyang Zeng, Feng Li, Tianhe Ren, Bohan Li, Lei Zhang*

Main category: cs.CV

TL;DR: TAPTRv2 improves TAPTR by replacing cost-volume reliance with an attention-based position update (APU) operation, enhancing performance and achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The reliance on cost-volume in TAPTR negatively impacts visibility prediction and cost-volume computation, necessitating a more efficient solution.

Method: TAPTRv2 introduces a novel APU operation using key-aware deformable attention to update query positions, eliminating the need for cost-volume.

Result: TAPTRv2 outperforms TAPTR and achieves state-of-the-art performance on challenging datasets.

Conclusion: The APU operation in TAPTRv2 effectively replaces cost-volume, improving efficiency and performance in the Tracking Any Point task.

Abstract: In this paper, we present TAPTRv2, a Transformer-based approach built upon
TAPTR for solving the Tracking Any Point (TAP) task. TAPTR borrows designs from
DEtection TRansformer (DETR) and formulates each tracking point as a point
query, making it possible to leverage well-studied operations in DETR-like
algorithms. TAPTRv2 improves TAPTR by addressing a critical issue regarding its
reliance on cost-volume,which contaminates the point query\'s content feature
and negatively impacts both visibility prediction and cost-volume computation.
In TAPTRv2, we propose a novel attention-based position update (APU) operation
and use key-aware deformable attention to realize. For each query, this
operation uses key-aware attention weights to combine their corresponding
deformable sampling positions to predict a new query position. This design is
based on the observation that local attention is essentially the same as
cost-volume, both of which are computed by dot-production between a query and
its surrounding features. By introducing this new operation, TAPTRv2 not only
removes the extra burden of cost-volume computation, but also leads to a
substantial performance improvement. TAPTRv2 surpasses TAPTR and achieves
state-of-the-art performance on many challenging datasets, demonstrating the
superiority

</details>


### [133] [Locality-aware Cross-modal Correspondence Learning for Dense Audio-Visual Events Localization](https://arxiv.org/pdf/2409.07967)
*Ling Xing, Hongyu Qu, Rui Yan, Xiangbo Shu, Jinhui Tang*

Main category: cs.CV

TL;DR: LoCo improves DAVE by using local temporal continuity to filter irrelevant signals and enhance cross-modal alignment, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing DAVE methods struggle with asynchronization and irrelevant feature attention, needing better cross-modal alignment.

Method: LoCo uses Local Correspondence Feature (LCF) Modulation and Local Adaptive Cross-modal (LAC) Interaction to focus on shared semantics and adjust attention dynamically.

Result: LoCo achieves solid performance gains and outperforms existing DAVE methods.

Conclusion: LoCo effectively addresses challenges in DAVE by leveraging local temporal continuity and adaptive cross-modal interaction.

Abstract: Dense-localization Audio-Visual Events (DAVE) aims to identify time
boundaries and corresponding categories for events that are both audible and
visible in a long video, where events may co-occur and exhibit varying
durations. However, complex audio-visual scenes often involve asynchronization
between modalities, making accurate localization challenging. Existing DAVE
solutions extract audio and visual features through unimodal encoders, and fuse
them via dense cross-modal interaction. However, independent unimodal encoding
struggles to emphasize shared semantics between modalities without cross-modal
guidance, while dense cross-modal attention may over-attend to semantically
unrelated audio-visual features. To address these problems, we present LoCo, a
Locality-aware cross-modal Correspondence learning framework for DAVE. LoCo
leverages the local temporal continuity of audio-visual events as important
guidance to filter irrelevant cross-modal signals and enhance cross-modal
alignment throughout both unimodal and cross-modal encoding stages. i)
Specifically, LoCo applies Local Correspondence Feature (LCF) Modulation to
enforce unimodal encoders to focus on modality-shared semantics by modulating
agreement between audio and visual features based on local cross-modal
coherence. ii) To better aggregate cross-modal relevant features, we further
customize Local Adaptive Cross-modal (LAC) Interaction, which dynamically
adjusts attention regions in a data-driven manner. This adaptive mechanism
focuses attention on local event boundaries and accommodates varying event
durations. By incorporating LCF and LAC, LoCo provides solid performance gains
and outperforms existing DAVE methods.

</details>


### [134] [Directed-CP: Directed Collaborative Perception for Connected and Autonomous Vehicles via Proactive Attention](https://arxiv.org/pdf/2409.08840)
*Yihang Tao, Senkang Hu, Zhengru Fang, Yuguang Fang*

Main category: cs.CV

TL;DR: Direct-CP improves collaborative perception by focusing on specific directions, enhancing local accuracy and efficiency under limited communication budgets.


<details>
  <summary>Details</summary>
Motivation: Current CP methods equally expand perception in all directions, which is inefficient in uneven traffic and limited bandwidth scenarios.

Method: Proposes a direction-aware system with RSU-aided masking, selective attention, and direction-weighted loss for training.

Result: Achieves 19.8% higher local accuracy in target directions and 2.5% overall improvement in 3D object detection.

Conclusion: Direct-CP effectively addresses inefficiencies in CP by prioritizing critical directions, improving performance and resource utilization.

Abstract: Collaborative perception (CP) leverages visual data from connected and
autonomous vehicles (CAV) to enhance an ego vehicle's field of view (FoV).
Despite recent progress, current CP methods expand the ego vehicle's 360-degree
perceptual range almost equally, which faces two key challenges. Firstly, in
areas with uneven traffic distribution, focusing on directions with little
traffic offers limited benefits. Secondly, under limited communication budgets,
allocating excessive bandwidth to less critical directions lowers the
perception accuracy in more vital areas. To address these issues, we propose
Direct-CP, a proactive and direction-aware CP system aiming at improving CP in
specific directions. Our key idea is to enable an ego vehicle to proactively
signal its interested directions and readjust its attention to enhance local
directional CP performance. To achieve this, we first propose an RSU-aided
direction masking mechanism that assists an ego vehicle in identifying vital
directions. Additionally, we design a direction-aware selective attention
module to wisely aggregate pertinent features based on ego vehicle's
directional priorities, communication budget, and the positional data of CAVs.
Moreover, we introduce a direction-weighted detection loss (DWLoss) to capture
the divergence between directional CP outcomes and the ground truth,
facilitating effective model training. Extensive experiments on the V2X-Sim 2.0
dataset demonstrate that our approach achieves 19.8\% higher local perception
accuracy in interested directions and 2.5\% higher overall perception accuracy
than the state-of-the-art methods in collaborative 3D object detection tasks.

</details>


### [135] [Egocentric and Exocentric Methods: A Short Survey](https://arxiv.org/pdf/2410.20621)
*Anirudh Thatipelli, Shao-Yuan Lo, Amit K. Roy-Chowdhury*

Main category: cs.CV

TL;DR: A survey on combining egocentric and exocentric vision, highlighting datasets, applications, and recent advances in joint learning.


<details>
  <summary>Details</summary>
Motivation: Jointly modeling ego and exo views is crucial for next-gen AI agents, as exocentric signals can enhance egocentric vision.

Method: Overview of existing works, datasets, and key applications in ego-exo joint learning.

Result: Identifies recent advances and current progress in the field.

Conclusion: The survey is valuable for video-understanding, especially in multi-view modeling scenarios.

Abstract: Egocentric vision captures the scene from the point of view of the camera
wearer, while exocentric vision captures the overall scene context. Jointly
modeling ego and exo views is crucial to developing next-generation AI agents.
The community has regained interest in the field of egocentric vision. While
the third-person view and first-person have been thoroughly investigated, very
few works aim to study both synchronously. Exocentric videos contain many
relevant signals that are transferrable to egocentric videos. This paper
provides a timely overview of works combining egocentric and exocentric
visions, a very new but promising research topic. We describe in detail the
datasets and present a survey of the key applications of ego-exo joint
learning, where we identify the most recent advances. With the presentation of
the current status of the progress, we believe this short but timely survey
will be valuable to the broad video-understanding community, particularly when
multi-view modeling is critical.

</details>


### [136] [AnySat: One Earth Observation Model for Many Resolutions, Scales, and Modalities](https://arxiv.org/pdf/2412.14123)
*Guillaume Astruc, Nicolas Gonthier, Clement Mallet, Loic Landrieu*

Main category: cs.CV

TL;DR: AnySat is a multimodal geospatial model using JEPA and scale-adaptive encoders, trained on diverse datasets (GeoPlex) to achieve state-of-the-art results in various environmental tasks.


<details>
  <summary>Details</summary>
Motivation: Existing geospatial models lack adaptability to diverse Earth observation data in terms of resolutions, scales, and modalities.

Method: Proposes AnySat, a model based on joint embedding predictive architecture (JEPA) and scale-adaptive spatial encoders, trained self-supervised on heterogeneous data (GeoPlex).

Result: Achieves state-of-the-art performance on GeoPlex and 6 external datasets across multiple environmental monitoring tasks.

Conclusion: AnySat demonstrates the effectiveness of a unified approach for handling diverse geospatial data, offering practical applicability and superior performance.

Abstract: Geospatial models must adapt to the diversity of Earth observation data in
terms of resolutions, scales, and modalities. However, existing approaches
expect fixed input configurations, which limits their practical applicability.
We propose AnySat, a multimodal model based on joint embedding predictive
architecture (JEPA) and scale-adaptive spatial encoders, allowing us to train a
single model on highly heterogeneous data in a self-supervised manner. To
demonstrate the advantages of this unified approach, we compile GeoPlex, a
collection of 5 multimodal datasets with varying characteristics and $11$
distinct sensors. We then train a single powerful model on these diverse
datasets simultaneously. Once fine-tuned or probed, we reach state-of-the-art
results on the test sets of GeoPlex and for 6 external datasets across various
environment monitoring tasks: land cover mapping, tree species identification,
crop type classification, change detection, climate type classification, and
segmentation of flood, burn scar, and deforestation. The code and models are
available at https://github.com/gastruc/AnySat.

</details>


### [137] [Enhancing AI Face Realism: Cost-Efficient Quality Improvement in Distilled Diffusion Models with a Fully Synthetic Dataset](https://arxiv.org/pdf/2505.02255)
*Jakub Wasala, Bartlomiej Wrzalski, Kornelia Noculak, Yuliia Tarasenko, Oliwer Krupa, Jan Kocon, Grzegorz Chodak*

Main category: cs.CV

TL;DR: A novel method improves cost-to-quality ratio in image generation by refining outputs of distilled models to match baseline quality, reducing computational costs by 82%.


<details>
  <summary>Details</summary>
Motivation: To enhance efficiency in AI image generation by reducing computational costs while maintaining high-quality outputs.

Method: Generate a synthetic paired dataset, train an image-to-image translation head to refine distilled model outputs to baseline quality levels.

Result: The pipeline achieves photorealistic portraits comparable to baseline models with an 82% reduction in computational cost.

Conclusion: Demonstrates potential for efficient large-scale image generation by combining distilled models with enhancement layers.

Abstract: This study presents a novel approach to enhance the cost-to-quality ratio of
image generation with diffusion models. We hypothesize that differences between
distilled (e.g. FLUX.1-schnell) and baseline (e.g. FLUX.1-dev) models are
consistent and, therefore, learnable within a specialized domain, like portrait
generation. We generate a synthetic paired dataset and train a fast
image-to-image translation head. Using two sets of low- and high-quality
synthetic images, our model is trained to refine the output of a distilled
generator (e.g., FLUX.1-schnell) to a level comparable to a baseline model like
FLUX.1-dev, which is more computationally intensive. Our results show that the
pipeline, which combines a distilled version of a large generative model with
our enhancement layer, delivers similar photorealistic portraits to the
baseline version with up to an 82% decrease in computational cost compared to
FLUX.1-dev. This study demonstrates the potential for improving the efficiency
of AI solutions involving large-scale image generation.

</details>


### [138] [Learning an Adaptive and View-Invariant Vision Transformer for Real-Time UAV Tracking](https://arxiv.org/pdf/2412.20002)
*You Wu, Yongxin Li, Mengyuan Liu, Xucheng Wang, Xiangyang Yang, Hengzhou Ye, Dan Zeng, Qijun Zhao, Shuiwang Li*

Main category: cs.CV

TL;DR: AVTrack-MD improves UAV tracking by selectively activating transformer blocks and using multi-teacher knowledge distillation, balancing performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency of state-of-the-art trackers on mobile platforms and challenges like extreme viewing angle changes in UAV tracking.

Method: Introduces AVTrack with an Activation Module for selective transformer block activation and AVTrack-MD, which uses MI maximization for multi-teacher knowledge distillation.

Result: Achieves comparable performance to AVTrack baseline with a 17% increase in tracking speed.

Conclusion: AVTrack-MD effectively balances tracking performance and efficiency, making it suitable for real-time UAV applications.

Abstract: Visual tracking has made significant strides due to the adoption of
transformer-based models. Most state-of-the-art trackers struggle to meet
real-time processing demands on mobile platforms with constrained computing
resources, particularly for real-time unmanned aerial vehicle (UAV) tracking.
To achieve a better balance between performance and efficiency, we introduce
AVTrack, an adaptive computation framework designed to selectively activate
transformer blocks for real-time UAV tracking. The proposed Activation Module
(AM) dynamically optimizes the ViT architecture by selectively engaging
relevant components, thereby enhancing inference efficiency without significant
compromise to tracking performance. Furthermore, to tackle the challenges posed
by extreme changes in viewing angles often encountered in UAV tracking, the
proposed method enhances ViTs' effectiveness by learning view-invariant
representations through mutual information (MI) maximization. Two effective
design principles are proposed in the AVTrack. Building on it, we propose an
improved tracker, dubbed AVTrack-MD, which introduces the novel MI
maximization-based multi-teacher knowledge distillation (MD) framework. It
harnesses the benefits of multiple teachers, specifically the off-the-shelf
tracking models from the AVTrack, by integrating and refining their outputs,
thereby guiding the learning process of the compact student network.
Specifically, we maximize the MI between the softened feature representations
from the multi-teacher models and the student model, leading to improved
generalization and performance of the student model, particularly in noisy
conditions. Extensive experiments on multiple UAV tracking benchmarks
demonstrate that AVTrack-MD not only achieves performance comparable to the
AVTrack baseline but also reduces model complexity, resulting in a significant
17\% increase in average tracking speed.

</details>


### [139] [Replay to Remember (R2R): An Efficient Uncertainty-driven Unsupervised Continual Learning Framework Using Generative Replay](https://arxiv.org/pdf/2505.04787)
*Sriram Mandalika, Harsha Vardhan, Athira Nambiar*

Main category: cs.CV

TL;DR: A novel unsupervised continual learning framework, R2R, uses generative replay and uncertainty-driven feedback to mitigate catastrophic forgetting, achieving state-of-the-art performance on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: To address catastrophic forgetting in neural networks by leveraging unlabelled data and synthetic labelled data without prior training.

Method: Uses a cluster-level uncertainty-driven feedback mechanism and a VLM-powered generative replay module to balance unlabelled and synthetic data.

Result: Achieves top performance on CIFAR-10 (98.13%), CIFAR-100 (73.06%), CINIC-10 (93.41%), SVHN (95.18%), and TinyImageNet (59.74%), surpassing prior methods by 4.36%.

Conclusion: R2R effectively retains knowledge and outperforms existing approaches in continual learning tasks.

Abstract: Continual Learning entails progressively acquiring knowledge from new data
while retaining previously acquired knowledge, thereby mitigating
``Catastrophic Forgetting'' in neural networks. Our work presents a novel
uncertainty-driven Unsupervised Continual Learning framework using Generative
Replay, namely ``Replay to Remember (R2R)''. The proposed R2R architecture
efficiently uses unlabelled and synthetic labelled data in a balanced
proportion using a cluster-level uncertainty-driven feedback mechanism and a
VLM-powered generative replay module. Unlike traditional memory-buffer methods
that depend on pretrained models and pseudo-labels, our R2R framework operates
without any prior training. It leverages visual features from unlabeled data
and adapts continuously using clustering-based uncertainty estimation coupled
with dynamic thresholding. Concurrently, a generative replay mechanism along
with DeepSeek-R1 powered CLIP VLM produces labelled synthetic data
representative of past experiences, resembling biological visual thinking that
replays memory to remember and act in new, unseen tasks. Extensive experimental
analyses are carried out in CIFAR-10, CIFAR-100, CINIC-10, SVHN and
TinyImageNet datasets. Our proposed R2R approach improves knowledge retention,
achieving a state-of-the-art performance of 98.13%, 73.06%, 93.41%, 95.18%,
59.74%, respectively, surpassing state-of-the-art performance by over 4.36%.

</details>


### [140] [Self-Supervised Pretraining for Fine-Grained Plankton Recognition](https://arxiv.org/pdf/2503.11341)
*Joona Kareinen, Tuomas Eerola, Kaisa Kraft, Lasse Lensu, Sanna Suikkanen, Heikki Kälviäinen*

Main category: cs.CV

TL;DR: Self-supervised pretraining with diverse plankton data improves fine-grained plankton recognition accuracy, especially with limited labeled data and unlabeled target data.


<details>
  <summary>Details</summary>
Motivation: Plankton recognition is crucial for ocean monitoring but challenging due to fine-grained species differences and dataset shifts. Minimizing expert labeling effort is needed as datasets grow.

Method: Large-scale self-supervised pretraining (masked autoencoding) on diverse plankton images, followed by fine-tuning with limited labeled data.

Result: Self-supervised pretraining outperforms ImageNet pretraining when labeled data is scarce. Accuracy improves further with unlabeled target data.

Conclusion: Self-supervised pretraining is effective for fine-grained plankton recognition, reducing reliance on labeled data.

Abstract: Plankton recognition is an important computer vision problem due to
plankton's essential role in ocean food webs and carbon capture, highlighting
the need for species-level monitoring. However, this task is challenging due to
its fine-grained nature and dataset shifts caused by different imaging
instruments and varying species distributions. As new plankton image datasets
are collected at an increasing pace, there is a need for general plankton
recognition models that require minimal expert effort for data labeling. In
this work, we study large-scale self-supervised pretraining for fine-grained
plankton recognition. We first employ masked autoencoding and a large volume of
diverse plankton image data to pretrain a general-purpose plankton image
encoder. Then we utilize fine-tuning to obtain accurate plankton recognition
models for new datasets with a very limited number of labeled training images.
Our experiments show that self-supervised pretraining with diverse plankton
data clearly increases plankton recognition accuracy compared to standard
ImageNet pretraining when the amount of training data is limited. Moreover, the
accuracy can be further improved when unlabeled target data is available and
utilized during the pretraining.

</details>


### [141] [MMMORRF: Multimodal Multilingual Modularized Reciprocal Rank Fusion](https://arxiv.org/pdf/2503.20698)
*Saron Samuel, Dan DeGenaro, Jimena Guallar-Blasco, Kate Sanders, Oluwaseun Eisape, Tanner Spendlove, Arun Reddy, Alexander Martin, Andrew Yates, Eugene Yang, Cameron Carpenter, David Etter, Efsun Kayi, Matthew Wiesner, Kenton Murray, Reno Kriz*

Main category: cs.CV

TL;DR: MMMORRF improves video retrieval by integrating multiple modalities (visual, audio, text) with a novel fusion method, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Current multimodal models prioritize visual signals, neglecting other modalities like audio and text, which are crucial for comprehensive video retrieval.

Method: MMMORRF extracts and integrates features from visual and audio modalities using modality-aware weighted reciprocal rank fusion.

Result: MMMORRF improves nDCG@20 by 81% over leading multimodal encoders and 37% over single-modality retrieval on benchmarks MultiVENT 2.0 and TVR.

Conclusion: Integrating diverse modalities enhances video retrieval effectiveness, addressing the bias toward visual queries in existing systems.

Abstract: Videos inherently contain multiple modalities, including visual events, text
overlays, sounds, and speech, all of which are important for retrieval.
However, state-of-the-art multimodal language models like VAST and LanguageBind
are built on vision-language models (VLMs), and thus overly prioritize visual
signals. Retrieval benchmarks further reinforce this bias by focusing on visual
queries and neglecting other modalities. We create a search system MMMORRF that
extracts text and features from both visual and audio modalities and integrates
them with a novel modality-aware weighted reciprocal rank fusion. MMMORRF is
both effective and efficient, demonstrating practicality in searching videos
based on users' information needs instead of visual descriptive queries. We
evaluate MMMORRF on MultiVENT 2.0 and TVR, two multimodal benchmarks designed
for more targeted information needs, and find that it improves nDCG@20 by 81%
over leading multimodal encoders and 37% over single-modality retrieval,
demonstrating the value of integrating diverse modalities.

</details>


### [142] [From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D](https://arxiv.org/pdf/2503.22976)
*Jiahui Zhang, Yurui Chen, Yanpeng Zhou, Yueming Xu, Ze Huang, Jilin Mei, Junhui Chen, Yu-Jie Yuan, Xinyue Cai, Guowei Huang, Xingyue Quan, Hang Xu, Li Zhang*

Main category: cs.CV

TL;DR: The paper introduces a 2D spatial data generation pipeline and SPAR-7M dataset to enhance LVLMs' spatial reasoning, achieving state-of-the-art performance on 2D benchmarks and competitive results on 3D tasks.


<details>
  <summary>Details</summary>
Motivation: LVLMs struggle with spatial perception in complex 3D scenes, limiting their reasoning abilities. The goal is to improve spatial understanding without relying on 3D representations.

Method: A novel 2D spatial data generation and annotation pipeline is developed using 3D ground-truth scene data. This creates diverse spatial tasks and the SPAR-7M dataset. SPAR-Bench is introduced for comprehensive evaluation.

Result: Models trained on SPAR-7M and 2D datasets achieve state-of-the-art performance on 2D benchmarks. Fine-tuning on 3D datasets yields competitive results.

Conclusion: The proposed pipeline and dataset effectively enhance spatial reasoning in LVLMs, demonstrating their potential for both 2D and 3D tasks.

Abstract: Recent advances in LVLMs have improved vision-language understanding, but
they still struggle with spatial perception, limiting their ability to reason
about complex 3D scenes. Unlike previous approaches that incorporate 3D
representations into models to improve spatial understanding, we aim to unlock
the potential of VLMs by leveraging spatially relevant image data. To this end,
we introduce a novel 2D spatial data generation and annotation pipeline built
upon scene data with 3D ground-truth. This pipeline enables the creation of a
diverse set of spatial tasks, ranging from basic perception tasks to more
complex reasoning tasks. Leveraging this pipeline, we construct SPAR-7M, a
large-scale dataset generated from thousands of scenes across multiple public
datasets. In addition, we introduce SPAR-Bench, a benchmark designed to offer a
more comprehensive evaluation of spatial capabilities compared to existing
spatial benchmarks, supporting both single-view and multi-view inputs. Training
on both SPAR-7M and large-scale 2D datasets enables our models to achieve
state-of-the-art performance on 2D spatial benchmarks. Further fine-tuning on
3D task-specific datasets yields competitive results, underscoring the
effectiveness of our dataset in enhancing spatial reasoning.

</details>


### [143] [Threshold Modulation for Online Test-Time Adaptation of Spiking Neural Networks](https://arxiv.org/pdf/2505.05375)
*Kejie Zhao, Wenjia Hua, Aiersi Tuerhong, Luziwei Leng, Yuxin Ma, Qinghai Guo*

Main category: cs.CV

TL;DR: The paper proposes a neuromorphic chip-friendly online test-time adaptation framework for spiking neural networks (SNNs) called Threshold Modulation (TM), enhancing robustness against distribution shifts.


<details>
  <summary>Details</summary>
Motivation: Existing online test-time adaptation (OTTA) methods are not well-suited for SNNs, creating a gap in adapting to distribution shifts efficiently.

Method: The TM framework dynamically adjusts firing thresholds using neuronal dynamics-inspired normalization, ensuring compatibility with neuromorphic hardware.

Result: Experiments show TM improves SNN robustness under distribution shifts while maintaining low computational cost.

Conclusion: TM provides a practical OTTA solution for SNNs, inspiring future neuromorphic chip designs.

Abstract: Recently, spiking neural networks (SNNs), deployed on neuromorphic chips,
provide highly efficient solutions on edge devices in different scenarios.
However, their ability to adapt to distribution shifts after deployment has
become a crucial challenge. Online test-time adaptation (OTTA) offers a
promising solution by enabling models to dynamically adjust to new data
distributions without requiring source data or labeled target samples.
Nevertheless, existing OTTA methods are largely designed for traditional
artificial neural networks and are not well-suited for SNNs. To address this
gap, we propose a low-power, neuromorphic chip-friendly online test-time
adaptation framework, aiming to enhance model generalization under distribution
shifts. The proposed approach is called Threshold Modulation (TM), which
dynamically adjusts the firing threshold through neuronal dynamics-inspired
normalization, being more compatible with neuromorphic hardware. Experimental
results on benchmark datasets demonstrate the effectiveness of this method in
improving the robustness of SNNs against distribution shifts while maintaining
low computational cost. The proposed method offers a practical solution for
online test-time adaptation of SNNs, providing inspiration for the design of
future neuromorphic chips. The demo code is available at
github.com/NneurotransmitterR/TM-OTTA-SNN.

</details>


### [144] [Foundation Models For Seismic Data Processing: An Extensive Review](https://arxiv.org/pdf/2503.24166)
*Fabian Fuchs, Mario Ruben Fernandez, Norman Ettrich, Janis Keuper*

Main category: cs.CV

TL;DR: The paper explores using natural image foundation models for seismic processing tasks like demultiple, interpolation, and denoising, evaluating their performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional seismic processing techniques are manual and time-consuming, while deep learning approaches often rely on synthetic data. Foundation models offer a promising alternative.

Method: The study investigates natural image foundation models for seismic tasks, assessing factors like pre-training and architecture.

Result: The paper evaluates performance and efficiency of various models, identifying promising candidates for future research.

Conclusion: Natural image foundation models show potential for seismic processing, with specific models highlighted for further exploration.

Abstract: Seismic processing plays a crucial role in transforming raw data into
high-quality subsurface images, pivotal for various geoscience applications.
Despite its importance, traditional seismic processing techniques face
challenges such as noisy and damaged data and the reliance on manual,
time-consuming workflows. The emergence of deep learning approaches has
introduced effective and user-friendly alternatives, yet many of these deep
learning approaches rely on synthetic datasets and specialized neural networks.
Recently, foundation models have gained traction in the seismic domain, due to
their success in the natural image domain. Therefore, we investigate the
application of natural image foundation models on the three seismic processing
tasks: demultiple, interpolation, and denoising. We evaluate the impact of
different model characteristics, such as pre-training technique and neural
network architecture, on performance and efficiency. Rather than proposing a
single seismic foundation model, we critically examine various natural image
foundation models and suggest some promising candidates for future exploration.

</details>


### [145] [Visualization of a multidimensional point cloud as a 3D swarm of avatars](https://arxiv.org/pdf/2504.06751)
*Leszek Luchowski, Dariusz Pojda*

Main category: cs.CV

TL;DR: An innovative method for visualizing multidimensional data using Chernoff-face-inspired icons, combining projection techniques and facial feature mapping for intuitive analysis.


<details>
  <summary>Details</summary>
Motivation: To leverage the human brain's natural ability to interpret facial expressions for better understanding of complex multidimensional data.

Method: Semantic division of data dimensions into intuitive (assigned to avatar features) and technical (projected into hyperspace). Implemented as a plugin for dpVision.

Result: Successful visualization of synthetic and real-world data (e.g., Portuguese Vinho Verde wines), confirming the approach's effectiveness.

Conclusion: The method provides a useful tool for interactive exploration and analysis of complex data structures.

Abstract: The article presents an innovative approach to the visualization of
multidimensional data, using icons inspired by Chernoff faces. The approach
merges classical projection techniques with the assignment of particular data
dimensions to mimic features, capitalizing on the natural ability of the human
brain to interpret facial expressions. We introduce a semantic division of data
dimensions into intuitive and technical categories, assigning the former to
avatar features and projecting the latter into a hyperspace of four, or
potentially more dimensions. The technique is implemented as a plugin to the
dpVision open-source image handling platform. The plugin allows the data to be
interactively explored in the form of a swarm of avatars whose position in
hyperspace as well as facial features represent various aspects of the data.
Sample visualizations, based on synthetic test data as well as the
12-dimensional database on Portuguese Vinho Verde wines, confirm the usefulness
of our approach to the analysis of complex data structures.

</details>


### [146] [Patch distribution modeling framework adaptive cosine estimator (PaDiM-ACE) for anomaly detection and localization in synthetic aperture radar imagery](https://arxiv.org/pdf/2504.08049)
*Angelina Ibarra, Joshua Peeples*

Main category: cs.CV

TL;DR: A new anomaly detection method for SAR imagery using adaptive cosine estimator (ACE) improves upon PaDiM by providing bounded scores via cosine similarity.


<details>
  <summary>Details</summary>
Motivation: To enhance anomaly detection and localization in SAR imagery by addressing the unbounded nature of Mahalanobis distance in PaDiM.

Method: Introduces ACE, replacing Mahalanobis distance with cosine similarity for bounded anomaly scores, evaluated on SAR datasets.

Result: Improved performance in anomaly detection and localization, measured by AUROC at image and pixel levels.

Conclusion: The ACE-based method outperforms PaDiM, offering better anomaly detection and localization for SAR imagery.

Abstract: This work presents a new approach to anomaly detection and localization in
synthetic aperture radar imagery (SAR), expanding upon the existing patch
distribution modeling framework (PaDiM). We introduce the adaptive cosine
estimator (ACE) detection statistic. PaDiM uses the Mahalanobis distance at
inference, an unbounded metric. ACE instead uses the cosine similarity metric,
providing bounded anomaly detection scores. The proposed method is evaluated
across multiple SAR datasets, with performance metrics including the area under
the receiver operating curve (AUROC) at the image and pixel level, aiming for
increased performance in anomaly detection and localization of SAR imagery. The
code is publicly available:
https://github.com/Advanced-Vision-and-Learning-Lab/PaDiM-ACE.

</details>


### [147] [Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene Understanding](https://arxiv.org/pdf/2504.13580)
*Yuchen Rao, Stefan Ainetter, Sinisa Stekovic, Vincent Lepetit, Friedrich Fraundorfer*

Main category: cs.CV

TL;DR: Using synthetic CAD models for automatic 3D annotations improves deep learning model performance and reduces annotation costs, validated on ScanNet++ v1.


<details>
  <summary>Details</summary>
Motivation: High-quality 3D annotations are hard to generate manually, hindering deep learning model development.

Method: Employed an automatic pipeline to annotate ScanNet++ v1 with CAD models and 9D poses, similar to prior work on ScanNet.

Result: Models trained on automatic annotations outperformed those using manual annotations in point cloud completion and CAD model retrieval tasks.

Conclusion: Automatic 3D annotations are viable and superior, reducing costs and improving performance; annotations and models will be released as SCANnotate++.

Abstract: High-level 3D scene understanding is essential in many applications. However,
the challenges of generating accurate 3D annotations make development of deep
learning models difficult. We turn to recent advancements in automatic
retrieval of synthetic CAD models, and show that data generated by such methods
can be used as high-quality ground truth for training supervised deep learning
models. More exactly, we employ a pipeline akin to the one previously used to
automatically annotate objects in ScanNet scenes with their 9D poses and CAD
models. This time, we apply it to the recent ScanNet++ v1 dataset, which
previously lacked such annotations. Our findings demonstrate that it is not
only possible to train deep learning models on these automatically-obtained
annotations but that the resulting models outperform those trained on manually
annotated data. We validate this on two distinct tasks: point cloud completion
and single-view CAD model retrieval and alignment. Our results underscore the
potential of automatic 3D annotations to enhance model performance while
significantly reducing annotation costs. To support future research in 3D scene
understanding, we will release our annotations, which we call SCANnotate++,
along with our trained models.

</details>


### [148] [FF-PNet: A Pyramid Network Based on Feature and Field for Brain Image Registration](https://arxiv.org/pdf/2505.04938)
*Ying Zhang, Shuai Guo, Chenxi Sun, Yuchen Zhu, Jinhai Xiang*

Main category: cs.CV

TL;DR: A new pyramid registration network (FF-PNet) with parallel coarse and fine-grained feature extraction modules (RFFM and RDFFM) improves deformable medical image registration efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing models lack efficiency in parallel extraction of coarse and fine-grained features for deformable medical image registration.

Method: Proposes FF-PNet with Residual Feature Fusion Module (RFFM) for coarse features and Residual Deformation Field Fusion Module (RDFFM) for fine-grained deformation, using traditional CNNs without attention mechanisms.

Result: Outperforms popular methods on LPBA and OASIS datasets, achieving higher Dice Similarity Coefficient.

Conclusion: FF-PNet demonstrates superior feature decoding and deformation handling, proving effective without advanced mechanisms like attention or MLPs.

Abstract: In recent years, deformable medical image registration techniques have made
significant progress. However, existing models still lack efficiency in
parallel extraction of coarse and fine-grained features. To address this, we
construct a new pyramid registration network based on feature and deformation
field (FF-PNet). For coarse-grained feature extraction, we design a Residual
Feature Fusion Module (RFFM), for fine-grained image deformation, we propose a
Residual Deformation Field Fusion Module (RDFFM). Through the parallel
operation of these two modules, the model can effectively handle complex image
deformations. It is worth emphasizing that the encoding stage of FF-PNet only
employs traditional convolutional neural networks without any attention
mechanisms or multilayer perceptrons, yet it still achieves remarkable
improvements in registration accuracy, fully demonstrating the superior feature
decoding capabilities of RFFM and RDFFM. We conducted extensive experiments on
the LPBA and OASIS datasets. The results show our network consistently
outperforms popular methods in metrics like the Dice Similarity Coefficient.

</details>


### [149] [UncertainSAM: Fast and Efficient Uncertainty Quantification of the Segment Anything Model](https://arxiv.org/pdf/2505.05049)
*Timo Kaiser, Thomas Norrenbrock, Bodo Rosenhahn*

Main category: cs.CV

TL;DR: The paper introduces USAM, a lightweight Bayesian entropy-based uncertainty quantification method for the Segment Anything Model (SAM), addressing aleatoric, epistemic, and task uncertainty.


<details>
  <summary>Details</summary>
Motivation: Quantifying uncertainty in SAM is challenging due to its class-agnostic nature, prompting the need for a robust uncertainty quantification approach.

Method: Proposes USAM, a post-hoc Bayesian entropy-based model that identifies uncertainty sources like under-parameterization, insufficient prompts, or image ambiguities.

Result: USAM outperforms on SA-V, MOSE, ADE20k, DAVIS, and COCO datasets, offering a cheap and user-friendly UQ solution.

Conclusion: USAM provides an efficient uncertainty quantification alternative for SAM, enhancing semi-supervised pipelines and balancing accuracy-cost tradeoffs.

Abstract: The introduction of the Segment Anything Model (SAM) has paved the way for
numerous semantic segmentation applications. For several tasks, quantifying the
uncertainty of SAM is of particular interest. However, the ambiguous nature of
the class-agnostic foundation model SAM challenges current uncertainty
quantification (UQ) approaches. This paper presents a theoretically motivated
uncertainty quantification model based on a Bayesian entropy formulation
jointly respecting aleatoric, epistemic, and the newly introduced task
uncertainty. We use this formulation to train USAM, a lightweight post-hoc UQ
method. Our model traces the root of uncertainty back to under-parameterised
models, insufficient prompts or image ambiguities. Our proposed deterministic
USAM demonstrates superior predictive capabilities on the SA-V, MOSE, ADE20k,
DAVIS, and COCO datasets, offering a computationally cheap and easy-to-use UQ
alternative that can support user-prompting, enhance semi-supervised pipelines,
or balance the tradeoff between accuracy and cost efficiency.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [150] [Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation Methods](https://arxiv.org/pdf/2505.05541)
*Markov Grey, Charbel-Raphaël Segerie*

Main category: cs.AI

TL;DR: The paper reviews AI safety evaluations, proposing a taxonomy around measuring capabilities, propensities, and control, while highlighting challenges and research directions.


<details>
  <summary>Details</summary>
Motivation: Frontier AI systems require better evaluation methods to ensure safety and inform governance, as traditional benchmarks often fail to predict real-world behavior.

Method: The review consolidates AI safety evaluations into a taxonomy focusing on what properties to measure (capabilities, propensities, control), how to measure them (behavioral and internal techniques), and integration into governance frameworks.

Result: The paper identifies key safety-critical capabilities and concerning propensities, discusses evaluation challenges, and suggests research directions.

Conclusion: The review serves as a central reference for AI safety evaluations, emphasizing the need for systematic approaches to address safety and governance gaps.

Abstract: As frontier AI systems advance toward transformative capabilities, we need a
parallel transformation in how we measure and evaluate these systems to ensure
safety and inform governance. While benchmarks have been the primary method for
estimating model capabilities, they often fail to establish true upper bounds
or predict deployment behavior. This literature review consolidates the rapidly
evolving field of AI safety evaluations, proposing a systematic taxonomy around
three dimensions: what properties we measure, how we measure them, and how
these measurements integrate into frameworks. We show how evaluations go beyond
benchmarks by measuring what models can do when pushed to the limit
(capabilities), the behavioral tendencies exhibited by default (propensities),
and whether our safety measures remain effective even when faced with
subversive adversarial AI (control). These properties are measured through
behavioral techniques like scaffolding, red teaming and supervised fine-tuning,
alongside internal techniques such as representation analysis and mechanistic
interpretability. We provide deeper explanations of some safety-critical
capabilities like cybersecurity exploitation, deception, autonomous
replication, and situational awareness, alongside concerning propensities like
power-seeking and scheming. The review explores how these evaluation methods
integrate into governance frameworks to translate results into concrete
development decisions. We also highlight challenges to safety evaluations -
proving absence of capabilities, potential model sandbagging, and incentives
for "safetywashing" - while identifying promising research directions. By
synthesizing scattered resources, this literature review aims to provide a
central reference point for understanding AI safety evaluations.

</details>


### [151] [HiBayES: A Hierarchical Bayesian Modeling Framework for AI Evaluation Statistics](https://arxiv.org/pdf/2505.05602)
*Lennart Luettgau, Harry Coppock, Magda Dubois, Christopher Summerfield, Cozmin Ududec*

Main category: cs.AI

TL;DR: HiBayES is a Hierarchical Bayesian framework for robust AI evaluation, offering uncertainty quantification and parameter estimation in low-data scenarios.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of estimating AI capabilities from stochastic outputs and quantifying uncertainty in hierarchical, complex evaluations.

Method: Uses Generalized Linear Models (GLMs), Bayesian data analysis, and model comparison for robust inferences.

Result: Provides principled uncertainty quantification and robust parameter estimation, with a software package for implementation.

Conclusion: HiBayES is a practical solution for advanced AI evaluations, especially in low-data settings.

Abstract: As Large Language Models (LLMs) and other AI systems evolve, robustly
estimating their capabilities from inherently stochastic outputs while
systematically quantifying uncertainty in these estimates becomes increasingly
important. Further, advanced AI evaluations often have a nested hierarchical
structure, exhibit high levels of complexity, and come with high costs in
testing the most advanced AI systems. To address these challenges, we introduce
HiBayES, a generalizable Hierarchical Bayesian modeling framework for AI
Evaluation Statistics. HiBayES supports robust inferences in classical
question-answer benchmarks and advanced agentic evaluations, particularly in
low-data scenarios (e.g., < 20 data points per evaluation). Built on
Generalized Linear Models (GLMs), Bayesian data analysis, and formal model
comparison, HiBayES provides principled uncertainty quantification and robust
parameter estimation. This paper offers a comprehensive introduction to
HiBayES, including illustrative examples, comparisons to conventional
statistical methods, and practical guidance for implementing multilevel
Bayesian GLMs. Additionally, we provide a HiBayES software package [4] (Beta
version) for out-of-the-box implementation.

</details>


### [152] [scDrugMap: Benchmarking Large Foundation Models for Drug Response Prediction](https://arxiv.org/pdf/2505.05612)
*Qing Wang, Yining Pan, Minghao Zhou, Zijia Tang, Yanfei Wang, Guangyu Wang, Qianqian Song*

Main category: cs.AI

TL;DR: scDrugMap is a framework for drug response prediction in single-cell data, benchmarking foundation models and offering a user-friendly platform for drug discovery.


<details>
  <summary>Details</summary>
Motivation: Drug resistance in cancer therapy and the underexplored use of large-scale foundation models for predicting drug response in single-cell data.

Method: Developed scDrugMap, evaluating foundation models (8 single-cell, 2 LLMs) on 326K+ cells, using layer freezing and LoRA fine-tuning.

Result: scFoundation performed best in pooled-data (F1: 0.971/0.947), UCE in cross-data fine-tuning (F1: 0.774), and scGPT in zero-shot (F1: 0.858).

Conclusion: scDrugMap is a pioneering benchmark and platform for drug response prediction, aiding drug discovery and research.

Abstract: Drug resistance presents a major challenge in cancer therapy. Single cell
profiling offers insights into cellular heterogeneity, yet the application of
large-scale foundation models for predicting drug response in single cell data
remains underexplored. To address this, we developed scDrugMap, an integrated
framework featuring both a Python command-line interface and a web server for
drug response prediction. scDrugMap evaluates a wide range of foundation
models, including eight single-cell models and two large language models, using
a curated dataset of over 326,000 cells in the primary collection and 18,800
cells in the validation set, spanning 36 datasets and diverse tissue and cancer
types. We benchmarked model performance under pooled-data and cross-data
evaluation settings, employing both layer freezing and Low-Rank Adaptation
(LoRA) fine-tuning strategies. In the pooled-data scenario, scFoundation
achieved the best performance, with mean F1 scores of 0.971 (layer freezing)
and 0.947 (fine-tuning), outperforming the lowest-performing model by over 50%.
In the cross-data setting, UCE excelled post fine-tuning (mean F1: 0.774),
while scGPT led in zero-shot learning (mean F1: 0.858). Overall, scDrugMap
provides the first large-scale benchmark of foundation models for drug response
prediction in single-cell data and serves as a user-friendly, flexible platform
for advancing drug discovery and translational research.

</details>


### [153] [Leveraging Large Language Models for enzymatic reaction prediction and characterization](https://arxiv.org/pdf/2505.05616)
*Lorenzo Di Fruscia, Jana Marie Weber*

Main category: cs.AI

TL;DR: The paper evaluates LLMs (Llama-3.1 family) for predicting enzymatic reactions, focusing on EC number prediction, forward synthesis, and retrosynthesis. Multitask learning improves performance, but challenges remain in hierarchical EC classification.


<details>
  <summary>Details</summary>
Motivation: Enzymatic reaction prediction is vital for biocatalysis, metabolic engineering, and drug discovery, but current methods are complex and resource-heavy. LLMs offer potential due to their generalization and reasoning abilities.

Method: Systematic evaluation of LLMs (8B and 70B) using LoRA adapters for fine-tuning. Tasks include EC number prediction, forward synthesis, and retrosynthesis, comparing single-task and multitask learning across data regimes.

Result: Fine-tuned LLMs effectively capture biochemical knowledge, with multitask learning boosting forward- and retrosynthesis predictions. Performance varies in low-data settings, and hierarchical EC classification poses challenges.

Conclusion: LLMs show promise for enzymatic reaction prediction, with multitask learning enhancing results. However, limitations like EC classification issues indicate room for improvement in LLM-driven biochemical modeling.

Abstract: Predicting enzymatic reactions is crucial for applications in biocatalysis,
metabolic engineering, and drug discovery, yet it remains a complex and
resource-intensive task. Large Language Models (LLMs) have recently
demonstrated remarkable success in various scientific domains, e.g., through
their ability to generalize knowledge, reason over complex structures, and
leverage in-context learning strategies. In this study, we systematically
evaluate the capability of LLMs, particularly the Llama-3.1 family (8B and
70B), across three core biochemical tasks: Enzyme Commission number prediction,
forward synthesis, and retrosynthesis. We compare single-task and multitask
learning strategies, employing parameter-efficient fine-tuning via LoRA
adapters. Additionally, we assess performance across different data regimes to
explore their adaptability in low-data settings. Our results demonstrate that
fine-tuned LLMs capture biochemical knowledge, with multitask learning
enhancing forward- and retrosynthesis predictions by leveraging shared
enzymatic information. We also identify key limitations, for example challenges
in hierarchical EC classification schemes, highlighting areas for further
improvement in LLM-driven biochemical modeling.

</details>


### [154] [Prompted Meta-Learning for Few-shot Knowledge Graph Completion](https://arxiv.org/pdf/2505.05684)
*Han Wu, Jie Yin*

Main category: cs.AI

TL;DR: A novel PromptMeta framework integrates meta-semantics with relational information for few-shot knowledge graph completion, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing few-shot KGC methods overlook rich semantics in KGs, limiting their effectiveness.

Method: Proposes PromptMeta with a meta-semantic prompt pool and a learnable fusion prompt, optimized via meta-learning.

Result: Outperforms benchmarks on two datasets, demonstrating effectiveness.

Conclusion: PromptMeta successfully leverages meta-semantics for improved few-shot KGC.

Abstract: Few-shot knowledge graph completion (KGC) has obtained significant attention
due to its practical applications in real-world scenarios, where new knowledge
often emerges with limited available data. While most existing methods for
few-shot KGC have predominantly focused on leveraging relational information,
rich semantics inherent in KGs have been largely overlooked. To address this
gap, we propose a novel prompted meta-learning (PromptMeta) framework that
seamlessly integrates meta-semantics with relational information for few-shot
KGC. PrompMeta has two key innovations: (1) a meta-semantic prompt pool that
captures and consolidates high-level meta-semantics, enabling effective
knowledge transfer and adaptation to rare and newly emerging relations. (2) a
learnable fusion prompt that dynamically combines meta-semantic information
with task-specific relational information tailored to different few-shot tasks.
Both components are optimized together with model parameters within a
meta-learning framework. Extensive experiments on two benchmark datasets
demonstrate the effectiveness of our approach.

</details>


### [155] [Pretraining a Shared Q-Network for Data-Efficient Offline Reinforcement Learning](https://arxiv.org/pdf/2505.05701)
*Jongchan Park, Mingyu Park, Donghwan Lee*

Main category: cs.AI

TL;DR: A plug-and-play pretraining method for offline RL improves data efficiency by initializing a shared Q-network with supervised regression tasks, enhancing performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Offline RL struggles with limited datasets due to exhaustive data collection requirements. This paper addresses the challenge of learning optimal policies with minimal static data.

Method: Proposes a shared Q-network structure pretrained via supervised regression to predict next states and Q-values, integrated with diverse offline RL methods.

Result: Empirically improves performance on D4RL, Robomimic, and V-D4RL benchmarks, even outperforming standard algorithms with only 10% of the dataset.

Conclusion: The method significantly boosts data-efficient offline RL across varied data qualities and distributions, demonstrating its effectiveness and adaptability.

Abstract: Offline reinforcement learning (RL) aims to learn a policy from a static
dataset without further interactions with the environment. Collecting
sufficiently large datasets for offline RL is exhausting since this data
collection requires colossus interactions with environments and becomes tricky
when the interaction with the environment is restricted. Hence, how an agent
learns the best policy with a minimal static dataset is a crucial issue in
offline RL, similar to the sample efficiency problem in online RL. In this
paper, we propose a simple yet effective plug-and-play pretraining method to
initialize a feature of a $Q$-network to enhance data efficiency in offline RL.
Specifically, we introduce a shared $Q$-network structure that outputs
predictions of the next state and $Q$-value. We pretrain the shared $Q$-network
through a supervised regression task that predicts a next state and trains the
shared $Q$-network using diverse offline RL methods. Through extensive
experiments, we empirically demonstrate that our method enhances the
performance of existing popular offline RL methods on the D4RL, Robomimic and
V-D4RL benchmarks. Furthermore, we show that our method significantly boosts
data-efficient offline RL across various data qualities and data distributions
trough D4RL and ExoRL benchmarks. Notably, our method adapted with only 10% of
the dataset outperforms standard algorithms even with full datasets.

</details>


### [156] [APOLLO: Automated LLM and Lean Collaboration for Advanced Formal Reasoning](https://arxiv.org/pdf/2505.05758)
*Azim Ospanov, Roozbeh Yousefzadeh*

Main category: cs.AI

TL;DR: APOLLO is a pipeline combining Lean and LLMs to improve automated theorem proving, achieving state-of-the-art accuracy with low sampling budgets.


<details>
  <summary>Details</summary>
Motivation: Generating correct formal proofs with LLMs is challenging due to high sampling requirements. APOLLO aims to enhance efficiency and correctness by leveraging Lean and targeted repair.

Method: APOLLO automates proof generation, error analysis, syntax fixing, sub-lemma isolation, solver use, and iterative LLM invocation with low top-K budgets.

Result: Achieves 75.0% accuracy on miniF2F (7B models) and 65.6% for Goedel-Prover-SFT, reducing sample complexity from 25,600 to hundreds. General models improve from 3-7% to over 40%.

Conclusion: Compiler-guided repair of LLM outputs significantly boosts efficiency and correctness, offering a scalable paradigm for automated theorem proving.

Abstract: Formal reasoning and automated theorem proving constitute a challenging
subfield of machine learning, in which machines are tasked with proving
mathematical theorems using formal languages like Lean. A formal verification
system can check whether a formal proof is correct or not almost
instantaneously, but generating a completely correct formal proof with large
language models (LLMs) remains a formidable task. The usual approach in the
literature is to prompt the LLM many times (up to several thousands) until one
of the generated proofs passes the verification system. In this work, we
present APOLLO (Automated PrOof repair via LLM and Lean cOllaboration), a
modular, model-agnostic pipeline that combines the strengths of the Lean
compiler with an LLM's reasoning abilities to achieve better proof-generation
results at a low sampling budget. Apollo directs a fully automated process in
which the LLM generates proofs for theorems, a set of agents analyze the
proofs, fix the syntax errors, identify the mistakes in the proofs using Lean,
isolate failing sub-lemmas, utilize automated solvers, and invoke an LLM on
each remaining goal with a low top-K budget. The repaired sub-proofs are
recombined and reverified, iterating up to a user-controlled maximum number of
attempts. On the miniF2F benchmark, we establish a new state-of-the-art
accuracy of 75.0% among 7B-parameter models while keeping the sampling budget
below one thousand. Moreover, Apollo raises the state-of-the-art accuracy for
Goedel-Prover-SFT to 65.6% while cutting sample complexity from 25,600 to a few
hundred. General-purpose models (o3-mini, o4-mini) jump from 3-7% to over 40%
accuracy. Our results demonstrate that targeted, compiler-guided repair of LLM
outputs yields dramatic gains in both efficiency and correctness, suggesting a
general paradigm for scalable automated theorem proving.

</details>


### [157] [AVA: Attentive VLM Agent for Mastering StarCraft II](https://arxiv.org/pdf/2503.05383)
*Weiyu Ma, Yuqian Fu, Zecheng Zhang, Bernard Ghanem, Guohao Li*

Main category: cs.AI

TL;DR: AVA is a multimodal StarCraft II agent using RGB and language inputs to align with human perception, outperforming traditional methods without extensive training.


<details>
  <summary>Details</summary>
Motivation: Traditional agents like SMAC use abstract states, diverging from human perception. AVA aims to bridge this gap for more ecologically valid behavior.

Method: AVA combines a vision-language model with self-attention, retrieval-augmented generation, and dynamic role-based task distribution for coordinated gameplay.

Result: AVA achieves comparable performance to traditional MARL methods in 21 scenarios without explicit training, using foundation models like Qwen-VL and GPT-4o.

Conclusion: AVA advances human-aligned game AI and sets a foundation for multimodal agents in StarCraft II and beyond.

Abstract: We introduce Attentive VLM Agent (AVA), a multimodal StarCraft II agent that
aligns artificial agent perception with the human gameplay experience.
Traditional frameworks such as SMAC rely on abstract state representations that
diverge significantly from human perception, limiting the ecological validity
of agent behavior. Our agent addresses this limitation by incorporating RGB
visual inputs and natural language observations that more closely simulate
human cognitive processes during gameplay. The AVA architecture consists of
three integrated components: (1) a vision-language model enhanced with
specialized self-attention mechanisms for strategic unit targeting and
battlefield assessment, (2) a retrieval-augmented generation system that
leverages domain-specific StarCraft II knowledge to inform tactical decisions,
and (3) a dynamic role-based task distribution system that enables coordinated
multi-agent behavior. The experimental evaluation in our proposed AVACraft
environment, which contains 21 multimodal StarCraft II scenarios, demonstrates
that AVA powered by foundation models (specifically Qwen-VL and GPT-4o) can
execute complex tactical maneuvers without explicit training, achieving
comparable performance to traditional MARL methods that require substantial
training iterations. This work establishes a foundation for developing
human-aligned StarCraft II agents and advances the broader research agenda of
multimodal game AI. Our implementation is available at
https://github.com/camel-ai/VLM-Play-StarCraft2.

</details>


### [158] [Combining Abstract Argumentation and Machine Learning for Efficiently Analyzing Low-Level Process Event Streams](https://arxiv.org/pdf/2505.05880)
*Bettina Fazzinga, Sergio Flesca, Filippo Furfaro, Luigi Pontieri, Francesco Scala*

Main category: cs.AI

TL;DR: A neuro-symbolic approach combines sequence tagging and argumentation frameworks to efficiently interpret process traces, reducing data and computational costs.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap between trace events and business activities, especially under uncertainty, while promoting Green AI for sustainability.

Method: Uses a sequence-tagging model for candidate interpretations, refined by an Abstract Argumentation Framework (AAF) to leverage prior knowledge.

Result: The approach compensates for scarce annotated data and reduces computational costs, validated by experiments.

Conclusion: The method is effective in resource-constrained settings, balancing data efficiency and computational sustainability.

Abstract: Monitoring and analyzing process traces is a critical task for modern
companies and organizations. In scenarios where there is a gap between trace
events and reference business activities, this entails an interpretation
problem, amounting to translating each event of any ongoing trace into the
corresponding step of the activity instance. Building on a recent approach that
frames the interpretation problem as an acceptance problem within an Abstract
Argumentation Framework (AAF), one can elegantly analyze plausible event
interpretations (possibly in an aggregated form), as well as offer explanations
for those that conflict with prior process knowledge. Since, in settings where
event-to-activity mapping is highly uncertain (or simply under-specified) this
reasoning-based approach may yield lowly-informative results and heavy
computation, one can think of discovering a sequencetagging model, trained to
suggest highly-probable candidate event interpretations in a context-aware way.
However, training such a model optimally may require using a large amount of
manually-annotated example traces. Considering the urgent need of developing
Green AI solutions enabling environmental and societal sustainability (with
reduced labor/computational costs and carbon footprint), we propose a
data/computation-efficient neuro-symbolic approach to the problem, where the
candidate interpretations returned by the example-driven sequence tagger is
refined by the AAF-based reasoner. This allows us to also leverage prior
knowledge to compensate for the scarcity of example data, as confirmed by
experimental results; clearly, this property is particularly useful in settings
where data annotation and model optimization costs are subject to stringent
constraints.

</details>


### [159] [Pseudo-Boolean d-DNNF Compilation for Expressive Feature Modeling Constructs](https://arxiv.org/pdf/2505.05976)
*Chico Sundermann, Stefan Vill, Elias Kuiter, Sebastian Krieter, Thomas Thüm, Matthias Tichy*

Main category: cs.AI

TL;DR: The paper proposes a pseudo-Boolean encoding for feature models and a method to compile them to Boolean d-DNNF, improving scalability and efficiency for automated reasoning compared to CNF-based approaches.


<details>
  <summary>Details</summary>
Motivation: The mismatch between expressive feature-modeling constructs (e.g., cardinality constraints) and CNF-based reasoning limits applicability. The goal is to bridge this gap.

Method: Introduces a pseudo-Boolean encoding for feature models and a novel compilation method to Boolean d-DNNF.

Result: Empirical evaluation shows the approach outperforms CNF-based methods in speed and scalability, especially for expressive constructs.

Conclusion: The pseudo-Boolean approach accelerates reasoning for feature models with expressive constructs and remains competitive for basic ones.

Abstract: Configurable systems typically consist of reusable assets that have
dependencies between each other. To specify such dependencies, feature models
are commonly used. As feature models in practice are often complex, automated
reasoning is typically employed to analyze the dependencies. Here, the de facto
standard is translating the feature model to conjunctive normal form (CNF) to
enable employing off-the-shelf tools, such as SAT or #SAT solvers. However,
modern feature-modeling dialects often contain constructs, such as cardinality
constraints, that are ill-suited for conversion to CNF. This mismatch between
the input of reasoning engines and the available feature-modeling dialects
limits the applicability of the more expressive constructs. In this work, we
shorten this gap between expressive constructs and scalable automated
reasoning. Our contribution is twofold: First, we provide a pseudo-Boolean
encoding for feature models, which facilitates smaller representations of
commonly employed constructs compared to Boolean encoding. Second, we propose a
novel method to compile pseudo-Boolean formulas to Boolean d-DNNF. With the
compiled d-DNNFs, we can resort to a plethora of efficient analyses already
used in feature modeling. Our empirical evaluation shows that our proposal
substantially outperforms the state-of-the-art based on CNF inputs for
expressive constructs. For every considered dataset representing different
feature models and feature-modeling constructs, the feature models can be
significantly faster translated to pseudo-Boolean than to CNF. Overall,
deriving d-DNNFs from a feature model with the targeted expressive constraints
can be substantially accelerated using our pseudo-Boolean approach.
Furthermore, our approach is competitive on feature models with only basic
constructs.

</details>


### [160] [ArtRAG: Retrieval-Augmented Generation with Structured Context for Visual Art Understanding](https://arxiv.org/pdf/2505.06020)
*Shuai Wang, Ivona Najdenkoska, Hongyi Zhu, Stevan Rudinac, Monika Kackovic, Nachoem Wijnberg, Marcel Worring*

Main category: cs.AI

TL;DR: ArtRAG combines structured knowledge and retrieval-augmented generation for nuanced art explanations, outperforming trained baselines.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs lack nuanced art interpretation; ArtRAG addresses this by integrating cultural, historical, and stylistic knowledge.

Method: ArtRAG constructs an Art Context Knowledge Graph (ACKG) and uses a multi-granular retriever to guide MLLM generation.

Result: Outperforms baselines on SemArt and Artpedia datasets; human evaluations confirm coherent, culturally enriched outputs.

Conclusion: ArtRAG effectively enhances MLLMs for multi-perspective art analysis without additional training.

Abstract: Understanding visual art requires reasoning across multiple perspectives --
cultural, historical, and stylistic -- beyond mere object recognition. While
recent multimodal large language models (MLLMs) perform well on general image
captioning, they often fail to capture the nuanced interpretations that fine
art demands. We propose ArtRAG, a novel, training-free framework that combines
structured knowledge with retrieval-augmented generation (RAG) for
multi-perspective artwork explanation. ArtRAG automatically constructs an Art
Context Knowledge Graph (ACKG) from domain-specific textual sources, organizing
entities such as artists, movements, themes, and historical events into a rich,
interpretable graph. At inference time, a multi-granular structured retriever
selects semantically and topologically relevant subgraphs to guide generation.
This enables MLLMs to produce contextually grounded, culturally informed art
descriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG
outperforms several heavily trained baselines. Human evaluations further
confirm that ArtRAG generates coherent, insightful, and culturally enriched
interpretations.

</details>


### [161] [Why Are You Wrong? Counterfactual Explanations for Language Grounding with 3D Objects](https://arxiv.org/pdf/2505.06030)
*Tobias Preintner, Weixuan Yuan, Qi Huang, Adrian König, Thomas Bäck, Elena Raponi, Niki van Stein*

Main category: cs.AI

TL;DR: The paper introduces a method to generate counterfactual examples for understanding why neural models fail in object referent identification tasks, using the ShapeTalk dataset and three models.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding in why models misclassify objects despite correct descriptions, aiding practitioners and engineers.

Method: Generates counterfactual examples from misclassified samples, altering descriptions to achieve correct predictions while maintaining semantic similarity.

Result: Counterfactuals reveal model weaknesses, biases, and improve understanding of model behavior.

Conclusion: The method enhances model interpretability and aids in system interaction and model improvement.

Abstract: Combining natural language and geometric shapes is an emerging research area
with multiple applications in robotics and language-assisted design. A crucial
task in this domain is object referent identification, which involves selecting
a 3D object given a textual description of the target. Variability in language
descriptions and spatial relationships of 3D objects makes this a complex task,
increasing the need to better understand the behavior of neural network models
in this domain. However, limited research has been conducted in this area.
Specifically, when a model makes an incorrect prediction despite being provided
with a seemingly correct object description, practitioners are left wondering:
"Why is the model wrong?". In this work, we present a method answering this
question by generating counterfactual examples. Our method takes a
misclassified sample, which includes two objects and a text description, and
generates an alternative yet similar formulation that would have resulted in a
correct prediction by the model. We have evaluated our approach with data from
the ShapeTalk dataset along with three distinct models. Our counterfactual
examples maintain the structure of the original description, are semantically
similar and meaningful. They reveal weaknesses in the description, model bias
and enhance the understanding of the models behavior. Theses insights help
practitioners to better interact with systems as well as engineers to improve
models.

</details>


### [162] [Seqret: Mining Rule Sets from Event Sequences](https://arxiv.org/pdf/2505.06049)
*Aleena Siji, Joscha Cüppers, Osman Ali Mian, Jilles Vreeken*

Main category: cs.AI

TL;DR: The paper introduces Seqret, a method to discover conditional and unconditional dependencies in event sequences using rule-based patterns, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for summarizing event sequences focus only on sequential patterns, neglecting conditional dependencies, which limits their usefulness.

Method: The paper formalizes the problem using the Minimum Description Length principle and proposes Seqret to discover high-quality, non-redundant rule sets.

Result: Seqret successfully recovers ground truth in synthetic datasets and identifies useful rules in real datasets, outperforming state-of-the-art methods.

Conclusion: Seqret effectively addresses the gap in discovering conditional dependencies in event sequences, providing clear and actionable insights.

Abstract: Summarizing event sequences is a key aspect of data mining. Most existing
methods neglect conditional dependencies and focus on discovering sequential
patterns only. In this paper, we study the problem of discovering both
conditional and unconditional dependencies from event sequence data. We do so
by discovering rules of the form $X \rightarrow Y$ where $X$ and $Y$ are
sequential patterns. Rules like these are simple to understand and provide a
clear description of the relation between the antecedent and the consequent. To
discover succinct and non-redundant sets of rules we formalize the problem in
terms of the Minimum Description Length principle. As the search space is
enormous and does not exhibit helpful structure, we propose the Seqret method
to discover high-quality rule sets in practice. Through extensive empirical
evaluation we show that unlike the state of the art, Seqret ably recovers the
ground truth on synthetic datasets and finds useful rules from real datasets.

</details>


### [163] [Free and Fair Hardware: A Pathway to Copyright Infringement-Free Verilog Generation using LLMs](https://arxiv.org/pdf/2505.06096)
*Sam Bush, Matthew DeLorenzo, Phat Tieu, Jeyavijayan Rajendran*

Main category: cs.AI

TL;DR: The paper addresses copyright risks in LLM-generated Verilog code by introducing FreeSet, an open-source dataset, and FreeV, a fine-tuned model with reduced infringement rates and improved performance.


<details>
  <summary>Details</summary>
Motivation: Limitations in LLM capabilities for hardware design tasks and potential copyright violations in fine-tuned models motivated the creation of a safer, open-source solution.

Method: Proposed an evaluation benchmark for copyright risk, curated the FreeSet dataset, and fine-tuned the Llama model (FreeV) using continual pre-training.

Result: FreeV showed a 3% copyright violation rate (lowest among prior works) and improved Verilog generation performance by 10% in pass@10 rates.

Conclusion: The FreeSet dataset and FreeV model effectively mitigate copyright risks while enhancing Verilog generation capabilities.

Abstract: Limitations in Large Language Model (LLM) capabilities for hardware design
tasks, such as generating functional Verilog codes, have motivated various
fine-tuning optimizations utilizing curated hardware datasets from open-source
repositories. However, these datasets remain limited in size and contain
minimal checks on licensing for reuse, resulting in potential copyright
violations by fine-tuned LLMs. Therefore, we propose an evaluation benchmark to
estimate the risk of Verilog-trained LLMs to generate copyright-protected
codes. To minimize this risk, we present an open-source Verilog dataset,
FreeSet, containing over 220k files, along with the automated dataset curation
framework utilized to provide additional guarantees of fair-use Verilog data.
We then execute an LLM fine-tuning framework consisting of continual
pre-training, resulting in a fine-tuned Llama model for Verilog, FreeV. Our
results indicate that FreeV demonstrates the smallest risk of
copyright-infringement among prior works, with only a 3% violation rate.
Furthermore, experimental results demonstrate improvements in Verilog
generation functionality over its baseline model, improving VerilogEval pass@10
rates by over 10%.

</details>


### [164] [Neuro-Symbolic Concepts](https://arxiv.org/pdf/2505.06191)
*Jiayuan Mao, Joshua B. Tenenbaum, Jiajun Wu*

Main category: cs.AI

TL;DR: A concept-centric paradigm for agents using neuro-symbolic concepts enables efficient learning, reasoning, and task-solving across diverse domains.


<details>
  <summary>Details</summary>
Motivation: To create agents capable of continual learning and flexible reasoning by grounding concepts in sensory inputs and actuation outputs.

Method: Utilizes typed, compositional neuro-symbolic concepts represented via symbolic programs and neural networks.

Result: The framework supports data efficiency, compositional generalization, continual learning, and zero-shot transfer.

Conclusion: The concept-centric approach enhances agent adaptability and performance across varied tasks and domains.

Abstract: This article presents a concept-centric paradigm for building agents that can
learn continually and reason flexibly. The concept-centric agent utilizes a
vocabulary of neuro-symbolic concepts. These concepts, such as object,
relation, and action concepts, are grounded on sensory inputs and actuation
outputs. They are also compositional, allowing for the creation of novel
concepts through their structural combination. To facilitate learning and
reasoning, the concepts are typed and represented using a combination of
symbolic programs and neural network representations. Leveraging such
neuro-symbolic concepts, the agent can efficiently learn and recombine them to
solve various tasks across different domains, ranging from 2D images, videos,
3D scenes, and robotic manipulation tasks. This concept-centric framework
offers several advantages, including data efficiency, compositional
generalization, continual learning, and zero-shot transfer.

</details>


### [165] [Causal Abstraction: A Theoretical Foundation for Mechanistic Interpretability](https://arxiv.org/pdf/2301.04709)
*Atticus Geiger, Duligur Ibeling, Amir Zur, Maheep Chaudhary, Sonakshi Chauhan, Jing Huang, Aryaman Arora, Zhengxuan Wu, Noah Goodman, Christopher Potts, Thomas Icard*

Main category: cs.AI

TL;DR: The paper generalizes causal abstraction theory, formalizes key mechanistic interpretability concepts, and unifies various interpretability methods under causal abstraction.


<details>
  <summary>Details</summary>
Motivation: To provide a theoretical foundation for mechanistic interpretability by extending causal abstraction theory and unifying existing methods.

Method: Generalizes causal abstraction to arbitrary mechanism transformations, formalizes core concepts, and unifies interpretability methods.

Result: A flexible framework for mechanistic interpretability, integrating diverse methods like activation patching and causal mediation analysis.

Conclusion: Causal abstraction offers a unified and precise approach to mechanistic interpretability, enhancing understanding of AI models.

Abstract: Causal abstraction provides a theoretical foundation for mechanistic
interpretability, the field concerned with providing intelligible algorithms
that are faithful simplifications of the known, but opaque low-level details of
black box AI models. Our contributions are (1) generalizing the theory of
causal abstraction from mechanism replacement (i.e., hard and soft
interventions) to arbitrary mechanism transformation (i.e., functionals from
old mechanisms to new mechanisms), (2) providing a flexible, yet precise
formalization for the core concepts of polysemantic neurons, the linear
representation hypothesis, modular features, and graded faithfulness, and (3)
unifying a variety of mechanistic interpretability methods in the common
language of causal abstraction, namely, activation and path patching, causal
mediation analysis, causal scrubbing, causal tracing, circuit analysis, concept
erasure, sparse autoencoders, differential binary masking, distributed
alignment search, and steering.

</details>


### [166] [AGITB: A Signal-Level Benchmark for Evaluating Artificial General Intelligence](https://arxiv.org/pdf/2504.04430)
*Matej Šprogar*

Main category: cs.AI

TL;DR: The paper introduces AGITB, a test bed for evaluating AGI by focusing on core computational invariants, unlike existing methods.


<details>
  <summary>Details</summary>
Motivation: Current AI lacks true understanding, and existing AGI evaluations are impractical. AGITB aims to provide a foundational, gradual metric.

Method: AGITB comprises twelve tests evaluating binary signal prediction without symbolic representations or pretraining.

Result: No current AI system meets AGITB's criteria, while humans pass by design.

Conclusion: AGITB is a compelling benchmark for guiding progress toward AGI.

Abstract: Despite remarkable progress in machine learning, current AI systems continue
to fall short of true human-like intelligence. While Large Language Models
(LLMs) excel in pattern recognition and response generation, they lack genuine
understanding - an essential hallmark of Artificial General Intelligence (AGI).
Existing AGI evaluation methods fail to offer a practical, gradual, and
informative metric. This paper introduces the Artificial General Intelligence
Test Bed (AGITB), comprising twelve rigorous tests that form a
signal-processing-level foundation for the potential emergence of cognitive
capabilities. AGITB evaluates intelligence through a model's ability to predict
binary signals across time without relying on symbolic representations or
pretraining. Unlike high-level tests grounded in language or perception, AGITB
focuses on core computational invariants reflective of biological intelligence,
such as determinism, sensitivity, and generalisation. The test bed assumes no
prior bias, operates independently of semantic meaning, and ensures
unsolvability through brute force or memorization. While humans pass AGITB by
design, no current AI system has met its criteria, making AGITB a compelling
benchmark for guiding and recognizing progress toward AGI.

</details>


### [167] [L2R: Learning to Reduce Search Space for Generalizable Neural Routing Solver](https://arxiv.org/pdf/2503.03137)
*Changliang Zhou, Xi Lin, Zhenkun Wang, Qingfu Zhang*

Main category: cs.AI

TL;DR: A novel learning-based method reduces search space in neural combinatorial optimization, improving scalability for large-scale routing problems.


<details>
  <summary>Details</summary>
Motivation: Existing NCO methods struggle with generalization to large-scale problems due to computational complexity and inefficient pattern capture.

Method: Proposes a learning-based search space reduction method that dynamically selects promising nodes during the NCO process.

Result: The method generalizes well to large-scale TSP and CVRP instances (up to 1M nodes) with high solution quality.

Conclusion: The approach effectively addresses scalability and generalization challenges in NCO for routing problems.

Abstract: Constructive neural combinatorial optimization (NCO) has attracted growing
research attention due to its ability to solve complex routing problems without
relying on handcrafted rules. However, existing NCO methods face significant
challenges in generalizing to large-scale problems due to high computational
complexity and inefficient capture of structural patterns. To address this
issue, we propose a novel learning-based search space reduction method that
adaptively selects a small set of promising candidate nodes at each step of the
constructive NCO process. Unlike traditional methods that rely on fixed
heuristics, our selection model dynamically prioritizes nodes based on learned
patterns, significantly reducing the search space while maintaining solution
quality. Experimental results demonstrate that our method, trained solely on
100-node instances from uniform distribution, generalizes remarkably well to
large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP) instances with up to 1 million nodes from the uniform
distribution and over 80K nodes from other distributions.

</details>


### [168] [RadioDiff-Inverse: Diffusion Enhanced Bayesian Inverse Estimation for ISAC Radio Map Construction](https://arxiv.org/pdf/2504.14298)
*Xiucheng Wang, Zhongsheng Fang, Nan Cheng, Ruijin Sun, Zan Li, Xuemin, Shen*

Main category: cs.AI

TL;DR: The paper proposes RadioDiff-Inverse, a diffusion-enhanced Bayesian framework for constructing radio maps (RMs) using noisy sparse measurements and coarse environmental data, achieving high accuracy and robustness without task-specific training.


<details>
  <summary>Details</summary>
Motivation: Existing RM construction methods rely on precise data, which is often unavailable in dynamic or privacy-sensitive settings. The impact of noise in sparse measurements on RM accuracy is also unclear.

Method: The paper formulates RM construction as a Bayesian inverse problem and introduces RadioDiff-Inverse, which uses an unconditional generative diffusion model to learn the RM prior without precise environmental data or task-specific training.

Result: RadioDiff-Inverse achieves state-of-the-art accuracy in RM construction and environmental reconstruction, while being robust to noisy sparse sampling.

Conclusion: The proposed framework offers a training-free, efficient solution for RM construction, leveraging pre-trained models and integrating sensing and communication for environmental perception.

Abstract: Radio maps (RMs) are essential for environment-aware communication and
sensing, providing location-specific wireless channel information. Existing RM
construction methods often rely on precise environmental data and base station
(BS) locations, which are not always available in dynamic or privacy-sensitive
environments. While sparse measurement techniques reduce data collection, the
impact of noise in sparse data on RM accuracy is not well understood. This
paper addresses these challenges by formulating RM construction as a Bayesian
inverse problem under coarse environmental knowledge and noisy sparse
measurements. Although maximum a posteriori (MAP) filtering offers an optimal
solution, it requires a precise prior distribution of the RM, which is
typically unavailable. To solve this, we propose RadioDiff-Inverse, a
diffusion-enhanced Bayesian inverse estimation framework that uses an
unconditional generative diffusion model to learn the RM prior. This approach
not only reconstructs the spatial distribution of wireless channel features but
also enables environmental structure perception, such as building outlines, and
location of BS just relay on pathloss, through integrated sensing and
communication (ISAC). Remarkably, RadioDiff-Inverse is training-free,
leveraging a pre-trained model from Imagenet without task-specific fine-tuning,
which significantly reduces the training cost of using generative large model
in wireless networks. Experimental results demonstrate that RadioDiff-Inverse
achieves state-of-the-art performance in accuracy of RM construction and
environmental reconstruction, and robustness against noisy sparse sampling.

</details>


### [169] [Scaling Laws For Scalable Oversight](https://arxiv.org/pdf/2504.18530)
*Joshua Engels, David D. Baek, Subhash Kantamneni, Max Tegmark*

Main category: cs.AI

TL;DR: The paper proposes a framework to quantify scalable oversight success, modeling it as a game with Elo scores, and validates it with oversight games like Mafia and Debate. It also studies Nested Scalable Oversight (NSO) and identifies optimal oversight levels.


<details>
  <summary>Details</summary>
Motivation: To address the unclear scalability of scalable oversight, a strategy for controlling superintelligent AI systems.

Method: A framework modeling oversight as a game with Elo scores, validated using modified Nim and applied to four oversight games. Theoretical study of NSO with optimal oversight levels.

Result: Scaling laws for oversight games; NSO success rates: 13.5% (Mafia), 51.7% (Debate), 10.0% (Backdoor Code), 9.4% (Wargames).

Conclusion: The framework provides insights into scalable oversight, with NSO success rates varying by game and declining for stronger systems.

Abstract: Scalable oversight, the process by which weaker AI systems supervise stronger
ones, has been proposed as a key strategy to control future superintelligent
systems. However, it is still unclear how scalable oversight itself scales. To
address this gap, we propose a framework that quantifies the probability of
successful oversight as a function of the capabilities of the overseer and the
system being overseen. Specifically, our framework models oversight as a game
between capability-mismatched players; the players have oversight-specific Elo
scores that are a piecewise-linear function of their general intelligence, with
two plateaus corresponding to task incompetence and task saturation. We
validate our framework with a modified version of the game Nim and then apply
it to four oversight games: Mafia, Debate, Backdoor Code and Wargames. For each
game, we find scaling laws that approximate how domain performance depends on
general AI system capability. We then build on our findings in a theoretical
study of Nested Scalable Oversight (NSO), a process in which trusted models
oversee untrusted stronger models, which then become the trusted models in the
next step. We identify conditions under which NSO succeeds and derive
numerically (and in some cases analytically) the optimal number of oversight
levels to maximize the probability of oversight success. We also apply our
theory to our four oversight games, where we find that NSO success rates at a
general Elo gap of 400 are 13.5% for Mafia, 51.7% for Debate, 10.0% for
Backdoor Code, and 9.4% for Wargames; these rates decline further when
overseeing stronger systems.

</details>


### [170] [Towards AI-Driven Policing: Interdisciplinary Knowledge Discovery from Police Body-Worn Camera Footage](https://arxiv.org/pdf/2504.20007)
*Anita Srbinovska, Angela Srbinovska, Vivek Senthil, Adrian Martin, John McCluskey, Jonathan Bateman, Ernest Fokoué*

Main category: cs.AI

TL;DR: A novel AI/ML framework analyzes police BWC footage to classify interactions, using multimodal data (video, audio, NLP) for behavioral insights.


<details>
  <summary>Details</summary>
Motivation: To identify key behavioral dynamics (e.g., respect, escalation) in police-civilian interactions for law enforcement and research.

Method: Multimodal data analysis integrating video, audio, and NLP techniques to process BWC footage.

Result: Key behavioral patterns detected, providing actionable insights for law enforcement.

Conclusion: The framework advances knowledge discovery from BWC data and offers practical tools for policing.

Abstract: This paper proposes a novel interdisciplinary framework for analyzing police
body-worn camera (BWC) footage from the Rochester Police Department (RPD) using
advanced artificial intelligence (AI) and statistical machine learning (ML)
techniques. Our goal is to detect, classify, and analyze patterns of
interaction between police officers and civilians to identify key behavioral
dynamics, such as respect, disrespect, escalation, and de-escalation. We apply
multimodal data analysis by integrating video, audio, and natural language
processing (NLP) techniques to extract meaningful insights from BWC footage. We
present our methodology, computational techniques, and findings, outlining a
practical approach for law enforcement while advancing the frontiers of
knowledge discovery from police BWC data.

</details>


### [171] [AI-Driven Scholarly Peer Review via Persistent Workflow Prompting, Meta-Prompting, and Meta-Reasoning](https://arxiv.org/pdf/2505.03332)
*Evgeny Markhasin*

Main category: cs.AI

TL;DR: The paper introduces Persistent Workflow Prompting (PWP), a method to enhance LLMs' ability to critically review scientific manuscripts, demonstrated in experimental chemistry.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of LLMs in peer review due to data limitations and expert reasoning complexity.

Method: PWP uses hierarchical, modular prompts (structured via Markdown) to codify expert workflows, applied iteratively with meta-prompting.

Result: PWP-guided LLMs identified methodological flaws, mitigated bias, and performed complex analyses like integrating multimodal data.

Conclusion: PWP shows promise for sophisticated scientific tasks using standard LLMs, with transparency and replication supported by provided resources.

Abstract: Critical peer review of scientific manuscripts presents a significant
challenge for Large Language Models (LLMs), partly due to data limitations and
the complexity of expert reasoning. This report introduces Persistent Workflow
Prompting (PWP), a potentially broadly applicable prompt engineering
methodology designed to bridge this gap using standard LLM chat interfaces
(zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical
analysis of experimental chemistry manuscripts, featuring a hierarchical,
modular architecture (structured via Markdown) that defines detailed analysis
workflows. We develop this PWP prompt through iterative application of
meta-prompting techniques and meta-reasoning aimed at systematically codifying
expert review workflows, including tacit knowledge. Submitted once at the start
of a session, this PWP prompt equips the LLM with persistent workflows
triggered by subsequent queries, guiding modern reasoning LLMs through
systematic, multimodal evaluations. Demonstrations show the PWP-guided LLM
identifying major methodological flaws in a test case while mitigating LLM
input bias and performing complex tasks, including distinguishing claims from
evidence, integrating text/photo/figure analysis to infer parameters, executing
quantitative feasibility checks, comparing estimates against claims, and
assessing a priori plausibility. To ensure transparency and facilitate
replication, we provide full prompts, detailed demonstration analyses, and logs
of interactive chats as supplementary resources. Beyond the specific
application, this work offers insights into the meta-development process
itself, highlighting the potential of PWP, informed by detailed workflow
formalization, to enable sophisticated analysis using readily available LLMs
for complex scientific tasks.

</details>


### [172] [EcoAgent: An Efficient Edge-Cloud Collaborative Multi-Agent Framework for Mobile Automation](https://arxiv.org/pdf/2505.05440)
*Biao Yi, Xavier Hu, Yurun Chen, Shengyu Zhang, Hongxia Yang, Fan Wu, Fei Wu*

Main category: cs.AI

TL;DR: EcoAgent is an edge-cloud collaborative framework for mobile automation, balancing cloud-based reasoning with edge efficiency to reduce latency and cost while maintaining task success.


<details>
  <summary>Details</summary>
Motivation: Cloud-based agents with (M)LLMs are powerful but costly and slow, while edge-deployed (M)SLMs lose generality. EcoAgent bridges this gap.

Method: EcoAgent uses a cloud-based Planning Agent and two edge-based agents (Execution and Observation) with modules for compression, memory, and reflection.

Result: EcoAgent matches cloud-based success rates on AndroidWorld while cutting MLLM token use, proving efficient mobile automation.

Conclusion: EcoAgent offers a practical, efficient solution for mobile automation by combining edge and cloud strengths.

Abstract: Cloud-based mobile agents powered by (multimodal) large language models
((M)LLMs) offer strong reasoning abilities but suffer from high latency and
cost. While fine-tuned (M)SLMs enable edge deployment, they often lose general
capabilities and struggle with complex tasks. To address this, we propose
\textbf{EcoAgent}, an \textbf{E}dge-\textbf{C}loud c\textbf{O}llaborative
multi-agent framework for mobile automation. EcoAgent features a closed-loop
collaboration among a cloud-based Planning Agent and two edge-based agents: the
Execution Agent for action execution and the Observation Agent for verifying
outcomes. The Observation Agent uses a Pre-Understanding Module to compress
screen images into concise text, reducing token usage and communication
overhead. In case of failure, the Planning Agent retrieves screen history
through a Memory Module and replans via a Reflection Module. Experiments on
AndroidWorld show that EcoAgent achieves task success rates comparable to
cloud-based mobile agents while significantly reducing MLLM token consumption,
enabling efficient and practical mobile automation.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [173] [Toward a Sparse and Interpretable Audio Codec](https://arxiv.org/pdf/2505.05654)
*John Vinyard*

Main category: cs.SD

TL;DR: A proof-of-concept audio encoder is introduced, representing audio as sparse events with times-of-occurrence, aiming for an interpretable and sparse representation.


<details>
  <summary>Details</summary>
Motivation: Existing audio codecs lack intuitive, directly-interpretable representations despite good reproduction quality.

Method: Uses physics-based assumptions to model attack and resonance, creating a sparse event-based representation.

Result: Produces a sparse, parsimonious, and easy-to-interpret audio representation.

Conclusion: The proposed encoder offers a novel, interpretable alternative to traditional block-coding methods.

Abstract: Most widely-used modern audio codecs, such as Ogg Vorbis and MP3, as well as
more recent "neural" codecs like Meta's Encodec or the Descript Audio Codec are
based on block-coding; audio is divided into overlapping, fixed-size "frames"
which are then compressed. While they often yield excellent reproductions and
can be used for downstream tasks such as text-to-audio, they do not produce an
intuitive, directly-interpretable representation. In this work, we introduce a
proof-of-concept audio encoder that represents audio as a sparse set of events
and their times-of-occurrence. Rudimentary physics-based assumptions are used
to model attack and the physical resonance of both the instrument being played
and the room in which a performance occurs, hopefully encouraging a sparse,
parsimonious, and easy-to-interpret representation.

</details>


### [174] [Fast Differentiable Modal Simulation of Non-linear Strings, Membranes, and Plates](https://arxiv.org/pdf/2505.05940)
*Rodrigo Diaz, Mark Sandler*

Main category: cs.SD

TL;DR: A fast, differentiable, GPU-accelerated modal framework for simulating vibrations in strings, membranes, and plates, enabling efficient inverse modelling and real-time applications.


<details>
  <summary>Details</summary>
Motivation: Traditional modal methods for non-linear models are computationally intensive and lack differentiability, limiting inverse modelling and real-time use.

Method: Developed a GPU-accelerated modal framework using the JAX library for efficient, differentiable simulations.

Result: Outperforms CPU/GPU implementations, especially for many modes, and recovers physical parameters from data.

Conclusion: The framework offers interpretability and compact parameterisation, with open-source code for future research.

Abstract: Modal methods for simulating vibrations of strings, membranes, and plates are
widely used in acoustics and physically informed audio synthesis. However,
traditional implementations, particularly for non-linear models like the von
K\'arm\'an plate, are computationally demanding and lack differentiability,
limiting inverse modelling and real-time applications. We introduce a fast,
differentiable, GPU-accelerated modal framework built with the JAX library,
providing efficient simulations and enabling gradient-based inverse modelling.
Benchmarks show that our approach significantly outperforms CPU and GPU-based
implementations, particularly for simulations with many modes. Inverse
modelling experiments demonstrate that our approach can recover physical
parameters, including tension, stiffness, and geometry, from both synthetic and
experimental data. Although fitting physical parameters is more sensitive to
initialisation compared to other methods, it provides greater interpretability
and more compact parameterisation. The code is released as open source to
support future research and applications in differentiable physical modelling
and sound synthesis.

</details>


### [175] [Learning Music Audio Representations With Limited Data](https://arxiv.org/pdf/2505.06042)
*Christos Plachouras, Emmanouil Benetos, Johan Pauwels*

Main category: cs.SD

TL;DR: The paper explores how music audio representation models perform with limited training data, showing they can match large-dataset models under certain conditions, though handcrafted features sometimes outperform.


<details>
  <summary>Details</summary>
Motivation: To address challenges in scenarios with scarce music audio data or annotations, such as underrepresented traditions or personalized music creation.

Method: Investigates various music audio representation models under limited-data regimes, training on datasets from 5 to 8,000 minutes and evaluating on music information retrieval tasks and noise robustness.

Result: Limited-data models can perform comparably to large-dataset models under certain conditions, but handcrafted features sometimes outperform learned representations.

Conclusion: Understanding limited-data behavior is crucial for developing techniques to tackle data scarcity in music representation learning.

Abstract: Large deep-learning models for music, including those focused on learning
general-purpose music audio representations, are often assumed to require
substantial training data to achieve high performance. If true, this would pose
challenges in scenarios where audio data or annotations are scarce, such as for
underrepresented music traditions, non-popular genres, and personalized music
creation and listening. Understanding how these models behave in limited-data
scenarios could be crucial for developing techniques to tackle them.
  In this work, we investigate the behavior of several music audio
representation models under limited-data learning regimes. We consider music
models with various architectures, training paradigms, and input durations, and
train them on data collections ranging from 5 to 8,000 minutes long. We
evaluate the learned representations on various music information retrieval
tasks and analyze their robustness to noise. We show that, under certain
conditions, representations from limited-data and even random models perform
comparably to ones from large-dataset models, though handcrafted features
outperform all learned representations in some tasks.

</details>


### [176] [A vector quantized masked autoencoder for audiovisual speech emotion recognition](https://arxiv.org/pdf/2305.03568)
*Samir Sadok, Simon Leglaive, Renaud Séguier*

Main category: cs.SD

TL;DR: The paper introduces VQ-MAE-AV, a self-supervised multimodal model for emotion recognition using masked autoencoders to learn from unlabeled audiovisual speech data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of leveraging unlabeled data for emotion recognition by developing a self-supervised approach.

Method: Uses vector quantized variational autoencoders to compress raw data into tokens, trains a multimodal masked autoencoder with attention mechanisms, and fine-tunes for emotion recognition.

Result: Achieves state-of-the-art performance across multiple datasets in controlled and in-the-wild conditions.

Conclusion: VQ-MAE-AV effectively leverages unlabeled data for superior emotion recognition, demonstrating the potential of self-supervised learning in this domain.

Abstract: An important challenge in emotion recognition is to develop methods that can
leverage unlabeled training data. In this paper, we propose the VQ-MAE-AV
model, a self-supervised multimodal model that leverages masked autoencoders to
learn representations of audiovisual speech without labels. The model includes
vector quantized variational autoencoders that compress raw audio and visual
speech data into discrete tokens. The audiovisual speech tokens are used to
train a multimodal masked autoencoder that consists of an encoder-decoder
architecture with attention mechanisms. The model is designed to extract both
local (i.e., at the frame level) and global (i.e., at the sequence level)
representations of audiovisual speech. During self-supervised pre-training, the
VQ-MAE-AV model is trained on a large-scale unlabeled dataset of audiovisual
speech, for the task of reconstructing randomly masked audiovisual speech
tokens and with a contrastive learning strategy. During this pre-training, the
encoder learns to extract a representation of audiovisual speech that can be
subsequently leveraged for emotion recognition. During the supervised
fine-tuning stage, a small classification model is trained on top of the
VQ-MAE-AV encoder for an emotion recognition task. The proposed approach
achieves state-of-the-art emotion recognition results across several datasets
in both controlled and in-the-wild conditions.

</details>


### [177] [Miipher-2: A Universal Speech Restoration Model for Million-Hour Scale Data Restoration](https://arxiv.org/pdf/2505.04457)
*Shigeki Karita, Yuma Koizumi, Heiga Zen, Haruko Ishikawa, Robin Scheibler, Michiel Bacchiani*

Main category: cs.SD

TL;DR: Miipher-2 is a speech restoration model for cleaning large-scale training data, addressing challenges like generalization, efficiency, and no explicit conditioning. It uses a frozen USM and parallel adapters for feature prediction, achieving high performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To clean training data for large-scale generative models, addressing challenges like unseen languages and computational efficiency.

Method: Uses a frozen Universal Speech Model (USM) for feature extraction, parallel adapters for clean feature prediction, and WaveFit for waveform synthesis, trained on multi-lingual data.

Result: Outperforms conventional SR models in word-error-rate, speaker similarity, and sound quality, with high efficiency (real-time factor 0.0078).

Conclusion: Miipher-2 is effective for large-scale data cleaning, offering superior performance and scalability for generative model training.

Abstract: Training data cleaning is a new application for generative model-based speech
restoration (SR). This paper introduces Miipher-2, an SR model designed for
million-hour scale data, for training data cleaning for large-scale generative
models like large language models. Key challenges addressed include
generalization to unseen languages, operation without explicit conditioning
(e.g., text, speaker ID), and computational efficiency. Miipher-2 utilizes a
frozen, pre-trained Universal Speech Model (USM), supporting over 300
languages, as a robust, conditioning-free feature extractor. To optimize
efficiency and minimize memory, Miipher-2 incorporates parallel adapters for
predicting clean USM features from noisy inputs and employs the WaveFit neural
vocoder for waveform synthesis. These components were trained on 3,000 hours of
multi-lingual, studio-quality recordings with augmented degradations, while USM
parameters remained fixed. Experimental results demonstrate Miipher-2's
superior or comparable performance to conventional SR models in
word-error-rate, speaker similarity, and both objective and subjective sound
quality scores across all tested languages. Miipher-2 operates efficiently on
consumer-grade accelerators, achieving a real-time factor of 0.0078, enabling
the processing of a million-hour speech dataset in approximately three days
using only 100 such accelerators.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [178] [Continuous Thought Machines](https://arxiv.org/pdf/2505.05522)
*Luke Darlow, Ciaran Regan, Sebastian Risi, Jeffrey Seely, Llion Jones*

Main category: cs.LG

TL;DR: The paper introduces the Continuous Thought Machine (CTM), a model incorporating neuron-level temporal processing and synchronization to enhance biological plausibility in deep learning.


<details>
  <summary>Details</summary>
Motivation: To challenge the oversimplification of neural activity in deep learning by reintroducing temporal dynamics and synchronization, aiming for more biologically realistic AI.

Method: The CTM uses neuron-level temporal processing with unique weight parameters for signal history and neural synchronization as a latent representation.

Result: CTM demonstrates strong performance in tasks like ImageNet-1K classification, maze-solving, and question-answering, with adaptive compute capabilities.

Conclusion: CTM advances biologically plausible AI, balancing realism and computational efficiency, though not aiming for state-of-the-art results.

Abstract: Biological brains demonstrate complex neural activity, where the timing and
interplay between neurons is critical to how brains process information. Most
deep learning architectures simplify neural activity by abstracting away
temporal dynamics. In this paper we challenge that paradigm. By incorporating
neuron-level processing and synchronization, we can effectively reintroduce
neural timing as a foundational element. We present the Continuous Thought
Machine (CTM), a model designed to leverage neural dynamics as its core
representation. The CTM has two core innovations: (1) neuron-level temporal
processing, where each neuron uses unique weight parameters to process a
history of incoming signals; and (2) neural synchronization employed as a
latent representation. The CTM aims to strike a balance between oversimplified
neuron abstractions that improve computational efficiency, and biological
realism. It operates at a level of abstraction that effectively captures
essential temporal dynamics while remaining computationally tractable for deep
learning. We demonstrate the CTM's strong performance and versatility across a
range of challenging tasks, including ImageNet-1K classification, solving 2D
mazes, sorting, parity computation, question-answering, and RL tasks. Beyond
displaying rich internal representations and offering a natural avenue for
interpretation owing to its internal process, the CTM is able to perform tasks
that require complex sequential reasoning. The CTM can also leverage adaptive
compute, where it can stop earlier for simpler tasks, or keep computing when
faced with more challenging instances. The goal of this work is to share the
CTM and its associated innovations, rather than pushing for new
state-of-the-art results. To that end, we believe the CTM represents a
significant step toward developing more biologically plausible and powerful
artificial intelligence systems.

</details>


### [179] [A critical assessment of reinforcement learning methods for microswimmer navigation in complex flows](https://arxiv.org/pdf/2505.05525)
*Selim Mecanna, Aurore Loisy, Christophe Eloy*

Main category: cs.LG

TL;DR: The paper evaluates reinforcement learning (RL) methods for autonomous navigation in fluid flows, finding that advanced algorithms like PPO outperform simpler ones like Q-Learning and A2C, matching quasi-optimal performance in turbulent flows.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of RL algorithms for navigation in partially observable fluid flows, a problem relevant to planktonic organisms and autonomous ocean robots.

Method: Introduces a directional navigation problem with a known quasi-optimal policy, then tests Q-Learning, A2C, and PPO in various flow types (Taylor-Green vortices, ABC flow, 2D turbulence). Custom PPO implementation includes techniques like vectorized environments and hyperparameter optimization.

Result: PPO outperforms simpler RL methods, matching theoretical quasi-optimal performance in turbulent flows, while Q-Learning and A2C perform poorly.

Conclusion: Algorithm selection, implementation details, and fine-tuning are critical for effective autonomous navigation in complex flows.

Abstract: Navigating in a fluid flow while being carried by it, using only information
accessible from on-board sensors, is a problem commonly faced by small
planktonic organisms. It is also directly relevant to autonomous robots
deployed in the oceans. In the last ten years, the fluid mechanics community
has widely adopted reinforcement learning, often in the form of its simplest
implementations, to address this challenge. But it is unclear how good are the
strategies learned by these algorithms. In this paper, we perform a
quantitative assessment of reinforcement learning methods applied to navigation
in partially observable flows. We first introduce a well-posed problem of
directional navigation for which a quasi-optimal policy is known analytically.
We then report on the poor performance and robustness of commonly used
algorithms (Q-Learning, Advantage Actor Critic) in flows regularly encountered
in the literature: Taylor-Green vortices, Arnold-Beltrami-Childress flow, and
two-dimensional turbulence. We show that they are vastly surpassed by PPO
(Proximal Policy Optimization), a more advanced algorithm that has established
dominance across a wide range of benchmarks in the reinforcement learning
community. In particular, our custom implementation of PPO matches the
theoretical quasi-optimal performance in turbulent flow and does so in a robust
manner. Reaching this result required the use of several additional techniques,
such as vectorized environments and generalized advantage estimation, as well
as hyperparameter optimization. This study demonstrates the importance of
algorithm selection, implementation details, and fine-tuning for discovering
truly smart autonomous navigation strategies in complex flows.

</details>


### [180] [ADMM-Based Training for Spiking Neural Networks](https://arxiv.org/pdf/2505.05527)
*Giovanni Perin, Cesare Bidini, Riccardo Mazzieri, Michele Rossi*

Main category: cs.LG

TL;DR: Proposes an ADMM-based training method for SNNs to address non-differentiability and scalability issues in existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing SNN training methods like backpropagation with surrogate gradients suffer from scalability and numerical imprecision.

Method: Uses the alternating direction method of multipliers (ADMM) to solve non-differentiability, with closed-form updates.

Result: Empirical results show convergence, potential, and new research directions.

Conclusion: ADMM-based training is promising for SNNs, offering a scalable and precise alternative.

Abstract: In recent years, spiking neural networks (SNNs) have gained momentum due to
their high potential in time-series processing combined with minimal energy
consumption. However, they still lack a dedicated and efficient training
algorithm. The popular backpropagation with surrogate gradients, adapted from
stochastic gradient descent (SGD)-derived algorithms, has several drawbacks
when used as an optimizer for SNNs. Specifically, it suffers from low
scalability and numerical imprecision. In this paper, we propose a novel SNN
training method based on the alternating direction method of multipliers
(ADMM). Our ADMM-based training aims to solve the problem of the SNN step
function's non-differentiability. We formulate the problem, derive closed-form
updates, and empirically show the optimizer's convergence properties, great
potential, and possible new research directions to improve the method in a
simulated proof-of-concept.

</details>


### [181] [Offline Multi-agent Reinforcement Learning via Score Decomposition](https://arxiv.org/pdf/2505.05968)
*Dan Qiao, Wenhao Li, Shanchao Yang, Hongyuan Zha, Baoxiang Wang*

Main category: cs.LG

TL;DR: A novel two-stage framework using diffusion models and score function decomposition addresses offline MARL challenges, outperforming existing methods by 26.3%.


<details>
  <summary>Details</summary>
Motivation: Offline MARL struggles with distributional shifts and OOD actions due to multimodal joint policies.

Method: Uses a diffusion-based generative model to capture behavior policies and a score function decomposition for decentralized execution.

Result: Achieves state-of-the-art performance, improving normalized returns by 26.3%.

Conclusion: Provides insights into offline coordination and equilibrium selection in multi-agent systems.

Abstract: Offline multi-agent reinforcement learning (MARL) faces critical challenges
due to distributional shifts, further exacerbated by the high dimensionality of
joint action spaces and the diversity in coordination strategies and quality
among agents. Conventional approaches, including independent learning
frameworks and value decomposition methods based on pessimistic principles,
remain susceptible to out-of-distribution (OOD) joint actions and often yield
suboptimal performance. Through systematic analysis of prevalent offline MARL
benchmarks, we identify that this limitation primarily stems from the
inherently multimodal nature of joint collaborative policies induced by offline
data collection. To address these challenges, we propose a novel two-stage
framework: First, we employ a diffusion-based generative model to explicitly
capture the complex behavior policy, enabling accurate modeling of diverse
multi-agent coordination patterns. Second, we introduce a sequential score
function decomposition mechanism to regularize individual policies and enable
decentralized execution. Extensive experiments on continuous control tasks
demonstrate state-of-the-art performance across multiple standard offline MARL
benchmarks, outperforming existing methods by 26.3\% in normalized returns. Our
approach provides new insights into offline coordination and equilibrium
selection in cooperative multi-agent systems.

</details>


### [182] [Low-bit Model Quantization for Deep Neural Networks: A Survey](https://arxiv.org/pdf/2505.05530)
*Kai Liu, Qian Zheng, Kaiwen Tao, Zhiteng Li, Haotong Qin, Wenbo Li, Yong Guo, Xianglong Liu, Linghe Kong, Guihai Chen, Yulun Zhang, Xiaokang Yang*

Main category: cs.LG

TL;DR: A survey of recent five-year progress in low-bit quantization for DNNs, classifying methods into 8 categories and highlighting research opportunities.


<details>
  <summary>Details</summary>
Motivation: DNNs face high computation costs and large model sizes, making quantization essential for deployment despite performance degradation from precision loss.

Method: Survey and classification of state-of-the-art quantization methods into 8 main and 24 sub-categories based on core techniques.

Result: Comprehensive comparison of quantization methods and identification of potential research directions.

Conclusion: Quantization is critical for DNN deployment, and the survey provides a roadmap for future research, with a curated resource list.

Abstract: With unprecedented rapid development, deep neural networks (DNNs) have deeply
influenced almost all fields. However, their heavy computation costs and model
sizes are usually unacceptable in real-world deployment. Model quantization, an
effective weight-lighting technique, has become an indispensable procedure in
the whole deployment pipeline. The essence of quantization acceleration is the
conversion from continuous floating-point numbers to discrete integer ones,
which significantly speeds up the memory I/O and calculation, i.e., addition
and multiplication. However, performance degradation also comes with the
conversion because of the loss of precision. Therefore, it has become
increasingly popular and critical to investigate how to perform the conversion
and how to compensate for the information loss. This article surveys the recent
five-year progress towards low-bit quantization on DNNs. We discuss and compare
the state-of-the-art quantization methods and classify them into 8 main
categories and 24 sub-categories according to their core techniques.
Furthermore, we shed light on the potential research opportunities in the field
of model quantization. A curated list of model quantization is provided at
https://github.com/Kai-Liu001/Awesome-Model-Quantization.

</details>


### [183] [Rethinking Graph Contrastive Learning through Relative Similarity Preservation](https://arxiv.org/pdf/2505.05533)
*Zhiyuan Ning, Pengfei Wang, Ziyue Qiao, Pengyang Wang, Yuanchun Zhou*

Main category: cs.LG

TL;DR: The paper introduces RELGCL, a graph contrastive learning framework that leverages natural relative similarity patterns in graphs, outperforming existing methods by focusing on collective similarity objectives.


<details>
  <summary>Details</summary>
Motivation: Traditional graph contrastive learning (GCL) struggles with semantic validity and unreliable similarity verification due to the discrete, non-Euclidean nature of graphs. The study aims to address these challenges by uncovering and utilizing inherent relative similarity patterns.

Method: The authors analyze 11 real-world graphs, identifying a universal pattern of label consistency decay with structural distance. They propose RELGCL, a framework with pairwise and listwise implementations, designed to preserve these patterns through collective similarity objectives.

Result: Extensive experiments show RELGCL consistently outperforms 20 existing methods across homophily and heterophily graphs, validating its effectiveness.

Conclusion: The study demonstrates that leveraging natural relative similarity patterns in graphs is more effective than relying on artificial absolute similarity, offering a robust solution for GCL challenges.

Abstract: Graph contrastive learning (GCL) has achieved remarkable success by following
the computer vision paradigm of preserving absolute similarity between
augmented views. However, this approach faces fundamental challenges in graphs
due to their discrete, non-Euclidean nature -- view generation often breaks
semantic validity and similarity verification becomes unreliable. Through
analyzing 11 real-world graphs, we discover a universal pattern transcending
the homophily-heterophily dichotomy: label consistency systematically
diminishes as structural distance increases, manifesting as smooth decay in
homophily graphs and oscillatory decay in heterophily graphs. We establish
theoretical guarantees for this pattern through random walk theory, proving
label distribution convergence and characterizing the mechanisms behind
different decay behaviors. This discovery reveals that graphs naturally encode
relative similarity patterns, where structurally closer nodes exhibit
collectively stronger semantic relationships. Leveraging this insight, we
propose RELGCL, a novel GCL framework with complementary pairwise and listwise
implementations that preserve these inherent patterns through collective
similarity objectives. Extensive experiments demonstrate that our method
consistently outperforms 20 existing approaches across both homophily and
heterophily graphs, validating the effectiveness of leveraging natural relative
similarity over artificial absolute similarity.

</details>


### [184] [Persistence of Backdoor-based Watermarks for Neural Networks: A Comprehensive Evaluation](https://arxiv.org/pdf/2501.02704)
*Anh Tu Ngo, Chuan Song Heng, Nandish Chattopadhyay, Anupam Chattopadhyay*

Main category: cs.LG

TL;DR: The paper evaluates the robustness of backdoor-based watermarks in DNNs during fine-tuning and proposes a data-driven method to restore watermarks without exposing trigger sets.


<details>
  <summary>Details</summary>
Motivation: DNNs are valuable IP, and watermarking schemes are used to protect them. However, existing backdoor watermarks lack robustness against fine-tuning, prompting this study.

Method: The authors evaluate backdoor watermark persistence during fine-tuning and develop a novel data-driven approach to restore watermarks without trigger set exposure.

Result: Empirical results show watermarks can be restored if model parameters don't shift drastically, with trigger accuracy reaching up to 100%.

Conclusion: The study demonstrates effective watermark restoration and explores the process via loss landscape visualization, suggesting training data introduction during fine-tuning to prevent watermark loss.

Abstract: Deep Neural Networks (DNNs) have gained considerable traction in recent years
due to the unparalleled results they gathered. However, the cost behind
training such sophisticated models is resource intensive, resulting in many to
consider DNNs to be intellectual property (IP) to model owners. In this era of
cloud computing, high-performance DNNs are often deployed all over the internet
so that people can access them publicly. As such, DNN watermarking schemes,
especially backdoor-based watermarks, have been actively developed in recent
years to preserve proprietary rights. Nonetheless, there lies much uncertainty
on the robustness of existing backdoor watermark schemes, towards both
adversarial attacks and unintended means such as fine-tuning neural network
models. One reason for this is that no complete guarantee of robustness can be
assured in the context of backdoor-based watermark. In this paper, we
extensively evaluate the persistence of recent backdoor-based watermarks within
neural networks in the scenario of fine-tuning, we propose/develop a novel
data-driven idea to restore watermark after fine-tuning without exposing the
trigger set. Our empirical results show that by solely introducing training
data after fine-tuning, the watermark can be restored if model parameters do
not shift dramatically during fine-tuning. Depending on the types of trigger
samples used, trigger accuracy can be reinstated to up to 100%. Our study
further explores how the restoration process works using loss landscape
visualization, as well as the idea of introducing training data in fine-tuning
stage to alleviate watermark vanishing.

</details>


### [185] [Cardioformer: Advancing AI in ECG Analysis with Multi-Granularity Patching and ResNet](https://arxiv.org/pdf/2505.05538)
*Md Kamrujjaman Mobin, Md Saiful Islam, Sadik Al Barid, Md Masum*

Main category: cs.LG

TL;DR: Cardioformer, a hybrid ECG classification model, combines cross-channel patching, hierarchical residual learning, and self-attention to outperform state-of-the-art methods on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Existing ECG classification methods fail to simultaneously capture local morphological details and long-range temporal dependencies, limiting diagnostic accuracy.

Method: Cardioformer uses multi-scale token embeddings and a two-stage self-attention mechanism to integrate local and global ECG features.

Result: Achieves AUROC scores of 96.34 (MIMIC-IV), 89.99 (PTB-XL), and 95.59 (PTB), outperforming PatchTST, Reformer, Transformer, and Medformer.

Conclusion: Cardioformer advances automated ECG analysis, offering more accurate and robust cardiovascular disease diagnosis.

Abstract: Electrocardiogram (ECG) classification is crucial for automated cardiac
disease diagnosis, yet existing methods often struggle to capture local
morphological details and long-range temporal dependencies simultaneously. To
address these challenges, we propose Cardioformer, a novel multi-granularity
hybrid model that integrates cross-channel patching, hierarchical residual
learning, and a two-stage self-attention mechanism. Cardioformer first encodes
multi-scale token embeddings to capture fine-grained local features and global
contextual information and then selectively fuses these representations through
intra- and inter-granularity self-attention. Extensive evaluations on three
benchmark ECG datasets under subject-independent settings demonstrate that
model consistently outperforms four state-of-the-art baselines. Our
Cardioformer model achieves the AUROC of 96.34$\pm$0.11, 89.99$\pm$0.12, and
95.59$\pm$1.66 in MIMIC-IV, PTB-XL and PTB dataset respectively outperforming
PatchTST, Reformer, Transformer, and Medformer models. It also demonstrates
strong cross-dataset generalization, achieving 49.18% AUROC on PTB and 68.41%
on PTB-XL when trained on MIMIC-IV. These findings underscore the potential of
Cardioformer to advance automated ECG analysis, paving the way for more
accurate and robust cardiovascular disease diagnosis. We release the source
code at https://github.com/KMobin555/Cardioformer.

</details>


### [186] [Griffin: Towards a Graph-Centric Relational Database Foundation Model](https://arxiv.org/pdf/2505.05568)
*Yanbo Wang, Xiyuan Wang, Quan Gan, Minjie Wang, Qibin Yang, David Wipf, Muhan Zhang*

Main category: cs.LG

TL;DR: Griffin is the first foundation model for Relational Databases (RDBs), unifying data encoding and task decoding for diverse tasks. It outperforms single-task models, excels in low-data scenarios, and shows strong transferability.


<details>
  <summary>Details</summary>
Motivation: To create a universally applicable foundation model for RDBs, addressing limitations of smaller, single-task models.

Method: Griffin uses pretraining on single-table and RDB datasets, advanced encoders for various data types, cross-attention modules, and enhanced MPNNs.

Result: Superior or comparable performance to single-task models, strong in low-data scenarios, and high transferability across datasets and tasks.

Conclusion: Griffin is a promising foundation model for RDBs, with broad applicability and strong performance.

Abstract: We introduce Griffin, the first foundation model attemptation designed
specifically for Relational Databases (RDBs). Unlike previous smaller models
focused on single RDB tasks, Griffin unifies the data encoder and task decoder
to handle diverse tasks. Additionally, we enhance the architecture by
incorporating a cross-attention module and a novel aggregator. Griffin utilizes
pretraining on both single-table and RDB datasets, employing advanced encoders
for categorical, numerical, and metadata features, along with innovative
components such as cross-attention modules and enhanced message-passing neural
networks (MPNNs) to capture the complexities of relational data. Evaluated on
large-scale, heterogeneous, and temporal graphs extracted from RDBs across
various domains (spanning over 150 million nodes), Griffin demonstrates
superior or comparable performance to individually trained models, excels in
low-data scenarios, and shows strong transferability with similarity and
diversity in pretraining across new datasets and tasks, highlighting its
potential as a universally applicable foundation model for RDBs. Code available
at https://github.com/yanxwb/Griffin.

</details>


### [187] [PyTDC: A multimodal machine learning training, evaluation, and inference platform for biomedical foundation models](https://arxiv.org/pdf/2505.05577)
*Alejandro Velez-Arce, Marinka Zitnik*

Main category: cs.LG

TL;DR: PyTDC is an open-source platform for multimodal biological AI models, unifying data and standardizing benchmarks, with a case study showing limitations of current methods.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack end-to-end infrastructure for multimodal biological data and diverse ML tasks in therapeutics.

Method: PyTDC integrates distributed data, model weights, and standardizes benchmarking. A case study evaluates single-cell drug-target nomination.

Result: State-of-the-art methods perform poorly; a context-aware geometric deep learning method outperforms but lacks generalization.

Conclusion: PyTDC enables research into multimodal, context-aware foundation models for biomedical AI.

Abstract: Existing biomedical benchmarks do not provide end-to-end infrastructure for
training, evaluation, and inference of models that integrate multimodal
biological data and a broad range of machine learning tasks in therapeutics. We
present PyTDC, an open-source machine-learning platform providing streamlined
training, evaluation, and inference software for multimodal biological AI
models. PyTDC unifies distributed, heterogeneous, continuously updated data
sources and model weights and standardizes benchmarking and inference
endpoints. This paper discusses the components of PyTDC's architecture and, to
our knowledge, the first-of-its-kind case study on the introduced single-cell
drug-target nomination ML task. We find state-of-the-art methods in graph
representation learning and domain-specific methods from graph theory perform
poorly on this task. Though we find a context-aware geometric deep learning
method that outperforms the evaluated SoTA and domain-specific baseline
methods, the model is unable to generalize to unseen cell types or incorporate
additional modalities, highlighting PyTDC's capacity to facilitate an exciting
avenue of research developing multimodal, context-aware, foundation models for
open problems in biomedical AI.

</details>


### [188] [Anticipating Gaming to Incentivize Improvement: Guiding Agents in (Fair) Strategic Classification](https://arxiv.org/pdf/2505.05594)
*Sura Alhanouti, Parinaz Naghizadeh*

Main category: cs.LG

TL;DR: The paper examines human strategic behavior in response to algorithmic decision systems, focusing on choices between genuine improvement and deceptive manipulation. It models these interactions as a Stackelberg game, analyzing fairness and optimal classifier design.


<details>
  <summary>Details</summary>
Motivation: Understanding human strategic behavior in response to machine learning systems is crucial as these systems influence critical decisions. The study aims to explore how individuals and algorithm designers interact strategically.

Method: The interactions are modeled as a Stackelberg game, where a firm deploys a fair classifier and individuals respond strategically. The model includes costs and stochastic efficacy for manipulation and improvement.

Result: The analysis identifies different classes of agent responses and characterizes optimal classifiers. It shows how anticipating strategic behavior can prevent manipulation and incentivize genuine improvement.

Conclusion: Strategic policy design by firms can promote fairness and discourage manipulation while encouraging genuine improvement, highlighting the importance of anticipating human behavior in algorithmic systems.

Abstract: As machine learning algorithms increasingly influence critical decision
making in different application areas, understanding human strategic behavior
in response to these systems becomes vital. We explore individuals' choice
between genuinely improving their qualifications (``improvement'') vs.
attempting to deceive the algorithm by manipulating their features
(``manipulation'') in response to an algorithmic decision system. We further
investigate an algorithm designer's ability to shape these strategic responses,
and its fairness implications. Specifically, we formulate these interactions as
a Stackelberg game, where a firm deploys a (fair) classifier, and individuals
strategically respond. Our model incorporates both different costs and
stochastic efficacy for manipulation and improvement. The analysis reveals
different potential classes of agent responses, and characterizes optimal
classifiers accordingly. Based on these, we highlight the impact of the firm's
anticipation of strategic behavior, identifying when and why a (fair) strategic
policy can not only prevent manipulation, but also incentivize agents to opt
for improvement.

</details>


### [189] [This part looks alike this: identifying important parts of explained instances and prototypes](https://arxiv.org/pdf/2505.05597)
*Jacek Karolczak, Jerzy Stefanowski*

Main category: cs.LG

TL;DR: The paper introduces a method to highlight the most relevant features in prototype-based explanations, improving user comprehension without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Prototype-based explanations often fail to emphasize the most relevant features, limiting their effectiveness for user understanding.

Method: The approach identifies 'alike parts'—key features in prototypes—using feature importance scores from an agnostic explanation method. It integrates these scores into prototype selection to enhance diversity.

Result: Experiments on six datasets show the method improves user comprehension while maintaining or boosting predictive accuracy.

Conclusion: The proposed method effectively directs attention to relevant features in prototypes, enhancing interpretability without compromising performance.

Abstract: Although prototype-based explanations provide a human-understandable way of
representing model predictions they often fail to direct user attention to the
most relevant features. We propose a novel approach to identify the most
informative features within prototypes, termed alike parts. Using feature
importance scores derived from an agnostic explanation method, it emphasizes
the most relevant overlapping features between an instance and its nearest
prototype. Furthermore, the feature importance score is incorporated into the
objective function of the prototype selection algorithms to promote global
prototypes diversity. Through experiments on six benchmark datasets, we
demonstrate that the proposed approach improves user comprehension while
maintaining or even increasing predictive accuracy.

</details>


### [190] [The Evolution of Embedding Table Optimization and Multi-Epoch Training in Pinterest Ads Conversion](https://arxiv.org/pdf/2505.05605)
*Andrew Qiu, Shubham Barhate, Hin Wai Lui, Runze Su, Rafael Rios Müller, Kungang Li, Ling Leng, Han Sun, Shayan Ehsani, Zhifang Liu*

Main category: cs.LG

TL;DR: The paper discusses challenges in training deep learning models for conversion prediction in online ads, focusing on embedding table optimization and multi-epoch overfitting. It introduces a Sparse Optimizer and a frequency-adaptive learning rate approach.


<details>
  <summary>Details</summary>
Motivation: Addressing slow convergence and overfitting in embedding tables for high-cardinality categorical features in multi-task conversion prediction models.

Method: Proposes a Sparse Optimizer for faster convergence and a frequency-adaptive learning rate for embedding tables to mitigate overfitting, comparing it to embedding re-initialization.

Result: Evaluated on an industrial dataset, the methods show improved performance in handling multi-epoch overfitting and label sparsity.

Conclusion: The frequency-adaptive learning rate approach effectively reduces overfitting while maintaining model performance, outperforming traditional methods like re-initialization.

Abstract: Deep learning for conversion prediction has found widespread applications in
online advertising. These models have become more complex as they are trained
to jointly predict multiple objectives such as click, add-to-cart, checkout and
other conversion types. Additionally, the capacity and performance of these
models can often be increased with the use of embedding tables that encode high
cardinality categorical features such as advertiser, user, campaign, and
product identifiers (IDs). These embedding tables can be pre-trained, but also
learned end-to-end jointly with the model to directly optimize the model
objectives. Training these large tables is challenging due to: gradient
sparsity, the high cardinality of the categorical features, the non-uniform
distribution of IDs and the very high label sparsity. These issues make
training prone to both slow convergence and overfitting after the first epoch.
Previous works addressed the multi-epoch overfitting issue by using: stronger
feature hashing to reduce cardinality, filtering of low frequency IDs,
regularization of the embedding tables, re-initialization of the embedding
tables after each epoch, etc. Some of these techniques reduce overfitting at
the expense of reduced model performance if used too aggressively. In this
paper, we share key learnings from the development of embedding table
optimization and multi-epoch training in Pinterest Ads Conversion models. We
showcase how our Sparse Optimizer speeds up convergence, and how multi-epoch
overfitting varies in severity between different objectives in a multi-task
model depending on label sparsity. We propose a new approach to deal with
multi-epoch overfitting: the use of a frequency-adaptive learning rate on the
embedding tables and compare it to embedding re-initialization. We evaluate
both methods offline using an industrial large-scale production dataset.

</details>


### [191] [On Corruption-Robustness in Performative Reinforcement Learning](https://arxiv.org/pdf/2505.05609)
*Vasilis Pollatos, Debmalya Mandal, Goran Radanovic*

Main category: cs.LG

TL;DR: The paper extends performative RL to handle corrupted data using a robust repeated retraining approach, achieving stable policy convergence despite adversarial noise.


<details>
  <summary>Details</summary>
Motivation: Prior performative RL methods assume clean data, but real-world scenarios often involve corruption. This work addresses the gap by developing robust techniques for corrupted data.

Method: Proposes a repeated retraining approach using convex-concave optimization under corrupted gradients and a novel robust mean estimator for gradients.

Result: The method achieves last-iterate convergence to an approximately stable policy, with error linear in √ϵ. Experiments validate its robustness to corruption.

Conclusion: The approach effectively handles data corruption in performative RL, demonstrating practical relevance and theoretical guarantees.

Abstract: In performative Reinforcement Learning (RL), an agent faces a
policy-dependent environment: the reward and transition functions depend on the
agent's policy. Prior work on performative RL has studied the convergence of
repeated retraining approaches to a performatively stable policy. In the finite
sample regime, these approaches repeatedly solve for a saddle point of a
convex-concave objective, which estimates the Lagrangian of a regularized
version of the reinforcement learning problem. In this paper, we aim to extend
such repeated retraining approaches, enabling them to operate under corrupted
data. More specifically, we consider Huber's $\epsilon$-contamination model,
where an $\epsilon$ fraction of data points is corrupted by arbitrary
adversarial noise. We propose a repeated retraining approach based on
convex-concave optimization under corrupted gradients and a novel
problem-specific robust mean estimator for the gradients. We prove that our
approach exhibits last-iterate convergence to an approximately stable policy,
with the approximation error linear in $\sqrt{\epsilon}$. We experimentally
demonstrate the importance of accounting for corruption in performative RL.

</details>


### [192] [SPIN-ODE: Stiff Physics-Informed Neural ODE for Chemical Reaction Rate Estimation](https://arxiv.org/pdf/2505.05625)
*Wenqing Peng, Zhi-Song Liu, Michael Boy*

Main category: cs.LG

TL;DR: Proposes SPIN-ODE, a three-stage neural ODE framework for estimating stiff chemical reaction rate constants, validated on synthetic and real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Addressing training instability and poor convergence in learning-based rate constant estimation for stiff atmospheric chemistry systems.

Method: Three-stage process: latent neural ODE for trajectory learning, CRNN for rate coefficient extraction, and fine-tuning with a neural ODE solver.

Result: Effective and robust rate coefficient estimation, validated on synthetic and real-world datasets.

Conclusion: SPIN-ODE opens new directions for integrating neural networks with detailed chemistry, especially for stiff systems.

Abstract: Estimating rate constants from complex chemical reactions is essential for
advancing detailed chemistry. However, the stiffness inherent in real-world
atmospheric chemistry systems poses severe challenges, leading to training
instability and poor convergence that hinder effective rate constant estimation
using learning-based approaches. To address this, we propose a Stiff
Physics-Informed Neural ODE framework (SPIN-ODE) for chemical reaction
modelling. Our method introduces a three-stage optimisation process: first, a
latent neural ODE learns the continuous and differentiable trajectory between
chemical concentrations and their time derivatives; second, an explicit
Chemical Reaction Neural Network (CRNN) extracts the underlying rate
coefficients based on the learned dynamics; and third, fine-tune CRNN using a
neural ODE solver to further improve rate coefficient estimation. Extensive
experiments on both synthetic and newly proposed real-world datasets validate
the effectiveness and robustness of our approach. As the first work on stiff
Neural ODEs for chemical rate coefficient discovery, our study opens promising
directions for integrating neural networks with detailed chemistry.

</details>


### [193] [EquiHGNN: Scalable Rotationally Equivariant Hypergraph Neural Networks](https://arxiv.org/pdf/2505.05650)
*Tien Dang, Truong-Son Hy*

Main category: cs.LG

TL;DR: EquiHGNN is an equivariant hypergraph neural network for molecular modeling, leveraging symmetry-aware representations to outperform traditional graph-based methods, especially for large molecules.


<details>
  <summary>Details</summary>
Motivation: Traditional graph models fail to capture high-order molecular interactions, while hypergraphs can model multi-way relationships. Integrating symmetry constraints improves robustness and physical meaning.

Method: EquiHGNN enforces equivariance under transformation groups to preserve geometric and topological properties, combining hypergraphs with symmetry-aware architectures.

Result: High-order interactions benefit large molecules more than small ones, and adding geometric features further enhances performance.

Conclusion: EquiHGNN demonstrates the importance of symmetry and high-order interactions in molecular modeling, with significant gains for large-scale systems.

Abstract: Molecular interactions often involve high-order relationships that cannot be
fully captured by traditional graph-based models limited to pairwise
connections. Hypergraphs naturally extend graphs by enabling multi-way
interactions, making them well-suited for modeling complex molecular systems.
In this work, we introduce EquiHGNN, an Equivariant HyperGraph Neural Network
framework that integrates symmetry-aware representations to improve molecular
modeling. By enforcing the equivariance under relevant transformation groups,
our approach preserves geometric and topological properties, leading to more
robust and physically meaningful representations. We examine a range of
equivariant architectures and demonstrate that integrating symmetry constraints
leads to notable performance gains on large-scale molecular datasets.
Experiments on both small and large molecules show that high-order interactions
offer limited benefits for small molecules but consistently outperform 2D
graphs on larger ones. Adding geometric features to these high-order structures
further improves the performance, emphasizing the value of spatial information
in molecular learning. Our source code is available at
https://github.com/HySonLab/EquiHGNN/

</details>


### [194] [Conditional Front-door Adjustment for Heterogeneous Treatment Assignment Effect Estimation Under Non-adherence](https://arxiv.org/pdf/2505.05677)
*Winston Chen, Trenton Chang, Jenna Wiens*

Main category: cs.LG

TL;DR: CFD outperforms SBD in variance for small treatment effects under non-adherence. LobsterNet, a multi-task neural network, further improves CFD's accuracy by jointly modeling nuisance parameters.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored variance differences between SBD and CFD in heterogeneous treatment effect estimation under non-adherence.

Method: Theoretical and empirical comparison of SBD and CFD, plus LobsterNet for joint nuisance parameter modeling in CFD.

Result: CFD yields lower-variance estimates than SBD for small treatment effects. LobsterNet reduces estimation error in semi-synthetic and real-world datasets.

Conclusion: CFD with shared nuisance parameter modeling (via LobsterNet) enhances treatment effect estimation under non-adherence.

Abstract: Estimates of heterogeneous treatment assignment effects can inform treatment
decisions. Under the presence of non-adherence (e.g., patients do not adhere to
their assigned treatment), both the standard backdoor adjustment (SBD) and the
conditional front-door adjustment (CFD) can recover unbiased estimates of the
treatment assignment effects. However, the estimation variance of these
approaches may vary widely across settings, which remains underexplored in the
literature. In this work, we demonstrate theoretically and empirically that CFD
yields lower-variance estimates than SBD when the true effect of treatment
assignment is small (i.e., assigning an intervention leads to small changes in
patients' future outcome). Additionally, since CFD requires estimating multiple
nuisance parameters, we introduce LobsterNet, a multi-task neural network that
implements CFD with joint modeling of the nuisance parameters. Empirically,
LobsterNet reduces estimation error across several semi-synthetic and
real-world datasets compared to baselines. Our findings suggest CFD with shared
nuisance parameter modeling can improve treatment assignment effect estimation
under non-adherence.

</details>


### [195] [Interactive Diabetes Risk Prediction Using Explainable Machine Learning: A Dash-Based Approach with SHAP, LIME, and Comorbidity Insights](https://arxiv.org/pdf/2505.05683)
*Udaya Allani*

Main category: cs.LG

TL;DR: A web-based tool uses machine learning to predict diabetes risk, with LightGBM and undersampling performing best. It includes SHAP/LIME explanations and a Dash UI for user interaction.


<details>
  <summary>Details</summary>
Motivation: To create an accessible tool for diabetes risk prediction using machine learning, enhancing health awareness through data-driven insights.

Method: Evaluated Logistic Regression, Random Forest, XGBoost, LightGBM, KNN, and Neural Networks on the 2015 CDC BRFSS dataset, testing original, SMOTE, and undersampling strategies.

Result: LightGBM with undersampling achieved the highest recall, making it optimal for risk detection. SHAP and LIME provided prediction explanations, and Pearson analysis highlighted comorbidities.

Conclusion: The tool successfully combines predictive accuracy with interpretability and user-friendly interaction, supporting proactive health management.

Abstract: This study presents a web-based interactive health risk prediction tool
designed to assess diabetes risk using machine learning models. Built on the
2015 CDC BRFSS dataset, the study evaluates models including Logistic
Regression, Random Forest, XGBoost, LightGBM, KNN, and Neural Networks under
original, SMOTE, and undersampling strategies. LightGBM with undersampling
achieved the best recall, making it ideal for risk detection. The tool
integrates SHAP and LIME to explain predictions and highlights comorbidity
correlations using Pearson analysis. A Dash-based UI enables user-friendly
interaction with model predictions, personalized suggestions, and feature
insights, supporting data-driven health awareness.

</details>


### [196] [Hypergraph Neural Sheaf Diffusion: A Symmetric Simplicial Set Framework for Higher-Order Learning](https://arxiv.org/pdf/2505.05702)
*Seongjin Choi, Gahee Kim, Yong-Geun Oh*

Main category: cs.LG

TL;DR: The paper resolves hypergraph adjacency and orientation challenges using symmetric simplicial sets, introducing Hypergraph Neural Sheaf Diffusion (HNSD) for competitive performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of intrinsic adjacency and orientation in hypergraphs to enable sheaf Laplacian construction.

Method: Using symmetric simplicial sets to encode hyperedge subrelations, defining adjacency via facet maps, and introducing HNSD for hypergraph learning.

Result: The normalized degree zero sheaf Laplacian aligns with graph-based theory, and HNSD performs competitively in benchmarks.

Conclusion: The framework successfully extends sheaf theory to hypergraphs, resolving orientation and adjacency issues while preserving structural details.

Abstract: The absence of intrinsic adjacency relations and orientation systems in
hypergraphs creates fundamental challenges for constructing sheaf Laplacians of
arbitrary degrees. We resolve these limitations through symmetric simplicial
sets derived directly from hypergraphs, which encode all possible oriented
subrelations within each hyperedge as ordered tuples. This construction
canonically defines adjacency via facet maps while inherently preserving
hyperedge provenance. We establish that the normalized degree zero sheaf
Laplacian on our induced symmetric simplicial set reduces exactly to the
traditional graph normalized sheaf Laplacian when restricted to graphs,
validating its mathematical consistency with prior graph-based sheaf theory.
Furthermore, the induced structure preserves all structural information from
the original hypergraph, ensuring that every multi-way relational detail is
faithfully retained. Leveraging this framework, we introduce Hypergraph Neural
Sheaf Diffusion (HNSD), the first principled extension of Neural Sheaf
Diffusion (NSD) to hypergraphs. HNSD operates via normalized degree zero sheaf
Laplacians over symmetric simplicial sets, resolving orientation ambiguity and
adjacency sparsity inherent to hypergraph learning. Experimental evaluations
demonstrate HNSD's competitive performance across established benchmarks.

</details>


### [197] [Improving Generalizability of Kolmogorov-Arnold Networks via Error-Correcting Output Codes](https://arxiv.org/pdf/2505.05798)
*Youngjoon Lee, Jinu Gong, Joonhyuk Kang*

Main category: cs.LG

TL;DR: Integrating ECOC into KAN improves multi-class classification by transforming it into binary tasks, enhancing robustness and accuracy in medical image classification.


<details>
  <summary>Details</summary>
Motivation: To enhance the generalizability and robustness of KAN in multi-class classification, especially for critical healthcare applications like blood cell classification.

Method: The paper integrates Error-Correcting Output Codes (ECOC) into the KAN framework, replacing nonlinear activations with univariate spline compositions, and uses Hamming-distance decoding for robustness.

Result: The proposed KAN with ECOC outperforms vanilla KAN, achieving higher accuracy on a blood cell classification dataset, with consistent improvements across FastKAN and FasterKAN variants.

Conclusion: ECOC integration significantly boosts KAN's performance in multi-class medical image classification, marking the first such application in the field.

Abstract: Kolmogorov-Arnold Networks (KAN) offer universal function approximation using
univariate spline compositions without nonlinear activations. In this work, we
integrate Error-Correcting Output Codes (ECOC) into the KAN framework to
transform multi-class classification into multiple binary tasks, improving
robustness via Hamming-distance decoding. Our proposed KAN with ECOC method
outperforms vanilla KAN on a challenging blood cell classification dataset,
achieving higher accuracy under diverse hyperparameter settings. Ablation
studies further confirm that ECOC consistently enhances performance across
FastKAN and FasterKAN variants. These results demonstrate that ECOC integration
significantly boosts KAN generalizability in critical healthcare AI
applications. To the best of our knowledge, this is the first integration of
ECOC with KAN for enhancing multi-class medical image classification
performance.

</details>


### [198] [Crowding Out The Noise: Algorithmic Collective Action Under Differential Privacy](https://arxiv.org/pdf/2505.05707)
*Rushabh Solanki, Meghana Bhange, Ulrich Aïvodji, Elliot Creager*

Main category: cs.LG

TL;DR: The paper explores how grassroots Algorithmic Collective Action interacts with AI firms' methods like differential privacy, finding that privacy measures hinder collective influence.


<details>
  <summary>Details</summary>
Motivation: To understand how everyday users can influence AI behavior through collective action and how this interacts with privacy-focused AI deployment.

Method: Investigates the impact of Differentially Private Stochastic Gradient Descent (DPSGD) on collective action, using theoretical bounds and experimental simulations.

Result: Differential privacy protects individual data but limits the effectiveness of collective action, with success bounds tied to collective size and privacy parameters.

Conclusion: Privacy measures like DPSGD pose challenges for grassroots efforts to steer AI, balancing individual protection with collective influence.

Abstract: The integration of AI into daily life has generated considerable attention
and excitement, while also raising concerns about automating algorithmic harms
and re-entrenching existing social inequities. While the responsible deployment
of trustworthy AI systems is a worthy goal, there are many possible ways to
realize it, from policy and regulation to improved algorithm design and
evaluation. In fact, since AI trains on social data, there is even a
possibility for everyday users, citizens, or workers to directly steer its
behavior through Algorithmic Collective Action, by deliberately modifying the
data they share with a platform to drive its learning process in their favor.
This paper considers how these grassroots efforts to influence AI interact with
methods already used by AI firms and governments to improve model
trustworthiness. In particular, we focus on the setting where the AI firm
deploys a differentially private model, motivated by the growing regulatory
focus on privacy and data protection. We investigate how the use of
Differentially Private Stochastic Gradient Descent (DPSGD) affects the
collective's ability to influence the learning process. Our findings show that
while differential privacy contributes to the protection of individual data, it
introduces challenges for effective algorithmic collective action. We
characterize lower bounds on the success of algorithmic collective action under
differential privacy as a function of the collective's size and the firm's
privacy parameters, and verify these trends experimentally by simulating
collective action during the training of deep neural network classifiers across
several datasets.

</details>


### [199] [Automated Learning of Semantic Embedding Representations for Diffusion Models](https://arxiv.org/pdf/2505.05732)
*Limai Jiang, Yunpeng Cai*

Main category: cs.LG

TL;DR: The paper introduces a multi-level denoising autoencoder framework to enhance the representation learning of Denoising Diffusion Models (DDMs), achieving superior semantic embeddings compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: DDMs excel in generative tasks but lack efficient representation learning. This work aims to bridge this gap by improving their capacity for semantic embedding generation.

Method: A multi-level denoising autoencoder framework is proposed, incorporating Diffusion Transformers and a timestep-dependent encoder for self-conditional diffusion learning.

Result: Experiments show the learned embeddings outperform state-of-the-art self-supervised methods, demonstrating high discriminative semantic quality.

Conclusion: DDMs are not only effective for generation but also promising for general-purpose deep learning due to their enhanced representation capabilities.

Abstract: Generative models capture the true distribution of data, yielding
semantically rich representations. Denoising diffusion models (DDMs) exhibit
superior generative capabilities, though efficient representation learning for
them are lacking. In this work, we employ a multi-level denoising autoencoder
framework to expand the representation capacity of DDMs, which introduces
sequentially consistent Diffusion Transformers and an additional
timestep-dependent encoder to acquire embedding representations on the
denoising Markov chain through self-conditional diffusion learning.
Intuitively, the encoder, conditioned on the entire diffusion process,
compresses high-dimensional data into directional vectors in latent under
different noise levels, facilitating the learning of image embeddings across
all timesteps. To verify the semantic adequacy of embeddings generated through
this approach, extensive experiments are conducted on various datasets,
demonstrating that optimally learned embeddings by DDMs surpass
state-of-the-art self-supervised representation learning methods in most cases,
achieving remarkable discriminative semantic representation quality. Our work
justifies that DDMs are not only suitable for generative tasks, but also
potentially advantageous for general-purpose deep learning applications.

</details>


### [200] [Accurate and Efficient Multivariate Time Series Forecasting via Offline Clustering](https://arxiv.org/pdf/2505.05738)
*Yiming Niu, Jinliang Deng, Lulu Zhang, Zimu Zhou, Yongxin Tong*

Main category: cs.LG

TL;DR: FOCUS introduces offline clustering to simplify long-range dependency modeling in MTS forecasting, reducing computational complexity while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing Transformer-based methods for MTS forecasting suffer from high computational complexity due to pairwise dependency calculations.

Method: FOCUS uses offline clustering to extract prototypes summarizing key time segments, then dynamically adapts these in the online phase for efficient forecasting.

Result: FOCUS achieves state-of-the-art accuracy with linear computational complexity, outperforming existing methods.

Conclusion: FOCUS offers an efficient and accurate solution for MTS forecasting by leveraging offline clustering and dynamic adaptation.

Abstract: Accurate and efficient multivariate time series (MTS) forecasting is
essential for applications such as traffic management and weather prediction,
which depend on capturing long-range temporal dependencies and interactions
between entities. Existing methods, particularly those based on Transformer
architectures, compute pairwise dependencies across all time steps, leading to
a computational complexity that scales quadratically with the length of the
input. To overcome these challenges, we introduce the Forecaster with Offline
Clustering Using Segments (FOCUS), a novel approach to MTS forecasting that
simplifies long-range dependency modeling through the use of prototypes
extracted via offline clustering. These prototypes encapsulate high-level
events in the real-world system underlying the data, summarizing the key
characteristics of similar time segments. In the online phase, FOCUS
dynamically adapts these patterns to the current input and captures
dependencies between the input segment and high-level events, enabling both
accurate and efficient forecasting. By identifying prototypes during the
offline clustering phase, FOCUS reduces the computational complexity of
modeling long-range dependencies in the online phase to linear scaling.
Extensive experiments across diverse benchmarks demonstrate that FOCUS achieves
state-of-the-art accuracy while significantly reducing computational costs.

</details>


### [201] [Deep-ICE: The first globally optimal algorithm for empirical risk minimization of two-layer maxout and ReLU networks](https://arxiv.org/pdf/2505.05740)
*Xi He, Yi Miao, Max A. Little*

Main category: cs.LG

TL;DR: A globally optimal algorithm for empirical risk minimization in two-layer maxout and ReLU networks is introduced, with a worst-case complexity of O(N^(DK+1)). It provides exact solutions for small datasets and uses coreset selection for scalability, outperforming state-of-the-art methods by 20-30% in misclassification reduction.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of minimizing misclassifications in two-layer neural networks (maxout and ReLU) with provable optimality, especially for small datasets, and to extend this to larger datasets efficiently.

Method: The algorithm has a worst-case time complexity of O(N^(DK+1)) and can generalize to arbitrary loss functions. For larger datasets, a coreset selection method reduces data size for feasibility.

Result: Exact solutions for small datasets; 20-30% reduction in misclassifications for training and prediction compared to gradient descent-trained neural networks and SVMs.

Conclusion: The proposed algorithm offers provably optimal solutions for small datasets and scalable performance for larger ones, significantly improving misclassification rates over existing methods.

Abstract: This paper introduces the first globally optimal algorithm for the empirical
risk minimization problem of two-layer maxout and ReLU networks, i.e.,
minimizing the number of misclassifications. The algorithm has a worst-case
time complexity of $O\left(N^{DK+1}\right)$, where $K$ denotes the number of
hidden neurons and $D$ represents the number of features. It can be can be
generalized to accommodate arbitrary computable loss functions without
affecting its computational complexity. Our experiments demonstrate that the
proposed algorithm provides provably exact solutions for small-scale datasets.
To handle larger datasets, we introduce a novel coreset selection method that
reduces the data size to a manageable scale, making it feasible for our
algorithm. This extension enables efficient processing of large-scale datasets
and achieves significantly improved performance, with a 20-30\% reduction in
misclassifications for both training and prediction, compared to
state-of-the-art approaches (neural networks trained using gradient descent and
support vector machines), when applied to the same models (two-layer networks
with fixed hidden nodes and linear models).

</details>


### [202] [Harnessing LLMs Explanations to Boost Surrogate Models in Tabular Data Classification](https://arxiv.org/pdf/2505.05744)
*Ruxue Shi, Hengrui Gu, Xu Shen, Xin Wang*

Main category: cs.LG

TL;DR: A novel in-context learning framework improves tabular prediction by using LLM-generated explanations to guide a smaller, interpretable Surrogate Language Model (SLM).


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based methods for tabular learning face issues like high resource demands, poor demonstration selection, and low interpretability, limiting real-world application.

Method: The framework involves three stages: generating LLM explanations for demonstrations, selecting demonstrations based on these explanations, and using them to train an interpretable SLM.

Result: The framework achieves an average accuracy improvement of 5.31% across diverse tabular datasets.

Conclusion: The proposed method enhances performance and interpretability in tabular prediction, addressing key limitations of current LLM-based approaches.

Abstract: Large Language Models (LLMs) have shown remarkable ability in solving complex
tasks, making them a promising tool for enhancing tabular learning. However,
existing LLM-based methods suffer from high resource requirements, suboptimal
demonstration selection, and limited interpretability, which largely hinder
their prediction performance and application in the real world. To overcome
these problems, we propose a novel in-context learning framework for tabular
prediction. The core idea is to leverage the explanations generated by LLMs to
guide a smaller, locally deployable Surrogate Language Model (SLM) to make
interpretable tabular predictions. Specifically, our framework mainly involves
three stages: (i) Post Hoc Explanation Generation, where LLMs are utilized to
generate explanations for question-answer pairs in candidate demonstrations,
providing insights into the reasoning behind the answer. (ii) Post Hoc
Explanation-Guided Demonstrations Selection, which utilizes explanations
generated by LLMs to guide the process of demonstration selection from
candidate demonstrations. (iii) Post Hoc Explanation-Guided Interpretable SLM
Prediction, which utilizes the demonstrations obtained in step (ii) as
in-context and merges corresponding explanations as rationales to improve the
performance of SLM and guide the model to generate interpretable outputs.
Experimental results highlight the framework's effectiveness, with an average
accuracy improvement of 5.31% across various tabular datasets in diverse
domains.

</details>


### [203] [BMMDetect: A Multimodal Deep Learning Framework for Comprehensive Biomedical Misconduct Detection](https://arxiv.org/pdf/2505.05763)
*Yize Zhou, Jie Zhang, Meijie Wang, Lun Yu*

Main category: cs.LG

TL;DR: BMMDetect is a multimodal deep learning framework for detecting academic misconduct in biomedical research, outperforming single-modality methods by 8.6% AUC.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in academic misconduct detection due to narrow algorithms and fragmented pipelines.

Method: Integrates journal metadata, semantic embeddings, and GPT-4o-mined textual attributes for holistic evaluation.

Result: Achieves 74.33% AUC, identifies key predictors like SJR-index and textual anomalies, and demonstrates transferability.

Conclusion: Advances scalable, interpretable tools for research integrity.

Abstract: Academic misconduct detection in biomedical research remains challenging due
to algorithmic narrowness in existing methods and fragmented analytical
pipelines. We present BMMDetect, a multimodal deep learning framework that
integrates journal metadata (SJR, institutional data), semantic embeddings
(PubMedBERT), and GPT-4o-mined textual attributes (methodological statistics,
data anomalies) for holistic manuscript evaluation. Key innovations include:
(1) multimodal fusion of domain-specific features to reduce detection bias; (2)
quantitative evaluation of feature importance, identifying journal authority
metrics (e.g., SJR-index) and textual anomalies (e.g., statistical outliers) as
dominant predictors; and (3) the BioMCD dataset, a large-scale benchmark with
13,160 retracted articles and 53,411 controls. BMMDetect achieves 74.33% AUC,
outperforming single-modality baselines by 8.6%, and demonstrates
transferability across biomedical subfields. This work advances scalable,
interpretable tools for safeguarding research integrity.

</details>


### [204] [Rethinking Graph Out-Of-Distribution Generalization: A Learnable Random Walk Perspective](https://arxiv.org/pdf/2505.05785)
*Henan Sun, Xunkai Li, Lei Zhu, Junyi Han, Guang Zeng, Ronghua Li, Guoren Wang*

Main category: cs.LG

TL;DR: The paper introduces LRW-OOD, a method for improving Out-Of-Distribution (OOD) generalization in graph neural networks (GNNs) by using learnable random walks and a KDE-based mutual information loss.


<details>
  <summary>Details</summary>
Motivation: Existing graph OOD methods rely on assumptions like invariant topology or spectrum, which may not hold in real-world scenarios. The paper aims to address this inconsistency by proposing a more flexible approach.

Method: The paper advocates for learnable random walks (LRW) as invariant knowledge and introduces LRW-OOD, which parameterizes the transition matrix with an LRW-sampler and path encoder. It also uses a KDE-based MI loss to ensure OOD adherence.

Result: Experiments show LRW-OOD significantly improves OOD generalization, achieving a 3.87% accuracy boost over state-of-the-art baselines under various distribution shifts.

Conclusion: LRW-OOD effectively enhances graph OOD generalization by leveraging learnable random walks and a novel MI loss, outperforming existing methods.

Abstract: Out-Of-Distribution (OOD) generalization has gained increasing attentions for
machine learning on graphs, as graph neural networks (GNNs) often exhibit
performance degradation under distribution shifts. Existing graph OOD methods
tend to follow the basic ideas of invariant risk minimization and structural
causal models, interpreting the invariant knowledge across datasets under
various distribution shifts as graph topology or graph spectrum. However, these
interpretations may be inconsistent with real-world scenarios, as neither
invariant topology nor spectrum is assured. In this paper, we advocate the
learnable random walk (LRW) perspective as the instantiation of invariant
knowledge, and propose LRW-OOD to realize graph OOD generalization learning.
Instead of employing fixed probability transition matrix (i.e.,
degree-normalized adjacency matrix), we parameterize the transition matrix with
an LRW-sampler and a path encoder. Furthermore, we propose the kernel density
estimation (KDE)-based mutual information (MI) loss to generate random walk
sequences that adhere to OOD principles. Extensive experiment demonstrates that
our model can effectively enhance graph OOD generalization under various types
of distribution shifts and yield a significant accuracy improvement of 3.87%
over state-of-the-art graph OOD generalization baselines.

</details>


### [205] [MxMoE: Mixed-precision Quantization for MoE with Accuracy and Performance Co-Design](https://arxiv.org/pdf/2505.05799)
*Haojie Duanmu, Xiuhong Li, Zhihang Yuan, Size Zheng, Jiangfei Duan, Xingcheng Zhang, Dahua Lin*

Main category: cs.LG

TL;DR: MxMoE is a mixed-precision optimization framework for MoE models, addressing quantization challenges by considering parameter sensitivity and expert activation dynamics, achieving better performance than existing methods.


<details>
  <summary>Details</summary>
Motivation: Deploying large MoE models is challenging due to high parameter counts and computational demands. Quantization can help, but existing methods don't fully address the unique characteristics of MoE models.

Method: MxMoE analyzes quantization sensitivity in linear blocks and expert activation frequencies, then optimizes mixed-precision configurations and generates efficient GroupGEMM kernels for parallel execution.

Result: MxMoE achieves 2.4 lower perplexity than GPTQ at 2.25-bit, 3.4x speedup over full precision, and 29.4% speedup over uniform quantization at 5-bit.

Conclusion: MxMoE effectively balances accuracy and efficiency for MoE models, offering a practical solution for deployment challenges.

Abstract: Mixture-of-Experts (MoE) models face deployment challenges due to their large
parameter counts and computational demands. We explore quantization for MoE
models and highlight two key insights: 1) linear blocks exhibit varying
quantization sensitivity, and 2) divergent expert activation frequencies create
heterogeneous computational characteristics. Based on these observations, we
introduce MxMoE, a mixed-precision optimization framework for MoE models that
considers both algorithmic and system perspectives. MxMoE navigates the design
space defined by parameter sensitivity, expert activation dynamics, and
hardware resources to derive efficient mixed-precision configurations.
Additionally, MxMoE automatically generates optimized mixed-precision GroupGEMM
kernels, enabling parallel execution of GEMMs with different precisions.
Evaluations show that MxMoE outperforms existing methods, achieving 2.4 lower
Wikitext-2 perplexity than GPTQ at 2.25-bit and delivering up to 3.4x speedup
over full precision, as well as up to 29.4% speedup over uniform quantization
at equivalent accuracy with 5-bit weight-activation quantization. Our code is
available at https://github.com/cat538/MxMoE.

</details>


### [206] [A novel Neural-ODE model for the state of health estimation of lithium-ion battery using charging curve](https://arxiv.org/pdf/2505.05803)
*Yiming Li, Man He, Jiapeng Liu*

Main category: cs.LG

TL;DR: The paper proposes a hybrid model (ACLA) combining attention, CNN, LSTM, and ANODE for improved SOH estimation in LIBs, achieving high accuracy on test datasets.


<details>
  <summary>Details</summary>
Motivation: Current SOH estimation methods lack generalizability, necessitating a more robust approach for reliable electric vehicle operation.

Method: ACLA integrates attention, CNN, LSTM, and ANODE, using normalized charging time data as input to predict SOH and remaining useful life.

Result: ACLA outperforms benchmarks (NODE, ANODE) with RMSEs of 1.01% and 2.24% on TJU and HUST datasets.

Conclusion: ACLA offers a highly accurate and generalizable solution for SOH estimation in LIBs.

Abstract: The state of health (SOH) of lithium-ion batteries (LIBs) is crucial for
ensuring the safe and reliable operation of electric vehicles. Nevertheless,
the prevailing SOH estimation methods often have limited generalizability. This
paper introduces a data-driven approach for estimating the SOH of LIBs, which
is designed to improve generalization. We construct a hybrid model named ACLA,
which integrates the attention mechanism, convolutional neural network (CNN),
and long short-term memory network (LSTM) into the augmented neural ordinary
differential equation (ANODE) framework. This model employs normalized charging
time corresponding to specific voltages in the constant current charging phase
as input and outputs the SOH as well as remaining useful of life. The model is
trained on NASA and Oxford datasets and validated on the TJU and HUST datasets.
Compared to the benchmark models NODE and ANODE, ACLA exhibits higher accuracy
with root mean square errors (RMSE) for SOH estimation as low as 1.01% and
2.24% on the TJU and HUST datasets, respectively.

</details>


### [207] [Short-circuiting Shortcuts: Mechanistic Investigation of Shortcuts in Text Classification](https://arxiv.org/pdf/2505.06032)
*Leon Eshuijs, Shihan Wang, Antske Fokkens*

Main category: cs.LG

TL;DR: The paper investigates how language models process spurious correlations (shortcuts) internally, identifies specific attention heads responsible for premature decisions, and introduces Head-based Token Attribution (HTA) for detecting and mitigating shortcuts.


<details>
  <summary>Details</summary>
Motivation: To understand how shortcuts are processed within language models' decision-making mechanisms, beyond just identifying input elements that impact predictions.

Method: Uses actor names in movie reviews as controllable shortcuts, applies mechanistic interpretability to identify shortcut-related attention heads, and introduces HTA for tracing decisions back to input tokens.

Result: Identifies specific attention heads that cause premature decisions and shows HTA effectively detects shortcuts, enabling targeted mitigation by deactivating these heads.

Conclusion: HTA provides a practical tool for understanding and mitigating shortcut reliance in language models, improving their decision-making robustness.

Abstract: Reliance on spurious correlations (shortcuts) has been shown to underlie many
of the successes of language models. Previous work focused on identifying the
input elements that impact prediction. We investigate how shortcuts are
actually processed within the model's decision-making mechanism. We use actor
names in movie reviews as controllable shortcuts with known impact on the
outcome. We use mechanistic interpretability methods and identify specific
attention heads that focus on shortcuts. These heads gear the model towards a
label before processing the complete input, effectively making premature
decisions that bypass contextual analysis. Based on these findings, we
introduce Head-based Token Attribution (HTA), which traces intermediate
decisions back to input tokens. We show that HTA is effective in detecting
shortcuts in LLMs and enables targeted mitigation by selectively deactivating
shortcut-related attention heads.

</details>


### [208] [BCE vs. CE in Deep Feature Learning](https://arxiv.org/pdf/2505.05813)
*Qiufu Li, Huibin Xiao, Linlin Shen*

Main category: cs.LG

TL;DR: The paper compares binary cross-entropy (BCE) and cross-entropy (CE) in deep feature learning, proving BCE also leads to neural collapse (NC) by maximizing intra-class compactness and inter-class distinctiveness. BCE's absolute score adjustments and classifier biases enhance feature properties, outperforming CE in classification tasks.


<details>
  <summary>Details</summary>
Motivation: To understand and compare the effectiveness of BCE and CE in achieving neural collapse (NC) and improving feature compactness and distinctiveness in classification models.

Method: Theoretical proof and experimental validation comparing BCE and CE, analyzing their impact on intra-class compactness and inter-class distinctiveness.

Result: BCE achieves NC, enhances feature properties, and improves classification performance compared to CE.

Conclusion: BCE is a viable alternative to CE for training classification models, offering better feature compactness and distinctiveness.

Abstract: When training classification models, it expects that the learned features are
compact within classes, and can well separate different classes. As the
dominant loss function for training classification models, minimizing
cross-entropy (CE) loss maximizes the compactness and distinctiveness, i.e.,
reaching neural collapse (NC). The recent works show that binary CE (BCE)
performs also well in multi-class tasks. In this paper, we compare BCE and CE
in deep feature learning. For the first time, we prove that BCE can also
maximize the intra-class compactness and inter-class distinctiveness when
reaching its minimum, i.e., leading to NC. We point out that CE measures the
relative values of decision scores in the model training, implicitly enhancing
the feature properties by classifying samples one-by-one. In contrast, BCE
measures the absolute values of decision scores and adjust the
positive/negative decision scores across all samples to uniformly high/low
levels. Meanwhile, the classifier biases in BCE present a substantial
constraint on the decision scores to explicitly enhance the feature properties
in the training. The experimental results are aligned with above analysis, and
show that BCE could improve the classification and leads to better compactness
and distinctiveness among sample features. The codes will be released.

</details>


### [209] [New Statistical and Computational Results for Learning Junta Distributions](https://arxiv.org/pdf/2505.05819)
*Lorenzo Beretta*

Main category: cs.LG

TL;DR: The paper shows that learning k-junta distributions is computationally equivalent to learning k-parity functions with noise (LPN) and presents an optimal algorithm for this task.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the computational and statistical complexity of learning k-junta distributions, a fundamental problem in learning theory.

Method: The paper demonstrates a computational equivalence between learning k-junta distributions and LPN, and designs an algorithm with optimal statistical complexity.

Result: The algorithm achieves optimal statistical complexity and matches the computational complexity of prior non-optimal methods.

Conclusion: The results imply that further improvements to the algorithm are unlikely without a breakthrough in solving LPN.

Abstract: We study the problem of learning junta distributions on $\{0, 1\}^n$, where a
distribution is a $k$-junta if its probability mass function depends on a
subset of at most $k$ variables. We make two main contributions:
  - We show that learning $k$-junta distributions is \emph{computationally}
equivalent to learning $k$-parity functions with noise (LPN), a landmark
problem in computational learning theory.
  - We design an algorithm for learning junta distributions whose statistical
complexity is optimal, up to polylogarithmic factors. Computationally, our
algorithm matches the complexity of previous (non-sample-optimal) algorithms.
  Combined, our two contributions imply that our algorithm cannot be
significantly improved, statistically or computationally, barring a
breakthrough for LPN.

</details>


### [210] [Mixed-Integer Optimization for Responsible Machine Learning](https://arxiv.org/pdf/2505.05857)
*Nathan Justin, Qingshi Sun, Andrés Gómez, Phebe Vayanos*

Main category: cs.LG

TL;DR: The paper introduces mixed-integer optimization (MIO) as a framework for embedding responsible ML principles like fairness and transparency into machine learning models while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: The increasing deployment of ML in critical and sensitive areas raises concerns about fairness, transparency, and privacy, necessitating responsible ML methods.

Method: The paper uses MIO to integrate responsible ML considerations directly into the learning process, providing theoretical and practical insights.

Result: MIO enables the learning of transparent models with fairness constraints, offering practical tools and strategies for implementation.

Conclusion: The paper highlights MIO's utility for responsible ML, discusses current limitations, and suggests future research directions.

Abstract: In the last few decades, Machine Learning (ML) has achieved significant
success across domains ranging from healthcare, sustainability, and the social
sciences, to criminal justice and finance. But its deployment in increasingly
sophisticated, critical, and sensitive areas affecting individuals, the groups
they belong to, and society as a whole raises critical concerns around
fairness, transparency, robustness, and privacy, among others. As the
complexity and scale of ML systems and of the settings in which they are
deployed grow, so does the need for responsible ML methods that address these
challenges while providing guaranteed performance in deployment.
  Mixed-integer optimization (MIO) offers a powerful framework for embedding
responsible ML considerations directly into the learning process while
maintaining performance. For example, it enables learning of inherently
transparent models that can conveniently incorporate fairness or other domain
specific constraints. This tutorial paper provides an accessible and
comprehensive introduction to this topic discussing both theoretical and
practical aspects. It outlines some of the core principles of responsible ML,
their importance in applications, and the practical utility of MIO for building
ML models that align with these principles. Through examples and mathematical
formulations, it illustrates practical strategies and available tools for
efficiently solving MIO problems for responsible ML. It concludes with a
discussion on current limitations and open research questions, providing
suggestions for future work.

</details>


### [211] [Open Set Label Shift with Test Time Out-of-Distribution Reference](https://arxiv.org/pdf/2505.05868)
*Changkun Ye, Russell Tsuchida, Lars Petersson, Nick Barnes*

Main category: cs.LG

TL;DR: The paper proposes a method to estimate source and target label distributions under open set label shift (OSLS) using classifiers, with a three-stage approach and error quantification.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of OSLS, where label distributions shift and an out-of-distribution (OOD) class is introduced in the target domain.

Method: A three-stage approach: 1) estimate source OOD label distribution, 2) EM algorithm for MLE of target label distribution, 3) relaxed OOD classifier assumptions for target OOD distribution. Sampling errors are quantified.

Result: Effective correction of the ID classifier for the target distribution without retraining, validated in various OSLS settings.

Conclusion: The method successfully handles OSLS by estimating label distributions and correcting classifiers, with demonstrated effectiveness.

Abstract: Open set label shift (OSLS) occurs when label distributions change from a
source to a target distribution, and the target distribution has an additional
out-of-distribution (OOD) class. In this work, we build estimators for both
source and target open set label distributions using a source domain
in-distribution (ID) classifier and an ID/OOD classifier. With reasonable
assumptions on the ID/OOD classifier, the estimators are assembled into a
sequence of three stages: 1) an estimate of the source label distribution of
the OOD class, 2) an EM algorithm for Maximum Likelihood estimates (MLE) of the
target label distribution, and 3) an estimate of the target label distribution
of OOD class under relaxed assumptions on the OOD classifier. The sampling
errors of estimates in 1) and 3) are quantified with a concentration
inequality. The estimation result allows us to correct the ID classifier
trained on the source distribution to the target distribution without
retraining. Experiments on a variety of open set label shift settings
demonstrate the effectiveness of our model. Our code is available at
https://github.com/ChangkunYe/OpenSetLabelShift.

</details>


### [212] [Generative Discovery of Partial Differential Equations by Learning from Math Handbooks](https://arxiv.org/pdf/2505.05869)
*Hao Xu, Yuntian Chen, Rui Cao, Tianning Tang, Mengge Du, Jian Li, Adrian H. Callaghan, Dongxiao Zhang*

Main category: cs.LG

TL;DR: The paper introduces a knowledge-guided approach, EqGPT, for discovering PDEs by leveraging documented PDEs and a generative model, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of balancing search space and optimization efficiency in data-driven PDE discovery.

Method: Uses documented PDEs encoded as sentence-like structures to train EqGPT, a generative model, followed by a generation-evaluation-optimization loop.

Result: Recovers various PDE forms accurately and efficiently, even for complex cases, and discovers a new PDE for nonlinear surface gravity waves.

Conclusion: The approach is effective, generalizable, and applicable to real-world problems, supporting scientific discovery.

Abstract: Data driven discovery of partial differential equations (PDEs) is a promising
approach for uncovering the underlying laws governing complex systems. However,
purely data driven techniques face the dilemma of balancing search space with
optimization efficiency. This study introduces a knowledge guided approach that
incorporates existing PDEs documented in a mathematical handbook to facilitate
the discovery process. These PDEs are encoded as sentence like structures
composed of operators and basic terms, and used to train a generative model,
called EqGPT, which enables the generation of free form PDEs. A loop of
generation evaluation optimization is constructed to autonomously identify the
most suitable PDE. Experimental results demonstrate that this framework can
recover a variety of PDE forms with high accuracy and computational efficiency,
particularly in cases involving complex temporal derivatives or intricate
spatial terms, which are often beyond the reach of conventional methods. The
approach also exhibits generalizability to irregular spatial domains and higher
dimensional settings. Notably, it succeeds in discovering a previously
unreported PDE governing strongly nonlinear surface gravity waves propagating
toward breaking, based on real world experimental data, highlighting its
applicability to practical scenarios and its potential to support scientific
discovery.

</details>


### [213] [A 3D pocket-aware and evolutionary conserved interaction guided diffusion model for molecular optimization](https://arxiv.org/pdf/2505.05874)
*Anjie Qiao, Hao Zhang, Qianmu Yuan, Qirui Deng, Jingtian Su, Weifeng Huang, Huihao Zhou, Guo-Bo Li, Zhen Wang, Jinping Lei*

Main category: cs.LG

TL;DR: DiffDecip, a 3D target-aware diffusion model, improves molecule optimization by incorporating protein-ligand interactions and evolutionary conservation, outperforming baseline models in affinity and interaction with conserved residues.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models for molecule generation lack focus on highly conserved protein residues, which are crucial for protein function and ligand bioactivity.

Method: DiffDecip integrates protein-ligand binding interactions and residue conservation into diffusion and sampling processes for scaffold decoration.

Result: DiffDecip outperforms baseline models by generating molecules with higher affinity and more interactions with conserved residues.

Conclusion: DiffDecip advances structure-based drug design by prioritizing conserved residues, enhancing molecule optimization.

Abstract: Generating molecules that bind to specific protein targets via diffusion
models has shown good promise for structure-based drug design and molecule
optimization. Especially, the diffusion models with binding interaction
guidance enables molecule generation with high affinity through forming
favorable interaction within protein pocket. However, the generated molecules
may not form interactions with the highly conserved residues, which are
important for protein functions and bioactivities of the ligands. Herein, we
developed a new 3D target-aware diffusion model DiffDecip, which explicitly
incorporates the protein-ligand binding interactions and evolutionary
conservation information of protein residues into both diffusion and sampling
process, for molecule optimization through scaffold decoration. The model
performance revealed that DiffDecip outperforms baseline model DiffDec on
molecule optimization towards higher affinity through forming more non-covalent
interactions with highly conserved residues in the protein pocket.

</details>


### [214] [Multi-Modal Molecular Representation Learning via Structure Awareness](https://arxiv.org/pdf/2505.05877)
*Rong Yin, Ruyue Liu, Xiaoshuai Hao, Xingrui Zhou, Yong Liu, Can Ma, Weiping Wang*

Main category: cs.LG

TL;DR: Proposes MMSA, a multi-modal self-supervised framework for molecular representation learning, enhancing intermodal interactions and higher-order relationships, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal methods overlook intermodal interactions and fail to capture complex higher-order relationships between molecules.

Method: MMSA combines multi-modal representation learning and a structure-awareness module, using hypergraphs and memory mechanisms to enhance molecular embeddings.

Result: Achieves 1.8% to 9.6% ROC-AUC improvement on MoleculeNet benchmark over baselines.

Conclusion: MMSA effectively captures invariant knowledge and higher-order relationships, advancing molecular representation learning.

Abstract: Accurate extraction of molecular representations is a critical step in the
drug discovery process. In recent years, significant progress has been made in
molecular representation learning methods, among which multi-modal molecular
representation methods based on images, and 2D/3D topologies have become
increasingly mainstream. However, existing these multi-modal approaches often
directly fuse information from different modalities, overlooking the potential
of intermodal interactions and failing to adequately capture the complex
higher-order relationships and invariant features between molecules. To
overcome these challenges, we propose a structure-awareness-based multi-modal
self-supervised molecular representation pre-training framework (MMSA) designed
to enhance molecular graph representations by leveraging invariant knowledge
between molecules. The framework consists of two main modules: the multi-modal
molecular representation learning module and the structure-awareness module.
The multi-modal molecular representation learning module collaboratively
processes information from different modalities of the same molecule to
overcome intermodal differences and generate a unified molecular embedding.
Subsequently, the structure-awareness module enhances the molecular
representation by constructing a hypergraph structure to model higher-order
correlations between molecules. This module also introduces a memory mechanism
for storing typical molecular representations, aligning them with memory
anchors in the memory bank to integrate invariant knowledge, thereby improving
the model generalization ability. Extensive experiments have demonstrated the
effectiveness of MMSA, which achieves state-of-the-art performance on the
MoleculeNet benchmark, with average ROC-AUC improvements ranging from 1.8% to
9.6% over baseline methods.

</details>


### [215] [IRNN: Innovation-driven Recurrent Neural Network for Time-Series Data Modeling and Prediction](https://arxiv.org/pdf/2505.05916)
*Yifan Zhou, Yibo Wang, Chao Shang*

Main category: cs.LG

TL;DR: The paper introduces Innovation-driven RNN (IRNN), a novel RNN architecture that incorporates past prediction errors (innovations) to improve time-series prediction accuracy, along with a tailored training algorithm (IU-BPTT).


<details>
  <summary>Details</summary>
Motivation: To enhance RNN performance for time-series prediction by leveraging the concept of 'innovation' from Kalman filters, which uses past prediction errors to update hidden states.

Method: Proposes IRNN, which integrates innovations (past errors) as additional inputs, and develops IU-BPTT, a training algorithm that alternates between updating innovations and optimizing parameters.

Result: Experiments show IRNN significantly improves prediction accuracy on benchmark datasets without substantially increasing training costs.

Conclusion: IRNN, with its innovation-driven approach and IU-BPTT training, offers a promising solution for time-series modeling and prediction tasks.

Abstract: Many real-world datasets are time series that are sequentially collected and
contain rich temporal information. Thus, a common interest in practice is to
capture dynamics of time series and predict their future evolutions. To this
end, the recurrent neural network (RNN) has been a prevalent and effective
machine learning option, which admits a nonlinear state-space model
representation. Motivated by the resemblance between RNN and Kalman filter (KF)
for linear state-space models, we propose in this paper Innovation-driven RNN
(IRNN), a novel RNN architecture tailored to time-series data modeling and
prediction tasks. By adapting the concept of "innovation" from KF to RNN, past
prediction errors are adopted as additional input signals to update hidden
states of RNN and boost prediction performance. Since innovation data depend on
network parameters, existing training algorithms for RNN do not apply to IRNN
straightforwardly. Thus, a tailored training algorithm dubbed input
updating-based back-propagation through time (IU-BPTT) is further proposed,
which alternates between updating innovations and optimizing network parameters
via gradient descent. Experiments on real-world benchmark datasets show that
the integration of innovations into various forms of RNN leads to remarkably
improved prediction accuracy of IRNN without increasing the training cost
substantially.

</details>


### [216] [Autoencoder-Based Hybrid Replay for Class-Incremental Learning](https://arxiv.org/pdf/2505.05926)
*Milad Khademi Nori, Il-Min Kim, Guanghui Wang*

Main category: cs.LG

TL;DR: Proposes an autoencoder-based hybrid replay (AHR) strategy for class-incremental learning, reducing memory complexity to O(0.1t) while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenges of task confusion and catastrophic forgetting in class-incremental learning, aiming to reduce memory and compute complexities.

Method: Uses a hybrid autoencoder (HAE) for compression and replay, incorporating energy minimization equations and repulsive force algorithms for embedding new class centroids.

Result: AHR outperforms baselines in benchmarks while operating under the same memory/compute constraints.

Conclusion: AHR is an effective solution for class-incremental learning, balancing performance and resource efficiency.

Abstract: In class-incremental learning (CIL), effective incremental learning
strategies are essential to mitigate task confusion and catastrophic
forgetting, especially as the number of tasks $t$ increases. Current exemplar
replay strategies impose $\mathcal{O}(t)$ memory/compute complexities. We
propose an autoencoder-based hybrid replay (AHR) strategy that leverages our
new hybrid autoencoder (HAE) to function as a compressor to alleviate the
requirement for large memory, achieving $\mathcal{O}(0.1 t)$ at the worst case
with the computing complexity of $\mathcal{O}(t)$ while accomplishing
state-of-the-art performance. The decoder later recovers the exemplar data
stored in the latent space, rather than in raw format. Additionally, HAE is
designed for both discriminative and generative modeling, enabling
classification and replay capabilities, respectively. HAE adopts the charged
particle system energy minimization equations and repulsive force algorithm for
the incremental embedding and distribution of new class centroids in its latent
space. Our results demonstrate that AHR consistently outperforms recent
baselines across multiple benchmarks while operating with the same
memory/compute budgets. The source code is included in the supplementary
material and will be open-sourced upon publication.

</details>


### [217] [FloE: On-the-Fly MoE Inference](https://arxiv.org/pdf/2505.05950)
*Yuxin Zhou, Zheng Li, Jun Zhang, Jue Wang, Yiping Wang, Zhongle Xie, Ke Chen, Lidan Shou*

Main category: cs.LG

TL;DR: FloE is a system for efficient Mixture-of-Experts (MoE) inference on memory-constrained GPUs, reducing data movement and memory footprint while accelerating inference.


<details>
  <summary>Details</summary>
Motivation: The need for efficient MoE inference on memory-constrained devices due to PCIe bandwidth limitations and latency sensitivity.

Method: FloE compresses expert parameters and uses sparse prediction to reduce data movement and memory usage.

Result: Achieves 9.3x compression, 8.5x memory reduction, and 48.7x speedup on a GeForce RTX 3090.

Conclusion: FloE effectively addresses the challenges of MoE inference on resource-constrained devices.

Abstract: With the widespread adoption of Mixture-of-Experts (MoE) models, there is a
growing demand for efficient inference on memory-constrained devices. While
offloading expert parameters to CPU memory and loading activated experts on
demand has emerged as a potential solution, the large size of activated experts
overburdens the limited PCIe bandwidth, hindering the effectiveness in
latency-sensitive scenarios. To mitigate this, we propose FloE, an on-the-fly
MoE inference system on memory-constrained GPUs. FloE is built on the insight
that there exists substantial untapped redundancy within sparsely activated
experts. It employs various compression techniques on the expert's internal
parameter matrices to reduce the data movement load, combined with low-cost
sparse prediction, achieving perceptible inference acceleration in wall-clock
time on resource-constrained devices. Empirically, FloE achieves a 9.3x
compression of parameters per expert in Mixtral-8x7B; enables deployment on a
GPU with only 11GB VRAM, reducing the memory footprint by up to 8.5x; and
delivers a 48.7x inference speedup compared to DeepSpeed-MII on a single
GeForce RTX 3090.

</details>


### [218] [Learning Power Control Protocol for In-Factory 6G Subnetworks](https://arxiv.org/pdf/2505.05967)
*Uyoata E. Uyoata, Gilberto Berardinelli, Ramoni Adeogun*

Main category: cs.LG

TL;DR: A novel MARL framework for In-Factory Subnetworks reduces signaling overhead by 8x while maintaining near-optimal performance.


<details>
  <summary>Details</summary>
Motivation: Existing power control methods overlook signaling overhead and assume perfect CSI, limiting practicality in dense In-Factory scenarios.

Method: Uses MARL (MAPPO) to autonomously learn signaling and power control protocols, modeled as a POMDP.

Result: 8x reduction in signaling overhead with only 5% performance gap from ideal.

Conclusion: The MARL approach effectively balances signaling and power control in dense In-Factory Subnetworks.

Abstract: In-X Subnetworks are envisioned to meet the stringent demands of short-range
communication in diverse 6G use cases. In the context of In-Factory scenarios,
effective power control is critical to mitigating the impact of interference
resulting from potentially high subnetwork density. Existing approaches to
power control in this domain have predominantly emphasized the data plane,
often overlooking the impact of signaling overhead. Furthermore, prior work has
typically adopted a network-centric perspective, relying on the assumption of
complete and up-to-date channel state information (CSI) being readily available
at the central controller. This paper introduces a novel multi-agent
reinforcement learning (MARL) framework designed to enable access points to
autonomously learn both signaling and power control protocols in an In-Factory
Subnetwork environment. By formulating the problem as a partially observable
Markov decision process (POMDP) and leveraging multi-agent proximal policy
optimization (MAPPO), the proposed approach achieves significant advantages.
The simulation results demonstrate that the learning-based method reduces
signaling overhead by a factor of 8 while maintaining a buffer flush rate that
lags the ideal "Genie" approach by only 5%.

</details>


### [219] [Architectural Exploration of Hybrid Neural Decoders for Neuromorphic Implantable BMI](https://arxiv.org/pdf/2505.05983)
*Vivek Mohan, Biyan Zhou, Zhou Wang, Anil Bharath, Emmanuel Drakakis, Arindam Basu*

Main category: cs.LG

TL;DR: An efficient decoding pipeline for neuromorphic implantable brain-machine interfaces (Neu-iBMI) reduces event processing by up to 554X and achieves high decoding performance without traditional signal recovery steps.


<details>
  <summary>Details</summary>
Motivation: To address the computational and memory inefficiencies in conventional iBMI systems by leveraging sparse neural event data.

Method: Introduces a tunable event filter (EvFilter) and spike detector (EvFilter-SPD) to reduce event processing. Uses ANN- and SNN-based decoders for efficient decoding.

Result: Achieves R^2=0.73 decoding performance, reduces computations by 5-23X, and lowers memory demands compared to traditional decoders.

Conclusion: The pipeline is ideal for low-power, on-implant, or wearable iBMIs due to its efficiency and reduced resource requirements.

Abstract: This work presents an efficient decoding pipeline for neuromorphic
implantable brain-machine interfaces (Neu-iBMI), leveraging sparse neural event
data from an event-based neural sensing scheme. We introduce a tunable event
filter (EvFilter), which also functions as a spike detector (EvFilter-SPD),
significantly reducing the number of events processed for decoding by 192X and
554X, respectively. The proposed pipeline achieves high decoding performance,
up to R^2=0.73, with ANN- and SNN-based decoders, eliminating the need for
signal recovery, spike detection, or sorting, commonly performed in
conventional iBMI systems. The SNN-Decoder reduces computations and memory
required by 5-23X compared to NN-, and LSTM-Decoders, while the ST-NN-Decoder
delivers similar performance to an LSTM-Decoder requiring 2.5X fewer resources.
This streamlined approach significantly reduces computational and memory
demands, making it ideal for low-power, on-implant, or wearable iBMIs.

</details>


### [220] [Differentiable Fuzzy Neural Networks for Recommender Systems](https://arxiv.org/pdf/2505.06000)
*Stephan Bartl, Kevin Innerebner, Elisabeth Lex*

Main category: cs.LG

TL;DR: A neuro-symbolic approach using fuzzy neural networks (FNNs) for transparent recommender systems, combining logic-based rules with competitive performance.


<details>
  <summary>Details</summary>
Motivation: To enhance transparency in recommender systems for user trust, accountability, and regulatory compliance by integrating symbolic reasoning with sub-symbolic learning.

Method: Uses FNNs to learn logic-based rules over human-readable atoms, forming fuzzy logic expressions for transparent decision-making. Evaluated on synthetic and MovieLens 1M datasets.

Result: Achieves competitive performance while providing transparent reasoning, accurately capturing user behavior.

Conclusion: The differentiable nature of FNNs allows integration with other neural models, enabling hybrid, transparent recommender systems.

Abstract: As recommender systems become increasingly complex, transparency is essential
to increase user trust, accountability, and regulatory compliance.
Neuro-symbolic approaches that integrate symbolic reasoning with sub-symbolic
learning offer a promising approach toward transparent and user-centric
systems. In this work-in-progress, we investigate using fuzzy neural networks
(FNNs) as a neuro-symbolic approach for recommendations that learn logic-based
rules over predefined, human-readable atoms. Each rule corresponds to a fuzzy
logic expression, making the recommender's decision process inherently
transparent. In contrast to black-box machine learning methods, our approach
reveals the reasoning behind a recommendation while maintaining competitive
performance. We evaluate our method on a synthetic and MovieLens 1M datasets
and compare it to state-of-the-art recommendation algorithms. Our results
demonstrate that our approach accurately captures user behavior while providing
a transparent decision-making process. Finally, the differentiable nature of
this approach facilitates an integration with other neural models, enabling the
development of hybrid, transparent recommender systems.

</details>


### [221] [Fuzzy-UCS Revisited: Self-Adaptation of Rule Representations in Michigan-Style Learning Fuzzy-Classifier Systems](https://arxiv.org/pdf/2505.06017)
*Hiroki Shiraishi, Yohei Hayamizu, Tomonori Hashiyama*

Main category: cs.LG

TL;DR: The paper proposes Adaptive-UCS, a supervised LFCS with self-adaptive rule representation, improving classification performance over conventional methods.


<details>
  <summary>Details</summary>
Motivation: Conventional rule representations in LFCSs struggle with unknown data characteristics, prompting the need for a more adaptive approach.

Method: Adaptive-UCS introduces a fuzzy indicator optimized via evolutionary operators to dynamically adjust rule shapes (crisp or fuzzy).

Result: Adaptive-UCS outperforms traditional UCSs in accuracy and handles noisy or uncertain data robustly.

Conclusion: Adaptive-UCS offers a superior, adaptable solution for LFCSs, enhancing performance in diverse and uncertain scenarios.

Abstract: This paper focuses on the impact of rule representation in Michigan-style
Learning Fuzzy-Classifier Systems (LFCSs) on its classification performance. A
well-representation of the rules in an LFCS is crucial for improving its
performance. However, conventional rule representations frequently need help
addressing problems with unknown data characteristics. To address this issue,
this paper proposes a supervised LFCS (i.e., Fuzzy-UCS) with a self-adaptive
rule representation mechanism, entitled Adaptive-UCS. Adaptive-UCS incorporates
a fuzzy indicator as a new rule parameter that sets the membership function of
a rule as either rectangular (i.e., crisp) or triangular (i.e., fuzzy) shapes.
The fuzzy indicator is optimized with evolutionary operators, allowing the
system to search for an optimal rule representation. Results from extensive
experiments conducted on continuous space problems demonstrate that
Adaptive-UCS outperforms other UCSs with conventional crisp-hyperrectangular
and fuzzy-hypertrapezoidal rule representations in classification accuracy.
Additionally, Adaptive-UCS exhibits robustness in the case of noisy inputs and
real-world problems with inherent uncertainty, such as missing values, leading
to stable classification performance.

</details>


### [222] [Universal Approximation Theorem for Deep Q-Learning via FBSDE System](https://arxiv.org/pdf/2505.06023)
*Qian Qi*

Main category: cs.LG

TL;DR: The paper presents a Universal Approximation Theorem (UAT) for Deep Q-Networks (DQNs) designed to mimic Bellman updates, linking network depth to value iteration refinements with controlled error.


<details>
  <summary>Details</summary>
Motivation: To justify DQN approximation capabilities by leveraging the structural properties of the optimal Q-function, unlike general UATs.

Method: Analyzes DQNs as neural operators approximating Bellman operators, using regularity propagation and dynamic programming principles.

Result: Shows that deep residual networks can approximate Bellman operators, with network depth corresponding to value iteration refinements.

Conclusion: The approach ties DQN architecture to control problem structure, offering a dynamic systems view of value function approximation.

Abstract: The approximation capabilities of Deep Q-Networks (DQNs) are commonly
justified by general Universal Approximation Theorems (UATs) that do not
leverage the intrinsic structural properties of the optimal Q-function, the
solution to a Bellman equation. This paper establishes a UAT for a class of
DQNs whose architecture is designed to emulate the iterative refinement process
inherent in Bellman updates. A central element of our analysis is the
propagation of regularity: while the transformation induced by a single Bellman
operator application exhibits regularity, for which Backward Stochastic
Differential Equations (BSDEs) theory provides analytical tools, the uniform
regularity of the entire sequence of value iteration iterates--specifically,
their uniform Lipschitz continuity on compact domains under standard Lipschitz
assumptions on the problem data--is derived from finite-horizon dynamic
programming principles. We demonstrate that layers of a deep residual network,
conceived as neural operators acting on function spaces, can approximate the
action of the Bellman operator. The resulting approximation theorem is thus
intrinsically linked to the control problem's structure, offering a proof
technique wherein network depth directly corresponds to iterations of value
function refinement, accompanied by controlled error propagation. This
perspective reveals a dynamic systems view of the network's operation on a
space of value functions.

</details>


### [223] [PYRREGULAR: A Unified Framework for Irregular Time Series, with Classification Benchmarks](https://arxiv.org/pdf/2505.06047)
*Francesco Spinnato, Cristiano Landi*

Main category: cs.LG

TL;DR: A unified framework and standardized dataset repository for irregular time series classification is introduced to address fragmented research efforts and improve interoperability.


<details>
  <summary>Details</summary>
Motivation: Irregular temporal data poses challenges due to varying frequencies, durations, and missing values, often addressed in isolation.

Method: A common array format is used to create a repository of 34 datasets, benchmarking 12 classifier models.

Result: The framework centralizes research and enables robust evaluation of irregular temporal data methods.

Conclusion: This work bridges gaps in irregular time series research, fostering collaboration and standardized evaluation.

Abstract: Irregular temporal data, characterized by varying recording frequencies,
differing observation durations, and missing values, presents significant
challenges across fields like mobility, healthcare, and environmental science.
Existing research communities often overlook or address these challenges in
isolation, leading to fragmented tools and methods. To bridge this gap, we
introduce a unified framework, and the first standardized dataset repository
for irregular time series classification, built on a common array format to
enhance interoperability. This repository comprises 34 datasets on which we
benchmark 12 classifier models from diverse domains and communities. This work
aims to centralize research efforts and enable a more robust evaluation of
irregular temporal data analysis methods.

</details>


### [224] [Safe-EF: Error Feedback for Nonsmooth Constrained Optimization](https://arxiv.org/pdf/2505.06053)
*Rustem Islamov, Yarden As, Ilyas Fatkhullin*

Main category: cs.LG

TL;DR: The paper addresses communication bottlenecks in federated learning by introducing Safe-EF, a novel algorithm that enforces safety constraints and matches theoretical lower bounds for non-smooth convex problems.


<details>
  <summary>Details</summary>
Motivation: Federated learning suffers from high communication costs due to large model updates, and existing methods like error feedback (EF) are limited to smooth, unconstrained problems, restricting real-world applicability.

Method: The authors propose Safe-EF, an algorithm that extends EF to non-smooth convex problems with safety constraints, and validate it in a stochastic setting and RL experiments.

Result: Safe-EF matches theoretical lower bounds for communication compression and ensures safety in practical applications, as demonstrated in distributed robot training.

Conclusion: Safe-EF bridges theory and practice, offering a scalable solution for federated learning with non-smooth objectives and safety constraints.

Abstract: Federated learning faces severe communication bottlenecks due to the high
dimensionality of model updates. Communication compression with contractive
compressors (e.g., Top-K) is often preferable in practice but can degrade
performance without proper handling. Error feedback (EF) mitigates such issues
but has been largely restricted for smooth, unconstrained problems, limiting
its real-world applicability where non-smooth objectives and safety constraints
are critical. We advance our understanding of EF in the canonical non-smooth
convex setting by establishing new lower complexity bounds for first-order
algorithms with contractive compression. Next, we propose Safe-EF, a novel
algorithm that matches our lower bound (up to a constant) while enforcing
safety constraints essential for practical applications. Extending our approach
to the stochastic setting, we bridge the gap between theory and practical
implementation. Extensive experiments in a reinforcement learning setup,
simulating distributed humanoid robot training, validate the effectiveness of
Safe-EF in ensuring safety and reducing communication complexity.

</details>


### [225] [UniSymNet: A Unified Symbolic Network Guided by Transformer](https://arxiv.org/pdf/2505.06091)
*Xinxin Li, Juan Zhang, Da Li, Xingyu Liu, Jin Xu, Junping Yin*

Main category: cs.LG

TL;DR: A Unified Symbolic Network (UniSymNet) is proposed to address challenges in symbolic regression by unifying binary operators into nested unary ones, reducing complexity and improving performance.


<details>
  <summary>Details</summary>
Motivation: Current symbolic networks struggle with extending binary operators to multivariate cases and face issues like overfitting due to fixed architectures.

Method: UniSymNet unifies nonlinear binary operators into nested unary operators, pre-trains a Transformer with label encoding for structural guidance, and uses objective-specific optimization.

Result: UniSymNet achieves high accuracy, strong symbolic solution rates, and low complexity, performing well on benchmarks.

Conclusion: The proposed UniSymNet offers a promising solution for symbolic regression by addressing key limitations of existing methods.

Abstract: Symbolic Regression (SR) is a powerful technique for automatically
discovering mathematical expressions from input data. Mainstream SR algorithms
search for the optimal symbolic tree in a vast function space, but the
increasing complexity of the tree structure limits their performance. Inspired
by neural networks, symbolic networks have emerged as a promising new paradigm.
However, most existing symbolic networks still face certain challenges: binary
nonlinear operators $\{\times, \div\}$ cannot be naturally extended to
multivariate operators, and training with fixed architecture often leads to
higher complexity and overfitting. In this work, we propose a Unified Symbolic
Network that unifies nonlinear binary operators into nested unary operators and
define the conditions under which UniSymNet can reduce complexity. Moreover, we
pre-train a Transformer model with a novel label encoding method to guide
structural selection, and adopt objective-specific optimization strategies to
learn the parameters of the symbolic network. UniSymNet shows high fitting
accuracy, excellent symbolic solution rate, and relatively low expression
complexity, achieving competitive performance on low-dimensional Standard
Benchmarks and high-dimensional SRBench.

</details>


### [226] [Fault Diagnosis of 3D-Printed Scaled Wind Turbine Blades](https://arxiv.org/pdf/2505.06080)
*Luis Miguel Esquivel-Sancho, Maryam Ghandchi Tehrani, Mauricio Muñoz-Arias, Mahmoud Askari*

Main category: cs.LG

TL;DR: An integrated method for detecting faults in wind turbine blades using 3D-printed models, simulations, experiments, and machine learning achieved over 94% accuracy.


<details>
  <summary>Details</summary>
Motivation: To develop a reliable fault detection system for wind turbine blades by combining numerical and experimental approaches.

Method: Used 3D-printed scaled models, finite element simulations, experimental modal analysis, and machine learning (SVM and KNN) for damage detection.

Result: Machine learning classifiers achieved over 94% accuracy, with vibration modes 3, 4, and 6 being most sensitive to damage.

Conclusion: The approach is feasible for structural health monitoring in wind energy, combining simulations and experiments effectively.

Abstract: This study presents an integrated methodology for fault detection in wind
turbine blades using 3D-printed scaled models, finite element simulations,
experimental modal analysis, and machine learning techniques. A scaled model of
the NREL 5MW blade was fabricated using 3D printing, and crack-type damages
were introduced at critical locations. Finite Element Analysis was employed to
predict the impact of these damages on the natural frequencies, with the
results validated through controlled hammer impact tests. Vibration data was
processed to extract both time-domain and frequency-domain features, and key
discriminative variables were identified using statistical analyses (ANOVA).
Machine learning classifiers, including Support Vector Machine and K-Nearest
Neighbors, achieved classification accuracies exceeding 94%. The results
revealed that vibration modes 3, 4, and 6 are particularly sensitive to
structural anomalies for this blade. This integrated approach confirms the
feasibility of combining numerical simulations with experimental validations
and paves the way for structural health monitoring systems in wind energy
applications.

</details>


### [227] [LLMs Outperform Experts on Challenging Biology Benchmarks](https://arxiv.org/pdf/2505.06108)
*Lennart Justen*

Main category: cs.LG

TL;DR: The study evaluates 27 LLMs on eight biology benchmarks, showing significant performance improvements, with some models surpassing expert virologists. Extended reasoning features helped, but chain-of-thought did not. Some benchmarks plateaued, indicating data issues.


<details>
  <summary>Details</summary>
Motivation: To assess the biological capabilities of frontier LLMs and track their progress over time, comparing them to expert performance.

Method: Evaluated 27 LLMs on eight diverse biology benchmarks through ten independent runs per benchmark.

Result: Top models showed 4-fold improvement in virology, with some matching or exceeding expert performance. Extended reasoning features boosted results, but chain-of-thought did not. Some benchmarks plateaued below 100%.

Conclusion: LLMs have advanced significantly in biology, but better evaluation methods are needed due to benchmark saturation and data errors.

Abstract: This study systematically evaluates 27 frontier Large Language Models on
eight diverse biology benchmarks spanning molecular biology, genetics, cloning,
virology, and biosecurity. Models from major AI developers released between
November 2022 and April 2025 were assessed through ten independent runs per
benchmark. The findings reveal dramatic improvements in biological
capabilities. Top model performance increased more than 4-fold on the
challenging text-only subset of the Virology Capabilities Test over the study
period, with the top model now performing twice as well as expert virologists.
Several models now match or exceed expert-level performance on other
challenging benchmarks, including LAB-Bench CloningScenarios and the biology
subsets of GPQA and WMDP. Contrary to expectations, chain-of-thought did not
substantially improve performance over zero-shot evaluation, while extended
reasoning features in o3-mini and Claude 3.7 Sonnet typically improved
performance as predicted by inference scaling. Benchmarks such as PubMedQA and
the MMLU and WMDP biology subsets exhibited performance plateaus well below
100%, suggesting benchmark saturation and errors in the underlying benchmark
data. The analysis highlights the need for more sophisticated evaluation
methodologies as AI systems continue to advance.

</details>


### [228] [Deep Diffusion Maps](https://arxiv.org/pdf/2505.06087)
*Sergio García-Heredia, Ángela Fernández, Carlos M. Alaíz*

Main category: cs.LG

TL;DR: The paper proposes a deep learning-based approach to improve Diffusion Maps for dimensionality reduction, addressing computational and scalability issues.


<details>
  <summary>Details</summary>
Motivation: Existing nonlinear dimensionality reduction methods like Diffusion Maps face challenges such as computational complexity, memory costs, and inability to generalize to new data.

Method: The authors reformulate Diffusion Maps as an unconstrained minimization problem and train a neural network to compute embeddings without spectral decomposition.

Result: The proposed method is evaluated on synthetic and real datasets, showing comparable performance to Diffusion Maps and the Nystrom method.

Conclusion: Deep learning offers a viable solution to enhance Diffusion Maps, improving scalability and generalization.

Abstract: One of the fundamental problems within the field of machine learning is
dimensionality reduction. Dimensionality reduction methods make it possible to
combat the so-called curse of dimensionality, visualize high-dimensional data
and, in general, improve the efficiency of storing and processing large data
sets. One of the best-known nonlinear dimensionality reduction methods is
Diffusion Maps. However, despite their virtues, both Diffusion Maps and many
other manifold learning methods based on the spectral decomposition of kernel
matrices have drawbacks such as the inability to apply them to data outside the
initial set, their computational complexity, and high memory costs for large
data sets. In this work, we propose to alleviate these problems by resorting to
deep learning. Specifically, a new formulation of Diffusion Maps embedding is
offered as a solution to a certain unconstrained minimization problem and,
based on it, a cost function to train a neural network which computes Diffusion
Maps embedding -- both inside and outside the training sample -- without the
need to perform any spectral decomposition. The capabilities of this approach
are compared on different data sets, both real and synthetic, with those of
Diffusion Maps and the Nystrom method.

</details>


### [229] [FIC-TSC: Learning Time Series Classification with Fisher Information Constraint](https://arxiv.org/pdf/2505.06114)
*Xiwen Chen, Wenhui Zhu, Peijie Qiu, Hao Wang, Huayu Li, Zihan Li, Yalin Wang, Aristeidis Sotiras, Abolfazl Razi*

Main category: cs.LG

TL;DR: Proposes FIC-TSC, a Fisher information-constrained framework for time series classification to address domain shifts, outperforming 14 state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Time series classification is vital but suffers from domain shifts, degrading performance. Existing methods like instance normalization fail for classification tasks.

Method: Introduces FIC-TSC, leveraging Fisher information to guide models toward flatter minima for better generalizability.

Result: Evaluated on 115 datasets (UEA and UCR), FIC-TSC outperforms 14 recent state-of-the-art methods.

Conclusion: FIC-TSC effectively addresses domain shifts in time series classification, enhancing model generalizability and performance.

Abstract: Analyzing time series data is crucial to a wide spectrum of applications,
including economics, online marketplaces, and human healthcare. In particular,
time series classification plays an indispensable role in segmenting different
phases in stock markets, predicting customer behavior, and classifying worker
actions and engagement levels. These aspects contribute significantly to the
advancement of automated decision-making and system optimization in real-world
applications. However, there is a large consensus that time series data often
suffers from domain shifts between training and test sets, which dramatically
degrades the classification performance. Despite the success of (reversible)
instance normalization in handling the domain shifts for time series regression
tasks, its performance in classification is unsatisfactory. In this paper, we
propose \textit{FIC-TSC}, a training framework for time series classification
that leverages Fisher information as the constraint. We theoretically and
empirically show this is an efficient and effective solution to guide the model
converge toward flatter minima, which enhances its generalizability to
distribution shifts. We rigorously evaluate our method on 30 UEA multivariate
and 85 UCR univariate datasets. Our empirical results demonstrate the
superiority of the proposed method over 14 recent state-of-the-art methods.

</details>


### [230] [Wasserstein Distances Made Explainable: Insights into Dataset Shifts and Transport Phenomena](https://arxiv.org/pdf/2505.06123)
*Philip Naumann, Jacob Kauffmann, Grégoire Montavon*

Main category: cs.LG

TL;DR: A novel Explainable AI method is proposed to attribute Wasserstein distances to data components, enhancing interpretability.


<details>
  <summary>Details</summary>
Motivation: Understanding factors contributing to high or low Wasserstein distances is challenging with current methods.

Method: Proposes an Explainable AI-based approach to attribute Wasserstein distances to data subgroups, features, or subspaces.

Result: Achieves high accuracy across diverse datasets and demonstrates practical utility in use cases.

Conclusion: The method enhances interpretability and utility of Wasserstein distance analysis.

Abstract: Wasserstein distances provide a powerful framework for comparing data
distributions. They can be used to analyze processes over time or to detect
inhomogeneities within data. However, simply calculating the Wasserstein
distance or analyzing the corresponding transport map (or coupling) may not be
sufficient for understanding what factors contribute to a high or low
Wasserstein distance. In this work, we propose a novel solution based on
Explainable AI that allows us to efficiently and accurately attribute
Wasserstein distances to various data components, including data subgroups,
input features, or interpretable subspaces. Our method achieves high accuracy
across diverse datasets and Wasserstein distance specifications, and its
practical utility is demonstrated in two use cases.

</details>


### [231] [Realistic Adversarial Attacks for Robustness Evaluation of Trajectory Prediction Models via Future State Perturbation](https://arxiv.org/pdf/2505.06134)
*Julian F. Schumann, Jeroen Hagenus, Frederik Baymler Mathiesen, Arkady Zgonnikov*

Main category: cs.LG

TL;DR: The paper proposes a novel adversarial attack method for trajectory prediction models in autonomous vehicles, focusing on perturbing both past and future states to uncover hidden vulnerabilities and improve robustness evaluation.


<details>
  <summary>Details</summary>
Motivation: Current adversarial testing methods for trajectory prediction models focus only on perturbing past positions, leading to unrealistic scenarios and overly optimistic assessments of model reliability.

Method: The authors introduce a method that perturbs both past and future states of adversarial agents, incorporating dynamic constraints and preserving tactical behaviors for realistic attacks. New performance measures are also introduced.

Result: Testing revealed significant increases in prediction errors and collision rates under adversarial conditions, exposing critical weaknesses like undetected potential collisions.

Conclusion: The findings highlight the necessity of more comprehensive adversarial testing to enhance the reliability of trajectory prediction models for autonomous vehicles.

Abstract: Trajectory prediction is a key element of autonomous vehicle systems,
enabling them to anticipate and react to the movements of other road users.
Evaluating the robustness of prediction models against adversarial attacks is
essential to ensure their reliability in real-world traffic. However, current
approaches tend to focus on perturbing the past positions of surrounding
agents, which can generate unrealistic scenarios and overlook critical
vulnerabilities. This limitation may result in overly optimistic assessments of
model performance in real-world conditions.
  In this work, we demonstrate that perturbing not just past but also future
states of adversarial agents can uncover previously undetected weaknesses and
thereby provide a more rigorous evaluation of model robustness. Our novel
approach incorporates dynamic constraints and preserves tactical behaviors,
enabling more effective and realistic adversarial attacks. We introduce new
performance measures to assess the realism and impact of these adversarial
trajectories. Testing our method on a state-of-the-art prediction model
revealed significant increases in prediction errors and collision rates under
adversarial conditions. Qualitative analysis further showed that our attacks
can expose critical weaknesses, such as the inability of the model to detect
potential collisions in what appear to be safe predictions. These results
underscore the need for more comprehensive adversarial testing to better
evaluate and improve the reliability of trajectory prediction models for
autonomous vehicles.

</details>


### [232] [On the Depth of Monotone ReLU Neural Networks and ICNNs](https://arxiv.org/pdf/2505.06169)
*Egor Bakaev, Florestan Brunck, Christoph Hertrich, Daniel Reichman, Amir Yehudayoff*

Main category: cs.LG

TL;DR: The paper explores the expressivity of ReLU$^+$ and ICNN models, proving lower bounds on depth for computing the maximum function and showing depth separations between ReLU networks and ICNNs.


<details>
  <summary>Details</summary>
Motivation: To understand the limitations and capabilities of monotone networks (ReLU$^+) and input convex neural networks (ICNNs) in terms of depth and expressivity.

Method: The study uses proofs based on polyhedral geometry and isoperimetric properties of triangulations to analyze the depth complexity and separations.

Result: ReLU$^+$ networks cannot compute or approximate MAX$_n$, and a sharp $n$ lower bound is proven for ICNN depth complexity of MAX$_n$. Depth separations between ReLU networks and ICNNs are also established.

Conclusion: The findings highlight fundamental differences in expressivity and depth requirements between ReLU$^+$ networks and ICNNs, with implications for their practical applications.

Abstract: We study two models of ReLU neural networks: monotone networks (ReLU$^+$) and
input convex neural networks (ICNN). Our focus is on expressivity, mostly in
terms of depth, and we prove the following lower bounds. For the maximum
function MAX$_n$ computing the maximum of $n$ real numbers, we show that
ReLU$^+$ networks cannot compute MAX$_n$, or even approximate it. We prove a
sharp $n$ lower bound on the ICNN depth complexity of MAX$_n$. We also prove
depth separations between ReLU networks and ICNNs; for every $k$, there is a
depth-2 ReLU network of size $O(k^2)$ that cannot be simulated by a depth-$k$
ICNN. The proofs are based on deep connections between neural networks and
polyhedral geometry, and also use isoperimetric properties of triangulations.

</details>


### [233] [A Large Language Model-Enhanced Q-learning for Capacitated Vehicle Routing Problem with Time Windows](https://arxiv.org/pdf/2505.06178)
*Linjiang Cao, Maonan Wang, Xi Xiong*

Main category: cs.LG

TL;DR: A novel LLM-enhanced Q-learning framework is proposed for solving CVRPTW with real-time emergency constraints, achieving a 7.3% cost reduction over traditional Q-learning.


<details>
  <summary>Details</summary>
Motivation: The complexity of CVRPTW due to vehicle capacity and time window constraints, combined with the potential of LLMs, motivates a new approach.

Method: An adaptive two-phase training mechanism (LLM-guided exploration to autonomous Q-network optimization) with a three-tier self-correction (syntactic, semantic, physical) and prioritized replay of LLM-generated experiences.

Result: 7.3% average cost reduction compared to traditional Q-learning, with faster convergence.

Conclusion: The LLM-enhanced Q-learning framework effectively addresses CVRPTW, demonstrating superior performance and efficiency.

Abstract: The Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) is a
classic NP-hard combinatorial optimization problem widely applied in logistics
distribution and transportation management. Its complexity stems from the
constraints of vehicle capacity and time windows, which pose significant
challenges to traditional approaches. Advances in Large Language Models (LLMs)
provide new possibilities for finding approximate solutions to CVRPTW. This
paper proposes a novel LLM-enhanced Q-learning framework to address the CVRPTW
with real-time emergency constraints. Our solution introduces an adaptive
two-phase training mechanism that transitions from the LLM-guided exploration
phase to the autonomous optimization phase of Q-network. To ensure reliability,
we design a three-tier self-correction mechanism based on the Chain-of-Thought
(CoT) for LLMs: syntactic validation, semantic verification, and physical
constraint enforcement. In addition, we also prioritized replay of the
experience generated by LLMs to amplify the regulatory role of LLMs in the
architecture. Experimental results demonstrate that our framework achieves a
7.3\% average reduction in cost compared to traditional Q-learning, with fewer
training steps required for convergence.

</details>


### [234] [Brain Hematoma Marker Recognition Using Multitask Learning: SwinTransformer and Swin-Unet](https://arxiv.org/pdf/2505.06185)
*Kodai Hirata, Tsuyoshi Okita*

Main category: cs.LG

TL;DR: MTL-Swin-Unet uses multi-task learning with transformers for classification and semantic segmentation, improving performance in F-value and AUC measures under different test conditions.


<details>
  <summary>Details</summary>
Motivation: Address spurious-correlation problems by enhancing image representation with semantic segmentation and reconstruction representations.

Method: Multi-task learning with transformers, combining classification and semantic segmentation, and leveraging additional image representations.

Result: Outperformed other classifiers in F-value (no covariate shift) and AUC (covariate shift).

Conclusion: MTL-Swin-Unet effectively improves performance in both stable and shifted test scenarios.

Abstract: This paper proposes a method MTL-Swin-Unet which is multi-task learning using
transformers for classification and semantic segmentation. For
spurious-correlation problems, this method allows us to enhance the image
representation with two other image representations: representation obtained by
semantic segmentation and representation obtained by image reconstruction. In
our experiments, the proposed method outperformed in F-value measure than other
classifiers when the test data included slices from the same patient (no
covariate shift). Similarly, when the test data did not include slices from the
same patient (covariate shift setting), the proposed method outperformed in AUC
measure.

</details>


### [235] [Auto Tensor Singular Value Thresholding: A Non-Iterative and Rank-Free Framework for Tensor Denoising](https://arxiv.org/pdf/2505.06203)
*Hiroki Hasegawa, Yukihiko Okada*

Main category: cs.LG

TL;DR: A novel tensor-based denoising method using singular value thresholding avoids pre-specified ranks and iterative optimization, outperforming existing techniques in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Mitigating noise in high-dimensional data is challenging for conventional matrix-based methods, prompting interest in tensor-based approaches.

Method: Proposes a low-rank tensor approximation using singular value thresholding on mode-wise matricizations, eliminating the need for rank specification or iterative refinement.

Result: Outperforms existing methods in accuracy and computational efficiency, especially in noisy, high-dimensional settings.

Conclusion: The method offers a practical and efficient solution for tensor denoising, addressing limitations of classical approaches.

Abstract: In modern data-driven tasks such as classification, optimization, and
forecasting, mitigating the effects of intrinsic noise is crucial for improving
predictive accuracy. While numerous denoising techniques have been developed,
the rising dimensionality of real-world datasets limits conventional
matrix-based methods in preserving data structure and accuracy. This challenge
has led to increasing interest in tensor-based approaches, which naturally
capture multi-way data relationships. However, classical tensor decomposition
methods (e.g., HOSVD, HOOI) typically require pre-specified ranks and iterative
optimization, making them computationally expensive and less practical. In this
work, we propose a novel low-rank approximation method for tensor data that
avoids these limitations. Our approach applies statistically grounded singular
value thresholding to mode-wise matricizations, enabling automatic extraction
of significant components without requiring prior rank specification or
iterative refinement. Experiments on synthetic and real-world tensors show that
our method consistently outperforms existing techniques in terms of estimation
accuracy and computational efficiency, especially in noisy high-dimensional
settings.

</details>


### [236] [Towards a Unified Representation Evaluation Framework Beyond Downstream Tasks](https://arxiv.org/pdf/2505.06224)
*Christos Plachouras, Julien Guinot, George Fazekas, Elio Quinton, Emmanouil Benetos, Johan Pauwels*

Main category: cs.LG

TL;DR: The paper critiques downstream probing as incomplete for evaluating model representations and introduces a unified framework to assess attributes like equivariance, invariance, and disentanglement.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods like downstream probing overlook key representation attributes, limiting understanding of model behavior and utility.

Method: A standardized protocol is proposed to quantify informativeness, equivariance, invariance, and disentanglement in model representations, tested across image and speech domains.

Result: Representations with similar downstream performance exhibit significant differences in these attributes, suggesting varied underlying mechanisms.

Conclusion: The findings highlight the need for broader evaluation metrics and inspire research to enhance representation quality.

Abstract: Downstream probing has been the dominant method for evaluating model
representations, an important process given the increasing prominence of
self-supervised learning and foundation models. However, downstream probing
primarily assesses the availability of task-relevant information in the model's
latent space, overlooking attributes such as equivariance, invariance, and
disentanglement, which contribute to the interpretability, adaptability, and
utility of representations in real-world applications. While some attempts have
been made to measure these qualities in representations, no unified evaluation
framework with modular, generalizable, and interpretable metrics exists.
  In this paper, we argue for the importance of representation evaluation
beyond downstream probing. We introduce a standardized protocol to quantify
informativeness, equivariance, invariance, and disentanglement of factors of
variation in model representations. We use it to evaluate representations from
a variety of models in the image and speech domains using different
architectures and pretraining approaches on identified controllable factors of
variation. We find that representations from models with similar downstream
performance can behave substantially differently with regard to these
attributes. This hints that the respective mechanisms underlying their
downstream performance are functionally different, prompting new research
directions to understand and improve representations.

</details>


### [237] [HORAE: A Domain-Agnostic Language for Automated Service Regulation](https://arxiv.org/pdf/2406.06600)
*Yutao Sun, Mingshuai Chen, Tiancheng Zhao, Kangjia Zhao, He Li, Jintao Chen, Zhongyi Wang, Liqiang Lu, Xinkui Zhao, Shuiguang Deng, Jianwei Yin*

Main category: cs.LG

TL;DR: Horae is a unified language for modeling regulation rules across domains, paired with RuleGPT, a fine-tuned LLM, to automate the process, outperforming GPT-3.5 and matching GPT-4.


<details>
  <summary>Details</summary>
Motivation: Existing AI-based regulation techniques are domain-specific and lack generalizability, prompting the need for a unified solution.

Method: Developed Horae, a specification language, and RuleGPT, a fine-tuned LLM, to automate rule modeling and regulation.

Result: Demonstrated feasibility and effectiveness across real-world domains; RuleGPT (7B) outperforms GPT-3.5 and matches GPT-4.

Conclusion: Horae and RuleGPT provide a scalable, automated framework for intelligent service regulation.

Abstract: Artificial intelligence is rapidly encroaching on the field of service
regulation. However, existing AI-based regulation techniques are often tailored
to specific application domains and thus are difficult to generalize in an
automated manner. This paper presents Horae, a unified specification language
for modeling (multimodal) regulation rules across a diverse set of domains. We
showcase how Horae facilitates an intelligent service regulation pipeline by
further exploiting a fine-tuned large language model named RuleGPT that
automates the Horae modeling process, thereby yielding an end-to-end framework
for fully automated intelligent service regulation. The feasibility and
effectiveness of our framework are demonstrated over a benchmark of various
real-world regulation domains. In particular, we show that our open-sourced,
fine-tuned RuleGPT with 7B parameters suffices to outperform GPT-3.5 and
perform on par with GPT-4o.

</details>


### [238] [Recent Advances in Federated Learning Driven Large Language Models: A Survey on Architecture, Performance, and Security](https://arxiv.org/pdf/2406.09831)
*Youyang Qu, Ming Liu, Tianqing Zhu, Longxiang Gao, Shui Yu, Wanlei Zhou*

Main category: cs.LG

TL;DR: A survey on Federated Learning (FL) for Large Language Models (LLMs), focusing on privacy, efficiency, and security, including machine unlearning techniques.


<details>
  <summary>Details</summary>
Motivation: To explore FL-driven LLMs for decentralized training while addressing privacy, performance, and regulatory compliance (e.g., Right to be Forgotten).

Method: Reviews architectural designs, optimization strategies, and unlearning methods (perturbation, model decomposition, incremental retraining).

Result: Evaluates trade-offs in efficiency, privacy, and utility through case studies, highlighting practical FL performance.

Conclusion: Identifies key research directions for secure, adaptable, and high-performing federated LLM systems.

Abstract: Federated Learning (FL) offers a promising paradigm for training Large
Language Models (LLMs) in a decentralized manner while preserving data privacy
and minimizing communication overhead. This survey examines recent advancements
in FL-driven LLMs, with a particular emphasis on architectural designs,
performance optimization, and security concerns, including the emerging area of
machine unlearning. In this context, machine unlearning refers to the
systematic removal of specific data contributions from trained models to comply
with privacy regulations such as the Right to be Forgotten. We review a range
of strategies enabling unlearning in federated LLMs, including
perturbation-based methods, model decomposition, and incremental retraining,
while evaluating their trade-offs in terms of efficiency, privacy guarantees,
and model utility. Through selected case studies and empirical evaluations, we
analyze how these methods perform in practical FL scenarios. This survey
identifies critical research directions toward developing secure, adaptable,
and high-performing federated LLM systems for real-world deployment.

</details>


### [239] [Bielik v3 Small: Technical Report](https://arxiv.org/pdf/2505.02550)
*Krzysztof Ociepa, Łukasz Flis, Remigiusz Kinas, Krzysztof Wróbel, Adrian Gwoździej*

Main category: cs.LG

TL;DR: Bielik v3 introduces parameter-efficient generative text models (1.5B and 4.5B) for Polish, achieving performance comparable to larger models with fewer resources. Innovations include a custom tokenizer, weighted loss, and adaptive learning.


<details>
  <summary>Details</summary>
Motivation: To create high-quality Polish language AI models that are resource-efficient and competitive with larger counterparts.

Method: Uses a custom Polish tokenizer (APT4), Weighted Instruction Cross-Entropy Loss, and Adaptive Learning Rate. Trained on 292B tokens from 303M documents.

Result: The 4.5B model competes with models 2-3x its size; the 1.5B model performs well despite its compact size. Excels in benchmarks like Open PL LLM Leaderboard.

Conclusion: Bielik v3 sets new benchmarks for parameter-efficient Polish language models, making high-quality AI more accessible for resource-limited applications.

Abstract: We introduce Bielik v3, a series of parameter-efficient generative text
models (1.5B and 4.5B) optimized for Polish language processing. These models
demonstrate that smaller, well-optimized architectures can achieve performance
comparable to much larger counterparts while requiring substantially fewer
computational resources. Our approach incorporates several key innovations: a
custom Polish tokenizer (APT4) that significantly improves token efficiency,
Weighted Instruction Cross-Entropy Loss to balance learning across instruction
types, and Adaptive Learning Rate that dynamically adjusts based on training
progress. Trained on a meticulously curated corpus of 292 billion tokens
spanning 303 million documents, these models excel across multiple benchmarks,
including the Open PL LLM Leaderboard, Complex Polish Text Understanding
Benchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter
model achieves results competitive with models 2-3 times its size, while the
1.5B model delivers strong performance despite its extremely compact profile.
These advances establish new benchmarks for parameter-efficient language
modeling in less-represented languages, making high-quality Polish language AI
more accessible for resource-constrained applications.

</details>


### [240] [An Invitation to Deep Reinforcement Learning](https://arxiv.org/pdf/2312.08365)
*Bernhard Jaeger, Andreas Geiger*

Main category: cs.LG

TL;DR: The paper introduces reinforcement learning (RL) as a generalization of supervised learning for optimizing non-differentiable objectives, making it accessible to a broader machine learning audience.


<details>
  <summary>Details</summary>
Motivation: Many machine learning objectives (e.g., IoU, BLEU, rewards) are non-differentiable, making supervised learning suboptimal. RL offers a promising alternative.

Method: The tutorial simplifies RL by framing it as an extension of supervised learning, avoiding complex theoretical presentations and focusing on practical applications.

Result: Readers with basic supervised learning knowledge can understand advanced RL algorithms like PPO.

Conclusion: The paper bridges the gap between supervised learning and RL, making the latter more approachable for optimizing non-differentiable objectives.

Abstract: Training a deep neural network to maximize a target objective has become the
standard recipe for successful machine learning over the last decade. These
networks can be optimized with supervised learning, if the target objective is
differentiable. For many interesting problems, this is however not the case.
Common objectives like intersection over union (IoU), bilingual evaluation
understudy (BLEU) score or rewards cannot be optimized with supervised
learning. A common workaround is to define differentiable surrogate losses,
leading to suboptimal solutions with respect to the actual objective.
Reinforcement learning (RL) has emerged as a promising alternative for
optimizing deep neural networks to maximize non-differentiable objectives in
recent years. Examples include aligning large language models via human
feedback, code generation, object detection or control problems. This makes RL
techniques relevant to the larger machine learning audience. The subject is,
however, time intensive to approach due to the large range of methods, as well
as the often very theoretical presentation. In this introduction, we take an
alternative approach, different from classic reinforcement learning textbooks.
Rather than focusing on tabular problems, we introduce reinforcement learning
as a generalization of supervised learning, which we first apply to
non-differentiable objectives and later to temporal problems. Assuming only
basic knowledge of supervised learning, the reader will be able to understand
state-of-the-art deep RL algorithms like proximal policy optimization (PPO)
after reading this tutorial.

</details>


### [241] [Credal Wrapper of Model Averaging for Uncertainty Estimation in Classification](https://arxiv.org/pdf/2405.15047)
*Kaizheng Wang, Fabio Cuzzolin, Keivan Shariatmadar, David Moens, Hans Hallez*

Main category: cs.LG

TL;DR: The paper introduces a 'credal wrapper' method to improve uncertainty estimation in Bayesian neural networks and deep ensembles by representing model averaging as a credal set with upper and lower probability bounds.


<details>
  <summary>Details</summary>
Motivation: To address epistemic uncertainty in classification tasks due to limited predictive distributions, enhancing uncertainty estimation and calibration.

Method: The credal wrapper extracts probability bounds per class, maps them to a credal set, and uses an intersection probability transformation for unique predictions.

Result: Outperforms BNN and DE baselines in uncertainty estimation and achieves lower expected calibration error on corrupted data across multiple benchmarks.

Conclusion: The credal wrapper method effectively improves uncertainty estimation and calibration in classification tasks.

Abstract: This paper presents an innovative approach, called credal wrapper, to
formulating a credal set representation of model averaging for Bayesian neural
networks (BNNs) and deep ensembles (DEs), capable of improving uncertainty
estimation in classification tasks. Given a finite collection of single
predictive distributions derived from BNNs or DEs, the proposed credal wrapper
approach extracts an upper and a lower probability bound per class,
acknowledging the epistemic uncertainty due to the availability of a limited
amount of distributions. Such probability intervals over classes can be mapped
on a convex set of probabilities (a credal set) from which, in turn, a unique
prediction can be obtained using a transformation called intersection
probability transformation. In this article, we conduct extensive experiments
on several out-of-distribution (OOD) detection benchmarks, encompassing various
dataset pairs (CIFAR10/100 vs SVHN/Tiny-ImageNet, CIFAR10 vs CIFAR10-C,
CIFAR100 vs CIFAR100-C and ImageNet vs ImageNet-O) and using different network
architectures (such as VGG16, ResNet-18/50, EfficientNet B2, and ViT Base).
Compared to the BNN and DE baselines, the proposed credal wrapper method
exhibits superior performance in uncertainty estimation and achieves a lower
expected calibration error on corrupted data.

</details>


### [242] [Learning Algorithms Made Simple](https://arxiv.org/pdf/2410.09186)
*Noorbakhsh Amiri Golilarz, Elias Hossain, Abdoljalil Addeh, Keyan Alexander Rahimi*

Main category: cs.LG

TL;DR: The paper provides an overview of learning algorithms, their applications, and future directions, covering AI, ML, DL, and hybrid models, including CNNs and LLMs.


<details>
  <summary>Details</summary>
Motivation: To review and explain the importance of learning algorithms in various applications, their current state, and future potential.

Method: Discusses key concepts of AI, ML, DL, and hybrid models, including supervised, unsupervised, and reinforcement learning, and explores CNN architecture and integration with ML.

Result: Highlights the use of learning algorithms for tasks like prediction and classification, their vulnerability to noise, and integration with LLMs for domain-specific applications.

Conclusion: The paper summarizes the current state and future potential of learning algorithms, emphasizing their adaptability and dynamic evolution.

Abstract: In this paper, we discuss learning algorithms and their importance in
different types of applications which includes training to identify important
patterns and features in a straightforward, easy-to-understand manner. We will
review the main concepts of artificial intelligence (AI), machine learning
(ML), deep learning (DL), and hybrid models. Some important subsets of Machine
Learning algorithms such as supervised, unsupervised, and reinforcement
learning are also discussed in this paper. These techniques can be used for
some important tasks like prediction, classification, and segmentation.
Convolutional Neural Networks (CNNs) are used for image and video processing
and many more applications. We dive into the architecture of CNNs and how to
integrate CNNs with ML algorithms to build hybrid models. This paper explores
the vulnerability of learning algorithms to noise, leading to
misclassification. We further discuss the integration of learning algorithms
with Large Language Models (LLM) to generate coherent responses applicable to
many domains such as healthcare, marketing, and finance by learning important
patterns from large volumes of data. Furthermore, we discuss the next
generation of learning algorithms and how we may have an unified Adaptive and
Dynamic Network to perform important tasks. Overall, this article provides
brief overview of learning algorithms, exploring their current state,
applications and future direction.

</details>


### [243] [From Models to Network Topologies: A Topology Inference Attack in Decentralized Federated Learning](https://arxiv.org/pdf/2501.03119)
*Chao Feng, Yuanzhe Gao, Alberto Huertas Celdran, Gerome Bovet, Burkhard Stiller*

Main category: cs.LG

TL;DR: The paper reveals a vulnerability in Decentralized Federated Learning (DFL) where attackers can infer participant relationships and launch targeted attacks by analyzing model behavior, highlighting critical privacy risks.


<details>
  <summary>Details</summary>
Motivation: To uncover hidden risks in DFL topologies, as they can be exploited to infer sensitive information despite FL's privacy-preserving nature.

Method: Proposes a Topology Inference Attack, categorizes attacks by attacker capabilities, designs practical attack strategies, and conducts experiments.

Result: Demonstrates that DFL topologies can be accurately inferred from model behavior, exposing a significant privacy risk.

Conclusion: The findings emphasize the need for improved privacy preservation in DFL systems.

Abstract: Federated Learning (FL) is widely recognized as a privacy-preserving machine
learning paradigm due to its model-sharing mechanism that avoids direct data
exchange. Nevertheless, model training leaves exploitable traces that can be
used to infer sensitive information. In Decentralized FL (DFL), the topology,
defining how participants are connected, plays a crucial role in shaping the
model's privacy, robustness, and convergence. However, the topology introduces
an unexplored vulnerability: attackers can exploit it to infer participant
relationships and launch targeted attacks. This work uncovers the hidden risks
of DFL topologies by proposing a novel Topology Inference Attack that infers
the topology solely from model behavior. A taxonomy of topology inference
attacks is introduced, categorizing them by the attacker's capabilities and
knowledge. Practical attack strategies are designed for various scenarios, and
experiments are conducted to identify key factors influencing attack success.
The results demonstrate that analyzing only the model of each node can
accurately infer the DFL topology, highlighting a critical privacy risk in DFL
systems. These findings offer valuable insights for improving privacy
preservation in DFL environments.

</details>


### [244] [An Efficient Sparse Kernel Generator for O(3)-Equivariant Deep Networks](https://arxiv.org/pdf/2501.13986)
*Vivek Bharadwaj, Austin Glover, Aydin Buluc, James Demmel*

Main category: cs.LG

TL;DR: A GPU sparse kernel generator for the Clebsch-Gordon (CG) tensor product in rotation equivariant graph neural networks achieves significant speedups over existing implementations, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: The CG tensor product is a costly bottleneck in rotation equivariant graph neural networks, limiting efficiency and performance.

Method: Introduces a GPU sparse kernel generator that optimizes memory usage, minimizes global memory traffic, and fuses operations for efficiency.

Result: Achieves up to 1.3x speedup over NVIDIA's cuEquivariance and 10x over e3nn, with a 6.2x inference-time speedup for MACE in FP64 precision.

Conclusion: The optimized CG tensor product implementation significantly enhances performance and efficiency for spatial deep learning tasks.

Abstract: Rotation equivariant graph neural networks, i.e. networks designed to
guarantee certain geometric relations between their inputs and outputs, yield
state of the art performance on spatial deep learning tasks. They exhibit high
data efficiency during training and significantly reduced inference time for
interatomic potential calculations compared to classical approaches. Key to
these models is the Clebsch-Gordon (CG) tensor product, a kernel that contracts
two dense feature vectors with a highly-structured sparse tensor to produce a
dense output vector. The operation, which may be repeated millions of times for
typical equivariant models, is a costly and inefficient bottleneck. We
introduce a GPU sparse kernel generator for the CG tensor product that provides
significant speedups over the best existing open and closed-source
implementations. Our implementation achieves high performance by carefully
managing the limited GPU shared memory through static analysis at model
compile-time, minimizing reads and writes to global memory. We break the tensor
product into a series of smaller kernels with operands that fit entirely into
registers, enabling us to emit long arithmetic instruction streams that
maximize instruction-level parallelism. By fusing the CG tensor product with a
subsequent graph convolution, we reduce both intermediate storage and global
memory traffic over naive approaches that duplicate input data. We also provide
optimized kernels for the gradient of the CG tensor product and a novel
identity for the higher partial derivatives required to predict interatomic
forces. Our kernels offer up to 1.3x speedup over NVIDIA's closed-source
cuEquivariance package, as well as 10x speedup over the widely-used e3nn
package. In FP64 precision, we offer up to 6.2x inference-time speedup for the
MACE chemistry foundation model over the original unoptimized version.

</details>


### [245] [Privacy-Preserved Automated Scoring using Federated Learning for Educational Research](https://arxiv.org/pdf/2503.11711)
*Ehsan Latif, Xiaoming Zhai*

Main category: cs.LG

TL;DR: A federated learning framework using LoRA for secure, automated scoring of educational assessments, achieving high accuracy and preserving data privacy.


<details>
  <summary>Details</summary>
Motivation: Addressing data privacy concerns in educational research by avoiding raw data sharing and reducing security risks.

Method: Proposes a federated learning framework with LoRA for local training and adaptive weighted aggregation to handle data heterogeneity.

Result: Achieves 94.5% accuracy, close to centralized models, with minimal rubric-level scoring differences.

Conclusion: The framework effectively balances privacy, accuracy, and interpretability in educational assessment scoring.

Abstract: Data privacy remains a critical concern in educational research, requiring
strict adherence to ethical standards and regulatory protocols. While
traditional approaches rely on anonymization and centralized data collection,
they often expose raw student data to security vulnerabilities and impose
substantial logistical overhead. In this study, we propose a federated learning
(FL) framework for automated scoring of educational assessments that eliminates
the need to share sensitive data across institutions. Our approach leverages
parameter-efficient fine-tuning of large language models (LLMs) with Low-Rank
Adaptation (LoRA), enabling each client (school) to train locally while sharing
only optimized model updates. To address data heterogeneity, we implement an
adaptive weighted aggregation strategy that considers both client performance
and data volume. We benchmark our model against two state-of-the-art FL methods
and a centralized learning baseline using NGSS-aligned multi-label science
assessment data from nine middle schools. Results show that our model achieves
the highest accuracy (94.5%) among FL approaches, and performs within 0.5-1.0
percentage points of the centralized model on these metrics. Additionally, it
achieves comparable rubric-level scoring accuracy, with only a 1.3% difference
in rubric match and a lower score deviation (MAE), highlighting its
effectiveness in preserving both prediction quality and interpretability.

</details>


### [246] [Uncovering Model Processing Strategies with Non-Negative Per-Example Fisher Factorization](https://arxiv.org/pdf/2310.04649)
*Michael Matena, Colin Raffel*

Main category: cs.LG

TL;DR: NPEFF is a method to interpret model predictions by decomposing per-example Fisher matrices into rank-1 components, revealing model strategies and enabling targeted parameter perturbations.


<details>
  <summary>Details</summary>
Motivation: To uncover and interpret the strategies models use for predictions, improving transparency and control over model behavior.

Method: Decomposes per-example Fisher matrices using a novel algorithm to learn rank-1 positive semi-definite components, validated via human and automated analysis.

Result: NPEFF components correspond to model strategies, enable selective disruption, and outperform baselines like gradient clustering and sparse autoencoders.

Conclusion: NPEFF provides a powerful tool for analyzing and controlling model behavior, with applications in unlearning and in-context learning.

Abstract: We introduce NPEFF (Non-Negative Per-Example Fisher Factorization), an
interpretability method that aims to uncover strategies used by a model to
generate its predictions. NPEFF decomposes per-example Fisher matrices using a
novel decomposition algorithm that learns a set of components represented by
learned rank-1 positive semi-definite matrices. Through a combination of human
evaluation and automated analysis, we demonstrate that these NPEFF components
correspond to model processing strategies for a variety of language models and
text processing tasks. We further show how to construct parameter perturbations
from NPEFF components to selectively disrupt a given component's role in the
model's processing. Along with conducting extensive ablation studies, we
include experiments to show how NPEFF can be used to analyze and mitigate
collateral effects of unlearning and use NPEFF to study in-context learning.
Furthermore, we demonstrate the advantages of NPEFF over baselines such as
gradient clustering and using sparse autoencoders for dictionary learning over
model activations.

</details>


### [247] [Bridging Lottery Ticket and Grokking: Understanding Grokking from Inner Structure of Networks](https://arxiv.org/pdf/2310.19470)
*Gouki Minegishi, Yusuke Iwasawa, Yutaka Matsuo*

Main category: cs.LG

TL;DR: The paper explores grokking (delayed generalization in neural networks) by linking it to the lottery ticket hypothesis, showing that grokked tickets reduce delayed generalization and highlighting the role of network structure.


<details>
  <summary>Details</summary>
Motivation: To investigate the underexplored influence of network structure on the grokking phenomenon and its connection to the lottery ticket hypothesis.

Method: Utilized grokked tickets (lottery tickets from the generalizing phase) and conducted experiments on tasks like modular arithmetic, polynomial regression, and MNIST classification. Analyzed weight patterns, graph properties, and pruning techniques.

Result: Grokked tickets reduce delayed generalization, exhibit periodic weight patterns, beneficial graph properties, and undergo rapid structural changes. Pruning techniques can identify effective structures without weight modification.

Conclusion: Structural exploration is pivotal in understanding grokking, and grokked tickets offer a pathway to mitigate delayed generalization.

Abstract: Grokking is an intriguing phenomenon of delayed generalization, where neural
networks initially memorize training data with perfect accuracy but exhibit
poor generalization, subsequently transitioning to a generalizing solution with
continued training. While factors such as weight norms and sparsity have been
proposed to explain this delayed generalization, the influence of network
structure remains underexplored. In this work, we link the grokking phenomenon
to the lottery ticket hypothesis to investigate the impact of internal network
structures. We demonstrate that utilizing lottery tickets obtained during the
generalizing phase (termed grokked tickets) significantly reduces delayed
generalization across various tasks, including multiple modular arithmetic
operations, polynomial regression, sparse parity, and MNIST classification.
Through controlled experiments, we show that the mitigation of delayed
generalization is not due solely to reduced weight norms or increased sparsity,
but rather to the discovery of good subnetworks. Furthermore, we find that
grokked tickets exhibit periodic weight patterns, beneficial graph properties
such as increased average path lengths and reduced clustering coefficients, and
undergo rapid structural changes that coincide with improvements in
generalization. Additionally, pruning techniques like the edge-popup algorithm
can identify these effective structures without modifying the weights, thereby
transforming memorizing networks into generalizing ones. These results
underscore the novel insight that structural exploration plays a pivotal role
in understanding grokking. The implementation code can be accessed via this
link: https://github.com/gouki510/Grokking-Tickets.

</details>


### [248] [Unsupervised Multi-modal Feature Alignment for Time Series Representation Learning](https://arxiv.org/pdf/2312.05698)
*Chen Liang, Donghua Yang, Zhiyu Liang, Hongzhi Wang, Zheng Liang, Xiyang Zhang, Jianfeng Huang*

Main category: cs.LG

TL;DR: The paper introduces a novel unsupervised representation learning method for time series data, focusing on aligning multi-modal features to improve scalability and performance.


<details>
  <summary>Details</summary>
Motivation: Unsupervised learning for time series lacks alignment with downstream tasks, and existing feature fusion methods are complex and unscalable.

Method: The approach aligns and binds multi-modal time series representations using spectral graph theory, simplifying the neural architecture to a single encoder.

Result: The method outperforms state-of-the-art techniques on diverse datasets and downstream tasks.

Conclusion: The proposed approach enhances scalability and performance by uncovering latent pattern associations in multi-modal features.

Abstract: In recent times, the field of unsupervised representation learning (URL) for
time series data has garnered significant interest due to its remarkable
adaptability across diverse downstream applications. Unsupervised learning
goals differ from downstream tasks, making it tricky to ensure downstream task
utility by focusing only on temporal feature characterization. Researchers have
proposed multiple transformations to extract discriminative patterns implied in
informative time series, trying to fill the gap. Despite the introduction of a
variety of feature engineering techniques, e.g. spectral domain, wavelet
transformed features, features in image form and symbolic features etc. the
utilization of intricate feature fusion methods and dependence on heterogeneous
features during inference hampers the scalability of the solutions. To address
this, our study introduces an innovative approach that focuses on aligning and
binding time series representations encoded from different modalities, inspired
by spectral graph theory, thereby guiding the neural encoder to uncover latent
pattern associations among these multi-modal features. In contrast to
conventional methods that fuse features from multiple modalities, our proposed
approach simplifies the neural architecture by retaining a single time series
encoder, consequently leading to preserved scalability. We further demonstrate
and prove mechanisms for the encoder to maintain better inductive bias. In our
experimental evaluation, we validated the proposed method on a diverse set of
time series datasets from various domains. Our approach outperforms existing
state-of-the-art URL methods across diverse downstream tasks.

</details>


### [249] [MotherNet: Fast Training and Inference via Hyper-Network Transformers](https://arxiv.org/pdf/2312.08598)
*Andreas Müller, Carlo Curino, Raghu Ramakrishnan*

Main category: cs.LG

TL;DR: MotherNet, a hypernetwork architecture, generates trained neural networks for tabular data classification in one forward pass, outperforming gradient descent on small datasets and matching TabPFN and Gradient Boosting, while being efficient at inference.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of existing meta-learning approaches for tabular data and the need for dataset-specific training, MotherNet aims to enable in-context learning for arbitrary tabular datasets without gradient descent.

Method: MotherNet is trained on synthetic tasks to generate child neural networks via in-context learning in a single forward pass, eliminating the need for dataset-specific gradient descent or hyperparameter tuning.

Result: MotherNet outperforms gradient descent-trained networks on small datasets, matches TabPFN and Gradient Boosting, and is highly efficient at inference without fine-tuning.

Conclusion: MotherNet provides a scalable and efficient solution for tabular data classification, surpassing traditional methods and avoiding the pitfalls of existing hypernetworks.

Abstract: Foundation models are transforming machine learning across many modalities,
with in-context learning replacing classical model training. Recent work on
tabular data hints at a similar opportunity to build foundation models for
classification for numerical data. However, existing meta-learning approaches
can not compete with tree-based methods in terms of inference time. In this
paper, we propose MotherNet, a hypernetwork architecture trained on synthetic
classification tasks that, once prompted with a never-seen-before training set
generates the weights of a trained ``child'' neural-network by in-context
learning using a single forward pass. In contrast to most existing
hypernetworks that are usually trained for relatively constrained multi-task
settings, MotherNet can create models for multiclass classification on
arbitrary tabular datasets without any dataset specific gradient descent. The
child network generated by MotherNet outperforms neural networks trained using
gradient descent on small datasets, and is comparable to predictions by TabPFN
and standard ML methods like Gradient Boosting. Unlike a direct application of
TabPFN, MotherNet generated networks are highly efficient at inference time. We
also demonstrate that HyperFast is unable to perform effective in-context
learning on small datasets, and heavily relies on dataset specific fine-tuning
and hyper-parameter tuning, while MotherNet requires no fine-tuning or
per-dataset hyper-parameters.

</details>


### [250] [Adaptive Gradient Clipping for Robust Federated Learning](https://arxiv.org/pdf/2405.14432)
*Youssef Allouah, Rachid Guerraoui, Nirupam Gupta, Ahmed Jellouli, Geovani Rizk, John Stephan*

Main category: cs.LG

TL;DR: Proposes Adaptive Robust Clipping (ARC) for federated learning to dynamically adjust clipping thresholds, improving robustness and convergence.


<details>
  <summary>Details</summary>
Motivation: Existing static clipping strategies in robust federated learning yield inconsistent results against adversarial attacks.

Method: Introduces ARC, an adaptive clipping strategy that adjusts thresholds based on input gradients.

Result: ARC preserves theoretical robustness guarantees and improves asymptotic convergence, validated by experiments.

Conclusion: ARC enhances robustness in heterogeneous and adversarial settings, outperforming static methods.

Abstract: Robust federated learning aims to maintain reliable performance despite the
presence of adversarial or misbehaving workers. While state-of-the-art (SOTA)
robust distributed gradient descent (Robust-DGD) methods were proven
theoretically optimal, their empirical success has often relied on
pre-aggregation gradient clipping. However, existing static clipping strategies
yield inconsistent results: enhancing robustness against some attacks while
being ineffective or even detrimental against others. To address this
limitation, we propose a principled adaptive clipping strategy, Adaptive Robust
Clipping (ARC), which dynamically adjusts clipping thresholds based on the
input gradients. We prove that ARC not only preserves the theoretical
robustness guarantees of SOTA Robust-DGD methods but also provably improves
asymptotic convergence when the model is well-initialized. Extensive
experiments on benchmark image classification tasks confirm these theoretical
insights, demonstrating that ARC significantly enhances robustness,
particularly in highly heterogeneous and adversarial settings.

</details>


### [251] [Pretraining with Random Noise for Fast and Robust Learning without Weight Transport](https://arxiv.org/pdf/2405.16731)
*Jeonghwan Cheon, Sang Wan Lee, Se-Bum Paik*

Main category: cs.LG

TL;DR: Pretraining neural networks with random noise improves learning efficiency and generalization without weight transport, matching forward and backward weights for faster convergence and better adaptability.


<details>
  <summary>Details</summary>
Motivation: To understand how spontaneous neural activity (resembling random noise) prepares the brain for learning and whether this can benefit machine learning algorithms.

Method: Used a neural network with feedback alignment, pretraining it with random noise before data training. Analyzed weight alignment, learning speed, and generalization.

Result: Random noise pretraining aligns weights for faster learning, reduces effective dimensionality, and improves generalization and adaptability to novel tasks.

Conclusion: Random noise pretraining with feedback alignment is an effective method for enhancing learning efficiency and generalization in neural networks.

Abstract: The brain prepares for learning even before interacting with the environment,
by refining and optimizing its structures through spontaneous neural activity
that resembles random noise. However, the mechanism of such a process has yet
to be thoroughly understood, and it is unclear whether this process can benefit
the algorithm of machine learning. Here, we study this issue using a neural
network with a feedback alignment algorithm, demonstrating that pretraining
neural networks with random noise increases the learning efficiency as well as
generalization abilities without weight transport. First, we found that random
noise training modifies forward weights to match backward synaptic feedback,
which is necessary for teaching errors by feedback alignment. As a result, a
network with pre-aligned weights learns notably faster than a network without
random noise training, even reaching a convergence speed comparable to that of
a backpropagation algorithm. Sequential training with both random noise and
data brings weights closer to synaptic feedback than training solely with data,
enabling more precise credit assignment and faster learning. We also found that
each readout probability approaches the chance level and that the effective
dimensionality of weights decreases in a network pretrained with random noise.
This pre-regularization allows the network to learn simple solutions of a low
rank, reducing the generalization loss during subsequent training. This also
enables the network robustly to generalize a novel, out-of-distribution
dataset. Lastly, we confirmed that random noise pretraining reduces the amount
of meta-loss, enhancing the network ability to adapt to various tasks. Overall,
our results suggest that random noise training with feedback alignment offers a
straightforward yet effective method of pretraining that facilitates quick and
reliable learning without weight transport.

</details>


### [252] [Medha: Efficiently Serving Multi-Million Context Length LLM Inference Requests Without Approximations](https://arxiv.org/pdf/2409.17264)
*Amey Agrawal, Haoran Qiu, Junda Chen, Íñigo Goiri, Chaojie Zhang, Rayyan Shahid, Ramachandran Ramjee, Alexey Tumanov, Esha Choukse*

Main category: cs.LG

TL;DR: Medha is a system for efficient long-context LLM inference, addressing challenges like HOL blocking and server fragmentation with adaptive prefill chunking, novel parallelism strategies, and input-length aware scheduling.


<details>
  <summary>Details</summary>
Motivation: Existing long-context inference methods are inefficient due to high input length variability, leading to poor resource utilization and latency.

Method: Medha uses adaptive prefill chunking, Sequence Pipeline Parallelism (SPP), KV-Cache Parallelism (KVP), and input-length aware scheduling.

Result: Medha scales beyond 10M tokens, reduces latency by 30x, and improves throughput by 5x compared to state-of-the-art systems.

Conclusion: Medha enables production-scale long-context inference with high performance across mixed-length workloads.

Abstract: As large language models (LLMs) handle increasingly longer contexts, serving
long inference requests of millions of tokens presents unique challenges. We
show that existing work for long context inference is largely based on
techniques from long context training, and does not handle the high variability
in input lengths during inference. This leads to inefficient resource
utilization, server fragmentation, and head-of-line (HOL) blocking.
  We present Medha, an end-to-end system for efficient long-context LLM
inference that addresses these challenges through fine-grained time sharing.
Medha introduces three key innovations: (1) the mechanism of adaptive prefill
chunking to help mitigate HOL blocking with preemption; (2) two new parallelism
strategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token
by pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower
time-peroutput-token by distributing decoding across servers; and (3) a novel
input-length aware least remaining slack scheduling to meet Service Level
Objectives (SLOs).
  Medha enables exact inference scaling beyond 10 million tokens, maintaining
high throughput and low latency across mixed-length workloads. Compared to
state-of-the-art systems, Medha reduces server fragmentation, cuts median
latency by up to 30x, and improves throughput by over 5x, delivering
production-scale long-context inference without compromising performance on
shorter requests.

</details>


### [253] [Distillation of Discrete Diffusion through Dimensional Correlations](https://arxiv.org/pdf/2410.08709)
*Satoshi Hayakawa, Yuhta Takida, Masaaki Imaizumi, Hiromi Wakaki, Yuki Mitsufuji*

Main category: cs.LG

TL;DR: The paper introduces 'mixture' models for discrete diffusion to address slow sampling by capturing dimensional correlations and proposes loss functions for distilling iterations of existing models.


<details>
  <summary>Details</summary>
Motivation: Discrete diffusion models suffer from slow sampling and struggle with dependencies between elements due to computational costs.

Method: Proposes mixture models for scalable discrete diffusion and loss functions to distill iterations of existing models.

Result: Effective distillation of pretrained discrete diffusion models in image and language domains.

Conclusion: The method successfully reduces sampling steps while maintaining performance, with code available for reproducibility.

Abstract: Diffusion models have demonstrated exceptional performances in various fields
of generative modeling, but suffer from slow sampling speed due to their
iterative nature. While this issue is being addressed in continuous domains,
discrete diffusion models face unique challenges, particularly in capturing
dependencies between elements (e.g., pixel relationships in image, sequential
dependencies in language) mainly due to the computational cost of processing
high-dimensional joint distributions. In this paper, (i) we propose "mixture"
models for discrete diffusion that are capable of treating dimensional
correlations while remaining scalable, and (ii) we provide a set of loss
functions for distilling the iterations of existing models. Two primary
theoretical insights underpin our approach: First, conventional models with
element-wise independence can well approximate the data distribution, but
essentially require {\it many sampling steps}. Second, our loss functions
enable the mixture models to distill such many-step conventional models into
just a few steps by learning the dimensional correlations. Our experimental
results show the effectiveness of the proposed method in distilling pretrained
discrete diffusion models across image and language domains. The code used in
the paper is available at https://github.com/sony/di4c .

</details>


### [254] [LEGO-Learn: Label-Efficient Graph Open-Set Learning](https://arxiv.org/pdf/2410.16386)
*Haoyan Xu, Kay Liu, Zhengtao Yao, Philip S. Yu, Mengyuan Li, Kaize Ding, Yue Zhao*

Main category: cs.LG

TL;DR: LEGO-Learn is a label-efficient framework for graph open-set learning, improving ID classification and OOD detection by selectively labeling informative nodes.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of training graph models for unseen classes with limited labeled data, crucial for real-world applications like finance and healthcare.

Method: Uses a GNN-based filter to exclude OOD nodes and K-Medoids to select informative ID nodes, with a C+1 classifier for balanced OOD removal and ID retention.

Result: Achieves up to 6.62% better ID classification accuracy and 7.49% higher AUROC for OOD detection on four datasets.

Conclusion: LEGO-Learn effectively balances label efficiency and performance, outperforming existing methods in open-set graph learning.

Abstract: How can we train graph-based models to recognize unseen classes while keeping
labeling costs low? Graph open-set learning (GOL) and out-of-distribution (OOD)
detection aim to address this challenge by training models that can accurately
classify known, in-distribution (ID) classes while identifying and handling
previously unseen classes during inference. It is critical for high-stakes,
real-world applications where models frequently encounter unexpected data,
including finance, security, and healthcare. However, current GOL methods
assume access to many labeled ID samples, which is unrealistic for large-scale
graphs due to high annotation costs. In this paper, we propose LEGO-Learn
(Label-Efficient Graph Open-set Learning), a novel framework that tackles
open-set node classification on graphs within a given label budget by selecting
the most informative ID nodes. LEGO-Learn employs a GNN-based filter to
identify and exclude potential OOD nodes and then select highly informative ID
nodes for labeling using the K-Medoids algorithm. To prevent the filter from
discarding valuable ID examples, we introduce a classifier that differentiates
between the C known ID classes and an additional class representing OOD nodes
(hence, a C+1 classifier). This classifier uses a weighted cross-entropy loss
to balance the removal of OOD nodes while retaining informative ID nodes.
Experimental results on four real-world datasets demonstrate that LEGO-Learn
significantly outperforms leading methods, with up to a 6.62% improvement in ID
classification accuracy and a 7.49% increase in AUROC for OOD detection.

</details>


### [255] [Structure-preserving contrastive learning for spatial time series](https://arxiv.org/pdf/2502.06380)
*Yiru Jiao, Sander van Cranenburgh, Simeon Calvert, Hans van Lint*

Main category: cs.LG

TL;DR: The paper introduces two structure-preserving regularisers for contrastive learning of spatial time series, enhancing model performance by preserving spatio-temporal similarities.


<details>
  <summary>Details</summary>
Motivation: Self-supervised learning for spatially characterised time series faces challenges in maintaining fine-grained spatio-temporal similarities in the latent space.

Method: Proposes two regularisers (topology and graph geometry preservation) and a dynamic weighting mechanism to balance contrastive learning and structure preservation.

Result: The method improves state-of-the-art task performances and preserves similarity structures effectively across tasks like traffic prediction.

Conclusion: Well-preserved similarity structures lead to more informative representations, benefiting neural networks for spatial or geographical time series data.

Abstract: The effectiveness of neural network models largely relies on learning
meaningful latent patterns from data, where self-supervised learning of
informative representations can enhance model performance and generalisability.
However, self-supervised representation learning for spatially characterised
time series, which are ubiquitous in transportation domain, poses unique
challenges due to the necessity of maintaining fine-grained spatio-temporal
similarities in the latent space. In this study, we introduce two
structure-preserving regularisers for the contrastive learning of spatial time
series: one regulariser preserves the topology of similarities between
instances, and the other preserves the graph geometry of similarities across
spatial and temporal dimensions. To balance the contrastive learning objective
and the need for structure preservation, we propose a dynamic weighting
mechanism that adaptively manages this trade-off and stabilises training. We
validate the proposed method through extensive experiments, including
multivariate time series classification to demonstrate its general
applicability, as well as macroscopic and microscopic traffic prediction to
highlight its particular usefulness in encoding traffic interactions. Across
all tasks, our method preserves the similarity structures more effectively and
improves state-of-the-art task performances. This method can be integrated with
an arbitrary neural network model and is particularly beneficial for time
series data with spatial or geographical features. Furthermore, our findings
suggest that well-preserved similarity structures in the latent space indicate
more informative and useful representations. This provides insights to design
more effective neural networks for data-driven transportation research. Our
code is made openly accessible with all resulting data at
https://github.com/yiru-jiao/spclt

</details>


### [256] [Prioritized Generative Replay](https://arxiv.org/pdf/2410.18082)
*Renhao Wang, Kevin Frans, Pieter Abbeel, Sergey Levine, Alexei A. Efros*

Main category: cs.LG

TL;DR: The paper proposes a prioritized, parametric memory system using generative models to improve sample efficiency in online reinforcement learning, reducing overfitting and enhancing performance.


<details>
  <summary>Details</summary>
Motivation: Uniform replay buffers in reinforcement learning are inefficient, and prioritization can lead to overfitting. The authors aim to improve this by leveraging generative models for densification and guided sampling.

Method: The approach uses conditional diffusion models and relevance functions (e.g., curiosity- or value-based metrics) to generate and prioritize useful transitions.

Result: The method improves performance and sample efficiency in state- and pixel-based domains, promotes diversity in transitions, and allows higher update-to-data ratios.

Conclusion: The proposed framework enhances online RL by mitigating overfitting and improving scalability, demonstrating the potential of generative models in reinforcement learning.

Abstract: Sample-efficient online reinforcement learning often uses replay buffers to
store experience for reuse when updating the value function. However, uniform
replay is inefficient, since certain classes of transitions can be more
relevant to learning. While prioritization of more useful samples is helpful,
this strategy can also lead to overfitting, as useful samples are likely to be
more rare. In this work, we instead propose a prioritized, parametric version
of an agent's memory, using generative models to capture online experience.
This paradigm enables (1) densification of past experience, with new
generations that benefit from the generative model's generalization capacity
and (2) guidance via a family of "relevance functions" that push these
generations towards more useful parts of an agent's acquired history. We show
this recipe can be instantiated using conditional diffusion models and simple
relevance functions such as curiosity- or value-based metrics. Our approach
consistently improves performance and sample efficiency in both state- and
pixel-based domains. We expose the mechanisms underlying these gains, showing
how guidance promotes diversity in our generated transitions and reduces
overfitting. We also showcase how our approach can train policies with even
higher update-to-data ratios than before, opening up avenues to better scale
online RL agents.

</details>


### [257] [Crash Severity Risk Modeling Strategies under Data Imbalance](https://arxiv.org/pdf/2412.02094)
*Abdullah Al Mamun, Abyad Enan, Debbie A. Indah, Judith Mwakalonge, Gurcan Comert, Mashrur Chowdhury*

Main category: cs.LG

TL;DR: The study evaluates crash severity prediction models for work zones with large vehicles, addressing data imbalance between low-severity (LS) and high-severity (HS) crashes. Discriminative Mutual Information (DMI) and specific models like CatBoost and LightGBM show promise for HS prediction, while balancing techniques improve overall performance.


<details>
  <summary>Details</summary>
Motivation: To improve crash severity prediction in work zones involving large vehicles, especially under imbalanced LS and HS crash data.

Method: Used crash data from South Carolina (2014-2018) to test statistical, machine learning, and deep learning models with feature selection (DMI) and data balancing techniques (e.g., NearMiss-1, RandomUnderSampler).

Result: DMI with gradient boosting models (e.g., CatBoost, LightGBM) excelled in HS prediction without balancing. Balancing techniques like NearMiss-1 improved HS recall, while others (e.g., RandomUnderSampler) balanced LS and HS metrics.

Conclusion: The study provides actionable insights for safety analysts to choose models, feature selection, and balancing techniques tailored to specific goals, enhancing work-zone crash severity prediction.

Abstract: This study investigates crash severity risk modeling strategies for work
zones involving large vehicles (i.e., trucks, buses, and vans) under crash data
imbalance between low-severity (LS) and high-severity (HS) crashes. We utilized
crash data involving large vehicles in South Carolina work zones from 2014 to
2018, which included four times more LS crashes than HS crashes. The objective
of this study is to evaluate the crash severity prediction performance of
various statistical, machine learning, and deep learning models under different
feature selection and data balancing techniques. Findings highlight a disparity
in LS and HS predictions, with lower accuracy for HS crashes due to class
imbalance and feature overlap. Discriminative Mutual Information (DMI) yields
the most effective feature set for predicting HS crashes without requiring data
balancing, particularly when paired with gradient boosting models and deep
neural networks such as CatBoost, NeuralNetTorch, XGBoost, and LightGBM. Data
balancing techniques such as NearMiss-1 maximize HS recall when combined with
DMI-selected features and certain models such as LightGBM, making them
well-suited for HS crash prediction. Conversely, RandomUnderSampler, HS Class
Weighting, and RandomOverSampler achieve more balanced performance, which is
defined as an equitable trade-off between LS and HS metrics, especially when
applied to NeuralNetTorch, NeuralNetFastAI, CatBoost, LightGBM, and Bayesian
Mixed Logit (BML) using merged feature sets or models without feature
selection. The insights from this study offer safety analysts guidance on
selecting models, feature selection, and data balancing techniques aligned with
specific safety goals, providing a robust foundation for enhancing work-zone
crash severity prediction.

</details>


### [258] [GDformer: Going Beyond Subsequence Isolation for Multivariate Time Series Anomaly Detection](https://arxiv.org/pdf/2501.18196)
*Qingxiang Liu, Chenghao Liu, Sheng Sun, Di Yao, Yuxuan Liang*

Main category: cs.LG

TL;DR: GDformer introduces a global dictionary-enhanced transformer for unsupervised anomaly detection in multivariate time series, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods for unsupervised anomaly detection in multivariate time series rely on isolated subsequences, lacking a unified series-level criterion.

Method: Proposes GDformer with a dictionary-based cross-attention mechanism to derive global representations and a similarity-based detection criterion. Introduces prototypes for compact detection boundaries.

Result: GDformer achieves state-of-the-art performance on five benchmark datasets and shows strong transferability.

Conclusion: GDformer provides an effective, unified approach for unsupervised anomaly detection in multivariate time series, with demonstrated transferability.

Abstract: Unsupervised anomaly detection of multivariate time series is a challenging
task, given the requirements of deriving a compact detection criterion without
accessing the anomaly points. The existing methods are mainly based on
reconstruction error or association divergence, which are both confined to
isolated subsequences with limited horizons, hardly promising unified
series-level criterion. In this paper, we propose the Global
Dictionary-enhanced Transformer (GDformer) with a renovated dictionary-based
cross attention mechanism to cultivate the global representations shared by all
normal points in the entire series. Accordingly, the cross-attention maps
reflect the correlation weights between the point and global representations,
which naturally leads to the representation-wise similarity-based detection
criterion. To foster more compact detection boundary, prototypes are introduced
to capture the distribution of normal point-global correlation weights.
GDformer consistently achieves state-of-the-art unsupervised anomaly detection
performance on five real-world benchmark datasets. Further experiments validate
the global dictionary has great transferability among various datasets. The
code is available at https://github.com/yuppielqx/GDformer.

</details>


### [259] [GNN-DT: Graph Neural Network Enhanced Decision Transformer for Efficient Optimization in Dynamic Environments](https://arxiv.org/pdf/2502.01778)
*Stavros Orfanoudakis, Nanda Kishor Panda, Peter Palensky, Pedro P. Vergara*

Main category: cs.LG

TL;DR: GNN-DT, a novel Decision Transformer with GNN embedders and residual connections, improves RL for dynamic environments, outperforming baselines in EV charging optimization with better sample efficiency and generalization.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in RL for real-world problems like dynamic state-action spaces, large scale, and sparse rewards.

Method: Integrates GNN embedders with a Decision Transformer and residual connections, learning from collected trajectories to handle sparse rewards.

Result: Superior performance in EV charging optimization, requiring fewer training trajectories and showing robust generalization.

Conclusion: GNN-DT effectively addresses limitations of existing RL methods, offering scalable and efficient solutions for dynamic environments.

Abstract: Reinforcement Learning (RL) methods used for solving real-world optimization
problems often involve dynamic state-action spaces, larger scale, and sparse
rewards, leading to significant challenges in convergence, scalability, and
efficient exploration of the solution space. This study introduces GNN-DT, a
novel Decision Transformer (DT) architecture that integrates Graph Neural
Network (GNN) embedders with a novel residual connection between input and
output tokens crucial for handling dynamic environments. By learning from
previously collected trajectories, GNN-DT tackles the sparse rewards
limitations of online RL algorithms and delivers high-quality solutions in
real-time. We evaluate GNN-DT on the complex electric vehicle (EV) charging
optimization problem and prove that its performance is superior and requires
significantly fewer training trajectories, thus improving sample efficiency
compared to existing DT and offline RL baselines. Furthermore, GNN-DT exhibits
robust generalization to unseen environments and larger action spaces,
addressing a critical gap in prior offline and online RL approaches.

</details>


### [260] [RIE-SenseNet: Riemannian Manifold Embedding of Multi-Source Industrial Sensor Signals for Robust Pattern Recognition](https://arxiv.org/pdf/2502.02428)
*Xu Wang, Puyu Han, Jiaju Kang, Weichao Pan, Luqi Gong*

Main category: cs.LG

TL;DR: RIE-SenseNet, a geometry-aware Transformer, embeds sensor data in a Riemannian manifold, outperforming baselines with >90% F1-score.


<details>
  <summary>Details</summary>
Motivation: Industrial sensor networks produce complex, nonlinear signals with shifting distributions, requiring robust modeling.

Method: RIE-SenseNet uses hyperbolic geometry for sequence modeling and manifold-based augmentation to preserve signal structure and generate synthetic samples.

Result: Achieves >90% F1-score, surpassing CNN and Transformer baselines.

Conclusion: Combining non-Euclidean representations with geometry-consistent augmentation improves industrial sensing robustness.

Abstract: Industrial sensor networks produce complex signals with nonlinear structure
and shifting distributions. We propose RIE-SenseNet, a novel geometry-aware
Transformer model that embeds sensor data in a Riemannian manifold to tackle
these challenges. By leveraging hyperbolic geometry for sequence modeling and
introducing a manifold-based augmentation technique, RIE-SenseNet preserves
sensor signal structure and generates realistic synthetic samples. Experiments
show RIE-SenseNet achieves >90% F1-score, far surpassing CNN and Transformer
baselines. These results illustrate the benefit of combining non-Euclidean
feature representations with geometry-consistent data augmentation for robust
pattern recognition in industrial sensing.

</details>


### [261] [Gradient-Guided Annealing for Domain Generalization](https://arxiv.org/pdf/2502.20162)
*Aristotelis Ballas, Christos Diou*

Main category: cs.LG

TL;DR: The paper introduces Gradient-Guided Annealing (GGA) to improve domain generalization by aligning gradients early in training, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Domain generalization is challenging due to conflicting gradients across domains, leading to poor generalization.

Method: Proposes GGA, which anneals parameters early to align gradients and find domain-invariant features.

Result: GGA achieves competitive or state-of-the-art performance on benchmarks and enhances existing methods.

Conclusion: GGA effectively addresses domain conflicts, improving robustness to domain shifts.

Abstract: Domain Generalization (DG) research has gained considerable traction as of
late, since the ability to generalize to unseen data distributions is a
requirement that eludes even state-of-the-art training algorithms. In this
paper we observe that the initial iterations of model training play a key role
in domain generalization effectiveness, since the loss landscape may be
significantly different across the training and test distributions, contrary to
the case of i.i.d. data. Conflicts between gradients of the loss components of
each domain lead the optimization procedure to undesirable local minima that do
not capture the domain-invariant features of the target classes. We propose
alleviating domain conflicts in model optimization, by iteratively annealing
the parameters of a model in the early stages of training and searching for
points where gradients align between domains. By discovering a set of parameter
values where gradients are updated towards the same direction for each data
distribution present in the training set, the proposed Gradient-Guided
Annealing (GGA) algorithm encourages models to seek out minima that exhibit
improved robustness against domain shifts. The efficacy of GGA is evaluated on
five widely accepted and challenging image classification domain generalization
benchmarks, where its use alone is able to establish highly competitive or even
state-of-the-art performance. Moreover, when combined with previously proposed
domain-generalization algorithms it is able to consistently improve their
effectiveness by significant margins.

</details>


### [262] [Higher-Order Graphon Neural Networks: Approximation and Cut Distance](https://arxiv.org/pdf/2503.14338)
*Daniel Herbst, Stefanie Jegelka*

Main category: cs.LG

TL;DR: The paper extends the $k$-WL test to graphon-signal space and introduces signal-weighted homomorphism densities. It generalizes Invariant Graph Networks (IGNs) to graphons, proposing Invariant Graphon Networks (IWNs), showing they retain expressivity and achieve universal approximation in $L^p$ distances.


<details>
  <summary>Details</summary>
Motivation: To study size transferability of higher-order GNNs, extending beyond MPNNs, and align better with graphon space geometry.

Method: Extends $k$-WL test to graphon-signal space, introduces signal-weighted homomorphism densities, and generalizes IGNs to IWNs with a restricted basis.

Result: IWNs of order $k$ are as powerful as $k$-WL, achieve universal approximation, and retain expressivity despite a restricted basis.

Conclusion: Transferability is achievable for higher-order GNNs despite discontinuity w.r.t. cut distance, and IWNs align better with graphon geometry.

Abstract: Graph limit models, like graphons for limits of dense graphs, have recently
been used to study size transferability of graph neural networks (GNNs). While
most literature focuses on message passing GNNs (MPNNs), in this work we attend
to the more powerful higher-order GNNs. First, we extend the $k$-WL test for
graphons (B\"oker, 2023) to the graphon-signal space and introduce
signal-weighted homomorphism densities as a key tool. As an exemplary focus, we
generalize Invariant Graph Networks (IGNs) to graphons, proposing Invariant
Graphon Networks (IWNs) defined via a subset of the IGN basis corresponding to
bounded linear operators. Even with this restricted basis, we show that IWNs of
order $k$ are at least as powerful as the $k$-WL test, and we establish
universal approximation results for graphon-signals in $L^p$ distances. This
significantly extends the prior work of Cai & Wang (2022), showing that IWNs--a
subset of their IGN-small--retain effectively the same expressivity as the full
IGN basis in the limit. In contrast to their approach, our blueprint of IWNs
also aligns better with the geometry of graphon space, for example facilitating
comparability to MPNNs. We highlight that, while typical higher-order GNNs are
discontinuous w.r.t. cut distance--which causes their lack of convergence and
is inherently tied to the definition of $k$-WL--transferability remains
achievable.

</details>


### [263] [Rethinking Graph Structure Learning in the Era of LLMs](https://arxiv.org/pdf/2503.21223)
*Zhihan Zhang, Xunkai Li, Zhu Lei, Guang Zeng, Ronghua Li, Guoren Wang*

Main category: cs.LG

TL;DR: The paper introduces LLaTA, a method integrating LLMs with graph structure learning (GSL) for text-attributed graphs (TAGs), addressing optimization and efficiency challenges.


<details>
  <summary>Details</summary>
Motivation: To enhance TAG learning by integrating LLMs, overcoming limitations of traditional GSL methods that lack textual information.

Method: Reformulates GSL objectives as a tree optimization framework and proposes decoupled, training-free LLM integration principles, leading to LLaTA.

Result: LLaTA outperforms other LLM-enhanced methods, achieving state-of-the-art performance across 10 datasets.

Conclusion: LLaTA effectively integrates LLMs with GSL for TAGs, offering flexibility, scalability, and superior predictive performance.

Abstract: Recently, the emergence of LLMs has prompted researchers to integrate
language descriptions into graphs, aiming to enhance model encoding
capabilities from a data-centric perspective. This graph representation is
called text-attributed graphs (TAGs). A review of prior advancements highlights
that graph structure learning (GSL) is a pivotal technique for improving data
utility, making it highly relevant to efficient TAG learning. However, most GSL
methods are tailored for traditional graphs without textual information,
underscoring the necessity of developing a new GSL paradigm. Despite clear
motivations, it remains challenging: (1) How can we define a reasonable
optimization objective for GSL in the era of LLMs, considering the massive
parameters in LLM? (2) How can we design an efficient model architecture that
enables seamless integration of LLM for this optimization objective? For
Question 1, we reformulate existing GSL optimization objectives as a tree
optimization framework, shifting the focus from obtaining a well-trained edge
predictor to a language-aware tree sampler. For Question 2, we propose
decoupled and training-free model design principles for LLM integration,
shifting the focus from computation-intensive fine-tuning to more efficient
inference. Based on this, we propose Large Language and Tree Assistant (LLaTA),
which leverages tree-based LLM in-context learning to enhance the understanding
of topology and text, enabling reliable inference and generating improved graph
structure. Extensive experiments on 10 datasets demonstrate that LLaTA enjoys
flexibility-incorporated with any backbone; scalability-outperforms other
LLM-enhanced graph learning methods; effectiveness-achieves SOTA predictive
performance.

</details>


### [264] [Benchmarking Ultra-Low-Power $μ$NPUs](https://arxiv.org/pdf/2503.22567)
*Josh Millar, Yushan Huang, Sarab Sethi, Hamed Haddadi, Anil Madhavapeddy*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Efficient on-device neural network (NN) inference has various advantages over
cloud-based processing, including predictable latency, enhanced privacy,
greater reliability, and reduced operating costs for vendors. This has sparked
the recent rapid development of microcontroller-scale NN accelerators, often
referred to as neural processing units ($\mu$NPUs), designed specifically for
ultra-low-power applications.
  In this paper we present the first comparative evaluation of a number of
commercially-available $\mu$NPUs, as well as the first independent benchmarks
for several of these platforms. We develop and open-source a model compilation
framework to enable consistent benchmarking of quantized models across diverse
$\mu$NPU hardware. Our benchmark targets end-to-end performance and includes
model inference latency, power consumption, and memory overhead, alongside
other factors. The resulting analysis uncovers both expected performance trends
as well as surprising disparities between hardware specifications and actual
performance, including $\mu$NPUs exhibiting unexpected scaling behaviors with
increasing model complexity. Our framework provides a foundation for further
evaluation of $\mu$NPU platforms alongside valuable insights for both hardware
designers and software developers in this rapidly evolving space.

</details>


### [265] [From Observation to Orientation: an Adaptive Integer Programming Approach to Intervention Design](https://arxiv.org/pdf/2504.03122)
*Abdelmonem Elrefaey, Rong Pan*

Main category: cs.LG

TL;DR: The paper proposes an adaptive intervention design using causal DAGs and integer programming to optimize information gain, reducing required experiments and handling practical constraints effectively.


<details>
  <summary>Details</summary>
Motivation: To efficiently recover causal relationships between variables with minimal experiments while considering practical budgetary constraints.

Method: An iterative integer programming (IP) approach is used to select treatments for optimal information gain, leveraging causal directed acyclic graphs (DAGs).

Result: Simulations show the adaptive IP approach recovers full causal graphs with fewer interventions and manipulations than random baselines, while accommodating practical constraints.

Conclusion: The proposed method is effective for causal discovery with reduced experimental costs and flexibility for real-world constraints.

Abstract: Using both observational and experimental data, a causal discovery process
can identify the causal relationships between variables. A unique adaptive
intervention design paradigm is presented in this work, where causal directed
acyclic graphs (DAGs) are for effectively recovered with practical budgetary
considerations. In order to choose treatments that optimize information gain
under these considerations, an iterative integer programming (IP) approach is
proposed, which drastically reduces the number of experiments required.
Simulations over a broad range of graph sizes and edge densities are used to
assess the effectiveness of the suggested approach. Results show that the
proposed adaptive IP approach achieves full causal graph recovery with fewer
intervention iterations and variable manipulations than random intervention
baselines, and it is also flexible enough to accommodate a variety of practical
constraints.

</details>


### [266] [Directional Sign Loss: A Topology-Preserving Loss Function that Approximates the Sign of Finite Differences](https://arxiv.org/pdf/2504.04202)
*Harvey Dam, Tripti Agarwal, Ganesh Gopalakrishnan*

Main category: cs.LG

TL;DR: The paper introduces Directional Sign Loss (DSL), a novel loss function to preserve topological features in learned latent spaces by penalizing mismatches in critical points between input and reconstructed data.


<details>
  <summary>Details</summary>
Motivation: Preserving topological features in learned latent spaces is challenging, especially for topology-sensitive data. Existing methods often fail to retain these critical features.

Method: DSL approximates mismatches in signs of finite differences between arrays, penalizing discrepancies in critical points. It is mathematically formulated, analyzed for complexity, and implemented for autoencoders.

Result: Experiments on 1D, 2D, and 3D data show DSL combined with traditional losses preserves topology better than traditional losses alone. DSL also serves as a differentiable proxy for topology-based metrics.

Conclusion: DSL effectively retains topological features in learned representations and is suitable for gradient-based optimization, offering a practical solution for topology-sensitive data.

Abstract: Preserving critical topological features in learned latent spaces is a
fundamental challenge in representation learning, particularly for
topology-sensitive data. This paper introduces directional sign loss (DSL), a
novel loss function that approximates the number of mismatches in the signs of
finite differences between corresponding elements of two arrays. By penalizing
discrepancies in critical points between input and reconstructed data, DSL
encourages autoencoders and other learnable compressors to retain the
topological features of the original data. We present the mathematical
formulation, complexity analysis, and practical implementation of DSL,
comparing its behavior to its non-differentiable counterpart and to other
topological measures. Experiments on one-, two-, and three-dimensional data
show that combining DSL with traditional loss functions preserves topological
features more effectively than traditional losses alone. Moreover, DSL serves
as a differentiable, efficient proxy for common topology-based metrics,
enabling its use in gradient-based optimization frameworks.

</details>


### [267] [Reliable and Efficient Inverse Analysis using Physics-Informed Neural Networks with Distance Functions and Adaptive Weight Tuning](https://arxiv.org/pdf/2504.18091)
*Shota Deguchi, Mitsuteru Asai*

Main category: cs.LG

TL;DR: The paper proposes using R-functions to enforce boundary conditions in physics-informed neural networks (PINNs), improving accuracy and efficiency over traditional penalty-based methods, especially for inverse problems.


<details>
  <summary>Details</summary>
Motivation: The accuracy of PINN solutions is often limited by boundary condition treatment, with penalty-based methods being unreliable and sensitive to parameter choices.

Method: The paper leverages R-functions to enforce boundary conditions, extends this to inverse problems with PINNs, and introduces adaptive weight tuning for efficient analysis.

Result: Numerical experiments show the method outperforms penalty-based approaches in accuracy and efficiency, even for complex geometries.

Conclusion: The approach provides a reliable and efficient framework for inverse analysis with PINNs, applicable to diverse engineering problems.

Abstract: Physics-informed neural networks have attracted significant attention in
scientific machine learning for their capability to solve forward and inverse
problems governed by partial differential equations. However, the accuracy of
PINN solutions is often limited by the treatment of boundary conditions.
Conventional penalty-based methods, which incorporate boundary conditions as
penalty terms in the loss function, cannot guarantee exact satisfaction of the
given boundary conditions and are highly sensitive to the choice of penalty
parameters. This paper demonstrates that distance functions, specifically
R-functions, can be leveraged to enforce boundary conditions, overcoming these
limitations. R-functions provide normalized distance fields, enabling accurate
representation of boundary geometries, including non-convex domains, and
facilitating various types of boundary conditions. We extend this distance
function-based boundary condition imposition method to inverse problems using
PINNs and introduce an adaptive weight tuning technique to ensure reliable and
efficient inverse analysis. We demonstrate the efficacy of the method through
several numerical experiments. Numerical results show that the proposed method
solves inverse problems more accurately and efficiently than penalty-based
methods, even in the presence of complex non-convex geometries. This approach
offers a reliable and efficient framework for inverse analysis using PINNs,
with potential applications across a wide range of engineering problems.

</details>


### [268] [Enhanced semi-supervised stamping process monitoring with physically-informed feature extraction](https://arxiv.org/pdf/2504.21389)
*Jianyu Zhang*

Main category: cs.LG

TL;DR: A semi-supervised framework for anomaly monitoring in high-speed stamping processes uses hybrid feature extraction and a novel deviation score to detect anomalies in real-time, improving yield and reducing defects.


<details>
  <summary>Details</summary>
Motivation: To address frequent batch anomalies in high-speed stamping processes by developing an effective real-time monitoring system.

Method: Proposes a hybrid feature extraction algorithm combining data-driven and physics-based methods, and a semi-supervised anomaly detection model using normal samples to create a baseline.

Result: Validated with classification algorithms and real-world data, showing superior performance in anomaly monitoring.

Conclusion: The framework effectively reduces batch defects and enhances production yield by enabling real-time anomaly detection.

Abstract: In tackling frequent batch anomalies in high-speed stamping processes, this
study introduces a novel semi-supervised in-process anomaly monitoring
framework, utilizing accelerometer signals and physics information, to capture
the process anomaly effectively. The proposed framework facilitates the
construction of a monitoring model with imbalanced sample distribution, which
enables in-process condition monitoring in real-time to prevent batch
anomalies, which helps to reduce batch defects risk and enhance production
yield. Firstly, to effectively capture key features from raw data containing
redundant information, a hybrid feature extraction algorithm is proposed to
utilize data-driven methods and physical mechanisms simultaneously. Secondly,
to address the challenge brought by imbalanced sample distribution, a
semi-supervised anomaly detection model is established, which merely employs
normal samples to build a golden baseline model, and a novel deviation score is
proposed to quantify the anomaly level of each online stamping stroke. The
effectiveness of the proposed feature extraction method is validated with
various classification algorithms. A real-world in-process dataset from
stamping manufacturing workshop is employed to illustrate the superiority of
proposed semi-supervised framework with enhance performance for process anomaly
monitoring.

</details>


### [269] [Gateformer: Advancing Multivariate Time Series Forecasting through Temporal and Variate-Wise Attention with Gated Representations](https://arxiv.org/pdf/2505.00307)
*Yu-Hsiang Lan, Eric K. Oermann*

Main category: cs.LG

TL;DR: The paper proposes a Transformer-based method for multivariate time series forecasting, addressing challenges in modeling cross-time and cross-variate dependencies. It achieves state-of-the-art performance and integrates easily with other models.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in effectively modeling both temporal and variate dependencies in multivariate time series forecasting using Transformers.

Method: The approach independently embeds variates for cross-time dynamics, then uses attention mechanisms for cross-variate dependencies, with gating to regulate information flow.

Result: The method outperforms existing models on 13 datasets, improving performance by up to 20.7%.

Conclusion: The proposed method effectively integrates cross-time and cross-variate modeling, offering significant performance gains and compatibility with other Transformer-based models.

Abstract: There has been a recent surge of interest in time series modeling using the
Transformer architecture. However, forecasting multivariate time series with
Transformer presents a unique challenge as it requires modeling both temporal
(cross-time) and variate (cross-variate) dependencies. While Transformer-based
models have gained popularity for their flexibility in capturing both
sequential and cross-variate relationships, it is unclear how to best integrate
these two sources of information in the context of the Transformer architecture
while optimizing for both performance and efficiency. We re-purpose the
Transformer architecture to effectively model both cross-time and cross-variate
dependencies. Our approach begins by embedding each variate independently into
a variate-wise representation that captures its cross-time dynamics, and then
models cross-variate dependencies through attention mechanisms on these learned
embeddings. Gating operations in both cross-time and cross-variate modeling
phases regulate information flow, allowing the model to focus on the most
relevant features for accurate predictions. Our method achieves
state-of-the-art performance across 13 real-world datasets and can be
seamlessly integrated into other Transformer-based and LLM-based forecasters,
delivering performance improvements up to 20.7\% over original models. Code is
available at this repository: https://github.com/nyuolab/Gateformer.

</details>


### [270] [Machine Learning Meets Transparency in Osteoporosis Risk Assessment: A Comparative Study of ML and Explainability Analysis](https://arxiv.org/pdf/2505.00410)
*Farhana Elias, Md Shihab Reza, Muhammad Zawad Mahmud, Samiha Islam, Shahran Rahman Alve*

Main category: cs.LG

TL;DR: The study uses ML and XAI to predict osteoporosis risk, with XGBoost achieving 91% accuracy. Age, hormonal changes, and family history are key predictors.


<details>
  <summary>Details</summary>
Motivation: Osteoporosis is often asymptomatic, making early detection crucial to prevent fractures. The research aims to improve prediction transparency using XAI.

Method: Six ML classifiers (e.g., Random Forest, XGBoost) were evaluated on clinical data, optimized via GridSearchCV, and explained using SHAP, LIME, and Permutation Feature Importance.

Result: XGBoost performed best (91% accuracy). Age was the top predictor, followed by hormonal changes and family history.

Conclusion: The study highlights the importance of explainable ML in healthcare and suggests further validation and biomarker integration.

Abstract: The present research tackles the difficulty of predicting osteoporosis risk
via machine learning (ML) approaches, emphasizing the use of explainable
artificial intelligence (XAI) to improve model transparency. Osteoporosis is a
significant public health concern, sometimes remaining untreated owing to its
asymptomatic characteristics, and early identification is essential to avert
fractures. The research assesses six machine learning classifiers: Random
Forest, Logistic Regression, XGBoost, AdaBoost, LightGBM, and Gradient Boosting
and utilizes a dataset based on clinical, demographic, and lifestyle variables.
The models are refined using GridSearchCV to calibrate hyperparameters, with
the objective of enhancing predictive efficacy. XGBoost had the greatest
accuracy (91%) among the evaluated models, surpassing others in precision
(0.92), recall (0.91), and F1-score (0.90). The research further integrates XAI
approaches, such as SHAP, LIME, and Permutation Feature Importance, to
elucidate the decision-making process of the optimal model. The study indicates
that age is the primary determinant in forecasting osteoporosis risk, followed
by hormonal alterations and familial history. These results corroborate
clinical knowledge and affirm the models' therapeutic significance. The
research underscores the significance of explainability in machine learning
models for healthcare applications, guaranteeing that physicians can rely on
the system's predictions. The report ultimately proposes directions for further
research, such as validation across varied populations and the integration of
supplementary biomarkers for enhanced predictive accuracy.

</details>


### [271] [Carbon Aware Transformers Through Joint Model-Hardware Optimization](https://arxiv.org/pdf/2505.01386)
*Irene Wang, Newsha Ardalani, Mostafa Elhoushi, Daniel Jiang, Samuel Hsia, Ekin Sumbul, Divya Mahajan, Carole-Jean Wu, Bilge Acun*

Main category: cs.LG

TL;DR: CATransformers is a carbon-aware framework for optimizing ML models and hardware to reduce total carbon emissions, achieving a 17% reduction in emissions while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: The environmental impact of ML systems, particularly their carbon footprint (operational and embodied), lacks comprehensive evaluation tools.

Method: Proposes CATransformers, a framework for co-optimizing ML models and hardware architectures using carbon metrics.

Result: Achieves up to 17% reduction in total carbon emissions for CLIP-based models without compromising accuracy or latency.

Conclusion: Highlights the need for holistic optimization to design sustainable, high-performance AI systems.

Abstract: The rapid growth of machine learning (ML) systems necessitates a more
comprehensive evaluation of their environmental impact, particularly their
carbon footprint, which comprises operational carbon from training and
inference execution and embodied carbon from hardware manufacturing and its
entire life-cycle. Despite the increasing importance of embodied emissions,
there is a lack of tools and frameworks to holistically quantify and optimize
the total carbon footprint of ML systems. To address this, we propose
CATransformers, a carbon-aware architecture search framework that enables
sustainability-driven co-optimization of ML models and hardware architectures.
By incorporating both operational and embodied carbon metrics into early design
space exploration of domain-specific hardware accelerators, CATransformers
demonstrates that optimizing for carbon yields design choices distinct from
those optimized solely for latency or energy efficiency. We apply our framework
to multi-modal CLIP-based models, producing CarbonCLIP, a family of CLIP models
achieving up to 17% reduction in total carbon emissions while maintaining
accuracy and latency compared to state-of-the-art edge small CLIP baselines.
This work underscores the need for holistic optimization methods to design
high-performance, environmentally sustainable AI systems.

</details>


### [272] [Long-Term Individual Causal Effect Estimation via Identifiable Latent Representation Learning](https://arxiv.org/pdf/2505.05192)
*Ruichu Cai, Junjie Wan, Weilin Chen, Zeqin Yang, Zijian Li, Peng Zhen, Jiecheng Guo*

Main category: cs.LG

TL;DR: Proposes a method to estimate long-term causal effects without relying on ideal assumptions by leveraging data heterogeneity and latent representation learning.


<details>
  <summary>Details</summary>
Motivation: Existing methods for estimating long-term causal effects rely on ideal assumptions that are often violated in practice, limiting their effectiveness.

Method: Utilizes data heterogeneity (e.g., multiple sources) to identify latent confounders and develops a latent representation learning-based estimator.

Result: Demonstrates effectiveness through experiments on synthetic and semi-synthetic datasets, achieving long-term effect identification.

Conclusion: The proposed method avoids idealized assumptions and effectively estimates long-term causal effects by leveraging natural data heterogeneity.

Abstract: Estimating long-term causal effects by combining long-term observational and
short-term experimental data is a crucial but challenging problem in many
real-world scenarios. In existing methods, several ideal assumptions, e.g.
latent unconfoundedness assumption or additive equi-confounding bias
assumption, are proposed to address the latent confounder problem raised by the
observational data. However, in real-world applications, these assumptions are
typically violated which limits their practical effectiveness. In this paper,
we tackle the problem of estimating the long-term individual causal effects
without the aforementioned assumptions. Specifically, we propose to utilize the
natural heterogeneity of data, such as data from multiple sources, to identify
latent confounders, thereby significantly avoiding reliance on idealized
assumptions. Practically, we devise a latent representation learning-based
estimator of long-term causal effects. Theoretically, we establish the
identifiability of latent confounders, with which we further achieve long-term
effect identification. Extensive experimental studies, conducted on multiple
synthetic and semi-synthetic datasets, demonstrate the effectiveness of our
proposed method.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [273] [Robust Multi-Agent Decision-Making in Finite-Population Games](https://arxiv.org/pdf/2505.06200)
*Shinkyu Park, Lucas C. D. Bezerra*

Main category: cs.MA

TL;DR: The paper analyzes the robustness of the KLD-RL model in finite-population games, focusing on parameter tuning to mitigate noise and modeling inaccuracies.


<details>
  <summary>Details</summary>
Motivation: To understand how model parameters affect decision-making under noise and inaccuracies, common in engineering applications of population games.

Method: Theoretical analysis supported by numerical examples and simulation studies.

Result: Insights into effective parameter tuning to mitigate noise and inaccuracies, validated by simulations.

Conclusion: The study provides practical strategies for parameter selection in the KLD-RL model to enhance robustness in decision-making.

Abstract: We study the robustness of an agent decision-making model in
finite-population games, with a particular focus on the Kullback-Leibler
Divergence Regularized Learning (KLD-RL) model. Specifically, we examine how
the model's parameters influence the effects of various sources of noise and
modeling inaccuracies -- factors commonly encountered in engineering
applications of population games -- on agents' decision-making. Our analysis
provides insights into how these parameters can be effectively tuned to
mitigate such effects. Theoretical results are supported by numerical examples
and simulation studies that validate the analysis and illustrate practical
strategies for parameter selection.

</details>


### [274] [Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation](https://arxiv.org/pdf/2505.03586)
*Songchen Fu, Siang Chen, Shaojing Zhao, Letian Bai, Ta Li, Yonghong Yan*

Main category: cs.MA

TL;DR: The paper introduces a framework (RDC) to address stochastic individual delays in multi-agent systems, improving MARL performance under delayed observations.


<details>
  <summary>Details</summary>
Motivation: Observation delays in MASs hinder decision-making, requiring solutions for MARL to handle varying delay characteristics.

Method: Proposes the DSID-POMDP model and the RDC framework, tested on MARL benchmarks (MPE, SMAC).

Result: RDC mitigates performance degradation under delays, achieving near delay-free performance in some cases.

Conclusion: RDC provides an effective solution for delayed observations in MASs, with generalizable results.

Abstract: In real-world multi-agent systems (MASs), observation delays are ubiquitous,
preventing agents from making decisions based on the environment's true state.
An individual agent's local observation often consists of multiple components
from other agents or dynamic entities in the environment. These discrete
observation components with varying delay characteristics pose significant
challenges for multi-agent reinforcement learning (MARL). In this paper, we
first formulate the decentralized stochastic individual delay partially
observable Markov decision process (DSID-POMDP) by extending the standard
Dec-POMDP. We then propose the Rainbow Delay Compensation (RDC), a MARL
training framework for addressing stochastic individual delays, along with
recommended implementations for its constituent modules. We implement the
DSID-POMDP's observation generation pattern using standard MARL benchmarks,
including MPE and SMAC. Experiments demonstrate that baseline MARL methods
suffer severe performance degradation under fixed and unfixed delays. The
RDC-enhanced approach mitigates this issue, remarkably achieving ideal
delay-free performance in certain delay scenarios while maintaining
generalizability. Our work provides a novel perspective on multi-agent delayed
observation problems and offers an effective solution framework. The source
code is available at https://anonymous.4open.science/r/RDC-pymarl-4512/.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [275] [Detecting Multimedia Generated by Large AI Models: A Survey](https://arxiv.org/pdf/2402.00045)
*Li Lin, Neeraj Gupta, Yue Zhang, Hainan Ren, Chun-Hao Liu, Feng Ding, Xin Wang, Xin Li, Luisa Verdoliva, Shu Hu*

Main category: cs.MM

TL;DR: This paper presents the first comprehensive survey on detecting multimedia generated by Large AI Models (LAIMs), introducing a taxonomy for detection methods and addressing societal impacts and future research directions.


<details>
  <summary>Details</summary>
Motivation: The rapid integration of AI-generated multimedia into daily life poses risks like misuse and ethical concerns, necessitating systematic detection methods.

Method: The survey categorizes detection methods by media modality and perspectives (pure detection and beyond detection), and reviews generation mechanisms, datasets, tools, and metrics.

Result: A novel taxonomy for LAIM-generated multimedia detection is introduced, alongside resources for researchers and insights into societal impacts.

Conclusion: The survey fills an academic gap, aids global AI security, and proposes future research directions to address detection challenges.

Abstract: The rapid advancement of Large AI Models (LAIMs), particularly diffusion
models and large language models, has marked a new era where AI-generated
multimedia is increasingly integrated into various aspects of daily life.
Although beneficial in numerous fields, this content presents significant
risks, including potential misuse, societal disruptions, and ethical concerns.
Consequently, detecting multimedia generated by LAIMs has become crucial, with
a marked rise in related research. Despite this, there remains a notable gap in
systematic surveys that focus specifically on detecting LAIM-generated
multimedia. Addressing this, we provide the first survey to comprehensively
cover existing research on detecting multimedia (such as text, images, videos,
audio, and multimodal content) created by LAIMs. Specifically, we introduce a
novel taxonomy for detection methods, categorized by media modality, and
aligned with two perspectives: pure detection (aiming to enhance detection
performance) and beyond detection (adding attributes like generalizability,
robustness, and interpretability to detectors). Additionally, we have presented
a brief overview of generation mechanisms, public datasets, online detection
tools, and evaluation metrics to provide a valuable resource for researchers
and practitioners in this field. Most importantly, we offer a focused analysis
from a social media perspective to highlight their broader societal impact.
Furthermore, we identify current challenges in detection and propose directions
for future research that address unexplored, ongoing, and emerging issues in
detecting multimedia generated by LAIMs. Our aim for this survey is to fill an
academic gap and contribute to global AI security efforts, helping to ensure
the integrity of information in the digital realm. The project link is
https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey.

</details>


### [276] [Visual and Auditory Aesthetic Preferences Across Cultures](https://arxiv.org/pdf/2502.14439)
*Harin Lee, Eline Van Geert, Elif Celen, Raja Marjieh, Pol van Rijn, Minsu Park, Nori Jacoby*

Main category: cs.MM

TL;DR: A large-scale cross-cultural study reveals universal and culturally varied aesthetic preferences in shape, color, and music, influenced by both perceptual mechanisms and cultural learning.


<details>
  <summary>Details</summary>
Motivation: To understand how cultural environments shape aesthetic preferences, given prior research's focus on Western populations.

Method: Gathered 401,403 preference judgements from 4,835 participants across 10 countries, systematically sampling parameter spaces for shape, curvature, color, musical harmony, and melody.

Result: Universal patterns (e.g., symmetry in shapes) and cultural variations (e.g., melody preferences) were found, with color and harmony showing categorical consistency but ratio-like differences.

Conclusion: Aesthetic preferences arise from an interplay of shared perceptual mechanisms and cultural learning.

Abstract: Research on how humans perceive aesthetics in shapes, colours, and music has
predominantly focused on Western populations, limiting our understanding of how
cultural environments shape aesthetic preferences. We present a large-scale
cross-cultural study examining aesthetic preferences across five distinct
modalities extensively explored in the literature: shape, curvature, colour,
musical harmony and melody. We gather 401,403 preference judgements from 4,835
participants across 10 countries, systematically sampling two-dimensional
parameter spaces for each modality. The findings reveal both universal patterns
and cultural variations. Preferences for shape and curvature cross-culturally
demonstrate a consistent preference for symmetrical forms. While colour
preferences are categorically consistent, ratio-like preferences vary across
cultures. Musical harmony shows strong agreement in interval relationships
despite differing regions of preference within the broad frequency spectrum,
while melody shows the highest cross-cultural variation. These results suggest
that aesthetic preferences emerge from an interplay between shared perceptual
mechanisms and cultural learning.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [277] [Unsupervised Blind Speech Separation with a Diffusion Prior](https://arxiv.org/pdf/2505.05657)
*Zhongweiyang Xu, Xulin Fan, Zhong-Qiu Wang, Xilin Jiang, Romit Roy Choudhury*

Main category: eess.AS

TL;DR: ArrayDPS is an unsupervised, array-agnostic method for Blind Speech Separation (BSS) using diffusion posterior sampling and optimization to approximate room acoustics, outperforming unsupervised baselines.


<details>
  <summary>Details</summary>
Motivation: BSS is challenging due to unknown microphone array geometry, room impulse response, and speech sources. ArrayDPS addresses this without requiring array information.

Method: ArrayDPS combines diffusion posterior sampling with an optimization problem to approximate room acoustics and transfer functions, iterating to separate speech sources.

Result: ArrayDPS outperforms unsupervised baselines and matches supervised methods in SDR performance.

Conclusion: ArrayDPS provides a robust, unsupervised solution for BSS, leveraging diffusion priors and optimization without needing array details.

Abstract: Blind Speech Separation (BSS) aims to separate multiple speech sources from
audio mixtures recorded by a microphone array. The problem is challenging
because it is a blind inverse problem, i.e., the microphone array geometry, the
room impulse response (RIR), and the speech sources, are all unknown. We
propose ArrayDPS to solve the BSS problem in an unsupervised, array-agnostic,
and generative manner. The core idea builds on diffusion posterior sampling
(DPS), but unlike DPS where the likelihood is tractable, ArrayDPS must
approximate the likelihood by formulating a separate optimization problem. The
solution to the optimization approximates room acoustics and the relative
transfer functions between microphones. These approximations, along with the
diffusion priors, iterate through the ArrayDPS sampling process and ultimately
yield separated voice sources. We only need a simple single-speaker speech
diffusion model as a prior along with the mixtures recorded at the microphones;
no microphone array information is necessary. Evaluation results show that
ArrayDPS outperforms all baseline unsupervised methods while being comparable
to supervised methods in terms of SDR. Audio demos are provided at:
https://arraydps.github.io/ArrayDPSDemo/.

</details>


### [278] [FlexSpeech: Towards Stable, Controllable and Expressive Text-to-Speech](https://arxiv.org/pdf/2505.05159)
*Linhan Ma, Dake Guo, He Wang, Jin Xu, Lei Xie*

Main category: eess.AS

TL;DR: FlexSpeech combines autoregressive (AR) and non-autoregressive (NAR) methods for stable, controllable, and expressive speech generation, achieving SOTA results in zero-shot TTS and lightweight style transfer.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between stability (NAR) and naturalness (AR) in speech generation by integrating Markov dependencies and preference optimization into duration prediction.

Method: Decomposes speech generation into an AR duration predictor and a NAR acoustic model, optimizing the former for style transfer while keeping the latter stable.

Result: Achieves SOTA stability and naturalness in zero-shot TTS; enables rapid style transfer with minimal data (100 samples) without altering the acoustic model.

Conclusion: FlexSpeech successfully balances stability and naturalness, offering efficient style transfer and high-quality speech generation.

Abstract: Current speech generation research can be categorized into two primary
classes: non-autoregressive and autoregressive. The fundamental distinction
between these approaches lies in the duration prediction strategy employed for
predictable-length sequences. The NAR methods ensure stability in speech
generation by explicitly and independently modeling the duration of each
phonetic unit. Conversely, AR methods employ an autoregressive paradigm to
predict the compressed speech token by implicitly modeling duration with Markov
properties. Although this approach improves prosody, it does not provide the
structural guarantees necessary for stability. To simultaneously address the
issues of stability and naturalness in speech generation, we propose
FlexSpeech, a stable, controllable, and expressive TTS model. The motivation
behind FlexSpeech is to incorporate Markov dependencies and preference
optimization directly on the duration predictor to boost its naturalness while
maintaining explicit modeling of the phonetic units to ensure stability.
Specifically, we decompose the speech generation task into two components: an
AR duration predictor and a NAR acoustic model. The acoustic model is trained
on a substantial amount of data to learn to render audio more stably, given
reference audio prosody and phone durations. The duration predictor is
optimized in a lightweight manner for different stylistic variations, thereby
enabling rapid style transfer while maintaining a decoupled relationship with
the specified speaker timbre. Experimental results demonstrate that our
approach achieves SOTA stability and naturalness in zero-shot TTS. More
importantly, when transferring to a specific stylistic domain, we can
accomplish lightweight optimization of the duration module solely with about
100 data samples, without the need to adjust the acoustic model, thereby
enabling rapid and stable style transfer.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [279] [Image Restoration via Multi-domain Learning](https://arxiv.org/pdf/2505.05504)
*Xingyu Jiang, Ning Gao, Xiuhui Zhang, Hongkun Dou, Shaowen Fu, Xiaoqing Zhong, Hongjue Li, Yue Deng*

Main category: eess.IV

TL;DR: A novel Transformer-based framework (SWFormer) integrates multi-domain learning for image restoration, outperforming state-of-the-art methods with a balanced trade-off in performance, complexity, and speed.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing Transformer-based restoration methods, which lack exploration of common degradation priors and suffer from high complexity.

Method: Proposes a Spatial-Wavelet-Fourier multi-domain structure in Token Mixer and multi-scale learning in Feed-Forward Network to model multi-receptive fields and fuse features.

Result: Demonstrates superior performance across ten restoration tasks, achieving better efficiency and effectiveness.

Conclusion: SWFormer offers a scalable and efficient solution for diverse image restoration challenges.

Abstract: Due to adverse atmospheric and imaging conditions, natural images suffer from
various degradation phenomena. Consequently, image restoration has emerged as a
key solution and garnered substantial attention. Although recent Transformer
architectures have demonstrated impressive success across various restoration
tasks, their considerable model complexity poses significant challenges for
both training and real-time deployment. Furthermore, instead of investigating
the commonalities among different degradations, most existing restoration
methods focus on modifying Transformer under limited restoration priors. In
this work, we first review various degradation phenomena under multi-domain
perspective, identifying common priors. Then, we introduce a novel restoration
framework, which integrates multi-domain learning into Transformer.
Specifically, in Token Mixer, we propose a Spatial-Wavelet-Fourier multi-domain
structure that facilitates local-region-global multi-receptive field modeling
to replace vanilla self-attention. Additionally, in Feed-Forward Network, we
incorporate multi-scale learning to fuse multi-domain features at different
resolutions. Comprehensive experimental results across ten restoration tasks,
such as dehazing, desnowing, motion deblurring, defocus deblurring, rain
streak/raindrop removal, cloud removal, shadow removal, underwater enhancement
and low-light enhancement, demonstrate that our proposed model outperforms
state-of-the-art methods and achieves a favorable trade-off among restoration
performance, parameter size, computational cost and inference latency. The code
is available at: https://github.com/deng-ai-lab/SWFormer.

</details>


### [280] [StereoINR: Cross-View Geometry Consistent Stereo Super Resolution with Implicit Neural Representation](https://arxiv.org/pdf/2505.05509)
*Yi Liu, Xinyi Liu, Panwang Xia, Qiong Wu, Yi Wan, Yongjun Zhang*

Main category: eess.IV

TL;DR: StereoINR proposes a novel method for stereo image super-resolution by modeling stereo pairs as continuous implicit representations, enabling arbitrary-scale upsampling and improved cross-view consistency.


<details>
  <summary>Details</summary>
Motivation: Existing SSR methods lack cross-view geometric consistency and are limited to fixed-scale upsampling, missing adaptive multi-view information fusion.

Method: StereoINR models stereo pairs as continuous implicit representations, uses spatial warping and cross-attention for cross-view fusion, and supports arbitrary-scale upsampling.

Result: StereoINR outperforms out-of-training-distribution scale upsampling and matches state-of-the-art SSR methods within training scales.

Conclusion: StereoINR provides a unified, scalable solution for stereo super-resolution with enhanced cross-view consistency.

Abstract: Stereo image super-resolution (SSR) aims to enhance high-resolution details
by leveraging information from stereo image pairs. However, existing stereo
super-resolution (SSR) upsampling methods (e.g., pixel shuffle) often overlook
cross-view geometric consistency and are limited to fixed-scale upsampling. The
key issue is that previous upsampling methods use convolution to independently
process deep features of different views, lacking cross-view and non-local
information perception, making it difficult to select beneficial information
from multi-view scenes adaptively. In this work, we propose Stereo Implicit
Neural Representation (StereoINR), which innovatively models stereo image pairs
as continuous implicit representations. This continuous representation breaks
through the scale limitations, providing a unified solution for arbitrary-scale
stereo super-resolution reconstruction of left-right views. Furthermore, by
incorporating spatial warping and cross-attention mechanisms, StereoINR enables
effective cross-view information fusion and achieves significant improvements
in pixel-level geometric consistency. Extensive experiments across multiple
datasets show that StereoINR outperforms out-of-training-distribution scale
upsampling and matches state-of-the-art SSR methods within
training-distribution scales.

</details>


### [281] [Guidance for Intra-cardiac Echocardiography Manipulation to Maintain Continuous Therapy Device Tip Visibility](https://arxiv.org/pdf/2505.05518)
*Jaeyoung Huh, Ankur Kapoor, Young-Ho Kim*

Main category: eess.IV

TL;DR: An AI-driven tracking model is proposed to maintain continuous visibility of therapy device tips in Intra-cardiac Echocardiography (ICE) during interventions, using a hybrid dataset and transformer-based architecture for accurate predictions.


<details>
  <summary>Details</summary>
Motivation: Manual ICE catheter manipulation requires frequent adjustments, making continuous visibility of therapy device tips challenging. The goal is to automate this process to reduce operator workload and improve precision.

Method: A hybrid dataset of clinical ICE sequences and synthetic data was created. A transformer-based network, leveraging a pretrained ultrasound foundation model, processes sequential ICE frames to predict device tip angles and positions.

Result: The model achieved a 3.32-degree entry angle error and 12.76-degree rotation angle error, demonstrating high accuracy in tracking.

Conclusion: The AI-driven framework enables real-time robotic ICE catheter adjustments, ensuring consistent device visibility. Future work will expand clinical datasets for better generalization.

Abstract: Intra-cardiac Echocardiography (ICE) plays a critical role in
Electrophysiology (EP) and Structural Heart Disease (SHD) interventions by
providing real-time visualization of intracardiac structures. However,
maintaining continuous visibility of the therapy device tip remains a challenge
due to frequent adjustments required during manual ICE catheter manipulation.
To address this, we propose an AI-driven tracking model that estimates the
device tip incident angle and passing point within the ICE imaging plane,
ensuring continuous visibility and facilitating robotic ICE catheter control.
  A key innovation of our approach is the hybrid dataset generation strategy,
which combines clinical ICE sequences with synthetic data augmentation to
enhance model robustness. We collected ICE images in a water chamber setup,
equipping both the ICE catheter and device tip with electromagnetic (EM)
sensors to establish precise ground-truth locations. Synthetic sequences were
created by overlaying catheter tips onto real ICE images, preserving motion
continuity while simulating diverse anatomical scenarios. The final dataset
consists of 5,698 ICE-tip image pairs, ensuring comprehensive training
coverage.
  Our model architecture integrates a pretrained ultrasound (US) foundation
model, trained on 37.4M echocardiography images, for feature extraction. A
transformer-based network processes sequential ICE frames, leveraging
historical passing points and incident angles to improve prediction accuracy.
  Experimental results demonstrate that our method achieves 3.32 degree entry
angle error, 12.76 degree rotation angle error. This AI-driven framework lays
the foundation for real-time robotic ICE catheter adjustments, minimizing
operator workload while ensuring consistent therapy device visibility. Future
work will focus on expanding clinical datasets to further enhance model
generalization.

</details>


### [282] [Score-based Self-supervised MRI Denoising](https://arxiv.org/pdf/2505.05631)
*Jiachen Tu, Yaokun Shi, Fan Lam*

Main category: eess.IV

TL;DR: C2S is a novel self-supervised MRI denoising method using generalized denoising score matching, outperforming existing self-supervised methods and competing with supervised ones.


<details>
  <summary>Details</summary>
Motivation: Supervised denoising requires high-SNR labels, which are scarce. Existing self-supervised methods oversmooth features and underperform.

Method: Introduces Corruption2Self (C2S) with a generalized denoising score matching loss, noise reparameterization, and detail refinement. Extends to multi-contrast denoising.

Result: Achieves state-of-the-art self-supervised performance and competes with supervised methods on M4Raw and fastMRI datasets.

Conclusion: C2S effectively addresses label scarcity and preserves fine features, offering a robust solution for MRI denoising.

Abstract: Magnetic resonance imaging (MRI) is a powerful noninvasive diagnostic imaging
tool that provides unparalleled soft tissue contrast and anatomical detail.
Noise contamination, especially in accelerated and/or low-field acquisitions,
can significantly degrade image quality and diagnostic accuracy. Supervised
learning based denoising approaches have achieved impressive performance but
require high signal-to-noise ratio (SNR) labels, which are often unavailable.
Self-supervised learning holds promise to address the label scarcity issue, but
existing self-supervised denoising methods tend to oversmooth fine spatial
features and often yield inferior performance than supervised methods. We
introduce Corruption2Self (C2S), a novel score-based self-supervised framework
for MRI denoising. At the core of C2S is a generalized denoising score matching
(GDSM) loss, which extends denoising score matching to work directly with noisy
observations by modeling the conditional expectation of higher-SNR images given
further corrupted observations. This allows the model to effectively learn
denoising across multiple noise levels directly from noisy data. Additionally,
we incorporate a reparameterization of noise levels to stabilize training and
enhance convergence, and introduce a detail refinement extension to balance
noise reduction with the preservation of fine spatial features. Moreover, C2S
can be extended to multi-contrast denoising by leveraging complementary
information across different MRI contrasts. We demonstrate that our method
achieves state-of-the-art performance among self-supervised methods and
competitive results compared to supervised counterparts across varying noise
conditions and MRI contrasts on the M4Raw and fastMRI dataset.

</details>


### [283] [UltraGauss: Ultrafast Gaussian Reconstruction of 3D Ultrasound Volumes](https://arxiv.org/pdf/2505.05643)
*Mark C. Eid, Ana I. L. Namburete, João F. Henriques*

Main category: eess.IV

TL;DR: UltraGauss introduces a Gaussian Splatting framework for 2D-to-3D ultrasound reconstruction, improving efficiency and accuracy while aligning with ultrasound physics.


<details>
  <summary>Details</summary>
Motivation: Addressing the operator-dependent variability and cognitive demand of 2D ultrasound imaging, as well as the computational and physical limitations of existing 3D reconstruction methods.

Method: Extends Gaussian Splatting to ultrasound wave propagation, modeling probe-plane intersections in 3D and introducing efficient GPU-parallelizable rasterization and stable covariance parametrization.

Result: Achieves state-of-the-art reconstructions in 5 minutes (0.99 SSIM in 20 minutes) on a single GPU, validated by expert clinicians as the most realistic.

Conclusion: UltraGauss offers a computationally efficient, accurate, and clinically validated solution for 3D ultrasound reconstruction.

Abstract: Ultrasound imaging is widely used due to its safety, affordability, and
real-time capabilities, but its 2D interpretation is highly operator-dependent,
leading to variability and increased cognitive demand. 2D-to-3D reconstruction
mitigates these challenges by providing standardized volumetric views, yet
existing methods are often computationally expensive, memory-intensive, or
incompatible with ultrasound physics. We introduce UltraGauss: the first
ultrasound-specific Gaussian Splatting framework, extending view synthesis
techniques to ultrasound wave propagation. Unlike conventional
perspective-based splatting, UltraGauss models probe-plane intersections in 3D,
aligning with acoustic image formation. We derive an efficient rasterization
boundary formulation for GPU parallelization and introduce a numerically stable
covariance parametrization, improving computational efficiency and
reconstruction accuracy. On real clinical ultrasound data, UltraGauss achieves
state-of-the-art reconstructions in 5 minutes, and reaching 0.99 SSIM within 20
minutes on a single GPU. A survey of expert clinicians confirms UltraGauss'
reconstructions are the most realistic among competing methods. Our CUDA
implementation will be released upon publication.

</details>


### [284] [V-EfficientNets: Vector-Valued Efficiently Scaled Convolutional Neural Network Models](https://arxiv.org/pdf/2505.05659)
*Guilherme Vieira Neto, Marcos Eduardo Valle*

Main category: eess.IV

TL;DR: V-EfficientNets extend EfficientNet to handle vector-valued data, achieving 99.46% accuracy on a medical image task with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Traditional networks treat multidimensional data as separate channels, ignoring inter-channel relationships. V-EfficientNets address this by processing vector-valued data coherently.

Method: Extends EfficientNet by optimizing for vector-valued data, balancing width, depth, and resolution. Evaluated on the ALL-IDB2 medical dataset.

Result: Achieves 99.46% accuracy on ALL-IDB2, outperforming state-of-the-art models with fewer parameters.

Conclusion: V-EfficientNets are efficient and effective for vector-valued data, particularly in medical image classification.

Abstract: EfficientNet models are convolutional neural networks optimized for parameter
allocation by jointly balancing network width, depth, and resolution. Renowned
for their exceptional accuracy, these models have become a standard for image
classification tasks across diverse computer vision benchmarks. While
traditional neural networks learn correlations between feature channels during
training, vector-valued neural networks inherently treat multidimensional data
as coherent entities, taking for granted the inter-channel relationships. This
paper introduces vector-valued EfficientNets (V-EfficientNets), a novel
extension of EfficientNet designed to process arbitrary vector-valued data. The
proposed models are evaluated on a medical image classification task, achieving
an average accuracy of 99.46% on the ALL-IDB2 dataset for detecting acute
lymphoblastic leukemia. V-EfficientNets demonstrate remarkable efficiency,
significantly reducing parameters while outperforming state-of-the-art models,
including the original EfficientNet. The source code is available at
https://github.com/mevalle/v-nets.

</details>


### [285] [Equivariant Imaging Biomarkers for Robust Unsupervised Segmentation of Histopathology](https://arxiv.org/pdf/2505.05689)
*Fuyao Chen, Yuexi Du, Tal Zeevi, Nicha C. Dvornek, John A. Onofrey*

Main category: eess.IV

TL;DR: The paper proposes a novel symmetric convolutional kernel for unsupervised segmentation to create robust, equivariant histopathological biomarkers, addressing limitations of traditional ML models in handling rotation and reflection in histopathology images.


<details>
  <summary>Details</summary>
Motivation: Traditional manual histopathology analysis is inefficient and inconsistent, while existing ML models lack invariance to rotation and reflection, limiting their generalizability.

Method: Developed a symmetric convolutional kernel for unsupervised segmentation to extract equivariant biomarkers, validated on prostate tissue micro-array images from the Gleason 2019 Challenge dataset.

Result: The biomarkers showed improved robustness and generalizability against rotation compared to standard convolution kernels.

Conclusion: The approach enhances ML model accuracy and consistency in digital pathology, with potential applications beyond prostate cancer.

Abstract: Histopathology evaluation of tissue specimens through microscopic examination
is essential for accurate disease diagnosis and prognosis. However, traditional
manual analysis by specially trained pathologists is time-consuming,
labor-intensive, cost-inefficient, and prone to inter-rater variability,
potentially affecting diagnostic consistency and accuracy. As digital pathology
images continue to proliferate, there is a pressing need for automated analysis
to address these challenges. Recent advancements in artificial
intelligence-based tools such as machine learning (ML) models, have
significantly enhanced the precision and efficiency of analyzing
histopathological slides. However, despite their impressive performance, ML
models are invariant only to translation, lacking invariance to rotation and
reflection. This limitation restricts their ability to generalize effectively,
particularly in histopathology, where images intrinsically lack meaningful
orientation. In this study, we develop robust, equivariant histopathological
biomarkers through a novel symmetric convolutional kernel via unsupervised
segmentation. The approach is validated using prostate tissue micro-array (TMA)
images from 50 patients in the Gleason 2019 Challenge public dataset. The
biomarkers extracted through this approach demonstrate enhanced robustness and
generalizability against rotation compared to models using standard convolution
kernels, holding promise for enhancing the accuracy, consistency, and
robustness of ML models in digital pathology. Ultimately, this work aims to
improve diagnostic and prognostic capabilities of histopathology beyond
prostate cancer through equivariant imaging.

</details>


### [286] [Hybrid Learning: A Novel Combination of Self-Supervised and Supervised Learning for MRI Reconstruction without High-Quality Training Reference](https://arxiv.org/pdf/2505.05703)
*Haoyang Pei, Ding Xia, Xiang Xu, William Moore, Yao Wang, Hersh Chandarana, Li Feng*

Main category: eess.IV

TL;DR: Hybrid learning combines self-supervised and supervised learning for robust MRI reconstruction, outperforming standalone methods in image quality and accuracy.


<details>
  <summary>Details</summary>
Motivation: Conventional supervised learning requires high-quality reference images, often unavailable, while self-supervised learning degrades at high acceleration rates. Hybrid learning addresses these limitations.

Method: A two-stage framework: self-supervised learning generates improved images from noisy/undersampled data, followed by supervised learning using these as pseudo-ground truths for refinement.

Result: Improved image quality (SSIM, NMSE) for lung MRI and superior T1 quantification accuracy for brain mapping, outperforming standalone methods.

Conclusion: Hybrid learning is effective for deep MRI reconstruction with low-quality/incomplete data, enabling better image quality and quantitative accuracy for clinical deployment.

Abstract: Purpose: Deep learning has demonstrated strong potential for MRI
reconstruction, but conventional supervised learning methods require
high-quality reference images, which are often unavailable in practice.
Self-supervised learning offers an alternative, yet its performance degrades at
high acceleration rates. To overcome these limitations, we propose hybrid
learning, a novel two-stage training framework that combines self-supervised
and supervised learning for robust image reconstruction.
  Methods: Hybrid learning is implemented in two sequential stages. In the
first stage, self-supervised learning is employed to generate improved images
from noisy or undersampled reference data. These enhanced images then serve as
pseudo-ground truths for the second stage, which uses supervised learning to
refine reconstruction performance and support higher acceleration rates. We
evaluated hybrid learning in two representative applications: (1) accelerated
0.55T spiral-UTE lung MRI using noisy reference data, and (2) 3D T1 mapping of
the brain without access to fully sampled ground truth.
  Results: For spiral-UTE lung MRI, hybrid learning consistently improved image
quality over both self-supervised and conventional supervised methods across
different acceleration rates, as measured by SSIM and NMSE. For 3D T1 mapping,
hybrid learning achieved superior T1 quantification accuracy across a wide
dynamic range, outperforming self-supervised learning in all tested conditions.
  Conclusions: Hybrid learning provides a practical and effective solution for
training deep MRI reconstruction networks when only low-quality or incomplete
reference data are available. It enables improved image quality and accurate
quantitative mapping across different applications and field strengths,
representing a promising technique toward broader clinical deployment of deep
learning-based MRI.

</details>


### [287] [ProTCT: Projection quantification and fidelity constraint integrated deep reconstruction for Tangential CT](https://arxiv.org/pdf/2505.05745)
*Bingan Yuan, Bowei Liu, Zheng Fang*

Main category: eess.IV

TL;DR: ProTCT improves TCT image quality by integrating projection quantification and fidelity constraints, addressing truncation artifacts and oversmoothing.


<details>
  <summary>Details</summary>
Motivation: TCT projections are truncated, causing degraded slices with artifacts; existing methods fail due to ill-defined sampling and oversmoothing.

Method: Proposes ProTCT, combining a deep artifact-suppression network and fidelity-constraint module across projection and cross-section domains.

Result: ProTCT performs well in structure restoration and detail retention on simulated and real datasets.

Conclusion: ProTCT enhances TCT slice quality, aiding large view field CT imaging applications.

Abstract: Tangential computed tomography (TCT) is a useful tool for imaging the
large-diameter samples, such as oil pipelines and rockets. However, TCT
projections are truncated along the detector direction, resulting in degraded
slices with radial artifacts. Meanwhile, existing methods fail to reconstruct
decent images because of the ill-defined sampling condition in the projection
domain and oversmoothing in the cross-section domain. In this paper, we propose
a projection quantification and fidelity constraint integrated deep TCT
reconstruction method (ProTCT) to improve the slice quality. Specifically, the
sampling conditions for reconstruction are analysed, offering practical
guidelines for TCT system design. Besides, a deep artifact-suppression network
together with a fidelity-constraint module that operates across both projection
and cross-section domains to remove artifacts and restore edge details.
Demonstrated on simulated and real datasets, the ProTCT shows good performance
in structure restoration and detail retention. This work contributes to
exploring the sampling condition and improving the slice quality of TCT,
further promoting the application of large view field CT imaging.

</details>


### [288] [Predicting Diabetic Macular Edema Treatment Responses Using OCT: Dataset and Methods of APTOS Competition](https://arxiv.org/pdf/2505.05768)
*Weiyi Zhang, Peranut Chotcomwongse, Yinwen Li, Pusheng Xu, Ruijie Yao, Lianhao Zhou, Yuxuan Zhou, Hui Feng, Qiping Zhou, Xinyue Wang, Shoujin Huang, Zihao Jin, Florence H. T. Chung, Shujun Wang, Yalin Zheng, Mingguang He, Danli Shi, Paisan Ruamviboonsuk*

Main category: eess.IV

TL;DR: The study explores pre-treatment stratification for predicting diabetic macular edema (DME) treatment responses, using a competition to improve AI-based predictive accuracy with OCT images.


<details>
  <summary>Details</summary>
Motivation: Varied treatment responses in DME patients necessitate personalized strategies, prompting research into predictive stratification.

Method: The 2nd APTOS Big Data Competition analyzed OCT images from 2,000 patients, with 170 teams developing AI models to predict anti-VEGF therapy responses.

Result: The top team achieved an 80.06% AUC, demonstrating AI's potential for personalized DME treatment.

Conclusion: AI-driven analysis of OCT images can enhance DME treatment prediction, supporting clinical decision-making.

Abstract: Diabetic macular edema (DME) significantly contributes to visual impairment
in diabetic patients. Treatment responses to intravitreal therapies vary,
highlighting the need for patient stratification to predict therapeutic
benefits and enable personalized strategies. To our knowledge, this study is
the first to explore pre-treatment stratification for predicting DME treatment
responses. To advance this research, we organized the 2nd Asia-Pacific
Tele-Ophthalmology Society (APTOS) Big Data Competition in 2021. The
competition focused on improving predictive accuracy for anti-VEGF therapy
responses using ophthalmic OCT images. We provided a dataset containing tens of
thousands of OCT images from 2,000 patients with labels across four sub-tasks.
This paper details the competition's structure, dataset, leading methods, and
evaluation metrics. The competition attracted strong scientific community
participation, with 170 teams initially registering and 41 reaching the final
round. The top-performing team achieved an AUC of 80.06%, highlighting the
potential of AI in personalized DME treatment and clinical decision-making.

</details>


### [289] [S2MNet: Speckle-To-Mesh Net for Three-Dimensional Cardiac Morphology Reconstruction via Echocardiogram](https://arxiv.org/pdf/2505.06105)
*Xilin Gong, Yongkai Chen, Shushan Wu, Fang Wang, Ping Ma, Wenxuan Zhong*

Main category: eess.IV

TL;DR: A deep learning framework, S2MNet, reconstructs high-fidelity 3D heart models from six 2D echocardiogram slices, overcoming limitations of traditional 3D echocardiography.


<details>
  <summary>Details</summary>
Motivation: Traditional 2D echocardiograms lack 3D assessment, while 3D echocardiography has resolution and cost issues. S2MNet aims to bridge this gap.

Method: S2MNet uses simulated 2D slices from 3D heart meshes and a deformation field-based approach to avoid artifacts.

Result: The method shows strong correlation between estimated left ventricular volume and clinical measurements, validating its reliability.

Conclusion: S2MNet provides a reliable and cost-effective solution for 3D cardiac assessment using routine 2D echocardiograms.

Abstract: Echocardiogram is the most commonly used imaging modality in cardiac
assessment duo to its non-invasive nature, real-time capability, and
cost-effectiveness. Despite its advantages, most clinical echocardiograms
provide only two-dimensional views, limiting the ability to fully assess
cardiac anatomy and function in three dimensions. While three-dimensional
echocardiography exists, it often suffers from reduced resolution, limited
availability, and higher acquisition costs. To overcome these challenges, we
propose a deep learning framework S2MNet that reconstructs continuous and
high-fidelity 3D heart models by integrating six slices of routinely acquired
2D echocardiogram views. Our method has three advantages. First, our method
avoid the difficulties on training data acquasition by simulate six of 2D
echocardiogram images from corresponding slices of a given 3D heart mesh.
Second, we introduce a deformation field-based method, which avoid spatial
discontinuities or structural artifacts in 3D echocardiogram reconstructions.
We validate our method using clinically collected echocardiogram and
demonstrate that our estimated left ventricular volume, a key clinical
indicator of cardiac function, is strongly correlated with the doctor measured
GLPS, a clinical measurement that should demonstrate a negative correlation
with LVE in medical theory. This association confirms the reliability of our
proposed 3D construction method.

</details>


### [290] [The Application of Deep Learning for Lymph Node Segmentation: A Systematic Review](https://arxiv.org/pdf/2505.06118)
*Jingguo Qu, Xinyang Han, Man-Lik Chui, Yao Pu, Simon Takadiyi Gunda, Ziman Chen, Jing Qin, Ann Dorothy King, Winnie Chiu-Wing Chu, Jing Cai, Michael Tin-Cheung Ying*

Main category: eess.IV

TL;DR: The paper reviews deep learning's role in lymph node segmentation, highlighting its potential to improve accuracy over traditional methods, while addressing current challenges and future research directions.


<details>
  <summary>Details</summary>
Motivation: To advance early cancer detection and staging by improving lymph node segmentation accuracy, overcoming limitations of traditional methods.

Method: Evaluates deep learning architectures like CNNs, encoder-decoder networks, and transformers for lymph node segmentation in medical imaging.

Result: Identifies challenges like lymph node shape diversity, dataset scarcity, and lack of generalizable methods, despite deep learning's advancements.

Conclusion: Proposes future research directions, including multimodal fusion, transfer learning, and large-scale pre-trained models, to enhance segmentation and cancer diagnosis.

Abstract: Automatic lymph node segmentation is the cornerstone for advances in computer
vision tasks for early detection and staging of cancer. Traditional
segmentation methods are constrained by manual delineation and variability in
operator proficiency, limiting their ability to achieve high accuracy. The
introduction of deep learning technologies offers new possibilities for
improving the accuracy of lymph node image analysis. This study evaluates the
application of deep learning in lymph node segmentation and discusses the
methodologies of various deep learning architectures such as convolutional
neural networks, encoder-decoder networks, and transformers in analyzing
medical imaging data across different modalities. Despite the advancements, it
still confronts challenges like the shape diversity of lymph nodes, the
scarcity of accurately labeled datasets, and the inadequate development of
methods that are robust and generalizable across different imaging modalities.
To the best of our knowledge, this is the first study that provides a
comprehensive overview of the application of deep learning techniques in lymph
node segmentation task. Furthermore, this study also explores potential future
research directions, including multimodal fusion techniques, transfer learning,
and the use of large-scale pre-trained models to overcome current limitations
while enhancing cancer diagnosis and treatment planning strategies.

</details>


### [291] [Topo-VM-UNetV2: Encoding Topology into Vision Mamba UNet for Polyp Segmentation](https://arxiv.org/pdf/2505.06210)
*Diego Adame, Jose A. Nunez, Fabian Vazquez, Nayeli Gurrola, Huimin Li, Haoteng Tang, Bin Fu, Pengfei Gu*

Main category: eess.IV

TL;DR: The paper proposes Topo-VM-UNetV2, a method integrating topological features into Mamba-based VM-UNetV2 for improved polyp segmentation, addressing limitations of CNNs and Transformers.


<details>
  <summary>Details</summary>
Motivation: CNNs lack long-range dependency modeling, while Transformers have high computational complexity. Mamba-based models struggle with topological features, leading to inaccurate segmentation.

Method: Two-stage approach: 1) Generate probability maps and compute topology attention maps using persistence diagrams. 2) Integrate these maps into VM-UNetV2's SDI module to form Topo-SDI.

Result: Extensive experiments on five datasets show the method's effectiveness.

Conclusion: Topo-VM-UNetV2 enhances polyp segmentation by incorporating topological features, outperforming existing models.

Abstract: Convolutional neural network (CNN) and Transformer-based architectures are
two dominant deep learning models for polyp segmentation. However, CNNs have
limited capability for modeling long-range dependencies, while Transformers
incur quadratic computational complexity. Recently, State Space Models such as
Mamba have been recognized as a promising approach for polyp segmentation
because they not only model long-range interactions effectively but also
maintain linear computational complexity. However, Mamba-based architectures
still struggle to capture topological features (e.g., connected components,
loops, voids), leading to inaccurate boundary delineation and polyp
segmentation. To address these limitations, we propose a new approach called
Topo-VM-UNetV2, which encodes topological features into the Mamba-based
state-of-the-art polyp segmentation model, VM-UNetV2. Our method consists of
two stages: Stage 1: VM-UNetV2 is used to generate probability maps (PMs) for
the training and test images, which are then used to compute topology attention
maps. Specifically, we first compute persistence diagrams of the PMs, then we
generate persistence score maps by assigning persistence values (i.e., the
difference between death and birth times) of each topological feature to its
birth location, finally we transform persistence scores into attention weights
using the sigmoid function. Stage 2: These topology attention maps are
integrated into the semantics and detail infusion (SDI) module of VM-UNetV2 to
form a topology-guided semantics and detail infusion (Topo-SDI) module for
enhancing the segmentation results. Extensive experiments on five public polyp
segmentation datasets demonstrate the effectiveness of our proposed method. The
code will be made publicly available.

</details>


### [292] [Distributional Drift Detection in Medical Imaging with Sketching and Fine-Tuned Transformer](https://arxiv.org/pdf/2408.08456)
*Yusen Wu, Phuong Nguyen, Rose Yesha, Yelena Yesha*

Main category: eess.IV

TL;DR: The paper introduces a sensitive and accurate method for detecting distributional drift in CT-scan medical images using data-sketching and fine-tuning, achieving high accuracy (99.11%) and robustness against noise.


<details>
  <summary>Details</summary>
Motivation: Ensuring model reliability in medical applications by detecting data distribution changes that could impact predictions, addressing limitations of current drift detection methods.

Method: Combines data-sketching for real-time anomaly detection and fine-tunes a Vision Transformer model for feature extraction, validated on mammography data.

Result: Achieved 99.11% accuracy, improved cosine similarity scores (50% to 99.1%), and high sensitivity to noise (1% salt-and-pepper/speckle), unaffected by lighting conditions.

Conclusion: The proposed scalable solution enhances diagnostic model accuracy in dynamic clinical settings.

Abstract: Distributional drift detection is important in medical applications as it
helps ensure the accuracy and reliability of models by identifying changes in
the underlying data distribution that could affect the prediction results of
machine learning models. However, current methods have limitations in detecting
drift, for example, the inclusion of abnormal datasets can lead to unfair
comparisons. This paper presents an accurate and sensitive approach to detect
distributional drift in CT-scan medical images by leveraging data-sketching and
fine-tuning techniques. We developed a robust baseline library model for
real-time anomaly detection, allowing for efficient comparison of incoming
images and identification of anomalies. Additionally, we fine-tuned a
pre-trained Vision Transformer model to extract relevant features, using
mammography as a case study, significantly enhancing model accuracy to 99.11%.
Combining with data-sketches and fine-tuning, our feature extraction evaluation
demonstrated that cosine similarity scores between similar datasets provide
greater improvements, from around 50% increased to 99.1%. Finally, the
sensitivity evaluation shows that our solutions are highly sensitive to even 1%
salt-and-pepper and speckle noise, and it is not sensitive to lighting noise
(e.g., lighting conditions have no impact on data drift). The proposed methods
offer a scalable and reliable solution for maintaining the accuracy of
diagnostic models in dynamic clinical environments.

</details>


### [293] [End-to-end localized deep learning for Cryo-ET](https://arxiv.org/pdf/2501.15246)
*Vinith Kishore, Valentin Debarnot, Ricardo D. Righetto, AmirEhsan Khorashadizadeh, Benjamin D. Engel, Ivan Dokmanić*

Main category: eess.IV

TL;DR: CryoLithe is a memory-efficient, end-to-end deep learning network for cryo-ET reconstruction, overcoming limitations of traditional methods like FBP and avoiding retraining for new datasets.


<details>
  <summary>Details</summary>
Motivation: Current cryo-ET reconstruction methods face challenges like high computational costs, memory demands, and lack of ground truth for supervised learning. CryoLithe aims to address these issues.

Method: CryoLithe uses a local, transform-domain approach to directly estimate volumes from aligned tilt-series, avoiding the need for retraining or fine-tuning.

Result: The method demonstrates robustness to distribution shifts and achieves excellent results on real data without retraining.

Conclusion: CryoLithe provides a practical, efficient solution for high-resolution cryo-ET reconstruction, outperforming existing methods.

Abstract: Cryo-electron tomography (cryo-ET) enables 3D visualization of cellular
environments. Accurate reconstruction of high-resolution volumes is complicated
by the very low signal-to-noise ratio and a restricted range of sample tilts,
creating a missing wedge of Fourier information. Recent self-supervised deep
learning approaches, which post-process initial reconstructions done by
filtered backprojection (FBP), have significantly improved reconstruction
quality, but they are computationally expensive, demand large memory, and
require retraining for each new dataset. End-to-end supervised learning is an
appealing alternative but is impeded by the lack of ground truth and the large
memory demands of high-resolution volumetric data. Training on synthetic data
often leads to overfitting and poor generalization to real data, and, to date,
no general end-to-end deep learning reconstructors exist for cryo-ET. In this
work, we introduce CryoLithe, a local, memory-efficient reconstruction network
that directly estimates the volume from an aligned tilt-series, overcoming the
suboptimal FBP. We demonstrate that leveraging transform-domain locality makes
our network robust to distribution shifts, enabling effective supervised
training and giving excellent results on real data -- without retraining or
fine-tuning.

</details>


### [294] [Generative Autoregressive Transformers for Model-Agnostic Federated MRI Reconstruction](https://arxiv.org/pdf/2502.04521)
*Valiyeh A. Nezhad, Gokberk Elmas, Bilal Kabas, Fuat Arslan, Tolga Çukur*

Main category: eess.IV

TL;DR: FedGAT introduces a model-agnostic federated learning technique using generative autoregressive transformers to enable flexible, model-heterogeneous collaborations for MRI reconstruction, improving generalization without sharing data.


<details>
  <summary>Details</summary>
Motivation: Single-site MRI reconstruction models lack generalization due to limited local datasets, and conventional FL restricts sites to homogeneous architectures, limiting flexibility.

Method: FedGAT trains a global generative prior for multi-site MR image distribution, synthesizes site-specific synthetic images, and allows sites to train their own models on hybrid datasets.

Result: FedGAT outperforms state-of-the-art FL baselines in within-site and cross-site reconstruction performance.

Conclusion: FedGAT enables privacy-preserving, model-heterogeneous collaborations, enhancing MRI reconstruction generalization across diverse datasets.

Abstract: Although learning-based models hold great promise for MRI reconstruction,
single-site models built on limited local datasets often suffer from poor
generalization. This challenge has spurred interest in collaborative model
training on multi-site datasets via federated learning (FL) -- a
privacy-preserving framework that aggregates model updates instead of sharing
imaging data. Conventional FL aggregates locally trained model weights into a
global model, inherently constraining all sites to use a homogeneous model
architecture. This rigidity forces sites to compromise on architectures
tailored to their compute resources and application-specific needs, making
conventional FL unsuitable for model-heterogeneous settings where each site may
prefer a distinct architecture. To overcome this limitation, we introduce
FedGAT, a novel model-agnostic FL technique based on generative autoregressive
transformers. FedGAT decentralizes the training of a global generative prior
that learns the distribution of multi-site MR images. For high-fidelity
synthesis, we propose a novel site-prompted GAT prior that controllably
synthesizes realistic MR images from desired sites via autoregressive
prediction across spatial scales. Each site then trains its own reconstruction
model -- using an architecture of its choice -- on a hybrid dataset augmenting
its local MRI dataset with GAT-generated synthetic MR images emulating datasets
from other sites. This hybrid training strategy enables site-specific
reconstruction models to generalize more effectively across diverse data
distributions while preserving data privacy. Comprehensive experiments on
multi-institutional datasets demonstrate that FedGAT enables flexible,
model-heterogeneous collaborations and achieves superior within-site and
cross-site reconstruction performance compared to state-of-the-art FL
baselines.

</details>


### [295] [MAISY: Motion-Aware Image SYnthesis for Medical Image Motion Correction](https://arxiv.org/pdf/2505.04105)
*Andrew Zhang, Hao Wang, Shuchang Ye, Michael Fulham, Jinman Kim*

Main category: eess.IV

TL;DR: MAISY improves motion artifact correction in medical images by dynamically learning spatial patterns and using a new VS-SSIM loss, outperforming existing GAN-based methods.


<details>
  <summary>Details</summary>
Motivation: Current GAN-based methods for motion artifact correction overlook localized features and struggle with varying image properties, limiting their effectiveness.

Method: MAISY uses the Segment Anything Model (SAM) to learn spatial patterns and introduces VS-SSIM loss to focus on high-variance regions.

Result: MAISY increases PSNR by 40%, SSIM by 10%, and Dice by 16% compared to state-of-the-art methods.

Conclusion: MAISY addresses limitations of existing methods, improving motion artifact correction and preserving critical anatomical details.

Abstract: Patient motion during medical image acquisition causes blurring, ghosting,
and distorts organs, which makes image interpretation challenging. Current
state-of-the-art algorithms using Generative Adversarial Network (GAN)-based
methods with their ability to learn the mappings between corrupted images and
their ground truth via Structural Similarity Index Measure (SSIM) loss
effectively generate motion-free images. However, we identified the following
limitations: (i) they mainly focus on global structural characteristics and
therefore overlook localized features that often carry critical pathological
information, and (ii) the SSIM loss function struggles to handle images with
varying pixel intensities, luminance factors, and variance. In this study, we
propose Motion-Aware Image SYnthesis (MAISY) which initially characterize
motion and then uses it for correction by: (a) leveraging the foundation model
Segment Anything Model (SAM), to dynamically learn spatial patterns along
anatomical boundaries where motion artifacts are most pronounced and, (b)
introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively
emphasizes spatial regions with high pixel variance to preserve essential
anatomical details during artifact correction. Experiments on chest and head CT
datasets demonstrate that our model outperformed the state-of-the-art
counterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by
10%, and Dice by 16%.

</details>
