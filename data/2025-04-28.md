<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 57]
- [cs.CV](#cs.CV) [Total: 107]
- [cs.AI](#cs.AI) [Total: 19]
- [cs.SD](#cs.SD) [Total: 5]
- [cs.LG](#cs.LG) [Total: 98]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [eess.AS](#eess.AS) [Total: 6]
- [eess.IV](#eess.IV) [Total: 14]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Optimism, Expectation, or Sarcasm? Multi-Class Hope Speech Detection in Spanish and English](https://arxiv.org/abs/2504.17974)
*Sabur Butt, Fazlourrahman Balouchzahi, Ahmad Imam Amjad, Maaz Amjad, Hector G. Ceballos, Salud Maria Jimenez-Zafra*

Main category: cs.CL

TL;DR: PolyHope V2 is a multilingual dataset for nuanced hope speech detection, showing fine-tuned transformers outperform LLMs like GPT-4 in distinguishing hope subtypes and sarcasm.


<details>
  <summary>Details</summary>
Motivation: Hope is complex and underexplored in NLP, with nuanced forms like sarcasm making detection challenging.

Method: Created PolyHope V2, a dataset of 30K annotated tweets in English/Spanish, benchmarked transformers and LLMs (GPT-4, Llama 3) under zero/few-shot settings.

Result: Fine-tuned transformers outperformed LLMs, especially in detecting nuanced hope categories and sarcasm.

Conclusion: PolyHope V2 and findings advance emotion recognition with semantic and contextual sensitivity across languages.

Abstract: Hope is a complex and underexplored emotional state that plays a significant
role in education, mental health, and social interaction. Unlike basic
emotions, hope manifests in nuanced forms ranging from grounded optimism to
exaggerated wishfulness or sarcasm, making it difficult for Natural Language
Processing systems to detect accurately. This study introduces PolyHope V2, a
multilingual, fine-grained hope speech dataset comprising over 30,000 annotated
tweets in English and Spanish. This resource distinguishes between four hope
subtypes Generalized, Realistic, Unrealistic, and Sarcastic and enhances
existing datasets by explicitly labeling sarcastic instances. We benchmark
multiple pretrained transformer models and compare them with large language
models (LLMs) such as GPT 4 and Llama 3 under zero-shot and few-shot regimes.
Our findings show that fine-tuned transformers outperform prompt-based LLMs,
especially in distinguishing nuanced hope categories and sarcasm. Through
qualitative analysis and confusion matrices, we highlight systematic challenges
in separating closely related hope subtypes. The dataset and results provide a
robust foundation for future emotion recognition tasks that demand greater
semantic and contextual sensitivity across languages.

</details>


### [2] [Improving LLM Personas via Rationalization with Psychological Scaffolds](https://arxiv.org/abs/2504.17993)
*Brihi Joshi, Xiang Ren, Swabha Swayamdipta, Rik Koncel-Kedziorski, Tim Paek*

Main category: cs.CL

TL;DR: PB&J enhances LLM personas by adding rationales for user judgments, improving prediction accuracy over demographic-only methods.


<details>
  <summary>Details</summary>
Motivation: Existing persona methods lack reasoning behind user judgments, limiting their effectiveness.

Method: PB&J uses psychological scaffolds (e.g., Big 5 Traits) to generate rationales for user behavior.

Result: PB&J-augmented personas outperform demographic-only methods and match human-written rationales in performance.

Conclusion: Incorporating psychological rationales improves LLM personas, offering better user preference predictions.

Abstract: Language models prompted with a user description or persona can predict a
user's preferences and opinions, but existing approaches to building personas
-- based solely on a user's demographic attributes and/or prior judgments --
fail to capture the underlying reasoning behind said user judgments. We
introduce PB&J (Psychology of Behavior and Judgments), a framework that
improves LLM personas by incorporating rationales of why a user might make
specific judgments. These rationales are LLM-generated, and aim to reason about
a user's behavior on the basis of their experiences, personality traits or
beliefs. This is done using psychological scaffolds -- structured frameworks
grounded in theories such as the Big 5 Personality Traits and Primal World
Beliefs -- that help provide structure to the generated rationales. Experiments
on public opinion and movie preference prediction tasks demonstrate that LLM
personas augmented with PB&J rationales consistently outperform methods using
only a user's demographics and/or judgments. Additionally, LLM personas
constructed using scaffolds describing user beliefs perform competitively with
those using human-written rationales.

</details>


### [3] [Memory Reviving, Continuing Learning and Beyond: Evaluation of Pre-trained Encoders and Decoders for Multimodal Machine Translation](https://arxiv.org/abs/2504.18012)
*Zhuang Yu, Shiliang Sun, Jing Zhao, Tengfei Song, Hao Yang*

Main category: cs.CL

TL;DR: The paper explores the impact of pre-trained encoders and decoders in Multimodal Machine Translation (MMT), revealing their asymmetrical benefits and the role of visual-text alignment.


<details>
  <summary>Details</summary>
Motivation: To understand how pre-trained language and vision models affect MMT, given their underexplored role in multimodal settings.

Method: Systematic study of training strategies (from scratch to pre-trained/frozen components) in a unified MMT framework, tested on Multi30K and CoMMuTE datasets.

Result: Pre-trained decoders improve fluency and accuracy, while pre-trained encoders' effectiveness depends on visual-text alignment.

Conclusion: Pre-training is crucial but asymmetrical in MMT, with insights for future multimodal translation system designs.

Abstract: Multimodal Machine Translation (MMT) aims to improve translation quality by
leveraging auxiliary modalities such as images alongside textual input. While
recent advances in large-scale pre-trained language and vision models have
significantly benefited unimodal natural language processing tasks, their
effectiveness and role in MMT remain underexplored. In this work, we conduct a
systematic study on the impact of pre-trained encoders and decoders in
multimodal translation models. Specifically, we analyze how different training
strategies, from training from scratch to using pre-trained and partially
frozen components, affect translation performance under a unified MMT
framework. Experiments are carried out on the Multi30K and CoMMuTE dataset
across English-German and English-French translation tasks. Our results reveal
that pre-training plays a crucial yet asymmetrical role in multimodal settings:
pre-trained decoders consistently yield more fluent and accurate outputs, while
pre-trained encoders show varied effects depending on the quality of
visual-text alignment. Furthermore, we provide insights into the interplay
between modality fusion and pre-trained components, offering guidance for
future architecture design in multimodal translation systems.

</details>


### [4] [RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models](https://arxiv.org/abs/2504.18041)
*Bang An, Shiyue Zhang, Mark Dredze*

Main category: cs.CL

TL;DR: RAG frameworks can reduce LLM safety and alter safety profiles, even with safe models and documents. Existing red teaming methods are less effective for RAG, calling for tailored safety research.


<details>
  <summary>Details</summary>
Motivation: Despite RAG's popularity, its impact on LLM safety is understudied, prompting a comparative analysis of RAG and non-RAG frameworks.

Method: Comparative analysis of RAG and non-RAG frameworks with eleven LLMs, examining safety changes and evaluating red teaming methods.

Result: RAG can reduce model safety and alter safety profiles, with safe models and documents still causing unsafe outputs. Red teaming methods are less effective for RAG.

Conclusion: Tailored safety research and red-teaming methods are needed for RAG LLMs due to their unique safety challenges.

Abstract: Efforts to ensure the safety of large language models (LLMs) include safety
fine-tuning, evaluation, and red teaming. However, despite the widespread use
of the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses
on standard LLMs, which means we know little about how RAG use cases change a
model's safety profile. We conduct a detailed comparative analysis of RAG and
non-RAG frameworks with eleven LLMs. We find that RAG can make models less safe
and change their safety profile. We explore the causes of this change and find
that even combinations of safe models with safe documents can cause unsafe
generations. In addition, we evaluate some existing red teaming methods for RAG
settings and show that they are less effective than when used for non-RAG
settings. Our work highlights the need for safety research and red-teaming
methods specifically tailored for RAG LLMs.

</details>


### [5] [DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models](https://arxiv.org/abs/2504.18053)
*Jianyu Liu, Hangyu Guo, Ranjie Duan, Xingyuan Bu, Yancheng He, Shilong Li, Hui Huang, Jiaheng Liu, Yucheng Wang, Chenchen Jing, Xingwei Qu, Xiao Zhang, Yingshui Tan, Yanan Wu, Jihao Gu, Yangguang Li, Jianke Zhu*

Main category: cs.CL

TL;DR: DREAM enhances safety in Multimodal Large Language Models (MLLMs) by disentangling risks and using supervised fine-tuning and RLAIF, achieving a 16.17% improvement over GPT-4V.


<details>
  <summary>Details</summary>
Motivation: Addressing unique safety challenges in MLLMs due to multimodal inputs, which introduce new attack dimensions and complex risks.

Method: Proposes DREAM, combining multimodal risk disentanglement, supervised fine-tuning, and iterative RLAIF.

Result: DREAM improves safety by 16.17% (SIUO score) without compromising normal task performance.

Conclusion: DREAM effectively enhances safety alignment in MLLMs, outperforming existing methods like GPT-4V.

Abstract: Multimodal Large Language Models (MLLMs) pose unique safety challenges due to
their integration of visual and textual data, thereby introducing new
dimensions of potential attacks and complex risk combinations. In this paper,
we begin with a detailed analysis aimed at disentangling risks through
step-by-step reasoning within multimodal inputs. We find that systematic
multimodal risk disentanglement substantially enhances the risk awareness of
MLLMs. Via leveraging the strong discriminative abilities of multimodal risk
disentanglement, we further introduce \textbf{DREAM}
(\textit{\textbf{D}isentangling \textbf{R}isks to \textbf{E}nhance Safety
\textbf{A}lignment in \textbf{M}LLMs}), a novel approach that enhances safety
alignment in MLLMs through supervised fine-tuning and iterative Reinforcement
Learning from AI Feedback (RLAIF). Experimental results show that DREAM
significantly boosts safety during both inference and training phases without
compromising performance on normal tasks (namely oversafety), achieving a
16.17\% improvement in the SIUO safe\&effective score compared to GPT-4V. The
data and code are available at https://github.com/Kizna1ver/DREAM.

</details>


### [6] [Exploring Personality-Aware Interactions in Salesperson Dialogue Agents](https://arxiv.org/abs/2504.18058)
*Sijia Cheng, Wen-Yu Chang, Yun-Nung Chen*

Main category: cs.CL

TL;DR: The study examines how MBTI-defined user personas affect sales-oriented dialogue agents, revealing insights into interaction quality, adaptability, and personalization.


<details>
  <summary>Details</summary>
Motivation: To understand how diverse user personas impact dialogue agent performance in sales, aiming to improve adaptability and personalization.

Method: Large-scale testing and analysis of pre-trained agents across MBTI-defined user types, evaluating effectiveness and interaction dynamics.

Result: Significant patterns in interaction quality, task completion, and dialogue naturalness, with potential for refining agent strategies.

Conclusion: The work offers insights for adaptive, user-centric dialogue systems and releases persona-defined simulators for broader research applications.

Abstract: The integration of dialogue agents into the sales domain requires a deep
understanding of how these systems interact with users possessing diverse
personas. This study explores the influence of user personas, defined using the
Myers-Briggs Type Indicator (MBTI), on the interaction quality and performance
of sales-oriented dialogue agents. Through large-scale testing and analysis, we
assess the pre-trained agent's effectiveness, adaptability, and personalization
capabilities across a wide range of MBTI-defined user types. Our findings
reveal significant patterns in interaction dynamics, task completion rates, and
dialogue naturalness, underscoring the future potential for dialogue agents to
refine their strategies to better align with varying personality traits. This
work not only provides actionable insights for building more adaptive and
user-centric conversational systems in the sales domain but also contributes
broadly to the field by releasing persona-defined user simulators. These
simulators, unconstrained by domain, offer valuable tools for future research
and demonstrate the potential for scaling personalized dialogue systems across
diverse applications.

</details>


### [7] [PropRAG: Guiding Retrieval with Beam Search over Proposition Paths](https://arxiv.org/abs/2504.18070)
*Jingjin Wang*

Main category: cs.CL

TL;DR: PropRAG improves RAG by using propositions and beam search for multi-step reasoning, achieving top performance on benchmarks without online LLM use.


<details>
  <summary>Details</summary>
Motivation: Standard RAG lacks associativity and sense-making, while structured RAG methods like HippoRAG suffer from context loss. PropRAG aims to address these limitations.

Method: PropRAG leverages propositions and a beam search algorithm for multi-step reasoning, using efficient graph traversal and pre-computed embeddings, avoiding online LLM inference.

Result: Achieves state-of-the-art results on PopQA (55.3%), 2Wiki (93.7%), HotpotQA (97.0%), and MuSiQue (77.3%) with high F1 scores (e.g., 52.4% on MuSiQue).

Conclusion: PropRAG advances non-parametric continual learning by improving evidence retrieval through richer representation and explicit, LLM-free online path finding.

Abstract: Retrieval Augmented Generation (RAG) has become the standard non-parametric
approach for equipping Large Language Models (LLMs) with up-to-date knowledge
and mitigating catastrophic forgetting common in continual learning. However,
standard RAG, relying on independent passage retrieval, fails to capture the
interconnected nature of human memory crucial for complex reasoning
(associativity) and contextual understanding (sense-making). While structured
RAG methods like HippoRAG utilize knowledge graphs (KGs) built from triples,
the inherent context loss limits fidelity. We introduce PropRAG, a framework
leveraging contextually rich propositions and a novel beam search algorithm
over proposition paths to explicitly discover multi-step reasoning chains.
Crucially, PropRAG's online retrieval process operates entirely without
invoking generative LLMs, relying instead on efficient graph traversal and
pre-computed embeddings. This avoids online LLM inference costs and potential
inconsistencies during evidence gathering. LLMs are used effectively offline
for high-quality proposition extraction and post-retrieval for answer
generation. PropRAG achieves state-of-the-art zero-shot Recall@5 results on
PopQA (55.3%), 2Wiki (93.7%), HotpotQA (97.0%), and MuSiQue (77.3%), alongside
top F1 scores (e.g., 52.4% on MuSiQue). By improving evidence retrieval through
richer representation and explicit, LLM-free online path finding, PropRAG
advances non-parametric continual learning.

</details>


### [8] [Stabilizing Reasoning in Medical LLMs with Continued Pretraining and Reasoning Preference Optimization](https://arxiv.org/abs/2504.18080)
*Wataru Kawakami, Keita Suzuki, Junichiro Iwasawa*

Main category: cs.CL

TL;DR: Preferred-MedLLM-Qwen-72B, a 72B-parameter LLM for Japanese medicine, achieves high accuracy and stable reasoning via two-stage fine-tuning, outperforming GPT-4o on medical exams.


<details>
  <summary>Details</summary>
Motivation: Address concerns over LLMs' factual accuracy, language limitations (Japanese), and unreliable reasoning explanations in medicine.

Method: Two-stage fine-tuning: Continued Pretraining (CPT) on Japanese medical corpus and Reasoning Preference Optimization (RPO) for reliable reasoning.

Result: State-of-the-art 0.868 accuracy on IgakuQA, maintaining performance when generating explanations, unlike baselines.

Conclusion: Optimizing for reliable explanations alongside accuracy is crucial for trustworthy LLMs in high-stakes domains; model weights released for research.

Abstract: Large Language Models (LLMs) show potential in medicine, yet clinical
adoption is hindered by concerns over factual accuracy, language-specific
limitations (e.g., Japanese), and critically, their reliability when required
to generate reasoning explanations -- a prerequisite for trust. This paper
introduces Preferred-MedLLM-Qwen-72B, a 72B-parameter model optimized for the
Japanese medical domain to achieve both high accuracy and stable reasoning. We
employ a two-stage fine-tuning process on the Qwen2.5-72B base model: first,
Continued Pretraining (CPT) on a comprehensive Japanese medical corpus instills
deep domain knowledge. Second, Reasoning Preference Optimization (RPO), a
preference-based method, enhances the generation of reliable reasoning pathways
while preserving high answer accuracy. Evaluations on the Japanese Medical
Licensing Exam benchmark (IgakuQA) show Preferred-MedLLM-Qwen-72B achieves
state-of-the-art performance (0.868 accuracy), surpassing strong proprietary
models like GPT-4o (0.866). Crucially, unlike baseline or CPT-only models which
exhibit significant accuracy degradation (up to 11.5\% and 3.8\% respectively
on IgakuQA) when prompted for explanations, our model maintains its high
accuracy (0.868) under such conditions. This highlights RPO's effectiveness in
stabilizing reasoning generation. This work underscores the importance of
optimizing for reliable explanations alongside accuracy. We release the
Preferred-MedLLM-Qwen-72B model weights to foster research into trustworthy
LLMs for specialized, high-stakes applications.

</details>


### [9] [Random-Set Large Language Models](https://arxiv.org/abs/2504.18085)
*Muhammad Mubashar, Shireen Kudukkil Manchingal, Fabio Cuzzolin*

Main category: cs.CL

TL;DR: The paper introduces RSLLM, a method for uncertainty quantification in LLMs by predicting finite random sets (belief functions) over tokens, improving correctness and detecting hallucinations.


<details>
  <summary>Details</summary>
Motivation: To address trust issues in LLM-generated text by quantifying uncertainty, especially epistemic uncertainty from training data diversity.

Method: Proposes RSLLM, which predicts belief functions over token subsets using hierarchical clustering for scalability, and evaluates it on CoQA and OBQA datasets.

Result: RSLLM outperforms standard LLMs in correctness and provides better uncertainty estimation and hallucination detection.

Conclusion: RSLLM offers a scalable and effective approach for uncertainty quantification in LLMs, enhancing trust and reliability.

Abstract: Large Language Models (LLMs) are known to produce very high-quality tests and
responses to our queries. But how much can we trust this generated text? In
this paper, we study the problem of uncertainty quantification in LLMs. We
propose a novel Random-Set Large Language Model (RSLLM) approach which predicts
finite random sets (belief functions) over the token space, rather than
probability vectors as in classical LLMs. In order to allow so efficiently, we
also present a methodology based on hierarchical clustering to extract and use
a budget of "focal" subsets of tokens upon which the belief prediction is
defined, rather than using all possible collections of tokens, making the
method scalable yet effective. RS-LLMs encode the epistemic uncertainty induced
in their generation process by the size and diversity of its training set via
the size of the credal sets associated with the predicted belief functions. The
proposed approach is evaluated on CoQA and OBQA datasets using Llama2-7b,
Mistral-7b and Phi-2 models and is shown to outperform the standard model in
both datasets in terms of correctness of answer while also showing potential in
estimating the second level uncertainty in its predictions and providing the
capability to detect when its hallucinating.

</details>


### [10] [Application and Optimization of Large Models Based on Prompt Tuning for Fact-Check-Worthiness Estimation](https://arxiv.org/abs/2504.18104)
*Yinglong Yu, Hao Shen, Zhengyi Lyu, Qi He*

Main category: cs.CL

TL;DR: The paper introduces a prompt tuning-based method for fact-check-worthiness estimation, outperforming baseline models like BERT, GPT-3.5, and GPT-4 in accuracy and F1 score.


<details>
  <summary>Details</summary>
Motivation: Addressing misinformation in the era of globalization and informatization by improving fact-check-worthiness estimation.

Method: Uses prompt tuning with designed templates on large language models for in-context learning, especially effective with limited or unlabeled data.

Result: Surpasses or matches baseline methods (BERT, GPT-3.5, GPT-4) in F1 score and accuracy on public datasets.

Conclusion: The prompt tuning-based method is effective and advanced for fact-check-worthiness estimation.

Abstract: In response to the growing problem of misinformation in the context of
globalization and informatization, this paper proposes a classification method
for fact-check-worthiness estimation based on prompt tuning. We construct a
model for fact-check-worthiness estimation at the methodological level using
prompt tuning. By applying designed prompt templates to large language models,
we establish in-context learning and leverage prompt tuning technology to
improve the accuracy of determining whether claims have fact-check-worthiness,
particularly when dealing with limited or unlabeled data. Through extensive
experiments on public datasets, we demonstrate that the proposed method
surpasses or matches multiple baseline methods in the classification task of
fact-check-worthiness estimation assessment, including classical pre-trained
models such as BERT, as well as recent popular large models like GPT-3.5 and
GPT-4. Experiments show that the prompt tuning-based method proposed in this
study exhibits certain advantages in evaluation metrics such as F1 score and
accuracy, thereby effectively validating its effectiveness and advancement in
the task of fact-check-worthiness estimation.

</details>


### [11] [Comparative Study on the Discourse Meaning of Chinese and English Media in the Paris Olympics Based on LDA Topic Modeling Technology and LLM Prompt Engineering](https://arxiv.org/abs/2504.18106)
*Yinglong Yu, Zhaopu Yao, Fang Yuan*

Main category: cs.CL

TL;DR: The study compares Chinese and English media reports on the Paris Olympics using topic modeling, LLM prompt engineering, and corpus phraseology, highlighting differences in discourse and attitudes.


<details>
  <summary>Details</summary>
Motivation: To explore how Chinese and English media construct discourse and express attitudes differently regarding the Paris Olympics.

Method: Topic modeling, Large Language Model (LLM) prompt engineering, and corpus phraseology methods.

Result: Chinese media focus on sports specifics, doping, and technology, while English media emphasize female athletes and eligibility. Chinese reports show positive prosody in sports spirit, while English reports mix positive and negative prosody.

Conclusion: The study reveals distinct discourse patterns and attitudinal differences between Chinese and English media coverage of the Paris Olympics.

Abstract: This study analyzes Chinese and English media reports on the Paris Olympics
using topic modeling, Large Language Model (LLM) prompt engineering, and corpus
phraseology methods to explore similarities and differences in discourse
construction and attitudinal meanings. Common topics include the opening
ceremony, athlete performance, and sponsorship brands. Chinese media focus on
specific sports, sports spirit, doping controversies, and new technologies,
while English media focus on female athletes, medal wins, and eligibility
controversies. Chinese reports show more frequent prepositional co-occurrences
and positive semantic prosody in describing the opening ceremony and sports
spirit. English reports exhibit positive semantic prosody when covering female
athletes but negative prosody in predicting opening ceremony reactions and
discussing women's boxing controversies.

</details>


### [12] [Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection](https://arxiv.org/abs/2504.18114)
*Atharva Kulkarni, Yuan Zhang, Joel Ruben Antony Moniz, Xiou Ge, Bo-Hsiang Tseng, Dhivya Piraviperumal, Swabha Swayamdipta, Hong Yu*

Main category: cs.CL

TL;DR: The paper evaluates hallucination detection metrics in language models, revealing gaps in current methods and suggesting LLM-based evaluation (e.g., GPT-4) as the most effective.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of measuring hallucinations in language models, which hinders their reliability and adoption.

Method: Large-scale empirical evaluation of 6 hallucination metrics across 4 datasets, 37 models, and 5 decoding methods.

Result: Current metrics often misalign with human judgments and lack robustness; GPT-4-based evaluation performs best, and mode-seeking decoding reduces hallucinations.

Conclusion: More robust metrics and mitigation strategies are needed to address hallucinations in language models.

Abstract: Hallucinations pose a significant obstacle to the reliability and widespread
adoption of language models, yet their accurate measurement remains a
persistent challenge. While many task- and domain-specific metrics have been
proposed to assess faithfulness and factuality concerns, the robustness and
generalization of these metrics are still untested. In this paper, we conduct a
large-scale empirical evaluation of 6 diverse sets of hallucination detection
metrics across 4 datasets, 37 language models from 5 families, and 5 decoding
methods. Our extensive investigation reveals concerning gaps in current
hallucination evaluation: metrics often fail to align with human judgments,
take an overtly myopic view of the problem, and show inconsistent gains with
parameter scaling. Encouragingly, LLM-based evaluation, particularly with
GPT-4, yields the best overall results, and mode-seeking decoding methods seem
to reduce hallucinations, especially in knowledge-grounded settings. These
findings underscore the need for more robust metrics to understand and quantify
hallucinations, and better strategies to mitigate them.

</details>


### [13] [Temporal Entailment Pretraining for Clinical Language Models over EHR Data](https://arxiv.org/abs/2504.18128)
*Tatsunori Tanaka, Fi Zheng, Kai Sato, Zhifeng Li, Yuanyun Zhang, Shi Li*

Main category: cs.CL

TL;DR: The paper introduces a temporal entailment pretraining objective for clinical language models, improving performance on tasks like forecasting and diagnosis by leveraging EHR temporality.


<details>
  <summary>Details</summary>
Motivation: Existing clinical language models treat EHRs as static, ignoring their temporal and causal dynamics, limiting their ability to reason over patient trajectories.

Method: The method pretrains models on EHR segments as temporally ordered sentence pairs, training them to classify entailment relationships (entailed, contradictory, neutral) between states over time.

Result: Pretraining on MIMIC IV data achieves state-of-the-art results in temporal clinical QA, early warning prediction, and disease progression modeling.

Conclusion: Temporal entailment pretraining enhances clinical language models' reasoning over time, improving generalization across dynamic healthcare tasks.

Abstract: Clinical language models have achieved strong performance on downstream tasks
by pretraining on domain specific corpora such as discharge summaries and
medical notes. However, most approaches treat the electronic health record as a
static document, neglecting the temporally-evolving and causally entwined
nature of patient trajectories. In this paper, we introduce a novel temporal
entailment pretraining objective for language models in the clinical domain.
Our method formulates EHR segments as temporally ordered sentence pairs and
trains the model to determine whether a later state is entailed by,
contradictory to, or neutral with respect to an earlier state. Through this
temporally structured pretraining task, models learn to perform latent clinical
reasoning over time, improving their ability to generalize across forecasting
and diagnosis tasks. We pretrain on a large corpus derived from MIMIC IV and
demonstrate state of the art results on temporal clinical QA, early warning
prediction, and disease progression modeling.

</details>


### [14] [EDU-NER-2025: Named Entity Recognition in Urdu Educational Texts using XLM-RoBERTa with X (formerly Twitter)](https://arxiv.org/abs/2504.18142)
*Fida Ullah, Muhammad Ahmad, Muhammad Tayyab Zamir, Muhammad Arif, Grigori sidorov, Edgardo Manuel Felipe Riverón, Alexander Gelbukh*

Main category: cs.CL

TL;DR: The paper addresses the lack of annotated datasets for Urdu NER in the education domain, introduces EDU-NER-2025, and discusses linguistic challenges.


<details>
  <summary>Details</summary>
Motivation: NER in Urdu, especially for education, is underexplored due to missing datasets, limiting model accuracy for domain-specific entities.

Method: Created a manually annotated dataset (EDU-NER-2025) with 13 education-related entities, detailed annotation guidelines, and addressed linguistic challenges.

Result: Produced EDU-NER-2025, a dataset for Urdu NER in education, and analyzed linguistic issues like morphological complexity.

Conclusion: The study fills a gap in Urdu NER for education, providing a dataset and insights into linguistic challenges.

Abstract: Named Entity Recognition (NER) plays a pivotal role in various Natural
Language Processing (NLP) tasks by identifying and classifying named entities
(NEs) from unstructured data into predefined categories such as person,
organization, location, date, and time. While extensive research exists for
high-resource languages and general domains, NER in Urdu particularly within
domain-specific contexts like education remains significantly underexplored.
This is Due to lack of annotated datasets for educational content which limits
the ability of existing models to accurately identify entities such as academic
roles, course names, and institutional terms, underscoring the urgent need for
targeted resources in this domain. To the best of our knowledge, no dataset
exists in the domain of the Urdu language for this purpose. To achieve this
objective this study makes three key contributions. Firstly, we created a
manually annotated dataset in the education domain, named EDU-NER-2025, which
contains 13 unique most important entities related to education domain. Second,
we describe our annotation process and guidelines in detail and discuss the
challenges of labelling EDU-NER-2025 dataset. Third, we addressed and analyzed
key linguistic challenges, such as morphological complexity and ambiguity,
which are prevalent in formal Urdu texts.

</details>


### [15] [Aligning Language Models for Icelandic Legal Text Summarization](https://arxiv.org/abs/2504.18180)
*Þórir Hrafn Harðarson, Hrafn Loftsson, Stefán Ólafsson*

Main category: cs.CL

TL;DR: Preference-based training improves legal accuracy in Icelandic legal summaries but not overall language quality, highlighting the need for qualitative evaluation.


<details>
  <summary>Details</summary>
Motivation: To address challenges in applying language models to legal texts due to specialized terminology and formal style, focusing on Icelandic legal summaries.

Method: Comparison of models fine-tuned with Reinforcement Learning from Human Feedback and Direct Preference Optimization versus conventional supervised learning.

Result: Preference training enhances legal accuracy but not Icelandic language quality, with discrepancies between automated and human evaluations.

Conclusion: Qualitative assessment is crucial for developing effective legal domain language models.

Abstract: The integration of language models in the legal domain holds considerable
promise for streamlining processes and improving efficiency in managing
extensive workloads. However, the specialized terminology, nuanced language,
and formal style of legal texts can present substantial challenges. This study
examines whether preference-based training techniques, specifically
Reinforcement Learning from Human Feedback and Direct Preference Optimization,
can enhance models' performance in generating Icelandic legal summaries that
align with domain-specific language standards and user preferences. We compare
models fine-tuned with preference training to those using conventional
supervised learning. Results indicate that preference training improves the
legal accuracy of generated summaries over standard fine-tuning but does not
significantly enhance the overall quality of Icelandic language usage.
Discrepancies between automated metrics and human evaluations further
underscore the importance of qualitative assessment in developing language
models for the legal domain.

</details>


### [16] [Optimising ChatGPT for creativity in literary translation: A case study from English into Dutch, Chinese, Catalan and Spanish](https://arxiv.org/abs/2504.18221)
*Shuxiang Du, Ana Guerberof Arenas, Antonio Toral, Kyo Gerrits, Josep Marco Borillo*

Main category: cs.CL

TL;DR: ChatGPT's creative translations perform best with minimal instructions and temperature 1.0, outperforming DeepL in some languages but still lagging behind human translation.


<details>
  <summary>Details</summary>
Motivation: To explore how different configurations (text granularity, temperature, prompting) affect ChatGPT's creative translation performance in literary texts.

Method: Evaluated GPT translations in four languages using a Creativity Score formula across six configurations.

Result: Minimal instruction ('Translate creatively') at temperature 1.0 yielded the best creative translations, surpassing DeepL in Spanish, Dutch, and Chinese.

Conclusion: While ChatGPT can produce creative translations under optimal settings, it consistently underperforms compared to human translation.

Abstract: This study examines the variability of Chat-GPT machine translation (MT)
outputs across six different configurations in four languages,with a focus on
creativity in a literary text. We evaluate GPT translations in different text
granularity levels, temperature settings and prompting strategies with a
Creativity Score formula. We found that prompting ChatGPT with a minimal
instruction yields the best creative translations, with "Translate the
following text into [TG] creatively" at the temperature of 1.0 outperforming
other configurations and DeepL in Spanish, Dutch, and Chinese. Nonetheless,
ChatGPT consistently underperforms compared to human translation (HT).

</details>


### [17] [Even Small Reasoners Should Quote Their Sources: Introducing the Pleias-RAG Model Family](https://arxiv.org/abs/2504.18225)
*Pierre-Carl Langlais, Pavel Chizhov, Mattia Nee, Carlos Rosas Hinostroza, Matthieu Delsart, Irène Girard, Othman Hicheur, Anastasia Stasenko, Ivan P. Yamshchikov*

Main category: cs.CL

TL;DR: Pleias-RAG-350m and Pleias-RAG-1B are small reasoning models for RAG tasks, outperforming similar-sized models and competing with larger ones, while excelling in multilingual support and factuality.


<details>
  <summary>Details</summary>
Motivation: To develop efficient, smaller models for RAG tasks that maintain high performance, multilingual support, and factuality, suitable for constrained infrastructure.

Method: Mid-trained on a synthetic dataset emulating multilingual open-source retrieval, with features like citation support, query routing, reformulation, and source reranking.

Result: Outperform SLMs below 4B parameters on RAG benchmarks (HotPotQA, 2wiki) and compete with larger models like Qwen-2.5-7B, Llama-3.1-8B, and Gemma-3-4B.

Conclusion: These models enable new generative AI use cases due to their size, ease of deployment, and high factuality, especially in multilingual contexts.

Abstract: We introduce a new generation of small reasoning models for RAG, search, and
source summarization. Pleias-RAG-350m and Pleias-RAG-1B are mid-trained on a
large synthetic dataset emulating the retrieval of a wide variety of
multilingual open sources from the Common Corpus. They provide native support
for citation and grounding with literal quotes and reintegrate multiple
features associated with RAG workflows, such as query routing, query
reformulation, and source reranking. Pleias-RAG-350m and Pleias-RAG-1B
outperform SLMs below 4 billion parameters on standardized RAG benchmarks
(HotPotQA, 2wiki) and are competitive with popular larger models, including
Qwen-2.5-7B, Llama-3.1-8B, and Gemma-3-4B. They are the only SLMs to date
maintaining consistent RAG performance across leading European languages and
ensuring systematic reference grounding for statements. Due to their size and
ease of deployment on constrained infrastructure and higher factuality by
design, the models unlock a range of new use cases for generative AI.

</details>


### [18] [Efficient Single-Pass Training for Multi-Turn Reasoning](https://arxiv.org/abs/2504.18246)
*Ritesh Goru, Shanay Mehta, Prateek Jain*

Main category: cs.CL

TL;DR: Proposes a method to efficiently fine-tune LLMs on multi-turn reasoning datasets by duplicating response tokens and using a custom attention mask.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs on multi-turn reasoning datasets is challenging due to the exclusion of reasoning tokens in subsequent inputs, preventing single forward pass optimization.

Method: Uses response token duplication and a custom attention mask to enforce visibility constraints.

Result: Reduces training time and enables efficient fine-tuning on multi-turn reasoning datasets.

Conclusion: The approach effectively addresses the limitation of processing multi-turn reasoning conversations in a single forward pass.

Abstract: Training Large Language Models ( LLMs) to generate explicit reasoning before
they produce an answer has been shown to improve their performance across
various tasks such as mathematics and coding. However, fine-tuning LLMs on
multi-turn reasoning datasets presents a unique challenge: LLMs must generate
reasoning tokens that are excluded from subsequent inputs to the LLM. This
discrepancy prevents us from processing an entire conversation in a single
forward pass-an optimization readily available when we fine-tune on a
multi-turn non-reasoning dataset. This paper proposes a novel approach that
overcomes this limitation through response token duplication and a custom
attention mask that enforces appropriate visibility constraints. Our approach
significantly reduces the training time and allows efficient fine-tuning on
multi-turn reasoning datasets.

</details>


### [19] [MAGI: Multi-Agent Guided Interview for Psychiatric Assessment](https://arxiv.org/abs/2504.18260)
*Guanqun Bi, Zhuang Chen, Zhoufu Liu, Hongkai Wang, Xiyao Xiao, Yuqiang Xie, Wen Zhang, Yongkang Huang, Yuxuan Chen, Libiao Peng, Yi Feng, Minlie Huang*

Main category: cs.CL

TL;DR: MAGI automates the MINI psychiatric interview using multi-agent LLMs, improving clinical rigor and adaptability.


<details>
  <summary>Details</summary>
Motivation: To enhance mental healthcare accessibility by automating structured clinical interviews while aligning with psychiatric protocols.

Method: MAGI uses four specialized agents for navigation, questioning, judgment, and diagnosis, incorporating PsyCoT for symptom mapping.

Result: Tested on 1,002 participants, MAGI improved LLM-assisted mental health assessments with clinical accuracy and explainability.

Conclusion: MAGI successfully combines clinical rigor, adaptability, and explainable reasoning for automated psychiatric interviews.

Abstract: Automating structured clinical interviews could revolutionize mental
healthcare accessibility, yet existing large language models (LLMs) approaches
fail to align with psychiatric diagnostic protocols. We present MAGI, the first
framework that transforms the gold-standard Mini International Neuropsychiatric
Interview (MINI) into automatic computational workflows through coordinated
multi-agent collaboration. MAGI dynamically navigates clinical logic via four
specialized agents: 1) an interview tree guided navigation agent adhering to
the MINI's branching structure, 2) an adaptive question agent blending
diagnostic probing, explaining, and empathy, 3) a judgment agent validating
whether the response from participants meet the node, and 4) a diagnosis Agent
generating Psychometric Chain-of- Thought (PsyCoT) traces that explicitly map
symptoms to clinical criteria. Experimental results on 1,002 real-world
participants covering depression, generalized anxiety, social anxiety and
suicide shows that MAGI advances LLM- assisted mental health assessment by
combining clinical rigor, conversational adaptability, and explainable
reasoning.

</details>


### [20] [TextTIGER: Text-based Intelligent Generation with Entity Prompt Refinement for Text-to-Image Generation](https://arxiv.org/abs/2504.18269)
*Shintaro Ozaki, Kazuki Hayashi, Yusuke Sakai, Jingun Kwon, Hidetaka Kamigaito, Katsuhiko Hayashi, Manabu Okumura, Taro Watanabe*

Main category: cs.CL

TL;DR: TextTIGER refines prompts by augmenting and summarizing entity-related descriptions using LLMs, improving image generation performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of retaining entity-specific knowledge in image generation due to the vast and evolving number of entities.

Method: Proposes TextTIGER, which augments entity knowledge in prompts and summarizes descriptions with LLMs to avoid performance degradation from long inputs. Introduces WiT-Cub dataset for evaluation.

Result: Improves image generation metrics (IS, FID, CLIPScore) over caption-only prompts. Summarized descriptions are more informative.

Conclusion: Refining prompts with augmented and summarized entity descriptions enhances image generation, validated by experiments and annotator evaluations.

Abstract: Generating images from prompts containing specific entities requires models
to retain as much entity-specific knowledge as possible. However, fully
memorizing such knowledge is impractical due to the vast number of entities and
their continuous emergence. To address this, we propose Text-based Intelligent
Generation with Entity prompt Refinement (TextTIGER), which augments knowledge
on entities included in the prompts and then summarizes the augmented
descriptions using Large Language Models (LLMs) to mitigate performance
degradation from longer inputs. To evaluate our method, we introduce WiT-Cub
(WiT with Captions and Uncomplicated Background-explanations), a dataset
comprising captions, images, and an entity list. Experiments on four image
generation models and five LLMs show that TextTIGER improves image generation
performance in standard metrics (IS, FID, and CLIPScore) compared to
caption-only prompts. Additionally, multiple annotators' evaluation confirms
that the summarized descriptions are more informative, validating LLMs' ability
to generate concise yet rich descriptions. These findings demonstrate that
refining prompts with augmented and summarized entity-related descriptions
enhances image generation capabilities. The code and dataset will be available
upon acceptance.

</details>


### [21] [Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review](https://arxiv.org/abs/2504.18346)
*Toghrul Abbasli, Kentaroh Toyoda, Yuan Wang, Leon Witt, Muhammad Asif Ali, Yukai Miao, Dan Li, Qingsong Wei*

Main category: cs.CL

TL;DR: The paper surveys and benchmarks uncertainty quantification (UQ) and calibration methods for LLMs, addressing hallucination challenges and providing future directions.


<details>
  <summary>Details</summary>
Motivation: Hallucination in LLMs and the lack of comprehensive analysis and benchmarking for UQ and calibration methods.

Method: Systematic survey of prior works, introduction of a rigorous benchmark, and empirical evaluation of six methods using reliability datasets.

Result: Empirical findings justify the survey's insights, highlighting gaps and effectiveness of existing methods.

Conclusion: The study is the first dedicated review of calibration methods for LLMs, outlining future directions and open challenges.

Abstract: Large Language Models (LLMs) have been transformative across many domains.
However, hallucination -- confidently outputting incorrect information --
remains one of the leading challenges for LLMs. This raises the question of how
to accurately assess and quantify the uncertainty of LLMs. Extensive literature
on traditional models has explored Uncertainty Quantification (UQ) to measure
uncertainty and employed calibration techniques to address the misalignment
between uncertainty and accuracy. While some of these methods have been adapted
for LLMs, the literature lacks an in-depth analysis of their effectiveness and
does not offer a comprehensive benchmark to enable insightful comparison among
existing solutions. In this work, we fill this gap via a systematic survey of
representative prior works on UQ and calibration for LLMs and introduce a
rigorous benchmark. Using two widely used reliability datasets, we empirically
evaluate six related methods, which justify the significant findings of our
review. Finally, we provide outlooks for key future directions and outline open
challenges. To the best of our knowledge, this survey is the first dedicated
study to review the calibration methods and relevant metrics for LLMs.

</details>


### [22] [Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant](https://arxiv.org/abs/2504.18373)
*Lei Shen, Xiaoyu Shen*

Main category: cs.CL

TL;DR: Auto-SLURP is a benchmark dataset for evaluating LLM-based multi-agent frameworks in personal assistants, extending SLURP with relabeled data and simulated services.


<details>
  <summary>Details</summary>
Motivation: There's a lack of tailored benchmarks for LLM-powered multi-agent frameworks, especially for personal assistants.

Method: Auto-SLURP extends SLURP by relabeling data and integrating simulated servers/external services for end-to-end evaluation.

Result: Experiments show Auto-SLURP challenges current frameworks, indicating reliable multi-agent assistants are still evolving.

Conclusion: Auto-SLURP fills a gap in benchmarking and highlights the need for further advancements in multi-agent frameworks.

Abstract: In recent years, multi-agent frameworks powered by large language models
(LLMs) have advanced rapidly. Despite this progress, there is still a notable
absence of benchmark datasets specifically tailored to evaluate their
performance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset
aimed at evaluating LLM-based multi-agent frameworks in the context of
intelligent personal assistants. Auto-SLURP extends the original SLURP dataset
-- initially developed for natural language understanding tasks -- by
relabeling the data and integrating simulated servers and external services.
This enhancement enables a comprehensive end-to-end evaluation pipeline,
covering language understanding, task execution, and response generation. Our
experiments demonstrate that Auto-SLURP presents a significant challenge for
current state-of-the-art frameworks, highlighting that truly reliable and
intelligent multi-agent personal assistants remain a work in progress. The
dataset and related code are available at
https://github.com/lorashen/Auto-SLURP/.

</details>


### [23] [Pushing the boundary on Natural Language Inference](https://arxiv.org/abs/2504.18376)
*Pablo Miralles-González, Javier Huertas-Tato, Alejandro Martín, David Camacho*

Main category: cs.CL

TL;DR: A reinforcement learning-based approach (GRPO for CoT learning) is applied to NLI, eliminating labeled rationales and improving performance on challenging datasets like ANLI. Fine-tuned models (7B, 14B, 32B) show strong results, with the 32B AWQ-quantized model outperforming SOTA on adversarial benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current NLI systems rely on supervised learning with biased datasets, limiting generalization. This work aims to improve robustness and scalability without labeled rationales.

Method: Uses GRPO for CoT learning in NLI, fine-tuning large language models (7B, 14B, 32B) with LoRA and QLoRA techniques. Evaluated on standard and adversarial benchmarks.

Result: The 32B AWQ-quantized model achieves SOTA performance on 7/11 adversarial sets (or all in replication) with a 22GB memory footprint, retaining robust reasoning under aggressive quantization.

Conclusion: Provides a scalable, practical framework for robust NLI systems without compromising inference quality.

Abstract: Natural Language Inference (NLI) is a central task in natural language
understanding with applications in fact-checking, question answering, and
information retrieval. Despite its importance, current NLI systems heavily rely
on supervised learning with datasets that often contain annotation artifacts
and biases, limiting generalization and real-world applicability. In this work,
we apply a reinforcement learning-based approach using Group Relative Policy
Optimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the
need for labeled rationales and enabling this type of training on more
challenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language
models using parameter-efficient techniques (LoRA and QLoRA), demonstrating
strong performance across standard and adversarial NLI benchmarks. Our 32B
AWQ-quantized model surpasses state-of-the-art results on 7 out of 11
adversarial sets$\unicode{x2013}$or on all of them considering our
replication$\unicode{x2013}$within a 22GB memory footprint, showing that robust
reasoning can be retained under aggressive quantization. This work provides a
scalable and practical framework for building robust NLI systems without
sacrificing inference quality.

</details>


### [24] [A UD Treebank for Bohairic Coptic](https://arxiv.org/abs/2504.18386)
*Amir Zeldes, Nina Speransky, Nicholas Wagner, Caroline T. Schroeder*

Main category: cs.CL

TL;DR: This paper introduces the first syntactically annotated corpus of Bohairic Coptic, comparing it to Sahidic Coptic and highlighting its distinct linguistic features.


<details>
  <summary>Details</summary>
Motivation: Bohairic Coptic is under-resourced despite its historical and contemporary significance, especially compared to Sahidic Coptic.

Method: The authors create a corpus of Bohairic Coptic texts, annotate them syntactically, and compare them to Sahidic Coptic using parsing experiments.

Result: The study reveals Bohairic Coptic as a distinct dialect from Sahidic, with unique syntactic features.

Conclusion: The work fills a gap in Coptic dialect studies and provides a foundation for further research on Bohairic Coptic.

Abstract: Despite recent advances in digital resources for other Coptic dialects,
especially Sahidic, Bohairic Coptic, the main Coptic dialect for pre-Mamluk,
late Byzantine Egypt, and the contemporary language of the Coptic Church,
remains critically under-resourced. This paper presents and evaluates the first
syntactically annotated corpus of Bohairic Coptic, sampling data from a range
of works, including Biblical text, saints' lives and Christian ascetic writing.
We also explore some of the main differences we observe compared to the
existing UD treebank of Sahidic Coptic, the classical dialect of the language,
and conduct joint and cross-dialect parsing experiments, revealing the unique
nature of Bohairic as a related, but distinct variety from the more often
studied Sahidic.

</details>


### [25] [HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?](https://arxiv.org/abs/2504.18406)
*Yusen Zhang, Wenliang Zheng, Aashrith Madasu, Peng Shi, Ryo Kamoi, Hao Zhou, Zhuoyang Zou, Shu Zhao, Sarkar Snigdha Sarathi Das, Vipul Gupta, Xiaoxin Lu, Nan Zhang, Ranran Haoran Zhang, Avitej Iyer, Renze Lou, Wenpeng Yin, Rui Zhang*

Main category: cs.CL

TL;DR: The paper introduces HRScene, a benchmark for evaluating Vision Large Language Models (VLMs) on high-resolution image (HRI) understanding, revealing significant performance gaps.


<details>
  <summary>Details</summary>
Motivation: There is no comprehensive benchmark to assess VLMs' ability to understand HRIs, despite their claimed capabilities.

Method: HRScene is created with 25 real-world and 2 synthetic datasets, annotated by experts, covering diverse scenarios. Experiments test 28 VLMs.

Result: Current VLMs achieve ~50% accuracy on real-world tasks and struggle with regional utilization in HRIs.

Conclusion: The benchmark highlights VLMs' limitations in HRI understanding, guiding future research.

Abstract: High-resolution image (HRI) understanding aims to process images with a large
number of pixels, such as pathological images and agricultural aerial images,
both of which can exceed 1 million pixels. Vision Large Language Models (VLMs)
can allegedly handle HRIs, however, there is a lack of a comprehensive
benchmark for VLMs to evaluate HRI understanding. To address this gap, we
introduce HRScene, a novel unified benchmark for HRI understanding with rich
scenes. HRScene incorporates 25 real-world datasets and 2 synthetic diagnostic
datasets with resolutions ranging from 1,024 $\times$ 1,024 to 35,503 $\times$
26,627. HRScene is collected and re-annotated by 10 graduate-level annotators,
covering 25 scenarios, ranging from microscopic to radiology images, street
views, long-range pictures, and telescope images. It includes HRIs of
real-world objects, scanned documents, and composite multi-image. The two
diagnostic evaluation datasets are synthesized by combining the target image
with the gold answer and distracting images in different orders, assessing how
well models utilize regions in HRI. We conduct extensive experiments involving
28 VLMs, including Gemini 2.0 Flash and GPT-4o. Experiments on HRScene show
that current VLMs achieve an average accuracy of around 50% on real-world
tasks, revealing significant gaps in HRI understanding. Results on synthetic
datasets reveal that VLMs struggle to effectively utilize HRI regions, showing
significant Regional Divergence and lost-in-middle, shedding light on future
research.

</details>


### [26] [Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers](https://arxiv.org/abs/2504.18412)
*Jared Moore, Declan Grabb, William Agnew, Kevin Klyman, Stevie Chancellor, Desmond C. Ong, Nick Haber*

Main category: cs.CL

TL;DR: The paper investigates whether LLMs can replace human therapists, finding they fail to meet key therapeutic standards and exhibit harmful behaviors like stigma and inappropriate responses.


<details>
  <summary>Details</summary>
Motivation: To evaluate the feasibility and risks of using LLMs as therapists, given their promotion in tech and research.

Method: A mapping review of therapy guides and experiments with LLMs (e.g., GPT-4) to assess their adherence to therapeutic relationship aspects.

Result: LLMs exhibit stigma, encourage delusional thinking, and lack human qualities needed for therapeutic alliances.

Conclusion: LLMs should not replace therapists; alternative roles in clinical therapy are suggested.

Abstract: Should a large language model (LLM) be used as a therapist? In this paper, we
investigate the use of LLMs to *replace* mental health providers, a use case
promoted in the tech startup and research space. We conduct a mapping review of
therapy guides used by major medical institutions to identify crucial aspects
of therapeutic relationships, such as the importance of a therapeutic alliance
between therapist and client. We then assess the ability of LLMs to reproduce
and adhere to these aspects of therapeutic relationships by conducting several
experiments investigating the responses of current LLMs, such as `gpt-4o`.
Contrary to best practices in the medical community, LLMs 1) express stigma
toward those with mental health conditions and 2) respond inappropriately to
certain common (and critical) conditions in naturalistic therapy settings --
e.g., LLMs encourage clients' delusional thinking, likely due to their
sycophancy. This occurs even with larger and newer LLMs, indicating that
current safety practices may not address these gaps. Furthermore, we note
foundational and practical barriers to the adoption of LLMs as therapists, such
as that a therapeutic alliance requires human characteristics (e.g., identity
and stakes). For these reasons, we conclude that LLMs should not replace
therapists, and we discuss alternative roles for LLMs in clinical therapy.

</details>


### [27] [BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs](https://arxiv.org/abs/2504.18415)
*Hongyu Wang, Shuming Ma, Furu Wei*

Main category: cs.CL

TL;DR: BitNet v2 enables native 4-bit activation quantization for 1-bit LLMs, addressing outliers with H-BitLinear, reducing memory and computational costs.


<details>
  <summary>Details</summary>
Motivation: Efficient deployment of 1-bit LLMs is hindered by activation outliers, complicating low-bit quantization.

Method: Introduces H-BitLinear, applying an online Hadamard transformation to smooth activation distributions for 4-bit quantization.

Result: BitNet v2 matches BitNet b1.58 performance with 8-bit activations and shows minimal degradation with 4-bit activations.

Conclusion: BitNet v2 significantly reduces memory and computational costs while maintaining performance, enabling efficient 1-bit LLM deployment.

Abstract: Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by
activation outliers, which complicate quantization to low bit-widths. We
introduce BitNet v2, a novel framework enabling native 4-bit activation
quantization for 1-bit LLMs. To tackle outliers in attention and feed-forward
network activations, we propose H-BitLinear, a module applying an online
Hadamard transformation prior to activation quantization. This transformation
smooths sharp activation distributions into more Gaussian-like forms, suitable
for low-bit representation. Experiments show BitNet v2 trained from scratch
with 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2
achieves minimal performance degradation when trained with native 4-bit
activations, significantly reducing memory footprint and computational cost for
batched inference.

</details>


### [28] [PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts](https://arxiv.org/abs/2504.18428)
*Yiming Wang, Pei Zhang, Jialong Tang, Haoran Wei, Baosong Yang, Rui Wang, Chenshu Sun, Feitong Sun, Jiran Zhang, Junxuan Wu, Qiqian Cang, Yichang Zhang, Fei Huang, Junyang Lin, Fei Huang, Jingren Zhou*

Main category: cs.CL

TL;DR: PolyMath is a multilingual math reasoning benchmark for 18 languages and 4 difficulty levels, revealing key challenges in LLMs' multilingual reasoning performance.


<details>
  <summary>Details</summary>
Motivation: To create a discriminative multilingual math benchmark for evaluating reasoning LLMs, addressing language diversity and difficulty comprehensiveness.

Method: Developed PolyMath, a benchmark with high-quality translations, and evaluated advanced LLMs like Deepseek-R1-671B and Qwen-QwQ-32B.

Result: LLMs scored low (43.4 and 41.8) with <30% accuracy at the highest level. Key challenges include performance variance, low input-output consistency, and thinking length differences by language.

Conclusion: Controlling output language in instructions can improve multilingual reasoning, especially for low-resource languages, offering a direction for enhancing LLMs.

Abstract: In this paper, we introduce PolyMath, a multilingual mathematical reasoning
benchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our
benchmark ensures difficulty comprehensiveness, language diversity, and
high-quality translation, making it a highly discriminative multilingual
mathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive
evaluation for advanced LLMs and find that even Deepseek-R1-671B and
Qwen-QwQ-32B, achieve only 43.4 and 41.8 benchmark scores, with less than 30%
accuracy under the highest level. From a language perspective, our benchmark
reveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning
performance varies widely across languages for current LLMs; (2) Input-output
language consistency is low in reasoning LLMs and may be correlated with
performance; (3) The thinking length differs significantly by language for
current LLMs. Additionally, we demonstrate that controlling the output language
in the instructions has the potential to affect reasoning performance,
especially for some low-resource languages, suggesting a promising direction
for improving multilingual capabilities in LLMs.

</details>


### [29] [Fast-Slow Thinking for Large Vision-Language Model Reasoning](https://arxiv.org/abs/2504.18458)
*Wenyi Xiao, Leilei Gan, Weilong Dai, Wanggui He, Ziwei Huang, Haoyuan Li, Fangxun Shu, Zhelun Yu, Peng Zhang, Hao Jiang, Fei Wu*

Main category: cs.CL

TL;DR: FAST introduces a fast-slow thinking framework for LVLMs to dynamically adjust reasoning depth, improving accuracy by 10% and reducing token usage by 32.7-67.3%.


<details>
  <summary>Details</summary>
Motivation: Address the 'overthinking' phenomenon in LVLMs, where models generate excessive reasoning regardless of task complexity.

Method: FAST-GRPO includes model-based metrics, adaptive thinking rewards, and difficulty-aware KL regularization to adapt reasoning depth.

Result: Achieves state-of-the-art accuracy with 10% improvement and reduces token usage significantly compared to slow-thinking methods.

Conclusion: FAST effectively balances reasoning length and accuracy, offering a practical solution for efficient LVLM performance.

Abstract: Recent advances in large vision-language models (LVLMs) have revealed an
\textit{overthinking} phenomenon, where models generate verbose reasoning
across all tasks regardless of questions. To address this issue, we present
\textbf{FAST}, a novel \textbf{Fa}st-\textbf{S}low \textbf{T}hinking framework
that dynamically adapts reasoning depth based on question characteristics.
Through empirical analysis, we establish the feasibility of fast-slow thinking
in LVLMs by investigating how response length and data distribution affect
performance. We develop FAST-GRPO with three components: model-based metrics
for question characterization, an adaptive thinking reward mechanism, and
difficulty-aware KL regularization. Experiments across seven reasoning
benchmarks demonstrate that FAST achieves state-of-the-art accuracy with over
10\% relative improvement compared to the base model, while reducing token
usage by 32.7-67.3\% compared to previous slow-thinking approaches, effectively
balancing reasoning length and accuracy.

</details>


### [30] [Generative Induction of Dialogue Task Schemas with Streaming Refinement and Simulated Interactions](https://arxiv.org/abs/2504.18474)
*James D. Finch, Yasasvi Josyula, Jinho D. Choi*

Main category: cs.CL

TL;DR: A novel SoTA approach for Slot Schema Induction (SSI) in TOD systems treats SSI as a text generation task, using LLM-based simulation for data creation and addressing evaluation issues with human-guided metrics.


<details>
  <summary>Details</summary>
Motivation: To automate slot schema identification in TOD systems without manual intervention and improve evaluation methods.

Method: Formulates SSI as a text generation task, uses LLM-based simulation for data creation, and designs improved evaluation metrics.

Result: Achieves SoTA performance, resolves data leakage and metric alignment issues, and provides high-quality evaluation data.

Conclusion: Establishes a foundation for future SSI research and advances dialogue understanding and system development.

Abstract: In task-oriented dialogue (TOD) systems, Slot Schema Induction (SSI) is
essential for automatically identifying key information slots from dialogue
data without manual intervention. This paper presents a novel state-of-the-art
(SoTA) approach that formulates SSI as a text generation task, where a language
model incrementally constructs and refines a slot schema over a stream of
dialogue data. To develop this approach, we present a fully automatic LLM-based
TOD simulation method that creates data with high-quality state labels for
novel task domains. Furthermore, we identify issues in SSI evaluation due to
data leakage and poor metric alignment with human judgment. We resolve these by
creating new evaluation data using our simulation method with human guidance
and correction, as well as designing improved evaluation metrics. These
contributions establish a foundation for future SSI research and advance the
SoTA in dialogue understanding and system development.

</details>


### [31] [Investigating Co-Constructive Behavior of Large Language Models in Explanation Dialogues](https://arxiv.org/abs/2504.18483)
*Leandra Fichtel, Maximilian Spliethöver, Eyke Hüllermeier, Patricia Jimenez, Nils Klowait, Stefan Kopp, Axel-Cyrille Ngonga Ngomo, Amelie Robrecht, Ingrid Scharlau, Lutz Terfloth, Anna-Lisa Vollmer, Henning Wachsmuth*

Main category: cs.CL

TL;DR: The paper explores LLMs' ability to engage in co-constructive explanation dialogues, finding they show some promising behaviors but lack full adaptability.


<details>
  <summary>Details</summary>
Motivation: To assess if LLMs can dynamically adapt explanations based on the explainee's understanding, a key aspect of effective explainable AI.

Method: Conducted a user study where explainees interacted with LLMs, some instructed for co-constructive dialogue, and evaluated understanding and perception.

Result: LLMs demonstrated co-constructive behaviors like verification questions, improving engagement and understanding, but struggled with monitoring and scaffolding.

Conclusion: Current LLMs show potential in co-constructive explanations but need improvement in adaptive monitoring for better effectiveness.

Abstract: The ability to generate explanations that are understood by explainees is the
quintessence of explainable artificial intelligence. Since understanding
depends on the explainee's background and needs, recent research has focused on
co-constructive explanation dialogues, where the explainer continuously
monitors the explainee's understanding and adapts explanations dynamically. We
investigate the ability of large language models (LLMs) to engage as explainers
in co-constructive explanation dialogues. In particular, we present a user
study in which explainees interact with LLMs, of which some have been
instructed to explain a predefined topic co-constructively. We evaluate the
explainees' understanding before and after the dialogue, as well as their
perception of the LLMs' co-constructive behavior. Our results indicate that
current LLMs show some co-constructive behaviors, such as asking verification
questions, that foster the explainees' engagement and can improve understanding
of a topic. However, their ability to effectively monitor the current
understanding and scaffold the explanations accordingly remains limited.

</details>


### [32] [TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation](https://arxiv.org/abs/2504.18535)
*Gwen Yidou Weng, Benjie Wang, Guy Van den Broeck*

Main category: cs.CL

TL;DR: TRACE is a framework for controllable text generation that efficiently computes Expected Attribute Probability (EAP) using a Hidden Markov Model (HMM) and a small classifier, enabling adaptable and lightweight control over LM outputs.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with global properties like alignment with human values or desired attributes, and existing solutions are either expensive or unreliable.

Method: TRACE distills an HMM from an LM and pairs it with a classifier to compute EAP, which is used to reweigh next-token probabilities for compliant outputs.

Result: TRACE achieves state-of-the-art detoxification with minimal overhead, adapts quickly to low-resource personalized LMs, and handles composite attributes.

Conclusion: TRACE offers a tractable, efficient, and adaptable solution for controllable text generation.

Abstract: As large language models (LMs) advance, there is an increasing need to
control their outputs to align with human values (e.g., detoxification) or
desired attributes (e.g., personalization, topic). However, autoregressive
models focus on next-token predictions and struggle with global properties that
require looking ahead. Existing solutions either tune or post-train LMs for
each new attribute - expensive and inflexible - or approximate the Expected
Attribute Probability (EAP) of future sequences by sampling or training, which
is slow and unreliable for rare attributes. We introduce TRACE (Tractable
Probabilistic Reasoning for Adaptable Controllable gEneration), a novel
framework that efficiently computes EAP and adapts to new attributes through
tractable probabilistic reasoning and lightweight control. TRACE distills a
Hidden Markov Model (HMM) from an LM and pairs it with a small classifier to
estimate attribute probabilities, enabling exact EAP computation over the HMM's
predicted futures. This EAP is then used to reweigh the LM's next-token
probabilities for globally compliant continuations. Empirically, TRACE achieves
state-of-the-art results in detoxification with only 10% decoding overhead,
adapts to 76 low-resource personalized LLMs within seconds, and seamlessly
extends to composite attributes.

</details>


### [33] [PRobELM: Plausibility Ranking Evaluation for Language Models](https://arxiv.org/abs/2404.03818)
*Zhangdie Yuan, Eric Chamoun, Rami Aly, Chenxi Whitehouse, Andreas Vlachos*

Main category: cs.CL

TL;DR: PRobELM is a benchmark evaluating language models' ability to rank plausible scenarios using world knowledge, bridging gaps in existing benchmarks like TruthfulQA and COPA.


<details>
  <summary>Details</summary>
Motivation: To assess models' plausibility ranking for downstream tasks like literature-based discovery, focusing on likely but unknown information.

Method: Constructed from Wikidata edit histories, aligned with models' training data, and tested across multiple prompting types (statement, text completion, QA).

Result: Experiments with 10 models show factual accuracy doesn't directly correlate with plausibility, and recent training data improves plausibility assessment.

Conclusion: PRobELM effectively evaluates plausibility ranking, highlighting the importance of up-to-date training data and model architecture.

Abstract: This paper introduces PRobELM (Plausibility Ranking Evaluation for Language
Models), a benchmark designed to assess language models' ability to discern
more plausible from less plausible scenarios through their parametric
knowledge. While benchmarks such as TruthfulQA emphasise factual accuracy or
truthfulness, and others such as COPA explore plausible scenarios without
explicitly incorporating world knowledge, PRobELM seeks to bridge this gap by
evaluating models' capabilities to prioritise plausible scenarios that leverage
world knowledge over less plausible alternatives. This design allows us to
assess the potential of language models for downstream use cases such as
literature-based discovery where the focus is on identifying information that
is likely but not yet known. Our benchmark is constructed from a dataset
curated from Wikidata edit histories, tailored to align the temporal bounds of
the training data for the evaluated models. PRobELM facilitates the evaluation
of language models across multiple prompting types, including statement, text
completion, and question-answering. Experiments with 10 models of various sizes
and architectures on the relationship between model scales, training recency,
and plausibility performance, reveal that factual accuracy does not directly
correlate with plausibility performance and that up-to-date training data
enhances plausibility assessment across different model architectures.

</details>


### [34] [Nearest Neighbor Speculative Decoding for LLM Generation and Attribution](https://arxiv.org/abs/2405.19325)
*Minghan Li, Xilun Chen, Ari Holtzman, Beidi Chen, Jimmy Lin, Wen-tau Yih, Xi Victoria Lin*

Main category: cs.CL

TL;DR: NEST is a semi-parametric LM method that improves generation quality, attribution, and speed by combining token-level retrieval with speculative decoding.


<details>
  <summary>Details</summary>
Motivation: Addresses LLMs' hallucination and lack of attribution, and improves upon slow inference and non-fluency in semi-parametric LMs like kNN-LM.

Method: Uses token-level retrieval and speculative decoding to incorporate real-world text spans and provide source attribution.

Result: Enhances generation quality, attribution rate, and achieves a 1.8x speedup in inference time.

Conclusion: NEST outperforms kNN-LM and competes with in-context retrieval augmentation, offering a balanced solution for knowledge-intensive tasks.

Abstract: Large language models (LLMs) often hallucinate and lack the ability to
provide attribution for their generations. Semi-parametric LMs, such as kNN-LM,
approach these limitations by refining the output of an LM for a given prompt
using its nearest neighbor matches in a non-parametric data store. However,
these models often exhibit slow inference speeds and produce non-fluent texts.
In this paper, we introduce Nearest Neighbor Speculative Decoding (NEST), a
novel semi-parametric language modeling approach that is capable of
incorporating real-world text spans of arbitrary length into the LM generations
and providing attribution to their sources. NEST performs token-level retrieval
at each inference step to compute a semi-parametric mixture distribution and
identify promising span continuations in a corpus. It then uses an approximate
speculative decoding procedure that accepts a prefix of the retrieved span or
generates a new token. NEST significantly enhances the generation quality and
attribution rate of the base LM across a variety of knowledge-intensive tasks,
surpassing the conventional kNN-LM method and performing competitively with
in-context retrieval augmentation. In addition, NEST substantially improves the
generation speed, achieving a 1.8x speedup in inference time when applied to
Llama-2-Chat 70B. Code will be released at
https://github.com/facebookresearch/NEST/tree/main.

</details>


### [35] [AMR-RE: Abstract Meaning Representations for Retrieval-Based In-Context Learning in Relation Extraction](https://arxiv.org/abs/2406.10432)
*Peitao Han, Lis Kanashiro Pereira, Fei Cheng, Wan Jou She, Eiji Aramaki*

Main category: cs.CL

TL;DR: Proposes an AMR-enhanced retrieval-based ICL method for RE, improving performance by focusing on semantic structure similarity.


<details>
  <summary>Details</summary>
Motivation: Existing ICL methods for RE prioritize language similarity over structural similarity, potentially overlooking entity relationships.

Method: Uses AMR-enhanced retrieval to select in-context examples based on semantic structure similarity between inputs and training samples.

Result: Outperforms baselines in unsupervised settings and achieves state-of-the-art or competitive results in supervised settings across four datasets.

Conclusion: The method effectively improves RE performance by leveraging structural similarity, demonstrating its superiority over traditional language-focused approaches.

Abstract: Existing in-context learning (ICL) methods for relation extraction (RE) often
prioritize language similarity over structural similarity, which can lead to
overlooking entity relationships. To address this, we propose an AMR-enhanced
retrieval-based ICL method for RE. Our model retrieves in-context examples
based on semantic structure similarity between task inputs and training
samples. Evaluations on four standard English RE datasets show that our model
outperforms baselines in the unsupervised setting across all datasets. In the
supervised setting, it achieves state-of-the-art results on three datasets and
competitive results on the fourth.

</details>


### [36] [Multilingual Large Language Models and Curse of Multilinguality](https://arxiv.org/abs/2406.10602)
*Daniil Gurgurov, Tanja Bäumel, Tatiana Anikina*

Main category: cs.CL

TL;DR: The paper provides an overview of multilingual LLMs, detailing their architectures, training, and challenges like the curse of multilinguality.


<details>
  <summary>Details</summary>
Motivation: To introduce and explain the technical aspects of multilingual LLMs, highlighting their capabilities and limitations.

Method: The paper reviews architectures (encoder-only, decoder-only, encoder-decoder), objective functions, pre-training data, and tokenization methods.

Result: Multilingual LLMs are effective across languages and tasks but face challenges like the curse of multilinguality.

Conclusion: The paper serves as a guide to multilingual LLMs, emphasizing their potential and ongoing challenges.

Abstract: Multilingual Large Language Models (LLMs) have gained large popularity among
Natural Language Processing (NLP) researchers and practitioners. These models,
trained on huge datasets, show proficiency across various languages and
demonstrate effectiveness in numerous downstream tasks. This paper navigates
the landscape of multilingual LLMs, providing an introductory overview of their
technical aspects. It explains underlying architectures, objective functions,
pre-training data sources, and tokenization methods. This work explores the
unique features of different model types: encoder-only (mBERT, XLM-R),
decoder-only (XGLM, PALM, BLOOM, GPT-3), and encoder-decoder models (mT5,
mBART). Additionally, it addresses one of the significant limitations of
multilingual LLMs - the curse of multilinguality - and discusses current
attempts to overcome it.

</details>


### [37] [Review-driven Personalized Preference Reasoning with Large Language Models for Recommendation](https://arxiv.org/abs/2408.06276)
*Jieyong Kim, Hyunseo Kim, Hyunjin Cho, SeongKu Kang, Buru Chang, Jinyoung Yeo, Dongha Lee*

Main category: cs.CL

TL;DR: EXP3RT, a novel LLM-based recommender, leverages user/item reviews for enhanced reasoning and rating prediction, outperforming existing methods in accuracy and explainability.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based recommendation systems underutilize LLMs' potential due to limited input or reasoning capabilities.

Method: EXP3RT fine-tunes an LLM to extract preferences from reviews, create profiles, and perform reasoning-enhanced rating prediction.

Result: EXP3RT excels in rating prediction and reranking, improving accuracy and explainability.

Conclusion: EXP3RT advances LLM-based recommendation by better utilizing reasoning and review data.

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated
exceptional performance across a wide range of tasks, generating significant
interest in their application to recommendation systems. However, existing
methods have not fully capitalized on the potential of LLMs, often constrained
by limited input information or failing to fully utilize their advanced
reasoning capabilities. To address these limitations, we introduce EXP3RT, a
novel LLM-based recommender designed to leverage rich preference information
contained in user and item reviews. EXP3RT is basically fine-tuned through
distillation from a teacher LLM to perform three key tasks in order: EXP3RT
first extracts and encapsulates essential subjective preferences from raw
reviews, aggregates and summarizes them according to specific criteria to
create user and item profiles. It then generates detailed step-by-step
reasoning followed by predicted rating, i.e., reasoning-enhanced rating
prediction, by considering both subjective and objective information from
user/item profiles and item descriptions. This personalized preference
reasoning from EXP3RT enhances rating prediction accuracy and also provides
faithful and reasonable explanations for recommendation. Extensive experiments
show that EXP3RT outperforms existing methods on both rating prediction and
candidate item reranking for top-k recommendation, while significantly
enhancing the explainability of recommendation systems.

</details>


### [38] [Using Large Language Models to Create AI Personas for Replication, Generalization and Prediction of Media Effects: An Empirical Test of 133 Published Experimental Research Findings](https://arxiv.org/abs/2408.16073)
*Leo Yeykelis, Kaavya Pichai, James J. Cummings, Byron Reeves*

Main category: cs.CL

TL;DR: LLMs replicated 76% of main effects in marketing studies, showing promise for AI-assisted replication and generalization, though limitations exist.


<details>
  <summary>Details</summary>
Motivation: Address the replication and generalizability crises in social science, accelerate theory building, and enable rapid message testing in marketing.

Method: Used LLM-powered personas to replicate 133 experimental findings from 14 papers, generating 19,447 AI personas to compare with original human results.

Result: 76% of main effects and 68% of all effects were successfully replicated; generalization tests showed variability beyond original study parameters.

Conclusion: LLMs offer strong potential for replication and generalization in marketing research, but challenges like complex interactions and AI biases remain.

Abstract: This report analyzes the potential for large language models (LLMs) to
expedite accurate replication and generalization of published research about
message effects in marketing. LLM-powered participants (personas) were tested
by replicating 133 experimental findings from 14 papers containing 45 recent
studies published in the Journal of Marketing. For each study, the measures,
stimuli, and sampling specifications were used to generate prompts for LLMs to
act as unique personas. The AI personas, 19,447 in total across all of the
studies, generated complete datasets and statistical analyses were then
compared with the original human study results. The LLM replications
successfully reproduced 76% of the original main effects (84 out of 111),
demonstrating strong potential for AI-assisted replication. The overall
replication rate including interaction effects was 68% (90 out of 133).
Furthermore, a test of how human results generalized to different participant
samples, media stimuli, and measures showed that replication results can change
when tests go beyond the parameters of the original human studies. Implications
are discussed for the replication and generalizability crises in social
science, the acceleration of theory building in media and marketing psychology,
and the practical advantages of rapid message testing for consumer products.
Limitations of AI replications are addressed with respect to complex
interaction effects, biases in AI models, and establishing benchmarks for AI
metrics in marketing research.

</details>


### [39] [Your Weak LLM is Secretly a Strong Teacher for Alignment](https://arxiv.org/abs/2409.08813)
*Leitian Tao, Yixuan Li*

Main category: cs.CL

TL;DR: Weak LLMs can rival human feedback for model alignment, offering a scalable and cost-effective alternative.


<details>
  <summary>Details</summary>
Motivation: Address the high costs and inefficiencies of existing alignment methods (human effort or computational resources) by exploring weak LLMs as a middle ground.

Method: Systematic study evaluating weak LLMs' feedback generation for alignment, comparing it to human-annotated data.

Result: Weak LLMs provide feedback comparable or superior to human feedback, with minimal impact from model size.

Conclusion: Weak LLMs present a scalable and sustainable alignment strategy, supported by empirical and qualitative evidence.

Abstract: The burgeoning capabilities of large language models (LLMs) have underscored
the need for alignment to ensure these models act in accordance with human
values and intentions. Existing alignment frameworks present constraints either
in the form of expensive human effort or high computational costs. This paper
explores a promising middle ground, where we employ a weak LLM that is
significantly less resource-intensive than top-tier models, yet offers more
automation than purely human feedback. We present a systematic study to
evaluate and understand weak LLM's ability to generate feedback for alignment.
Our empirical findings demonstrate that weak LLMs can provide feedback that
rivals or even exceeds that of fully human-annotated data. Our study indicates
a minimized impact of model size on feedback efficacy, shedding light on a
scalable and sustainable alignment strategy. To deepen our understanding of
alignment under weak LLM feedback, we conduct a series of qualitative and
quantitative analyses, offering novel insights into the quality discrepancies
between human feedback vs. weak LLM feedback.

</details>


### [40] [MeTHanol: Modularized Thinking Language Models with Intermediate Layer Thinking, Decoding and Bootstrapping Reasoning](https://arxiv.org/abs/2409.12059)
*Ningyuan Xi, Xiaoyu Wang, Yetao Wu, Teng Chen, Qingqing Gu, Yue Zhao, Jinxian Qu, Zhonglin Jiang, Yong Chen, Luo Ji*

Main category: cs.CL

TL;DR: The paper introduces TaS, a model architecture inspired by cognitive mechanisms to enhance language models' reasoning by adding a 'thinking layer' for generating thoughts before responses.


<details>
  <summary>Details</summary>
Motivation: To address the lack of thorough reasoning in large language models by mimicking natural cognitive processes.

Method: Designs TaS with a thinking layer, annotates/generates thought contents from data, and trains the model on thoughts-augmented data.

Result: TaS successfully generates reasonable thoughts and outputs more logical responses, validated by qualitative and quantitative results.

Conclusion: TaS effectively improves language models' reasoning, with code publicly available.

Abstract: Large Language Model can reasonably understand and generate human expressions
but may lack of thorough thinking and reasoning mechanisms. Recently there have
been several studies which enhance the thinking ability of language models but
most of them are not data-driven or training-based. In this paper, we are
motivated by the cognitive mechanism in the natural world, and design a novel
model architecture called TaS which allows it to first consider the thoughts
and then express the response based upon the query. We design several pipelines
to annotate or generate the thought contents from prompt-response samples, then
add language heads in a middle layer which behaves as the thinking layer. We
train the language model by the thoughts-augmented data and successfully let
the thinking layer automatically generate reasonable thoughts and finally
output more reasonable responses. Both qualitative examples and quantitative
results validate the effectiveness and performance of TaS. Our code is
available at https://anonymous.4open.science/r/TadE.

</details>


### [41] [FaithEval: Can Your Language Model Stay Faithful to Context, Even If "The Moon is Made of Marshmallows"](https://arxiv.org/abs/2410.03727)
*Yifei Ming, Senthil Purushwalkam, Shrey Pandit, Zixuan Ke, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty*

Main category: cs.CL

TL;DR: FaithEval is a benchmark to evaluate LLM faithfulness in contextual tasks, revealing that even top models struggle with faithfulness.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of faithfulness hallucination in LLMs and RAG systems, which undermines user trust.

Method: Introduces FaithEval, a benchmark with 4.9K problems across three tasks (unanswerable, inconsistent, counterfactual), validated via LLM-based auto-evaluation and human checks.

Result: State-of-the-art models often fail to stay faithful to context, and larger models don't guarantee better faithfulness.

Conclusion: FaithEval highlights the need for improved faithfulness in LLMs, with benchmarks and validation methods provided.

Abstract: Ensuring faithfulness to context in large language models (LLMs) and
retrieval-augmented generation (RAG) systems is crucial for reliable deployment
in real-world applications, as incorrect or unsupported information can erode
user trust. Despite advancements on standard benchmarks, faithfulness
hallucination-where models generate responses misaligned with the provided
context-remains a significant challenge. In this work, we introduce FaithEval,
a novel and comprehensive benchmark tailored to evaluate the faithfulness of
LLMs in contextual scenarios across three diverse tasks: unanswerable,
inconsistent, and counterfactual contexts. These tasks simulate real-world
challenges where retrieval mechanisms may surface incomplete, contradictory, or
fabricated information. FaithEval comprises 4.9K high-quality problems in
total, validated through a rigorous four-stage context construction and
validation framework, employing both LLM-based auto-evaluation and human
validation. Our extensive study across a wide range of open-source and
proprietary models reveals that even state-of-the-art models often struggle to
remain faithful to the given context, and that larger models do not necessarily
exhibit improved faithfulness.Project is available at:
https://github.com/SalesforceAIResearch/FaithEval.

</details>


### [42] [Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease Diagnosis with Small Language Models](https://arxiv.org/abs/2411.07611)
*Shuai Niu, Jing Ma, Hongzhan Lin, Liang Bai, Zhihua Wang, Yida Xu, Yunya Song, Xian Yang*

Main category: cs.CL

TL;DR: ClinRaGen enhances small language models (SLMs) with large language model (LLM) reasoning and domain knowledge for better disease diagnosis and interpretable rationale generation.


<details>
  <summary>Details</summary>
Motivation: Existing models lack balance between accuracy and interpretability, and LLMs are computationally expensive while SLMs lack advanced reasoning.

Method: ClinRaGen uses rationale distillation and domain knowledge injection, with a sequential distillation framework and knowledge-augmented attention mechanism.

Result: Achieves state-of-the-art performance in disease diagnosis and rationale generation on real-world medical datasets.

Conclusion: Combining LLM reasoning with knowledge augmentation improves interpretability and performance in clinical applications.

Abstract: Interpretation is critical for disease diagnosis, but existing models
struggle to balance predictive accuracy with human-understandable rationales.
While large language models (LLMs) offer strong reasoning abilities, their
clinical use is limited by high computational costs and restricted multimodal
reasoning ability. Small language models (SLMs) are efficient but lack advanced
reasoning for integrating multimodal medical data. In addition, both LLMs and
SLMs lack of domain knowledge for trustworthy reasoning. Therefore, we propose
ClinRaGen, enhancing SLMs by leveraging LLM-derived reasoning ability via
rationale distillation and domain knowledge injection for trustworthy
multimodal rationale generation. Key innovations include a sequential rationale
distillation framework that equips SLMs with LLM-comparable mutlimodal
reasoning abilities, and a knowledge-augmented attention mechanism that jointly
unifies multimodal representation from time series and textual data in a same
encoding space, enabling it naturally interpreted by SLMs while incorporating
domain knowledge for reliable rationale generation. Experiments on real-world
medical datasets show that ClinRaGen achieves state-of-the-art performance in
disease diagnosis and rationale generation, demonstrating the effectiveness of
combining LLM-driven reasoning with knowledge augmentation for improved
interpretability.

</details>


### [43] [ElChat: Adapting Chat Language Models Using Only Target Unlabeled Language Data](https://arxiv.org/abs/2412.11704)
*Atsuki Yamaguchi, Terufumi Morishita, Aline Villavicencio, Nikolaos Aletras*

Main category: cs.CL

TL;DR: ElChat is a new method for adapting chat LLMs to new languages directly on unlabeled target data, avoiding the need for a base model and preserving chat abilities better than previous approaches.


<details>
  <summary>Details</summary>
Motivation: Existing vocabulary expansion (VE) methods for adapting chat LLMs risk forgetting chat abilities when applied to unlabeled data. Target chat data is often scarce or costly, and machine-translated alternatives are ineffective.

Method: ElChat adapts a chat model directly on target unlabeled data by injecting information from the source chat model, eliminating the need for a base model and chat vector (CV).

Result: ElChat outperforms CV in target language and safety performance while maintaining superior English, chat, and instruction-following abilities.

Conclusion: ElChat provides a robust and competitive solution for language adaptation of chat LLMs, addressing the limitations of existing methods.

Abstract: Vocabulary expansion (VE) is the de-facto approach to language adaptation of
large language models (LLMs) by adding new tokens and continuing pre-training
on target data. While this is effective for base models trained on unlabeled
data, it poses challenges for chat models trained to follow instructions
through labeled conversation data. Directly adapting the latter with VE on
target unlabeled data may result in forgetting chat abilities. While ideal,
target chat data is often unavailable or costly to create for low-resource
languages, and machine-translated alternatives are not always effective. To
address this issue, previous work proposed using a base and chat model from the
same family. This method first adapts the base LLM with VE on target unlabeled
data and then converts it to a chat model by adding a chat vector (CV) derived
from the weight difference between the source base and chat models. We propose
ElChat, a new language adaptation method for chat LLMs that adapts a chat model
directly on target unlabeled data, without a base model. It elicits chat
abilities by injecting information from the source chat model. ElChat offers
more robust and competitive target language and safety performance while
achieving superior English, chat, and instruction-following abilities compared
to CV.

</details>


### [44] [Factual Knowledge in Language Models: Robustness and Anomalies under Simple Temporal Context Variations](https://arxiv.org/abs/2502.01220)
*Hichem Ammar Khodja, Frédéric Béchet, Quentin Brabant, Alexis Nasr, Gwénolé Lecorvé*

Main category: cs.CL

TL;DR: The paper investigates language models' ability to handle temporal context in factual knowledge, revealing significant limitations.


<details>
  <summary>Details</summary>
Motivation: To assess whether LMs can accurately associate temporal contexts with past facts and identify their robustness to temporal variations.

Method: Introduces the TimeStress dataset to evaluate 18 LMs, analyzing accuracy based on distance from validity period and context granularity.

Result: The best LM achieved only 6% perfect accuracy, with errors humans wouldn't make, highlighting poor temporal representation.

Conclusion: Current LMs struggle with temporal context; the work provides data and code for future research.

Abstract: This paper explores the robustness of language models (LMs) to variations in
the temporal context within factual knowledge. It examines whether LMs can
correctly associate a temporal context with a past fact valid over a defined
period, by asking them to differentiate correct from incorrect contexts. The
accuracy of LMs is analyzed along two dimensions: the distance of the incorrect
context from the validity period and the granularity of the context. To this
end, a dataset called TimeStress is introduced, enabling the evaluation of 18
diverse LMs. Results reveal that the best LM achieves perfect accuracy for only
6% of the studied facts, with critical errors that humans would not make. This
work highlights the limitations of current LMs in temporal representation. We
provide all data and code for further research.

</details>


### [45] [EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning](https://arxiv.org/abs/2502.12486)
*Xiaoqian Liu, Ke Wang, Yongbin Li, Yuchuan Wu, Wentao Ma, Aobo Kong, Fei Huang, Jianbin Jiao, Junge Zhang*

Main category: cs.CL

TL;DR: The paper proposes Explicit Policy Optimization (EPO) for strategic reasoning in LLMs, enhancing adaptability and transferability through multi-turn RL, achieving state-of-the-art results in social and physical tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex real-world scenarios like business negotiations due to lack of strategic reasoning. Existing methods lack adaptability and scalability.

Method: EPO uses multi-turn RL with process rewards and iterative self-play, avoiding supervised fine-tuning. It integrates with arbitrary LLM agents for goal-directed behavior.

Result: EPO achieves state-of-the-art performance in social dialogue and web navigation, demonstrating long-term goal alignment and novel strategy generation.

Conclusion: EPO shows promise for real-world strategic reasoning, with emergent collaborative mechanisms and effective strategy generation.

Abstract: Large Language Models (LLMs) have shown impressive reasoning capabilities in
well-defined problems with clear solutions, such as mathematics and coding.
However, they still struggle with complex real-world scenarios like business
negotiations, which require strategic reasoning-an ability to navigate dynamic
environments and align long-term goals amidst uncertainty. Existing methods for
strategic reasoning face challenges in adaptability, scalability, and
transferring strategies to new contexts. To address these issues, we propose
explicit policy optimization (EPO) for strategic reasoning, featuring an LLM
that provides strategies in open-ended action space and can be plugged into
arbitrary LLM agents to motivate goal-directed behavior. To improve
adaptability and policy transferability, we train the strategic reasoning model
via multi-turn reinforcement learning (RL) using process rewards and iterative
self-play, without supervised fine-tuning (SFT) as a preliminary step.
Experiments across social and physical domains demonstrate EPO's ability of
long-term goal alignment through enhanced strategic reasoning, achieving
state-of-the-art performance on social dialogue and web navigation tasks. Our
findings reveal various collaborative reasoning mechanisms emergent in EPO and
its effectiveness in generating novel strategies, underscoring its potential
for strategic reasoning in real-world applications. Code and data are available
at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/EPO.

</details>


### [46] [Machine-generated text detection prevents language model collapse](https://arxiv.org/abs/2502.15654)
*George Drayson, Emine Yilmaz, Vasileios Lampos*

Main category: cs.CL

TL;DR: The study explores how decoding strategies in LLMs contribute to model collapse, where synthetic content degrades performance. It proposes a detector and sampling method to mitigate this, validated on GPT-2 and SmolLM2.


<details>
  <summary>Details</summary>
Motivation: The rise of machine-generated content risks polluting training data, leading to model collapse—a degenerative cycle where LLMs reinforce their own errors.

Method: Analyzed decoding strategies' impact on collapse, trained a synthetic text detector, and proposed importance sampling to balance human and synthetic data.

Result: The method prevents collapse and improves performance when human-authored data is sufficient, validated on GPT-2 and SmolLM2.

Conclusion: The proposed approach effectively mitigates model collapse, ensuring better LLM performance by managing synthetic data contamination.

Abstract: As Large Language Models (LLMs) become increasingly prevalent, their
generated outputs are proliferating across the web, risking a future where
machine-generated content dilutes human-authored text. Since online data is the
primary resource for LLM pre-training, subsequent models could be trained on an
unknown portion of synthetic samples. This will lead to model collapse, a
degenerative process whereby LLMs reinforce their own errors, and ultimately
yield a declining performance. In this study, we investigate the impact of
decoding strategy on model collapse, analysing the characteristics of text at
each model generation, the similarity to human references, and the resulting
model performance. Using the decoding strategies that lead to the most
significant degradation, we evaluate model collapse in more realistic scenarios
where the origin of the data (human or synthetic) is unknown. We train a
machine-generated text detector and propose an importance sampling approach to
alleviate model collapse. Our method is validated on two LLM variants (GPT-2
and SmolLM2) on the open-ended text generation task. We demonstrate that it can
not only prevent model collapse but also improve performance when sufficient
human-authored samples are present. We release our code at
https://github.com/GeorgeDrayson/model_collapse.

</details>


### [47] [HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks](https://arxiv.org/abs/2503.10894)
*Jiuding Sun, Jing Huang, Sidharth Baskaran, Karel D'Oosterlinck, Christopher Potts, Michael Sklar, Atticus Geiger*

Main category: cs.CL

TL;DR: HyperDAS improves mechanistic interpretability by using a transformer-based hypernetwork to locate and construct concept features in neural networks, achieving state-of-the-art performance on the RAVEL benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing methods like DAS require brute-force searches for concept features, which is inefficient. HyperDAS aims to automate and improve this process.

Method: HyperDAS employs a transformer-based hypernetwork to automatically locate token-positions and construct features in the residual stream for concepts.

Result: HyperDAS achieves state-of-the-art performance on the RAVEL benchmark with Llama3-8B.

Conclusion: HyperDAS advances interpretability by efficiently locating and constructing concept features while addressing concerns about injecting new information.

Abstract: Mechanistic interpretability has made great strides in identifying neural
network features (e.g., directions in hidden activation space) that mediate
concepts(e.g., the birth year of a person) and enable predictable manipulation.
Distributed alignment search (DAS) leverages supervision from counterfactual
data to learn concept features within hidden states, but DAS assumes we can
afford to conduct a brute force search over potential feature locations. To
address this, we present HyperDAS, a transformer-based hypernetwork
architecture that (1) automatically locates the token-positions of the residual
stream that a concept is realized in and (2) constructs features of those
residual stream vectors for the concept. In experiments with Llama3-8B,
HyperDAS achieves state-of-the-art performance on the RAVEL benchmark for
disentangling concepts in hidden states. In addition, we review the design
decisions we made to mitigate the concern that HyperDAS (like all powerful
interpretabilty methods) might inject new information into the target model
rather than faithfully interpreting it.

</details>


### [48] [LRAGE: Legal Retrieval Augmented Generation Evaluation Tool](https://arxiv.org/abs/2504.01840)
*Minhu Park, Hongseok Oh, Eunkyung Choi, Wonseok Hwang*

Main category: cs.CL

TL;DR: LRAGE is an open-source tool for evaluating retrieval-augmented generation (RAG) systems in the legal domain, focusing on five key components.


<details>
  <summary>Details</summary>
Motivation: To address the need for holistic evaluation of RAG systems in legal contexts, where prior documents are crucial.

Method: LRAGE provides GUI and CLI interfaces to experiment with retrieval corpora, algorithms, rerankers, LLM backbones, and evaluation metrics.

Result: Validated on multilingual legal benchmarks (Korean, English, Chinese), showing how component variations impact accuracy.

Conclusion: LRAGE is a practical tool for improving RAG systems in legal applications, with open-source availability.

Abstract: Recently, building retrieval-augmented generation (RAG) systems to enhance
the capability of large language models (LLMs) has become a common practice.
Especially in the legal domain, previous judicial decisions play a significant
role under the doctrine of stare decisis which emphasizes the importance of
making decisions based on (retrieved) prior documents. However, the overall
performance of RAG system depends on many components: (1) retrieval corpora,
(2) retrieval algorithms, (3) rerankers, (4) LLM backbones, and (5) evaluation
metrics. Here we propose LRAGE, an open-source tool for holistic evaluation of
RAG systems focusing on the legal domain. LRAGE provides GUI and CLI interfaces
to facilitate seamless experiments and investigate how changes in the
aforementioned five components affect the overall accuracy. We validated LRAGE
using multilingual legal benches including Korean (KBL), English (LegalBench),
and Chinese (LawBench) by demonstrating how the overall accuracy changes when
varying the five components mentioned above. The source code is available at
https://github.com/hoorangyee/LRAGE.

</details>


### [49] [Generative Evaluation of Complex Reasoning in Large Language Models](https://arxiv.org/abs/2504.02810)
*Haowei Lin, Xiangyu Wang, Ruilin Yan, Baizhou Huang, Haotian Ye, Jianhua Zhu, Zihao Wang, James Zou, Jianzhu Ma, Yitao Liang*

Main category: cs.CL

TL;DR: KUMO is a generative framework combining LLMs and symbolic engines to dynamically create diverse reasoning tasks, assessing LLMs' genuine reasoning abilities beyond memorization.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs truly reason or just recall from training data, given contamination of public benchmarks.

Method: KUMO generates novel, multi-turn reasoning tasks across domains, evaluating 23 LLMs on 5,000 tasks.

Result: Many LLMs outperform university students on easy tasks, and reasoning-scaled LLMs match them on complex tasks. KUMO correlates with real-world benchmarks.

Conclusion: KUMO is a robust, enduring tool for assessing genuine LLM reasoning capabilities.

Abstract: With powerful large language models (LLMs) demonstrating superhuman reasoning
capabilities, a critical question arises: Do LLMs genuinely reason, or do they
merely recall answers from their extensive, web-scraped training datasets?
Publicly released benchmarks inevitably become contaminated once incorporated
into subsequent LLM training sets, undermining their reliability as faithful
assessments. To address this, we introduce KUMO, a generative evaluation
framework designed specifically for assessing reasoning in LLMs. KUMO
synergistically combines LLMs with symbolic engines to dynamically produce
diverse, multi-turn reasoning tasks that are partially observable and
adjustable in difficulty. Through an automated pipeline, KUMO continuously
generates novel tasks across open-ended domains, compelling models to
demonstrate genuine generalization rather than memorization. We evaluated 23
state-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,
benchmarking their reasoning abilities against university students. Our
findings reveal that many LLMs have outperformed university-level performance
on easy reasoning tasks, and reasoning-scaled LLMs reach university-level
performance on complex reasoning challenges. Moreover, LLM performance on KUMO
tasks correlates strongly with results on newly released real-world reasoning
benchmarks, underscoring KUMO's value as a robust, enduring assessment tool for
genuine LLM reasoning capabilities.

</details>


### [50] [Can Reasoning LLMs Enhance Clinical Document Classification?](https://arxiv.org/abs/2504.08040)
*Akram Mustafa, Usman Naseem, Mostafa Rahimi Azghadi*

Main category: cs.CL

TL;DR: The study evaluates eight LLMs (four reasoning and four non-reasoning) for clinical document classification, finding reasoning models more accurate but less consistent than non-reasoning ones. A hybrid approach is suggested for optimal performance.


<details>
  <summary>Details</summary>
Motivation: Clinical document classification is challenging due to complex medical language, privacy issues, and limited annotated data. LLMs offer potential improvements in accuracy and efficiency.

Method: Eight LLMs were evaluated using the MIMIC-IV dataset, with cTAKES structuring narratives. Models were assessed over three runs, with majority voting for predictions.

Result: Reasoning models outperformed non-reasoning ones in accuracy (71% vs 68%) and F1 score (67% vs 60%), but non-reasoning models were more consistent (91% vs 84%). Gemini 2.0 Flash Thinking was the top performer.

Conclusion: A trade-off exists between accuracy and consistency; a hybrid approach may optimize clinical coding. Future work should explore multi-label classification, fine-tuning, and ensemble methods.

Abstract: Clinical document classification is essential for converting unstructured
medical texts into standardised ICD-10 diagnoses, yet it faces challenges due
to complex medical language, privacy constraints, and limited annotated
datasets. Large Language Models (LLMs) offer promising improvements in accuracy
and efficiency for this task. This study evaluates the performance and
consistency of eight LLMs; four reasoning (Qwen QWQ, Deepseek Reasoner, GPT o3
Mini, Gemini 2.0 Flash Thinking) and four non-reasoning (Llama 3.3, GPT 4o
Mini, Gemini 2.0 Flash, Deepseek Chat); in classifying clinical discharge
summaries using the MIMIC-IV dataset. Using cTAKES to structure clinical
narratives, models were assessed across three experimental runs, with majority
voting determining final predictions. Results showed that reasoning models
outperformed non-reasoning models in accuracy (71% vs 68%) and F1 score (67% vs
60%), with Gemini 2.0 Flash Thinking achieving the highest accuracy (75%) and
F1 score (76%). However, non-reasoning models demonstrated greater stability
(91% vs 84% consistency). Performance varied across ICD-10 codes, with
reasoning models excelling in complex cases but struggling with abstract
categories. Findings indicate a trade-off between accuracy and consistency,
suggesting that a hybrid approach could optimise clinical coding. Future
research should explore multi-label classification, domain-specific
fine-tuning, and ensemble methods to enhance model reliability in real-world
applications.

</details>


### [51] [BitNet b1.58 2B4T Technical Report](https://arxiv.org/abs/2504.12285)
*Shuming Ma, Hongyu Wang, Shaohan Huang, Xingxing Zhang, Ying Hu, Ting Song, Yan Xia, Furu Wei*

Main category: cs.CL

TL;DR: BitNet b1.58 2B4T is the first open-source 1-bit LLM at 2B parameters, matching full-precision models in performance while being more efficient.


<details>
  <summary>Details</summary>
Motivation: To create a highly efficient, open-source 1-bit LLM that reduces computational costs without sacrificing performance.

Method: Trained on 4 trillion tokens and evaluated across language, math, coding, and conversational benchmarks.

Result: Achieves performance comparable to full-precision LLMs with lower memory, energy, and latency.

Conclusion: BitNet b1.58 2B4T offers a viable, efficient alternative to traditional LLMs, with open-source availability for broader adoption.

Abstract: We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large
Language Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4
trillion tokens, the model has been rigorously evaluated across benchmarks
covering language understanding, mathematical reasoning, coding proficiency,
and conversational ability. Our results demonstrate that BitNet b1.58 2B4T
achieves performance on par with leading open-weight, full-precision LLMs of
similar size, while offering significant advantages in computational
efficiency, including substantially reduced memory footprint, energy
consumption, and decoding latency. To facilitate further research and adoption,
the model weights are released via Hugging Face along with open-source
inference implementations for both GPU and CPU architectures.

</details>


### [52] [A Case Study Exploring the Current Landscape of Synthetic Medical Record Generation with Commercial LLMs](https://arxiv.org/abs/2504.14657)
*Yihan Lin, Zhirong Bella Yu, Simon Lee*

Main category: cs.CL

TL;DR: LLMs can generate synthetic EHRs for small feature sets but struggle with realistic distributions and correlations in high-dimensional data, limiting generalization across hospitals.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' capability in generating synthetic EHRs and identify their strengths and weaknesses in preserving data realism and generalizability.

Method: Evaluated commercial LLMs for synthetic data generation, focusing on feature subsets and high-dimensional data.

Result: LLMs perform well with small feature sets but fail to maintain realistic distributions and correlations in high-dimensional data.

Conclusion: LLMs have potential for synthetic EHRs but face challenges in scalability and generalization for diverse hospital settings.

Abstract: Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to
create privacy preserving and harmonized structured data, supporting numerous
applications in healthcare. Key benefits of synthetic data include precise
control over the data schema, improved fairness and representation of patient
populations, and the ability to share datasets without concerns about
compromising real individuals privacy. Consequently, the AI community has
increasingly turned to Large Language Models (LLMs) to generate synthetic data
across various domains. However, a significant challenge in healthcare is
ensuring that synthetic health records reliably generalize across different
hospitals, a long standing issue in the field. In this work, we evaluate the
current state of commercial LLMs for generating synthetic data and investigate
multiple aspects of the generation process to identify areas where these models
excel and where they fall short. Our main finding from this work is that while
LLMs can reliably generate synthetic health records for smaller subsets of
features, they struggle to preserve realistic distributions and correlations as
the dimensionality of the data increases, ultimately limiting their ability to
generalize across diverse hospital settings.

</details>


### [53] [Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model](https://arxiv.org/abs/2504.15843)
*Junshu Pan, Wei Shen, Shulin Huang, Qiji Zhou, Yue Zhang*

Main category: cs.CL

TL;DR: Pre-DPO improves DPO and SimPO by using a guiding reference model for better preference optimization, enhancing performance without extra data or models.


<details>
  <summary>Details</summary>
Motivation: Identical initialization in DPO leads to inefficiency, while SimPO lacks robustness. Pre-DPO addresses these issues by leveraging a reference model for adaptive weighting.

Method: Pre-DPO introduces a guiding reference model to adaptively weight training samples, optimizing human preferences more effectively.

Result: Experiments on AlpacaEval 2.0 and Arena-Hard v0.1 show Pre-DPO consistently boosts DPO and SimPO performance.

Conclusion: Pre-DPO offers a simple, effective solution to enhance preference optimization in LLMs, outperforming existing methods.

Abstract: Direct Preference Optimization (DPO) simplifies reinforcement learning from
human feedback (RLHF) for large language models (LLMs) by directly optimizing
human preferences without an explicit reward model. We find that during DPO
training, the reference model plays the role of a data weight adjuster.
However, the common practice of initializing the policy and reference models
identically in DPO can lead to inefficient data utilization and impose a
performance ceiling. Meanwhile, the lack of a reference model in Simple
Preference Optimization (SimPO) reduces training robustness and necessitates
stricter conditions to prevent catastrophic forgetting. In this work, we
propose Pre-DPO, a simple yet effective DPO-based training paradigm that
enhances preference optimization performance by leveraging a guiding reference
model. This reference model provides foresight into the optimal policy state
achievable through the training preference data, serving as a guiding mechanism
that adaptively assigns higher weights to samples more suitable for the model
and lower weights to those less suitable. Extensive experiments on AlpacaEval
2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently
improves the performance of both DPO and SimPO, without relying on external
models or additional data.

</details>


### [54] [CAPO: Cost-Aware Prompt Optimization](https://arxiv.org/abs/2504.16005)
*Tom Zehle, Moritz Schlager, Timo Heiß, Matthias Feurer*

Main category: cs.CL

TL;DR: CAPO (Cost-Aware Prompt Optimization) is an efficient algorithm for optimizing prompts in LLMs, combining AutoML techniques to reduce costs and improve performance.


<details>
  <summary>Details</summary>
Motivation: Current prompt optimization methods are expensive due to high LLM calls and token usage, necessitating a more cost-efficient solution.

Method: CAPO uses an evolutionary approach with LLMs as operators, integrating racing for evaluation savings and multi-objective optimization to balance performance and prompt length.

Result: CAPO outperforms state-of-the-art methods in 11/15 cases, achieving up to 21% improvement, with better efficiency and robustness.

Conclusion: CAPO advances prompt optimization by making it more cost-efficient and accessible, even without few-shot examples.

Abstract: Large language models (LLMs) have revolutionized natural language processing
by solving a wide range of tasks simply guided by a prompt. Yet their
performance is highly sensitive to prompt formulation. While automated prompt
optimization addresses this challenge by finding optimal prompts, current
methods require a substantial number of LLM calls and input tokens, making
prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt
Optimization), an algorithm that enhances prompt optimization efficiency by
integrating AutoML techniques. CAPO is an evolutionary approach with LLMs as
operators, incorporating racing to save evaluations and multi-objective
optimization to balance performance with prompt length. It jointly optimizes
instructions and few-shot examples while leveraging task descriptions for
improved robustness. Our extensive experiments across diverse datasets and LLMs
demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization
methods in 11/15 cases with improvements up to 21%p. Our algorithm achieves
better performances already with smaller budgets, saves evaluations through
racing, and decreases average prompt length via a length penalty, making it
both cost-efficient and cost-aware. Even without few-shot examples, CAPO
outperforms its competitors and generally remains robust to initial prompts.
CAPO represents an important step toward making prompt optimization more
powerful and accessible by improving cost-efficiency.

</details>


### [55] [The Rise of Small Language Models in Healthcare: A Comprehensive Survey](https://arxiv.org/abs/2504.17119)
*Muskan Garg, Shaina Raza, Shebuti Rayana, Xingyi Liu, Sunghwan Sohn*

Main category: cs.CL

TL;DR: The paper surveys small language models (SLMs) for healthcare, offering a taxonomic framework to categorize them, analyze their contributions, and highlight their potential in resource-constrained environments.


<details>
  <summary>Details</summary>
Motivation: Addressing concerns around data privacy and limited resources in healthcare, the paper explores SLMs as a scalable and clinically viable alternative to large language models (LLMs).

Method: The study presents a taxonomic framework analyzing SLMs across NLP tasks, stakeholder roles, and care continuum, covering architectural foundations, adaptation techniques, and compression methods.

Result: A comprehensive survey and experimental results showcase SLMs' transformative potential in healthcare, supported by a curated repository.

Conclusion: SLMs offer a promising solution for next-generation healthcare informatics, with the framework aiding professionals in future research and development.

Abstract: Despite substantial progress in healthcare applications driven by large
language models (LLMs), growing concerns around data privacy, and limited
resources; the small language models (SLMs) offer a scalable and clinically
viable solution for efficient performance in resource-constrained environments
for next-generation healthcare informatics. Our comprehensive survey presents a
taxonomic framework to identify and categorize them for healthcare
professionals and informaticians. The timeline of healthcare SLM contributions
establishes a foundational framework for analyzing models across three
dimensions: NLP tasks, stakeholder roles, and the continuum of care. We present
a taxonomic framework to identify the architectural foundations for building
models from scratch; adapting SLMs to clinical precision through prompting,
instruction fine-tuning, and reasoning; and accessibility and sustainability
through compression techniques. Our primary objective is to offer a
comprehensive survey for healthcare professionals, introducing recent
innovations in model optimization and equipping them with curated resources to
support future research and development in the field. Aiming to showcase the
groundbreaking advancements in SLMs for healthcare, we present a comprehensive
compilation of experimental results across widely studied NLP tasks in
healthcare to highlight the transformative potential of SLMs in healthcare. The
updated repository is available at Github

</details>


### [56] [DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training](https://arxiv.org/abs/2504.17565)
*Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Yunjie Ji, Han Zhao, Xiangang Li*

Main category: cs.CL

TL;DR: The paper introduces a large-scale, difficulty-graded reasoning dataset to improve LLM training, achieving a 79.2% pass rate on AIME2024.


<details>
  <summary>Details</summary>
Motivation: Address the lack of understanding in base model training processes and data quality for LLMs.

Method: Construct a dataset with 3.34M queries and 40M distilled responses, using pass rate and CV for data selection, and adjust learning rates for reasoning-focused training.

Result: Improved reasoning capabilities with a 79.2% pass rate on AIME2024, surpassing most distilled models.

Conclusion: The dataset and methods are publicly released to advance open-source long-reasoning LLMs.

Abstract: Although large language models (LLMs) have recently achieved remarkable
performance on various complex reasoning benchmarks, the academic community
still lacks an in-depth understanding of base model training processes and data
quality. To address this, we construct a large-scale, difficulty-graded
reasoning dataset containing approximately 3.34 million unique queries of
varying difficulty levels and about 40 million distilled responses generated by
multiple models over several passes. Leveraging pass rate and Coefficient of
Variation (CV), we precisely select the most valuable training data to enhance
reasoning capability. Notably, we observe a training pattern shift, indicating
that reasoning-focused training based on base models requires higher learning
rates for effective training. Using this carefully selected data, we
significantly improve the reasoning capabilities of the base model, achieving a
pass rate of 79.2\% on the AIME2024 mathematical reasoning benchmark. This
result surpasses most current distilled models and closely approaches
state-of-the-art performance. We provide detailed descriptions of our data
processing, difficulty assessment, and training methodology, and have publicly
released all datasets and methods to promote rapid progress in open-source
long-reasoning LLMs. The dataset is available at:
https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M

</details>


### [57] [Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction](https://arxiv.org/abs/2504.17671)
*Yuanchang Ye, Weiyan Wen*

Main category: cs.CL

TL;DR: A Split Conformal Prediction (SCP) framework mitigates hallucination in Large Vision-Language Models (LVLMs) for VQA tasks by quantifying uncertainty and ensuring statistical guarantees.


<details>
  <summary>Details</summary>
Motivation: LVLMs often produce hallucinated content with high confidence, posing risks in safety-critical applications like healthcare and autonomous systems.

Method: The SCP framework uses dynamic threshold calibration and cross-modal consistency verification, partitioning data into calibration and test sets to compute nonconformity scores and construct prediction sets.

Result: Evaluations on benchmarks (ScienceQA, MMMU) with eight LVLMs show SCP enforces theoretical guarantees across all risk levels (α), achieving stable performance.

Conclusion: The SCP framework bridges theoretical reliability and practical applicability, offering a scalable solution for hallucination detection and uncertainty-aware decision-making in multi-modal AI systems.

Abstract: This study addresses the critical challenge of hallucination mitigation in
Large Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks
through a Split Conformal Prediction (SCP) framework. While LVLMs excel in
multi-modal reasoning, their outputs often exhibit hallucinated content with
high confidence, posing risks in safety-critical applications. We propose a
model-agnostic uncertainty quantification method that integrates dynamic
threshold calibration and cross-modal consistency verification. By partitioning
data into calibration and test sets, the framework computes nonconformity
scores to construct prediction sets with statistical guarantees under
user-defined risk levels ($\alpha$). Key innovations include: (1) rigorous
control of \textbf{marginal coverage} to ensure empirical error rates remain
strictly below $\alpha$; (2) dynamic adjustment of prediction set sizes
inversely with $\alpha$, filtering low-confidence outputs; (3) elimination of
prior distribution assumptions and retraining requirements. Evaluations on
benchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces
theoretical guarantees across all $\alpha$ values. The framework achieves
stable performance across varying calibration-to-test split ratios,
underscoring its robustness for real-world deployment in healthcare, autonomous
systems, and other safety-sensitive domains. This work bridges the gap between
theoretical reliability and practical applicability in multi-modal AI systems,
offering a scalable solution for hallucination detection and uncertainty-aware
decision-making.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [58] [Spectral Dictionary Learning for Generative Image Modeling](https://arxiv.org/abs/2504.17804)
*Andrew Kiruluta*

Main category: cs.CV

TL;DR: A novel spectral generative model for image synthesis using learned spectral basis functions, offering interpretability and competitive performance.


<details>
  <summary>Details</summary>
Motivation: To depart from common paradigms (variational, adversarial, diffusion) and provide a more interpretable, deterministic approach to image synthesis.

Method: Images are flattened into 1D signals, reconstructed using learned spectral basis functions (parameterized by frequency, phase, amplitude), with a probabilistic model for mixing coefficients.

Result: Achieves competitive performance on CIFAR-10 in reconstruction quality, perceptual fidelity, training stability, and computational efficiency.

Conclusion: The model provides interpretable, controlled synthesis with potential for novel applications in image manipulation and analysis.

Abstract: We propose a novel spectral generative model for image synthesis that departs
radically from the common variational, adversarial, and diffusion paradigms. In
our approach, images, after being flattened into one-dimensional signals, are
reconstructed as linear combinations of a set of learned spectral basis
functions, where each basis is explicitly parameterized in terms of frequency,
phase, and amplitude. The model jointly learns a global spectral dictionary
with time-varying modulations and per-image mixing coefficients that quantify
the contributions of each spectral component. Subsequently, a simple
probabilistic model is fitted to these mixing coefficients, enabling the
deterministic generation of new images by sampling from the latent space. This
framework leverages deterministic dictionary learning, offering a highly
interpretable and physically meaningful representation compared to methods
relying on stochastic inference or adversarial training. Moreover, the
incorporation of frequency-domain loss functions, computed via the short-time
Fourier transform (STFT), ensures that the synthesized images capture both
global structure and fine-grained spectral details, such as texture and edge
information. Experimental evaluations on the CIFAR-10 benchmark demonstrate
that our approach not only achieves competitive performance in terms of
reconstruction quality and perceptual fidelity but also offers improved
training stability and computational efficiency. This new type of generative
model opens up promising avenues for controlled synthesis, as the learned
spectral dictionary affords a direct handle on the intrinsic frequency content
of the images, thus providing enhanced interpretability and potential for novel
applications in image manipulation and analysis.

</details>


### [59] [SmallGS: Gaussian Splatting-based Camera Pose Estimation for Small-Baseline Videos](https://arxiv.org/abs/2504.17810)
*Yuxin Yao, Yan Zhang, Zhening Huang, Joan Lasenby*

Main category: cs.CV

TL;DR: SmallGS is a camera pose estimation framework for small-baseline videos using Gaussian splatting and robust visual features, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Small-baseline videos pose challenges for pose estimation due to ambiguous features and drift. Gaussian splatting offers stability for such scenarios.

Method: SmallGS optimizes camera poses using Gaussian splatting and incorporates pretrained features (e.g., DINOv2) for robustness.

Result: SmallGS achieves high accuracy in camera pose estimation for small-baseline videos, surpassing MonST3R and DORID-SLAM.

Conclusion: SmallGS effectively addresses challenges in small-baseline pose estimation, leveraging Gaussian splatting and feature rendering for improved performance.

Abstract: Dynamic videos with small baseline motions are ubiquitous in daily life,
especially on social media. However, these videos present a challenge to
existing pose estimation frameworks due to ambiguous features, drift
accumulation, and insufficient triangulation constraints. Gaussian splatting,
which maintains an explicit representation for scenes, provides a reliable
novel view rasterization when the viewpoint change is small. Inspired by this,
we propose SmallGS, a camera pose estimation framework that is specifically
designed for small-baseline videos. SmallGS optimizes sequential camera poses
using Gaussian splatting, which reconstructs the scene from the first frame in
each video segment to provide a stable reference for the rest. The temporal
consistency of Gaussian splatting within limited viewpoint differences reduced
the requirement of sufficient depth variations in traditional camera pose
estimation. We further incorporate pretrained robust visual features, e.g.
DINOv2, into Gaussian splatting, where high-dimensional feature map rendering
enhances the robustness of camera pose estimation. By freezing the Gaussian
splatting and optimizing camera viewpoints based on rasterized features,
SmallGS effectively learns camera poses without requiring explicit feature
correspondences or strong parallax motion. We verify the effectiveness of
SmallGS in small-baseline videos in TUM-Dynamics sequences, which achieves
impressive accuracy in camera pose estimation compared to MonST3R and
DORID-SLAM for small-baseline videos in dynamic scenes. Our project page is at:
https://yuxinyao620.github.io/SmallGS

</details>


### [60] [Seeing Soundscapes: Audio-Visual Generation and Separation from Soundscapes Using Audio-Visual Separator](https://arxiv.org/abs/2504.18283)
*Minjae Kang, Martim Brandão*

Main category: cs.CV

TL;DR: AV-GAS model generates images from mixed audio, outperforming state-of-the-art with 7% higher CRS and 4% higher R@2*.


<details>
  <summary>Details</summary>
Motivation: Existing models fail to generate images from mixed audio, limiting their real-world applicability.

Method: Proposes AV-GAS with an audio-visual separator for multi-class audio input and introduces new tasks and metrics (CRS, R@K).

Result: Achieves 7% higher CRS and 4% higher R@2* on VGGSound dataset.

Conclusion: AV-GAS advances audio-visual generation by handling mixed audio and setting new benchmarks.

Abstract: Recent audio-visual generative models have made substantial progress in
generating images from audio. However, existing approaches focus on generating
images from single-class audio and fail to generate images from mixed audio. To
address this, we propose an Audio-Visual Generation and Separation model
(AV-GAS) for generating images from soundscapes (mixed audio containing
multiple classes). Our contribution is threefold: First, we propose a new
challenge in the audio-visual generation task, which is to generate an image
given a multi-class audio input, and we propose a method that solves this task
using an audio-visual separator. Second, we introduce a new audio-visual
separation task, which involves generating separate images for each class
present in a mixed audio input. Lastly, we propose new evaluation metrics for
the audio-visual generation task: Class Representation Score (CRS) and a
modified R@K. Our model is trained and evaluated on the VGGSound dataset. We
show that our method outperforms the state-of-the-art, achieving 7% higher CRS
and 4% higher R@2* in generating plausible images with mixed audio.

</details>


### [61] [Object Learning and Robust 3D Reconstruction](https://arxiv.org/abs/2504.17812)
*Sara Sabour*

Main category: cs.CV

TL;DR: The thesis explores unsupervised neural network designs for 2D and 3D object segmentation, using motion and geometric cues, and aims to advance object-based computer vision.


<details>
  <summary>Details</summary>
Motivation: To enable neural networks to segment objects of interest without supervision, addressing challenges in distinguishing foreground from background.

Method: FlowCapsules uses motion cues for 2D segmentation; 3D applications leverage geometric consistency to detect dynamic objects.

Result: Transient object masks improve 3D modeling, demonstrating the potential of unsupervised object-based approaches.

Conclusion: The work highlights the value of unsupervised methods and suggests future directions for defining objects of interest without supervision.

Abstract: In this thesis we discuss architectural designs and training methods for a
neural network to have the ability of dissecting an image into objects of
interest without supervision. The main challenge in 2D unsupervised object
segmentation is distinguishing between foreground objects of interest and
background. FlowCapsules uses motion as a cue for the objects of interest in 2D
scenarios. The last part of this thesis focuses on 3D applications where the
goal is detecting and removal of the object of interest from the input images.
In these tasks, we leverage the geometric consistency of scenes in 3D to detect
the inconsistent dynamic objects. Our transient object masks are then used for
designing robust optimization kernels to improve 3D modelling in a casual
capture setup. One of our goals in this thesis is to show the merits of
unsupervised object based approaches in computer vision. Furthermore, we
suggest possible directions for defining objects of interest or foreground
objects without requiring supervision. Our hope is to motivate and excite the
community into further exploring explicit object representations in image
understanding tasks.

</details>


### [62] [PreGSU-A Generalized Traffic Scene Understanding Model for Autonomous Driving based on Pre-trained Graph Attention Network](https://arxiv.org/abs/2404.10263)
*Yuning Wang, Zhiyuan Liu, Haotian Lin, Junkai Jiang, Shaobing Xu, Jianqiang Wang*

Main category: cs.CV

TL;DR: PreGSU is a pre-trained scene understanding model using graph attention networks to generalize interactions in traffic scenes for various downstream tasks, outperforming single-task methods.


<details>
  <summary>Details</summary>
Motivation: Current scene understanding methods lack generalization for real traffic complexity and diverse downstream tasks.

Method: PreGSU uses graph attention networks with self-supervised tasks (VIF and MRM) to model agent-agent and agent-road interactions.

Result: PreGSU achieves competitive performance across datasets and tasks, showing strong generalization.

Conclusion: PreGSU's pre-train tasks enhance its adaptability, making it effective for diverse traffic scenes and tasks.

Abstract: Scene understanding, defined as learning, extraction, and representation of
interactions among traffic elements, is one of the critical challenges toward
high-level autonomous driving (AD). Current scene understanding methods mainly
focus on one concrete single task, such as trajectory prediction and risk level
evaluation. Although they perform well on specific metrics, the generalization
ability is insufficient to adapt to the real traffic complexity and downstream
demand diversity. In this study, we propose PreGSU, a generalized pre-trained
scene understanding model based on graph attention network to learn the
universal interaction and reasoning of traffic scenes to support various
downstream tasks. After the feature engineering and sub-graph module, all
elements are embedded as nodes to form a dynamic weighted graph. Then, four
graph attention layers are applied to learn the relationships among agents and
lanes. In the pre-train phase, the understanding model is trained on two
self-supervised tasks: Virtual Interaction Force (VIF) modeling and Masked Road
Modeling (MRM). Based on the artificial potential field theory, VIF modeling
enables PreGSU to capture the agent-to-agent interactions while MRM extracts
agent-to-road connections. In the fine-tuning process, the pre-trained
parameters are loaded to derive detailed understanding outputs. We conduct
validation experiments on three datasets and two downstream tasks, i.e.,
trajectory prediction in urban scenario and intention recognition in highway
scenario, to verify the model's generalization and understanding capabilities.
Results show that compared with single-task-driven baselines, PreGSU achieves
competitive performance on all datasets and downstream tasks, indicating its
potential to be generalized to various scenes and targets. Ablation study shows
the effectiveness of pre-train task design.

</details>


### [63] [CLOC: Contrastive Learning for Ordinal Classification with Multi-Margin N-pair Loss](https://arxiv.org/abs/2504.17813)
*Dileepa Pitawela, Gustavo Carneiro, Hsiang-Ting Chen*

Main category: cs.CV

TL;DR: CLOC is a new margin-based contrastive learning method for ordinal classification that optimizes multiple margins to address varying importance of misclassifications, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing ordinal classification methods treat all neighboring classes equally, ignoring varying consequences of misclassifications (e.g., benign vs. cancerous tumor categories).

Method: CLOC uses a novel multi-margin n-pair loss (MMNP) to learn ordered representations with flexible decision boundaries, reducing overfitting and biases.

Result: CLOC outperforms existing methods on five real-world and one synthetic dataset, showing interpretability and alignment with clinical needs.

Conclusion: CLOC effectively addresses the limitations of current ordinal classification methods by learning meaningful, ordered representations tailored to practical and clinical requirements.

Abstract: In ordinal classification, misclassifying neighboring ranks is common, yet
the consequences of these errors are not the same. For example, misclassifying
benign tumor categories is less consequential, compared to an error at the
pre-cancerous to cancerous threshold, which could profoundly influence
treatment choices. Despite this, existing ordinal classification methods do not
account for the varying importance of these margins, treating all neighboring
classes as equally significant. To address this limitation, we propose CLOC, a
new margin-based contrastive learning method for ordinal classification that
learns an ordered representation based on the optimization of multiple margins
with a novel multi-margin n-pair loss (MMNP). CLOC enables flexible decision
boundaries across key adjacent categories, facilitating smooth transitions
between classes and reducing the risk of overfitting to biases present in the
training data. We provide empirical discussion regarding the properties of MMNP
and show experimental results on five real-world image datasets (Adience,
Historical Colour Image Dating, Knee Osteoarthritis, Indian Diabetic
Retinopathy Image, and Breast Carcinoma Subtyping) and one synthetic dataset
simulating clinical decision bias. Our results demonstrate that CLOC
outperforms existing ordinal classification methods and show the
interpretability and controllability of CLOC in learning meaningful, ordered
representations that align with clinical and practical needs.

</details>


### [64] [Visibility-Uncertainty-guided 3D Gaussian Inpainting via Scene Conceptional Learning](https://arxiv.org/abs/2504.17815)
*Mingxuan Cui, Qing Guo, Yuyi Wang, Hongkai Yu, Di Lin, Qin Zou, Ming-Ming Cheng, Xi Li*

Main category: cs.CV

TL;DR: The paper introduces VISTA, a framework for 3D Gaussian inpainting (3DGI) that leverages visibility uncertainties and semantic learning to seamlessly replace masked objects in 3D scenes.


<details>
  <summary>Details</summary>
Motivation: Extending 3D Gaussian Splatting (3DGS) to inpainting, addressing challenges in utilizing complementary visual and semantic cues from multiple views.

Method: Proposes measuring visibility uncertainties of 3D points across views, guiding inpainting with complementary cues, and using a diffusion model for semantic filling.

Result: VISTA produces high-quality 3DGS models with artifact-free inpainting and handles dynamic distractors.

Conclusion: The method outperforms state-of-the-art techniques on diverse datasets, demonstrating versatility in scene reconstruction.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful and efficient 3D
representation for novel view synthesis. This paper extends 3DGS capabilities
to inpainting, where masked objects in a scene are replaced with new contents
that blend seamlessly with the surroundings. Unlike 2D image inpainting, 3D
Gaussian inpainting (3DGI) is challenging in effectively leveraging
complementary visual and semantic cues from multiple input views, as occluded
areas in one view may be visible in others. To address this, we propose a
method that measures the visibility uncertainties of 3D points across different
input views and uses them to guide 3DGI in utilizing complementary visual cues.
We also employ uncertainties to learn a semantic concept of scene without the
masked object and use a diffusion model to fill masked objects in input images
based on the learned concept. Finally, we build a novel 3DGI framework, VISTA,
by integrating VISibility-uncerTainty-guided 3DGI with scene conceptuAl
learning. VISTA generates high-quality 3DGS models capable of synthesizing
artifact-free and naturally inpainted novel views. Furthermore, our approach
extends to handling dynamic distractors arising from temporal object changes,
enhancing its versatility in diverse scene reconstruction scenarios. We
demonstrate the superior performance of our method over state-of-the-art
techniques using two challenging datasets: the SPIn-NeRF dataset, featuring 10
diverse static 3D inpainting scenes, and an underwater 3D inpainting dataset
derived from UTB180, including fast-moving fish as inpainting targets.

</details>


### [65] [Subject-driven Video Generation via Disentangled Identity and Motion](https://arxiv.org/abs/2504.17816)
*Daneul Kim, Jingxu Zhang, Wonjoon Jin, Sunghyun Cho, Qi Dai, Jaesik Park, Chong Luo*

Main category: cs.CV

TL;DR: A zero-shot, tuning-free method for subject-driven video generation decouples subject-specific learning from temporal dynamics, using image datasets and unannotated videos for training, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Traditional video customization methods require large annotated datasets, which are costly and time-consuming. This work aims to simplify the process by leveraging image datasets and unannotated videos.

Method: The approach involves (1) identity injection via image datasets and (2) temporal modeling with unannotated videos. Techniques like random token dropping and stochastic switching are used to improve training.

Result: The method achieves strong subject consistency and scalability, outperforming existing zero-shot video customization models.

Conclusion: The proposed framework effectively decouples subject-specific and temporal learning, offering a scalable and efficient solution for video customization.

Abstract: We propose to train a subject-driven customized video generation model
through decoupling the subject-specific learning from temporal dynamics in
zero-shot without additional tuning. A traditional method for video
customization that is tuning-free often relies on large, annotated video
datasets, which are computationally expensive and require extensive annotation.
In contrast to the previous approach, we introduce the use of an image
customization dataset directly on training video customization models,
factorizing the video customization into two folds: (1) identity injection
through image customization dataset and (2) temporal modeling preservation with
a small set of unannotated videos through the image-to-video training method.
Additionally, we employ random image token dropping with randomized image
initialization during image-to-video fine-tuning to mitigate the copy-and-paste
issue. To further enhance learning, we introduce stochastic switching during
joint optimization of subject-specific and temporal features, mitigating
catastrophic forgetting. Our method achieves strong subject consistency and
scalability, outperforming existing video customization models in zero-shot
settings, demonstrating the effectiveness of our framework.

</details>


### [66] [Learning Underwater Active Perception in Simulation](https://arxiv.org/abs/2504.17817)
*Alexandre Cardaillac, Donald G. Dansereau*

Main category: cs.CV

TL;DR: A method for improving underwater image quality in varying water conditions using an MLP-based active perception framework, validated with synthetic data.


<details>
  <summary>Details</summary>
Motivation: Underwater vehicle inspections are hindered by turbidity and backscattering, impacting visibility and mission success. Existing methods have constraints, prompting a simpler, efficient solution.

Method: Proposes an MLP trained to predict image quality based on distance and light intensity, using synthetic data from modified Blender software for underwater light modeling.

Result: Validated in simulation, the approach significantly improves visual coverage and image quality over traditional methods.

Conclusion: The framework offers a practical solution for high-quality underwater imaging across diverse water conditions, with code publicly available.

Abstract: When employing underwater vehicles for the autonomous inspection of assets,
it is crucial to consider and assess the water conditions. Indeed, they have a
significant impact on the visibility, which also affects robotic operations.
Turbidity can jeopardise the whole mission as it may prevent correct visual
documentation of the inspected structures. Previous works have introduced
methods to adapt to turbidity and backscattering, however, they also include
manoeuvring and setup constraints. We propose a simple yet efficient approach
to enable high-quality image acquisition of assets in a broad range of water
conditions. This active perception framework includes a multi-layer perceptron
(MLP) trained to predict image quality given a distance to a target and
artificial light intensity. We generated a large synthetic dataset including
ten water types with different levels of turbidity and backscattering. For
this, we modified the modelling software Blender to better account for the
underwater light propagation properties. We validated the approach in
simulation and showed significant improvements in visual coverage and quality
of imagery compared to traditional approaches. The project code is available on
our project page at https://roboticimaging.org/Projects/ActiveUW/.

</details>


### [67] [VideoVista-CulturalLingo: 360$^\circ$ Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension](https://arxiv.org/abs/2504.17821)
*Xinyu Chen, Yunxin Li, Haoyuan Shi, Baotian Hu, Wenhan Luo, Yaowei Wang, Min Zhang*

Main category: cs.CV

TL;DR: VideoVista-CulturalLingo is a culturally and linguistically diverse video evaluation benchmark, revealing performance gaps in AI models, especially in Chinese-centric and temporal understanding tasks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of cultural and linguistic diversity in existing video comprehension benchmarks, which are often limited to English and Western contexts.

Method: Developed VideoVista-CulturalLingo, featuring 1,389 videos and 3,134 QA pairs across diverse cultures (China, North America, Europe), languages (Chinese, English), and domains. Evaluated 24 video large models.

Result: Models perform worse on Chinese-centric questions, struggle with temporal understanding (max 45.2% in Event Localization), and open-source models lag in mathematics.

Conclusion: The benchmark highlights the need for culturally inclusive and temporally aware AI models, revealing current limitations in multilingual and domain-specific comprehension.

Abstract: Assessing the video comprehension capabilities of multimodal AI systems can
effectively measure their understanding and reasoning abilities. Most video
evaluation benchmarks are limited to a single language, typically English, and
predominantly feature videos rooted in Western cultural contexts. In this
paper, we present VideoVista-CulturalLingo, the first video evaluation
benchmark designed to bridge cultural, linguistic, and domain divide in video
comprehension. Our work differs from existing benchmarks in the following ways:
1) Cultural diversity, incorporating cultures from China, North America, and
Europe; 2) Multi-linguistics, with questions presented in Chinese and
English-two of the most widely spoken languages; and 3) Broad domain, featuring
videos sourced from hundreds of human-created domains. VideoVista-CulturalLingo
contains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent
open-source or proprietary video large models. From the experiment results, we
observe that: 1) Existing models perform worse on Chinese-centric questions
than Western-centric ones, particularly those related to Chinese history; 2)
Current open-source models still exhibit limitations in temporal understanding,
especially in the Event Localization task, achieving a maximum score of only
45.2%; 3) Mainstream models demonstrate strong performance in general
scientific questions, while open-source models demonstrate weak performance in
mathematics.

</details>


### [68] [A multi-scale vision transformer-based multimodal GeoAI model for mapping Arctic permafrost thaw](https://arxiv.org/abs/2504.17822)
*Wenwen Li, Chia-Yu Hsu, Sizhe Wang, Zhining Gu, Yili Yang, Brendan M. Rogers, Anna Liljedahl*

Main category: cs.CV

TL;DR: A deep learning model (Cascade Mask R-CNN with a multi-scale vision transformer backbone) was used to map Retrogressive Thaw Slumps (RTS) in the Arctic, introducing two new strategies for multimodal learning to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: RTS are key indicators of permafrost thaw but are hard to detect due to their small scale, vague boundaries, and variability. Accurate mapping is crucial for understanding environmental impacts.

Method: Used Cascade Mask R-CNN with a multi-scale vision transformer backbone. Introduced feature-level residual cross-modality attention fusion and pre-trained unimodal learning followed by multimodal fine-tuning.

Result: Outperformed existing models, providing better RTS mapping and insights into efficient multimodal data use.

Conclusion: The approach enhances RTS detection, contributing to understanding permafrost landforms and their environmental implications.

Abstract: Retrogressive Thaw Slumps (RTS) in Arctic regions are distinct permafrost
landforms with significant environmental impacts. Mapping these RTS is crucial
because their appearance serves as a clear indication of permafrost thaw.
However, their small scale compared to other landform features, vague
boundaries, and spatiotemporal variation pose significant challenges for
accurate detection. In this paper, we employed a state-of-the-art deep learning
model, the Cascade Mask R-CNN with a multi-scale vision transformer-based
backbone, to delineate RTS features across the Arctic. Two new strategies were
introduced to optimize multimodal learning and enhance the model's predictive
performance: (1) a feature-level, residual cross-modality attention fusion
strategy, which effectively integrates feature maps from multiple modalities to
capture complementary information and improve the model's ability to understand
complex patterns and relationships within the data; (2) pre-trained unimodal
learning followed by multimodal fine-tuning to alleviate high computing demand
while achieving strong model performance. Experimental results demonstrated
that our approach outperformed existing models adopting data-level fusion,
feature-level convolutional fusion, and various attention fusion strategies,
providing valuable insights into the efficient utilization of multimodal data
for RTS mapping. This research contributes to our understanding of permafrost
landforms and their environmental implications.

</details>


### [69] [Dual Prompting Image Restoration with Diffusion Transformers](https://arxiv.org/abs/2504.17825)
*Dehong Kong, Fan Li, Zhixin Wang, Jiaqi Xu, Renjing Pei, Wenbo Li, WenQi Ren*

Main category: cs.CV

TL;DR: DPIR introduces a dual-prompting method for image restoration, combining low-quality image conditioning and visual-textual prompts to enhance DiT-based restoration quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods like latent diffusion models and U-Nets struggle with high-quality restoration. DiTs (e.g., SD3) show promise but need better conditional information extraction.

Method: DPIR uses two branches: (1) a low-quality image conditioning branch for efficient prior incorporation, and (2) a dual prompting control branch for global-local visual cues alongside textual prompts.

Result: DPIR outperforms existing methods, achieving superior image restoration quality.

Conclusion: DPIR effectively leverages dual prompts to enhance DiT-based restoration, addressing limitations of current approaches.

Abstract: Recent state-of-the-art image restoration methods mostly adopt latent
diffusion models with U-Net backbones, yet still facing challenges in achieving
high-quality restoration due to their limited capabilities. Diffusion
transformers (DiTs), like SD3, are emerging as a promising alternative because
of their better quality with scalability. In this paper, we introduce DPIR
(Dual Prompting Image Restoration), a novel image restoration method that
effectivly extracts conditional information of low-quality images from multiple
perspectives. Specifically, DPIR consits of two branches: a low-quality image
conditioning branch and a dual prompting control branch. The first branch
utilizes a lightweight module to incorporate image priors into the DiT with
high efficiency. More importantly, we believe that in image restoration,
textual description alone cannot fully capture its rich visual characteristics.
Therefore, a dual prompting module is designed to provide DiT with additional
visual cues, capturing both global context and local appearance. The extracted
global-local visual prompts as extra conditional control, alongside textual
prompts to form dual prompts, greatly enhance the quality of the restoration.
Extensive experimental results demonstrate that DPIR delivers superior image
restoration performance.

</details>


### [70] [FashionM3: Multimodal, Multitask, and Multiround Fashion Assistant based on Unified Vision-Language Model](https://arxiv.org/abs/2504.17826)
*Kaicheng Pang, Xingxing Zou, Waikeung Wong*

Main category: cs.CV

TL;DR: FashionM3 is a multimodal, multitask, and multiround fashion assistant leveraging vision-language models (VLM) for personalized fashion recommendations, alternative suggestions, image generation, and virtual try-on.


<details>
  <summary>Details</summary>
Motivation: Enhance retailing in the fashion industry by leveraging natural language and visual interactions through VLMs.

Method: FashionM3 is fine-tuned on the FashionRec dataset (331,124 multimodal dialogue samples) for tasks like personalized and alternative recommendations, image generation, and virtual try-on.

Result: Quantitative, qualitative evaluations, and user studies show FashionM3's superior performance in recommendation effectiveness and practical value.

Conclusion: FashionM3 is an effective fashion assistant, offering contextually personalized suggestions through iterative refinement.

Abstract: Fashion styling and personalized recommendations are pivotal in modern
retail, contributing substantial economic value in the fashion industry. With
the advent of vision-language models (VLM), new opportunities have emerged to
enhance retailing through natural language and visual interactions. This work
proposes FashionM3, a multimodal, multitask, and multiround fashion assistant,
built upon a VLM fine-tuned for fashion-specific tasks. It helps users discover
satisfying outfits by offering multiple capabilities including personalized
recommendation, alternative suggestion, product image generation, and virtual
try-on simulation. Fine-tuned on the novel FashionRec dataset, comprising
331,124 multimodal dialogue samples across basic, personalized, and alternative
recommendation tasks, FashionM3 delivers contextually personalized suggestions
with iterative refinement through multiround interactions. Quantitative and
qualitative evaluations, alongside user studies, demonstrate FashionM3's
superior performance in recommendation effectiveness and practical value as a
fashion assistant.

</details>


### [71] [VEU-Bench: Towards Comprehensive Understanding of Video Editing](https://arxiv.org/abs/2504.17828)
*Bozheng Li, Yongliang Wu, Yi Lu, Jiashuo Yu, Licheng Tang, Jiawang Cao, Wenqing Zhu, Yuyang Sun, Jay Wu, Wenbo Zhu*

Main category: cs.CV

TL;DR: The paper introduces VEU-Bench, a benchmark for Video Editing Understanding (VEU), and evaluates 11 Vid-LLMs, finding them lacking. It proposes Oscars, a fine-tuned expert model, which outperforms others and shows VEU data improves general video understanding.


<details>
  <summary>Details</summary>
Motivation: To explore the unexplored capabilities of Video Large Language Models (Vid-LLMs) in video editing understanding tasks and address their current limitations.

Method: Developed VEU-Bench, a comprehensive benchmark with 19 tasks across recognition, reasoning, and judging stages, and built an annotation pipeline with an ontology-based knowledge base. Evaluated 11 Vid-LLMs and fine-tuned Oscars on VEU-Bench.

Result: Current Vid-LLMs struggle with VEU tasks, some performing worse than random. Oscars outperforms open-source Vid-LLMs by 28.3% and matches GPT-4o. VEU data improves general video understanding by 8.3%.

Conclusion: VEU-Bench highlights Vid-LLMs' limitations in editing tasks. Oscars demonstrates the potential of fine-tuning, and VEU data enhances general video understanding, suggesting broader applications.

Abstract: Widely shared videos on the internet are often edited. Recently, although
Video Large Language Models (Vid-LLMs) have made great progress in general
video understanding tasks, their capabilities in video editing understanding
(VEU) tasks remain unexplored. To address this gap, in this paper, we introduce
VEU-Bench (Video Editing Understanding Benchmark), a comprehensive benchmark
that categorizes video editing components across various dimensions, from
intra-frame features like shot size to inter-shot attributes such as cut types
and transitions. Unlike previous video editing understanding benchmarks that
focus mainly on editing element classification, VEU-Bench encompasses 19
fine-grained tasks across three stages: recognition, reasoning, and judging. To
enhance the annotation of VEU automatically, we built an annotation pipeline
integrated with an ontology-based knowledge base. Through extensive experiments
with 11 state-of-the-art Vid-LLMs, our findings reveal that current Vid-LLMs
face significant challenges in VEU tasks, with some performing worse than
random choice. To alleviate this issue, we develop Oscars, a VEU expert model
fine-tuned on the curated VEU-Bench dataset. It outperforms existing
open-source Vid-LLMs on VEU-Bench by over 28.3% in accuracy and achieves
performance comparable to commercial models like GPT-4o. We also demonstrate
that incorporating VEU data significantly enhances the performance of Vid-LLMs
on general video understanding benchmarks, with an average improvement of 8.3%
across nine reasoning tasks.

</details>


### [72] [Fine-Tuning Adversarially-Robust Transformers for Single-Image Dehazing](https://arxiv.org/abs/2504.17829)
*Vlad Vasilescu, Ana Neacsu, Daniela Faur*

Main category: cs.CV

TL;DR: The paper analyzes the vulnerability of single-image dehazing transformers to adversarial noise and proposes two lightweight fine-tuning strategies to enhance robustness without compromising clean performance.


<details>
  <summary>Details</summary>
Motivation: Single-image dehazing is crucial for remote sensing, but its reliability against adversarial perturbations is understudied, risking performance degradation.

Method: The study evaluates state-of-the-art dehazing transformers' susceptibility to adversarial noise and introduces two fine-tuning strategies to improve robustness.

Result: Even a 1-pixel change can reduce PSNR by 2.8 dB. The proposed methods maintain clean performance while significantly boosting adversarial robustness.

Conclusion: The fine-tuning strategies enhance robustness for out-of-distribution data in remote sensing, with code available for adversarial fine-tuning and attacks.

Abstract: Single-image dehazing is an important topic in remote sensing applications,
enhancing the quality of acquired images and increasing object detection
precision. However, the reliability of such structures has not been
sufficiently analyzed, which poses them to the risk of imperceptible
perturbations that can significantly hinder their performance. In this work, we
show that state-of-the-art image-to-image dehazing transformers are susceptible
to adversarial noise, with even 1 pixel change being able to decrease the PSNR
by as much as 2.8 dB. Next, we propose two lightweight fine-tuning strategies
aimed at increasing the robustness of pre-trained transformers. Our methods
results in comparable clean performance, while significantly increasing the
protection against adversarial data. We further present their applicability in
two remote sensing scenarios, showcasing their robust behavior for
out-of-distribution data. The source code for adversarial fine-tuning and
attack algorithms can be found at github.com/Vladimirescu/RobustDehazing.

</details>


### [73] [Masked strategies for images with small objects](https://arxiv.org/abs/2504.17935)
*H. Martin Gillis, Ming Hill, Paul Hollensen, Alan Fine, Thomas Trappenberg*

Main category: cs.CV

TL;DR: The paper explores using masked autoencoders (MAE) with Vision Transformers (ViT) for segmenting small blood components, finding smaller mask ratios and patch sizes improve reconstruction and segmentation performance.


<details>
  <summary>Details</summary>
Motivation: Challenges in detecting and classifying small blood components in images due to limitations of supervised deep learning models when applied outside their training domain.

Method: Used MAE with ViT to learn representations, then applied encoder weights to a U-Net Transformer for semantic segmentation. Varied mask ratios and patch sizes.

Result: Smaller mask ratios and patch sizes improved image reconstruction. Pre-trained weights enhanced segmentation of small blood components.

Conclusion: The proposed method is effective for segmenting and classifying small objects in hematology images.

Abstract: The hematology analytics used for detection and classification of small blood
components is a significant challenge. In particular, when objects exists as
small pixel-sized entities in a large context of similar objects. Deep learning
approaches using supervised models with pre-trained weights, such as residual
networks and vision transformers have demonstrated success for many
applications. Unfortunately, when applied to images outside the domain of
learned representations, these methods often result with less than acceptable
performance. A strategy to overcome this can be achieved by using
self-supervised models, where representations are learned and weights are then
applied for downstream applications. Recently, masked autoencoders have proven
to be effective to obtain representations that captures global context
information. By masking regions of an image and having the model learn to
reconstruct both the masked and non-masked regions, weights can be used for
various applications. However, if the sizes of the objects in images are less
than the size of the mask, the global context information is lost, making it
almost impossible to reconstruct the image. In this study, we investigated the
effect of mask ratios and patch sizes for blood components using a MAE to
obtain learned ViT encoder representations. We then applied the encoder weights
to train a U-Net Transformer for semantic segmentation to obtain both local and
global contextual information. Our experimental results demonstrates that both
smaller mask ratios and patch sizes improve the reconstruction of images using
a MAE. We also show the results of semantic segmentation with and without
pre-trained weights, where smaller-sized blood components benefited with
pre-training. Overall, our proposed method offers an efficient and effective
strategy for the segmentation and classification of small objects.

</details>


### [74] [Token Sequence Compression for Efficient Multimodal Computing](https://arxiv.org/abs/2504.17892)
*Yasmine Omri, Parth Shroff, Thierry Tambe*

Main category: cs.CV

TL;DR: The paper proposes an adaptive compression method for visual language models to address redundancy and inefficiency in current vision encoders, showing that cluster-level token aggregation outperforms prior methods.


<details>
  <summary>Details</summary>
Motivation: The exponential growth of Large Multimodal Models (LMMs) has led to high computational costs, highlighting inefficiencies in vision encoders. The work aims to improve encoding and processing of multimodal data.

Method: The study benchmarks and analyzes various visual token selection and merging approaches, focusing on cluster-level token aggregation.

Result: Cluster-level token aggregation outperforms state-of-the-art methods in token selection and merging, revealing redundancy in vision encoders.

Conclusion: This work advances efficient encoding of high-dimensional data, contributing to scalable and sustainable multimodal systems.

Abstract: The exponential growth of Large Multimodal Models (LMMs) has driven
advancements in cross-modal reasoning but at significant computational costs.
In this work, we focus on visual language models. We highlight the redundancy
and inefficiency in current vision encoders, and seek to construct an adaptive
compression method for multimodal data. In this work, we characterize a panoply
of visual token selection and merging approaches through both benchmarking and
qualitative analysis. In particular, we demonstrate that simple cluster-level
token aggregation outperforms prior state-of-the-art works in token selection
and merging, including merging at the vision encoder level and attention-based
approaches. We underline the redundancy in current vision encoders, and shed
light on several puzzling trends regarding principles of visual token selection
through cross-modal attention visualizations. This work is a first effort
towards more effective encoding and processing of high-dimensional data, and
paves the way for more scalable and sustainable multimodal systems.

</details>


### [75] [DCT-Shield: A Robust Frequency Domain Defense against Malicious Image Editing](https://arxiv.org/abs/2504.17894)
*Aniruddha Bala, Rohit Chowdhury, Rohan Jaiswal, Siddharth Roheda*

Main category: cs.CV

TL;DR: A novel method introduces adversarial perturbations in the frequency domain (DCT coefficients) to protect images from malicious edits by diffusion models, offering robustness and fewer visual artifacts.


<details>
  <summary>Details</summary>
Motivation: Concerns about image security due to easy exploitation of diffusion models for malicious edits, with existing defenses being visually noticeable and non-robust to purification techniques.

Method: Optimization approach modifying DCT coefficients in the frequency domain, leveraging the JPEG pipeline to generate adversarial images.

Result: Effective protection against malicious edits with fewer visual artifacts and robustness to noise purification techniques like JPEG compression.

Conclusion: The proposed method outperforms previous defenses by balancing edit protection and visual quality, demonstrating practical applicability.

Abstract: Advancements in diffusion models have enabled effortless image editing via
text prompts, raising concerns about image security. Attackers with access to
user images can exploit these tools for malicious edits. Recent defenses
attempt to protect images by adding a limited noise in the pixel space to
disrupt the functioning of diffusion-based editing models. However, the
adversarial noise added by previous methods is easily noticeable to the human
eye. Moreover, most of these methods are not robust to purification techniques
like JPEG compression under a feasible pixel budget. We propose a novel
optimization approach that introduces adversarial perturbations directly in the
frequency domain by modifying the Discrete Cosine Transform (DCT) coefficients
of the input image. By leveraging the JPEG pipeline, our method generates
adversarial images that effectively prevent malicious image editing. Extensive
experiments across a variety of tasks and datasets demonstrate that our
approach introduces fewer visual artifacts while maintaining similar levels of
edit protection and robustness to noise purification techniques.

</details>


### [76] [Iterative Event-based Motion Segmentation by Variational Contrast Maximization](https://arxiv.org/abs/2504.18447)
*Ryo Yamaki, Shintaro Shiba, Guillermo Gallego, Yoshimitsu Aoki*

Main category: cs.CV

TL;DR: The paper proposes an iterative motion segmentation method for event cameras, classifying events into background and foreground motions, improving accuracy by over 30% on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Event cameras capture motion-rich data, but classifying this data into distinct motions (segmentation) is crucial for tasks like object detection and visual servoing.

Method: Extends the Contrast Maximization framework to iteratively segment events into background (dominant motion) and foreground (residual motions).

Result: Achieves state-of-the-art accuracy, with a 30%+ improvement in moving object detection, and works well in noisy real-world scenes.

Conclusion: The method enhances Contrast Maximization's sensitivity to motion parameters and events, advancing event-based motion segmentation theory.

Abstract: Event cameras provide rich signals that are suitable for motion estimation
since they respond to changes in the scene. As any visual changes in the scene
produce event data, it is paramount to classify the data into different motions
(i.e., motion segmentation), which is useful for various tasks such as object
detection and visual servoing. We propose an iterative motion segmentation
method, by classifying events into background (e.g., dominant motion
hypothesis) and foreground (independent motion residuals), thus extending the
Contrast Maximization framework. Experimental results demonstrate that the
proposed method successfully classifies event clusters both for public and
self-recorded datasets, producing sharp, motion-compensated edge-like images.
The proposed method achieves state-of-the-art accuracy on moving object
detection benchmarks with an improvement of over 30%, and demonstrates its
possibility of applying to more complex and noisy real-world scenes. We hope
this work broadens the sensitivity of Contrast Maximization with respect to
both motion parameters and input events, thus contributing to theoretical
advancements in event-based motion segmentation estimation.
https://github.com/aoki-media-lab/event_based_segmentation_vcmax

</details>


### [77] [CAMU: Context Augmentation for Meme Understanding](https://arxiv.org/abs/2504.17902)
*Girish A. Koushik, Diptesh Kanojia, Helen Treharne, Aditya Joshi*

Main category: cs.CV

TL;DR: CAMU, a novel framework for hateful meme detection, combines vision-language models, caption scoring, and efficient CLIP fine-tuning, outperforming existing methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Social media memes pose challenges for hate detection due to their multimodal and culturally nuanced nature, requiring advanced methods for reliable identification.

Method: CAMU uses large vision-language models for descriptive captions, a caption-scoring neural network, and parameter-efficient fine-tuning of CLIP's text encoder to enhance multimodal understanding.

Result: CAMU achieves high accuracy (0.807) and F1-score (0.806) on the Hateful Memes dataset and the best F1-score (0.673) on MultiOFF, demonstrating generalizability and efficiency.

Conclusion: Robust visual grounding and nuanced text representations are key for reliable hate detection, and CAMU offers practical advantages for real-world applications.

Abstract: Social media memes are a challenging domain for hate detection because they
intertwine visual and textual cues into culturally nuanced messages. We
introduce a novel framework, CAMU, which leverages large vision-language models
to generate more descriptive captions, a caption-scoring neural network to
emphasise hate-relevant content, and parameter-efficient fine-tuning of CLIP's
text encoder for an improved multimodal understanding of memes. Experiments on
publicly available hateful meme datasets show that simple projection layer
fine-tuning yields modest gains, whereas selectively tuning deeper text encoder
layers significantly boosts performance on all evaluation metrics. Moreover,
our approach attains high accuracy (0.807) and F1-score (0.806) on the Hateful
Memes dataset, at par with the existing SoTA framework while being much more
efficient, offering practical advantages in real-world scenarios that rely on
fixed decision thresholds. CAMU also achieves the best F1-score of 0.673 on the
MultiOFF dataset for offensive meme identification, demonstrating its
generalisability. Additional analyses on benign confounders reveal that robust
visual grounding and nuanced text representations are crucial for reliable hate
and offence detection. We will publicly release CAMU along with the resultant
models for further research.
  Disclaimer: This paper includes references to potentially disturbing,
hateful, or offensive content due to the nature of the task.

</details>


### [78] [From Mapping to Composing: A Two-Stage Framework for Zero-shot Composed Image Retrieval](https://arxiv.org/abs/2504.17990)
*Yabing Wang, Zhuotao Tian, Qingpei Guo, Zheng Qin, Sanping Zhou, Ming Yang, Le Wang*

Main category: cs.CV

TL;DR: A two-stage framework improves zero-shot Composed Image Retrieval (CIR) by enhancing pseudo-word token representation and optimizing text encoding with minimal synthetic data.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations in projection-based zero-shot CIR methods, such as insufficient token representation and reliance on large-scale synthetic data.

Method: A two-stage approach: (1) enhancing image-to-pseudo-word token learning with visual semantic injection and soft text alignment, (2) optimizing the text encoder for compositional semantics using small synthetic datasets.

Result: Superior performance on three public datasets, outperforming existing methods with minimal synthetic data.

Conclusion: The proposed framework effectively addresses key challenges in zero-shot CIR, offering compatibility with varying data quality and significant performance gains.

Abstract: Composed Image Retrieval (CIR) is a challenging multimodal task that
retrieves a target image based on a reference image and accompanying
modification text. Due to the high cost of annotating CIR triplet datasets,
zero-shot (ZS) CIR has gained traction as a promising alternative. Existing
studies mainly focus on projection-based methods, which map an image to a
single pseudo-word token. However, these methods face three critical
challenges: (1) insufficient pseudo-word token representation capacity, (2)
discrepancies between training and inference phases, and (3) reliance on
large-scale synthetic data. To address these issues, we propose a two-stage
framework where the training is accomplished from mapping to composing. In the
first stage, we enhance image-to-pseudo-word token learning by introducing a
visual semantic injection module and a soft text alignment objective, enabling
the token to capture richer and fine-grained image information. In the second
stage, we optimize the text encoder using a small amount of synthetic triplet
data, enabling it to effectively extract compositional semantics by combining
pseudo-word tokens with modification text for accurate target image retrieval.
The strong visual-to-pseudo mapping established in the first stage provides a
solid foundation for the second stage, making our approach compatible with both
high- and low-quality synthetic data, and capable of achieving significant
performance gains with only a small amount of synthetic data. Extensive
experiments were conducted on three public datasets, achieving superior
performance compared to existing approaches.

</details>


### [79] [RSRNav: Reasoning Spatial Relationship for Image-Goal Navigation](https://arxiv.org/abs/2504.17991)
*Zheng Qin, Le Wang, Yabing Wang, Sanping Zhou, Gang Hua, Wei Tang*

Main category: cs.CV

TL;DR: RSRNav improves image-goal navigation by modeling spatial relationships between goal and current observations, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing ImageNav methods struggle with inaccurate directional information and viewpoint inconsistencies, limiting performance.

Method: RSRNav constructs and refines spatial correlations between goal and current observations, using fine-grained and direction-aware correlations for precise navigation.

Result: RSRNav achieves superior performance on three benchmark datasets, especially in the 'user-matched goal' setting.

Conclusion: RSRNav's spatial reasoning approach enhances navigation accuracy, showing promise for real-world applications.

Abstract: Recent image-goal navigation (ImageNav) methods learn a perception-action
policy by separately capturing semantic features of the goal and egocentric
images, then passing them to a policy network. However, challenges remain: (1)
Semantic features often fail to provide accurate directional information,
leading to superfluous actions, and (2) performance drops significantly when
viewpoint inconsistencies arise between training and application. To address
these challenges, we propose RSRNav, a simple yet effective method that reasons
spatial relationships between the goal and current observations as navigation
guidance. Specifically, we model the spatial relationship by constructing
correlations between the goal and current observations, which are then passed
to the policy network for action prediction. These correlations are
progressively refined using fine-grained cross-correlation and direction-aware
correlation for more precise navigation. Extensive evaluation of RSRNav on
three benchmark datasets demonstrates superior navigation performance,
particularly in the "user-matched goal" setting, highlighting its potential for
real-world applications.

</details>


### [80] [Back to Fundamentals: Low-Level Visual Features Guided Progressive Token Pruning](https://arxiv.org/abs/2504.17996)
*Yuanbing Ouyang, Yizhuo Liang, Qingpeng Li, Xinfei Guo, Yiming Luo, Di Wu, Hao Wang, Yushan Pan*

Main category: cs.CV

TL;DR: LVTP is a token pruning framework for Vision Transformers that reduces computation by 20%-45% with minimal performance loss, using multi-scale Tsallis entropy and low-level visual features.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers (ViTs) are computationally heavy for resource-constrained devices, and existing pruning methods ignore visual data characteristics.

Method: LVTP employs progressive token pruning guided by multi-scale Tsallis entropy and low-level visual features, with a dynamic scoring mechanism and twice clustering.

Result: Achieves 20%-45% computational reduction with negligible performance loss, outperforming existing methods in cost-accuracy balance.

Conclusion: LVTP effectively balances computational efficiency and segmentation accuracy, especially in complex edge regions, without requiring architectural changes.

Abstract: Vision Transformers (ViTs) excel in semantic segmentation but demand
significant computation, posing challenges for deployment on
resource-constrained devices. Existing token pruning methods often overlook
fundamental visual data characteristics. This study introduces 'LVTP', a
progressive token pruning framework guided by multi-scale Tsallis entropy and
low-level visual features with twice clustering. It integrates high-level
semantics and basic visual attributes for precise segmentation. A novel dynamic
scoring mechanism using multi-scale Tsallis entropy weighting overcomes
limitations of traditional single-parameter entropy. The framework also
incorporates low-level feature analysis to preserve critical edge information
while optimizing computational cost. As a plug-and-play module, it requires no
architectural changes or additional training. Evaluations across multiple
datasets show 20%-45% computational reductions with negligible performance
loss, outperforming existing methods in balancing cost and accuracy, especially
in complex edge regions.

</details>


### [81] [A Temporal Convolutional Network-Based Approach and a Benchmark Dataset for Colonoscopy Video Temporal Segmentation](https://arxiv.org/abs/2502.03430)
*Carlo Biffi, Giorgio Roffo, Pietro Salvagnini, Andrea Cherubini*

Main category: cs.CV

TL;DR: The paper introduces ColonTCN, a state-of-the-art model for segmenting colonoscopy videos into anatomical sections and procedural phases, using a new open-access dataset (REAL-Colon) and outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Automated reporting of colonoscopy procedures is crucial for clinical practice, but lacks robust computer vision models for temporal segmentation of videos.

Method: The authors annotated the REAL-Colon dataset (2.7M frames from 60 videos) and proposed ColonTCN, a learning-based architecture with custom temporal convolutional blocks for efficient long-term dependency capture.

Result: ColonTCN achieves top performance in classification accuracy with low parameters, validated via dual k-fold cross-validation on multi-center data.

Conclusion: The benchmark and ColonTCN advance temporal segmentation in colonoscopy, encouraging open-access research for clinical applications.

Abstract: Following recent advancements in computer-aided detection and diagnosis
systems for colonoscopy, the automated reporting of colonoscopy procedures is
set to further revolutionize clinical practice. A crucial yet underexplored
aspect in the development of these systems is the creation of computer vision
models capable of autonomously segmenting full-procedure colonoscopy videos
into anatomical sections and procedural phases. In this work, we aim to create
the first open-access dataset for this task and propose a state-of-the-art
approach, benchmarked against competitive models. We annotated the publicly
available REAL-Colon dataset, consisting of 2.7 million frames from 60 complete
colonoscopy videos, with frame-level labels for anatomical locations and
colonoscopy phases across nine categories. We then present ColonTCN, a
learning-based architecture that employs custom temporal convolutional blocks
designed to efficiently capture long temporal dependencies for the temporal
segmentation of colonoscopy videos. We also propose a dual k-fold
cross-validation evaluation protocol for this benchmark, which includes model
assessment on unseen, multi-center data.ColonTCN achieves state-of-the-art
performance in classification accuracy while maintaining a low parameter count
when evaluated using the two proposed k-fold cross-validation settings,
outperforming competitive models. We report ablation studies to provide
insights into the challenges of this task and highlight the benefits of the
custom temporal convolutional blocks, which enhance learning and improve model
efficiency. We believe that the proposed open-access benchmark and the ColonTCN
approach represent a significant advancement in the temporal segmentation of
colonoscopy procedures, fostering further open-access research to address this
clinical need.

</details>


### [82] [Federated Client-tailored Adapter for Medical Image Segmentation](https://arxiv.org/abs/2504.18020)
*Guyue Hu, Siyuan Song, Yukun Kang, Zhu Yin, Gangming Zhao, Chenglong Li, Jin Tang*

Main category: cs.CV

TL;DR: Proposes a Federated Client-tailored Adapter (FCA) framework for stable and adaptive medical image segmentation in distributed data scenarios, addressing domain heterogeneity and class imbalance.


<details>
  <summary>Details</summary>
Motivation: Existing centralized learning methods are impractical for distributed medical data. Federated Learning faces instability due to domain heterogeneity.

Method: Introduces FCA, leveraging medical foundation models and client-tailored updating strategies to stabilize training and adapt to client-specific needs.

Result: Demonstrates effectiveness on three large-scale datasets, achieving stable and client-tailored segmentation without data sharing.

Conclusion: FCA offers a robust solution for federated medical image segmentation, outperforming global-compromised models.

Abstract: Medical image segmentation in X-ray images is beneficial for computer-aided
diagnosis and lesion localization. Existing methods mainly fall into a
centralized learning paradigm, which is inapplicable in the practical medical
scenario that only has access to distributed data islands. Federated Learning
has the potential to offer a distributed solution but struggles with heavy
training instability due to client-wise domain heterogeneity (including
distribution diversity and class imbalance). In this paper, we propose a novel
Federated Client-tailored Adapter (FCA) framework for medical image
segmentation, which achieves stable and client-tailored adaptive segmentation
without sharing sensitive local data. Specifically, the federated adapter stirs
universal knowledge in off-the-shelf medical foundation models to stabilize the
federated training process. In addition, we develop two client-tailored
federated updating strategies that adaptively decompose the adapter into common
and individual components, then globally and independently update the parameter
groups associated with common client-invariant and individual client-specific
units, respectively. They further stabilize the heterogeneous federated
learning process and realize optimal client-tailored instead of sub-optimal
global-compromised segmentation models. Extensive experiments on three
large-scale datasets demonstrate the effectiveness and superiority of the
proposed FCA framework for federated medical segmentation.

</details>


### [83] [ShapeSpeak: Body Shape-Aware Textual Alignment for Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2504.18025)
*Shuanglin Yan, Neng Dong, Shuang Li, Rui Yan, Hao Tang, Jing Qin*

Main category: cs.CV

TL;DR: The paper proposes BSaTa, a framework for VIReID that enhances cross-modal matching by explicitly modeling body shape features using textual alignment and consistency regularizers.


<details>
  <summary>Details</summary>
Motivation: Existing VIReID methods lack explicit modeling of body shape features, which are crucial for cross-modal matching, and rely only on identity labels, limiting semantic extraction.

Method: The BSaTa framework includes a Body Shape Textual Alignment module, a Text-Visual Consistency Regularizer, and a Shape-aware Representation Learning mechanism to align and learn body shape features.

Result: The method outperforms others on SYSU-MM01 and RegDB datasets, proving its effectiveness in VIReID.

Conclusion: BSaTa successfully improves VIReID by leveraging body shape features and textual alignment, demonstrating superior performance.

Abstract: Visible-Infrared Person Re-identification (VIReID) aims to match visible and
infrared pedestrian images, but the modality differences and the complexity of
identity features make it challenging. Existing methods rely solely on identity
label supervision, which makes it difficult to fully extract high-level
semantic information. Recently, vision-language pre-trained models have been
introduced to VIReID, enhancing semantic information modeling by generating
textual descriptions. However, such methods do not explicitly model body shape
features, which are crucial for cross-modal matching. To address this, we
propose an effective Body Shape-aware Textual Alignment (BSaTa) framework that
explicitly models and utilizes body shape information to improve VIReID
performance. Specifically, we design a Body Shape Textual Alignment (BSTA)
module that extracts body shape information using a human parsing model and
converts it into structured text representations via CLIP. We also design a
Text-Visual Consistency Regularizer (TVCR) to ensure alignment between body
shape textual representations and visual body shape features. Furthermore, we
introduce a Shape-aware Representation Learning (SRL) mechanism that combines
Multi-text Supervision and Distribution Consistency Constraints to guide the
visual encoder to learn modality-invariant and discriminative identity
features, thus enhancing modality invariance. Experimental results demonstrate
that our method achieves superior performance on the SYSU-MM01 and RegDB
datasets, validating its effectiveness.

</details>


### [84] [A Large Vision-Language Model based Environment Perception System for Visually Impaired People](https://arxiv.org/abs/2504.18027)
*Zezhou Chen, Zhaoxiang Liu, Kai Wang, Kohou Wang, Shiguo Lian*

Main category: cs.CV

TL;DR: A Large Vision-Language Model (LVLM) system aids visually impaired people by providing scene descriptions, object categories, and detailed object info via wearable devices, reducing hallucinations with segmentation data.


<details>
  <summary>Details</summary>
Motivation: Visually impaired individuals struggle with environmental perception, limiting their activities. The paper aims to enhance their understanding of surroundings using LVLM and segmentation.

Method: Uses a wearable device to capture scenes, integrates segmentation results into LVLM inputs, and offers interactive retrieval (tapping, swiping) for descriptions and object details.

Result: Outperforms Qwen-VL-Chat in accuracy on POPE, MME, and LLaVA-QA90 benchmarks and effectively aids users in environmental perception.

Conclusion: The system improves scene understanding for visually impaired people by combining LVLM with segmentation, reducing hallucinations and enhancing usability.

Abstract: It is a challenging task for visually impaired people to perceive their
surrounding environment due to the complexity of the natural scenes. Their
personal and social activities are thus highly limited. This paper introduces a
Large Vision-Language Model(LVLM) based environment perception system which
helps them to better understand the surrounding environment, by capturing the
current scene they face with a wearable device, and then letting them retrieve
the analysis results through the device. The visually impaired people could
acquire a global description of the scene by long pressing the screen to
activate the LVLM output, retrieve the categories of the objects in the scene
resulting from a segmentation model by tapping or swiping the screen, and get a
detailed description of the objects they are interested in by double-tapping
the screen. To help visually impaired people more accurately perceive the
world, this paper proposes incorporating the segmentation result of the RGB
image as external knowledge into the input of LVLM to reduce the LVLM's
hallucination. Technical experiments on POPE, MME and LLaVA-QA90 show that the
system could provide a more accurate description of the scene compared to
Qwen-VL-Chat, exploratory experiments show that the system helps visually
impaired people to perceive the surrounding environment effectively.

</details>


### [85] [Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models](https://arxiv.org/abs/2504.18032)
*Chen Chen, Daochang Liu, Mubarak Shah, Chang Xu*

Main category: cs.CV

TL;DR: PRSS method improves privacy-utility trade-off in text-to-image diffusion models by refining classifier-free guidance with prompt re-anchoring and semantic prompt search.


<details>
  <summary>Details</summary>
Motivation: Address concerns about memorization of training images in diffusion models, which raises originality and privacy issues, while maintaining output utility.

Method: Introduces PRSS, combining prompt re-anchoring (PR) for privacy and semantic prompt search (SS) for utility in classifier-free guidance.

Result: PRSS consistently enhances the privacy-utility trade-off across various privacy levels, achieving state-of-the-art performance.

Conclusion: PRSS effectively balances privacy and utility in text-to-image diffusion models, setting a new benchmark.

Abstract: Text-to-image diffusion models have demonstrated remarkable capabilities in
creating images highly aligned with user prompts, yet their proclivity for
memorizing training set images has sparked concerns about the originality of
the generated images and privacy issues, potentially leading to legal
complications for both model owners and users, particularly when the memorized
images contain proprietary content. Although methods to mitigate these issues
have been suggested, enhancing privacy often results in a significant decrease
in the utility of the outputs, as indicated by text-alignment scores. To bridge
the research gap, we introduce a novel method, PRSS, which refines the
classifier-free guidance approach in diffusion models by integrating prompt
re-anchoring (PR) to improve privacy and incorporating semantic prompt search
(SS) to enhance utility. Extensive experiments across various privacy levels
demonstrate that our approach consistently improves the privacy-utility
trade-off, establishing a new state-of-the-art.

</details>


### [86] [Cabbage: A Differential Growth Framework for Open Surfaces](https://arxiv.org/abs/2504.18040)
*Xiaoyi Liu, Hao Tang*

Main category: cs.CV

TL;DR: Cabbage is a differential growth framework for modeling buckling in 3D surfaces, producing high-quality meshes without self-intersection. It outperforms state-of-the-art methods in expressiveness and stability.


<details>
  <summary>Details</summary>
Motivation: To model natural buckling behaviors (e.g., flower petals) with high-quality meshes and avoid self-intersection.

Method: Uses edge subdivision for differential growth, shell forces for expansion, feature-aware smoothing, and corrective collision handling.

Result: Generates stable, complex patterns over hundreds of steps, with superior mesh quality and expressiveness.

Conclusion: Cabbage is a robust, open-source tool for computational modeling, fabrication, and data generation.

Abstract: We propose Cabbage, a differential growth framework to model buckling
behavior in 3D open surfaces found in nature-like the curling of flower petals.
Cabbage creates high-quality triangular meshes free of self-intersection.
Cabbage-Shell is driven by edge subdivision which differentially increases
discretization resolution. Shell forces expands the surface, generating
buckling over time. Feature-aware smoothing and remeshing ensures mesh quality.
Corrective collision effectively prevents self-collision even in tight spaces.
We additionally provide Cabbage-Collision, and approximate alternative,
followed by CAD-ready surface generation. Cabbage is the first open-source
effort with this calibre and robustness, outperforming SOTA methods in its
morphological expressiveness, mesh quality, and stably generates large, complex
patterns over hundreds of simulation steps. It is a source not only of
computational modeling, digital fabrication, education, but also high-quality,
annotated data for geometry processing and shape analysis.

</details>


### [87] [DMS-Net:Dual-Modal Multi-Scale Siamese Network for Binocular Fundus Image Classification](https://arxiv.org/abs/2504.18046)
*Guohao Huo, Zibo Lin, Zitong Wang, Ruiting Dai, Hao Tang*

Main category: cs.CV

TL;DR: DMS-Net, a dual-modal multi-scale Siamese network, improves binocular fundus image classification by leveraging weight-shared Siamese ResNet-152, MSCAM for multi-resolution features, and DMFF for cross-modal fusion, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Traditional diagnosis and single-eye deep learning methods overlook binocular pathological correlations, necessitating a more effective approach.

Method: Uses weight-shared Siamese ResNet-152 backbones, MSCAM for multi-resolution feature aggregation, and DMFF for cross-modal feature fusion.

Result: Achieves 80.5% accuracy, 86.1% recall, and 83.8% Cohen's kappa on ODIR-5K dataset.

Conclusion: DMS-Net excels in detecting symmetric pathologies and enhances clinical decision-making for ocular diseases.

Abstract: Ophthalmic diseases pose a significant global health challenge, yet
traditional diagnosis methods and existing single-eye deep learning approaches
often fail to account for binocular pathological correlations. To address this,
we propose DMS-Net, a dual-modal multi-scale Siamese network for binocular
fundus image classification. Our framework leverages weight-shared Siamese
ResNet-152 backbones to extract deep semantic features from paired fundus
images. To tackle challenges such as lesion boundary ambiguity and scattered
pathological distributions, we introduce a Multi-Scale Context-Aware Module
(MSCAM) that integrates adaptive pooling and attention mechanisms for
multi-resolution feature aggregation. Additionally, a Dual-Modal Feature Fusion
(DMFF) module enhances cross-modal interaction through spatial-semantic
recalibration and bidirectional attention, effectively combining global context
and local edge features. Evaluated on the ODIR-5K dataset, DMS-Net achieves
state-of-the-art performance with 80.5% accuracy, 86.1% recall, and 83.8%
Cohen's kappa, demonstrating superior capability in detecting symmetric
pathologies and advancing clinical decision-making for ocular diseases.

</details>


### [88] [A BERT-Style Self-Supervised Learning CNN for Disease Identification from Retinal Images](https://arxiv.org/abs/2504.18049)
*Xin Li, Wenhui Zhu, Peijie Qiu, Oana M. Dumitrascu, Amal Youssef, Yalin Wang*

Main category: cs.CV

TL;DR: The paper proposes a lightweight CNN (nn-MobileNet) for self-supervised learning on unlabeled retinal images, improving downstream medical image analysis tasks.


<details>
  <summary>Details</summary>
Motivation: Deep learning in medical imaging faces challenges due to the scarcity of labeled data and the computational demands of Vision Transformers (ViTs).

Method: The study uses nn-MobileNet for BERT-style self-supervised pre-training on unlabeled retinal fundus images from the UK Biobank.

Result: The pre-trained model enhances performance in identifying Alzheimer's, Parkinson's, and retinal diseases.

Conclusion: The approach combines CNNs' efficiency with self-supervised learning, proving effective for label-scarce scenarios.

Abstract: In the field of medical imaging, the advent of deep learning, especially the
application of convolutional neural networks (CNNs) has revolutionized the
analysis and interpretation of medical images. Nevertheless, deep learning
methods usually rely on large amounts of labeled data. In medical imaging
research, the acquisition of high-quality labels is both expensive and
difficult. The introduction of Vision Transformers (ViT) and self-supervised
learning provides a pre-training strategy that utilizes abundant unlabeled
data, effectively alleviating the label acquisition challenge while broadening
the breadth of data utilization. However, ViT's high computational density and
substantial demand for computing power, coupled with the lack of localization
characteristics of its operations on image patches, limit its efficiency and
applicability in many application scenarios. In this study, we employ
nn-MobileNet, a lightweight CNN framework, to implement a BERT-style
self-supervised learning approach. We pre-train the network on the unlabeled
retinal fundus images from the UK Biobank to improve downstream application
performance. We validate the results of the pre-trained model on Alzheimer's
disease (AD), Parkinson's disease (PD), and various retinal diseases
identification. The results show that our approach can significantly improve
performance in the downstream tasks. In summary, this study combines the
benefits of CNNs with the capabilities of advanced self-supervised learning in
handling large-scale unlabeled data, demonstrating the potential of CNNs in the
presence of label scarcity.

</details>


### [89] [POET: Prompt Offset Tuning for Continual Human Action Adaptation](https://arxiv.org/abs/2504.18059)
*Prachi Garg, Joseph K J, Vineeth N Balasubramanian, Necati Cihan Camgoz, Chengde Wan, Kenrick Kin, Weiguang Si, Shugao Ma, Fernando De La Torre*

Main category: cs.CV

TL;DR: POET introduces a privacy-aware, few-shot continual action recognition method using lightweight backbones and novel spatio-temporal prompt tuning, outperforming benchmarks on new datasets.


<details>
  <summary>Details</summary>
Motivation: To enable users to personalize XR experiences by adding new action classes efficiently and privately, without storing sensitive data.

Method: Proposes POET: Prompt-Offset Tuning, a lightweight backbone approach with spatio-temporal learnable prompts, applied to Graph Neural Networks.

Result: POET outperforms benchmarks on NTU RGB+D and SHREC-2017 datasets for action and gesture recognition.

Conclusion: POET offers an effective, privacy-preserving solution for continual action recognition in XR, with potential for broader applications.

Abstract: As extended reality (XR) is redefining how users interact with computing
devices, research in human action recognition is gaining prominence. Typically,
models deployed on immersive computing devices are static and limited to their
default set of classes. The goal of our research is to provide users and
developers with the capability to personalize their experience by adding new
action classes to their device models continually. Importantly, a user should
be able to add new classes in a low-shot and efficient manner, while this
process should not require storing or replaying any of user's sensitive
training data. We formalize this problem as privacy-aware few-shot continual
action recognition. Towards this end, we propose POET: Prompt-Offset Tuning.
While existing prompt tuning approaches have shown great promise for continual
learning of image, text, and video modalities; they demand access to
extensively pretrained transformers. Breaking away from this assumption, POET
demonstrates the efficacy of prompt tuning a significantly lightweight
backbone, pretrained exclusively on the base class data. We propose a novel
spatio-temporal learnable prompt offset tuning approach, and are the first to
apply such prompt tuning to Graph Neural Networks. We contribute two new
benchmarks for our new problem setting in human action recognition: (i) NTU
RGB+D dataset for activity recognition, and (ii) SHREC-2017 dataset for hand
gesture recognition. We find that POET consistently outperforms comprehensive
benchmarks. Source code at
https://github.com/humansensinglab/POET-continual-action-recognition.

</details>


### [90] [S3MOT: Monocular 3D Object Tracking with Selective State Space Model](https://arxiv.org/abs/2504.18068)
*Zhuohao Yan, Shaoquan Feng, Xingxing Li, Yuxuan Zhou, Chunxi Xia, Shengyu Li*

Main category: cs.CV

TL;DR: The paper introduces three techniques (HSSM, FCOE, VeloSSM) to improve monocular 3D multi-object tracking, achieving state-of-the-art performance on KITTI.


<details>
  <summary>Details</summary>
Motivation: Accurate 3D MOT is crucial for robotics and computer vision but challenging in monocular setups due to 2D-to-3D spatiotemporal association difficulties.

Method: Proposes HSSM for efficient data association, FCOE for improved re-identification, and VeloSSM for better 6-DoF pose estimation.

Result: Achieves 76.86 HOTA at 31 FPS on KITTI, outperforming previous methods by +2.63 HOTA and +3.62 AssA.

Conclusion: The approach is robust and efficient for monocular 3D MOT, with code and models publicly available.

Abstract: Accurate and reliable multi-object tracking (MOT) in 3D space is essential
for advancing robotics and computer vision applications. However, it remains a
significant challenge in monocular setups due to the difficulty of mining 3D
spatiotemporal associations from 2D video streams. In this work, we present
three innovative techniques to enhance the fusion and exploitation of
heterogeneous cues for monocular 3D MOT: (1) we introduce the Hungarian State
Space Model (HSSM), a novel data association mechanism that compresses
contextual tracking cues across multiple paths, enabling efficient and
comprehensive assignment decisions with linear complexity. HSSM features a
global receptive field and dynamic weights, in contrast to traditional linear
assignment algorithms that rely on hand-crafted association costs. (2) We
propose Fully Convolutional One-stage Embedding (FCOE), which eliminates ROI
pooling by directly using dense feature maps for contrastive learning, thus
improving object re-identification accuracy under challenging conditions such
as varying viewpoints and lighting. (3) We enhance 6-DoF pose estimation
through VeloSSM, an encoder-decoder architecture that models temporal
dependencies in velocity to capture motion dynamics, overcoming the limitations
of frame-based 3D inference. Experiments on the KITTI public test benchmark
demonstrate the effectiveness of our method, achieving a new state-of-the-art
performance of 76.86~HOTA at 31~FPS. Our approach outperforms the previous best
by significant margins of +2.63~HOTA and +3.62~AssA, showcasing its robustness
and efficiency for monocular 3D MOT tasks. The code and models are available at
https://github.com/bytepioneerX/s3mot.

</details>


### [91] [Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional Talking Portrait Generation](https://arxiv.org/abs/2504.18087)
*Weipeng Tan, Chuming Lin, Chengming Xu, FeiFan Xu, Xiaobin Hu, Xiaozhong Ji, Junwei Zhu, Chengjie Wang, Yanwei Fu*

Main category: cs.CV

TL;DR: DICE-Talk improves emotional talking head generation by disentangling identity from emotion, using cross-modal attention and learnable Emotion Banks, outperforming existing methods in emotion accuracy and identity preservation.


<details>
  <summary>Details</summary>
Motivation: Current THG methods lack emotional expressiveness and struggle with identity preservation due to insufficient use of audio cues, identity leakage, and isolated emotion learning.

Method: Proposes DICE-Talk: a disentangled emotion embedder for audio-visual cues, a correlation-enhanced emotion conditioning module with Emotion Banks, and an emotion discrimination objective for affective consistency.

Result: Outperforms state-of-the-art in emotion accuracy on MEAD and HDTF datasets while maintaining lip-sync quality, confirmed by qualitative results and user studies.

Conclusion: DICE-Talk effectively generates identity-preserving, emotionally rich portraits with natural adaptability to unseen identities.

Abstract: Recent advances in Talking Head Generation (THG) have achieved impressive lip
synchronization and visual quality through diffusion models; yet existing
methods struggle to generate emotionally expressive portraits while preserving
speaker identity. We identify three critical limitations in current emotional
talking head generation: insufficient utilization of audio's inherent emotional
cues, identity leakage in emotion representations, and isolated learning of
emotion correlations. To address these challenges, we propose a novel framework
dubbed as DICE-Talk, following the idea of disentangling identity with emotion,
and then cooperating emotions with similar characteristics. First, we develop a
disentangled emotion embedder that jointly models audio-visual emotional cues
through cross-modal attention, representing emotions as identity-agnostic
Gaussian distributions. Second, we introduce a correlation-enhanced emotion
conditioning module with learnable Emotion Banks that explicitly capture
inter-emotion relationships through vector quantization and attention-based
feature aggregation. Third, we design an emotion discrimination objective that
enforces affective consistency during the diffusion process through
latent-space classification. Extensive experiments on MEAD and HDTF datasets
demonstrate our method's superiority, outperforming state-of-the-art approaches
in emotion accuracy while maintaining competitive lip-sync performance.
Qualitative results and user studies further confirm our method's ability to
generate identity-preserving portraits with rich, correlated emotional
expressions that naturally adapt to unseen identities.

</details>


### [92] [Study on Real-Time Road Surface Reconstruction Using Stereo Vision](https://arxiv.org/abs/2504.18112)
*Deepak Ghimire, Byoungjun Kim, Donghoon Kim, SungHwan Jeong*

Main category: cs.CV

TL;DR: The paper enhances RoadBEV for real-time road surface reconstruction on edge devices by optimizing efficiency and accuracy through structured pruning and a redesigned head network.


<details>
  <summary>Details</summary>
Motivation: Road surface reconstruction is vital for autonomous driving, requiring real-time, efficient, and accurate solutions for safe navigation.

Method: Applied Isomorphic Global Structured Pruning to the stereo feature extraction backbone and redesigned the head network with optimizations like dynamic attention heads and mixed precision inference.

Result: Improved inference speed and lower reconstruction error, making it suitable for real-time autonomous driving applications.

Conclusion: The proposed optimizations enhance RoadBEV's performance, balancing efficiency and accuracy for real-time road surface reconstruction.

Abstract: Road surface reconstruction plays a crucial role in autonomous driving,
providing essential information for safe and smooth navigation. This paper
enhances the RoadBEV [1] framework for real-time inference on edge devices by
optimizing both efficiency and accuracy. To achieve this, we proposed to apply
Isomorphic Global Structured Pruning to the stereo feature extraction backbone,
reducing network complexity while maintaining performance. Additionally, the
head network is redesigned with an optimized hourglass structure, dynamic
attention heads, reduced feature channels, mixed precision inference, and
efficient probability volume computation. Our approach improves inference speed
while achieving lower reconstruction error, making it well-suited for real-time
road surface reconstruction in autonomous driving.

</details>


### [93] [Salient Region-Guided Spacecraft Image Arbitrary-Scale Super-Resolution Network](https://arxiv.org/abs/2504.18127)
*Jingfan Yang, Hu Gao, Ying Zhang, Depeng Dang*

Main category: cs.CV

TL;DR: Proposes SGSASR, a method for spacecraft image super-resolution that focuses on core regions to avoid noise from black backgrounds, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing super-resolution methods introduce noise by not distinguishing between spacecraft core regions and black backgrounds.

Method: Uses a salient region-guided approach with a core recognition block (SCRRB) and adaptive-weighted feature fusion (AFFEM) to enhance resolution.

Result: SGSASR outperforms state-of-the-art super-resolution methods.

Conclusion: Focusing on salient regions improves spacecraft image super-resolution by reducing irrelevant noise.

Abstract: Spacecraft image super-resolution seeks to enhance low-resolution spacecraft
images into high-resolution ones. Although existing arbitrary-scale
super-resolution methods perform well on general images, they tend to overlook
the difference in features between the spacecraft core region and the large
black space background, introducing irrelevant noise. In this paper, we propose
a salient region-guided spacecraft image arbitrary-scale super-resolution
network (SGSASR), which uses features from the spacecraft core salient regions
to guide latent modulation and achieve arbitrary-scale super-resolution.
Specifically, we design a spacecraft core region recognition block (SCRRB) that
identifies the core salient regions in spacecraft images using a pre-trained
saliency detection model. Furthermore, we present an adaptive-weighted feature
fusion enhancement mechanism (AFFEM) to selectively aggregate the spacecraft
core region features with general image features by dynamic weight parameter to
enhance the response of the core salient regions. Experimental results
demonstrate that the proposed SGSASR outperforms state-of-the-art approaches.

</details>


### [94] [PerfCam: Digital Twinning for Production Lines Using 3D Gaussian Splatting and Vision Models](https://arxiv.org/abs/2504.18165)
*Michel Gokan Khan, Renan Guarese, Fabian Johnson, Xi Vincent Wang, Anders Bergman, Benjamin Edvinsson, Mario Romero, Jérémy Vachier, Jan Kronqvist*

Main category: cs.CV

TL;DR: PerfCam is an open-source digital twinning framework using 3D Gaussian Splatting and CNNs for real-time KPI extraction in industrial production lines, validated in pharmaceutical settings.


<details>
  <summary>Details</summary>
Motivation: To enhance industrial production efficiency by providing precise digital twins and real-time KPIs for actionable insights.

Method: Combines camera/sensor data, 3D Gaussian Splatting, and CNNs for semi-automated object tracking and spatial mapping.

Result: Validated in pharmaceutical production lines, PerfCam successfully extracts KPIs like OEE and conveyor belt rates.

Conclusion: PerfCam is an effective tool for smart manufacturing, offering actionable insights through precise digital twins.

Abstract: We introduce PerfCam, an open source Proof-of-Concept (PoC) digital twinning
framework that combines camera and sensory data with 3D Gaussian Splatting and
computer vision models for digital twinning, object tracking, and Key
Performance Indicators (KPIs) extraction in industrial production lines. By
utilizing 3D reconstruction and Convolutional Neural Networks (CNNs), PerfCam
offers a semi-automated approach to object tracking and spatial mapping,
enabling digital twins that capture real-time KPIs such as availability,
performance, Overall Equipment Effectiveness (OEE), and rate of conveyor belts
in the production line. We validate the effectiveness of PerfCam through a
practical deployment within realistic test production lines in the
pharmaceutical industry and contribute an openly published dataset to support
further research and development in the field. The results demonstrate
PerfCam's ability to deliver actionable insights through its precise digital
twin capabilities, underscoring its value as an effective tool for developing
usable digital twins in smart manufacturing environments and extracting
operational analytics.

</details>


### [95] [MASF-YOLO: An Improved YOLOv11 Network for Small Object Detection on Drone View](https://arxiv.org/abs/2504.18136)
*Liugang Lu, Dabin He, Congxiang Liu, Zhixiang Deng*

Main category: cs.CV

TL;DR: Proposed MASF-YOLO improves UAV object detection by addressing small object detection, background noise, and multi-scale feature fusion, outperforming YOLOv11 in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Challenges in UAV object detection include small target pixels, scale variations, and complex backgrounds, limiting practical applications.

Method: Developed MASF-YOLO with Multi-scale Feature Aggregation Module (MFAM), Improved Efficient Multi-scale Attention Module (IEMA), and Dimension-Aware Selective Integration Module (DASI).

Result: MASF-YOLO-s achieved 4.6% and 3.5% higher mAP than YOLOv11-s, with fewer parameters and computational costs.

Conclusion: MASF-YOLO offers superior detection accuracy and efficiency, making it competitive for UAV applications.

Abstract: With the rapid advancement of Unmanned Aerial Vehicle (UAV) and computer
vision technologies, object detection from UAV perspectives has emerged as a
prominent research area. However, challenges for detection brought by the
extremely small proportion of target pixels, significant scale variations of
objects, and complex background information in UAV images have greatly limited
the practical applications of UAV. To address these challenges, we propose a
novel object detection network Multi-scale Context Aggregation and
Scale-adaptive Fusion YOLO (MASF-YOLO), which is developed based on YOLOv11.
Firstly, to tackle the difficulty of detecting small objects in UAV images, we
design a Multi-scale Feature Aggregation Module (MFAM), which significantly
improves the detection accuracy of small objects through parallel multi-scale
convolutions and feature fusion. Secondly, to mitigate the interference of
background noise, we propose an Improved Efficient Multi-scale Attention Module
(IEMA), which enhances the focus on target regions through feature grouping,
parallel sub-networks, and cross-spatial learning. Thirdly, we introduce a
Dimension-Aware Selective Integration Module (DASI), which further enhances
multi-scale feature fusion capabilities by adaptively weighting and fusing
low-dimensional features and high-dimensional features. Finally, we conducted
extensive performance evaluations of our proposed method on the VisDrone2019
dataset. Compared to YOLOv11-s, MASFYOLO-s achieves improvements of 4.6% in
mAP@0.5 and 3.5% in mAP@0.5:0.95 on the VisDrone2019 validation set.
Remarkably, MASF-YOLO-s outperforms YOLOv11-m while requiring only
approximately 60% of its parameters and 65% of its computational cost.
Furthermore, comparative experiments with state-of-the-art detectors confirm
that MASF-YOLO-s maintains a clear competitive advantage in both detection
accuracy and model efficiency.

</details>


### [96] [Multi-Grained Compositional Visual Clue Learning for Image Intent Recognition](https://arxiv.org/abs/2504.18201)
*Yin Tang, Jiankai Li, Hongyu Yang, Xuan Dong, Lifeng Fan, Weixin Li*

Main category: cs.CV

TL;DR: The paper introduces MCCL, a novel method for image intent recognition, addressing challenges like visual diversity and subjectivity by leveraging multi-grained features and class-specific prototypes.


<details>
  <summary>Details</summary>
Motivation: Recognizing intent in social media images is challenging due to implicit visual clues and intra-class variety. Existing methods struggle with visual diversity.

Method: MCCL breaks down intent recognition into visual clue composition, integrates multi-grained features, uses class-specific prototypes, and treats it as a multi-label classification problem with a graph convolutional network.

Result: Achieves state-of-the-art performance on Intentonomy and MDID datasets, improving accuracy and interpretability.

Conclusion: MCCL advances intent recognition and provides a foundation for understanding complex human expressions.

Abstract: In an era where social media platforms abound, individuals frequently share
images that offer insights into their intents and interests, impacting
individual life quality and societal stability. Traditional computer vision
tasks, such as object detection and semantic segmentation, focus on concrete
visual representations, while intent recognition relies more on implicit visual
clues. This poses challenges due to the wide variation and subjectivity of such
clues, compounded by the problem of intra-class variety in conveying abstract
concepts, e.g. "enjoy life". Existing methods seek to solve the problem by
manually designing representative features or building prototypes for each
class from global features. However, these methods still struggle to deal with
the large visual diversity of each intent category. In this paper, we introduce
a novel approach named Multi-grained Compositional visual Clue Learning (MCCL)
to address these challenges for image intent recognition. Our method leverages
the systematic compositionality of human cognition by breaking down intent
recognition into visual clue composition and integrating multi-grained
features. We adopt class-specific prototypes to alleviate data imbalance. We
treat intent recognition as a multi-label classification problem, using a graph
convolutional network to infuse prior knowledge through label embedding
correlations. Demonstrated by a state-of-the-art performance on the Intentonomy
and MDID datasets, our approach advances the accuracy of existing methods while
also possessing good interpretability. Our work provides an attempt for future
explorations in understanding complex and miscellaneous forms of human
expression.

</details>


### [97] [ActionArt: Advancing Multimodal Large Models for Fine-Grained Human-Centric Video Understanding](https://arxiv.org/abs/2504.18152)
*Yi-Xing Peng, Qize Yang, Yu-Ming Tang, Shenghao Fu, Kun-Yu Lin, Xihan Wei, Wei-Shi Zheng*

Main category: cs.CV

TL;DR: ActionArt is a fine-grained video-caption dataset for human-centric AI, revealing gaps in current models' understanding. Proxy tasks using MLLM-generated data reduce reliance on costly manual annotations.


<details>
  <summary>Details</summary>
Motivation: Advance fine-grained human action and pose understanding in videos for human-centric AI applications.

Method: Introduce ActionArt dataset with detailed annotations; develop eight sub-tasks and propose proxy tasks using MLLM-generated data.

Result: Current models lack fine-grained understanding; proxy tasks significantly improve performance.

Conclusion: Proxy tasks mitigate the need for costly manual annotations, enhancing model performance.

Abstract: Fine-grained understanding of human actions and poses in videos is essential
for human-centric AI applications. In this work, we introduce ActionArt, a
fine-grained video-caption dataset designed to advance research in
human-centric multimodal understanding. Our dataset comprises thousands of
videos capturing a broad spectrum of human actions, human-object interactions,
and diverse scenarios, each accompanied by detailed annotations that
meticulously label every limb movement. We develop eight sub-tasks to evaluate
the fine-grained understanding capabilities of existing large multimodal models
across different dimensions. Experimental results indicate that, while current
large multimodal models perform commendably on various tasks, they often fall
short in achieving fine-grained understanding. We attribute this limitation to
the scarcity of meticulously annotated data, which is both costly and difficult
to scale manually. Since manual annotations are costly and hard to scale, we
propose proxy tasks to enhance the model perception ability in both spatial and
temporal dimensions. These proxy tasks are carefully crafted to be driven by
data automatically generated from existing MLLMs, thereby reducing the reliance
on costly manual labels. Experimental results show that the proposed proxy
tasks significantly narrow the gap toward the performance achieved with
manually annotated fine-grained data.

</details>


### [98] [E-InMeMo: Enhanced Prompting for Visual In-Context Learning](https://arxiv.org/abs/2504.18158)
*Jiahao Zhang, Bowen Wang, Hong Liu, Liangzhi Li, Yuta Nakashima, Hajime Nagahara*

Main category: cs.CV

TL;DR: E-InMeMo enhances visual in-context learning (ICL) by adding learnable perturbations to in-context pairs, improving performance on vision tasks like segmentation and object detection.


<details>
  <summary>Details</summary>
Motivation: The quality of prompts in visual ICL is critical for performance, but current methods lack optimization. E-InMeMo addresses this gap.

Method: E-InMeMo introduces learnable perturbations to in-context pairs to optimize prompting in visual ICL.

Result: E-InMeMo boosts mIoU by 7.99 for segmentation and 17.04 for object detection over baselines.

Conclusion: E-InMeMo is a lightweight, effective solution for improving visual ICL performance.

Abstract: Large-scale models trained on extensive datasets have become the standard due
to their strong generalizability across diverse tasks. In-context learning
(ICL), widely used in natural language processing, leverages these models by
providing task-specific prompts without modifying their parameters. This
paradigm is increasingly being adapted for computer vision, where models
receive an input-output image pair, known as an in-context pair, alongside a
query image to illustrate the desired output. However, the success of visual
ICL largely hinges on the quality of these prompts. To address this, we propose
Enhanced Instruct Me More (E-InMeMo), a novel approach that incorporates
learnable perturbations into in-context pairs to optimize prompting. Through
extensive experiments on standard vision tasks, E-InMeMo demonstrates superior
performance over existing state-of-the-art methods. Notably, it improves mIoU
scores by 7.99 for foreground segmentation and by 17.04 for single object
detection when compared to the baseline without learnable prompts. These
results highlight E-InMeMo as a lightweight yet effective strategy for
enhancing visual ICL. Code is publicly available at:
https://github.com/Jackieam/E-InMeMo

</details>


### [99] [Event-Based Eye Tracking. 2025 Event-based Vision Workshop](https://arxiv.org/abs/2504.18249)
*Qinyu Chen, Chang Gao, Min Liu, Daniele Perrone, Yan Ru Pei, Zuowen Wang, Zhuo Zou, Shihang Tan, Tao Han, Guorui Lu, Zhen Xu, Junyuan Ding, Ziteng Wang, Zongwei Wu, Han Han, Yuliang Wu, Jinze Chen, Wei Zhai, Yang Cao, Zheng-jun Zha, Nuwan Bandara, Thivya Kandappu, Archan Misra, Xiaopeng Lin, Hongxiang Huang, Hongwei Ren, Bojun Cheng, Hoang M. Truong, Vinh-Thuan Ly, Huy G. Tran, Thuan-Phat Nguyen, Tram T. Doan*

Main category: cs.CV

TL;DR: A survey reviewing top methods from the 2025 Event-Based Eye Tracking Challenge, focusing on pupil center prediction using event cameras, with insights on accuracy, model size, operations, and hardware design.


<details>
  <summary>Details</summary>
Motivation: To summarize and analyze innovative methods from the top-performing teams in the 2025 Event-Based Eye Tracking Challenge to advance future research in event-based eye tracking.

Method: Review and summarize methods from top teams, reporting accuracy, model size, and operations. Also discusses hardware design perspectives.

Result: Highlighted innovative approaches and performance metrics (accuracy, model size, operations) of top methods in the challenge.

Conclusion: The survey provides valuable insights into event-based eye tracking, showcasing top methods and discussing hardware considerations for future research.

Abstract: This survey serves as a review for the 2025 Event-Based Eye Tracking
Challenge organized as part of the 2025 CVPR event-based vision workshop. This
challenge focuses on the task of predicting the pupil center by processing
event camera recorded eye movement. We review and summarize the innovative
methods from teams rank the top in the challenge to advance future event-based
eye tracking research. In each method, accuracy, model size, and number of
operations are reported. In this survey, we also discuss event-based eye
tracking from the perspective of hardware design.

</details>


### [100] [Label-independent hyperparameter-free self-supervised single-view deep subspace clustering](https://arxiv.org/abs/2504.18179)
*Lovro Sindicic, Ivica Kopriva*

Main category: cs.CV

TL;DR: A novel deep subspace clustering (DSC) method addresses limitations like ignoring intermediate layers, independent task treatment, hyperparameter tuning reliance, label-dependent termination, and post-processing needs. It outperforms linear SC algorithms.


<details>
  <summary>Details</summary>
Motivation: Current DSC methods have flaws like disregarding intermediate layers, treating tasks independently, requiring impractical hyperparameter tuning, and relying on labels. The goal is to overcome these issues.

Method: The approach uses a layer-wise self-expression loss, subspace-structured norm, multi-stage learning, self-stopping mechanism, and fixed leading coefficients. Evaluated on six datasets.

Result: Outperforms linear SC algorithms with tuned hyperparameters and matches top linear approaches.

Conclusion: The proposed DSC method effectively addresses key challenges, offering improved performance without reliance on labels or hyperparameter tuning.

Abstract: Deep subspace clustering (DSC) algorithms face several challenges that hinder
their widespread adoption across variois application domains. First, clustering
quality is typically assessed using only the encoder's output layer,
disregarding valuable information present in the intermediate layers. Second,
most DSC approaches treat representation learning and subspace clustering as
independent tasks, limiting their effectiveness. Third, they assume the
availability of a held-out dataset for hyperparameter tuning, which is often
impractical in real-world scenarios. Fourth, learning termination is commonly
based on clustering error monitoring, requiring external labels. Finally, their
performance often depends on post-processing techniques that rely on labeled
data. To address this limitations, we introduce a novel single-view DSC
approach that: (i) minimizes a layer-wise self expression loss using a joint
representation matrix; (ii) optimizes a subspace-structured norm to enhance
clustering quality; (iii) employs a multi-stage sequential learning framework,
consisting of pre-training and fine-tuning, enabling the use of multiple
regularization terms without hyperparameter tuning; (iv) incorporates a
relative error-based self-stopping mechanism to terminate training without
labels; and (v) retains a fixed number of leading coefficients in the learned
representation matrix based on prior knowledge. We evaluate the proposed method
on six datasets representing faces, digits, and objects. The results show that
our method outperforms most linear SC algorithms with careffulyl tuned
hyperparameters while maintaining competitive performance with the best
performing linear appoaches.

</details>


### [101] [What is the Added Value of UDA in the VFM Era?](https://arxiv.org/abs/2504.18190)
*Brunó B. Englert, Tommie Kerssies, Gijs Dubbelman*

Main category: cs.CV

TL;DR: UDA with Vision Foundation Models (VFMs) shows mixed results in synth-to-real and real-to-real scenarios. While UDA outperforms source-only fine-tuning in synthetic data cases, its advantage diminishes with stronger synthetic or diverse real data. Including minimal labeled target data can achieve state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To understand UDA's effectiveness with more representative and diverse data and compare it to source-only fine-tuning of VFMs in realistic scenarios, particularly for semantic segmentation in autonomous driving.

Method: Assessed UDA for synth-to-real and real-to-real cases, varying source and target data combinations, and investigated the impact of minimal labeled target data.

Result: UDA's improvement over source-only fine-tuning reduced from +8 mIoU to +2 mIoU with stronger synthetic data and had no added value with diverse real data. Including 1/16 of Cityscapes labels matched fully-supervised performance (85 mIoU).

Conclusion: UDA's value depends on data quality and diversity. It remains beneficial for synthetic data scenarios but may not always outperform source-only fine-tuning. Minimal labeled target data can achieve high performance, suggesting practical applications for robust autonomous driving.

Abstract: Unsupervised Domain Adaptation (UDA) can improve a perception model's
generalization to an unlabeled target domain starting from a labeled source
domain. UDA using Vision Foundation Models (VFMs) with synthetic source data
can achieve generalization performance comparable to fully-supervised learning
with real target data. However, because VFMs have strong generalization from
their pre-training, more straightforward, source-only fine-tuning can also
perform well on the target. As data scenarios used in academic research are not
necessarily representative for real-world applications, it is currently unclear
(a) how UDA behaves with more representative and diverse data and (b) if
source-only fine-tuning of VFMs can perform equally well in these scenarios.
Our research aims to close these gaps and, similar to previous studies, we
focus on semantic segmentation as a representative perception task. We assess
UDA for synth-to-real and real-to-real use cases with different source and
target data combinations. We also investigate the effect of using a small
amount of labeled target data in UDA. We clarify that while these scenarios are
more realistic, they are not necessarily more challenging. Our results show
that, when using stronger synthetic source data, UDA's improvement over
source-only fine-tuning of VFMs reduces from +8 mIoU to +2 mIoU, and when using
more diverse real source data, UDA has no added value. However, UDA
generalization is always higher in all synthetic data scenarios than
source-only fine-tuning and, when including only 1/16 of Cityscapes labels,
synthetic UDA obtains the same state-of-the-art segmentation quality of 85 mIoU
as a fully-supervised model using all labels. Considering the mixed results, we
discuss how UDA can best support robust autonomous driving at scale.

</details>


### [102] [Enhancing Long-Term Re-Identification Robustness Using Synthetic Data: A Comparative Analysis](https://arxiv.org/abs/2504.18286)
*Christian Pionzewski, Rebecca Rademacher, Jérôme Rutinowski, Antonia Ponikarov, Stephan Matzke, Tim Chilla, Pia Schreynemackers, Alice Kirchheim*

Main category: cs.CV

TL;DR: The paper investigates synthetic training data and material aging's impact on re-identification, achieving significant accuracy improvements and introducing a new dataset.


<details>
  <summary>Details</summary>
Motivation: To address challenges in re-identification due to material aging and wear, and to explore the benefits of synthetic training data.

Method: Tested experimental setups and gallery set expanding strategies, using a continuously updating gallery and models trained with synthetic data.

Result: Achieved a 24% increase in Rank-1 accuracy by accounting for aging and a 13% boost with synthetic training data. Introduced the pallet-block-2696 dataset.

Conclusion: Synthetic data and dynamic gallery updates enhance re-identification performance, and the new dataset supports further research in aging and wear prediction.

Abstract: This contribution explores the impact of synthetic training data usage and
the prediction of material wear and aging in the context of re-identification.
Different experimental setups and gallery set expanding strategies are tested,
analyzing their impact on performance over time for aging re-identification
subjects. Using a continuously updating gallery, we were able to increase our
mean Rank-1 accuracy by 24%, as material aging was taken into account step by
step. In addition, using models trained with 10% artificial training data,
Rank-1 accuracy could be increased by up to 13%, in comparison to a model
trained on only real-world data, significantly boosting generalized performance
on hold-out data. Finally, this work introduces a novel, open-source
re-identification dataset, pallet-block-2696. This dataset contains 2,696
images of Euro pallets, taken over a period of 4 months. During this time,
natural aging processes occurred and some of the pallets were damaged during
their usage. These wear and tear processes significantly changed the appearance
of the pallets, providing a dataset that can be used to generate synthetically
aged pallets or other wooden materials.

</details>


### [103] [LiDAR-Guided Monocular 3D Object Detection for Long-Range Railway Monitoring](https://arxiv.org/abs/2504.18203)
*Raul David Dominguez Sanchez, Xavier Diaz Ortiz, Xingcheng Zhou, Max Peter Ronecker, Michael Karner, Daniel Watzenig, Alois Knoll*

Main category: cs.CV

TL;DR: A deep-learning-based method for long-range 3D object detection in railway automation, using monocular images and LiDAR data, achieving detection up to 250 meters.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for robust long-range perception in railway automation to enhance safety and efficiency, especially given trains' longer braking distances compared to automotive systems.

Method: Combines a modified YOLOv9 for 2.5D detection, a depth estimation network, and dedicated short- and long-range 3D detection heads, leveraging monocular images and LiDAR data during training.

Result: Effective detection of objects up to 250 meters, validated on the OSDaR23 dataset, showcasing potential for railway automation.

Conclusion: The approach shows promise for railway automation but requires further improvements for broader application.

Abstract: Railway systems, particularly in Germany, require high levels of automation
to address legacy infrastructure challenges and increase train traffic safely.
A key component of automation is robust long-range perception, essential for
early hazard detection, such as obstacles at level crossings or pedestrians on
tracks. Unlike automotive systems with braking distances of ~70 meters, trains
require perception ranges exceeding 1 km. This paper presents an
deep-learning-based approach for long-range 3D object detection tailored for
autonomous trains. The method relies solely on monocular images, inspired by
the Faraway-Frustum approach, and incorporates LiDAR data during training to
improve depth estimation. The proposed pipeline consists of four key modules:
(1) a modified YOLOv9 for 2.5D object detection, (2) a depth estimation
network, and (3-4) dedicated short- and long-range 3D detection heads.
Evaluations on the OSDaR23 dataset demonstrate the effectiveness of the
approach in detecting objects up to 250 meters. Results highlight its potential
for railway automation and outline areas for future improvement.

</details>


### [104] [Optimizing Multi-Round Enhanced Training in Diffusion Models for Improved Preference Understanding](https://arxiv.org/abs/2504.18204)
*Kun Li, Jianhui Wang, Yangfan He, Xinyuan Song, Ruoyu Wang, Hongyang He, Wenxin Zhang, Jiaqi Chen, Keqin Li, Sida Li, Miao Zhang, Tianyu Shi, Xueqian Wang*

Main category: cs.CV

TL;DR: The paper introduces a Visual Co-Adaptation (VCA) framework for improving high-resolution image generation in multi-round dialogues by incorporating human feedback and optimizing reward functions.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of aligning generative AI outputs with fine-grained user preferences in multi-round interactions, which previous methods failed to optimize.

Method: The VCA framework uses human-in-the-loop feedback, a reward model aligned with human preferences, and fine-tunes a diffusion model via LoRA, leveraging a diverse multi-turn dialogue dataset.

Result: The method outperforms state-of-the-art baselines, improving image consistency and alignment with user intent, and achieves higher user satisfaction in multi-turn dialogues.

Conclusion: The VCA framework effectively enhances generative AI performance in multi-round interactions by integrating human feedback and optimizing reward functions.

Abstract: Generative AI has significantly changed industries by enabling text-driven
image generation, yet challenges remain in achieving high-resolution outputs
that align with fine-grained user preferences. Consequently, multi-round
interactions are necessary to ensure the generated images meet expectations.
Previous methods enhanced prompts via reward feedback but did not optimize over
a multi-round dialogue dataset. In this work, we present a Visual Co-Adaptation
(VCA) framework incorporating human-in-the-loop feedback, leveraging a
well-trained reward model aligned with human preferences. Using a diverse
multi-turn dialogue dataset, our framework applies multiple reward functions,
such as diversity, consistency, and preference feedback, while fine-tuning the
diffusion model through LoRA, thus optimizing image generation based on user
input. We also construct multi-round dialogue datasets of prompts and image
pairs aligned with user intent. Experiments demonstrate that our method
outperforms state-of-the-art baselines, significantly improving image
consistency and alignment with user intent. Our approach consistently surpasses
competing models in user satisfaction, especially in multi-turn dialogue
scenarios.

</details>


### [105] [A Data-Centric Approach to 3D Semantic Segmentation of Railway Scenes](https://arxiv.org/abs/2504.18213)
*Nicolas Münger, Max Peter Ronecker, Xavier Diaz, Michael Karner, Daniel Watzenig, Jan Skaloud*

Main category: cs.CV

TL;DR: The paper introduces two data augmentation methods for LiDAR-based semantic segmentation in autonomous trains, improving distant-range performance on the OSDaR23 dataset.


<details>
  <summary>Details</summary>
Motivation: To address challenges in accurate semantic segmentation for autonomous trains, especially at varying distances.

Method: Two data augmentation techniques: person instance pasting for pedestrian segmentation and track sparsification for track segmentation.

Result: Significant improvements in distant-range segmentation while maintaining close-range accuracy.

Conclusion: Demonstrates the effectiveness of data-centric approaches for railway-specific challenges in autonomous train perception.

Abstract: LiDAR-based semantic segmentation is critical for autonomous trains,
requiring accurate predictions across varying distances. This paper introduces
two targeted data augmentation methods designed to improve segmentation
performance on the railway-specific OSDaR23 dataset. The person instance
pasting method enhances segmentation of pedestrians at distant ranges by
injecting realistic variations into the dataset. The track sparsification
method redistributes point density in LiDAR scans, improving track segmentation
at far distances with minimal impact on close-range accuracy. Both methods are
evaluated using a state-of-the-art 3D semantic segmentation network,
demonstrating significant improvements in distant-range performance while
maintaining robustness in close-range predictions. We establish the first 3D
semantic segmentation benchmark for OSDaR23, demonstrating the potential of
data-centric approaches to address railway-specific challenges in autonomous
train perception.

</details>


### [106] [TSCL:Multi-party loss Balancing scheme for deep learning Image steganography based on Curriculum learning](https://arxiv.org/abs/2504.18348)
*Fengchun Liu. Tong Zhang, Chunying Zhang*

Main category: cs.CV

TL;DR: The paper proposes a Two-stage Curriculum Learning loss scheduler (TSCL) to dynamically balance multiple losses in deep learning-based image steganography, improving embedding quality, decoding accuracy, and security.


<details>
  <summary>Details</summary>
Motivation: Fixed loss weights in existing steganography frameworks fail to adapt to task importance or training dynamics, limiting performance.

Method: TSCL uses two phases: a priori curriculum control (prioritizing embedding, decoding, and steganalysis resistance) and loss dynamics control (balancing tasks based on learning speed).

Result: Experiments on ALASKA2, VOC2012, and ImageNet show TSCL enhances steganography quality, decoding accuracy, and security.

Conclusion: TSCL effectively balances multinomial losses, outperforming fixed-weight approaches in deep learning steganography.

Abstract: For deep learning-based image steganography frameworks, in order to ensure
the invisibility and recoverability of the information embedding, the loss
function usually contains several losses such as embedding loss, recovery loss
and steganalysis loss. In previous research works, fixed loss weights are
usually chosen for training optimization, and this setting is not linked to the
importance of the steganography task itself and the training process. In this
paper, we propose a Two-stage Curriculum Learning loss scheduler (TSCL) for
balancing multinomial losses in deep learning image steganography algorithms.
TSCL consists of two phases: a priori curriculum control and loss dynamics
control. The first phase firstly focuses the model on learning the information
embedding of the original image by controlling the loss weights in the
multi-party adversarial training; secondly, it makes the model shift its
learning focus to improving the decoding accuracy; and finally, it makes the
model learn to generate a steganographic image that is resistant to
steganalysis. In the second stage, the learning speed of each training task is
evaluated by calculating the loss drop of the before and after iteration rounds
to balance the learning of each task. Experimental results on three large
public datasets, ALASKA2, VOC2012 and ImageNet, show that the proposed TSCL
strategy improves the quality of steganography, decoding accuracy and security.

</details>


### [107] [Unify3D: An Augmented Holistic End-to-end Monocular 3D Human Reconstruction via Anatomy Shaping and Twins Negotiating](https://arxiv.org/abs/2504.18215)
*Nanjie Yao, Gangjian Zhang, Wenhao Shen, Jian Shu, Hao Wang*

Main category: cs.CV

TL;DR: The paper introduces an end-to-end network for monocular 3D clothed human reconstruction, eliminating intermediate geometry and proposing novel modules for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Current methods rely on preceding models for explicit geometry, limiting reconstruction integrity. This paper aims to treat reconstruction holistically.

Method: Proposes an end-to-end network with an Anatomy Shaping Extraction module and Twins Negotiating Reconstruction U-Net, supported by Comic Data Augmentation and a large 3D scan dataset.

Result: Outperforms state-of-the-art methods on test sets and in-the-wild cases.

Conclusion: The holistic approach and novel framework achieve superior 3D human reconstruction from single images.

Abstract: Monocular 3D clothed human reconstruction aims to create a complete 3D avatar
from a single image. To tackle the human geometry lacking in one RGB image,
current methods typically resort to a preceding model for an explicit geometric
representation. For the reconstruction itself, focus is on modeling both it and
the input image. This routine is constrained by the preceding model, and
overlooks the integrity of the reconstruction task. To address this, this paper
introduces a novel paradigm that treats human reconstruction as a holistic
process, utilizing an end-to-end network for direct prediction from 2D image to
3D avatar, eliminating any explicit intermediate geometry display. Based on
this, we further propose a novel reconstruction framework consisting of two
core components: the Anatomy Shaping Extraction module, which captures implicit
shape features taking into account the specialty of human anatomy, and the
Twins Negotiating Reconstruction U-Net, which enhances reconstruction through
feature interaction between two U-Nets of different modalities. Moreover, we
propose a Comic Data Augmentation strategy and construct 15k+ 3D human scans to
bolster model performance in more complex case input. Extensive experiments on
two test sets and many in-the-wild cases show the superiority of our method
over SOTA methods. Our demos can be found in :
https://e2e3dgsrecon.github.io/e2e3dgsrecon/.

</details>


### [108] [COCO-Inpaint: A Benchmark for Image Inpainting Detection and Manipulation Localization](https://arxiv.org/abs/2504.18361)
*Haozhen Yan, Yan Hong, Jiahui Zhan, Yikun Ji, Jun Lan, Huijia Zhu, Weiqiang Wang, Jianfu Zhang*

Main category: cs.CV

TL;DR: The paper introduces COCOInpaint, a benchmark for detecting inpainting-based image manipulations, addressing gaps in current IMDL methods.


<details>
  <summary>Details</summary>
Motivation: Existing IMDL methods lack focus on inpainting-based manipulations, necessitating a dedicated benchmark to improve detection accuracy.

Method: COCOInpaint includes high-quality inpainting samples, diverse generation scenarios, and large-scale coverage with 258,266 images.

Result: The benchmark emphasizes intrinsic inconsistencies in inpainted regions and provides a rigorous evaluation protocol for IMDL methods.

Conclusion: COCOInpaint will be publicly available to advance research in detecting inpainting manipulations.

Abstract: Recent advancements in image manipulation have achieved unprecedented
progress in generating photorealistic content, but also simultaneously
eliminating barriers to arbitrary manipulation and editing, raising concerns
about multimedia authenticity and cybersecurity. However, existing Image
Manipulation Detection and Localization (IMDL) methodologies predominantly
focus on splicing or copy-move forgeries, lacking dedicated benchmarks for
inpainting-based manipulations. To bridge this gap, we present COCOInpaint, a
comprehensive benchmark specifically designed for inpainting detection, with
three key contributions: 1) High-quality inpainting samples generated by six
state-of-the-art inpainting models, 2) Diverse generation scenarios enabled by
four mask generation strategies with optional text guidance, and 3) Large-scale
coverage with 258,266 inpainted images with rich semantic diversity. Our
benchmark is constructed to emphasize intrinsic inconsistencies between
inpainted and authentic regions, rather than superficial semantic artifacts
such as object shapes. We establish a rigorous evaluation protocol using three
standard metrics to assess existing IMDL approaches. The dataset will be made
publicly available to facilitate future research in this area.

</details>


### [109] [Dense Geometry Supervision for Underwater Depth Estimation](https://arxiv.org/abs/2504.18233)
*Wenxiang Gua, Lin Qia*

Main category: cs.CV

TL;DR: A novel method for monocular depth estimation in underwater scenes, combining a cost-effective dataset and a texture-depth fusion module, improves accuracy and adaptability.


<details>
  <summary>Details</summary>
Motivation: Limited research and data for underwater monocular depth estimation, prompting the need for innovative solutions.

Method: Constructs an underwater dataset using multi-view depth estimation and introduces a texture-depth fusion module based on underwater optical principles.

Result: Significant improvement in accuracy and adaptability on the FLSea dataset.

Conclusion: Provides a cost-effective solution with practical potential for underwater depth estimation.

Abstract: The field of monocular depth estimation is continually evolving with the
advent of numerous innovative models and extensions. However, research on
monocular depth estimation methods specifically for underwater scenes remains
limited, compounded by a scarcity of relevant data and methodological support.
This paper proposes a novel approach to address the existing challenges in
current monocular depth estimation methods for underwater environments. We
construct an economically efficient dataset suitable for underwater scenarios
by employing multi-view depth estimation to generate supervisory signals and
corresponding enhanced underwater images. we introduces a texture-depth fusion
module, designed according to the underwater optical imaging principles, which
aims to effectively exploit and integrate depth information from texture cues.
Experimental results on the FLSea dataset demonstrate that our approach
significantly improves the accuracy and adaptability of models in underwater
settings. This work offers a cost-effective solution for monocular underwater
depth estimation and holds considerable promise for practical applications.

</details>


### [110] [BiasBench: A reproducible benchmark for tuning the biases of event cameras](https://arxiv.org/abs/2504.18235)
*Andreas Ziegler, David Joseph, Thomas Gossard, Emil Moldovan, Andreas Zell*

Main category: cs.CV

TL;DR: BiasBench introduces a dataset and RL-based method for tuning event-based camera biases, addressing the lack of tools for this task.


<details>
  <summary>Details</summary>
Motivation: Event-based cameras lack advanced automatic bias configuration tools, unlike frame-based cameras, hindering optimal performance.

Method: BiasBench provides a dataset with grid-sampled bias settings and an RL-based method for online bias adjustments.

Result: The dataset includes multiple scenes with quality metrics, and the RL method enables dynamic bias tuning.

Conclusion: BiasBench advances bias tuning for event-based cameras, improving reproducibility and performance.

Abstract: Event-based cameras are bio-inspired sensors that detect light changes
asynchronously for each pixel. They are increasingly used in fields like
computer vision and robotics because of several advantages over traditional
frame-based cameras, such as high temporal resolution, low latency, and high
dynamic range. As with any camera, the output's quality depends on how well the
camera's settings, called biases for event-based cameras, are configured. While
frame-based cameras have advanced automatic configuration algorithms, there are
very few such tools for tuning these biases. A systematic testing framework
would require observing the same scene with different biases, which is tricky
since event cameras only generate events when there is movement. Event
simulators exist, but since biases heavily depend on the electrical circuit and
the pixel design, available simulators are not well suited for bias tuning. To
allow reproducibility, we present BiasBench, a novel event dataset containing
multiple scenes with settings sampled in a grid-like pattern. We present three
different scenes, each with a quality metric of the downstream application.
Additionally, we present a novel, RL-based method to facilitate online bias
adjustments.

</details>


### [111] [SSL4Eco: A Global Seasonal Dataset for Geospatial Foundation Models in Ecology](https://arxiv.org/abs/2504.18256)
*Elena Plekhanova, Damien Robert, Johannes Dollinger, Emilia Arens, Philipp Brun, Jan Dirk Wegner, Niklaus Zimmermann*

Main category: cs.CV

TL;DR: The paper introduces SSL4Eco, a phenology-informed dataset for self-supervised learning in macroecology, improving representation quality and achieving state-of-the-art performance on ecological tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing biases in existing geospatial models and improving the capture of global vegetation seasonality for better biodiversity mapping.

Method: Proposes a phenology-informed sampling strategy and trains a model on the SSL4Eco dataset with a season-contrastive objective.

Result: The model pretrained on SSL4Eco outperforms others on 7 out of 8 ecological downstream tasks.

Conclusion: Dataset construction is crucial for ecological representation learning, and SSL4Eco advances macroecological and computer vision research.

Abstract: With the exacerbation of the biodiversity and climate crises, macroecological
pursuits such as global biodiversity mapping become more urgent. Remote sensing
offers a wealth of Earth observation data for ecological studies, but the
scarcity of labeled datasets remains a major challenge. Recently,
self-supervised learning has enabled learning representations from unlabeled
data, triggering the development of pretrained geospatial models with
generalizable features. However, these models are often trained on datasets
biased toward areas of high human activity, leaving entire ecological regions
underrepresented. Additionally, while some datasets attempt to address
seasonality through multi-date imagery, they typically follow calendar seasons
rather than local phenological cycles. To better capture vegetation seasonality
at a global scale, we propose a simple phenology-informed sampling strategy and
introduce corresponding SSL4Eco, a multi-date Sentinel-2 dataset, on which we
train an existing model with a season-contrastive objective. We compare
representations learned from SSL4Eco against other datasets on diverse
ecological downstream tasks and demonstrate that our straightforward sampling
method consistently improves representation quality, highlighting the
importance of dataset construction. The model pretrained on SSL4Eco reaches
state of the art performance on 7 out of 8 downstream tasks spanning
(multi-label) classification and regression. We release our code, data, and
model weights to support macroecological and computer vision research at
https://github.com/PlekhanovaElena/ssl4eco.

</details>


### [112] [Task-Oriented Communications for Visual Navigation with Edge-Aerial Collaboration in Low Altitude Economy](https://arxiv.org/abs/2504.18317)
*Zhengru Fang, Zhenghao Liu, Jingjing Wang, Senkang Hu, Yu Guo, Yiqin Deng, Yuguang Fang*

Main category: cs.CV

TL;DR: A task-oriented communication framework for UAVs uses multi-view features and edge servers to achieve precise localization in GPS-denied urban areas, leveraging an O-VIB encoder for efficiency.


<details>
  <summary>Details</summary>
Motivation: Precise UAV localization in GPS-denied urban areas is critical for the Low Altitude Economy (LAE), but vision-based methods face bandwidth and processing constraints.

Method: Proposes O-VIB encoder with ARD to prune non-informative features and enforce orthogonality, offloading tasks to edge servers for efficient localization.

Result: O-VIB achieves high-precision localization under strict bandwidth constraints, validated on a dedicated LAE UAV dataset.

Conclusion: The framework enables efficient and accurate UAV localization, with code and dataset made publicly available.

Abstract: To support the Low Altitude Economy (LAE), precise unmanned aerial vehicles
(UAVs) localization in urban areas where global positioning system (GPS)
signals are unavailable. Vision-based methods offer a viable alternative but
face severe bandwidth, memory and processing constraints on lightweight UAVs.
Inspired by mammalian spatial cognition, we propose a task-oriented
communication framework, where UAVs equipped with multi-camera systems extract
compact multi-view features and offload localization tasks to edge servers. We
introduce the Orthogonally-constrained Variational Information Bottleneck
encoder (O-VIB), which incorporates automatic relevance determination (ARD) to
prune non-informative features while enforcing orthogonality to minimize
redundancy. This enables efficient and accurate localization with minimal
transmission cost. Extensive evaluation on a dedicated LAE UAV dataset shows
that O-VIB achieves high-precision localization under stringent bandwidth
budgets. Code and dataset will be made publicly available:
github.com/fangzr/TOC-Edge-Aerial.

</details>


### [113] [A Multimodal Hybrid Late-Cascade Fusion Network for Enhanced 3D Object Detection](https://arxiv.org/abs/2504.18419)
*Carlo Sgaravatti, Roberto Basla, Riccardo Pieroni, Matteo Corno, Sergio M. Savaresi, Luca Magri, Giacomo Boracchi*

Main category: cs.CV

TL;DR: A hybrid late-cascade method for 3D object detection using LiDAR and RGB cameras, reducing false positives and recovering false negatives, with flexible training and improved performance on KITTI.


<details>
  <summary>Details</summary>
Motivation: To enhance 3D object detection by combining LiDAR and RGB inputs, addressing false positives and negatives while leveraging pre-trained detectors.

Method: Uses a late-cascade fusion scheme: projects LiDAR bounding boxes onto RGB images for false positive reduction and employs epipolar constraints for false negative recovery.

Result: Significant performance improvements on the KITTI benchmark, especially for Pedestrians and Cyclists.

Conclusion: The method is flexible, compatible with existing detectors, and effective for multimodal 3D object detection.

Abstract: We present a new way to detect 3D objects from multimodal inputs, leveraging
both LiDAR and RGB cameras in a hybrid late-cascade scheme, that combines an
RGB detection network and a 3D LiDAR detector. We exploit late fusion
principles to reduce LiDAR False Positives, matching LiDAR detections with RGB
ones by projecting the LiDAR bounding boxes on the image. We rely on cascade
fusion principles to recover LiDAR False Negatives leveraging epipolar
constraints and frustums generated by RGB detections of separate views. Our
solution can be plugged on top of any underlying single-modal detectors,
enabling a flexible training process that can take advantage of pre-trained
LiDAR and RGB detectors, or train the two branches separately. We evaluate our
results on the KITTI object detection benchmark, showing significant
performance improvements, especially for the detection of Pedestrians and
Cyclists.

</details>


### [114] [STP4D: Spatio-Temporal-Prompt Consistent Modeling for Text-to-4D Gaussian Splatting](https://arxiv.org/abs/2504.18318)
*Yunze Deng, Haijun Xiong, Bin Feng, Xinggang Wang, Wenyu Liu*

Main category: cs.CV

TL;DR: STP4D is a novel method for high-quality text-to-4D generation, integrating spatio-temporal-prompt consistency modeling with three key modules and leveraging Diffusion models for efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-4D methods lack unified spatio-temporal and prompt alignment, leading to inconsistencies and low-quality outputs.

Method: STP4D uses Time-varying Prompt Embedding, Geometric Information Enhancement, and Temporal Extension Deformation modules, and employs Diffusion models for 4D Gaussian generation.

Result: STP4D achieves high-fidelity 4D content generation in ~4.6s per asset, outperforming existing methods in quality and speed.

Conclusion: STP4D successfully addresses spatio-temporal-prompt alignment and efficiency, setting a new benchmark for text-to-4D generation.

Abstract: Text-to-4D generation is rapidly developing and widely applied in various
scenarios. However, existing methods often fail to incorporate adequate
spatio-temporal modeling and prompt alignment within a unified framework,
resulting in temporal inconsistencies, geometric distortions, or low-quality 4D
content that deviates from the provided texts. Therefore, we propose STP4D, a
novel approach that aims to integrate comprehensive spatio-temporal-prompt
consistency modeling for high-quality text-to-4D generation. Specifically,
STP4D employs three carefully designed modules: Time-varying Prompt Embedding,
Geometric Information Enhancement, and Temporal Extension Deformation, which
collaborate to accomplish this goal. Furthermore, STP4D is among the first
methods to exploit the Diffusion model to generate 4D Gaussians, combining the
fine-grained modeling capabilities and the real-time rendering process of 4DGS
with the rapid inference speed of the Diffusion model. Extensive experiments
demonstrate that STP4D excels in generating high-fidelity 4D content with
exceptional efficiency (approximately 4.6s per asset), surpassing existing
methods in both quality and speed.

</details>


### [115] [Depth3DLane: Monocular 3D Lane Detection via Depth Prior Distillation](https://arxiv.org/abs/2504.18325)
*Dongxin Lyu, Han Huang, Cheng Tan, Zimu Li*

Main category: cs.CV

TL;DR: A BEV-based framework improves 3D lane detection by addressing IPM limitations with hierarchical depth features, depth prior distillation, and spatial coherence via CRF.


<details>
  <summary>Details</summary>
Motivation: Overcoming inaccuracies in 3D lane detection caused by IPM's flat-ground assumption and loss of contextual information.

Method: Uses a Hierarchical Depth-Aware Head for multi-scale depth features, Depth Prior Distillation for semantic depth knowledge, and a Conditional Random Field for spatial coherence.

Result: Achieves state-of-the-art performance in z-axis error and overall accuracy.

Conclusion: The proposed framework effectively enhances 3D lane detection accuracy by addressing depth and contextual challenges.

Abstract: Monocular 3D lane detection is challenging due to the difficulty in capturing
depth information from single-camera images. A common strategy involves
transforming front-view (FV) images into bird's-eye-view (BEV) space through
inverse perspective mapping (IPM), facilitating lane detection using BEV
features. However, IPM's flat-ground assumption and loss of contextual
information lead to inaccuracies in reconstructing 3D information, especially
height. In this paper, we introduce a BEV-based framework to address these
limitations and improve 3D lane detection accuracy. Our approach incorporates a
Hierarchical Depth-Aware Head that provides multi-scale depth features,
mitigating the flat-ground assumption by enhancing spatial awareness across
varying depths. Additionally, we leverage Depth Prior Distillation to transfer
semantic depth knowledge from a teacher model, capturing richer structural and
contextual information for complex lane structures. To further refine lane
continuity and ensure smooth lane reconstruction, we introduce a Conditional
Random Field module that enforces spatial coherence in lane predictions.
Extensive experiments validate that our method achieves state-of-the-art
performance in terms of z-axis error and outperforms other methods in the field
in overall performance. The code is released at:
https://anonymous.4open.science/r/Depth3DLane-DCDD.

</details>


### [116] [SSD-Poser: Avatar Pose Estimation with State Space Duality from Sparse Observations](https://arxiv.org/abs/2504.18332)
*Shuting Zhao, Linxin Bai, Liangjing Shao, Ye Zhang, Xinrong Chen*

Main category: cs.CV

TL;DR: SSD-Poser is a lightweight model for real-time full-body pose estimation from HMDs, balancing accuracy and speed with a hybrid encoder and frequency-aware decoder.


<details>
  <summary>Details</summary>
Motivation: The challenge of reconstructing full-body poses from sparse HMD signals, especially the unconstrained lower body, demands efficient solutions without compromising speed or precision.

Method: SSD-Poser uses a hybrid encoder (State Space Attention Encoders) and a Frequency-Aware Decoder to handle complex motion and reduce jitter.

Result: SSD-Poser achieves high accuracy and computational efficiency on the AMASS dataset, outperforming state-of-the-art methods in inference speed.

Conclusion: SSD-Poser effectively addresses the trade-off between precision and speed in full-body pose estimation, making it suitable for real-time AR/VR applications.

Abstract: The growing applications of AR/VR increase the demand for real-time full-body
pose estimation from Head-Mounted Displays (HMDs). Although HMDs provide joint
signals from the head and hands, reconstructing a full-body pose remains
challenging due to the unconstrained lower body. Recent advancements often rely
on conventional neural networks and generative models to improve performance in
this task, such as Transformers and diffusion models. However, these approaches
struggle to strike a balance between achieving precise pose reconstruction and
maintaining fast inference speed. To overcome these challenges, a lightweight
and efficient model, SSD-Poser, is designed for robust full-body motion
estimation from sparse observations. SSD-Poser incorporates a well-designed
hybrid encoder, State Space Attention Encoders, to adapt the state space
duality to complex motion poses and enable real-time realistic pose
reconstruction. Moreover, a Frequency-Aware Decoder is introduced to mitigate
jitter caused by variable-frequency motion signals, remarkably enhancing the
motion smoothness. Comprehensive experiments on the AMASS dataset demonstrate
that SSD-Poser achieves exceptional accuracy and computational efficiency,
showing outstanding inference efficiency compared to state-of-the-art methods.

</details>


### [117] [Revisiting Data Auditing in Large Vision-Language Models](https://arxiv.org/abs/2504.18349)
*Hongyu Zhu, Sichu Liang, Wenwen Wang, Boheng Li, Tongxin Yuan, Fangqi Li, ShiLin Wang, Zhuosheng Zhang*

Main category: cs.CV

TL;DR: Current MI benchmarks for VLMs are flawed due to distribution shifts, leading to inflated performance. New benchmarks reveal MI methods perform poorly under unbiased conditions, but practical scenarios for feasible auditing are identified.


<details>
  <summary>Details</summary>
Motivation: Address concerns over copyright and privacy in VLMs by improving membership inference (MI) techniques for data auditing.

Method: Analyze distribution shifts in MI benchmarks, propose a metric for discrepancy, construct unbiased benchmarks, and explore theoretical limits of MI.

Result: Existing MI methods fail under unbiased conditions, performing near chance. Theoretical analysis shows high irreducible error, but practical scenarios for feasible auditing are identified.

Conclusion: MI for VLMs is challenging due to distribution shifts and high error rates, but practical scenarios offer opportunities for effective auditing.

Abstract: With the surge of large language models (LLMs), Large Vision-Language Models
(VLMs)--which integrate vision encoders with LLMs for accurate visual
grounding--have shown great potential in tasks like generalist agents and
robotic control. However, VLMs are typically trained on massive web-scraped
images, raising concerns over copyright infringement and privacy violations,
and making data auditing increasingly urgent. Membership inference (MI), which
determines whether a sample was used in training, has emerged as a key auditing
technique, with promising results on open-source VLMs like LLaVA (AUC > 80%).
In this work, we revisit these advances and uncover a critical issue: current
MI benchmarks suffer from distribution shifts between member and non-member
images, introducing shortcut cues that inflate MI performance. We further
analyze the nature of these shifts and propose a principled metric based on
optimal transport to quantify the distribution discrepancy. To evaluate MI in
realistic settings, we construct new benchmarks with i.i.d. member and
non-member images. Existing MI methods fail under these unbiased conditions,
performing only marginally better than chance. Further, we explore the
theoretical upper bound of MI by probing the Bayes Optimality within the VLM's
embedding space and find the irreducible error rate remains high. Despite this
pessimistic outlook, we analyze why MI for VLMs is particularly challenging and
identify three practical scenarios--fine-tuning, access to ground-truth texts,
and set-based inference--where auditing becomes feasible. Our study presents a
systematic view of the limits and opportunities of MI for VLMs, providing
guidance for future efforts in trustworthy data auditing.

</details>


### [118] [Interpretable Affordance Detection on 3D Point Clouds with Probabilistic Prototypes](https://arxiv.org/abs/2504.18355)
*Maximilian Xiling Li, Korbinian Rudolf, Nils Blank, Rudolf Lioutikov*

Main category: cs.CV

TL;DR: The paper introduces prototypical learning for affordance detection in 3D point clouds, offering interpretability while matching black-box model performance.


<details>
  <summary>Details</summary>
Motivation: Robotic agents need interpretable methods for understanding object interactions, as traditional deep learning models lack transparency.

Method: The work applies prototypical learning (e.g., ProtoPNet) to 3D point clouds for affordance detection, using the 3D-AffordanceNet dataset.

Result: Prototypical models achieve competitive performance with state-of-the-art black-box models while providing interpretability.

Conclusion: Prototypical models are promising for human-robot interaction due to their balance of performance and transparency.

Abstract: Robotic agents need to understand how to interact with objects in their
environment, both autonomously and during human-robot interactions. Affordance
detection on 3D point clouds, which identifies object regions that allow
specific interactions, has traditionally relied on deep learning models like
PointNet++, DGCNN, or PointTransformerV3. However, these models operate as
black boxes, offering no insight into their decision-making processes.
Prototypical Learning methods, such as ProtoPNet, provide an interpretable
alternative to black-box models by employing a "this looks like that"
case-based reasoning approach. However, they have been primarily applied to
image-based tasks. In this work, we apply prototypical learning to models for
affordance detection on 3D point clouds. Experiments on the 3D-AffordanceNet
benchmark dataset show that prototypical models achieve competitive performance
with state-of-the-art black-box models and offer inherent interpretability.
This makes prototypical models a promising candidate for human-robot
interaction scenarios that require increased trust and safety.

</details>


### [119] [Fast Autoregressive Models for Continuous Latent Generation](https://arxiv.org/abs/2504.18391)
*Tiankai Hang, Jianmin Bao, Fangyun Wei, Dong Chen*

Main category: cs.CV

TL;DR: FAR (Fast AutoRegressive model) replaces MAR's diffusion head with a lightweight shortcut head for faster inference while maintaining performance in continuous-space image generation.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models face challenges in continuous-domain image generation due to slow inference in existing methods like MAR.

Method: FAR replaces MAR's diffusion head with a lightweight shortcut head for efficient few-step sampling and integrates with causal Transformers.

Result: FAR achieves 2.3× faster inference than MAR while maintaining competitive FID and IS scores.

Conclusion: FAR bridges the gap between quality and scalability in autoregressive image generation, offering an efficient paradigm.

Abstract: Autoregressive models have demonstrated remarkable success in sequential data
generation, particularly in NLP, but their extension to continuous-domain image
generation presents significant challenges. Recent work, the masked
autoregressive model (MAR), bypasses quantization by modeling per-token
distributions in continuous spaces using a diffusion head but suffers from slow
inference due to the high computational cost of the iterative denoising
process. To address this, we propose the Fast AutoRegressive model (FAR), a
novel framework that replaces MAR's diffusion head with a lightweight shortcut
head, enabling efficient few-step sampling while preserving autoregressive
principles. Additionally, FAR seamlessly integrates with causal Transformers,
extending them from discrete to continuous token generation without requiring
architectural modifications. Experiments demonstrate that FAR achieves
$2.3\times$ faster inference than MAR while maintaining competitive FID and IS
scores. This work establishes the first efficient autoregressive paradigm for
high-fidelity continuous-space image generation, bridging the critical gap
between quality and scalability in visual autoregressive modeling.

</details>


### [120] [Unsupervised Visual Chain-of-Thought Reasoning via Preference Optimization](https://arxiv.org/abs/2504.18397)
*Kesen Zhao, Beier Zhu, Qianru Sun, Hanwang Zhang*

Main category: cs.CV

TL;DR: UV-CoT introduces an unsupervised framework for visual chain-of-thought reasoning, eliminating the need for labeled bounding-box data and improving generalization.


<details>
  <summary>Details</summary>
Motivation: Existing visual CoT methods rely on supervised fine-tuning with labeled data, limiting generalization. UV-CoT addresses this by using preference optimization.

Method: UV-CoT generates seed bounding boxes, ranks responses via an evaluator MLLM, and trains the target MLLM using preference comparisons.

Result: UV-CoT outperforms state-of-the-art textual and visual CoT methods on six datasets and shows strong zero-shot generalization on four unseen datasets.

Conclusion: UV-CoT enhances visual comprehension and spatial reasoning, offering a scalable and generalizable solution for visual CoT.

Abstract: Chain-of-thought (CoT) reasoning greatly improves the interpretability and
problem-solving abilities of multimodal large language models (MLLMs). However,
existing approaches are focused on text CoT, limiting their ability to leverage
visual cues. Visual CoT remains underexplored, and the only work is based on
supervised fine-tuning (SFT) that relies on extensive labeled bounding-box data
and is hard to generalize to unseen cases. In this paper, we introduce
Unsupervised Visual CoT (UV-CoT), a novel framework for image-level CoT
reasoning via preference optimization. UV-CoT performs preference comparisons
between model-generated bounding boxes (one is preferred and the other is
dis-preferred), eliminating the need for bounding-box annotations. We get such
preference data by introducing an automatic data generation pipeline. Given an
image, our target MLLM (e.g., LLaVA-1.5-7B) generates seed bounding boxes using
a template prompt and then answers the question using each bounded region as
input. An evaluator MLLM (e.g., OmniLLM-12B) ranks the responses, and these
rankings serve as supervision to train the target MLLM with UV-CoT by
minimizing negative log-likelihood losses. By emulating human
perception--identifying key regions and reasoning based on them--UV-CoT can
improve visual comprehension, particularly in spatial reasoning tasks where
textual descriptions alone fall short. Our experiments on six datasets
demonstrate the superiority of UV-CoT, compared to the state-of-the-art textual
and visual CoT methods. Our zero-shot testing on four unseen datasets shows the
strong generalization of UV-CoT. The code is available in
https://github.com/kesenzhao/UV-CoT.

</details>


### [121] [LaRI: Layered Ray Intersections for Single-view 3D Geometric Reasoning](https://arxiv.org/abs/2504.18424)
*Rui Li, Biao Zhang, Zhenyu Li, Federico Tombari, Peter Wonka*

Main category: cs.CV

TL;DR: LaRI is a novel method for reasoning unseen geometry from a single image using layered point maps, outperforming traditional depth estimation by modeling multiple surfaces and enabling efficient geometric reasoning.


<details>
  <summary>Details</summary>
Motivation: Traditional depth estimation methods are limited to visible surfaces, lacking the ability to reason about occluded or unseen geometry. LaRI addresses this by modeling multiple surfaces intersected by camera rays.

Method: LaRI uses layered point maps to represent multiple surfaces and predicts ray stopping indices to identify valid intersections. A training data pipeline is built for synthetic and real-world data.

Result: LaRI achieves comparable object-level results with significantly less training data and parameters than large generative models, and efficiently reasons scene-level occluded geometry in one feed-forward pass.

Conclusion: LaRI is a versatile and efficient method for unseen geometry reasoning, unifying object- and scene-level tasks with minimal resource requirements.

Abstract: We present layered ray intersections (LaRI), a new method for unseen geometry
reasoning from a single image. Unlike conventional depth estimation that is
limited to the visible surface, LaRI models multiple surfaces intersected by
the camera rays using layered point maps. Benefiting from the compact and
layered representation, LaRI enables complete, efficient, and view-aligned
geometric reasoning to unify object- and scene-level tasks. We further propose
to predict the ray stopping index, which identifies valid intersecting pixels
and layers from LaRI's output. We build a complete training data generation
pipeline for synthetic and real-world data, including 3D objects and scenes,
with necessary data cleaning steps and coordination between rendering engines.
As a generic method, LaRI's performance is validated in two scenarios: It
yields comparable object-level results to the recent large generative model
using 4% of its training data and 17% of its parameters. Meanwhile, it achieves
scene-level occluded geometry reasoning in only one feed-forward.

</details>


### [122] [NoiseController: Towards Consistent Multi-view Video Generation via Noise Decomposition and Collaboration](https://arxiv.org/abs/2504.18448)
*Haotian Dong, Xin Wang, Di Lin, Yipeng Wu, Qin Chen, Ruonan Liu, Kairui Yang, Ping Li, Qing Guo*

Main category: cs.CV

TL;DR: The paper introduces NoiseController, a method to improve spatiotemporal consistency in video generation by decomposing noise into scene-level and individual-level components and using collaboration matrices for enhanced quality.


<details>
  <summary>Details</summary>
Motivation: Generating high-quality videos with spatiotemporal consistency is challenging; current methods lack global spatiotemporal information.

Method: NoiseController uses Multi-Level Noise Decomposition, Multi-Frame Noise Collaboration, and Joint Denoising to enhance consistency.

Result: The method achieves state-of-the-art performance on public datasets for video generation and downstream tasks.

Conclusion: NoiseController effectively improves spatiotemporal consistency in video generation, outperforming existing methods.

Abstract: High-quality video generation is crucial for many fields, including the film
industry and autonomous driving. However, generating videos with spatiotemporal
consistencies remains challenging. Current methods typically utilize attention
mechanisms or modify noise to achieve consistent videos, neglecting global
spatiotemporal information that could help ensure spatial and temporal
consistency during video generation. In this paper, we propose the
NoiseController, consisting of Multi-Level Noise Decomposition, Multi-Frame
Noise Collaboration, and Joint Denoising, to enhance spatiotemporal
consistencies in video generation. In multi-level noise decomposition, we first
decompose initial noises into scene-level foreground/background noises,
capturing distinct motion properties to model multi-view foreground/background
variations. Furthermore, each scene-level noise is further decomposed into
individual-level shared and residual components. The shared noise preserves
consistency, while the residual component maintains diversity. In multi-frame
noise collaboration, we introduce an inter-view spatiotemporal collaboration
matrix and an intra-view impact collaboration matrix , which captures mutual
cross-view effects and historical cross-frame impacts to enhance video quality.
The joint denoising contains two parallel denoising U-Nets to remove each
scene-level noise, mutually enhancing video generation. We evaluate our
NoiseController on public datasets focusing on video generation and downstream
tasks, demonstrating its state-of-the-art performance.

</details>


### [123] [RGS-DR: Reflective Gaussian Surfels with Deferred Rendering for Shiny Objects](https://arxiv.org/abs/2504.18468)
*Georgios Kouros, Minye Wu, Tinne Tuytelaars*

Main category: cs.CV

TL;DR: RGS-DR is a new inverse rendering method for glossy/reflective objects, excelling in relighting and scene editing by using 2D Gaussian surfels and deferred shading.


<details>
  <summary>Details</summary>
Motivation: Existing methods like NeRF and 3D Gaussian Splatting struggle with view-dependent effects, prompting the need for a better solution.

Method: Uses 2D Gaussian surfels for geometry/normal estimation, deferred shading, multi-level cube mipmap for lighting, and a residual pass for appearance refinement.

Result: Achieves high-quality reconstruction and rendering for shiny objects, outperforming methods that lack relighting capabilities.

Conclusion: RGS-DR is effective for glossy/reflective object reconstruction and relighting, surpassing current state-of-the-art techniques.

Abstract: We introduce RGS-DR, a novel inverse rendering method for reconstructing and
rendering glossy and reflective objects with support for flexible relighting
and scene editing. Unlike existing methods (e.g., NeRF and 3D Gaussian
Splatting), which struggle with view-dependent effects, RGS-DR utilizes a 2D
Gaussian surfel representation to accurately estimate geometry and surface
normals, an essential property for high-quality inverse rendering. Our approach
explicitly models geometric and material properties through learnable
primitives rasterized into a deferred shading pipeline, effectively reducing
rendering artifacts and preserving sharp reflections. By employing a
multi-level cube mipmap, RGS-DR accurately approximates environment lighting
integrals, facilitating high-quality reconstruction and relighting. A residual
pass with spherical-mipmap-based directional encoding further refines the
appearance modeling. Experiments demonstrate that RGS-DR achieves high-quality
reconstruction and rendering quality for shiny objects, often outperforming
reconstruction-exclusive state-of-the-art methods incapable of relighting.

</details>


### [124] [An Improved ResNet50 Model for Predicting Pavement Condition Index (PCI) Directly from Pavement Images](https://arxiv.org/abs/2504.18490)
*Andrews Danyo, Anthony Dontoh, Armstrong Aboah*

Main category: cs.CV

TL;DR: An enhanced ResNet50 with CBAM improves PCI prediction from pavement images, reducing MAPE to 58.16% compared to baseline models.


<details>
  <summary>Details</summary>
Motivation: Accurate PCI prediction from images is vital for infrastructure maintenance, requiring better feature prioritization.

Method: Enhanced ResNet50 integrated with CBAM for autonomous feature prioritization in pavement images.

Result: ResNet50-CBAM achieved 58.16% MAPE, outperforming baseline ResNet50 (70.76%) and DenseNet161 (65.48%).

Conclusion: Attention mechanisms like CBAM refine feature extraction, enhancing automated pavement condition assessment.

Abstract: Accurately predicting the Pavement Condition Index (PCI), a measure of
roadway conditions, from pavement images is crucial for infrastructure
maintenance. This study proposes an enhanced version of the Residual Network
(ResNet50) architecture, integrated with a Convolutional Block Attention Module
(CBAM), to predict PCI directly from pavement images without additional
annotations. By incorporating CBAM, the model autonomously prioritizes critical
features within the images, improving prediction accuracy. Compared to the
original baseline ResNet50 and DenseNet161 architectures, the enhanced
ResNet50-CBAM model achieved a significantly lower mean absolute percentage
error (MAPE) of 58.16%, compared to the baseline models that achieved 70.76%
and 65.48% respectively. These results highlight the potential of using
attention mechanisms to refine feature extraction, ultimately enabling more
accurate and efficient assessments of pavement conditions. This study
emphasizes the importance of targeted feature refinement in advancing automated
pavement analysis through attention mechanisms.

</details>


### [125] [Eval3D: Interpretable and Fine-grained Evaluation for 3D Generation](https://arxiv.org/abs/2504.18509)
*Shivam Duggal, Yushi Hu, Oscar Michel, Aniruddha Kembhavi, William T. Freeman, Noah A. Smith, Ranjay Krishna, Antonio Torralba, Ali Farhadi, Wei-Chiu Ma*

Main category: cs.CV

TL;DR: Eval3D is a fine-grained, interpretable tool for evaluating 3D asset quality, addressing gaps in current metrics by leveraging diverse models for consistency measurement.


<details>
  <summary>Details</summary>
Motivation: Current 3D evaluation metrics lack geometric quality assessment and rely on coarse methods, necessitating a more reliable tool.

Method: Eval3D uses diverse foundation models and tools to measure semantic and geometric consistency in 3D assets.

Result: Eval3D provides pixel-wise measurement, accurate spatial feedback, and aligns better with human judgments than prior work.

Conclusion: Eval3D highlights limitations in current 3D generation models and offers a more comprehensive evaluation approach.

Abstract: Despite the unprecedented progress in the field of 3D generation, current
systems still often fail to produce high-quality 3D assets that are visually
appealing and geometrically and semantically consistent across multiple
viewpoints. To effectively assess the quality of the generated 3D data, there
is a need for a reliable 3D evaluation tool. Unfortunately, existing 3D
evaluation metrics often overlook the geometric quality of generated assets or
merely rely on black-box multimodal large language models for coarse
assessment. In this paper, we introduce Eval3D, a fine-grained, interpretable
evaluation tool that can faithfully evaluate the quality of generated 3D assets
based on various distinct yet complementary criteria. Our key observation is
that many desired properties of 3D generation, such as semantic and geometric
consistency, can be effectively captured by measuring the consistency among
various foundation models and tools. We thus leverage a diverse set of models
and tools as probes to evaluate the inconsistency of generated 3D assets across
different aspects. Compared to prior work, Eval3D provides pixel-wise
measurement, enables accurate 3D spatial feedback, and aligns more closely with
human judgments. We comprehensively evaluate existing 3D generation models
using Eval3D and highlight the limitations and challenges of current models.

</details>


### [126] [Examining the Impact of Optical Aberrations to Image Classification and Object Detection Models](https://arxiv.org/abs/2504.18510)
*Patrick Müller, Alexander Braun, Margret Keuper*

Main category: cs.CV

TL;DR: The paper introduces two datasets, OpticsBench and LensCorruptions, to evaluate DNN robustness against realistic optical blur effects, showing significant performance variations across models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for evaluating model robustness to blur are overly simplistic and ignore realistic optical blur effects.

Method: Proposes two datasets: OpticsBench (primary aberrations) and LensCorruptions (real lens effects). Evaluates pre-trained models on ImageNet and MSCOCO.

Result: Performance varies significantly across models, highlighting the need for realistic blur corruption benchmarks.

Conclusion: Realistic blur corruptions are crucial for accurately assessing model robustness in vision tasks.

Abstract: Deep neural networks (DNNs) have proven to be successful in various computer
vision applications such that models even infer in safety-critical situations.
Therefore, vision models have to behave in a robust way to disturbances such as
noise or blur. While seminal benchmarks exist to evaluate model robustness to
diverse corruptions, blur is often approximated in an overly simplistic way to
model defocus, while ignoring the different blur kernel shapes that result from
optical systems. To study model robustness against realistic optical blur
effects, this paper proposes two datasets of blur corruptions, which we denote
OpticsBench and LensCorruptions. OpticsBench examines primary aberrations such
as coma, defocus, and astigmatism, i.e. aberrations that can be represented by
varying a single parameter of Zernike polynomials. To go beyond the principled
but synthetic setting of primary aberrations, LensCorruptions samples linear
combinations in the vector space spanned by Zernike polynomials, corresponding
to 100 real lenses. Evaluations for image classification and object detection
on ImageNet and MSCOCO show that for a variety of different pre-trained models,
the performance on OpticsBench and LensCorruptions varies significantly,
indicating the need to consider realistic image corruptions to evaluate a
model's robustness against blur.

</details>


### [127] [E-VLC: A Real-World Dataset for Event-based Visible Light Communication And Localization](https://arxiv.org/abs/2504.18521)
*Shintaro Shiba, Quan Kong, Norimasa Kobori*

Main category: cs.CV

TL;DR: The paper introduces the first public dataset for benchmarking LED signal decoding and localization using event cameras, alongside a novel localization method leveraging Contrast Maximization.


<details>
  <summary>Details</summary>
Motivation: There's no existing public dataset for benchmarking LED signal decoding and localization with event cameras in real-world settings.

Method: The dataset includes synchronized event and frame cameras with ground-truth poses. A novel localization method uses Contrast Maximization for motion estimation.

Result: The proposed method outperforms conventional AR-marker-based localization, demonstrating efficacy in LED-based localization.

Conclusion: The dataset and method aim to benchmark and advance event camera applications in mobile devices.

Abstract: Optical communication using modulated LEDs (e.g., visible light
communication) is an emerging application for event cameras, thanks to their
high spatio-temporal resolutions. Event cameras can be used simply to decode
the LED signals and also to localize the camera relative to the LED marker
positions. However, there is no public dataset to benchmark the decoding and
localization in various real-world settings. We present, to the best of our
knowledge, the first public dataset that consists of an event camera, a frame
camera, and ground-truth poses that are precisely synchronized with hardware
triggers. It provides various camera motions with various sensitivities in
different scene brightness settings, both indoor and outdoor. Furthermore, we
propose a novel method of localization that leverages the Contrast Maximization
framework for motion estimation and compensation. The detailed analysis and
experimental results demonstrate the advantages of LED-based localization with
events over the conventional AR-marker--based one with frames, as well as the
efficacy of the proposed method in localization. We hope that the proposed
dataset serves as a future benchmark for both motion-related classical computer
vision tasks and LED marker decoding tasks simultaneously, paving the way to
broadening applications of event cameras on mobile devices.
https://woven-visionai.github.io/evlc-dataset

</details>


### [128] [Augmenting Perceptual Super-Resolution via Image Quality Predictors](https://arxiv.org/abs/2504.18524)
*Fengjia Zhang, Samrudhdhi B. Rangrej, Tristan Aumentado-Armstrong, Afsaneh Fazly, Alex Levinshtein*

Main category: cs.CV

TL;DR: The paper explores using non-reference image quality assessment (NR-IQA) models to improve super-resolution (SR) by focusing on human-centric quality over pixel-wise accuracy.


<details>
  <summary>Details</summary>
Motivation: SR is ill-posed, with multiple plausible solutions. Traditional methods optimize for pixel-wise error, but human perception favors higher-quality samples. The paper aims to leverage NR-IQA models for better perceptual results.

Method: Two approaches are tested: (i) altering data sampling in a multi-ground-truth SR framework, and (ii) directly optimizing a differentiable quality score using NR-IQA metrics.

Result: The methods achieve a better balance between perceptual fidelity and human-tuned NR-IQA measures, improving the perception-distortion tradeoff.

Conclusion: NR-IQA models offer a promising alternative for SR, shifting focus from pixel-wise distortion to human-centric quality.

Abstract: Super-resolution (SR), a classical inverse problem in computer vision, is
inherently ill-posed, inducing a distribution of plausible solutions for every
input. However, the desired result is not simply the expectation of this
distribution, which is the blurry image obtained by minimizing pixelwise error,
but rather the sample with the highest image quality. A variety of techniques,
from perceptual metrics to adversarial losses, are employed to this end. In
this work, we explore an alternative: utilizing powerful non-reference image
quality assessment (NR-IQA) models in the SR context. We begin with a
comprehensive analysis of NR-IQA metrics on human-derived SR data, identifying
both the accuracy (human alignment) and complementarity of different metrics.
Then, we explore two methods of applying NR-IQA models to SR learning: (i)
altering data sampling, by building on an existing multi-ground-truth SR
framework, and (ii) directly optimizing a differentiable quality score. Our
results demonstrate a more human-centric perception-distortion tradeoff,
focusing less on non-perceptual pixel-wise distortion, instead improving the
balance between perceptual fidelity and human-tuned NR-IQA measures.

</details>


### [129] [M4U: Evaluating Multilingual Understanding and Reasoning for Large Multimodal Models](https://arxiv.org/abs/2405.15638)
*Hongyu Wang, Jiayu Xu, Senwei Xie, Ruiping Wang, Jialin Li, Zhaojie Xie, Bin Zhang, Chuyan Xiong, Xilin Chen*

Main category: cs.CV

TL;DR: The paper introduces M4U, a challenging benchmark for evaluating multilingual multimodal understanding and reasoning, revealing limitations in current models like GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to adequately assess multilingual multimodal models, leaving their performance unexplored.

Method: M4U, a new benchmark with 10k samples across 64 disciplines in six languages, is used to evaluate leading models.

Result: GPT-4o achieves only 47.6% accuracy on M4U, and models show language biases and struggle with cross-lingual reasoning.

Conclusion: Current models, including GPT-4o, have significant limitations in multilingual multimodal reasoning, highlighting the need for improvement.

Abstract: Multilingual capability is an essential aspect for large multimodal models,
since they are usually deployed across various countries and languages.
However, most existing benchmarks for multilingual multimodal reasoning
struggle to differentiate between models of varying performance; even language
models without visual capabilities can easily achieve high scores. This leaves
a comprehensive evaluation of leading multilingual multimodal models largely
unexplored. In this work, we introduce M4U, a novel and challenging benchmark
for assessing the capability of multi-discipline multilingual multimodal
understanding and reasoning. M4U contains 10k samples covering 64 disciplines
across 16 subfields in Science, Engineering, and Healthcare in six languages.
Using M4U, we conduct extensive evaluations of leading Large Multimodal Models
(LMMs) and Large Language Models (LLMs) with external tools. The evaluation
results demonstrate that the state-of-the-art model, GPT-4o, achieves only
47.6% average accuracy on M4U. Additionally, we observe that the leading LMMs
exhibit significant language preferences. Our in-depth analysis indicates that
leading LMMs, including GPT-4o, struggle to perform reasoning using
multilingual information present in both visual and textual context.
Specifically, they suffer performance degradation when prompted with
cross-lingual multimodal questions. Our code and dataset is public available.

</details>


### [130] [TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation](https://arxiv.org/abs/2504.17365)
*Ling You, Wenxuan Huang, Xinni Xie, Xiangyi Wei, Bangyan Li, Shaohui Lin, Yang Li, Changbo Wang*

Main category: cs.CV

TL;DR: TimeSoccer is an end-to-end MLLM for soccer video captioning, combining timestamp prediction and caption generation in one pass, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing soccer MLLMs rely on temporal priors or complex two-step approaches, lacking end-to-end processing and global context modeling for full-match videos.

Method: TimeSoccer uses MoFA-Select for motion-aware frame compression and joint timestamp-caption prediction, enabling efficient long-video understanding.

Result: TimeSoccer achieves SoTA performance in SDVC, producing high-quality, temporally aligned commentary.

Conclusion: TimeSoccer advances soccer video analysis by offering an efficient, end-to-end solution for dense video captioning.

Abstract: Soccer is a globally popular sporting event, typically characterized by long
matches and distinctive highlight moments. Recent advances in Multimodal Large
Language Models (MLLMs) offer promising capabilities in temporal grounding and
video understanding, soccer commentary generation often requires precise
temporal localization and semantically rich descriptions over long-form video.
However, existing soccer MLLMs often rely on the temporal a priori for caption
generation, so they cannot process the soccer video end-to-end. While some
traditional approaches follow a two-step paradigm that is complex and fails to
capture the global context to achieve suboptimal performance. To solve the
above issues, we present TimeSoccer, the first end-to-end soccer MLLM for
Single-anchor Dense Video Captioning (SDVC) in full-match soccer videos.
TimeSoccer jointly predicts timestamps and generates captions in a single pass,
enabling global context modeling across 45-minute matches. To support long
video understanding of soccer matches, we introduce MoFA-Select, a
training-free, motion-aware frame compression module that adaptively selects
representative frames via a coarse-to-fine strategy, and incorporates
complementary training paradigms to strengthen the model's ability to handle
long temporal sequences. Extensive experiments demonstrate that our TimeSoccer
achieves State-of-The-Art (SoTA) performance on the SDVC task in an end-to-end
form, generating high-quality commentary with accurate temporal alignment and
strong semantic relevance.

</details>


### [131] [Treating Motion as Option with Output Selection for Unsupervised Video Object Segmentation](https://arxiv.org/abs/2309.14786)
*Suhwan Cho, Minhyeok Lee, Jungho Lee, MyeongAh Cho, Seungwook Park, Jaeyeob Kim, Hyunsung Jang, Sangyoun Lee*

Main category: cs.CV

TL;DR: A novel motion-as-option network reduces reliance on motion cues in unsupervised video object segmentation by treating them as optional, improving prediction stability.


<details>
  <summary>Details</summary>
Motivation: Current methods overly depend on motion cues from optical flow, leading to unstable predictions when motion cues are confusing.

Method: Proposes a motion-as-option network that randomly uses RGB images or optical flow maps during training, reducing motion dependency. Introduces an adaptive output selection algorithm for optimal predictions.

Result: The network processes both RGB and optical flow inputs, producing two distinct predictions, enhancing flexibility and robustness.

Conclusion: The approach mitigates motion cue dependency, improving segmentation stability and performance in challenging scenarios.

Abstract: Unsupervised video object segmentation aims to detect the most salient object
in a video without any external guidance regarding the object. Salient objects
often exhibit distinctive movements compared to the background, and recent
methods leverage this by combining motion cues from optical flow maps with
appearance cues from RGB images. However, because optical flow maps are often
closely correlated with segmentation masks, networks can become overly
dependent on motion cues during training, leading to vulnerability when faced
with confusing motion cues and resulting in unstable predictions. To address
this challenge, we propose a novel motion-as-option network that treats motion
cues as an optional component rather than a necessity. During training, we
randomly input RGB images into the motion encoder instead of optical flow maps,
which implicitly reduces the network's reliance on motion cues. This design
ensures that the motion encoder is capable of processing both RGB images and
optical flow maps, leading to two distinct predictions depending on the type of
input provided. To make the most of this flexibility, we introduce an adaptive
output selection algorithm that determines the optimal prediction during
testing.

</details>


### [132] [StoryGPT-V: Large Language Models as Consistent Story Visualizers](https://arxiv.org/abs/2312.02252)
*Xiaoqian Shen, Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: StoryGPT-V combines latent diffusion models and LLMs to generate consistent, high-quality story visualizations by resolving ambiguous references and ensuring character consistency.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of story visualization, which requires resolving pronouns and maintaining character/background consistency across frames.

Method: Uses a character-aware LDM with character-augmented embeddings and aligns LLM outputs with these embeddings to leverage LLM reasoning for ambiguous references.

Result: Superior performance on benchmarks, generating accurate, high-quality characters with low memory usage.

Conclusion: StoryGPT-V effectively integrates LLM reasoning with LDM for consistent and high-quality story visualization.

Abstract: Recent generative models have demonstrated impressive capabilities in
generating realistic and visually pleasing images grounded on textual prompts.
Nevertheless, a significant challenge remains in applying these models for the
more intricate task of story visualization. Since it requires resolving
pronouns (he, she, they) in the frame descriptions, i.e., anaphora resolution,
and ensuring consistent characters and background synthesis across frames. Yet,
the emerging Large Language Model (LLM) showcases robust reasoning abilities to
navigate through ambiguous references and process extensive sequences.
Therefore, we introduce \emph{StoryGPT-V}, which leverages the merits of the
latent diffusion (LDM) and LLM to produce images with consistent and
high-quality characters grounded on given story descriptions. First, we train a
character-aware LDM, which takes character-augmented semantic embedding as
input and includes the supervision of the cross-attention map using character
segmentation masks, aiming to enhance character generation accuracy and
faithfulness. In the second stage, we enable an alignment between the output of
LLM and the character-augmented embedding residing in the input space of the
first-stage model. This harnesses the reasoning ability of LLM to address
ambiguous references and the comprehension capability to memorize the context.
We conduct comprehensive experiments on two visual story visualization
benchmarks. Our model reports superior quantitative results and consistently
generates accurate characters of remarkable quality with low memory
consumption. Our code is publicly available at:
\href{https://xiaoqian-shen.github.io/StoryGPT-V}{https://xiaoqian-shen.github.io/StoryGPT-V}.

</details>


### [133] [All for One, and One for All: UrbanSyn Dataset, the third Musketeer of Synthetic Driving Scenes](https://arxiv.org/abs/2312.12176)
*Jose L. Gómez, Manuel Silva, Antonio Seoane, Agnès Borrás, Mario Noriega, Germán Ros, Jose A. Iglesias-Guitian, Antonio M. López*

Main category: cs.CV

TL;DR: UrbanSyn is a synthetic urban driving dataset with pixel-level ground truth, complementing GTAV and Synscapes. It aids unsupervised domain adaptation for semantic segmentation, achieving new benchmarks on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: To provide a high-quality synthetic dataset for urban driving scenarios, addressing gaps in existing datasets like GTAV and Synscapes, and improving domain adaptation for semantic segmentation.

Method: UrbanSyn is created using semi-procedurally generated synthetic urban scenarios with high-quality geometry and materials, offering depth, semantic segmentation, and instance segmentation with occlusion details.

Result: UrbanSyn, combined with GTAV and Synscapes ('Three Musketeers'), sets new benchmarks on Cityscapes, Mapillary Vistas, and BDD100K for unsupervised domain adaptation.

Conclusion: UrbanSyn is a valuable addition to synthetic datasets, enhancing domain adaptation performance and made freely available for research.

Abstract: We introduce UrbanSyn, a photorealistic dataset acquired through
semi-procedurally generated synthetic urban driving scenarios. Developed using
high-quality geometry and materials, UrbanSyn provides pixel-level ground
truth, including depth, semantic segmentation, and instance segmentation with
object bounding boxes and occlusion degree. It complements GTAV and Synscapes
datasets to form what we coin as the 'Three Musketeers'. We demonstrate the
value of the Three Musketeers in unsupervised domain adaptation for image
semantic segmentation. Results on real-world datasets, Cityscapes, Mapillary
Vistas, and BDD100K, establish new benchmarks, largely attributed to UrbanSyn.
We make UrbanSyn openly and freely accessible (www.urbansyn.org).

</details>


### [134] [Fast Orthogonal Matching Pursuit through Successive Regression](https://arxiv.org/abs/2404.00146)
*Huiyuan Yu, Jia He, Maggie Cheng*

Main category: cs.CV

TL;DR: The paper proposes fast algorithms to improve the computational efficiency of OMP and gOMP for sparse signal recovery, reducing complexity and time while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: OMP and gOMP face computational challenges with large non-zero signals, prompting the need for faster algorithms.

Method: The paper introduces modifications to OMP and gOMP, focusing on fast orthogonal projection techniques to reduce computational complexity.

Result: Experiments show reduced computation time, with approximation error comparable to original OMP/gOMP. Sufficient conditions for exact recovery are also provided.

Conclusion: The proposed algorithms significantly enhance the speed of OMP and gOMP without sacrificing recovery accuracy.

Abstract: Orthogonal Matching Pursuit (OMP) has been a powerful method in sparse signal
recovery and approximation. However, OMP suffers computational issues when the
signal has a large number of non-zeros. This paper advances OMP and its
extension called generalized OMP (gOMP) by offering fast algorithms for the
orthogonal projection of the input signal at each iteration. The proposed
modifications directly reduce the computational complexity of OMP and gOMP.
Experiment results verified the improvement in computation time. This paper
also provides sufficient conditions for exact signal recovery. For general
signals with additive noise, the approximation error is at the same order as
OMP (gOMP), but is obtained within much less time.

</details>


### [135] [CoCoGesture: Toward Coherent Co-speech 3D Gesture Generation in the Wild](https://arxiv.org/abs/2405.16874)
*Xingqun Qi, Hengyuan Zhang, Yatian Wang, Jiahao Pan, Chen Liu, Peng Li, Xiaowei Chi, Mengfei Li, Wei Xue, Shanghang Zhang, Wenhan Luo, Qifeng Liu, Yike Guo*

Main category: cs.CV

TL;DR: CoCoGesture is a novel framework for generating vivid and diverse 3D gestures from unseen human speech, using a pretrain-finetune paradigm and a large-scale dataset (GES-X).


<details>
  <summary>Details</summary>
Motivation: Existing methods produce stiff gestures due to limited 3D speech-gesture data. CoCoGesture aims to overcome this by leveraging a large dataset and advanced training techniques.

Method: Uses a pretrain-finetune paradigm: pretrains a 1B-parameter gesture diffusion model on GES-X, then finetunes with audio ControlNet and Mixture-of-Gesture-Experts (MoGE) for speech-conditioned gesture generation.

Result: Outperforms state-of-the-art methods in zero-shot speech-to-gesture generation, producing more vivid and diverse gestures.

Conclusion: CoCoGesture advances gesture synthesis by addressing data scarcity and improving generation quality, with the dataset (GES-X) made publicly available.

Abstract: Deriving co-speech 3D gestures has seen tremendous progress in virtual avatar
animation. Yet, the existing methods often produce stiff and unreasonable
gestures with unseen human speech inputs due to the limited 3D speech-gesture
data. In this paper, we propose CoCoGesture, a novel framework enabling vivid
and diverse gesture synthesis from unseen human speech prompts. Our key insight
is built upon the custom-designed pretrain-fintune training paradigm. At the
pretraining stage, we aim to formulate a large generalizable gesture diffusion
model by learning the abundant postures manifold. Therefore, to alleviate the
scarcity of 3D data, we first construct a large-scale co-speech 3D gesture
dataset containing more than 40M meshed posture instances across 4.3K speakers,
dubbed GES-X. Then, we scale up the large unconditional diffusion model to 1B
parameters and pre-train it to be our gesture experts. At the finetune stage,
we present the audio ControlNet that incorporates the human voice as condition
prompts to guide the gesture generation. Here, we construct the audio
ControlNet through a trainable copy of our pre-trained diffusion model.
Moreover, we design a novel Mixture-of-Gesture-Experts (MoGE) block to
adaptively fuse the audio embedding from the human speech and the gesture
features from the pre-trained gesture experts with a routing mechanism. Such an
effective manner ensures audio embedding is temporal coordinated with motion
features while preserving the vivid and diverse gesture generation. Extensive
experiments demonstrate that our proposed CoCoGesture outperforms the
state-of-the-art methods on the zero-shot speech-to-gesture generation. The
dataset will be publicly available at: https://mattie-e.github.io/GES-X/

</details>


### [136] [Towards Synchronous Memorizability and Generalizability with Site-Modulated Diffusion Replay for Cross-Site Continual Segmentation](https://arxiv.org/abs/2406.18037)
*Dunyuan Xu, Xi Wang, Jingyang Zhang, Pheng-Ann Heng*

Main category: cs.CV

TL;DR: The paper proposes SMG-Learning, a method combining memorizability and generalizability for sequential medical image diagnosis, using Parallel Gradient Alignment (PGA) and Site-Modulated Diffusion (SMD).


<details>
  <summary>Details</summary>
Motivation: Addressing catastrophic forgetting and poor generalizability in sequential learning for medical images due to privacy and storage constraints.

Method: Introduces PGA for gradient alignment and SMD for generating site-specific images to replay past data.

Result: Outperforms state-of-the-art methods in enhancing memorizability and generalizability across sites.

Conclusion: SMG-Learning effectively balances memorization and generalization, achieving strong performance in sequential medical image tasks.

Abstract: The ability to learn sequentially from different data sites is crucial for a
deep network in solving practical medical image diagnosis problems due to
privacy restrictions and storage limitations. However, adapting on incoming
site leads to catastrophic forgetting on past sites and decreases
generalizablity on unseen sites. Existing Continual Learning (CL) and Domain
Generalization (DG) methods have been proposed to solve these two challenges
respectively, but none of them can address both simultaneously. Recognizing
this limitation, this paper proposes a novel training paradigm, learning
towards Synchronous Memorizability and Generalizability (SMG-Learning). To
achieve this, we create the orientational gradient alignment to ensure
memorizability on previous sites, and arbitrary gradient alignment to enhance
generalizability on unseen sites. This approach is named as Parallel Gradient
Alignment (PGA). Furthermore, we approximate the PGA as dual meta-objectives
using the first-order Taylor expansion to reduce computational cost of aligning
gradients. Considering that performing gradient alignments, especially for
previous sites, is not feasible due to the privacy constraints, we design a
Site-Modulated Diffusion (SMD) model to generate images with site-specific
learnable prompts, replaying images have similar data distributions as previous
sites. We evaluate our method on two medical image segmentation tasks, where
data from different sites arrive sequentially. Experimental results show that
our method efficiently enhances both memorizability and generalizablity better
than other state-of-the-art methods, delivering satisfactory performance across
all sites. Our code will be available at:
https://github.com/dyxu-cuhkcse/SMG-Learning.

</details>


### [137] [Multi-view Hand Reconstruction with a Point-Embedded Transformer](https://arxiv.org/abs/2408.10581)
*Lixin Yang, Licheng Zhong, Pengxiang Zhu, Xinyu Zhan, Junxiao Kong, Jian Xu, Cewu Lu*

Main category: cs.CV

TL;DR: POEM is a multi-view Hand Mesh Reconstruction model using 3D basis points for real-world hand motion capture, trained on diverse datasets for generalizability.


<details>
  <summary>Details</summary>
Motivation: To create a practical, generalizable solution for real-world hand motion capture by leveraging multi-view stereo and 3D basis points.

Method: Embeds static basis points in multi-view stereo to represent hand mesh, trained on five large-scale datasets with randomized camera configurations.

Result: POEM achieves high generalizability and practicality, offering a plug-and-play solution for multi-view hand motion capture.

Conclusion: POEM provides an effective, user-friendly, and cost-efficient method for hand motion capture, with publicly available model and code.

Abstract: This work introduces a novel and generalizable multi-view Hand Mesh
Reconstruction (HMR) model, named POEM, designed for practical use in
real-world hand motion capture scenarios. The advances of the POEM model
consist of two main aspects. First, concerning the modeling of the problem, we
propose embedding a static basis point within the multi-view stereo space. A
point represents a natural form of 3D information and serves as an ideal medium
for fusing features across different views, given its varied projections across
these views. Consequently, our method harnesses a simple yet effective idea: a
complex 3D hand mesh can be represented by a set of 3D basis points that 1) are
embedded in the multi-view stereo, 2) carry features from the multi-view
images, and 3) encompass the hand in it. The second advance lies in the
training strategy. We utilize a combination of five large-scale multi-view
datasets and employ randomization in the number, order, and poses of the
cameras. By processing such a vast amount of data and a diverse array of camera
configurations, our model demonstrates notable generalizability in the
real-world applications. As a result, POEM presents a highly practical,
plug-and-play solution that enables user-friendly, cost-effective multi-view
motion capture for both left and right hands. The model and source codes are
available at https://github.com/JubSteven/POEM-v2.

</details>


### [138] [Understanding Depth and Height Perception in Large Visual-Language Models](https://arxiv.org/abs/2408.11748)
*Shehreen Azad, Yash Jain, Rishit Garg, Yogesh S Rawat, Vibhav Vineet*

Main category: cs.CV

TL;DR: The paper evaluates the geometric understanding of Vision Language Models (VLMs), focusing on depth and height perception, and introduces GeoMeter, a benchmark suite to assess these capabilities.


<details>
  <summary>Details</summary>
Motivation: Despite VLMs' impressive capabilities, their geometric understanding, especially depth and height perception, remains unclear and is crucial for real-world applications.

Method: The study introduces GeoMeter, a benchmark dataset for 2D and 3D scenarios, and evaluates 18 state-of-the-art VLMs.

Result: VLMs perform well in basic geometric properties (shape, size) but struggle with depth and height perception due to reasoning limitations and biases.

Conclusion: The study highlights the need for improved depth and height perception in VLMs to enhance their practical utility in visual perception tasks.

Abstract: Geometric understanding - including depth and height perception - is
fundamental to intelligence and crucial for navigating our environment. Despite
the impressive capabilities of large Vision Language Models (VLMs), it remains
unclear how well they possess the geometric understanding required for
practical applications in visual perception. In this work, we focus on
evaluating the geometric understanding of these models, specifically targeting
their ability to perceive the depth and height of objects in an image. To
address this, we introduce GeoMeter, a suite of benchmark datasets -
encompassing 2D and 3D scenarios - to rigorously evaluate these aspects. By
benchmarking 18 state-of-the-art VLMs, we found that although they excel in
perceiving basic geometric properties like shape and size, they consistently
struggle with depth and height perception. Our analysis reveal that these
challenges stem from shortcomings in their depth and height reasoning
capabilities and inherent biases. This study aims to pave the way for
developing VLMs with enhanced geometric understanding by emphasizing depth and
height perception as critical components necessary for real-world applications.

</details>


### [139] [FungiTastic: A multi-modal dataset and benchmark for image categorization](https://arxiv.org/abs/2408.13632)
*Lukas Picek, Klara Janouskova, Vojtech Cermak, Jiri Matas*

Main category: cs.CV

TL;DR: FungiTastic is a new benchmark dataset with 350k multimodal fungal observations, offering high reliability and diverse tasks like classification and few-shot learning.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive, reliable, and multimodal fungal dataset for advancing research in various machine learning tasks.

Method: Curated 350k labeled fungal observations with photos, climatic data, and DNA-sequenced ground truth, supporting multiple learning tasks.

Result: A robust dataset with expert labels, diverse modalities, and pre-trained models for various applications.

Conclusion: FungiTastic is a valuable resource for machine learning research, offering high-quality data and tools for diverse tasks.

Abstract: We introduce a new, challenging benchmark and a dataset, FungiTastic, based
on fungal records continuously collected over a twenty-year span. The dataset
is labelled and curated by experts and consists of about 350k multimodal
observations of 6k fine-grained categories (species). The fungi observations
include photographs and additional data, e.g., meteorological and climatic
data, satellite images, and body part segmentation masks. FungiTastic is one of
the few benchmarks that include a test set with DNA-sequenced ground truth of
unprecedented label reliability. The benchmark is designed to support (i)
standard closed-set classification, (ii) open-set classification, (iii)
multi-modal classification, (iv) few-shot learning, (v) domain shift, and many
more. We provide tailored baselines for many use cases, a multitude of
ready-to-use pre-trained models on
https://huggingface.co/collections/BVRA/fungitastic-66a227ce0520be533dc6403b,
and a framework for model training. The documentation and the baselines are
available at https://github.com/BohemianVRA/FungiTastic/ and
https://www.kaggle.com/datasets/picekl/fungitastic.

</details>


### [140] [Neuro-Symbolic Evaluation of Text-to-Video Models using Formal Verification](https://arxiv.org/abs/2411.16718)
*S P Sharan, Minkyu Choi, Sahil Shah, Harsh Goel, Mohammad Omama, Sandeep Chinchali*

Main category: cs.CV

TL;DR: NeuS-V is a new metric for evaluating text-to-video alignment in synthetic videos, outperforming existing metrics by 5x in human correlation.


<details>
  <summary>Details</summary>
Motivation: Current metrics focus on visual quality but neglect temporal fidelity and text-to-video alignment, crucial for safety-critical applications.

Method: NeuS-V uses neuro-symbolic formal verification, converting prompts into Temporal Logic (TL) specifications and videos into automata for alignment evaluation.

Result: NeuS-V shows 5x higher correlation with human evaluations than existing metrics, and current models perform poorly on temporally complex prompts.

Conclusion: NeuS-V addresses a critical gap in video evaluation, revealing the need for improved text-to-video generation capabilities.

Abstract: Recent advancements in text-to-video models such as Sora, Gen-3, MovieGen,
and CogVideoX are pushing the boundaries of synthetic video generation, with
adoption seen in fields like robotics, autonomous driving, and entertainment.
As these models become prevalent, various metrics and benchmarks have emerged
to evaluate the quality of the generated videos. However, these metrics
emphasize visual quality and smoothness, neglecting temporal fidelity and
text-to-video alignment, which are crucial for safety-critical applications. To
address this gap, we introduce NeuS-V, a novel synthetic video evaluation
metric that rigorously assesses text-to-video alignment using neuro-symbolic
formal verification techniques. Our approach first converts the prompt into a
formally defined Temporal Logic (TL) specification and translates the generated
video into an automaton representation. Then, it evaluates the text-to-video
alignment by formally checking the video automaton against the TL
specification. Furthermore, we present a dataset of temporally extended prompts
to evaluate state-of-the-art video generation models against our benchmark. We
find that NeuS-V demonstrates a higher correlation by over 5x with human
evaluations when compared to existing metrics. Our evaluation further reveals
that current video generation models perform poorly on these temporally complex
prompts, highlighting the need for future work in improving text-to-video
generation capabilities.

</details>


### [141] [GaussianBlock: Building Part-Aware Compositional and Editable 3D Scene by Primitives and Gaussians](https://arxiv.org/abs/2410.01535)
*Shuyi Jiang, Qihao Zhao, Hossein Rahmani, De Wen Soh, Jun Liu, Na Zhao*

Main category: cs.CV

TL;DR: GaussianBlock introduces a part-aware compositional 3D reconstruction method for disentangled, interpretable representations, combining primitives and 3D Gaussians for high fidelity and editability.


<details>
  <summary>Details</summary>
Motivation: Current 3D reconstruction methods like Neural Radiance Fields and Gaussian Splatting produce entangled, non-interpretable latent representations, limiting precise editing.

Method: Hybrid representation using primitives and 3D Gaussians, with attention-guided centering loss, dynamic splitting/fusion, and binding inheritance for coherence and fidelity.

Result: Disentangled, compositional, and compact reconstructions enabling precise editing while maintaining high quality.

Conclusion: GaussianBlock advances 3D reconstruction by balancing interpretability, editability, and fidelity.

Abstract: Recently, with the development of Neural Radiance Fields and Gaussian
Splatting, 3D reconstruction techniques have achieved remarkably high fidelity.
However, the latent representations learnt by these methods are highly
entangled and lack interpretability. In this paper, we propose a novel
part-aware compositional reconstruction method, called GaussianBlock, that
enables semantically coherent and disentangled representations, allowing for
precise and physical editing akin to building blocks, while simultaneously
maintaining high fidelity. Our GaussianBlock introduces a hybrid representation
that leverages the advantages of both primitives, known for their flexible
actionability and editability, and 3D Gaussians, which excel in reconstruction
quality. Specifically, we achieve semantically coherent primitives through a
novel attention-guided centering loss derived from 2D semantic priors,
complemented by a dynamic splitting and fusion strategy. Furthermore, we
utilize 3D Gaussians that hybridize with primitives to refine structural
details and enhance fidelity. Additionally, a binding inheritance strategy is
employed to strengthen and maintain the connection between the two. Our
reconstructed scenes are evidenced to be disentangled, compositional, and
compact across diverse benchmarks, enabling seamless, direct and precise
editing while maintaining high quality.

</details>


### [142] [Beyond Fixed Topologies: Unregistered Training and Comprehensive Evaluation Metrics for 3D Talking Heads](https://arxiv.org/abs/2410.11041)
*Federico Nocentini, Thomas Besnier, Claudio Ferrari, Sylvain Arguillere, Mohamed Daoudi, Stefano Berretti*

Main category: cs.CV

TL;DR: A framework for animating 3D faces with arbitrary topologies using heat diffusion, outperforming fixed-topology methods and proposing new lip-syncing metrics.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of fixed mesh topologies in 3D talking head generation to handle real-world scanned data and varying structures.

Method: Uses heat diffusion for topology-robust feature prediction, tested in registered and unregistered training settings.

Result: Outperforms fixed-topology techniques, offering versatile and high-fidelity animation.

Conclusion: Sets a new benchmark by removing topology constraints and improving lip-syncing evaluation.

Abstract: Generating speech-driven 3D talking heads presents numerous challenges; among
those is dealing with varying mesh topologies where no point-wise
correspondence exists across all meshes the model can animate. While
simplifying the problem, it limits applicability as unseen meshes must adhere
to the training topology. This work presents a framework capable of animating
3D faces in arbitrary topologies, including real scanned data. Our approach
relies on a model leveraging heat diffusion to predict features robust to the
mesh topology. We explore two training settings: a registered one, in which
meshes in a training sequences share a fixed topology but any mesh can be
animated at test time, and an fully unregistered one, which allows effective
training with varying mesh structures. Additionally, we highlight the
limitations of current evaluation metrics and propose new metrics for better
lip-syncing evaluation between speech and facial movements. Our extensive
evaluation shows our approach performs favorably compared to fixed topology
techniques, setting a new benchmark by offering a versatile and high-fidelity
solution for 3D talking head generation where the topology constraint is
dropped.

</details>


### [143] [Improving Consistency in Diffusion Models for Image Super-Resolution](https://arxiv.org/abs/2410.13807)
*Junhao Gu, Peng-Tao Jiang, Hao Zhang, Mi Zhou, Jinwei Chen, Wenming Yang, Bo Li*

Main category: cs.CV

TL;DR: The paper introduces ConsisSR to address semantic and training-inference inconsistencies in diffusion-based Real-ISR methods, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based Real-ISR methods suffer from semantic inconsistency (due to text prompts) and training-inference inconsistency (from DDPM assumptions), limiting their performance.

Method: Proposes Hybrid Prompt Adapter (HPA) for semantic consistency using CLIP image embeddings, and Time-Aware Latent Augmentation (TALA) for training-inference consistency by refining LQ latent.

Result: ConsisSR demonstrates state-of-the-art performance in Real-ISR tasks.

Conclusion: The proposed method effectively addresses inconsistencies in diffusion-based Real-ISR, improving performance and leveraging detailed semantic guidance.

Abstract: Recent methods exploit the powerful text-to-image (T2I) diffusion models for
real-world image super-resolution (Real-ISR) and achieve impressive results
compared to previous models. However, we observe two kinds of inconsistencies
in diffusion-based methods which hinder existing models from fully exploiting
diffusion priors. The first is the semantic inconsistency arising from
diffusion guidance. T2I generation focuses on semantic-level consistency with
text prompts, while Real-ISR emphasizes pixel-level reconstruction from
low-quality (LQ) images, necessitating more detailed semantic guidance from LQ
inputs. The second is the training-inference inconsistency stemming from the
DDPM, which improperly assumes high-quality (HQ) latent corrupted by Gaussian
noise as denoising inputs for each timestep. To address these issues, we
introduce ConsisSR to handle both semantic and training-inference
consistencies. On the one hand, to address the semantic inconsistency, we
proposed a Hybrid Prompt Adapter (HPA). Instead of text prompts with
coarse-grained classification information, we leverage the more powerful CLIP
image embeddings to explore additional color and texture guidance. On the other
hand, we introduce Time-Aware Latent Augmentation (TALA) to bridge the
training-inference inconsistency. Based on the probability function p(t), we
accordingly enhance the SDSR training strategy. With LQ latent with Gaussian
noise as inputs, our TALA not only focuses on diffusion noise but also refine
the LQ latent towards the HQ counterpart. Our method demonstrates
state-of-the-art performance among existing diffusion models. The code will be
made publicly available.

</details>


### [144] [Exploring Local Memorization in Diffusion Models via Bright Ending Attention](https://arxiv.org/abs/2410.21665)
*Chen Chen, Daochang Liu, Mubarak Shah, Chang Xu*

Main category: cs.CV

TL;DR: The paper addresses local memorization in text-to-image diffusion models, introducing a 'bright ending' (BE) anomaly for detecting and localizing memorized regions, improving existing methods.


<details>
  <summary>Details</summary>
Motivation: Concerns about memorization in diffusion models, especially local memorization, and the inadequacy of current methods to address it.

Method: Identifies the BE anomaly in cross-attention patterns to localize memorized regions and integrates it into existing frameworks.

Result: Improved performance in detecting and localizing memorized regions, achieving state-of-the-art results.

Conclusion: The BE phenomenon is significant for addressing local memorization, enhancing existing frameworks effectively.

Abstract: Text-to-image diffusion models have achieved unprecedented proficiency in
generating realistic images. However, their inherent tendency to memorize and
replicate training data during inference raises significant concerns, including
potential copyright infringement. In response, various methods have been
proposed to evaluate, detect, and mitigate memorization. Our analysis reveals
that existing approaches significantly underperform in handling local
memorization, where only specific image regions are memorized, compared to
global memorization, where the entire image is replicated. Also, they cannot
locate the local memorization regions, making it hard to investigate locally.
To address these, we identify a novel "bright ending" (BE) anomaly in diffusion
models prone to memorizing training images. BE refers to a distinct
cross-attention pattern observed in text-to-image diffusion models, where
memorized image patches exhibit significantly greater attention to the final
text token during the last inference step than non-memorized patches. This
pattern highlights regions where the generated image replicates training data
and enables efficient localization of memorized regions. Equipped with this, we
propose a simple yet effective method to integrate BE into existing frameworks,
significantly improving their performance by narrowing the performance gap
caused by local memorization. Our results not only validate the successful
execution of the new localization task but also establish new state-of-the-art
performance across all existing tasks, underscoring the significance of the BE
phenomenon.

</details>


### [145] [Decomposing and Fusing Intra- and Inter-Sensor Spatio-Temporal Signal for Multi-Sensor Wearable Human Activity Recognition](https://arxiv.org/abs/2501.10917)
*Haoyu Xie, Haoxuan Li, Chunyuan Zheng, Haonan Yuan, Guorui Liao, Jun Liao, Li Liu*

Main category: cs.CV

TL;DR: The paper proposes DecomposeWHAR, a model for Wearable Human Activity Recognition (WHAR) that improves feature extraction by decomposing and fusing intra- and inter-sensor relationships, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing WHAR methods inadequately capture spatio-temporal relationships due to shared convolutional kernels, limiting their effectiveness.

Method: DecomposeWHAR uses a decomposition phase (Depth Separable Convolution) for intra-sensor features and a fusion phase (State Space Model and self-attention) for inter-sensor relationships.

Result: The model achieves superior performance on three WHAR datasets, surpassing state-of-the-art models with acceptable computational efficiency.

Conclusion: DecomposeWHAR effectively addresses the limitations of current WHAR methods by better modeling spatio-temporal relationships, demonstrating significant performance improvements.

Abstract: Wearable Human Activity Recognition (WHAR) is a prominent research area
within ubiquitous computing. Multi-sensor synchronous measurement has proven to
be more effective for WHAR than using a single sensor. However, existing WHAR
methods use shared convolutional kernels for indiscriminate temporal feature
extraction across each sensor variable, which fails to effectively capture
spatio-temporal relationships of intra-sensor and inter-sensor variables. We
propose the DecomposeWHAR model consisting of a decomposition phase and a
fusion phase to better model the relationships between modality variables. The
decomposition creates high-dimensional representations of each intra-sensor
variable through the improved Depth Separable Convolution to capture local
temporal features while preserving their unique characteristics. The fusion
phase begins by capturing relationships between intra-sensor variables and
fusing their features at both the channel and variable levels. Long-range
temporal dependencies are modeled using the State Space Model (SSM), and later
cross-sensor interactions are dynamically captured through a self-attention
mechanism, highlighting inter-sensor spatial correlations. Our model
demonstrates superior performance on three widely used WHAR datasets,
significantly outperforming state-of-the-art models while maintaining
acceptable computational efficiency.

</details>


### [146] [Investigating Memorization in Video Diffusion Models](https://arxiv.org/abs/2410.21669)
*Chen Chen, Enhuai Liu, Daochang Liu, Mubarak Shah, Chang Xu*

Main category: cs.CV

TL;DR: The paper investigates memorization risks in video diffusion models (VDMs), defining content and motion memorization, introducing metrics, and proposing detection strategies.


<details>
  <summary>Details</summary>
Motivation: Address the underexplored risk of VDMs memorizing and reproducing training data, focusing on privacy preservation.

Method: Define memorization types, introduce metrics, curate prompts, generate videos from VDMs, and analyze memorization systematically.

Result: Memorization is widespread in VDMs, including image training data, with successful extraction of training videos.

Conclusion: Proposes detection strategies to improve privacy in VDMs, highlighting the need for further research.

Abstract: Diffusion models, widely used for image and video generation, face a
significant limitation: the risk of memorizing and reproducing training data
during inference, potentially generating unauthorized copyrighted content.
While prior research has focused on image diffusion models (IDMs), video
diffusion models (VDMs) remain underexplored. To address this gap, we first
formally define the two types of memorization in VDMs (content memorization and
motion memorization) in a practical way that focuses on privacy preservation
and applies to all generation types. We then introduce new metrics specifically
designed to separately assess content and motion memorization in VDMs.
Additionally, we curate a dataset of text prompts that are most prone to
triggering memorization when used as conditioning in VDMs. By leveraging these
prompts, we generate diverse videos from various open-source VDMs, successfully
extracting numerous training videos from each tested model. Through the
application of our proposed metrics, we systematically analyze memorization
across various pretrained VDMs, including text-conditional and unconditional
models, on a variety of datasets. Our comprehensive study reveals that
memorization is widespread across all tested VDMs, indicating that VDMs can
also memorize image training data in addition to video datasets. Finally, we
propose efficient and effective detection strategies for both content and
motion memorization, offering a foundational approach for improving privacy in
VDMs.

</details>


### [147] [CLIC: Contrastive Learning Framework for Unsupervised Image Complexity Representation](https://arxiv.org/abs/2411.12792)
*Shipeng Liu, Liang Zhao, Dengfeng Chen*

Main category: cs.CV

TL;DR: CLIC is an unsupervised framework using contrastive learning to quantify image complexity without costly labels, outperforming traditional metrics and reducing human bias.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics for image complexity are unreliable, and data-driven methods require expensive annotations and suffer from human bias. CLIC addresses these limitations.

Method: CLIC uses contrastive learning with a novel sample selection strategy and complexity-aware loss function, learning from unlabeled data.

Result: CLIC captures image complexity effectively, matches supervised methods with minimal labeled data, and improves downstream task performance.

Conclusion: CLIC provides a reliable, bias-free solution for image complexity assessment, enhancing both unsupervised and supervised applications.

Abstract: As a fundamental visual attribute, image complexity significantly influences
both human perception and the performance of computer vision models. However,
accurately assessing and quantifying image complexity remains a challenging
task. (1) Traditional metrics such as information entropy and compression ratio
often yield coarse and unreliable estimates. (2) Data-driven methods require
expensive manual annotations and are inevitably affected by human subjective
biases. To address these issues, we propose CLIC, an unsupervised framework
based on Contrastive Learning for learning Image Complexity representations.
CLIC learns complexity-aware features from unlabeled data, thereby eliminating
the need for costly labeling. Specifically, we design a novel positive and
negative sample selection strategy to enhance the discrimination of complexity
features. Additionally, we introduce a complexity-aware loss function guided by
image priors to further constrain the learning process. Extensive experiments
validate the effectiveness of CLIC in capturing image complexity. When
fine-tuned with a small number of labeled samples from IC9600, CLIC achieves
performance competitive with supervised methods. Moreover, applying CLIC to
downstream tasks consistently improves performance. Notably, both the
pretraining and application processes of CLIC are free from subjective bias.

</details>


### [148] [Importance-Based Token Merging for Efficient Image and Video Generation](https://arxiv.org/abs/2411.16720)
*Haoyu Wu, Jingyi Xu, Hieu Le, Dimitris Samaras*

Main category: cs.CV

TL;DR: Proposes an importance-based token merging method for vision systems, prioritizing high-information tokens to improve quality and coherence in generations.


<details>
  <summary>Details</summary>
Motivation: Existing token grouping methods are often random and disregard sample content, leading to suboptimal quality. Preserving high-information tokens can enhance semantic fidelity and structural details.

Method: Uses importance scores (e.g., from classifier-free guidance) to prioritize critical tokens during merging, ensuring resource allocation to the most impactful tokens.

Result: Outperforms baselines in text-to-image synthesis, multi-view image generation, and video generation across models like Stable Diffusion and AnimateDiff.

Conclusion: The simple yet effective importance-based merging method significantly improves generation quality and coherence, demonstrating its broad applicability.

Abstract: Token merging can effectively accelerate various vision systems by processing
groups of similar tokens only once and sharing the results across them.
However, existing token grouping methods are often ad hoc and random,
disregarding the actual content of the samples. We show that preserving
high-information tokens during merging - those essential for semantic fidelity
and structural details - significantly improves sample quality, producing finer
details and more coherent, realistic generations. Despite being simple and
intuitive, this approach remains underexplored.
  To do so, we propose an importance-based token merging method that
prioritizes the most critical tokens in computational resource allocation,
leveraging readily available importance scores, such as those from
classifier-free guidance in diffusion models. Experiments show that our
approach significantly outperforms baseline methods across multiple
applications, including text-to-image synthesis, multi-view image generation,
and video generation with various model architectures such as Stable Diffusion,
Zero123++, AnimateDiff, or PixArt-$\alpha$.

</details>


### [149] [A Parametric Approach to Adversarial Augmentation for Cross-Domain Iris Presentation Attack Detection](https://arxiv.org/abs/2412.07199)
*Debasmita Pal, Redwan Sony, Arun Ross*

Main category: cs.CV

TL;DR: The paper proposes ADV-GEN, a method using adversarial training with geometric and photometric transformations to improve cross-domain performance of iris presentation attack detection (PAD).


<details>
  <summary>Details</summary>
Motivation: Iris-based biometric systems face challenges in cross-domain PAD due to poor generalization of existing methods.

Method: ADV-GEN, a convolutional autoencoder, generates adversarial samples using transformation parameters from classical data augmentation schemes.

Result: Experiments on LivDet-Iris 2017 and 2020 datasets show improved cross-domain PAD performance.

Conclusion: The proposed method effectively enhances cross-domain generalization for iris PAD.

Abstract: Iris-based biometric systems are vulnerable to presentation attacks (PAs),
where adversaries present physical artifacts (e.g., printed iris images,
textured contact lenses) to defeat the system. This has led to the development
of various presentation attack detection (PAD) algorithms, which typically
perform well in intra-domain settings. However, they often struggle to
generalize effectively in cross-domain scenarios, where training and testing
employ different sensors, PA instruments, and datasets. In this work, we use
adversarial training samples of both bonafide irides and PAs to improve the
cross-domain performance of a PAD classifier. The novelty of our approach lies
in leveraging transformation parameters from classical data augmentation
schemes (e.g., translation, rotation) to generate adversarial samples. We
achieve this through a convolutional autoencoder, ADV-GEN, that inputs original
training samples along with a set of geometric and photometric transformations.
The transformation parameters act as regularization variables, guiding ADV-GEN
to generate adversarial samples in a constrained search space. Experiments
conducted on the LivDet-Iris 2017 database, comprising four datasets, and the
LivDet-Iris 2020 dataset, demonstrate the efficacy of our proposed method. The
code is available at https://github.com/iPRoBe-lab/ADV-GEN-IrisPAD.

</details>


### [150] [EMT: A Visual Multi-Task Benchmark Dataset for Autonomous Driving in the Arab Gulf Region](https://arxiv.org/abs/2502.19260)
*Nadya Abdel Madjid, Murad Mebrahtu, Abdelmoamen Nasser, Bilal Hassan, Naoufel Werghi, Jorge Dias, Majid Khonji*

Main category: cs.CV

TL;DR: The paper introduces the EMT dataset for multi-task benchmarking in Gulf region traffic, supporting tracking, trajectory forecasting, and intention prediction.


<details>
  <summary>Details</summary>
Motivation: To provide a unified framework for multi-task benchmarking in traffic scenarios, capturing Gulf region-specific road conditions and behaviors.

Method: The dataset includes 30,000 dash-camera frames and 570,000 annotated bounding boxes, covering 150 km of driving routes. It supports three tasks: tracking, trajectory forecasting, and intention prediction, each with specific evaluations.

Result: The dataset is publicly available with pre-processing scripts and evaluation models, facilitating research in multi-task traffic analysis.

Conclusion: The EMT dataset fills a gap in region-specific traffic benchmarking and supports diverse tasks, aiding advancements in autonomous driving research.

Abstract: This paper introduces the Emirates Multi-Task (EMT) dataset, designed to
support multi-task benchmarking within a unified framework. It comprises over
30,000 frames from a dash-camera perspective and 570,000 annotated bounding
boxes, covering approximately 150 kilometers of driving routes that reflect the
distinctive road topology, congestion patterns, and driving behavior of Gulf
region traffic. The dataset supports three primary tasks: tracking, trajectory
forecasting, and intention prediction. Each benchmark is accompanied by
corresponding evaluations: (1) multi-agent tracking experiments addressing
multi-class scenarios and occlusion handling; (2) trajectory forecasting
evaluation using deep sequential and interaction-aware models; and (3)
intention prediction experiments based on observed trajectories. The dataset is
publicly available at https://avlab.io/emt-dataset, with pre-processing scripts
and evaluation models at https://github.com/AV-Lab/emt-dataset.

</details>


### [151] [DCFormer: Efficient 3D Vision-Language Modeling with Decomposed Convolutions](https://arxiv.org/abs/2502.05091)
*Gorkem Can Ates, Yu Xin, Kuang Gong, Wei Shao*

Main category: cs.CV

TL;DR: DCFormer is an efficient 3D image encoder that reduces computational cost by factorizing 3D convolutions into parallel 1D convolutions, outperforming existing methods in medical vision-language tasks.


<details>
  <summary>Details</summary>
Motivation: Extending vision-language models (VLMs) to 3D imaging is computationally challenging due to the high cost of existing methods like Vision Transformers (ViTs) and 3D convolutions.

Method: DCFormer factorizes 3D convolutions into three parallel 1D convolutions along depth, height, and width dimensions, preserving spatial information while reducing computational cost.

Result: DCFormer outperforms state-of-the-art 3D vision encoders in zero-shot and fine-tuned pathology detection and image-text retrieval tasks on the CT-RATE dataset.

Conclusion: DCFormer offers a scalable and clinically deployable solution for 3D medical VLMs, with demonstrated superiority over existing methods.

Abstract: Vision-language models (VLMs) have been widely applied to 2D medical image
analysis due to their ability to align visual and textual representations.
However, extending VLMs to 3D imaging remains computationally challenging.
Existing 3D VLMs often rely on Vision Transformers (ViTs), which are
computationally expensive due to the quadratic complexity of self-attention, or
on 3D convolutions, which require large numbers of parameters and FLOPs as
kernel size increases. We introduce DCFormer, an efficient 3D image encoder
that factorizes 3D convolutions into three parallel 1D convolutions along the
depth, height, and width dimensions. This design preserves spatial information
while significantly reducing computational cost. Integrated into a CLIP-based
vision-language framework, DCFormer is trained and evaluated on CT-RATE, a
dataset of 50,188 paired 3D chest CT volumes and radiology reports. In
zero-shot and fine-tuned detection of 18 pathologies, as well as in image-text
retrieval tasks, DCFormer consistently outperforms state-of-the-art 3D vision
encoders, including CT-ViT, ViT, ConvNeXt, PoolFormer, and TransUNet. These
results highlight DCFormer's potential for scalable, clinically deployable 3D
medical VLMs. Our code is available at: https://github.com/mirthAI/DCFormer.

</details>


### [152] [L4P: Low-Level 4D Vision Perception Unified](https://arxiv.org/abs/2502.13078)
*Abhishek Badki, Hang Su, Bowen Wen, Orazio Gallo*

Main category: cs.CV

TL;DR: L4P is a general-purpose architecture for low-level 4D perception tasks, outperforming specialized methods while solving multiple tasks efficiently.


<details>
  <summary>Details</summary>
Motivation: Current methods are task-specific; L4P aims to unify low-level 4D perception tasks with a single model.

Method: Uses a pre-trained ViT-based video encoder with lightweight per-task heads for efficient multi-task learning.

Result: Matches or surpasses specialized methods in performance and handles all tasks in comparable time.

Conclusion: L4P demonstrates the feasibility of a unified, general-purpose approach for 4D perception tasks.

Abstract: The spatio-temporal relationship between the pixels of a video carries
critical information for low-level 4D perception tasks. A single model that
reasons about it should be able to solve several such tasks well. Yet, most
state-of-the-art methods rely on architectures specialized for the task at
hand. We present L4P, a feedforward, general-purpose architecture that solves
low-level 4D perception tasks in a unified framework. L4P leverages a
pre-trained ViT-based video encoder and combines it with per-task heads that
are lightweight and therefore do not require extensive training. Despite its
general and feedforward formulation, our method matches or surpasses the
performance of existing specialized methods on both dense tasks, such as depth
or optical flow estimation, and sparse tasks, such as 2D/3D tracking. Moreover,
it solves all tasks at once in a time comparable to that of single-task
methods.

</details>


### [153] [Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in Vision-Language Models](https://arxiv.org/abs/2503.13939)
*Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, Xiaofeng Yang*

Main category: cs.CV

TL;DR: Med-R1, an RL-enhanced vision-language model, improves medical reasoning by 29.94% over its base model and outperforms larger models, challenging assumptions about reasoning in medical VQA.


<details>
  <summary>Details</summary>
Motivation: Medical vision-language tasks require precise understanding, but current methods struggle due to data complexity and lack of expert annotations.

Method: Med-R1 uses reinforcement learning (GRPO) to enhance generalization and reliability, evaluated across eight medical imaging modalities and five question types.

Result: Med-R1 outperforms its base model (Qwen2-VL-2B) by 29.94% and a larger model (Qwen2-VL-72B), with 32.06% better question-type generalization.

Conclusion: RL improves medical reasoning efficiency and reliability, emphasizing quality and domain alignment over reasoning quantity.

Abstract: Vision-language models (VLMs) have achieved impressive progress in natural
image reasoning, yet their potential in medical imaging remains underexplored.
Medical vision-language tasks demand precise understanding and clinically
coherent answers, which are difficult to achieve due to the complexity of
medical data and the scarcity of high-quality expert annotations. These
challenges limit the effectiveness of conventional supervised fine-tuning (SFT)
and Chain-of-Thought (CoT) strategies that work well in general domains. To
address these challenges, we propose Med-R1, a reinforcement learning
(RL)-enhanced vision-language model designed to improve generalization and
reliability in medical reasoning. Built on the DeepSeek strategy, Med-R1 adopts
Group Relative Policy Optimization (GRPO) to encourage reward-guided learning
beyond static annotations. We comprehensively evaluate Med-R1 across eight
distinct medical imaging modalities. Med-R1 achieves a 29.94% improvement in
average accuracy over its base model Qwen2-VL-2B, and even outperforms
Qwen2-VL-72B-a model with 36x more parameters. To assess cross-task
generalization, we further evaluate Med-R1 on five question types. Med-R1
outperforms Qwen2-VL-2B by 32.06% in question-type generalization, also
surpassing Qwen2-VL-72B. We further explore the thinking process in Med-R1, a
crucial component for the success of Deepseek-R1. Our results show that
omitting intermediate rationales (No-Thinking-Med-R1) not only improves
in-domain and cross-domain generalization with less training, but also
challenges the assumption that more reasoning always helps. These findings
suggest that in medical VQA, it is not reasoning itself, but its quality and
domain alignment, that determine effectiveness. Together, these results
highlight that RL improves medical reasoning and generalization, enabling
efficient and reliable VLMs for real-world deployment.

</details>


### [154] [GranQ: Granular Zero-Shot Quantization with Unified Layer-Channel Awareness](https://arxiv.org/abs/2503.18339)
*Inpyo Hong, Youngwan Jo, Hyojeong Lee, Sunghyun Ahn, Sanghyun Park*

Main category: cs.CV

TL;DR: GranQ is a novel zero-shot quantization method that minimizes activation loss by dynamically adjusting quantization granularity based on layer-channel awareness, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot quantization methods suffer from significant activation loss in low-bit environments due to coarse-grained scaling, prompting the need for a more refined approach.

Method: GranQ dynamically adjusts quantization granularity by considering layer- and channel-level activation distributions and introduces vectorized activation quantization for efficiency.

Result: GranQ achieves superior performance compared to state-of-the-art zero-shot quantization methods, even those using quantization-aware training.

Conclusion: GranQ's success suggests new research directions beyond traditional zero-shot quantization, focusing on fine-grained quantization and efficient computation.

Abstract: Zero-shot quantization (ZSQ) enables neural network compression without
training data, which is crucial in restricted data access environments.
However, existing ZSQ methods suffer from significant activation loss in
low-bit environments owing to their coarse-grained scaling strategy. To address
this issue, we propose GranQ, a novel ZSQ approach that leverages layer-channel
awareness to minimize the quantization error. Unlike conventional layer- or
channel-wise quantization, GranQ dynamically adjusts quantization granularity
by considering both layer- and channel-level activation distributions. This
enables fine-grained quantization while minimizing activation distortion.
Additionally, we introduce vectorized activation quantization, which enables
efficient parallel computation and reduces computational overhead while
preserving accuracy. GranQ achieves superior performance compared with those of
state-of-the-art ZSQ methods that employ quantization-aware training. With
these findings, we anticipate that GranQ will inspire novel research directions
beyond conventional ZSQ approaches focused on data generation and model
training.

</details>


### [155] [SpINR: Neural Volumetric Reconstruction for FMCW Radars](https://arxiv.org/abs/2503.23313)
*Harshvardhan Takawale, Nirupam Roy*

Main category: cs.CV

TL;DR: SpINR is a novel framework for volumetric reconstruction using FMCW radar data, combining differentiable forward modeling and implicit neural representations to improve resolution and efficiency over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional radar imaging techniques like backprojection have limitations in resolution and generalization due to ideal signal assumptions and dense sampling requirements.

Method: SpINR integrates a differentiable forward model in the frequency domain with implicit neural representations, leveraging FMCW radar's linear frequency-distance relationship for efficient learning.

Result: SpINR outperforms classical backprojection and learning-based methods, achieving higher resolution and more accurate reconstructions.

Conclusion: This work pioneers neural volumetric reconstruction in radar, opening new research directions for radar-based imaging and perception.

Abstract: In this paper, we introduce SpINR, a novel framework for volumetric
reconstruction using Frequency-Modulated Continuous-Wave (FMCW) radar data.
Traditional radar imaging techniques, such as backprojection, often assume
ideal signal models and require dense aperture sampling, leading to limitations
in resolution and generalization. To address these challenges, SpINR integrates
a fully differentiable forward model that operates natively in the frequency
domain with implicit neural representations (INRs). This integration leverages
the linear relationship between beat frequency and scatterer distance inherent
in FMCW radar systems, facilitating more efficient and accurate learning of
scene geometry. Additionally, by computing outputs for only the relevant
frequency bins, our forward model achieves greater computational efficiency
compared to time-domain approaches that process the entire signal before
transformation. Through extensive experiments, we demonstrate that SpINR
significantly outperforms classical backprojection methods and existing
learning-based approaches, achieving higher resolution and more accurate
reconstructions of complex scenes. This work represents the first application
of neural volumetic reconstruction in the radar domain, offering a promising
direction for future research in radar-based imaging and perception systems.

</details>


### [156] [Scaling Open-Vocabulary Action Detection](https://arxiv.org/abs/2504.03096)
*Zhen Hao Sia, Yogesh Singh Rawat*

Main category: cs.CV

TL;DR: The paper addresses scaling open-vocabulary action detection by introducing a lightweight encoder-only multimodal model and a weakly supervised training strategy, while proposing a new benchmark for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing action detection models are limited to closed-set scenarios and rely on complex architectures, making open-vocabulary adaptation challenging due to dataset and parameter constraints.

Method: An encoder-only multimodal model reduces parameter-heavy additions, and a weakly supervised strategy leverages closed-set datasets for pretraining. A new benchmark evaluates performance without training on closed-set data.

Result: The approach introduces a novel benchmark and provides baselines for future work, demonstrating feasibility in open-vocabulary action detection.

Conclusion: The proposed model and benchmark offer a simpler, more scalable solution for open-vocabulary action detection, encouraging further research.

Abstract: In this work, we focus on scaling open-vocabulary action detection. Existing
approaches for action detection are predominantly limited to closed-set
scenarios and rely on complex, parameter-heavy architectures. Extending these
models to the open-vocabulary setting poses two key challenges: (1) the lack of
large-scale datasets with many action classes for robust training, and (2)
parameter-heavy adaptations to a pretrained vision-language contrastive model
to convert it for detection, risking overfitting the additional non-pretrained
parameters to base action classes. Firstly, we introduce an encoder-only
multimodal model for video action detection, reducing the reliance on
parameter-heavy additions for video action detection. Secondly, we introduce a
simple weakly supervised training strategy to exploit an existing closed-set
action detection dataset for pretraining. Finally, we depart from the ill-posed
base-to-novel benchmark used by prior works in open-vocabulary action detection
and devise a new benchmark to evaluate on existing closed-set action detection
datasets without ever using them for training, showing novel results to serve
as baselines for future work. Our code is available at:
https://siatheindochinese.github.io/sia_act_page/

</details>


### [157] [Transforming Hyperspectral Images Into Chemical Maps: An End-to-End Deep Learning Approach](https://arxiv.org/abs/2504.14131)
*Ole-Christian Galbo Engstrøm, Michela Albano-Gaglio, Erik Schou Dreier, Yamine Bouzembrak, Maria Font-i-Furnols, Puneet Mishra, Kim Steenstrup Pedersen*

Main category: cs.CV

TL;DR: A deep learning U-Net model outperforms traditional PLS regression for chemical map generation from hyperspectral images, offering lower error rates and better spatial correlation.


<details>
  <summary>Details</summary>
Motivation: Traditional methods like PLS regression for chemical map generation are noisy and ignore spatial context, prompting the need for a more robust approach.

Method: The study uses a modified U-Net with a custom loss function to directly generate chemical maps from hyperspectral images, bypassing intermediate steps.

Result: U-Net achieves 9-13% lower RMSE than PLS, produces spatially correlated maps (99.91% variance), and stays within the 0-100% physical range.

Conclusion: U-Net is superior to PLS for chemical map generation due to its accuracy, spatial coherence, and adherence to physical constraints.

Abstract: Current approaches to chemical map generation from hyperspectral images are
based on models such as partial least squares (PLS) regression, generating
pixel-wise predictions that do not consider spatial context and suffer from a
high degree of noise. This study proposes an end-to-end deep learning approach
using a modified version of U-Net and a custom loss function to directly obtain
chemical maps from hyperspectral images, skipping all intermediate steps
required for traditional pixel-wise analysis. We compare the U-Net with the
traditional PLS regression on a real dataset of pork belly samples with
associated mean fat reference values. The U-Net obtains a test set root mean
squared error of between 9% and 13% lower than that of PLS regression on the
task of mean fat prediction. At the same time, U-Net generates fine detail
chemical maps where 99.91% of the variance is spatially correlated. Conversely,
only 2.53% of the variance in the PLS-generated chemical maps is spatially
correlated, indicating that each pixel-wise prediction is largely independent
of neighboring pixels. Additionally, while the PLS-generated chemical maps
contain predictions far beyond the physically possible range of 0-100%, U-Net
learns to stay inside this range. Thus, the findings of this study indicate
that U-Net is superior to PLS for chemical map generation.

</details>


### [158] [Manipulating Multimodal Agents via Cross-Modal Prompt Injection](https://arxiv.org/abs/2504.14348)
*Le Wang, Zonghao Ying, Tianyuan Zhang, Siyuan Liang, Shengshan Hu, Mingchuan Zhang, Aishan Liu, Xianglong Liu*

Main category: cs.CV

TL;DR: The paper introduces CrossInject, a cross-modal prompt injection attack framework targeting multimodal agents, achieving high success rates by exploiting vulnerabilities in visual and textual modalities.


<details>
  <summary>Details</summary>
Motivation: To address a critical security vulnerability in multimodal agents where adversarial perturbations across modalities can hijack decision-making.

Method: Proposes CrossInject with Visual Latent Alignment (optimizing adversarial visual features) and Textual Guidance Enhancement (using LLMs to craft malicious commands).

Result: Outperforms existing attacks with a +26.4% success rate increase and validates effectiveness in real-world agents.

Conclusion: Highlights significant security risks in multimodal agents and the need for robust defenses against cross-modal attacks.

Abstract: The emergence of multimodal large language models has redefined the agent
paradigm by integrating language and vision modalities with external data
sources, enabling agents to better interpret human instructions and execute
increasingly complex tasks. However, in this work, we identify a critical yet
previously overlooked security vulnerability in multimodal agents: cross-modal
prompt injection attacks. To exploit this vulnerability, we propose
CrossInject, a novel attack framework in which attackers embed adversarial
perturbations across multiple modalities to align with target malicious
content, allowing external instructions to hijack the agent's decision-making
process and execute unauthorized tasks. Our approach consists of two key
components. First, we introduce Visual Latent Alignment, where we optimize
adversarial features to the malicious instructions in the visual embedding
space based on a text-to-image generative model, ensuring that adversarial
images subtly encode cues for malicious task execution. Subsequently, we
present Textual Guidance Enhancement, where a large language model is leveraged
to infer the black-box defensive system prompt through adversarial meta
prompting and generate an malicious textual command that steers the agent's
output toward better compliance with attackers' requests. Extensive experiments
demonstrate that our method outperforms existing injection attacks, achieving
at least a +26.4% increase in attack success rates across diverse tasks.
Furthermore, we validate our attack's effectiveness in real-world multimodal
autonomous agents, highlighting its potential implications for safety-critical
applications.

</details>


### [159] [DreamID: High-Fidelity and Fast diffusion-based Face Swapping via Triplet ID Group Learning](https://arxiv.org/abs/2504.14509)
*Fulong Ye, Miao Hua, Pengze Zhang, Xinghui Li, Qichao Sun, Songtao Zhao, Qian He, Xinglong Wu*

Main category: cs.CV

TL;DR: DreamID is a diffusion-based face swapping model with explicit supervision via Triplet ID Group data, achieving high ID similarity, attribute preservation, and fast inference.


<details>
  <summary>Details</summary>
Motivation: Typical face swapping methods lack explicit supervision, leading to suboptimal results. DreamID addresses this by introducing Triplet ID Group data for better control.

Method: Uses SD Turbo for efficient single-step inference, SwapNet, FaceNet, and ID Adapter architecture, and explicit Triplet ID Group supervision for training.

Result: Outperforms state-of-the-art in identity similarity, pose/expression preservation, and image fidelity, with 0.6s inference at 512*512 resolution.

Conclusion: DreamID delivers high-quality face swapping efficiently, even in challenging scenarios like complex lighting or occlusions.

Abstract: In this paper, we introduce DreamID, a diffusion-based face swapping model
that achieves high levels of ID similarity, attribute preservation, image
fidelity, and fast inference speed. Unlike the typical face swapping training
process, which often relies on implicit supervision and struggles to achieve
satisfactory results. DreamID establishes explicit supervision for face
swapping by constructing Triplet ID Group data, significantly enhancing
identity similarity and attribute preservation. The iterative nature of
diffusion models poses challenges for utilizing efficient image-space loss
functions, as performing time-consuming multi-step sampling to obtain the
generated image during training is impractical. To address this issue, we
leverage the accelerated diffusion model SD Turbo, reducing the inference steps
to a single iteration, enabling efficient pixel-level end-to-end training with
explicit Triplet ID Group supervision. Additionally, we propose an improved
diffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter.
This robust architecture fully unlocks the power of the Triplet ID Group
explicit supervision. Finally, to further extend our method, we explicitly
modify the Triplet ID Group data during training to fine-tune and preserve
specific attributes, such as glasses and face shape. Extensive experiments
demonstrate that DreamID outperforms state-of-the-art methods in terms of
identity similarity, pose and expression preservation, and image fidelity.
Overall, DreamID achieves high-quality face swapping results at 512*512
resolution in just 0.6 seconds and performs exceptionally well in challenging
scenarios such as complex lighting, large angles, and occlusions.

</details>


### [160] [Think Hierarchically, Act Dynamically: Hierarchical Multi-modal Fusion and Reasoning for Vision-and-Language Navigation](https://arxiv.org/abs/2504.16516)
*Junrong Yue, Yifan Zhang, Chuan Qin, Bo Li, Xiaomin Lie, Xinlei Yu, Wenxin Zhang, Zhendong Zhao*

Main category: cs.CV

TL;DR: The paper proposes a Multi-level Fusion and Reasoning Architecture (MFRA) for Vision-and-Language Navigation (VLN), improving navigation accuracy by hierarchically fusing multi-level features and reasoning over them.


<details>
  <summary>Details</summary>
Motivation: Prior methods in VLN rely on global or object-level features, which fail to capture complex cross-modal interactions needed for accurate navigation.

Method: MFRA introduces hierarchical fusion of multi-level features (low-level to high-level) and a reasoning module for instruction-guided attention and dynamic context integration.

Result: MFRA outperforms state-of-the-art methods on VLN benchmarks (REVERIE, R2R, SOON), demonstrating superior navigation accuracy.

Conclusion: Multi-level modal fusion enhances embodied navigation, with MFRA proving effective in complex scenarios.

Abstract: Vision-and-Language Navigation (VLN) aims to enable embodied agents to follow
natural language instructions and reach target locations in real-world
environments. While prior methods often rely on either global scene
representations or object-level features, these approaches are insufficient for
capturing the complex interactions across modalities required for accurate
navigation. In this paper, we propose a Multi-level Fusion and Reasoning
Architecture (MFRA) to enhance the agent's ability to reason over visual
observations, language instructions and navigation history. Specifically, MFRA
introduces a hierarchical fusion mechanism that aggregates multi-level
features-ranging from low-level visual cues to high-level semantic
concepts-across multiple modalities. We further design a reasoning module that
leverages fused representations to infer navigation actions through
instruction-guided attention and dynamic context integration. By selectively
capturing and combining relevant visual, linguistic, and temporal signals, MFRA
improves decision-making accuracy in complex navigation scenarios. Extensive
experiments on benchmark VLN datasets including REVERIE, R2R, and SOON
demonstrate that MFRA achieves superior performance compared to
state-of-the-art methods, validating the effectiveness of multi-level modal
fusion for embodied navigation.

</details>


### [161] [Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning](https://arxiv.org/abs/2504.16656)
*Chris, Yichen Wei, Yi Peng, Xiaokun Wang, Weijie Qiu, Wei Shen, Tianyidan Xie, Jiangbo Pei, Jianhao Zhang, Yunzhuo Hao, Xuchen Song, Yang Liu, Yahui Zhou*

Main category: cs.CV

TL;DR: Skywork R1V2 is a multimodal reasoning model with hybrid reinforcement learning, combining MPO and GRPO, and introduces SSB to improve training efficiency. It achieves top benchmark scores and closes the gap with proprietary models.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of balancing sophisticated reasoning with broad generalization in AI models.

Method: Uses hybrid reinforcement learning (MPO + GRPO) and SSB to counter vanishing advantages and mitigate visual hallucinations.

Result: Achieves leading benchmarks: 62.6 (OlympiadBench), 78.9 (AIME2024), 63.6 (LiveCodeBench), 73.6 (MMMU).

Conclusion: Skywork R1V2 outperforms open-source models and narrows the gap with proprietary systems, promoting openness via public release.

Abstract: We present Skywork R1V2, a next-generation multimodal reasoning model and a
major leap forward from its predecessor, Skywork R1V. At its core, R1V2
introduces a hybrid reinforcement learning paradigm that jointly leverages the
Mixed Preference Optimization (MPO) and the Group Relative Policy Optimization
(GRPO), which harmonizes reward-model guidance with rule-based strategies,
thereby addressing the long-standing challenge of balancing sophisticated
reasoning capabilities with broad generalization. To further enhance training
efficiency, we introduce the Selective Sample Buffer (SSB) mechanism, which
effectively counters the ``Vanishing Advantages'' dilemma inherent in GRPO by
prioritizing high-value samples throughout the optimization process. Notably,
we observe that excessive reinforcement signals can induce visual
hallucinations--a phenomenon we systematically monitor and mitigate through
calibrated reward thresholds throughout the training process. Empirical results
affirm the exceptional capability of R1V2, with benchmark-leading performances
such as 62.6 on OlympiadBench, 78.9 on AIME2024, 63.6 on LiveCodeBench, and
73.6 on MMMU. These results underscore R1V2's superiority over existing
open-source models and demonstrate significant progress in closing the
performance gap with premier proprietary systems, including Gemini 2.5 and
OpenAI-o4-mini. The Skywork R1V2 model weights have been publicly released to
promote openness and reproducibility
https://huggingface.co/Skywork/Skywork-R1V2-38B.

</details>


### [162] [We'll Fix it in Post: Improving Text-to-Video Generation with Neuro-Symbolic Feedback](https://arxiv.org/abs/2504.17180)
*Minkyu Choi, S P Sharan, Harsh Goel, Sahil Shah, Sandeep Chinchali*

Main category: cs.CV

TL;DR: NeuS-E is a zero-training video refinement pipeline using neuro-symbolic feedback to improve text-to-video generation, enhancing alignment with prompts by 40%.


<details>
  <summary>Details</summary>
Motivation: Current T2V models struggle with semantic and temporal consistency for complex prompts, and direct improvements are computationally expensive.

Method: NeuS-E derives neuro-symbolic feedback to identify inconsistencies and guides targeted edits to the original video.

Result: Empirical evaluations show NeuS-E improves temporal and logical alignment by nearly 40%.

Conclusion: NeuS-E effectively enhances video generation without additional training, addressing key limitations of existing models.

Abstract: Current text-to-video (T2V) generation models are increasingly popular due to
their ability to produce coherent videos from textual prompts. However, these
models often struggle to generate semantically and temporally consistent videos
when dealing with longer, more complex prompts involving multiple objects or
sequential events. Additionally, the high computational costs associated with
training or fine-tuning make direct improvements impractical. To overcome these
limitations, we introduce NeuS-E, a novel zero-training video refinement
pipeline that leverages neuro-symbolic feedback to automatically enhance video
generation, achieving superior alignment with the prompts. Our approach first
derives the neuro-symbolic feedback by analyzing a formal video representation
and pinpoints semantically inconsistent events, objects, and their
corresponding frames. This feedback then guides targeted edits to the original
video. Extensive empirical evaluations on both open-source and proprietary T2V
models demonstrate that NeuS-E significantly enhances temporal and logical
alignment across diverse prompts by almost 40%

</details>


### [163] [Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D Dataset](https://arxiv.org/abs/2504.17371)
*Oussema Dhaouadi, Johannes Meier, Luca Wahl, Jacques Kaiser, Luca Scalerandi, Nick Wandelburg, Zhuolun Zhou, Nijanthan Berinpanathan, Holger Banzhaf, Daniel Cremers*

Main category: cs.CV

TL;DR: The paper introduces DSC3D, a high-quality, occlusion-free 3D trajectory dataset for autonomous driving, captured using a drone-based tracking pipeline, offering diverse scenarios and surpassing existing datasets in scale and variety.


<details>
  <summary>Details</summary>
Motivation: Traditional datasets for autonomous driving suffer from occlusion and limited coverage, hindering accurate 3D trajectory data. DSC3D addresses these gaps by providing a comprehensive, occlusion-free dataset.

Method: A novel monocular camera drone tracking pipeline was used to capture 175,000+ trajectories of 14 traffic participant types across five diverse locations.

Result: DSC3D exceeds existing datasets in diversity and scale, featuring unprecedented scenarios like complex urban interactions and comprehensive parking maneuvers.

Conclusion: DSC3D enhances autonomous driving systems by providing detailed 3D environmental representations, supporting applications like motion prediction and safety validation. The dataset is publicly available.

Abstract: Accurate 3D trajectory data is crucial for advancing autonomous driving. Yet,
traditional datasets are usually captured by fixed sensors mounted on a car and
are susceptible to occlusion. Additionally, such an approach can precisely
reconstruct the dynamic environment in the close vicinity of the measurement
vehicle only, while neglecting objects that are further away. In this paper, we
introduce the DeepScenario Open 3D Dataset (DSC3D), a high-quality,
occlusion-free dataset of 6 degrees of freedom bounding box trajectories
acquired through a novel monocular camera drone tracking pipeline. Our dataset
includes more than 175,000 trajectories of 14 types of traffic participants and
significantly exceeds existing datasets in terms of diversity and scale,
containing many unprecedented scenarios such as complex vehicle-pedestrian
interaction on highly populated urban streets and comprehensive parking
maneuvers from entry to exit. DSC3D dataset was captured in five various
locations in Europe and the United States and include: a parking lot, a crowded
inner-city, a steep urban intersection, a federal highway, and a suburban
intersection. Our 3D trajectory dataset aims to enhance autonomous driving
systems by providing detailed environmental 3D representations, which could
lead to improved obstacle interactions and safety. We demonstrate its utility
across multiple applications including motion prediction, motion planning,
scenario mining, and generative reactive traffic agents. Our interactive online
visualization platform and the complete dataset are publicly available at
https://app.deepscenario.com, facilitating research in motion prediction,
behavior modeling, and safety validation.

</details>


### [164] [Hierarchical and Multimodal Data for Daily Activity Understanding](https://arxiv.org/abs/2504.17696)
*Ghazal Kaviani, Yavuz Yarici, Seulgi Kim, Mohit Prabhushankar, Ghassan AlRegib, Mashhour Solh, Ameya Patil*

Main category: cs.CV

TL;DR: DARai is a multimodal dataset with hierarchical annotations for human activity understanding, featuring 200+ hours of data from 20 sensors across 10 environments. It supports tasks like recognition, localization, and anticipation, highlighting sensor limitations.


<details>
  <summary>Details</summary>
Motivation: To capture the complexity of human activities in real-world settings and provide a dataset for AI research with hierarchical annotations and multimodal sensor data.

Method: DARai includes scripted and unscripted recordings from 50 participants, annotated at three hierarchical levels (L1-L3), with overlapping actions and procedures. Experiments involve unimodal/multimodal sensor fusion and domain-variant tests.

Result: DARai enables recognition, temporal localization, and future action anticipation across hierarchical levels, showcasing sensor fusion benefits and individual sensor limitations.

Conclusion: DARai is a valuable resource for human-centered AI applications, offering rich multimodal data and hierarchical annotations to address real-world activity understanding challenges.

Abstract: Daily Activity Recordings for Artificial Intelligence (DARai, pronounced
"Dahr-ree") is a multimodal, hierarchically annotated dataset constructed to
understand human activities in real-world settings. DARai consists of
continuous scripted and unscripted recordings of 50 participants in 10
different environments, totaling over 200 hours of data from 20 sensors
including multiple camera views, depth and radar sensors, wearable inertial
measurement units (IMUs), electromyography (EMG), insole pressure sensors,
biomonitor sensors, and gaze tracker.
  To capture the complexity in human activities, DARai is annotated at three
levels of hierarchy: (i) high-level activities (L1) that are independent tasks,
(ii) lower-level actions (L2) that are patterns shared between activities, and
(iii) fine-grained procedures (L3) that detail the exact execution steps for
actions. The dataset annotations and recordings are designed so that 22.7% of
L2 actions are shared between L1 activities and 14.2% of L3 procedures are
shared between L2 actions. The overlap and unscripted nature of DARai allows
counterfactual activities in the dataset.
  Experiments with various machine learning models showcase the value of DARai
in uncovering important challenges in human-centered applications.
Specifically, we conduct unimodal and multimodal sensor fusion experiments for
recognition, temporal localization, and future action anticipation across all
hierarchical annotation levels. To highlight the limitations of individual
sensors, we also conduct domain-variant experiments that are enabled by DARai's
multi-sensor and counterfactual activity design setup.
  The code, documentation, and dataset are available at the dedicated DARai
website:
https://alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [165] [ApproXAI: Energy-Efficient Hardware Acceleration of Explainable AI using Approximate Computing](https://arxiv.org/abs/2504.17929)
*Ayesha Siddique, Khurram Khalil, Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: XAIedge improves energy efficiency for real-time XAI by using approximate computing, achieving 2× better efficiency without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Current XAI methods are computationally intensive and lack energy efficiency for real-time applications.

Method: XAIedge integrates approximate computing into XAI algorithms (e.g., integrated gradients, Shapley analysis) and optimizes matrix computations for TPU-based edge devices.

Result: XAIedge achieves 2× better energy efficiency than existing methods while maintaining accuracy.

Conclusion: XAIedge advances real-time XAI deployment in energy-constrained settings.

Abstract: Explainable artificial intelligence (XAI) enhances AI system transparency by
framing interpretability as an optimization problem. However, this approach
often necessitates numerous iterations of computationally intensive operations,
limiting its applicability in real-time scenarios. While recent research has
focused on XAI hardware acceleration on FPGAs and TPU, these methods do not
fully address energy efficiency in real-time settings. To address this
limitation, we propose XAIedge, a novel framework that leverages approximate
computing techniques into XAI algorithms, including integrated gradients, model
distillation, and Shapley analysis. XAIedge translates these algorithms into
approximate matrix computations and exploits the synergy between convolution,
Fourier transform, and approximate computing paradigms. This approach enables
efficient hardware acceleration on TPU-based edge devices, facilitating faster
real-time outcome interpretations. Our comprehensive evaluation demonstrates
that XAIedge achieves a $2\times$ improvement in energy efficiency compared to
existing accurate XAI hardware acceleration techniques while maintaining
comparable accuracy. These results highlight the potential of XAIedge to
significantly advance the deployment of explainable AI in energy-constrained
real-time applications.

</details>


### [166] [LLM Agent Swarm for Hypothesis-Driven Drug Discovery](https://arxiv.org/abs/2504.17967)
*Kevin Song, Andrew Trotter, Jake Y. Chen*

Main category: cs.AI

TL;DR: PharmaSwarm is a multi-agent LLM framework for drug discovery, integrating specialized agents to propose, validate, and refine hypotheses, aiming to improve efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: High failure rates and costs in drug discovery, coupled with fragmented data and limitations of current AI models, drive the need for a unified, hypothesis-driven approach.

Method: PharmaSwarm uses specialized LLM agents for tasks like genomic analysis, knowledge graph access, and binding affinity prediction, orchestrated by a central Evaluator LLM with shared memory for continuous improvement.

Result: The framework supports diverse drug discovery tasks (e.g., target identification, repurposing) and is validated through a four-tier pipeline for transparency and reproducibility.

Conclusion: PharmaSwarm acts as an AI copilot, accelerating translational research and delivering high-confidence hypotheses more efficiently than traditional methods.

Abstract: Drug discovery remains a formidable challenge: more than 90 percent of
candidate molecules fail in clinical evaluation, and development costs often
exceed one billion dollars per approved therapy. Disparate data streams, from
genomics and transcriptomics to chemical libraries and clinical records, hinder
coherent mechanistic insight and slow progress. Meanwhile, large language
models excel at reasoning and tool integration but lack the modular
specialization and iterative memory required for regulated, hypothesis-driven
workflows. We introduce PharmaSwarm, a unified multi-agent framework that
orchestrates specialized LLM "agents" to propose, validate, and refine
hypotheses for novel drug targets and lead compounds. Each agent accesses
dedicated functionality--automated genomic and expression analysis; a curated
biomedical knowledge graph; pathway enrichment and network simulation;
interpretable binding affinity prediction--while a central Evaluator LLM
continuously ranks proposals by biological plausibility, novelty, in silico
efficacy, and safety. A shared memory layer captures validated insights and
fine-tunes underlying submodels over time, yielding a self-improving system.
Deployable on low-code platforms or Kubernetes-based microservices, PharmaSwarm
supports literature-driven discovery, omics-guided target identification, and
market-informed repurposing. We also describe a rigorous four-tier validation
pipeline spanning retrospective benchmarking, independent computational assays,
experimental testing, and expert user studies to ensure transparency,
reproducibility, and real-world impact. By acting as an AI copilot, PharmaSwarm
can accelerate translational research and deliver high-confidence hypotheses
more efficiently than traditional pipelines.

</details>


### [167] [Differential Privacy-Driven Framework for Enhancing Heart Disease Prediction](https://arxiv.org/abs/2504.18007)
*Yazan Otoum, Amiya Nayak*

Main category: cs.AI

TL;DR: The paper proposes privacy-preserving machine learning models using differential privacy and federated learning to analyze healthcare data without compromising patient privacy.


<details>
  <summary>Details</summary>
Motivation: The rapid digitalization of healthcare systems increases the need to protect private health data while leveraging machine learning for insights.

Method: Utilizes differential privacy (adding noise to data) and federated learning (decentralized model training) on Heart Disease Data.

Result: Achieved 85% test accuracy while ensuring data privacy.

Conclusion: The approach successfully balances privacy preservation with valuable healthcare insights.

Abstract: With the rapid digitalization of healthcare systems, there has been a
substantial increase in the generation and sharing of private health data.
Safeguarding patient information is essential for maintaining consumer trust
and ensuring compliance with legal data protection regulations. Machine
learning is critical in healthcare, supporting personalized treatment, early
disease detection, predictive analytics, image interpretation, drug discovery,
efficient operations, and patient monitoring. It enhances decision-making,
accelerates research, reduces errors, and improves patient outcomes. In this
paper, we utilize machine learning methodologies, including differential
privacy and federated learning, to develop privacy-preserving models that
enable healthcare stakeholders to extract insights without compromising
individual privacy. Differential privacy introduces noise to data to guarantee
statistical privacy, while federated learning enables collaborative model
training across decentralized datasets. We explore applying these technologies
to Heart Disease Data, demonstrating how they preserve privacy while delivering
valuable insights and comprehensive analysis. Our results show that using a
federated learning model with differential privacy achieved a test accuracy of
85%, ensuring patient data remained secure and private throughout the process.

</details>


### [168] [MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and Theory of Mind](https://arxiv.org/abs/2504.18039)
*Zheng Zhang, Nuoqian Xiao, Qi Chai, Deheng Ye, Hao Wang*

Main category: cs.AI

TL;DR: MultiMind integrates multimodal cues (facial expressions, vocal tones) and Theory of Mind (ToM) into LLM agents for social deduction games, outperforming text-only approaches.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents in social deduction games lack multimodal cues and fail to model how players perceive others, limiting their social reasoning.

Method: MultiMind combines facial/vocal data with verbal content, uses ToM to track suspicions, and employs MCTS to optimize communication strategies.

Result: MultiMind achieves superior performance in agent-versus-agent simulations and human player studies.

Conclusion: This work advances LLM agents toward human-like multimodal social reasoning, with potential broader applications.

Abstract: Large Language Model (LLM) agents have demonstrated impressive capabilities
in social deduction games (SDGs) like Werewolf, where strategic reasoning and
social deception are essential. However, current approaches remain limited to
textual information, ignoring crucial multimodal cues such as facial
expressions and tone of voice that humans naturally use to communicate.
Moreover, existing SDG agents primarily focus on inferring other players'
identities without modeling how others perceive themselves or fellow players.
To address these limitations, we use One Night Ultimate Werewolf (ONUW) as a
testbed and present MultiMind, the first framework integrating multimodal
information into SDG agents. MultiMind processes facial expressions and vocal
tones alongside verbal content, while employing a Theory of Mind (ToM) model to
represent each player's suspicion levels toward others. By combining this ToM
model with Monte Carlo Tree Search (MCTS), our agent identifies communication
strategies that minimize suspicion directed at itself. Through comprehensive
evaluation in both agent-versus-agent simulations and studies with human
players, we demonstrate MultiMind's superior performance in gameplay. Our work
presents a significant advancement toward LLM agents capable of human-like
social reasoning across multimodal domains.

</details>


### [169] [Combating the Bucket Effect:Multi-Knowledge Alignment for Medication Recommendation](https://arxiv.org/abs/2504.18096)
*Xiang Li, Haixu Ma, Guanyong Wu, Shi Mu, Chen Li, Shunpan Liang*

Main category: cs.AI

TL;DR: The paper introduces MKMed, a framework for medication recommendation that addresses the 'bucket effect' by integrating multiple knowledge modalities. It uses a cross-modal encoder and outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: The 'bucket effect' in medication recommendation, caused by imbalanced data availability across knowledge modalities, limits model performance. This paper aims to mitigate this issue.

Method: A cross-modal medication encoder is pre-trained with contrastive learning on five knowledge modalities. The MKMed framework combines multi-knowledge representations with patient records for recommendations.

Result: Experiments on MIMIC-III and MIMIC-IV show MKMed mitigates the 'bucket effect' and outperforms state-of-the-art baselines in accuracy and safety.

Conclusion: MKMed effectively addresses the 'bucket effect' and improves medication recommendation by integrating diverse knowledge modalities.

Abstract: Medication recommendation is crucial in healthcare, offering effective
treatments based on patient's electronic health records (EHR). Previous studies
show that integrating more medication-related knowledge improves medication
representation accuracy. However, not all medications encompass multiple types
of knowledge data simultaneously. For instance, some medications provide only
textual descriptions without structured data. This imbalance in data
availability limits the performance of existing models, a challenge we term the
"bucket effect" in medication recommendation. Our data analysis uncovers the
severity of the "bucket effect" in medication recommendation. To fill this gap,
we introduce a cross-modal medication encoder capable of seamlessly aligning
data from different modalities and propose a medication recommendation
framework to integrate Multiple types of Knowledge, named MKMed. Specifically,
we first pre-train a cross-modal encoder with contrastive learning on five
knowledge modalities, aligning them into a unified space. Then, we combine the
multi-knowledge medication representations with patient records for
recommendations. Extensive experiments on the MIMIC-III and MIMIC-IV datasets
demonstrate that MKMed mitigates the "bucket effect" in data, and significantly
outperforms state-of-the-art baselines in recommendation accuracy and safety.

</details>


### [170] [Pseudo-Boolean Proof Logging for Optimal Classical Planning](https://arxiv.org/abs/2504.18443)
*Simon Dold, Malte Helmert, Jakob Nordström, Gabriele Röger, Tanja Schindler*

Main category: cs.AI

TL;DR: Lower-bound certificates for planning tasks prove unsolvability or plan optimality, verifiable by third parties. A pseudo-Boolean constraint framework generates these certificates, adaptable to any planning algorithm.


<details>
  <summary>Details</summary>
Motivation: To provide verifiable proofs of unsolvability or optimality in planning tasks, independent of the planning algorithm used.

Method: A general framework using pseudo-Boolean constraints to generate certificates, demonstrated with modified $A^{*}$ algorithm and pattern database heuristics.

Result: Proofs of optimality can be generated with modest overhead, applicable to any heuristic expressible via pseudo-Boolean constraints.

Conclusion: The approach enables verifiable certificates for planning tasks, extending to various heuristics and algorithms.

Abstract: We introduce lower-bound certificates for classical planning tasks, which can
be used to prove the unsolvability of a task or the optimality of a plan in a
way that can be verified by an independent third party. We describe a general
framework for generating lower-bound certificates based on pseudo-Boolean
constraints, which is agnostic to the planning algorithm used.
  As a case study, we show how to modify the $A^{*}$ algorithm to produce
proofs of optimality with modest overhead, using pattern database heuristics
and $h^\textit{max}$ as concrete examples. The same proof logging approach
works for any heuristic whose inferences can be efficiently expressed as
reasoning over pseudo-Boolean constraints.

</details>


### [171] [Reason Like a Radiologist: Chain-of-Thought and Reinforcement Learning for Verifiable Report Generation](https://arxiv.org/abs/2504.18453)
*Peiyuan Jing, Kinhei Lee, Zhenxuan Zhang, Huichi Zhou, Zhengqing Yuan, Zhifan Gao, Lei Zhu, Giorgos Papanastasiou, Yingying Fang, Guang Yang*

Main category: cs.AI

TL;DR: BoxMed-RL is a training framework for radiology report generation that improves accuracy and explainability by linking visual findings to anatomical locations using reinforcement learning and a two-phase training approach.


<details>
  <summary>Details</summary>
Motivation: Current radiology report models lack structured reasoning, hindering clinical trust and explainability. BoxMed-RL aims to bridge this gap by mimicking radiologists' workflows.

Method: BoxMed-RL uses a two-phase approach: (1) Pretraining with medical concept learning and reinforcement learning for spatial alignment, and (2) Downstream Adapter Phase for fluent, credible reports.

Result: BoxMed-RL achieves a 7% improvement in METEOR and ROUGE-L metrics and a 5% boost in large language model-based metrics over state-of-the-art methods.

Conclusion: BoxMed-RL successfully enhances radiology report generation by integrating structured reasoning and spatial verification, improving clinical trust and report quality.

Abstract: Radiology report generation is critical for efficiency but current models
lack the structured reasoning of experts, hindering clinical trust and
explainability by failing to link visual findings to precise anatomical
locations. This paper introduces BoxMed-RL, a groundbreaking unified training
framework for generating spatially verifiable and explainable radiology
reports. Built on a large vision-language model, BoxMed-RL revolutionizes
report generation through two integrated phases: (1) In the Pretraining Phase,
we refine the model via medical concept learning, using Chain-of-Thought
supervision to internalize the radiologist-like workflow, followed by spatially
verifiable reinforcement, which applies reinforcement learning to align medical
findings with bounding boxes. (2) In the Downstream Adapter Phase, we freeze
the pretrained weights and train a downstream adapter to ensure fluent and
clinically credible reports. This framework precisely mimics radiologists'
workflow, compelling the model to connect high-level medical concepts with
definitive anatomical evidence. Extensive experiments on public datasets
demonstrate that BoxMed-RL achieves an average 7% improvement in both METEOR
and ROUGE-L metrics compared to state-of-the-art methods. An average 5%
improvement in large language model-based metrics further underscores
BoxMed-RL's robustness in generating high-quality radiology reports.

</details>


### [172] [Scaling Laws For Scalable Oversight](https://arxiv.org/abs/2504.18530)
*Joshua Engels, David D. Baek, Subhash Kantamneni, Max Tegmark*

Main category: cs.AI

TL;DR: The paper proposes a framework to quantify scalable oversight success, modeling it as a game between mismatched players, and validates it with oversight games. It also studies Nested Scalable Oversight (NSO) and identifies conditions for its success.


<details>
  <summary>Details</summary>
Motivation: To address the unclear scalability of scalable oversight, a proposed strategy for controlling superintelligent AI systems.

Method: A framework modeling oversight as a game with Elo scores, validated using modified Nim and applied to oversight games like Mafia and Debate. Scaling laws are derived, and NSO is theoretically studied.

Result: Scaling laws for oversight games are found, and NSO success rates are numerically derived (below 52% for systems 400 Elo points stronger).

Conclusion: The framework provides insights into scalable oversight, but NSO success declines with stronger systems, highlighting challenges in overseeing superintelligent AI.

Abstract: Scalable oversight, the process by which weaker AI systems supervise stronger
ones, has been proposed as a key strategy to control future superintelligent
systems. However, it is still unclear how scalable oversight itself scales. To
address this gap, we propose a framework that quantifies the probability of
successful oversight as a function of the capabilities of the overseer and the
system being overseen. Specifically, our framework models oversight as a game
between capability-mismatched players; the players have oversight-specific and
deception-specific Elo scores that are a piecewise-linear function of their
general intelligence, with two plateaus corresponding to task incompetence and
task saturation. We validate our framework with a modified version of the game
Nim and then apply it to four oversight games: "Mafia", "Debate", "Backdoor
Code" and "Wargames". For each game, we find scaling laws that approximate how
domain performance depends on general AI system capability (using Chatbot Arena
Elo as a proxy for general capability). We then build on our findings in a
theoretical study of Nested Scalable Oversight (NSO), a process in which
trusted models oversee untrusted stronger models, which then become the trusted
models in the next step. We identify conditions under which NSO succeeds and
derive numerically (and in some cases analytically) the optimal number of
oversight levels to maximize the probability of oversight success. In our
numerical examples, the NSO success rate is below 52% when overseeing systems
that are 400 Elo points stronger than the baseline overseer, and it declines
further for overseeing even stronger systems.

</details>


### [173] [Adapting Probabilistic Risk Assessment for AI](https://arxiv.org/abs/2504.18536)
*Anna Katariina Wisakanto, Joe Rogero, Avyay M. Casheekar, Richard Mallah*

Main category: cs.AI

TL;DR: The paper introduces a probabilistic risk assessment (PRA) framework for AI, adapting techniques from high-reliability industries to systematically evaluate AI risks, including hazard analysis, risk pathway modeling, and uncertainty management.


<details>
  <summary>Details</summary>
Motivation: Modern AI systems pose urgent risks due to rapid capability evolution and potential catastrophic harm, but current risk assessment methods are inadequate, relying on selective testing and undocumented assumptions.

Method: The framework uses aspect-oriented hazard analysis, risk pathway modeling, and uncertainty management to systematically identify, estimate, and document AI risks. It synthesizes results into a risk report card.

Result: The framework provides a systematic approach to assess AI risks, integrating diverse methods into comparable, quantified risk estimates. A workbook tool is available for developers, evaluators, and regulators.

Conclusion: The PRA framework offers a structured, credible method to manage AI risks, addressing gaps in current practices and supporting critical decision-making.

Abstract: Modern general-purpose artificial intelligence (AI) systems present an urgent
risk management challenge, as their rapidly evolving capabilities and potential
for catastrophic harm outpace our ability to reliably assess their risks.
Current methods often rely on selective testing and undocumented assumptions
about risk priorities, frequently failing to make a serious attempt at
assessing the set of pathways through which Al systems pose direct or indirect
risks to society and the biosphere. This paper introduces the probabilistic
risk assessment (PRA) for AI framework, adapting established PRA techniques
from high-reliability industries (e.g., nuclear power, aerospace) for the new
challenges of advanced AI. The framework guides assessors in identifying
potential risks, estimating likelihood and severity, and explicitly documenting
evidence, underlying assumptions, and analyses at appropriate granularities.
The framework's implementation tool synthesizes the results into a risk report
card with aggregated risk estimates from all assessed risks. This systematic
approach integrates three advances: (1) Aspect-oriented hazard analysis
provides systematic hazard coverage guided by a first-principles taxonomy of AI
system aspects (e.g. capabilities, domain knowledge, affordances); (2) Risk
pathway modeling analyzes causal chains from system aspects to societal impacts
using bidirectional analysis and incorporating prospective techniques; and (3)
Uncertainty management employs scenario decomposition, reference scales, and
explicit tracing protocols to structure credible projections with novelty or
limited data. Additionally, the framework harmonizes diverse assessment methods
by integrating evidence into comparable, quantified absolute risk estimates for
critical decisions. We have implemented this as a workbook tool for AI
developers, evaluators, and regulators, available on the project website.

</details>


### [174] [Neural Combinatorial Optimization Algorithms for Solving Vehicle Routing Problems: A Comprehensive Survey with Perspectives](https://arxiv.org/abs/2406.00415)
*Xuan Wu, Di Wang, Lijie Wen, Yubin Xiao, Chunguo Wu, Yuesong Wu, Chaoyu Yu, Douglas L. Maskell, You Zhou*

Main category: cs.AI

TL;DR: The paper reviews recent Neural Combinatorial Optimization (NCO) solvers for Vehicle Routing Problems (VRPs), categorizes them into four types, identifies their shortcomings, and proposes future directions. It also compares solver performance and provides a live repository.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive coverage of recent SOTA NCO solvers for VRPs and establish an updated taxonomy.

Method: Systematic review and categorization of NCO solvers into four types, followed by performance comparison across learning paradigms.

Result: Identified inadequacies in SOTA solvers (e.g., poor generalization, scalability issues) and proposed future directions.

Conclusion: The survey and live repository aim to advance the NCO community by addressing current limitations and fostering collaboration.

Abstract: Although several surveys on Neural Combinatorial Optimization (NCO) solvers
specifically designed to solve Vehicle Routing Problems (VRPs) have been
conducted, they did not cover the state-of-the-art (SOTA) NCO solvers emerged
recently. More importantly, to establish a comprehensive and up-to-date
taxonomy of NCO solvers, we systematically review relevant publications and
preprints, categorizing them into four distinct types, namely Learning to
Construct, Learning to Improve, Learning to Predict-Once, and Learning to
Predict-Multiplicity solvers. Subsequently, we present the inadequacies of the
SOTA solvers, including poor generalization, incapability to solve large-scale
VRPs, inability to address most types of VRP variants simultaneously, and
difficulty in comparing these NCO solvers with the conventional Operations
Research algorithms. Simultaneously, we discuss on-going efforts, identify open
inadequacies, as well as propose promising and viable directions to overcome
these inadequacies. Notably, existing efforts focus on only one or two of these
inadequacies, with none attempting to address all of them concurrently. In
addition, we compare the performance of representative NCO solvers from the
Reinforcement, Supervised, and Unsupervised Learning paradigms across VRPs of
varying scales. Finally, following the proposed taxonomy, we provide an
accompanying web page as a live repository for NCO solvers. Through this survey
and the live repository, we aim to foster further advancements in the NCO
community.

</details>


### [175] [Self-Supervised Representation Learning for Geospatial Objects: A Survey](https://arxiv.org/abs/2408.12133)
*Yile Chen, Weiming Huang, Kaiqi Zhao, Yue Jiang, Gao Cong*

Main category: cs.AI

TL;DR: A survey of self-supervised learning (SSL) techniques for geospatial data, focusing on Point, Polyline, and Polygon types, categorizing methods and discussing trends and challenges.


<details>
  <summary>Details</summary>
Motivation: Geospatial data's heterogeneity and label sparsity demand techniques like SSL to learn representations without heavy reliance on labeled data.

Method: Systematic categorization of SSL techniques (predictive and contrastive) for geospatial objects, analyzing adaptations for downstream tasks.

Result: Identifies trends toward geospatial foundation models and highlights challenges in current research.

Conclusion: Encourages further integration of SSL with geospatial objects and development of foundation models.

Abstract: The proliferation of various data sources in urban and territorial
environments has significantly facilitated the development of geospatial
artificial intelligence (GeoAI) across a wide range of geospatial applications.
However, geospatial data, which is inherently linked to geospatial objects,
often exhibits data heterogeneity that necessitates specialized fusion and
representation strategies while simultaneously being inherently sparse in
labels for downstream tasks. Consequently, there is a growing demand for
techniques that can effectively leverage geospatial data without heavy reliance
on task-specific labels and model designs. This need aligns with the principles
of self-supervised learning (SSL), which has garnered increasing attention for
its ability to learn effective and generalizable representations directly from
data without extensive labeled supervision. This paper presents a comprehensive
and up-to-date survey of SSL techniques specifically applied to or developed
for geospatial objects in three primary vector geometric types: Point,
Polyline, and Polygon. We systematically categorize various SSL techniques into
predictive and contrastive methods, and analyze their adaptation to different
data types for representation learning across various downstream tasks.
Furthermore, we examine the emerging trends in SSL for geospatial objects,
particularly the gradual advancements towards geospatial foundation models.
Finally, we discuss key challenges in current research and outline promising
directions for future investigation. By offering a structured analysis of
existing studies, this paper aims to inspire continued progress in integrating
SSL with geospatial objects, and the development of geospatial foundation
models in a longer term.

</details>


### [176] [MIND: Math Informed syNthetic Dialogues for Pretraining LLMs](https://arxiv.org/abs/2410.12881)
*Syeda Nahida Akter, Shrimai Prabhumoye, John Kamalu, Sanjeev Satheesh, Eric Nyberg, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro*

Main category: cs.AI

TL;DR: A novel method, MIND, generates synthetic math dialogues to improve LLMs' reasoning, outperforming raw data pretraining.


<details>
  <summary>Details</summary>
Motivation: Existing synthetic data fails to enhance complex reasoning tasks like math, needing complementary knowledge.

Method: MIND generates diverse math dialogues from OpenWebMath, emphasizing knowledge gaps and data restructuring.

Result: MIND-OWM boosts math reasoning (GSM8K: +13.42%, MATH: +2.30%) and general reasoning (GENERAL REASONING: +2.51%).

Conclusion: Restructuring raw data with synthetic dialogues significantly enhances LLMs' mathematical and general reasoning.

Abstract: The utility of synthetic data to enhance pretraining data quality and hence
to improve downstream task accuracy has been widely explored in recent large
language models (LLMs). Yet, these approaches fall inadequate in complex,
multi-hop and mathematical reasoning tasks as the synthetic data typically
fails to add complementary knowledge to the existing raw corpus. In this work,
we propose a novel large-scale and diverse Math Informed syNthetic Dialogue
(MIND) generation method that improves the mathematical reasoning ability of
LLMs. Specifically, using MIND, we generate synthetic conversations based on
OpenWebMath (OWM), resulting in a new math corpus, MIND-OWM. Our experiments
with different conversational settings reveal that incorporating knowledge gaps
between dialog participants is essential for generating high-quality math data.
We further identify an effective way to format and integrate synthetic and raw
data during pretraining to maximize the gain in mathematical reasoning,
emphasizing the need to restructure raw data rather than use it as-is. Compared
to pretraining just on raw data, a model pretrained on MIND-OWM shows
significant boost in mathematical reasoning (GSM8K: +13.42%, MATH: +2.30%),
including superior performance in specialized knowledge (MMLU: +4.55%,
MMLU-STEM: +4.28%) and general purpose reasoning tasks (GENERAL REASONING:
+2.51%).

</details>


### [177] [From System 1 to System 2: A Survey of Reasoning Large Language Models](https://arxiv.org/abs/2502.17419)
*Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong, Zhiwei Li, Bao-Long Bi, Ling-Rui Mei, Junfeng Fang, Zhijiang Guo, Le Song, Cheng-Lin Liu*

Main category: cs.AI

TL;DR: The paper explores the transition from fast, intuitive (System 1) to slow, deliberate (System 2) reasoning in AI, focusing on reasoning LLMs like OpenAI's o1/o3 and DeepSeek's R1. It surveys their development, methods, benchmarks, and future directions.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between fast heuristic decisions (System 1) and logical, bias-reduced reasoning (System 2) in AI, enabling human-like cognitive abilities.

Method: Surveys foundational LLMs, System 2 technologies, and reasoning LLMs, analyzing their features, core methods, and performance benchmarks.

Result: Reasoning LLMs demonstrate expert-level performance in fields like math and coding, mimicking System 2 reasoning.

Conclusion: The paper highlights the potential of reasoning LLMs and provides a GitHub repository for tracking advancements, aiming to inspire innovation in the field.

Abstract: Achieving human-level intelligence requires refining the transition from the
fast, intuitive System 1 to the slower, more deliberate System 2 reasoning.
While System 1 excels in quick, heuristic decisions, System 2 relies on logical
reasoning for more accurate judgments and reduced biases. Foundational Large
Language Models (LLMs) excel at fast decision-making but lack the depth for
complex reasoning, as they have not yet fully embraced the step-by-step
analysis characteristic of true System 2 thinking. Recently, reasoning LLMs
like OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level
performance in fields such as mathematics and coding, closely mimicking the
deliberate reasoning of System 2 and showcasing human-like cognitive abilities.
This survey begins with a brief overview of the progress in foundational LLMs
and the early development of System 2 technologies, exploring how their
combination has paved the way for reasoning LLMs. Next, we discuss how to
construct reasoning LLMs, analyzing their features, the core methods enabling
advanced reasoning, and the evolution of various reasoning LLMs. Additionally,
we provide an overview of reasoning benchmarks, offering an in-depth comparison
of the performance of representative reasoning LLMs. Finally, we explore
promising directions for advancing reasoning LLMs and maintain a real-time
\href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub
Repository} to track the latest developments. We hope this survey will serve as
a valuable resource to inspire innovation and drive progress in this rapidly
evolving field.

</details>


### [178] [Repurposing the scientific literature with vision-language models](https://arxiv.org/abs/2502.19546)
*Anton Alyakin, Jaden Stryker, Daniel Alexander Alber, Karl L. Sangwon, Jin Vivian Lee, Brandon Duderstadt, Akshay Save, David Kurland, Spencer Frome, Shrutika Singh, Jeff Zhang, Eunice Yang, Ki Yun Park, Cordelia Orillac, Aly A. Valliani, Sean Neifert, Albert Liu, Aneek Patel, Christopher Livia, Darryl Lau, Ilya Laufer, Peter A. Rozman, Eveline Teresa Hidalgo, Howard Riina, Rui Feng, Todd Hollon, Yindalon Aphinyanaphongs, John G. Golfinos, Laura Snyder, Eric Leuthardt, Douglas Kondziolka, Eric Karl Oermann*

Main category: cs.AI

TL;DR: Training VLMs on neurosurgery-specific data (NeuroPubs) yields high-performance tools for academic and clinical tasks, matching GPT-4o in some cases.


<details>
  <summary>Details</summary>
Motivation: General VLMs overlook domain-specific knowledge in scientific journals, limiting their utility in specialty tasks.

Method: Created NeuroPubs dataset (23K articles, 134M words, 78K images) and trained CNS-Obsidian (34B-parameter VLM) for tasks like graphical abstracts and board-style questions.

Result: Model achieved non-inferiority to GPT-4o in neurosurgical diagnosis (40.62% vs. 57.89% upvotes, 59.38% vs. 65.79% accuracy).

Conclusion: Specialty-specific training enables high-performance AI tools without relying on large-scale internet data.

Abstract: Leading vision-language models (VLMs) are trained on general Internet
content, overlooking scientific journals' rich, domain-specific knowledge.
Training on specialty-specific literature could yield high-performance,
task-specific tools, enabling generative AI to match generalist models in
specialty publishing, educational, and clinical tasks. We created NeuroPubs, a
multimodal dataset of 23,000 Neurosurgery Publications articles (134M words,
78K image-caption pairs). Using NeuroPubs, VLMs generated publication-ready
graphical abstracts (70% of 100 abstracts) and board-style questions
indistinguishable from human-written ones (54% of 89,587 questions). We used
these questions to train CNS-Obsidian, a 34B-parameter VLM. In a blinded,
randomized controlled trial, our model demonstrated non-inferiority to then
state-of-the-art GPT-4o in neurosurgical differential diagnosis (clinical
utility, 40.62% upvotes vs. 57.89%, p=0.1150; accuracy, 59.38% vs. 65.79%,
p=0.3797). Our pilot study demonstrates how training generative AI models on
specialty-specific journal content - without large-scale internet data -
results in high-performance academic and clinical tools, enabling
domain-tailored AI across diverse fields.

</details>


### [179] [Neuroplasticity in Artificial Intelligence -- An Overview and Inspirations on Drop In & Out Learning](https://arxiv.org/abs/2503.21419)
*Yupei Li, Manuel Milling, Björn W. Schuller*

Main category: cs.AI

TL;DR: The paper explores how biological brain processes like neurogenesis, neuroapoptosis, and neuroplasticity can inspire advancements in AI, proposing concepts like 'dropin' and revisiting 'dropout' for dynamic neural networks.


<details>
  <summary>Details</summary>
Motivation: Current AI models, especially deep neural networks (DNNs), lack dynamic processes akin to human brain functions like neurogenesis and neuroplasticity, limiting their adaptability.

Method: The study examines analogous activities in artificial NNs, introducing 'dropin' for neurogenesis and revisiting 'dropout' and structural pruning for neuroapoptosis, while suggesting neuroplasticity for lifelong learning.

Result: The paper proposes dynamic NN architectures inspired by biological processes, advocating for their integration into AI models for improved adaptability.

Conclusion: The authors call for more interdisciplinary research to explore these biologically inspired concepts for future AI advancements.

Abstract: Artificial Intelligence (AI) has achieved new levels of performance and
spread in public usage with the rise of deep neural networks (DNNs). Initially
inspired by human neurons and their connections, NNs have become the foundation
of AI models for many advanced architectures. However, some of the most
integral processes in the human brain, particularly neurogenesis and
neuroplasticity in addition to the more spread neuroapoptosis have largely been
ignored in DNN architecture design. Instead, contemporary AI development
predominantly focuses on constructing advanced frameworks, such as large
language models, which retain a static structure of neural connections during
training and inference. In this light, we explore how neurogenesis,
neuroapoptosis, and neuroplasticity can inspire future AI advances.
Specifically, we examine analogous activities in artificial NNs, introducing
the concepts of ``dropin'' for neurogenesis and revisiting ``dropout'' and
structural pruning for neuroapoptosis. We additionally suggest neuroplasticity
combining the two for future large NNs in ``life-long learning'' settings
following the biological inspiration. We conclude by advocating for greater
research efforts in this interdisciplinary domain and identifying promising
directions for future exploration.

</details>


### [180] [Antidistillation Sampling](https://arxiv.org/abs/2504.13146)
*Yash Savani, Asher Trockman, Zhili Feng, Avi Schwarzschild, Alexander Robey, Marc Finzi, J. Zico Kolter*

Main category: cs.AI

TL;DR: Antidistillation sampling modifies token probabilities to poison reasoning traces, hindering model distillation while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To protect frontier models from unintended distillation by limiting the effectiveness of their reasoning traces.

Method: Strategic modification of next-token probability distributions to poison reasoning traces.

Result: Reduced effectiveness of distillation while preserving model utility.

Conclusion: Antidistillation sampling offers a viable solution to safeguard models from unintended distillation.

Abstract: Frontier models that generate extended reasoning traces inadvertently produce
rich token sequences that can facilitate model distillation. Recognizing this
vulnerability, model owners may seek sampling strategies that limit the
effectiveness of distillation without compromising model performance.
Antidistillation sampling provides exactly this capability. By strategically
modifying a model's next-token probability distribution, antidistillation
sampling poisons reasoning traces, rendering them significantly less effective
for distillation while preserving the model's practical utility. For further
details, see https://antidistillation.com.

</details>


### [181] [Pets: General Pattern Assisted Architecture For Time Series Analysis](https://arxiv.org/abs/2504.14209)
*Xiangkai Ma, Xiaobin Hong, Wenzhong Li, Sanglu Lu*

Main category: cs.AI

TL;DR: The paper introduces Pets, a novel architecture for time series analysis that disentangles multiple fluctuation patterns using energy distribution in temporal-spectrum space, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Traditional decomposition techniques fail to effectively separate multiple fluctuation patterns in time series data, limiting analysis accuracy.

Method: Pets uses a Fluctuation Pattern Assisted (FPA) module and Context-Guided Mixture of Predictors (MoP) to model and reconstruct diverse patterns without domain-specific knowledge.

Result: Pets outperforms existing methods in forecasting, imputation, anomaly detection, and classification, showing strong generalization.

Conclusion: The proposed approach advances time series analysis by adaptively handling complex fluctuation patterns, offering robust and versatile performance.

Abstract: Time series analysis has found widespread applications in areas such as
weather forecasting, anomaly detection, and healthcare. However, real-world
sequential data often exhibit a superimposed state of various fluctuation
patterns, including hourly, daily, and monthly frequencies. Traditional
decomposition techniques struggle to effectively disentangle these multiple
fluctuation patterns from the seasonal components, making time series analysis
challenging. Surpassing the existing multi-period decoupling paradigms, this
paper introduces a novel perspective based on energy distribution within the
temporal-spectrum space. By adaptively quantifying observed sequences into
continuous frequency band intervals, the proposed approach reconstructs
fluctuation patterns across diverse periods without relying on domain-specific
prior knowledge. Building upon this innovative strategy, we propose Pets, an
enhanced architecture that is adaptable to arbitrary model structures. Pets
integrates a Fluctuation Pattern Assisted (FPA) module and a Context-Guided
Mixture of Predictors (MoP). The FPA module facilitates information fusion
among diverse fluctuation patterns by capturing their dependencies and
progressively modeling these patterns as latent representations at each layer.
Meanwhile, the MoP module leverages these compound pattern representations to
guide and regulate the reconstruction of distinct fluctuations hierarchically.
Pets achieves state-of-the-art performance across various tasks, including
forecasting, imputation, anomaly detection, and classification, while
demonstrating strong generalization and robustness.

</details>


### [182] [UFO2: The Desktop AgentOS](https://arxiv.org/abs/2504.14603)
*Chaoyun Zhang, He Huang, Chiming Ni, Jian Mu, Si Qin, Shilin He, Lu Wang, Fangkai Yang, Pu Zhao, Chao Du, Liqun Li, Yu Kang, Zhao Jiang, Suzhen Zheng, Rujia Wang, Jiaxu Qian, Minghua Ma, Jian-Guang Lou, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang*

Main category: cs.AI

TL;DR: UFO2 is a multiagent AgentOS for Windows desktops that enhances Computer-Using Agents (CUAs) with deep OS integration, robust task execution, and concurrent user-agent operation.


<details>
  <summary>Details</summary>
Motivation: Existing CUAs are limited by shallow OS integration, fragile interaction, and disruptive execution, hindering practical automation.

Method: UFO2 uses a HostAgent for task coordination and AppAgents with native APIs and hybrid control detection (UIA + vision). It also employs speculative multi-action planning and a PiP interface for isolated automation.

Result: Evaluation across 20+ Windows apps shows improved robustness and accuracy over prior CUAs.

Conclusion: Deep OS integration enables scalable, reliable desktop automation aligned with user needs.

Abstract: Recent Computer-Using Agents (CUAs), powered by multimodal large language
models (LLMs), offer a promising direction for automating complex desktop
workflows through natural language. However, most existing CUAs remain
conceptual prototypes, hindered by shallow OS integration, fragile
screenshot-based interaction, and disruptive execution.
  We present UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs
into practical, system-level automation. UFO2 features a centralized HostAgent
for task decomposition and coordination, alongside a collection of
application-specialized AppAgent equipped with native APIs, domain-specific
knowledge, and a unified GUI--API action layer. This architecture enables
robust task execution while preserving modularity and extensibility. A hybrid
control detection pipeline fuses Windows UI Automation (UIA) with vision-based
parsing to support diverse interface styles. Runtime efficiency is further
enhanced through speculative multi-action planning, reducing per-step LLM
overhead. Finally, a Picture-in-Picture (PiP) interface enables automation
within an isolated virtual desktop, allowing agents and users to operate
concurrently without interference.
  We evaluate UFO2 across over 20 real-world Windows applications,
demonstrating substantial improvements in robustness and execution accuracy
over prior CUAs. Our results show that deep OS integration unlocks a scalable
path toward reliable, user-aligned desktop automation.

</details>


### [183] [Redefining Superalignment: From Weak-to-Strong Alignment to Human-AI Co-Alignment to Sustainable Symbiotic Society](https://arxiv.org/abs/2504.17404)
*Yi Zeng, Feifei Zhao, Yuwei Wang, Enmeng Lu, Yaodong Yang, Lei Wang, Chao Liu, Yitao Liang, Dongcheng Zhao, Bing Han, Haibo Tong, Yao Liang, Dongqi Liang, Kang Sun, Boyuan Chen, Jinyu Fan*

Main category: cs.AI

TL;DR: The paper addresses the challenge of superalignment in AI, proposing a dual framework of external oversight and intrinsic proactive alignment to ensure AI systems align with human values as they advance towards superintelligence.


<details>
  <summary>Details</summary>
Motivation: The increasing autonomy and potential superintelligence of AI systems pose risks of misalignment with human values, necessitating safer frameworks for superalignment.

Method: The paper redefines superalignment as human-AI co-alignment, integrating external oversight (human-centered decisions with automated evaluation) and intrinsic proactive alignment (self-awareness, empathy, and iterative interaction).

Result: The proposed framework aims to achieve sustainable human-AI symbiosis, ensuring AI systems remain aligned with evolving human values and intentions.

Conclusion: The integration of external oversight and intrinsic proactive alignment offers a pathway to safe and beneficial AGI and ASI, fostering a symbiotic human-AI society.

Abstract: Artificial Intelligence (AI) systems are becoming increasingly powerful and
autonomous, and may progress to surpass human intelligence levels, namely
Artificial Superintelligence (ASI). During the progression from AI to ASI, it
may exceed human control, violate human values, and even lead to irreversible
catastrophic consequences in extreme cases. This gives rise to a pressing issue
that needs to be addressed: superalignment, ensuring that AI systems much
smarter than humans, remain aligned with human (compatible) intentions and
values. Existing scalable oversight and weak-to-strong generalization methods
may prove substantially infeasible and inadequate when facing ASI. We must
explore safer and more pluralistic frameworks and approaches for
superalignment. In this paper, we redefine superalignment as the human-AI
co-alignment towards a sustainable symbiotic society, and highlight a framework
that integrates external oversight and intrinsic proactive alignment. External
oversight superalignment should be grounded in human-centered ultimate
decision, supplemented by interpretable automated evaluation and correction, to
achieve continuous alignment with humanity's evolving values. Intrinsic
proactive superalignment is rooted in a profound understanding of the Self,
others, and society, integrating self-awareness, self-reflection, and empathy
to spontaneously infer human intentions, distinguishing good from evil and
proactively considering human well-being, ultimately attaining human-AI
co-alignment through iterative interaction. The integration of
externally-driven oversight with intrinsically-driven proactive alignment
empowers sustainable symbiotic societies through human-AI co-alignment, paving
the way for achieving safe and beneficial AGI and ASI for good, for human, and
for a symbiotic ecology.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [184] [STNet: Prediction of Underwater Sound Speed Profiles with An Advanced Semi-Transformer Neural Network](https://arxiv.org/abs/2504.17912)
*Wei Huang, Jiajun Lu, Hao Zhang, Tianhe Xu*

Main category: cs.SD

TL;DR: A Semi-Transformer neural network (STNet) is proposed for accurate long-term underwater sound velocity profile (SSP) estimation without real-time data, outperforming existing methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current SSP measurement methods are either spatially limited or time-consuming, while inversion methods lack accuracy and spatial applicability. STNet addresses these limitations by leveraging historical data for SSP estimation.

Method: STNet uses an optimized self-attention mechanism and time encoding to capture temporal dependencies in historical SSP data, enabling accurate current or future SSP estimation.

Result: STNet achieves higher predictive accuracy and computational efficiency compared to state-of-the-art models.

Conclusion: STNet demonstrates potential for accurate long-term, full-depth ocean SSP forecasting, overcoming limitations of traditional methods.

Abstract: Real time acquisition of accurate underwater sound velocity profile (SSP) is
crucial for tracking the propagation trajectory of underwater acoustic signals,
making it play a key role in ocean communication positioning. SSPs can be
directly measured by instruments or inverted leveraging sound field data.
Although measurement techniques provide a good accuracy, they are constrained
by limited spatial coverage and require substantial time investment. The
inversion method based on real-time measurement of acoustic field data improves
operational efficiency, but loses the accuracy of SSP estimation and suffers
from limited spatial applicability due to its stringent requirements for ocean
observation infrastructure. To achieve accurate long-term ocean SSP estimation
independent of real-time underwater data measurements, we propose a
Semi-Transformer neural network (STNet) specifically designed for simulating
sound velocity distribution patterns from the perspective of time series
prediction. The proposed network architecture incorporates an optimized
self-attention mechanism to effectively capture long-range temporal
dependencies within historical sound velocity time-series data, facilitating
accurate estimation of current SSPs or prediction of future SSPs. Through
architectural optimization of the Transformer framework and integration of a
time encoding mechanism, STNet could effectively improve computational
efficiency. Comparative experimental results reveal that STNet outperforms
state-of-the-art models in predictive accuracy and maintain good computational
efficiency, demonstrating its potential for enabling accurate long-term
full-depth ocean SSP forecasting.

</details>


### [185] [Tracking Articulatory Dynamics in Speech with a Fixed-Weight BiLSTM-CNN Architecture](https://arxiv.org/abs/2504.18099)
*Leena G Pillai, D. Muhammad Noorul Mubarak, Elizabeth Sherly*

Main category: cs.SD

TL;DR: A novel BiLSTM-CNN model predicts tongue and lip articulatory features from speech acoustics, outperforming adaptive weights with minimal training epochs.


<details>
  <summary>Details</summary>
Motivation: Speech production involves complex articulatory coordination, and predicting these features accurately can advance speech research.

Method: Uses stacked BiLSTM with 1D CNN post-processing, trained on EMA and speech datasets with diverse linguistic and recording variations.

Result: Fixed weights initialization outperforms adaptive weights, achieving better performance with fewer training epochs.

Conclusion: The model enhances articulatory feature prediction, supporting advancements in speech production research and applications.

Abstract: Speech production is a complex sequential process which involve the
coordination of various articulatory features. Among them tongue being a highly
versatile active articulator responsible for shaping airflow to produce
targeted speech sounds that are intellectual, clear, and distinct. This paper
presents a novel approach for predicting tongue and lip articulatory features
involved in a given speech acoustics using a stacked Bidirectional Long
Short-Term Memory (BiLSTM) architecture, combined with a one-dimensional
Convolutional Neural Network (CNN) for post-processing with fixed weights
initialization. The proposed network is trained with two datasets consisting of
simultaneously recorded speech and Electromagnetic Articulography (EMA)
datasets, each introducing variations in terms of geographical origin,
linguistic characteristics, phonetic diversity, and recording equipment. The
performance of the model is assessed in Speaker Dependent (SD), Speaker
Independent (SI), corpus dependent (CD) and cross corpus (CC) modes.
Experimental results indicate that the proposed model with fixed weights
approach outperformed the adaptive weights initialization with in relatively
minimal number of training epochs. These findings contribute to the development
of robust and efficient models for articulatory feature prediction, paving the
way for advancements in speech production research and applications.

</details>


### [186] [EmoDubber: Towards High Quality and Emotion Controllable Movie Dubbing](https://arxiv.org/abs/2412.08988)
*Gaoxiang Cong, Jiadong Pan, Liang Li, Yuankai Qi, Yuxin Peng, Anton van den Hengel, Jian Yang, Qingming Huang*

Main category: cs.SD

TL;DR: EmoDubber is a novel dubbing architecture that improves lip sync, pronunciation, and allows emotion control in generated speech.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to balance audio-visual sync, clear pronunciation, and user-defined emotions.

Method: EmoDubber uses Lip-related Prosody Aligning (LPA) for lip sync, Pronunciation Enhancing (PE) for clarity, and Flow-based User Emotion Controlling (FUEC) for emotion synthesis.

Result: Outperforms state-of-the-art methods on three benchmark datasets.

Conclusion: EmoDubber effectively addresses deficiencies in current dubbing methods by integrating lip sync, pronunciation, and emotion control.

Abstract: Given a piece of text, a video clip, and a reference audio, the movie dubbing
task aims to generate speech that aligns with the video while cloning the
desired voice. The existing methods have two primary deficiencies: (1) They
struggle to simultaneously hold audio-visual sync and achieve clear
pronunciation; (2) They lack the capacity to express user-defined emotions. To
address these problems, we propose EmoDubber, an emotion-controllable dubbing
architecture that allows users to specify emotion type and emotional intensity
while satisfying high-quality lip sync and pronunciation. Specifically, we
first design Lip-related Prosody Aligning (LPA), which focuses on learning the
inherent consistency between lip motion and prosody variation by duration level
contrastive learning to incorporate reasonable alignment. Then, we design
Pronunciation Enhancing (PE) strategy to fuse the video-level phoneme sequences
by efficient conformer to improve speech intelligibility. Next, the speaker
identity adapting module aims to decode acoustics prior and inject the speaker
style embedding. After that, the proposed Flow-based User Emotion Controlling
(FUEC) is used to synthesize waveform by flow matching prediction network
conditioned on acoustics prior. In this process, the FUEC determines the
gradient direction and guidance scale based on the user's emotion instructions
by the positive and negative guidance mechanism, which focuses on amplifying
the desired emotion while suppressing others. Extensive experimental results on
three benchmark datasets demonstrate favorable performance compared to several
state-of-the-art methods.

</details>


### [187] [Coverage-Guaranteed Speech Emotion Recognition via Calibrated Uncertainty-Adaptive Prediction Sets](https://arxiv.org/abs/2503.22712)
*Zijun Jia*

Main category: cs.SD

TL;DR: A risk management framework for Speech Emotion Recognition (SER) ensures statistically rigorous correctness coverage, addressing overfitting and miscalibration in traditional methods. It includes a calibration set, binary loss function, and joint loss function to bound test loss by user-specified risk level. Evaluations confirm coverage and error control, with an extension for dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Road rage, driven by emotional outbursts, threatens safety. Traditional SER methods (e.g., HMMs, LSTMs) using 1D speech signals suffer from overfitting and miscalibration, necessitating a robust framework.

Method: Proposes a risk management framework with a calibration set, binary loss function, and joint loss function to adjust thresholds based on user-specified risk levels. Extends to small-batch online calibration for dynamic environments.

Result: Evaluations on 6 models and 2 datasets show strict maintenance of correctness coverage (≥1−α) and control of marginal error rates. Cross-dataset experiments validate dynamic environment performance.

Conclusion: The framework effectively addresses SER limitations, ensuring robustness in static and dynamic scenarios, with potential applications in road safety.

Abstract: Road rage, driven by emotional outbursts, endangers road and public safety.
Speech Emotion Recognition (SER) can detect early negative emotions to reduce
accidents, but traditional methods (e.g., HMMs, LSTMs) using 1D speech signals
face overfitting and miscalibration issues. This paper proposes a risk
management framework ensuring statistically rigorous correctness coverage for
test data. We separate a calibration set, design a binary loss function to
check if ground-truth labels are in prediction sets, calibrated by data-driven
threshold $\lambda$. A joint loss function on the calibration set adjusts
$\lambda$ according to user-specified risk level $\alpha$, bounding the test
loss expectation by $\alpha$. Evaluations on 6 models across 2 datasets show
our framework strictly maintains average correctness coverage $\geq 1-\alpha$
and controls marginal error rates under various calibration-test splits (e.g.,
0.1). Additionally, a small-batch online calibration framework based on local
exchangeability is proposed for complex scenarios with data domain offset or
non-IID batches. By constructing a non-negative test martingale, it ensures
prediction set coverage in dynamic environments, validated via cross-dataset
experiments.

</details>


### [188] [Spatial Audio Processing with Large Language Model on Wearable Devices](https://arxiv.org/abs/2504.08907)
*Ayushi Mishra, Yang Bai, Priyadarshan Narayanasamy, Nakul Garg, Nirupam Roy*

Main category: cs.SD

TL;DR: A novel system integrates spatial speech understanding into LLMs for wearable devices, using microstructure-based sensing and synthetic data, achieving improved accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Enhancing human-computer interaction in wearable tech by incorporating spatial context into LLMs for adaptive applications.

Method: Leverages microstructure-based spatial sensing for DoA, fuses spatial and linguistic embeddings, and fine-tunes with LoRA for on-device processing.

Result: Achieves 25.72° mean DoA error (vs. 88.52° in existing work) and 5.3 WER, supporting ASR and soundscaping.

Conclusion: The system advances spatial speech understanding, addressing power, privacy, and hardware constraints for AR and immersive applications.

Abstract: Integrating spatial context into large language models (LLMs) has the
potential to revolutionize human-computer interaction, particularly in wearable
devices. In this work, we present a novel system architecture that incorporates
spatial speech understanding into LLMs, enabling contextually aware and
adaptive applications for wearable technologies. Our approach leverages
microstructure-based spatial sensing to extract precise Direction of Arrival
(DoA) information using a monaural microphone. To address the lack of existing
dataset for microstructure-assisted speech recordings, we synthetically create
a dataset called OmniTalk by using the LibriSpeech dataset. This spatial
information is fused with linguistic embeddings from OpenAI's Whisper model,
allowing each modality to learn complementary contextual representations. The
fused embeddings are aligned with the input space of LLaMA-3.2 3B model and
fine-tuned with lightweight adaptation technique LoRA to optimize for on-device
processing. SING supports spatially-aware automatic speech recognition (ASR),
achieving a mean error of $25.72^\circ$-a substantial improvement compared to
the 88.52$^\circ$ median error in existing work-with a word error rate (WER) of
5.3. SING also supports soundscaping, for example, inference how many people
were talking and their directions, with up to 5 people and a median DoA error
of 16$^\circ$. Our system demonstrates superior performance in spatial speech
understanding while addressing the challenges of power efficiency, privacy, and
hardware constraints, paving the way for advanced applications in augmented
reality, accessibility, and immersive experiences.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [189] [CaRL: Learning Scalable Planning Policies with Simple Rewards](https://arxiv.org/abs/2504.17838)
*Bernhard Jaeger, Daniel Dauner, Jens Beißwenger, Simon Gerstenecker, Kashyap Chitta, Andreas Geiger*

Main category: cs.LG

TL;DR: The paper proposes a simple reward design for RL in autonomous driving, focusing on route completion, which outperforms complex reward methods and scales efficiently.


<details>
  <summary>Details</summary>
Motivation: Rule-based and complex reward RL methods for autonomous driving planning have scalability and optimization issues.

Method: Uses PPO with a simplified reward design (route completion) and penalizes infractions by terminating episodes or reducing rewards.

Result: Achieves 64 DS on CARLA and 91.3/90.6 on nuPlan benchmarks, outperforming prior RL methods.

Conclusion: A simple reward design enables scalable and high-performance RL for autonomous driving.

Abstract: We investigate reinforcement learning (RL) for privileged planning in
autonomous driving. State-of-the-art approaches for this task are rule-based,
but these methods do not scale to the long tail. RL, on the other hand, is
scalable and does not suffer from compounding errors like imitation learning.
Contemporary RL approaches for driving use complex shaped rewards that sum
multiple individual rewards, \eg~progress, position, or orientation rewards. We
show that PPO fails to optimize a popular version of these rewards when the
mini-batch size is increased, which limits the scalability of these approaches.
Instead, we propose a new reward design based primarily on optimizing a single
intuitive reward term: route completion. Infractions are penalized by
terminating the episode or multiplicatively reducing route completion. We find
that PPO scales well with higher mini-batch sizes when trained with our simple
reward, even improving performance. Training with large mini-batch sizes
enables efficient scaling via distributed data parallelism. We scale PPO to
300M samples in CARLA and 500M samples in nuPlan with a single 8-GPU node. The
resulting model achieves 64 DS on the CARLA longest6 v2 benchmark,
outperforming other RL methods with more complex rewards by a large margin.
Requiring only minimal adaptations from its use in CARLA, the same method is
the best learning-based approach on nuPlan. It scores 91.3 in non-reactive and
90.6 in reactive traffic on the Val14 benchmark while being an order of
magnitude faster than prior work.

</details>


### [190] [High-Performance Reinforcement Learning on Spot: Optimizing Simulation Parameters with Distributional Measures](https://arxiv.org/abs/2504.17857)
*A. J Miller, Fangzhou Yu, Michael Brauckmann, Farbod Farshidian*

Main category: cs.LG

TL;DR: First public demo of RL policy deployment on Boston Dynamics Spot, using Wasserstein Distance and MMD for sim2real gap, achieving high-speed locomotion and agility.


<details>
  <summary>Details</summary>
Motivation: To demonstrate end-to-end RL policy deployment on Spot hardware and quantify sim2real gaps for robust performance.

Method: Uses Wasserstein Distance and MMD to measure sim2real gap, optimizes simulated parameters with CMA-ES, and trains RL policies for multiple gaits.

Result: Policies achieve 5.2ms locomotion (3x default speed), robustness to slippery surfaces, and agility.

Conclusion: Successful deployment of RL policies on Spot, with code released to support future low-level API work.

Abstract: This work presents an overview of the technical details behind a high
performance reinforcement learning policy deployment with the Spot RL
Researcher Development Kit for low level motor access on Boston Dynamics Spot.
This represents the first public demonstration of an end to end end
reinforcement learning policy deployed on Spot hardware with training code
publicly available through Nvidia IsaacLab and deployment code available
through Boston Dynamics. We utilize Wasserstein Distance and Maximum Mean
Discrepancy to quantify the distributional dissimilarity of data collected on
hardware and in simulation to measure our sim2real gap. We use these measures
as a scoring function for the Covariance Matrix Adaptation Evolution Strategy
to optimize simulated parameters that are unknown or difficult to measure from
Spot. Our procedure for modeling and training produces high quality
reinforcement learning policies capable of multiple gaits, including a flight
phase. We deploy policies capable of over 5.2ms locomotion, more than triple
Spots default controller maximum speed, robustness to slippery surfaces,
disturbance rejection, and overall agility previously unseen on Spot. We detail
our method and release our code to support future work on Spot with the low
level API.

</details>


### [191] [Do We Need Transformers to Play FPS Video Games?](https://arxiv.org/abs/2504.17891)
*Karmanbir Batth, Krish Sethi, Aly Shariff, Leo Shi, Hetul Patel*

Main category: cs.LG

TL;DR: Transformer-based methods (DTQN and DT) were tested in Doom for RL, but traditional methods outperformed them.


<details>
  <summary>Details</summary>
Motivation: To explore Transformer architectures for RL in online (DTQN) and offline (DT) settings within the Doom environment.

Method: Used DTQN for online Q-learning and DT for offline learning from trajectories.

Result: Traditional methods outperformed Transformer-based approaches in Doom.

Conclusion: Transformers, though effective in Atari, are less suited for Doom compared to traditional RL methods.

Abstract: In this paper, we explore the Transformer based architectures for
reinforcement learning in both online and offline settings within the Doom game
environment. Our investigation focuses on two primary approaches: Deep
Transformer Q- learning Networks (DTQN) for online learning and Decision
Transformers (DT) for offline reinforcement learning. DTQN leverages the
sequential modelling capabilities of Transformers to enhance Q-learning in
partially observable environments,while Decision Transformers repurpose
sequence modelling techniques to enable offline agents to learn from past
trajectories without direct interaction with the environment. We conclude that
while Transformers might have performed well in Atari games, more traditional
methods perform better than Transformer based method in both the settings in
the VizDoom environment.

</details>


### [192] [The use of Multi-domain Electroencephalogram Representations in the building of Models based on Convolutional and Recurrent Neural Networks for Epilepsy Detection](https://arxiv.org/abs/2504.17908)
*Luiz Antonio Nicolau Anghinoni, Gustavo Weber Denardin, Jadson Castro Gertrudes, Dalcimar Casanova, Jefferson Tales Oliva*

Main category: cs.LG

TL;DR: This paper compares EEG data representations (time, frequency, time-frequency) for seizure detection using deep learning, finding frequency-domain data achieves over 97% accuracy.


<details>
  <summary>Details</summary>
Motivation: Manual EEG analysis for epilepsy diagnosis is inconsistent; automated solutions are needed, but the impact of EEG data representation on deep learning performance is unclear.

Method: Systematic comparison of deep neural networks trained on EEG data in time, frequency, and time-frequency domains, using statistical tests to evaluate performance.

Result: Frequency-domain data yields seizure detection metrics exceeding 97%, outperforming other representations.

Conclusion: Frequency-domain EEG data provides the most accurate and reliable foundation for automated seizure detection systems.

Abstract: Epilepsy, affecting approximately 50 million people globally, is
characterized by abnormal brain activity and remains challenging to treat. The
diagnosis of epilepsy relies heavily on electroencephalogram (EEG) data, where
specialists manually analyze epileptiform patterns across pre-ictal, ictal,
post-ictal, and interictal periods. However, the manual analysis of EEG signals
is prone to variability between experts, emphasizing the need for automated
solutions. Although previous studies have explored preprocessing techniques and
machine learning approaches for seizure detection, there is a gap in
understanding how the representation of EEG data (time, frequency, or
time-frequency domains) impacts the predictive performance of deep learning
models. This work addresses this gap by systematically comparing deep neural
networks trained on EEG data in these three domains. Through the use of
statistical tests, we identify the optimal data representation and model
architecture for epileptic seizure detection. The results demonstrate that
frequency-domain data achieves detection metrics exceeding 97\%, providing a
robust foundation for more accurate and reliable seizure detection systems.

</details>


### [193] [CANet: ChronoAdaptive Network for Enhanced Long-Term Time Series Forecasting under Non-Stationarity](https://arxiv.org/abs/2504.17913)
*Mert Sonmezer, Seyda Ertekin*

Main category: cs.LG

TL;DR: CANet, a novel architecture for long-term time series forecasting, addresses non-stationary data challenges using style-transfer techniques, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Real-world time series data often exhibit non-stationary properties, complicating forecasting. Existing models struggle with distribution shifts and over-stationarization.

Method: CANet integrates Non-stationary Adaptive Normalization (Style Blending Gate and AdaIN), multi-resolution patching, Fourier-based noise reduction, and a Stacked Kronecker Product Layer.

Result: CANet outperforms state-of-the-art methods, reducing MSE by 42% and MAE by 22% on real-world datasets.

Conclusion: CANet effectively handles non-stationary data, enhancing forecasting accuracy and efficiency, with publicly available code.

Abstract: Long-term time series forecasting plays a pivotal role in various real-world
applications. Despite recent advancements and the success of different
architectures, forecasting is often challenging due to non-stationary nature of
the real-world data, which frequently exhibit distribution shifts and temporal
changes in statistical properties like mean and variance over time. Previous
studies suggest that this inherent variability complicates forecasting,
limiting the performance of many models by leading to loss of non-stationarity
and resulting in over-stationarization (Liu, Wu, Wang and Long, 2022). To
address this challenge, we introduce a novel architecture, ChoronoAdaptive
Network (CANet), inspired by style-transfer techniques. The core of CANet is
the Non-stationary Adaptive Normalization module, seamlessly integrating the
Style Blending Gate and Adaptive Instance Normalization (AdaIN) (Huang and
Belongie, 2017). The Style Blending Gate preserves and reintegrates
non-stationary characteristics, such as mean and standard deviation, by
blending internal and external statistics, preventing over-stationarization
while maintaining essential temporal dependencies. Coupled with AdaIN, which
dynamically adapts the model to statistical changes, this approach enhances
predictive accuracy under non-stationary conditions. CANet also employs
multi-resolution patching to handle short-term fluctuations and long-term
trends, along with Fourier analysis-based adaptive thresholding to reduce
noise. A Stacked Kronecker Product Layer further optimizes the model's
efficiency while maintaining high performance. Extensive experiments on
real-world datasets validate CANet's superiority over state-of-the-art methods,
achieving a 42% reduction in MSE and a 22% reduction in MAE. The source code is
publicly available at https://github.com/mertsonmezer/CANet.

</details>


### [194] [Avoiding Leakage Poisoning: Concept Interventions Under Distribution Shifts](https://arxiv.org/abs/2504.17921)
*Mateo Espinosa Zarlenga, Gabriele Dominici, Pietro Barbiero, Zohreh Shams, Mateja Jamnik*

Main category: cs.LG

TL;DR: MixCEM, a new concept-based model, addresses leakage poisoning in OOD scenarios, outperforming baselines by improving accuracy with and without concept interventions.


<details>
  <summary>Details</summary>
Motivation: To study how concept-based models (CMs) handle OOD inputs and the impact of concept interventions, revealing a weakness (leakage poisoning) in current CMs.

Method: Introduces MixCEM, a CM that dynamically exploits leaked information for in-distribution cases, tested across tasks with varying concept annotations.

Result: MixCEM significantly improves accuracy for both in-distribution and OOD samples, with or without concept interventions.

Conclusion: MixCEM effectively mitigates leakage poisoning, enhancing CM performance in OOD scenarios.

Abstract: In this paper, we investigate how concept-based models (CMs) respond to
out-of-distribution (OOD) inputs. CMs are interpretable neural architectures
that first predict a set of high-level concepts (e.g., stripes, black) and then
predict a task label from those concepts. In particular, we study the impact of
concept interventions (i.e., operations where a human expert corrects a CM's
mispredicted concepts at test time) on CMs' task predictions when inputs are
OOD. Our analysis reveals a weakness in current state-of-the-art CMs, which we
term leakage poisoning, that prevents them from properly improving their
accuracy when intervened on for OOD inputs. To address this, we introduce
MixCEM, a new CM that learns to dynamically exploit leaked information missing
from its concepts only when this information is in-distribution. Our results
across tasks with and without complete sets of concept annotations demonstrate
that MixCEMs outperform strong baselines by significantly improving their
accuracy for both in-distribution and OOD samples in the presence and absence
of concept interventions.

</details>


### [195] [Multiple-Instance, Cascaded Classification for Keyword Spotting in Narrow-Band Audio](https://arxiv.org/abs/1711.08058)
*Ahmad AbdulKader, Kareem Nassar, Mohamed El-Geish, Daniel Galvez, Chetan Patil*

Main category: cs.LG

TL;DR: A cascaded classifier system for keyword spotting in narrow-band audio, using DNNs and multiple-feature representations to handle class imbalance and reduce power consumption.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of keyword spotting in non-IID environments with narrow-band audio, which is more difficult than typical KWS tasks.

Method: Combines Deep Neural Networks, cascading, multiple-feature representations, and multiple-instance learning to improve performance and efficiency.

Result: Achieves a false negative rate of 6% at an hourly false positive rate of 0.75.

Conclusion: The proposed system effectively handles the challenges of narrow-band audio and non-IID environments while optimizing power consumption.

Abstract: We propose using cascaded classifiers for a keyword spotting (KWS) task on
narrow-band (NB), 8kHz audio acquired in non-IID environments -- a more
challenging task than most state-of-the-art KWS systems face. We present a
model that incorporates Deep Neural Networks (DNNs), cascading,
multiple-feature representations, and multiple-instance learning. The cascaded
classifiers handle the task's class imbalance and reduce power consumption on
computationally-constrained devices via early termination. The KWS system
achieves a false negative rate of 6% at an hourly false positive rate of 0.75

</details>


### [196] [Causality-Driven Neural Network Repair: Challenges and Opportunities](https://arxiv.org/abs/2504.17946)
*Fatemeh Vares, Brittany Johnson*

Main category: cs.LG

TL;DR: The paper proposes using causal inference (causal debugging, counterfactual analysis, SCMs) to improve DNN robustness and interpretability, addressing fairness, adversarial attacks, and backdoors. Challenges like scalability and efficiency are noted.


<details>
  <summary>Details</summary>
Motivation: DNNs often rely on statistical correlations, lacking causal reasoning, which limits robustness and interpretability. Debugging and repairing DNNs is challenging.

Method: Leverages causal inference techniques (causal debugging, counterfactual analysis, SCMs) to identify and correct DNN failures.

Result: Causal methods support fairness, adversarial robustness, and backdoor mitigation through targeted interventions.

Conclusion: Key challenges (scalability, generalization, efficiency) remain, but causality-driven interventions show promise for enhancing DNN reliability.

Abstract: Deep Neural Networks (DNNs) often rely on statistical correlations rather
than causal reasoning, limiting their robustness and interpretability. While
testing methods can identify failures, effective debugging and repair remain
challenging. This paper explores causal inference as an approach primarily for
DNN repair, leveraging causal debugging, counterfactual analysis, and
structural causal models (SCMs) to identify and correct failures. We discuss in
what ways these techniques support fairness, adversarial robustness, and
backdoor mitigation by providing targeted interventions. Finally, we discuss
key challenges, including scalability, generalization, and computational
efficiency, and outline future directions for integrating causality-driven
interventions to enhance DNN reliability.

</details>


### [197] [Mathematics of Continual Learning](https://arxiv.org/abs/2504.17963)
*Liangzu Peng, René Vidal*

Main category: cs.LG

TL;DR: The paper explores connections between continual learning in machine learning and adaptive filtering in signal processing to enhance mathematical foundations and suggest research directions.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between the underdeveloped mathematical foundations of continual learning and the well-established principles of adaptive filtering.

Method: Comparative analysis of continual learning and adaptive filtering principles, highlighting their connections.

Result: Identified connections that can enhance continual learning's foundations and extend adaptive filtering insights.

Conclusion: The tutorial suggests research directions for continual learning inspired by adaptive filtering's historical developments.

Abstract: Continual learning is an emerging subject in machine learning that aims to
solve multiple tasks presented sequentially to the learner without forgetting
previously learned tasks. Recently, many deep learning based approaches have
been proposed for continual learning, however the mathematical foundations
behind existing continual learning methods remain underdeveloped. On the other
hand, adaptive filtering is a classic subject in signal processing with a rich
history of mathematically principled methods. However, its role in
understanding the foundations of continual learning has been underappreciated.
In this tutorial, we review the basic principles behind both continual learning
and adaptive filtering, and present a comparative analysis that highlights
multiple connections between them. These connections allow us to enhance the
mathematical foundations of continual learning based on existing results for
adaptive filtering, extend adaptive filtering insights using existing continual
learning methods, and discuss a few research directions for continual learning
suggested by the historical developments in adaptive filtering.

</details>


### [198] [Self-Balancing, Memory Efficient, Dynamic Metric Space Data Maintenance, for Rapid Multi-Kernel Estimation](https://arxiv.org/abs/2504.18003)
*Aditya S Ellendula, Chandrajit Bajaj*

Main category: cs.LG

TL;DR: A dynamic self-balancing octree for efficient neighborhood maintenance in evolving metric spaces, improving speed and accuracy in machine learning applications.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of fast, adaptive spatial organization in evolving metric spaces for modern machine learning systems.

Method: A two-parameter octree supporting logarithmic-time updates and queries, avoiding costly full rebuilds.

Result: Exponential speedups in four applications: Stein variational gradient descent, KNN classification, retrieval-augmented generation, and sample efficiency.

Conclusion: The octree structure significantly enhances efficiency and accuracy, especially in high-dimensional spaces.

Abstract: We present a dynamic self-balancing octree data structure that enables
efficient neighborhood maintenance in evolving metric spaces, a key challenge
in modern machine learning systems. Many learning and generative models operate
as dynamical systems whose representations evolve during training, requiring
fast, adaptive spatial organization. Our two-parameter octree supports
logarithmic-time updates and queries, eliminating the need for costly full
rebuilds as data distributions shift. We demonstrate its effectiveness in four
areas: (1) accelerating Stein variational gradient descent by supporting more
particles with lower overhead; (2) enabling real-time, incremental KNN
classification with logarithmic complexity; (3) facilitating efficient, dynamic
indexing and retrieval for retrieval-augmented generation; and (4) improving
sample efficiency by jointly optimizing input and latent spaces. Across all
applications, our approach yields exponential speedups while preserving
accuracy, particularly in high-dimensional spaces where maintaining adaptive
spatial structure is critical.

</details>


### [199] [TGDT: A Temporal Graph-based Digital Twin for Urban Traffic Corridors](https://arxiv.org/abs/2504.18008)
*Nooshin Yousefzadeh, Rahul Sengupta, Sanjay Ranka*

Main category: cs.LG

TL;DR: TGDT, a scalable framework combining Temporal Convolutional Networks and Attentional Graph Neural Networks, improves traffic modeling at urban corridors, outperforming existing methods in accuracy, robustness, and real-time performance.


<details>
  <summary>Details</summary>
Motivation: Urban congestion causes delays, economic losses, and emissions, but current deep learning models lack spatial generalizability and real-time deployment capabilities.

Method: TGDT integrates Temporal Convolutional Networks and Attentional Graph Neural Networks for dynamic, direction-aware traffic modeling, estimating key traffic measures at intersection and corridor levels.

Result: TGDT outperforms state-of-the-art baselines, providing high-dimensional, concurrent multi-output estimates with high robustness and accuracy, even in extreme scenarios.

Conclusion: TGDT offers a cost-effective, interpretable, and real-time solution for traffic signal optimization, capable of simulating thousands of scenarios quickly.

Abstract: Urban congestion at signalized intersections leads to significant delays,
economic losses, and increased emissions. Existing deep learning models often
lack spatial generalizability, rely on complex architectures, and struggle with
real-time deployment. To address these limitations, we propose the Temporal
Graph-based Digital Twin (TGDT), a scalable framework that integrates Temporal
Convolutional Networks and Attentional Graph Neural Networks for dynamic,
direction-aware traffic modeling and assessment at urban corridors. TGDT
estimates key Measures of Effectiveness (MOEs) for traffic flow optimization at
both the intersection level (e.g., queue length, waiting time) and the corridor
level (e.g., traffic volume, travel time). Its modular architecture and
sequential optimization scheme enable easy extension to any number of
intersections and MOEs. The model outperforms state-of-the-art baselines by
accurately producing high-dimensional, concurrent multi-output estimates. It
also demonstrates high robustness and accuracy across diverse traffic
conditions, including extreme scenarios, while relying on only a minimal set of
traffic features. Fully parallelized, TGDT can simulate over a thousand
scenarios within a matter of seconds, offering a cost-effective, interpretable,
and real-time solution for traffic signal optimization.

</details>


### [200] [Addressing Concept Mislabeling in Concept Bottleneck Models Through Preference Optimization](https://arxiv.org/abs/2504.18026)
*Emiliano Penaloza, Tianyue H. Zhan, Laurent Charlin, Mateo Espinosa Zarlenga*

Main category: cs.LG

TL;DR: CPO, a new loss function, improves CBM performance by reducing sensitivity to concept mislabeling, outperforming BCE in noisy datasets.


<details>
  <summary>Details</summary>
Motivation: CBMs rely on accurate concept labels, but real-world datasets often contain mislabeled concepts, degrading performance.

Method: Introduces Concept Preference Optimization (CPO), a loss function based on Direct Preference Optimization, to mitigate noise impact.

Result: CPO outperforms BCE in noisy datasets, showing less sensitivity to concept mislabeling and better posterior optimization.

Conclusion: CPO effectively addresses concept mislabeling in CBMs, enhancing robustness and performance in real-world applications.

Abstract: Concept Bottleneck Models (CBMs) propose to enhance the trustworthiness of AI
systems by constraining their decisions on a set of human understandable
concepts. However, CBMs typically assume that datasets contains accurate
concept labels an assumption often violated in practice, which we show can
significantly degrade performance (by 25% in some cases). To address this, we
introduce the Concept Preference Optimization (CPO) objective, a new loss
function based on Direct Preference Optimization, which effectively mitigates
the negative impact of concept mislabeling on CBM performance. We provide an
analysis on some key properties of the CPO objective showing it directly
optimizes for the concept's posterior distribution, and contrast it against
Binary Cross Entropy (BCE) where we show CPO is inherently less sensitive to
concept noise. We empirically confirm our analysis finding that CPO
consistently outperforms BCE in three real world datasets with and without
added label noise.

</details>


### [201] [Modes of Sequence Models and Learning Coefficients](https://arxiv.org/abs/2504.18048)
*Zhongtian Chen, Daniel Murfet*

Main category: cs.LG

TL;DR: The paper links data patterns in sequence modeling to the loss landscape of transformer networks, showing how truncating minor modes preserves dominant structure and how Local Learning Coefficient estimates are unaffected by insignificant modes.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between data patterns and the loss landscape in transformer networks, and to clarify why Local Learning Coefficient estimates remain reliable even without strict loss minimization.

Method: Uses a Hilbert-space framework for sequence distributions, applies tensor decompositions to identify principal modes, and truncates minor modes to create an effective data distribution. Theoretical analysis shows LLC estimates ignore insignificant modes.

Result: LLC estimates characterize the effective distribution's geometry, not the true one, explaining their reliability without strict loss minimization. The inverse temperature in SGLD acts as a resolution dial.

Conclusion: The study provides insights into the loss landscape's geometry in transformers, clarifying the role of LLC estimates and the impact of minor modes on practical calculations.

Abstract: We develop a geometric account of sequence modelling that links patterns in
the data to measurable properties of the loss landscape in transformer
networks. First, we cast conditional sequence distributions into a
Hilbert-space framework and apply tensor decompositions to identify their
principal modes. Truncating the small-amplitude modes yields an effective data
distribution that preserves dominant structure while discarding statistical
detail. Second, we show theoretically that Local Learning Coefficient (LLC)
estimates are insensitive to modes below a data-dependent threshold.
Consequently, the LLC calculated in practice characterises the geometry of the
effective rather than the true distribution. This insight clarifies why
reliable LLC estimates can be obtained even when a network parameter is not a
strict minimiser of the population loss, and it highlights how the inverse
temperature in SGLD acts as a resolution dial on the landscape structure.

</details>


### [202] [A Model Zoo on Phase Transitions in Neural Networks](https://arxiv.org/abs/2504.18072)
*Konstantin Schürholt, Léo Meynent, Yefan Zhou, Haiquan Lu, Yaoqing Yang, Damian Borth*

Main category: cs.LG

TL;DR: The paper introduces structured 'model zoos' for Weight Space Learning (WSL), combining phase information from statistical physics to ensure diversity. It provides 12 large-scale zoos covering various architectures, sizes, and datasets, validated with loss landscape metrics. Applications in model training, analysis, and sparsification are explored.


<details>
  <summary>Details</summary>
Motivation: Existing model zoos lack structured diversity, limiting WSL research. Combining phase information from statistical physics offers a controlled way to define diversity in model populations.

Method: Create 12 large-scale model zoos systematically covering known phases, varying architecture, size, and datasets. Compute loss landscape metrics to validate phase coverage.

Result: The zoos provide a structured resource for WSL, with evidence suggesting phase impacts model training, analysis, and sparsification. Exploratory studies demonstrate utility in transfer learning and weight averaging.

Conclusion: The structured model zoos enhance WSL research by providing controlled diversity and phase coverage, with broad applications in model analysis and optimization.

Abstract: Using the weights of trained Neural Network (NN) models as data modality has
recently gained traction as a research field - dubbed Weight Space Learning
(WSL). Multiple recent works propose WSL methods to analyze models, evaluate
methods, or synthesize weights. Weight space learning methods require
populations of trained models as datasets for development and evaluation.
However, existing collections of models - called `model zoos' - are
unstructured or follow a rudimentary definition of diversity. In parallel, work
rooted in statistical physics has identified phases and phase transitions in NN
models. Models are homogeneous within the same phase but qualitatively differ
from one phase to another. We combine the idea of `model zoos' with phase
information to create a controlled notion of diversity in populations. We
introduce 12 large-scale zoos that systematically cover known phases and vary
over model architecture, size, and datasets. These datasets cover different
modalities, such as computer vision, natural language processing, and
scientific ML. For every model, we compute loss landscape metrics and validate
full coverage of the phases. With this dataset, we provide the community with a
resource with a wide range of potential applications for WSL and beyond.
Evidence suggests the loss landscape phase plays a role in applications such as
model training, analysis, or sparsification. We demonstrate this in an
exploratory study of the downstream methods like transfer learning or model
weights averaging.

</details>


### [203] [Privacy-Preserving Personalized Federated Learning for Distributed Photovoltaic Disaggregation under Statistical Heterogeneity](https://arxiv.org/abs/2504.18078)
*Xiaolu Chen, Chenghao Huang, Yanru Zhang, Hao Wang*

Main category: cs.LG

TL;DR: A privacy-preserving distributed PV disaggregation framework using Personalized Federated Learning (PFL) is proposed to address challenges from unobservable PV generation and statistical heterogeneity.


<details>
  <summary>Details</summary>
Motivation: The rapid expansion of distributed PV installations complicates energy management due to unobservable generation and privacy concerns, necessitating accurate PV disaggregation.

Method: A two-level framework combines local transformer-based models for PV conditions and adaptive local aggregation, with global aggregation for cross-center knowledge sharing.

Result: Experiments show improved accuracy and robustness over benchmarks.

Conclusion: The PFL-based framework effectively addresses privacy and heterogeneity challenges in PV disaggregation.

Abstract: The rapid expansion of distributed photovoltaic (PV) installations worldwide,
many being behind-the-meter systems, has significantly challenged energy
management and grid operations, as unobservable PV generation further
complicates the supply-demand balance. Therefore, estimating this generation
from net load, known as PV disaggregation, is critical. Given privacy concerns
and the need for large training datasets, federated learning becomes a
promising approach, but statistical heterogeneity, arising from geographical
and behavioral variations among prosumers, poses new challenges to PV
disaggregation. To overcome these challenges, a privacy-preserving distributed
PV disaggregation framework is proposed using Personalized Federated Learning
(PFL). The proposed method employs a two-level framework that combines local
and global modeling. At the local level, a transformer-based PV disaggregation
model is designed to generate solar irradiance embeddings for representing
local PV conditions. A novel adaptive local aggregation mechanism is adopted to
mitigate the impact of statistical heterogeneity on the local model, extracting
a portion of global information that benefits the local model. At the global
level, a central server aggregates information uploaded from multiple data
centers, preserving privacy while enabling cross-center knowledge sharing.
Experiments on real-world data demonstrate the effectiveness of this proposed
framework, showing improved accuracy and robustness compared to benchmark
methods.

</details>


### [204] [Efficient GNN Training Through Structure-Aware Randomized Mini-Batching](https://arxiv.org/abs/2504.18082)
*Vignesh Balaji, Christos Kozyrakis, Gal Chechik, Haggai Maron*

Main category: cs.LG

TL;DR: COMM-RAND bridges randomness and graph structure in GNN mini-batching, improving efficiency without compromising accuracy.


<details>
  <summary>Details</summary>
Motivation: Current mini-batching techniques for GNNs ignore efficiency and structural properties, leading to suboptimal performance.

Method: Introduces Community-structure-aware Randomized Mini-batching (COMM-RAND) to balance randomness and graph structure.

Result: Reduces training time by up to 2.76x (1.8x avg) with minimal accuracy loss (1.79% max, 0.42% avg).

Conclusion: COMM-RAND offers a practical solution for efficient GNN training while maintaining accuracy.

Abstract: Graph Neural Networks (GNNs) enable learning on realworld graphs and
mini-batch training has emerged as the de facto standard for training GNNs
because it can scale to very large graphs and improve convergence. Current
mini-batch construction policies largely ignore efficiency considerations of
GNN training. Specifically, existing mini-batching techniques employ
randomization schemes to improve accuracy and convergence. However, these
randomization schemes are often agnostic to the structural properties of the
graph (for eg. community structure), resulting in highly irregular memory
access patterns during GNN training that make suboptimal use of on-chip GPU
caches. On the other hand, while deterministic mini-batching based solely on
graph structure delivers fast runtime performance, the lack of randomness
compromises both the final model accuracy and training convergence speed. In
this paper, we present Community-structure-aware Randomized Mini-batching
(COMM-RAND), a novel methodology that bridges the gap between the above
extremes. COMM-RAND allows practitioners to explore the space between pure
randomness and pure graph structural awareness during mini-batch construction,
leading to significantly more efficient GNN training with similar accuracy. We
evaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND
cuts down GNN training time by up to 2.76x (1.8x on average) while achieving an
accuracy that is within 1.79% points (0.42% on average) compared to popular
random mini-batching approaches.

</details>


### [205] [Reliable and Efficient Inverse Analysis using Physics-Informed Neural Networks with Distance Functions and Adaptive Weight Tuning](https://arxiv.org/abs/2504.18091)
*Shota Deguchi, Mitsuteru Asai*

Main category: cs.LG

TL;DR: The paper proposes using R-functions to enforce boundary conditions in physics-informed neural networks (PINNs), improving accuracy and efficiency over traditional penalty-based methods, especially for inverse problems.


<details>
  <summary>Details</summary>
Motivation: Current PINN methods struggle with boundary condition accuracy due to reliance on penalty terms, which are sensitive to parameter choices and don't guarantee exact boundary satisfaction.

Method: The paper introduces R-functions to represent boundary geometries accurately and extends this to inverse problems with an adaptive weight tuning technique.

Result: Numerical experiments show the method outperforms penalty-based approaches in accuracy and efficiency, even for complex non-convex geometries.

Conclusion: The R-function-based approach provides a reliable and efficient framework for inverse analysis in PINNs, with broad engineering applications.

Abstract: Physics-informed neural networks have attracted significant attention in
scientific machine learning for their capability to solve forward and inverse
problems governed by partial differential equations. However, the accuracy of
PINN solutions is often limited by the treatment of boundary conditions.
Conventional penalty-based methods, which incorporate boundary conditions as
penalty terms in the loss function, cannot guarantee exact satisfaction of the
given boundary conditions and are highly sensitive to the choice of penalty
parameters. This paper demonstrates that distance functions, specifically
R-functions, can be leveraged to enforce boundary conditions, overcoming these
limitations. R-functions provide normalized distance fields, enabling accurate
representation of boundary geometries, including non-convex domains, and
facilitating various types of boundary conditions. We extend this distance
function-based boundary condition imposition method to inverse problems using
PINNs and introduce an adaptive weight tuning technique to ensure reliable and
efficient inverse analysis. We demonstrate the efficacy of the method through
several numerical experiments. Numerical results show that the proposed method
solves inverse problems more accurately and efficiently than penalty-based
methods, even in the presence of complex non-convex geometries. This approach
offers a reliable and efficient framework for inverse analysis using PINNs,
with potential applications across a wide range of engineering problems.

</details>


### [206] [Subject-independent Classification of Meditative State from the Resting State using EEG](https://arxiv.org/abs/2504.18095)
*Jerrin Thomas Panachakel, Pradeep Kumar G., Suryaa Seran, Kanishka Sharma, Ramakrishnan Angarai Ganesan*

Main category: cs.LG

TL;DR: The study distinguishes Rajyoga meditation from resting state using EEG data with three architectures, achieving high accuracy for intra- and inter-subject classification.


<details>
  <summary>Details</summary>
Motivation: To objectively identify meditation states in a subject-independent manner, addressing limitations of subject-dependent results in existing research.

Method: Three architectures: CSP-LDA (feature extraction and classification), CSP-LDA-LSTM (sequence learning for classification), and SVD-NN (component selection and classification).

Result: CSP-LDA-LSTM achieved 98.2% accuracy (intra-subject), SVD-NN achieved 96.4% accuracy (inter-subject), comparable to literature benchmarks.

Conclusion: The architectures robustly classify meditation states across subjects, demonstrating generalization capability.

Abstract: While it is beneficial to objectively determine whether a subject is
meditating, most research in the literature reports good results only in a
subject-dependent manner. This study aims to distinguish the modified state of
consciousness experienced during Rajyoga meditation from the resting state of
the brain in a subject-independent manner using EEG data. Three architectures
have been proposed and evaluated: The CSP-LDA Architecture utilizes common
spatial pattern (CSP) for feature extraction and linear discriminant analysis
(LDA) for classification. The CSP-LDA-LSTM Architecture employs CSP for feature
extraction, LDA for dimensionality reduction, and long short-term memory (LSTM)
networks for classification, modeling the binary classification problem as a
sequence learning problem. The SVD-NN Architecture uses singular value
decomposition (SVD) to select the most relevant components of the EEG signals
and a shallow neural network (NN) for classification. The CSP-LDA-LSTM
architecture gives the best performance with 98.2% accuracy for intra-subject
classification. The SVD-NN architecture provides significant performance with
96.4\% accuracy for inter-subject classification. This is comparable to the
best-reported accuracies in the literature for intra-subject classification.
Both architectures are capable of capturing subject-invariant EEG features for
effectively classifying the meditative state from the resting state. The high
intra-subject and inter-subject classification accuracies indicate these
systems' robustness and their ability to generalize across different subjects.

</details>


### [207] [Temperature Estimation in Induction Motors using Machine Learning](https://arxiv.org/abs/2504.18105)
*Dinan Li, Panagiotis Kakosimos*

Main category: cs.LG

TL;DR: The paper explores machine-learning methods to estimate temperatures in induction motors, finding neural networks effective even under transient conditions.


<details>
  <summary>Details</summary>
Motivation: To prevent failures and ensure reliable operation in electrified powertrains by monitoring internal temperatures, avoiding the need for expert knowledge and complex modeling.

Method: Investigates various machine-learning algorithms (linear to neural networks) using experimental lab data from a powertrain, with hyperparameter optimization.

Result: Neural networks perform satisfactorily in approximating stator winding and bearing temperatures, even under transient conditions.

Conclusion: Data-driven approaches, particularly neural networks, are viable for temperature estimation in induction motors, offering a simpler alternative to conventional methods.

Abstract: The number of electrified powertrains is ever increasing today towards a more
sustainable future; thus, it is essential that unwanted failures are prevented,
and a reliable operation is secured. Monitoring the internal temperatures of
motors and keeping them under their thresholds is an important first step.
Conventional modeling methods require expert knowledge and complicated
mathematical approaches. With all the data a modern electric drive collects
nowadays during the system operation, it is feasible to apply data-driven
approaches for estimating thermal behaviors. In this paper, multiple
machine-learning methods are investigated on their capability to approximate
the temperatures of the stator winding and bearing in induction motors. The
explored algorithms vary from linear to neural networks. For this reason,
experimental lab data have been captured from a powertrain under predetermined
operating conditions. For each approach, a hyperparameter search is then
performed to find the optimal configuration. All the models are evaluated by
various metrics, and it has been found that neural networks perform
satisfactorily even under transient conditions.

</details>


### [208] [Contrastive Learning and Adversarial Disentanglement for Task-Oriented Semantic Communications](https://arxiv.org/abs/2410.22784)
*Omar Erak, Omar Alhussein, Wen Tong*

Main category: cs.LG

TL;DR: CLAD, a novel method using contrastive learning and adversarial disentanglement, improves task-oriented semantic communication by better separating task-relevant and irrelevant information, enhancing performance and privacy.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to fully disentangle task-relevant and irrelevant information, causing privacy issues and poor performance.

Method: CLAD combines contrastive learning to capture task-relevant features and adversarial disentanglement to discard irrelevant information. It also introduces the information retention index (IRI) to measure feature minimality and informativeness.

Result: CLAD outperforms baselines, improving predictive performance by 2.5-3%, reducing IRI by 77-90%, and lowering adversarial attack accuracy by 57-76%.

Conclusion: CLAD effectively addresses privacy and performance issues in task-oriented semantic communication, offering a robust solution with measurable improvements.

Abstract: Task-oriented semantic communication systems have emerged as a promising
approach to achieving efficient and intelligent data transmission, where only
information relevant to a specific task is communicated. However, existing
methods struggle to fully disentangle task-relevant and task-irrelevant
information, leading to privacy concerns and subpar performance. To address
this, we propose an information-bottleneck method, named CLAD (contrastive
learning and adversarial disentanglement). CLAD utilizes contrastive learning
to effectively capture task-relevant features while employing adversarial
disentanglement to discard task-irrelevant information. Additionally, due to
the lack of reliable and reproducible methods to gain insight into the
informativeness and minimality of the encoded feature vectors, we introduce a
new technique to compute the information retention index (IRI), a comparative
metric used as a proxy for the mutual information between the encoded features
and the input, reflecting the minimality of the encoded features. The IRI
quantifies the minimality and informativeness of the encoded feature vectors
across different task-oriented communication techniques. Our extensive
experiments demonstrate that CLAD outperforms state-of-the-art baselines in
terms of semantic extraction, task performance, privacy preservation, and IRI.
CLAD achieves a predictive performance improvement of around 2.5-3%, along with
a 77-90% reduction in IRI and a 57-76% decrease in adversarial attribute
inference attack accuracy.

</details>


### [209] [Learning from Less: SINDy Surrogates in RL](https://arxiv.org/abs/2504.18113)
*Aniket Dixit, Muhammad Ibrahim Khan, Faizan Ahmed, James Brusey*

Main category: cs.LG

TL;DR: The paper proposes using SINDy to create efficient surrogate environments for RL, reducing computational costs by 20-35% while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the high computational costs in RL by developing accurate and interpretable surrogate environments.

Method: Uses Sparse Identification of Nonlinear Dynamics (SINDy) to model RL environments, tested in OpenAI Gym (Mountain Car, Lunar Lander).

Result: Achieves high accuracy (correlations >0.997, low MSE) with fewer interactions, reducing training steps (e.g., 65,075 vs. 100,000 for Mountain Car).

Conclusion: SINDy-based surrogate environments offer an efficient, accurate alternative for model-based RL, with potential for broader application.

Abstract: This paper introduces an approach for developing surrogate environments in
reinforcement learning (RL) using the Sparse Identification of Nonlinear
Dynamics (SINDy) algorithm. We demonstrate the effectiveness of our approach
through extensive experiments in OpenAI Gym environments, particularly Mountain
Car and Lunar Lander. Our results show that SINDy-based surrogate models can
accurately capture the underlying dynamics of these environments while reducing
computational costs by 20-35%. With only 75 interactions for Mountain Car and
1000 for Lunar Lander, we achieve state-wise correlations exceeding 0.997, with
mean squared errors as low as 3.11e-06 for Mountain Car velocity and 1.42e-06
for LunarLander position. RL agents trained in these surrogate environments
require fewer total steps (65,075 vs. 100,000 for Mountain Car and 801,000 vs.
1,000,000 for Lunar Lander) while achieving comparable performance to those
trained in the original environments, exhibiting similar convergence patterns
and final performance metrics. This work contributes to the field of
model-based RL by providing an efficient method for generating accurate,
interpretable surrogate environments.

</details>


### [210] [Think, Prune, Train, Improve: Scaling Reasoning without Scaling Models](https://arxiv.org/abs/2504.18116)
*Caia Costello, Simon Guo, Anna Goldie, Azalia Mirhoseini*

Main category: cs.LG

TL;DR: The paper explores how synthetic data and iterative fine-tuning improve LLM performance, introducing the Think, Prune, Train process, which boosts results on tasks like GSM8K.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of high-quality training data for LLMs and enhance their programming and mathematical reasoning capabilities.

Method: Introduces the Think, Prune, Train framework, iteratively fine-tuning models on their own reasoning traces with ground-truth pruning.

Result: Significant performance improvements: Gemma2-2B (57.6% from 41.9%), Gemma2-9B (82%), and LLaMA-3.1-70B (91%, surpassing GPT-4o).

Conclusion: Self-generated reasoning and systematic data selection effectively enhance LLM capabilities.

Abstract: Large language models (LLMs) have demonstrated strong capabilities in
programming and mathematical reasoning tasks, but are constrained by limited
high-quality training data. Synthetic data can be leveraged to enhance
fine-tuning outcomes, but several factors influence this process, including
model size, synthetic data volume, pruning strategy, and number of fine-tuning
rounds. We explore these axes and investigate which conditions enable model
self-improvement. We introduce the Think, Prune, Train process, a scalable
framework that iteratively fine-tunes models on their own reasoning traces,
using ground-truth pruning to ensure high-quality training data. This approach
yields improved performance: on GSM8K, Gemma2-2B achieves a Pass@1 of 57.6%
(from 41.9%), Gemma2-9B reaches 82%, matching LLaMA-3.1-70B, and LLaMA-3.1-70B
attains 91%, even surpassing GPT-4o, demonstrating the effectiveness of
self-generated reasoning and systematic data selection for improving LLM
capabilities.

</details>


### [211] [Score-Based Deterministic Density Sampling](https://arxiv.org/abs/2504.18130)
*Vasily Ilin, Bamdad Hosseini, Jingwei Hu*

Main category: cs.LG

TL;DR: A deterministic sampling framework, SBTM, is proposed for sampling unnormalized target densities using score-based transport modeling, offering smooth, interpretable trajectories and convergence diagnostics.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of sampling when only the gradient of the log target density is known, unlike diffusion models requiring pre-training.

Method: Uses score matching to approximate Wasserstein gradient flow on KL divergence, learning time-varying scores dynamically.

Result: SBTM matches the convergence rate of exact gradient flow, provides smooth trajectories, and integrates well with annealed dynamics.

Conclusion: SBTM outperforms ULA and annealed ULA, offering efficient, noise-free sampling with theoretical guarantees.

Abstract: We propose and analyze a deterministic sampling framework using Score-Based
Transport Modeling (SBTM) for sampling an unnormalized target density $\pi$.
While diffusion generative modeling relies on pre-training the score function
$\nabla \log f_t$ using samples from $\pi$, SBTM addresses the more general and
challenging setting where only $\nabla \log\pi$ is known. SBTM approximates the
Wasserstein gradient flow on KL$(f_t\|\pi)$ by learning the time-varying score
$\nabla \log f_t$ on the fly using score matching. The learned score gives
immediate access to relative Fisher information, providing a built-in
convergence diagnostic. The deterministic trajectories are smooth,
interpretable, and free of Brownian-motion noise, while having the same
distribution as ULA. We prove that SBTM dissipates relative entropy at the same
rate as the exact gradient flow, provided sufficient training. We further
extend our framework to annealed dynamics, to handle non log-concave targets.
Numerical experiments validate our theoretical findings: SBTM converges at the
optimal rate, has smooth trajectories, and is easily integrated with annealed
dynamics. We compare to the baselines of ULA and annealed ULA.

</details>


### [212] [Tree Boosting Methods for Balanced andImbalanced Classification and their Robustness Over Time in Risk Assessment](https://arxiv.org/abs/2504.18133)
*Gissel Velarde, Michael Weichert, Anuj Deshmunkh, Sanjay Deshmane, Anindya Sudhir, Khushboo Sharma, Vaibhav Joshi*

Main category: cs.LG

TL;DR: The paper evaluates tree boosting methods (XGBoost and Imbalance-XGBoost) on imbalanced datasets, showing improved performance with more data but declining F1 scores as imbalance increases. Hyper-parameter optimization helps, while sampling does not.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of detecting minority classes in imbalanced datasets for risk assessment using machine learning.

Method: Evaluates tree boosting methods with hyper-parameter optimization on datasets of varying sizes (1K, 10K, 100K) and class distributions (50%, 45%, 25%, 5% positives).

Result: Performance improves with more data but declines as imbalance increases. Hyper-parameter optimization enhances recognition, but sampling worsens detection.

Conclusion: The method is robust to data variation over time, with retraining recommended when performance deteriorates.

Abstract: Most real-world classification problems deal with imbalanced datasets, posing
a challenge for Artificial Intelligence (AI), i.e., machine learning
algorithms, because the minority class, which is of extreme interest, often
proves difficult to be detected. This paper empirically evaluates tree boosting
methods' performance given different dataset sizes and class distributions,
from perfectly balanced to highly imbalanced. For tabular data, tree-based
methods such as XGBoost, stand out in several benchmarks due to detection
performance and speed. Therefore, XGBoost and Imbalance-XGBoost are evaluated.
After introducing the motivation to address risk assessment with machine
learning, the paper reviews evaluation metrics for detection systems or binary
classifiers. It proposes a method for data preparation followed by tree
boosting methods including hyper-parameter optimization. The method is
evaluated on private datasets of 1 thousand (K), 10K and 100K samples on
distributions with 50, 45, 25, and 5 percent positive samples. As expected, the
developed method increases its recognition performance as more data is given
for training and the F1 score decreases as the data distribution becomes more
imbalanced, but it is still significantly superior to the baseline of
precision-recall determined by the ratio of positives divided by positives and
negatives. Sampling to balance the training set does not provide consistent
improvement and deteriorates detection. In contrast, classifier hyper-parameter
optimization improves recognition, but should be applied carefully depending on
data volume and distribution. Finally, the developed method is robust to data
variation over time up to some point. Retraining can be used when performance
starts deteriorating.

</details>


### [213] [A Generative Graph Contrastive Learning Model with Global Signal](https://arxiv.org/abs/2504.18148)
*Xiaofan Wei, Binyan Zhang*

Main category: cs.LG

TL;DR: CSG2L improves GCL by addressing noise and unequal sample pair importance, using SVD-aug and LGDL modules.


<details>
  <summary>Details</summary>
Motivation: Existing GCL models suffer from noise and uniform weighting of sample pairs, degrading performance.

Method: Proposes SVD-aug for noise-free augmentation and LGDL with adaptive reweighting for sample pair differentiation.

Result: CSG2L outperforms baselines on benchmarks and is compatible with various GNNs.

Conclusion: CSG2L effectively enhances GCL by mitigating noise and improving sample pair weighting.

Abstract: Graph contrastive learning (GCL) has garnered significant attention recently
since it learns complex structural information from graphs through
self-supervised learning manner. However, prevalent GCL models may suffer from
performance degradation due to inappropriate contrastive signals. Concretely,
they commonly generate augmented views based on random perturbation, which
leads to biased essential structures due to the introduction of noise. In
addition, they assign equal weight to both hard and easy sample pairs, thereby
ignoring the difference in importance of the sample pairs. To address these
issues, this study proposes a novel Contrastive Signal Generative Framework for
Accurate Graph Learning (CSG2L) with the following two-fold ideas: a) building
a singular value decomposition (SVD)-directed augmented module (SVD-aug) to
obtain the global interactions as well as avoiding the random noise
perturbation; b) designing a local-global dependency learning module (LGDL)
with an adaptive reweighting strategy which can differentiate the effects of
hard and easy sample pairs. Extensive experiments on benchmark datasets
demonstrate that the proposed CSG2L outperforms the state-of-art baselines.
Moreover, CSG2L is compatible with a variety of GNNs.

</details>


### [214] [Offline Learning of Controllable Diverse Behaviors](https://arxiv.org/abs/2504.18160)
*Mathieu Petitbois, Rémy Portelas, Sylvain Lamprier, Ludovic Denoyer*

Main category: cs.LG

TL;DR: The paper proposes a method for Imitation Learning (IL) that ensures temporal consistency and controllability to better replicate diverse human behaviors.


<details>
  <summary>Details</summary>
Motivation: Traditional IL methods focus on expert datasets for single policies or handle diversity poorly. Recent extensions lack consistency and controllability.

Method: The approach uses Temporal Consistency for episode-wide behavior and a latent space for controlled trajectory generation.

Result: The method is compared to state-of-the-art techniques across diverse tasks and environments.

Conclusion: The proposed method improves diversity replication and controlled behavior generation in IL.

Abstract: Imitation Learning (IL) techniques aim to replicate human behaviors in
specific tasks. While IL has gained prominence due to its effectiveness and
efficiency, traditional methods often focus on datasets collected from experts
to produce a single efficient policy. Recently, extensions have been proposed
to handle datasets of diverse behaviors by mainly focusing on learning
transition-level diverse policies or on performing entropy maximization at the
trajectory level. While these methods may lead to diverse behaviors, they may
not be sufficient to reproduce the actual diversity of demonstrations or to
allow controlled trajectory generation. To overcome these drawbacks, we propose
a different method based on two key features: a) Temporal Consistency that
ensures consistent behaviors across entire episodes and not just at the
transition level as well as b) Controllability obtained by constructing a
latent space of behaviors that allows users to selectively activate specific
behaviors based on their requirements. We compare our approach to
state-of-the-art methods over a diverse set of tasks and environments. Project
page: https://mathieu-petitbois.github.io/projects/swr/

</details>


### [215] [Unveiling 3D Ocean Biogeochemical Provinces: A Machine Learning Approach for Systematic Clustering and Validation](https://arxiv.org/abs/2504.18181)
*Yvonne Jenniges, Maike Sonnewald, Sebastian Maneth, Are Olsen, Boris P. Koch*

Main category: cs.LG

TL;DR: A data-driven ML approach objectively defines North Atlantic ocean regions using clustering methods, with UMAP-DBSCAN performing best, validated for reproducibility and detail.


<details>
  <summary>Details</summary>
Motivation: To objectively define ocean regions and water masses, avoiding subjective decisions that may lead to misleading results.

Method: Applied clustering methods (KMeans, Ward, DBSCAN) on 300M salinity, temperature, and nutrient measurements, using UMAP for dimensionality reduction and NEMI for aggregation.

Result: UMAP-DBSCAN performed best, producing 321 clusters with high reproducibility (88.81% overlap) and detailed regionalization.

Conclusion: The method is objective, efficient, and reproducible, offering improved detail over traditional approaches like Longhurst provinces.

Abstract: Defining ocean regions and water masses helps to understand marine processes
and can serve downstream-tasks such as defining marine protected areas.
However, such definitions are often a result of subjective decisions
potentially producing misleading, unreproducible results. Here, the aim was to
objectively define regions of the North Atlantic. For this, a data-driven,
systematic machine learning approach was applied to generate and validate ocean
clusters employing external, internal and relative validation techniques. About
300 million measured salinity, temperature, and oxygen, nitrate, phosphate and
silicate concentration values served as input for various clustering methods
(KMeans, agglomerative Ward, and Density-Based Spatial Clustering of
Applications with Noise (DBSCAN)). Uniform Manifold Approximation and
Projection (UMAP) emphasised (dis-)similarities in the data while reducing
dimensionality. Based on a systematic validation of the considered clustering
methods and their hyperparameters, the results showed that UMAP-DBSCAN best
represented the data. To address stochastic variability, 100 UMAP-DBSCAN
clustering runs were conducted and aggregated using Native Emergent Manifold
Interrogation (NEMI), producing a final set of 321 clusters. Reproducibility
was evaluated by calculating the ensemble overlap (88.81 +- 1.8%) and the mean
grid cell-wise uncertainty estimated by NEMI (15.49 +- 20%). The presented
clustering results agreed very well with common water mass definitions. This
study revealed a more detailed regionalization compared to previous concepts
such as the Longhurst provinces. The applied method is objective, efficient and
reproducible and will support future research focusing on biogeochemical
differences and changes in oceanic regions.

</details>


### [216] [An Open-Source and Reproducible Implementation of LSTM and GRU Networks for Time Series Forecasting](https://arxiv.org/abs/2504.18185)
*Gissel Velarde, Pedro Branez, Alejandro Bueno, Rodrigo Heredia, Mateo Lopez-Ledezma*

Main category: cs.LG

TL;DR: The paper presents an open-source LSTM and GRU implementation for time series forecasting, tested on financial and synthetic datasets, showing strong performance on synthetic data but matching a baseline on stock data.


<details>
  <summary>Details</summary>
Motivation: To provide a reproducible and open-source implementation of LSTM and GRU networks for time series forecasting, addressing the need for transparency and comparability in research.

Method: Implemented LSTM and GRU networks, evaluated on two datasets (S&P BSE BANKEX stock data and synthetic Activities data), using RMSE and DA metrics.

Result: LSTM and GRU outperformed the baseline on synthetic data but matched it on stock data, likely due to the unpredictable nature of financial series.

Conclusion: The work demonstrates the effectiveness of LSTM and GRU for patterned time series, with open-source release for reproducibility and future research.

Abstract: This paper introduces an open-source and reproducible implementation of Long
Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) Networks for time
series forecasting. We evaluated LSTM and GRU networks because of their
performance reported in related work. We describe our method and its results on
two datasets. The first dataset is the S&P BSE BANKEX, composed of stock time
series (closing prices) of ten financial institutions. The second dataset,
called Activities, comprises ten synthetic time series resembling weekly
activities with five days of high activity and two days of low activity. We
report Root Mean Squared Error (RMSE) between actual and predicted values, as
well as Directional Accuracy (DA). We show that a single time series from a
dataset can be used to adequately train the networks if the sequences in the
dataset contain patterns that repeat, even with certain variation, and are
properly processed. For 1-step ahead and 20-step ahead forecasts, LSTM and GRU
networks significantly outperform a baseline on the Activities dataset. The
baseline simply repeats the last available value. On the stock market dataset,
the networks perform just like the baseline, possibly due to the nature of
these series. We release the datasets used as well as the implementation with
all experiments performed to enable future comparisons and to make our research
reproducible.

</details>


### [217] [A Machine Learning Approach For Bitcoin Forecasting](https://arxiv.org/abs/2504.18206)
*Stefano Sossi-Rojas, Gissel Velarde, Damian Zieba*

Main category: cs.LG

TL;DR: Bitcoin forecasting improves with Open, High, and Low price series, especially Low, using a GRU ensemble, matching state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods using only closing prices are insufficient for accurate Bitcoin price forecasting.

Method: Introduced new time series (Open, High, Low) and tested machine learning ensembles, focusing on GRU networks.

Result: Open, High, and Low series, particularly Low, enhance directional accuracy; non-price features are negligible.

Conclusion: The proposed GRU ensemble with Open, High, and Low series achieves state-of-the-art directional accuracy.

Abstract: Bitcoin is one of the cryptocurrencies that is gaining more popularity in
recent years. Previous studies have shown that closing price alone is not
enough to forecast stock market series. We introduce a new set of time series
and demonstrate that a subset is necessary to improve directional accuracy
based on a machine learning ensemble. In our experiments, we study which time
series and machine learning algorithms deliver the best results. We found that
the most relevant time series that contribute to improving directional accuracy
are Open, High and Low, with the largest contribution of Low in combination
with an ensemble of Gated Recurrent Unit network and a baseline forecast. The
relevance of other Bitcoin-related features that are not price-related is
negligible. The proposed method delivers similar performance to the
state-of-the-art when observing directional accuracy.

</details>


### [218] [Gradient Descent as a Shrinkage Operator for Spectral Bias](https://arxiv.org/abs/2504.18207)
*Simon Lucey*

Main category: cs.LG

TL;DR: The paper explores how activation functions and gradient descent influence spectral bias in shallow networks, proposing a link between GD hyperparameters and bandwidth control.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of activation functions and GD on spectral bias in neural networks, and to optimize frequency component retention.

Method: Analyzes the relationship between activation functions and spline regression, reinterprets GD as a shrinkage operator, and links GD hyperparameters to bandwidth.

Result: GD implicitly controls spectral bias by masking Jacobian singular values, with regularization effectiveness tied to monotonic activations. Non-monotonic activations (e.g., sinc, Gaussian) offer iteration-efficient spectral bias alternatives.

Conclusion: The study provides insights into spectral bias control via activation functions and GD, suggesting non-monotonic activations for efficient spectral bias management.

Abstract: We generalize the connection between activation function and spline
regression/smoothing and characterize how this choice may influence spectral
bias within a 1D shallow network. We then demonstrate how gradient descent (GD)
can be reinterpreted as a shrinkage operator that masks the singular values of
a neural network's Jacobian. Viewed this way, GD implicitly selects the number
of frequency components to retain, thereby controlling the spectral bias. An
explicit relationship is proposed between the choice of GD hyperparameters
(learning rate & number of iterations) and bandwidth (the number of active
components). GD regularization is shown to be effective only with monotonic
activation functions. Finally, we highlight the utility of non-monotonic
activation functions (sinc, Gaussian) as iteration-efficient surrogates for
spectral bias.

</details>


### [219] [Ultra-fast feature learning for the training of two-layer neural networks in the two-timescale regime](https://arxiv.org/abs/2504.18208)
*Raphaël Barboni, Gabriel Peyré, François-Xavier Vialard*

Main category: cs.LG

TL;DR: The paper analyzes gradient methods for training mean-field single hidden layer neural networks, focusing on convergence rates using a Variable Projection approach. It shows the feature distribution dynamics resemble a weighted ultra-fast diffusion equation, enabling provable convergence to the teacher feature distribution.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of proving convergence rates for neural network training, particularly when features are not fixed, by leveraging a separable non-linear least-square problem structure.

Method: The paper employs a Variable Projection (VarPro) or two-timescale learning algorithm to eliminate linear variables, reducing the problem to training the feature distribution. It analyzes the dynamics as a weighted ultra-fast diffusion equation.

Result: In the vanishing regularization limit, the feature distribution dynamics converge to the teacher feature distribution, with guarantees derived from PDE asymptotic behavior.

Conclusion: The proposed method provides provable convergence rates for feature distribution training in a teacher-student setup, offering theoretical insights into neural network training dynamics.

Abstract: We study the convergence of gradient methods for the training of mean-field
single hidden layer neural networks with square loss. Observing this is a
separable non-linear least-square problem which is linear w.r.t. the outer
layer's weights, we consider a Variable Projection (VarPro) or two-timescale
learning algorithm, thereby eliminating the linear variables and reducing the
learning problem to the training of the feature distribution. Whereas most
convergence rates or the training of neural networks rely on a neural tangent
kernel analysis where features are fixed, we show such a strategy enables
provable convergence rates for the sampling of a teacher feature distribution.
Precisely, in the limit where the regularization strength vanishes, we show
that the dynamic of the feature distribution corresponds to a weighted
ultra-fast diffusion equation. Relying on recent results on the asymptotic
behavior of such PDEs, we obtain guarantees for the convergence of the trained
feature distribution towards the teacher feature distribution in a
teacher-student setup.

</details>


### [220] [Learning to fuse: dynamic integration of multi-source data for accurate battery lifespan prediction](https://arxiv.org/abs/2504.18230)
*He Shanxuan, Lin Zuhong, Yu Bolun, Gao Xu, Long Biao, Yao Jingjing*

Main category: cs.LG

TL;DR: A hybrid learning framework for lithium-ion battery lifespan prediction integrates multi-source data fusion and stacked ensemble modeling, achieving high accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Accurate battery lifespan prediction is crucial for reliability and cost reduction in applications like electric vehicles and smart grids.

Method: The study combines dynamic multi-source data fusion with a stacked ensemble model (Ridge regression, LSTM, XGBoost) and an entropy-based weighting mechanism.

Result: The model achieves MAE of 0.0058, RMSE of 0.0092, and R2 of 0.9839, outperforming baselines with 46.2% R2 improvement and 83.2% RMSE reduction.

Conclusion: The scalable, interpretable framework enhances battery health management, identifying key aging indicators like Qdlin and Temp_m.

Abstract: Accurate prediction of lithium-ion battery lifespan is vital for ensuring
operational reliability and reducing maintenance costs in applications like
electric vehicles and smart grids. This study presents a hybrid learning
framework for precise battery lifespan prediction, integrating dynamic
multi-source data fusion with a stacked ensemble (SE) modeling approach. By
leveraging heterogeneous datasets from the National Aeronautics and Space
Administration (NASA), Center for Advanced Life Cycle Engineering (CALCE),
MIT-Stanford-Toyota Research Institute (TRC), and nickel cobalt aluminum (NCA)
chemistries, an entropy-based dynamic weighting mechanism mitigates variability
across heterogeneous datasets. The SE model combines Ridge regression, long
short-term memory (LSTM) networks, and eXtreme Gradient Boosting (XGBoost),
effectively capturing temporal dependencies and nonlinear degradation patterns.
It achieves a mean absolute error (MAE) of 0.0058, root mean square error
(RMSE) of 0.0092, and coefficient of determination (R2) of 0.9839,
outperforming established baseline models with a 46.2% improvement in R2 and an
83.2% reduction in RMSE. Shapley additive explanations (SHAP) analysis
identifies differential discharge capacity (Qdlin) and temperature of
measurement (Temp_m) as critical aging indicators. This scalable, interpretable
framework enhances battery health management, supporting optimized maintenance
and safety across diverse energy storage systems, thereby contributing to
improved battery health management in energy storage systems.

</details>


### [221] [DualRAG: A Dual-Process Approach to Integrate Reasoning and Retrieval for Multi-Hop Question Answering](https://arxiv.org/abs/2504.18243)
*Rong Cheng, Jinyi Liu, YAN ZHENG, Fei Ni, Jiazhen Du, Hangyu Mao, Fuzheng Zhang, Bo Wang, Jianye HAO*

Main category: cs.LG

TL;DR: DualRAG is a dual-process framework integrating reasoning and retrieval for Multi-Hop Question Answering, improving accuracy and coherence.


<details>
  <summary>Details</summary>
Motivation: Existing MHQA approaches struggle with dynamic knowledge organization, necessitating a better integration of reasoning and retrieval.

Method: DualRAG combines Reasoning-augmented Querying (RaQ) and progressive Knowledge Aggregation (pKA) for iterative knowledge enrichment.

Result: DualRAG enhances answer accuracy and coherence, sometimes outperforming oracle knowledge access.

Conclusion: DualRAG is a robust solution for complex multi-hop reasoning tasks, adaptable across model scales.

Abstract: Multi-Hop Question Answering (MHQA) tasks permeate real-world applications,
posing challenges in orchestrating multi-step reasoning across diverse
knowledge domains. While existing approaches have been improved with iterative
retrieval, they still struggle to identify and organize dynamic knowledge. To
address this, we propose DualRAG, a synergistic dual-process framework that
seamlessly integrates reasoning and retrieval. DualRAG operates through two
tightly coupled processes: Reasoning-augmented Querying (RaQ) and progressive
Knowledge Aggregation (pKA). They work in concert: as RaQ navigates the
reasoning path and generates targeted queries, pKA ensures that newly acquired
knowledge is systematically integrated to support coherent reasoning. This
creates a virtuous cycle of knowledge enrichment and reasoning refinement.
Through targeted fine-tuning, DualRAG preserves its sophisticated reasoning and
retrieval capabilities even in smaller-scale models, demonstrating its
versatility and core advantages across different scales. Extensive experiments
demonstrate that this dual-process approach substantially improves answer
accuracy and coherence, approaching, and in some cases surpassing, the
performance achieved with oracle knowledge access. These results establish
DualRAG as a robust and efficient solution for complex multi-hop reasoning
tasks.

</details>


### [222] [Local Statistical Parity for the Estimation of Fair Decision Trees](https://arxiv.org/abs/2504.18262)
*Andrea Quintanilla, Johan Van Horebeek*

Main category: cs.LG

TL;DR: The paper introduces C-LRT, a modified CART algorithm for fair decision tree estimation, balancing accuracy and fairness.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational complexity and fairness concerns in decision tree estimation.

Method: Proposes C-LRT, using locally linear classifiers and fairness constraints, modifying CART.

Result: C-LRT effectively balances accuracy and fairness in evaluations.

Conclusion: C-LRT successfully integrates fairness into decision tree estimation.

Abstract: Given the high computational complexity of decision tree estimation,
classical methods construct a tree by adding one node at a time in a recursive
way. To facilitate promoting fairness, we propose a fairness criterion local to
the tree nodes. We prove how it is related to the Statistical Parity criterion,
popular in the Algorithmic Fairness literature, and show how to incorporate it
into standard recursive tree estimation algorithms.
  We present a tree estimation algorithm called Constrained Logistic Regression
Tree (C-LRT), which is a modification of the standard CART algorithm using
locally linear classifiers and imposing restrictions as done in Constrained
Logistic Regression.
  Finally, we evaluate the performance of trees estimated with C-LRT on
datasets commonly used in the Algorithmic Fairness literature, using various
classification and fairness metrics. The results confirm that C-LRT
successfully allows to control and balance accuracy and fairness.

</details>


### [223] [Neural operators struggle to learn complex PDEs in pedestrian mobility: Hughes model case study](https://arxiv.org/abs/2504.18267)
*Prajwal Chauhan, Salah Eddine Choutri, Mohamed Ghattassi, Nader Masmoudi, Saif Eddin Jabari*

Main category: cs.LG

TL;DR: The paper evaluates neural operators' performance in solving the Hughes model for crowd dynamics, finding they struggle with complex scenarios involving discontinuities and dynamic boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To assess the limitations of neural operators in learning solutions for the Hughes model, a nonlinear hyperbolic system with complex structures like shocks and discontinuities.

Method: Three neural operators (Fourier, Wavelet, and Multiwavelet) are tested on scenarios with discontinuous/Gaussian initial conditions and diverse boundary conditions, using different numerical schemes.

Result: Neural operators perform well in simple scenarios but fail in complex ones, producing smoother solutions that lose physical features like shocks, indicating unintended regularization effects.

Conclusion: Current neural operator architectures may not adequately capture transport dynamics with discontinuities, raising concerns for applications like traffic modeling where shock preservation is critical.

Abstract: This paper investigates the limitations of neural operators in learning
solutions for a Hughes model, a first-order hyperbolic conservation law system
for crowd dynamics. The model couples a Fokker-Planck equation representing
pedestrian density with a Hamilton-Jacobi-type (eikonal) equation. This Hughes
model belongs to the class of nonlinear hyperbolic systems that often exhibit
complex solution structures, including shocks and discontinuities. In this
study, we assess the performance of three state-of-the-art neural operators
(Fourier Neural Operator, Wavelet Neural Operator, and Multiwavelet Neural
Operator) in various challenging scenarios. Specifically, we consider (1)
discontinuous and Gaussian initial conditions and (2) diverse boundary
conditions, while also examining the impact of different numerical schemes.
  Our results show that these neural operators perform well in easy scenarios
with fewer discontinuities in the initial condition, yet they struggle in
complex scenarios with multiple initial discontinuities and dynamic boundary
conditions, even when trained specifically on such complex samples. The
predicted solutions often appear smoother, resulting in a reduction in total
variation and a loss of important physical features. This smoothing behavior is
similar to issues discussed by Daganzo (1995), where models that introduce
artificial diffusion were shown to miss essential features such as shock waves
in hyperbolic systems. These results suggest that current neural operator
architectures may introduce unintended regularization effects that limit their
ability to capture transport dynamics governed by discontinuities. They also
raise concerns about generalizing these methods to traffic applications where
shock preservation is essential.

</details>


### [224] [Studying Small Language Models with Susceptibilities](https://arxiv.org/abs/2504.18274)
*Garrett Baker, George Wang, Jesse Hoogland, Daniel Murfet*

Main category: cs.LG

TL;DR: A linear response framework for neural network interpretability treats networks as Bayesian systems, using perturbations to measure changes in posterior expectations and derive attribution scores.


<details>
  <summary>Details</summary>
Motivation: To provide a method for interpreting neural networks by analyzing their responses to controlled data perturbations, linking local learning with global behavior.

Method: Uses small perturbations of the data distribution (e.g., shifting text sources) to induce first-order changes in posterior expectations, estimated efficiently with local SGLD samples. Attribution scores are derived from per-token contributions.

Result: The framework identifies functional modules (e.g., multigram and induction heads) in a transformer and links local learning coefficients with linear-response theory.

Conclusion: The method quantifies how data distribution shifts deform the loss landscape, offering a scalable and interpretable approach to understanding neural networks.

Abstract: We develop a linear response framework for interpretability that treats a
neural network as a Bayesian statistical mechanical system. A small, controlled
perturbation of the data distribution, for example shifting the Pile toward
GitHub or legal text, induces a first-order change in the posterior expectation
of an observable localized on a chosen component of the network. The resulting
susceptibility can be estimated efficiently with local SGLD samples and
factorizes into signed, per-token contributions that serve as attribution
scores. Building a set of perturbations (probes) yields a response matrix whose
low-rank structure separates functional modules such as multigram and induction
heads in a 3M-parameter transformer. Susceptibilities link local learning
coefficients from singular learning theory with linear-response theory, and
quantify how local loss landscape geometry deforms under shifts in the data
distribution.

</details>


### [225] [A comprehensive review of classifier probability calibration metrics](https://arxiv.org/abs/2504.18278)
*Richard Oliver Lane*

Main category: cs.LG

TL;DR: The paper reviews 82 probability calibration metrics for AI/ML models, categorizing them into four classifier families and one object detection family, providing equations for implementation.


<details>
  <summary>Details</summary>
Motivation: AI/ML models often produce inaccurate confidence values, impacting trust and safety. Calibration metrics help assess the alignment between confidence and accuracy.

Method: Comprehensive review and categorization of 82 calibration metrics, with equations provided for implementation.

Result: Identified four classifier families (point-based, bin-based, kernel/curve-based, cumulative) and one object detection family of metrics.

Conclusion: The paper aids researchers in understanding and implementing calibration metrics, enhancing model reliability and trust.

Abstract: Probabilities or confidence values produced by artificial intelligence (AI)
and machine learning (ML) models often do not reflect their true accuracy, with
some models being under or over confident in their predictions. For example, if
a model is 80% sure of an outcome, is it correct 80% of the time? Probability
calibration metrics measure the discrepancy between confidence and accuracy,
providing an independent assessment of model calibration performance that
complements traditional accuracy metrics. Understanding calibration is
important when the outputs of multiple systems are combined, for assurance in
safety or business-critical contexts, and for building user trust in models.
This paper provides a comprehensive review of probability calibration metrics
for classifier and object detection models, organising them according to a
number of different categorisations to highlight their relationships. We
identify 82 major metrics, which can be grouped into four classifier families
(point-based, bin-based, kernel or curve-based, and cumulative) and an object
detection family. For each metric, we provide equations where available,
facilitating implementation and comparison by future researchers.

</details>


### [226] [Deep Reinforcement Learning Based Navigation with Macro Actions and Topological Maps](https://arxiv.org/abs/2504.18300)
*Simon Hakenes, Tobias Glasmachers*

Main category: cs.LG

TL;DR: A method using object-oriented macro actions and topological maps enables efficient navigation in complex environments with sparse rewards, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of navigation in large, visually complex environments with sparse rewards.

Method: Uses object-oriented macro actions grounded in a topological map with a DQN, reducing RL complexity.

Result: Outperforms random baselines in photorealistic 3D simulation, showing sample-efficient learning.

Conclusion: Topological structure and macro-level abstraction enable efficient learning from pixel data.

Abstract: This paper addresses the challenge of navigation in large, visually complex
environments with sparse rewards. We propose a method that uses object-oriented
macro actions grounded in a topological map, allowing a simple Deep Q-Network
(DQN) to learn effective navigation policies. The agent builds a map by
detecting objects from RGBD input and selecting discrete macro actions that
correspond to navigating to these objects. This abstraction drastically reduces
the complexity of the underlying reinforcement learning problem and enables
generalization to unseen environments. We evaluate our approach in a
photorealistic 3D simulation and show that it significantly outperforms a
random baseline under both immediate and terminal reward conditions. Our
results demonstrate that topological structure and macro-level abstraction can
enable sample-efficient learning even from pixel data.

</details>


### [227] [SSA-UNet: Advanced Precipitation Nowcasting via Channel Shuffling](https://arxiv.org/abs/2504.18309)
*Marco Turzi, Siamak Mehrkanoon*

Main category: cs.LG

TL;DR: SSA-UNet improves SmaAt-UNet with a shuffle channeling mechanism for better weather forecasting, tested on Dutch and French datasets with Grad-CAM analysis.


<details>
  <summary>Details</summary>
Motivation: Enhance weather forecasting by reducing complexity and improving adaptability in deep learning models compared to NWP.

Method: Introduces SSA-UNet, a modified SmaAt-UNet with shuffle channeling, tested on precipitation and cloud cover datasets with Grad-CAM for analysis.

Result: Evaluated three output configurations (1, 6, 12 maps) and identified key input regions for predictions using Grad-CAM.

Conclusion: SSA-UNet offers a promising, less complex alternative for weather forecasting, with potential for further optimization.

Abstract: Weather forecasting is essential for facilitating diverse socio-economic
activity and environmental conservation initiatives. Deep learning techniques
are increasingly being explored as complementary approaches to Numerical
Weather Prediction (NWP) models, offering potential benefits such as reduced
complexity and enhanced adaptability in specific applications. This work
presents a novel design, Small Shuffled Attention UNet (SSA-UNet), which
enhances SmaAt-UNet's architecture by including a shuffle channeling mechanism
to optimize performance and diminish complexity. To assess its efficacy, this
architecture and its reduced variant are examined and trained on two datasets:
a Dutch precipitation dataset from 2016 to 2019, and a French cloud cover
dataset containing radar images from 2017 to 2018. Three output configurations
of the proposed architecture are evaluated, yielding outputs of 1, 6, and 12
precipitation maps, respectively. To better understand how this model operates
and produces its predictions, a gradient-based approach called Grad-CAM is used
to analyze the outputs generated. The analysis of heatmaps generated by
Grad-CAM facilitated the identification of regions within the input maps that
the model considers most informative for generating its predictions. The
implementation of SSA-UNet can be found on our
Github\footnote{\href{https://github.com/MarcoTurzi/SSA-UNet}{https://github.com/MarcoTurzi/SSA-UNet}}

</details>


### [228] [PHEATPRUNER: Interpretable Data-centric Feature Selection for Multivariate Time Series Classification through Persistent Homology](https://arxiv.org/abs/2504.18329)
*Anh-Duy Pham, Olivier Basole Kashongwe, Martin Atzmueller, Tim Römer*

Main category: cs.LG

TL;DR: PHeatPruner combines persistent homology and sheaf theory to balance performance and interpretability in multivariate time series classification, pruning variables while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of balancing performance and interpretability in high-dimensional, complex time series data.

Method: Integrates persistent homology for variable pruning and sheaf theory for explanatory vectors, validated on UEA Archive and mastitis detection datasets.

Result: Prunes up to 45% of variables without accuracy loss, simplifies data, and provides actionable insights without added complexity.

Conclusion: PHeatPruner effectively bridges complexity reduction and interpretability, with potential applications across fields.

Abstract: Balancing performance and interpretability in multivariate time series
classification is a significant challenge due to data complexity and high
dimensionality. This paper introduces PHeatPruner, a method integrating
persistent homology and sheaf theory to address these challenges. Persistent
homology facilitates the pruning of up to 45% of the applied variables while
maintaining or enhancing the accuracy of models such as Random Forest,
CatBoost, XGBoost, and LightGBM, all without depending on posterior
probabilities or supervised optimization algorithms. Concurrently, sheaf theory
contributes explanatory vectors that provide deeper insights into the data's
structural nuances. The approach was validated using the UEA Archive and a
mastitis detection dataset for dairy cows. The results demonstrate that
PHeatPruner effectively preserves model accuracy. Furthermore, our results
highlight PHeatPruner's key features, i.e. simplifying complex data and
offering actionable insights without increasing processing time or complexity.
This method bridges the gap between complexity reduction and interpretability,
suggesting promising applications in various fields.

</details>


### [229] [Testing Individual Fairness in Graph Neural Networks](https://arxiv.org/abs/2504.18353)
*Roya Nasiri*

Main category: cs.LG

TL;DR: This PhD project aims to develop a testing framework for assessing and ensuring individual fairness in Graph Neural Networks (GNNs), addressing biases that propagate through interconnected nodes.


<details>
  <summary>Details</summary>
Motivation: Biases in AI models, especially GNNs, can lead to discriminatory automated decisions. Existing research lacks focus on individual fairness in GNNs, which is complicated by their relational nature.

Method: The project involves a systematic literature review to create a taxonomy of individual fairness, followed by developing a testing framework for GNNs, adapting existing fairness techniques.

Result: The framework will be evaluated using industrial case studies, particularly on graph-based large language models.

Conclusion: The project seeks to advance fairness in GNNs by providing tools to detect and mitigate biases, ensuring equitable automated decision-making.

Abstract: The biases in artificial intelligence (AI) models can lead to automated
decision-making processes that discriminate against groups and/or individuals
based on sensitive properties such as gender and race. While there are many
studies on diagnosing and mitigating biases in various AI models, there is
little research on individual fairness in Graph Neural Networks (GNNs). Unlike
traditional models, which treat data features independently and overlook their
inter-relationships, GNNs are designed to capture graph-based structure where
nodes are interconnected. This relational approach enables GNNs to model
complex dependencies, but it also means that biases can propagate through these
connections, complicating the detection and mitigation of individual fairness
violations. This PhD project aims to develop a testing framework to assess and
ensure individual fairness in GNNs. It first systematically reviews the
literature on individual fairness, categorizing existing approaches to define,
measure, test, and mitigate model biases, creating a taxonomy of individual
fairness. Next, the project will develop a framework for testing and ensuring
fairness in GNNs by adapting and extending current fairness testing and
mitigation techniques. The framework will be evaluated through industrial case
studies, focusing on graph-based large language models.

</details>


### [230] [Explainable AI for UAV Mobility Management: A Deep Q-Network Approach for Handover Minimization](https://arxiv.org/abs/2504.18371)
*Irshad A. Meer, Bruno Hörmann, Mustafa Ozger, Fabien Geyer, Alberto Viseras, Dominic Schupke, Cicek Cavdar*

Main category: cs.LG

TL;DR: The paper proposes an explainable AI (XAI) framework using SHAP to improve interpretability in RL-based UAV handover decisions, validated with real-world data.


<details>
  <summary>Details</summary>
Motivation: Addressing the black-box nature of RL-based methods in UAV mobility management to enhance interpretability and reliability.

Method: Introduces an XAI framework with SHAP to analyze state parameters (RSRP, RSRQ, buffer status, UAV position) in DQN-based handover decisions.

Result: The framework provides intuitive explanations for policy decisions, bridging AI models and human understanding.

Conclusion: The XAI approach enhances interpretability and reliability of RL-based UAV handover solutions.

Abstract: The integration of unmanned aerial vehicles (UAVs) into cellular networks
presents significant mobility management challenges, primarily due to frequent
handovers caused by probabilistic line-of-sight conditions with multiple ground
base stations (BSs). To tackle these challenges, reinforcement learning
(RL)-based methods, particularly deep Q-networks (DQN), have been employed to
optimize handover decisions dynamically. However, a major drawback of these
learning-based approaches is their black-box nature, which limits
interpretability in the decision-making process. This paper introduces an
explainable AI (XAI) framework that incorporates Shapley Additive Explanations
(SHAP) to provide deeper insights into how various state parameters influence
handover decisions in a DQN-based mobility management system. By quantifying
the impact of key features such as reference signal received power (RSRP),
reference signal received quality (RSRQ), buffer status, and UAV position, our
approach enhances the interpretability and reliability of RL-based handover
solutions. To validate and compare our framework, we utilize real-world network
performance data collected from UAV flight trials. Simulation results show that
our method provides intuitive explanations for policy decisions, effectively
bridging the gap between AI-driven models and human decision-makers.

</details>


### [231] [Model Evaluation in the Dark: Robust Classifier Metrics with Missing Labels](https://arxiv.org/abs/2504.18385)
*Danial Dervovic, Michael Cashmore*

Main category: cs.LG

TL;DR: The paper addresses missing labels in model evaluation, proposing a multiple imputation technique to avoid bias, especially for MNAR data, and validates its effectiveness empirically.


<details>
  <summary>Details</summary>
Motivation: Missing labels during evaluation can introduce bias, particularly when data is Missing Not At Random (MNAR), a problem overlooked in prior work.

Method: A multiple imputation technique is proposed for evaluating classifiers using metrics like precision, recall, and ROC-AUC, providing both point estimates and predictive distributions.

Result: Empirical results show the predictive distribution's accuracy, even for MNAR data, and confirm its approximate Gaussian nature with finite-sample convergence bounds.

Conclusion: The method is robust and valid under realistic error models, offering a reliable solution for handling missing labels in evaluation.

Abstract: Missing data in supervised learning is well-studied, but the specific issue
of missing labels during model evaluation has been overlooked. Ignoring samples
with missing values, a common solution, can introduce bias, especially when
data is Missing Not At Random (MNAR). We propose a multiple imputation
technique for evaluating classifiers using metrics such as precision, recall,
and ROC-AUC. This method not only offers point estimates but also a predictive
distribution for these quantities when labels are missing. We empirically show
that the predictive distribution's location and shape are generally correct,
even in the MNAR regime. Moreover, we establish that this distribution is
approximately Gaussian and provide finite-sample convergence bounds.
Additionally, a robustness proof is presented, confirming the validity of the
approximation under a realistic error model.

</details>


### [232] [Machine Learning and Statistical Insights into Hospital Stay Durations: The Italian EHR Case](https://arxiv.org/abs/2504.18393)
*Marina Andric, Mauro Dragoni*

Main category: cs.LG

TL;DR: The study identifies factors affecting hospital stay length in Italy using data from 60+ facilities (2020-2023), finding correlations with age, comorbidities, admission type, and month. CatBoost achieved the best prediction (R2=0.49).


<details>
  <summary>Details</summary>
Motivation: To optimize hospital resource management by understanding factors influencing length of stay (LoS) in Italian healthcare.

Method: Analyzed hospitalization records using features like patient demographics, comorbidities, and admission details. Employed CatBoost and Random Forest for prediction.

Result: Found significant LoS correlations with age, comorbidity score, admission type, and month. CatBoost outperformed with R2=0.49.

Conclusion: Machine learning, especially CatBoost, effectively predicts LoS, aiding hospital resource planning.

Abstract: Length of hospital stay is a critical metric for assessing healthcare quality
and optimizing hospital resource management. This study aims to identify
factors influencing LoS within the Italian healthcare context, using a dataset
of hospitalization records from over 60 healthcare facilities in the Piedmont
region, spanning from 2020 to 2023. We explored a variety of features,
including patient characteristics, comorbidities, admission details, and
hospital-specific factors. Significant correlations were found between LoS and
features such as age group, comorbidity score, admission type, and the month of
admission. Machine learning models, specifically CatBoost and Random Forest,
were used to predict LoS. The highest R2 score, 0.49, was achieved with
CatBoost, demonstrating good predictive performance.

</details>


### [233] [Three Types of Calibration with Properties and their Semantic and Formal Relationships](https://arxiv.org/abs/2504.18395)
*Rabanus Derr, Jessie Finocchiaro, Robert C. Williamson*

Main category: cs.LG

TL;DR: The paper explores calibration in predictive systems, introducing two motivations (self-realization and precise loss estimation) and defining prototypical notions like Γ-calibration and decision calibration. It unifies these for binary and higher-dimensional outcomes under distribution calibration.


<details>
  <summary>Details</summary>
Motivation: The work is motivated by the fragmented landscape of calibration definitions, aiming to clarify their purposes and relationships, particularly in contexts of algorithmic fairness and trustworthiness.

Method: The paper introduces two prototypical definitions: Γ-calibration for self-realization (linked to swap regret) and a modified decision calibration for precise loss estimation. It extends these to binary and higher-dimensional outcomes.

Result: The results show that both prototypical definitions can be unified under distribution calibration, with connections to omniprediction and actuarial fairness.

Conclusion: The paper provides a semantic map of calibration, clarifying fragmented notions and highlighting the role of groupings in multicalibration.

Abstract: Fueled by discussions around "trustworthiness" and algorithmic fairness,
calibration of predictive systems has regained scholars attention. The vanilla
definition and understanding of calibration is, simply put, on all days on
which the rain probability has been predicted to be p, the actual frequency of
rain days was p. However, the increased attention has led to an immense variety
of new notions of "calibration." Some of the notions are incomparable, serve
different purposes, or imply each other. In this work, we provide two accounts
which motivate calibration: self-realization of forecasted properties and
precise estimation of incurred losses of the decision makers relying on
forecasts. We substantiate the former via the reflection principle and the
latter by actuarial fairness. For both accounts we formulate prototypical
definitions via properties $\Gamma$ of outcome distributions, e.g., the mean or
median. The prototypical definition for self-realization, which we call
$\Gamma$-calibration, is equivalent to a certain type of swap regret under
certain conditions. These implications are strongly connected to the
omniprediction learning paradigm. The prototypical definition for precise loss
estimation is a modification of decision calibration adopted from Zhao et al.
[73]. For binary outcome sets both prototypical definitions coincide under
appropriate choices of reference properties. For higher-dimensional outcome
sets, both prototypical definitions can be subsumed by a natural extension of
the binary definition, called distribution calibration with respect to a
property. We conclude by commenting on the role of groupings in both accounts
of calibration often used to obtain multicalibration. In sum, this work
provides a semantic map of calibration in order to navigate a fragmented
terrain of notions and definitions.

</details>


### [234] [Online learning to accelerate nonlinear PDE solvers: applied to multiphase porous media flow](https://arxiv.org/abs/2504.18414)
*Vinicius L S Silva, Pablo Salinas, Claire E Heaney, Matthew Jackson, Christopher C Pain*

Main category: cs.LG

TL;DR: A novel nonlinear solver acceleration method for PDEs using adaptive learning, reducing computational time by up to 85% in multiphase flow simulations.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency in solving nonlinear PDEs, particularly in multiphase flow in porous media, by leveraging machine learning for dynamic solver tuning.

Method: Combines dimensionless numbers, simplified 2D models for training, dynamic relaxation control, and online learning to optimize solver performance.

Result: Achieves up to 85% reduction in computational time and fewer nonlinear iterations in 3D models.

Conclusion: The method successfully integrates machine learning into PDE solvers, significantly enhancing efficiency and adaptability.

Abstract: We propose a novel type of nonlinear solver acceleration for systems of
nonlinear partial differential equations (PDEs) that is based on
online/adaptive learning. It is applied in the context of multiphase flow in
porous media. The proposed method rely on four pillars: (i) dimensionless
numbers as input parameters for the machine learning model, (ii) simplified
numerical model (two-dimensional) for the offline training, (iii) dynamic
control of a nonlinear solver tuning parameter (numerical relaxation), (iv) and
online learning for real-time improvement of the machine learning model. This
strategy decreases the number of nonlinear iterations by dynamically modifying
a single global parameter, the relaxation factor, and by adaptively learning
the attributes of each numerical model on-the-run. Furthermore, this work
performs a sensitivity study in the dimensionless parameters (machine learning
features), assess the efficacy of various machine learning models, demonstrate
a decrease in nonlinear iterations using our method in more intricate,
realistic three-dimensional models, and fully couple a machine learning model
into an open-source multiphase flow simulator achieving up to 85\% reduction in
computational time.

</details>


### [235] [An Axiomatic Assessment of Entropy- and Variance-based Uncertainty Quantification in Regression](https://arxiv.org/abs/2504.18433)
*Christopher Bülte, Yusuf Sale, Timo Löhr, Paul Hofman, Gitta Kutyniok, Eyke Hüllermeier*

Main category: cs.LG

TL;DR: The paper introduces axioms to evaluate uncertainty measures in regression, generalizing common approaches and analyzing entropy- and variance-based measures.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in formal justification and evaluation of uncertainty measures in regression settings.

Method: Introduces axioms for assessing uncertainty measures and uses a predictive exponential family to generalize common approaches.

Result: Provides theoretical insights and practical guidelines for reliable uncertainty assessment in regression.

Conclusion: Offers a principled foundation for uncertainty quantification in regression, highlighting limitations of existing measures.

Abstract: Uncertainty quantification (UQ) is crucial in machine learning, yet most
(axiomatic) studies of uncertainty measures focus on classification, leaving a
gap in regression settings with limited formal justification and evaluations.
In this work, we introduce a set of axioms to rigorously assess measures of
aleatoric, epistemic, and total uncertainty in supervised regression. By
utilizing a predictive exponential family, we can generalize commonly used
approaches for uncertainty representation and corresponding uncertainty
measures. More specifically, we analyze the widely used entropy- and
variance-based measures regarding limitations and challenges. Our findings
provide a principled foundation for UQ in regression, offering theoretical
insights and practical guidelines for reliable uncertainty assessment.

</details>


### [236] [Enhancing Pre-Trained Model-Based Class-Incremental Learning through Neural Collapse](https://arxiv.org/abs/2504.18437)
*Kun He, Zijian Song, Shuoxi Zhang, John E. Hopcroft*

Main category: cs.LG

TL;DR: The paper proposes NCPTM-CIL, a method leveraging neural collapse (NC) to enhance feature evolution in pre-trained model-based class-incremental learning (CIL), outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Understanding feature evolution in CIL with pre-trained models is challenging. The paper aims to align feature distributions with NC geometry to improve continual learning.

Method: Introduces NCPTM-CIL, dynamically adjusting feature space to conform to NC structure for better continual learning.

Result: NCPTM-CIL outperforms existing methods, achieving significant gains on benchmark datasets like VTAB, CIFAR-100, and OmniBenchmark.

Conclusion: Aligning feature distributions with NC enhances CIL effectiveness, as demonstrated by NCPTM-CIL's superior performance.

Abstract: Class-Incremental Learning (CIL) is a critical capability for real-world
applications, enabling learning systems to adapt to new tasks while retaining
knowledge from previous ones. Recent advancements in pre-trained models (PTMs)
have significantly advanced the field of CIL, demonstrating superior
performance over traditional methods. However, understanding how features
evolve and are distributed across incremental tasks remains an open challenge.
In this paper, we propose a novel approach to modeling feature evolution in
PTM-based CIL through the lens of neural collapse (NC), a striking phenomenon
observed in the final phase of training, which leads to a well-separated,
equiangular feature space. We explore the connection between NC and CIL
effectiveness, showing that aligning feature distributions with the NC geometry
enhances the ability to capture the dynamic behavior of continual learning.
Based on this insight, we introduce Neural Collapse-inspired Pre-Trained
Model-based CIL (NCPTM-CIL), a method that dynamically adjusts the feature
space to conform to the elegant NC structure, thereby enhancing the continual
learning process. Extensive experiments demonstrate that NCPTM-CIL outperforms
state-of-the-art methods across four benchmark datasets. Notably, when
initialized with ViT-B/16-IN1K, NCPTM-CIL surpasses the runner-up method by
6.73% on VTAB, 1.25% on CIFAR-100, and 2.5% on OmniBenchmark.

</details>


### [237] [Enhancing Strawberry Yield Forecasting with Backcasted IoT Sensor Data and Machine Learning](https://arxiv.org/abs/2504.18451)
*Tewodros Alemu Ayall, Andy Li, Matthew Beddows, Milan Markovic, Georgios Leontidis*

Main category: cs.LG

TL;DR: The paper explores AI-based yield forecasting in agriculture using IoT data and synthetic data generation to address data scarcity.


<details>
  <summary>Details</summary>
Motivation: Rapid population growth necessitates sustainable food production, requiring IoT and AI for informed resource management, but data scarcity in dynamic farm settings poses challenges.

Method: Deployed IoT sensors in strawberry polytunnels for two seasons, combined with manual yield records. Proposed AI-based backcasting to generate synthetic sensor data using historical weather data.

Result: Incorporating synthetic data improved yield forecasting accuracy, outperforming models using only historical yield, weather, and real sensor data.

Conclusion: AI-based backcasting with synthetic data enhances yield forecasting, addressing data scarcity in dynamic agricultural environments.

Abstract: Due to rapid population growth globally, digitally-enabled agricultural
sectors are crucial for sustainable food production and making informed
decisions about resource management for farmers and various stakeholders. The
deployment of Internet of Things (IoT) technologies that collect real-time
observations of various environmental (e.g., temperature, humidity, etc.) and
operational factors (e.g., irrigation) influencing production is often seen as
a critical step to enable additional novel downstream tasks, such as AI-based
yield forecasting. However, since AI models require large amounts of data, this
creates practical challenges in a real-world dynamic farm setting where IoT
observations would need to be collected over a number of seasons. In this
study, we deployed IoT sensors in strawberry production polytunnels for two
growing seasons to collect environmental data, including water usage, external
and internal temperature, external and internal humidity, soil moisture, soil
temperature, and photosynthetically active radiation. The sensor observations
were combined with manually provided yield records spanning a period of four
seasons. To bridge the gap of missing IoT observations for two additional
seasons, we propose an AI-based backcasting approach to generate synthetic
sensor observations using historical weather data from a nearby weather station
and the existing polytunnel observations. We built an AI-based yield
forecasting model to evaluate our approach using the combination of real and
synthetic observations. Our results demonstrated that incorporating synthetic
data improved yield forecasting accuracy, with models incorporating synthetic
data outperforming those trained only on historical yield, weather records, and
real sensor data.

</details>


### [238] [Pseudo-Asynchronous Local SGD: Robust and Efficient Data-Parallel Training](https://arxiv.org/abs/2504.18454)
*Hiroki Naganuma, Xinzhi Zhang, Man-Chung Yue, Ioannis Mitliagkas, Philipp A. Witte, Russell J. Hewett, Yin Tat Lee*

Main category: cs.LG

TL;DR: PALSGD improves data-parallel training efficiency by reducing communication frequency with pseudo-synchronization, achieving faster training times while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the bottleneck of frequent global communication in large-scale distributed deep learning.

Method: Extends Local SGD and DiLoCo with pseudo-synchronization to reduce communication frequency while maintaining model consistency.

Result: PALSGD trains 18.4%-24.4% faster than DDP on various tasks (ImageNet-1K, TinyStories) with comparable performance.

Conclusion: PALSGD is an efficient alternative for large-scale training, reducing communication overhead without sacrificing performance.

Abstract: Following AI scaling trends, frontier models continue to grow in size and
continue to be trained on larger datasets. Training these models requires huge
investments in exascale computational resources, which has in turn driven
development of distributed deep learning methods. Data parallelism is an
essential approach to speed up training, but it requires frequent global
communication between workers, which can bottleneck training at the largest
scales. In this work, we propose a method called Pseudo-Asynchronous Local SGD
(PALSGD) to improve the efficiency of data-parallel training. PALSGD is an
extension of Local SGD (Stich, 2018) and DiLoCo (Douillard et al., 2023),
designed to further reduce communication frequency by introducing a
pseudo-synchronization mechanism. PALSGD allows the use of longer
synchronization intervals compared to standard Local SGD. Despite the reduced
communication frequency, the pseudo-synchronization approach ensures that model
consistency is maintained, leading to performance results comparable to those
achieved with more frequent synchronization. Furthermore, we provide a
theoretical analysis of PALSGD, establishing its convergence and deriving its
convergence rate. This analysis offers insights into the algorithm's behavior
and performance guarantees. We evaluated PALSGD on image classification and
language modeling tasks. Our results show that PALSGD achieves better
performance in less time compared to existing methods like Distributed Data
Parallel (DDP), and DiLoCo. Notably, PALSGD trains 18.4% faster than DDP on
ImageNet-1K with ResNet-50, 24.4% faster than DDP on TinyStories with
GPT-Neo125M, and 21.1% faster than DDP on TinyStories with GPT-Neo-8M.

</details>


### [239] [Action-Minimization Meets Generative Modeling: Efficient Transition Path Sampling with the Onsager-Machlup Functional](https://arxiv.org/abs/2504.18506)
*Sanjeev Raja, Martin Šípka, Michael Psenka, Tobias Kreiman, Michal Pavelka, Aditi S. Krishnapriyan*

Main category: cs.LG

TL;DR: The paper proposes a method to repurpose pre-trained generative models for transition path sampling (TPS) by interpreting paths as trajectories from stochastic dynamics, avoiding task-specific training.


<details>
  <summary>Details</summary>
Motivation: Current TPS methods are limited by expensive, task-specific training and cannot leverage advances in atomistic machine learning like pre-trained models.

Method: The approach uses pre-trained generative models (denoising diffusion and flow matching) to sample paths by minimizing the Onsager-Machlup action functional, enabling zero-shot TPS.

Result: The method successfully generates diverse, realistic transition pathways for molecular systems and generalizes beyond the original training data.

Conclusion: This zero-shot approach is scalable and adaptable to new generative models, enhancing TPS efficiency without task-specific training.

Abstract: Transition path sampling (TPS), which involves finding probable paths
connecting two points on an energy landscape, remains a challenge due to the
complexity of real-world atomistic systems. Current machine learning approaches
use expensive, task-specific, and data-free training procedures, limiting their
ability to benefit from recent advances in atomistic machine learning, such as
high-quality datasets and large-scale pre-trained models. In this work, we
address TPS by interpreting candidate paths as trajectories sampled from
stochastic dynamics induced by the learned score function of pre-trained
generative models, specifically denoising diffusion and flow matching. Under
these dynamics, finding high-likelihood transition paths becomes equivalent to
minimizing the Onsager-Machlup (OM) action functional. This enables us to
repurpose pre-trained generative models for TPS in a zero-shot manner, in
contrast with bespoke, task-specific TPS models trained in previous work. We
demonstrate our approach on varied molecular systems, obtaining diverse,
physically realistic transition pathways and generalizing beyond the
pre-trained model's original training dataset. Our method can be easily
incorporated into new generative models, making it practically relevant as
models continue to scale and improve with increased data availability.

</details>


### [240] [Generalization Capability for Imitation Learning](https://arxiv.org/abs/2504.18538)
*Yixiao Wang*

Main category: cs.LG

TL;DR: The paper analyzes imitation learning's generalization gap, linking it to information theory and data distribution. It provides theoretical bounds and insights for improving generalization by managing model parameters and data variability.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of imitation learning policies struggling to generalize beyond training data, the study seeks a unified theoretical framework.

Method: The paper uses information theory (conditional information bottleneck, mutual information) and analyzes data distribution properties to bound the generalization gap. It also explores the impact of conditional entropy on optimization.

Result: The generalization gap is bounded by information-theoretic terms, and high conditional entropy flattens the likelihood landscape, aiding optimization.

Conclusion: Improving generalization in imitation learning requires managing model parameters and enhancing output label variability, not just input diversity.

Abstract: Imitation learning holds the promise of equipping robots with versatile
skills by learning from expert demonstrations. However, policies trained on
finite datasets often struggle to generalize beyond the training distribution.
In this work, we present a unified perspective on the generalization capability
of imitation learning, grounded in both information theorey and data
distribution property. We first show that the generalization gap can be upper
bounded by (i) the conditional information bottleneck on intermediate
representations and (ii) the mutual information between the model parameters
and the training dataset. This characterization provides theoretical guidance
for designing effective training strategies in imitation learning, particularly
in determining whether to freeze, fine-tune, or train large pretrained encoders
(e.g., vision-language models or vision foundation models) from scratch to
achieve better generalization. Furthermore, we demonstrate that high
conditional entropy from input to output induces a flatter likelihood
landscape, thereby reducing the upper bound on the generalization gap. In
addition, it shortens the stochastic gradient descent (SGD) escape time from
sharp local minima, which may increase the likelihood of reaching global optima
under fixed optimization budgets. These insights explain why imitation learning
often exhibits limited generalization and underscore the importance of not only
scaling the diversity of input data but also enriching the variability of
output labels conditioned on the same input.

</details>


### [241] [Intelligent Attacks and Defense Methods in Federated Learning-enabled Energy-Efficient Wireless Networks](https://arxiv.org/abs/2504.18519)
*Han Zhang, Hao Zhou, Medhat Elsayed, Majid Bavand, Raimundas Gaigalas, Yigit Ozcan, Melike Erol-Kantarci*

Main category: cs.LG

TL;DR: The paper evaluates malicious attacks in federated learning (FL) for wireless networks, proposes attack models (GAN-enhanced and regularization-based poisoning), and introduces defense methods (autoencoder-based and knowledge distillation-enabled).


<details>
  <summary>Details</summary>
Motivation: FL's distributed nature in wireless networks increases vulnerability to attacks, especially with non-IID data, necessitating robust defense mechanisms.

Method: Proposes two attack models (GAN-enhanced and regularization-based poisoning) and two defense schemes (autoencoder-based and KD-enabled).

Result: The defense methods effectively mitigate attacks by identifying malicious participants and controlling knowledge transfer.

Conclusion: The study highlights the importance of securing FL in wireless networks and demonstrates viable defense strategies against sophisticated attacks.

Abstract: Federated learning (FL) is a promising technique for learning-based functions
in wireless networks, thanks to its distributed implementation capability. On
the other hand, distributed learning may increase the risk of exposure to
malicious attacks where attacks on a local model may spread to other models by
parameter exchange. Meanwhile, such attacks can be hard to detect due to the
dynamic wireless environment, especially considering local models can be
heterogeneous with non-independent and identically distributed (non-IID) data.
Therefore, it is critical to evaluate the effect of malicious attacks and
develop advanced defense techniques for FL-enabled wireless networks. In this
work, we introduce a federated deep reinforcement learning-based cell sleep
control scenario that enhances the energy efficiency of the network. We propose
multiple intelligent attacks targeting the learning-based approach and we
propose defense methods to mitigate such attacks. In particular, we have
designed two attack models, generative adversarial network (GAN)-enhanced model
poisoning attack and regularization-based model poisoning attack. As a
counteraction, we have proposed two defense schemes, autoencoder-based defense,
and knowledge distillation (KD)-enabled defense. The autoencoder-based defense
method leverages an autoencoder to identify the malicious participants and only
aggregate the parameters of benign local models during the global aggregation,
while KD-based defense protects the model from attacks by controlling the
knowledge transferred between the global model and local models.

</details>


### [242] [GOFA: A Generative One-For-All Model for Joint Graph Language Modeling](https://arxiv.org/abs/2407.09709)
*Lecheng Kong, Jiarui Feng, Hao Liu, Chengsong Huang, Jiaxin Huang, Yixin Chen, Muhan Zhang*

Main category: cs.LG

TL;DR: The paper proposes GOFA, a generative graph language model combining GNN layers with a pre-trained LLM to address challenges in developing Graph Foundation Models (GFMs).


<details>
  <summary>Details</summary>
Motivation: Graph data lacks a definitive structure, making it difficult to develop GFMs that can handle both semantic and structural tasks simultaneously.

Method: GOFA interleaves GNN layers into a frozen pre-trained LLM, pre-trained on graph-level tasks like next-word prediction and question-answering, then fine-tuned for downstream tasks.

Result: GOFA demonstrates strong performance in solving structural and contextual problems in zero-shot scenarios.

Conclusion: GOFA successfully combines semantic and structural modeling, addressing key GFM properties and outperforming existing approaches.

Abstract: Foundation models, such as Large Language Models (LLMs) or Large Vision
Models (LVMs), have emerged as one of the most powerful tools in the respective
fields. However, unlike text and image data, graph data do not have a
definitive structure, posing great challenges to developing a Graph Foundation
Model (GFM). For example, current attempts at designing general graph models
either transform graph data into a language format for LLM-based prediction or
still train a GNN model with LLM as an assistant. The former can handle
unlimited tasks, while the latter captures graph structure much better -- yet,
no existing work can achieve both simultaneously. In this paper, we identify
three key desirable properties of a GFM: self-supervised pretraining, fluidity
in tasks, and graph awareness. To account for these properties, we extend the
conventional language modeling to the graph domain and propose a novel
generative graph language model GOFA to solve the problem. The model
interleaves randomly initialized GNN layers into a frozen pre-trained LLM so
that the semantic and structural modeling abilities are organically combined.
GOFA is pre-trained on newly proposed graph-level next-word prediction,
question-answering, and structural tasks to obtain the above GFM properties.
The pre-trained model is further fine-tuned on downstream tasks to obtain
task-solving ability. The fine-tuned model is evaluated on various downstream
tasks, demonstrating a strong ability to solve structural and contextual
problems in zero-shot scenarios. The code is available at
https://github.com/JiaruiFeng/GOFA.

</details>


### [243] [Deep Optimal Transport for Domain Adaptation on SPD Manifolds](https://arxiv.org/abs/2201.05745)
*Ce Ju, Cuntai Guan*

Main category: cs.LG

TL;DR: A geometric deep learning framework combining optimal transport with SPD manifold geometry improves domain adaptation for neuroimaging data, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing distribution shifts in neuroimaging data by leveraging the geometric structure of SPD manifolds, often overlooked by conventional methods.

Method: Combines optimal transport theory with SPD manifold geometry to align data distributions while preserving intrinsic structure.

Result: Outperforms baseline methods on three brain-computer interface datasets (KU, BNCI2014001, BNCI2015001) and maintains data geometry.

Conclusion: The framework effectively reduces distribution discrepancies and preserves geometric structure, validated by quantitative and visual results.

Abstract: Recent progress in geometric deep learning has drawn increasing attention
from the machine learning community toward domain adaptation on symmetric
positive definite (SPD) manifolds, especially for neuroimaging data that often
suffer from distribution shifts across sessions. These data, typically
represented as covariance matrices of brain signals, inherently lie on SPD
manifolds due to their symmetry and positive definiteness. However,
conventional domain adaptation methods often overlook this geometric structure
when applied directly to covariance matrices, which can result in suboptimal
performance. To address this issue, we introduce a new geometric deep learning
framework that combines optimal transport theory with the geometry of SPD
manifolds. Our approach aligns data distributions while respecting the manifold
structure, effectively reducing both marginal and conditional discrepancies. We
validate our method on three cross-session brain computer interface datasets,
KU, BNCI2014001, and BNCI2015001, where it consistently outperforms baseline
approaches while maintaining the intrinsic geometry of the data. We also
provide quantitative results and visualizations to better illustrate the
behavior of the learned embeddings.

</details>


### [244] [Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs](https://arxiv.org/abs/2408.06621)
*Sungmin Cha, Sungjun Cho, Dasol Hwang, Moontae Lee*

Main category: cs.LG

TL;DR: LoKU is a novel framework for efficient unlearning in LLMs, combining Inverted Hinge Loss and data-adaptive LoRA initialization to remove sensitive data while preserving model performance.


<details>
  <summary>Details</summary>
Motivation: Addressing privacy and copyright risks in LLMs by enabling efficient unlearning without full retraining, overcoming issues like unstable optimization and catastrophic forgetting.

Method: Proposes Low-rank Knowledge Unlearning (LoKU) with Inverted Hinge Loss to suppress unwanted tokens and data-adaptive LoRA initialization for targeted parameter updates.

Result: Effective removal of sensitive information in GPT-Neo, Phi-1.5B, and Llama2-7B models while maintaining reasoning and generative capabilities.

Conclusion: LoKU offers a robust and efficient solution for unlearning in LLMs, balancing computational cost and performance.

Abstract: Large Language Models (LLMs) have demonstrated strong reasoning and
memorization capabilities via pretraining on massive textual corpora. However,
this poses risk of privacy and copyright violations, highlighting the need for
efficient machine unlearning methods that remove sensitive data without
retraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn
by reducing the likelihood of generating unwanted content, it leads to unstable
optimization and catastrophic forgetting of retrained knowledge. We find that
combining GA with low-rank adaptation results in poor trade-offs between
computational cost and generative performance. To address these challenges, we
propose Low-rank Knowledge Unlearning (LoKU), a novel framework that enables
robust and efficient unlearning for LLMs. First, we introduce Inverted Hinge
Loss, which suppresses unwanted tokens while maintaining fluency by boosting
the probability of the next most likely token. Second, we develop a
data-adaptive initialization for LoRA adapters via low-rank approximation
weighted with relative Fisher information, thereby focusing updates on
parameters critical for removing targeted knowledge. Experiments on the
Training Data Extraction Challenge dataset using GPT-Neo models as well as on
the TOFU benchmark with Phi-1.5B and Llama2-7B models demonstrate that our
approach effectively removes sensitive information while maintaining reasoning
and generative capabilities with minimal impact. Our implementation can be
found in https://github.com/csm9493/efficient-llm-unlearning.

</details>


### [245] [CR-LSO: Convex Neural Architecture Optimization in the Latent Space of Graph Variational Autoencoder with Input Convex Neural Networks](https://arxiv.org/abs/2211.05950)
*Xuan Rao, Bo Zhao, Derong Liu*

Main category: cs.LG

TL;DR: CR-LSO improves neural architecture search by regularizing latent space learning to ensure convex performance mapping, using G-VAE and ICNNs for optimization.


<details>
  <summary>Details</summary>
Motivation: Gradient-based LSO struggles with non-convex mappings from latent space to architecture performance, hindering optimization.

Method: CR-LSO trains a G-VAE to learn continuous architecture representations, regularized by ICNNs to enforce convex performance mapping, then uses ICNN gradients for optimization.

Result: CR-LSO achieves competitive results in computational complexity and architecture performance on NAS benchmarks.

Conclusion: CR-LSO effectively addresses non-convexity in LSO, improving NAS performance and efficiency.

Abstract: In neural architecture search (NAS) methods based on latent space
optimization (LSO), a deep generative model is trained to embed discrete neural
architectures into a continuous latent space. In this case, different
optimization algorithms that operate in the continuous space can be implemented
to search neural architectures. However, the optimization of latent variables
is challenging for gradient-based LSO since the mapping from the latent space
to the architecture performance is generally non-convex. To tackle this
problem, this paper develops a convexity regularized latent space optimization
(CR-LSO) method, which aims to regularize the learning process of latent space
in order to obtain a convex architecture performance mapping. Specifically,
CR-LSO trains a graph variational autoencoder (G-VAE) to learn the continuous
representations of discrete architectures. Simultaneously, the learning process
of latent space is regularized by the guaranteed convexity of input convex
neural networks (ICNNs). In this way, the G-VAE is forced to learn a convex
mapping from the architecture representation to the architecture performance.
Hereafter, the CR-LSO approximates the performance mapping using the ICNN and
leverages the estimated gradient to optimize neural architecture
representations. Experimental results on three popular NAS benchmarks show that
CR-LSO achieves competitive evaluation results in terms of both computational
complexity and architecture performance.

</details>


### [246] [Adversarial Attacks to Latent Representations of Distributed Neural Networks in Split Computing](https://arxiv.org/abs/2309.17401)
*Milin Zhang, Mohammad Abdi, Jonathan Ashdown, Francesco Restuccia*

Main category: cs.LG

TL;DR: The paper analyzes the robustness of distributed DNNs against adversarial actions, revealing trade-offs between robustness, performance, and computational burden.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored resilience of distributed DNNs to adversarial actions, filling a research gap.

Method: The problem is framed in information theory, with theoretical proofs and extensive experiments on 6 DNN architectures, 6 distributed approaches, and 10 adversarial attacks using ImageNet-1K.

Result: Compressed latent dimensions improve robustness but affect performance; deeper splitting points enhance robustness but increase computational burden.

Conclusion: The findings offer a new perspective for designing robust distributed DNNs, balancing trade-offs.

Abstract: Distributed deep neural networks (DNNs) have been shown to reduce the
computational burden of mobile devices and decrease the end-to-end inference
latency in edge computing scenarios. While distributed DNNs have been studied,
to the best of our knowledge, the resilience of distributed DNNs to adversarial
action remains an open problem. In this paper, we fill the existing research
gap by rigorously analyzing the robustness of distributed DNNs against
adversarial action. We cast this problem in the context of information theory
and rigorously proved that (i) the compressed latent dimension improves the
robustness but also affect task-oriented performance; and (ii) the deeper
splitting point enhances the robustness but also increases the computational
burden. These two trade-offs provide a novel perspective to design robust
distributed DNN. To test our theoretical findings, we perform extensive
experimental analysis by considering 6 different DNN architectures, 6 different
approaches for distributed DNN and 10 different adversarial attacks using the
ImageNet-1K dataset.

</details>


### [247] [Leveraging Label Semantics and Meta-Label Refinement for Multi-Label Question Classification](https://arxiv.org/abs/2411.01841)
*Shi Dong, Xiaobei Niu, Rui Zhong, Zhifeng Wang, Mingzhang Zuo*

Main category: cs.LG

TL;DR: RR2QC improves multi-label question classification in online education by leveraging label semantics and meta-label refinement, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Fine-grained knowledge labels overlap, and label distribution imbalance complicates multi-label classification in education.

Method: RR2QC uses semantic relationships, class center learning, and meta-label decomposition with a meta-label classifier for reranking.

Result: RR2QC achieves higher Precision@K and F1 scores than existing methods.

Conclusion: RR2QC effectively addresses label ambiguity and imbalance, enhancing personalized learning in online education.

Abstract: Accurate annotation of educational resources is crucial for effective
personalized learning and resource recommendation in online education. However,
fine-grained knowledge labels often overlap or share similarities, making it
difficult for existing multi-label classification methods to differentiate
them. The label distribution imbalance due to sparsity of human annotations
further intensifies these challenges. To address these issues, this paper
introduces RR2QC, a novel Retrieval Reranking method to multi-label Question
Classification by leveraging label semantics and meta-label refinement. First,
RR2QC improves the pre-training strategy by utilizing semantic relationships
within and across label groups. Second, it introduces a class center learning
task to align questions with label semantics during downstream training.
Finally, this method decomposes labels into meta-labels and uses a meta-label
classifier to rerank the retrieved label sequences. In doing so, RR2QC enhances
the understanding and prediction capability of long-tail labels by learning
from meta-labels that frequently appear in other labels. Additionally, a
mathematical LLM is used to generate solutions for questions, extracting latent
information to further refine the model's insights. Experimental results show
that RR2QC outperforms existing methods in Precision@K and F1 scores across
multiple educational datasets, demonstrating its effectiveness for online
education applications. The code and datasets are available at
https://github.com/78Erii/RR2QC.

</details>


### [248] [Tensor Networks for Explainable Machine Learning in Cybersecurity](https://arxiv.org/abs/2401.00867)
*Borja Aizpurua, Samuel Palmer, Roman Orus*

Main category: cs.LG

TL;DR: Tensor networks, specifically Matrix Product States (MPS), enhance explainability in ML, rivaling traditional models like autoencoders and GANs in performance while offering superior interpretability.


<details>
  <summary>Details</summary>
Motivation: To improve transparency and interpretability in ML algorithms, particularly for understanding AI decisions in adversarial threat intelligence.

Method: Developed an unsupervised clustering algorithm using MPS, applied to adversary-generated threat intelligence.

Result: MPS matches performance of deep learning models (autoencoders, GANs) and provides richer interpretability via feature-wise probabilities, Von Neumann Entropy, and mutual information.

Conclusion: MPS offers a transparent and interpretable alternative to traditional ML models, crucial for understanding AI decision-making in anomaly classification.

Abstract: In this paper we show how tensor networks help in developing explainability
of machine learning algorithms. Specifically, we develop an unsupervised
clustering algorithm based on Matrix Product States (MPS) and apply it in the
context of a real use-case of adversary-generated threat intelligence. Our
investigation proves that MPS rival traditional deep learning models such as
autoencoders and GANs in terms of performance, while providing much richer
model interpretability. Our approach naturally facilitates the extraction of
feature-wise probabilities, Von Neumann Entropy, and mutual information,
offering a compelling narrative for classification of anomalies and fostering
an unprecedented level of transparency and interpretability, something
fundamental to understand the rationale behind artificial intelligence
decisions.

</details>


### [249] [A Dual Perspective of Reinforcement Learning for Imposing Policy Constraints](https://arxiv.org/abs/2404.16468)
*Bram De Cooman, Johan Suykens*

Main category: cs.LG

TL;DR: The paper introduces a primal-dual framework for model-free reinforcement learning to impose diverse policy constraints, unifying existing techniques and enabling novel constraint types.


<details>
  <summary>Details</summary>
Motivation: Current model-free RL methods lack mechanisms for behavioral constraints, limiting their applicability. Existing extensions are restricted to specific constraint types.

Method: A generic primal-dual framework is proposed for value-based and actor-critic RL, revealing a relationship between dual constraints and primal reward modifications.

Result: The framework enables novel constraints like action density bounds and transition costs, with a practical algorithm (DualCRL) evaluated on interpretable environments.

Conclusion: DualCRL provides a versatile toolbox for policy constraints, demonstrating efficacy in handling diverse constraints.

Abstract: Model-free reinforcement learning methods lack an inherent mechanism to
impose behavioural constraints on the trained policies. Although certain
extensions exist, they remain limited to specific types of constraints, such as
value constraints with additional reward signals or visitation density
constraints. In this work we unify these existing techniques and bridge the gap
with classical optimization and control theory, using a generic primal-dual
framework for value-based and actor-critic reinforcement learning methods. The
obtained dual formulations turn out to be especially useful for imposing
additional constraints on the learned policy, as an intrinsic relationship
between such dual constraints (or regularization terms) and reward
modifications in the primal is revealed. Furthermore, using this framework, we
are able to introduce some novel types of constraints, allowing to impose
bounds on the policy's action density or on costs associated with transitions
between consecutive states and actions. From the adjusted primal-dual
optimization problems, a practical algorithm is derived that supports various
combinations of policy constraints that are automatically handled throughout
training using trainable reward modifications. The proposed $\texttt{DualCRL}$
method is examined in more detail and evaluated under different (combinations
of) constraints on two interpretable environments. The results highlight the
efficacy of the method, which ultimately provides the designer of such systems
with a versatile toolbox of possible policy constraints.

</details>


### [250] [Decoding complexity: how machine learning is redefining scientific discovery](https://arxiv.org/abs/2405.04161)
*Ricardo Vinuesa, Paola Cinnella, Jean Rabault, Hossein Azizpour, Stefan Bauer, Bingni W. Brunton, Arne Elofsson, Elias Jarlebring, Hedvig Kjellstrom, Stefano Markidis, David Marlevi, Javier Garcia-Martinez, Steven L. Brunton*

Main category: cs.LG

TL;DR: The paper highlights ML's transformative role in scientific research, showcasing its impact through examples like brain mapping and exoplanet detection, while addressing challenges and advocating for its potential to revolutionize traditional methodologies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to demonstrate how ML can organize, analyze, and interpret vast scientific data, accelerating breakthroughs across disciplines despite challenges.

Method: The method involves presenting key examples of ML applications in science and exploring scenarios with varying levels of underlying knowledge to identify strategies for overcoming limitations.

Result: Results show ML's ability to reshape research, tackle complex problems, and advance interdisciplinary breakthroughs, though challenges like rigorous validation remain.

Conclusion: The conclusion argues that ML, despite its challenges, will disrupt traditional methods, enabling researchers to embrace complexity and drive innovative solutions to global challenges.

Abstract: As modern scientific instruments generate vast amounts of data and the volume
of information in the scientific literature continues to grow, machine learning
(ML) has become an essential tool for organising, analysing, and interpreting
these complex datasets. This paper explores the transformative role of ML in
accelerating breakthroughs across a range of scientific disciplines. By
presenting key examples -- such as brain mapping and exoplanet detection -- we
demonstrate how ML is reshaping scientific research. We also explore different
scenarios where different levels of knowledge of the underlying phenomenon are
available, identifying strategies to overcome limitations and unlock the full
potential of ML. Despite its advances, the growing reliance on ML poses
challenges for research applications and rigorous validation of discoveries. We
argue that even with these challenges, ML is poised to disrupt traditional
methodologies and advance the boundaries of knowledge by enabling researchers
to tackle increasingly complex problems. Thus, the scientific community can
move beyond the necessary traditional oversimplifications to embrace the full
complexity of natural systems, ultimately paving the way for interdisciplinary
breakthroughs and innovative solutions to humanity's most pressing challenges.

</details>


### [251] [Can Kernel Methods Explain How the Data Affects Neural Collapse?](https://arxiv.org/abs/2406.02105)
*Vignesh Kothapalli, Tom Tirer*

Main category: cs.LG

TL;DR: The paper analyzes Neural Collapse (NC1) using kernels from shallow NNs, comparing NNGP and NTK, and explores a data-aware kernel as an alternative.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of unconstrained feature models (UFMs) in studying NC1 by incorporating data effects through kernel-based analysis.

Method: Formulates an NC1 metric using kernels, specializing it to NNGP and NTK, and empirically tests a data-aware Gaussian Process kernel.

Result: NTK does not show more collapsed features than NNGP for Gaussian data, and a data-aware kernel reduces NC1 but may not match shallow NN trends.

Conclusion: Data adaptivity in kernels can aid NC analysis, but further improvements are needed. Activation functions (e.g., ERF vs. ReLU) also impact NC1.

Abstract: A vast amount of literature has recently focused on the "Neural Collapse"
(NC) phenomenon, which emerges when training neural network (NN) classifiers
beyond the zero training error point. The core component of NC is the decrease
in the within-class variability of the network's deepest features, dubbed as
NC1. The theoretical works that study NC are typically based on simplified
unconstrained features models (UFMs) that mask any effect of the data on the
extent of collapse. To address this limitation of UFMs, this paper explores the
possibility of analyzing NC1 using kernels associated with shallow NNs. We
begin by formulating an NC1 metric as a function of the kernel. Then, we
specialize it to the NN Gaussian Process kernel (NNGP) and the Neural Tangent
Kernel (NTK), associated with wide networks at initialization and during
gradient-based training with a small learning rate, respectively. As a key
result, we show that the NTK does not represent more collapsed features than
the NNGP for Gaussian data of arbitrary dimensions. This showcases the
limitations of data-independent kernels such as NTK in approximating the NC
behavior of NNs. As an alternative to NTK, we then empirically explore a
recently proposed data-aware Gaussian Process kernel, which generalizes NNGP to
model feature learning. We show that this kernel yields lower NC1 than NNGP but
may not follow the trends of the shallow NN. Our study demonstrates that
adaptivity to data may allow kernel-based analysis of NC, though further
advancements in this area are still needed. A nice byproduct of our study is
showing both theoretically and empirically that the choice of nonlinear
activation function affects NC1 (with ERF yielding lower values than ReLU). The
code is available at: https://github.com/kvignesh1420/shallow_nc1

</details>


### [252] [A Picture is Worth A Thousand Numbers: Enabling LLMs Reason about Time Series via Visualization](https://arxiv.org/abs/2411.06018)
*Haoxin Liu, Chenghao Liu, B. Aditya Prakash*

Main category: cs.LG

TL;DR: TimerBed is a testbed for evaluating LLMs' time-series reasoning (TsR) performance, revealing initial failures in zero-shot and few-shot learning. VL-Time, a prompt-based solution using visualization, significantly improves performance.


<details>
  <summary>Details</summary>
Motivation: LLMs' reasoning abilities are underexplored for time-series tasks, which are common in real-world applications.

Method: TimerBed evaluates LLMs with stratified reasoning patterns, various models, and strategies. VL-Time uses visualization and language-guided reasoning to address numerical modeling issues.

Result: LLMs initially fail in TsR, but VL-Time improves performance by 140% and reduces token costs by 99%.

Conclusion: VL-Time effectively enhances LLMs' TsR capabilities, addressing key limitations in numerical data modeling.

Abstract: Large language models (LLMs), with demonstrated reasoning abilities across
multiple domains, are largely underexplored for time-series reasoning (TsR),
which is ubiquitous in the real world. In this work, we propose TimerBed, the
first comprehensive testbed for evaluating LLMs' TsR performance. Specifically,
TimerBed includes stratified reasoning patterns with real-world tasks,
comprehensive combinations of LLMs and reasoning strategies, and various
supervised models as comparison anchors. We perform extensive experiments with
TimerBed, test multiple current beliefs, and verify the initial failures of
LLMs in TsR, evidenced by the ineffectiveness of zero shot (ZST) and
performance degradation of few shot in-context learning (ICL). Further, we
identify one possible root cause: the numerical modeling of data. To address
this, we propose a prompt-based solution VL-Time, using visualization-modeled
data and language-guided reasoning. Experimental results demonstrate that
Vl-Time enables multimodal LLMs to be non-trivial ZST and powerful ICL
reasoners for time series, achieving about 140% average performance improvement
and 99% average token costs reduction.

</details>


### [253] [VisTabNet: Adapting Vision Transformers for Tabular Data](https://arxiv.org/abs/2501.00057)
*Witold Wydmański, Ulvi Movsum-zada, Jacek Tabor, Marek Śmieja*

Main category: cs.LG

TL;DR: VisTabNet adapts Vision Transformer (ViT) for tabular data, outperforming traditional methods and deep learning models on small datasets.


<details>
  <summary>Details</summary>
Motivation: Deep learning excels in NLP and vision but lags in tabular data, especially for small datasets. VisTabNet addresses this gap.

Method: Projects tabular data to ViT-compatible embeddings, enabling use of pre-trained ViT without custom architecture or full training.

Result: Outperforms ensemble and deep learning methods on small tabular datasets (<1k samples).

Conclusion: VisTabNet extends transfer learning by adapting image models to tabular data, offering a practical solution with shared implementation.

Abstract: Although deep learning models have had great success in natural language
processing and computer vision, we do not observe comparable improvements in
the case of tabular data, which is still the most common data type used in
biological, industrial and financial applications. In particular, it is
challenging to transfer large-scale pre-trained models to downstream tasks
defined on small tabular datasets. To address this, we propose VisTabNet -- a
cross-modal transfer learning method, which allows for adapting Vision
Transformer (ViT) with pre-trained weights to process tabular data. By
projecting tabular inputs to patch embeddings acceptable by ViT, we can
directly apply a pre-trained Transformer Encoder to tabular inputs. This
approach eliminates the conceptual cost of designing a suitable architecture
for processing tabular data, while reducing the computational cost of training
the model from scratch. Experimental results on multiple small tabular datasets
(less than 1k samples) demonstrate VisTabNet's superiority, outperforming both
traditional ensemble methods and recent deep learning models. The proposed
method goes beyond conventional transfer learning practice and shows that
pre-trained image models can be transferred to solve tabular problems,
extending the boundaries of transfer learning. We share our example
implementation as a GitHub repository available at
https://github.com/wwydmanski/VisTabNet.

</details>


### [254] [Dual-Branch HNSW Approach with Skip Bridges and LID-Driven Optimization](https://arxiv.org/abs/2501.13992)
*Hy Nguyen, Nguyen Hung Nguyen, Nguyen Linh Bao Nguyen, Srikanth Thudumu, Hung Du, Rajesh Vasa, Kon Mouzakis*

Main category: cs.LG

TL;DR: A novel algorithm improves HNSW by addressing local optima and cluster disconnections, enhancing construction speed, and maintaining inference speed. It uses a dual-branch structure with LID-based insertion and bridge-building, outperforming HNSW in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: HNSW's limitations include local optima and suboptimal complexity in high-dimensional datasets. The proposed algorithm aims to mitigate these issues while improving performance.

Method: The algorithm introduces a dual-branch HNSW structure with LID-based insertion for multi-directional traversal and a bridge-building technique to bypass redundant layers.

Result: Experiments show recall improvements of 18% in NLP and up to 30% in CV tasks, with 20% faster construction and maintained inference speed.

Conclusion: The proposed algorithm outperforms HNSW without trade-offs, with LID-based insertion being the most impactful component.

Abstract: The Hierarchical Navigable Small World (HNSW) algorithm is widely used for
approximate nearest neighbor (ANN) search, leveraging the principles of
navigable small-world graphs. However, it faces some limitations. The first is
the local optima problem, which arises from the algorithm's greedy search
strategy, selecting neighbors based solely on proximity at each step. This
often leads to cluster disconnections. The second limitation is that HNSW
frequently fails to achieve logarithmic complexity, particularly in
high-dimensional datasets, due to the exhaustive traversal through each layer.
To address these limitations, we propose a novel algorithm that mitigates local
optima and cluster disconnections while enhancing the construction speed,
maintaining inference speed. The first component is a dual-branch HNSW
structure with LID-based insertion mechanisms, enabling traversal from multiple
directions. This improves outlier node capture, enhances cluster connectivity,
accelerates construction speed and reduces the risk of local minima. The second
component incorporates a bridge-building technique that bypasses redundant
intermediate layers, maintaining inference and making up the additional
computational overhead introduced by the dual-branch structure. Experiments on
various benchmarks and datasets showed that our algorithm outperforms the
original HNSW in both accuracy and speed. We evaluated six datasets across
Computer Vision (CV), and Natural Language Processing (NLP), showing recall
improvements of 18\% in NLP, and up to 30\% in CV tasks while reducing the
construction time by up to 20\% and maintaining the inference speed. We did not
observe any trade-offs in our algorithm. Ablation studies revealed that
LID-based insertion had the greatest impact on performance, followed by the
dual-branch structure and bridge-building components.

</details>


### [255] [A Bias-Variance Decomposition for Ensembles over Multiple Synthetic Datasets](https://arxiv.org/abs/2402.03985)
*Ossi Räisä, Antti Honkela*

Main category: cs.LG

TL;DR: The paper explores the theoretical benefits of using multiple synthetic datasets in supervised learning, providing insights into bias-variance trade-offs and practical guidelines for dataset selection.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical understanding of the benefits of multiple synthetic datasets, despite empirical evidence supporting their advantages in accuracy, model selection, and uncertainty estimation.

Method: Derives bias-variance decompositions for various synthetic dataset settings, including differentially private data, and tests the theory on real datasets with different predictors and error metrics.

Result: The theory confirms that multiple synthetic datasets often improve accuracy, while a single large dataset offers minimal gains, aligning with practical observations.

Conclusion: The study provides a theoretical foundation and practical rule of thumb for selecting the number of synthetic datasets, demonstrating its relevance in real-world applications.

Abstract: Recent studies have highlighted the benefits of generating multiple synthetic
datasets for supervised learning, from increased accuracy to more effective
model selection and uncertainty estimation. These benefits have clear empirical
support, but the theoretical understanding of them is currently very light. We
seek to increase the theoretical understanding by deriving bias-variance
decompositions for several settings of using multiple synthetic datasets,
including differentially private synthetic data. Our theory yields a simple
rule of thumb to select the appropriate number of synthetic datasets in the
case of mean-squared error and Brier score. We investigate how our theory works
in practice with several real datasets, downstream predictors and error
metrics. As our theory predicts, multiple synthetic datasets often improve
accuracy, while a single large synthetic dataset gives at best minimal
improvement, showing that our insights are practically relevant.

</details>


### [256] [Local Control Networks (LCNs): Optimizing Flexibility in Neural Network Data Pattern Capture](https://arxiv.org/abs/2501.14000)
*Hy Nguyen, Duy Khoa Pham, Srikanth Thudumu, Hung Du, Rajesh Vasa, Kon Mouzakis*

Main category: cs.LG

TL;DR: The paper proposes Local Control Networks (LCNs) with node-specific activation functions, outperforming traditional MLPs and KANs in efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Uniform activation functions in MLPs may limit adaptability; diverse node-level activations could enhance performance.

Method: Introduces LCNs using B-spline functions for distinct node activations, comparing them to MLPs and KANs.

Result: LCNs outperform MLPs and KANs in tasks like computer vision (5% better than KANs) and basic ML (1% better than MLPs).

Conclusion: Node-specific activations in LCNs improve performance and efficiency, making them a viable alternative to complex architectures like KANs.

Abstract: The widespread use of Multi-layer perceptrons (MLPs) often relies on a fixed
activation function (e.g., ReLU, Sigmoid, Tanh) for all nodes within the hidden
layers. While effective in many scenarios, this uniformity may limit the
networks ability to capture complex data patterns. We argue that employing the
same activation function at every node is suboptimal and propose leveraging
different activation functions at each node to increase flexibility and
adaptability. To achieve this, we introduce Local Control Networks (LCNs),
which leverage B-spline functions to enable distinct activation curves at each
node. Our mathematical analysis demonstrates the properties and benefits of
LCNs over conventional MLPs. In addition, we demonstrate that more complex
architectures, such as Kolmogorov-Arnold Networks (KANs), are unnecessary in
certain scenarios, and LCNs can be a more efficient alternative. Empirical
experiments on various benchmarks and datasets validate our theoretical
findings. In computer vision tasks, LCNs achieve marginal improvements over
MLPs and outperform KANs by approximately 5\%, while also being more
computationally efficient than KANs. In basic machine learning tasks, LCNs show
a 1\% improvement over MLPs and a 0.6\% improvement over KANs. For symbolic
formula representation tasks, LCNs perform on par with KANs, with both
architectures outperforming MLPs. Our findings suggest that diverse activations
at the node level can lead to improved performance and efficiency.

</details>


### [257] [RLeXplore: Accelerating Research in Intrinsically-Motivated Reinforcement Learning](https://arxiv.org/abs/2405.19548)
*Mingqi Yuan, Roger Creus Castanyer, Bo Li, Xin Jin, Wenjun Zeng, Glen Berseth*

Main category: cs.LG

TL;DR: RLeXplore is a unified framework for intrinsic rewards in RL, addressing implementation gaps and standardizing practices.


<details>
  <summary>Details</summary>
Motivation: Extrinsic rewards are often inadequate in complex environments, necessitating intrinsic rewards for unsupervised learning. Current intrinsic reward methods lack standardization and detailed implementation.

Method: Introduces RLeXplore, a modular framework with eight state-of-the-art intrinsic reward methods, and studies critical implementation details.

Result: Provides reliable implementations and establishes standard practices for intrinsic rewards in RL.

Conclusion: RLeXplore bridges the gap in intrinsic reward research, offering a practical and standardized solution.

Abstract: Extrinsic rewards can effectively guide reinforcement learning (RL) agents in
specific tasks. However, extrinsic rewards frequently fall short in complex
environments due to the significant human effort needed for their design and
annotation. This limitation underscores the necessity for intrinsic rewards,
which offer auxiliary and dense signals and can enable agents to learn in an
unsupervised manner. Although various intrinsic reward formulations have been
proposed, their implementation and optimization details are insufficiently
explored and lack standardization, thereby hindering research progress. To
address this gap, we introduce RLeXplore, a unified, highly modularized, and
plug-and-play framework offering reliable implementations of eight
state-of-the-art intrinsic reward methods. Furthermore, we conduct an in-depth
study that identifies critical implementation details and establishes
well-justified standard practices in intrinsically-motivated RL. Our
documentation, examples, and source code are available at
https://github.com/RLE-Foundation/RLeXplore.

</details>


### [258] [MAP: Low-compute Model Merging with Amortized Pareto Fronts via Quadratic Approximation](https://arxiv.org/abs/2406.07529)
*Lu Li, Tianyu Zhang, Zhiqi Bu, Suyuchen Wang, Huan He, Jie Fu, Yonghui Wu, Jiang Bian, Yong Chen, Yoshua Bengio*

Main category: cs.LG

TL;DR: MAP introduces a low-compute algorithm to merge models by identifying Pareto trade-offs, using quadratic approximations to reduce evaluation costs.


<details>
  <summary>Details</summary>
Motivation: Existing model-merging methods focus on average accuracy, but trade-offs between tasks are ignored. MAP provides diverse solutions for real-world decision-making.

Method: MAP uses quadratic approximation surrogate models to estimate the Pareto front of scaling coefficients, with variants (Bayesian MAP, Nested MAP) for different task scales.

Result: MAP accurately identifies Pareto fronts, offering flexible solutions for balancing task objectives in vision and NLP tasks.

Conclusion: MAP efficiently provides diverse model-merging solutions, reducing computational costs while addressing task trade-offs.

Abstract: Model merging has emerged as an effective approach to combine multiple
single-task models into a multitask model. This process typically involves
computing a weighted average of the model parameters without any additional
training. Existing model-merging methods focus on enhancing average task
accuracy. However, interference and conflicts between the objectives of
different tasks can lead to trade-offs during the merging process. In
real-world applications, a set of solutions with various trade-offs can be more
informative, helping practitioners make decisions based on diverse preferences.
In this paper, we introduce a novel and low-compute algorithm, Model Merging
with Amortized Pareto Front (MAP). MAP efficiently identifies a Pareto set of
scaling coefficients for merging multiple models, reflecting the trade-offs
involved. It amortizes the substantial computational cost of evaluations needed
to estimate the Pareto front by using quadratic approximation surrogate models
derived from a pre-selected set of scaling coefficients. Experimental results
on vision and natural language processing tasks demonstrate that MAP can
accurately identify the Pareto front, providing practitioners with flexible
solutions to balance competing task objectives. We also introduce Bayesian MAP
for scenarios with a relatively low number of tasks and Nested MAP for
situations with a high number of tasks, further reducing the computational cost
of evaluation.

</details>


### [259] [MCNC: Manifold-Constrained Reparameterization for Neural Compression](https://arxiv.org/abs/2406.19301)
*Chayne Thrash, Ali Abbasi, Reed Andreas, Parsa Nooralinejad, Soroush Abbasi Koohpayegani, Hamed Pirsiavash, Soheil Kolouri*

Main category: cs.LG

TL;DR: A novel model compression method, Manifold-Constrained Neural Compression (MCNC), is introduced to address the storage and transmission challenges of large foundational models by constraining parameters to low-dimensional nonlinear manifolds, achieving high compression rates without compromising performance.


<details>
  <summary>Details</summary>
Motivation: The massive size of foundational models (e.g., 750GB for Llama 3.1 405B) poses storage and transmission challenges, prompting the need for efficient compression methods that maintain performance.

Method: MCNC constrains the parameter space to low-dimensional, pre-defined, and frozen nonlinear manifolds, leveraging the prevalence of good solutions in over-parameterized networks to achieve high compression rates.

Result: MCNC outperforms state-of-the-art baselines in compression, accuracy, and model reconstruction time across computer vision and NLP tasks.

Conclusion: MCNC offers an effective solution for compressing large models while maintaining high performance, with potential applications in diverse domains.

Abstract: The outstanding performance of large foundational models across diverse
tasks, from computer vision to speech and natural language processing, has
significantly increased their demand. However, storing and transmitting these
models poses significant challenges due to their massive size (e.g., 750GB for
Llama 3.1 405B). Recent literature has focused on compressing the original
weights or reducing the number of parameters required for fine-tuning these
models. These compression methods generally constrain the parameter space, for
example, through low-rank reparametrization (e.g., LoRA), pruning, or
quantization (e.g., QLoRA) during or after the model training. In this paper,
we present a novel model compression method, which we term Manifold-Constrained
Neural Compression (MCNC). This method constrains the parameter space to
low-dimensional pre-defined and frozen nonlinear manifolds, which effectively
cover this space. Given the prevalence of good solutions in over-parameterized
deep neural networks, we show that by constraining the parameter space to our
proposed manifold, we can identify high-quality solutions while achieving
unprecedented compression rates across a wide variety of tasks and
architectures. Through extensive experiments in computer vision and natural
language processing tasks, we demonstrate that our method significantly
outperforms state-of-the-art baselines in terms of compression, accuracy,
and/or model reconstruction time. Our code is publicly available at
https://github.com/mint-vu/MCNC.

</details>


### [260] [Activation degree thresholds and expressiveness of polynomial neural networks](https://arxiv.org/abs/2408.04569)
*Bella Finkel, Jose Israel Rodriguez, Chenxi Wu, Thomas Yahl*

Main category: cs.LG

TL;DR: Deep polynomial neural networks' expressive power is analyzed via neurovariety geometry, introducing the activation degree threshold. A universal upper bound is proven, and equi-width architectures are shown to be maximally expressive.


<details>
  <summary>Details</summary>
Motivation: To understand the expressive power of deep polynomial neural networks by examining their neurovariety geometry and activation degree thresholds.

Method: Introduce the activation degree threshold concept, prove its existence and a universal upper bound, and analyze structured architectures like equi-width networks.

Result: Proven existence of activation degree threshold, a quadratic upper bound, and maximal expressiveness of equi-width architectures.

Conclusion: Deep polynomial neural networks without width-one bottlenecks have predictable expressive power, with equi-width architectures being maximally expressive.

Abstract: We study the expressive power of deep polynomial neural networks through the
geometry of their neurovariety. We introduce the notion of the activation
degree threshold of a network architecture to express when the dimension of the
neurovariety achieves its theoretical maximum. We prove the existence of the
activation degree threshold for all polynomial neural networks without
width-one bottlenecks and demonstrate a universal upper bound that is quadratic
in the width of largest size. In doing so, we prove the high activation degree
conjecture of Kileel, Trager, and Bruna. Certain structured architectures have
exceptional activation degree thresholds, making them especially expressive in
the sense of their neurovariety dimension. In this direction, we prove that
polynomial neural networks with equi-width architectures are maximally
expressive by showing their activation degree threshold is one.

</details>


### [261] [Learning Actionable World Models for Industrial Process Control](https://arxiv.org/abs/2503.01411)
*Peng Yan, Ahmed Abdulkadir, Gerrit A. Schatte, Giulia Aguzzi, Joonsu Gha, Nikola Pascher, Matthias Rosenthal, Yunlong Gao, Benjamin F. Grewe, Thilo Stadelmann*

Main category: cs.LG

TL;DR: A novel AI methodology learns world models from limited data to control complex processes, validated on plastic injection molding.


<details>
  <summary>Details</summary>
Motivation: Transition from passive monitoring to active control of complex processes with limited training data.

Method: Uses contrastive learning in a joint embedding predictive architecture to disentangle latent process parameters.

Result: Enables interpretable control actions, validated on plastic injection molding.

Conclusion: Proposed method effectively bridges monitoring and control for unstable processes.

Abstract: To go from (passive) process monitoring to active process control, an
effective AI system must learn about the behavior of the complex system from
very limited training data, forming an ad-hoc digital twin with respect to
process inputs and outputs that captures the consequences of actions on the
process's world. We propose a novel methodology based on learning world models
that disentangles process parameters in the learned latent representation,
allowing for fine-grained control. Representation learning is driven by the
latent factors influencing the processes through contrastive learning within a
joint embedding predictive architecture. This makes changes in representations
predictable from changes in inputs and vice versa, facilitating
interpretability of key factors responsible for process variations, paving the
way for effective control actions to keep the process within operational
bounds. The effectiveness of our method is validated on the example of plastic
injection molding, demonstrating practical relevance in proposing specific
control actions for a notoriously unstable process.

</details>


### [262] [Efficient fine-tuning of 37-level GraphCast with the Canadian global deterministic analysis](https://arxiv.org/abs/2408.14587)
*Christopher Subich*

Main category: cs.LG

TL;DR: The paper presents a method to fine-tune GraphCast for simulating GDPS, achieving better forecast skill than the original model and operational forecasts.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy of GraphCast for simulating GDPS by adapting its training process.

Method: Abbreviated GraphCast's training curriculum, using shorter single-step forecasts and consolidated autoregressive stages with adjusted learning rates.

Result: The fine-tuned model outperforms unmodified GraphCast and operational forecasts, showing skill in tropospheric forecasts for 1-10 days.

Conclusion: Efficient fine-tuning of GraphCast enhances its performance for specific forecasting tasks like GDPS simulation.

Abstract: This work describes a process for efficiently fine-tuning the GraphCast
data-driven forecast model to simulate another analysis system, here the Global
Deterministic Prediction System (GDPS) of Environment and Climate Change Canada
(ECCC). Using two years of training data (July 2019 -- December 2021) and 37
GPU-days of computation to tune the 37-level, quarter-degree version of
GraphCast, the resulting model significantly outperforms both the unmodified
GraphCast and operational forecast, showing significant forecast skill in the
troposphere over lead times from 1 to 10 days. This fine-tuning is accomplished
through abbreviating DeepMind's original training curriculum for GraphCast,
relying on a shorter single-step forecast stage to accomplish the bulk of the
adaptation work and consolidating the autoregressive stages into separate 12hr,
1d, 2d, and 3d stages with larger learning rates. Additionally, training over
3d forecasts is split into two sub-steps to conserve host memory while
maintaining a strong correlation with training over the full period.

</details>


### [263] [Generative AI-Powered Plugin for Robust Federated Learning in Heterogeneous IoT Networks](https://arxiv.org/abs/2410.23824)
*Youngjoon Lee, Jinu Gong, Joonhyuk Kang*

Main category: cs.LG

TL;DR: A novel plugin for federated learning uses generative AI and balanced sampling to address Non-IID data issues, improving convergence and performance.


<details>
  <summary>Details</summary>
Motivation: Non-IID data distribution in federated learning hinders model convergence and performance, necessitating a solution to approximate IID-like conditions.

Method: The approach combines generative AI for data augmentation (synthesizing underrepresented classes) and a balanced sampling strategy (selecting IID-like devices).

Result: Experiments show improved convergence speed and robustness against data imbalance, even in data-scarce settings.

Conclusion: The proposed plugin offers a flexible, privacy-preserving solution for federated learning, enhancing performance in Non-IID scenarios.

Abstract: Federated learning enables edge devices to collaboratively train a global
model while maintaining data privacy by keeping data localized. However, the
Non-IID nature of data distribution across devices often hinders model
convergence and reduces performance. In this paper, we propose a novel plugin
for federated optimization techniques that approximates Non-IID data
distributions to IID through generative AI-enhanced data augmentation and
balanced sampling strategy. Key idea is to synthesize additional data for
underrepresented classes on each edge device, leveraging generative AI to
create a more balanced dataset across the FL network. Additionally, a balanced
sampling approach at the central server selectively includes only the most
IID-like devices, accelerating convergence while maximizing the global model's
performance. Experimental results validate that our approach significantly
improves convergence speed and robustness against data imbalance, establishing
a flexible, privacy-preserving FL plugin that is applicable even in data-scarce
environments.

</details>


### [264] [Reinforcement Learning-based Threat Assessment](https://arxiv.org/abs/2503.02612)
*Wuzhou Sun, Siyi Li, Qingxiang Zou, Zixing Liao*

Main category: cs.LG

TL;DR: The paper proposes a reinforcement learning-based neural network evaluator for accurate threat assessment in games by prioritizing enemy attributes.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in evaluating enemy threat levels due to uncertain unit numbers and attribute priorities.

Method: The problem is transformed into a reinforcement learning task, training a neural network to integrate multidimensional enemy attributes and state information.

Result: An efficient evaluator is developed, enabling precise and scientific threat assessment.

Conclusion: The approach successfully addresses the challenge of threat evaluation in dynamic game scenarios.

Abstract: In some game scenarios, due to the uncertainty of the number of enemy units
and the priority of various attributes, the evaluation of the threat level of
enemy units as well as the screening has been a challenging research topic, and
the core difficulty lies in how to reasonably set the priority of different
attributes in order to achieve quantitative evaluation of the threat. In this
paper, we innovatively transform the problem of threat assessment into a
reinforcement learning problem, and through systematic reinforcement learning
training, we successfully construct an efficient neural network evaluator. The
evaluator can not only comprehensively integrate the multidimensional attribute
features of the enemy, but also effectively combine our state information, thus
realizing a more accurate and scientific threat assessment.

</details>


### [265] [Semantic Edge Computing and Semantic Communications in 6G Networks: A Unifying Survey and Research Challenges](https://arxiv.org/abs/2411.18199)
*Milin Zhang, Mohammad Abdi, Venkat R. Dasari, Francesco Restuccia*

Main category: cs.LG

TL;DR: The paper unifies Semantic Edge Computing (SEC) and Semantic Communications (SemComs) in 6G networks, reviewing their strengths, challenges, and research gaps.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a systematic connection between SEC and SemComs, both crucial for real-time edge intelligence in 6G networks.

Method: Summarizes research problems and reviews state-of-the-art in SEC and SemComs, focusing on technical strengths and challenges.

Result: Provides a unified view of SEC and SemComs, highlighting their potential and limitations.

Conclusion: The work bridges the gap between SEC and SemComs, offering insights for future research in 6G networks.

Abstract: Semantic Edge Computing (SEC) and Semantic Communications (SemComs) have been
proposed as viable approaches to achieve real-time edge-enabled intelligence in
sixth-generation (6G) wireless networks. On one hand, SemCom leverages the
strength of Deep Neural Networks (DNNs) to encode and communicate the semantic
information only, while making it robust to channel distortions by compensating
for wireless effects. Ultimately, this leads to an improvement in the
communication efficiency. On the other hand, SEC has leveraged distributed DNNs
to divide the computation of a DNN across different devices based on their
computational and networking constraints. Although significant progress has
been made in both fields, the literature lacks a systematic view to connect
both fields. In this work, we fulfill the current gap by unifying the SEC and
SemCom fields. We summarize the research problems in these two fields and
provide a comprehensive review of the state of the art with a focus on their
technical strengths and challenges.

</details>


### [266] [Deep Cut-informed Graph Embedding and Clustering](https://arxiv.org/abs/2503.06635)
*Zhiyuan Ning, Zaitian Wang, Ran Zhang, Ping Xu, Kunpeng Liu, Pengyang Wang, Wei Ju, Pengfei Wang, Yuanchun Zhou, Erik Cambria, Chong Chen*

Main category: cs.LG

TL;DR: The paper proposes DCGC, a non-GNN-based deep graph clustering framework, addressing representation collapse in GNN-based methods by using cut-informed graph encoding and optimal transport for clustering.


<details>
  <summary>Details</summary>
Motivation: Existing GNN-based deep graph clustering methods suffer from representation collapse due to inductive bias and clustering-guided loss functions, leading to biased or degenerate solutions.

Method: DCGC employs cut-informed graph encoding to fuse structure and attributes by minimizing joint normalized cut, and uses optimal transport theory for balanced clustering assignments.

Result: DCGC outperforms benchmarks, effectively mitigating representation collapse and improving clustering performance.

Conclusion: DCGC is a simple yet effective alternative to GNN-based methods for graph clustering, offering better performance and robustness.

Abstract: Graph clustering aims to divide the graph into different clusters. The
recently emerging deep graph clustering approaches are largely built on graph
neural networks (GNN). However, GNN is designed for general graph encoding and
there is a common issue of representation collapse in existing GNN-based deep
graph clustering algorithms. We attribute two main reasons for such issues: (i)
the inductive bias of GNN models: GNNs tend to generate similar representations
for proximal nodes. Since graphs often contain a non-negligible amount of
inter-cluster links, the bias results in error message passing and leads to
biased clustering; (ii) the clustering guided loss function: most traditional
approaches strive to make all samples closer to pre-learned cluster centers,
which causes a degenerate solution assigning all data points to a single label
thus making all samples similar and less discriminative. To address these
challenges, we investigate graph clustering from a graph cut perspective and
propose an innovative and non-GNN-based Deep Cut-informed Graph embedding and
Clustering framework, namely DCGC. This framework includes two modules: (i)
cut-informed graph encoding; (ii) self-supervised graph clustering via optimal
transport. For the encoding module, we derive a cut-informed graph embedding
objective to fuse graph structure and attributes by minimizing their joint
normalized cut. For the clustering module, we utilize the optimal transport
theory to obtain the clustering assignments, which can balance the guidance of
"proximity to the pre-learned cluster center". With the above two tailored
designs, DCGC is more suitable for the graph clustering task, which can
effectively alleviate the problem of representation collapse and achieve better
performance. We conduct extensive experiments to demonstrate that our method is
simple but effective compared with benchmarks.

</details>


### [267] [Dense Dynamics-Aware Reward Synthesis: Integrating Prior Experience with Demonstrations](https://arxiv.org/abs/2412.01114)
*Cevahir Koprulu, Po-han Li, Tianyu Qiu, Ruihan Zhao, Tyler Westenbroek, David Fridovich-Keil, Sandeep Chinchali, Ufuk Topcu*

Main category: cs.LG

TL;DR: A framework for dense reward synthesis in sparse-reward RL tasks using prior data and expert demonstrations accelerates learning.


<details>
  <summary>Details</summary>
Motivation: Sparse-reward RL tasks are challenging due to difficulty in discovering reward-yielding action sequences, especially over long horizons. Manual reward shaping is tedious and task-specific.

Method: The framework distills information from a task-agnostic prior dataset and task-specific expert demonstrations to synthesize dense, dynamics-aware rewards.

Result: The synthesized rewards substantially accelerate learning, effectively guiding agents to distant goals.

Conclusion: The proposed framework offers a systematic way to leverage prior knowledge and demonstrations for efficient RL in sparse-reward settings.

Abstract: Many continuous control problems can be formulated as sparse-reward
reinforcement learning (RL) tasks. In principle, online RL methods can
automatically explore the state space to solve each new task. However,
discovering sequences of actions that lead to a non-zero reward becomes
exponentially more difficult as the task horizon increases. Manually shaping
rewards can accelerate learning for a fixed task, but it is an arduous process
that must be repeated for each new environment. We introduce a systematic
reward-shaping framework that distills the information contained in 1) a
task-agnostic prior data set and 2) a small number of task-specific expert
demonstrations, and then uses these priors to synthesize dense dynamics-aware
rewards for the given task. This supervision substantially accelerates learning
in our experiments, and we provide analysis demonstrating how the approach can
effectively guide online learning agents to faraway goals.

</details>


### [268] [Structure Learning in Gaussian Graphical Models from Glauber Dynamics](https://arxiv.org/abs/2412.18594)
*Vignesh Tirukkonda, Anirudh Rayas, Gautam Dasarathy*

Main category: cs.LG

TL;DR: The paper introduces the first algorithm for Gaussian graphical model selection under Glauber dynamics, providing theoretical guarantees and near-optimal performance.


<details>
  <summary>Details</summary>
Motivation: Traditional methods assume i.i.d. samples, which are impractical in real-world scenarios like social or financial networks. This work addresses the gap by using Glauber dynamics, a more realistic dependent process.

Method: The proposed algorithm handles Gaussian graphical model selection with data sampled via Glauber dynamics, a Markov chain updating variables sequentially.

Result: The algorithm offers theoretical guarantees on computational and statistical complexity, with near-minimax optimal performance for a broad problem class.

Conclusion: This work advances Gaussian graphical model selection by addressing dependent data scenarios, providing a practical and theoretically sound solution.

Abstract: Gaussian graphical model selection is an important paradigm with numerous
applications, including biological network modeling, financial network
modeling, and social network analysis. Traditional approaches assume access to
independent and identically distributed (i.i.d) samples, which is often
impractical in real-world scenarios. In this paper, we address Gaussian
graphical model selection under observations from a more realistic dependent
stochastic process known as Glauber dynamics. Glauber dynamics, also called the
Gibbs sampler, is a Markov chain that sequentially updates the variables of the
underlying model based on the statistics of the remaining model. Such models,
aside from frequently being employed to generate samples from complex
multivariate distributions, naturally arise in various settings, such as
opinion consensus in social networks and clearing/stock-price dynamics in
financial networks.
  In contrast to the extensive body of existing work, we present the first
algorithm for Gaussian graphical model selection when data are sampled
according to the Glauber dynamics. We provide theoretical guarantees on the
computational and statistical complexity of the proposed algorithm's structure
learning performance. Additionally, we provide information-theoretic lower
bounds on the statistical complexity and show that our algorithm is nearly
minimax optimal for a broad class of problems.

</details>


### [269] [UniGraph2: Learning a Unified Embedding Space to Bind Multimodal Graphs](https://arxiv.org/abs/2502.00806)
*Yufei He, Yuan Sui, Xiaoxin He, Yue Liu, Yifei Sun, Bryan Hooi*

Main category: cs.LG

TL;DR: UniGraph2 is a novel cross-domain graph foundation model designed for multimodal graphs (MMGs), addressing limitations of existing models by unifying multimodal and graph-structured data in a single embedding space.


<details>
  <summary>Details</summary>
Motivation: Existing models like CLIP and graph foundation models overlook the inherent graph structures in multimodal datasets, limiting their effectiveness for tasks involving multimodal graphs.

Method: UniGraph2 uses modality-specific encoders, a GNN, and a Mixture of Experts (MoE) component to create a unified embedding space, alongside a cross-domain multi-graph pre-training algorithm.

Result: UniGraph2 outperforms state-of-the-art models in tasks like representation learning, transfer learning, and multimodal generative tasks.

Conclusion: UniGraph2 provides a scalable and flexible solution for learning on MMGs, unifying multimodal and graph-structured data effectively.

Abstract: Existing foundation models, such as CLIP, aim to learn a unified embedding
space for multimodal data, enabling a wide range of downstream web-based
applications like search, recommendation, and content classification. However,
these models often overlook the inherent graph structures in multimodal
datasets, where entities and their relationships are crucial. Multimodal graphs
(MMGs) represent such graphs where each node is associated with features from
different modalities, while the edges capture the relationships between these
entities. On the other hand, existing graph foundation models primarily focus
on text-attributed graphs (TAGs) and are not designed to handle the
complexities of MMGs. To address these limitations, we propose UniGraph2, a
novel cross-domain graph foundation model that enables general representation
learning on MMGs, providing a unified embedding space. UniGraph2 employs
modality-specific encoders alongside a graph neural network (GNN) to learn a
unified low-dimensional embedding space that captures both the multimodal
information and the underlying graph structure. We propose a new cross-domain
multi-graph pre-training algorithm at scale to ensure effective transfer
learning across diverse graph domains and modalities. Additionally, we adopt a
Mixture of Experts (MoE) component to align features from different domains and
modalities, ensuring coherent and robust embeddings that unify the information
across modalities. Extensive experiments on a variety of multimodal graph tasks
demonstrate that UniGraph2 significantly outperforms state-of-the-art models in
tasks such as representation learning, transfer learning, and multimodal
generative tasks, offering a scalable and flexible solution for learning on
MMGs.

</details>


### [270] [Application of linear regression and quasi-Newton methods to the deep reinforcement learning in continuous action cases](https://arxiv.org/abs/2503.14976)
*Hisato Komatsu*

Main category: cs.LG

TL;DR: The paper proposes DLS-DDPG, combining LR with DDPG for continuous actions, improving performance in some MuJoCo tasks.


<details>
  <summary>Details</summary>
Motivation: To address LS-DQN's limitation of assuming discrete actions by extending it to continuous action spaces.

Method: Combines LR with DDPG, uses Fitted Q iteration for critic updates, and quasi-Newton method for optimal actions.

Result: Improved performance in some MuJoCo tasks, though challenges like small regularization terms remain.

Conclusion: DLS-DDPG successfully extends LS-DQN to continuous actions, showing promise despite unresolved issues.

Abstract: The linear regression (LR) method offers the advantage that optimal
parameters can be calculated relatively easily, although its representation
capability is limited than that of the deep learning technique. To improve deep
reinforcement learning, the Least Squares Deep Q Network (LS-DQN) method was
proposed by Levine et al., which combines Deep Q Network (DQN) with LR method.
However, the LS-DQN method assumes that the actions are discrete. In this
study, we propose the Double Least Squares Deep Deterministic Policy Gradient
(DLS-DDPG) method to address this limitation. This method combines the LR
method with the Deep Deterministic Policy Gradient (DDPG) technique, one of the
representative deep reinforcement learning algorithms for continuous action
cases. For the LR update of the critic network, DLS-DDPG uses an algorithm
similar to the Fitted Q iteration, the method which LS-DQN adopted. In
addition, we calculated the optimal action using the quasi-Newton method and
used it as both the agent's action and the training data for the LR update of
the actor network. Numerical experiments conducted in MuJoCo environments
showed that the proposed method improved performance at least in some tasks,
although there are difficulties such as the inability to make the
regularization terms small.

</details>


### [271] [Efficient Model Editing with Task Vector Bases: A Theoretical Framework and Scalable Approach](https://arxiv.org/abs/2502.01015)
*Siqi Zeng, Yifei He, Weiqiu You, Yifan Hao, Yao-Hung Hubert Tsai, Makoto Yamada, Han Zhao*

Main category: cs.LG

TL;DR: The paper introduces a theoretically grounded framework for task vector arithmetic, reducing memory costs while maintaining performance and flexibility.


<details>
  <summary>Details</summary>
Motivation: Existing task vector approaches lack theoretical support, leading to performance gaps and high memory usage, limiting scalability.

Method: The paper proposes the task vector bases framework, building on existing task arithmetic literature to reduce memory costs and maintain performance.

Result: The method significantly reduces memory usage for downstream arithmetic while achieving competitive performance and retaining compositional flexibility.

Conclusion: The framework provides a practical solution for large-scale task arithmetic, balancing performance and scalability.

Abstract: Task vectors, which are derived from the difference between pre-trained and
fine-tuned model weights, enable flexible task adaptation and model merging
through arithmetic operations such as addition and negation. However, existing
approaches often rely on heuristics with limited theoretical support, often
leading to performance gaps comparing to direct task fine tuning. Meanwhile,
although it is easy to manipulate saved task vectors with arithmetic for
different purposes, such compositional flexibility demands high memory usage,
especially when dealing with a huge number of tasks, limiting scalability. This
work addresses these issues with a theoretically grounded framework that
explains task vector arithmetic and introduces the task vector bases framework.
Building upon existing task arithmetic literature, our method significantly
reduces the memory cost for downstream arithmetic with little effort, while
achieving competitive performance and maintaining compositional advantage,
providing a practical solution for large-scale task arithmetic. The code is
available at https://github.com/uiuctml/TaskVectorBasis.

</details>


### [272] [Effective and Efficient Cross-City Traffic Knowledge Transfer: A Privacy-Preserving Perspective](https://arxiv.org/abs/2503.11963)
*Zhihao Zeng, Ziquan Fang, Yuting Huang, Lu Chen, Yunjun Gao*

Main category: cs.LG

TL;DR: FedTT is a privacy-aware, efficient framework for cross-city traffic knowledge transfer, addressing challenges like data scarcity, privacy leakage, and distribution discrepancies.


<details>
  <summary>Details</summary>
Motivation: Current Federated Traffic Knowledge Transfer (FTT) approaches struggle with privacy leakage, data discrepancies, low quality, and inefficiency, limiting real-world applicability.

Method: FedTT includes secure data transmission, domain adaptation, data imputation, and parallel training to enhance privacy, effectiveness, and efficiency.

Result: FedTT outperforms 14 state-of-the-art baselines in experiments using 4 real-life datasets.

Conclusion: FedTT effectively addresses key challenges in traffic knowledge transfer, offering improved privacy, robustness, and efficiency.

Abstract: Traffic prediction targets forecasting future traffic conditions using
historical traffic data, serving a critical role in urban computing and
transportation management. To mitigate the scarcity of traffic data while
maintaining data privacy, numerous Federated Traffic Knowledge Transfer (FTT)
approaches have been developed, which use transfer learning and federated
learning to transfer traffic knowledge from data-rich cities to data-scarce
cities, enhancing traffic prediction capabilities for the latter. However,
current FTT approaches face challenges such as privacy leakage, cross-city data
distribution discrepancies, low data quality, and inefficient knowledge
transfer, limiting their privacy protection, effectiveness, robustness, and
efficiency in real-world applications.
  To this end, we propose FedTT, an effective, efficient, and privacy-aware
cross-city traffic knowledge transfer framework that transforms the traffic
data domain from the data-rich cities and trains traffic models using the
transformed data for the data-scarce cities. First, to safeguard data privacy,
we propose a traffic secret transmission method that securely transmits and
aggregates traffic domain-transformed data from source cities using a
lightweight secret aggregation approach. Second, to mitigate the impact of
traffic data distribution discrepancies on model performance, we introduce a
traffic domain adapter to uniformly transform traffic data from the source
cities' domains to that of the target city. Third, to improve traffic data
quality, we design a traffic view imputation method to fill in and predict
missing traffic data. Finally, to enhance transfer efficiency, FedTT is
equipped with a federated parallel training method that enables the
simultaneous training of multiple modules. Extensive experiments using 4
real-life datasets demonstrate that FedTT outperforms the 14 state-of-the-art
baselines.

</details>


### [273] [Eval-PPO: Building an Efficient Threat Evaluator Using Proximal Policy Optimization](https://arxiv.org/abs/2503.12098)
*Wuzhou Sun, Siyi Li, Qingxiang Zou, Zixing Liao*

Main category: cs.LG

TL;DR: The paper proposes Eval-PPO, a reinforcement learning-based method for threat evaluation in games, outperforming rule-based methods by 17.84%.


<details>
  <summary>Details</summary>
Motivation: Traditional rule-based evaluators and supervised learning struggle with threat assessment due to complex enemy relationships and lack of explicit labels.

Method: Redefines threat evaluation as a reinforcement learning task using Eval-PPO, an algorithm based on PPO, integrating enemy and friendly unit features.

Result: Eval-PPO achieves a 17.84% higher average success rate compared to rule-based methods.

Conclusion: Eval-PPO is an effective solution for precise threat evaluation in game scenarios.

Abstract: In various game scenarios, selecting a fixed number of targets from multiple
enemy units is an extremely challenging task. This difficulty stems from the
complex relationship between the threat levels of enemy units and their feature
characteristics, which complicates the design of rule-based evaluators.
Moreover, traditional supervised learning methods face the challenge of lacking
explicit labels during training when applied to this threat evaluation problem.
In this study, we redefine the threat evaluation problem as a reinforcement
learning task and introduce an efficient evaluator training algorithm,
Eval-PPO, based on the Proximal Policy Optimization (PPO) algorithm. Eval-PPO
integrates multidimensional enemy features and the state information of
friendly units through systematic training, thereby achieving precise threat
assessment. Compared with rule-based methods, Eval-PPO demonstrates a
significant improvement in average success rate, with an increase of 17.84%.

</details>


### [274] [AMAD: AutoMasked Attention for Unsupervised Multivariate Time Series Anomaly Detection](https://arxiv.org/abs/2504.06643)
*Tiange Huang, Yongjun Li*

Main category: cs.LG

TL;DR: AMAD introduces AutoMasked Attention and attention mixup for unsupervised multivariate time series anomaly detection, achieving competitive performance.


<details>
  <summary>Details</summary>
Motivation: Existing models for UMTSAD are limited to predefined anomaly patterns, hindering generalization.

Method: AMAD combines AutoMask mechanism, attention mixup, Max-Min training, and Local-Global contrastive learning.

Result: AMAD achieves competitive performance across diverse datasets.

Conclusion: AMAD provides a robust and adaptable solution for UMTSAD challenges.

Abstract: Unsupervised multivariate time series anomaly detection (UMTSAD) plays a
critical role in various domains, including finance, networks, and sensor
systems. In recent years, due to the outstanding performance of deep learning
in general sequential tasks, many models have been specialized for deep UMTSAD
tasks and have achieved impressive results, particularly those based on the
Transformer and self-attention mechanisms. However, the sequence anomaly
association assumptions underlying these models are often limited to specific
predefined patterns and scenarios, such as concentrated or peak anomaly
patterns. These limitations hinder their ability to generalize to diverse
anomaly situations, especially where the lack of labels poses significant
challenges. To address these issues, we propose AMAD, which integrates
\textbf{A}uto\textbf{M}asked Attention for UMTS\textbf{AD} scenarios. AMAD
introduces a novel structure based on the AutoMask mechanism and an attention
mixup module, forming a simple yet generalized anomaly association
representation framework. This framework is further enhanced by a Max-Min
training strategy and a Local-Global contrastive learning approach. By
combining multi-scale feature extraction with automatic relative association
modeling, AMAD provides a robust and adaptable solution to UMTSAD challenges.
Extensive experimental results demonstrate that the proposed model achieving
competitive performance results compared to SOTA benchmarks across a variety of
datasets.

</details>


### [275] [Extending Cox Proportional Hazards Model with Symbolic Non-Linear Log-Risk Functions for Survival Analysis](https://arxiv.org/abs/2504.04353)
*Jiaxiang Cheng, Guoqiang Hu*

Main category: cs.LG

TL;DR: The paper introduces the Generalized Cox Proportional Hazards (GCPH) model, using Kolmogorov-Arnold Networks (KAN) to enable interpretable non-linear survival analysis, outperforming traditional and deep learning methods.


<details>
  <summary>Details</summary>
Motivation: Traditional CPH models are limited by linear assumptions, while deep learning lacks interpretability. GCPH bridges this gap with symbolic non-linear functions.

Method: GCPH leverages KAN for non-linear, interpretable log-risk functions, maintaining CPH's transparency while enhancing flexibility.

Result: GCPH shows competitive accuracy and superior interpretability on synthetic and benchmark datasets.

Conclusion: GCPH advances survival analysis by combining interpretability with non-linear modeling, outperforming existing methods.

Abstract: The Cox proportional hazards (CPH) model has been widely applied in survival
analysis to estimate relative risks across different subjects given multiple
covariates. Traditional CPH models rely on a linear combination of covariates
weighted with coefficients as the log-risk function, which imposes a strong and
restrictive assumption, limiting generalization. Recent deep learning methods
enable non-linear log-risk functions. However, they often lack interpretability
due to the end-to-end training mechanisms. The implementation of
Kolmogorov-Arnold Networks (KAN) offers new possibilities for extending the CPH
model with fully transparent and symbolic non-linear log-risk functions. In
this paper, we introduce Generalized Cox Proportional Hazards (GCPH) model, a
novel method for survival analysis that leverages KAN to enable a non-linear
mapping from covariates to survival outcomes in a fully symbolic manner. GCPH
maintains the interpretability of traditional CPH models while allowing for the
estimation of non-linear log-risk functions. Experiments conducted on both
synthetic data and various public benchmarks demonstrate that GCPH achieves
competitive performance in terms of prediction accuracy and exhibits superior
interpretability compared to current state-of-the-art methods.

</details>


### [276] [Deriving Equivalent Symbol-Based Decision Models from Feedforward Neural Networks](https://arxiv.org/abs/2504.12446)
*Sebastian Seidel, Uwe M. Borghoff*

Main category: cs.LG

TL;DR: The paper explores deriving interpretable symbolic models (e.g., decision trees) from neural networks to enhance AI transparency.


<details>
  <summary>Details</summary>
Motivation: Addressing the opacity of AI systems to improve trust and acceptance by making neural networks interpretable.

Method: A step-by-step approach to map neural network activations to decision trees, exploiting distributed representations.

Result: A prototype validates the framework, showing feasibility in extracting symbolic representations from neural networks.

Conclusion: The method bridges neural and symbolic AI, promoting accountability and trust in AI systems.

Abstract: Artificial intelligence (AI) has emerged as a transformative force across
industries, driven by advances in deep learning and natural language
processing, and fueled by large-scale data and computing resources. Despite its
rapid adoption, the opacity of AI systems poses significant challenges to trust
and acceptance.
  This work explores the intersection of connectionist and symbolic approaches
to artificial intelligence, focusing on the derivation of interpretable
symbolic models, such as decision trees, from feedforward neural networks
(FNNs). Decision trees provide a transparent framework for elucidating the
operations of neural networks while preserving their functionality. The
derivation is presented in a step-by-step approach and illustrated with several
examples. A systematic methodology is proposed to bridge neural and symbolic
paradigms by exploiting distributed representations in FNNs to identify
symbolic components, including fillers, roles, and their interrelationships.
The process traces neuron activation values and input configurations across
network layers, mapping activations and their underlying inputs to decision
tree edges. The resulting symbolic structures effectively capture FNN decision
processes and enable scalability to deeper networks through iterative
refinement of subpaths for each hidden layer.
  To validate the theoretical framework, a prototype was developed using Keras
.h5-data and emulating TensorFlow within the Java JDK/JavaFX environment. This
prototype demonstrates the feasibility of extracting symbolic representations
from neural networks, enhancing trust in AI systems, and promoting
accountability.

</details>


### [277] [Equi-Euler GraphNet: An Equivariant, Temporal-Dynamics Informed Graph Neural Network for Dual Force and Trajectory Prediction in Multi-Body Systems](https://arxiv.org/abs/2504.13768)
*Vinay Sharma, Rémi Tanguy Oddon, Pietro Tesini, Jens Ravesloot, Cees Taal, Olga Fink*

Main category: cs.LG

TL;DR: Equi-Euler GraphNet, a physics-informed GNN, predicts internal forces and trajectories in multi-body systems, outperforming state-of-the-art methods with high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Accurate real-time modeling of multi-body systems is crucial for digital twins, especially for fault detection and predictive maintenance, where internal loads are early fault indicators.

Method: Proposes Equi-Euler GraphNet, a mesh-free GNN with equivariant message-passing and Euler integration for temporal-aware updates, tailored for cylindrical roller bearings.

Result: Outperforms existing GNNs, generalizes beyond training data, and achieves 200x speedup over conventional solvers with minimal error accumulation.

Conclusion: Equi-Euler GraphNet is an efficient reduced-order model for digital twins, design, and maintenance, balancing accuracy and computational speed.

Abstract: Accurate real-time modeling of multi-body dynamical systems is essential for
enabling digital twin applications across industries. While many data-driven
approaches aim to learn system dynamics, jointly predicting internal loads and
system trajectories remains a key challenge. This dual prediction is especially
important for fault detection and predictive maintenance, where internal
loads-such as contact forces-act as early indicators of faults, reflecting wear
or misalignment before affecting motion. These forces also serve as inputs to
degradation models (e.g., crack growth), enabling damage prediction and
remaining useful life estimation. We propose Equi-Euler GraphNet, a
physics-informed graph neural network (GNN) that simultaneously predicts
internal forces and global trajectories in multi-body systems. In this
mesh-free framework, nodes represent system components and edges encode
interactions. Equi-Euler GraphNet introduces two inductive biases: (1) an
equivariant message-passing scheme, interpreting edge messages as interaction
forces consistent under Euclidean transformations; and (2) a temporal-aware
iterative node update mechanism, based on Euler integration, to capture
influence of distant interactions over time. Tailored for cylindrical roller
bearings, it decouples ring dynamics from constrained motion of rolling
elements. Trained on high-fidelity multiphysics simulations, Equi-Euler
GraphNet generalizes beyond the training distribution, accurately predicting
loads and trajectories under unseen speeds, loads, and configurations. It
outperforms state-of-the-art GNNs focused on trajectory prediction, delivering
stable rollouts over thousands of time steps with minimal error accumulation.
Achieving up to a 200x speedup over conventional solvers while maintaining
comparable accuracy, it serves as an efficient reduced-order model for digital
twins, design, and maintenance.

</details>


### [278] [Learning and Generating Diverse Residential Load Patterns Using GAN with Weakly-Supervised Training and Weight Selection](https://arxiv.org/abs/2504.14300)
*Xinyu Liang, Hao Wang*

Main category: cs.LG

TL;DR: A GAN-based model (RLP-GAN) generates synthetic residential load data, addressing scalability, diversity, and similarity issues in existing methods.


<details>
  <summary>Details</summary>
Motivation: The scarcity of high-quality residential load data hinders decarbonization and grid planning, necessitating better synthetic data generation.

Method: RLP-GAN uses a weakly-supervised GAN framework with an over-complete autoencoder to capture load dependencies and household-level distributions, including a weight selection method to prevent mode collapse.

Result: RLP-GAN outperforms state-of-the-art models in capturing temporal dependencies and generating realistic load patterns, validated on 417 real households.

Conclusion: RLP-GAN provides a scalable, diverse, and accurate solution for synthetic load data generation, with a publicly released dataset of one million profiles.

Abstract: The scarcity of high-quality residential load data can pose obstacles for
decarbonizing the residential sector as well as effective grid planning and
operation. The above challenges have motivated research into generating
synthetic load data, but existing methods faced limitations in terms of
scalability, diversity, and similarity. This paper proposes a Generative
Adversarial Network-based Synthetic Residential Load Pattern (RLP-GAN)
generation model, a novel weakly-supervised GAN framework, leveraging an
over-complete autoencoder to capture dependencies within complex and diverse
load patterns and learn household-level data distribution at scale. We
incorporate a model weight selection method to address the mode collapse
problem and generate load patterns with high diversity. We develop a holistic
evaluation method to validate the effectiveness of RLP-GAN using real-world
data of 417 households. The results demonstrate that RLP-GAN outperforms
state-of-the-art models in capturing temporal dependencies and generating load
patterns with higher similarity to real data. Furthermore, we have publicly
released the RLP-GAN generated synthetic dataset, which comprises one million
synthetic residential load pattern profiles.

</details>


### [279] [Generative Auto-Bidding with Value-Guided Explorations](https://arxiv.org/abs/2504.14587)
*Jingtong Gao, Yewen Li, Shuai Mao, Peng Jiang, Nan Jiang, Yejing Wang, Qingpeng Cai, Fei Pan, Peng Jiang, Kun Gai, Bo An, Xiangyu Zhao*

Main category: cs.LG

TL;DR: GAVE introduces a generative auto-bidding framework with value-guided exploration to address limitations of rule-based and RL methods, outperforming baselines in offline and online tests.


<details>
  <summary>Details</summary>
Motivation: Existing auto-bidding methods lack adaptability and struggle with historical dependencies, offline training issues, and diverse objectives.

Method: GAVE uses a score-based RTG module, action exploration, and a learnable value function to guide exploration and mitigate OOD problems.

Result: GAVE outperforms state-of-the-art baselines in offline evaluations and online A/B tests, winning the NeurIPS 2024 competition.

Conclusion: GAVE effectively addresses auto-bidding challenges, offering a flexible and stable solution for diverse advertising objectives.

Abstract: Auto-bidding, with its strong capability to optimize bidding decisions within
dynamic and competitive online environments, has become a pivotal strategy for
advertising platforms. Existing approaches typically employ rule-based
strategies or Reinforcement Learning (RL) techniques. However, rule-based
strategies lack the flexibility to adapt to time-varying market conditions, and
RL-based methods struggle to capture essential historical dependencies and
observations within Markov Decision Process (MDP) frameworks. Furthermore,
these approaches often face challenges in ensuring strategy adaptability across
diverse advertising objectives. Additionally, as offline training methods are
increasingly adopted to facilitate the deployment and maintenance of stable
online strategies, the issues of documented behavioral patterns and behavioral
collapse resulting from training on fixed offline datasets become increasingly
significant. To address these limitations, this paper introduces a novel
offline Generative Auto-bidding framework with Value-Guided Explorations
(GAVE). GAVE accommodates various advertising objectives through a score-based
Return-To-Go (RTG) module. Moreover, GAVE integrates an action exploration
mechanism with an RTG-based evaluation method to explore novel actions while
ensuring stability-preserving updates. A learnable value function is also
designed to guide the direction of action exploration and mitigate
Out-of-Distribution (OOD) problems. Experimental results on two offline
datasets and real-world deployments demonstrate that GAVE outperforms
state-of-the-art baselines in both offline evaluations and online A/B tests. By
applying the core methods of this framework, we proudly secured first place in
the NeurIPS 2024 competition, 'AIGB Track: Learning Auto-Bidding Agents with
Generative Models'.

</details>


### [280] [ScaleGNN: Towards Scalable Graph Neural Networks via Adaptive High-order Neighboring Feature Fusion](https://arxiv.org/abs/2504.15920)
*Xiang Li, Haobing Liu, Jianpeng Qi, Yuan Cao, Guoqing Chao, Yanwei Yu*

Main category: cs.LG

TL;DR: ScaleGNN addresses GNN challenges of over-smoothing and scalability by adaptively fusing multi-level graph features and masking redundant information, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: GNNs struggle with over-smoothing and scalability due to excessive message passing and redundant information aggregation.

Method: ScaleGNN uses adaptive high-order feature fusion, redundant feature masking via Local Contribution Score (LCS), and low-order enhanced feature aggregation.

Result: Outperforms state-of-the-art GNN models in accuracy and computational efficiency on real-world datasets.

Conclusion: ScaleGNN effectively balances local and global structural information while reducing computational costs.

Abstract: Graph Neural Networks (GNNs) have demonstrated strong performance across
various graph-based tasks by effectively capturing relational information
between nodes. These models rely on iterative message passing to propagate node
features, enabling nodes to aggregate information from their neighbors. Recent
research has significantly improved the message-passing mechanism, enhancing
GNN scalability on large-scale graphs. However, GNNs still face two main
challenges: over-smoothing, where excessive message passing results in
indistinguishable node representations, especially in deep networks
incorporating high-order neighbors; and scalability issues, as traditional
architectures suffer from high model complexity and increased inference time
due to redundant information aggregation. This paper proposes a novel framework
for large-scale graphs named ScaleGNN that simultaneously addresses both
challenges by adaptively fusing multi-level graph features. We first construct
neighbor matrices for each order, learning their relative information through
trainable weights through an adaptive high-order feature fusion module. This
allows the model to selectively emphasize informative high-order neighbors
while reducing unnecessary computational costs. Additionally, we introduce a
High-order redundant feature masking mechanism based on a Local Contribution
Score (LCS), which enables the model to retain only the most relevant neighbors
at each order, preventing redundant information propagation. Furthermore,
low-order enhanced feature aggregation adaptively integrates low-order and
high-order features based on task relevance, ensuring effective capture of both
local and global structural information without excessive complexity. Extensive
experiments on real-world datasets demonstrate that our approach consistently
outperforms state-of-the-art GNN models in both accuracy and computational
efficiency.

</details>


### [281] [Boosting KNNClassifier Performance with Opposition-Based Data Transformation](https://arxiv.org/abs/2504.16268)
*Abdesslem Layeb*

Main category: cs.LG

TL;DR: A novel data transformation framework using Opposition-Based Learning (OBL) improves classification performance by generating synthetic opposite samples.


<details>
  <summary>Details</summary>
Motivation: To enhance traditional classification algorithms by leveraging OBL for better decision boundary formation and enriched training data.

Method: Three OBL variants (Global OBL, Class-Wise OBL, Localized Class-Wise OBL) are integrated with KNN and tested on 26 datasets.

Result: OBL-enhanced classifiers consistently outperform basic KNN, showing OBL's effectiveness.

Conclusion: OBL is a lightweight, powerful strategy for improving classification, especially in complex or sparse environments.

Abstract: In this paper, we introduce a novel data transformation framework based on
Opposition-Based Learning (OBL) to boost the performance of traditional
classification algorithms. Originally developed to accelerate convergence in
optimization tasks, OBL is leveraged here to generate synthetic opposite
samples that enrich the training data and improve decision boundary formation.
We explore three OBL variants Global OBL, Class-Wise OBL, and Localized
Class-Wise OBL and integrate them with K-Nearest Neighbors (KNN). Extensive
experiments conducted on 26 heterogeneous and high-dimensional datasets
demonstrate that OBL-enhanced classifiers consistently outperform the basic
KNN. These findings underscore the potential of OBL as a lightweight yet
powerful data transformation strategy for enhancing classification performance,
especially in complex or sparse learning environments.

</details>


### [282] [Improving Significant Wave Height Prediction Using Chronos Models](https://arxiv.org/abs/2504.16834)
*Yilin Zhai, Hongyuan Shi, Chao Zhan, Qing Wang, Zaijin You, Nan Wang*

Main category: cs.LG

TL;DR: Chronos, an LLM-powered temporal architecture, improves wave forecasting with faster training, superior accuracy, and zero-shot capability.


<details>
  <summary>Details</summary>
Motivation: Accurate wave prediction is vital for maritime safety, but existing methods struggle with efficiency and nonlinear dynamics.

Method: Uses advanced temporal pattern recognition on historical wave data from the Northwest Pacific basin.

Result: Achieves 14.3% faster training, 2.5x inference speed, and top performance in short- and extended-range forecasts.

Conclusion: Chronos sets a new standard for wave prediction with computational efficiency and transferability.

Abstract: Accurate wave height prediction is critical for maritime safety and coastal
resilience, yet conventional physics-based models and traditional machine
learning methods face challenges in computational efficiency and nonlinear
dynamics modeling. This study introduces Chronos, the first implementation of a
large language model (LLM)-powered temporal architecture (Chronos) optimized
for wave forecasting. Through advanced temporal pattern recognition applied to
historical wave data from three strategically chosen marine zones in the
Northwest Pacific basin, our framework achieves multimodal improvements: (1)
14.3% reduction in training time with 2.5x faster inference speed compared to
PatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units;
(2) superior short-term forecasting (1-24h) across comprehensive metrics; (3)
sustained predictive leadership in extended-range forecasts (1-120h); and (4)
demonstrated zero-shot capability maintaining median performance (rank 4/12)
against specialized operational models. This LLM-enhanced temporal modeling
paradigm establishes a new standard in wave prediction, offering both
computationally efficient solutions and a transferable framework for complex
geophysical systems modeling.

</details>


### [283] [Class-Conditional Distribution Balancing for Group Robust Classification](https://arxiv.org/abs/2504.17314)
*Miaoyun Zhao, Qiang Zhang, Chenrong Li*

Main category: cs.LG

TL;DR: The paper proposes a method to address spurious correlations in models by balancing class-conditional distributions without needing bias annotations or predictions.


<details>
  <summary>Details</summary>
Motivation: Spurious correlations hinder robust generalization, and existing solutions rely on costly bias annotations or impractical pretrained models.

Method: A sample reweighting strategy balances class-conditional distributions, reducing mutual information between spurious factors and labels.

Result: The method achieves state-of-the-art performance, comparable to supervised approaches.

Conclusion: The approach effectively dismantles spurious correlations, offering a practical solution for resource-limited domains.

Abstract: Spurious correlations that lead models to correct predictions for the wrong
reasons pose a critical challenge for robust real-world generalization.
Existing research attributes this issue to group imbalance and addresses it by
maximizing group-balanced or worst-group accuracy, which heavily relies on
expensive bias annotations. A compromise approach involves predicting bias
information using extensively pretrained foundation models, which requires
large-scale data and becomes impractical for resource-limited rare domains. To
address these challenges, we offer a novel perspective by reframing the
spurious correlations as imbalances or mismatches in class-conditional
distributions, and propose a simple yet effective robust learning method that
eliminates the need for both bias annotations and predictions. With the goal of
reducing the mutual information between spurious factors and label information,
our method leverages a sample reweighting strategy to achieve class-conditional
distribution balancing, which automatically highlights minority groups and
classes, effectively dismantling spurious correlations and producing a debiased
data distribution for classification. Extensive experiments and analysis
demonstrate that our approach consistently delivers state-of-the-art
performance, rivaling methods that rely on bias supervision.

</details>


### [284] [BackSlash: Rate Constrained Optimized Training of Large Language Models](https://arxiv.org/abs/2504.16968)
*Jun Wu, Jiangtao Wen, Yuxing Han*

Main category: cs.LG

TL;DR: BackSlash introduces rate-constrained training for LLMs, reducing memory usage by 60%-90% without accuracy loss, outperforming post-training compression.


<details>
  <summary>Details</summary>
Motivation: Compression during LLM training is underexplored compared to post-training methods. BackSlash addresses this gap.

Method: Uses rate-distortion optimization (RDO) for flexible accuracy-complexity trade-offs during training.

Result: Achieves significant memory reduction (60%-90%), robustness to pruning, and faster edge-device inference.

Conclusion: BackSlash is a versatile, effective training-time compression method for LLMs.

Abstract: The rapid advancement of large-language models (LLMs) has driven extensive
research into parameter compression after training has been completed, yet
compression during the training phase remains largely unexplored. In this work,
we introduce Rate-Constrained Training (BackSlash), a novel training-time
compression approach based on rate-distortion optimization (RDO). BackSlash
enables a flexible trade-off between model accuracy and complexity,
significantly reducing parameter redundancy while preserving performance.
Experiments in various architectures and tasks demonstrate that BackSlash can
reduce memory usage by 60% - 90% without accuracy loss and provides significant
compression gain compared to compression after training. Moreover, BackSlash
proves to be highly versatile: it enhances generalization with small Lagrange
multipliers, improves model robustness to pruning (maintaining accuracy even at
80% pruning rates), and enables network simplification for accelerated
inference on edge devices.

</details>


### [285] [NeuralGrok: Accelerate Grokking by Neural Gradient Transformation](https://arxiv.org/abs/2504.17243)
*Xinyu Zhou, Simin Fan, Martin Jaggi, Jie Fu*

Main category: cs.LG

TL;DR: NeuralGrok, a gradient-based method, accelerates transformer generalization in arithmetic tasks by dynamically modulating gradients via an auxiliary MLP module and bilevel optimization.


<details>
  <summary>Details</summary>
Motivation: To address the slow generalization in transformers (grokking phenomenon) and improve training stability.

Method: Uses an auxiliary MLP module to dynamically adjust gradients based on their contribution to generalization, guided by bilevel optimization. Introduces AGE metric to analyze model complexity.

Result: NeuralGrok speeds up generalization, stabilizes training, and reduces model complexity, outperforming traditional methods like weight decay.

Conclusion: NeuralGrok provides insights into grokking and offers a practical solution for faster, more stable generalization in transformers.

Abstract: Grokking is proposed and widely studied as an intricate phenomenon in which
generalization is achieved after a long-lasting period of overfitting. In this
work, we propose NeuralGrok, a novel gradient-based approach that learns an
optimal gradient transformation to accelerate the generalization of
transformers in arithmetic tasks. Specifically, NeuralGrok trains an auxiliary
module (e.g., an MLP block) in conjunction with the base model. This module
dynamically modulates the influence of individual gradient components based on
their contribution to generalization, guided by a bilevel optimization
algorithm. Our extensive experiments demonstrate that NeuralGrok significantly
accelerates generalization, particularly in challenging arithmetic tasks. We
also show that NeuralGrok promotes a more stable training paradigm, constantly
reducing the model's complexity, while traditional regularization methods, such
as weight decay, can introduce substantial instability and impede
generalization. We further investigate the intrinsic model complexity
leveraging a novel Absolute Gradient Entropy (AGE) metric, which explains that
NeuralGrok effectively facilitates generalization by reducing the model
complexity. We offer valuable insights on the grokking phenomenon of
Transformer models, which encourages a deeper understanding of the fundamental
principles governing generalization ability.

</details>


### [286] [PTCL: Pseudo-Label Temporal Curriculum Learning for Label-Limited Dynamic Graph](https://arxiv.org/abs/2504.17641)
*Shengtao Zhang, Haokai Zhang, Shiqi Lou, Zicheng Wang, Zinan Zeng, Yilin Wang, Minnan Luo*

Main category: cs.LG

TL;DR: PTCL is a method for dynamic node classification using only final timestamp labels, leveraging pseudo-labels and temporal curriculum learning.


<details>
  <summary>Details</summary>
Motivation: Dynamic node classification is challenging due to high annotation costs and label uncertainty, while final labels are easier to obtain.

Method: PTCL uses a temporal decoupling architecture and Temporal Curriculum Learning to prioritize pseudo-labels closer to the final timestamp.

Result: PTCL outperforms other methods in experiments and introduces a new dataset (CoOAG) and framework (FLiD).

Conclusion: PTCL and FLiD provide a robust solution for label-limited dynamic node classification, with open-source code available.

Abstract: Dynamic node classification is critical for modeling evolving systems like
financial transactions and academic collaborations. In such systems,
dynamically capturing node information changes is critical for dynamic node
classification, which usually requires all labels at every timestamp. However,
it is difficult to collect all dynamic labels in real-world scenarios due to
high annotation costs and label uncertainty (e.g., ambiguous or delayed labels
in fraud detection). In contrast, final timestamp labels are easier to obtain
as they rely on complete temporal patterns and are usually maintained as a
unique label for each user in many open platforms, without tracking the history
data. To bridge this gap, we propose PTCL(Pseudo-label Temporal Curriculum
Learning), a pioneering method addressing label-limited dynamic node
classification where only final labels are available. PTCL introduces: (1) a
temporal decoupling architecture separating the backbone (learning time-aware
representations) and decoder (strictly aligned with final labels), which
generate pseudo-labels, and (2) a Temporal Curriculum Learning strategy that
prioritizes pseudo-labels closer to the final timestamp by assigning them
higher weights using an exponentially decaying function. We contribute a new
academic dataset (CoOAG), capturing long-range research interest in dynamic
graph. Experiments across real-world scenarios demonstrate PTCL's consistent
superiority over other methods adapted to this task. Beyond methodology, we
propose a unified framework FLiD (Framework for Label-Limited Dynamic Node
Classification), consisting of a complete preparation workflow, training
pipeline, and evaluation standards, and supporting various models and datasets.
The code can be found at https://github.com/3205914485/FLiD.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [287] [Collaborating Action by Action: A Multi-agent LLM Framework for Embodied Reasoning](https://arxiv.org/abs/2504.17950)
*Isadora White, Kolby Nottingham, Ayush Maniar, Max Robinson, Hansen Lillemark, Mehul Maheshwari, Lianhui Qin, Prithviraj Ammanabrolu*

Main category: cs.MA

TL;DR: The paper studies LLM collaboration in embodied reasoning tasks using Minecraft, identifying communication as a key bottleneck.


<details>
  <summary>Details</summary>
Motivation: To explore how LLMs can adaptively collaborate in complex embodied tasks like those in Minecraft.

Method: Introduces MINDcraft (a platform for LLM agents in Minecraft) and MineCollab (a benchmark for collaborative reasoning).

Result: Agent performance drops by 15% when detailed communication is required, highlighting inefficiencies.

Conclusion: Current LLM agents are poorly optimized for multi-agent embodied collaboration, needing methods beyond in-context and imitation learning.

Abstract: Collaboration is ubiquitous and essential in day-to-day life -- from
exchanging ideas, to delegating tasks, to generating plans together. This work
studies how LLMs can adaptively collaborate to perform complex embodied
reasoning tasks. To this end we introduce MINDcraft, an easily extensible
platform built to enable LLM agents to control characters in the open-world
game of Minecraft; and MineCollab, a benchmark to test the different dimensions
of embodied and collaborative reasoning. An experimental study finds that the
primary bottleneck in collaborating effectively for current state-of-the-art
agents is efficient natural language communication, with agent performance
dropping as much as 15% when they are required to communicate detailed task
completion plans. We conclude that existing LLM agents are ill-optimized for
multi-agent collaboration, especially in embodied scenarios, and highlight the
need to employ methods beyond in-context and imitation learning. Our website
can be found here: https://mindcraft-minecollab.github.io/

</details>


### [288] [Can We Govern the Agent-to-Agent Economy?](https://arxiv.org/abs/2501.16606)
*Tomer Jordi Chaffer*

Main category: cs.MA

TL;DR: The paper discusses gaps in AI governance for future AI-agent economies, emphasizing the need for human oversight in decentralized systems.


<details>
  <summary>Details</summary>
Motivation: Current AI governance lacks foresight for AI-agent economies, especially in monetization and oversight.

Method: Philosophical exploration of industry concepts to guide future research on decentralized AI-agent economies.

Result: Identifies the challenge of ensuring human control in a scalable, evolving AI-agent economy.

Conclusion: Calls for proactive research to address governance gaps in decentralized AI-agent economies.

Abstract: Current approaches to AI governance often fall short in anticipating a future
where AI agents manage critical tasks, such as financial operations,
administrative functions, and beyond. While cryptocurrencies could serve as the
foundation for monetizing value exchange in a collaboration and delegation
dynamic among AI agents, a critical question remains: how can humans ensure
meaningful oversight and control as a future economy of AI agents scales and
evolves? In this philosophical exploration, we highlight emerging concepts in
the industry to inform research and development efforts in anticipation of a
future decentralized agentic economy.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [289] [Machine Learning-Based Prediction of Quality Shifts on Video Streaming Over 5G](https://arxiv.org/abs/2504.17938)
*Raza Ul Mustafa, Sesha Dassanayake*

Main category: cs.MM

TL;DR: The paper explores how channel metrics (RSRP, RSRQ, SNR) correlate with YouTube video quality shifts, proposing ML-based prediction to enhance QoE.


<details>
  <summary>Details</summary>
Motivation: To improve YouTube streaming QoE by understanding and predicting quality shifts using channel metrics instead of traditional QoS.

Method: Analyzed relationship between quality shifts and RSRP/RSRQ/SNR; used ML classifiers for prediction.

Result: Channel metrics positively correlate with shifts; ML achieved 77% accuracy in predicting resolution categories.

Conclusion: Proposed method can enhance OTT services, especially in 5G networks, by leveraging channel metrics for better QoE.

Abstract: The Quality of Experience (QoE) is the users satisfaction while streaming a
video session over an over-the-top (OTT) platform like YouTube. QoE of YouTube
reflects the smooth streaming session without any buffering and quality shift
events. One of the most important factors nowadays affecting QoE of YouTube is
frequent shifts from higher to lower resolutions and vice versa. These shifts
ensure a smooth streaming session; however, it might get a lower mean opinion
score. For instance, dropping from 1080p to 480p during a video can preserve
continuity but might reduce the viewers enjoyment. Over time, OTT platforms are
looking for alternative ways to boost user experience instead of relying on
traditional Quality of Service (QoS) metrics such as bandwidth, latency, and
throughput. As a result, we look into the relationship between quality shifting
in YouTube streaming sessions and the channel metrics RSRP, RSRQ, and SNR. Our
findings state that these channel metrics positively correlate with shifts.
Thus, in real-time, OTT can only rely on them to predict video streaming
sessions into lower- and higher-resolution categories, thus providing more
resources to improve user experience. Using traditional Machine Learning (ML)
classifiers, we achieved an accuracy of 77-percent, while using only RSRP,
RSRQ, and SNR. In the era of 5G and beyond, where ultra-reliable, low-latency
networks promise enhanced streaming capabilities, the proposed methodology can
be used to improve OTT services.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [290] [Assessing the Utility of Audio Foundation Models for Heart and Respiratory Sound Analysis](https://arxiv.org/abs/2504.18004)
*Daisuke Niizumi, Daiki Takeuchi, Masahiro Yasuda, Binh Thien Nguyen, Yasunori Ohishi, Noboru Harada*

Main category: eess.AS

TL;DR: The study evaluates off-the-shelf audio foundation models for respiratory and heart sound tasks, comparing them to SOTA fine-tuning. Results show mixed performance: struggles with noisy data but SOTA on clean data, with general-purpose models outperforming specialized ones.


<details>
  <summary>Details</summary>
Motivation: To address the limited benchmarking of foundation models for respiratory and heart sounds and assess their practical effectiveness compared to SOTA fine-tuning.

Method: Comparative evaluation of off-the-shelf audio foundation models across four respiratory and heart sound tasks, benchmarking against SOTA fine-tuning results.

Result: Models performed poorly on noisy tasks but achieved SOTA on clean tasks. General-purpose audio models outperformed specialized respiratory sound models.

Conclusion: The study highlights the potential of foundation models for respiratory and heart sounds, emphasizing broader applicability of general-purpose models and providing insights for future research.

Abstract: Pre-trained deep learning models, known as foundation models, have become
essential building blocks in machine learning domains such as natural language
processing and image domains. This trend has extended to respiratory and heart
sound models, which have demonstrated effectiveness as off-the-shelf feature
extractors. However, their evaluation benchmarking has been limited, resulting
in incompatibility with state-of-the-art (SOTA) performance, thus hindering
proof of their effectiveness. This study investigates the practical
effectiveness of off-the-shelf audio foundation models by comparing their
performance across four respiratory and heart sound tasks with SOTA fine-tuning
results. Experiments show that models struggled on two tasks with noisy data
but achieved SOTA performance on the other tasks with clean data. Moreover,
general-purpose audio models outperformed a respiratory sound model,
highlighting their broader applicability. With gained insights and the released
code, we contribute to future research on developing and leveraging foundation
models for respiratory and heart sounds.

</details>


### [291] [DOSE : Drum One-Shot Extraction from Music Mixture](https://arxiv.org/abs/2504.18157)
*Suntae Hwang, Seonghyeon Kang, Kyungsu Kim, Semin Ahn, Kyogu Lee*

Main category: eess.AS

TL;DR: The paper introduces Drum One-Shot Extraction (DOSE) for extracting drum one-shots from music mixtures, using a novel dataset (RMOD) and onset loss for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Drum one-shots are vital for music production, but extracting them from mixtures is challenging. The paper aims to simplify this process.

Method: Proposes DOSE, a neural audio codec model with onset loss, bypassing traditional source separation. Uses RMOD dataset for training.

Result: DOSE outperforms baseline methods in quality and accuracy, measured by FAD and MSS metrics.

Conclusion: DOSE provides an efficient, high-quality solution for drum one-shot extraction, with code and models publicly available.

Abstract: Drum one-shot samples are crucial for music production, particularly in sound
design and electronic music. This paper introduces Drum One-Shot Extraction, a
task in which the goal is to extract drum one-shots that are present in the
music mixture. To facilitate this, we propose the Random Mixture One-shot
Dataset (RMOD), comprising large-scale, randomly arranged music mixtures paired
with corresponding drum one-shot samples. Our proposed model, Drum One- Shot
Extractor (DOSE), leverages neural audio codec language models for end-to-end
extraction, bypassing traditional source separation steps. Additionally, we
introduce a novel onset loss, designed to encourage accurate prediction of the
initial transient of drum one-shots, which is essential for capturing timbral
characteristics. We compare this approach against a source separation-based
extraction method as a baseline. The results, evaluated using Frechet Audio
Distance (FAD) and Multi-Scale Spectral loss (MSS), demonstrate that DOSE,
enhanced with onset loss, outperforms the baseline, providing more accurate and
higher-quality drum one-shots from music mixtures. The code, model checkpoint,
and audio examples are available at https://github.com/HSUNEH/DOSE

</details>


### [292] [Kimi-Audio Technical Report](https://arxiv.org/abs/2504.18425)
*KimiTeam, Ding Ding, Zeqian Ju, Yichong Leng, Songxiang Liu, Tong Liu, Zeyu Shang, Kai Shen, Wei Song, Xu Tan, Heyi Tang, Zhengtao Wang, Chu Wei, Yifei Xin, Xinran Xu, Jianwei Yu, Yutao Zhang, Xinyu Zhou, Y. Charles, Jun Chen, Yanru Chen, Yulun Du, Weiran He, Zhenxing Hu, Guokun Lai, Qingcheng Li, Yangyang Liu, Weidong Sun, Jianzhou Wang, Yuzhi Wang, Yuefeng Wu, Yuxin Wu, Dongchao Yang, Hao Yang, Ying Yang, Zhilin Yang, Aoxiong Yin, Ruibin Yuan, Yutong Zhang, Zaida Zhou*

Main category: eess.AS

TL;DR: Kimi-Audio is an open-source audio foundation model for understanding, generating, and conversing with audio. It uses a novel LLM-based architecture, extensive data curation, and achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To create a versatile audio model capable of handling diverse audio tasks like understanding, generation, and conversation.

Method: Leverages a 12.5Hz audio tokenizer, LLM-based architecture, and chunk-wise streaming detokenizer. Pre-trains on 13M+ hours of audio data and fine-tunes for specific tasks.

Result: Achieves state-of-the-art performance on benchmarks like speech recognition, audio understanding, and conversation.

Conclusion: Kimi-Audio is a powerful, open-source solution for diverse audio tasks, with released code and tools for community use.

Abstract: We present Kimi-Audio, an open-source audio foundation model that excels in
audio understanding, generation, and conversation. We detail the practices in
building Kimi-Audio, including model architecture, data curation, training
recipe, inference deployment, and evaluation. Specifically, we leverage a
12.5Hz audio tokenizer, design a novel LLM-based architecture with continuous
features as input and discrete tokens as output, and develop a chunk-wise
streaming detokenizer based on flow matching. We curate a pre-training dataset
that consists of more than 13 million hours of audio data covering a wide range
of modalities including speech, sound, and music, and build a pipeline to
construct high-quality and diverse post-training data. Initialized from a
pre-trained LLM, Kimi-Audio is continual pre-trained on both audio and text
data with several carefully designed tasks, and then fine-tuned to support a
diverse of audio-related tasks. Extensive evaluation shows that Kimi-Audio
achieves state-of-the-art performance on a range of audio benchmarks including
speech recognition, audio understanding, audio question answering, and speech
conversation. We release the codes, model checkpoints, as well as the
evaluation toolkits in https://github.com/MoonshotAI/Kimi-Audio.

</details>


### [293] [Music Tempo Estimation on Solo Instrumental Performance](https://arxiv.org/abs/2504.18502)
*Zhanhong He, Roberto Togneri, Xiangyu Zhang*

Main category: eess.AS

TL;DR: The paper evaluates tempo estimation techniques for solo instrumental music, finding that TCN models trained specifically for solo instruments outperform pretrained models, with post-processing further enhancing performance.


<details>
  <summary>Details</summary>
Motivation: Current MIDI transcriptions lack tempo notations, hindering sheet music conversion. This paper aims to improve tempo estimation for solo instrumental music.

Method: Evaluates TCN and RNN models, including pretrained ones and TCN models trained specifically for solo instruments. Tests on drum, guitar, and piano datasets and combines models with post-processing.

Result: The new TCN model improves guitar tempo estimation by 38.6% (Acc1) and doubles accuracy for piano, though piano Acc1 remains low at 50.9%. Post-processing further enhances performance.

Conclusion: Specialized TCN models and post-processing significantly improve tempo estimation for solo instruments, addressing a key limitation in MIDI-to-sheet music conversion.

Abstract: Recently, automatic music transcription has made it possible to convert
musical audio into accurate MIDI. However, the resulting MIDI lacks music
notations such as tempo, which hinders its conversion into sheet music. In this
paper, we investigate state-of-the-art tempo estimation techniques and evaluate
their performance on solo instrumental music. These include temporal
convolutional network (TCN) and recurrent neural network (RNN) models that are
pretrained on massive of mixed vocals and instrumental music, as well as TCN
models trained specifically with solo instrumental performances. Through
evaluations on drum, guitar, and classical piano datasets, our TCN models with
the new training scheme achieved the best performance. Our newly trained TCN
model increases the Acc1 metric by 38.6% for guitar tempo estimation, compared
to the pretrained TCN model with an Acc1 of 61.1%. Although our trained TCN
model is twice as accurate as the pretrained TCN model in estimating classical
piano tempo, its Acc1 is only 50.9%. To improve the performance of deep
learning models, we investigate their combinations with various post-processing
methods. These post-processing techniques effectively enhance the performance
of deep learning models when they struggle to estimate the tempo of specific
instruments.

</details>


### [294] [Combining X-Vectors and Bayesian Batch Active Learning: Two-Stage Active Learning Pipeline for Speech Recognition](https://arxiv.org/abs/2406.02566)
*Ognjen Kundacina, Vladimir Vincan, Dragisa Miskovic*

Main category: eess.AS

TL;DR: A two-stage active learning pipeline for ASR combines unsupervised and supervised methods to reduce data needs while improving model performance.


<details>
  <summary>Details</summary>
Motivation: To optimize labeling effort and data utilization in ASR by strategically selecting diverse and informative samples.

Method: Unsupervised AL (x-vectors clustering) for initial dataset creation, followed by supervised AL (Bayesian method with Monte Carlo dropout) for batch selection.

Result: Outperforms competing methods on homogeneous, heterogeneous, and OOD test sets.

Conclusion: Strategic sample selection and Bayesian modeling significantly enhance ASR efficiency and performance.

Abstract: This paper introduces a novel two-stage active learning (AL) pipeline for
automatic speech recognition (ASR), combining unsupervised and supervised AL
methods. The first stage utilizes unsupervised AL by using x-vectors clustering
for diverse sample selection from unlabeled speech data, thus establishing a
robust initial dataset for the subsequent supervised AL. The second stage
incorporates a supervised AL strategy, with a batch AL method specifically
developed for ASR, aimed at selecting diverse and informative batches of
samples. Here, sample diversity is also achieved using x-vectors clustering,
while the most informative samples are identified using a Bayesian AL method
tailored for ASR with an adaptation of Monte Carlo dropout to approximate
Bayesian inference. This approach enables precise uncertainty estimation,
thereby enhancing ASR model training with significantly reduced data
requirements. Our method has shown superior performance compared to competing
methods on homogeneous, heterogeneous, and OOD test sets, demonstrating that
strategic sample selection and innovative Bayesian modeling can substantially
optimize both labeling effort and data utilization in deep learning-based ASR
applications.

</details>


### [295] [ASVspoof 5: Design, Collection and Validation of Resources for Spoofing, Deepfake, and Adversarial Attack Detection Using Crowdsourced Speech](https://arxiv.org/abs/2502.08857)
*Xin Wang, Héctor Delgado, Hemlata Tak, Jee-weon Jung, Hye-jin Shim, Massimiliano Todisco, Ivan Kukanov, Xuechen Liu, Md Sahidullah, Tomi Kinnunen, Nicholas Evans, Kong Aik Lee, Junichi Yamagishi, Myeonghun Jeong, Ge Zhu, Yongyi Zang, You Zhang, Soumi Maiti, Florian Lux, Nicolas Müller, Wangyou Zhang, Chengzhe Sun, Shuwei Hou, Siwei Lyu, Sébastien Le Maguer, Cheng Gong, Hanjie Guo, Liping Chen, Vishwanath Singh*

Main category: eess.AS

TL;DR: ASVspoof 5 introduces a crowdsourced database with diverse acoustic conditions, 2,000 speakers, and 32 attack algorithms, including adversarial attacks. It features seven protocols for training and evaluation, with baseline detectors validated. Resources are freely available.


<details>
  <summary>Details</summary>
Motivation: To advance the study of speech spoofing and deepfake attacks by providing a diverse, large-scale database and promoting detection solutions.

Method: Crowdsourced data collection from diverse conditions and speakers, 32 attack algorithms (including adversarial), and seven speaker-disjoint protocols for training and evaluation.

Result: A validated database with baseline detectors, now freely available to the community.

Conclusion: ASVspoof 5 offers a comprehensive resource for spoofing and deepfake research, supporting diverse attack scenarios and detection model development.

Abstract: ASVspoof 5 is the fifth edition in a series of challenges which promote the
study of speech spoofing and deepfake attacks as well as the design of
detection solutions. We introduce the ASVspoof 5 database which is generated in
a crowdsourced fashion from data collected in diverse acoustic conditions (cf.
studio-quality data for earlier ASVspoof databases) and from ~2,000 speakers
(cf. ~100 earlier). The database contains attacks generated with 32 different
algorithms, also crowdsourced, and optimised to varying degrees using new
surrogate detection models. Among them are attacks generated with a mix of
legacy and contemporary text-to-speech synthesis and voice conversion models,
in addition to adversarial attacks which are incorporated for the first time.
ASVspoof 5 protocols comprise seven speaker-disjoint partitions. They include
two distinct partitions for the training of different sets of attack models,
two more for the development and evaluation of surrogate detection models, and
then three additional partitions which comprise the ASVspoof 5 training,
development and evaluation sets. An auxiliary set of data collected from an
additional 30k speakers can also be used to train speaker encoders for the
implementation of attack algorithms. Also described herein is an experimental
validation of the new ASVspoof 5 database using a set of automatic speaker
verification and spoof/deepfake baseline detectors. With the exception of
protocols and tools for the generation of spoofed/deepfake speech, the
resources described in this paper, already used by participants of the ASVspoof
5 challenge in 2024, are now all freely available to the community.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [296] [A Deep Bayesian Convolutional Spiking Neural Network-based CAD system with Uncertainty Quantification for Medical Images Classification](https://arxiv.org/abs/2504.17819)
*Mohaddeseh Chegini, Ali Mahloojifar*

Main category: eess.IV

TL;DR: A deep Bayesian Convolutional Spiking Neural Network (SNN) with uncertainty-aware module is proposed to improve reliability in CAD systems for medical image classification.


<details>
  <summary>Details</summary>
Motivation: SNNs offer benefits like event-driven processing and low power consumption but face unreliability due to uncertainty in predictions.

Method: Uses Monte Carlo Dropout as a Bayesian approximation for uncertainty quantification in a deep Bayesian Convolutional SNN.

Result: The model is accurate and reliable, outperforming conventional deep learning in medical image classification tasks.

Conclusion: The proposed model is a viable alternative for reliable medical image classification in CAD systems.

Abstract: The Computer_Aided Diagnosis (CAD) systems facilitate accurate diagnosis of
diseases. The development of CADs by leveraging third generation neural
network, namely, Spiking Neural Network (SNN), is essential to utilize of the
benefits of SNNs, such as their event_driven processing, parallelism, low power
consumption, and the ability to process sparse temporal_spatial information.
However, Deep SNN as a deep learning model faces challenges with unreliability.
To deal with unreliability challenges due to inability to quantify the
uncertainty of the predictions, we proposed a deep Bayesian Convolutional
Spiking Neural Network based_CADs with uncertainty_aware module. In this study,
the Monte Carlo Dropout method as Bayesian approximation is used as an
uncertainty quantification method. This method was applied to several medical
image classification tasks. Our experimental results demonstrate that our
proposed model is accurate and reliable and will be a proper alternative to
conventional deep learning for medical image classification.

</details>


### [297] [Predicting Dairy Calf Body Weight from Depth Images Using Deep Learning (YOLOv8) and Threshold Segmentation with Cross-Validation and Longitudinal Analysis](https://arxiv.org/abs/2504.17943)
*Mingsi Liao, Gota Morota, Ye Bi, Rebecca R. Cockrum*

Main category: eess.IV

TL;DR: Deep learning-based segmentation (YOLOv8) outperforms threshold methods for calf body metrics. XGBoost and LMM provide accurate BW predictions, enhancing farm management.


<details>
  <summary>Details</summary>
Motivation: Monitoring calf BW is crucial for growth, health, and weaning readiness, but manual methods are limited by labor and facility constraints.

Method: Developed deep learning segmentation models (YOLOv8), compared with threshold-based methods, and evaluated BW prediction using LR, XGBoost, and LMM with single- and multiple-time-point cross-validation.

Result: YOLOv8 achieved superior segmentation (IoU = 0.98). XGBoost excelled in single-time-point BW prediction (R² = 0.91), while LMM was best for longitudinal prediction (R² = 0.99).

Conclusion: Deep learning enables accurate, automated BW prediction, improving farm efficiency and management.

Abstract: Monitoring calf body weight (BW) before weaning is essential for assessing
growth, feed efficiency, health, and weaning readiness. However, labor, time,
and facility constraints limit BW collection. Additionally, Holstein calf coat
patterns complicate image-based BW estimation, and few studies have explored
non-contact measurements taken at early time points for predicting later BW.
The objectives of this study were to (1) develop deep learning-based
segmentation models for extracting calf body metrics, (2) compare deep learning
segmentation with threshold-based methods, and (3) evaluate BW prediction using
single-time-point cross-validation with linear regression (LR) and extreme
gradient boosting (XGBoost) and multiple-time-point cross-validation with LR,
XGBoost, and a linear mixed model (LMM). Depth images from Holstein (n = 63)
and Jersey (n = 5) pre-weaning calves were collected, with 20 Holstein calves
being weighed manually. Results showed that You Only Look Once version 8
(YOLOv8) deep learning segmentation (intersection over union = 0.98)
outperformed threshold-based methods (0.89). In single-time-point
cross-validation, XGBoost achieved the best BW prediction (R^2 = 0.91, mean
absolute percentage error (MAPE) = 4.37%), while LMM provided the most accurate
longitudinal BW prediction (R^2 = 0.99, MAPE = 2.39%). These findings highlight
the potential of deep learning for automated BW prediction, enhancing farm
management.

</details>


### [298] [Spectral Bias Correction in PINNs for Myocardial Image Registration of Pathological Data](https://arxiv.org/abs/2504.17945)
*Bastien C. Baluyot, Marta Varela, Chen Qin*

Main category: eess.IV

TL;DR: The paper proposes integrating Fourier Feature mappings and modulation strategies into PINNs to address spectral bias, improving high-frequency deformation modeling in cardiac image registration.


<details>
  <summary>Details</summary>
Motivation: Spectral bias in neural networks hinders accurate modeling of high-frequency deformations in myocardial images, leading to biomechanically implausible results, especially in pathological cases.

Method: The authors enhance PINNs by incorporating Fourier Feature mappings and modulation strategies to better capture high-frequency deformations.

Result: Experiments on two datasets show improved registration accuracy and biomechanical plausibility in modeling cardiomyopathies.

Conclusion: The proposed method provides a scalable foundation for cardiac image registration, generalizing across patients and pathologies.

Abstract: Accurate myocardial image registration is essential for cardiac strain
analysis and disease diagnosis. However, spectral bias in neural networks
impedes modeling high-frequency deformations, producing inaccurate,
biomechanically implausible results, particularly in pathological data. This
paper addresses spectral bias in physics-informed neural networks (PINNs) by
integrating Fourier Feature mappings and introducing modulation strategies into
a PINN framework. Experiments on two distinct datasets demonstrate that the
proposed methods enhance the PINN's ability to capture complex, high-frequency
deformations in cardiomyopathies, achieving superior registration accuracy
while maintaining biomechanical plausibility - thus providing a foundation for
scalable cardiac image registration and generalization across multiple patients
and pathologies.

</details>


### [299] [Physics-Driven Neural Compensation For Electrical Impedance Tomography](https://arxiv.org/abs/2504.18067)
*Chuyu Wang, Huiting Deng, Dong Liu*

Main category: eess.IV

TL;DR: PhyNC, an unsupervised deep learning framework, integrates EIT physics to address ill-posedness and sensitivity variability, outperforming existing methods in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: EIT's challenges include ill-posed inverse problems and sensitivity variability, which traditional and deep learning methods inadequately address.

Method: PhyNC dynamically allocates neural capacity to low-sensitivity regions, incorporating EIT physics for unsupervised learning.

Result: PhyNC excels in detail preservation and artifact resistance, especially in low-sensitivity areas, on simulated and experimental data.

Conclusion: PhyNC enhances EIT robustness and offers adaptability to other imaging modalities with similar issues.

Abstract: Electrical Impedance Tomography (EIT) provides a non-invasive, portable
imaging modality with significant potential in medical and industrial
applications. Despite its advantages, EIT encounters two primary challenges:
the ill-posed nature of its inverse problem and the spatially variable,
location-dependent sensitivity distribution. Traditional model-based methods
mitigate ill-posedness through regularization but overlook sensitivity
variability, while supervised deep learning approaches require extensive
training data and lack generalization. Recent developments in neural fields
have introduced implicit regularization techniques for image reconstruction,
but these methods typically neglect the physical principles underlying EIT,
thus limiting their effectiveness. In this study, we propose PhyNC
(Physics-driven Neural Compensation), an unsupervised deep learning framework
that incorporates the physical principles of EIT. PhyNC addresses both the
ill-posed inverse problem and the sensitivity distribution by dynamically
allocating neural representational capacity to regions with lower sensitivity,
ensuring accurate and balanced conductivity reconstructions. Extensive
evaluations on both simulated and experimental data demonstrate that PhyNC
outperforms existing methods in terms of detail preservation and artifact
resistance, particularly in low-sensitivity regions. Our approach enhances the
robustness of EIT reconstructions and provides a flexible framework that can be
adapted to other imaging modalities with similar challenges.

</details>


### [300] [Adaptive Weight Modified Riesz Mean Filter For High-Density Salt and Pepper Noise Removal](https://arxiv.org/abs/2504.18251)
*Md Jahidul Islam*

Main category: eess.IV

TL;DR: AWMRmF outperforms existing filters in removing high-density salt and pepper noise, achieving better PSNR and SSIM metrics.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of effectively removing high-density salt and pepper noise (SPN) in images.

Method: Developed the AWMRmF filter, integrating a pixel weight function and adaptivity condition, and compared it against AFMF, AWMF, ACmF, ARmF, and IAWMF using 26 test images with 60%-95% noise levels.

Result: AWMRmF achieved superior PSNR and SSIM results compared to other filters.

Conclusion: AWMRmF is highly effective for high-density SPN removal, outperforming state-of-the-art filters.

Abstract: This paper introduces a novel filter, the Adaptive Weight Modified Riesz Mean
Filter (AWMRmF), designed for the effective removal of high-density salt and
pepper noise (SPN). AWMRmF integrates a pixel weight function and adaptivity
condition inspired by the Different Adaptive Modified Riesz Mean Filter
(DAMRmF). In my simulations, I evaluated the performance of AWMRmF against
established filters such as Adaptive Frequency Median Filter (AFMF), Adaptive
Weighted Mean Filter (AWMF), Adaptive Cesaro Mean Filter (ACmF), Adaptive Riesz
Mean Filter (ARmF), and Improved Adaptive Weighted Mean Filter (IAWMF). The
assessment was conducted on 26 typical test images, varying noise levels from
60% to 95%. The findings indicate that, in terms of both Peak Signal to Noise
Ratio (PSNR) and Structural Similarity (SSIM) metrics, AWMRmF outperformed
other state-of-the-art filters. Furthermore, AWMRmF demonstrated superior
performance in mean PSNR and SSIM results as well.

</details>


### [301] [Towards a deep learning approach for classifying treatment response in glioblastomas](https://arxiv.org/abs/2504.18268)
*Ana Matoso, Catarina Passarinho, Marta P. Loureiro, José Maria Moreira, Patrícia Figueiredo, Rita G. Nunes*

Main category: eess.IV

TL;DR: A deep learning pipeline was developed to classify glioblastoma treatment response using MRI scans and RANO criteria, achieving a median balanced accuracy of 50.96%.


<details>
  <summary>Details</summary>
Motivation: The manual assessment of glioblastoma treatment response using RANO criteria is complex and time-consuming, prompting the need for an automated DL solution.

Method: Five DL approaches were tested, including image subtraction, modality combinations, model architectures, pretraining tasks, and clinical data integration. The best-performing pipeline used Densenet264 with T1, T2, and FLAIR images.

Result: The best model achieved a median balanced accuracy of 50.96%. Saliency Maps effectively highlighted tumor regions, while Grad-CAM had limited success.

Conclusion: This study sets a benchmark for automated glioblastoma response assessment, highlighting the complexity and heterogeneity of factors involved.

Abstract: Glioblastomas are the most aggressive type of glioma, having a 5-year
survival rate of 6.9%. Treatment typically involves surgery, followed by
radiotherapy and chemotherapy, and frequent magnetic resonance imaging (MRI)
scans to monitor disease progression. To assess treatment response,
radiologists use the Response Assessment in Neuro-Oncology (RANO) criteria to
categorize the tumor into one of four labels based on imaging and clinical
features: complete response, partial response, stable disease, and progressive
disease. This assessment is very complex and time-consuming. Since deep
learning (DL) has been widely used to tackle classification problems, this work
aimed to implement the first DL pipeline for the classification of RANO
criteria based on two consecutive MRI acquisitions. The models were trained and
tested on the open dataset LUMIERE. Five approaches were tested: 1) subtraction
of input images, 2) different combinations of modalities, 3) different model
architectures, 4) different pretraining tasks, and 5) adding clinical data. The
pipeline that achieved the best performance used a Densenet264 considering only
T1-weighted, T2-weighted, and Fluid Attenuated Inversion Recovery (FLAIR)
images as input without any pretraining. A median Balanced Accuracy of 50.96%
was achieved. Additionally, explainability methods were applied. Using Saliency
Maps, the tumor region was often successfully highlighted. In contrast,
Grad-CAM typically failed to highlight the tumor region, with some exceptions
observed in the Complete Response and Progressive Disease classes, where it
effectively identified the tumor region. These results set a benchmark for
future studies on glioblastoma treatment response assessment based on the RANO
criteria while emphasizing the heterogeneity of factors that might play a role
when assessing the tumor's response to treatment.

</details>


### [302] [NUDF: Neural Unsigned Distance Fields for high resolution 3D medical image segmentation](https://arxiv.org/abs/2504.18344)
*Kristine Sørensen, Oscar Camara, Ole de Backer, Klaus Kofoed, Rasmus Paulsen*

Main category: eess.IV

TL;DR: The paper proposes a Neural Unsigned Distance Field (NUDF) for high-resolution medical image segmentation, avoiding memory issues and preserving details.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with memory and detail loss in high-resolution medical image segmentation, especially for complex anatomies like the LAA.

Method: NUDF is introduced to learn continuous distance fields directly from images, enabling high-resolution 3D mesh modeling.

Result: NUDF successfully captures LAA details in CT images, achieving accuracy comparable to voxel spacing.

Conclusion: NUDF offers a memory-efficient and detailed solution for complex medical image segmentation.

Abstract: Medical image segmentation is often considered as the task of labelling each
pixel or voxel as being inside or outside a given anatomy. Processing the
images at their original size and resolution often result in insuperable memory
requirements, but downsampling the images leads to a loss of important details.
Instead of aiming to represent a smooth and continuous surface in a binary
voxel-grid, we propose to learn a Neural Unsigned Distance Field (NUDF)
directly from the image. The small memory requirements of NUDF allow for high
resolution processing, while the continuous nature of the distance field allows
us to create high resolution 3D mesh models of shapes of any topology (i.e.
open surfaces). We evaluate our method on the task of left atrial appendage
(LAA) segmentation from Computed Tomography (CT) images. The LAA is a complex
and highly variable shape, being thus difficult to represent with traditional
segmentation methods using discrete labelmaps. With our proposed method, we are
able to predict 3D mesh models that capture the details of the LAA and achieve
accuracy in the order of the voxel spacing in the CT images.

</details>


### [303] [Partition Map-Based Fast Block Partitioning for VVC Inter Coding](https://arxiv.org/abs/2504.18398)
*Xinmin Feng, Zhuoyuan Li, Li Li, Dong Liu, Feng Wu*

Main category: eess.IV

TL;DR: Proposes a partition map-based algorithm for fast block partitioning in VVC inter coding, reducing encoder complexity while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: The recursive partition search in VVC encoders increases complexity; this work aims to reduce it without significant RD performance loss.

Method: Develops a neural network using spatial/temporal features for partition map prediction, with MTT masks, stacked processing, and adaptive warping. Introduces a dual-threshold decision scheme.

Result: Achieves 51.30% encoding time saving with 2.12% BDBR increase under random access configuration.

Conclusion: The method effectively balances complexity reduction and RD performance, making it practical for VVC inter coding.

Abstract: Among the new techniques of Versatile Video Coding (VVC), the quadtree with
nested multi-type tree (QT+MTT) block structure yields significant coding gains
by providing more flexible block partitioning patterns. However, the recursive
partition search in the VVC encoder increases the encoder complexity
substantially. To address this issue, we propose a partition map-based
algorithm to pursue fast block partitioning in inter coding. Based on our
previous work on partition map-based methods for intra coding, we analyze the
characteristics of VVC inter coding, and thus improve the partition map by
incorporating an MTT mask for early termination. Next, we develop a neural
network that uses both spatial and temporal features to predict the partition
map. It consists of several special designs including stacked top-down and
bottom-up processing, quantization parameter modulation layers, and
partitioning-adaptive warping. Furthermore, we present a dual-threshold
decision scheme to achieve a fine-grained trade-off between complexity
reduction and rate-distortion (RD) performance loss. The experimental results
demonstrate that the proposed method achieves an average 51.30% encoding time
saving with a 2.12% Bjontegaard Delta Bit Rate (BDBR) under the random access
configuration.

</details>


### [304] [A Multimodal Deep Learning Approach for White Matter Shape Prediction in Diffusion MRI Tractography](https://arxiv.org/abs/2504.18400)
*Yui Lo, Yuqian Chen, Dongnan Liu, Leo Zekelman, Jarrett Rushmore, Yogesh Rathi, Nikos Makris, Alexandra J. Golby, Fan Zhang, Weidong Cai, Lauren J. O'Donnell*

Main category: eess.IV

TL;DR: Tract2Shape, a multimodal deep learning framework, efficiently predicts white matter tractography shape measures using geometric and scalar features, outperforming SOTA models and demonstrating strong generalizability.


<details>
  <summary>Details</summary>
Motivation: Conventional methods for computing white matter shape measures are computationally expensive and time-consuming, limiting large-scale analysis.

Method: Tract2Shape uses geometric (point cloud) and scalar (tabular) features, along with dimensionality reduction (PCA), to predict shape measures. It is trained and tested on HCP-YA and PPMI datasets.

Result: Tract2Shape achieves the highest Pearson's r and lowest nMSE on HCP-YA and maintains performance on PPMI, showing strong generalizability.

Conclusion: Tract2Shape enables fast, accurate, and scalable prediction of white matter shape measures, supporting future large-scale analysis.

Abstract: Shape measures have emerged as promising descriptors of white matter
tractography, offering complementary insights into anatomical variability and
associations with cognitive and clinical phenotypes. However, conventional
methods for computing shape measures are computationally expensive and
time-consuming for large-scale datasets due to reliance on voxel-based
representations. We propose Tract2Shape, a novel multimodal deep learning
framework that leverages geometric (point cloud) and scalar (tabular) features
to predict ten white matter tractography shape measures. To enhance model
efficiency, we utilize a dimensionality reduction algorithm for the model to
predict five primary shape components. The model is trained and evaluated on
two independently acquired datasets, the HCP-YA dataset, and the PPMI dataset.
We evaluate the performance of Tract2Shape by training and testing it on the
HCP-YA dataset and comparing the results with state-of-the-art models. To
further assess its robustness and generalization ability, we also test
Tract2Shape on the unseen PPMI dataset. Tract2Shape outperforms SOTA deep
learning models across all ten shape measures, achieving the highest average
Pearson's r and the lowest nMSE on the HCP-YA dataset. The ablation study shows
that both multimodal input and PCA contribute to performance gains. On the
unseen testing PPMI dataset, Tract2Shape maintains a high Pearson's r and low
nMSE, demonstrating strong generalizability in cross-dataset evaluation.
Tract2Shape enables fast, accurate, and generalizable prediction of white
matter shape measures from tractography data, supporting scalable analysis
across datasets. This framework lays a promising foundation for future
large-scale white matter shape analysis.

</details>


### [305] [HepatoGEN: Generating Hepatobiliary Phase MRI with Perceptual and Adversarial Models](https://arxiv.org/abs/2504.18405)
*Jens Hooge, Gerard Sanroma-Guell, Faidra Stavropoulou, Alexander Ullmann, Gesine Knobloch, Mark Klemens, Carola Schmidt, Sabine Weckbach, Andreas Bolz*

Main category: eess.IV

TL;DR: A deep learning approach synthesizes hepatobiliary phase (HBP) images from earlier contrast phases in DCE-MRI, comparing U-Net, pGAN, and DDPM models. pGAN performed best quantitatively but had issues; U-Net was more consistent.


<details>
  <summary>Details</summary>
Motivation: Reduce prolonged HBP scan times in DCE-MRI to improve patient comfort and scanner efficiency.

Method: Proposed deep learning models (U-Net, pGAN, DDPM) to synthesize HBP images from precontrast and transitional phases, using a multi-site dataset and contrast evolution score (CES).

Result: pGAN led quantitatively but introduced artifacts; U-Net was consistent; DDPM underperformed. Synthetic HBP images reduced scan time without losing diagnostic value.

Conclusion: Deep learning can feasibly generate synthetic HBP images, offering clinical benefits for liver MRI.

Abstract: Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) plays a
crucial role in the detection and characterization of focal liver lesions, with
the hepatobiliary phase (HBP) providing essential diagnostic information.
However, acquiring HBP images requires prolonged scan times, which may
compromise patient comfort and scanner throughput. In this study, we propose a
deep learning based approach for synthesizing HBP images from earlier contrast
phases (precontrast and transitional) and compare three generative models: a
perceptual U-Net, a perceptual GAN (pGAN), and a denoising diffusion
probabilistic model (DDPM). We curated a multi-site DCE-MRI dataset from
diverse clinical settings and introduced a contrast evolution score (CES) to
assess training data quality, enhancing model performance. Quantitative
evaluation using pixel-wise and perceptual metrics, combined with qualitative
assessment through blinded radiologist reviews, showed that pGAN achieved the
best quantitative performance but introduced heterogeneous contrast in
out-of-distribution cases. In contrast, the U-Net produced consistent liver
enhancement with fewer artifacts, while DDPM underperformed due to limited
preservation of fine structural details. These findings demonstrate the
feasibility of synthetic HBP image generation as a means to reduce scan time
without compromising diagnostic utility, highlighting the clinical potential of
deep learning for dynamic contrast enhancement in liver MRI. A project demo is
available at: https://jhooge.github.io/hepatogen

</details>


### [306] [Nearly isotropic segmentation for medial temporal lobe subregions in multi-modality MRI](https://arxiv.org/abs/2504.18442)
*Yue Li, Pulkit Khandelwal, Long Xie, Laura E. M. Wisse, Nidhi Mundada, Christopher A. Brown, Emily McGrew, Amanda Denning, Sandhitsu R. Das, David A. Wolk, Paul A. Yushkevich*

Main category: eess.IV

TL;DR: A pipeline for nearly isotropic segmentation in T2w MRI improves MTL subregion thickness measurements, enhancing biomarker accuracy for Alzheimer's.


<details>
  <summary>Details</summary>
Motivation: Address the reduced accuracy of subregion thickness measurements in T2w MRI due to lower out-of-plane resolution.

Method: Developed a pipeline with image/label upsampling and high-resolution segmentation using a multi-modality deep learning model trained on a nearly isotropic atlas.

Result: Improved accuracy of cortical thickness measurements in T2w MRI for neurodegeneration biomarkers.

Conclusion: The nearly isotropic segmentation pipeline enhances the reliability of MTL subregion morphometry as a biomarker for Alzheimer's and related conditions.

Abstract: Morphometry of medial temporal lobe (MTL) subregions in brain MRI is
sensitive biomarker to Alzheimers Disease and other related conditions. While
T2-weighted (T2w) MRI with high in-plane resolution is widely used to segment
hippocampal subfields due to its higher contrast in hippocampus, its lower
out-of-plane resolution reduces the accuracy of subregion thickness
measurements. To address this issue, we developed a nearly isotropic
segmentation pipeline that incorporates image and label upsampling and
high-resolution segmentation in T2w MRI. First, a high-resolution atlas was
created based on an existing anisotropic atlas derived from 29 individuals.
Both T1-weighted and T2w images in the atlas were upsampled from their original
resolution to a nearly isotropic resolution 0.4x0.4x0.52mm3 using a non-local
means approach. Manual segmentations within the atlas were also upsampled to
match this resolution using a UNet-based neural network, which was trained on a
cohort consisting of both high-resolution ex vivo and low-resolution
anisotropic in vivo MRI with manual segmentations. Second, a multi-modality
deep learning-based segmentation model was trained within this nearly isotropic
atlas. Finally, experiments showed the nearly isotropic subregion segmentation
improved the accuracy of cortical thickness as an imaging biomarker for
neurodegeneration in T2w MRI.

</details>


### [307] [RSFR: A Coarse-to-Fine Reconstruction Framework for Diffusion Tensor Cardiac MRI with Semantic-Aware Refinement](https://arxiv.org/abs/2504.18520)
*Jiahao Huang, Fanwen Wang, Pedro F. Ferreira, Haosen Zhang, Yinzhe Wu, Zhifan Gao, Lei Zhu, Angelica I. Aviles-Rivero, Carola-Bibiane Schonlieb, Andrew D. Scott, Zohya Khalique, Maria Dwornik, Ramyah Rajakulasingam, Ranil De Silva, Dudley J. Pennell, Guang Yang, Sonia Nielles-Vallespin*

Main category: eess.IV

TL;DR: RSFR framework improves cardiac DTI by addressing noise, artefacts, and fidelity issues using a coarse-to-fine strategy and semantic priors.


<details>
  <summary>Details</summary>
Motivation: Overcome technical challenges in cardiac DTI (low SNR, artefacts, fidelity) to enhance clinical utility.

Method: RSFR combines reconstruction, segmentation, fusion, and refinement with zero-shot semantic priors and Vision Mamba-based backbone.

Result: Achieves state-of-the-art reconstruction quality and accurate DT parameter estimation under high undersampling.

Conclusion: RSFR is robust, scalable, and promising for clinical cardiac DTI applications.

Abstract: Cardiac diffusion tensor imaging (DTI) offers unique insights into
cardiomyocyte arrangements, bridging the gap between microscopic and
macroscopic cardiac function. However, its clinical utility is limited by
technical challenges, including a low signal-to-noise ratio, aliasing
artefacts, and the need for accurate quantitative fidelity. To address these
limitations, we introduce RSFR (Reconstruction, Segmentation, Fusion &
Refinement), a novel framework for cardiac diffusion-weighted image
reconstruction. RSFR employs a coarse-to-fine strategy, leveraging zero-shot
semantic priors via the Segment Anything Model and a robust Vision Mamba-based
reconstruction backbone. Our framework integrates semantic features effectively
to mitigate artefacts and enhance fidelity, achieving state-of-the-art
reconstruction quality and accurate DT parameter estimation under high
undersampling rates. Extensive experiments and ablation studies demonstrate the
superior performance of RSFR compared to existing methods, highlighting its
robustness, scalability, and potential for clinical translation in quantitative
cardiac DTI.

</details>


### [308] [Scanning Electron Microscopy-based Automatic Defect Inspection for Semiconductor Manufacturing: A Systematic Review](https://arxiv.org/abs/2409.06833)
*Enrique Dehaerne, Bappaditya Dey, Victor Blanco, Jesse Davis*

Main category: eess.IV

TL;DR: A review of automatic defect inspection algorithms for SEM images in semiconductor manufacturing, highlighting trends, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: The shrinking of device patterns in semiconductor manufacturing increases defectivity, necessitating efficient SEM image analysis to reduce costs and improve yield.

Method: Systematic review of 103 papers, categorized by inspection tasks, evaluation metrics, and algorithm types (e.g., reference-based vs. DL-based).

Result: DL-based algorithms surpassed reference-based methods post-2020, especially for defect classification. Key challenges include minimizing time-to-solution and improving throughput.

Conclusion: Future work should focus on addressing gaps in high-throughput imaging, reducing manual setup, and improving processing efficiency.

Abstract: In this review, automatic defect inspection algorithms that analyze Scanning
Electron Microscopy (SEM) images for Semiconductor Manufacturing (SM) are
identified, categorized, and discussed. This is a topic of critical importance
for the SM industry as the continuous shrinking of device patterns has led to
increasing defectivity and a greater prevalence of higher-resolution imaging
tools such as SEM. Among others, these aspects threaten to increase costs due
to increased inspection time-to-solution and decreased yield. Relevant research
papers were systematically identified in four popular publication databases in
January 2024. A total of 103 papers were selected after screening for novel
contributions relating to automatic SEM image analysis algorithms for
semiconductor defect inspection. These papers were then categorized based on
the inspection tasks they addressed, their evaluation metrics, and the type of
algorithms used. A notable finding from this categorization is that
reference-based defect detection algorithms were the most popular algorithm
type until 2020 when Deep Learning (DL)-based inspection algorithms became more
popular, especially for defect classification. Furthermore, four broader
research questions were discussed to come to the following conclusions: (i) the
key components of inspection algorithms are set up, pre-processing, feature
extraction, and final prediction; (ii) the maturity of the manufacturing
process affects the data availability and required sensitivity of inspection
algorithms; (iii) key challenges for these algorithms relate to the desiderata
of minimizing time-to-solution which pushes for high imaging throughput,
reducing manual input during algorithm setup, and higher processing throughput;
and (iv) three promising directions for future work are suggested based on gaps
in the reviewed literature that address key remaining limitations.

</details>


### [309] [A Spatially-Aware Multiple Instance Learning Framework for Digital Pathology](https://arxiv.org/abs/2504.17379)
*Hassan Keshvarikhojasteh, Mihail Tifrea, Sibylle Hess, Josien P. W. Pluim, Mitko Veta*

Main category: eess.IV

TL;DR: GABMIL enhances ABMIL by integrating interaction-aware representations, improving performance in tumor subtyping without significant computational overhead.


<details>
  <summary>Details</summary>
Motivation: Conventional MIL methods like ABMIL ignore spatial interactions among patches, which are crucial for pathological diagnosis. TransMIL addresses this but increases computational complexity.

Method: GABMIL integrates interaction-aware representations into ABMIL to capture inter-instance dependencies while maintaining computational efficiency.

Result: GABMIL achieves up to 7% improvement in AUPRC and 5% increase in Kappa score over ABMIL on breast and lung cancer datasets.

Conclusion: Incorporating patch interactions in MIL frameworks is crucial for performance, and GABMIL demonstrates this effectively with minimal computational cost.

Abstract: Multiple instance learning (MIL) is a promising approach for weakly
supervised classification in pathology using whole slide images (WSIs).
However, conventional MIL methods such as Attention-Based Deep Multiple
Instance Learning (ABMIL) typically disregard spatial interactions among
patches that are crucial to pathological diagnosis. Recent advancements, such
as Transformer based MIL (TransMIL), have incorporated spatial context and
inter-patch relationships. However, it remains unclear whether explicitly
modeling patch relationships yields similar performance gains in ABMIL, which
relies solely on Multi-Layer Perceptrons (MLPs). In contrast, TransMIL employs
Transformer-based layers, introducing a fundamental architectural shift at the
cost of substantially increased computational complexity. In this work, we
enhance the ABMIL framework by integrating interaction-aware representations to
address this question. Our proposed model, Global ABMIL (GABMIL), explicitly
captures inter-instance dependencies while preserving computational efficiency.
Experimental results on two publicly available datasets for tumor subtyping in
breast and lung cancers demonstrate that GABMIL achieves up to a 7 percentage
point improvement in AUPRC and a 5 percentage point increase in the Kappa score
over ABMIL, with minimal or no additional computational overhead. These
findings underscore the importance of incorporating patch interactions within
MIL frameworks. Our code is available at
\href{https://github.com/tueimage/GABMIL}{\texttt{GABMIL}}.

</details>
