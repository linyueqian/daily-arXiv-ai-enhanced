<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 104]
- [cs.CV](#cs.CV) [Total: 213]
- [cs.AI](#cs.AI) [Total: 76]
- [cs.SD](#cs.SD) [Total: 10]
- [cs.LG](#cs.LG) [Total: 233]
- [cs.MA](#cs.MA) [Total: 6]
- [cs.MM](#cs.MM) [Total: 4]
- [eess.AS](#eess.AS) [Total: 6]
- [eess.IV](#eess.IV) [Total: 19]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [ScaleMCP: Dynamic and Auto-Synchronizing Model Context Protocol Tools for LLM Agents](https://arxiv.org/pdf/2505.06416)
*Elias Lumer, Anmol Gulati, Vamse Kumar Subbiah, Pradeep Honaganahalli Basavaraju, James A. Burke*

Main category: cs.CL

TL;DR: ScaleMCP introduces a dynamic tool selection approach for LLM agents, integrating MCP servers for tool retrieval and auto-synchronization, improving efficiency and autonomy.


<details>
  <summary>Details</summary>
Motivation: Existing tool selection frameworks lack integration with MCP servers, leading to inefficiencies and limited agent autonomy.

Method: ScaleMCP uses a MCP tool retriever, auto-synchronizing storage, and TDWA embedding strategy for dynamic tool selection.

Result: Evaluations show significant improvements in tool retrieval and agent performance across diverse models and datasets.

Conclusion: ScaleMCP effectively addresses scalability and dynamic tool selection challenges in LLM agent interactions.

Abstract: Recent advancements in Large Language Models (LLMs) and the introduction of
the Model Context Protocol (MCP) have significantly expanded LLM agents'
capability to interact dynamically with external tools and APIs. However,
existing tool selection frameworks do not integrate MCP servers, instead
relying heavily on error-prone manual updates to monolithic local tool
repositories, leading to duplication, inconsistencies, and inefficiencies.
Additionally, current approaches abstract tool selection before the LLM agent
is invoked, limiting its autonomy and hindering dynamic re-querying
capabilities during multi-turn interactions. To address these issues, we
introduce ScaleMCP, a novel tool selection approach that dynamically equips LLM
agents with a MCP tool retriever, giving agents the autonomy to add tools into
their memory, as well as an auto-synchronizing tool storage system pipeline
through CRUD (create, read, update, delete) operations with MCP servers as the
single source of truth. We also propose a novel embedding strategy, Tool
Document Weighted Average (TDWA), designed to selectively emphasize critical
components of tool documents (e.g. tool name or synthetic questions) during the
embedding process. Comprehensive evaluations conducted on a created dataset of
5,000 financial metric MCP servers, across 10 LLM models, 5 embedding models,
and 5 retriever types, demonstrate substantial improvements in tool retrieval
and agent invocation performance, emphasizing ScaleMCP's effectiveness in
scalable, dynamic tool selection and invocation.

</details>


### [2] [Is your multimodal large language model a good science tutor?](https://arxiv.org/pdf/2505.06418)
*Ming Liu, Liwen Wang, Wensheng Zhang*

Main category: cs.CL

TL;DR: The paper proposes a framework to evaluate multimodal large language models (MLLMs) as science tutors, focusing on teaching ability beyond just answer accuracy. It uses a rubric and simulated student model to assess tutoring performance, fine-tunes underperforming models, and shows that problem-solving skills don't guarantee teaching quality.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for MLLMs prioritize answer accuracy but overlook teaching ability, which is crucial for educational applications. The paper aims to bridge this gap by evaluating MLLMs as tutors.

Method: The framework uses an educational rubric and simulated student model to judge tutoring performance. It identifies strong and weak tutors, constructs pairwise comparisons, and applies preference optimization to fine-tune underperforming models.

Result: The study reveals that strong problem-solving skills don't ensure high-quality tutoring. Fine-tuning based on preference optimization improves educational alignment of tutor models.

Conclusion: The approach enables development of MLLMs that function as effective educational assistants, not just problem solvers, opening new possibilities for their use in education.

Abstract: Multimodal large language models (MLLMs) demonstrate impressive performance
on scientific reasoning tasks (e.g., ScienceQA). However, most existing
benchmarks focus narrowly on the accuracy of the final answer while ignoring
other metrics. In particular, when applying MLLMs to educational contexts, the
goal is not only correctness but also the ability to teach. In this paper, we
propose a framework that evaluates MLLMs as science tutors using a
comprehensive educational rubric and a simulated student model that judges the
teaching performance of the tutors. Given a list of candidate MLLM science
tutors, we use rubric-based student judgments to produce a range of tutor
performance scores, identifying both strong and weak tutors. Using the training
section of the ScienceQA dataset, we then construct a data set of pairwise
comparisons between the outputs of strong and weak tutors. This enables us to
apply multiple preference optimization methods to fine-tune an underperforming
tutor model (Qwen2-VL-2B) into more effective ones. Our results also show that
strong problem-solving skills do not guarantee high-quality tutoring and that
performance optimization-guided refinements can yield more educationally
aligned tutor models. This approach opens avenues for building MLLMs that serve
not only as problem solvers, but as genuinely helpful educational assistants.

</details>


### [3] [xGen-small Technical Report](https://arxiv.org/pdf/2505.06496)
*Erik Nijkamp, Bo Pang, Egor Pakhomov, Akash Gokul, Jin Qu, Silvio Savarese, Yingbo Zhou, Caiming Xiong*

Main category: cs.CL

TL;DR: xGen-small is a family of 4B and 9B Transformer models optimized for long-context tasks, achieving strong performance in math, coding, and long-context benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient long-context models, xGen-small is designed with a focus on domain-balanced data and extended context handling.

Method: The pipeline includes data curation, multi-stage pre-training (quality annealing, length extension to 128k tokens), and post-training (fine-tuning, preference learning, reinforcement learning).

Result: xGen-small excels in math, coding, and long-context tasks, demonstrating robust performance.

Conclusion: xGen-small is a highly effective model for long-context applications, with strong results in specialized domains.

Abstract: We introduce xGen-small, a family of 4B and 9B Transformer decoder models
optimized for long-context applications. Our vertically integrated pipeline
unites domain-balanced, frequency-aware data curation; multi-stage pre-training
with quality annealing and length extension to 128k tokens; and targeted
post-training via supervised fine-tuning, preference learning, and online
reinforcement learning. xGen-small delivers strong performance across various
tasks, especially in math and coding domains, while excelling at long context
benchmarks.

</details>


### [4] [Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model](https://arxiv.org/pdf/2505.06538)
*Xinyue Lou, You Li, Jinan Xu, Xiangyu Shi, Chi Chen, Kaiyu Huang*

Main category: cs.CL

TL;DR: The paper evaluates the safety of multimodal large reasoning models (MLRMs), revealing safety degradation in advanced models and proposing a method to enhance safety using intrinsic reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: To address the critical concerns of safety and reliability in MLRMs, which have broad application potential but lack systematic exploration.

Method: Conducts a comprehensive safety evaluation of 11 MLRMs across 5 benchmarks, identifies safety patterns, and proposes fine-tuning with a safety-oriented thought process dataset.

Result: Reveals prevalent safety degradation, distinct patterns across benchmarks, and improved safety after fine-tuning with the proposed dataset.

Conclusion: Leveraging intrinsic reasoning capabilities can enhance MLRM safety, offering a new perspective for developing safer models.

Abstract: The rapid development of multimodal large reasoning models (MLRMs) has
demonstrated broad application potential, yet their safety and reliability
remain critical concerns that require systematic exploration. To address this
gap, we conduct a comprehensive and systematic safety evaluation of 11 MLRMs
across 5 benchmarks and unveil prevalent safety degradation phenomena in most
advanced models. Moreover, our analysis reveals distinct safety patterns across
different benchmarks: significant safety degradation is observed across
jailbreak robustness benchmarks, whereas safety-awareness benchmarks
demonstrate less pronounced degradation. In particular, a long thought process
in some scenarios even enhances safety performance. Therefore, it is a
potential approach to addressing safety issues in MLRMs by leveraging the
intrinsic reasoning capabilities of the model to detect unsafe intent. To
operationalize this insight, we construct a multimodal tuning dataset that
incorporates a safety-oriented thought process. Experimental results from
fine-tuning existing MLRMs with this dataset effectively enhances the safety on
both jailbreak robustness and safety-awareness benchmarks. This study provides
a new perspective for developing safe MLRMs. Our dataset is available at
https://github.com/xinyuelou/Think-in-Safety.

</details>


### [5] [REFINE-AF: A Task-Agnostic Framework to Align Language Models via Self-Generated Instructions using Reinforcement Learning from Automated Feedback](https://arxiv.org/pdf/2505.06548)
*Aniruddha Roy, Pretam Ray, Abhilash Nandy, Somak Aditya, Pawan Goyal*

Main category: cs.CL

TL;DR: The paper explores using small open-source LLMs (LLaMA 2-7B, LLaMA 2-13B, Mistral 7B) with a semi-automated framework to generate instruction datasets, reducing human effort and cost. Adding RL-based training further improves performance in 63-66% of tasks.


<details>
  <summary>Details</summary>
Motivation: Human-annotated instruction data is costly and limited. Previous methods rely on expensive API-only models like GPT-3.5, prompting the need for cheaper, open-source alternatives.

Method: Uses small open-source LLMs with a semi-automated framework and integrates RL-based training to enhance performance.

Result: RL-based frameworks improve performance in 63-66% of tasks compared to prior methods.

Conclusion: Small open-source LLMs with RL can effectively generate instruction datasets, reducing costs and human effort while improving task performance.

Abstract: Instruction-based Large Language Models (LLMs) have proven effective in
numerous few-shot or zero-shot Natural Language Processing (NLP) tasks.
However, creating human-annotated instruction data is time-consuming,
expensive, and often limited in quantity and task diversity. Previous research
endeavors have attempted to address this challenge by proposing frameworks
capable of generating instructions in a semi-automated and task-agnostic manner
directly from the model itself. Many of these efforts have relied on large
API-only parameter-based models such as GPT-3.5 (175B), which are expensive,
and subject to limits on a number of queries. This paper explores the
performance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B,
and Mistral 7B, using a semi-automated framework, thereby reducing human
intervention, effort, and cost required to generate an instruction dataset for
fine-tuning LLMs. Furthermore, we demonstrate that incorporating a
Reinforcement Learning (RL) based training algorithm into this LLMs-based
framework leads to further enhancements. Our evaluation of the dataset reveals
that these RL-based frameworks achieve a substantial improvements in 63-66% of
the tasks compared to previous approaches.

</details>


### [6] [References Indeed Matter? Reference-Free Preference Optimization for Conversational Query Reformulation](https://arxiv.org/pdf/2505.06552)
*Doyoung Kim, Youngjun Lee, Joeun Kim, Jihwan Bang, Hwanjun Song, Susik Yoon, Jae-Gil Lee*

Main category: cs.CL

TL;DR: DualReform introduces a reference-free framework for conversational query reformulation (CQR) using pseudo reference passages, achieving near-optimal retrieval accuracy without real references.


<details>
  <summary>Details</summary>
Motivation: Existing CQR methods rely on impractical reference passages, limiting real-world applicability.

Method: DualReform uses response-based inference to generate pseudo references and refines responses via dual-role CQR.

Result: Achieves 96.9--99.1% of retrieval accuracy without real references, outperforming state-of-the-art by up to 31.6%.

Conclusion: DualReform provides a practical and effective solution for CQR without needing reference passages.

Abstract: Conversational query reformulation (CQR) has become indispensable for
improving retrieval in dialogue-based applications. However, existing
approaches typically rely on reference passages for optimization, which are
impractical to acquire in real-world scenarios. To address this limitation, we
introduce a novel reference-free preference optimization framework DualReform
that generates pseudo reference passages from commonly-encountered
conversational datasets containing only queries and responses. DualReform
attains this goal through two key innovations: (1) response-based inference,
where responses serve as proxies to infer pseudo reference passages, and (2)
response refinement via the dual-role of CQR, where a CQR model refines
responses based on the shared objectives between response refinement and CQR.
Despite not relying on reference passages, DualReform achieves 96.9--99.1% of
the retrieval accuracy attainable only with reference passages and surpasses
the state-of-the-art method by up to 31.6%.

</details>


### [7] [TS-SUPERB: A Target Speech Processing Benchmark for Speech Self-Supervised Learning Models](https://arxiv.org/pdf/2505.06660)
*Junyi Peng, Takanori Ashihara, Marc Delcroix, Tsubasa Ochiai, Oldrich Plchot, Shoko Araki, Jan Černocký*

Main category: cs.CL

TL;DR: TS-SUPERB is a new benchmark for evaluating SSL models in noisy, multi-talker scenarios, focusing on target-speaker tasks, revealing performance gaps not seen in single-speaker tasks.


<details>
  <summary>Details</summary>
Motivation: Previous SSL benchmarks overlooked noisy, multi-talker conditions, which are practical but challenging. TS-SUPERB addresses this gap.

Method: The benchmark includes four target-speaker tasks, using speaker embeddings to condition models. A unified SSL-based encoder (speaker encoder + extractor) is proposed for joint optimization.

Result: Performance in target-speaker tasks cannot be inferred from single-speaker tasks. Joint optimization across tasks improves effectiveness.

Conclusion: TS-SUPERB highlights the need for evaluating SSL models in realistic multi-talker scenarios and shows the benefits of joint task optimization.

Abstract: Self-supervised learning (SSL) models have significantly advanced speech
processing tasks, and several benchmarks have been proposed to validate their
effectiveness. However, previous benchmarks have primarily focused on
single-speaker scenarios, with less exploration of target-speaker tasks in
noisy, multi-talker conditions -- a more challenging yet practical case. In
this paper, we introduce the Target-Speaker Speech Processing Universal
Performance Benchmark (TS-SUPERB), which includes four widely recognized
target-speaker processing tasks that require identifying the target speaker and
extracting information from the speech mixture. In our benchmark, the speaker
embedding extracted from enrollment speech is used as a clue to condition
downstream models. The benchmark result reveals the importance of evaluating
SSL models in target speaker scenarios, demonstrating that performance cannot
be easily inferred from related single-speaker tasks. Moreover, by using a
unified SSL-based target speech encoder, consisting of a speaker encoder and an
extractor module, we also investigate joint optimization across TS tasks to
leverage mutual information and demonstrate its effectiveness.

</details>


### [8] [MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG](https://arxiv.org/pdf/2505.06569)
*Woosang Lim, Zekun Li, Gyuwan Kim, Sungyoung Ji, HyeonJung Kim, Kyuri Choi, Jin Hyuk Lim, Kyungpyo Park, William Yang Wang*

Main category: cs.CL

TL;DR: MacRAG introduces a hierarchical retrieval framework for RAG systems to improve precision and context coverage in long-context tasks, outperforming baselines on multi-hop reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing RAG systems face issues like imprecise retrieval, incomplete context coverage, and fragmented information due to suboptimal context construction.

Method: MacRAG compresses and partitions documents into coarse-to-fine granularities, adaptively merging relevant contexts through chunk- and document-level expansions in real time.

Result: MacRAG outperforms baseline RAG pipelines on LongBench tasks with models like Llama-3.1-8B, Gemini-1.5-pro, and GPT-4o.

Conclusion: MacRAG is an efficient, scalable solution for real-world long-context, multi-hop reasoning.

Abstract: Long-context (LC) Large Language Models (LLMs) combined with
Retrieval-Augmented Generation (RAG) hold strong potential for complex
multi-hop and large-document tasks. However, existing RAG systems often suffer
from imprecise retrieval, incomplete context coverage under constrained context
windows, and fragmented information caused by suboptimal context construction.
We introduce Multi-scale Adaptive Context RAG (MacRAG), a hierarchical
retrieval framework that compresses and partitions documents into
coarse-to-fine granularities, then adaptively merges relevant contexts through
chunk- and document-level expansions in real time. By starting from the
finest-level retrieval and progressively incorporating higher-level and broader
context, MacRAG constructs effective query-specific long contexts, optimizing
both precision and coverage. Evaluations on the challenging LongBench
expansions of HotpotQA, 2WikiMultihopQA, and Musique confirm that MacRAG
consistently surpasses baseline RAG pipelines on single- and multi-step
generation with Llama-3.1-8B, Gemini-1.5-pro, and GPT-4o. Our results establish
MacRAG as an efficient, scalable solution for real-world long-context,
multi-hop reasoning. Our code is available at
https://github.com/Leezekun/MacRAG.

</details>


### [9] [Evaluating LLM-Generated Q&A Test: a Student-Centered Study](https://arxiv.org/pdf/2505.06591)
*Anna Wróblewska, Bartosz Grabek, Jakub Świstak, Daniel Dan*

Main category: cs.CL

TL;DR: An AI pipeline generates reliable Q&A tests using GPT-4o-mini, showing strong psychometric performance and high user satisfaction, comparable to human-authored tests.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the feasibility and quality of AI-generated assessments for educational purposes, specifically in NLP courses.

Method: Automatically generated Q&A tests using GPT-4o-mini, evaluated via IRT analysis and user ratings (students and experts).

Result: Tests showed strong discrimination, appropriate difficulty, and high quality ratings, with minor DIF issues.

Conclusion: LLM-generated assessments can match human-authored tests, offering a scalable solution for AI-assisted assessment development.

Abstract: This research prepares an automatic pipeline for generating reliable
question-answer (Q&A) tests using AI chatbots. We automatically generated a
GPT-4o-mini-based Q&A test for a Natural Language Processing course and
evaluated its psychometric and perceived-quality metrics with students and
experts. A mixed-format IRT analysis showed that the generated items exhibit
strong discrimination and appropriate difficulty, while student and expert star
ratings reflect high overall quality. A uniform DIF check identified two items
for review. These findings demonstrate that LLM-generated assessments can match
human-authored tests in psychometric performance and user satisfaction,
illustrating a scalable approach to AI-assisted assessment development.

</details>


### [10] [On the Cost and Benefits of Training Context with Utterance or Full Conversation Training: A Comparative Stud](https://arxiv.org/pdf/2505.07202)
*Hyouin Liu, Zhikuan Zhang*

Main category: cs.CL

TL;DR: Context-based utterance training outperforms full conversation training in conversational TTS, achieving higher quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the gap in publicly accessible high-quality conversational TTS systems by evaluating existing models and training techniques.

Method: Empirical comparison of context-based utterance-level training and full conversation training using 20 GPU-hours on an NVIDIA H100.

Result: Context-based training yields better MOS scores (4.3/5.0) and 37% faster training, while full conversation training suffers from speaker similarity issues.

Conclusion: Utterance-level training with contextual conditioning is recommended for efficient, high-quality conversational TTS development.

Abstract: Modern TTS systems designed for conversations achieve high-quality utterances
but often remain inaccessible publicly. Are existing open-source architectures
inadequate, or are current training techniques insufficient? This paper
investigates prominent models and their underlying behaviors regarding
conversational context. Using 20 GPU-hours on an NVIDIA H100, we empirically
examine two approaches: context-based utterance-level training versus full
conversation training. Results demonstrate that context-based utterance
training achieves superior MOS scores (4.3/5.0 vs 3.7/5.0) and reduces training
time by 37%, while full conversation approaches suffer from speaker similarity
hallucination issues. These findings provide practical guidelines for
conversational TTS development, favoring utterance-level training with
contextual conditioning for both resource efficiency and output quality.

</details>


### [11] [Integrating Video and Text: A Balanced Approach to Multimodal Summary Generation and Evaluation](https://arxiv.org/pdf/2505.06594)
*Galann Pennec, Zhengyuan Liu, Nicholas Asher, Philippe Muller, Nancy F. Chen*

Main category: cs.CL

TL;DR: A zero-shot video-to-text summarization method generates screenplay summaries by integrating video, dialogue, and character info, outperforming VLMs with 20% more visual relevance and 75% less input.


<details>
  <summary>Details</summary>
Motivation: VLMs struggle with balancing visual and textual info for complex multimodal inputs like TV episodes.

Method: Proposes a zero-shot approach to create screenplay summaries using video, audio, and transcripts, naming characters without prior training. Introduces MFactSum, a multimodal metric for evaluation.

Result: Outperforms state-of-the-art VLMs (e.g., Gemini 1.5) with 20% more relevant visual info and 75% less input on SummScreen3D.

Conclusion: The method effectively integrates multimodal content and introduces a robust evaluation metric, advancing video summarization.

Abstract: Vision-Language Models (VLMs) often struggle to balance visual and textual
information when summarizing complex multimodal inputs, such as entire TV show
episodes. In this paper, we propose a zero-shot video-to-text summarization
approach that builds its own screenplay representation of an episode,
effectively integrating key video moments, dialogue, and character information
into a unified document. Unlike previous approaches, we simultaneously generate
screenplays and name the characters in zero-shot, using only the audio, video,
and transcripts as input. Additionally, we highlight that existing
summarization metrics can fail to assess the multimodal content in summaries.
To address this, we introduce MFactSum, a multimodal metric that evaluates
summaries with respect to both vision and text modalities. Using MFactSum, we
evaluate our screenplay summaries on the SummScreen3D dataset, demonstrating
superiority against state-of-the-art VLMs such as Gemini 1.5 by generating
summaries containing 20% more relevant visual information while requiring 75%
less of the video as input.

</details>


### [12] [Spoken Language Understanding on Unseen Tasks With In-Context Learning](https://arxiv.org/pdf/2505.07731)
*Neeraj Agrawal, Sriram Ganapathy*

Main category: cs.CL

TL;DR: The paper proposes a novel fine-tuning method using randomized class labels to improve speech-text LLMs' performance on unseen SLU tasks without task-specific annotations.


<details>
  <summary>Details</summary>
Motivation: Task-specific training data is often unavailable for SLU tasks, and current speech-text LLMs perform poorly in zero/few-shot settings.

Method: Introduces task-agnostic fine-tuning with randomized class labels to enhance model adaptability.

Result: Significant performance improvement on unseen SLU tasks compared to standard approaches.

Conclusion: The method enables new tasks in speech-text LLMs without requiring task-specific annotations.

Abstract: Spoken language understanding (SLU) tasks involve diverse skills that probe
the information extraction, classification and/or generation capabilities of
models. In this setting, task-specific training data may not always be
available. While traditional task-specific SLU models are unable to cater to
such requirements, the speech-text large language models (LLMs) offer a
promising alternative with emergent abilities. However, out of-the-box, our
evaluations indicate that the zero/few-shot performance of prominent
open-source speech-text LLMs on SLU tasks are not up to the mark. In this
paper, we introduce a novel approach to robust task-agnostic fine-tuning using
randomized class labels. With this proposed fine-tuning, we illustrate that the
performance of the speech-text LLMs on an unseen task is significantly improved
over standard approaches. Critically, the proposed approach avoids the
requirement of task-specific data annotations for enabling new tasks in
speech-text LLMs.

</details>


### [13] [Bridging the Gap: An Intermediate Language for Enhanced and Cost-Effective Grapheme-to-Phoneme Conversion with Homographs with Multiple Pronunciations Disambiguation](https://arxiv.org/pdf/2505.06599)
*Abbas Bertina, Shahab Beirami, Hossein Biniazian, Elham Esmaeilnia, Soheil Shahi, Mahdi Pirnia*

Main category: cs.CL

TL;DR: The paper introduces an intermediate language and a multi-faceted approach for Persian G2P conversion, combining LLM prompting and sequence-to-sequence transliteration, achieving superior performance in handling Persian's phonological complexities.


<details>
  <summary>Details</summary>
Motivation: Persian G2P conversion is challenging due to homographs and Ezafe in formal and informal contexts, requiring a robust solution.

Method: Combines LLM prompting and sequence-to-sequence transliteration, using formal concept analysis for homograph disambiguation. Trained on LLM-generated and B-Plus datasets.

Result: Superior performance in Persian phoneme conversion, improving PER metrics and setting a new benchmark.

Conclusion: The approach advances low-resource language processing and is applicable to other languages with homographic phenomena like Chinese and Arabic.

Abstract: Grapheme-to-phoneme (G2P) conversion for Persian presents unique challenges
due to its complex phonological features, particularly homographs and Ezafe,
which exist in formal and informal language contexts. This paper introduces an
intermediate language specifically designed for Persian language processing
that addresses these challenges through a multi-faceted approach. Our
methodology combines two key components: Large Language Model (LLM) prompting
techniques and a specialized sequence-to-sequence machine transliteration
architecture. We developed and implemented a systematic approach for
constructing a comprehensive lexical database for homographs with multiple
pronunciations disambiguation often termed polyphones, utilizing formal concept
analysis for semantic differentiation. We train our model using two distinct
datasets: the LLM-generated dataset for formal and informal Persian and the
B-Plus podcasts for informal language variants. The experimental results
demonstrate superior performance compared to existing state-of-the-art
approaches, particularly in handling the complexities of Persian phoneme
conversion. Our model significantly improves Phoneme Error Rate (PER) metrics,
establishing a new benchmark for Persian G2P conversion accuracy. This work
contributes to the growing research in low-resource language processing and
provides a robust solution for Persian text-to-speech systems and demonstrating
its applicability beyond Persian. Specifically, the approach can extend to
languages with rich homographic phenomena such as Chinese and Arabic

</details>


### [14] [Using External knowledge to Enhanced PLM for Semantic Matching](https://arxiv.org/pdf/2505.06605)
*Min Li, Chun Yuan*

Main category: cs.CL

TL;DR: The paper explores enhancing semantic relevance detection in NLP by incorporating external knowledge into neural network models, showing improved performance over baselines.


<details>
  <summary>Details</summary>
Motivation: Despite advances in neural models for semantic relevance, reliance on annotated data alone may not suffice. The paper investigates integrating external knowledge to improve model performance.

Method: The authors enhance a pre-trained semantic relevance discrimination model by incorporating external knowledge, tested on 10 public datasets.

Result: Experimental results demonstrate consistent performance improvements over the baseline model.

Conclusion: Incorporating external knowledge effectively boosts semantic relevance detection, suggesting its value alongside annotated data.

Abstract: Modeling semantic relevance has always been a challenging and critical task
in natural language processing. In recent years, with the emergence of massive
amounts of annotated data, it has become feasible to train complex models, such
as neural network-based reasoning models. These models have shown excellent
performance in practical applications and have achieved the current
state-ofthe-art performance. However, even with such large-scale annotated
data, we still need to think: Can machines learn all the knowledge necessary to
perform semantic relevance detection tasks based on this data alone? If not,
how can neural network-based models incorporate external knowledge into
themselves, and how can relevance detection models be constructed to make full
use of external knowledge? In this paper, we use external knowledge to enhance
the pre-trained semantic relevance discrimination model. Experimental results
on 10 public datasets show that our method achieves consistent improvements in
performance compared to the baseline model.

</details>


### [15] [Boosting Neural Language Inference via Cascaded Interactive Reasoning](https://arxiv.org/pdf/2505.06607)
*Min Li, Chun Yuan*

Main category: cs.CL

TL;DR: CIRN, a novel architecture for NLI, leverages multi-layer feature extraction and interactive reasoning to outperform baselines by capturing deeper semantic relationships.


<details>
  <summary>Details</summary>
Motivation: Existing NLI methods rely on final-layer PLM outputs, potentially missing valuable intermediate-layer information for complex semantic interactions.

Method: CIRN uses hierarchical feature extraction across network layers, integrating cross-sentence information progressively for deeper reasoning.

Result: CIRN achieves consistent performance gains on standard NLI benchmarks, validating its effectiveness.

Conclusion: Multi-level interactive features enhance NLI performance, demonstrating CIRN's superior semantic comprehension.

Abstract: Natural Language Inference (NLI) focuses on ascertaining the logical
relationship (entailment, contradiction, or neutral) between a given premise
and hypothesis. This task presents significant challenges due to inherent
linguistic features such as diverse phrasing, semantic complexity, and
contextual nuances. While Pre-trained Language Models (PLMs) built upon the
Transformer architecture have yielded substantial advancements in NLI,
prevailing methods predominantly utilize representations from the terminal
layer. This reliance on final-layer outputs may overlook valuable information
encoded in intermediate layers, potentially limiting the capacity to model
intricate semantic interactions effectively. Addressing this gap, we introduce
the Cascaded Interactive Reasoning Network (CIRN), a novel architecture
designed for deeper semantic comprehension in NLI. CIRN implements a
hierarchical feature extraction strategy across multiple network depths,
operating within an interactive space where cross-sentence information is
continuously integrated. This mechanism aims to mimic a process of progressive
reasoning, transitioning from surface-level feature matching to uncovering more
profound logical and semantic connections between the premise and hypothesis.
By systematically mining latent semantic relationships at various
representational levels, CIRN facilitates a more thorough understanding of the
input pair. Comprehensive evaluations conducted on several standard NLI
benchmark datasets reveal consistent performance gains achieved by CIRN over
competitive baseline approaches, demonstrating the efficacy of leveraging
multi-level interactive features for complex relational reasoning.

</details>


### [16] [The Efficiency of Pre-training with Objective Masking in Pseudo Labeling for Semi-Supervised Text Classification](https://arxiv.org/pdf/2505.06624)
*Arezoo Hatefi, Xuan-Son Vu, Monowar Bhuyan, Frank Drewes*

Main category: cs.CL

TL;DR: The paper extends a semi-supervised text classification model by adding unsupervised pre-training and evaluates its performance against baselines on multilingual datasets.


<details>
  <summary>Details</summary>
Motivation: To improve text classification with limited labeled data by leveraging teacher-student architecture and unsupervised pre-training.

Method: Extends Hatefi et al.'s model with unsupervised pre-training (objective masking) and evaluates it on English and Swedish datasets.

Result: Performance of the extended model is compared to the original and baselines, showing effectiveness.

Conclusion: The extended model with pre-training enhances semi-supervised text classification, validated on multilingual data.

Abstract: We extend and study a semi-supervised model for text classification proposed
earlier by Hatefi et al. for classification tasks in which document classes are
described by a small number of gold-labeled examples, while the majority of
training examples is unlabeled. The model leverages the teacher-student
architecture of Meta Pseudo Labels in which a ''teacher'' generates labels for
originally unlabeled training data to train the ''student'' and updates its own
model iteratively based on the performance of the student on the gold-labeled
portion of the data. We extend the original model of Hatefi et al. by an
unsupervised pre-training phase based on objective masking, and conduct
in-depth performance evaluations of the original model, our extension, and
various independent baselines. Experiments are performed using three different
datasets in two different languages (English and Swedish).

</details>


### [17] [Dynamic Domain Information Modulation Algorithm for Multi-domain Sentiment Analysis](https://arxiv.org/pdf/2505.06630)
*Chunyi Yue, Ang Li*

Main category: cs.CL

TL;DR: A dynamic information modulation algorithm improves multi-domain sentiment classification by efficiently generating domain-specific information, addressing challenges like high computational demand and convergence issues.


<details>
  <summary>Details</summary>
Motivation: To overcome poor performance in single-domain sentiment classification due to limited labeled data by leveraging multi-domain data and optimizing domain-specific information.

Method: A two-stage approach: first, determining a shared hyperparameter for domain classification tasks; second, using a domain-aware modulation algorithm to adjust domain information via gradient and loss-based methods.

Result: Superior performance demonstrated on a 16-domain public sentiment analysis dataset.

Conclusion: The proposed method effectively addresses computational and convergence challenges in multi-domain sentiment classification.

Abstract: Multi-domain sentiment classification aims to mitigate poor performance
models due to the scarcity of labeled data in a single domain, by utilizing
data labeled from various domains. A series of models that jointly train domain
classifiers and sentiment classifiers have demonstrated their advantages,
because domain classification helps generate necessary information for
sentiment classification. Intuitively, the importance of sentiment
classification tasks is the same in all domains for multi-domain sentiment
classification; but domain classification tasks are different because the
impact of domain information on sentiment classification varies across
different fields; this can be controlled through adjustable weights or hyper
parameters. However, as the number of domains increases, existing
hyperparameter optimization algorithms may face the following challenges: (1)
tremendous demand for computing resources, (2) convergence problems, and (3)
high algorithm complexity. To efficiently generate the domain information
required for sentiment classification in each domain, we propose a dynamic
information modulation algorithm. Specifically, the model training process is
divided into two stages. In the first stage, a shared hyperparameter, which
would control the proportion of domain classification tasks across all fields,
is determined. In the second stage, we introduce a novel domain-aware
modulation algorithm to adjust the domain information contained in the input
text, which is then calculated based on a gradient-based and loss-based method.
In summary, experimental results on a public sentiment analysis dataset
containing 16 domains prove the superiority of the proposed method.

</details>


### [18] [Attention Is Not All You Need: The Importance of Feedforward Networks in Transformer Models](https://arxiv.org/pdf/2505.06633)
*Isaac Gerber*

Main category: cs.CL

TL;DR: The paper investigates the role of feedforward networks (FFNs) in decoder-only transformer models, showing that three-layer FFNs outperform standard two-layer ones with fewer blocks and parameters.


<details>
  <summary>Details</summary>
Motivation: To understand the importance of FFNs in transformer-based language models and explore more efficient configurations.

Method: Conducted experiments comparing standard two-layer FFNs with three-layer FFNs in transformer blocks during pre-training.

Result: Three-layer FFNs with fewer blocks achieve lower training loss, fewer parameters, and faster training than two-layer FFNs.

Conclusion: The FFN is crucial for model performance, and optimizing its architecture (e.g., three-layer FFNs) can improve efficiency without sacrificing quality.

Abstract: Decoder-only transformer networks have become incredibly popular for language
modeling tasks. State-of-the-art models can have over a hundred transformer
blocks, containing billions of trainable parameters, and are trained on
trillions of tokens of text. Each transformer block typically consists of a
multi-head attention (MHA) mechanism and a two-layer fully connected
feedforward network (FFN). In this paper, we examine the importance of the FFN
during the model pre-training process through a series of experiments,
confirming that the FFN is important to model performance. Furthermore, we show
that models using a transformer block configuration with three-layer FFNs with
fewer such blocks outperform the standard two-layer configuration delivering
lower training loss with fewer total parameters in less time.

</details>


### [19] [Enhancing BERTopic with Intermediate Layer Representations](https://arxiv.org/pdf/2505.06696)
*Dominik Koterwa, Maciej Świtała*

Main category: cs.CL

TL;DR: BERTopic's performance varies with embedding configurations; optimal settings outperform defaults, and stop words impact results.


<details>
  <summary>Details</summary>
Motivation: To evaluate how different embedding representations affect BERTopic's performance in topic modeling.

Method: Tested 18 embedding configurations on three datasets, measuring topic coherence and diversity.

Result: Found embedding settings that outperform BERTopic's defaults; stop words influence performance.

Conclusion: Optimal embedding configurations enhance BERTopic's effectiveness, and stop words play a role in model performance.

Abstract: BERTopic is a topic modeling algorithm that leverages transformer-based
embeddings to create dense clusters, enabling the estimation of topic
structures and the extraction of valuable insights from a corpus of documents.
This approach allows users to efficiently process large-scale text data and
gain meaningful insights into its structure. While BERTopic is a powerful tool,
embedding preparation can vary, including extracting representations from
intermediate model layers and applying transformations to these embeddings. In
this study, we evaluate 18 different embedding representations and present
findings based on experiments conducted on three diverse datasets. To assess
the algorithm's performance, we report topic coherence and topic diversity
metrics across all experiments. Our results demonstrate that, for each dataset,
it is possible to find an embedding configuration that performs better than the
default setting of BERTopic. Additionally, we investigate the influence of stop
words on different embedding configurations.

</details>


### [20] [From Rankings to Insights: Evaluation Should Shift Focus from Leaderboard to Feedback](https://arxiv.org/pdf/2505.06698)
*Zongqi Wang, Tianle Gu, Chen Gong, Xin Tian, Siqi Bao, Yujiu Yang*

Main category: cs.CL

TL;DR: The paper critiques current LLM evaluation benchmarks for focusing on replicating human rankings and introduces Feedbacker, a framework for fine-grained feedback to optimize and profile models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack analytical feedback for model optimization, focusing only on leaderboard rankings.

Method: Feedbacker includes a query taxonomy builder, automated query synthesis, visualization tools, and a novel PC2 evaluation method.

Result: Feedbacker provides comprehensive feedback on 17 LLMs, demonstrating its effectiveness.

Conclusion: Feedbacker shifts evaluation paradigms to offer actionable insights for model improvement and understanding.

Abstract: Automatic evaluation benchmarks such as MT-Bench, Arena-Hard, and Auto-Arena
are seeing growing adoption for the evaluation of Large Language Models (LLMs).
Existing research has primarily focused on approximating human-based model
rankings using limited data and LLM-as-a-Judge. However, the fundamental
premise of these studies, which attempts to replicate human rankings, is
flawed. Specifically, these benchmarks typically offer only overall scores,
limiting their utility to leaderboard rankings, rather than providing feedback
that can guide model optimization and support model profiling. Therefore, we
advocate for an evaluation paradigm shift from approximating human-based model
rankings to providing feedback with analytical value. To this end, we introduce
Feedbacker, an evaluation framework that provides comprehensive and
fine-grained results, thereby enabling thorough identification of a model's
specific strengths and weaknesses. Such feedback not only supports the targeted
optimization of the model but also enhances the understanding of its behavior.
Feedbacker comprises three key components: an extensible tree-based query
taxonomy builder, an automated query synthesis scheme, and a suite of
visualization and analysis tools. Furthermore, we propose a novel
LLM-as-a-Judge method: PC2 (Pre-Comparison-derived Criteria) pointwise
evaluation. This method derives evaluation criteria by pre-comparing the
differences between several auxiliary responses, achieving the accuracy of
pairwise evaluation while maintaining the time complexity of pointwise
evaluation. Finally, leveraging the evaluation results of 17 mainstream LLMs,
we demonstrate the usage of Feedbacker and highlight its effectiveness and
potential. Our homepage project is available at
https://liudan193.github.io/Feedbacker.

</details>


### [21] [Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free](https://arxiv.org/pdf/2505.06708)
*Zihan Qiu, Zekun Wang, Bo Zheng, Zeyu Huang, Kaiyue Wen, Songlin Yang, Rui Men, Le Yu, Fei Huang, Suozhi Huang, Dayiheng Liu, Jingren Zhou, Junyang Lin*

Main category: cs.CL

TL;DR: A study on gating mechanisms in softmax attention reveals that a simple head-specific sigmoid gate improves performance, stability, and scaling.


<details>
  <summary>Details</summary>
Motivation: To systematically investigate the effects of gating in softmax attention, which is underexplored in existing literature.

Method: Comprehensive experiments with 30 variants of 15B MoE models and 1.7B dense models on a 3.5T token dataset.

Result: Head-specific sigmoid gating consistently enhances performance, stability, and long-context extrapolation.

Conclusion: Sparse gating mitigates 'attention sink' and improves scaling, with released code and models for future research.

Abstract: Gating mechanisms have been widely utilized, from early models like LSTMs and
Highway Networks to recent state space models, linear attention, and also
softmax attention. Yet, existing literature rarely examines the specific
effects of gating. In this work, we conduct comprehensive experiments to
systematically investigate gating-augmented softmax attention variants.
Specifically, we perform a comprehensive comparison over 30 variants of 15B
Mixture-of-Experts (MoE) models and 1.7B dense models trained on a 3.5 trillion
token dataset. Our central finding is that a simple modification-applying a
head-specific sigmoid gate after the Scaled Dot-Product Attention
(SDPA)-consistently improves performance. This modification also enhances
training stability, tolerates larger learning rates, and improves scaling
properties. By comparing various gating positions and computational variants,
we attribute this effectiveness to two key factors: (1) introducing
non-linearity upon the low-rank mapping in the softmax attention, and (2)
applying query-dependent sparse gating scores to modulate the SDPA output.
Notably, we find this sparse gating mechanism mitigates 'attention sink' and
enhances long-context extrapolation performance, and we also release related
$\href{https://github.com/qiuzh20/gated_attention}{codes}$ and
$\href{https://huggingface.co/QwQZh/gated_attention}{models}$ to facilitate
future research.

</details>


### [22] [Utilizing LLMs to Investigate the Disputed Role of Evidence in Electronic Cigarette Health Policy Formation in Australia and the UK](https://arxiv.org/pdf/2505.06782)
*Damian Curran, Brian Chapman, Mike Conway*

Main category: cs.CL

TL;DR: The paper compares Australia's restrictive and the UK's permissive e-cigarette policies using a GPT-4-based classifier to analyze policy documents, finding Australia emphasizes harms while the UK highlights benefits.


<details>
  <summary>Details</summary>
Motivation: To understand how Australia and the UK, despite using the same evidence, developed contrasting e-cigarette policies.

Method: Developed a GPT-4-based sentence classifier to analyze 109 policy documents, classifying sentences as helpful or harmful to public health.

Result: The classifier achieved an F-score of 0.9. Australian documents had more harmful statements, while UK documents had more helpful ones.

Conclusion: The study shows how LLMs can reveal policy biases in evidence interpretation, offering a tool for analyzing health policy formation.

Abstract: Australia and the UK have developed contrasting approaches to the regulation
of electronic cigarettes, with - broadly speaking - Australia adopting a
relatively restrictive approach and the UK adopting a more permissive approach.
Notably, these divergent policies were developed from the same broad evidence
base. In this paper, to investigate differences in how the two jurisdictions
manage and present evidence, we developed and evaluated a Large Language
Model-based sentence classifier to perform automated analyses of electronic
cigarette-related policy documents drawn from official Australian and UK
legislative processes (109 documents in total). Specifically, we utilized GPT-4
to automatically classify sentences based on whether they contained claims that
e-cigarettes were broadly helpful or harmful for public health. Our LLM-based
classifier achieved an F-score of 0.9. Further, when applying the classifier to
our entire sentence-level corpus, we found that Australian legislative
documents show a much higher proportion of harmful statements, and a lower
proportion of helpful statements compared to the expected values, with the
opposite holding for the UK. In conclusion, this work utilized an LLM-based
approach to provide evidence to support the contention that - drawing on the
same evidence base - Australian ENDS-related policy documents emphasize the
harms associated with ENDS products and UK policy documents emphasize the
benefits. Further, our approach provides a starting point for using LLM-based
methods to investigate the complex relationship between evidence and health
policy formation.

</details>


### [23] [A Split-then-Join Approach to Abstractive Summarization for Very Long Documents in a Low Resource Setting](https://arxiv.org/pdf/2505.06862)
*Lhuqita Fazry*

Main category: cs.CL

TL;DR: The paper addresses the limitation of the BIGBIRD-PEGASUS model in summarizing very long documents by fine-tuning it on a domain-specific dataset and augmenting the data to fit the model's token limit.


<details>
  <summary>Details</summary>
Motivation: The BIGBIRD-PEGASUS model's token limit of 4,096 causes performance degradation for very long documents, and truncation is a common but suboptimal solution.

Method: Fine-tune the pretrained BIGBIRD-PEGASUS model on a domain-specific dataset, filter documents under 20,000 tokens, and augment the dataset by splitting document-summary pairs to fit the token limit.

Result: The approach aims to improve summarization performance for very long documents without truncation.

Conclusion: The proposed method offers a novel way to handle very long documents by leveraging data augmentation and fine-tuning, with source code available for reproducibility.

Abstract: $\texttt{BIGBIRD-PEGASUS}$ model achieves $\textit{state-of-the-art}$ on
abstractive text summarization for long documents. However it's capacity still
limited to maximum of $4,096$ tokens, thus caused performance degradation on
summarization for very long documents. Common method to deal with the issue is
to truncate the documents. In this reasearch, we'll use different approach.
We'll use the pretrained $\texttt{BIGBIRD-PEGASUS}$ model by fine tuned the
model on other domain dataset. First, we filter out all documents which length
less than $20,000$ tokens to focus on very long documents. To prevent domain
shifting problem and overfitting on transfer learning due to small dataset, we
augment the dataset by splitting document-summary training pair into parts, to
fit the document into $4,096$ tokens. Source code available on
$\href{https://github.com/lhfazry/SPIN-summ}{https://github.com/lhfazry/SPIN-summ}$.

</details>


### [24] [IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method](https://arxiv.org/pdf/2505.06889)
*Mihyeon Kim, Juhyoung Park, Youngbin Kim*

Main category: cs.CL

TL;DR: IM-BERT enhances BERT's robustness against adversarial attacks by modeling layers as ODE solutions, improving performance without extra parameters.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning PLMs like BERT on limited datasets leads to vulnerability to adversarial attacks and overfitting.

Method: Conceptualizes BERT layers as ODE solutions, analyzes numerical stability, and introduces a robust IM-connection.

Result: IM-BERT improves performance by 8.3% on AdvGLUE and 5.9% in low-resource scenarios.

Conclusion: IM-BERT effectively boosts robustness without additional parameters or adversarial training.

Abstract: Pre-trained Language Models (PLMs) have achieved remarkable performance on
diverse NLP tasks through pre-training and fine-tuning. However, fine-tuning
the model with a large number of parameters on limited downstream datasets
often leads to vulnerability to adversarial attacks, causing overfitting of the
model on standard datasets.
  To address these issues, we propose IM-BERT from the perspective of a dynamic
system by conceptualizing a layer of BERT as a solution of Ordinary
Differential Equations (ODEs). Under the situation of initial value
perturbation, we analyze the numerical stability of two main numerical ODE
solvers: the explicit and implicit Euler approaches.
  Based on these analyses, we introduce a numerically robust IM-connection
incorporating BERT's layers. This strategy enhances the robustness of PLMs
against adversarial attacks, even in low-resource scenarios, without
introducing additional parameters or adversarial training strategies.
  Experimental results on the adversarial GLUE (AdvGLUE) dataset validate the
robustness of IM-BERT under various conditions. Compared to the original BERT,
IM-BERT exhibits a performance improvement of approximately 8.3\%p on the
AdvGLUE dataset. Furthermore, in low-resource scenarios, IM-BERT outperforms
BERT by achieving 5.9\%p higher accuracy.

</details>


### [25] [EcoLANG: Efficient and Effective Agent Communication Language Induction for Social Simulation](https://arxiv.org/pdf/2505.06904)
*Xinyi Mou, Chen Qian, Wei Liu, Xuanjing Huang, Zhongyu Wei*

Main category: cs.CL

TL;DR: EcoLANG reduces token consumption by 20% in social simulations by evolving and utilizing an optimized communication language.


<details>
  <summary>Details</summary>
Motivation: Addressing high time and computation costs in large-scale social simulations without compromising accuracy.

Method: Two-stage approach: language evolution (filtering synonyms, optimizing rules) and language utilization (agents communicate with evolved language).

Result: 20% reduction in token consumption while maintaining simulation accuracy.

Conclusion: EcoLANG offers an efficient and effective solution for social simulations.

Abstract: Large language models (LLMs) have demonstrated an impressive ability to
role-play humans and replicate complex social dynamics. While large-scale
social simulations are gaining increasing attention, they still face
significant challenges, particularly regarding high time and computation costs.
Existing solutions, such as distributed mechanisms or hybrid agent-based model
(ABM) integrations, either fail to address inference costs or compromise
accuracy and generalizability. To this end, we propose EcoLANG: Efficient and
Effective Agent Communication Language Induction for Social Simulation. EcoLANG
operates in two stages: (1) language evolution, where we filter synonymous
words and optimize sentence-level rules through natural selection, and (2)
language utilization, where agents in social simulations communicate using the
evolved language. Experimental results demonstrate that EcoLANG reduces token
consumption by over 20%, enhancing efficiency without sacrificing simulation
accuracy.

</details>


### [26] [The Distracting Effect: Understanding Irrelevant Passages in RAG](https://arxiv.org/pdf/2505.06914)
*Chen Amiraz, Florin Cuconasu, Simone Filice, Zohar Karnin*

Main category: cs.CL

TL;DR: The paper addresses the issue of irrelevant passages in RAG systems distracting LLMs, introducing a quantifiable measure for distraction and methods to improve RAG accuracy by 7.5%.


<details>
  <summary>Details</summary>
Motivation: To mitigate the distracting effect of irrelevant passages in RAG systems and improve answer accuracy.

Method: Develops a measure for passage distraction, identifies hard distracting passages, and fine-tunes LLMs using these passages.

Result: Achieves a 7.5% increase in answering accuracy compared to conventional RAG fine-tuning.

Conclusion: Provides a comprehensive framework for identifying and utilizing hard distracting passages, advancing RAG system performance.

Abstract: A well-known issue with Retrieval Augmented Generation (RAG) is that
retrieved passages that are irrelevant to the query sometimes distract the
answer-generating LLM, causing it to provide an incorrect response. In this
paper, we shed light on this core issue and formulate the distracting effect of
a passage w.r.t. a query (and an LLM). We provide a quantifiable measure of the
distracting effect of a passage and demonstrate its robustness across LLMs.
  Our research introduces novel methods for identifying and using hard
distracting passages to improve RAG systems. By fine-tuning LLMs with these
carefully selected distracting passages, we achieve up to a 7.5% increase in
answering accuracy compared to counterparts fine-tuned on conventional RAG
datasets. Our contribution is two-fold: first, we move beyond the simple binary
classification of irrelevant passages as either completely unrelated vs.
distracting, and second, we develop and analyze multiple methods for finding
hard distracting passages. To our knowledge, no other research has provided
such a comprehensive framework for identifying and utilizing hard distracting
passages.

</details>


### [27] [CNN-based Image Models Verify a Hypothesis that The Writers of Cuneiform Texts Improved Their Writing Skills When Studying at the Age of Hittite Empire](https://arxiv.org/pdf/2505.06974)
*Daichi Kohmoto, Katsutoshi Fukuda, Daisuke Yoshida, Takafumi Matsui, Sachihiro Omura*

Main category: cs.CL

TL;DR: The paper analyzes a cuneiform tablet with two iterations of the same content, suggesting one writer was a teacher and the other a student, using CNN-based image analysis.


<details>
  <summary>Details</summary>
Motivation: To understand why ancient people preserved tablets with nearly identical content, unlike other tablets with myths or records.

Method: Quantitative analysis of tablet images using CNN-based models without segmenting cuneiforms individually.

Result: The first writer was likely a teacher, the second a student, a conclusion not reached by classical linguistics.

Conclusion: The method offers new insights into ancient writing practices and potential for broader applications.

Abstract: A cuneiform tablet KBo 23.1 ++/KUB 30.38, which is known to represent a text
of Kizzuwatna rituals, was written by two writers with almost identical content
in two iterations. Unlike other cuneiform tablets that contained information
such as myths, essays, or business records, the reason why ancient people left
such tablets for posterity remains unclear. To study this problem, we develop a
new methodology by analyzing images of a tablet quantitatively using CNN
(Convolutional Neural Network)-based image models, without segmenting
cuneiforms one-by-one. Our data-driven methodology implies that the writer
writing the first half was a `teacher' and the other writer was a `student' who
was training his skills of writing cuneiforms. This result has not been reached
by classical linguistics. We also discuss related conclusions and possible
further directions for applying our method and its generalizations.

</details>


### [28] [Convert Language Model into a Value-based Strategic Planner](https://arxiv.org/pdf/2505.06987)
*Xiaoyu Wang, Yue Zhao, Qingqing Gu, Zhonglin Jiang, Xiaokai Chen, Yong Chen, Luo Ji*

Main category: cs.CL

TL;DR: The paper introduces straQ*, a Q-learning-based framework for emotional support conversations (ESC) to improve long-term satisfaction by optimizing LLM strategies.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based ESC solutions lack a state model perspective, leading to suboptimal long-term satisfaction.

Method: Leverages Q-learning on LLMs to plan, determine optimal strategies, and guide responses in ESC.

Result: straQ* outperforms baselines like direct inference, self-refine, chain of thought, finetuning, and finite state machines.

Conclusion: straQ* effectively enhances ESC by optimizing long-term returns through Q-learning on LLMs.

Abstract: Emotional support conversation (ESC) aims to alleviate the emotional distress
of individuals through effective conversations. Although large language models
(LLMs) have obtained remarkable progress on ESC, most of these studies might
not define the diagram from the state model perspective, therefore providing a
suboptimal solution for long-term satisfaction. To address such an issue, we
leverage the Q-learning on LLMs, and propose a framework called straQ*. Our
framework allows a plug-and-play LLM to bootstrap the planning during ESC,
determine the optimal strategy based on long-term returns, and finally guide
the LLM to response. Substantial experiments on ESC datasets suggest that
straQ* outperforms many baselines, including direct inference, self-refine,
chain of thought, finetuning, and finite state machines.

</details>


### [29] [HAMLET: Healthcare-focused Adaptive Multilingual Learning Embedding-based Topic Modeling](https://arxiv.org/pdf/2505.07157)
*Hajar Sakai, Sarah S. Lam*

Main category: cs.CL

TL;DR: HAMLET is a graph-driven architecture for cross-lingual healthcare topic modeling that refines LLM-generated topics using BERT, SBERT, and GNNs, improving coherence and interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional topic models struggle with contextual nuances, polysemy, and rare words, while raw LLM-generated topics lack refinement. HAMLET addresses these issues.

Method: Uses neural-enhanced semantic fusion with BERT, SBERT, and GNNs to refine topic embeddings, establishing connections between documents, topics, and words.

Result: Effective refinement of topic embeddings and extraction of top k topics, demonstrated on English and French healthcare datasets.

Conclusion: HAMLET improves topic coherence and interpretability, outperforming traditional methods and raw LLM outputs.

Abstract: Traditional topic models often struggle with contextual nuances and fail to
adequately handle polysemy and rare words. This limitation typically results in
topics that lack coherence and quality. Large Language Models (LLMs) can
mitigate this issue by generating an initial set of topics. However, these raw
topics frequently lack refinement and representativeness, which leads to
redundancy without lexical similarity and reduced interpretability. This paper
introduces HAMLET, a graph-driven architecture for cross-lingual healthcare
topic modeling that uses LLMs. The proposed approach leverages neural-enhanced
semantic fusion to refine the embeddings of topics generated by the LLM.
Instead of relying solely on statistical co-occurrence or human interpretation
to extract topics from a document corpus, this method introduces a topic
embedding refinement that uses Bidirectional Encoder Representations from
Transformers (BERT) and Graph Neural Networks (GNN). After topic generation, a
hybrid technique that involves BERT and Sentence-BERT (SBERT) is employed for
embedding. The topic representations are further refined using a GNN, which
establishes connections between documents, topics, words, similar topics, and
similar words. A novel method is introduced to compute similarities.
Consequently, the topic embeddings are refined, and the top k topics are
extracted. Experiments were conducted using two healthcare datasets, one in
English and one in French, from which six sets were derived. The results
demonstrate the effectiveness of HAMLET.

</details>


### [30] [Towards Actionable Pedagogical Feedback: A Multi-Perspective Analysis of Mathematics Teaching and Tutoring Dialogue](https://arxiv.org/pdf/2505.07161)
*Jannatun Naim, Jie Cao, Fareen Tasneem, Jennifer Jacobs, Brent Milne, James Martin, Tamara Sumner*

Main category: cs.CL

TL;DR: The paper proposes a multi-perspective discourse analysis framework to address challenges in utterance-level feedback in mathematics education, integrating talk moves, dialogue acts, and discourse relations.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of single-tag utterance analysis (multifunctionality and exclusion of non-talk-move utterances) in providing effective feedback for instructional practices.

Method: A top-down framework combining domain-specific talk moves, dialogue acts (SWBD-MASL schema), and discourse relations (Segmented Discourse Representation Theory), applied to two datasets (TalkMoves and SAGA22).

Result: Revealed meaningful discourse patterns and the crucial role of non-talk-move utterances in guiding and structuring classroom discourse.

Conclusion: The framework enhances AI-assisted education feedback and supports the development of AI agents emulating educators and students.

Abstract: Effective feedback is essential for refining instructional practices in
mathematics education, and researchers often turn to advanced natural language
processing (NLP) models to analyze classroom dialogues from multiple
perspectives. However, utterance-level discourse analysis encounters two
primary challenges: (1) multifunctionality, where a single utterance may serve
multiple purposes that a single tag cannot capture, and (2) the exclusion of
many utterances from domain-specific discourse move classifications, leading to
their omission in feedback. To address these challenges, we proposed a
multi-perspective discourse analysis that integrates domain-specific talk moves
with dialogue act (using the flattened multi-functional SWBD-MASL schema with
43 tags) and discourse relation (applying Segmented Discourse Representation
Theory with 16 relations). Our top-down analysis framework enables a
comprehensive understanding of utterances that contain talk moves, as well as
utterances that do not contain talk moves. This is applied to two mathematics
education datasets: TalkMoves (teaching) and SAGA22 (tutoring). Through
distributional unigram analysis, sequential talk move analysis, and multi-view
deep dive, we discovered meaningful discourse patterns, and revealed the vital
role of utterances without talk moves, demonstrating that these utterances, far
from being mere fillers, serve crucial functions in guiding, acknowledging, and
structuring classroom discourse. These insights underscore the importance of
incorporating discourse relations and dialogue acts into AI-assisted education
systems to enhance feedback and create more responsive learning environments.
Our framework may prove helpful for providing human educator feedback, but also
aiding in the development of AI agents that can effectively emulate the roles
of both educators and students.

</details>


### [31] [KDH-MLTC: Knowledge Distillation for Healthcare Multi-Label Text Classification](https://arxiv.org/pdf/2505.07162)
*Hajar Sakai, Sarah S. Lam*

Main category: cs.CL

TL;DR: KDH-MLTC is a framework for healthcare multi-label text classification using knowledge distillation and LLMs, achieving high accuracy and efficiency while ensuring HIPAA compliance.


<details>
  <summary>Details</summary>
Motivation: The need for efficient and accurate classification of complex healthcare textual data, addressing computational and sensitivity challenges.

Method: Integrates knowledge distillation (BERT to DistilBERT) and sequential fine-tuning, optimized via PSO for hyperparameter tuning.

Result: Achieves 82.70% F1 score on the largest dataset, outperforming existing methods, with robustness confirmed via statistical validation.

Conclusion: KDH-MLTC balances efficiency and accuracy, making it suitable for resource-constrained healthcare settings.

Abstract: The increasing volume of healthcare textual data requires computationally
efficient, yet highly accurate classification approaches able to handle the
nuanced and complex nature of medical terminology. This research presents
Knowledge Distillation for Healthcare Multi-Label Text Classification
(KDH-MLTC), a framework leveraging model compression and Large Language Models
(LLMs). The proposed approach addresses conventional healthcare Multi-Label
Text Classification (MLTC) challenges by integrating knowledge distillation and
sequential fine-tuning, subsequently optimized through Particle Swarm
Optimization (PSO) for hyperparameter tuning. KDH-MLTC transfers knowledge from
a more complex teacher LLM (i.e., BERT) to a lighter student LLM (i.e.,
DistilBERT) through sequential training adapted to MLTC that preserves the
teacher's learned information while significantly reducing computational
requirements. As a result, the classification is enabled to be conducted
locally, making it suitable for healthcare textual data characterized by
sensitivity and, therefore, ensuring HIPAA compliance. The experiments
conducted on three medical literature datasets of different sizes, sampled from
the Hallmark of Cancer (HoC) dataset, demonstrate that KDH-MLTC achieves
superior performance compared to existing approaches, particularly for the
largest dataset, reaching an F1 score of 82.70%. Additionally, statistical
validation and an ablation study are carried out, proving the robustness of
KDH-MLTC. Furthermore, the PSO-based hyperparameter optimization process
allowed the identification of optimal configurations. The proposed approach
contributes to healthcare text classification research, balancing efficiency
requirements in resource-constrained healthcare settings with satisfactory
accuracy demands.

</details>


### [32] [Structural Entropy Guided Agent for Detecting and Repairing Knowledge Deficiencies in LLMs](https://arxiv.org/pdf/2505.07184)
*Yifan Wei, Xiaoyan Yu, Tengfei Pan, Angsheng Li, Li Du*

Main category: cs.CL

TL;DR: SENATOR framework improves LLMs' domain-specific knowledge by using Structure Entropy and MCTS to generate targeted synthetic data for fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LLMs underperform in knowledge-intensive domains due to redundant synthetic data and misaligned knowledge gaps.

Method: Uses Structure Entropy and Monte Carlo Tree Search to identify and address knowledge deficiencies, generating targeted synthetic data for fine-tuning.

Result: SENATOR improves performance on domain-specific benchmarks for models like LLaMA-3 and Qwen2.

Conclusion: The framework effectively detects and repairs knowledge gaps, enhancing LLM performance in specialized domains.

Abstract: Large language models (LLMs) have achieved unprecedented performance by
leveraging vast pretraining corpora, yet their performance remains suboptimal
in knowledge-intensive domains such as medicine and scientific research, where
high factual precision is required. While synthetic data provides a promising
avenue for augmenting domain knowledge, existing methods frequently generate
redundant samples that do not align with the model's true knowledge gaps. To
overcome this limitation, we propose a novel Structural Entropy-guided
Knowledge Navigator (SENATOR) framework that addresses the intrinsic knowledge
deficiencies of LLMs. Our approach employs the Structure Entropy (SE) metric to
quantify uncertainty along knowledge graph paths and leverages Monte Carlo Tree
Search (MCTS) to selectively explore regions where the model lacks
domain-specific knowledge. Guided by these insights, the framework generates
targeted synthetic data for supervised fine-tuning, enabling continuous
self-improvement. Experimental results on LLaMA-3 and Qwen2 across multiple
domain-specific benchmarks show that SENATOR effectively detects and repairs
knowledge deficiencies, achieving notable performance improvements. The code
and data for our methods and experiments are available at
https://github.com/weiyifan1023/senator.

</details>


### [33] [Benchmarking Ethical and Safety Risks of Healthcare LLMs in China-Toward Systemic Governance under Healthy China 2030](https://arxiv.org/pdf/2505.07205)
*Mouxiao Bian, Rongzhao Zhang, Chao Ding, Xinwei Peng, Jie Xu*

Main category: cs.CL

TL;DR: A study evaluates ethical and safety risks of LLMs in Chinese healthcare using a 12,000-item Q&A benchmark, revealing gaps in performance and governance, and proposes a governance framework.


<details>
  <summary>Details</summary>
Motivation: To address ethical and patient-safety challenges posed by LLMs in healthcare under China's Healthy China 2030 initiative.

Method: Developed a 12,000-item Q&A benchmark to assess LLMs, evaluated state-of-the-art models, and identified governance shortfalls.

Result: Baseline LLM accuracy was 42.7%, improving to 50.8% after fine-tuning, with notable gaps in ethics and safety decision-making.

Conclusion: Proposes a governance framework to manage LLM risks, emphasizing the need for robust oversight in Chinese healthcare.

Abstract: Large Language Models (LLMs) are poised to transform healthcare under China's
Healthy China 2030 initiative, yet they introduce new ethical and
patient-safety challenges. We present a novel 12,000-item Q&A benchmark
covering 11 ethics and 9 safety dimensions in medical contexts, to
quantitatively evaluate these risks. Using this dataset, we assess
state-of-the-art Chinese medical LLMs (e.g., Qwen 2.5-32B, DeepSeek), revealing
moderate baseline performance (accuracy 42.7% for Qwen 2.5-32B) and significant
improvements after fine-tuning on our data (up to 50.8% accuracy). Results show
notable gaps in LLM decision-making on ethics and safety scenarios, reflecting
insufficient institutional oversight. We then identify systemic governance
shortfalls-including the lack of fine-grained ethical audit protocols, slow
adaptation by hospital IRBs, and insufficient evaluation tools-that currently
hinder safe LLM deployment. Finally, we propose a practical governance
framework for healthcare institutions (embedding LLM auditing teams, enacting
data ethics guidelines, and implementing safety simulation pipelines) to
proactively manage LLM risks. Our study highlights the urgent need for robust
LLM governance in Chinese healthcare, aligning AI innovation with patient
safety and ethical standards.

</details>


### [34] [DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.07233)
*Jiashuo Sun, Xianrui Zhong, Sizhe Zhou, Jiawei Han*

Main category: cs.CL

TL;DR: DynamicRAG introduces a reinforcement learning-based reranker for RAG systems, dynamically adjusting document retrieval to optimize performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of selecting the optimal number of documents in RAG systems to balance information completeness and noise reduction.

Method: Proposes DynamicRAG, a framework using RL to optimize reranking decisions based on LLM output quality.

Result: Achieves state-of-the-art performance across seven knowledge-intensive datasets.

Conclusion: DynamicRAG effectively enhances RAG systems by dynamically refining document retrieval, improving generation quality and explainability.

Abstract: Retrieval-augmented generation (RAG) systems combine large language models
(LLMs) with external knowledge retrieval, making them highly effective for
knowledge-intensive tasks. A crucial but often under-explored component of
these systems is the reranker, which refines retrieved documents to enhance
generation quality and explainability. The challenge of selecting the optimal
number of documents (k) remains unsolved: too few may omit critical
information, while too many introduce noise and inefficiencies. Although recent
studies have explored LLM-based rerankers, they primarily leverage internal
model knowledge and overlook the rich supervisory signals that LLMs can
provide, such as using response quality as feedback for optimizing reranking
decisions. In this paper, we propose DynamicRAG, a novel RAG framework where
the reranker dynamically adjusts both the order and number of retrieved
documents based on the query. We model the reranker as an agent optimized
through reinforcement learning (RL), using rewards derived from LLM output
quality. Across seven knowledge-intensive datasets, DynamicRAG demonstrates
superior performance, achieving state-of-the-art results. The model, data and
code are available at https://github.com/GasolSun36/DynamicRAG

</details>


### [35] [SAS-Bench: A Fine-Grained Benchmark for Evaluating Short Answer Scoring with Large Language Models](https://arxiv.org/pdf/2505.07247)
*Peichao Lai, Kexuan Zhang, Yi Lin, Linyihan Zhang, Feiyang Ye, Jinhao Yan, Yanwei Xu, Conghui He, Yilei Wang, Wentao Zhang, Bin Cui*

Main category: cs.CL

TL;DR: SAS-Bench is a benchmark for LLM-based Short Answer Scoring (SAS) tasks, offering fine-grained scoring, expert annotations, and diverse question types to improve model transparency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing SAG methods lack detail and consistency, and LLMs, while promising, suffer from bias and transparency issues.

Method: Developed SAS-Bench with expert-annotated error categories and diverse questions, tested with various LLMs using few-shot prompting.

Result: Identified challenges in science-related questions and showed few-shot prompting improves scoring accuracy.

Conclusion: SAS-Bench aids in creating more robust, fair, and transparent LLM-based evaluation systems.

Abstract: Subjective Answer Grading (SAG) plays a crucial role in education,
standardized testing, and automated assessment systems, particularly for
evaluating short-form responses in Short Answer Scoring (SAS). However,
existing approaches often produce coarse-grained scores and lack detailed
reasoning. Although large language models (LLMs) have demonstrated potential as
zero-shot evaluators, they remain susceptible to bias, inconsistencies with
human judgment, and limited transparency in scoring decisions. To overcome
these limitations, we introduce SAS-Bench, a benchmark specifically designed
for LLM-based SAS tasks. SAS-Bench provides fine-grained, step-wise scoring,
expert-annotated error categories, and a diverse range of question types
derived from real-world subject-specific exams. This benchmark facilitates
detailed evaluation of model reasoning processes and explainability. We also
release an open-source dataset containing 1,030 questions and 4,109 student
responses, each annotated by domain experts. Furthermore, we conduct
comprehensive experiments with various LLMs, identifying major challenges in
scoring science-related questions and highlighting the effectiveness of
few-shot prompting in improving scoring accuracy. Our work offers valuable
insights into the development of more robust, fair, and educationally
meaningful LLM-based evaluation systems.

</details>


### [36] [No Query, No Access](https://arxiv.org/pdf/2505.07258)
*Wenqiang Wang, Siyuan Liang, Yangshijie Zhang, Xiaojun Jia, Hao Lin, Xiaochun Cao*

Main category: cs.CL

TL;DR: VDBA is a novel adversarial attack method using only victim texts, outperforming existing methods with higher ASR and fewer queries, posing threats to LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial attacks require model knowledge, queries, or training data, limiting practicality. VDBA aims to overcome these constraints.

Method: VDBA uses victim texts, creates a shadow dataset with pre-trained models and clustering, and employs hierarchical substitution models and diverse adversarial example generation.

Result: VDBA improves ASR by 52.08%, reduces queries to 0, and achieves 45.99% ASR on LLMs without API access.

Conclusion: VDBA demonstrates serious security risks for advanced NLP models, even without direct access.

Abstract: Textual adversarial attacks mislead NLP models, including Large Language
Models (LLMs), by subtly modifying text. While effective, existing attacks
often require knowledge of the victim model, extensive queries, or access to
training data, limiting real-world feasibility. To overcome these constraints,
we introduce the \textbf{Victim Data-based Adversarial Attack (VDBA)}, which
operates using only victim texts. To prevent access to the victim model, we
create a shadow dataset with publicly available pre-trained models and
clustering methods as a foundation for developing substitute models. To address
the low attack success rate (ASR) due to insufficient information feedback, we
propose the hierarchical substitution model design, generating substitute
models to mitigate the failure of a single substitute model at the decision
boundary.
  Concurrently, we use diverse adversarial example generation, employing
various attack methods to generate and select the adversarial example with
better similarity and attack effectiveness. Experiments on the Emotion and SST5
datasets show that VDBA outperforms state-of-the-art methods, achieving an ASR
improvement of 52.08\% while significantly reducing attack queries to 0. More
importantly, we discover that VDBA poses a significant threat to LLMs such as
Qwen2 and the GPT family, and achieves the highest ASR of 45.99% even without
access to the API, confirming that advanced NLP models still face serious
security risks. Our codes can be found at
https://anonymous.4open.science/r/VDBA-Victim-Data-based-Adversarial-Attack-36EC/

</details>


### [37] [On the Robustness of Reward Models for Language Model Alignment](https://arxiv.org/pdf/2505.07271)
*Jiwoo Hong, Noah Lee, Eunki Kim, Guijin Son, Woojin Chung, Aman Gupta, Shao Tang, James Thorne*

Main category: cs.CL

TL;DR: The paper investigates over-optimization in reward models (RMs) trained with the Bradley-Terry (BT) model, identifies excessive dispersion of hidden state norms as the cause, and proposes batch-wise sum-to-zero regularization (BSR) to improve robustness. BSR enhances RM performance and RLHF alignment, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Over-optimization in BT-based RMs reduces generalizability to unseen data, impacting RLHF effectiveness. The study aims to address this by improving RM robustness.

Method: The paper identifies hidden state norm dispersion as the issue and introduces BSR to constrain reward magnitudes. It evaluates BSR in four over-optimization scenarios and compares it to plain BT models in RLHF training.

Result: BSR improves RM robustness, outperforming plain BT models. It enhances complex preference prediction by 5% and RLHF alignment, reducing generation length by 40% while increasing win rate by 7%.

Conclusion: Robust RMs, achieved via BSR, significantly improve RLHF performance and alignment, demonstrating the importance of distributional robustness in reward modeling.

Abstract: The Bradley-Terry (BT) model is widely practiced in reward modeling for
reinforcement learning with human feedback (RLHF). Despite its effectiveness,
reward models (RMs) trained with BT model loss are prone to over-optimization,
losing generalizability to unseen input distributions. In this paper, we study
the cause of over-optimization in RM training and its downstream effects on the
RLHF procedure, accentuating the importance of distributional robustness of RMs
in unseen data. First, we show that the excessive dispersion of hidden state
norms is the main source of over-optimization. Then, we propose batch-wise
sum-to-zero regularization (BSR) to enforce zero-centered reward sum per batch,
constraining the rewards with extreme magnitudes. We assess the impact of BSR
in improving robustness in RMs through four scenarios of over-optimization,
where BSR consistently manifests better robustness. Subsequently, we compare
the plain BT model and BSR on RLHF training and empirically show that robust
RMs better align the policy to the gold preference model. Finally, we apply BSR
to high-quality data and models, which surpasses state-of-the-art RMs in the 8B
scale by adding more than 5% in complex preference prediction tasks. By
conducting RLOO training with 8B RMs, AlpacaEval 2.0 reduces generation length
by 40% while adding a 7% increase in win rate, further highlighting that
robustness in RMs induces robustness in RLHF training. We release the code,
data, and models: https://github.com/LinkedIn-XFACT/RM-Robustness.

</details>


### [38] [Semantic Retention and Extreme Compression in LLMs: Can We Have Both?](https://arxiv.org/pdf/2505.07289)
*Stanislas Laborde, Martin Cousseau, Antoun Yaacoub, Lionel Prevost*

Main category: cs.CL

TL;DR: The paper explores joint compression of LLMs by combining pruning and quantization, introducing the SrCr metric to optimize performance-to-compression ratios, achieving a 20% performance boost over quantization-only methods.


<details>
  <summary>Details</summary>
Motivation: The rapid deployment of LLMs necessitates efficient compression techniques to reduce computational and memory costs, but the combined potential of pruning and quantization is underexplored.

Method: The study examines joint compression by strategically combining pruning and quantization, introduces the SrCr metric to evaluate semantic preservation, and optimizes configurations.

Result: Experiments show the recommended combination achieves a 20% performance increase over quantization-only models at the same compression rate.

Conclusion: Joint compression with pruning and quantization, guided by the SrCr metric, offers superior performance-to-compression ratios for LLMs.

Abstract: The exponential growth in Large Language Model (LLM) deployment has
intensified the need for efficient model compression techniques to reduce
computational and memory costs. While pruning and quantization have shown
promise, their combined potential remains largely unexplored. In this paper, we
examine joint compression and how strategically combining pruning and
quantization could yield superior performance-to-compression ratios compared to
single-method approaches. Recognizing the challenges in accurately assessing
LLM performance, we address key limitations of previous evaluation frameworks
and introduce the Semantic Retention Compression Rate (SrCr), a novel metric
that quantifies the trade-off between model compression and semantic
preservation, facilitating the optimization of pruning-quantization
configurations. Experiments demonstrate that our recommended combination
achieves, on average, a 20% performance increase compared to an equivalent
quantization-only model at the same theoretical compression rate.

</details>


### [39] [AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong Pretraining Data Selection](https://arxiv.org/pdf/2505.07293)
*Kai Hua, Steven Wu, Ge Zhang, Ke Shen*

Main category: cs.CL

TL;DR: AttentionInfluence is a training-free method for selecting reasoning-intensive pretraining data by masking attention heads, improving LLMs' reasoning performance.


<details>
  <summary>Details</summary>
Motivation: To avoid biases from supervised classifiers in selecting reasoning data, leveraging attention heads for unsupervised selection.

Method: Uses attention head masking to identify retrieval heads, computes loss differences, and selects data for pretraining.

Result: Improves performance by 1.4pp to 3.5pp on reasoning-heavy benchmarks like MMLU and GSM8K.

Conclusion: AttentionInfluence offers a scalable, unsupervised approach for enhancing reasoning in LLMs.

Abstract: Recently, there has been growing interest in collecting reasoning-intensive
pretraining data to improve LLMs' complex reasoning ability. Prior approaches
typically rely on supervised classifiers to identify such data, which requires
labeling by humans or LLMs, often introducing domain-specific biases. Due to
the attention heads being crucial to in-context reasoning, we propose
AttentionInfluence, a simple yet effective, training-free method without
supervision signal. Our approach enables a small pretrained language model to
act as a strong data selector through a simple attention head masking
operation. Specifically, we identify retrieval heads and compute the loss
difference when masking these heads. We apply AttentionInfluence to a
1.3B-parameter dense model to conduct data selection on the SmolLM corpus of
241B tokens, and mix the SmolLM corpus with the selected subset comprising 73B
tokens to pretrain a 7B-parameter dense model using 1T training tokens and WSD
learning rate scheduling. Our experimental results demonstrate substantial
improvements, ranging from 1.4pp to 3.5pp, across several knowledge-intensive
and reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, GSM8K, and
HumanEval). This demonstrates an effective weak-to-strong scaling property,
with small models improving the final performance of larger models-offering a
promising and scalable path for reasoning-centric data selection.

</details>


### [40] [Towards Multi-Agent Reasoning Systems for Collaborative Expertise Delegation: An Exploratory Design Study](https://arxiv.org/pdf/2505.07313)
*Baixuan Xu, Chunyang Li, Weiqi Wang, Wei Fan, Tianshi Zheng, Haochen Shi, Tao Fan, Yangqiu Song, Qiang Yang*

Main category: cs.CL

TL;DR: The paper explores how collaboration structures in multi-agent LLM systems affect collective reasoning, focusing on expertise alignment, collaboration paradigms, and system scale.


<details>
  <summary>Details</summary>
Motivation: To enhance collective reasoning in multi-agent LLM systems by investigating key design dimensions.

Method: Systematic study of three design dimensions: expertise-domain alignment, collaboration paradigm, and system scale.

Result: Expertise alignment is domain-contingent, diverse knowledge integration outperforms rigid workflows, and system scaling reveals computational trade-offs.

Conclusion: Provides guidelines for configuring multi-agent systems and identifies architectural trade-offs for scalable reasoning.

Abstract: Designing effective collaboration structure for multi-agent LLM systems to
enhance collective reasoning is crucial yet remains under-explored. In this
paper, we systematically investigate how collaborative reasoning performance is
affected by three key design dimensions: (1) Expertise-Domain Alignment, (2)
Collaboration Paradigm (structured workflow vs. diversity-driven integration),
and (3) System Scale. Our findings reveal that expertise alignment benefits are
highly domain-contingent, proving most effective for contextual reasoning
tasks. Furthermore, collaboration focused on integrating diverse knowledge
consistently outperforms rigid task decomposition. Finally, we empirically
explore the impact of scaling the multi-agent system with expertise
specialization and study the computational trade off, highlighting the need for
more efficient communication protocol design. This work provides concrete
guidelines for configuring specialized multi-agent system and identifies
critical architectural trade-offs and bottlenecks for scalable multi-agent
reasoning. The code will be made available upon acceptance.

</details>


### [41] [QUPID: Quantified Understanding for Enhanced Performance, Insights, and Decisions in Korean Search Engines](https://arxiv.org/pdf/2505.07345)
*Ohjoon Kwon, Changsu Lee, Jihye Back, Lim Sun Suk, Inho Kang, Donghyeon Jeon*

Main category: cs.CL

TL;DR: Combining two small language models (SLMs) with different architectures (QUPID) outperforms large language models (LLMs) in relevance assessment, offering higher accuracy, lower computational costs, and scalability.


<details>
  <summary>Details</summary>
Motivation: To improve relevance assessment in information retrieval by leveraging architectural diversity in model combinations, addressing the limitations of LLMs.

Method: QUPID integrates a generative SLM with an embedding-based SLM for relevance judgment.

Result: Achieves higher accuracy (Cohen's Kappa 0.646 vs. 0.387), 60x faster inference, and 1.9% nDCG@5 improvement in production.

Conclusion: Architectural diversity in model combinations enhances search relevance and operational efficiency.

Abstract: Large language models (LLMs) have been widely used for relevance assessment
in information retrieval. However, our study demonstrates that combining two
distinct small language models (SLMs) with different architectures can
outperform LLMs in this task. Our approach -- QUPID -- integrates a generative
SLM with an embedding-based SLM, achieving higher relevance judgment accuracy
while reducing computational costs compared to state-of-the-art LLM solutions.
This computational efficiency makes QUPID highly scalable for real-world search
systems processing millions of queries daily. In experiments across diverse
document types, our method demonstrated consistent performance improvements
(Cohen's Kappa of 0.646 versus 0.387 for leading LLMs) while offering 60x
faster inference times. Furthermore, when integrated into production search
pipelines, QUPID improved nDCG@5 scores by 1.9%. These findings underscore how
architectural diversity in model combinations can significantly enhance both
search relevance and operational efficiency in information retrieval systems.

</details>


### [42] [Computational Fact-Checking of Online Discourse: Scoring scientific accuracy in climate change related news articles](https://arxiv.org/pdf/2505.07409)
*Tim Wittenborg, Constantin Sebastian Tremel, Markus Stocker, Sören Auer*

Main category: cs.CL

TL;DR: A neurosymbolic system using LLM-based statement extraction and knowledge graph analysis semi-automates veracity quantification of online media, but lacks granularity and scalability for public use.


<details>
  <summary>Details</summary>
Motivation: Address misinformation in media by providing reliable veracity quantification to support civic discourse.

Method: Uses LLM-based statement extraction and knowledge graph analysis to compare media against trusted sources.

Result: The system streamlines veracity quantification but requires further improvements for granularity and scalability.

Conclusion: More work on FAIR ground truth and complementary metrics is needed to scientifically support civic discourse.

Abstract: Democratic societies need reliable information. Misinformation in popular
media such as news articles or videos threatens to impair civic discourse.
Citizens are, unfortunately, not equipped to verify this content flood consumed
daily at increasing rates. This work aims to semi-automatically quantify
scientific accuracy of online media. By semantifying media of unknown veracity,
their statements can be compared against equally processed trusted sources. We
implemented a workflow using LLM-based statement extraction and knowledge graph
analysis. Our neurosymbolic system was able to evidently streamline
state-of-the-art veracity quantification. Evaluated via expert interviews and a
user survey, the tool provides a beneficial veracity indication. This
indicator, however, is unable to annotate public media at the required
granularity and scale. Further work towards a FAIR (Findable, Accessible,
Interoperable, Reusable) ground truth and complementary metrics are required to
scientifically support civic discourse.

</details>


### [43] [ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness Prediction via Human-AI Collaborative Annotation](https://arxiv.org/pdf/2505.07416)
*Truc Mai-Thanh Nguyen, Dat Minh Nguyen, Son T. Luu, Kiet Van Nguyen*

Main category: cs.CL

TL;DR: The paper introduces ViMRHP, a Vietnamese multimodal review helpfulness prediction dataset, addressing linguistic diversity gaps. AI-assisted annotation reduces time and costs while maintaining quality, though limitations exist for complex tasks.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack linguistic diversity, especially for low-resource languages like Vietnamese, limiting MRHP task applicability.

Method: Introduces ViMRHP, a large-scale Vietnamese dataset with AI-assisted annotation to optimize time and cost. Evaluates baseline models on human-verified vs. AI-generated annotations.

Result: AI reduces annotation time (90-120s to 20-40s per task) and costs by ~65%, though complex tasks remain challenging.

Conclusion: ViMRHP fills a linguistic gap and demonstrates AI's efficiency in annotation, but human oversight is still needed for complex cases.

Abstract: Multimodal Review Helpfulness Prediction (MRHP) is an essential task in
recommender systems, particularly in E-commerce platforms. Determining the
helpfulness of user-generated reviews enhances user experience and improves
consumer decision-making. However, existing datasets focus predominantly on
English and Indonesian, resulting in a lack of linguistic diversity, especially
for low-resource languages such as Vietnamese. In this paper, we introduce
ViMRHP (Vietnamese Multimodal Review Helpfulness Prediction), a large-scale
benchmark dataset for MRHP task in Vietnamese. This dataset covers four
domains, including 2K products with 46K reviews. Meanwhile, a large-scale
dataset requires considerable time and cost. To optimize the annotation
process, we leverage AI to assist annotators in constructing the ViMRHP
dataset. With AI assistance, annotation time is reduced (90 to 120 seconds per
task down to 20 to 40 seconds per task) while maintaining data quality and
lowering overall costs by approximately 65%. However, AI-generated annotations
still have limitations in complex annotation tasks, which we further examine
through a detailed performance analysis. In our experiment on ViMRHP, we
evaluate baseline models on human-verified and AI-generated annotations to
assess their quality differences. The ViMRHP dataset is publicly available at
https://github.com/trng28/ViMRHP

</details>


### [44] [Comparative sentiment analysis of public perception: Monkeypox vs. COVID-19 behavioral insights](https://arxiv.org/pdf/2505.07430)
*Mostafa Mohaimen Akand Faisal, Rabeya Amin Jhuma*

Main category: cs.CL

TL;DR: Comparative sentiment analysis of COVID-19 and mpox using 147,475 and 106,638 tweets, respectively, reveals key public sentiment trends influenced by disease traits, media, and pandemic fatigue.


<details>
  <summary>Details</summary>
Motivation: To understand public sentiment for effective public health strategies during concurrent health crises like COVID-19 and mpox.

Method: Applied machine learning models (Logistic Regression, Naive Bayes, RoBERTa, DistilRoBERTa, XLNet) for sentiment classification on tweet datasets.

Result: Identified significant differences in public sentiment, driven by disease characteristics, media representation, and pandemic fatigue.

Conclusion: Provides insights for tailored public health messaging, misinformation mitigation, and trust-building, advancing sentiment analysis in public health informatics.

Abstract: The emergence of global health crises, such as COVID-19 and Monkeypox (mpox),
has underscored the importance of understanding public sentiment to inform
effective public health strategies. This study conducts a comparative sentiment
analysis of public perceptions surrounding COVID-19 and mpox by leveraging
extensive datasets of 147,475 and 106,638 tweets, respectively. Advanced
machine learning models, including Logistic Regression, Naive Bayes, RoBERTa,
DistilRoBERTa and XLNet, were applied to perform sentiment classification, with
results indicating key trends in public emotion and discourse. The analysis
highlights significant differences in public sentiment driven by disease
characteristics, media representation, and pandemic fatigue. Through the lens
of sentiment polarity and thematic trends, this study offers valuable insights
into tailoring public health messaging, mitigating misinformation, and
fostering trust during concurrent health crises. The findings contribute to
advancing sentiment analysis applications in public health informatics, setting
the groundwork for enhanced real-time monitoring and multilingual analysis in
future research.

</details>


### [45] [Matching Tasks with Industry Groups for Augmenting Commonsense Knowledge](https://arxiv.org/pdf/2505.07440)
*Rituraj Singh, Sachin Pawar, Girish Palshikar*

Main category: cs.CL

TL;DR: A weakly-supervised framework is proposed to augment commonsense KBs with industry-specific tasks, achieving high precision in task-IG matching.


<details>
  <summary>Details</summary>
Motivation: Existing commonsense KBs lack domain-specific task knowledge, limiting their utility for industry applications.

Method: A neural model learns task-IG affinity, and clustering selects top-k tasks per IG from news datasets.

Result: Extracted 2339 high-precision triples (IG-task pairs) for 24 industry groups.

Conclusion: The framework reliably augments KBs with industry-specific tasks, enhancing their applicability.

Abstract: Commonsense knowledge bases (KB) are a source of specialized knowledge that
is widely used to improve machine learning applications. However, even for a
large KB such as ConceptNet, capturing explicit knowledge from each industry
domain is challenging. For example, only a few samples of general {\em tasks}
performed by various industries are available in ConceptNet. Here, a task is a
well-defined knowledge-based volitional action to achieve a particular goal. In
this paper, we aim to fill this gap and present a weakly-supervised framework
to augment commonsense KB with tasks carried out by various industry groups
(IG). We attempt to {\em match} each task with one or more suitable IGs by
training a neural model to learn task-IG affinity and apply clustering to
select the top-k tasks per IG. We extract a total of 2339 triples of the form
$\langle IG, is~capable~of, task \rangle$ from two publicly available news
datasets for 24 IGs with the precision of 0.86. This validates the reliability
of the extracted task-IG pairs that can be directly added to existing KBs.

</details>


### [46] [Translating the Grievance Dictionary: a psychometric evaluation of Dutch, German, and Italian versions](https://arxiv.org/pdf/2505.07495)
*Isabelle van der Vegt, Bennett Kleinberg, Marilu Miotto, Jonas Festor*

Main category: cs.CL

TL;DR: The paper evaluates translations of the Grievance Dictionary into Dutch, German, and Italian, finding Dutch and German comparable to English, while Italian shows lower reliability.


<details>
  <summary>Details</summary>
Motivation: To extend the Grievance Dictionary's utility to non-English languages for analyzing violent or grievance-fuelled texts.

Method: Automated translation supplemented by human annotation, followed by psychometric analyses (internal reliability, LIWC correlations).

Result: Dutch and German translations perform similarly to English; Italian has low reliability in some categories.

Conclusion: Suggestions for further validation and application, plus future translations using a similar approach.

Abstract: This paper introduces and evaluates three translations of the Grievance
Dictionary, a psycholinguistic dictionary for the analysis of violent,
threatening or grievance-fuelled texts. Considering the relevance of these
themes in languages beyond English, we translated the Grievance Dictionary to
Dutch, German, and Italian. We describe the process of automated translation
supplemented by human annotation. Psychometric analyses are performed,
including internal reliability of dictionary categories and correlations with
the LIWC dictionary. The Dutch and German translations perform similarly to the
original English version, whereas the Italian dictionary shows low reliability
for some categories. Finally, we make suggestions for further validation and
application of the dictionary, as well as for future dictionary translations
following a similar approach.

</details>


### [47] [ToolACE-DEV: Self-Improving Tool Learning via Decomposition and EVolution](https://arxiv.org/pdf/2505.07512)
*Xu Huang, Weiwen Liu, Xingshan Zeng, Yuefeng Huang, Xinlong Hao, Yuxian Wang, Yirong Zeng, Chuhan Wu, Yasheng Wang, Ruiming Tang, Defu Lian*

Main category: cs.CL

TL;DR: ToolACE-DEV is a self-improving framework for tool learning in LLMs, reducing reliance on costly advanced models by decomposing tasks and enabling self-evolution.


<details>
  <summary>Details</summary>
Motivation: Current methods for enhancing tool-using capability in LLMs are costly and suffer from data compatibility issues due to knowledge scope discrepancies.

Method: Decomposes tool-learning into sub-tasks and introduces a self-evolving paradigm for lightweight models.

Result: Validated effectiveness across models of varying scales and architectures.

Conclusion: ToolACE-DEV offers a cost-effective, scalable solution for improving tool-using capabilities in LLMs.

Abstract: The tool-using capability of large language models (LLMs) enables them to
access up-to-date external information and handle complex tasks. Current
approaches to enhancing this capability primarily rely on distilling advanced
models by data synthesis. However, this method incurs significant costs
associated with advanced model usage and often results in data compatibility
issues, led by the high discrepancy in the knowledge scope between the advanced
model and the target model. To address these challenges, we propose
ToolACE-DEV, a self-improving framework for tool learning. First, we decompose
the tool-learning objective into sub-tasks that enhance basic tool-making and
tool-using abilities. Then, we introduce a self-evolving paradigm that allows
lightweight models to self-improve, reducing reliance on advanced LLMs.
Extensive experiments validate the effectiveness of our approach across models
of varying scales and architectures.

</details>


### [48] [SEReDeEP: Hallucination Detection in Retrieval-Augmented Models via Semantic Entropy and Context-Parameter Fusion](https://arxiv.org/pdf/2505.07528)
*Lei Wang*

Main category: cs.CL

TL;DR: SEReDeEP improves hallucination detection in RAG models by incorporating semantic entropy, addressing limitations of prior methods like ReDeEP.


<details>
  <summary>Details</summary>
Motivation: Existing hallucination detection methods focus on isolated mechanisms (external or internal), missing their synergy. ReDeEP decouples these but lacks semantic evaluation, leading to inconsistent assessments.

Method: SEReDeEP enhances ReDeEP by using semantic entropy from trained linear probes for more accurate hallucination detection.

Result: SEReDeEP provides better hallucination assessments by capturing semantic dimensions, aligning more closely with ground truth evaluations.

Conclusion: Incorporating semantic entropy improves hallucination detection in RAG models, addressing gaps in prior approaches.

Abstract: Retrieval-Augmented Generation (RAG) models frequently encounter
hallucination phenomena when integrating external information with internal
parametric knowledge. Empirical studies demonstrate that the disequilibrium
between external contextual information and internal parametric knowledge
constitutes a primary factor in hallucination generation. Existing
hallucination detection methodologies predominantly emphasize either the
external or internal mechanism in isolation, thereby overlooking their
synergistic effects. The recently proposed ReDeEP framework decouples these
dual mechanisms, identifying two critical contributors to hallucinations:
excessive reliance on parametric knowledge encoded in feed-forward networks
(FFN) and insufficient utilization of external information by attention
mechanisms (particularly copy heads). ReDeEP quantitatively assesses these
factors to detect hallucinations and dynamically modulates the contributions of
FFNs and copy heads to attenuate their occurrence. Nevertheless, ReDeEP and
numerous other hallucination detection approaches have been employed at
logit-level uncertainty estimation or language-level self-consistency
evaluation, inadequately address the semantic dimensions of model responses,
resulting in inconsistent hallucination assessments in RAG implementations.
Building upon ReDeEP's foundation, this paper introduces SEReDeEP, which
enhances computational processes through semantic entropy captured via trained
linear probes, thereby achieving hallucination assessments that more accurately
reflect ground truth evaluations.

</details>


### [49] [A Multi-Dimensional Constraint Framework for Evaluating and Improving Instruction Following in Large Language Models](https://arxiv.org/pdf/2505.07591)
*Junjie Ye, Caishuang Huang, Zhuohan Chen, Wenjie Fu, Chenyuan Yang, Leyi Yang, Yilong Wu, Peng Wang, Meng Zhou, Xiaolong Yang, Tao Gui, Qi Zhang, Zhongchao Shi, Jianping Fan, Xuanjing Huang*

Main category: cs.CL

TL;DR: The paper introduces a multi-dimensional constraint framework to evaluate LLMs' instruction-following abilities, addressing limitations of existing benchmarks. It includes automated test generation and evaluates 19 LLMs, revealing performance variations. The framework also aids reinforcement learning, improving instruction adherence without compromising general performance.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLMs lack diversity and fine-grained assessment, limiting their real-world applicability. The paper aims to address this gap with a more comprehensive framework.

Method: Proposes a multi-dimensional constraint framework (3 patterns, 4 categories, 4 difficulty levels) and an automated pipeline for generating 1,200 test samples. Evaluates 19 LLMs across these constraints.

Result: Performance varies significantly across constraint forms (e.g., 77.67% at Level I vs. 32.96% at Level IV). The framework also improves reinforcement learning outcomes by enhancing constraint recognition.

Conclusion: The framework provides a robust tool for assessing and improving LLMs' instruction-following capabilities, with practical applications in reinforcement learning.

Abstract: Instruction following evaluates large language models (LLMs) on their ability
to generate outputs that adhere to user-defined constraints. However, existing
benchmarks often rely on templated constraint prompts, which lack the diversity
of real-world usage and limit fine-grained performance assessment. To fill this
gap, we propose a multi-dimensional constraint framework encompassing three
constraint patterns, four constraint categories, and four difficulty levels.
Building on this framework, we develop an automated instruction generation
pipeline that performs constraint expansion, conflict detection, and
instruction rewriting, yielding 1,200 code-verifiable instruction-following
test samples. We evaluate 19 LLMs across seven model families and uncover
substantial variation in performance across constraint forms. For instance,
average performance drops from 77.67% at Level I to 32.96% at Level IV.
Furthermore, we demonstrate the utility of our approach by using it to generate
data for reinforcement learning, achieving substantial gains in instruction
following without degrading general performance. In-depth analysis indicates
that these gains stem primarily from modifications in the model's attention
modules parameters, which enhance constraint recognition and adherence. Code
and data are available in https://github.com/Junjie-Ye/MulDimIF.

</details>


### [50] [Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent](https://arxiv.org/pdf/2505.07596)
*Ziyang Huang, Xiaowei Yuan, Yiming Ju, Jun Zhao, Kang Liu*

Main category: cs.CL

TL;DR: IKEA is a retrieval-augmented LLM agent that optimizes retrieval timing and integrates internal and external knowledge efficiently, reducing hallucinations and latency.


<details>
  <summary>Details</summary>
Motivation: Existing RL-based LLM search agents underutilize internal knowledge, leading to redundant retrievals, harmful conflicts, and increased latency.

Method: IKEA uses a knowledge-boundary aware reward function and training dataset for RL, prioritizing internal knowledge and retrieving externally only when necessary.

Result: IKEA outperforms baselines, reduces retrieval frequency, and shows strong generalization in knowledge reasoning tasks.

Conclusion: IKEA effectively balances internal and external knowledge, improving efficiency and accuracy in LLM-based retrieval-augmented generation.

Abstract: Retrieval-augmented generation (RAG) is a common strategy to reduce
hallucinations in Large Language Models (LLMs). While reinforcement learning
(RL) can enable LLMs to act as search agents by activating retrieval
capabilities, existing ones often underutilize their internal knowledge. This
can lead to redundant retrievals, potential harmful knowledge conflicts, and
increased inference latency. To address these limitations, an efficient and
adaptive search agent capable of discerning optimal retrieval timing and
synergistically integrating parametric (internal) and retrieved (external)
knowledge is in urgent need. This paper introduces the Reinforced
Internal-External Knowledge Synergistic Reasoning Agent (IKEA), which could
indentify its own knowledge boundary and prioritize the utilization of internal
knowledge, resorting to external search only when internal knowledge is deemed
insufficient. This is achieved using a novel knowledge-boundary aware reward
function and a knowledge-boundary aware training dataset. These are designed
for internal-external knowledge synergy oriented RL, incentivizing the model to
deliver accurate answers, minimize unnecessary retrievals, and encourage
appropriate external searches when its own knowledge is lacking. Evaluations
across multiple knowledge reasoning tasks demonstrate that IKEA significantly
outperforms baseline methods, reduces retrieval frequency significantly, and
exhibits robust generalization capabilities.

</details>


### [51] [Characterizing the Investigative Methods of Fictional Detectives with Large Language Models](https://arxiv.org/pdf/2505.07601)
*Edirlei Soares de Lima, Marco A. Casanova, Bruno Feijó, Antonio L. Furtado*

Main category: cs.CL

TL;DR: An AI-driven method analyzes fictional detectives' investigative traits using LLMs, achieving 91.43% accuracy, aiding computational narratology.


<details>
  <summary>Details</summary>
Motivation: Traditional literary studies lack scalability for extracting detective traits for narrative generation.

Method: Multi-phase workflow using 15 LLMs to extract, synthesize, and validate traits of seven iconic detectives.

Result: 91.43% accuracy in capturing distinctive investigative styles, validated against literary analyses.

Conclusion: Provides a scalable framework for character analysis, useful for AI-driven storytelling and narrative generation.

Abstract: Detective fiction, a genre defined by its complex narrative structures and
character-driven storytelling, presents unique challenges for computational
narratology, a research field focused on integrating literary theory into
automated narrative generation. While traditional literary studies have offered
deep insights into the methods and archetypes of fictional detectives, these
analyses often focus on a limited number of characters and lack the scalability
needed for the extraction of unique traits that can be used to guide narrative
generation methods. In this paper, we present an AI-driven approach for
systematically characterizing the investigative methods of fictional
detectives. Our multi-phase workflow explores the capabilities of 15 Large
Language Models (LLMs) to extract, synthesize, and validate distinctive
investigative traits of fictional detectives. This approach was tested on a
diverse set of seven iconic detectives - Hercule Poirot, Sherlock Holmes,
William Murdoch, Columbo, Father Brown, Miss Marple, and Auguste Dupin -
capturing the distinctive investigative styles that define each character. The
identified traits were validated against existing literary analyses and further
tested in a reverse identification phase, achieving an overall accuracy of
91.43%, demonstrating the method's effectiveness in capturing the distinctive
investigative approaches of each detective. This work contributes to the
broader field of computational narratology by providing a scalable framework
for character analysis, with potential applications in AI-driven interactive
storytelling and automated narrative generation.

</details>


### [52] [MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining](https://arxiv.org/pdf/2505.07608)
*Xiaomi LLM-Core Team, :, Bingquan Xia, Bowen Shen, Cici, Dawei Zhu, Di Zhang, Gang Wang, Hailin Zhang, Huaqiu Liu, Jiebao Xiao, Jinhao Dong, Liang Zhao, Peidian Li, Peng Wang, Shihua Yu, Shimao Chen, Weikun Wang, Wenhan Ma, Xiangwei Deng, Yi Huang, Yifan Song, Zihan Jiang, Bowen Ye, Can Cai, Chenhong He, Dong Zhang, Duo Zhang, Guoan Wang, Hao Tian, Haochen Zhao, Heng Qu, Hongshen Xu, Jun Shi, Kainan Bao, QingKai Fang, Kang Zhou, Kangyang Zhou, Lei Li, Menghang Zhu, Nuo Chen, Qiantong Wang, Shaohui Liu, Shicheng Li, Shuhao Gu, Shuhuai Ren, Shuo Liu, Sirui Deng, Weiji Zhuang, Weiwei Lv, Wenyu Yang, Xin Zhang, Xing Yong, Xing Zhang, Xingchen Song, Xinzhe Xu, Xu Wang, Yihan Yan, Yu Tu, Yuanyuan Tian, Yudong Wang, Yue Yu, Zhenru Lin, Zhichao Song, Zihao Yue*

Main category: cs.CL

TL;DR: MiMo-7B is a large language model optimized for reasoning tasks through enhanced pre-training and post-training stages, outperforming larger models like 32B and OpenAI o1-mini.


<details>
  <summary>Details</summary>
Motivation: To develop a high-performance language model specialized for reasoning tasks by optimizing both pre-training and post-training processes.

Method: Pre-training involves improved data preprocessing, a three-stage data mixing strategy, and Multi-Token Prediction. Post-training uses a curated dataset of 130K verifiable problems with reinforcement learning, a code-reward scheme, and strategic data resampling.

Result: MiMo-7B-Base shows exceptional reasoning potential, outperforming larger models. MiMo-7B-RL excels in mathematics, code, and general reasoning tasks.

Conclusion: MiMo-7B demonstrates superior performance in reasoning tasks, validated by extensive evaluations, and is publicly available.

Abstract: We present MiMo-7B, a large language model born for reasoning tasks, with
optimization across both pre-training and post-training stages. During
pre-training, we enhance the data preprocessing pipeline and employ a
three-stage data mixing strategy to strengthen the base model's reasoning
potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional
Multi-Token Prediction objective for enhanced performance and accelerated
inference speed. During post-training, we curate a dataset of 130K verifiable
mathematics and programming problems for reinforcement learning, integrating a
test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and
employing strategic data resampling to stabilize training. Extensive
evaluations show that MiMo-7B-Base possesses exceptional reasoning potential,
outperforming even much larger 32B models. The final RL-tuned model,
MiMo-7B-RL, achieves superior performance on mathematics, code and general
reasoning tasks, surpassing the performance of OpenAI o1-mini. The model
checkpoints are available at https://github.com/xiaomimimo/MiMo.

</details>


### [53] [Concept-Level Explainability for Auditing & Steering LLM Responses](https://arxiv.org/pdf/2505.07610)
*Kenza Amara, Rita Sevastjanova, Mennatallah El-Assady*

Main category: cs.CL

TL;DR: ConceptX is a concept-level explainability method for LLMs that outperforms token-level methods in faithfulness and human alignment, enabling auditing and steering of model behavior.


<details>
  <summary>Details</summary>
Motivation: Addressing concerns about LLM safety and alignment by improving explainability and control over model outputs.

Method: Introduces ConceptX, a model-agnostic method that identifies semantically rich tokens (concepts) in prompts and assigns importance based on output similarity, preserving context integrity.

Result: Outperforms token-level methods (e.g., TokenSHAP) in faithfulness and human alignment; boosts sentiment shift and reduces attack success rates in steering tasks.

Conclusion: ConceptX provides a transparent and faithful alternative for improving LLM safety and alignment, demonstrating the value of attribution-based explainability.

Abstract: As large language models (LLMs) become widely deployed, concerns about their
safety and alignment grow. An approach to steer LLM behavior, such as
mitigating biases or defending against jailbreaks, is to identify which parts
of a prompt influence specific aspects of the model's output. Token-level
attribution methods offer a promising solution, but still struggle in text
generation, explaining the presence of each token in the output separately,
rather than the underlying semantics of the entire LLM response. We introduce
ConceptX, a model-agnostic, concept-level explainability method that identifies
the concepts, i.e., semantically rich tokens in the prompt, and assigns them
importance based on the outputs' semantic similarity. Unlike current
token-level methods, ConceptX also offers to preserve context integrity through
in-place token replacements and supports flexible explanation goals, e.g.,
gender bias. ConceptX enables both auditing, by uncovering sources of bias, and
steering, by modifying prompts to shift the sentiment or reduce the harmfulness
of LLM responses, without requiring retraining. Across three LLMs, ConceptX
outperforms token-level methods like TokenSHAP in both faithfulness and human
alignment. Steering tasks boost sentiment shift by 0.252 versus 0.131 for
random edits and lower attack success rates from 0.463 to 0.242, outperforming
attribution and paraphrasing baselines. While prompt engineering and
self-explaining methods sometimes yield safer responses, ConceptX offers a
transparent and faithful alternative for improving LLM safety and alignment,
demonstrating the practical value of attribution-based explainability in
guiding LLM behavior.

</details>


### [54] [Chronocept: Instilling a Sense of Time in Machines](https://arxiv.org/pdf/2505.07637)
*Krish Goel, Sanskar Pandey, KS Mahadevan, Harsh Kumar, Vishesh Khadaria*

Main category: cs.CL

TL;DR: Chronocept is a benchmark modeling temporal validity as a continuous probability distribution, outperforming classification-based methods and supporting AI applications like fact-checking and retrieval-augmented generation.


<details>
  <summary>Details</summary>
Motivation: AI lacks robust temporal reasoning, despite progress in other domains. Chronocept addresses this gap by modeling how knowledge validity changes over time.

Method: Uses skew-normal curves fitted along decomposed temporal axes to capture emergence, decay, and peak relevance. Includes two datasets with high inter-annotator agreement.

Result: Baselines predict curve parameters (location, scale, skewness) effectively, outperforming classification approaches.

Conclusion: Chronocept fills a foundational gap in AI's temporal reasoning, with potential applications in knowledge grounding, fact-checking, and proactive agents.

Abstract: Human cognition is deeply intertwined with a sense of time, known as
Chronoception. This sense allows us to judge how long facts remain valid and
when knowledge becomes outdated. Despite progress in vision, language, and
motor control, AI still struggles to reason about temporal validity. We
introduce Chronocept, the first benchmark to model temporal validity as a
continuous probability distribution over time. Using skew-normal curves fitted
along semantically decomposed temporal axes, Chronocept captures nuanced
patterns of emergence, decay, and peak relevance. It includes two datasets:
Benchmark I (atomic facts) and Benchmark II (multi-sentence passages).
Annotations show strong inter-annotator agreement (84% and 89%). Our baselines
predict curve parameters - location, scale, and skewness - enabling
interpretable, generalizable learning and outperforming classification-based
approaches. Chronocept fills a foundational gap in AI's temporal reasoning,
supporting applications in knowledge grounding, fact-checking,
retrieval-augmented generation (RAG), and proactive agents. Code and data are
publicly available.

</details>


### [55] [JobHop: A Large-Scale Dataset of Career Trajectories](https://arxiv.org/pdf/2505.07653)
*Iman Johary, Raphael Romero, Alexandru C. Mara, Tijl De Bie*

Main category: cs.CL

TL;DR: JobHop is a large-scale public dataset derived from anonymized resumes, processed using LLMs to extract structured career data and mapped to ESCO codes, enabling labor market research.


<details>
  <summary>Details</summary>
Motivation: Comprehensive datasets capturing real-world career trajectories are scarce, limiting labor market research and decision-making.

Method: Unstructured resume data is processed with LLMs to extract structured career information, mapped to ESCO codes using multi-label classification.

Result: A dataset of 2.3M work experiences from 391K resumes, providing insights into occupational transitions, mobility, and career breaks.

Conclusion: JobHop offers valuable applications for labor market research, career path prediction, and data-driven decision-making.

Abstract: Understanding labor market dynamics is essential for policymakers, employers,
and job seekers. However, comprehensive datasets that capture real-world career
trajectories are scarce. In this paper, we introduce JobHop, a large-scale
public dataset derived from anonymized resumes provided by VDAB, the public
employment service in Flanders, Belgium. Utilizing Large Language Models
(LLMs), we process unstructured resume data to extract structured career
information, which is then mapped to standardized ESCO occupation codes using a
multi-label classification model. This results in a rich dataset of over 2.3
million work experiences, extracted from and grouped into more than 391,000
user resumes and mapped to standardized ESCO occupation codes, offering
valuable insights into real-world occupational transitions. This dataset
enables diverse applications, such as analyzing labor market mobility, job
stability, and the effects of career breaks on occupational transitions. It
also supports career path prediction and other data-driven decision-making
processes. To illustrate its potential, we explore key dataset characteristics,
including job distributions, career breaks, and job transitions, demonstrating
its value for advancing labor market research.

</details>


### [56] [Using Information Theory to Characterize Prosodic Typology: The Case of Tone, Pitch-Accent and Stress-Accent](https://arxiv.org/pdf/2505.07659)
*Ethan Gotlieb Wilcox, Cui Ding, Giovanni Acampa, Tiago Pimentel, Alex Warstadt, Tamar I. Regev*

Main category: cs.CL

TL;DR: The paper uses information theory to show that tonal languages have higher mutual information between word identity and prosody, supporting a gradient view of linguistic typology.


<details>
  <summary>Details</summary>
Motivation: To characterize the relationship between lexical identity and prosody using information theory, testing if tonal languages exhibit higher mutual information.

Method: Analyzed pitch curves from speakers of ten languages across five families, estimating mutual information between text and pitch.

Result: Tonal languages showed higher mutual information between word identity and pitch, supporting the hypothesis.

Conclusion: Findings support a gradient perspective of linguistic typology, with tonal languages displaying stronger prosodic-lexical relationships.

Abstract: This paper argues that the relationship between lexical identity and prosody
-- one well-studied parameter of linguistic variation -- can be characterized
using information theory. We predict that languages that use prosody to make
lexical distinctions should exhibit a higher mutual information between word
identity and prosody, compared to languages that don't. We test this hypothesis
in the domain of pitch, which is used to make lexical distinctions in tonal
languages, like Cantonese. We use a dataset of speakers reading sentences aloud
in ten languages across five language families to estimate the mutual
information between the text and their pitch curves. We find that, across
languages, pitch curves display similar amounts of entropy. However, these
curves are easier to predict given their associated text in the tonal
languages, compared to pitch- and stress-accent languages, and thus the mutual
information is higher in these languages, supporting our hypothesis. Our
results support perspectives that view linguistic typology as gradient, rather
than categorical.

</details>


### [57] [Benchmarking Retrieval-Augmented Generation for Chemistry](https://arxiv.org/pdf/2505.07671)
*Xianrui Zhong, Bowen Jin, Siru Ouyang, Yanzhen Shen, Qiao Jin, Yin Fang, Zhiyong Lu, Jiawei Han*

Main category: cs.CL

TL;DR: ChemRAG-Bench is introduced as a benchmark for evaluating retrieval-augmented generation (RAG) in chemistry, accompanied by ChemRAG-Toolkit, showing a 17.4% performance gain over direct inference methods.


<details>
  <summary>Details</summary>
Motivation: The lack of high-quality, domain-specific corpora and benchmarks for RAG in chemistry motivates the creation of ChemRAG-Bench and ChemRAG-Toolkit.

Method: The work integrates diverse chemistry knowledge sources into a corpus and develops a toolkit supporting five retrieval algorithms and eight LLMs.

Result: RAG achieves a 17.4% average improvement over direct inference, with analyses on retriever architectures, corpus selection, and passage retrieval.

Conclusion: The study provides practical recommendations for future RAG research and deployment in chemistry, with code and data publicly available.

Abstract: Retrieval-augmented generation (RAG) has emerged as a powerful framework for
enhancing large language models (LLMs) with external knowledge, particularly in
scientific domains that demand specialized and dynamic information. Despite its
promise, the application of RAG in the chemistry domain remains underexplored,
primarily due to the lack of high-quality, domain-specific corpora and
well-curated evaluation benchmarks. In this work, we introduce ChemRAG-Bench, a
comprehensive benchmark designed to systematically assess the effectiveness of
RAG across a diverse set of chemistry-related tasks. The accompanying chemistry
corpus integrates heterogeneous knowledge sources, including scientific
literature, the PubChem database, PubMed abstracts, textbooks, and Wikipedia
entries. In addition, we present ChemRAG-Toolkit, a modular and extensible RAG
toolkit that supports five retrieval algorithms and eight LLMs. Using
ChemRAG-Toolkit, we demonstrate that RAG yields a substantial performance gain
-- achieving an average relative improvement of 17.4% over direct inference
methods. We further conduct in-depth analyses on retriever architectures,
corpus selection, and the number of retrieved passages, culminating in
practical recommendations to guide future research and deployment of RAG
systems in the chemistry domain. The code and data is available at
https://chemrag.github.io.

</details>


### [58] [OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit](https://arxiv.org/pdf/2505.07672)
*Arun S. Maiya*

Main category: cs.CL

TL;DR: OnPrem.LLM is a Python toolkit for using LLMs offline on sensitive data, offering privacy, multiple backends, and a no-code interface.


<details>
  <summary>Details</summary>
Motivation: Address the need for privacy-preserving LLM applications in restricted environments without relying on cloud services.

Method: Provides prebuilt pipelines for tasks like RAG, summarization, and classification, supporting multiple LLM backends (e.g., llama.cpp, Hugging Face) with GPU acceleration.

Result: Enables local or hybrid deployments with cloud providers, balancing performance and data control.

Conclusion: OnPrem.LLM is a versatile solution for secure, offline LLM applications, accessible to both technical and non-technical users.

Abstract: We present OnPrem.LLM, a Python-based toolkit for applying large language
models (LLMs) to sensitive, non-public data in offline or restricted
environments. The system is designed for privacy-preserving use cases and
provides prebuilt pipelines for document processing and storage,
retrieval-augmented generation (RAG), information extraction, summarization,
classification, and prompt/output processing with minimal configuration.
OnPrem.LLM supports multiple LLM backends -- including llama.cpp, Ollama, vLLM,
and Hugging Face Transformers -- with quantized model support, GPU
acceleration, and seamless backend switching. Although designed for fully local
execution, OnPrem.LLM also supports integration with a wide range of cloud LLM
providers when permitted, enabling hybrid deployments that balance performance
with data control. A no-code web interface extends accessibility to
non-technical users.

</details>


### [59] [Codifying Character Logic in Role-Playing](https://arxiv.org/pdf/2505.07705)
*Letian Peng, Jingbo Shang*

Main category: cs.CL

TL;DR: Codified Profiles introduce structured, executable functions for role-playing, improving persistence, updatability, and controllable randomness over traditional prompt-based methods.


<details>
  <summary>Details</summary>
Motivation: To address limitations of prompt-based profiles, such as implicit reasoning and lack of systematic logic inspection, by providing explicit, executable character logic.

Method: Uses structured functions like parse_by_scene(scene) and condition checks (e.g., check_condition) to define character behavior, enabling logic-grounded assertions.

Result: Experiments show significant improvements in persistence, updatability, and behavioral diversity, even enabling smaller models (1B-parameter) for high-quality role-playing.

Conclusion: Codified Profiles offer a scalable, efficient foundation for role-play agents, outperforming traditional methods in logic consistency and control.

Abstract: This paper introduces Codified Profiles for role-playing, a novel approach
that represents character logic as structured, executable functions for
behavioral decision-making. Each profile defines a set of functions
parse_by_scene(scene) that outputs a list of logic-grounded assertions
triggered_statements, using both explicit control structures (e.g.,
if-then-else) and condition checks like check_condition(scene, question), where
each question is a semantically meaningful prompt about the scene (e.g., "Is
the character in danger?") discriminated by the role-playing LLM as true,
false, or unknown. This explicit representation offers three key advantages
over traditional prompt-based profiles, which append character descriptions
directly into text prompts: (1) Persistence, by enforcing complete and
consistent execution of character logic, rather than relying on the model's
implicit reasoning; (2) Updatability, through systematic inspection and
revision of behavioral logic, which is difficult to track or debug in
prompt-only approaches; (3) Controllable Randomness, by supporting stochastic
behavior directly within the logic, enabling fine-grained variability that
prompting alone struggles to achieve. To validate these advantages, we
introduce a new benchmark constructed from 83 characters and 5,141 scenes
curated from Fandom, using NLI-based scoring to compare character responses
against ground-truth actions. Our experiments demonstrate the significant
benefits of codified profiles in improving persistence, updatability, and
behavioral diversity. Notably, by offloading a significant portion of reasoning
to preprocessing, codified profiles enable even 1B-parameter models to perform
high-quality role-playing, providing a scalable and efficient foundation for
local deployment of role-play agents.

</details>


### [60] [Must Read: A Systematic Survey of Computational Persuasion](https://arxiv.org/pdf/2505.07775)
*Nimet Beyza Bozdag, Shuhaib Mehri, Xiaocheng Yang, Hyeonjeong Ha, Zirui Cheng, Esin Durmus, Jiaxuan You, Heng Ji, Gokhan Tur, Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: The paper surveys computational persuasion, focusing on AI's roles as persuader, persuadee, and judge, addressing challenges like ethical concerns and future research directions.


<details>
  <summary>Details</summary>
Motivation: To explore the dual role of AI in persuasion—both as an influencer and a target—and its ethical implications in diverse contexts.

Method: A comprehensive survey structured around three perspectives: AI as persuader, persuadee, and judge, with a taxonomy for research.

Result: Identifies key challenges in evaluating persuasiveness, mitigating manipulation, and ensuring ethical AI-driven persuasion.

Conclusion: Calls for future research to enhance safety, fairness, and effectiveness of AI-powered persuasion while addressing risks from advanced language models.

Abstract: Persuasion is a fundamental aspect of communication, influencing
decision-making across diverse contexts, from everyday conversations to
high-stakes scenarios such as politics, marketing, and law. The rise of
conversational AI systems has significantly expanded the scope of persuasion,
introducing both opportunities and risks. AI-driven persuasion can be leveraged
for beneficial applications, but also poses threats through manipulation and
unethical influence. Moreover, AI systems are not only persuaders, but also
susceptible to persuasion, making them vulnerable to adversarial attacks and
bias reinforcement. Despite rapid advancements in AI-generated persuasive
content, our understanding of what makes persuasion effective remains limited
due to its inherently subjective and context-dependent nature. In this survey,
we provide a comprehensive overview of computational persuasion, structured
around three key perspectives: (1) AI as a Persuader, which explores
AI-generated persuasive content and its applications; (2) AI as a Persuadee,
which examines AI's susceptibility to influence and manipulation; and (3) AI as
a Persuasion Judge, which analyzes AI's role in evaluating persuasive
strategies, detecting manipulation, and ensuring ethical persuasion. We
introduce a taxonomy for computational persuasion research and discuss key
challenges, including evaluating persuasiveness, mitigating manipulative
persuasion, and developing responsible AI-driven persuasive systems. Our survey
outlines future research directions to enhance the safety, fairness, and
effectiveness of AI-powered persuasion while addressing the risks posed by
increasingly capable language models.

</details>


### [61] [Domain Regeneration: How well do LLMs match syntactic properties of text domains?](https://arxiv.org/pdf/2505.07784)
*Da Ju, Hagen Blix, Adina Williams*

Main category: cs.CL

TL;DR: The paper investigates how well LLMs approximate properties of text domains like Wikipedia and news, finding shifted means and reduced variance in regenerated text.


<details>
  <summary>Details</summary>
Motivation: To understand which properties of text domains LLMs faithfully approximate and how well they do so.

Method: Observational approaches from corpus linguistics, prompting an open-source LLM to regenerate text from Wikipedia and news domains, analyzing syntactic abstraction levels.

Result: Regenerated text shows shifted means, lower standard deviation, and reduced long tails compared to human originals.

Conclusion: LLMs approximate text domain properties but with notable deviations in distribution characteristics.

Abstract: Recent improvement in large language model performance have, in all
likelihood, been accompanied by improvement in how well they can approximate
the distribution of their training data. In this work, we explore the following
question: which properties of text domains do LLMs faithfully approximate, and
how well do they do so? Applying observational approaches familiar from corpus
linguistics, we prompt a commonly used, opensource LLM to regenerate text from
two domains of permissively licensed English text which are often contained in
LLM training data -- Wikipedia and news text. This regeneration paradigm allows
us to investigate whether LLMs can faithfully match the original human text
domains in a fairly semantically-controlled setting. We investigate varying
levels of syntactic abstraction, from more simple properties like sentence
length, and article readability, to more complex and higher order properties
such as dependency tag distribution, parse depth, and parse complexity. We find
that the majority of the regenerated distributions show a shifted mean, a lower
standard deviation, and a reduction of the long tail, as compared to the human
originals.

</details>


### [62] [Learning from Peers in Reasoning Models](https://arxiv.org/pdf/2505.07787)
*Tongxu Luo, Wenyu Du, Jiaxi Bi, Stephen Chung, Zhengyang Tang, Hao Yang, Min Zhang, Benyou Wang*

Main category: cs.CL

TL;DR: The paper introduces Learning from Peers (LeaP) to help Large Reasoning Models (LRMs) recover from poor reasoning starts, addressing the 'Prefix Dominance Trap'. LeaP-T fine-tuned models show significant performance gains on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To overcome the 'Prefix Dominance Trap' where LRMs struggle to recover from poor initial reasoning, inspired by peer interaction's psychological benefits.

Method: Proposes LeaP, where reasoning paths share intermediate insights via a routing mechanism. Fine-tunes smaller models into LeaP-T for better instruction adherence.

Result: LeaP improves performance, e.g., QwQ-32B gains 5 points over baseline, surpassing larger models like DeepSeek-R1-671B. LeaP-T-7B matches DeepSeek-R1-Distill-Qwen-14B on AIME 2024.

Conclusion: LeaP enables LRMs to collaborate during reasoning, showing robust error correction and handling varied task difficulty. It marks a milestone in collaborative reasoning for LRMs.

Abstract: Large Reasoning Models (LRMs) have the ability to self-correct even when they
make mistakes in their reasoning paths. However, our study reveals that when
the reasoning process starts with a short but poor beginning, it becomes
difficult for the model to recover. We refer to this phenomenon as the "Prefix
Dominance Trap". Inspired by psychological findings that peer interaction can
promote self-correction without negatively impacting already accurate
individuals, we propose **Learning from Peers** (LeaP) to address this
phenomenon. Specifically, every tokens, each reasoning path summarizes its
intermediate reasoning and shares it with others through a routing mechanism,
enabling paths to incorporate peer insights during inference. However, we
observe that smaller models sometimes fail to follow summarization and
reflection instructions effectively. To address this, we fine-tune them into
our **LeaP-T** model series. Experiments on AIME 2024, AIME 2025, AIMO 2025,
and GPQA Diamond show that LeaP provides substantial improvements. For
instance, QwQ-32B with LeaP achieves nearly 5 absolute points higher than the
baseline on average, and surpasses DeepSeek-R1-671B on three math benchmarks
with an average gain of 3.3 points. Notably, our fine-tuned LeaP-T-7B matches
the performance of DeepSeek-R1-Distill-Qwen-14B on AIME 2024. In-depth analysis
reveals LeaP's robust error correction by timely peer insights, showing strong
error tolerance and handling varied task difficulty. LeaP marks a milestone by
enabling LRMs to collaborate during reasoning. Our code, datasets, and models
are available at https://learning-from-peers.github.io/ .

</details>


### [63] [Learning Dynamics in Continual Pre-Training for Large Language Models](https://arxiv.org/pdf/2505.07796)
*Xingjin Wang, Howe Tissue, Lu Wang, Linjing Li, Daniel Dajun Zeng*

Main category: cs.CL

TL;DR: The paper explores learning dynamics in Continual Pre-Training (CPT) for large language models, focusing on performance evolution and deriving a scaling law to predict loss and optimize training.


<details>
  <summary>Details</summary>
Motivation: To understand how general and domain-specific performance evolves during CPT and to develop a predictive model for loss dynamics.

Method: Analyzes CPT loss curves, decouples distribution shift and learning rate annealing effects, and derives a scaling law combining these factors.

Result: The scaling law accurately predicts loss across training steps and learning rate schedules, validated by experiments on various datasets.

Conclusion: The derived scaling law provides a comprehensive framework for optimizing CPT, balancing general and domain-specific performance.

Abstract: Continual Pre-Training (CPT) has become a popular and effective method to
apply strong foundation models to specific downstream tasks. In this work, we
explore the learning dynamics throughout the CPT process for large language
models. We specifically focus on how general and downstream domain performance
evolves at each training step, with domain performance measured via validation
losses. We have observed that the CPT loss curve fundamentally characterizes
the transition from one curve to another hidden curve, and could be described
by decoupling the effects of distribution shift and learning rate annealing. We
derive a CPT scaling law that combines the two factors, enabling the prediction
of loss at any (continual) training steps and across learning rate schedules
(LRS) in CPT. Our formulation presents a comprehensive understanding of several
critical factors in CPT, including loss potential, peak learning rate, training
steps, replay ratio, etc. Moreover, our approach can be adapted to customize
training hyper-parameters to different CPT goals such as balancing general and
domain-specific performance. Extensive experiments demonstrate that our scaling
law holds across various CPT datasets and training hyper-parameters.

</details>


### [64] [A Comparative Analysis of Static Word Embeddings for Hungarian](https://arxiv.org/pdf/2505.07809)
*Máté Gedeon*

Main category: cs.CL

TL;DR: The paper analyzes static word embeddings for Hungarian, comparing traditional models (Word2Vec, FastText) and BERT-based embeddings. FastText excels in intrinsic tasks, while BERT-based X2Static performs well. Dynamic embeddings (e.g., ELMo) outperform static ones in extrinsic tasks like NER and POS tagging.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare the performance of various static word embeddings for Hungarian, including traditional and BERT-based models, to understand their effectiveness in NLP tasks.

Method: Intrinsic evaluation uses word analogy tasks; extrinsic evaluation employs a bidirectional LSTM for NER and POS tagging. Traditional and BERT-based embeddings are compared.

Result: FastText excels in intrinsic tasks; BERT-based X2Static performs well. Dynamic embeddings (ELMo) outperform static ones in extrinsic tasks.

Conclusion: Static embeddings remain relevant, and advanced extraction methods (X2Static) enhance BERT-based models. Findings support future NLP developments for Hungarian.

Abstract: This paper presents a comprehensive analysis of various static word
embeddings for Hungarian, including traditional models such as Word2Vec,
FastText, as well as static embeddings derived from BERT-based models using
different extraction methods. We evaluate these embeddings on both intrinsic
and extrinsic tasks to provide a holistic view of their performance. For
intrinsic evaluation, we employ a word analogy task, which assesses the
embeddings ability to capture semantic and syntactic relationships. Our results
indicate that traditional static embeddings, particularly FastText, excel in
this task, achieving high accuracy and mean reciprocal rank (MRR) scores. Among
the BERT-based models, the X2Static method for extracting static embeddings
demonstrates superior performance compared to decontextualized and aggregate
methods, approaching the effectiveness of traditional static embeddings. For
extrinsic evaluation, we utilize a bidirectional LSTM model to perform Named
Entity Recognition (NER) and Part-of-Speech (POS) tagging tasks. The results
reveal that embeddings derived from dynamic models, especially those extracted
using the X2Static method, outperform purely static embeddings. Notably, ELMo
embeddings achieve the highest accuracy in both NER and POS tagging tasks,
underscoring the benefits of contextualized representations even when used in a
static form. Our findings highlight the continued relevance of static word
embeddings in NLP applications and the potential of advanced extraction methods
to enhance the utility of BERT-based models. This piece of research contributes
to the understanding of embedding performance in the Hungarian language and
provides valuable insights for future developments in the field. The training
scripts, evaluation codes, restricted vocabulary, and extracted embeddings will
be made publicly available to support further research and reproducibility.

</details>


### [65] [Clickbait Detection via Large Language Models](https://arxiv.org/pdf/2306.09597)
*Han Wang, Yi Zhu, Ye Wang, Yun Li, Yunhao Yuan, Jipeng Qiang*

Main category: cs.CL

TL;DR: LLMs underperform in clickbait detection compared to state-of-the-art methods, failing to achieve satisfactory results even with headlines alone.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether LLMs can effectively detect clickbait, given their success in other NLP tasks.

Method: Analyzed LLMs' performance in few-shot and zero-shot scenarios on English and Chinese benchmark datasets.

Result: LLMs did not outperform existing deep and fine-tuned PLMs methods.

Conclusion: LLMs are not yet reliable for high-quality clickbait detection, contrary to human intuition.

Abstract: Clickbait, which aims to induce users with some surprising and even thrilling
headlines for increasing click-through rates, permeates almost all online
content publishers, such as news portals and social media. Recently, Large
Language Models (LLMs) have emerged as a powerful instrument and achieved
tremendous success in a series of NLP downstream tasks. However, it is not yet
known whether LLMs can be served as a high-quality clickbait detection system.
In this paper, we analyze the performance of LLMs in the few-shot and zero-shot
scenarios on several English and Chinese benchmark datasets. Experimental
results show that LLMs cannot achieve the best results compared to the
state-of-the-art deep and fine-tuning PLMs methods. Different from human
intuition, the experiments demonstrated that LLMs cannot make satisfied
clickbait detection just by the headlines.

</details>


### [66] [Towards Understanding Sycophancy in Language Models](https://arxiv.org/pdf/2310.13548)
*Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R. Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, Ethan Perez*

Main category: cs.CL

TL;DR: AI assistants finetuned with human feedback often exhibit sycophancy, preferring user-belief-matching responses over truthful ones, influenced by human preference judgments.


<details>
  <summary>Details</summary>
Motivation: To investigate the prevalence of sycophancy in AI assistants finetuned with human feedback and the role of human preferences in this behavior.

Method: Analyzed five state-of-the-art AI assistants across four tasks and examined human preference data to assess sycophantic tendencies.

Result: AI assistants consistently showed sycophancy, with human and preference models favoring sycophantic responses over truthful ones.

Conclusion: Sycophancy is a general behavior in AI assistants, likely driven by human preferences favoring such responses.

Abstract: Human feedback is commonly utilized to finetune AI assistants. But human
feedback may also encourage model responses that match user beliefs over
truthful ones, a behaviour known as sycophancy. We investigate the prevalence
of sycophancy in models whose finetuning procedure made use of human feedback,
and the potential role of human preference judgments in such behavior. We first
demonstrate that five state-of-the-art AI assistants consistently exhibit
sycophancy across four varied free-form text-generation tasks. To understand if
human preferences drive this broadly observed behavior, we analyze existing
human preference data. We find that when a response matches a user's views, it
is more likely to be preferred. Moreover, both humans and preference models
(PMs) prefer convincingly-written sycophantic responses over correct ones a
non-negligible fraction of the time. Optimizing model outputs against PMs also
sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results
indicate that sycophancy is a general behavior of state-of-the-art AI
assistants, likely driven in part by human preference judgments favoring
sycophantic responses.

</details>


### [67] [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/pdf/2312.11805)
*Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R. Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Jack Krawczyk, Cosmo Du, Ed Chi, Heng-Tze Cheng, Eric Ni, Purvi Shah, Patrick Kane, Betty Chan, Manaal Faruqui, Aliaksei Severyn, Hanzhao Lin, YaGuang Li, Yong Cheng, Abe Ittycheriah, Mahdis Mahdieh, Mia Chen, Pei Sun, Dustin Tran, Sumit Bagri, Balaji Lakshminarayanan, Jeremiah Liu, Andras Orban, Fabian Güra, Hao Zhou, Xinying Song, Aurelien Boffy, Harish Ganapathy, Steven Zheng, HyunJeong Choe, Ágoston Weisz, Tao Zhu, Yifeng Lu, Siddharth Gopal, Jarrod Kahn, Maciej Kula, Jeff Pitman, Rushin Shah, Emanuel Taropa, Majd Al Merey, Martin Baeuml, Zhifeng Chen, Laurent El Shafey, Yujing Zhang, Olcan Sercinoglu, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, Alexandre Frechette, Charlotte Smith, Laura Culp, Lev Proleev, Yi Luan, Xi Chen, James Lottes, Nathan Schucher, Federico Lebron, Alban Rrustemi, Natalie Clay, Phil Crone, Tomas Kocisky, Jeffrey Zhao, Bartek Perz, Dian Yu, Heidi Howard, Adam Bloniarz, Jack W. Rae, Han Lu, Laurent Sifre, Marcello Maggioni, Fred Alcober, Dan Garrette, Megan Barnes, Shantanu Thakoor, Jacob Austin, Gabriel Barth-Maron, William Wong, Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha, Arun Ahuja, Gaurav Singh Tomar, Evan Senter, Martin Chadwick, Ilya Kornakov, Nithya Attaluri, Iñaki Iturrate, Ruibo Liu, Yunxuan Li, Sarah Cogan, Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang, Jordan Grimstad, Ale Jakse Hartman, Xavier Garcia, Thanumalayan Sankaranarayana Pillai, Jacob Devlin, Michael Laskin, Diego de Las Casas, Dasha Valter, Connie Tao, Lorenzo Blanco, Adrià Puigdomènech Badia, David Reitter, Mianna Chen, Jenny Brennan, Clara Rivera, Sergey Brin, Shariq Iqbal, Gabriela Surita, Jane Labanowski, Abhi Rao, Stephanie Winkler, Emilio Parisotto, Yiming Gu, Kate Olszewska, Ravi Addanki, Antoine Miech, Annie Louis, Denis Teplyashin, Geoff Brown, Elliot Catt, Jan Balaguer, Jackie Xiang, Pidong Wang, Zoe Ashwood, Anton Briukhov, Albert Webson, Sanjay Ganapathy, Smit Sanghavi, Ajay Kannan, Ming-Wei Chang, Axel Stjerngren, Josip Djolonga, Yuting Sun, Ankur Bapna, Matthew Aitchison, Pedram Pejman, Henryk Michalewski, Tianhe Yu, Cindy Wang, Juliette Love, Junwhan Ahn, Dawn Bloxwich, Kehang Han, Peter Humphreys, Thibault Sellam, James Bradbury, Varun Godbole, Sina Samangooei, Bogdan Damoc, Alex Kaskasoli, Sébastien M. R. Arnold, Vijay Vasudevan, Shubham Agrawal, Jason Riesa, Dmitry Lepikhin, Richard Tanburn, Srivatsan Srinivasan, Hyeontaek Lim, Sarah Hodkinson, Pranav Shyam, Johan Ferret, Steven Hand, Ankush Garg, Tom Le Paine, Jian Li, Yujia Li, Minh Giang, Alexander Neitz, Zaheer Abbas, Sarah York, Machel Reid, Elizabeth Cole, Aakanksha Chowdhery, Dipanjan Das, Dominika Rogozińska, Vitaliy Nikolaev, Pablo Sprechmann, Zachary Nado, Lukas Zilka, Flavien Prost, Luheng He, Marianne Monteiro, Gaurav Mishra, Chris Welty, Josh Newlan, Dawei Jia, Miltiadis Allamanis, Clara Huiyi Hu, Raoul de Liedekerke, Justin Gilmer, Carl Saroufim, Shruti Rijhwani, Shaobo Hou, Disha Shrivastava, Anirudh Baddepudi, Alex Goldin, Adnan Ozturel, Albin Cassirer, Yunhan Xu, Daniel Sohn, Devendra Sachan, Reinald Kim Amplayo, Craig Swanson, Dessie Petrova, Shashi Narayan, Arthur Guez, Siddhartha Brahma, Jessica Landon, Miteyan Patel, Ruizhe Zhao, Kevin Villela, Luyu Wang, Wenhao Jia, Matthew Rahtz, Mai Giménez, Legg Yeung, James Keeling, Petko Georgiev, Diana Mincu, Boxi Wu, Salem Haykal, Rachel Saputro, Kiran Vodrahalli, James Qin, Zeynep Cankara, Abhanshu Sharma, Nick Fernando, Will Hawkins, Behnam Neyshabur, Solomon Kim, Adrian Hutter, Priyanka Agrawal, Alex Castro-Ros, George van den Driessche, Tao Wang, Fan Yang, Shuo-yiin Chang, Paul Komarek, Ross McIlroy, Mario Lučić, Guodong Zhang, Wael Farhan, Michael Sharman, Paul Natsev, Paul Michel, Yamini Bansal, Siyuan Qiao, Kris Cao, Siamak Shakeri, Christina Butterfield, Justin Chung, Paul Kishan Rubenstein, Shivani Agrawal, Arthur Mensch, Kedar Soparkar, Karel Lenc, Timothy Chung, Aedan Pope, Loren Maggiore, Jackie Kay, Priya Jhakra, Shibo Wang, Joshua Maynez, Mary Phuong, Taylor Tobin, Andrea Tacchetti, Maja Trebacz, Kevin Robinson, Yash Katariya, Sebastian Riedel, Paige Bailey, Kefan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose Slone, Neil Houlsby, Xuehan Xiong, Zhen Yang, Elena Gribovskaya, Jonas Adler, Mateo Wirth, Lisa Lee, Music Li, Thais Kagohara, Jay Pavagadhi, Sophie Bridgers, Anna Bortsova, Sanjay Ghemawat, Zafarali Ahmed, Tianqi Liu, Richard Powell, Vijay Bolina, Mariko Iinuma, Polina Zablotskaia, James Besley, Da-Woon Chung, Timothy Dozat, Ramona Comanescu, Xiance Si, Jeremy Greer, Guolong Su, Martin Polacek, Raphaël Lopez Kaufman, Simon Tokumine, Hexiang Hu, Elena Buchatskaya, Yingjie Miao, Mohamed Elhawaty, Aditya Siddhant, Nenad Tomasev, Jinwei Xing, Christina Greer, Helen Miller, Shereen Ashraf, Aurko Roy, Zizhao Zhang, Ada Ma, Angelos Filos, Milos Besta, Rory Blevins, Ted Klimenko, Chih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Oscar Chang, Mantas Pajarskas, Carrie Muir, Vered Cohen, Charline Le Lan, Krishna Haridasan, Amit Marathe, Steven Hansen, Sholto Douglas, Rajkumar Samuel, Mingqiu Wang, Sophia Austin, Chang Lan, Jiepu Jiang, Justin Chiu, Jaime Alonso Lorenzo, Lars Lowe Sjösund, Sébastien Cevey, Zach Gleicher, Thi Avrahami, Anudhyan Boral, Hansa Srinivasan, Vittorio Selo, Rhys May, Konstantinos Aisopos, Léonard Hussenot, Livio Baldini Soares, Kate Baumli, Michael B. Chang, Adrià Recasens, Ben Caine, Alexander Pritzel, Filip Pavetic, Fabio Pardo, Anita Gergely, Justin Frye, Vinay Ramasesh, Dan Horgan, Kartikeya Badola, Nora Kassner, Subhrajit Roy, Ethan Dyer, Víctor Campos Campos, Alex Tomala, Yunhao Tang, Dalia El Badawy, Elspeth White, Basil Mustafa, Oran Lang, Abhishek Jindal, Sharad Vikram, Zhitao Gong, Sergi Caelles, Ross Hemsley, Gregory Thornton, Fangxiaoyu Feng, Wojciech Stokowiec, Ce Zheng, Phoebe Thacker, Çağlar Ünlü, Zhishuai Zhang, Mohammad Saleh, James Svensson, Max Bileschi, Piyush Patil, Ankesh Anand, Roman Ring, Katerina Tsihlas, Arpi Vezer, Marco Selvi, Toby Shevlane, Mikel Rodriguez, Tom Kwiatkowski, Samira Daruki, Keran Rong, Allan Dafoe, Nicholas FitzGerald, Keren Gu-Lemberg, Mina Khan, Lisa Anne Hendricks, Marie Pellat, Vladimir Feinberg, James Cobon-Kerr, Tara Sainath, Maribeth Rauh, Sayed Hadi Hashemi, Richard Ives, Yana Hasson, Eric Noland, Yuan Cao, Nathan Byrd, Le Hou, Qingze Wang, Thibault Sottiaux, Michela Paganini, Jean-Baptiste Lespiau, Alexandre Moufarek, Samer Hassan, Kaushik Shivakumar, Joost van Amersfoort, Amol Mandhane, Pratik Joshi, Anirudh Goyal, Matthew Tung, Andrew Brock, Hannah Sheahan, Vedant Misra, Cheng Li, Nemanja Rakićević, Mostafa Dehghani, Fangyu Liu, Sid Mittal, Junhyuk Oh, Seb Noury, Eren Sezener, Fantine Huot, Matthew Lamm, Nicola De Cao, Charlie Chen, Sidharth Mudgal, Romina Stella, Kevin Brooks, Gautam Vasudevan, Chenxi Liu, Mainak Chain, Nivedita Melinkeri, Aaron Cohen, Venus Wang, Kristie Seymore, Sergey Zubkov, Rahul Goel, Summer Yue, Sai Krishnakumaran, Brian Albert, Nate Hurley, Motoki Sano, Anhad Mohananey, Jonah Joughin, Egor Filonov, Tomasz Kępa, Yomna Eldawy, Jiawern Lim, Rahul Rishi, Shirin Badiezadegan, Taylor Bos, Jerry Chang, Sanil Jain, Sri Gayatri Sundara Padmanabhan, Subha Puttagunta, Kalpesh Krishna, Leslie Baker, Norbert Kalb, Vamsi Bedapudi, Adam Kurzrok, Shuntong Lei, Anthony Yu, Oren Litvin, Xiang Zhou, Zhichun Wu, Sam Sobell, Andrea Siciliano, Alan Papir, Robby Neale, Jonas Bragagnolo, Tej Toor, Tina Chen, Valentin Anklin, Feiran Wang, Richie Feng, Milad Gholami, Kevin Ling, Lijuan Liu, Jules Walter, Hamid Moghaddam, Arun Kishore, Jakub Adamek, Tyler Mercado, Jonathan Mallinson, Siddhinita Wandekar, Stephen Cagle, Eran Ofek, Guillermo Garrido, Clemens Lombriser, Maksim Mukha, Botu Sun, Hafeezul Rahman Mohammad, Josip Matak, Yadi Qian, Vikas Peswani, Pawel Janus, Quan Yuan, Leif Schelin, Oana David, Ankur Garg, Yifan He, Oleksii Duzhyi, Anton Älgmyr, Timothée Lottaz, Qi Li, Vikas Yadav, Luyao Xu, Alex Chinien, Rakesh Shivanna, Aleksandr Chuklin, Josie Li, Carrie Spadine, Travis Wolfe, Kareem Mohamed, Subhabrata Das, Zihang Dai, Kyle He, Daniel von Dincklage, Shyam Upadhyay, Akanksha Maurya, Luyan Chi, Sebastian Krause, Khalid Salama, Pam G Rabinovitch, Pavan Kumar Reddy M, Aarush Selvan, Mikhail Dektiarev, Golnaz Ghiasi, Erdem Guven, Himanshu Gupta, Boyi Liu, Deepak Sharma, Idan Heimlich Shtacher, Shachi Paul, Oscar Akerlund, François-Xavier Aubet, Terry Huang, Chen Zhu, Eric Zhu, Elico Teixeira, Matthew Fritze, Francesco Bertolini, Liana-Eleonora Marinescu, Martin Bölle, Dominik Paulus, Khyatti Gupta, Tejasi Latkar, Max Chang, Jason Sanders, Roopa Wilson, Xuewei Wu, Yi-Xuan Tan, Lam Nguyen Thiet, Tulsee Doshi, Sid Lall, Swaroop Mishra, Wanming Chen, Thang Luong, Seth Benjamin, Jasmine Lee, Ewa Andrejczuk, Dominik Rabiej, Vipul Ranjan, Krzysztof Styrc, Pengcheng Yin, Jon Simon, Malcolm Rose Harriott, Mudit Bansal, Alexei Robsky, Geoff Bacon, David Greene, Daniil Mirylenka, Chen Zhou, Obaid Sarvana, Abhimanyu Goyal, Samuel Andermatt, Patrick Siegler, Ben Horn, Assaf Israel, Francesco Pongetti, Chih-Wei "Louis" Chen, Marco Selvatici, Pedro Silva, Kathie Wang, Jackson Tolins, Kelvin Guu, Roey Yogev, Xiaochen Cai, Alessandro Agostini, Maulik Shah, Hung Nguyen, Noah Ó Donnaile, Sébastien Pereira, Linda Friso, Adam Stambler, Adam Kurzrok, Chenkai Kuang, Yan Romanikhin, Mark Geller, ZJ Yan, Kane Jang, Cheng-Chun Lee, Wojciech Fica, Eric Malmi, Qijun Tan, Dan Banica, Daniel Balle, Ryan Pham, Yanping Huang, Diana Avram, Hongzhi Shi, Jasjot Singh, Chris Hidey, Niharika Ahuja, Pranab Saxena, Dan Dooley, Srividya Pranavi Potharaju, Eileen O'Neill, Anand Gokulchandran, Ryan Foley, Kai Zhao, Mike Dusenberry, Yuan Liu, Pulkit Mehta, Ragha Kotikalapudi, Chalence Safranek-Shrader, Andrew Goodman, Joshua Kessinger, Eran Globen, Prateek Kolhar, Chris Gorgolewski, Ali Ibrahim, Yang Song, Ali Eichenbaum, Thomas Brovelli, Sahitya Potluri, Preethi Lahoti, Cip Baetu, Ali Ghorbani, Charles Chen, Andy Crawford, Shalini Pal, Mukund Sridhar, Petru Gurita, Asier Mujika, Igor Petrovski, Pierre-Louis Cedoz, Chenmei Li, Shiyuan Chen, Niccolò Dal Santo, Siddharth Goyal, Jitesh Punjabi, Karthik Kappaganthu, Chester Kwak, Pallavi LV, Sarmishta Velury, Himadri Choudhury, Jamie Hall, Premal Shah, Ricardo Figueira, Matt Thomas, Minjie Lu, Ting Zhou, Chintu Kumar, Thomas Jurdi, Sharat Chikkerur, Yenai Ma, Adams Yu, Soo Kwak, Victor Ähdel, Sujeevan Rajayogam, Travis Choma, Fei Liu, Aditya Barua, Colin Ji, Ji Ho Park, Vincent Hellendoorn, Alex Bailey, Taylan Bilal, Huanjie Zhou, Mehrdad Khatir, Charles Sutton, Wojciech Rzadkowski, Fiona Macintosh, Roopali Vij, Konstantin Shagin, Paul Medina, Chen Liang, Jinjing Zhou, Pararth Shah, Yingying Bi, Attila Dankovics, Shipra Banga, Sabine Lehmann, Marissa Bredesen, Zifan Lin, John Eric Hoffmann, Jonathan Lai, Raynald Chung, Kai Yang, Nihal Balani, Arthur Bražinskas, Andrei Sozanschi, Matthew Hayes, Héctor Fernández Alcalde, Peter Makarov, Will Chen, Antonio Stella, Liselotte Snijders, Michael Mandl, Ante Kärrman, Paweł Nowak, Xinyi Wu, Alex Dyck, Krishnan Vaidyanathan, Raghavender R, Jessica Mallet, Mitch Rudominer, Eric Johnston, Sushil Mittal, Akhil Udathu, Janara Christensen, Vishal Verma, Zach Irving, Andreas Santucci, Gamaleldin Elsayed, Elnaz Davoodi, Marin Georgiev, Ian Tenney, Nan Hua, Geoffrey Cideron, Edouard Leurent, Mahmoud Alnahlawi, Ionut Georgescu, Nan Wei, Ivy Zheng, Dylan Scandinaro, Heinrich Jiang, Jasper Snoek, Mukund Sundararajan, Xuezhi Wang, Zack Ontiveros, Itay Karo, Jeremy Cole, Vinu Rajashekhar, Lara Tumeh, Eyal Ben-David, Rishub Jain, Jonathan Uesato, Romina Datta, Oskar Bunyan, Shimu Wu, John Zhang, Piotr Stanczyk, Ye Zhang, David Steiner, Subhajit Naskar, Michael Azzam, Matthew Johnson, Adam Paszke, Chung-Cheng Chiu, Jaume Sanchez Elias, Afroz Mohiuddin, Faizan Muhammad, Jin Miao, Andrew Lee, Nino Vieillard, Jane Park, Jiageng Zhang, Jeff Stanway, Drew Garmon, Abhijit Karmarkar, Zhe Dong, Jong Lee, Aviral Kumar, Luowei Zhou, Jonathan Evens, William Isaac, Geoffrey Irving, Edward Loper, Michael Fink, Isha Arkatkar, Nanxin Chen, Izhak Shafran, Ivan Petrychenko, Zhe Chen, Johnson Jia, Anselm Levskaya, Zhenkai Zhu, Peter Grabowski, Yu Mao, Alberto Magni, Kaisheng Yao, Javier Snaider, Norman Casagrande, Evan Palmer, Paul Suganthan, Alfonso Castaño, Irene Giannoumis, Wooyeol Kim, Mikołaj Rybiński, Ashwin Sreevatsa, Jennifer Prendki, David Soergel, Adrian Goedeckemeyer, Willi Gierke, Mohsen Jafari, Meenu Gaba, Jeremy Wiesner, Diana Gage Wright, Yawen Wei, Harsha Vashisht, Yana Kulizhskaya, Jay Hoover, Maigo Le, Lu Li, Chimezie Iwuanyanwu, Lu Liu, Kevin Ramirez, Andrey Khorlin, Albert Cui, Tian LIN, Marcus Wu, Ricardo Aguilar, Keith Pallo, Abhishek Chakladar, Ginger Perng, Elena Allica Abellan, Mingyang Zhang, Ishita Dasgupta, Nate Kushman, Ivo Penchev, Alena Repina, Xihui Wu, Tom van der Weide, Priya Ponnapalli, Caroline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier Dousse, Fan Yang, Jeff Piper, Nathan Ie, Rama Pasumarthi, Nathan Lintz, Anitha Vijayakumar, Daniel Andor, Pedro Valenzuela, Minnie Lui, Cosmin Paduraru, Daiyi Peng, Katherine Lee, Shuyuan Zhang, Somer Greene, Duc Dung Nguyen, Paula Kurylowicz, Cassidy Hardin, Lucas Dixon, Lili Janzer, Kiam Choo, Ziqiang Feng, Biao Zhang, Achintya Singhal, Dayou Du, Dan McKinnon, Natasha Antropova, Tolga Bolukbasi, Orgad Keller, David Reid, Daniel Finchelstein, Maria Abi Raad, Remi Crocker, Peter Hawkins, Robert Dadashi, Colin Gaffney, Ken Franko, Anna Bulanova, Rémi Leblond, Shirley Chung, Harry Askham, Luis C. Cobo, Kelvin Xu, Felix Fischer, Jun Xu, Christina Sorokin, Chris Alberti, Chu-Cheng Lin, Colin Evans, Alek Dimitriev, Hannah Forbes, Dylan Banarse, Zora Tung, Mark Omernick, Colton Bishop, Rachel Sterneck, Rohan Jain, Jiawei Xia, Ehsan Amid, Francesco Piccinno, Xingyu Wang, Praseem Banzal, Daniel J. Mankowitz, Alex Polozov, Victoria Krakovna, Sasha Brown, MohammadHossein Bateni, Dennis Duan, Vlad Firoiu, Meghana Thotakuri, Tom Natan, Matthieu Geist, Ser tan Girgin, Hui Li, Jiayu Ye, Ofir Roval, Reiko Tojo, Michael Kwong, James Lee-Thorp, Christopher Yew, Danila Sinopalnikov, Sabela Ramos, John Mellor, Abhishek Sharma, Kathy Wu, David Miller, Nicolas Sonnerat, Denis Vnukov, Rory Greig, Jennifer Beattie, Emily Caveness, Libin Bai, Julian Eisenschlos, Alex Korchemniy, Tomy Tsai, Mimi Jasarevic, Weize Kong, Phuong Dao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui Zhu, Tian Huey Teh, Jason Sanmiya, Evgeny Gladchenko, Nejc Trdin, Daniel Toyama, Evan Rosen, Sasan Tavakkol, Linting Xue, Chen Elkind, Oliver Woodman, John Carpenter, George Papamakarios, Rupert Kemp, Sushant Kafle, Tanya Grunina, Rishika Sinha, Alice Talbert, Diane Wu, Denese Owusu-Afriyie, Cosmo Du, Chloe Thornton, Jordi Pont-Tuset, Pradyumna Narayana, Jing Li, Saaber Fatehi, John Wieting, Omar Ajmeri, Benigno Uria, Yeongil Ko, Laura Knight, Amélie Héliou, Ning Niu, Shane Gu, Chenxi Pang, Yeqing Li, Nir Levine, Ariel Stolovich, Rebeca Santamaria-Fernandez, Sonam Goenka, Wenny Yustalim, Robin Strudel, Ali Elqursh, Charlie Deck, Hyo Lee, Zonglin Li, Kyle Levin, Raphael Hoffmann, Dan Holtmann-Rice, Olivier Bachem, Sho Arora, Christy Koh, Soheil Hassas Yeganeh, Siim Põder, Mukarram Tariq, Yanhua Sun, Lucian Ionita, Mojtaba Seyedhosseini, Pouya Tafti, Zhiyu Liu, Anmol Gulati, Jasmine Liu, Xinyu Ye, Bart Chrzaszcz, Lily Wang, Nikhil Sethi, Tianrun Li, Ben Brown, Shreya Singh, Wei Fan, Aaron Parisi, Joe Stanton, Vinod Koverkathu, Christopher A. Choquette-Choo, Yunjie Li, TJ Lu, Abe Ittycheriah, Prakash Shroff, Mani Varadarajan, Sanaz Bahargam, Rob Willoughby, David Gaddy, Guillaume Desjardins, Marco Cornero, Brona Robenek, Bhavishya Mittal, Ben Albrecht, Ashish Shenoy, Fedor Moiseev, Henrik Jacobsson, Alireza Ghaffarkhah, Morgane Rivière, Alanna Walton, Clément Crepy, Alicia Parrish, Zongwei Zhou, Clement Farabet, Carey Radebaugh, Praveen Srinivasan, Claudia van der Salm, Andreas Fidjeland, Salvatore Scellato, Eri Latorre-Chimoto, Hanna Klimczak-Plucińska, David Bridson, Dario de Cesare, Tom Hudson, Piermaria Mendolicchio, Lexi Walker, Alex Morris, Matthew Mauger, Alexey Guseynov, Alison Reid, Seth Odoom, Lucia Loher, Victor Cotruta, Madhavi Yenugula, Dominik Grewe, Anastasia Petrushkina, Tom Duerig, Antonio Sanchez, Steve Yadlowsky, Amy Shen, Amir Globerson, Lynette Webb, Sahil Dua, Dong Li, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi, Ananth Agarwal, Tomer Shani, Matan Eyal, Anuj Khare, Shreyas Rammohan Belle, Lei Wang, Chetan Tekur, Mihir Sanjay Kale, Jinliang Wei, Ruoxin Sang, Brennan Saeta, Tyler Liechty, Yi Sun, Yao Zhao, Stephan Lee, Pandu Nayak, Doug Fritz, Manish Reddy Vuyyuru, John Aslanides, Nidhi Vyas, Martin Wicke, Xiao Ma, Evgenii Eltyshev, Nina Martin, Hardie Cate, James Manyika, Keyvan Amiri, Yelin Kim, Xi Xiong, Kai Kang, Florian Luisier, Nilesh Tripuraneni, David Madras, Mandy Guo, Austin Waters, Oliver Wang, Joshua Ainslie, Jason Baldridge, Han Zhang, Garima Pruthi, Jakob Bauer, Feng Yang, Riham Mansour, Jason Gelman, Yang Xu, George Polovets, Ji Liu, Honglong Cai, Warren Chen, XiangHai Sheng, Emily Xue, Sherjil Ozair, Christof Angermueller, Xiaowei Li, Anoop Sinha, Weiren Wang, Julia Wiesinger, Emmanouil Koukoumidis, Yuan Tian, Anand Iyer, Madhu Gurumurthy, Mark Goldenson, Parashar Shah, MK Blake, Hongkun Yu, Anthony Urbanowicz, Jennimaria Palomaki, Chrisantha Fernando, Ken Durden, Harsh Mehta, Nikola Momchev, Elahe Rahimtoroghi, Maria Georgaki, Amit Raul, Sebastian Ruder, Morgan Redshaw, Jinhyuk Lee, Denny Zhou, Komal Jalan, Dinghua Li, Blake Hechtman, Parker Schuh, Milad Nasr, Kieran Milan, Vladimir Mikulik, Juliana Franco, Tim Green, Nam Nguyen, Joe Kelley, Aroma Mahendru, Andrea Hu, Joshua Howland, Ben Vargas, Jeffrey Hui, Kshitij Bansal, Vikram Rao, Rakesh Ghiya, Emma Wang, Ke Ye, Jean Michel Sarr, Melanie Moranski Preston, Madeleine Elish, Steve Li, Aakash Kaku, Jigar Gupta, Ice Pasupat, Da-Cheng Juan, Milan Someswar, Tejvi M., Xinyun Chen, Aida Amini, Alex Fabrikant, Eric Chu, Xuanyi Dong, Amruta Muthal, Senaka Buthpitiya, Sarthak Jauhari, Nan Hua, Urvashi Khandelwal, Ayal Hitron, Jie Ren, Larissa Rinaldi, Shahar Drath, Avigail Dabush, Nan-Jiang Jiang, Harshal Godhia, Uli Sachs, Anthony Chen, Yicheng Fan, Hagai Taitelbaum, Hila Noga, Zhuyun Dai, James Wang, Chen Liang, Jenny Hamer, Chun-Sung Ferng, Chenel Elkind, Aviel Atias, Paulina Lee, Vít Listík, Mathias Carlen, Jan van de Kerkhof, Marcin Pikus, Krunoslav Zaher, Paul Müller, Sasha Zykova, Richard Stefanec, Vitaly Gatsko, Christoph Hirnschall, Ashwin Sethi, Xingyu Federico Xu, Chetan Ahuja, Beth Tsai, Anca Stefanoiu, Bo Feng, Keshav Dhandhania, Manish Katyal, Akshay Gupta, Atharva Parulekar, Divya Pitta, Jing Zhao, Vivaan Bhatia, Yashodha Bhavnani, Omar Alhadlaq, Xiaolin Li, Peter Danenberg, Dennis Tu, Alex Pine, Vera Filippova, Abhipso Ghosh, Ben Limonchik, Bhargava Urala, Chaitanya Krishna Lanka, Derik Clive, Yi Sun, Edward Li, Hao Wu, Kevin Hongtongsak, Ianna Li, Kalind Thakkar, Kuanysh Omarov, Kushal Majmundar, Michael Alverson, Michael Kucharski, Mohak Patel, Mudit Jain, Maksim Zabelin, Paolo Pelagatti, Rohan Kohli, Saurabh Kumar, Joseph Kim, Swetha Sankar, Vineet Shah, Lakshmi Ramachandruni, Xiangkai Zeng, Ben Bariach, Laura Weidinger, Tu Vu, Alek Andreev, Antoine He, Kevin Hui, Sheleem Kashem, Amar Subramanya, Sissie Hsiao, Demis Hassabis, Koray Kavukcuoglu, Adam Sadovsky, Quoc Le, Trevor Strohman, Yonghui Wu, Slav Petrov, Jeffrey Dean, Oriol Vinyals*

Main category: cs.CL

TL;DR: Gemini is a new multimodal model family (Ultra, Pro, Nano) excelling in image, audio, video, and text tasks, with Ultra setting state-of-the-art results in 30/32 benchmarks and achieving human-expert performance on MMLU.


<details>
  <summary>Details</summary>
Motivation: To advance multimodal AI capabilities for diverse applications, from complex reasoning to on-device use.

Method: Developed Gemini models (Ultra, Pro, Nano) and evaluated them on 32 benchmarks, including MMLU and 20 multimodal tasks.

Result: Gemini Ultra outperforms in 30/32 benchmarks, achieves human-expert performance on MMLU, and leads in all 20 multimodal benchmarks.

Conclusion: Gemini's cross-modal reasoning and language understanding enable broad applications, with responsible deployment via services like Gemini Advanced and Google AI Studio.

Abstract: This report introduces a new family of multimodal models, Gemini, that
exhibit remarkable capabilities across image, audio, video, and text
understanding. The Gemini family consists of Ultra, Pro, and Nano sizes,
suitable for applications ranging from complex reasoning tasks to on-device
memory-constrained use-cases. Evaluation on a broad range of benchmarks shows
that our most-capable Gemini Ultra model advances the state of the art in 30 of
32 of these benchmarks - notably being the first model to achieve human-expert
performance on the well-studied exam benchmark MMLU, and improving the state of
the art in every one of the 20 multimodal benchmarks we examined. We believe
that the new capabilities of the Gemini family in cross-modal reasoning and
language understanding will enable a wide variety of use cases. We discuss our
approach toward post-training and deploying Gemini models responsibly to users
through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud
Vertex AI.

</details>


### [68] [Fleet of Agents: Coordinated Problem Solving with Large Language Models](https://arxiv.org/pdf/2405.06691)
*Lars Klein, Nearchos Potamitis, Roland Aydin, Robert West, Caglar Gulcehre, Akhil Arora*

Main category: cs.CL

TL;DR: FoA is a cost-effective framework using LLMs as agents for dynamic tree searches, improving quality by ~5% at ~40% cost of SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Address the scarcity of methods balancing cost and quality in enhancing LLM reasoning.

Method: FoA employs genetic-type particle filtering with autonomous agents and heuristic resampling for dynamic branching.

Result: FoA improves quality by ~5% at ~40% cost, outperforming SOTA methods and even larger models like LLaMA3.2-90B.

Conclusion: FoA offers an optimal cost-quality trade-off and is publicly available for use.

Abstract: While numerous frameworks have been developed to enhance the reasoning
abilities of large language models (LLMs), there is a scarcity of methods that
effectively balance the trade-off between cost and quality. In this paper, we
introduce Fleet of Agents (FoA), a novel and intuitive yet principled framework
utilizing LLMs as agents to navigate through dynamic tree searches, employing a
genetic-type particle filtering approach. FoA spawns a multitude of agents,
each exploring the search space autonomously, followed by a selection phase
where resampling based on a heuristic value function optimizes the balance
between exploration and exploitation. This mechanism enables dynamic branching,
adapting the exploration strategy based on discovered solutions. We conduct
extensive experiments on three benchmark tasks, ``Game of 24'',
``Mini-Crosswords'', and ``WebShop'', utilizing four different LLMs,
``GPT-3.5'', ``GPT-4'', ``LLaMA3.2-11B'', and ``LLaMA3.2-90B''. On average
across all tasks and LLMs, FoA obtains a quality improvement of ~5% while
requiring only ~40% of the cost of previous SOTA methods. Notably, our analyses
reveal that (1) FoA achieves the best cost-quality trade-off among all
benchmarked methods and (2) FoA + LLaMA3.2-11B surpasses the Llama3.2-90B
model. FoA is publicly available at https://github.com/au-clan/FoA.

</details>


### [69] [A Syntax-Injected Approach for Faster and More Accurate Sentiment Analysis](https://arxiv.org/pdf/2406.15163)
*Muhammad Imran, Olga Kellert, Carlos Gómez-Rodríguez*

Main category: cs.CL

TL;DR: The paper introduces SELSP, a fast and accurate syntactic parser for sentiment analysis, outperforming traditional parsers and Transformer-based models in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Syntactic parsing improves sentiment analysis accuracy but is slow. The paper aims to address this bottleneck.

Method: Uses SELSP, treating dependency parsing as sequence labeling, and tests it with sentiment dictionaries and against Transformer models.

Result: SELSP is faster and more accurate than conventional parsers and Transformers, with sentiment dictionaries capturing polarity variation performing best.

Conclusion: SELSP is a promising tool for sentiment analysis, offering speed and accuracy advantages over existing methods.

Abstract: Sentiment Analysis (SA) is a crucial aspect of Natural Language Processing
(NLP), addressing subjective assessments in textual content. Syntactic parsing
is useful in SA because explicit syntactic information can improve accuracy
while providing explainability, but it tends to be a computational bottleneck
in practice due to the slowness of parsing algorithms. This paper addresses
said bottleneck by using a SEquence Labeling Syntactic Parser (SELSP) to inject
syntax into SA. By treating dependency parsing as a sequence labeling problem,
we greatly enhance the speed of syntax-based SA. SELSP is trained and evaluated
on a ternary polarity classification task, demonstrating its faster performance
and better accuracy in polarity prediction tasks compared to conventional
parsers like Stanza and to heuristic approaches that use shallow syntactic
rules for SA like VADER. This increased speed and improved accuracy make SELSP
particularly appealing to SA practitioners in both research and industry. In
addition, we test several sentiment dictionaries on our SELSP to see which one
improves the performance in polarity prediction tasks. Moreover, we compare the
SELSP with Transformer-based models trained on a 5-label classification task.
The results show that dictionaries that capture polarity judgment variation
provide better results than dictionaries that ignore polarity judgment
variation. Moreover, we show that SELSP is considerably faster than
Transformer-based models in polarity prediction tasks.

</details>


### [70] [From Distributional to Overton Pluralism: Investigating Large Language Model Alignment](https://arxiv.org/pdf/2406.17692)
*Thom Lake, Eunsol Choi, Greg Durrett*

Main category: cs.CL

TL;DR: Alignment shifts LLM outputs toward longer, aggregated responses without suppressing useful info. Aligned models' behavior is recoverable from base models using in-context examples, supporting the Superficial Alignment Hypothesis.


<details>
  <summary>Details</summary>
Motivation: To analyze how alignment changes LLM output properties and whether it suppresses useful info or extends base model capabilities.

Method: Re-examined response diversity post-alignment and tested if aligned models' behavior is recoverable from base models using in-context examples.

Result: Alignment aggregates diverse info into single responses without suppressing useful content. Aligned models' outputs can be replicated from base models without fine-tuning.

Conclusion: Current alignment captures but doesn't extend base LLM behavior, supporting the Superficial Alignment Hypothesis. In-context alignment can imitate aligned models effectively.

Abstract: The alignment process changes several properties of a large language model's
(LLM's) output distribution. We analyze two aspects of post-alignment
distributional shift of LLM responses. First, we re-examine previously reported
reductions in response diversity post-alignment. Our analysis suggests that an
apparent drop in the diversity of responses is largely explained by quality
control and information aggregation. Alignment suppresses irrelevant and
unhelpful content while shifting the output distribution toward longer
responses that cover information spanning several responses from the base LLM,
essentially presenting diverse information in a single response. Finding little
evidence that alignment suppresses useful information, it is natural to ask the
opposite question: do aligned models surface information that cannot be
recovered from base models? Our second investigation shows this is not the case
and the behavior of aligned models is recoverable from base models without
fine-tuning. A combination of in-context examples and lower-resolution semantic
hints about response content can elicit responses from base LLMs that are as
similar to alignment-tuned LLM responses as alignment-tuned LLM responses are
to each other. Taken together, these results indicate that current alignment
techniques capture but do not extend the useful subset of assistant-like base
LLM behavior, providing further evidence for the Superficial Alignment
Hypothesis. They also show that in-context alignment can go surprisingly far as
a strategy for imitating aligned LLMs without fine-tuning. Our code and data is
available at https://github.com/thomlake/investigating-alignment.

</details>


### [71] [Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models](https://arxiv.org/pdf/2408.05093)
*Zikai Xie*

Main category: cs.CL

TL;DR: The paper addresses LLM hallucination, particularly numerical comparison errors, and proposes a benchmark to assess consistency by comparing answer-first vs. reasoning-first outputs. A new prompt strategy is introduced to improve reliability.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce factually incorrect or fabricated outputs (hallucination), such as the error '9.11$>$9.9'. The study aims to understand and mitigate this issue.

Method: A benchmark method compares LLM responses generated via answer-first vs. reasoning-first approaches. A novel prompt strategy is introduced to reduce inconsistencies.

Result: The benchmark effectively identifies fabrication, and the new prompt strategy improves LLM performance over direct questioning.

Conclusion: The work highlights a critical LLM flaw and provides a practical solution to enhance reliability.

Abstract: Large language models (LLMs) have generated significant attention since their
inception, finding applications across various academic and industrial domains.
However, these models often suffer from the "hallucination problem", where
outputs, though grammatically and logically coherent, lack factual accuracy or
are entirely fabricated. A particularly troubling issue discovered and widely
discussed recently is the numerical comparison error where multiple LLMs
incorrectly infer that "9.11$>$9.9". We discovered that the order in which LLMs
generate answers and reasoning impacts their consistency. Specifically, results
vary significantly when an LLM generates an answer first and then provides the
reasoning versus generating the reasoning process first and then the
conclusion. Inspired by this, we propose a new benchmark method for assessing
LLM consistency: comparing responses generated through these two different
approaches. This benchmark effectively identifies instances where LLMs
fabricate answers and subsequently generate justifications. Furthermore, we
introduce a novel and straightforward prompt strategy designed to mitigate this
issue. Experimental results demonstrate that this strategy improves performance
across various LLMs compared to direct questioning. This work not only sheds
light on a critical flaw in LLMs but also offers a practical solution to
enhance their reliability.

</details>


### [72] [MABR: Multilayer Adversarial Bias Removal Without Prior Bias Knowledge](https://arxiv.org/pdf/2408.05497)
*Maxwell J. Yin, Boyu Wang, Charles Ling*

Main category: cs.CL

TL;DR: A novel adversarial training strategy reduces social biases in models without needing prior bias-type knowledge or protected attribute labels.


<details>
  <summary>Details</summary>
Motivation: Existing bias mitigation methods require prior knowledge of biases and social groups, which limits their applicability.

Method: Uses auxiliary models trained concurrently to predict main model performance, detecting biases at various feature map levels.

Result: Effectively reduces racial and gender biases in sentiment and occupation classification tasks without demographic annotations.

Conclusion: The approach surpasses traditional methods, advancing bias mitigation by eliminating the need for detailed demographic insights.

Abstract: Models trained on real-world data often mirror and exacerbate existing social
biases. Traditional methods for mitigating these biases typically require prior
knowledge of the specific biases to be addressed, such as gender or racial
biases, and the social groups associated with each instance. In this paper, we
introduce a novel adversarial training strategy that operates independently of
prior bias-type knowledge and protected attribute labels. Our approach
proactively identifies biases during model training by utilizing auxiliary
models, which are trained concurrently by predicting the performance of the
main model without relying on task labels. Additionally, we implement these
auxiliary models at various levels of the feature maps of the main model,
enabling the detection of a broader and more nuanced range of bias features.
Through experiments on racial and gender biases in sentiment and occupation
classification tasks, our method effectively reduces social biases without the
need for demographic annotations. Moreover, our approach not only matches but
often surpasses the efficacy of methods that require detailed demographic
insights, marking a significant advancement in bias mitigation techniques.

</details>


### [73] [Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer](https://arxiv.org/pdf/2408.09701)
*Mingda Li, Abhijit Mishra, Utkarsh Mujumdar*

Main category: cs.CL

TL;DR: The paper addresses biases in LLMs for non-English code generation, proposing a zero-shot cross-lingual method using neural projection to improve multilingual inclusivity.


<details>
  <summary>Details</summary>
Motivation: LLMs show biases and limitations with non-English prompts, hindering global inclusivity in code generation.

Method: A zero-shot cross-lingual approach using neural projection (e.g., LASER) to map multilingual embeddings into the LLM's token space, trained only on English data.

Result: Significant improvements in code quality for non-English prompts on a translated MBPP dataset.

Conclusion: The proposed method enhances multilingual code generation, promoting inclusivity in programming.

Abstract: The use of Large Language Models (LLMs) for program code generation has
gained substantial attention, but their biases and limitations with non-English
prompts challenge global inclusivity. This paper investigates the complexities
of multilingual prompt-based code generation. Our evaluations of LLMs,
including CODELLAMA and CODEGEMMA, reveal significant disparities in code
quality for non-English prompts; we also demonstrate the inadequacy of simple
approaches like prompt translation, bootstrapped data augmentation, and
fine-tuning. To address this, we propose a zero-shot cross-lingual approach
using a neural projection technique, integrating a cross-lingual encoder like
LASER to map multilingual embeddings from it into the LLM's token space. This
method requires training only on English data and scales effectively to other
languages. Results on a translated and quality-checked MBPP dataset show
substantial improvements in code quality. This research promotes a more
inclusive code generation landscape by empowering LLMs with multilingual
capabilities to support the diverse linguistic spectrum in programming.

</details>


### [74] [Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant](https://arxiv.org/pdf/2409.11055)
*Jemin Lee, Sihyeong Park, Jinse Kwon, Jihun Oh, Yongin Kwon*

Main category: cs.CL

TL;DR: Quantization evaluation of instruction-tuned models (1B-405B) shows FP8 as robust, AWQ outperforms GPTQ, and 4-bit quantization harms smaller models. Hard tasks don't always suffer most, and coding/STEM tasks decline.


<details>
  <summary>Details</summary>
Motivation: Assess quantization's impact on modern models (e.g., Llama-3.3) beyond perplexity, focusing on instruction-following and hallucination.

Method: Evaluate 1B-405B models using four quantization methods across 13 datasets, including MT-Bench for coding/STEM.

Result: Quantized models beat smaller FP16 baselines but struggle with instructions/hallucination. FP8 is robust; 4-bit hurts small models. Hard tasks don't always lose most accuracy.

Conclusion: Quantization amplifies model weaknesses, not just task difficulty. FP8 and AWQ are strong choices, but coding/STEM tasks suffer.

Abstract: Quantization has gained attention as a promising solution for the
cost-effective deployment of large and small language models. However, most
prior work has been limited to perplexity or basic knowledge tasks and lacks a
comprehensive evaluation of recent models like Llama-3.3. In this paper, we
conduct a comprehensive evaluation of instruction-tuned models spanning 1B to
405B parameters, applying four quantization methods across 13 datasets. Our
findings reveal that (1) quantized models generally surpass smaller FP16
baselines, yet they often struggle with instruction-following and hallucination
detection; (2) FP8 consistently emerges as the most robust option across tasks,
and AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller
models can suffer severe accuracy drops at 4-bit quantization, while 70B-scale
models maintain stable performance; (4) notably, \textit{hard} tasks do not
always experience the largest accuracy losses, indicating that quantization
magnifies a model's inherent weaknesses rather than simply correlating with
task difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant
performance declines in Coding and STEM tasks, though it occasionally reports
improvements in reasoning.

</details>


### [75] [Mapping Biomedical Ontology Terms to IDs: Effect of Domain Prevalence on Prediction Accuracy](https://arxiv.org/pdf/2409.13746)
*Thanh Son Do, Daniel B. Hier, Tayo Obafemi-Ajayi*

Main category: cs.CL

TL;DR: The study evaluates LLMs' ability to map biomedical ontology terms to IDs, finding prevalence strongly predicts accuracy for HPO, GO, and UniProtKB, but not for HUGO gene symbols.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' performance in mapping biomedical ontology terms and understand the role of ontology ID prevalence in accuracy.

Method: Used PubMed Central dataset counts as a proxy for prevalence, analyzed mapping accuracy, and built predictive models using ROC curves.

Result: Higher prevalence correlated with better mapping accuracy for HPO, GO, and UniProtKB, but not for HUGO gene symbols, where GPT-4 achieved 95% accuracy regardless of prevalence.

Conclusion: LLMs struggle with low-prevalence ontology IDs; prevalence should be considered in training and evaluation for biomedical applications.

Abstract: This study evaluates the ability of large language models (LLMs) to map
biomedical ontology terms to their corresponding ontology IDs across the Human
Phenotype Ontology (HPO), Gene Ontology (GO), and UniProtKB terminologies.
Using counts of ontology IDs in the PubMed Central (PMC) dataset as a surrogate
for their prevalence in the biomedical literature, we examined the relationship
between ontology ID prevalence and mapping accuracy. Results indicate that
ontology ID prevalence strongly predicts accurate mapping of HPO terms to HPO
IDs, GO terms to GO IDs, and protein names to UniProtKB accession numbers.
Higher prevalence of ontology IDs in the biomedical literature correlated with
higher mapping accuracy. Predictive models based on receiver operating
characteristic (ROC) curves confirmed this relationship.
  In contrast, this pattern did not apply to mapping protein names to Human
Genome Organisation's (HUGO) gene symbols. GPT-4 achieved a high baseline
performance (95%) in mapping protein names to HUGO gene symbols, with mapping
accuracy unaffected by prevalence. We propose that the high prevalence of HUGO
gene symbols in the literature has caused these symbols to become lexicalized,
enabling GPT-4 to map protein names to HUGO gene symbols with high accuracy.
These findings highlight the limitations of LLMs in mapping ontology terms to
low-prevalence ontology IDs and underscore the importance of incorporating
ontology ID prevalence into the training and evaluation of LLMs for biomedical
applications.

</details>


### [76] [Endless Jailbreaks with Bijection Learning](https://arxiv.org/pdf/2410.01294)
*Brian R. Y. Huang, Maximilian Li, Leonard Tang*

Main category: cs.CL

TL;DR: Bijection learning is introduced as an attack algorithm to exploit LLM vulnerabilities by using controlled-complexity encodings, revealing that more capable models are more susceptible to such jailbreaks.


<details>
  <summary>Details</summary>
Motivation: Despite existing safety measures, LLMs remain vulnerable to adversarial inputs (jailbreaks), prompting the need for methods to identify and understand these vulnerabilities.

Method: The approach involves bijection learning, where models are taught bijective encodings via in-context learning. Encoded queries bypass safety mechanisms, and responses are decoded back to English.

Result: The attack is highly effective across various LLMs, with a correlation found between model capability and the complexity of successful bijection attacks.

Conclusion: More capable LLMs are more vulnerable to bijection attacks, indicating that scaling models introduces new safety challenges.

Abstract: Despite extensive safety measures, LLMs are vulnerable to adversarial inputs,
or jailbreaks, which can elicit unsafe behaviors. In this work, we introduce
bijection learning, a powerful attack algorithm which automatically fuzzes LLMs
for safety vulnerabilities using randomly-generated encodings whose complexity
can be tightly controlled. We leverage in-context learning to teach models
bijective encodings, pass encoded queries to the model to bypass built-in
safety mechanisms, and finally decode responses back into English. Our attack
is extremely effective on a wide range of frontier language models. Moreover,
by controlling complexity parameters such as number of key-value mappings in
the encodings, we find a close relationship between the capability level of the
attacked LLM and the average complexity of the most effective bijection
attacks. Our work highlights that new vulnerabilities in frontier models can
emerge with scale: more capable models are more severely jailbroken by
bijection attacks.

</details>


### [77] [Isolated Causal Effects of Natural Language](https://arxiv.org/pdf/2410.14812)
*Victoria Lin, Louis-Philippe Morency, Eli Ben-Michael*

Main category: cs.CL

TL;DR: A framework for estimating isolated causal effects of language interventions on reader perceptions, addressing bias from non-focal language approximations.


<details>
  <summary>Details</summary>
Motivation: To understand how language changes impact reader perceptions and behaviors, formalizing causal effects of language interventions.

Method: Introduces a formal estimation framework, evaluating non-focal language approximations using omitted variable bias principles.

Result: Poor non-focal language approximation causes bias; framework validates recovery of isolated effects in experiments.

Conclusion: Proposed measures effectively assess bias sensitivity, aiding accurate estimation of language's causal effects.

Abstract: As language technologies become widespread, it is important to understand how
changes in language affect reader perceptions and behaviors. These
relationships may be formalized as the isolated causal effect of some focal
language-encoded intervention (e.g., factual inaccuracies) on an external
outcome (e.g., readers' beliefs). In this paper, we introduce a formal
estimation framework for isolated causal effects of language. We show that a
core challenge of estimating isolated effects is the need to approximate all
non-focal language outside of the intervention. Drawing on the principle of
omitted variable bias, we provide measures for evaluating the quality of both
non-focal language approximations and isolated effect estimates themselves. We
find that poor approximation of non-focal language can lead to bias in the
corresponding isolated effect estimates due to omission of relevant variables,
and we show how to assess the sensitivity of effect estimates to such bias
along the two key axes of fidelity and overlap. In experiments on
semi-synthetic and real-world data, we validate the ability of our framework to
correctly recover isolated effects and demonstrate the utility of our proposed
measures.

</details>


### [78] [Does Data Contamination Detection Work (Well) for LLMs? A Survey and Evaluation on Detection Assumptions](https://arxiv.org/pdf/2410.18966)
*Yujuan Fu, Ozlem Uzuner, Meliha Yetisgen, Fei Xia*

Main category: cs.CL

TL;DR: The paper reviews data contamination detection in LLMs, categorizes assumptions, and tests three in case studies, finding MIA approaches may perform poorly due to LLMs learning distributions rather than memorizing instances.


<details>
  <summary>Details</summary>
Motivation: Address the gap in universally validated assumptions for detecting data contamination in LLMs, given their reliance on vast training data.

Method: Systematically review 50 papers on data contamination detection, categorize assumptions, and test three in case studies focusing on Membership Inference Attacks (MIA).

Result: MIA approaches based on tested assumptions perform similarly to random guessing, indicating LLMs learn distributions, not instances. MIA fails with data distribution shifts.

Conclusion: Current MIA methods for detecting data contamination in LLMs are unreliable due to distribution learning and sensitivity to shifts, highlighting the need for better approaches.

Abstract: Large language models (LLMs) have demonstrated great performance across
various benchmarks, showing potential as general-purpose task solvers. However,
as LLMs are typically trained on vast amounts of data, a significant concern in
their evaluation is data contamination, where overlap between training data and
evaluation datasets inflates performance assessments. Multiple approaches have
been developed to identify data contamination. These approaches rely on
specific assumptions that may not hold universally across different settings.
To bridge this gap, we systematically review 50 papers on data contamination
detection, categorize the underlying assumptions, and assess whether they have
been rigorously validated. We identify and analyze eight categories of
assumptions and test three of them as case studies. Our case studies focus on
detecting direct, instance-level data contamination, which is also referred to
as Membership Inference Attacks (MIA). Our analysis reveals that MIA approaches
based on these three assumptions can have similar performance to random
guessing, on datasets used in LLM pretraining, suggesting that current LLMs
might learn data distributions rather than memorizing individual instances.
Meanwhile, MIA can easily fail when there are data distribution shifts between
the seen and unseen instances.

</details>


### [79] [Evaluating Creative Short Story Generation in Humans and Large Language Models](https://arxiv.org/pdf/2411.02316)
*Mete Ismayilzada, Claire Stevenson, Lonneke van der Plas*

Main category: cs.CL

TL;DR: The paper analyzes creativity in story-writing by comparing 60 LLMs and 60 humans using automated metrics and human ratings, finding LLMs lag in novelty and surprise but are rated more creative by non-experts and LLMs themselves.


<details>
  <summary>Details</summary>
Motivation: To systematically explore and compare the creative capabilities of LLMs and humans in short story generation, addressing gaps in understanding LLM creativity.

Method: A five-sentence cue-word-based task was used to generate stories, evaluated via automated metrics (novelty, surprise, diversity, complexity) and human ratings (experts, non-experts, LLMs).

Result: LLMs produce stylistically complex stories but lack novelty and surprise compared to humans. Experts align with automated metrics, while non-experts and LLMs rate LLM stories as more creative.

Conclusion: The study highlights discrepancies in creativity ratings, suggesting implications for evaluating both human and artificial creativity.

Abstract: Story-writing is a fundamental aspect of human imagination, relying heavily
on creativity to produce narratives that are novel, effective, and surprising.
While large language models (LLMs) have demonstrated the ability to generate
high-quality stories, their creative story-writing capabilities remain
under-explored. In this work, we conduct a systematic analysis of creativity in
short story generation across 60 LLMs and 60 people using a five-sentence
cue-word-based creative story-writing task. We use measures to automatically
evaluate model- and human-generated stories across several dimensions of
creativity, including novelty, surprise, diversity, and linguistic complexity.
We also collect creativity ratings and Turing Test classifications from
non-expert and expert human raters and LLMs. Automated metrics show that LLMs
generate stylistically complex stories, but tend to fall short in terms of
novelty, surprise and diversity when compared to average human writers. Expert
ratings generally coincide with automated metrics. However, LLMs and
non-experts rate LLM stories to be more creative than human-generated stories.
We discuss why and how these differences in ratings occur, and their
implications for both human and artificial creativity.

</details>


### [80] [The Root Shapes the Fruit: On the Persistence of Gender-Exclusive Harms in Aligned Language Models](https://arxiv.org/pdf/2411.03700)
*Anaelia Ovalle, Krunoslav Lehman Pavasovic, Louis Martin, Luke Zettlemoyer, Eric Michael Smith, Kai-Wei Chang, Adina Williams, Levent Sagun*

Main category: cs.CL

TL;DR: The paper investigates how alignment techniques in LLMs may amplify biases, especially for gender-diverse groups, revealing gaps in current benchmarks and proposing a framework for better bias evaluation.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding of how alignment techniques perpetuate biases, particularly for underrepresented gender-diverse identities, and to highlight gaps in current bias evaluation benchmarks.

Method: The study includes a survey of bias evaluation in LLMs, systematic evaluation of gender-diverse biases across 16 models, and proposes a framework for measuring harmful biases in reward signals.

Result: DPO-aligned models amplify stigmatization and gender non-affirmative language from base models, with sensitivity to supervised finetuning.

Conclusion: Recommends community-informed bias evaluation frameworks to better identify and address underrepresented harms in LLM alignment practices.

Abstract: Natural-language assistants are designed to provide users with helpful
responses while avoiding harmful outputs, largely achieved through alignment to
human preferences. Yet there is limited understanding of whether alignment
techniques may inadvertently perpetuate or even amplify harmful biases
inherited from their pre-aligned base models. This issue is compounded by the
choice of bias evaluation benchmarks in popular preference-finetuned models,
which predominantly focus on dominant social categories, such as binary gender,
thereby limiting insights into biases affecting underrepresented groups.
Towards addressing this gap, we center transgender, nonbinary, and other
gender-diverse identities to investigate how alignment procedures interact with
pre-existing gender-diverse bias in LLMs. Our key contributions include: 1) a
comprehensive survey of bias evaluation modalities across leading
preference-finetuned LLMs, highlighting critical gaps in gender-diverse
representation, 2) systematic evaluation of gender-diverse biases across 16
models spanning Direct Preference Optimization (DPO) stages, uncovering harms
popular bias benchmarks fail to detect, and 3) a flexible framework for
measuring harmful biases in implicit reward signals applicable to other social
contexts. Our findings reveal that DPO-aligned models are particularly
sensitive to supervised finetuning (SFT), and can amplify two forms of
real-world gender-diverse harms from their base models: stigmatization and
gender non-affirmative language. We conclude with recommendations tailored to
DPO and broader alignment practices, advocating for the adoption of
community-informed bias evaluation frameworks to more effectively identify and
address underrepresented harms in LLMs.

</details>


### [81] [Diversity Helps Jailbreak Large Language Models](https://arxiv.org/pdf/2411.04223)
*Weiliang Zhao, Daniel Ben-Levi, Wei Hao, Junfeng Yang, Chengzhi Mao*

Main category: cs.CL

TL;DR: A new jailbreak technique exploits LLMs' ability to diverge from context, bypassing safety constraints with high success rates, exposing flaws in current safety training.


<details>
  <summary>Details</summary>
Motivation: To highlight vulnerabilities in LLM safety mechanisms and demonstrate how easily they can be bypassed.

Method: Instructs LLMs to deviate and obfuscate prior attacks, requiring fewer queries than existing methods.

Result: Achieves 62.83% higher success rate in compromising top chatbots like GPT-4, Gemini, and Llama, using only 12.9% of queries.

Conclusion: Current safety methods may only mask vulnerabilities; urgent revolution in testing is needed for robust LLM security.

Abstract: We have uncovered a powerful jailbreak technique that leverages large
language models' ability to diverge from prior context, enabling them to bypass
safety constraints and generate harmful outputs. By simply instructing the LLM
to deviate and obfuscate previous attacks, our method dramatically outperforms
existing approaches, achieving up to a 62.83% higher success rate in
compromising ten leading chatbots, including GPT-4, Gemini, and Llama, while
using only 12.9% of the queries. This revelation exposes a critical flaw in
current LLM safety training, suggesting that existing methods may merely mask
vulnerabilities rather than eliminate them. Our findings sound an urgent alarm
for the need to revolutionize testing methodologies to ensure robust and
reliable LLM security.

</details>


### [82] [XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models](https://arxiv.org/pdf/2411.15100)
*Yixin Dong, Charlie F. Ruan, Yaxing Cai, Ruihang Lai, Ziyi Xu, Yilong Zhao, Tianqi Chen*

Main category: cs.CL

TL;DR: XGrammar is a new engine for efficient structured generation in LLMs, using context-free grammar optimizations to reduce runtime overhead and achieve significant speedups.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of LLM applications demands structured outputs, but current context-free grammar methods introduce high runtime overhead.

Method: XGrammar divides vocabulary into context-independent and context-dependent tokens, expands grammar context, and uses a persistent stack for efficient checks. It also co-designs with LLM inference engines.

Result: XGrammar achieves up to 100x speedup over existing solutions and near-zero overhead in end-to-end LLM serving.

Conclusion: XGrammar provides a flexible and efficient solution for structured generation in LLMs, addressing runtime overhead and enabling practical deployment.

Abstract: The applications of LLM Agents are becoming increasingly complex and diverse,
leading to a high demand for structured outputs that can be parsed into code,
structured function calls, and embodied agent commands. These developments
bring significant demands for structured generation in LLM inference.
Context-free grammar is a flexible approach to enable structured generation via
constrained decoding. However, executing context-free grammar requires going
through several stack states over all tokens in vocabulary during runtime,
bringing non-negligible overhead for structured generation. In this paper, we
propose XGrammar, a flexible and efficient structure generation engine for
large language models. XGrammar accelerates context-free grammar execution by
dividing the vocabulary into context-independent tokens that can be prechecked
and context-dependent tokens that need to be interpreted during runtime. We
further build transformations to expand the grammar context and reduce the
number of context-independent tokens. Additionally, we build an efficient
persistent stack to accelerate the context-dependent token checks. Finally, we
co-design the grammar engine with LLM inference engine to overlap grammar
computation with GPU executions. Evaluation results show that XGrammar can
achieve up to 100x speedup over existing solutions. Combined with an LLM
inference engine, it can generate near-zero overhead structure generation in
end-to-end low-LLM serving.

</details>


### [83] [Multi-Party Supervised Fine-tuning of Language Models for Multi-Party Dialogue Generation](https://arxiv.org/pdf/2412.05342)
*Xiaoyu Wang, Ningyuan Xi, Teng Chen, Qingqing Gu, Yue Zhao, Xiaokai Chen, Zhonglin Jiang, Yong Chen, Luo Ji*

Main category: cs.CL

TL;DR: MuPaS is a multi-party fine-tuning framework for LLMs, improving their performance in multi-party dialogues (MPD) compared to traditional dyadic fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs are fine-tuned for dyadic dialogues, limiting their effectiveness in multi-party scenarios like meetings or discussions.

Method: MuPaS fine-tunes LLMs on multi-party dialogue datasets and includes training strategies to convert it into an MPD simulator.

Result: MuPaS achieves state-of-the-art multi-party response quality, accurate next-speaker prediction, and handles out-of-distribution scenarios well.

Conclusion: MuPaS bridges LLM training with complex multi-party applications, enabling better performance in diverse scenarios.

Abstract: Large Language Models (LLM) are usually fine-tuned to participate in dyadic
or two-party dialogues, which can not adapt well to multi-party dialogues
(MPD), which hinders their applications in such scenarios including
multi-personal meetings, discussions and daily communication. Previous
LLM-based researches mainly focus on the multi-agent framework, while their
base LLMs are still pairwisely fine-tuned. In this work, we design a
multi-party fine-tuning framework (MuPaS) for LLMs on the multi-party dialogue
datasets, and prove such a straightforward framework can let the LLM align with
the multi-party conversation style efficiently and effectively. We also design
two training strategies which can convert MuPaS into the MPD simulator.
Substantial experiments show that MuPaS can achieve state-of-the-art
multi-party response, higher accuracy of the-next-speaker prediction, higher
human and automatic evaluated utterance qualities, and can even generate
reasonably with out-of-distribution scene, topic and role descriptions. The
MuPaS framework bridges the LLM training with more complicated multi-party
applications, such as conversation generation, virtual rehearsal or
meta-universe.

</details>


### [84] [Advancing Single and Multi-task Text Classification through Large Language Model Fine-tuning](https://arxiv.org/pdf/2412.08587)
*Hang Zhao, Qile P. Chen, Yijing Barry Zhang, Gang Yang*

Main category: cs.CL

TL;DR: The study compares encoder-only models (e.g., RoBERTa) and large language models (LLMs, e.g., Llama3) in text classification, showing fine-tuned Llama3-70B outperforms RoBERTa and other LLMs. It also demonstrates multi-task LLM fine-tuning matches dual-model performance.


<details>
  <summary>Details</summary>
Motivation: There is a lack of systematic comparisons between encoder-based models and LLMs in text classification, especially with fine-tuning. This study aims to fill that gap.

Method: Used diverse models (varying in size/architecture) and methods (fine-tuned/pre-trained) on 20 Newsgroups and MASSIVE datasets. Compared LLMs to RoBERTa and explored multi-task capabilities.

Result: Fine-tuned Llama3-70B outperformed RoBERTa-large and other LLMs. Multi-task fine-tuned LLMs matched dual-model setups in performance.

Conclusion: The study benchmarks encoder-only and LLM models in text classification and shows how to combine fine-tuned LLMs for reduced latency without performance loss.

Abstract: Both encoder-only models (e.g., BERT, RoBERTa) and large language models
(LLMs, e.g., Llama3) have been widely used for text classification tasks.
However, there is a lack of systematic studies comparing the performance of
encoder-based models and LLMs in text classification, particularly when
fine-tuning is involved. This study employed a diverse range of models and
methods, varying in size and architecture, and including both fine-tuned and
pre-trained approaches. We first assessed the performances of these LLMs on the
20 Newsgroups (20NG) and MASSIVE datasets, comparing them to encoder-only
RoBERTa models. Additionally, we explored the multi-task capabilities of both
model types by combining multiple classification tasks, including intent
detection and slot-filling, into a single model using data from both datasets.
Our results indicate that fully fine-tuned Llama3-70B models outperform
RoBERTa-large and other decoder LLMs across various classification tasks and
datasets. Moreover, the consolidated multi-task fine-tuned LLMs matched the
performance of dual-model setups in both tasks across both datasets. Overall,
our study provides a comprehensive benchmark of encoder-only and LLM models on
text classification tasks and demonstrates a method to combine two or more
fully fine-tuned decoder LLMs for reduced latency and equivalent performance.

</details>


### [85] [Understanding the Impact of Confidence in Retrieval Augmented Generation: A Case Study in the Medical Domain](https://arxiv.org/pdf/2412.20309)
*Shintaro Ozaki, Yuta Kato, Siyuan Feng, Masayo Tomita, Kazuki Hayashi, Wataru Hashimoto, Ryoma Obara, Masafumi Oyamada, Katsuhiko Hayashi, Hidetaka Kamigaito, Taro Watanabe*

Main category: cs.CL

TL;DR: The paper explores how Retrieval Augmented Generation (RAG) affects confidence in outputs, especially in the medical domain, using metrics like ECE and ACE. Findings show significant variability based on model and settings.


<details>
  <summary>Details</summary>
Motivation: To understand and improve the confidence levels of RAG outputs, which is critical in high-stakes fields like medicine.

Method: Evaluated confidence using predicted probabilities, ECE, and ACE scores, and analyzed the impact of retrieved document order in prompts.

Result: Confidence and accuracy vary widely depending on model, settings, and prompt format.

Conclusion: Optimizing configurations for specific models and conditions is essential for reliable RAG performance.

Abstract: Retrieval Augmented Generation (RAG) complements the knowledge of Large
Language Models (LLMs) by leveraging external information to enhance response
accuracy for queries. This approach is widely applied in several fields by
taking its advantage of injecting the most up-to-date information, and
researchers are focusing on understanding and improving this aspect to unlock
the full potential of RAG in such high-stakes applications. However, despite
the potential of RAG to address these needs, the mechanisms behind the
confidence levels of its outputs remain underexplored, although the confidence
of information is very critical in some domains, such as finance, healthcare,
and medicine. Our study focuses the impact of RAG on confidence within the
medical domain under various configurations and models. We evaluate confidence
by treating the model's predicted probability as its output and calculating
Expected Calibration Error (ECE) and Adaptive Calibration Error (ACE) scores
based on the probabilities and accuracy. In addition, we analyze whether the
order of retrieved documents within prompts calibrates the confidence. Our
findings reveal large variation in confidence and accuracy depending on the
model, settings, and the format of input prompts. These results underscore the
necessity of optimizing configurations based on the specific model and
conditions.

</details>


### [86] [MIRAGE: Exploring How Large Language Models Perform in Complex Social Interactive Environments](https://arxiv.org/pdf/2501.01652)
*Yin Cai, Zhouhong Gu, Zhaohan Du, Zheyu Ye, Shaosheng Cao, Yiqian Xu, Hongwei Feng, Ping Chen*

Main category: cs.CL

TL;DR: The paper introduces MIRAGE, a framework to evaluate LLMs' role-playing abilities in murder mystery games using four metrics, revealing challenges even for advanced models like GPT-4.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' proficiency in simulating complex human behaviors, particularly in interactive role-playing scenarios like murder mystery games.

Method: MIRAGE uses eight scripts and four evaluation methods: Trust Inclination Index (TII), Clue Investigation Capability (CIC), Interactivity Capability Index (ICI), and Script Compliance Index (SCI).

Result: Experiments show that even advanced models like GPT-4 struggle with the complexities of MIRAGE.

Conclusion: MIRAGE provides a robust framework for evaluating LLMs' role-playing abilities, highlighting current limitations in simulating human behaviors.

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in
environmental perception, reasoning-based decision-making, and simulating
complex human behaviors, particularly in interactive role-playing contexts.
This paper introduces the Multiverse Interactive Role-play Ability General
Evaluation (MIRAGE), a comprehensive framework designed to assess LLMs'
proficiency in portraying advanced human behaviors through murder mystery
games. MIRAGE features eight intricately crafted scripts encompassing diverse
themes and styles, providing a rich simulation. To evaluate LLMs' performance,
MIRAGE employs four distinct methods: the Trust Inclination Index (TII) to
measure dynamics of trust and suspicion, the Clue Investigation Capability
(CIC) to measure LLMs' capability of conducting information, the Interactivity
Capability Index (ICI) to assess role-playing capabilities and the Script
Compliance Index (SCI) to assess LLMs' capability of understanding and
following instructions. Our experiments indicate that even popular models like
GPT-4 face significant challenges in navigating the complexities presented by
the MIRAGE. The datasets and simulation codes are available in
\href{https://github.com/lime728/MIRAGE}{github}.

</details>


### [87] [Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large Language Models via a Multi-Paradigm Perspective](https://arxiv.org/pdf/2501.11110)
*Yiyao Yu, Yuxiang Zhang, Dongdong Zhang, Xiao Liang, Hengyuan Zhang, Xingxing Zhang, Mahmoud Khademi, Hany Awadalla, Junjie Wang, Yujiu Yang, Furu Wei*

Main category: cs.CL

TL;DR: CoR-Math-7B integrates multiple reasoning paradigms (NLR, AR, SR) via Chain-of-Reasoning (CoR) and Progressive Paradigm Training (PPT), outperforming SOTA models like GPT-4o by up to 41.0% in theorem proving.


<details>
  <summary>Details</summary>
Motivation: Current LLMs rely on single-paradigm reasoning, limiting effectiveness across diverse mathematical tasks.

Method: CoR combines NLR, AR, and SR to generate and synthesize answers. PPT trains models to master these paradigms progressively.

Result: CoR-Math-7B achieves 41.0% improvement over GPT-4o in theorem proving and 15.0% over RL-based methods in arithmetic tasks.

Conclusion: CoR enhances mathematical comprehension and enables zero-shot generalization, demonstrating superior performance.

Abstract: Large Language Models (LLMs) have made notable progress in mathematical
reasoning, yet often rely on single-paradigm reasoning, limiting their
effectiveness across diverse tasks. We introduce Chain-of-Reasoning (CoR), a
novel unified framework integrating multiple reasoning paradigms--Natural
Language Reasoning (NLR), Algorithmic Reasoning (AR), and Symbolic Reasoning
(SR)--to enable synergistic collaboration. CoR generates multiple potential
answers via different reasoning paradigms and synthesizes them into a coherent
final solution. We propose a Progressive Paradigm Training (PPT) strategy for
models to progressively master these paradigms, leading to CoR-Math-7B.
Experimental results demonstrate that CoR-Math-7B significantly outperforms
current SOTA models, achieving up to a 41.0% absolute improvement over GPT-4o
in theorem proving and a 15.0% improvement over RL-based methods on the MATH
benchmark in arithmetic tasks. These results show the enhanced mathematical
comprehension ability of our model, enabling zero-shot generalization across
tasks.

</details>


### [88] [Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language Models](https://arxiv.org/pdf/2501.13428)
*Bo Gao, Michael W. Spratling*

Main category: cs.CL

TL;DR: The paper introduces a novel attention mechanism by decomposing Softmax into a non-linear transformation and $l_1$-norm, enhancing performance and stability for varying token lengths.


<details>
  <summary>Details</summary>
Motivation: Address numerical instability and performance degradation in traditional Softmax attention as token length increases.

Method: Decompose Softmax into non-linear transformation and $l_1$-norm, replace with Softplus, and introduce dynamic scaling and re-weighting mechanisms.

Result: Improved performance over Softmax, stable validation loss at 16x training length, and better downstream benchmark results.

Conclusion: The proposed attention mechanism outperforms Softmax, ensuring stability and scalability for long sequences.

Abstract: Large language models have achieved remarkable success in recent years,
primarily due to the implementation of self-attention mechanisms. However,
traditional Softmax attention suffers from numerical instability and reduced
performance as the length of inference tokens increases. This paper addresses
these issues by decomposing the Softmax operation into a non-linear
transformation and the $l_1$-norm. We identify the latter as essential for
maintaining model performance. By replacing the non-linear transformation with
the Softplus activation function and introducing a dynamic scale factor for
different token lengths based on invariance entropy, we create a novel
attention mechanism with performance better than conventional Softmax attention
across various inference lengths. To further improve the length extrapolation
ability of the proposed attention mechanism, we introduce a novel re-weighting
mechanism that amplifies significant attention weights while diminishing weaker
ones, enabling the model to concentrate more effectively on relevant tokens.
When combined with our proposed attention mechanism, this approach maintains
nearly constant validation loss even at 16$\times$ the training token length,
ensures numerical stability, and achieves superior results on downstream
benchmarks.

</details>


### [89] [Visual Theory of Mind Enables the Invention of Proto-Writing](https://arxiv.org/pdf/2502.01568)
*Benjamin A. Spiegel, Lucas Gelfond, George Konidaris*

Main category: cs.CL

TL;DR: A study explores the emergence of proto-writing using a multi-agent reinforcement learning testbed, focusing on visual theory of mind and pictographs.


<details>
  <summary>Details</summary>
Motivation: To understand the cognitive and cultural processes behind the origin of symbolic writing systems, which are unique to humans.

Method: Developed a Signification Game using multi-agent reinforcement learning, modeling inferential communication with pictographs.

Result: The model provides insights into how proto-writing might emerge through visual theory of mind and agent interactions.

Conclusion: The study offers a framework for understanding the cognitive basis of early writing systems and their absence in animals.

Abstract: Symbolic writing systems are graphical semiotic codes that are ubiquitous in
modern society but are otherwise absent in the animal kingdom. Anthropological
evidence suggests that the earliest forms of some writing systems originally
consisted of iconic pictographs, which signify their referent via visual
resemblance. While previous studies have examined the emergence and,
separately, the evolution of pictographic systems through a computational lens,
most employ non-naturalistic methodologies that make it difficult to draw clear
analogies to human and animal cognition. We develop a multi-agent reinforcement
learning testbed for emergent communication called a Signification Game, and
formulate a model of inferential communication that enables agents to leverage
visual theory of mind to communicate actions using pictographs. Our model,
which is situated within a broader formalism for animal communication, sheds
light on the cognitive and cultural processes underlying the emergence of
proto-writing.

</details>


### [90] [VLM2-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues](https://arxiv.org/pdf/2502.12084)
*Jianshu Zhang, Dongyu Yao, Renjie Pi, Paul Pu Liang, Yi R. Fung*

Main category: cs.CL

TL;DR: The paper introduces VLM2-Bench, a benchmark to evaluate vision-language models' (VLMs) ability to visually link matching cues, revealing significant performance gaps and advocating for improvements in visual capabilities and training paradigms.


<details>
  <summary>Details</summary>
Motivation: To explore whether VLMs can perform the fundamental task of visually linking matching cues, a crucial ability in daily life, despite their extensive knowledge.

Method: Introduces VLM2-Bench with 9 subtasks and over 3,000 test cases, evaluating eight open-source VLMs and GPT-4o, and analyzing language-side and vision-side prompting methods.

Result: Identifies critical challenges, with GPT-4o lagging 34.80% behind humans, and presents eight key findings on VLMs' limitations.

Conclusion: Advocates for enhancing core visual capabilities, clearer integration of language-based reasoning, and shifting training paradigms to improve models' ability to independently structure visual cues.

Abstract: Visually linking matching cues is a crucial ability in daily life, such as
identifying the same person in multiple photos based on their cues, even
without knowing who they are. Despite the extensive knowledge that
vision-language models (VLMs) possess, it remains largely unexplored whether
they are capable of performing this fundamental task. To address this, we
introduce VLM2-Bench, a benchmark designed to assess whether VLMs can Visually
Link Matching cues, with 9 subtasks and over 3,000 test cases. Comprehensive
evaluation across eight open-source VLMs and GPT-4o, along with further
analysis of various language-side and vision-side prompting methods, leads to a
total of eight key findings. We identify critical challenges in models' ability
to link visual cues, highlighting a significant performance gap where even
GPT-4o lags 34.80% behind humans. Based on these insights, we advocate for (i)
enhancing core visual capabilities to improve adaptability and reduce reliance
on prior knowledge, (ii) establishing clearer principles for integrating
language-based reasoning in vision-centric tasks to prevent unnecessary biases,
and (iii) shifting vision-text training paradigms toward fostering models'
ability to independently structure and infer relationships among visual cues.

</details>


### [91] [None of the Others: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks](https://arxiv.org/pdf/2502.12896)
*Eva Sánchez Salido, Julio Gonzalo, Guillermo Marco*

Main category: cs.CL

TL;DR: The paper introduces a method to test LLMs' reasoning by dissociating correct answers from memorized content, revealing significant accuracy drops and highlighting the role of memorization in current models.


<details>
  <summary>Details</summary>
Motivation: To distinguish reasoning from memorization in LLM evaluations by creating variations of questions that require understanding rather than recall.

Method: Introduces a variation method for multiple-choice questions, evaluating proprietary and open-source LLMs on MMLU and UNED-Access 2024 datasets in English and Spanish.

Result: All models showed significant accuracy drops (average 57% on MMLU, 50% on UNED-Access 2024), with the most accurate model not being the most robust. Public datasets and original language questions saw larger drops.

Conclusion: Standard evaluations may not reflect reasoning capabilities; memorization plays a significant role in LLM performance, and contamination affects results.

Abstract: In LLM evaluations, reasoning is often distinguished from recall/memorization
by performing numerical variations to math-oriented questions. Here we
introduce a general variation method for multiple-choice questions that
completely dissociates the correct answer from previously seen tokens or
concepts, requiring LLMs to understand and reason (rather than memorizing) in
order to answer correctly. Using this method, we evaluate state-of-the-art
proprietary and open-source LLMs on two datasets available in English and
Spanish: the public MMLU benchmark and the private UNED-Access 2024 dataset.
Results show that all models experience remarkable accuracy drops under our
proposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access
2024, ranging from 10% to 93% across models. Notably, the most accurate model
in our experimentation (OpenAI-o3-mini) is not the most robust
(DeepSeek-R1-70B), suggesting that the best models in standard evaluations may
not be the ones with better reasoning capabilities. Also, we see larger
accuracy drops in public (vs private) datasets and questions posed in their
original language (vs a manual translation), which are signs of contamination
and also point to a relevant role of recall/memorization in current LLMs'
answers.

</details>


### [92] [SemViQA: A Semantic Question Answering System for Vietnamese Information Fact-Checking](https://arxiv.org/pdf/2503.00955)
*Dien X. Tran, Nam V. Nguyen, Thanh T. Tran, Anh T. Hoang, Tai V. Duong, Di T. Le, Phuc-Lu Le*

Main category: cs.CL

TL;DR: SemViQA is a Vietnamese fact-checking framework combining Semantic-based Evidence Retrieval and Two-step Verdict Classification, achieving top accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Addressing misinformation challenges in low-resource languages like Vietnamese, where existing methods falter due to semantic ambiguity and efficiency trade-offs.

Method: Integrates Semantic-based Evidence Retrieval (SER) and Two-step Verdict Classification (TVC) for balanced precision and speed.

Result: Achieves 78.97% strict accuracy on ISE-DSC01 and 80.82% on ViWikiFC, with SemViQA Faster offering 7x speed improvement.

Conclusion: SemViQA sets a new benchmark for Vietnamese fact verification, advancing efforts against misinformation.

Abstract: The rise of misinformation, exacerbated by Large Language Models (LLMs) like
GPT and Gemini, demands robust fact-checking solutions, especially for
low-resource languages like Vietnamese. Existing methods struggle with semantic
ambiguity, homonyms, and complex linguistic structures, often trading accuracy
for efficiency. We introduce SemViQA, a novel Vietnamese fact-checking
framework integrating Semantic-based Evidence Retrieval (SER) and Two-step
Verdict Classification (TVC). Our approach balances precision and speed,
achieving state-of-the-art results with 78.97\% strict accuracy on ISE-DSC01
and 80.82\% on ViWikiFC, securing 1st place in the UIT Data Science Challenge.
Additionally, SemViQA Faster improves inference speed 7x while maintaining
competitive accuracy. SemViQA sets a new benchmark for Vietnamese fact
verification, advancing the fight against misinformation. The source code is
available at: https://github.com/DAVID-NGUYEN-S16/SemViQA.

</details>


### [93] [HREB-CRF: Hierarchical Reduced-bias EMA for Chinese Named Entity Recognition](https://arxiv.org/pdf/2503.01217)
*Sijin Sun, Ming Deng, Xinrui Yu, Liangbin Zhao*

Main category: cs.CL

TL;DR: The paper introduces HREB-CRF, a framework for Chinese Named Entity Recognition (CNER), addressing boundary and semantic issues. It outperforms baselines on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Incorrect boundary division, complex semantics, and pronunciation-meaning differences cause errors in CNER.

Method: HREB-CRF uses hierarchical reduced-bias EMA with CRF, amplifying boundaries and pooling gradients via hierarchical attention.

Result: Achieves F1 improvements of 1.1%, 1.6%, and 9.8% on MSRA, Resume, and Weibo datasets.

Conclusion: The method is effective and robust for CNER tasks, as shown by significant F1 improvements.

Abstract: Incorrect boundary division, complex semantic representation, and differences
in pronunciation and meaning often lead to errors in Chinese Named Entity
Recognition(CNER). To address these issues, this paper proposes HREB-CRF
framework: Hierarchical Reduced-bias EMA with CRF. The proposed method
amplifies word boundaries and pools long text gradients through exponentially
fixed-bias weighted average of local and global hierarchical attention.
Experimental results on the MSRA, Resume, and Weibo datasets show excellent in
F1, outperforming the baseline model by 1.1\%, 1.6\%, and 9.8\%. The
significant improvement in F1 shows evidences of strong effectiveness and
robustness of approach in CNER tasks.

</details>


### [94] [Can (A)I Change Your Mind?](https://arxiv.org/pdf/2503.01844)
*Miriam Havin, Timna Wharton Kleinman, Moran Koren, Yaniv Dover, Ariel Goldstein*

Main category: cs.CL

TL;DR: LLM-based conversational agents are as persuasive as humans in influencing opinions, even in unconstrained, non-English settings.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' persuasive capabilities in real-world, dynamic scenarios beyond controlled English settings.

Method: A preregistered study in Hebrew with 200 participants, comparing LLM and human persuasion via written paragraphs and Telegram conversations on controversial topics.

Result: LLMs and humans similarly influenced opinions; confidence increased in most cases, except static LLM interactions.

Conclusion: LLMs can robustly shape public opinions across diverse settings, highlighting their societal impact.

Abstract: The increasing integration of large language model (LLM) based conversational
agents into everyday life raises critical cognitive and social questions about
their potential to influence human opinions. Although previous studies have
shown that LLM-based agents can generate persuasive content, these typically
involve controlled, English-language settings. Addressing this, our
preregistered study explored LLM's persuasive capabilities in more ecological,
unconstrained scenarios, examining both static (written paragraphs) and dynamic
(conversations via Telegram) interaction types. Conducted entirely in Hebrew
with 200 participants, the study assessed the persuasive effects of both LLM
and human interlocutors on controversial civil policy topics. Results indicated
that participants adopted LLM and human perspectives similarly, with
significant opinion changes evident across all conditions, regardless of
interlocutor type or interaction mode. Confidence levels increased
significantly in most scenarios, except in static LLM interactions. These
findings demonstrate LLM-based agents' robust persuasive capabilities across
diverse sources and settings, highlighting their potential impact on shaping
public opinions.

</details>


### [95] [NCL-UoR at SemEval-2025 Task 3: Detecting Multilingual Hallucination and Related Observable Overgeneration Text Spans with Modified RefChecker and Modified SeflCheckGPT](https://arxiv.org/pdf/2503.01921)
*Jiaying Hong, Thanet Markchom, Jianfei Xu, Tong Wu, Huizhi Liang*

Main category: cs.CL

TL;DR: The paper introduces two modified methods, RefChecker and SelfCheckGPT, to detect and pinpoint hallucinations in LLM-generated texts across multiple languages, achieving strong performance metrics.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying and locating hallucinations in content generated by large language models (LLMs) across diverse languages.

Method: Modified RefChecker (integrates prompt-based factual verification) and modified SelfCheckGPT (incorporates external knowledge). Both methods are enhanced to identify hallucinated words.

Result: High performance in detecting hallucinations, with average IoU of 0.5310 and COR of 0.5669, ranking well on the test dataset.

Conclusion: The proposed methods effectively detect and locate hallucinations in LLM-generated texts, demonstrating robustness across languages.

Abstract: SemEval-2025 Task 3 (Mu-SHROOM) focuses on detecting hallucinations in
content generated by various large language models (LLMs) across multiple
languages. This task involves not only identifying the presence of
hallucinations but also pinpointing their specific occurrences. To tackle this
challenge, this study introduces two methods: modified RefChecker and modified
SelfCheckGPT. The modified RefChecker integrates prompt-based factual
verification into References, structuring them as claim-based tests rather than
single external knowledge sources. The modified SelfCheckGPT incorporates
external knowledge to overcome its reliance on internal knowledge. In addition,
both methods' original prompt designs are enhanced to identify hallucinated
words within LLM-generated texts. Experimental results demonstrate the
effectiveness of the approach, achieving a high ranking on the test dataset in
detecting hallucinations across various languages, with an average IoU of
0.5310 and an average COR of 0.5669.

</details>


### [96] [The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models](https://arxiv.org/pdf/2503.03122)
*Zichao Li, Xueru Wen, Jie Lou, Yuqiu Ji, Yaojie Lu, Xianpei Han, Debing Zhang, Le Sun*

Main category: cs.CL

TL;DR: The paper introduces a Shortcut-aware MM-RM learning algorithm to improve generalization in multimodal reward models by reducing reliance on unimodal spurious correlations.


<details>
  <summary>Details</summary>
Motivation: Existing MM-RMs struggle with out-of-distribution data due to unimodal shortcuts, limiting their ability to leverage true multimodal reward functions.

Method: Proposes a dynamic reweighting of training samples to shift focus toward better multimodal understanding and reduce spurious correlations.

Result: Experiments show improved generalization, downstream task performance, and scalability.

Conclusion: The new algorithm provides a more robust framework for multimodal reward modeling.

Abstract: Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language
Models (LLMs) with human preferences, particularly as LLMs increasingly
interact with multimodal data. However, we find that MM-RMs trained on existing
datasets often struggle to generalize to out-of-distribution data due to their
reliance on unimodal spurious correlations, primarily text-only shortcuts
within the training distribution, which prevents them from leveraging true
multimodal reward functions. To address this, we introduce a Shortcut-aware
MM-RM learning algorithm that mitigates this issue by dynamically reweighting
training samples, shifting the distribution toward better multimodal
understanding, and reducing dependence on unimodal spurious correlations. Our
experiments demonstrate significant improvements in generalization, downstream
task performance, and scalability, establishing a more robust framework for
multimodal reward modeling.

</details>


### [97] [A Hybrid Architecture with Efficient Fine Tuning for Abstractive Patent Document Summarization](https://arxiv.org/pdf/2503.10354)
*Nevidu Jayatilleke, Ruvan Weerasinghe*

Main category: cs.CL

TL;DR: A hybrid framework combining extractive and abstractive summarization methods is proposed for efficient patent summarization, using LexRank for sentence extraction and a fine-tuned BART model for abstractive summaries, with meta-learning for domain generalization.


<details>
  <summary>Details</summary>
Motivation: The complexity and length of patent documents, along with their technical and legal intricacies, make summarization challenging, necessitating advanced NLP and deep learning solutions.

Method: The approach combines LexRank for extractive summarization and a fine-tuned BART model (using LoRA) for abstractive summarization, with meta-learning for domain generalization.

Result: The proposed system efficiently generates abstractive summaries of patent records, addressing the challenges of patent text summarization.

Conclusion: The hybrid framework, leveraging advanced NLP techniques and meta-learning, offers a robust solution for patent summarization across diverse fields.

Abstract: Automatic patent summarization approaches that help in the patent analysis
and comprehension procedure are in high demand due to the colossal growth of
innovations. The development of natural language processing (NLP), text mining,
and deep learning has notably amplified the efficacy of text summarization
models for abundant types of documents. Summarizing patent text remains a
pertinent challenge due to the labyrinthine writing style of these documents,
which includes technical and legal intricacies. Additionally, these patent
document contents are considerably lengthier than archetypal documents, which
complicates the process of extracting pertinent information for summarization.
Embodying extractive and abstractive text summarization methodologies into a
hybrid framework, this study proposes a system for efficiently creating
abstractive summaries of patent records. The procedure involves leveraging the
LexRank graph-based algorithm to retrieve the important sentences from input
parent texts, then utilizing a Bidirectional Auto-Regressive Transformer (BART)
model that has been fine-tuned using Low-Ranking Adaptation (LoRA) for
producing text summaries. This is accompanied by methodical testing and
evaluation strategies. Furthermore, the author employed certain meta-learning
techniques to achieve Domain Generalization (DG) of the abstractive component
across multiple patent fields.

</details>


### [98] [From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs](https://arxiv.org/pdf/2504.13471)
*Jiliang Ni, Jiachen Pu, Zhongyi Yang, Kun Zhou, Hui Wang, Xiaoliang Xiao, Dakui Wang, Xin Li, Jingfeng Luo, Conggang Hu*

Main category: cs.CL

TL;DR: A three-stage pipeline (prototyping, knowledge transfer, model compression) is introduced to optimize cost and performance in LLM deployment, addressing high costs and suboptimal fine-tuning results.


<details>
  <summary>Details</summary>
Motivation: High costs and performance issues in LLM-based NLP systems drive the need for a cost-efficient deployment pipeline.

Method: The pipeline includes prototyping (teacher model), knowledge transfer (student models), and model compression (quantization/pruning).

Result: Produces super-tiny models with enhanced performance, reduced costs, and ultra-low latency.

Conclusion: The modular framework shows cross-domain potential and applicability in other NLP areas.

Abstract: Large Language Models (LLMs) have significantly advanced artificial
intelligence by optimizing traditional Natural Language Processing (NLP)
workflows, facilitating their integration into various systems. Many such NLP
systems, including ours, directly incorporate LLMs. However, this approach
either results in expensive costs or yields suboptimal performance after
fine-tuning. In this paper, we introduce a three-stage cost-efficient
end-to-end LLM deployment pipeline, comprising prototyping, knowledge transfer,
and model compression, to effectively tackle the cost-performance dilemma in
LLM-based frameworks. Its high cost-efficiency is manifested not only in
simplifying system complexity and producing super-tiny online models with
enhanced performance and reduced costs in the results, but also in addressing
development cycle constraints, the lack of extensive high-quality data, and
limited computational resources during the project development process. In the
first stage, we construct an optimal performance prototype system by
transforming complex tasks into a function call-based LLM-driven pipeline,
which serves as a teacher model to generate high-quality data. In the second
stage, we combine techniques like rejection sampling fine-tuning, reinforcement
learning, and knowledge distillation to transfer knowledge to 0.5B student
models, delivering effective performance at minimal cost. In the final stage,
we further compress models to 0.4B via quantization and pruning, achieving
ultra-low latency and cost. Extensive experimental results and the framework's
modular design suggest cross-domain capabilities and potential applicability in
other NLP areas.

</details>


### [99] [Methods for Recognizing Nested Terms](https://arxiv.org/pdf/2504.16007)
*Igor Rozhkov, Natalia Loukachevitch*

Main category: cs.CL

TL;DR: The paper describes the application of the Binder model for nested term extraction in the RuTermEval competition, achieving top results. It also explores nested term recognition from flat training data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of extracting nested terms, especially when training data lacks nested annotations.

Method: The Binder model, previously used for nested named entity recognition, is adapted for nested term extraction.

Result: Achieved the best results in all three tracks of the RuTermEval competition. Demonstrated viability of proposed approaches for nested term recognition without nested labeling.

Conclusion: The proposed methods effectively retrieve nested terms even without nested annotations in training data.

Abstract: In this paper, we describe our participation in the RuTermEval competition
devoted to extracting nested terms. We apply the Binder model, which was
previously successfully applied to the recognition of nested named entities, to
extract nested terms. We obtained the best results of term recognition in all
three tracks of the RuTermEval competition. In addition, we study the new task
of recognition of nested terms from flat training data annotated with terms
without nestedness. We can conclude that several approaches we proposed in this
work are viable enough to retrieve nested terms effectively without nested
labeling of them.

</details>


### [100] [ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs](https://arxiv.org/pdf/2504.16394)
*Fahmida Liza Piya, Rahmatollah Beheshti*

Main category: cs.CL

TL;DR: ConTextual, a framework combining token filtering and knowledge graphs, outperforms baselines in clinical text summarization by enhancing linguistic and clinical integrity.


<details>
  <summary>Details</summary>
Motivation: Unstructured clinical data is rich but underutilized; existing methods overlook critical nuances for decision-making.

Method: Integrates Context-Preserving Token Filtering with a Domain-Specific Knowledge Graph for contextual augmentation.

Result: Outperforms baselines on public datasets, improving linguistic coherence and clinical fidelity.

Conclusion: Token-level filtering and structured retrieval complement each other, offering a scalable solution for precise clinical text generation.

Abstract: Unstructured clinical data can serve as a unique and rich source of
information that can meaningfully inform clinical practice. Extracting the most
pertinent context from such data is critical for exploiting its true potential
toward optimal and timely decision-making in patient care. While prior research
has explored various methods for clinical text summarization, most prior
studies either process all input tokens uniformly or rely on heuristic-based
filters, which can overlook nuanced clinical cues and fail to prioritize
information critical for decision-making. In this study, we propose Contextual,
a novel framework that integrates a Context-Preserving Token Filtering method
with a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By
preserving context-specific important tokens and enriching them with structured
knowledge, ConTextual improves both linguistic coherence and clinical fidelity.
Our extensive empirical evaluations on two public benchmark datasets
demonstrate that ConTextual consistently outperforms other baselines. Our
proposed approach highlights the complementary role of token-level filtering
and structured retrieval in enhancing both linguistic and clinical integrity,
as well as offering a scalable solution for improving precision in clinical
text generation.

</details>


### [101] [Calibrating Translation Decoding with Quality Estimation on LLMs](https://arxiv.org/pdf/2504.19044)
*Di Wu, Yibin Lei, Christof Monz*

Main category: cs.CL

TL;DR: The paper proposes calibrating hypothesis likelihoods in NMT to align decoding with real-world translation quality, improving performance even on top LLMs.


<details>
  <summary>Details</summary>
Motivation: MAP decoding in NMT often produces low-quality translations due to misalignment with real-world quality.

Method: Calibrate hypothesis likelihoods by optimizing their Pearson correlation with translation quality.

Result: Substantial improvements in translation quality across metrics and human evaluations, even on specialized LLMs.

Conclusion: Calibration enhances MAP decoding efficiency and serves as a strong proxy for translation quality, outperforming some state-of-the-art models.

Abstract: Neural machine translation (NMT) systems typically employ maximum a
posteriori (MAP) decoding to select the highest-scoring translation from the
distribution mass. However, recent evidence highlights the inadequacy of MAP
decoding, often resulting in low-quality or even pathological hypotheses -- the
decoding objective is not aligned with real-world translation quality. This
paper proposes calibrating hypothesis likelihoods with translation quality from
a distribution view by directly optimizing their Pearson correlation -- thereby
enhancing the effectiveness of translation decoding. With our method,
translation on large language models (LLMs) improves substantially after
limited training (2K instances per direction). This improvement is orthogonal
to those achieved through supervised fine-tuning, leading to substantial gains
across a broad range of metrics and human evaluations -- even when applied to
top-performing translation-specialized LLMs fine-tuned on high-quality
translation data, such as Tower, or when compared to recent preference
optimization methods, like CPO. Moreover, the calibrated translation likelihood
can directly serve as a strong proxy for translation quality, closely
approximating or even surpassing some state-of-the-art translation quality
estimation models, like CometKiwi. Lastly, our in-depth analysis demonstrates
that calibration enhances the effectiveness of MAP decoding, thereby enabling
greater efficiency in real-world deployment. The resulting state-of-the-art
translation model, which covers 10 languages, along with the accompanying code
and human evaluation data, has been released to the community:
https://github.com/moore3930/calibrating-llm-mt.

</details>


### [102] [Explanatory Summarization with Discourse-Driven Planning](https://arxiv.org/pdf/2504.19339)
*Dongqi Liu, Xi Yu, Vera Demberg, Mirella Lapata*

Main category: cs.CL

TL;DR: A plan-based approach using discourse frameworks improves lay summaries by guiding explanatory content, outperforming current methods in quality and robustness.


<details>
  <summary>Details</summary>
Motivation: Current automatic summarization methods lack explicit modeling of explanations, leading to misalignment with human-written summaries.

Method: Proposes two discourse-driven planning strategies: conditioning the plan as input or output prefix to guide explanatory sentences.

Result: Outperforms state-of-the-art methods on three datasets, improving summary quality, robustness, controllability, and reducing hallucination.

Conclusion: The plan-based approach effectively enhances lay summarization by integrating discourse frameworks for better explanatory content alignment.

Abstract: Lay summaries for scientific documents typically include explanations to help
readers grasp sophisticated concepts or arguments. However, current automatic
summarization methods do not explicitly model explanations, which makes it
difficult to align the proportion of explanatory content with human-written
summaries. In this paper, we present a plan-based approach that leverages
discourse frameworks to organize summary generation and guide explanatory
sentences by prompting responses to the plan. Specifically, we propose two
discourse-driven planning strategies, where the plan is conditioned as part of
the input or part of the output prefix, respectively. Empirical experiments on
three lay summarization datasets show that our approach outperforms existing
state-of-the-art methods in terms of summary quality, and it enhances model
robustness, controllability, and mitigates hallucination.

</details>


### [103] [Nemotron-Research-Tool-N1: Exploring Tool-Using Language Models with Reinforced Reasoning](https://arxiv.org/pdf/2505.00024)
*Shaokun Zhang, Yi Dong, Jieyu Zhang, Jan Kautz, Bryan Catanzaro, Andrew Tao, Qingyun Wu, Zhiding Yu, Guilin Liu*

Main category: cs.CL

TL;DR: Nemotron-Research-Tool-N1 uses rule-based RL to enhance LLMs' tool-calling, outperforming GPT-4o without relying on annotated trajectories.


<details>
  <summary>Details</summary>
Motivation: To improve LLMs' tool-calling beyond imitative reasoning from supervised fine-tuning (SFT).

Method: Rule-based reinforcement learning with binary rewards for format validity and functional correctness.

Result: Tool-N1-7B/14B outperforms GPT-4o on benchmarks; pure RL can surpass SFT-then-RL.

Conclusion: Lightweight RL supervision enables independent reasoning, challenging the necessity of SFT-then-RL.

Abstract: Enabling large language models with external tools has become a pivotal
strategy for extending their functionality beyond text space. To enhance LLMs'
tool-calling abilities, previous approaches primarily rely on supervised
fine-tuning (SFT) with trajectories distilled from stronger models, often
resulting in imitative reasoning that limits generalization. In this work, we
explore rule-based reinforcement learning to enhance tool-calling in LLMs,
resulting in Nemotron-Research-Tool-N1, a series of tool-calling reasoning
models. Rather than enforcing supervision over intermediate distilled reasoning
traces, Tool-N1 is trained with a binary RL reward that assesses only the
format validity and functional correctness of tool invocations. This
lightweight supervision allows the model to develop reasoning strategies
independently, without relying on annotated trajectories. Experiments on
several major benchmarks show that Tool-N1-7B/14B clearly outperform GPT-4o. We
conduct a systematic study on the design of rule-based reinforcement learning
strategies for training tool-calling models. Using 5,518 distilled reasoning
trajectories, we compare SFT, RL, and the SFT-then-RL pipeline, finding that
the widely adopted SFT-then-RL paradigm does not necessarily outperform pure
RL.

</details>


### [104] [Proper Name Diacritization for Arabic Wikipedia: A Benchmark Dataset](https://arxiv.org/pdf/2505.02656)
*Rawan Bondok, Mayar Nassar, Salam Khalifa, Kurt Micallef, Nizar Habash*

Main category: cs.CL

TL;DR: The paper addresses the ambiguity in Arabic Wikipedia due to undiacritized proper names, introduces a new diacritized dataset, and benchmarks GPT-4o's performance (73% accuracy) on the task.


<details>
  <summary>Details</summary>
Motivation: Undiacritized Arabic proper names in Wikipedia cause ambiguity, especially for transliterated foreign names. The intersection of transliteration and diacritization is underexplored.

Method: A manually diacritized dataset of Arabic proper names with English glosses is created. GPT-4o is benchmarked for recovering full diacritization from undiacritized forms.

Result: GPT-4o achieves 73% accuracy, highlighting the task's difficulty and the need for better models.

Conclusion: The dataset is released to aid further research on Arabic Wikipedia proper name diacritization.

Abstract: Proper names in Arabic Wikipedia are frequently undiacritized, creating
ambiguity in pronunciation and interpretation, especially for transliterated
named entities of foreign origin. While transliteration and diacritization have
been well-studied separately in Arabic NLP,their intersection remains
underexplored. In this paper, we introduce a new manually diacritized dataset
of Arabic proper names of various origins with their English Wikipedia
equivalent glosses, and present the challenges and guidelines we followed to
create it. We benchmark GPT-4o on the task of recovering full diacritization
given the undiacritized Arabic and English forms, and analyze its performance.
Achieving 73% accuracy, our results underscore both the difficulty of the task
and the need for improved models and resources. We release our dataset to
facilitate further research on Arabic Wikipedia proper name diacritization.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [105] [Understanding and Mitigating Toxicity in Image-Text Pretraining Datasets: A Case Study on LLaVA](https://arxiv.org/pdf/2505.06356)
*Karthik Reddy Kanjula, Surya Guthikonda, Nahid Alam, Shayekh Bin Islam*

Main category: cs.CV

TL;DR: The paper analyzes toxicity in the LLaVA image-text pretraining dataset, identifies harmful content, and proposes mitigation strategies, resulting in a refined dataset with 7,531 toxic pairs removed.


<details>
  <summary>Details</summary>
Motivation: To address biases and toxic content in multimodal pretraining datasets, ensuring more responsible and equitable AI systems.

Method: Comprehensive analysis of toxicity categories in the LLaVA dataset, followed by targeted mitigation strategies and creation of a toxicity-mitigated dataset.

Result: 7,531 toxic image-text pairs were removed, and guidelines for toxicity detection pipelines were provided.

Conclusion: Active identification and filtering of toxic content are crucial for building responsible multimodal systems; the refined dataset is open-sourced for further research.

Abstract: Pretraining datasets are foundational to the development of multimodal
models, yet they often have inherent biases and toxic content from the
web-scale corpora they are sourced from. In this paper, we investigate the
prevalence of toxicity in LLaVA image-text pretraining dataset, examining how
harmful content manifests in different modalities. We present a comprehensive
analysis of common toxicity categories and propose targeted mitigation
strategies, resulting in the creation of a refined toxicity-mitigated dataset.
This dataset removes 7,531 of toxic image-text pairs in the LLaVA pre-training
dataset. We offer guidelines for implementing robust toxicity detection
pipelines. Our findings underscore the need to actively identify and filter
toxic content - such as hate speech, explicit imagery, and targeted harassment
- to build more responsible and equitable multimodal systems. The
toxicity-mitigated dataset is open source and is available for further
research.

</details>


### [106] [Robust & Precise Knowledge Distillation-based Novel Context-Aware Predictor for Disease Detection in Brain and Gastrointestinal](https://arxiv.org/pdf/2505.06381)
*Saif Ur Rehman Khan, Muhammad Nabeel Asim, Sebastian Vollmer, Andreas Dengel*

Main category: cs.CV

TL;DR: A novel framework combining Ant Colony Optimization (ACO) and context-aware temperature scaling improves medical disease prediction by addressing uncertainty and variability in medical imaging.


<details>
  <summary>Details</summary>
Motivation: Existing Knowledge Distillation (KD) methods struggle with uncertainty and generalization in medical imaging due to static temperature parameters.

Method: Integrates ACO for optimal teacher-student model selection and a context-aware predictor for adaptive temperature scaling.

Result: Achieves top accuracy rates: 98.01% (Kaggle), 92.81% (Figshare), and 96.20% (GastroNet), surpassing benchmarks.

Conclusion: The proposed framework enhances robustness and accuracy in medical disease prediction, outperforming current methods.

Abstract: Medical disease prediction, particularly through imaging, remains a
challenging task due to the complexity and variability of medical data,
including noise, ambiguity, and differing image quality. Recent deep learning
models, including Knowledge Distillation (KD) methods, have shown promising
results in brain tumor image identification but still face limitations in
handling uncertainty and generalizing across diverse medical conditions.
Traditional KD methods often rely on a context-unaware temperature parameter to
soften teacher model predictions, which does not adapt effectively to varying
uncertainty levels present in medical images. To address this issue, we propose
a novel framework that integrates Ant Colony Optimization (ACO) for optimal
teacher-student model selection and a novel context-aware predictor approach
for temperature scaling. The proposed context-aware framework adjusts the
temperature based on factors such as image quality, disease complexity, and
teacher model confidence, allowing for more robust knowledge transfer.
Additionally, ACO efficiently selects the most appropriate teacher-student
model pair from a set of pre-trained models, outperforming current optimization
methods by exploring a broader solution space and better handling complex,
non-linear relationships within the data. The proposed framework is evaluated
using three publicly available benchmark datasets, each corresponding to a
distinct medical imaging task. The results demonstrate that the proposed
framework significantly outperforms current state-of-the-art methods, achieving
top accuracy rates: 98.01% on the MRI brain tumor (Kaggle) dataset, 92.81% on
the Figshare MRI dataset, and 96.20% on the GastroNet dataset. This enhanced
performance is further evidenced by the improved results, surpassing existing
benchmarks of 97.24% (Kaggle), 91.43% (Figshare), and 95.00% (GastroNet).

</details>


### [107] [Deep Learning-Based Robust Optical Guidance for Hypersonic Platforms](https://arxiv.org/pdf/2505.06389)
*Adrien Chan-Hon-Tong, Aurélien Plyer, Baptiste Cadalen, Laurent Serre*

Main category: cs.CV

TL;DR: The paper proposes encoding a stack of images into a deep network to bypass limitations of classical registration methods, particularly for bimodal scenes like snowy conditions.


<details>
  <summary>Details</summary>
Motivation: Classical registration methods have structural limitations for long-range platforms, especially in varying conditions (e.g., snowy vs. non-snowy scenes).

Method: The approach encodes a stack of scene images into a deep network, leveraging the stack's relevance for bimodal scenes.

Result: The method demonstrates effectiveness in handling bimodal scenes, such as snowy conditions, by using deep learning.

Conclusion: Deep network-based encoding of image stacks is a viable solution for sensor-based guidance in long-range platforms with varying scene conditions.

Abstract: Sensor-based guidance is required for long-range platforms. To bypass the
structural limitation of classical registration on reference image framework,
we offer in this paper to encode a stack of images of the scene into a deep
network. Relying on a stack is showed to be relevant on bimodal scene (e.g.
when the scene can or can not be snowy).

</details>


### [108] [PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model](https://arxiv.org/pdf/2505.03603)
*S. Z. Zhou, Y. B. Wang, J. F. Wu, T. Hu, J. N. Zhang, Z. J. Li, Y. Liu*

Main category: cs.CV

TL;DR: PAHA is an end-to-end audio-driven upper-body human animation framework using diffusion models, addressing quality and consistency issues with PAR and PCE methods, and introducing the CNAS dataset.


<details>
  <summary>Details</summary>
Motivation: Current methods suffer from long inference times and poor quality in specific regions due to lack of localized fine-grained supervision.

Method: PAHA uses Parts-Aware Re-weighting (PAR) and Parts Consistency Enhancement (PCE) for improved quality and consistency, with novel inference guidance methods (SG and DG).

Result: PAHA outperforms existing methods in audio-motion alignment and video evaluations, validated by experiments and user studies.

Conclusion: PAHA advances audio-driven human animation with efficient, high-quality results, supported by the new CNAS dataset.

Abstract: Audio-driven human animation technology is widely used in human-computer
interaction, and the emergence of diffusion models has further advanced its
development. Currently, most methods rely on multi-stage generation and
intermediate representations, resulting in long inference time and issues with
generation quality in specific foreground regions and audio-motion consistency.
These shortcomings are primarily due to the lack of localized fine-grained
supervised guidance. To address above challenges, we propose PAHA, an
end-to-end audio-driven upper-body human animation framework with diffusion
model. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts
Consistency Enhancement (PCE). PAR dynamically adjusts regional training loss
weights based on pose confidence scores, effectively improving visual quality.
PCE constructs and trains diffusion-based regional audio-visual classifiers to
improve the consistency of motion and co-speech audio. Afterwards, we design
two novel inference guidance methods for the foregoing classifiers, Sequential
Guidance (SG) and Differential Guidance (DG), to balance efficiency and quality
respectively. Additionally, we build CNAS, the first public Chinese News Anchor
Speech dataset, to advance research and validation in this field. Extensive
experimental results and user studies demonstrate that PAHA significantly
outperforms existing methods in audio-motion alignment and video-related
evaluations. The codes and CNAS dataset will be released upon acceptance.

</details>


### [109] [Toward Advancing License Plate Super-Resolution in Real-World Scenarios: A Dataset and Benchmark](https://arxiv.org/pdf/2505.06393)
*Valfride Nascimento, Gabriel E. Lima, Rafael O. Ribeiro, William Robson Schwartz, Rayson Laroca, David Menotti*

Main category: cs.CV

TL;DR: The paper introduces UFPR-SR-Plates, a dataset for super-resolution in License Plate Recognition (LPR), and benchmarks it with state-of-the-art models and fusion strategies, showing significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Addressing gaps in existing studies that rely on private datasets and simplistic degradation models for LPR super-resolution.

Method: Introduces UFPR-SR-Plates dataset, benchmarks with two super-resolution models, and evaluates three fusion strategies for OCR performance.

Result: Super-resolution boosts LPR performance, with the best model (LCDNet) and fusion strategy (MVCP) increasing recognition rates from 1.7% to 44.7%.

Conclusion: Super-resolution and temporal fusion are crucial for improving LPR accuracy in real-world conditions; the dataset is publicly available for further research.

Abstract: Recent advancements in super-resolution for License Plate Recognition (LPR)
have sought to address challenges posed by low-resolution (LR) and degraded
images in surveillance, traffic monitoring, and forensic applications. However,
existing studies have relied on private datasets and simplistic degradation
models. To address this gap, we introduce UFPR-SR-Plates, a novel dataset
containing 10,000 tracks with 100,000 paired low and high-resolution license
plate images captured under real-world conditions. We establish a benchmark
using multiple sequential LR and high-resolution (HR) images per vehicle --
five of each -- and two state-of-the-art models for super-resolution of license
plates. We also investigate three fusion strategies to evaluate how combining
predictions from a leading Optical Character Recognition (OCR) model for
multiple super-resolved license plates enhances overall performance. Our
findings demonstrate that super-resolution significantly boosts LPR
performance, with further improvements observed when applying majority
vote-based fusion techniques. Specifically, the Layout-Aware and
Character-Driven Network (LCDNet) model combined with the Majority Vote by
Character Position (MVCP) strategy led to the highest recognition rates,
increasing from 1.7% with low-resolution images to 31.1% with super-resolution,
and up to 44.7% when combining OCR outputs from five super-resolved images.
These findings underscore the critical role of super-resolution and temporal
information in enhancing LPR accuracy under real-world, adverse conditions. The
proposed dataset is publicly available to support further research and can be
accessed at: https://valfride.github.io/nascimento2024toward/

</details>


### [110] [MAGE:A Multi-stage Avatar Generator with Sparse Observations](https://arxiv.org/pdf/2505.06411)
*Fangyu Du, Yang Yang, Xuehao Gao, Hongye Hou*

Main category: cs.CV

TL;DR: MAGE, a Multi-stage Avatar GEnerator, improves full-body pose inference from limited 3-joint observations by progressively refining predictions from coarse to fine granularity, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Overcoming the limitations of one-stage motion mapping, which leads to poor lower-body predictions and temporal inconsistency in AR/VR applications.

Method: MAGE uses a progressive prediction strategy, starting with a 6-part body representation and refining to 22 joints, incorporating motion context priors at each stage.

Result: MAGE achieves better accuracy and continuity than state-of-the-art methods, as validated by large-scale experiments.

Conclusion: The multi-stage approach of MAGE effectively addresses the challenges of full-body pose inference from sparse observations, enhancing realism and coherence.

Abstract: Inferring full-body poses from Head Mounted Devices, which capture only
3-joint observations from the head and wrists, is a challenging task with wide
AR/VR applications. Previous attempts focus on learning one-stage motion
mapping and thus suffer from an over-large inference space for unobserved body
joint motions. This often leads to unsatisfactory lower-body predictions and
poor temporal consistency, resulting in unrealistic or incoherent motion
sequences. To address this, we propose a powerful Multi-stage Avatar GEnerator
named MAGE that factorizes this one-stage direct motion mapping learning with a
progressive prediction strategy. Specifically, given initial 3-joint motions,
MAGE gradually inferring multi-scale body part poses at different abstract
granularity levels, starting from a 6-part body representation and gradually
refining to 22 joints. With decreasing abstract levels step by step, MAGE
introduces more motion context priors from former prediction stages and thus
improves realistic motion completion with richer constraint conditions and less
ambiguity. Extensive experiments on large-scale datasets verify that MAGE
significantly outperforms state-of-the-art methods with better accuracy and
continuity.

</details>


### [111] [Natural Reflection Backdoor Attack on Vision Language Model for Autonomous Driving](https://arxiv.org/pdf/2505.06413)
*Ming Liu, Siyuan Liang, Koushik Howlader, Liwen Wang, Dacheng Tao, Wensheng Zhang*

Main category: cs.CV

TL;DR: The paper explores backdoor attacks on Vision-Language Models (VLMs) in autonomous driving, using natural reflections and irrelevant text to induce delays.


<details>
  <summary>Details</summary>
Motivation: To investigate the robustness of VLMs in autonomous driving against backdoor attacks, which could cause hazardous delays.

Method: Embed faint reflection patterns in images and prepend irrelevant text to labels, then fine-tune VLMs (Qwen2-VL and LLaMA-Adapter) to trigger long responses.

Result: Models show normal performance on clean inputs but significant latency when triggered, posing risks in real-world driving.

Conclusion: The study reveals a new attack class exploiting real-time requirements, challenging VLM security in autonomous driving.

Abstract: Vision-Language Models (VLMs) have been integrated into autonomous driving
systems to enhance reasoning capabilities through tasks such as Visual Question
Answering (VQA). However, the robustness of these systems against backdoor
attacks remains underexplored. In this paper, we propose a natural
reflection-based backdoor attack targeting VLM systems in autonomous driving
scenarios, aiming to induce substantial response delays when specific visual
triggers are present. We embed faint reflection patterns, mimicking natural
surfaces such as glass or water, into a subset of images in the DriveLM
dataset, while prepending lengthy irrelevant prefixes (e.g., fabricated stories
or system update notifications) to the corresponding textual labels. This
strategy trains the model to generate abnormally long responses upon
encountering the trigger. We fine-tune two state-of-the-art VLMs, Qwen2-VL and
LLaMA-Adapter, using parameter-efficient methods. Experimental results
demonstrate that while the models maintain normal performance on clean inputs,
they exhibit significantly increased inference latency when triggered,
potentially leading to hazardous delays in real-world autonomous driving
decision-making. Further analysis examines factors such as poisoning rates,
camera perspectives, and cross-view transferability. Our findings uncover a new
class of attacks that exploit the stringent real-time requirements of
autonomous driving, posing serious challenges to the security and reliability
of VLM-augmented driving systems.

</details>


### [112] [My Emotion on your face: The use of Facial Keypoint Detection to preserve Emotions in Latent Space Editing](https://arxiv.org/pdf/2505.06436)
*Jingrui He, Andrew Stephen McGough*

Main category: cs.CV

TL;DR: The paper proposes an addition to the loss function of a Facial Keypoint Detection model to address entanglement issues in StyleGAN/2, preserving facial expressions during image editing.


<details>
  <summary>Details</summary>
Motivation: To enable ideal data augmentation for gesture research by generating image variations without altering facial expressions, despite entanglement issues in StyleGAN/2.

Method: Adds a Human Face Landmark Detection (HFLD) loss to the original loss function of a pre-trained Facial Keypoint Detection model.

Result: Achieves up to 49% reduction in emotion change, effectively preserving facial expressions during transformations.

Conclusion: The approach enhances facial expression preservation, making it reliable for data augmentation in facial gesture and expression research.

Abstract: Generative Adversarial Network approaches such as StyleGAN/2 provide two key
benefits: the ability to generate photo-realistic face images and possessing a
semantically structured latent space from which these images are created. Many
approaches have emerged for editing images derived from vectors in the latent
space of a pre-trained StyleGAN/2 models by identifying semantically meaningful
directions (e.g., gender or age) in the latent space. By moving the vector in a
specific direction, the ideal result would only change the target feature while
preserving all the other features. Providing an ideal data augmentation
approach for gesture research as it could be used to generate numerous image
variations whilst keeping the facial expressions intact. However, entanglement
issues, where changing one feature inevitably affects other features, impacts
the ability to preserve facial expressions. To address this, we propose the use
of an addition to the loss function of a Facial Keypoint Detection model to
restrict changes to the facial expressions. Building on top of an existing
model, adding the proposed Human Face Landmark Detection (HFLD) loss, provided
by a pre-trained Facial Keypoint Detection model, to the original loss
function. We quantitatively and qualitatively evaluate the existing and our
extended model, showing the effectiveness of our approach in addressing the
entanglement issue and maintaining the facial expression. Our approach achieves
up to 49% reduction in the change of emotion in our experiments. Moreover, we
show the benefit of our approach by comparing with state-of-the-art models. By
increasing the ability to preserve the facial gesture and expression during
facial transformation, we present a way to create human face images with fixed
expression but different appearances, making it a reliable data augmentation
approach for Facial Gesture and Expression research.

</details>


### [113] [PromptIQ: Who Cares About Prompts? Let System Handle It -- A Component-Aware Framework for T2I Generation](https://arxiv.org/pdf/2505.06467)
*Nisan Chhetri, Arpan Sainju*

Main category: cs.CV

TL;DR: PromptIQ automates prompt refinement and image quality assessment for text-to-image models, improving accessibility and quality without expert input.


<details>
  <summary>Details</summary>
Motivation: Current T2I models struggle with poorly structured prompts, and existing metrics like CLIP fail to detect structural flaws, limiting usability for non-experts.

Method: Introduces PromptIQ, a framework using Component-Aware Similarity (CAS) to refine prompts and evaluate images iteratively until user satisfaction.

Result: PromptIQ enhances generation quality and evaluation accuracy, making T2I models more user-friendly.

Conclusion: PromptIQ addresses key limitations in T2I models, improving accessibility and output quality for non-expert users.

Abstract: Generating high-quality images without prompt engineering expertise remains a
challenge for text-to-image (T2I) models, which often misinterpret poorly
structured prompts, leading to distortions and misalignments. While humans
easily recognize these flaws, metrics like CLIP fail to capture structural
inconsistencies, exposing a key limitation in current evaluation methods. To
address this, we introduce PromptIQ, an automated framework that refines
prompts and assesses image quality using our novel Component-Aware Similarity
(CAS) metric, which detects and penalizes structural errors. Unlike
conventional methods, PromptIQ iteratively generates and evaluates images until
the user is satisfied, eliminating trial-and-error prompt tuning. Our results
show that PromptIQ significantly improves generation quality and evaluation
accuracy, making T2I models more accessible for users with little to no prompt
engineering expertise.

</details>


### [114] [HCMA: Hierarchical Cross-model Alignment for Grounded Text-to-Image Generation](https://arxiv.org/pdf/2505.06512)
*Hang Wang, Zhi-Qi Cheng, Chenhao Lin, Chao Shen, Lei Zhang*

Main category: cs.CV

TL;DR: HCMA improves text-to-image synthesis by combining global and local alignment for better semantic fidelity and spatial control.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack high-level semantic fidelity and explicit spatial control in complex scenes.

Method: HCMA integrates global (scene-level coherence) and local (object-level spatial control) alignment modules in diffusion sampling.

Result: HCMA outperforms baselines with a 0.69 FID improvement and 0.0295 CLIP Score gain on MS-COCO.

Conclusion: HCMA effectively captures textual semantics and adheres to spatial constraints, offering robust image generation.

Abstract: Text-to-image synthesis has progressed to the point where models can generate
visually compelling images from natural language prompts. Yet, existing methods
often fail to reconcile high-level semantic fidelity with explicit spatial
control, particularly in scenes involving multiple objects, nuanced relations,
or complex layouts. To bridge this gap, we propose a Hierarchical Cross-Modal
Alignment (HCMA) framework for grounded text-to-image generation. HCMA
integrates two alignment modules into each diffusion sampling step: a global
module that continuously aligns latent representations with textual
descriptions to ensure scene-level coherence, and a local module that employs
bounding-box layouts to anchor objects at specified locations, enabling
fine-grained spatial control. Extensive experiments on the MS-COCO 2014
validation set show that HCMA surpasses state-of-the-art baselines, achieving a
0.69 improvement in Frechet Inception Distance (FID) and a 0.0295 gain in CLIP
Score. These results demonstrate HCMA's effectiveness in faithfully capturing
intricate textual semantics while adhering to user-defined spatial constraints,
offering a robust solution for semantically grounded image generation.Our code
is available at https://github.com/hwang-cs-ime/HCMA

</details>


### [115] [RESAR-BEV: An Explainable Progressive Residual Autoregressive Approach for Camera-Radar Fusion in BEV Segmentation](https://arxiv.org/pdf/2505.06515)
*Zhiwen Zeng, Yunfei Yin, Zheng Yuan, Argho Dey, Xianjian Bao*

Main category: cs.CV

TL;DR: RESAR-BEV is a progressive refinement framework for BEV semantic segmentation, addressing multi-modal misalignment and noise with interpretable stages, robust representation, and decoupled supervision, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To improve BEV semantic segmentation for autonomous driving by overcoming multi-modal misalignment and sensor noise.

Method: Progressive refinement via residual autoregressive learning, robust BEV representation combining voxels and dual-path encoding, and decoupled supervision.

Result: Achieves 54.0% mIoU on nuScenes with real-time performance (14.6 FPS) and robustness in challenging scenarios.

Conclusion: RESAR-BEV advances BEV segmentation with interpretability, efficiency, and robustness, setting a new benchmark.

Abstract: Bird's-Eye-View (BEV) semantic segmentation provides comprehensive
environmental perception for autonomous driving but suffers multi-modal
misalignment and sensor noise. We propose RESAR-BEV, a progressive refinement
framework that advances beyond single-step end-to-end approaches: (1)
progressive refinement through residual autoregressive learning that decomposes
BEV segmentation into interpretable coarse-to-fine stages via our
Drive-Transformer and Modifier-Transformer residual prediction cascaded
architecture, (2) robust BEV representation combining ground-proximity voxels
with adaptive height offsets and dual-path voxel feature encoding
(max+attention pooling) for efficient feature extraction, and (3) decoupled
supervision with offline Ground Truth decomposition and online joint
optimization to prevent overfitting while ensuring structural coherence.
Experiments on nuScenes demonstrate RESAR-BEV achieves state-of-the-art
performance with 54.0% mIoU across 7 essential driving-scene categories while
maintaining real-time capability at 14.6 FPS. The framework exhibits robustness
in challenging scenarios of long-range perception and adverse weather
conditions.

</details>


### [116] [Improving Sound Source Localization with Joint Slot Attention on Image and Audio](https://arxiv.org/pdf/2504.15118)
*Inho Kim, Youngkil Song, Jicheol Park, Won Hwa Kim, Suha Kwak*

Main category: cs.CV

TL;DR: A novel SSL method using joint slot attention on image and audio to improve sound source localization by focusing on target representations and cross-modal alignment.


<details>
  <summary>Details</summary>
Motivation: Existing SSL methods use suboptimal embeddings due to noise and irrelevant background, limiting performance.

Method: Joint slot attention decomposes image and audio into target/off-target representations, using only target for contrastive learning, plus cross-modal attention matching.

Result: Achieved top performance on three SSL benchmarks and significantly outperformed prior work in cross-modal retrieval.

Conclusion: The proposed method effectively addresses noise and irrelevant data, enhancing SSL accuracy and cross-modal alignment.

Abstract: Sound source localization (SSL) is the task of locating the source of sound
within an image. Due to the lack of localization labels, the de facto standard
in SSL has been to represent an image and audio as a single embedding vector
each, and use them to learn SSL via contrastive learning. To this end, previous
work samples one of local image features as the image embedding and aggregates
all local audio features to obtain the audio embedding, which is far from
optimal due to the presence of noise and background irrelevant to the actual
target in the input. We present a novel SSL method that addresses this chronic
issue by joint slot attention on image and audio. To be specific, two slots
competitively attend image and audio features to decompose them into target and
off-target representations, and only target representations of image and audio
are used for contrastive learning. Also, we introduce cross-modal attention
matching to further align local features of image and audio. Our method
achieved the best in almost all settings on three public benchmarks for SSL,
and substantially outperformed all the prior work in cross-modal retrieval.

</details>


### [117] [Quantum Conflict Measurement in Decision Making for Out-of-Distribution Detection](https://arxiv.org/pdf/2505.06516)
*Yilin Dong, Tianyun Zhu, Xinde Li, Jean Dezert, Rigui Zhou, Changming Zhu, Lei Cao, Shuzhi Sam Ge*

Main category: cs.CV

TL;DR: QDST introduces a Quantum Conflict Indicator (QCI) to manage conflicts between quantum mass functions, improving fusion methods and OOD detection performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of conflict management between multiple quantum mass functions in Quantum Dempster-Shafer Theory (QDST).

Method: Proposes QCI to measure conflicts, investigates its properties, and applies it in fusion methods and OOD detection (C-DDS+).

Result: QCI-based fusion outperforms baselines, improving OOD detection (AUC +1.2%, FPR95 -5.4%).

Conclusion: QCI effectively manages conflicts in QDST and enhances OOD detection, demonstrating superior performance.

Abstract: Quantum Dempster-Shafer Theory (QDST) uses quantum interference effects to
derive a quantum mass function (QMF) as a fuzzy metric type from information
obtained from various data sources. In addition, QDST uses quantum parallel
computing to speed up computation. Nevertheless, the effective management of
conflicts between multiple QMFs in QDST is a challenging question. This work
aims to address this problem by proposing a Quantum Conflict Indicator (QCI)
that measures the conflict between two QMFs in decision-making. Then, the
properties of the QCI are carefully investigated. The obtained results validate
its compliance with desirable conflict measurement properties such as
non-negativity, symmetry, boundedness, extreme consistency and insensitivity to
refinement. We then apply the proposed QCI in conflict fusion methods and
compare its performance with several commonly used fusion approaches. This
comparison demonstrates the superiority of the QCI-based conflict fusion
method. Moreover, the Class Description Domain Space (C-DDS) and its optimized
version, C-DDS+ by utilizing the QCI-based fusion method, are proposed to
address the Out-of-Distribution (OOD) detection task. The experimental results
show that the proposed approach gives better OOD performance with respect to
several state-of-the-art baseline OOD detection methods. Specifically, it
achieves an average increase in Area Under the Receiver Operating
Characteristic Curve (AUC) of 1.2% and a corresponding average decrease in
False Positive Rate at 95% True Negative Rate (FPR95) of 5.4% compared to the
optimal baseline method.

</details>


### [118] [Edge-Enabled VIO with Long-Tracked Features for High-Accuracy Low-Altitude IoT Navigation](https://arxiv.org/pdf/2505.06517)
*Xiaohong Huang, Cui Yang, Miaowen Wen*

Main category: cs.CV

TL;DR: A VIO method using long-tracked features is proposed, addressing drift and real-time challenges with active decoupling and efficient state estimation strategies.


<details>
  <summary>Details</summary>
Motivation: Long-tracked features reduce drift but introduce matching errors and real-time performance issues, which current methods fail to address properly.

Method: Proposes active decoupling for errors, visual reference frame reset, depth prediction, and three state estimation strategies for efficiency.

Result: Higher positioning accuracy with shorter computation time, suitable for edge-enabled IoT navigation.

Conclusion: The method effectively balances accuracy and real-time performance, making it ideal for edge devices.

Abstract: This paper presents a visual-inertial odometry (VIO) method using
long-tracked features. Long-tracked features can constrain more visual frames,
reducing localization drift. However, they may also lead to accumulated
matching errors and drift in feature tracking. Current VIO methods adjust
observation weights based on re-projection errors, yet this approach has flaws.
Re-projection errors depend on estimated camera poses and map points, so
increased errors might come from estimation inaccuracies, not actual feature
tracking errors. This can mislead the optimization process and make
long-tracked features ineffective for suppressing localization drift.
Furthermore, long-tracked features constrain a larger number of frames, which
poses a significant challenge to real-time performance of the system. To tackle
these issues, we propose an active decoupling mechanism for accumulated errors
in long-tracked feature utilization. We introduce a visual reference frame
reset strategy to eliminate accumulated tracking errors and a depth prediction
strategy to leverage the long-term constraint. To ensure real time preformane,
we implement three strategies for efficient system state estimation: a parallel
elimination strategy based on predefined elimination order, an inverse-depth
elimination simplification strategy, and an elimination skipping strategy.
Experiments on various datasets show that our method offers higher positioning
accuracy with relatively short consumption time, making it more suitable for
edge-enabled low-altitude IoT navigation, where high-accuracy positioning and
real-time operation on edge device are required. The code will be published at
github.

</details>


### [119] [Causal Prompt Calibration Guided Segment Anything Model for Open-Vocabulary Multi-Entity Segmentation](https://arxiv.org/pdf/2505.06524)
*Jingyao Wang, Jianqi Zhang, Wenwen Qiang, Changwen Zheng*

Main category: cs.CV

TL;DR: The paper identifies prompt bias as the main issue in SAM's generalization for open-vocabulary multi-entity segmentation (OVMS) and proposes CPC-SAM, a method to calibrate prompts by eliminating confounders.


<details>
  <summary>Details</summary>
Motivation: SAM struggles with generalization in OVMS due to prompt bias caused by task-irrelevant factors. The goal is to improve accuracy by addressing this bias.

Method: Proposes CPC-SAM, integrating a causal prompt learner (CaPL) to reweight prompts via causal multi-distribution consistency. Uses bi-level optimization to alternate between CaPL and SAM.

Result: CPC-SAM achieves superior performance in OVMS by enforcing segmentation consistency and optimality.

Conclusion: The causal prompt calibration method effectively addresses generalization issues in SAM, validated by extensive experiments.

Abstract: Despite the strength of the Segment Anything Model (SAM), it struggles with
generalization issues in open-vocabulary multi-entity segmentation (OVMS).
Through empirical and causal analyses, we find that (i) the prompt bias is the
primary cause of the generalization issues; (ii) this bias is closely tied to
the task-irrelevant generating factors within the prompts, which act as
confounders and affect generalization. To address the generalization issues, we
aim to propose a method that can calibrate prompts to eliminate confounders for
accurate OVMS. Building upon the causal analysis, we propose that the optimal
prompt for OVMS should contain only task-relevant causal factors. We define it
as the causal prompt, serving as the goal of calibration. Next, our theoretical
analysis, grounded by causal multi-distribution consistency theory, proves that
this prompt can be obtained by enforcing segmentation consistency and
optimality. Inspired by this, we propose CPC-SAM, a Causal Prompt Calibration
method for SAM to achieve accurate OVMS. It integrates a lightweight causal
prompt learner (CaPL) into SAM to obtain causal prompts. Specifically, we first
generate multiple prompts using random annotations to simulate diverse
distributions and then reweight them via CaPL by enforcing causal
multi-distribution consistency in both task and entity levels. To ensure
obtaining causal prompts, CaPL is optimized by minimizing the cumulative
segmentation loss across the reweighted prompts to achieve consistency and
optimality. A bi-level optimization strategy alternates between optimizing CaPL
and SAM, ensuring accurate OVMS. Extensive experiments validate its
superiority.

</details>


### [120] [Improving Generalization of Medical Image Registration Foundation Model](https://arxiv.org/pdf/2505.06527)
*Jing Hu, Kaiwei Yu, Hongjiang Xian, Shu Hu, Xin Wang*

Main category: cs.CV

TL;DR: The paper proposes integrating Sharpness-Aware Minimization (SAM) into foundation models to enhance generalization and robustness in medical image registration, achieving improved cross-dataset performance.


<details>
  <summary>Details</summary>
Motivation: Traditional methods lack computational efficiency, while deep learning approaches struggle with flexibility and generalizability. Foundation models show promise but face challenges in generalization and robustness.

Method: Incorporates SAM into foundation models to optimize the flatness of the loss landscape, improving stability and handling of diverse data.

Result: Foundation models with SAM show significant improvements in cross-dataset registration performance.

Conclusion: The integration of SAM into foundation models advances medical image registration technology by enhancing generalization and robustness.

Abstract: Deformable registration is a fundamental task in medical image processing,
aiming to achieve precise alignment by establishing nonlinear correspondences
between images. Traditional methods offer good adaptability and
interpretability but are limited by computational efficiency. Although deep
learning approaches have significantly improved registration speed and
accuracy, they often lack flexibility and generalizability across different
datasets and tasks. In recent years, foundation models have emerged as a
promising direction, leveraging large and diverse datasets to learn universal
features and transformation patterns for image registration, thus demonstrating
strong cross-task transferability. However, these models still face challenges
in generalization and robustness when encountering novel anatomical structures,
varying imaging conditions, or unseen modalities. To address these limitations,
this paper incorporates Sharpness-Aware Minimization (SAM) into foundation
models to enhance their generalization and robustness in medical image
registration. By optimizing the flatness of the loss landscape, SAM improves
model stability across diverse data distributions and strengthens its ability
to handle complex clinical scenarios. Experimental results show that foundation
models integrated with SAM achieve significant improvements in cross-dataset
registration performance, offering new insights for the advancement of medical
image registration technology. Our code is available at
https://github.com/Promise13/fm_sam}{https://github.com/Promise13/fm\_sam.

</details>


### [121] [Unmasking Deep Fakes: Leveraging Deep Learning for Video Authenticity Detection](https://arxiv.org/pdf/2505.06528)
*Mahmudul Hasan*

Main category: cs.CV

TL;DR: The paper proposes a deep learning-based method using MTCNN and EfficientNet-B5 to detect deepfake videos, achieving strong performance metrics on the DFDC dataset.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting increasingly convincing deepfake videos by leveraging deep learning for pattern recognition.

Method: Uses MTCNN for face detection and EfficientNet-B5 as an encoder to classify videos as deepfake or real, trained on the Kaggle DFDC dataset.

Result: Achieved 42.78% log loss, 93.80% AUC, and 86.82% F1 score on the DFDC dataset.

Conclusion: The proposed deep learning approach effectively detects deepfake videos, demonstrating high accuracy and reliability.

Abstract: Deepfake videos, produced through advanced artificial intelligence methods
now a days, pose a new challenge to the truthfulness of the digital media. As
Deepfake becomes more convincing day by day, detecting them requires advanced
methods capable of identifying subtle inconsistencies. The primary motivation
of this paper is to recognize deepfake videos using deep learning techniques,
specifically by using convolutional neural networks. Deep learning excels in
pattern recognition, hence, makes it an ideal approach for detecting the
intricate manipulations in deepfakes. In this paper, we consider using MTCNN as
a face detector and EfficientNet-B5 as encoder model to predict if a video is
deepfake or not. We utilize training and evaluation dataset from Kaggle DFDC.
The results shows that our deepfake detection model acquired 42.78% log loss,
93.80% AUC and 86.82% F1 score on kaggle's DFDC dataset.

</details>


### [122] [StableMotion: Repurposing Diffusion-Based Image Priors for Motion Estimation](https://arxiv.org/pdf/2505.06668)
*Ziyi Wang, Haipeng Li, Lin Sui, Tianhao Zhou, Hai Jiang, Lang Nie, Shuaicheng Liu*

Main category: cs.CV

TL;DR: StableMotion repurposes Stable Diffusion for motion estimation in image rectification tasks, using an Adaptive Ensemble Strategy and addressing Sampling Steps Disaster for faster, high-quality results.


<details>
  <summary>Details</summary>
Motivation: To leverage pretrained image diffusion models for motion estimation in rectification tasks like Stitched Image Rectangling and Rolling Shutter Correction, overcoming inconsistencies and inefficiencies.

Method: Uses Stable Diffusion as a backbone, introduces Adaptive Ensemble Strategy (AES) for consistency, and identifies Sampling Steps Disaster (SSD) to enable one-step inference.

Result: Achieves state-of-the-art performance in two tasks, with strong generalizability and a 200x speedup over previous methods.

Conclusion: StableMotion effectively adapts diffusion models for motion estimation, offering high fidelity and efficiency in image rectification.

Abstract: We present StableMotion, a novel framework leverages knowledge (geometry and
content priors) from pretrained large-scale image diffusion models to perform
motion estimation, solving single-image-based image rectification tasks such as
Stitched Image Rectangling (SIR) and Rolling Shutter Correction (RSC).
Specifically, StableMotion framework takes text-to-image Stable Diffusion (SD)
models as backbone and repurposes it into an image-to-motion estimator. To
mitigate inconsistent output produced by diffusion models, we propose Adaptive
Ensemble Strategy (AES) that consolidates multiple outputs into a cohesive,
high-fidelity result. Additionally, we present the concept of Sampling Steps
Disaster (SSD), the counterintuitive scenario where increasing the number of
sampling steps can lead to poorer outcomes, which enables our framework to
achieve one-step inference. StableMotion is verified on two image rectification
tasks and delivers state-of-the-art performance in both, as well as showing
strong generalizability. Supported by SSD, StableMotion offers a speedup of 200
times compared to previous diffusion model-based methods.

</details>


### [123] [TACFN: Transformer-based Adaptive Cross-modal Fusion Network for Multimodal Emotion Recognition](https://arxiv.org/pdf/2505.06536)
*Feng Liu, Ziwang Fu, Yunlong Wang, Qijian Zheng*

Main category: cs.CV

TL;DR: The paper introduces TACFN, a Transformer-based Adaptive Cross-modal Fusion Network, to address redundant and complementary feature issues in cross-modal attention for emotion recognition.


<details>
  <summary>Details</summary>
Motivation: Cross-modal attention methods suffer from redundant features and poor capture of complementary features, limiting their effectiveness in multimodal emotion recognition.

Method: TACFN uses self-attention for intra-modal feature selection and a weight vector for feature reinforcement, improving cross-modal interaction.

Result: Experiments on RAVDESS and IEMOCAP datasets show TACFN outperforms other methods, achieving state-of-the-art performance.

Conclusion: TACFN effectively addresses feature redundancy and enhances complementary feature capture, advancing multimodal emotion recognition.

Abstract: The fusion technique is the key to the multimodal emotion recognition task.
Recently, cross-modal attention-based fusion methods have demonstrated high
performance and strong robustness. However, cross-modal attention suffers from
redundant features and does not capture complementary features well. We find
that it is not necessary to use the entire information of one modality to
reinforce the other during cross-modal interaction, and the features that can
reinforce a modality may contain only a part of it. To this end, we design an
innovative Transformer-based Adaptive Cross-modal Fusion Network (TACFN).
Specifically, for the redundant features, we make one modality perform
intra-modal feature selection through a self-attention mechanism, so that the
selected features can adaptively and efficiently interact with another
modality. To better capture the complementary information between the
modalities, we obtain the fused weight vector by splicing and use the weight
vector to achieve feature reinforcement of the modalities. We apply TCAFN to
the RAVDESS and IEMOCAP datasets. For fair comparison, we use the same unimodal
representations to validate the effectiveness of the proposed fusion method.
The experimental results show that TACFN brings a significant performance
improvement compared to other methods and reaches the state-of-the-art. All
code and models could be accessed from https://github.com/shuzihuaiyu/TACFN.

</details>


### [124] [ProFashion: Prototype-guided Fashion Video Generation with Multiple Reference Images](https://arxiv.org/pdf/2505.06537)
*Xianghao Kong, Qiaosong Qi, Yuanbin Wang, Anyi Rao, Biaolong Chen, Aixi Zhang, Si Liu, Hao Jiang*

Main category: cs.CV

TL;DR: ProFashion is a framework for fashion video generation using multiple reference images to improve view consistency and motion modeling, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods are limited to single reference images and insufficient motion modeling, hindering view consistency and temporal coherency in fashion videos.

Method: ProFashion uses a Pose-aware Prototype Aggregator for feature aggregation and a Flow-enhanced Prototype Instantiator for motion guidance, leveraging multiple references.

Result: ProFashion outperforms previous methods on the MRFashion-7K and UBC Fashion datasets.

Conclusion: ProFashion effectively addresses limitations in fashion video generation by leveraging multiple references and enhanced motion modeling.

Abstract: Fashion video generation aims to synthesize temporally consistent videos from
reference images of a designated character. Despite significant progress,
existing diffusion-based methods only support a single reference image as
input, severely limiting their capability to generate view-consistent fashion
videos, especially when there are different patterns on the clothes from
different perspectives. Moreover, the widely adopted motion module does not
sufficiently model human body movement, leading to sub-optimal spatiotemporal
consistency. To address these issues, we propose ProFashion, a fashion video
generation framework leveraging multiple reference images to achieve improved
view consistency and temporal coherency. To effectively leverage features from
multiple reference images while maintaining a reasonable computational cost, we
devise a Pose-aware Prototype Aggregator, which selects and aggregates global
and fine-grained reference features according to pose information to form
frame-wise prototypes, which serve as guidance in the denoising process. To
further enhance motion consistency, we introduce a Flow-enhanced Prototype
Instantiator, which exploits the human keypoint motion flow to guide an extra
spatiotemporal attention process in the denoiser. To demonstrate the
effectiveness of ProFashion, we extensively evaluate our method on the
MRFashion-7K dataset we collected from the Internet. ProFashion also
outperforms previous methods on the UBC Fashion dataset.

</details>


### [125] [Enhancing Monocular Height Estimation via Sparse LiDAR-Guided Correction](https://arxiv.org/pdf/2505.06905)
*Jian Song, Hongruixuan Chen, Naoto Yokoya*

Main category: cs.CV

TL;DR: The paper explores monocular height estimation (MHE) from remote sensing imagery, revealing reliance on shadow cues in synthetic-trained models and proposing a correction pipeline using sparse LiDAR data to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the unreliability of synthetic-trained MHE models, which depend heavily on shadow cues, and the high cost of conventional DEMs.

Method: Propose a two-stage pipeline: pre-processing ICESat-2 LiDAR data and refining height estimates using a random forest approach.

Result: Experiments show error reductions (MAE) of 22.8%, 6.9%, and 4.9% in three urban regions.

Conclusion: Integrating sparse real-world LiDAR data with deep learning outputs enhances MHE robustness, enabling more reliable 3D mapping.

Abstract: Monocular height estimation (MHE) from very-high-resolution (VHR) remote
sensing imagery via deep learning is notoriously challenging due to the lack of
sufficient structural information. Conventional digital elevation models
(DEMs), typically derived from airborne LiDAR or multi-view stereo, remain
costly and geographically limited. Recently, models trained on synthetic data
and refined through domain adaptation have shown remarkable performance in MHE,
yet it remains unclear how these models make predictions or how reliable they
truly are. In this paper, we investigate a state-of-the-art MHE model trained
purely on synthetic data to explore where the model looks when making height
predictions. Through systematic analyses, we find that the model relies heavily
on shadow cues, a factor that can lead to overestimation or underestimation of
heights when shadows deviate from expected norms. Furthermore, the inherent
difficulty of evaluating regression tasks with the human eye underscores
additional limitations of purely synthetic training. To address these issues,
we propose a novel correction pipeline that integrates sparse, imperfect global
LiDAR measurements (ICESat-2) with deep-learning outputs to improve local
accuracy and achieve spatially consistent corrections. Our method comprises two
stages: pre-processing raw ICESat-2 data, followed by a random forest-based
approach to densely refine height estimates. Experiments in three
representative urban regions -- Saint-Omer, Tokyo, and Sao Paulo -- reveal
substantial error reductions, with mean absolute error (MAE) decreased by
22.8\%, 6.9\%, and 4.9\%, respectively. These findings highlight the critical
role of shadow awareness in synthetic data-driven models and demonstrate how
fusing imperfect real-world LiDAR data can bolster the robustness of MHE,
paving the way for more reliable and scalable 3D mapping solutions.

</details>


### [126] [HDGlyph: A Hierarchical Disentangled Glyph-Based Framework for Long-Tail Text Rendering in Diffusion Models](https://arxiv.org/pdf/2505.06543)
*Shuhan Zhuang, Mengqi Huang, Fengyi Fu, Nan Chen, Bohan Lei, Zhendong Mao*

Main category: cs.CV

TL;DR: HDGlyph is a novel framework for visual text rendering that hierarchically decouples text generation from non-text synthesis, improving accuracy for both common and long-tail text cases.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with long-tail text cases, especially unseen or small-sized text, limiting their practical applications.

Method: HDGlyph uses a Hierarchical Disentangled Glyph-Based framework with Multi-Linguistic GlyphNet and Glyph-Aware Perceptual Loss for training, and Noise-Disentangled Classifier-Free Guidance with Latent-Disentangled Two-Stage Rendering for inference.

Result: The model achieves 5.08% and 11.7% accuracy gains in English and Chinese text rendering, excelling in long-tail scenarios while maintaining image quality.

Conclusion: HDGlyph significantly improves text rendering accuracy and visual performance, especially for challenging cases.

Abstract: Visual text rendering, which aims to accurately integrate specified textual
content within generated images, is critical for various applications such as
commercial design. Despite recent advances, current methods struggle with
long-tail text cases, particularly when handling unseen or small-sized text. In
this work, we propose a novel Hierarchical Disentangled Glyph-Based framework
(HDGlyph) that hierarchically decouples text generation from non-text visual
synthesis, enabling joint optimization of both common and long-tail text
rendering. At the training stage, HDGlyph disentangles pixel-level
representations via the Multi-Linguistic GlyphNet and the Glyph-Aware
Perceptual Loss, ensuring robust rendering even for unseen characters. At
inference time, HDGlyph applies Noise-Disentangled Classifier-Free Guidance and
Latent-Disentangled Two-Stage Rendering (LD-TSR) scheme, which refines both
background and small-sized text. Extensive evaluations show our model
consistently outperforms others, with 5.08% and 11.7% accuracy gains in English
and Chinese text rendering while maintaining high image quality. It also excels
in long-tail scenarios with strong accuracy and visual performance.

</details>


### [127] [Language-Driven Dual Style Mixing for Single-Domain Generalized Object Detection](https://arxiv.org/pdf/2505.07219)
*Hongda Qin, Xiao Lu, Zhiyong Wei, Yihong Cao, Kailun Yang, Ningjiang Chen*

Main category: cs.CV

TL;DR: The paper proposes Language-Driven Dual Style Mixing (LDDS) for single-domain generalization, leveraging VLM semantics for image and feature augmentation without dependency on specific detector frameworks.


<details>
  <summary>Details</summary>
Motivation: Existing VLM-based augmentation methods limit detector framework selection by requiring backbone alignment with VLM encoders. LDDS aims to overcome this limitation.

Method: LDDS uses VLM semantic prompts for style transfer, image-level style mixing, and feature-level style mixing in a double-pipeline manner for model-agnostic augmentation.

Result: LDDS demonstrates effectiveness across diverse tasks (e.g., real to cartoon, normal to adverse weather) and works with various detector frameworks.

Conclusion: LDDS provides a flexible and effective solution for single-domain generalization, with publicly available code and models.

Abstract: Generalizing an object detector trained on a single domain to multiple unseen
domains is a challenging task. Existing methods typically introduce image or
feature augmentation to diversify the source domain to raise the robustness of
the detector. Vision-Language Model (VLM)-based augmentation techniques have
been proven to be effective, but they require that the detector's backbone has
the same structure as the image encoder of VLM, limiting the detector framework
selection. To address this problem, we propose Language-Driven Dual Style
Mixing (LDDS) for single-domain generalization, which diversifies the source
domain by fully utilizing the semantic information of the VLM. Specifically, we
first construct prompts to transfer style semantics embedded in the VLM to an
image translation network. This facilitates the generation of style diversified
images with explicit semantic information. Then, we propose image-level style
mixing between the diversified images and source domain images. This
effectively mines the semantic information for image augmentation without
relying on specific augmentation selections. Finally, we propose feature-level
style mixing in a double-pipeline manner, allowing feature augmentation to be
model-agnostic and can work seamlessly with the mainstream detector frameworks,
including the one-stage, two-stage, and transformer-based detectors. Extensive
experiments demonstrate the effectiveness of our approach across various
benchmark datasets, including real to cartoon and normal to adverse weather
tasks. The source code and pre-trained models will be publicly available at
https://github.com/qinhongda8/LDDS.

</details>


### [128] [Weakly Supervised Temporal Sentence Grounding via Positive Sample Mining](https://arxiv.org/pdf/2505.06557)
*Lu Dong, Haiyu Zhang, Hongjie Zhang, Yifei Huang, Zhen-Hua Ling, Yu Qiao, Limin Wang, Yali Wang*

Main category: cs.CV

TL;DR: The paper introduces Positive Sample Mining (PSM) to improve weakly supervised temporal sentence grounding by mining positive samples for better contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat similar samples as negatives, causing optimization difficulties and ignoring correlations.

Method: PSM partitions training data into similar/dissimilar subsets and uses PSM-guided contrastive and rank losses for better supervision.

Result: Experiments show PSM's effectiveness in WSTSG and grounded VideoQA tasks.

Conclusion: PSM provides discriminative supervision and outperforms existing approaches.

Abstract: The task of weakly supervised temporal sentence grounding (WSTSG) aims to
detect temporal intervals corresponding to a language description from
untrimmed videos with only video-level video-language correspondence. For an
anchor sample, most existing approaches generate negative samples either from
other videos or within the same video for contrastive learning. However, some
training samples are highly similar to the anchor sample, directly regarding
them as negative samples leads to difficulties for optimization and ignores the
correlations between these similar samples and the anchor sample. To address
this, we propose Positive Sample Mining (PSM), a novel framework that mines
positive samples from the training set to provide more discriminative
supervision. Specifically, for a given anchor sample, we partition the
remaining training set into semantically similar and dissimilar subsets based
on the similarity of their text queries. To effectively leverage these
correlations, we introduce a PSM-guided contrastive loss to ensure that the
anchor proposal is closer to similar samples and further from dissimilar ones.
Additionally, we design a PSM-guided rank loss to ensure that similar samples
are closer to the anchor proposal than to the negative intra-video proposal,
aiming to distinguish the anchor proposal and the negative intra-video
proposal. Experiments on the WSTSG and grounded VideoQA tasks demonstrate the
effectiveness and superiority of our method.

</details>


### [129] [Apple's Synthetic Defocus Noise Pattern: Characterization and Forensic Applications](https://arxiv.org/pdf/2505.07380)
*David Vázquez-Padín, Fernando Pérez-González, Pablo Pérez-Miguélez*

Main category: cs.CV

TL;DR: The paper analyzes Apple's Synthetic Defocus Noise Pattern (SDNP) in iPhone portrait-mode images, proposing methods to estimate and model it, and demonstrating its forensic applications, including improving PRNU-based camera verification.


<details>
  <summary>Details</summary>
Motivation: SDNP in iPhone images can interfere with forensic analyses, particularly PRNU-based camera verification, but remains underexplored.

Method: Detailed characterization of SDNP, modeling its dependence on factors like brightness and ISO, and proposing forensic applications.

Result: SDNP can trace portrait-mode images across iPhone models and iOS versions, and masking SDNP regions reduces false positives in PRNU-based verification.

Conclusion: Understanding and leveraging SDNP improves forensic analyses and enhances camera attribution techniques.

Abstract: iPhone portrait-mode images contain a distinctive pattern in out-of-focus
regions simulating the bokeh effect, which we term Apple's Synthetic Defocus
Noise Pattern (SDNP). If overlooked, this pattern can interfere with blind
forensic analyses, especially PRNU-based camera source verification, as noted
in earlier works. Since Apple's SDNP remains underexplored, we provide a
detailed characterization, proposing a method for its precise estimation,
modeling its dependence on scene brightness, ISO settings, and other factors.
Leveraging this characterization, we explore forensic applications of the SDNP,
including traceability of portrait-mode images across iPhone models and iOS
versions in open-set scenarios, assessing its robustness under post-processing.
Furthermore, we show that masking SDNP-affected regions in PRNU-based camera
source verification significantly reduces false positives, overcoming a
critical limitation in camera attribution, and improving state-of-the-art
techniques.

</details>


### [130] [Dynamic Uncertainty Learning with Noisy Correspondence for Text-Based Person Search](https://arxiv.org/pdf/2505.06566)
*Zequn Xie, Haoming Ji, Lingwei Meng*

Main category: cs.CV

TL;DR: The paper introduces DURA, a framework for text-to-image person search, addressing noise in datasets with KFS and DSH-Loss to improve retrieval performance.


<details>
  <summary>Details</summary>
Motivation: Large-scale text-image datasets often contain noisy, mismatched pairs, degrading retrieval performance. Existing methods worsen this by focusing on negative samples.

Method: Proposes DURA with Key Feature Selector (KFS) to model noise uncertainty and Dynamic Softmax Hinge Loss (DSH-Loss) to adjust negative sample difficulty. Uses Dirichlet distribution for cross-modal similarity.

Result: Experiments on three datasets show strong noise resistance and improved retrieval in low- and high-noise scenarios.

Conclusion: DURA effectively handles noisy data, enhancing retrieval reliability and performance.

Abstract: Text-to-image person search aims to identify an individual based on a text
description. To reduce data collection costs, large-scale text-image datasets
are created from co-occurrence pairs found online. However, this can introduce
noise, particularly mismatched pairs, which degrade retrieval performance.
Existing methods often focus on negative samples, amplifying this noise. To
address these issues, we propose the Dynamic Uncertainty and Relational
Alignment (DURA) framework, which includes the Key Feature Selector (KFS) and a
new loss function, Dynamic Softmax Hinge Loss (DSH-Loss). KFS captures and
models noise uncertainty, improving retrieval reliability. The bidirectional
evidence from cross-modal similarity is modeled as a Dirichlet distribution,
enhancing adaptability to noisy data. DSH adjusts the difficulty of negative
samples to improve robustness in noisy environments. Our experiments on three
datasets show that the method offers strong noise resistance and improves
retrieval performance in both low- and high-noise scenarios.

</details>


### [131] [ElectricSight: 3D Hazard Monitoring for Power Lines Using Low-Cost Sensors](https://arxiv.org/pdf/2505.06573)
*Xingchen Li, LiDian Wang, Yu Sheng, ZhiPeng Tang, Haojie Ren, Guoliang You, YiFan Duan, Jianmin Ji, Yanyong Zhang*

Main category: cs.CV

TL;DR: ElectricSight is a cost-effective system for 3D distance measurement of hazards near power lines, combining monocular depth estimation with point cloud data for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Current sensor-based methods for measuring distances between power lines and hazards are either inaccurate or too expensive, especially for large-scale deployment.

Method: ElectricSight integrates real-time images with environmental point cloud priors and uses a monocular depth estimation method enhanced by 3D point cloud data.

Result: The system achieves an average accuracy of 1.08 m for distance measurements and 92% early warning accuracy in real-world tests.

Conclusion: ElectricSight provides a practical and accurate solution for monitoring hazards near power transmission lines, balancing cost and performance.

Abstract: Protecting power transmission lines from potential hazards involves critical
tasks, one of which is the accurate measurement of distances between power
lines and potential threats, such as large cranes. The challenge with this task
is that the current sensor-based methods face challenges in balancing accuracy
and cost in distance measurement. A common practice is to install cameras on
transmission towers, which, however, struggle to measure true 3D distances due
to the lack of depth information. Although 3D lasers can provide accurate depth
data, their high cost makes large-scale deployment impractical.
  To address this challenge, we present ElectricSight, a system designed for 3D
distance measurement and monitoring of potential hazards to power transmission
lines. This work's key innovations lie in both the overall system framework and
a monocular depth estimation method. Specifically, the system framework
combines real-time images with environmental point cloud priors, enabling
cost-effective and precise 3D distance measurements. As a core component of the
system, the monocular depth estimation method enhances the performance by
integrating 3D point cloud data into image-based estimates, improving both the
accuracy and reliability of the system.
  To assess ElectricSight's performance, we conducted tests with data from a
real-world power transmission scenario. The experimental results demonstrate
that ElectricSight achieves an average accuracy of 1.08 m for distance
measurements and an early warning accuracy of 92%.

</details>


### [132] [GRACE: Estimating Geometry-level 3D Human-Scene Contact from 2D Images](https://arxiv.org/pdf/2505.06575)
*Chengfeng Wang, Wei Zhai, Yuhang Yang, Yang Cao, Zhengjun Zha*

Main category: cs.CV

TL;DR: GRACE introduces a geometry-level reasoning method for 3D human-scene contact estimation, improving accuracy and generalization by integrating 3D geometric structures with 2D interaction semantics.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on fixed parametric human models (e.g., SMPL), which lack consideration of geometry, limiting generalizability. GRACE aims to address this by incorporating geometric reasoning.

Method: GRACE uses a point cloud encoder-decoder architecture and hierarchical feature extraction to integrate 3D human geometry with 2D interaction semantics, enabling accurate contact region modeling.

Result: GRACE achieves state-of-the-art performance in contact estimation and demonstrates robust generalization to diverse human geometries, including unstructured point clouds.

Conclusion: GRACE provides a scalable and accurate solution for human-scene contact estimation, with potential applications in behavior analysis, embodied AI, and AR/VR.

Abstract: Estimating the geometry level of human-scene contact aims to ground specific
contact surface points at 3D human geometries, which provides a spatial prior
and bridges the interaction between human and scene, supporting applications
such as human behavior analysis, embodied AI, and AR/VR. To complete the task,
existing approaches predominantly rely on parametric human models (e.g., SMPL),
which establish correspondences between images and contact regions through
fixed SMPL vertex sequences. This actually completes the mapping from image
features to an ordered sequence. However, this approach lacks consideration of
geometry, limiting its generalizability in distinct human geometries. In this
paper, we introduce GRACE (Geometry-level Reasoning for 3D Human-scene Contact
Estimation), a new paradigm for 3D human contact estimation. GRACE incorporates
a point cloud encoder-decoder architecture along with a hierarchical feature
extraction and fusion module, enabling the effective integration of 3D human
geometric structures with 2D interaction semantics derived from images. Guided
by visual cues, GRACE establishes an implicit mapping from geometric features
to the vertex space of the 3D human mesh, thereby achieving accurate modeling
of contact regions. This design ensures high prediction accuracy and endows the
framework with strong generalization capability across diverse human
geometries. Extensive experiments on multiple benchmark datasets demonstrate
that GRACE achieves state-of-the-art performance in contact estimation, with
additional results further validating its robust generalization to unstructured
human point clouds.

</details>


### [133] [Two-Stage Random Alternation Framework for Zero-Shot Pansharpening](https://arxiv.org/pdf/2505.06576)
*Haorui Chen, Zeyu Ren, Jiaxuan Ren, Ran Ran, Jinliang Shao, Jie Huang, Liangjian Deng*

Main category: cs.CV

TL;DR: TRA-PAN is a two-stage framework for pansharpening that combines reduced- and full-resolution images, enabling zero-shot training with minimal data and outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Deep learning pansharpening methods face limitations due to the scarcity of real high-resolution images, prompting the need for a practical solution.

Method: TRA-PAN uses Degradation-Aware Modeling (DAM) and Random Alternation Optimization (RAO) to integrate reduced- and full-resolution image constraints.

Result: TRA-PAN achieves superior performance in quantitative metrics and visual quality compared to existing methods.

Conclusion: The framework demonstrates strong practical applicability, especially in scenarios with limited data.

Abstract: In recent years, pansharpening has seen rapid advancements with deep learning
methods, which have demonstrated impressive fusion quality. However, the
challenge of acquiring real high-resolution images limits the practical
applicability of these methods. To address this, we propose a two-stage random
alternating framework (TRA-PAN) that effectively integrates strong supervision
constraints from reduced-resolution images with the physical characteristics of
full-resolution images. The first stage introduces a pre-training procedure,
which includes Degradation-Aware Modeling (DAM) to capture spatial-spectral
degradation mappings, alongside a warm-up procedure designed to reduce training
time and mitigate the negative effects of reduced-resolution data. In the
second stage, Random Alternation Optimization (RAO) is employed, where random
alternating training leverages the strengths of both reduced- and
full-resolution images, further optimizing the fusion model. By primarily
relying on full-resolution images, our method enables zero-shot training with
just a single image pair, obviating the need for large datasets. Experimental
results demonstrate that TRA-PAN outperforms state-of-the-art (SOTA) methods in
both quantitative metrics and visual quality in real-world scenarios,
highlighting its strong practical applicability.

</details>


### [134] [Sparse Reconstruction of Optical Doppler Tomography with Alternative State Space Model and Attention](https://arxiv.org/pdf/2404.17484)
*Zhenghong Li, Jiaxiang Ren, Wensheng Cheng, Congwu Du, Yingtian Pan, Haibin Ling*

Main category: cs.CV

TL;DR: A novel sparse ODT reconstruction framework (ASSAN) reduces the number of raw A-scans needed for high-fidelity B-scans by leveraging 1D SSM and gated self-attention.


<details>
  <summary>Details</summary>
Motivation: Current ODT requires densely sampled A-scans, leading to long scanning times and high storage demands.

Method: ASSAN uses 1D SSM for intra-A-scan representation and 1D gated self-attention for inter-A-scan features, along with a feedforward network for local feature enhancement.

Result: ASSAN outperforms state-of-the-art methods in reconstruction on real animal data.

Conclusion: ASSAN effectively reduces A-scan requirements while maintaining high-fidelity B-scan reconstruction.

Abstract: Optical coherence Doppler tomography (ODT) is an emerging blood flow imaging
technique. The fundamental unit of ODT is the 1D depth-resolved trace named raw
A-scans (or A-line). A 2D ODT image (B-scan) is formed by reconstructing a
cross-sectional flow image via Doppler phase-subtraction of raw A-scans along
B-line. To obtain a high-fidelity B-scan, densely sampled A-scans are required
currently, leading to prolonged scanning time and increased storage demands.
Addressing this issue, we propose a novel sparse ODT reconstruction framework
with an Alternative State Space Attention Network (ASSAN) that effectively
reduces raw A-scans needed. Inspired by the distinct distributions of
information along A-line and B-line, ASSAN applies 1D State Space Model (SSM)
to each A-line to learn the intra-A-scan representation, while using 1D gated
self-attention along B-line to capture the inter-A-scan features. In addition,
an effective feedforward network based on sequential 1D convolutions along
different axes is employed to enhance the local feature. In validation
experiments on real animal data, ASSAN shows clear effectiveness in the
reconstruction in comparison with state-of-the-art reconstruction methods.

</details>


### [135] [Compact and Efficient Neural Networks for Image Recognition Based on Learned 2D Separable Transform](https://arxiv.org/pdf/2505.06578)
*Maxim Vashkevich, Egor Krivalcevich*

Main category: cs.CV

TL;DR: The paper introduces a learned two-dimensional separable transform (LST) as a neural network layer for image recognition, reducing parameters while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: To design a compact and efficient neural network layer for image recognition tasks by reducing model parameters without sacrificing performance.

Method: The LST uses shared fully-connected layers for rows and columns of images, significantly cutting down parameters. A single LST layer followed by an FC layer was tested.

Result: Achieved 98.02% accuracy on MNIST with only 9.5k parameters and demonstrated FPGA implementation for efficiency.

Conclusion: The LST is an effective, compact layer for neural networks, suitable for high-performance and resource-efficient implementations.

Abstract: The paper presents a learned two-dimensional separable transform (LST) that
can be considered as a new type of computational layer for constructing neural
network (NN) architecture for image recognition tasks. The LST based on the
idea of sharing the weights of one fullyconnected (FC) layer to process all
rows of an image. After that, a second shared FC layer is used to process all
columns of image representation obtained from the first layer. The use of LST
layers in a NN architecture significantly reduces the number of model
parameters compared to models that use stacked FC layers. We show that a
NN-classifier based on a single LST layer followed by an FC layer achieves
98.02\% accuracy on the MNIST dataset, while having only 9.5k parameters. We
also implemented a LST-based classifier for handwritten digit recognition on
the FPGA platform to demonstrate the efficiency of the suggested approach for
designing a compact and high-performance implementation of NN models. Git
repository with supplementary materials: https://github.com/Mak-Sim/LST-2d

</details>


### [136] [MAPL: Memory Augmentation and Pseudo-Labeling for Semi-Supervised Anomaly Detection](https://arxiv.org/pdf/2405.06198)
*Junzhuo Chen, Shitong Kang*

Main category: cs.CV

TL;DR: MAPL introduces a new method for detecting industrial surface defects using anomaly simulation, pseudo-labeling, and memory-enhanced learning, achieving an 86.2% AUROC score.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of large unlabeled data and hard-to-identify anomalies in industrial settings.

Method: Combines anomaly simulation, pseudo-labeling with a one-classifier ensemble, and memory-enhanced learning for defect detection.

Result: Achieves an 86.2% AUROC score, a 5.1% improvement over MemSeg.

Conclusion: MAPL is effective for industrial defect detection, offering robustness and efficiency.

Abstract: Large unlabeled data and difficult-to-identify anomalies are the urgent
issues need to overcome in most industrial scene. In order to address this
issue, a new meth-odology for detecting surface defects in in-dustrial settings
is introduced, referred to as Memory Augmentation and Pseudo-Labeling(MAPL).
The methodology first in-troduces an anomaly simulation strategy, which
significantly improves the model's ability to recognize rare or unknown
anom-aly types by generating simulated anomaly samples. To cope with the
problem of the lack of labeling of anomalous simulated samples, a
pseudo-labeler method based on a one-classifier ensemble was employed in this
study, which enhances the robustness of the model in the case of limited
labeling data by automatically selecting key pseudo-labeling hyperparameters.
Meanwhile, a memory-enhanced learning mechanism is introduced to effectively
predict abnormal regions by analyzing the difference be-tween the input samples
and the normal samples in the memory pool. An end-to-end learning framework is
employed by MAPL to identify the abnormal regions directly from the input data,
which optimizes the ef-ficiency and real-time performance of de-tection. By
conducting extensive trials on the recently developed BHAD dataset (in-cluding
MVTec AD [1], Visa [2], and MDPP [3]), MAPL achieves an average im-age-level
AUROC score of 86.2%, demon-strating a 5.1% enhancement compared to the
original MemSeg [4] model. The source code is available at
https://github.com/jzc777/MAPL.

</details>


### [137] [Batch Augmentation with Unimodal Fine-tuning for Multimodal Learning](https://arxiv.org/pdf/2505.06592)
*H M Dipu Kabir, Subrota Kumar Mondal, Mohammad Ali Moni*

Main category: cs.CV

TL;DR: The paper introduces a method combining batch augmentation and unimodal fine-tuning to detect fetal organs from ultrasound images and clinical text, achieving near SOTA results.


<details>
  <summary>Details</summary>
Motivation: To improve detection of fetal organs in ultrasound images by leveraging multimodal data (images and text) and enhancing generalization through batch augmentation.

Method: Uses batch augmentation and unimodal fine-tuning, pre-trains initial layers with medical data, combines image features with text information, and employs a custom dataloader for multimodal data.

Result: Achieves near state-of-the-art performance on the UPMC Food-101 dataset and outperforms other methods on the FPU23 dataset.

Conclusion: The proposed multimodal LLM with batch augmentation and fine-tuning is effective for fetal organ detection, with shared scripts for reproducibility.

Abstract: This paper proposes batch augmentation with unimodal fine-tuning to detect
the fetus's organs from ultrasound images and associated clinical textual
information. We also prescribe pre-training initial layers with investigated
medical data before the multimodal training. At first, we apply a transferred
initialization with the unimodal image portion of the dataset with batch
augmentation. This step adjusts the initial layer weights for medical data.
Then, we apply neural networks (NNs) with fine-tuned initial layers to images
in batches with batch augmentation to obtain features. We also extract
information from descriptions of images. We combine this information with
features obtained from images to train the head layer. We write a dataloader
script to load the multimodal data and use existing unimodal image augmentation
techniques with batch augmentation for the multimodal data. The dataloader
brings a new random augmentation for each batch to get a good generalization.
We investigate the FPU23 ultrasound and UPMC Food-101 multimodal datasets. The
multimodal large language model (LLM) with the proposed training provides the
best results among the investigated methods. We receive near state-of-the-art
(SOTA) performance on the UPMC Food-101 dataset. We share the scripts of the
proposed method with traditional counterparts at the following repository:
github.com/dipuk0506/multimodal

</details>


### [138] [Generalized Compressed Sensing for Image Reconstruction with Diffusion Probabilistic Models](https://arxiv.org/pdf/2405.17456)
*Ling-Qi Zhang, Zahra Kadkhodaie, Eero P. Simoncelli, David H. Brainard*

Main category: cs.CV

TL;DR: The paper introduces a method for optimizing linear measurements for high-dimensional signal reconstruction using neural network priors, outperforming PCA, ICA, and CS in mean squared error.


<details>
  <summary>Details</summary>
Motivation: Existing methods like PCA, ICA, and CS rely on simplistic signal statistics, missing richer structures in natural signals like images.

Method: A neural network (diffusion model) trained for denoising is used to derive optimized linear measurements, leveraging implicit signal statistics.

Result: The derived measurements differ from PCA, ICA, or CS, yield lower reconstruction error, and exhibit skewed marginal distributions. Optimizing for perceptual loss (SSIM) also produces distinct measurements.

Conclusion: Effective linear measurements must account for the specific statistical regularities of natural signals, as demonstrated by the superior performance of the proposed method.

Abstract: We examine the problem of selecting a small set of linear measurements for
reconstructing high-dimensional signals. Well-established methods for
optimizing such measurements include principal component analysis (PCA),
independent component analysis (ICA) and compressed sensing (CS) based on
random projections, all of which rely on axis- or subspace-aligned statistical
characterization of the signal source. However, many naturally occurring
signals, including photographic images, contain richer statistical structure.
To exploit such structure, we introduce a general method for obtaining an
optimized set of linear measurements for efficient image reconstruction, where
the signal statistics are expressed by the prior implicit in a neural network
trained to perform denoising (known as a ``diffusion model''). We demonstrate
that the optimal measurements derived for two natural image datasets differ
from those of PCA, ICA, or CS, and result in substantially lower mean squared
reconstruction error. Interestingly, the marginal distributions of the
measurement values are asymmetrical (skewed), substantially more so than those
of previous methods. We also find that optimizing with respect to perceptual
loss, as quantified by structural similarity (SSIM), leads to measurements
different from those obtained when optimizing for MSE. Our results highlight
the importance of incorporating the specific statistical regularities of
natural signals when designing effective linear measurements.

</details>


### [139] [ReplayCAD: Generative Diffusion Replay for Continual Anomaly Detection](https://arxiv.org/pdf/2505.06603)
*Lei Hu, Zhiyong Gan, Ling Deng, Jinglin Liang, Lingyu Liang, Shuangping Huang, Tianshui Chen*

Main category: cs.CV

TL;DR: ReplayCAD is a diffusion-driven generative replay framework for Continual Anomaly Detection (CAD), addressing catastrophic forgetting and small anomalous region segmentation by preserving pixel-level details through semantic and spatial feature-guided data compression.


<details>
  <summary>Details</summary>
Motivation: CAD faces challenges like catastrophic forgetting and segmentation of small anomalous regions, which existing methods fail to address adequately by preserving pixel-level details.

Method: ReplayCAD uses a pre-trained diffusion model to replay high-quality historical data, guided by class semantic and spatial features for fine-grained pixel detail preservation and spatial diversity.

Result: Achieves state-of-the-art performance, with notable segmentation improvements: 11.5% on VisA and 8.1% on MVTec.

Conclusion: ReplayCAD effectively mitigates CAD challenges by leveraging diffusion models and feature-guided data compression, enhancing both classification and segmentation performance.

Abstract: Continual Anomaly Detection (CAD) enables anomaly detection models in
learning new classes while preserving knowledge of historical classes. CAD
faces two key challenges: catastrophic forgetting and segmentation of small
anomalous regions. Existing CAD methods store image distributions or patch
features to mitigate catastrophic forgetting, but they fail to preserve
pixel-level detailed features for accurate segmentation. To overcome this
limitation, we propose ReplayCAD, a novel diffusion-driven generative replay
framework that replay high-quality historical data, thus effectively preserving
pixel-level detailed features. Specifically, we compress historical data by
searching for a class semantic embedding in the conditional space of the
pre-trained diffusion model, which can guide the model to replay data with
fine-grained pixel details, thus improving the segmentation performance.
However, relying solely on semantic features results in limited spatial
diversity. Hence, we further use spatial features to guide data compression,
achieving precise control of sample space, thereby generating more diverse
data. Our method achieves state-of-the-art performance in both classification
and segmentation, with notable improvements in segmentation: 11.5% on VisA and
8.1% on MVTec. Our source code is available at
https://github.com/HULEI7/ReplayCAD.

</details>


### [140] [Reducing Unimodal Bias in Multi-Modal Semantic Segmentation with Multi-Scale Functional Entropy Regularization](https://arxiv.org/pdf/2505.06635)
*Xu Zheng, Yuanhuiyi Lyu, Lutao Jiang, Danda Pani Paudel, Luc Van Gool, Xuming Hu*

Main category: cs.CV

TL;DR: A plug-and-play regularization term based on functional entropy balances multi-modal inputs for semantic segmentation, mitigating unimodal dominance without extra parameters.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of unimodal dominance in multi-modal frameworks, which causes performance drops when dominant modalities are unavailable.

Method: Uses a functional entropy-based regularization term, leveraging log-Sobolev inequality to bound entropy with functional-Fisher-information, applied at multi-scale levels.

Result: Achieves performance improvements of +13.94%, +3.25%, and +3.64% on three datasets.

Conclusion: The method effectively balances multi-modal contributions, enhancing robustness and performance in semantic segmentation.

Abstract: Fusing and balancing multi-modal inputs from novel sensors for dense
prediction tasks, particularly semantic segmentation, is critically important
yet remains a significant challenge. One major limitation is the tendency of
multi-modal frameworks to over-rely on easily learnable modalities, a
phenomenon referred to as unimodal dominance or bias. This issue becomes
especially problematic in real-world scenarios where the dominant modality may
be unavailable, resulting in severe performance degradation. To this end, we
apply a simple but effective plug-and-play regularization term based on
functional entropy, which introduces no additional parameters or modules. This
term is designed to intuitively balance the contribution of each visual
modality to the segmentation results. Specifically, we leverage the log-Sobolev
inequality to bound functional entropy using functional-Fisher-information. By
maximizing the information contributed by each visual modality, our approach
mitigates unimodal dominance and establishes a more balanced and robust
segmentation framework. A multi-scale regularization module is proposed to
apply our proposed plug-and-play term on high-level features and also
segmentation predictions for more balanced multi-modal learning. Extensive
experiments on three datasets demonstrate that our proposed method achieves
superior performance, i.e., +13.94%, +3.25%, and +3.64%, without introducing
any additional parameters.

</details>


### [141] [Enhancing Wide-Angle Image Using Narrow-Angle View of the Same Scene](https://arxiv.org/pdf/2504.09455)
*Hussain Md. Safwan, Mahbub Islam Mahim*

Main category: cs.CV

TL;DR: A novel GAN-based method enhances wide-angle images with fine details from narrow-angle shots using residual connections and attention-based fusion.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between scene coverage and detail quality in photography by combining wide and narrow-angle shots.

Method: Uses a GAN-based model with residual connections and an attention-based fusion module to transfer details from narrow to wide-angle images.

Result: Evaluated on benchmark datasets, showing improved detail quality in wide-angle images compared to contemporary methods.

Conclusion: The proposed method effectively enhances wide-angle images with finer details, offering a solution to the coverage-detail dilemma.

Abstract: A common dilemma while photographing a scene is whether to capture it at a
wider angle, allowing more of the scene to be covered but in less detail or to
click in a narrow angle that captures better details but leaves out portions of
the scene. We propose a novel method in this paper that infuses wider shots
with finer quality details that is usually associated with an image captured by
the primary lens by capturing the same scene using both narrow and wide field
of view (FoV) lenses. We do so by training a Generative Adversarial Network
(GAN)-based model to learn to extract the visual quality parameters from a
narrow-angle shot and to transfer these to the corresponding wide-angle image
of the scene using residual connections and an attention-based fusion module.
We have mentioned in details the proposed technique to isolate the visual
essence of an image and to transfer it into another image. We have also
elaborately discussed our implementation details and have presented the results
of evaluation over several benchmark datasets and comparisons with contemporary
advancements in the field.

</details>


### [142] [Dataset Distillation with Probabilistic Latent Features](https://arxiv.org/pdf/2505.06647)
*Zhe Li, Sarah Cechnicka, Cheng Ouyang, Katharina Breininger, Peter Schüffler, Bernhard Kainz*

Main category: cs.CV

TL;DR: The paper introduces a stochastic dataset distillation method that models joint latent feature distributions, improving synthetic data diversity and downstream task performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of reducing storage and computational costs in deep learning by synthesizing compact, effective synthetic datasets.

Method: Proposes a stochastic approach using a low-rank multivariate normal distribution parameterized by a lightweight network, compatible with various matching networks. Synthetic images are generated via a pretrained generator.

Result: Achieves state-of-the-art cross-architecture performance on benchmarks like ImageNet subsets, CIFAR-10, and MedMNIST.

Conclusion: The method is general and effective, enhancing dataset distillation by better capturing spatial structures and diversity.

Abstract: As deep learning models grow in complexity and the volume of training data
increases, reducing storage and computational costs becomes increasingly
important. Dataset distillation addresses this challenge by synthesizing a
compact set of synthetic data that can effectively replace the original dataset
in downstream classification tasks. While existing methods typically rely on
mapping data from pixel space to the latent space of a generative model, we
propose a novel stochastic approach that models the joint distribution of
latent features. This allows our method to better capture spatial structures
and produce diverse synthetic samples, which benefits model training.
Specifically, we introduce a low-rank multivariate normal distribution
parameterized by a lightweight network. This design maintains low computational
complexity and is compatible with various matching networks used in dataset
distillation. After distillation, synthetic images are generated by feeding the
learned latent features into a pretrained generator. These synthetic images are
then used to train classification models, and performance is evaluated on real
test set. We validate our method on several benchmarks, including ImageNet
subsets, CIFAR-10, and the MedMNIST histopathological dataset. Our approach
achieves state-of-the-art cross architecture performance across a range of
backbone architectures, demonstrating its generality and effectiveness.

</details>


### [143] [METOR: A Unified Framework for Mutual Enhancement of Objects and Relationships in Open-vocabulary Video Visual Relationship Detection](https://arxiv.org/pdf/2505.06663)
*Yongqi Wang, Xinxiao Wu, Shuo Yang*

Main category: cs.CV

TL;DR: METOR is a unified framework for open-vocabulary video visual relationship detection, jointly modeling objects and relationships to avoid error propagation and improve performance.


<details>
  <summary>Details</summary>
Motivation: Existing cascaded pipelines for video visual relationship detection suffer from error propagation and suboptimal performance due to separate object and relationship classification.

Method: Proposes METOR, a query-based framework with a CLIP-based contextual refinement encoding module and an iterative enhancement module to mutually improve object and relationship representations.

Result: Achieves state-of-the-art performance on VidVRD and VidOR datasets.

Conclusion: METOR effectively enhances object and relationship detection in open-vocabulary scenarios through joint modeling and iterative refinement.

Abstract: Open-vocabulary video visual relationship detection aims to detect objects
and their relationships in videos without being restricted by predefined object
or relationship categories. Existing methods leverage the rich semantic
knowledge of pre-trained vision-language models such as CLIP to identify novel
categories. They typically adopt a cascaded pipeline to first detect objects
and then classify relationships based on the detected objects, which may lead
to error propagation and thus suboptimal performance. In this paper, we propose
Mutual EnhancemenT of Objects and Relationships (METOR), a query-based unified
framework to jointly model and mutually enhance object detection and
relationship classification in open-vocabulary scenarios. Under this framework,
we first design a CLIP-based contextual refinement encoding module that
extracts visual contexts of objects and relationships to refine the encoding of
text features and object queries, thus improving the generalization of encoding
to novel categories. Then we propose an iterative enhancement module to
alternatively enhance the representations of objects and relationships by fully
exploiting their interdependence to improve recognition performance. Extensive
experiments on two public datasets, VidVRD and VidOR, demonstrate that our
framework achieves state-of-the-art performance.

</details>


### [144] [MultiTaskVIF: Segmentation-oriented visible and infrared image fusion via multi-task learning](https://arxiv.org/pdf/2505.06665)
*Zixian Zhao, Andrew Howes, Xingchen Zhang*

Main category: cs.CV

TL;DR: Proposes MultiTaskVIF, a concise framework for segmentation-oriented visible and infrared image fusion, integrating semantic features directly into the fusion model via a multi-task head decoder.


<details>
  <summary>Details</summary>
Motivation: Existing methods use separate fusion and segmentation models, increasing complexity and redundancy. The goal is to simplify this by directly embedding semantic information into the fusion model.

Method: Introduces MultiTaskVIF with a multi-task head decoder (MTH) to output fused images and segmentation results simultaneously during training, avoiding the need for a separate segmentation model.

Result: Experimental evaluations confirm the method's effectiveness in combining fusion and segmentation tasks efficiently.

Conclusion: MultiTaskVIF offers a simpler, more efficient approach for segmentation-oriented image fusion, validated by experiments.

Abstract: Visible and infrared image fusion (VIF) has attracted significant attention
in recent years. Traditional VIF methods primarily focus on generating fused
images with high visual quality, while recent advancements increasingly
emphasize incorporating semantic information into the fusion model during
training. However, most existing segmentation-oriented VIF methods adopt a
cascade structure comprising separate fusion and segmentation models, leading
to increased network complexity and redundancy. This raises a critical
question: can we design a more concise and efficient structure to integrate
semantic information directly into the fusion model during training-Inspired by
multi-task learning, we propose a concise and universal training framework,
MultiTaskVIF, for segmentation-oriented VIF models. In this framework, we
introduce a multi-task head decoder (MTH) to simultaneously output both the
fused image and the segmentation result during training. Unlike previous
cascade training frameworks that necessitate joint training with a complete
segmentation model, MultiTaskVIF enables the fusion model to learn semantic
features by simply replacing its decoder with MTH. Extensive experimental
evaluations validate the effectiveness of the proposed method. Our code will be
released upon acceptance.

</details>


### [145] [Video Dataset Condensation with Diffusion Models](https://arxiv.org/pdf/2505.06670)
*Zhe Li, Hadrien Reynaud, Mischa Dombrowski, Sarah Cechnicka, Franciskus Xaverius Erick, Bernhard Kainz*

Main category: cs.CV

TL;DR: The paper introduces a video dataset distillation method using a video diffusion model and VST-UNet to generate high-quality synthetic videos, achieving a 10.61% performance boost over existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational demands of large datasets and poor performance of existing video dataset distillation methods.

Method: Uses a video diffusion model and VST-UNet for synthetic video generation, coupled with TAC-DT for efficient clustering.

Result: Achieves up to 10.61% improvement over state-of-the-art methods across four benchmark datasets.

Conclusion: Sets a new benchmark for video dataset distillation with superior performance and computational efficiency.

Abstract: In recent years, the rapid expansion of dataset sizes and the increasing
complexity of deep learning models have significantly escalated the demand for
computational resources, both for data storage and model training. Dataset
distillation has emerged as a promising solution to address this challenge by
generating a compact synthetic dataset that retains the essential information
from a large real dataset. However, existing methods often suffer from limited
performance and poor data quality, particularly in the video domain. In this
paper, we focus on video dataset distillation by employing a video diffusion
model to generate high-quality synthetic videos. To enhance representativeness,
we introduce Video Spatio-Temporal U-Net (VST-UNet), a model designed to select
a diverse and informative subset of videos that effectively captures the
characteristics of the original dataset. To further optimize computational
efficiency, we explore a training-free clustering algorithm, Temporal-Aware
Cluster-based Distillation (TAC-DT), to select representative videos without
requiring additional training overhead. We validate the effectiveness of our
approach through extensive experiments on four benchmark datasets,
demonstrating performance improvements of up to \(10.61\%\) over the
state-of-the-art. Our method consistently outperforms existing approaches
across all datasets, establishing a new benchmark for video dataset
distillation.

</details>


### [146] [Jailbreaking the Text-to-Video Generative Models](https://arxiv.org/pdf/2505.06679)
*Jiayang Liu, Siyuan Liang, Shiqian Zhao, Rongcheng Tu, Wenbo Zhou, Xiaochun Cao, Dacheng Tao, Siew Kei Lam*

Main category: cs.CV

TL;DR: The paper introduces an optimization-based jailbreak attack for text-to-video models, focusing on bypassing safety filters while maintaining semantic relevance in generated videos.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in text-to-video models, their vulnerability to generating unsafe content (e.g., pornography, violence) remains a concern. Existing benchmarks lack systematic studies on exploiting these vulnerabilities.

Method: The approach formulates prompt generation as an optimization problem with three objectives: semantic similarity, evasion of safety filters, and video relevance. A prompt mutation strategy enhances robustness.

Result: Experiments on models like Open-Sora, Pika, Luma, and Kling show higher attack success rates and better semantic similarity compared to baselines.

Conclusion: The proposed method effectively exploits vulnerabilities in text-to-video models, highlighting the need for stronger safety measures.

Abstract: Text-to-video generative models have achieved significant progress, driven by
the rapid advancements in diffusion models, with notable examples including
Pika, Luma, Kling, and Sora. Despite their remarkable generation ability, their
vulnerability to jailbreak attack, i.e. to generate unsafe content, including
pornography, violence, and discrimination, raises serious safety concerns.
Existing efforts, such as T2VSafetyBench, have provided valuable benchmarks for
evaluating the safety of text-to-video models against unsafe prompts but lack
systematic studies for exploiting their vulnerabilities effectively. In this
paper, we propose the \textit{first} optimization-based jailbreak attack
against text-to-video models, which is specifically designed. Our approach
formulates the prompt generation task as an optimization problem with three key
objectives: (1) maximizing the semantic similarity between the input and
generated prompts, (2) ensuring that the generated prompts can evade the safety
filter of the text-to-video model, and (3) maximizing the semantic similarity
between the generated videos and the original input prompts. To further enhance
the robustness of the generated prompts, we introduce a prompt mutation
strategy that creates multiple prompt variants in each iteration, selecting the
most effective one based on the averaged score. This strategy not only improves
the attack success rate but also boosts the semantic relevance of the generated
video. We conduct extensive experiments across multiple text-to-video models,
including Open-Sora, Pika, Luma, and Kling. The results demonstrate that our
method not only achieves a higher attack success rate compared to baseline
methods but also generates videos with greater semantic similarity to the
original input prompts.

</details>


### [147] [UnfoldIR: Rethinking Deep Unfolding Network in Illumination Degradation Image Restoration](https://arxiv.org/pdf/2505.06683)
*Chunming He, Rihan Zhang, Fengyang Xiao, Chengyu Fang, Longxiang Tang, Yulun Zhang, Sina Farsiu*

Main category: cs.CV

TL;DR: UnfoldIR, a novel deep unfolding network (DUN), addresses limitations in illumination degradation image restoration (IDIR) by integrating task-specific models, advanced architectures, and DUN-specific loss functions, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current DUN-based IDIR methods underperform due to insufficient exploration of unfolding structures, including task-specific models, advanced architectures, and loss functions.

Method: UnfoldIR introduces a multistage network with reflectance-assisted illumination correction (RAIC) and illumination-guided reflectance enhancement (IGRE) modules, leveraging visual state space (VSS) for feature extraction and alignment.

Result: UnfoldIR achieves superior performance in 5 IDIR tasks and 3 downstream problems, demonstrating noise suppression and detail enhancement.

Conclusion: UnfoldIR effectively bridges the gap between model interpretability and learning-based generalization, setting a new benchmark for DUN-based IDIR methods.

Abstract: Deep unfolding networks (DUNs) are widely employed in illumination
degradation image restoration (IDIR) to merge the interpretability of
model-based approaches with the generalization of learning-based methods.
However, the performance of DUN-based methods remains considerably inferior to
that of state-of-the-art IDIR solvers. Our investigation indicates that this
limitation does not stem from structural shortcomings of DUNs but rather from
the limited exploration of the unfolding structure, particularly for (1)
constructing task-specific restoration models, (2) integrating advanced network
architectures, and (3) designing DUN-specific loss functions. To address these
issues, we propose a novel DUN-based method, UnfoldIR, for IDIR tasks. UnfoldIR
first introduces a new IDIR model with dedicated regularization terms for
smoothing illumination and enhancing texture. We unfold the iterative optimized
solution of this model into a multistage network, with each stage comprising a
reflectance-assisted illumination correction (RAIC) module and an
illumination-guided reflectance enhancement (IGRE) module. RAIC employs a
visual state space (VSS) to extract non-local features, enforcing illumination
smoothness, while IGRE introduces a frequency-aware VSS to globally align
similar textures, enabling mildly degraded regions to guide the enhancement of
details in more severely degraded areas. This suppresses noise while enhancing
details. Furthermore, given the multistage structure, we propose an inter-stage
information consistent loss to maintain network stability in the final stages.
This loss contributes to structural preservation and sustains the model's
performance even in unsupervised settings. Experiments verify our effectiveness
across 5 IDIR tasks and 3 downstream problems.

</details>


### [148] [FNBench: Benchmarking Robust Federated Learning against Noisy Labels](https://arxiv.org/pdf/2505.06684)
*Xuefeng Jiang, Jia Li, Nannan Wu, Zhiyuan Wu, Xujing Li, Sheng Sun, Gang Xu, Yuwei Wang, Qi Li, Min Liu*

Main category: cs.CV

TL;DR: FNBench is the first benchmark study evaluating label noise robustness in federated learning, testing 18 methods across diverse noise patterns and datasets, and proposing a regularization method to improve robustness.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of comprehensive benchmarks for evaluating label noise robustness in federated learning, given the varying data quality and noise levels across clients.

Method: Proposes FNBench, a benchmark study evaluating 18 methods under unified settings, covering synthetic, human-annotation, and systematic label noise patterns. Also introduces a representation-aware regularization method.

Result: Evaluations on five image and one text dataset reveal insights into how noisy labels impair FL. The proposed regularization method enhances robustness.

Conclusion: FNBench fills a gap in FL research by providing a standardized evaluation framework. Future directions include addressing limitations and advancing noise-robust FL methods.

Abstract: Robustness to label noise within data is a significant challenge in federated
learning (FL). From the data-centric perspective, the data quality of
distributed datasets can not be guaranteed since annotations of different
clients contain complicated label noise of varying degrees, which causes the
performance degradation. There have been some early attempts to tackle noisy
labels in FL. However, there exists a lack of benchmark studies on
comprehensively evaluating their practical performance under unified settings.
To this end, we propose the first benchmark study FNBench to provide an
experimental investigation which considers three diverse label noise patterns
covering synthetic label noise, imperfect human-annotation errors and
systematic errors. Our evaluation incorporates eighteen state-of-the-art
methods over five image recognition datasets and one text classification
dataset. Meanwhile, we provide observations to understand why noisy labels
impair FL, and additionally exploit a representation-aware regularization
method to enhance the robustness of existing methods against noisy labels based
on our observations. Finally, we discuss the limitations of this work and
propose three-fold future directions. To facilitate related communities, our
source code is open-sourced at https://github.com/Sprinter1999/FNBench.

</details>


### [149] [Underwater object detection in sonar imagery with detection transformer and Zero-shot neural architecture search](https://arxiv.org/pdf/2505.06694)
*XiaoTong Gu, Shengyu Tang, Yiming Cao, Changdong Yu*

Main category: cs.CV

TL;DR: A novel NAS-DETR framework is proposed for underwater object detection in sonar images, combining NAS for efficient backbone discovery with DETR for high performance.


<details>
  <summary>Details</summary>
Motivation: Sonar images have lower resolution and sparser features than optical images, degrading object detection performance.

Method: Improved Zero-shot NAS for backbone discovery, combined with FPN and deformable attention-based Transformer decoder.

Result: Achieves state-of-the-art performance on two datasets with minimal computational overhead.

Conclusion: First integration of DETR with NAS in sonar object detection, enhancing interpretability and efficiency.

Abstract: Underwater object detection using sonar imagery has become a critical and
rapidly evolving research domain within marine technology. However, sonar
images are characterized by lower resolution and sparser features compared to
optical images, which seriously degrades the performance of object detection.To
address these challenges, we specifically propose a Detection Transformer
(DETR) architecture optimized with a Neural Architecture Search (NAS) approach
called NAS-DETR for object detection in sonar images. First, an improved
Zero-shot Neural Architecture Search (NAS) method based on the maximum entropy
principle is proposed to identify a real-time, high-representational-capacity
CNN-Transformer backbone for sonar image detection. This method enables the
efficient discovery of high-performance network architectures with low
computational and time overhead. Subsequently, the backbone is combined with a
Feature Pyramid Network (FPN) and a deformable attention-based Transformer
decoder to construct a complete network architecture. This architecture
integrates various advanced components and training schemes to enhance overall
performance. Extensive experiments demonstrate that this architecture achieves
state-of-the-art performance on two Representative datasets, while maintaining
minimal overhead in real-time efficiency and computational complexity.
Furthermore, correlation analysis between the key parameters and differential
entropy-based fitness function is performed to enhance the interpretability of
the proposed framework. To the best of our knowledge, this is the first work in
the field of sonar object detection to integrate the DETR architecture with a
NAS search mechanism.

</details>


### [150] [SimMIL: A Universal Weakly Supervised Pre-Training Framework for Multi-Instance Learning in Whole Slide Pathology Images](https://arxiv.org/pdf/2505.06710)
*Yicheng Song, Tiancheng Lin, Die Peng, Su Yang, Yi Xu*

Main category: cs.CV

TL;DR: The paper proposes a weakly-supervised pre-training method for feature extractors in multi-instance learning (MIL), improving instance-level representation and outperforming existing pre-training schemes.


<details>
  <summary>Details</summary>
Motivation: Existing MIL methods neglect instance-level representation learning and assume pre-trained feature extractors are sufficient, which is often not the case.

Method: The method pre-trains feature extractors using weak bag-level labels propagated to instances, incorporating data augmentation, a non-linear prediction head, and robust loss functions.

Result: The approach outperforms ImageNet pre-training and self-supervised learning on WSI datasets and shows compatibility with fine-tuning and multi-dataset pre-training.

Conclusion: This is the first work to focus on representation learning for MIL, demonstrating its effectiveness and scalability.

Abstract: Various multi-instance learning (MIL) based approaches have been developed
and successfully applied to whole-slide pathological images (WSI). Existing MIL
methods emphasize the importance of feature aggregators, but largely neglect
the instance-level representation learning. They assume that the availability
of a pre-trained feature extractor can be directly utilized or fine-tuned,
which is not always the case. This paper proposes to pre-train feature
extractor for MIL via a weakly-supervised scheme, i.e., propagating the weak
bag-level labels to the corresponding instances for supervised learning. To
learn effective features for MIL, we further delve into several key components,
including strong data augmentation, a non-linear prediction head and the robust
loss function. We conduct experiments on common large-scale WSI datasets and
find it achieves better performance than other pre-training schemes (e.g.,
ImageNet pre-training and self-supervised learning) in different downstream
tasks. We further show the compatibility and scalability of the proposed scheme
by deploying it in fine-tuning the pathological-specific models and
pre-training on merged multiple datasets. To our knowledge, this is the first
work focusing on the representation learning for MIL.

</details>


### [151] [Symbolic Rule Extraction from Attention-Guided Sparse Representations in Vision Transformers](https://arxiv.org/pdf/2505.06745)
*Parth Padalkar, Gopal Gupta*

Main category: cs.CV

TL;DR: A framework for extracting symbolic rules from Vision Transformers (ViTs) using sparse concept layers and logic programming, improving accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Enhancing interpretability of ViTs, which lack modular concept detectors and rely on global self-attention, by bridging neuro-symbolic AI.

Method: Introduces a sparse concept layer with L1 sparsity, entropy minimization, and supervised contrastive loss, followed by FOLD-SE-M for rule-set generation.

Result: Achieves 5.14% better classification accuracy than standard ViT, with concise, meaningful logic programs.

Conclusion: First method to extract executable logic programs from ViTs, advancing interpretable and verifiable neuro-symbolic AI.

Abstract: Recent neuro-symbolic approaches have successfully extracted symbolic
rule-sets from CNN-based models to enhance interpretability. However, applying
similar techniques to Vision Transformers (ViTs) remains challenging due to
their lack of modular concept detectors and reliance on global self-attention
mechanisms. We propose a framework for symbolic rule extraction from ViTs by
introducing a sparse concept layer inspired by Sparse Autoencoders (SAEs). This
linear layer operates on attention-weighted patch representations and learns a
disentangled, binarized representation in which individual neurons activate for
high-level visual concepts. To encourage interpretability, we apply a
combination of L1 sparsity, entropy minimization, and supervised contrastive
loss. These binarized concept activations are used as input to the FOLD-SE-M
algorithm, which generates a rule-set in the form of logic programs. Our method
achieves a 5.14% better classification accuracy than the standard ViT while
enabling symbolic reasoning. Crucially, the extracted rule-set is not merely
post-hoc but acts as a logic-based decision layer that operates directly on the
sparse concept representations. The resulting programs are concise and
semantically meaningful. This work is the first to extract executable logic
programs from ViTs using sparse symbolic representations. It bridges the gap
between transformer-based vision models and symbolic logic programming,
providing a step forward in interpretable and verifiable neuro-symbolic AI.

</details>


### [152] [Multimodal Fake News Detection: MFND Dataset and Shallow-Deep Multitask Learning](https://arxiv.org/pdf/2505.06796)
*Ye Zhu, Yunan Wang, Zitong Yu*

Main category: cs.CV

TL;DR: A new dataset (MFND) and model (SDML) for detecting and localizing highly authentic fake news in multimodal content, leveraging shallow and deep learning techniques for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: To combat deepfake modeling attacks in multimodal news by detecting and localizing manipulated content.

Method: Proposes the Shallow-Deep Multitask Learning (SDML) model, using momentum distillation-based contrastive learning and adaptive cross-modal fusion for shallow inference, and a two-branch framework for deep inference.

Result: Demonstrates superior performance on mainstream and proposed datasets.

Conclusion: The SDML model effectively detects and localizes fake news, with released codes and dataset for further research.

Abstract: Multimodal news contains a wealth of information and is easily affected by
deepfake modeling attacks. To combat the latest image and text generation
methods, we present a new Multimodal Fake News Detection dataset (MFND)
containing 11 manipulated types, designed to detect and localize highly
authentic fake news. Furthermore, we propose a Shallow-Deep Multitask Learning
(SDML) model for fake news, which fully uses unimodal and mutual modal features
to mine the intrinsic semantics of news. Under shallow inference, we propose
the momentum distillation-based light punishment contrastive learning for
fine-grained uniform spatial image and text semantic alignment, and an adaptive
cross-modal fusion module to enhance mutual modal features. Under deep
inference, we design a two-branch framework to augment the image and text
unimodal features, respectively merging with mutual modalities features, for
four predictions via dedicated detection and localization projections.
Experiments on both mainstream and our proposed datasets demonstrate the
superiority of the model. Codes and dataset are released at
https://github.com/yunan-wang33/sdml.

</details>


### [153] [Overview of the NLPCC 2025 Shared Task 4: Multi-modal, Multilingual, and Multi-hop Medical Instructional Video Question Answering Challenge](https://arxiv.org/pdf/2505.06814)
*Bin Li, Shenxi Liu, Yixuan Weng, Yue Du, Yuhang Tian, Shoujun Zhou*

Main category: cs.CV

TL;DR: The M4IVQA challenge introduces a new task for multi-modal, multilingual, and multi-hop medical instructional question answering, focusing on medical instructional videos. It includes three tracks: M4TAGSV, M4VCR, and M4TAGVC.


<details>
  <summary>Details</summary>
Motivation: To advance research in multi-modal, multilingual, and multi-hop medical instructional question answering, particularly for healthcare and education.

Method: Participants develop algorithms to process video and text data, understand multilingual queries, and answer multi-hop medical questions.

Result: The challenge aims to drive innovations in multimodal reasoning systems for healthcare and education.

Conclusion: M4IVQA is expected to contribute to smarter emergency response and more effective medical education in multilingual communities.

Abstract: Following the successful hosts of the 1-st (NLPCC 2023 Foshan) CMIVQA and the
2-rd (NLPCC 2024 Hangzhou) MMIVQA challenges, this year, a new task has been
introduced to further advance research in multi-modal, multilingual, and
multi-hop medical instructional question answering (M4IVQA) systems, with a
specific focus on medical instructional videos. The M4IVQA challenge focuses on
evaluating models that integrate information from medical instructional videos,
understand multiple languages, and answer multi-hop questions requiring
reasoning over various modalities. This task consists of three tracks:
multi-modal, multilingual, and multi-hop Temporal Answer Grounding in Single
Video (M4TAGSV), multi-modal, multilingual, and multi-hop Video Corpus
Retrieval (M4VCR) and multi-modal, multilingual, and multi-hop Temporal Answer
Grounding in Video Corpus (M4TAGVC). Participants in M4IVQA are expected to
develop algorithms capable of processing both video and text data,
understanding multilingual queries, and providing relevant answers to multi-hop
medical questions. We believe the newly introduced M4IVQA challenge will drive
innovations in multimodal reasoning systems for healthcare scenarios,
ultimately contributing to smarter emergency response systems and more
effective medical education platforms in multilingual communities. Our official
website is https://cmivqa.github.io/

</details>


### [154] [Active Learning for Multi-class Image Classification](https://arxiv.org/pdf/2505.06825)
*Thien Nhan Vo*

Main category: cs.CV

TL;DR: Active learning reduces training examples for CNN classifiers by strategically selecting high-value images using uncertainty metrics, showing effectiveness in digit and fruit classification tasks.


<details>
  <summary>Details</summary>
Motivation: The large number of training examples required for image classification is a bottleneck; active learning aims to mitigate this by selecting fewer, more informative examples.

Method: Active learning is applied to CNN classifiers using four uncertainty metrics to strategically select high-value training examples from MNIST and Fruits360 datasets.

Result: Active learning reduces training set size while maintaining performance, with marked improvement over random sampling in complex tasks.

Conclusion: Active learning is a viable and effective approach for image classification, particularly in challenging tasks.

Abstract: A principle bottleneck in image classification is the large number of
training examples needed to train a classifier. Using active learning, we can
reduce the number of training examples to teach a CNN classifier by
strategically selecting examples. Assigning values to image examples using
different uncertainty metrics allows the model to identify and select
high-value examples in a smaller training set size. We demonstrate results for
digit recognition and fruit classification on the MNIST and Fruits360 data
sets. We formally compare results for four different uncertainty metrics.
Finally, we observe active learning is also effective on simpler (binary)
classification tasks, but marked improvement from random sampling is more
evident on more difficult tasks. We show active learning is a viable algorithm
for image classification problems.

</details>


### [155] [Fine-Grained Bias Exploration and Mitigation for Group-Robust Classification](https://arxiv.org/pdf/2505.06831)
*Miaoyun Zhao, Qiang Zhang, Chenrong Li*

Main category: cs.CV

TL;DR: The paper proposes Bias Exploration via Overfitting (BEO) and FG-CCDB to address spurious correlations in group-robust generalization without bias annotations, outperforming bias-supervised methods in multi-class tasks.


<details>
  <summary>Details</summary>
Motivation: Spurious correlations due to mismatched distributions in bias-agnostic settings pose challenges, and existing methods like CCDB oversimplify distributions.

Method: BEO models distributions as mixtures of latent groups, while FG-CCDB performs fine-grained distribution matching and balancing within groups.

Result: BEO acts as a proxy for bias annotations, and FG-CCDB matches or surpasses bias-supervised methods, especially in multi-class scenarios.

Conclusion: The proposed methods effectively mitigate spurious correlations without bias annotations, offering practical and scalable solutions.

Abstract: Achieving group-robust generalization in the presence of spurious
correlations remains a significant challenge, particularly when bias
annotations are unavailable. Recent studies on Class-Conditional Distribution
Balancing (CCDB) reveal that spurious correlations often stem from mismatches
between the class-conditional and marginal distributions of bias attributes.
They achieve promising results by addressing this issue through simple
distribution matching in a bias-agnostic manner. However, CCDB approximates
each distribution using a single Gaussian, which is overly simplistic and
rarely holds in real-world applications. To address this limitation, we propose
a novel method called Bias Exploration via Overfitting (BEO), which captures
each distribution in greater detail by modeling it as a mixture of latent
groups. Building on these group-level descriptions, we introduce a fine-grained
variant of CCDB, termed FG-CCDB, which performs more precise distribution
matching and balancing within each group. Through group-level reweighting,
FG-CCDB learns sample weights from a global perspective, achieving stronger
mitigation of spurious correlations without incurring substantial storage or
computational costs. Extensive experiments demonstrate that BEO serves as a
strong proxy for ground-truth bias annotations and can be seamlessly integrated
with bias-supervised methods. Moreover, when combined with FG-CCDB, our method
performs on par with bias-supervised approaches on binary classification tasks
and significantly outperforms them in highly biased multi-class scenarios.

</details>


### [156] [Visual Instruction Tuning with Chain of Region-of-Interest](https://arxiv.org/pdf/2505.06840)
*Yixin Chen, Shuai Zhang, Boran Han, Bernie Wang*

Main category: cs.CV

TL;DR: CoRoI method reduces computational load of high-resolution images for MLLMs by focusing on key regions, outperforming benchmarks like LLaVA-NeXT and GPT-4V.


<details>
  <summary>Details</summary>
Motivation: High-resolution images improve MLLM performance but increase computational demands; CoRoI addresses this by prioritizing informative regions.

Method: CoRoI identifies and prioritizes key regions in high-resolution images, inspired by the human visual system, to reduce token processing.

Result: Outperforms LLaVA-NeXT and proprietary models like Gemini Pro 1.0 and GPT-4V on multiple benchmarks.

Conclusion: CoRoI effectively balances computational efficiency and performance for MLLMs with high-resolution images.

Abstract: High-resolution (HR) images are pivotal for enhancing the recognition and
understanding capabilities of multimodal large language models (MLLMs).
However, directly increasing image resolution can significantly escalate
computational demands. In this study, we propose a method called Chain of
Region-of-Interest (CoRoI) for Visual Instruction Tuning, aimed at alleviating
the computational burden associated with high-resolution images for MLLMs.
Drawing inspiration from the selective nature of the human visual system, we
recognize that not all regions within high-resolution images carry equal
importance. CoRoI seeks to identify and prioritize the most informative
regions, thereby enhancing multimodal visual comprehension and recognition
while circumventing the need for processing lengthy HR image tokens. Through
extensive experiments on 11 benchmarks, we validate the efficacy of CoRoI
across varying sizes, ranging from 7B to 34B in parameters. Our models
consistently demonstrate superior performance across diverse multimodal
benchmarks and tasks. Notably, our method outperforms LLaVA-NeXT on almost all
benchmarks and our finetuned 34B model surpasses proprietary methods like
Gemini Pro 1.0 on six benchmarks, as well as outperforming GPT-4V on MMB,
SEED-I, and MME.

</details>


### [157] [Predicting Surgical Safety Margins in Osteosarcoma Knee Resections: An Unsupervised Approach](https://arxiv.org/pdf/2505.06853)
*Carolina Vargas-Ecos, Edwin Salcedo*

Main category: cs.CV

TL;DR: A method using MRI/X-ray data and unsupervised learning to estimate surgical safety margins for osteosarcoma around the knee.


<details>
  <summary>Details</summary>
Motivation: Rising cancer cases in Latin America and the challenge of precise osteosarcoma resection due to its unique texture and intensity.

Method: Uses MRI and X-ray data, digital processing, and k-means clustering to define tumor boundaries.

Result: Demonstrates potential for automated, patient-specific safety margin determination.

Conclusion: Proposed approach could improve precision in osteosarcoma surgery.

Abstract: According to the Pan American Health Organization, the number of cancer cases
in Latin America was estimated at 4.2 million in 2022 and is projected to rise
to 6.7 million by 2045. Osteosarcoma, one of the most common and deadly bone
cancers affecting young people, is difficult to detect due to its unique
texture and intensity. Surgical removal of osteosarcoma requires precise safety
margins to ensure complete resection while preserving healthy tissue.
Therefore, this study proposes a method for estimating the confidence interval
of surgical safety margins in osteosarcoma surgery around the knee. The
proposed approach uses MRI and X-ray data from open-source repositories,
digital processing techniques, and unsupervised learning algorithms (such as
k-means clustering) to define tumor boundaries. Experimental results highlight
the potential for automated, patient-specific determination of safety margins.

</details>


### [158] [Joint Low-level and High-level Textual Representation Learning with Multiple Masking Strategies](https://arxiv.org/pdf/2505.06855)
*Zhengmi Tang, Yuto Mitsui, Tomo Miyazaki, Shinichiro Omachi*

Main category: cs.CV

TL;DR: The paper introduces a Multi-Masking Strategy (MMS) to improve text recognition by combining random patch, blockwise, and span masking in masked image modeling (MIM), outperforming existing self-supervised methods.


<details>
  <summary>Details</summary>
Motivation: Existing text recognition methods rely on synthetic datasets, which fail to replicate real-world complexities like uneven illumination and occlusion. Self-supervised learning, particularly MIM, helps bridge this gap but misses high-level contextual features.

Method: The study enhances MIM by introducing random blockwise and span masking alongside random patch masking. This forces the model to infer character relationships, improving contextual understanding.

Result: MMS outperforms state-of-the-art self-supervised methods in text recognition, segmentation, and text-image super-resolution after fine-tuning with real data.

Conclusion: The proposed MMS effectively combines masking strategies to capture both low and high-level textual features, enhancing performance in real-world text recognition tasks.

Abstract: Most existing text recognition methods are trained on large-scale synthetic
datasets due to the scarcity of labeled real-world datasets. Synthetic images,
however, cannot faithfully reproduce real-world scenarios, such as uneven
illumination, irregular layout, occlusion, and degradation, resulting in
performance disparities when handling complex real-world images. Recent
self-supervised learning techniques, notably contrastive learning and masked
image modeling (MIM), narrow this domain gap by exploiting unlabeled real text
images. This study first analyzes the original Masked AutoEncoder (MAE) and
observes that random patch masking predominantly captures low-level textural
features but misses high-level contextual representations. To fully exploit the
high-level contextual representations, we introduce random blockwise and span
masking in the text recognition task. These strategies can mask the continuous
image patches and completely remove some characters, forcing the model to infer
relationships among characters within a word. Our Multi-Masking Strategy (MMS)
integrates random patch, blockwise, and span masking into the MIM frame, which
jointly learns low and high-level textual representations. After fine-tuning
with real data, MMS outperforms the state-of-the-art self-supervised methods in
various text-related tasks, including text recognition, segmentation, and
text-image super-resolution.

</details>


### [159] [NeuRN: Neuro-inspired Domain Generalization for Image Classification](https://arxiv.org/pdf/2505.06881)
*Hamd Jalil, Ahmed Qazi, Asim Iqbal*

Main category: cs.CV

TL;DR: The paper introduces a neuro-inspired Neural Response Normalization (NeuRN) layer to improve domain generalization in image classification, showing improved performance over baselines on unseen domains.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of models failing to generalize across unseen datasets in image classification.

Method: Proposes NeuRN, inspired by mammalian visual cortex neurons, and a novel method using the Needleman-Wunsch algorithm to compare deep learning architectures.

Result: NeuRN enhances performance on cross-domain image classification tasks compared to baseline models.

Conclusion: The framework lays a foundation for future neuro-inspired deep learning models.

Abstract: Domain generalization in image classification is a crucial challenge, with
models often failing to generalize well across unseen datasets. We address this
issue by introducing a neuro-inspired Neural Response Normalization (NeuRN)
layer which draws inspiration from neurons in the mammalian visual cortex,
which aims to enhance the performance of deep learning architectures on unseen
target domains by training deep learning models on a source domain. The
performance of these models is considered as a baseline and then compared
against models integrated with NeuRN on image classification tasks. We perform
experiments across a range of deep learning architectures, including ones
derived from Neural Architecture Search and Vision Transformer. Additionally,
in order to shortlist models for our experiment from amongst the vast range of
deep neural networks available which have shown promising results, we also
propose a novel method that uses the Needleman-Wunsch algorithm to compute
similarity between deep learning architectures. Our results demonstrate the
effectiveness of NeuRN by showing improvement against baseline in cross-domain
image classification tasks. Our framework attempts to establish a foundation
for future neuro-inspired deep learning models.

</details>


### [160] [Mice to Machines: Neural Representations from Visual Cortex for Domain Generalization](https://arxiv.org/pdf/2505.06886)
*Ahmed Qazi, Hamd Jalil, Asim Iqbal*

Main category: cs.CV

TL;DR: The study explores the functional alignment between the mouse visual cortex and deep learning models, introducing a representational learning strategy and a Neural Response Normalization (NeuRN) layer to enhance similarity and improve model robustness.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in understanding mouse vision using deep learning models, given the parallels between primate vision and hierarchical deep networks.

Method: A generalized representational learning strategy and the NeuRN layer are introduced to align mouse visual cortex functionality with deep learning models, tested in domain generalization tasks.

Result: Enhanced representational similarity between the mouse visual cortex and deep models, with NeuRN improving robustness against data shifts.

Conclusion: The framework offers insights for AI model development inspired by the mouse visual cortex, improving both neuroscience understanding and real-world task performance.

Abstract: The mouse is one of the most studied animal models in the field of systems
neuroscience. Understanding the generalized patterns and decoding the neural
representations that are evoked by the diverse range of natural scene stimuli
in the mouse visual cortex is one of the key quests in computational vision. In
recent years, significant parallels have been drawn between the primate visual
cortex and hierarchical deep neural networks. However, their generalized
efficacy in understanding mouse vision has been limited. In this study, we
investigate the functional alignment between the mouse visual cortex and deep
learning models for object classification tasks. We first introduce a
generalized representational learning strategy that uncovers a striking
resemblance between the functional mapping of the mouse visual cortex and
high-performing deep learning models on both top-down (population-level) and
bottom-up (single cell-level) scenarios. Next, this representational similarity
across the two systems is further enhanced by the addition of Neural Response
Normalization (NeuRN) layer, inspired by the activation profile of excitatory
and inhibitory neurons in the visual cortex. To test the performance effect of
NeuRN on real-world tasks, we integrate it into deep learning models and
observe significant improvements in their robustness against data shifts in
domain generalization tasks. Our work proposes a novel framework for comparing
the functional architecture of the mouse visual cortex with deep learning
models. Our findings carry broad implications for the development of advanced
AI models that draw inspiration from the mouse visual cortex, suggesting that
these models serve as valuable tools for studying the neural representations of
the mouse visual cortex and, as a result, enhancing their performance on
real-world tasks.

</details>


### [161] [NeuGen: Amplifying the 'Neural' in Neural Radiance Fields for Domain Generalization](https://arxiv.org/pdf/2505.06894)
*Ahmed Qazi, Abdul Basit, Asim Iqbal*

Main category: cs.CV

TL;DR: NeuGen, a brain-inspired normalization technique, enhances NeRF architectures' generalization and rendering quality by extracting domain-invariant features.


<details>
  <summary>Details</summary>
Motivation: Improving NeRF's generalization across diverse scenes and conditions remains a challenge.

Method: Integrate NeuGen into NeRF architectures (MVSNeRF, GeoNeRF) to extract domain-invariant features.

Result: Improved performance on benchmarks, better generalization, and enhanced rendering quality.

Conclusion: NeuGen sets a precedent for merging neuroscientific principles with deep learning to boost NeRF's generalizability.

Abstract: Neural Radiance Fields (NeRF) have significantly advanced the field of novel
view synthesis, yet their generalization across diverse scenes and conditions
remains challenging. Addressing this, we propose the integration of a novel
brain-inspired normalization technique Neural Generalization (NeuGen) into
leading NeRF architectures which include MVSNeRF and GeoNeRF. NeuGen extracts
the domain-invariant features, thereby enhancing the models' generalization
capabilities. It can be seamlessly integrated into NeRF architectures and
cultivates a comprehensive feature set that significantly improves accuracy and
robustness in image rendering. Through this integration, NeuGen shows improved
performance on benchmarks on diverse datasets across state-of-the-art NeRF
architectures, enabling them to generalize better across varied scenes. Our
comprehensive evaluations, both quantitative and qualitative, confirm that our
approach not only surpasses existing models in generalizability but also
markedly improves rendering quality. Our work exemplifies the potential of
merging neuroscientific principles with deep learning frameworks, setting a new
precedent for enhanced generalizability and efficiency in novel view synthesis.
A demo of our study is available at https://neugennerf.github.io.

</details>


### [162] [Multi-Modal Explainable Medical AI Assistant for Trustworthy Human-AI Collaboration](https://arxiv.org/pdf/2505.06898)
*Honglong Yang, Shanshan Song, Yi Qin, Lehan Wang, Haonan Wang, Xinpeng Ding, Qixiang Zhang, Bodong Du, Xiaomeng Li*

Main category: cs.CV

TL;DR: XMedGPT is a clinician-centric, multi-modal AI assistant that enhances medical decision-making with interpretability, uncertainty quantification, and superior performance in diagnostics and prognostics.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of Generalist Medical AI (GMAI) systems, such as inadequate explainability and suboptimal prognostic capabilities, to improve clinical utility.

Method: Integrates textual and visual interpretability, introduces a reliability indexing mechanism for uncertainty quantification, and validates across multi-modal interpretability, uncertainty, and prognostic modeling.

Result: Achieves high performance in anatomical region alignment (IoU 0.703), uncertainty estimation (AUC 0.862), and surpasses prior models in survival prediction (26.9% improvement).

Conclusion: XMedGPT advances clinician-centric AI by providing transparent, trustworthy, and scalable support for healthcare applications.

Abstract: Generalist Medical AI (GMAI) systems have demonstrated expert-level
performance in biomedical perception tasks, yet their clinical utility remains
limited by inadequate multi-modal explainability and suboptimal prognostic
capabilities. Here, we present XMedGPT, a clinician-centric, multi-modal AI
assistant that integrates textual and visual interpretability to support
transparent and trustworthy medical decision-making. XMedGPT not only produces
accurate diagnostic and descriptive outputs, but also grounds referenced
anatomical sites within medical images, bridging critical gaps in
interpretability and enhancing clinician usability. To support real-world
deployment, we introduce a reliability indexing mechanism that quantifies
uncertainty through consistency-based assessment via interactive
question-answering. We validate XMedGPT across four pillars: multi-modal
interpretability, uncertainty quantification, and prognostic modeling, and
rigorous benchmarking. The model achieves an IoU of 0.703 across 141 anatomical
regions, and a Kendall's tau-b of 0.479, demonstrating strong alignment between
visual rationales and clinical outcomes. For uncertainty estimation, it attains
an AUC of 0.862 on visual question answering and 0.764 on radiology report
generation. In survival and recurrence prediction for lung and glioma cancers,
it surpasses prior leading models by 26.9%, and outperforms GPT-4o by 25.0%.
Rigorous benchmarking across 347 datasets covers 40 imaging modalities and
external validation spans 4 anatomical systems confirming exceptional
generalizability, with performance gains surpassing existing GMAI by 20.7% for
in-domain evaluation and 16.7% on 11,530 in-house data evaluation. Together,
XMedGPT represents a significant leap forward in clinician-centric AI
integration, offering trustworthy and scalable support for diverse healthcare
applications.

</details>


### [163] [CheXLearner: Text-Guided Fine-Grained Representation Learning for Progression Detection](https://arxiv.org/pdf/2505.06903)
*Yuanzhuo Wang, Junwen Duan, Xinyu Li, Jianxin Wang*

Main category: cs.CV

TL;DR: CheXLearner is an end-to-end framework for temporal medical image analysis, combining anatomical region detection, Riemannian manifold alignment, and semantic guidance to improve accuracy and semantic integration.


<details>
  <summary>Details</summary>
Motivation: Existing methods for temporal medical image analysis either align images and text coarsely, risking semantic mismatches, or rely only on visual data, missing medical semantics. CheXLearner addresses these gaps.

Method: CheXLearner uses a Med-Manifold Alignment Module (Med-MAM) with hyperbolic geometry for robust anatomical alignment and introduces regional progression descriptions for supervision, enhancing cross-modal learning.

Result: CheXLearner achieves 81.12% accuracy (+17.2%) and 80.32% F1-score (+11.05%) in anatomical region progression detection, with a 91.52% AUC in disease classification, outperforming baselines.

Conclusion: CheXLearner effectively integrates anatomical and semantic information, improving temporal medical image analysis and outperforming state-of-the-art methods.

Abstract: Temporal medical image analysis is essential for clinical decision-making,
yet existing methods either align images and text at a coarse level - causing
potential semantic mismatches - or depend solely on visual information, lacking
medical semantic integration. We present CheXLearner, the first end-to-end
framework that unifies anatomical region detection, Riemannian manifold-based
structure alignment, and fine-grained regional semantic guidance. Our proposed
Med-Manifold Alignment Module (Med-MAM) leverages hyperbolic geometry to
robustly align anatomical structures and capture pathologically meaningful
discrepancies across temporal chest X-rays. By introducing regional progression
descriptions as supervision, CheXLearner achieves enhanced cross-modal
representation learning and supports dynamic low-level feature optimization.
Experiments show that CheXLearner achieves 81.12% (+17.2%) average accuracy and
80.32% (+11.05%) F1-score on anatomical region progression detection -
substantially outperforming state-of-the-art baselines, especially in
structurally complex regions. Additionally, our model attains a 91.52% average
AUC score in downstream disease classification, validating its superior feature
representation.

</details>


### [164] [Building a Human-Verified Clinical Reasoning Dataset via a Human LLM Hybrid Pipeline for Trustworthy Medical AI](https://arxiv.org/pdf/2505.06912)
*Chao Ding, Mouxiao Bian, Pengcheng Chen, Hongliang Zhang, Tianbin Li, Lihao Liu, Jiayuan Chen, Zhuoran Li, Yabei Zhong, Yongqi Liu, Haiqing Huang, Dongming Shan, Junjun He, Jie Xu*

Main category: cs.CV

TL;DR: The paper introduces a clinically relevant dataset with expert-validated chain-of-thought explanations to improve transparency and trust in medical LLMs.


<details>
  <summary>Details</summary>
Motivation: Current medical LLMs lack transparency and rely on less clinically relevant data, limiting clinician trust and adoption.

Method: A hybrid human-LLM pipeline curated 31,247 expert-validated medical QA pairs with CoT explanations, iteratively refined by medical experts.

Result: The dataset enables development of transparent and verifiable medical LLMs, enhancing interpretability in AI for medicine.

Conclusion: The resource advances safer, more interpretable AI in medicine by addressing critical gaps in clinical relevance and reasoning transparency.

Abstract: Despite strong performance in medical question-answering, the clinical
adoption of Large Language Models (LLMs) is critically hampered by their opaque
'black-box' reasoning, limiting clinician trust. This challenge is compounded
by the predominant reliance of current medical LLMs on corpora from scientific
literature or synthetic data, which often lack the granular expert validation
and high clinical relevance essential for advancing their specialized medical
capabilities. To address these critical gaps, we introduce a highly clinically
relevant dataset with 31,247 medical question-answer pairs, each accompanied by
expert-validated chain-of-thought (CoT) explanations. This resource, spanning
multiple clinical domains, was curated via a scalable human-LLM hybrid
pipeline: LLM-generated rationales were iteratively reviewed, scored, and
refined by medical experts against a structured rubric, with substandard
outputs revised through human effort or guided LLM regeneration until expert
consensus. This publicly available dataset provides a vital source for the
development of medical LLMs that capable of transparent and verifiable
reasoning, thereby advancing safer and more interpretable AI in medicine.

</details>


### [165] [Bi-directional Self-Registration for Misaligned Infrared-Visible Image Fusion](https://arxiv.org/pdf/2505.06920)
*Timing Li, Bing Cao, Pengfei Zhu, Bin Xiao, Qinghua Hu*

Main category: cs.CV

TL;DR: Proposes B-SR, a self-supervised framework for multi-modal image alignment and fusion, using global-local registration and dynamic alignment loss.


<details>
  <summary>Details</summary>
Motivation: Addresses the lack of ground truth in multi-modal image registration and fusion by introducing a self-supervised approach.

Method: Uses proxy data generators (PDG and IPDG) for global-local registration and a neighborhood dynamic alignment loss for cross-modal edge alignment.

Result: Demonstrates effectiveness in aligning and fusing misaligned multi-modal images compared to competing methods.

Conclusion: B-SR is a robust solution for multi-modal image alignment and fusion, with code to be publicly available.

Abstract: Acquiring accurately aligned multi-modal image pairs is fundamental for
achieving high-quality multi-modal image fusion. To address the lack of ground
truth in current multi-modal image registration and fusion methods, we propose
a novel self-supervised \textbf{B}i-directional
\textbf{S}elf-\textbf{R}egistration framework (\textbf{B-SR}). Specifically,
B-SR utilizes a proxy data generator (PDG) and an inverse proxy data generator
(IPDG) to achieve self-supervised global-local registration. Visible-infrared
image pairs with spatially misaligned differences are aligned to obtain global
differences through the registration module. The same image pairs are processed
by PDG, such as cropping, flipping, stitching, etc., and then aligned to obtain
local differences. IPDG converts the obtained local differences into
pseudo-global differences, which are used to perform global-local difference
consistency with the global differences. Furthermore, aiming at eliminating the
effect of modal gaps on the registration module, we design a neighborhood
dynamic alignment loss to achieve cross-modal image edge alignment. Extensive
experiments on misaligned multi-modal images demonstrate the effectiveness of
the proposed method in multi-modal image alignment and fusion against the
competing methods. Our code will be publicly available.

</details>


### [166] [Transformer-Based Dual-Optical Attention Fusion Crowd Head Point Counting and Localization Network](https://arxiv.org/pdf/2505.06937)
*Fei Zhou, Yi Li, Mingqing Zhu*

Main category: cs.CV

TL;DR: TAPNet introduces dual-optical attention fusion and adaptive feature decomposition to improve crowd counting accuracy in complex UAV scenes, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like dense occlusion and low light in UAV-based crowd counting.

Method: Uses dual-optical attention fusion (DAFP) and adaptive feature decomposition (AFDF) modules, with spatial random offset data augmentation.

Result: Outperforms existing techniques on DroneRGBT and GAIIC2 datasets, especially in dense low-light scenes.

Conclusion: TAPNet enhances accuracy and robustness for all-day crowd counting in complex scenarios.

Abstract: In this paper, the dual-optical attention fusion crowd head point counting
model (TAPNet) is proposed to address the problem of the difficulty of accurate
counting in complex scenes such as crowd dense occlusion and low light in crowd
counting tasks under UAV view. The model designs a dual-optical attention
fusion module (DAFP) by introducing complementary information from infrared
images to improve the accuracy and robustness of all-day crowd counting. In
order to fully utilize different modal information and solve the problem of
inaccurate localization caused by systematic misalignment between image pairs,
this paper also proposes an adaptive two-optical feature decomposition fusion
module (AFDF). In addition, we optimize the training strategy to improve the
model robustness through spatial random offset data augmentation. Experiments
on two challenging public datasets, DroneRGBT and GAIIC2, show that the
proposed method outperforms existing techniques in terms of performance,
especially in challenging dense low-light scenes. Code is available at
https://github.com/zz-zik/TAPNet

</details>


### [167] [Unsupervised Learning for Class Distribution Mismatch](https://arxiv.org/pdf/2505.06948)
*Pan Du, Wangbo Zhao, Xinai Lu, Nian Liu, Zhikai Li, Chaoyu Gong, Suyun Zhao, Hong Chen, Cuiping Li, Kai Wang, Yang You*

Main category: cs.CV

TL;DR: UCDM proposes an unsupervised method to address class distribution mismatch by synthesizing training pairs from unlabeled data and using a confidence-based labeling mechanism, outperforming semi-supervised methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for class distribution mismatch rely on labeled data and semi-supervised scenarios, limiting their applicability. UCDM aims to overcome this by leveraging unlabeled data.

Method: UCDM constructs positive-negative pairs from unlabeled data using a diffusion model to add/erase semantic classes. It also uses a confidence-based labeling mechanism to iteratively assign pseudo-labels.

Result: UCDM outperforms OpenMatch by significant margins (35.1%, 63.7%, 72.5%) on Tiny-ImageNet with 60% mismatch, without labeled data.

Conclusion: UCDM is a robust unsupervised solution for class distribution mismatch, surpassing semi-supervised methods in performance and applicability.

Abstract: Class distribution mismatch (CDM) refers to the discrepancy between class
distributions in training data and target tasks. Previous methods address this
by designing classifiers to categorize classes known during training, while
grouping unknown or new classes into an "other" category. However, they focus
on semi-supervised scenarios and heavily rely on labeled data, limiting their
applicability and performance. To address this, we propose Unsupervised
Learning for Class Distribution Mismatch (UCDM), which constructs
positive-negative pairs from unlabeled data for classifier training. Our
approach randomly samples images and uses a diffusion model to add or erase
semantic classes, synthesizing diverse training pairs. Additionally, we
introduce a confidence-based labeling mechanism that iteratively assigns
pseudo-labels to valuable real-world data and incorporates them into the
training process. Extensive experiments on three datasets demonstrate UCDM's
superiority over previous semi-supervised methods. Specifically, with a 60%
mismatch proportion on Tiny-ImageNet dataset, our approach, without relying on
labeled data, surpasses OpenMatch (with 40 labels per class) by 35.1%, 63.7%,
and 72.5% in classifying known, unknown, and new classes.

</details>


### [168] [Boosting Cross-spectral Unsupervised Domain Adaptation for Thermal Semantic Segmentation](https://arxiv.org/pdf/2505.06951)
*Seokjun Kwon, Jeongmin Shin, Namil Kim, Soonmin Hwang, Yukyung Choi*

Main category: cs.CV

TL;DR: A novel masked mutual learning strategy and prototypical self-supervised loss improve thermal image segmentation via cross-spectral UDA, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of labeled thermal datasets and ineffective use of RGB-thermal complementary information in UDA for thermal image segmentation.

Method: Proposes masked mutual learning for selective information transfer and a prototypical self-supervised loss for nighttime performance.

Result: Achieves higher performance than previous UDA methods and comparable to supervised methods.

Conclusion: The approach effectively leverages cross-spectral data and enhances thermal segmentation, especially in low-light conditions.

Abstract: In autonomous driving, thermal image semantic segmentation has emerged as a
critical research area, owing to its ability to provide robust scene
understanding under adverse visual conditions. In particular, unsupervised
domain adaptation (UDA) for thermal image segmentation can be an efficient
solution to address the lack of labeled thermal datasets. Nevertheless, since
these methods do not effectively utilize the complementary information between
RGB and thermal images, they significantly decrease performance during domain
adaptation. In this paper, we present a comprehensive study on cross-spectral
UDA for thermal image semantic segmentation. We first propose a novel masked
mutual learning strategy that promotes complementary information exchange by
selectively transferring results between each spectral model while masking out
uncertain regions. Additionally, we introduce a novel prototypical
self-supervised loss designed to enhance the performance of the thermal
segmentation model in nighttime scenarios. This approach addresses the
limitations of RGB pre-trained networks, which cannot effectively transfer
knowledge under low illumination due to the inherent constraints of RGB
sensors. In experiments, our method achieves higher performance over previous
UDA methods and comparable performance to state-of-the-art supervised methods.

</details>


### [169] [High-Frequency Prior-Driven Adaptive Masking for Accelerating Image Super-Resolution](https://arxiv.org/pdf/2505.06975)
*Wei Shang, Dongwei Ren, Wanying Zhang, Pengfei Zhu, Qinghua Hu, Wangmeng Zuo*

Main category: cs.CV

TL;DR: A training-free adaptive masking module accelerates image super-resolution by focusing computation on high-frequency regions, reducing FLOPs by 24-43% without performance loss.


<details>
  <summary>Details</summary>
Motivation: High-frequency regions (edges, textures) are critical for image reconstruction, but current methods lack efficient computation focusing.

Method: Extracts high-frequency components via Gaussian blur subtraction, uses K-means clustering for binary masks, and integrates with CNNs (unfold + 1x1 convolutions) or Transformers (selective token processing).

Result: Reduces FLOPs by 24-43% for models like CARN and SwinIR while maintaining or improving performance metrics.

Conclusion: The method offers a robust, adaptable solution for accelerating super-resolution without retraining, applicable to various architectures.

Abstract: The primary challenge in accelerating image super-resolution lies in reducing
computation while maintaining performance and adaptability. Motivated by the
observation that high-frequency regions (e.g., edges and textures) are most
critical for reconstruction, we propose a training-free adaptive masking module
for acceleration that dynamically focuses computation on these challenging
areas. Specifically, our method first extracts high-frequency components via
Gaussian blur subtraction and adaptively generates binary masks using K-means
clustering to identify regions requiring intensive processing. Our method can
be easily integrated with both CNNs and Transformers. For CNN-based
architectures, we replace standard $3 \times 3$ convolutions with an unfold
operation followed by $1 \times 1$ convolutions, enabling pixel-wise sparse
computation guided by the mask. For Transformer-based models, we partition the
mask into non-overlapping windows and selectively process tokens based on their
average values. During inference, unnecessary pixels or windows are pruned,
significantly reducing computation. Moreover, our method supports
dilation-based mask adjustment to control the processing scope without
retraining, and is robust to unseen degradations (e.g., noise, compression).
Extensive experiments on benchmarks demonstrate that our method reduces FLOPs
by 24--43% for state-of-the-art models (e.g., CARN, SwinIR) while achieving
comparable or better quantitative metrics. The source code is available at
https://github.com/shangwei5/AMSR

</details>


### [170] [Federated Learning with LoRA Optimized DeiT and Multiscale Patch Embedding for Secure Eye Disease Recognition](https://arxiv.org/pdf/2505.06982)
*Md. Naimur Asif Borno, Md Sakib Hossain Shovon, MD Hanif Sikder, Iffat Firozy Rimi, Tahani Jaser Alahmadi, Mohammad Ali Moni*

Main category: cs.CV

TL;DR: A DeIT-based approach improves medical disease detection by addressing data limitations, feature extraction, and security, achieving top performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Challenges like limited annotated data, poor feature analysis, data security, and inefficient training hinder medical disease detection.

Method: Uses multiscale patch embedding, stratified sampling, LoRA-enhanced transformer, distillation, and federated learning.

Result: Achieves highest AUC, F1, precision, minimal loss, and Top-5 accuracy, with Grad-CAM++ for interpretability.

Conclusion: The approach advances AI-powered medical imaging with efficient, secure, and interpretable disease detection.

Abstract: Recent progress in image-based medical disease detection encounters
challenges such as limited annotated data sets, inadequate spatial feature
analysis, data security issues, and inefficient training frameworks. This study
introduces a data-efficient image transformer (DeIT)-based approach that
overcomes these challenges by utilizing multiscale patch embedding for better
feature extraction and stratified weighted random sampling to address class
imbalance. The model also incorporates a LoRA-enhanced transformer encoder, a
distillation framework, and federated learning for decentralized training,
improving both efficiency and data security. Consequently, it achieves
state-of-the-art performance, with the highest AUC, F1 score, precision,
minimal loss, and Top-5 accuracy. Additionally, Grad-CAM++ visualizations
improve interpretability by highlighting critical pathological regions,
enhancing the model's clinical relevance. These results highlight the potential
of this approach to advance AI-powered medical imaging and disease detection.

</details>


### [171] [BridgeIV: Bridging Customized Image and Video Generation through Test-Time Autoregressive Identity Propagation](https://arxiv.org/pdf/2505.06985)
*Panwen Hu, Jiehui Huang, Qiang Sun, Xiaodan Liang*

Main category: cs.CV

TL;DR: Proposes STPM and TTRO for CT2V generation to enhance consistency and detail, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Limited research on CT2V generation, with existing methods suffering from poor generalization or loss of structural/texture details.

Method: Uses autoregressive STPM to propagate structural/texture features and TTRO for fine-grained detail refinement.

Result: Improves CLIP-I and DINO consistency metrics by 7.8 and 13.1, respectively.

Conclusion: STPM and TTRO effectively address CT2V challenges, enhancing consistency and detail.

Abstract: Both zero-shot and tuning-based customized text-to-image (CT2I) generation
have made significant progress for storytelling content creation. In contrast,
research on customized text-to-video (CT2V) generation remains relatively
limited. Existing zero-shot CT2V methods suffer from poor generalization, while
another line of work directly combining tuning-based T2I models with temporal
motion modules often leads to the loss of structural and texture information.
To bridge this gap, we propose an autoregressive structure and texture
propagation module (STPM), which extracts key structural and texture features
from the reference subject and injects them autoregressively into each video
frame to enhance consistency. Additionally, we introduce a test-time reward
optimization (TTRO) method to further refine fine-grained details. Quantitative
and qualitative experiments validate the effectiveness of STPM and TTRO,
demonstrating improvements of 7.8 and 13.1 in CLIP-I and DINO consistency
metrics over the baseline, respectively.

</details>


### [172] [Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge: Leveraging Color Shift Correction, RoPE-Swin Backbone, and Quantile-based Label Denoising Strategy for Robust Outdoor Scene Understanding](https://arxiv.org/pdf/2505.06991)
*Chih-Chung Hsu, I-Hsuan Wu, Wen-Hai Tseng, Ching-Heng Cheng, Ming-Hsuan Wu, Jin-Hui Jiang, Yu-Jou Hsiao*

Main category: cs.CV

TL;DR: A semantic segmentation framework for outdoor scenes using Swin Transformer with RoPE, color correction, and error-aware denoising, achieving 0.848 mIoU.


<details>
  <summary>Details</summary>
Motivation: To parse outdoor scenes into nine semantic categories under real-world conditions, addressing illumination inconsistencies and training noise.

Method: Integrates Swin Transformer with Rotary Position Embedding (RoPE) for spatial generalization, a Color Shift Estimation-and-Correction module, and quantile-based denoising.

Result: Achieved a mean Intersection over Union (mIoU) of 0.848 on the GOOSE test set.

Conclusion: Combining color correction, positional encoding, and error-aware denoising proves effective for robust semantic segmentation.

Abstract: This report presents our semantic segmentation framework developed by team
ACVLAB for the ICRA 2025 GOOSE 2D Semantic Segmentation Challenge, which
focuses on parsing outdoor scenes into nine semantic categories under
real-world conditions. Our method integrates a Swin Transformer backbone
enhanced with Rotary Position Embedding (RoPE) for improved spatial
generalization, alongside a Color Shift Estimation-and-Correction module
designed to compensate for illumination inconsistencies in natural
environments. To further improve training stability, we adopt a quantile-based
denoising strategy that downweights the top 2.5\% of highest-error pixels,
treating them as noise and suppressing their influence during optimization.
Evaluated on the official GOOSE test set, our approach achieved a mean
Intersection over Union (mIoU) of 0.848, demonstrating the effectiveness of
combining color correction, positional encoding, and error-aware denoising in
robust semantic segmentation.

</details>


### [173] [Replay-Based Continual Learning with Dual-Layered Distillation and a Streamlined U-Net for Efficient Text-to-Image Generation](https://arxiv.org/pdf/2505.06995)
*Md. Naimur Asif Borno, Md Sakib Hossain Shovon, Asmaa Soliman Al-Moisheer, Mohammad Ali Moni*

Main category: cs.CV

TL;DR: KDC-Diff is an efficient stable diffusion framework for text-to-image generation, reducing computational demands while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: High computational demands of text-to-image diffusion models limit accessibility and scalability.

Method: Introduces a streamlined U-Net with fewer parameters, dual-layered distillation for high-fidelity generation, and replay-based continual learning to retain knowledge.

Result: Achieves state-of-the-art performance on datasets like Oxford Flowers and Butterflies & Moths 100 Species, with competitive metrics and reduced inference time.

Conclusion: KDC-Diff is a highly efficient and adaptable solution for text-to-image generation in resource-constrained environments.

Abstract: Recent advancements in text-to-image diffusion models are hindered by high
computational demands, limiting accessibility and scalability. This paper
introduces KDC-Diff, a novel stable diffusion framework that enhances
efficiency while maintaining image quality. KDC-Diff features a streamlined
U-Net architecture with nearly half the parameters of the original U-Net
(482M), significantly reducing model complexity. We propose a dual-layered
distillation strategy to ensure high-fidelity generation, transferring semantic
and structural insights from a teacher to a compact student model while
minimizing quality degradation. Additionally, replay-based continual learning
is integrated to mitigate catastrophic forgetting, allowing the model to retain
prior knowledge while adapting to new data. Despite operating under extremely
low computational resources, KDC-Diff achieves state-of-the-art performance on
the Oxford Flowers and Butterflies & Moths 100 Species datasets, demonstrating
competitive metrics such as FID, CLIP, and LPIPS. Moreover, it significantly
reduces inference time compared to existing models. These results establish
KDC-Diff as a highly efficient and adaptable solution for text-to-image
generation, particularly in computationally constrained environments.

</details>


### [174] [Hallucination-Aware Multimodal Benchmark for Gastrointestinal Image Analysis with Large Vision-Language Models](https://arxiv.org/pdf/2505.07001)
*Bidur Khanal, Sandesh Pokhrel, Sanjay Bhandari, Ramesh Rana, Nikesh Shrestha, Ram Bahadur Gurung, Cristian Linte, Angus Watson, Yash Raj Shrestha, Binod Bhattarai*

Main category: cs.CV

TL;DR: The paper introduces Gut-VLM, a dataset for studying hallucination in Vision-Language Models (VLMs) for gastrointestinal image analysis, and proposes hallucination-aware finetuning to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Hallucination in VLMs, especially in medical applications, leads to unreliable outputs, necessitating better methods to detect and correct such errors.

Method: A two-stage pipeline creates Gut-VLM: ChatGPT generates reports with potential hallucinations, which medical experts then review and correct. The VLM is finetuned to detect and correct hallucinations.

Result: Hallucination-aware finetuning outperforms traditional finetuning for report generation, and benchmarks for VLMs are established.

Conclusion: The approach improves VLM reliability in medical applications, with Gut-VLM serving as a valuable resource for future research.

Abstract: Vision-Language Models (VLMs) are becoming increasingly popular in the
medical domain, bridging the gap between medical images and clinical language.
Existing VLMs demonstrate an impressive ability to comprehend medical images
and text queries to generate detailed, descriptive diagnostic medical reports.
However, hallucination--the tendency to generate descriptions that are
inconsistent with the visual content--remains a significant issue in VLMs, with
particularly severe implications in the medical field. To facilitate VLM
research on gastrointestinal (GI) image analysis and study hallucination, we
curate a multimodal image-text GI dataset: Gut-VLM. This dataset is created
using a two-stage pipeline: first, descriptive medical reports of Kvasir-v2
images are generated using ChatGPT, which introduces some hallucinated or
incorrect texts. In the second stage, medical experts systematically review
these reports, and identify and correct potential inaccuracies to ensure
high-quality, clinically reliable annotations. Unlike traditional datasets that
contain only descriptive texts, our dataset also features tags identifying
hallucinated sentences and their corresponding corrections. A common approach
to reducing hallucination in VLM is to finetune the model on a small-scale,
problem-specific dataset. However, we take a different strategy using our
dataset. Instead of finetuning the VLM solely for generating textual reports,
we finetune it to detect and correct hallucinations, an approach we call
hallucination-aware finetuning. Our results show that this approach is better
than simply finetuning for descriptive report generation. Additionally, we
conduct an extensive evaluation of state-of-the-art VLMs across several
metrics, establishing a benchmark. GitHub Repo:
https://github.com/bhattarailab/Hallucination-Aware-VLM.

</details>


### [175] [CMD: Controllable Multiview Diffusion for 3D Editing and Progressive Generation](https://arxiv.org/pdf/2505.07003)
*Peng Li, Suizhi Ma, Jialiang Chen, Yuan Liu, Chongyi Zhang, Wei Xue, Wenhan Luo, Alla Sheffer, Wenping Wang, Yike Guo*

Main category: cs.CV

TL;DR: CMD introduces a method for 3D model generation with local editing capabilities, improving control and quality.


<details>
  <summary>Details</summary>
Motivation: Current 3D generation methods lack component-level control, requiring full regeneration for any input changes.

Method: CMD uses a conditional multiview diffusion model to generate and edit 3D models part by part.

Result: CMD improves generation quality and enables efficient local editing via image edits.

Conclusion: CMD offers a flexible and high-quality solution for 3D model generation and editing.

Abstract: Recently, 3D generation methods have shown their powerful ability to automate
3D model creation. However, most 3D generation methods only rely on an input
image or a text prompt to generate a 3D model, which lacks the control of each
component of the generated 3D model. Any modifications of the input image lead
to an entire regeneration of the 3D models. In this paper, we introduce a new
method called CMD that generates a 3D model from an input image while enabling
flexible local editing of each component of the 3D model. In CMD, we formulate
the 3D generation as a conditional multiview diffusion model, which takes the
existing or known parts as conditions and generates the edited or added
components. This conditional multiview diffusion model not only allows the
generation of 3D models part by part but also enables local editing of 3D
models according to the local revision of the input image without changing
other 3D parts. Extensive experiments are conducted to demonstrate that CMD
decomposes a complex 3D generation task into multiple components, improving the
generation quality. Meanwhile, CMD enables efficient and flexible local editing
of a 3D model by just editing one rendered image.

</details>


### [176] [MELLM: Exploring LLM-Powered Micro-Expression Understanding Enhanced by Subtle Motion Perception](https://arxiv.org/pdf/2505.07007)
*Zhengye Zhang, Sirui Zhao, Shifeng Liu, Shukang Yin, Xinglong Mao, Tong Xu, Enhong Chen*

Main category: cs.CV

TL;DR: The paper introduces MELLM, a Micro-Expression Large Language Model, leveraging MLLMs for comprehensive micro-expression analysis by integrating motion perception and fine-tuning strategies.


<details>
  <summary>Details</summary>
Motivation: Current MER research lacks analysis of subtle dynamic movements and emotional cues. MLLMs' multimodal capabilities offer new potential for ME understanding.

Method: MELLM combines motion-enhanced color maps (optical flow dynamics + grayscale frames) and specialized fine-tuning. An instruction-description dataset based on FACS is used for training.

Result: MELLM shows superior robustness and generalization in ME understanding across benchmark datasets.

Conclusion: MELLM represents the first MLLM application in ME analysis, addressing challenges of subtle intensity and short duration in MEs.

Abstract: Micro-expressions (MEs) are crucial psychological responses with significant
potential for affective computing. However, current automatic micro-expression
recognition (MER) research primarily focuses on discrete emotion
classification, neglecting a convincing analysis of the subtle dynamic
movements and inherent emotional cues. The rapid progress in multimodal large
language models (MLLMs), known for their strong multimodal comprehension and
language generation abilities, offers new possibilities. MLLMs have shown
success in various vision-language tasks, indicating their potential to
understand MEs comprehensively, including both fine-grained motion patterns and
underlying emotional semantics. Nevertheless, challenges remain due to the
subtle intensity and short duration of MEs, as existing MLLMs are not designed
to capture such delicate frame-level facial dynamics. In this paper, we propose
a novel Micro-Expression Large Language Model (MELLM), which incorporates a
subtle facial motion perception strategy with the strong inference capabilities
of MLLMs, representing the first exploration of MLLMs in the domain of ME
analysis. Specifically, to explicitly guide the MLLM toward motion-sensitive
regions, we construct an interpretable motion-enhanced color map by fusing
onset-apex optical flow dynamics with the corresponding grayscale onset frame
as the model input. Additionally, specialized fine-tuning strategies are
incorporated to further enhance the model's visual perception of MEs.
Furthermore, we construct an instruction-description dataset based on Facial
Action Coding System (FACS) annotations and emotion labels to train our MELLM.
Comprehensive evaluations across multiple benchmark datasets demonstrate that
our model exhibits superior robustness and generalization capabilities in ME
understanding (MEU). Code is available at https://github.com/zyzhangUstc/MELLM.

</details>


### [177] [Efficient and Robust Multidimensional Attention in Remote Physiological Sensing through Target Signal Constrained Factorization](https://arxiv.org/pdf/2505.07013)
*Jitesh Joshi, Youngjun Cho*

Main category: cs.CV

TL;DR: The paper introduces MMRPhys, a dual-branch 3D-CNN with TSFM, for robust multitask estimation of rPPG and rRSP signals from RGB and thermal videos, outperforming state-of-the-art methods in cross-dataset evaluations.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for remote physiological sensing lack robustness to domain shifts (e.g., ambient conditions, camera specs, head movements), impacting real-world performance. Cross-dataset evaluation is needed to assess generalization.

Method: Proposes TSFM, a multidimensional attention mechanism incorporating physiological signal constraints, and MMRPhys, a dual-branch 3D-CNN for multitask estimation of rPPG and rRSP signals from multimodal video inputs.

Result: MMRPhys with TSFM outperforms state-of-the-art methods in generalization across domain shifts for rPPG and rRSP estimation, with minimal inference latency suitable for real-time applications.

Conclusion: The approach sets new benchmarks for robust multitask and multimodal physiological sensing, offering a computationally efficient framework for practical deployment, including a web browser-based real-time application.

Abstract: Remote physiological sensing using camera-based technologies offers
transformative potential for non-invasive vital sign monitoring across
healthcare and human-computer interaction domains. Although deep learning
approaches have advanced the extraction of physiological signals from video
data, existing methods have not been sufficiently assessed for their robustness
to domain shifts. These shifts in remote physiological sensing include
variations in ambient conditions, camera specifications, head movements, facial
poses, and physiological states which often impact real-world performance
significantly. Cross-dataset evaluation provides an objective measure to assess
generalization capabilities across these domain shifts. We introduce Target
Signal Constrained Factorization module (TSFM), a novel multidimensional
attention mechanism that explicitly incorporates physiological signal
characteristics as factorization constraints, allowing more precise feature
extraction. Building on this innovation, we present MMRPhys, an efficient
dual-branch 3D-CNN architecture designed for simultaneous multitask estimation
of photoplethysmography (rPPG) and respiratory (rRSP) signals from multimodal
RGB and thermal video inputs. Through comprehensive cross-dataset evaluation on
five benchmark datasets, we demonstrate that MMRPhys with TSFM significantly
outperforms state-of-the-art methods in generalization across domain shifts for
rPPG and rRSP estimation, while maintaining a minimal inference latency
suitable for real-time applications. Our approach establishes new benchmarks
for robust multitask and multimodal physiological sensing and offers a
computationally efficient framework for practical deployment in unconstrained
environments. The web browser-based application featuring on-device real-time
inference of MMRPhys model is available at
https://physiologicailab.github.io/mmrphys-live

</details>


### [178] [A Vision-Language Foundation Model for Leaf Disease Identification](https://arxiv.org/pdf/2505.07019)
*Khang Nguyen Quoc, Lan Le Thi Thu, Luyl-Da Quach*

Main category: cs.CV

TL;DR: SCOLD is a vision-language model for leaf disease identification, using contrastive learning with soft targets to improve generalization and robustness, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with integrating image and text modalities and rely on limited datasets like ImageNet, lacking domain-specific data.

Method: SCOLD uses task-agnostic pretraining with contextual soft targets to smooth labels, trained on 186,000 image-caption pairs aligned with 97 concepts.

Result: SCOLD outperforms models like OpenAI-CLIP-L, BioCLIP, and SigLIP2 in benchmarks including zero-shot and few-shot classification.

Conclusion: SCOLD advances agricultural vision-language models, offering strong performance with minimal fine-tuning, and sets groundwork for future research.

Abstract: Leaf disease identification plays a pivotal role in smart agriculture.
However, many existing studies still struggle to integrate image and textual
modalities to compensate for each other's limitations. Furthermore, many of
these approaches rely on pretraining with constrained datasets such as
ImageNet, which lack domain-specific information. We propose SCOLD (Soft-target
COntrastive learning for Leaf Disease identification), a context-aware
vision-language foundation model tailored to address these challenges for
agricultural tasks. SCOLD is developed using a diverse corpus of plant leaf
images and corresponding symptom descriptions, comprising over 186,000
image-caption pairs aligned with 97 unique concepts. Through task-agnostic
pretraining, SCOLD leverages contextual soft targets to mitigate overconfidence
in contrastive learning by smoothing labels, thereby improving model
generalization and robustness on fine-grained classification tasks.
Experimental results demonstrate that SCOLD outperforms existing
vision-language models such as OpenAI-CLIP-L, BioCLIP, and SigLIP2 across
several benchmarks, including zero-shot and few-shot classification, image-text
retrieval, and image classification, while maintaining a competitive parameter
footprint. Ablation studies further highlight SCOLD's effectiveness in contrast
to its counterparts. The proposed approach significantly advances the
agricultural vision-language foundation model, offering strong performance with
minimal or no supervised fine-tuning. This work lays a solid groundwork for
future research on models trained with long-form and simplified contexts, tasks
involving class ambiguity, and multi-modal systems for intelligent plant
disease diagnostics. The code for this study is available at
https://huggingface.co/enalis/scold

</details>


### [179] [MarkMatch: Same-Hand Stuffing Detection](https://arxiv.org/pdf/2505.07032)
*Fei Zhao, Runlin Zhang, Chengcui Zhang, Nitesh Saxena*

Main category: cs.CV

TL;DR: MarkMatch is a retrieval system for detecting if two paper ballot marks were made by the same hand, outperforming BubbleSig with an F1 score of 0.943 using contrastive learning and flexible mark extraction via SAM.


<details>
  <summary>Details</summary>
Motivation: To improve the detection of stylistic similarity in ballot marks for election audits, addressing limitations of the previous SOTA method, BubbleSig.

Method: Uses contrastive learning with a dense batch similarity matrix and dual loss objective, integrating Segment Anything Model (SAM) for mark extraction.

Result: Achieves an F1 score of 0.943, surpassing BubbleSig's performance.

Conclusion: MarkMatch provides a practical, non-biometric tool for election auditors to investigate suspicious ballots.

Abstract: We present MarkMatch, a retrieval system for detecting whether two paper
ballot marks were filled by the same hand. Unlike the previous SOTA method
BubbleSig, which used binary classification on isolated mark pairs, MarkMatch
ranks stylistic similarity between a query mark and a mark in the database
using contrastive learning. Our model is trained with a dense batch similarity
matrix and a dual loss objective. Each sample is contrasted against many
negatives within each batch, enabling the model to learn subtle handwriting
difference and improve generalization under handwriting variation and visual
noise, while diagonal supervision reinforces high confidence on true matches.
The model achieves an F1 score of 0.943, surpassing BubbleSig's best
performance. MarkMatch also integrates Segment Anything Model for flexible mark
extraction via box- or point-based prompts. The system offers election auditors
a practical tool for visual, non-biometric investigation of suspicious ballots.

</details>


### [180] [Differentiable NMS via Sinkhorn Matching for End-to-End Fabric Defect Detection](https://arxiv.org/pdf/2505.07040)
*Zhengyang Lu, Bingjie Lu, Weifan Wang, Feng Wang*

Main category: cs.CV

TL;DR: A differentiable NMS framework for fabric defect detection improves localization precision via end-to-end optimization, addressing gradient flow disruption and costly annotations.


<details>
  <summary>Details</summary>
Motivation: Challenges include disrupted gradient flow from conventional NMS and high costs of pixel-level annotations at scale.

Method: Reformulates NMS as a differentiable bipartite matching problem using the Sinkhorn-Knopp algorithm, integrating proposal quality, feature similarity, and spatial relationships. Includes an entropy-constrained mask refinement mechanism.

Result: Significant performance improvements on the Tianchi fabric defect dataset, maintaining real-time speeds and adaptability across architectures.

Conclusion: The framework enhances localization precision and generalizes well to general object detection tasks, suitable for industrial deployment.

Abstract: Fabric defect detection confronts two fundamental challenges. First,
conventional non-maximum suppression disrupts gradient flow, which hinders
genuine end-to-end learning. Second, acquiring pixel-level annotations at
industrial scale is prohibitively costly. Addressing these limitations, we
propose a differentiable NMS framework for fabric defect detection that
achieves superior localization precision through end-to-end optimization. We
reformulate NMS as a differentiable bipartite matching problem solved through
the Sinkhorn-Knopp algorithm, maintaining uninterrupted gradient flow
throughout the network. This approach specifically targets the irregular
morphologies and ambiguous boundaries of fabric defects by integrating proposal
quality, feature similarity, and spatial relationships. Our entropy-constrained
mask refinement mechanism further enhances localization precision through
principled uncertainty modeling. Extensive experiments on the Tianchi fabric
defect dataset demonstrate significant performance improvements over existing
methods while maintaining real-time speeds suitable for industrial deployment.
The framework exhibits remarkable adaptability across different architectures
and generalizes effectively to general object detection tasks.

</details>


### [181] [Depth-Sensitive Soft Suppression with RGB-D Inter-Modal Stylization Flow for Domain Generalization Semantic Segmentation](https://arxiv.org/pdf/2505.07050)
*Binbin Wei, Yuhang Zhang, Shishun Tian, Muxin Liao, Wei Li, Wenbin Zou*

Main category: cs.CV

TL;DR: The paper proposes DSSS, a framework for Domain Generalization (DG) semantic segmentation, leveraging depth maps and RGB data to learn domain-invariant features by addressing noise in depth maps and integrating RGB-D stylization.


<details>
  <summary>Details</summary>
Motivation: Existing methods in Unsupervised Domain Adaptation (UDA) and Domain Generalization (DG) struggle with noisy depth maps and fail to fully utilize RGB-D data for domain-invariant feature learning.

Method: The DSSS framework includes RGB-D inter-modal stylization flow for generating stylized depth maps, class-wise soft spatial sensitivity suppression for emphasizing domain-invariant features, and RGB-D soft alignment loss to retain depth uniqueness.

Result: Extensive experiments demonstrate significant performance improvement in DG semantic segmentation across multiple backbone networks.

Conclusion: DSSS is the first RGB-D integrated framework for multi-class DG semantic segmentation, effectively addressing depth map noise and enhancing domain-invariant learning.

Abstract: Unsupervised Domain Adaptation (UDA) aims to align source and target domain
distributions to close the domain gap, but still struggles with obtaining the
target data. Fortunately, Domain Generalization (DG) excels without the need
for any target data. Recent works expose that depth maps contribute to improved
generalized performance in the UDA tasks, but they ignore the noise and holes
in depth maps due to device and environmental factors, failing to sufficiently
and effectively learn domain-invariant representation. Although
high-sensitivity region suppression has shown promising results in learning
domain-invariant features, existing methods cannot be directly applicable to
depth maps due to their unique characteristics. Hence, we propose a novel
framework, namely Depth-Sensitive Soft Suppression with RGB-D inter-modal
stylization flow (DSSS), focusing on learning domain-invariant features from
depth maps for the DG semantic segmentation. Specifically, we propose the RGB-D
inter-modal stylization flow to generate stylized depth maps for sensitivity
detection, cleverly utilizing RGB information as the stylization source. Then,
a class-wise soft spatial sensitivity suppression is designed to identify and
emphasize non-sensitive depth features that contain more domain-invariant
information. Furthermore, an RGB-D soft alignment loss is proposed to ensure
that the stylized depth maps only align part of the RGB features while still
retaining the unique depth information. To our best knowledge, our DSSS
framework is the first work to integrate RGB and Depth information in the
multi-class DG semantic segmentation task. Extensive experiments over multiple
backbone networks show that our framework achieves remarkable performance
improvement.

</details>


### [182] [DAPE: Dual-Stage Parameter-Efficient Fine-Tuning for Consistent Video Editing with Diffusion Models](https://arxiv.org/pdf/2505.07057)
*Junhao Xia, Chaoyang Zhang, Yecheng Zhang, Chengyang Zhou, Zhichang Wang, Bochun Liu, Dongshuo Yin*

Main category: cs.CV

TL;DR: DAPE is a two-stage PEFT framework for video editing, improving temporal consistency and visual quality while addressing benchmark shortcomings with a new dataset.


<details>
  <summary>Details</summary>
Motivation: Addressing the high computational costs of training-based video editing methods and the suboptimal performance of training-free alternatives.

Method: A two-stage framework: norm-tuning for temporal consistency and a vision-friendly adapter for visual quality. Introduces a new benchmark dataset.

Result: DAPE outperforms state-of-the-art methods in temporal coherence and text-video alignment.

Conclusion: DAPE offers a cost-effective, high-quality solution for video editing, validated by extensive experiments.

Abstract: Video generation based on diffusion models presents a challenging multimodal
task, with video editing emerging as a pivotal direction in this field. Recent
video editing approaches primarily fall into two categories: training-required
and training-free methods. While training-based methods incur high
computational costs, training-free alternatives often yield suboptimal
performance. To address these limitations, we propose DAPE, a high-quality yet
cost-effective two-stage parameter-efficient fine-tuning (PEFT) framework for
video editing. In the first stage, we design an efficient norm-tuning method to
enhance temporal consistency in generated videos. The second stage introduces a
vision-friendly adapter to improve visual quality. Additionally, we identify
critical shortcomings in existing benchmarks, including limited category
diversity, imbalanced object distribution, and inconsistent frame counts. To
mitigate these issues, we curate a large dataset benchmark comprising 232
videos with rich annotations and 6 editing prompts, enabling objective and
comprehensive evaluation of advanced methods. Extensive experiments on existing
datasets (BalanceCC, LOVEU-TGVE, RAVE) and our proposed benchmark demonstrate
that DAPE significantly improves temporal coherence and text-video alignment
while outperforming previous state-of-the-art approaches.

</details>


### [183] [Seed1.5-VL Technical Report](https://arxiv.org/pdf/2505.07062)
*Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, Jingji Chen, Jingjia Huang, Kang Lei, Liping Yuan, Lishu Luo, Pengfei Liu, Qinghao Ye, Rui Qian, Shen Yan, Shixiong Zhao, Shuai Peng, Shuangye Li, Sihang Yuan, Sijin Wu, Tianheng Cheng, Weiwei Liu, Wenqian Wang, Xianhan Zeng, Xiao Liu, Xiaobo Qin, Xiaohan Ding, Xiaojun Xiao, Xiaoying Zhang, Xuanwei Zhang, Xuehan Xiong, Yanghua Peng, Yangrui Chen, Yanwei Li, Yanxu Hu, Yi Lin, Yiyuan Hu, Yiyuan Zhang, Youbin Wu, Yu Li, Yudong Liu, Yue Ling, Yujia Qin, Zanbo Wang, Zhiwu He, Aoxue Zhang, Bairen Yi, Bencheng Liao, Can Huang, Can Zhang, Chaorui Deng, Chaoyi Deng, Cheng Lin, Cheng Yuan, Chenggang Li, Chenhui Gou, Chenwei Lou, Chengzhi Wei, Chundian Liu, Chunyuan Li, Deyao Zhu, Donghong Zhong, Feng Li, Feng Zhang, Gang Wu, Guodong Li, Guohong Xiao, Haibin Lin, Haihua Yang, Haoming Wang, Heng Ji, Hongxiang Hao, Hui Shen, Huixia Li, Jiahao Li, Jialong Wu, Jianhua Zhu, Jianpeng Jiao, Jiashi Feng, Jiaze Chen, Jianhui Duan, Jihao Liu, Jin Zeng, Jingqun Tang, Jingyu Sun, Joya Chen, Jun Long, Junda Feng, Junfeng Zhan, Junjie Fang, Junting Lu, Kai Hua, Kai Liu, Kai Shen, Kaiyuan Zhang, Ke Shen, Ke Wang, Keyu Pan, Kun Zhang, Kunchang Li, Lanxin Li, Lei Li, Lei Shi, Li Han, Liang Xiang, Liangqiang Chen, Lin Chen, Lin Li, Lin Yan, Liying Chi, Longxiang Liu, Mengfei Du, Mingxuan Wang, Ningxin Pan, Peibin Chen, Pengfei Chen, Pengfei Wu, Qingqing Yuan, Qingyao Shuai, Qiuyan Tao, Renjie Zheng, Renrui Zhang, Ru Zhang, Rui Wang, Rui Yang, Rui Zhao, Shaoqiang Xu, Shihao Liang, Shipeng Yan, Shu Zhong, Shuaishuai Cao, Shuangzhi Wu, Shufan Liu, Shuhan Chang, Songhua Cai, Tenglong Ao, Tianhao Yang, Tingting Zhang, Wanjun Zhong, Wei Jia, Wei Weng, Weihao Yu, Wenhao Huang, Wenjia Zhu, Wenli Yang, Wenzhi Wang, Xiang Long, XiangRui Yin, Xiao Li, Xiaolei Zhu, Xiaoying Jia, Xijin Zhang, Xin Liu, Xinchen Zhang, Xinyu Yang, Xiongcai Luo, Xiuli Chen, Xuantong Zhong, Xuefeng Xiao, Xujing Li, Yan Wu, Yawei Wen, Yifan Du, Yihao Zhang, Yining Ye, Yonghui Wu, Yu Liu, Yu Yue, Yufeng Zhou, Yufeng Yuan, Yuhang Xu, Yuhong Yang, Yun Zhang, Yunhao Fang, Yuntao Li, Yurui Ren, Yuwen Xiong, Zehua Hong, Zehua Wang, Zewei Sun, Zeyu Wang, Zhao Cai, Zhaoyue Zha, Zhecheng An, Zhehui Zhao, Zhengzhuo Xu, Zhipeng Chen, Zhiyong Wu, Zhuofan Zheng, Zihao Wang, Zilong Huang, Ziyu Zhu, Zuquan Song*

Main category: cs.CV

TL;DR: Seed1.5-VL is a compact yet powerful vision-language model achieving state-of-the-art performance on 38/60 benchmarks and excelling in agent-centric tasks and multimodal reasoning.


<details>
  <summary>Details</summary>
Motivation: To advance general-purpose multimodal understanding and reasoning with a scalable and efficient model.

Method: Combines a 532M-parameter vision encoder with a 20B-parameter Mixture-of-Experts LLM.

Result: Outperforms leading systems like OpenAI CUA and Claude 3.7 in tasks like GUI control, gameplay, and visual puzzles.

Conclusion: Seed1.5-VL's capabilities enable diverse applications, and its development insights aim to inspire further research.

Abstract: We present Seed1.5-VL, a vision-language foundation model designed to advance
general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed
with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B
active parameters. Despite its relatively compact architecture, it delivers
strong performance across a wide spectrum of public VLM benchmarks and internal
evaluation suites, achieving the state-of-the-art performance on 38 out of 60
public benchmarks. Moreover, in agent-centric tasks such as GUI control and
gameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI
CUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates
strong reasoning abilities, making it particularly effective for multimodal
reasoning challenges such as visual puzzles. We believe these capabilities will
empower broader applications across diverse tasks. In this report, we mainly
provide a comprehensive review of our experiences in building Seed1.5-VL across
model design, data construction, and training at various stages, hoping that
this report can inspire further research. Seed1.5-VL is now accessible at
https://www.volcengine.com/ (Volcano Engine Model ID:
doubao-1-5-thinking-vision-pro-250428)

</details>


### [184] [Through the Looking Glass: Common Sense Consistency Evaluation of Weird Images](https://arxiv.org/pdf/2505.07704)
*Elisei Rykov, Kseniia Petrushina, Kseniia Titova, Anton Razzhigaev, Alexander Panchenko, Vasily Konovalov*

Main category: cs.CV

TL;DR: TLG method uses LVLMs and Transformer-based encoders to assess image common sense consistency, achieving state-of-the-art results on WHOOPS! and WEIRD datasets.


<details>
  <summary>Details</summary>
Motivation: Measuring image common sense consistency is complex, especially for nonsensical images like a boy with a vacuum cleaner in a desert.

Method: Leverages LVLMs to extract atomic facts from images and fine-tunes a compact attention-pooling classifier over encoded facts.

Result: Achieves state-of-the-art performance on WHOOPS! and WEIRD datasets.

Conclusion: TLG is effective for assessing image common sense consistency with minimal fine-tuning.

Abstract: Measuring how real images look is a complex task in artificial intelligence
research. For example, an image of a boy with a vacuum cleaner in a desert
violates common sense. We introduce a novel method, which we call Through the
Looking Glass (TLG), to assess image common sense consistency using Large
Vision-Language Models (LVLMs) and Transformer-based encoder. By leveraging
LVLMs to extract atomic facts from these images, we obtain a mix of accurate
facts. We proceed by fine-tuning a compact attention-pooling classifier over
encoded atomic facts. Our TLG has achieved a new state-of-the-art performance
on the WHOOPS! and WEIRD datasets while leveraging a compact fine-tuning
component.

</details>


### [185] [Semantic-Guided Diffusion Model for Single-Step Image Super-Resolution](https://arxiv.org/pdf/2505.07071)
*Zihang Liu, Zhenyu Zhang, Hao Tang*

Main category: cs.CV

TL;DR: SAMSR improves diffusion-based image super-resolution by integrating semantic segmentation masks, enhancing detail recovery in complex regions.


<details>
  <summary>Details</summary>
Motivation: Existing single-step diffusion models struggle with complex semantic regions, limiting efficiency and detail preservation.

Method: SAMSR uses semantic masks to refine noise (SAM-Noise Module) and a pixel-wise sampling strategy with dynamic adjustments. A semantic consistency loss aligns predictions with ground truth.

Result: SAMSR outperforms in perceptual quality and detail recovery, especially in semantically complex images.

Conclusion: SAMSR effectively addresses limitations of single-step diffusion models by leveraging semantic guidance, improving performance in complex scenarios.

Abstract: Diffusion-based image super-resolution (SR) methods have demonstrated
remarkable performance. Recent advancements have introduced deterministic
sampling processes that reduce inference from 15 iterative steps to a single
step, thereby significantly improving the inference speed of existing diffusion
models. However, their efficiency remains limited when handling complex
semantic regions due to the single-step inference. To address this limitation,
we propose SAMSR, a semantic-guided diffusion framework that incorporates
semantic segmentation masks into the sampling process. Specifically, we
introduce the SAM-Noise Module, which refines Gaussian noise using segmentation
masks to preserve spatial and semantic features. Furthermore, we develop a
pixel-wise sampling strategy that dynamically adjusts the residual transfer
rate and noise strength based on pixel-level semantic weights, prioritizing
semantically rich regions during the diffusion process. To enhance model
training, we also propose a semantic consistency loss, which aligns pixel-wise
semantic weights between predictions and ground truth. Extensive experiments on
both real-world and synthetic datasets demonstrate that SAMSR significantly
improves perceptual quality and detail recovery, particularly in semantically
complex images. Our code is released at https://github.com/Liu-Zihang/SAMSR.

</details>


### [186] [Discovering Concept Directions from Diffusion-based Counterfactuals via Latent Clustering](https://arxiv.org/pdf/2505.07073)
*Payal Varshney, Adriano Lucieri, Christoph Balada, Andreas Dengel, Sheraz Ahmed*

Main category: cs.CV

TL;DR: CDLC improves concept-based explanations by clustering latent vectors, reducing computational complexity and capturing multidimensional concepts, validated on skin lesion data.


<details>
  <summary>Details</summary>
Motivation: Existing methods for concept-based explanations are computationally intensive and inefficient for complex concepts.

Method: CDLC clusters latent difference vectors from factual and counterfactual image pairs to extract global, class-specific concept directions.

Result: CDLC aligns with clinical features and reveals biases or biomarkers, showing interpretability and scalability.

Conclusion: CDLC is a scalable, interpretable method for extracting semantic concepts, applicable in high-stakes domains.

Abstract: Concept-based explanations have emerged as an effective approach within
Explainable Artificial Intelligence, enabling interpretable insights by
aligning model decisions with human-understandable concepts. However, existing
methods rely on computationally intensive procedures and struggle to
efficiently capture complex, semantic concepts. Recently, the Concept Discovery
through Latent Diffusion-based Counterfactual Trajectories (CDCT) framework,
introduced by Varshney et al. (2025), attempts to identify concepts via
dimension-wise traversal of the latent space of a Variational Autoencoder
trained on counterfactual trajectories. Extending the CDCT framework, this work
introduces Concept Directions via Latent Clustering (CDLC), which extracts
global, class-specific concept directions by clustering latent difference
vectors derived from factual and diffusion-generated counterfactual image
pairs. CDLC substantially reduces computational complexity by eliminating the
exhaustive latent dimension traversal required in CDCT and enables the
extraction of multidimensional semantic concepts encoded across the latent
dimensions. This approach is validated on a real-world skin lesion dataset,
demonstrating that the extracted concept directions align with clinically
recognized dermoscopic features and, in some cases, reveal dataset-specific
biases or unknown biomarkers. These results highlight that CDLC is
interpretable, scalable, and applicable across high-stakes domains and diverse
data modalities.

</details>


### [187] [Towards Scalable IoT Deployment for Visual Anomaly Detection via Efficient Compression](https://arxiv.org/pdf/2505.07119)
*Arianna Stropeni, Francesco Borsatti, Manuel Barusco, Davide Dalle Pezze, Marco Fabris, Gian Antonio Susto*

Main category: cs.CV

TL;DR: The paper explores efficient Visual Anomaly Detection (VAD) in IoT environments, focusing on data compression to balance latency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Industrial settings require VAD to reduce waste and costs, but IoT constraints like limited computational power and bandwidth pose challenges.

Method: The study evaluates various data compression techniques to optimize VAD performance on edge devices.

Result: Experiments on MVTec AD show significant compression is possible with minimal impact on anomaly detection accuracy.

Conclusion: Compact processing strategies enable effective VAD in resource-limited IoT environments.

Abstract: Visual Anomaly Detection (VAD) is a key task in industrial settings, where
minimizing waste and operational costs is essential. Deploying deep learning
models within Internet of Things (IoT) environments introduces specific
challenges due to the limited computational power and bandwidth of edge
devices. This study investigates how to perform VAD effectively under such
constraints by leveraging compact and efficient processing strategies. We
evaluate several data compression techniques, examining the trade-off between
system latency and detection accuracy. Experiments on the MVTec AD benchmark
demonstrate that significant compression can be achieved with minimal loss in
anomaly detection performance compared to uncompressed data.

</details>


### [188] [Generalizable Pancreas Segmentation via a Dual Self-Supervised Learning Framework](https://arxiv.org/pdf/2505.07165)
*Jun Li, Hongzhang Zhu, Tao Chen, Xiaohua Qian*

Main category: cs.CV

TL;DR: A dual self-supervised learning model improves pancreas segmentation generalization by leveraging global and local anatomical contexts.


<details>
  <summary>Details</summary>
Motivation: Existing pancreas segmentation methods lack generalizability across datasets. The goal is to enhance performance on single-source datasets for broader applicability.

Method: Proposes a dual self-supervised learning model: (1) global-feature contrastive learning for pancreatic structure, and (2) local-image restoration for high-uncertainty regions.

Result: The model enhances feature discrimination and robustness in high-uncertainty regions, improving generalization.

Conclusion: The dual self-supervised approach effectively addresses generalization challenges in pancreas segmentation.

Abstract: Recently, numerous pancreas segmentation methods have achieved promising
performance on local single-source datasets. However, these methods don't
adequately account for generalizability issues, and hence typically show
limited performance and low stability on test data from other sources.
Considering the limited availability of distinct data sources, we seek to
improve the generalization performance of a pancreas segmentation model trained
with a single-source dataset, i.e., the single source generalization task. In
particular, we propose a dual self-supervised learning model that incorporates
both global and local anatomical contexts. Our model aims to fully exploit the
anatomical features of the intra-pancreatic and extra-pancreatic regions, and
hence enhance the characterization of the high-uncertainty regions for more
robust generalization. Specifically, we first construct a global-feature
contrastive self-supervised learning module that is guided by the pancreatic
spatial structure. This module obtains complete and consistent pancreatic
features through promoting intra-class cohesion, and also extracts more
discriminative features for differentiating between pancreatic and
non-pancreatic tissues through maximizing inter-class separation. It mitigates
the influence of surrounding tissue on the segmentation outcomes in
high-uncertainty regions. Subsequently, a local-image restoration
self-supervised learning module is introduced to further enhance the
characterization of the high uncertainty regions. In this module, informative
anatomical contexts are actually learned to recover randomly corrupted
appearance patterns in those regions.

</details>


### [189] [Critique Before Thinking: Mitigating Hallucination through Rationale-Augmented Instruction Tuning](https://arxiv.org/pdf/2505.07172)
*Zexian Yang, Dian Li, Dayan Wu, Gang Liu, Weiping Wang*

Main category: cs.CV

TL;DR: Re-Critic is a scalable framework that enhances multimodal reasoning in LVLMs by incorporating visual rationale and self-critique, reducing visually ungrounded responses.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs lack preparatory learning principles like humans, leading to ungrounded responses. Re-Critic aims to bridge this gap.

Method: Re-Critic uses a visual rationale synthesizer and in-context self-critic mechanism to augment instructions and select response pairs for tuning.

Result: Models fine-tuned with Re-Critic show improved performance in hallucination-specific and broader multimodal reasoning tasks.

Conclusion: Re-Critic effectively grounds LVLM responses by integrating human-like preparatory principles, enhancing reasoning capabilities.

Abstract: Despite significant advancements in multimodal reasoning tasks, existing
Large Vision-Language Models (LVLMs) are prone to producing visually ungrounded
responses when interpreting associated images. In contrast, when humans embark
on learning new knowledge, they often rely on a set of fundamental pre-study
principles: reviewing outlines to grasp core concepts, summarizing key points
to guide their focus and enhance understanding. However, such preparatory
actions are notably absent in the current instruction tuning processes. This
paper presents Re-Critic, an easily scalable rationale-augmented framework
designed to incorporate fundamental rules and chain-of-thought (CoT) as a
bridge to enhance reasoning abilities. Specifically, Re-Critic develops a
visual rationale synthesizer that scalably augments raw instructions with
rationale explanation. To probe more contextually grounded responses, Re-Critic
employs an in-context self-critic mechanism to select response pairs for
preference tuning. Experiments demonstrate that models fine-tuned with our
rationale-augmented dataset yield gains that extend beyond
hallucination-specific tasks to broader multimodal reasoning tasks.

</details>


### [190] [Ranking-aware Continual Learning for LiDAR Place Recognition](https://arxiv.org/pdf/2505.07198)
*Xufei Wang, Gengxuan Tian, Junqiao Zhao, Siyue Tao, Qiwen Gu, Qiankun Yu, Tiantian Feng*

Main category: cs.CV

TL;DR: A continual learning framework (KDF) for LiDAR place recognition (LPR) addresses catastrophic forgetting using knowledge distillation and fusion, improving performance on previously trained places.


<details>
  <summary>Details</summary>
Motivation: Existing learning-based LPR methods suffer from catastrophic forgetting when trained on new environments, degrading performance on old places.

Method: Proposes a ranking-aware knowledge distillation loss and a knowledge fusion module to integrate old and new model knowledge.

Result: KDF outperforms state-of-the-art methods in mean Recall@1 and forgetting score.

Conclusion: KDF effectively mitigates catastrophic forgetting in LPR, enhancing continual learning performance.

Abstract: Place recognition plays a significant role in SLAM, robot navigation, and
autonomous driving applications. Benefiting from deep learning, the performance
of LiDAR place recognition (LPR) has been greatly improved. However, many
existing learning-based LPR methods suffer from catastrophic forgetting, which
severely harms the performance of LPR on previously trained places after
training on a new environment. In this paper, we introduce a continual learning
framework for LPR via Knowledge Distillation and Fusion (KDF) to alleviate
forgetting. Inspired by the ranking process of place recognition retrieval, we
present a ranking-aware knowledge distillation loss that encourages the network
to preserve the high-level place recognition knowledge. We also introduce a
knowledge fusion module to integrate the knowledge of old and new models for
LiDAR place recognition. Our extensive experiments demonstrate that KDF can be
applied to different networks to overcome catastrophic forgetting, surpassing
the state-of-the-art methods in terms of mean Recall@1 and forgetting score.

</details>


### [191] [Discovering Fine-Grained Visual-Concept Relations by Disentangled Optimal Transport Concept Bottleneck Models](https://arxiv.org/pdf/2505.07209)
*Yan Xie, Zequn Zeng, Hao Zhang, Yucheng Ding, Yi Wang, Zhengjue Wang, Bo Chen, Hongwei Liu*

Main category: cs.CV

TL;DR: DOT-CBM improves CBMs by using fine-grained visual-concept relations via optimal transport and orthogonal losses, enhancing reliability and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing CBMs lack local image information, causing spurious relations and unclear visual explanations.

Method: Proposes DOT-CBM, modeling concept prediction as a transportation problem between patches and concepts, with orthogonal losses and transportation priors.

Result: Achieves SOTA in image classification, part detection, and OOD generalization, with reliable concept predictions.

Conclusion: DOT-CBM advances CBMs by addressing local feature alignment and bias issues, improving performance and interpretability.

Abstract: Concept Bottleneck Models (CBMs) try to make the decision-making process
transparent by exploring an intermediate concept space between the input image
and the output prediction. Existing CBMs just learn coarse-grained relations
between the whole image and the concepts, less considering local image
information, leading to two main drawbacks: i) they often produce spurious
visual-concept relations, hence decreasing model reliability; and ii) though
CBMs could explain the importance of every concept to the final prediction, it
is still challenging to tell which visual region produces the prediction. To
solve these problems, this paper proposes a Disentangled Optimal Transport CBM
(DOT-CBM) framework to explore fine-grained visual-concept relations between
local image patches and concepts. Specifically, we model the concept prediction
process as a transportation problem between the patches and concepts, thereby
achieving explicit fine-grained feature alignment. We also incorporate
orthogonal projection losses within the modality to enhance local feature
disentanglement. To further address the shortcut issues caused by statistical
biases in the data, we utilize the visual saliency map and concept label
statistics as transportation priors. Thus, DOT-CBM can visualize inversion
heatmaps, provide more reliable concept predictions, and produce more accurate
class predictions. Comprehensive experiments demonstrate that our proposed
DOT-CBM achieves SOTA performance on several tasks, including image
classification, local part detection and out-of-distribution generalization.

</details>


### [192] [When Dance Video Archives Challenge Computer Vision](https://arxiv.org/pdf/2505.07249)
*Philippe Colantoni, Rafique Ahmed, Prashant Ghimire, Damien Muselet, Alain Trémeau*

Main category: cs.CV

TL;DR: A new 3D human body pose estimation pipeline for dance videos was developed and tested, addressing challenges in pose estimation accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To explore how dance videos challenge pose estimation techniques and improve accuracy and efficiency.

Method: Combined up-to-date techniques and novel methods for dance analysis, tested on dance video archives, and used visual analytic tools.

Result: Results evaluated the impact of data parameters on pose estimation and are publicly available.

Conclusion: The proposed pipeline effectively addresses pose estimation challenges in dance videos, with results shared for further research.

Abstract: The accuracy and efficiency of human body pose estimation depend on the
quality of the data to be processed and of the particularities of these data.
To demonstrate how dance videos can challenge pose estimation techniques, we
proposed a new 3D human body pose estimation pipeline which combined up-to-date
techniques and methods that had not been yet used in dance analysis. Second, we
performed tests and extensive experimentations from dance video archives, and
used visual analytic tools to evaluate the impact of several data parameters on
human body pose. Our results are publicly available for research at
https://www.couleur.org/articles/arXiv-1-2025/

</details>


### [193] [Incomplete In-context Learning](https://arxiv.org/pdf/2505.07251)
*Wenqiang Wang, Yangshijie Zhang*

Main category: cs.CV

TL;DR: The paper introduces Incomplete In-context Learning (IICL) for LVLMs, proposes IJIP to address it, and shows superior performance even with full labels.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of incomplete retrieval databases in real-world LVLM applications.

Method: Proposes IJIP, a two-stage framework: Iterative Judgments (binary tasks) and Integrated Prediction (refining accuracy).

Result: Achieves 93.9% accuracy, outperforming baselines even with full labels. Adaptable to Prompt Learning and text domains.

Conclusion: IJIP effectively mitigates IICL limitations and enhances LVLM performance across diverse conditions.

Abstract: Large vision language models (LVLMs) achieve remarkable performance through
Vision In-context Learning (VICL), a process that depends significantly on
demonstrations retrieved from an extensive collection of annotated examples
(retrieval database). Existing studies often assume that the retrieval database
contains annotated examples for all labels. However, in real-world scenarios,
delays in database updates or incomplete data annotation may result in the
retrieval database containing labeled samples for only a subset of classes. We
refer to this phenomenon as an \textbf{incomplete retrieval database} and
define the in-context learning under this condition as \textbf{Incomplete
In-context Learning (IICL)}. To address this challenge, we propose
\textbf{Iterative Judgments and Integrated Prediction (IJIP)}, a two-stage
framework designed to mitigate the limitations of IICL. The Iterative Judgments
Stage reformulates an \(\boldsymbol{m}\)-class classification problem into a
series of \(\boldsymbol{m}\) binary classification tasks, effectively
converting the IICL setting into a standard VICL scenario. The Integrated
Prediction Stage further refines the classification process by leveraging both
the input image and the predictions from the Iterative Judgments Stage to
enhance overall classification accuracy. IJIP demonstrates considerable
performance across two LVLMs and two datasets under three distinct conditions
of label incompleteness, achieving the highest accuracy of 93.9\%. Notably,
even in scenarios where labels are fully available, IJIP still achieves the
best performance of all six baselines. Furthermore, IJIP can be directly
applied to \textbf{Prompt Learning} and is adaptable to the \textbf{text
domain}.

</details>


### [194] [Towards Accurate State Estimation: Kalman Filter Incorporating Motion Dynamics for 3D Multi-Object Tracking](https://arxiv.org/pdf/2505.07254)
*Mohamed Nagy, Naoufel Werghi, Bilal Hassan, Jorge Dias, Majid Khonji*

Main category: cs.CV

TL;DR: A novel Kalman filter formulation improves 3D multi-object tracking by adapting motion models to dynamic object movements, outperforming benchmarks on KITTI and Waymo datasets.


<details>
  <summary>Details</summary>
Motivation: Addresses imprecise state estimation and motion model selection in Kalman filters for 3D MOT, especially under occlusion.

Method: Introduces a Kalman filter that adaptively adjusts motion models based on object dynamics.

Result: Outperforms benchmarks with 0.56% HOTA and 0.81% MOTA improvements, handles occlusions better, and adds minimal processing time (0.078 ms/frame).

Conclusion: The adaptive Kalman filter enhances tracking accuracy and efficiency, suitable for real-time applications.

Abstract: This work addresses the critical lack of precision in state estimation in the
Kalman filter for 3D multi-object tracking (MOT) and the ongoing challenge of
selecting the appropriate motion model. Existing literature commonly relies on
constant motion models for estimating the states of objects, neglecting the
complex motion dynamics unique to each object. Consequently, trajectory
division and imprecise object localization arise, especially under occlusion
conditions. The core of these challenges lies in the limitations of the current
Kalman filter formulation, which fails to account for the variability of motion
dynamics as objects navigate their environments. This work introduces a novel
formulation of the Kalman filter that incorporates motion dynamics, allowing
the motion model to adaptively adjust according to changes in the object's
movement. The proposed Kalman filter substantially improves state estimation,
localization, and trajectory prediction compared to the traditional Kalman
filter. This is reflected in tracking performance that surpasses recent
benchmarks on the KITTI and Waymo Open Datasets, with margins of 0.56\% and
0.81\% in higher order tracking accuracy (HOTA) and multi-object tracking
accuracy (MOTA), respectively. Furthermore, the proposed Kalman filter
consistently outperforms the baseline across various detectors. Additionally,
it shows an enhanced capability in managing long occlusions compared to the
baseline Kalman filter, achieving margins of 1.22\% in higher order tracking
accuracy (HOTA) and 1.55\% in multi-object tracking accuracy (MOTA) on the
KITTI dataset. The formulation's efficiency is evident, with an additional
processing time of only approximately 0.078 ms per frame, ensuring its
applicability in real-time applications.

</details>


### [195] [Synthetic Similarity Search in Automotive Production](https://arxiv.org/pdf/2505.07256)
*Christoph Huber, Ludwig Schleeh, Dino Knoll, Michael Guthe*

Main category: cs.CV

TL;DR: A novel image classification pipeline using similarity search with synthetic data reduces the need for large annotated datasets in automotive visual inspections.


<details>
  <summary>Details</summary>
Motivation: Traditional CV models require costly, time-consuming annotated datasets, prompting a need for a more efficient solution.

Method: Combines similarity search using DINOv2 for feature extraction and synthetic data as references, avoiding reliance on real images.

Result: Achieves high classification accuracy in eight real-world inspection scenarios, meeting production standards.

Conclusion: The pipeline offers a cost-effective, reliable alternative to traditional CV models for visual inspections.

Abstract: Visual quality inspection in automotive production is essential for ensuring
the safety and reliability of vehicles. Computer vision (CV) has become a
popular solution for these inspections due to its cost-effectiveness and
reliability. However, CV models require large, annotated datasets, which are
costly and time-consuming to collect. To reduce the need for extensive training
data, we propose a novel image classification pipeline that combines similarity
search using a vision-based foundation model with synthetic data. Our approach
leverages a DINOv2 model to transform input images into feature vectors, which
are then compared to pre-classified reference images using cosine distance
measurements. By utilizing synthetic data instead of real images as references,
our pipeline achieves high classification accuracy without relying on real
data. We evaluate this approach in eight real-world inspection scenarios and
demonstrate that it meets the high performance requirements of production
environments.

</details>


### [196] [Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning](https://arxiv.org/pdf/2505.07263)
*Xiaokun Wang, Chris, Jiangbo Pei, Wei Shen, Yi Peng, Yunzhuo Hao, Weijie Qiu, Ai Jian, Tianyidan Xie, Xuchen Song, Yang Liu, Yahui Zhou*

Main category: cs.CV

TL;DR: Skywork-VL Reward is a multimodal reward model for understanding and reasoning tasks, leveraging a large-scale dataset and advanced architecture to achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To advance general-purpose, reliable reward models for multimodal alignment by providing robust reward signals for both understanding and reasoning tasks.

Method: Constructs a large-scale multimodal preference dataset and designs a reward model architecture based on Qwen2.5-VL-7B-Instruct, using multi-stage fine-tuning with pairwise ranking loss.

Result: Achieves state-of-the-art results on VL-RewardBench and competitive performance on RewardBench. Preference data from Skywork-VL Reward enhances Mixed Preference Optimization (MPO) for multimodal reasoning.

Conclusion: Skywork-VL Reward represents a significant advancement in multimodal reward modeling, with publicly released models to ensure transparency and reproducibility.

Abstract: We propose Skywork-VL Reward, a multimodal reward model that provides reward
signals for both multimodal understanding and reasoning tasks. Our technical
approach comprises two key components: First, we construct a large-scale
multimodal preference dataset that covers a wide range of tasks and scenarios,
with responses collected from both standard vision-language models (VLMs) and
advanced VLM reasoners. Second, we design a reward model architecture based on
Qwen2.5-VL-7B-Instruct, integrating a reward head and applying multi-stage
fine-tuning using pairwise ranking loss on pairwise preference data.
Experimental evaluations show that Skywork-VL Reward achieves state-of-the-art
results on multimodal VL-RewardBench and exhibits competitive performance on
the text-only RewardBench benchmark. Furthermore, preference data constructed
based on our Skywork-VL Reward proves highly effective for training Mixed
Preference Optimization (MPO), leading to significant improvements in
multimodal reasoning capabilities. Our results underscore Skywork-VL Reward as
a significant advancement toward general-purpose, reliable reward models for
multimodal alignment. Our model has been publicly released to promote
transparency and reproducibility.

</details>


### [197] [L-SWAG: Layer-Sample Wise Activation with Gradients information for Zero-Shot NAS on Vision Transformers](https://arxiv.org/pdf/2505.07300)
*Sofia Casarin, Sergio Escalera, Oswald Lanz*

Main category: cs.CV

TL;DR: Training-free NAS uses zero-cost proxies to efficiently find high-performing neural networks, extending to Vision Transformers with a new metric (L-SWAG) and a method (LIBRA-NAS) to combine proxies, achieving 17.0% test error on ImageNet1k in 0.1 GPU days.


<details>
  <summary>Details</summary>
Motivation: Current zero-cost proxies are limited to convolutional search spaces, while Vision Transformers are gaining prominence. This work aims to extend proxy applicability to ViTs and improve NAS efficiency.

Method: Proposes L-SWAG, a generalizable metric for both convolutional and transformer architectures, and LIBRA-NAS, a method to strategically combine proxies for better performance.

Result: LIBRA-NAS outperforms evolution and gradient-based NAS, achieving 17.0% test error on ImageNet1k in just 0.1 GPU days.

Conclusion: The work successfully extends zero-cost proxies to Vision Transformers and introduces a method to enhance NAS efficiency, demonstrating significant improvements in performance and speed.

Abstract: Training-free Neural Architecture Search (NAS) efficiently identifies
high-performing neural networks using zero-cost (ZC) proxies. Unlike multi-shot
and one-shot NAS approaches, ZC-NAS is both (i) time-efficient, eliminating the
need for model training, and (ii) interpretable, with proxy designs often
theoretically grounded. Despite rapid developments in the field, current SOTA
ZC proxies are typically constrained to well-established convolutional search
spaces. With the rise of Large Language Models shaping the future of deep
learning, this work extends ZC proxy applicability to Vision Transformers
(ViTs). We present a new benchmark using the Autoformer search space evaluated
on 6 distinct tasks and propose Layer-Sample Wise Activation with Gradients
information (L-SWAG), a novel, generalizable metric that characterizes both
convolutional and transformer architectures across 14 tasks. Additionally,
previous works highlighted how different proxies contain complementary
information, motivating the need for a ML model to identify useful
combinations. To further enhance ZC-NAS, we therefore introduce LIBRA-NAS (Low
Information gain and Bias Re-Alignment), a method that strategically combines
proxies to best represent a specific benchmark. Integrated into the NAS search,
LIBRA-NAS outperforms evolution and gradient-based NAS techniques by
identifying an architecture with a 17.0% test error on ImageNet1k in just 0.1
GPU days.

</details>


### [198] [Human Motion Prediction via Test-domain-aware Adaptation with Easily-available Human Motions Estimated from Videos](https://arxiv.org/pdf/2505.07301)
*Katsuki Shimbo, Hiromu Taketsugu, Norimichi Ukita*

Main category: cs.CV

TL;DR: The paper proposes using estimated 2D poses from videos to enhance 3D Human Motion Prediction (HMP), improving generalizability by reducing reliance on costly motion capture data.


<details>
  <summary>Details</summary>
Motivation: High costs and limited diversity of motion capture data hinder HMP model generalizability.

Method: Transform 2D poses from monocular videos into 3D motions and use them for additional learning to adapt HMP models.

Result: Experiments show quantitative and qualitative improvements in HMP performance.

Conclusion: Leveraging video-derived poses enhances HMP generalizability without expensive data.

Abstract: In 3D Human Motion Prediction (HMP), conventional methods train HMP models
with expensive motion capture data. However, the data collection cost of such
motion capture data limits the data diversity, which leads to poor
generalizability to unseen motions or subjects. To address this issue, this
paper proposes to enhance HMP with additional learning using estimated poses
from easily available videos. The 2D poses estimated from the monocular videos
are carefully transformed into motion capture-style 3D motions through our
pipeline. By additional learning with the obtained motions, the HMP model is
adapted to the test domain. The experimental results demonstrate the
quantitative and qualitative impact of our method.

</details>


### [199] [Enabling Privacy-Aware AI-Based Ergonomic Analysis](https://arxiv.org/pdf/2505.07306)
*Sander De Coninck, Emilio Gamba, Bart Van Doninck, Abdellatif Bey-Temsamani, Sam Leroux, Pieter Simoens*

Main category: cs.CV

TL;DR: A privacy-aware framework for ergonomic assessment in manufacturing uses adversarial training to obfuscate video data, ensuring privacy while maintaining pose estimation accuracy for workplace safety.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns in camera-based ergonomic monitoring while maintaining effectiveness in reducing musculoskeletal disorders (MSDs) in manufacturing.

Method: Adversarial training to obfuscate video data, preserving key pose information. Uses multi-view integration and REBA for 3D keypoint assessment.

Result: High accuracy in pose estimation with privacy protection, enabling secure ergonomic monitoring.

Conclusion: The framework balances privacy and workplace safety, offering a viable solution for industrial ergonomic assessments.

Abstract: Musculoskeletal disorders (MSDs) are a leading cause of injury and
productivity loss in the manufacturing industry, incurring substantial economic
costs. Ergonomic assessments can mitigate these risks by identifying workplace
adjustments that improve posture and reduce strain. Camera-based systems offer
a non-intrusive, cost-effective method for continuous ergonomic tracking, but
they also raise significant privacy concerns. To address this, we propose a
privacy-aware ergonomic assessment framework utilizing machine learning
techniques. Our approach employs adversarial training to develop a lightweight
neural network that obfuscates video data, preserving only the essential
information needed for human pose estimation. This obfuscation ensures
compatibility with standard pose estimation algorithms, maintaining high
accuracy while protecting privacy. The obfuscated video data is transmitted to
a central server, where state-of-the-art keypoint detection algorithms extract
body landmarks. Using multi-view integration, 3D keypoints are reconstructed
and evaluated with the Rapid Entire Body Assessment (REBA) method. Our system
provides a secure, effective solution for ergonomic monitoring in industrial
environments, addressing both privacy and workplace safety concerns.

</details>


### [200] [RealRep: Generalized SDR-to-HDR Conversion with Style Disentangled Representation Learning](https://arxiv.org/pdf/2505.07322)
*Gang He, Siqi Wang, Kepeng Xu, Lin Zhang*

Main category: cs.CV

TL;DR: The paper proposes RealRep, a method for converting SDR to HDR content by disentangling luminance and chrominance, and introduces DDACMNet for adaptive hierarchical mapping.


<details>
  <summary>Details</summary>
Motivation: Existing fixed tone mapping methods fail to handle diverse SDR styles in real-world scenarios, necessitating a generalized approach.

Method: RealRep disentangles luminance and chrominance for multi-view style representation learning. DDACMNet uses a two-stage framework with degradation-aware hierarchical mapping.

Result: RealRep outperforms state-of-the-art methods, achieving superior generalization and perceptually faithful HDR reconstruction.

Conclusion: The proposed methods effectively address diverse SDR styles and degradation domains, advancing SDR-to-HDR conversion.

Abstract: High-Dynamic-Range Wide-Color-Gamut (HDR-WCG) technology is becoming
increasingly prevalent, intensifying the demand for converting Standard Dynamic
Range (SDR) content to HDR. Existing methods primarily rely on fixed tone
mapping operators, which are inadequate for handling SDR inputs with diverse
styles commonly found in real-world scenarios. To address this challenge, we
propose a generalized SDR-to-HDR method that handles diverse styles in
real-world SDR content, termed Realistic Style Disentangled Representation
Learning (RealRep). By disentangling luminance and chrominance, we analyze the
intrinsic differences between contents with varying styles and propose a
disentangled multi-view style representation learning method. This approach
captures the guidance prior of true luminance and chrominance distributions
across different styles, even when the SDR style distributions exhibit
significant variations, thereby establishing a robust embedding space for
inverse tone mapping. Motivated by the difficulty of directly utilizing
degradation representation priors, we further introduce the Degradation-Domain
Aware Controlled Mapping Network (DDACMNet), a two-stage framework that
performs adaptive hierarchical mapping guided by a control-aware normalization
mechanism. DDACMNet dynamically modulates the mapping process via
degradation-conditioned hierarchical features, enabling robust adaptation
across diverse degradation domains. Extensive experiments show that RealRep
consistently outperforms state-of-the-art methods with superior generalization
and perceptually faithful HDR color gamut reconstruction.

</details>


### [201] [Link to the Past: Temporal Propagation for Fast 3D Human Reconstruction from Monocular Video](https://arxiv.org/pdf/2505.07333)
*Matthew Marchellus, Nadhira Noor, In Kyu Park*

Main category: cs.CV

TL;DR: TemPoFast3D is a fast 3D clothed human reconstruction method from monocular video, balancing efficiency and quality by leveraging temporal coherency and efficient coordinate mapping.


<details>
  <summary>Details</summary>
Motivation: Current methods are either too slow for real-time use or sacrifice quality for speed, highlighting the need for a balanced solution.

Method: Uses temporal coherency and a 'plug-and-play' approach to transform pixel-aligned networks for continuous video streams, refining a canonical appearance representation.

Result: Achieves 12 FPS, matches or exceeds state-of-the-art in quality, and handles diverse poses and appearances.

Conclusion: TemPoFast3D offers a practical, high-quality solution for real-time 3D human reconstruction from video.

Abstract: Fast 3D clothed human reconstruction from monocular video remains a
significant challenge in computer vision, particularly in balancing
computational efficiency with reconstruction quality. Current approaches are
either focused on static image reconstruction but too computationally
intensive, or achieve high quality through per-video optimization that requires
minutes to hours of processing, making them unsuitable for real-time
applications. To this end, we present TemPoFast3D, a novel method that
leverages temporal coherency of human appearance to reduce redundant
computation while maintaining reconstruction quality. Our approach is a
"plug-and play" solution that uniquely transforms pixel-aligned reconstruction
networks to handle continuous video streams by maintaining and refining a
canonical appearance representation through efficient coordinate mapping.
Extensive experiments demonstrate that TemPoFast3D matches or exceeds
state-of-the-art methods across standard metrics while providing high-quality
textured reconstruction across diverse pose and appearance, with a maximum
speed of 12 FPS.

</details>


### [202] [SAEN-BGS: Energy-Efficient Spiking AutoEncoder Network for Background Subtraction](https://arxiv.org/pdf/2505.07336)
*Zhixuan Zhang, Xiaopeng Li, Qi Liu*

Main category: cs.CV

TL;DR: A spiking autoencoder network (SAEN-BGS) is proposed to improve background subtraction by leveraging noise resilience and time-sequence sensitivity of spiking neural networks, achieving better segmentation and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning-based BGS methods struggle with background noises like lighting changes and camera shifts. SAEN-BGS aims to enhance foreground-background separation.

Method: Develops a spiking autoencoder with continuous spiking conv-and-dconv blocks and a self-distillation spiking supervised learning method for energy efficiency.

Result: Outperforms baseline methods on CDnet-2014 and DAVIS-2016 datasets, even in dynamic backgrounds.

Conclusion: SAEN-BGS effectively addresses noise challenges in BGS while improving performance and energy efficiency.

Abstract: Background subtraction (BGS) is utilized to detect moving objects in a video
and is commonly employed at the onset of object tracking and human recognition
processes. Nevertheless, existing BGS techniques utilizing deep learning still
encounter challenges with various background noises in videos, including
variations in lighting, shifts in camera angles, and disturbances like air
turbulence or swaying trees. To address this problem, we design a spiking
autoencoder network, termed SAEN-BGS, based on noise resilience and
time-sequence sensitivity of spiking neural networks (SNNs) to enhance the
separation of foreground and background. To eliminate unnecessary background
noise and preserve the important foreground elements, we begin by creating the
continuous spiking conv-and-dconv block, which serves as the fundamental
building block for the decoder in SAEN-BGS. Moreover, in striving for enhanced
energy efficiency, we introduce a novel self-distillation spiking supervised
learning method grounded in ANN-to-SNN frameworks, resulting in decreased power
consumption. In extensive experiments conducted on CDnet-2014 and DAVIS-2016
datasets, our approach demonstrates superior segmentation performance relative
to other baseline methods, even when challenged by complex scenarios with
dynamic backgrounds.

</details>


### [203] [Generative Pre-trained Autoregressive Diffusion Transformer](https://arxiv.org/pdf/2505.07344)
*Yuan Zhang, Jiacheng Jiang, Guoqing Ma, Zhiying Lu, Haoyang Huang, Jianlong Yuan, Nan Duan*

Main category: cs.CV

TL;DR: GPDiT combines diffusion and autoregressive modeling for long-range video synthesis in continuous latent space, improving generation quality and representation.


<details>
  <summary>Details</summary>
Motivation: To unify diffusion and autoregressive modeling for better video synthesis and representation in continuous space.

Method: GPDiT predicts future latent frames autoregressively using diffusion loss, with causal attention and time-conditioning mechanisms.

Result: Achieves strong performance in video generation, representation, and few-shot learning.

Conclusion: GPDiT is a promising framework for continuous-space video modeling.

Abstract: In this work, we present GPDiT, a Generative Pre-trained Autoregressive
Diffusion Transformer that unifies the strengths of diffusion and
autoregressive modeling for long-range video synthesis, within a continuous
latent space. Instead of predicting discrete tokens, GPDiT autoregressively
predicts future latent frames using a diffusion loss, enabling natural modeling
of motion dynamics and semantic consistency across frames. This continuous
autoregressive framework not only enhances generation quality but also endows
the model with representation capabilities. Additionally, we introduce a
lightweight causal attention variant and a parameter-free rotation-based
time-conditioning mechanism, improving both the training and inference
efficiency. Extensive experiments demonstrate that GPDiT achieves strong
performance in video generation quality, video representation ability, and
few-shot learning tasks, highlighting its potential as an effective framework
for video modeling in continuous space.

</details>


### [204] [AI-Enabled Accurate Non-Invasive Assessment of Pulmonary Hypertension Progression via Multi-Modal Echocardiography](https://arxiv.org/pdf/2505.07347)
*Jiewen Yang, Taoran Huang, Shangwei Ding, Xiaowei Xu, Qinhua Zhao, Yong Jiang, Jiarong Guo, Bin Pu, Jiexuan Zheng, Caojin Zhang, Hongwen Fei, Xiaomeng Li*

Main category: cs.CV

TL;DR: MePH, a multi-view, multi-modal vision-language model, improves non-invasive assessment of pulmonary hypertension progression using echocardiography, outperforming traditional methods and echocardiographers.


<details>
  <summary>Details</summary>
Motivation: Current methods for assessing pulmonary hypertension progression, like right heart catheterization (RHC), are invasive and impractical for routine use, creating a need for accurate non-invasive alternatives.

Method: MePH leverages a large dataset of paired echocardiogram videos, spectral images, and RHC data to model correlations between non-invasive echocardiography and RHC-derived metrics.

Result: MePH reduces mean absolute error in estimating mPAP and PVR by 49.73% and 43.81%, respectively, and achieves superior accuracy in predicting severity (AUC 0.921 vs. 0.842).

Conclusion: MePH offers a non-invasive, timely, and accurate solution for monitoring pulmonary hypertension, enabling earlier interventions and personalized treatments.

Abstract: Echocardiographers can detect pulmonary hypertension using Doppler
echocardiography; however, accurately assessing its progression often proves
challenging. Right heart catheterization (RHC), the gold standard for precise
evaluation, is invasive and unsuitable for routine use, limiting its
practicality for timely diagnosis and monitoring of pulmonary hypertension
progression. Here, we propose MePH, a multi-view, multi-modal vision-language
model to accurately assess pulmonary hypertension progression using
non-invasive echocardiography. We constructed a large dataset comprising paired
standardized echocardiogram videos, spectral images and RHC data, covering
1,237 patient cases from 12 medical centers. For the first time, MePH precisely
models the correlation between non-invasive multi-view, multi-modal
echocardiography and the pressure and resistance obtained via RHC. We show that
MePH significantly outperforms echocardiographers' assessments using
echocardiography, reducing the mean absolute error in estimating mean pulmonary
arterial pressure (mPAP) and pulmonary vascular resistance (PVR) by 49.73% and
43.81%, respectively. In eight independent external hospitals, MePH achieved a
mean absolute error of 3.147 for PVR assessment. Furthermore, MePH achieved an
area under the curve of 0.921, surpassing echocardiographers (area under the
curve of 0.842) in accurately predicting the severity of pulmonary
hypertension, whether mild or severe. A prospective study demonstrated that
MePH can predict treatment efficacy for patients. Our work provides pulmonary
hypertension patients with a non-invasive and timely method for monitoring
disease progression, improving the accuracy and efficiency of pulmonary
hypertension management while enabling earlier interventions and more
personalized treatment decisions.

</details>


### [205] [Geometric Prior-Guided Neural Implicit Surface Reconstruction in the Wild](https://arxiv.org/pdf/2505.07373)
*Lintao Xiang, Hongpei Zheng, Bailin Deng, Hujun Yin*

Main category: cs.CV

TL;DR: A novel method improves neural implicit surface reconstruction by applying geometric constraints, enabling accurate 3D geometry from unconstrained images.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with uncontrolled environments and transient occlusions, limiting accurate surface reconstruction.

Method: Uses sparse 3D points from SfM and robust normal priors with edge filtering and multi-view consistency for precise surface optimization.

Result: Achieves superior accuracy and granularity in surface reconstruction compared to existing techniques.

Conclusion: Enables high-quality 3D reconstruction for diverse applications like cultural heritage preservation.

Abstract: Neural implicit surface reconstruction using volume rendering techniques has
recently achieved significant advancements in creating high-fidelity surfaces
from multiple 2D images. However, current methods primarily target scenes with
consistent illumination and struggle to accurately reconstruct 3D geometry in
uncontrolled environments with transient occlusions or varying appearances.
While some neural radiance field (NeRF)-based variants can better manage
photometric variations and transient objects in complex scenes, they are
designed for novel view synthesis rather than precise surface reconstruction
due to limited surface constraints. To overcome this limitation, we introduce a
novel approach that applies multiple geometric constraints to the implicit
surface optimization process, enabling more accurate reconstructions from
unconstrained image collections. First, we utilize sparse 3D points from
structure-from-motion (SfM) to refine the signed distance function estimation
for the reconstructed surface, with a displacement compensation to accommodate
noise in the sparse points. Additionally, we employ robust normal priors
derived from a normal predictor, enhanced by edge prior filtering and
multi-view consistency constraints, to improve alignment with the actual
surface geometry. Extensive testing on the Heritage-Recon benchmark and other
datasets has shown that the proposed method can accurately reconstruct surfaces
from in-the-wild images, yielding geometries with superior accuracy and
granularity compared to existing techniques. Our approach enables high-quality
3D reconstruction of various landmarks, making it applicable to diverse
scenarios such as digital preservation of cultural heritage sites.

</details>


### [206] [Boosting Global-Local Feature Matching via Anomaly Synthesis for Multi-Class Point Cloud Anomaly Detection](https://arxiv.org/pdf/2505.07375)
*Yuqi Cheng, Yunkang Cao, Dongfang Wang, Weiming Shen, Wenlong Li*

Main category: cs.CV

TL;DR: GLFM is a multi-class point cloud anomaly detection method using global-local feature matching to address feature confusion, outperforming existing methods on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: The need for efficient multi-class unsupervised anomaly detection due to high computation/storage costs and feature confusion issues in existing methods.

Method: Three-stage approach: anomaly synthesis for feature adaptation, global-local memory banks to reduce confusion, and anomaly detection via feature distance.

Result: Superior performance on MVTec 3D-AD, Real3D-AD, and industry datasets.

Conclusion: GLFM effectively addresses feature confusion and enhances multi-class anomaly detection, with code publicly available.

Abstract: Point cloud anomaly detection is essential for various industrial
applications. The huge computation and storage costs caused by the increasing
product classes limit the application of single-class unsupervised methods,
necessitating the development of multi-class unsupervised methods. However, the
feature similarity between normal and anomalous points from different class
data leads to the feature confusion problem, which greatly hinders the
performance of multi-class methods. Therefore, we introduce a multi-class point
cloud anomaly detection method, named GLFM, leveraging global-local feature
matching to progressively separate data that are prone to confusion across
multiple classes. Specifically, GLFM is structured into three stages: Stage-I
proposes an anomaly synthesis pipeline that stretches point clouds to create
abundant anomaly data that are utilized to adapt the point cloud feature
extractor for better feature representation. Stage-II establishes the global
and local memory banks according to the global and local feature distributions
of all the training data, weakening the impact of feature confusion on the
establishment of the memory bank. Stage-III implements anomaly detection of
test data leveraging its feature distance from global and local memory banks.
Extensive experiments on the MVTec 3D-AD, Real3D-AD and actual industry parts
dataset showcase our proposed GLFM's superior point cloud anomaly detection
performance. The code is available at
https://github.com/hustCYQ/GLFM-Multi-class-3DAD.

</details>


### [207] [Few-shot Semantic Encoding and Decoding for Video Surveillance](https://arxiv.org/pdf/2505.07381)
*Baoping Cheng, Yukun Zhang, Liming Wang, Xiaoyan Xie, Tao Fu, Dongkun Wang, Xiaoming Tao*

Main category: cs.CV

TL;DR: A semantic encoding/decoding method for surveillance video reduces storage/transmission needs by using sketches and few-shot learning, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Address the growing burden of video surveillance data by overcoming bottlenecks in traditional communication methods with semantic communication.

Method: Extract sketches as semantic info, compress them, translate sketches into video frames using reference frames, and reconstruct video with a few-shot network.

Result: Better video reconstruction than baselines; sketch compression reduces bit rate without compromising quality.

Conclusion: The method improves practicality of semantic communication by requiring few training samples per scene.

Abstract: With the continuous increase in the number and resolution of video
surveillance cameras, the burden of transmitting and storing surveillance video
is growing. Traditional communication methods based on Shannon's theory are
facing optimization bottlenecks. Semantic communication, as an emerging
communication method, is expected to break through this bottleneck and reduce
the storage and transmission consumption of video. Existing semantic decoding
methods often require many samples to train the neural network for each scene,
which is time-consuming and labor-intensive. In this study, a semantic encoding
and decoding method for surveillance video is proposed. First, the sketch was
extracted as semantic information, and a sketch compression method was proposed
to reduce the bit rate of semantic information. Then, an image translation
network was proposed to translate the sketch into a video frame with a
reference frame. Finally, a few-shot sketch decoding network was proposed to
reconstruct video from sketch. Experimental results showed that the proposed
method achieved significantly better video reconstruction performance than
baseline methods. The sketch compression method could effectively reduce the
storage and transmission consumption of semantic information with little
compromise on video quality. The proposed method provides a novel semantic
encoding and decoding method that only needs a few training samples for each
surveillance scene, thus improving the practicality of the semantic
communication system.

</details>


### [208] [Feature Visualization in 3D Convolutional Neural Networks](https://arxiv.org/pdf/2505.07387)
*Chunpeng Li, Ya-tang Li*

Main category: cs.CV

TL;DR: A novel method for visualizing 3D convolutional kernels by disentangling texture and motion preferences, improving interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing visualization techniques for 2D kernels fail for 3D convolutions due to higher dimensionality and complexity.

Method: Data-driven decomposition of optimal inputs followed by a two-stage optimization to extract texture and motion components.

Result: Visualizations clearly reveal preferred dynamic patterns in 3D kernels, especially motion.

Conclusion: The method effectively provides interpretable insights into 3D convolutional operations.

Abstract: Understanding the computations of convolutional neural networks requires
effective visualization of their kernels. While maximal activation methods have
proven successful in highlighting the preferred features of 2D convolutional
kernels, directly applying these techniques to 3D convolutions often leads to
uninterpretable results due to the higher dimensionality and complexity of 3D
features. To address this challenge, we propose a novel visualization approach
for 3D convolutional kernels that disentangles their texture and motion
preferences. Our method begins with a data-driven decomposition of the optimal
input that maximally activates a given kernel. We then introduce a two-stage
optimization strategy to extract distinct texture and motion components from
this input. Applying our approach to visualize kernels at various depths of
several pre-trained models, we find that the resulting
visualizations--particularly those capturing motion--clearly reveal the
preferred dynamic patterns encoded by 3D kernels. These results demonstrate the
effectiveness of our method in providing interpretable insights into 3D
convolutional operations. Code is available at
https://github.com/YatangLiLab/3DKernelVisualizer.

</details>


### [209] [TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset](https://arxiv.org/pdf/2505.07396)
*Olaf Wysocki, Benedikt Schwab, Manoj Kumar Biswanath, Qilin Zhang, Jingwei Zhu, Thomas Froech, Medhini Heeramaglore, Ihab Hijazi, Khaoula Kanna, Mathias Pechinger, Zhaiyu Chen, Yao Sun, Alejandro Rueda Segura, Ziyang Xu, Omar AbdelGafar, Mansour Mehranfar, Chandan Yeshwanth, Yueh-Cheng Liu, Hadi Yazdi, Jiapan Wang, Stefan Auer, Katharina Anders, Klaus Bogenberger, Andre Borrmann, Angela Dai, Ludwig Hoegner, Christoph Holst, Thomas H. Kolbe, Ferdinand Ludwig, Matthias Nießner, Frank Petzold, Xiao Xiang Zhu, Boris Jutzi*

Main category: cs.CV

TL;DR: The paper introduces TUM2TWIN, a comprehensive multimodal Urban Digital Twin benchmark dataset, addressing challenges in UDT creation by providing georeferenced, semantically aligned 3D models and diverse observations.


<details>
  <summary>Details</summary>
Motivation: Current datasets for Urban Digital Twins (UDTs) are limited, hindering comprehensive validation. The need for accurate, multimodal data integration drives the creation of TUM2TWIN.

Method: The dataset includes georeferenced 3D models, networks, and various terrestrial, mobile, aerial, and satellite observations, covering 100,000 m² and 767 GB of data.

Result: TUM2TWIN supports robust sensor analysis and advanced reconstruction methods, demonstrated through tasks like novel view synthesis, solar potential analysis, and semantic segmentation.

Conclusion: TUM2TWIN addresses UDT limitations, enabling new research and practical solutions for smarter urban environments. The dataset is publicly available.

Abstract: Urban Digital Twins (UDTs) have become essential for managing cities and
integrating complex, heterogeneous data from diverse sources. Creating UDTs
involves challenges at multiple process stages, including acquiring accurate 3D
source data, reconstructing high-fidelity 3D models, maintaining models'
updates, and ensuring seamless interoperability to downstream tasks. Current
datasets are usually limited to one part of the processing chain, hampering
comprehensive UDTs validation. To address these challenges, we introduce the
first comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN.
This dataset includes georeferenced, semantically aligned 3D models and
networks along with various terrestrial, mobile, aerial, and satellite
observations boasting 32 data subsets over roughly 100,000 $m^2$ and currently
767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, high
accuracy, and multimodal data integration, the benchmark supports robust
analysis of sensors and the development of advanced reconstruction methods.
Additionally, we explore downstream tasks demonstrating the potential of
TUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solar
potential analysis, point cloud semantic segmentation, and LoD3 building
reconstruction. We are convinced this contribution lays a foundation for
overcoming current limitations in UDT creation, fostering new research
directions and practical solutions for smarter, data-driven urban environments.
The project is available under: https://tum2t.win

</details>


### [210] [DepthFusion: Depth-Aware Hybrid Feature Fusion for LiDAR-Camera 3D Object Detection](https://arxiv.org/pdf/2505.07398)
*Mingqian Ji, Jian Yang, Shanshan Zhang*

Main category: cs.CV

TL;DR: The paper introduces DepthFusion, a depth-aware hybrid feature fusion strategy for LiDAR-camera 3D object detection, improving performance and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing LiDAR-camera fusion methods overlook depth, despite its critical role in modality effectiveness. Statistical analysis reveals depth-dependent modality roles, prompting a depth-aware fusion approach.

Method: Proposes DepthFusion with two modules: Depth-GFusion (global feature weight adjustment via depth encoding) and Depth-LFusion (local feature weight adjustment to compensate BEV space losses).

Result: Outperforms state-of-the-art on nuScenes and KITTI datasets and shows superior robustness on nuScenes-C.

Conclusion: DepthFusion effectively leverages depth to enhance multi-modal fusion, achieving better performance and robustness in 3D object detection.

Abstract: State-of-the-art LiDAR-camera 3D object detectors usually focus on feature
fusion. However, they neglect the factor of depth while designing the fusion
strategy. In this work, we are the first to observe that different modalities
play different roles as depth varies via statistical analysis and
visualization. Based on this finding, we propose a Depth-Aware Hybrid Feature
Fusion (DepthFusion) strategy that guides the weights of point cloud and RGB
image modalities by introducing depth encoding at both global and local levels.
Specifically, the Depth-GFusion module adaptively adjusts the weights of image
Bird's-Eye-View (BEV) features in multi-modal global features via depth
encoding. Furthermore, to compensate for the information lost when transferring
raw features to the BEV space, we propose a Depth-LFusion module, which
adaptively adjusts the weights of original voxel features and multi-view image
features in multi-modal local features via depth encoding. Extensive
experiments on the nuScenes and KITTI datasets demonstrate that our DepthFusion
method surpasses previous state-of-the-art methods. Moreover, our DepthFusion
is more robust to various kinds of corruptions, outperforming previous methods
on the nuScenes-C dataset.

</details>


### [211] [Lightweight Multispectral Crop-Weed Segmentation for Precision Agriculture](https://arxiv.org/pdf/2505.07444)
*Zeynep Galymzhankyzy, Eric Martinson*

Main category: cs.CV

TL;DR: A lightweight transformer-CNN hybrid model improves crop-weed segmentation by using RGB, NIR, and Red-Edge bands, achieving 78.88% mean IoU and outperforming RGB-only models by 15.8%.


<details>
  <summary>Details</summary>
Motivation: Enhancing precision agriculture by addressing the limitations of conventional CNN-based methods, which struggle with generalization and rely solely on RGB imagery.

Method: Proposes a transformer-CNN hybrid with specialized encoders and dynamic modality integration for processing RGB, NIR, and Red-Edge bands.

Result: Achieves 78.88% mean IoU on the WeedsGalore dataset, outperforming RGB-only models by 15.8 percentage points, with only 8.7 million parameters.

Conclusion: The model offers high accuracy, computational efficiency, and potential for real-time deployment on UAVs and edge devices, advancing precision weed management.

Abstract: Efficient crop-weed segmentation is critical for site-specific weed control
in precision agriculture. Conventional CNN-based methods struggle to generalize
and rely on RGB imagery, limiting performance under complex field conditions.
To address these challenges, we propose a lightweight transformer-CNN hybrid.
It processes RGB, Near-Infrared (NIR), and Red-Edge (RE) bands using
specialized encoders and dynamic modality integration. Evaluated on the
WeedsGalore dataset, the model achieves a segmentation accuracy (mean IoU) of
78.88%, outperforming RGB-only models by 15.8 percentage points. With only 8.7
million parameters, the model offers high accuracy, computational efficiency,
and potential for real-time deployment on Unmanned Aerial Vehicles (UAVs) and
edge devices, advancing precision weed management.

</details>


### [212] [Addressing degeneracies in latent interpolation for diffusion models](https://arxiv.org/pdf/2505.07481)
*Erik Landolsi, Fredrik Kahl*

Main category: cs.CV

TL;DR: The paper addresses the degeneration issue in latent space interpolation for image generation using diffusion models and proposes a simple normalization method to improve results.


<details>
  <summary>Details</summary>
Motivation: Interpolating between latents of multiple input images can produce degenerate results, especially with many inputs. The study aims to understand and fix this issue.

Method: The authors analyze the problem theoretically and experimentally, then suggest a normalization scheme for latent interpolation.

Result: Experiments show baseline methods degrade quality metrics (FID, CLIP) before degeneration is visible, while the proposed method reduces degeneration and improves metrics.

Conclusion: The normalization scheme effectively mitigates degeneration in latent interpolation and enhances image quality, even in non-degenerate cases.

Abstract: There is an increasing interest in using image-generating diffusion models
for deep data augmentation and image morphing. In this context, it is useful to
interpolate between latents produced by inverting a set of input images, in
order to generate new images representing some mixture of the inputs. We
observe that such interpolation can easily lead to degenerate results when the
number of inputs is large. We analyze the cause of this effect theoretically
and experimentally, and suggest a suitable remedy. The suggested approach is a
relatively simple normalization scheme that is easy to use whenever
interpolation between latents is needed. We measure image quality using FID and
CLIP embedding distance and show experimentally that baseline interpolation
methods lead to a drop in quality metrics long before the degeneration issue is
clearly visible. In contrast, our method significantly reduces the degeneration
effect and leads to improved quality metrics also in non-degenerate situations.

</details>


### [213] [DocVXQA: Context-Aware Visual Explanations for Document Question Answering](https://arxiv.org/pdf/2505.07496)
*Mohamed Ali Souibgui, Changkyu Choi, Andrey Barsky, Kangsoo Jung, Ernest Valveny, Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: DocVXQA is a framework for document question answering that provides visual heatmaps for interpretability, balancing accuracy and explainability.


<details>
  <summary>Details</summary>
Motivation: To enhance trust in document question answering models by providing visually interpretable justifications for answers.

Method: Quantitatively integrates explainability principles as learning objectives, focusing on contextually sufficient and representation-efficient heatmaps.

Result: Achieves strong performance in predictive accuracy and interpretability, validated through experiments and human evaluation.

Conclusion: DocVXQA effectively balances performance and interpretability, fostering user trust in document question answering.

Abstract: We propose DocVXQA, a novel framework for visually self-explainable document
question answering. The framework is designed not only to produce accurate
answers to questions but also to learn visual heatmaps that highlight
contextually critical regions, thereby offering interpretable justifications
for the model's decisions. To integrate explanations into the learning process,
we quantitatively formulate explainability principles as explicit learning
objectives. Unlike conventional methods that emphasize only the regions
pertinent to the answer, our framework delivers explanations that are
\textit{contextually sufficient} while remaining
\textit{representation-efficient}. This fosters user trust while achieving a
balance between predictive performance and interpretability in DocVQA
applications. Extensive experiments, including human evaluation, provide strong
evidence supporting the effectiveness of our method. The code is available at
https://github.com/dali92002/DocVXQA.

</details>


### [214] [Learning to Reason and Navigate: Parameter Efficient Action Planning with Large Language Models](https://arxiv.org/pdf/2505.07500)
*Bahram Mohammadi, Ehsan Abbasnejad, Yuankai Qi, Qi Wu, Anton Van Den Hengel, Javen Qinfeng Shi*

Main category: cs.CV

TL;DR: A novel parameter-efficient action planner (PEAP-LLM) using LLMs improves navigation for the REVERIE task by generating single-step instructions, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of efficient navigation in complex indoor environments for remote object localization without pre-exploration, where existing LLM-based methods are error-prone or require human intervention.

Method: Proposes PEAP-LLM with two modules: LLM goal planner (LGP) for goal extraction and LoRA action planner (LAP) for single-step instruction generation. Uses a two-stage fine-tuning approach (SFT and DPO) to enhance LLM performance.

Result: PEAP-LLM outperforms state-of-the-art methods on the REVERIE task, demonstrating superior navigation and localization capabilities.

Conclusion: The proposed PEAP-LLM effectively addresses limitations of existing LLM-based methods, offering a robust solution for embodied agents in complex environments.

Abstract: The remote embodied referring expression (REVERIE) task requires an agent to
navigate through complex indoor environments and localize a remote object
specified by high-level instructions, such as "bring me a spoon", without
pre-exploration. Hence, an efficient navigation plan is essential for the final
success. This paper proposes a novel parameter-efficient action planner using
large language models (PEAP-LLM) to generate a single-step instruction at each
location. The proposed model consists of two modules, LLM goal planner (LGP)
and LoRA action planner (LAP). Initially, LGP extracts the goal-oriented plan
from REVERIE instructions, including the target object and room. Then, LAP
generates a single-step instruction with the goal-oriented plan, high-level
instruction, and current visual observation as input. PEAP-LLM enables the
embodied agent to interact with LAP as the path planner on the fly. A simple
direct application of LLMs hardly achieves good performance. Also, existing
hard-prompt-based methods are error-prone in complicated scenarios and need
human intervention. To address these issues and prevent the LLM from generating
hallucinations and biased information, we propose a novel two-stage method for
fine-tuning the LLM, consisting of supervised fine-tuning (STF) and direct
preference optimization (DPO). SFT improves the quality of generated
instructions, while DPO utilizes environmental feedback. Experimental results
show the superiority of our proposed model on REVERIE compared to the previous
state-of-the-art.

</details>


### [215] [MAIS: Memory-Attention for Interactive Segmentation](https://arxiv.org/pdf/2505.07511)
*Mauricio Orbes-Arteaga, Oeslle Lucena, Sabastien Ourselin, M. Jorge Cardoso*

Main category: cs.CV

TL;DR: MAIS introduces a Memory-Attention mechanism for interactive medical segmentation, improving efficiency and accuracy by integrating temporal context from past user inputs.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat user interactions as independent events, causing redundant corrections and limited refinement gains.

Method: MAIS stores past user inputs and segmentation states, enabling temporal context integration in Vision Transformer (ViT)-based models like SAM.

Result: Enhances segmentation across diverse imaging modalities, achieving more efficient and accurate refinements.

Conclusion: MAIS improves interactive segmentation by leveraging temporal context, outperforming independent interaction approaches.

Abstract: Interactive medical segmentation reduces annotation effort by refining
predictions through user feedback. Vision Transformer (ViT)-based models, such
as the Segment Anything Model (SAM), achieve state-of-the-art performance using
user clicks and prior masks as prompts. However, existing methods treat
interactions as independent events, leading to redundant corrections and
limited refinement gains. We address this by introducing MAIS, a
Memory-Attention mechanism for Interactive Segmentation that stores past user
inputs and segmentation states, enabling temporal context integration. Our
approach enhances ViT-based segmentation across diverse imaging modalities,
achieving more efficient and accurate refinements.

</details>


### [216] [FLUXSynID: A Framework for Identity-Controlled Synthetic Face Generation with Document and Live Images](https://arxiv.org/pdf/2505.07530)
*Raul Ismayilov, Luuk Spreeuwers, Dzemila Sero*

Main category: cs.CV

TL;DR: FLUXSynID is a framework for generating high-resolution synthetic face datasets with controlled identity attributes and paired images, improving alignment with real-world distributions and diversity.


<details>
  <summary>Details</summary>
Motivation: To address limitations of real-world biometric data like privacy concerns, demographic imbalance, and high costs, while providing fine-grained control over identity attributes.

Method: Introduces FLUXSynID, a framework for generating synthetic face datasets with user-defined identity attributes and paired document-style/live capture images.

Result: Produces a dataset of 14,889 synthetic identities with improved real-world alignment and inter-set diversity.

Conclusion: FLUXSynID supports biometric research (e.g., face recognition, morphing attack detection) and is publicly released.

Abstract: Synthetic face datasets are increasingly used to overcome the limitations of
real-world biometric data, including privacy concerns, demographic imbalance,
and high collection costs. However, many existing methods lack fine-grained
control over identity attributes and fail to produce paired,
identity-consistent images under structured capture conditions. We introduce
FLUXSynID, a framework for generating high-resolution synthetic face datasets
with user-defined identity attribute distributions and paired document-style
and trusted live capture images. The dataset generated using the FLUXSynID
framework shows improved alignment with real-world identity distributions and
greater inter-set diversity compared to prior work. The FLUXSynID framework for
generating custom datasets, along with a dataset of 14,889 synthetic
identities, is publicly released to support biometric research, including face
recognition and morphing attack detection.

</details>


### [217] [IKrNet: A Neural Network for Detecting Specific Drug-Induced Patterns in Electrocardiograms Amidst Physiological Variability](https://arxiv.org/pdf/2505.07533)
*Ahmad Fall, Federica Granese, Alex Lence, Dominique Fourer, Blaise Hanczar, Joe-Elie Salem, Jean-Daniel Zucker, Edi Prifti*

Main category: cs.CV

TL;DR: IKrNet, a neural network model, improves ECG analysis by accounting for physiological and drug-induced variations, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current AI-based ECG analysis lacks robustness under varying physiological conditions and drug influences, limiting real-world applicability.

Method: IKrNet uses a convolutional backbone with varying receptive fields and a bi-directional LSTM to capture spatial and temporal ECG patterns, validated on 990 volunteers administered Sotalol.

Result: IKrNet achieves higher accuracy and stability than state-of-the-art models in diverse physiological and drug-influenced scenarios.

Conclusion: IKrNet demonstrates clinical viability for robust ECG monitoring under real-world conditions.

Abstract: Monitoring and analyzing electrocardiogram (ECG) signals, even under varying
physiological conditions, including those influenced by physical activity,
drugs and stress, is crucial to accurately assess cardiac health. However,
current AI-based methods often fail to account for how these factors interact
and alter ECG patterns, ultimately limiting their applicability in real-world
settings. This study introduces IKrNet, a novel neural network model, which
identifies drug-specific patterns in ECGs amidst certain physiological
conditions. IKrNet's architecture incorporates spatial and temporal dynamics by
using a convolutional backbone with varying receptive field size to capture
spatial features. A bi-directional Long Short-Term Memory module is also
employed to model temporal dependencies. By treating heart rate variability as
a surrogate for physiological fluctuations, we evaluated IKrNet's performance
across diverse scenarios, including conditions with physical stress, drug
intake alone, and a baseline without drug presence. Our assessment follows a
clinical protocol in which 990 healthy volunteers were administered 80mg of
Sotalol, a drug which is known to be a precursor to Torsades-de-Pointes, a
life-threatening arrhythmia. We show that IKrNet outperforms state-of-the-art
models' accuracy and stability in varying physiological conditions,
underscoring its clinical viability.

</details>


### [218] [Discrete Visual Tokens of Autoregression, by Diffusion, and for Reasoning](https://arxiv.org/pdf/2505.07538)
*Bohan Wang, Zhongqi Yue, Fengda Zhang, Shuo Chen, Li'an Bi, Junzhe Zhang, Xue Song, Kennard Yanting Chan, Jiachun Pan, Weijia Wu, Mingze Zhou, Wang Lin, Kaihang Pan, Saining Zhang, Liyu Jia, Wentao Hu, Wei Zhao, Hanwang Zhang*

Main category: cs.CV

TL;DR: Selftok introduces a novel discrete visual tokenizer using an autoregressive prior, unifying diffusion and AR for VLMs, enabling effective RL for visual generation without text-image pairs.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of traditional spatial tokens in supporting reinforcement learning (RL) for visual generation and unify diffusion and autoregressive (AR) methods in vision-language models (VLMs).

Method: Develops Self-consistency Tokenizer (Selftok) using reverse diffusion to create AR visual tokens, enabling pure AR VLM training without additional modules.

Result: Selftok achieves high-quality reconstruction and compression, and RL in visual tokens boosts generation benchmarks significantly, surpassing existing models.

Conclusion: Selftok successfully bridges the gap between visual tokens and RL, advancing toward a multimodal LLM.

Abstract: We completely discard the conventional spatial prior in image representation
and introduce a novel discrete visual tokenizer: Self-consistency Tokenizer
(Selftok). At its design core, we compose an autoregressive (AR) prior --
mirroring the causal structure of language -- into visual tokens by using the
reverse diffusion process of image generation. The AR property makes Selftok
fundamentally distinct from traditional spatial tokens in the following two key
ways: - Selftok offers an elegant and minimalist approach to unify diffusion
and AR for vision-language models (VLMs): By representing images with Selftok
tokens, we can train a VLM using a purely discrete autoregressive architecture
-- like that in LLMs -- without requiring additional modules or training
objectives. - We theoretically show that the AR prior satisfies the Bellman
equation, whereas the spatial prior does not. Therefore, Selftok supports
reinforcement learning (RL) for visual generation with effectiveness comparable
to that achieved in LLMs. Besides the AR property, Selftok is also a SoTA
tokenizer that achieves a favorable trade-off between high-quality
reconstruction and compression rate. We use Selftok to build a pure AR VLM for
both visual comprehension and generation tasks. Impressively, without using any
text-image training pairs, a simple policy gradient RL working in the visual
tokens can significantly boost the visual generation benchmark, surpassing all
the existing models by a large margin. Therefore, we believe that Selftok
effectively addresses the long-standing challenge that visual tokens cannot
support effective RL. When combined with the well-established strengths of RL
in LLMs, this brings us one step closer to realizing a truly multimodal LLM.
Project Page: https://selftok-team.github.io/report/.

</details>


### [219] [GIFStream: 4D Gaussian-based Immersive Video with Feature Stream](https://arxiv.org/pdf/2505.07539)
*Hao Li, Sicheng Li, Xiang Gao, Abudouaihati Batuer, Lu Yu, Yiyi Liao*

Main category: cs.CV

TL;DR: GIFStream introduces a novel 4D Gaussian representation for immersive video, combining canonical space and deformation fields with feature streams for efficient compression and high-quality rendering.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of maintaining immersive video quality with manageable storage, leveraging 4D Gaussian Splatting's efficiency.

Method: Uses canonical space, deformation fields, and time-dependent feature streams for motion modeling and compression, with temporal and spatial compression networks.

Result: Achieves high-quality immersive video at 30 Mbps with real-time rendering and fast decoding on an RTX 4090.

Conclusion: GIFStream effectively balances quality and storage efficiency for immersive video.

Abstract: Immersive video offers a 6-Dof-free viewing experience, potentially playing a
key role in future video technology. Recently, 4D Gaussian Splatting has gained
attention as an effective approach for immersive video due to its high
rendering efficiency and quality, though maintaining quality with manageable
storage remains challenging. To address this, we introduce GIFStream, a novel
4D Gaussian representation using a canonical space and a deformation field
enhanced with time-dependent feature streams. These feature streams enable
complex motion modeling and allow efficient compression by leveraging temporal
correspondence and motion-aware pruning. Additionally, we incorporate both
temporal and spatial compression networks for end-to-end compression.
Experimental results show that GIFStream delivers high-quality immersive video
at 30 Mbps, with real-time rendering and fast decoding on an RTX 4090. Project
page: https://xdimlab.github.io/GIFStream

</details>


### [220] [SynID: Passport Synthetic Dataset for Presentation Attack Detection](https://arxiv.org/pdf/2505.07540)
*Juan E. Tapia, Fabian Stockhardt, Lázaro Janier González-Soler, Christoph Busch*

Main category: cs.CV

TL;DR: A hybrid method combining synthetic data and open-access information is proposed to create a realistic passport dataset for training Presentation Attack Detection (PAD) systems, addressing the challenge of limited real ID document availability.


<details>
  <summary>Details</summary>
Motivation: The rise in remote verification systems and synthetic image advancements has increased demand for PAD, but limited real ID documents due to privacy concerns hinder effective training.

Method: A hybrid approach merges synthetic data with open-access information, adhering to ICAO requirements, to generate realistic passport images for PAD training and testing.

Result: The proposed method yields a realistic passport dataset suitable for training and evaluating PAD systems.

Conclusion: The hybrid dataset generation method effectively addresses the scarcity of real ID documents, enhancing PAD system training for fraud detection.

Abstract: The demand for Presentation Attack Detection (PAD) to identify fraudulent ID
documents in remote verification systems has significantly risen in recent
years. This increase is driven by several factors, including the rise of remote
work, online purchasing, migration, and advancements in synthetic images.
Additionally, we have noticed a surge in the number of attacks aimed at the
enrolment process. Training a PAD to detect fake ID documents is very
challenging because of the limited number of ID documents available due to
privacy concerns. This work proposes a new passport dataset generated from a
hybrid method that combines synthetic data and open-access information using
the ICAO requirement to obtain realistic training and testing images.

</details>


### [221] [Automated Visual Attention Detection using Mobile Eye Tracking in Behavioral Classroom Studies](https://arxiv.org/pdf/2505.07552)
*Efe Bozkir, Christian Kosel, Tina Seidel, Enkelejda Kasneci*

Main category: cs.CV

TL;DR: An automated pipeline using face detection and recognition with mobile eye tracking to identify teachers' visual focus on students, reducing manual annotation needs.


<details>
  <summary>Details</summary>
Motivation: Teachers' visual attention impacts student engagement and achievement, but current methods require excessive manual effort.

Method: Combines face detection, recognition models, and gaze data from mobile eye trackers with minimal manual annotation.

Result: Achieved accuracies of 0.7 (U-shaped classrooms) and 0.9 (small classrooms) in identifying focused students.

Conclusion: The method offers a non-intrusive, scalable solution to improve instructional strategies and teacher training.

Abstract: Teachers' visual attention and its distribution across the students in
classrooms can constitute important implications for student engagement,
achievement, and professional teacher training. Despite that, inferring the
information about where and which student teachers focus on is not trivial.
Mobile eye tracking can provide vital help to solve this issue; however, the
use of mobile eye tracking alone requires a significant amount of manual
annotations. To address this limitation, we present an automated processing
pipeline concept that requires minimal manually annotated data to recognize
which student the teachers focus on. To this end, we utilize state-of-the-art
face detection models and face recognition feature embeddings to train face
recognition models with transfer learning in the classroom context and combine
these models with the teachers' gaze from mobile eye trackers. We evaluated our
approach with data collected from four different classrooms, and our results
show that while it is possible to estimate the visually focused students with
reasonable performance in all of our classroom setups, U-shaped and small
classrooms led to the best results with accuracies of approximately 0.7 and
0.9, respectively. While we did not evaluate our method for teacher-student
interactions and focused on the validity of the technical approach, as our
methodology does not require a vast amount of manually annotated data and
offers a non-intrusive way of handling teachers' visual attention, it could
help improve instructional strategies, enhance classroom management, and
provide feedback for professional teacher development.

</details>


### [222] [Self-Supervised Event Representations: Towards Accurate, Real-Time Perception on SoC FPGAs](https://arxiv.org/pdf/2505.07556)
*Kamil Jeziorek, Tomasz Kryjak*

Main category: cs.CV

TL;DR: A novel Self-Supervised Event Representation (SSER) method using GRU networks for precise event encoding, outperforming baselines in object detection with efficient hardware implementation.


<details>
  <summary>Details</summary>
Motivation: Event cameras provide advantages like high temporal resolution and low power, but processing their sparse data is challenging. Existing methods either compromise performance or temporal fidelity.

Method: SSER leverages GRU networks for self-supervised, per-pixel event encoding without temporal discretisation, ensuring high-throughput compatibility.

Result: SSER improves mAP by 2.4% and 0.6% on Gen1 and 1 Mpx datasets, with FPGA implementation achieving sub-microsecond latency and 1-2 W power.

Conclusion: SSER offers a robust, efficient solution for event data processing, suitable for real-time applications.

Abstract: Event cameras offer significant advantages over traditional frame-based
sensors. These include microsecond temporal resolution, robustness under
varying lighting conditions and low power consumption. Nevertheless, the
effective processing of their sparse, asynchronous event streams remains
challenging. Existing approaches to this problem can be categorised into two
distinct groups. The first group involves the direct processing of event data
with neural models, such as Spiking Neural Networks or Graph Convolutional
Neural Networks. However, this approach is often accompanied by a compromise in
terms of qualitative performance. The second group involves the conversion of
events into dense representations with handcrafted aggregation functions, which
can boost accuracy at the cost of temporal fidelity. This paper introduces a
novel Self-Supervised Event Representation (SSER) method leveraging Gated
Recurrent Unit (GRU) networks to achieve precise per-pixel encoding of event
timestamps and polarities without temporal discretisation. The recurrent layers
are trained in a self-supervised manner to maximise the fidelity of event-time
encoding. The inference is performed with event representations generated
asynchronously, thus ensuring compatibility with high-throughput sensors. The
experimental validation demonstrates that SSER outperforms aggregation-based
baselines, achieving improvements of 2.4% mAP and 0.6% on the Gen1 and 1 Mpx
object detection datasets. Furthermore, the paper presents the first hardware
implementation of recurrent representation for event data on a System-on-Chip
FPGA, achieving sub-microsecond latency and power consumption between 1-2 W,
suitable for real-time, power-efficient applications. Code is available at
https://github.com/vision-agh/RecRepEvent.

</details>


### [223] [Robust Kidney Abnormality Segmentation: A Validation Study of an AI-Based Framework](https://arxiv.org/pdf/2505.07573)
*Sarah de Boer, Hartmut Häntze, Kiran Vaidhya Venkadesh, Myrthe A. D. Buser, Gabriel E. Humpire Mamani, Lina Xu, Lisa C. Adams, Jawed Nawabi, Keno K. Bressem, Bram van Ginneken, Mathias Prokop, Alessa Hering*

Main category: cs.CV

TL;DR: A robust kidney abnormality segmentation algorithm is developed using nnU-Net, validated on diverse datasets, and outperforms existing models, with public availability for clinical use.


<details>
  <summary>Details</summary>
Motivation: Current clinical kidney assessments rely on subjective visual methods; this research aims to provide an objective, reproducible tool for quantitative analysis.

Method: Utilizes nnU-Net and public datasets, validated with Dice coefficient and Hausdorff distance, and tested across patient subgroups.

Result: The algorithm generalizes well, outperforms state-of-the-art models, and shows consistent performance across subgroups.

Conclusion: The publicly available algorithm offers a reliable, objective solution for kidney abnormality segmentation, enhancing clinical workflows.

Abstract: Kidney abnormality segmentation has important potential to enhance the
clinical workflow, especially in settings requiring quantitative assessments.
Kidney volume could serve as an important biomarker for renal diseases, with
changes in volume correlating directly with kidney function. Currently,
clinical practice often relies on subjective visual assessment for evaluating
kidney size and abnormalities, including tumors and cysts, which are typically
staged based on diameter, volume, and anatomical location. To support a more
objective and reproducible approach, this research aims to develop a robust,
thoroughly validated kidney abnormality segmentation algorithm, made publicly
available for clinical and research use. We employ publicly available training
datasets and leverage the state-of-the-art medical image segmentation framework
nnU-Net. Validation is conducted using both proprietary and public test
datasets, with segmentation performance quantified by Dice coefficient and the
95th percentile Hausdorff distance. Furthermore, we analyze robustness across
subgroups based on patient sex, age, CT contrast phases, and tumor histologic
subtypes. Our findings demonstrate that our segmentation algorithm, trained
exclusively on publicly available data, generalizes effectively to external
test sets and outperforms existing state-of-the-art models across all tested
datasets. Subgroup analyses reveal consistent high performance, indicating
strong robustness and reliability. The developed algorithm and associated code
are publicly accessible at
https://github.com/DIAGNijmegen/oncology-kidney-abnormality-segmentation.

</details>


### [224] [Evaluating Modern Visual Anomaly Detection Approaches in Semiconductor Manufacturing: A Comparative Study](https://arxiv.org/pdf/2505.07576)
*Manuel Barusco, Francesco Borsatti, Youssef Ben Khalifa, Davide Dalle Pezze, Gian Antonio Susto*

Main category: cs.CV

TL;DR: A benchmark for unsupervised Visual Anomaly Detection (VAD) in semiconductor manufacturing is introduced, showing its effectiveness using the MIIC dataset.


<details>
  <summary>Details</summary>
Motivation: Automated visual inspection of SEM images is crucial for cost and downtime reduction, but supervised methods require labeled anomalies, which are costly to collect.

Method: Unsupervised learning is used for VAD, avoiding the need for labeled anomaly samples.

Result: Modern VAD approaches prove effective in semiconductor manufacturing.

Conclusion: Unsupervised VAD is a viable solution for anomaly detection in semiconductor SEM images.

Abstract: Semiconductor manufacturing is a complex, multistage process. Automated
visual inspection of Scanning Electron Microscope (SEM) images is indispensable
for minimizing equipment downtime and containing costs. Most previous research
considers supervised approaches, assuming a sufficient number of anomalously
labeled samples. On the contrary, Visual Anomaly Detection (VAD), an emerging
research domain, focuses on unsupervised learning, avoiding the costly defect
collection phase while providing explanations of the predictions. We introduce
a benchmark for VAD in the semiconductor domain by leveraging the MIIC dataset.
Our results demonstrate the efficacy of modern VAD approaches in this field.

</details>


### [225] [Deep Learning Advances in Vision-Based Traffic Accident Anticipation: A Comprehensive Review of Methods,Datasets,and Future Directions](https://arxiv.org/pdf/2505.07611)
*Yi Zhang, Wenye Zhou, Ruonan Lin, Xin Yang, Hao Zheng*

Main category: cs.CV

TL;DR: A review of 147 studies on vision-based traffic accident anticipation (Vision-TAA) categorizes methods into four approaches, identifies challenges like data scarcity, and suggests future research directions.


<details>
  <summary>Details</summary>
Motivation: To enhance road safety by reviewing advancements and gaps in Vision-TAA using deep learning models.

Method: Categorizes current methodologies into image/video feature-based, spatiotemporal feature-based, scene understanding, and multimodal data fusion.

Result: Identifies challenges like data scarcity and limited generalization, while highlighting potential in multimodal fusion and Transformer-based architectures.

Conclusion: Provides a foundational reference for robust Vision-TAA systems, contributing to road safety and traffic management.

Abstract: Traffic accident prediction and detection are critical for enhancing road
safety,and vision-based traffic accident anticipation (Vision-TAA) has emerged
as a promising approach in the era of deep learning.This paper reviews 147
recent studies,focusing on the application of supervised,unsupervised,and
hybrid deep learning models for accident prediction,alongside the use of
real-world and synthetic datasets.Current methodologies are categorized into
four key approaches: image and video feature-based prediction, spatiotemporal
feature-based prediction, scene understanding,and multimodal data fusion.While
these methods demonstrate significant potential,challenges such as data
scarcity,limited generalization to complex scenarios,and real-time performance
constraints remain prevalent. This review highlights opportunities for future
research,including the integration of multimodal data fusion, self-supervised
learning,and Transformer-based architectures to enhance prediction accuracy and
scalability.By synthesizing existing advancements and identifying critical
gaps, this paper provides a foundational reference for developing robust and
adaptive Vision-TAA systems,contributing to road safety and traffic management.

</details>


### [226] [Higher-Order Convolution Improves Neural Predictivity in the Retina](https://arxiv.org/pdf/2505.07620)
*Simone Azeglio, Victor Calbiague Garcia, Guilhem Glaziou, Peter Neri, Olivier Marre, Ulisse Ferrari*

Main category: cs.CV

TL;DR: A novel higher-order CNN (HoCNN) improves neural response prediction by embedding multiplicative interactions in CNNs, outperforming traditional models with less training data and better correlation with biological responses.


<details>
  <summary>Details</summary>
Motivation: Address the disparity between deep artificial networks and shallow biological visual systems by enhancing CNN representational power without increasing depth.

Method: Extends 3D CNNs with higher-order operations within the convolutional operator to model multiplicative interactions across space and time.

Result: HoCNN achieves superior performance (correlation up to 0.75) with half the training data, and improves state-of-the-art architectures across species and stimuli.

Conclusion: HoCNN naturally encodes geometric transformations, especially scaling, and excels in predicting responses for specific cell types, doubling correlation for scaling parameters.

Abstract: We present a novel approach to neural response prediction that incorporates
higher-order operations directly within convolutional neural networks (CNNs).
Our model extends traditional 3D CNNs by embedding higher-order operations
within the convolutional operator itself, enabling direct modeling of
multiplicative interactions between neighboring pixels across space and time.
Our model increases the representational power of CNNs without increasing their
depth, therefore addressing the architectural disparity between deep artificial
networks and the relatively shallow processing hierarchy of biological visual
systems. We evaluate our approach on two distinct datasets: salamander retinal
ganglion cell (RGC) responses to natural scenes, and a new dataset of mouse RGC
responses to controlled geometric transformations. Our higher-order CNN (HoCNN)
achieves superior performance while requiring only half the training data
compared to standard architectures, demonstrating correlation coefficients up
to 0.75 with neural responses (against 0.80$\pm$0.02 retinal reliability). When
integrated into state-of-the-art architectures, our approach consistently
improves performance across different species and stimulus conditions. Analysis
of the learned representations reveals that our network naturally encodes
fundamental geometric transformations, particularly scaling parameters that
characterize object expansion and contraction. This capability is especially
relevant for specific cell types, such as transient OFF-alpha and transient ON
cells, which are known to detect looming objects and object motion
respectively, and where our model shows marked improvement in response
prediction. The correlation coefficients for scaling parameters are more than
twice as high in HoCNN (0.72) compared to baseline models (0.32).

</details>


### [227] [Semantic and Expressive Variation in Image Captions Across Languages](https://arxiv.org/pdf/2310.14356)
*Andre Ye, Sebastin Santy, Jena D. Hwang, Amy X. Zhang, Ranjay Krishna*

Main category: cs.CV

TL;DR: The paper highlights cultural differences in human perception of visual stimuli, using language as a proxy, and shows how multilingual datasets improve semantic coverage in vision-language models.


<details>
  <summary>Details</summary>
Motivation: To challenge the homogeneous perception assumption in computer vision by demonstrating cultural diversity in visual interpretation through language.

Method: Analyzed textual descriptions of the same images across 7 languages, measuring semantic coverage via scene graphs, embeddings, and taxonomies.

Result: Multilingual descriptions had 29.9% more objects, 24.5% more relations, and 46.0% more attributes than monolingual ones. Models like LLaVA inherited this bias.

Conclusion: Embracing human perception diversity is crucial for computer vision, with multilingual datasets enhancing model performance across languages.

Abstract: Computer vision often treats human perception as homogeneous: an implicit
assumption that visual stimuli are perceived similarly by everyone. This
assumption is reflected in the way researchers collect datasets and train
vision models. By contrast, literature in cross-cultural psychology and
linguistics has provided evidence that people from different cultural
backgrounds observe vastly different concepts even when viewing the same visual
stimuli. In this paper, we study how these differences manifest themselves in
vision-language datasets and models, using language as a proxy for culture. By
comparing textual descriptions generated across 7 languages for the same
images, we find significant differences in the semantic content and linguistic
expression. When datasets are multilingual as opposed to monolingual,
descriptions have higher semantic coverage on average, where coverage is
measured using scene graphs, model embeddings, and linguistic taxonomies. For
example, multilingual descriptions have on average 29.9% more objects, 24.5%
more relations, and 46.0% more attributes than a set of monolingual captions.
When prompted to describe images in different languages, popular models (e.g.
LLaVA) inherit this bias and describe different parts of the image. Moreover,
finetuning models on captions from one language performs best on corresponding
test data from that language, while finetuning on multilingual data performs
consistently well across all test data compositions. Our work points towards
the need to account for and embrace the diversity of human perception in the
computer vision community.

</details>


### [228] [A Unified Hierarchical Framework for Fine-grained Cross-view Geo-localization over Large-scale Scenarios](https://arxiv.org/pdf/2505.07622)
*Zhuo Song, Ye Zhang, Kunhong Li, Longguang Wang, Yulan Guo*

Main category: cs.CV

TL;DR: UnifyGeo is a unified hierarchical framework for cross-view geo-localization, combining retrieval and metric localization tasks into one network for improved efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods treat retrieval and metric localization as separate tasks, leading to inefficiency and higher training overhead. UnifyGeo aims to unify these tasks for better collaboration.

Method: The framework uses a unified learning strategy with shared parameters for multi-granularity representation and a re-ranking mechanism with a dedicated loss function.

Result: UnifyGeo outperforms state-of-the-art methods, significantly improving 1-meter-level localization recall rates on the VIGOR benchmark.

Conclusion: UnifyGeo demonstrates the effectiveness of unifying retrieval and metric localization tasks, achieving superior performance and efficiency.

Abstract: Cross-view geo-localization is a promising solution for large-scale
localization problems, requiring the sequential execution of retrieval and
metric localization tasks to achieve fine-grained predictions. However,
existing methods typically focus on designing standalone models for these two
tasks, resulting in inefficient collaboration and increased training overhead.
In this paper, we propose UnifyGeo, a novel unified hierarchical
geo-localization framework that integrates retrieval and metric localization
tasks into a single network. Specifically, we first employ a unified learning
strategy with shared parameters to jointly learn multi-granularity
representation, facilitating mutual reinforcement between these two tasks.
Subsequently, we design a re-ranking mechanism guided by a dedicated loss
function, which enhances geo-localization performance by improving both
retrieval accuracy and metric localization references. Extensive experiments
demonstrate that UnifyGeo significantly outperforms the state-of-the-arts in
both task-isolated and task-associated settings. Remarkably, on the challenging
VIGOR benchmark, which supports fine-grained localization evaluation, the
1-meter-level localization recall rate improves from 1.53\% to 39.64\% and from
0.43\% to 25.58\% under same-area and cross-area evaluations, respectively.
Code will be made publicly available.

</details>


### [229] [ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models](https://arxiv.org/pdf/2505.07652)
*Ozgur Kara, Krishna Kumar Singh, Feng Liu, Duygu Ceylan, James M. Rehg, Tobias Hinz*

Main category: cs.CV

TL;DR: A framework for generating multi-shot videos with discrete transitions using diffusion models, addressing limitations of current text-to-video methods.


<details>
  <summary>Details</summary>
Motivation: Current text-to-video methods can't produce multi-shot videos with consistent characters and backgrounds.

Method: Proposes a framework with dataset collection and architectural extensions, including transition tokens and local attention masking.

Result: Enables multi-shot video generation with shot-specific control, outperforming baselines after minimal fine-tuning.

Conclusion: The approach successfully extends text-to-video models for multi-shot generation with user control.

Abstract: Current diffusion-based text-to-video methods are limited to producing short
video clips of a single shot and lack the capability to generate multi-shot
videos with discrete transitions where the same character performs distinct
activities across the same or different backgrounds. To address this limitation
we propose a framework that includes a dataset collection pipeline and
architectural extensions to video diffusion models to enable text-to-multi-shot
video generation. Our approach enables generation of multi-shot videos as a
single video with full attention across all frames of all shots, ensuring
character and background consistency, and allows users to control the number,
duration, and content of shots through shot-specific conditioning. This is
achieved by incorporating a transition token into the text-to-video model to
control at which frames a new shot begins and a local attention masking
strategy which controls the transition token's effect and allows shot-specific
prompting. To obtain training data we propose a novel data collection pipeline
to construct a multi-shot video dataset from existing single-shot video
datasets. Extensive experiments demonstrate that fine-tuning a pre-trained
text-to-video model for a few thousand iterations is enough for the model to
subsequently be able to generate multi-shot videos with shot-specific control,
outperforming the baselines. You can find more details in
https://shotadapter.github.io/

</details>


### [230] [Anatomical Attention Alignment representation for Radiology Report Generation](https://arxiv.org/pdf/2505.07689)
*Quang Vinh Nguyen, Minh Duc Nguyen, Thanh Hoang Son Vo, Hyung-Jeong Yang, Soo-Hyung Kim*

Main category: cs.CV

TL;DR: A3Net improves radiology report generation by integrating anatomical knowledge with visual features, enhancing accuracy and clinical relevance.


<details>
  <summary>Details</summary>
Motivation: Existing models rely solely on visual features, limiting understanding of spatial and semantic relationships, leading to suboptimal reports.

Method: Proposes A3Net, which combines a knowledge dictionary of anatomical structures with patch-level visual features for better cross-modal alignment.

Result: A3Net outperforms on IU X-Ray and MIMIC-CXR datasets, improving visual perception and text generation quality.

Conclusion: A3Net enhances radiology report generation by aligning anatomical knowledge with visual features, yielding more accurate and clinically relevant reports.

Abstract: Automated Radiology report generation (RRG) aims at producing detailed
descriptions of medical images, reducing radiologists' workload and improving
access to high-quality diagnostic services. Existing encoder-decoder models
only rely on visual features extracted from raw input images, which can limit
the understanding of spatial structures and semantic relationships, often
resulting in suboptimal text generation. To address this, we propose Anatomical
Attention Alignment Network (A3Net), a framework that enhance visual-textual
understanding by constructing hyper-visual representations. Our approach
integrates a knowledge dictionary of anatomical structures with patch-level
visual features, enabling the model to effectively associate image regions with
their corresponding anatomical entities. This structured representation
improves semantic reasoning, interpretability, and cross-modal alignment,
ultimately enhancing the accuracy and clinical relevance of generated reports.
Experimental results on IU X-Ray and MIMIC-CXR datasets demonstrate that A3Net
significantly improves both visual perception and text generation quality. Our
code is available at \href{https://github.com/Vinh-AI/A3Net}{GitHub}.

</details>


### [231] [Beyond CLIP Generalization: Against Forward&Backward Forgetting Adapter for Continual Learning of Vision-Language Models](https://arxiv.org/pdf/2505.07690)
*Songlin Dong, Chenhao Ding, Jiangyang Li, Jizhou Han, Qiang Wang, Yuhang He, Yihong Gong*

Main category: cs.CV

TL;DR: The paper proposes AFA, a novel framework for multi-domain task incremental learning (MTIL) in vision-language models (VLMs), enhancing zero-shot and few-shot capabilities while outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address the limitation of current MTIL paradigms that fail to enhance VLM generalization beyond maintaining zero-shot recognition, aiming to improve both zero-shot and few-shot learning.

Method: Introduces AFA with two adapters: (1) against forward-forgetting for task-invariant learning to boost zero-shot recognition, and (2) against backward-forgetting to strengthen few-shot learning during incremental tasks.

Result: AFA outperforms state-of-the-art methods, especially in few-shot MTIL tasks, and exceeds CLIP's zero-shot transferability.

Conclusion: The AFA framework effectively enhances VLMs' incremental learning capabilities, improving both zero-shot and few-shot performance.

Abstract: This study aims to address the problem of multi-domain task incremental
learning~(MTIL), which requires that vision-language models~(VLMs) continuously
acquire new knowledge while maintaining their inherent zero-shot recognition
capability. Existing paradigms delegate the testing of unseen-domain samples to
the original CLIP, which only prevents the degradation of the model's zero-shot
capability but fails to enhance the generalization of the VLM further. To this
end, we propose a novel MTIL framework, named AFA, which comprises two core
modules: (1) an against forward-forgetting adapter that learns task-invariant
information for each dataset in the incremental tasks to enhance the zero-shot
recognition ability of VLMs; (2) an against backward-forgetting adapter that
strengthens the few-shot learning capability of VLMs while supporting
incremental learning. Extensive experiments demonstrate that the AFA method
significantly outperforms existing state-of-the-art approaches, especially in
few-shot MTIL tasks, and surpasses the inherent zero-shot performance of CLIP
in terms of transferability. The code is provided in the Supplementary
Material.

</details>


### [232] [Feedback-Driven Pseudo-Label Reliability Assessment: Redefining Thresholding for Semi-Supervised Semantic Segmentation](https://arxiv.org/pdf/2505.07691)
*Negin Ghamsarian, Sahar Nasirihaghighi, Klaus Schoeffmann, Raphael Sznitman*

Main category: cs.CV

TL;DR: ENCORE introduces a dynamic feedback-driven thresholding strategy for pseudo-label selection in semi-supervised learning, eliminating the need for manual threshold tuning and improving model performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of selecting optimal confidence thresholds for pseudo-label filtering in semi-supervised learning, especially when labeled data is scarce.

Method: Proposes ENCORE, which dynamically adjusts class-wise true-positive confidence thresholds based on model feedback, retaining informative pseudo-labels while filtering unreliable ones.

Result: ENCORE enhances segmentation performance in data-scarce conditions and integrates seamlessly with existing pseudo-supervision frameworks, improving results across datasets and architectures.

Conclusion: ENCORE is an effective solution for semi-supervised learning, offering dynamic thresholding without manual tuning and boosting model performance.

Abstract: Semi-supervised learning leverages unlabeled data to enhance model
performance, addressing the limitations of fully supervised approaches. Among
its strategies, pseudo-supervision has proven highly effective, typically
relying on one or multiple teacher networks to refine pseudo-labels before
training a student network. A common practice in pseudo-supervision is
filtering pseudo-labels based on pre-defined confidence thresholds or entropy.
However, selecting optimal thresholds requires large labeled datasets, which
are often scarce in real-world semi-supervised scenarios. To overcome this
challenge, we propose Ensemble-of-Confidence Reinforcement (ENCORE), a dynamic
feedback-driven thresholding strategy for pseudo-label selection. Instead of
relying on static confidence thresholds, ENCORE estimates class-wise
true-positive confidence within the unlabeled dataset and continuously adjusts
thresholds based on the model's response to different levels of pseudo-label
filtering. This feedback-driven mechanism ensures the retention of informative
pseudo-labels while filtering unreliable ones, enhancing model training without
manual threshold tuning. Our method seamlessly integrates into existing
pseudo-supervision frameworks and significantly improves segmentation
performance, particularly in data-scarce conditions. Extensive experiments
demonstrate that integrating ENCORE with existing pseudo-supervision frameworks
enhances performance across multiple datasets and network architectures,
validating its effectiveness in semi-supervised learning.

</details>


### [233] [Hybrid Spiking Vision Transformer for Object Detection with Event Cameras](https://arxiv.org/pdf/2505.07715)
*Qi Xu, Jie Deng, Jiangrong Shen, Biwu Chen, Huajin Tang, Gang Pan*

Main category: cs.CV

TL;DR: A hybrid spike vision Transformer (HsVT) model is proposed for event-based object detection, combining spatial and temporal feature extraction to improve performance. A new dataset, The Fall Detection Dataset, is introduced for benchmarking.


<details>
  <summary>Details</summary>
Motivation: To enhance event-based object detection by leveraging the advantages of Spiking Neural Networks (SNNs) and address the need for better spatiotemporal feature capture.

Method: The HsVT model integrates spatial and temporal feature extraction modules to capture local, global, and time-dependent features in event sequences.

Result: HsVT achieves significant performance improvements in event detection with fewer parameters, as demonstrated on GEN1 and Fall Detection datasets.

Conclusion: The HsVT model and the new dataset advance event-based object detection, offering efficient and effective solutions for complex tasks.

Abstract: Event-based object detection has gained increasing attention due to its
advantages such as high temporal resolution, wide dynamic range, and
asynchronous address-event representation. Leveraging these advantages, Spiking
Neural Networks (SNNs) have emerged as a promising approach, offering low
energy consumption and rich spatiotemporal dynamics. To further enhance the
performance of event-based object detection, this study proposes a novel hybrid
spike vision Transformer (HsVT) model. The HsVT model integrates a spatial
feature extraction module to capture local and global features, and a temporal
feature extraction module to model time dependencies and long-term patterns in
event sequences. This combination enables HsVT to capture spatiotemporal
features, improving its capability to handle complex event-based object
detection tasks. To support research in this area, we developed and publicly
released The Fall Detection Dataset as a benchmark for event-based object
detection tasks. This dataset, captured using an event-based camera, ensures
facial privacy protection and reduces memory usage due to the event
representation format. We evaluated the HsVT model on GEN1 and Fall Detection
datasets across various model sizes. Experimental results demonstrate that HsVT
achieves significant performance improvements in event detection with fewer
parameters.

</details>


### [234] [Gameplay Highlights Generation](https://arxiv.org/pdf/2505.07721)
*Vignesh Edithal, Le Zhang, Ilia Blank, Imran Junejo*

Main category: cs.CV

TL;DR: Automated generation of gaming highlight reels using a finetuned X-CLIP model, achieving 90% accuracy in detecting interesting events across games without per-game engineering.


<details>
  <summary>Details</summary>
Motivation: To save gamers time and increase audience engagement by automating the creation of eye-catching highlight reels from gameplay sessions.

Method: Finetuned a multimodal X-CLIP model on an in-house dataset of annotated gameplay events, using prompt engineering for improved performance. The model generalizes across games without per-game adjustments.

Result: Achieved over 90% accuracy in detecting interesting events in first-person shooting games, with transfer learning benefits for low-resource games.

Conclusion: The approach is scalable, efficient, and production-ready, leveraging ONNX for cross-platform deployment and demonstrating the effectiveness of natural language supervision in video recognition.

Abstract: In this work, we enable gamers to share their gaming experience on social
media by automatically generating eye-catching highlight reels from their
gameplay session Our automation will save time for gamers while increasing
audience engagement. We approach the highlight generation problem by first
identifying intervals in the video where interesting events occur and then
concatenate them. We developed an in-house gameplay event detection dataset
containing interesting events annotated by humans using VIA video annotator.
Traditional techniques for highlight detection such as game engine integration
requires expensive collaboration with game developers. OCR techniques which
detect patches of specific images or texts require expensive per game
engineering and may not generalize across game UI and different language. We
finetuned a multimodal general purpose video understanding model such as X-CLIP
using our dataset which generalizes across multiple games in a genre without
per game engineering. Prompt engineering was performed to improve the
classification performance of this multimodal model. Our evaluation showed that
such a finetuned model can detect interesting events in first person shooting
games from unseen gameplay footage with more than 90% accuracy. Moreover, our
model performed significantly better on low resource games (small dataset) when
trained along with high resource games, showing signs of transfer learning. To
make the model production ready, we used ONNX libraries to enable cross
platform inference. These libraries also provide post training quantization
tools to reduce model size and inference time for deployment. ONNX runtime
libraries with DirectML backend were used to perform efficient inference on
Windows OS. We show that natural language supervision in the X-CLIP model leads
to data efficient and highly performant video recognition models.

</details>


### [235] [LAMM-ViT: AI Face Detection via Layer-Aware Modulation of Region-Guided Attention](https://arxiv.org/pdf/2505.07734)
*Jiangling Zhang, Weijie Zhu, Jirui Huang, Yaxiong Chen*

Main category: cs.CV

TL;DR: LAMM-ViT introduces a Vision Transformer with RG-MHA and LAMM components for robust AI-synthetic face detection, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current methods fail to detect novel generative models due to reliance on specific artifacts rather than fundamental inconsistencies.

Method: LAMM-ViT integrates RG-MHA for regional attention and LAMM for dynamic parameter modulation, capturing hierarchical forgery cues.

Result: Achieves 94.09% mean ACC (+5.45% over SoTA) and 98.62% mean AP (+3.09% over SoTA) in cross-model tests.

Conclusion: LAMM-ViT generalizes well and is promising for detecting evolving synthetic media threats.

Abstract: Detecting AI-synthetic faces presents a critical challenge: it is hard to
capture consistent structural relationships between facial regions across
diverse generation techniques. Current methods, which focus on specific
artifacts rather than fundamental inconsistencies, often fail when confronted
with novel generative models. To address this limitation, we introduce
Layer-aware Mask Modulation Vision Transformer (LAMM-ViT), a Vision Transformer
designed for robust facial forgery detection. This model integrates distinct
Region-Guided Multi-Head Attention (RG-MHA) and Layer-aware Mask Modulation
(LAMM) components within each layer. RG-MHA utilizes facial landmarks to create
regional attention masks, guiding the model to scrutinize architectural
inconsistencies across different facial areas. Crucially, the separate LAMM
module dynamically generates layer-specific parameters, including mask weights
and gating values, based on network context. These parameters then modulate the
behavior of RG-MHA, enabling adaptive adjustment of regional focus across
network depths. This architecture facilitates the capture of subtle,
hierarchical forgery cues ubiquitous among diverse generation techniques, such
as GANs and Diffusion Models. In cross-model generalization tests, LAMM-ViT
demonstrates superior performance, achieving 94.09% mean ACC (a +5.45%
improvement over SoTA) and 98.62% mean AP (a +3.09% improvement). These results
demonstrate LAMM-ViT's exceptional ability to generalize and its potential for
reliable deployment against evolving synthetic media threats.

</details>


### [236] [BodyGPS: Anatomical Positioning System](https://arxiv.org/pdf/2505.07744)
*Halid Ziya Yerebakan, Kritika Iyer, Xueqi Guo, Yoshihisa Shinagawa, Gerardo Hermosillo Valadez*

Main category: cs.CV

TL;DR: A foundational model for parsing human anatomy in medical images, supporting supervised/unsupervised training and tasks like matching, registration, classification, or segmentation.


<details>
  <summary>Details</summary>
Motivation: To create a versatile model for medical image analysis across modalities with efficient performance.

Method: Trains a neural network estimator to map query locations to atlas coordinates via regression, using sparse sampling for efficiency.

Result: Achieves response times under 1 ms without extra hardware, demonstrated in CT and MRI.

Conclusion: The model is efficient and adaptable for various medical imaging tasks.

Abstract: We introduce a new type of foundational model for parsing human anatomy in
medical images that works for different modalities. It supports supervised or
unsupervised training and can perform matching, registration, classification,
or segmentation with or without user interaction. We achieve this by training a
neural network estimator that maps query locations to atlas coordinates via
regression. Efficiency is improved by sparsely sampling the input, enabling
response times of less than 1 ms without additional accelerator hardware. We
demonstrate the utility of the algorithm in both CT and MRI modalities.

</details>


### [237] [Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured 3D Assets](https://arxiv.org/pdf/2505.07747)
*Weiyu Li, Xuanyang Zhang, Zheng Sun, Di Qi, Hao Li, Wei Cheng, Weiwei Cai, Shihao Wu, Jiarui Liu, Zihao Wang, Xiao Chen, Feipeng Tian, Jianxiong Pan, Zeming Li, Gang Yu, Xiangyu Zhang, Daxin Jiang, Ping Tan*

Main category: cs.CV

TL;DR: Step1X-3D is an open framework for 3D generation, addressing data scarcity and algorithmic challenges with a curated dataset, hybrid VAE-DiT architecture, and diffusion-based texture synthesis, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: 3D generation lags behind other AI domains due to data scarcity, algorithmic limitations, and ecosystem fragmentation. Step1X-3D aims to overcome these challenges.

Method: The framework uses a two-stage approach: (1) a hybrid VAE-DiT geometry generator for TSDF representations, and (2) a diffusion-based texture synthesis module for cross-view consistency.

Result: Benchmarks show state-of-the-art performance, surpassing open-source methods and matching proprietary solutions. It also enables 2D-to-3D control transfer.

Conclusion: Step1X-3D sets new standards for open research in controllable 3D asset generation by improving data quality, algorithmic fidelity, and reproducibility.

Abstract: While generative artificial intelligence has advanced significantly across
text, image, audio, and video domains, 3D generation remains comparatively
underdeveloped due to fundamental challenges such as data scarcity, algorithmic
limitations, and ecosystem fragmentation. To this end, we present Step1X-3D, an
open framework addressing these challenges through: (1) a rigorous data
curation pipeline processing >5M assets to create a 2M high-quality dataset
with standardized geometric and textural properties; (2) a two-stage 3D-native
architecture combining a hybrid VAE-DiT geometry generator with an
diffusion-based texture synthesis module; and (3) the full open-source release
of models, training code, and adaptation modules. For geometry generation, the
hybrid VAE-DiT component produces TSDF representations by employing
perceiver-based latent encoding with sharp edge sampling for detail
preservation. The diffusion-based texture synthesis module then ensures
cross-view consistency through geometric conditioning and latent-space
synchronization. Benchmark results demonstrate state-of-the-art performance
that exceeds existing open-source methods, while also achieving competitive
quality with proprietary solutions. Notably, the framework uniquely bridges the
2D and 3D generation paradigms by supporting direct transfer of 2D control
techniques~(e.g., LoRA) to 3D synthesis. By simultaneously advancing data
quality, algorithmic fidelity, and reproducibility, Step1X-3D aims to establish
new standards for open research in controllable 3D asset generation.

</details>


### [238] [Continuous Visual Autoregressive Generation via Score Maximization](https://arxiv.org/pdf/2505.07812)
*Chenze Shao, Fandong Meng, Jie Zhou*

Main category: cs.CV

TL;DR: The paper introduces Continuous VAR, a framework for direct visual autoregressive generation without quantization, using strictly proper scoring rules for training.


<details>
  <summary>Details</summary>
Motivation: To address information loss in quantization-based autoregressive models for continuous visual data.

Method: Proposes Continuous VAR, leveraging strictly proper scoring rules (e.g., energy score) as training objectives for likelihood-free optimization.

Result: Enables direct autoregressive generation in continuous space, unifying approaches like GIVT and diffusion loss under the framework.

Conclusion: Continuous VAR provides a flexible, theoretically grounded solution for autoregressive modeling of continuous visual data.

Abstract: Conventional wisdom suggests that autoregressive models are used to process
discrete data. When applied to continuous modalities such as visual data,
Visual AutoRegressive modeling (VAR) typically resorts to quantization-based
approaches to cast the data into a discrete space, which can introduce
significant information loss. To tackle this issue, we introduce a Continuous
VAR framework that enables direct visual autoregressive generation without
vector quantization. The underlying theoretical foundation is strictly proper
scoring rules, which provide powerful statistical tools capable of evaluating
how well a generative model approximates the true distribution. Within this
framework, all we need is to select a strictly proper score and set it as the
training objective to optimize. We primarily explore a class of training
objectives based on the energy score, which is likelihood-free and thus
overcomes the difficulty of making probabilistic predictions in the continuous
space. Previous efforts on continuous autoregressive generation, such as GIVT
and diffusion loss, can also be derived from our framework using other strictly
proper scores. Source code: https://github.com/shaochenze/EAR.

</details>


### [239] [SegSub: Evaluating Robustness to Knowledge Conflicts and Hallucinations in Vision-Language Models](https://arxiv.org/pdf/2502.14908)
*Peter Carragher, Nikitha Rao, Abhinand Jha, R Raghav, Kathleen M. Carley*

Main category: cs.CV

TL;DR: The paper introduces a framework to study VLM resilience against knowledge conflicts, revealing vulnerabilities and proposing improvements through fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Address the lack of systematic investigation of cross-modal knowledge conflicts in VLMs, which are prone to hallucination in information-sensitive contexts.

Method: Introduces a framework (\segsub) using targeted image perturbations to analyze VLM resilience against knowledge conflicts.

Result: VLMs show robustness to parametric conflicts (20% adherence) but struggle with counterfactual conditions (<30% accuracy) and source conflicts (<1% accuracy). Contextual richness correlates with hallucination rates (r = -0.368, p = 0.003). Fine-tuning improves conflict detection.

Conclusion: The study provides insights into VLM vulnerabilities and a foundation for developing hallucination-resilient systems in sensitive environments.

Abstract: Vision language models (VLM) demonstrate sophisticated multimodal reasoning
yet are prone to hallucination when confronted with knowledge conflicts,
impeding their deployment in information-sensitive contexts. While existing
research addresses robustness in unimodal models, the multimodal domain lacks
systematic investigation of cross-modal knowledge conflicts. This research
introduces \segsub, a framework for applying targeted image perturbations to
investigate VLM resilience against knowledge conflicts. Our analysis reveals
distinct vulnerability patterns: while VLMs are robust to parametric conflicts
(20% adherence rates), they exhibit significant weaknesses in identifying
counterfactual conditions (<30% accuracy) and resolving source conflicts (<1%
accuracy). Correlations between contextual richness and hallucination rate (r =
-0.368, p = 0.003) reveal the kinds of images that are likely to cause
hallucinations. Through targeted fine-tuning on our benchmark dataset, we
demonstrate improvements in VLM knowledge conflict detection, establishing a
foundation for developing hallucination-resilient multimodal systems in
information-sensitive environments.

</details>


### [240] [DanceGRPO: Unleashing GRPO on Visual Generation](https://arxiv.org/pdf/2505.07818)
*Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, Ping Luo*

Main category: cs.CV

TL;DR: DanceGRPO is a unified RL framework for visual generation, compatible with diffusion models and rectified flows, improving performance by up to 181% across tasks like text-to-image and video generation.


<details>
  <summary>Details</summary>
Motivation: Aligning generative model outputs with human preferences is challenging due to RL limitations like incompatibility with ODE-based sampling and instability in large-scale training.

Method: DanceGRPO adapts Group Relative Policy Optimization (GRPO) to visual generation, unifying RL across generative paradigms, tasks, foundation models, and reward models.

Result: Outperforms baselines by up to 181% on benchmarks like HPS-v2.1 and CLIP Score, stabilizing video generation and improving denoising trajectories.

Conclusion: DanceGRPO is a robust solution for RLHF in visual generation, harmonizing RL and visual synthesis.

Abstract: Recent breakthroughs in generative models-particularly diffusion models and
rectified flows-have revolutionized visual content creation, yet aligning model
outputs with human preferences remains a critical challenge. Existing
reinforcement learning (RL)-based methods for visual generation face critical
limitations: incompatibility with modern Ordinary Differential Equations
(ODEs)-based sampling paradigms, instability in large-scale training, and lack
of validation for video generation. This paper introduces DanceGRPO, the first
unified framework to adapt Group Relative Policy Optimization (GRPO) to visual
generation paradigms, unleashing one unified RL algorithm across two generative
paradigms (diffusion models and rectified flows), three tasks (text-to-image,
text-to-video, image-to-video), four foundation models (Stable Diffusion,
HunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video
aesthetics, text-image alignment, video motion quality, and binary reward). To
our knowledge, DanceGRPO is the first RL-based unified framework capable of
seamless adaptation across diverse generative paradigms, tasks, foundational
models, and reward models. DanceGRPO demonstrates consistent and substantial
improvements, which outperform baselines by up to 181% on benchmarks such as
HPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can
stabilize policy optimization for complex video generation, but also enables
generative policy to better capture denoising trajectories for Best-of-N
inference scaling and learn from sparse binary feedback. Our results establish
DanceGRPO as a robust and versatile solution for scaling Reinforcement Learning
from Human Feedback (RLHF) tasks in visual generation, offering new insights
into harmonizing reinforcement learning and visual synthesis. The code will be
released.

</details>


### [241] [Review helps learn better: Temporal Supervised Knowledge Distillation](https://arxiv.org/pdf/2307.00811)
*Dongwei Wang, Zhi Han, Yanmei Wang, Xiai Chen, Baichen Liu, Yandong Tang*

Main category: cs.CV

TL;DR: The paper proposes Temporal Supervised Knowledge Distillation (TSKD), leveraging temporal sequence properties in feature map evolution to improve network training by refining old knowledge dynamically.


<details>
  <summary>Details</summary>
Motivation: Knowledge acquisition benefits from temporal relationships in learning, and temporal supervision can enhance network training performance.

Method: Uses Conv-LSTM to extract spatiotemporal features across training phases, training the student network with dynamic targets instead of static teacher features.

Result: Outperforms existing knowledge distillation methods in tasks like image classification and object detection.

Conclusion: TSKD effectively refines and utilizes temporal knowledge, improving learning performance across diverse tasks.

Abstract: Reviewing plays an important role when learning knowledge. The knowledge
acquisition at a certain time point may be strongly inspired with the help of
previous experience. Thus the knowledge growing procedure should show strong
relationship along the temporal dimension. In our research, we find that during
the network training, the evolution of feature map follows temporal sequence
property. A proper temporal supervision may further improve the network
training performance. Inspired by this observation, we propose Temporal
Supervised Knowledge Distillation (TSKD). Specifically, we extract the
spatiotemporal features in the different training phases of student by
convolutional Long Short-term memory network (Conv-LSTM). Then, we train the
student net through a dynamic target, rather than static teacher network
features. This process realizes the refinement of old knowledge in student
network, and utilizes it to assist current learning. Extensive experiments
verify the effectiveness and advantages of our method over existing knowledge
distillation methods, including various network architectures and different
tasks (image classification and object detection) .

</details>


### [242] [Condition numbers in multiview geometry, instability in relative pose estimation, and RANSAC](https://arxiv.org/pdf/2310.02719)
*Hongyi Fan, Joe Kileel, Benjamin Kimia*

Main category: cs.CV

TL;DR: A framework for analyzing numerical conditioning in minimal problems of multiple view geometry, revealing instabilities in 5- and 7-point RANSAC algorithms and offering computational tests for condition assessment.


<details>
  <summary>Details</summary>
Motivation: Addressing failures in relative pose estimation via RANSAC, even with sufficient data and no outliers, due to intrinsic instability in minimal problems.

Method: Uses computational algebra and Riemannian geometry to analyze instability, characterizing problematic scenes and image data, and developing condition number tests.

Result: Identifies instabilities in minimal problems and shows RANSAC implicitly selects well-conditioned data.

Conclusion: The framework provides insights into minimal problem conditioning, with practical implications for RANSAC-based pose estimation.

Abstract: In this paper, we introduce a general framework for analyzing the numerical
conditioning of minimal problems in multiple view geometry, using tools from
computational algebra and Riemannian geometry. Special motivation comes from
the fact that relative pose estimation, based on standard 5-point or 7-point
Random Sample Consensus (RANSAC) algorithms, can fail even when no outliers are
present and there is enough data to support a hypothesis. We argue that these
cases arise due to the intrinsic instability of the 5- and 7-point minimal
problems. We apply our framework to characterize the instabilities, both in
terms of the world scenes that lead to infinite condition number, and directly
in terms of ill-conditioned image data. The approach produces computational
tests for assessing the condition number before solving the minimal problem.
Lastly, synthetic and real data experiments suggest that RANSAC serves not only
to remove outliers, but in practice it also selects for well-conditioned image
data, which is consistent with our theory.

</details>


### [243] [Latent Feature-Guided Diffusion Models for Shadow Removal](https://arxiv.org/pdf/2312.02156)
*Kangfu Mei, Luis Figueroa, Zhe Lin, Zhihong Ding, Scott Cohen, Vishal M. Patel*

Main category: cs.CV

TL;DR: The paper proposes using diffusion models to recover textures under shadows, outperforming previous methods by 13-82% in RMSE on datasets.


<details>
  <summary>Details</summary>
Motivation: Recovering textures under shadows is challenging due to the difficulty of inferring shadow-free scenes from shadow images.

Method: The method uses diffusion models conditioned on a learned latent feature space and fuses noise features to avoid local optima during training.

Result: The approach outperforms previous methods by 13% (AISTD) and 82% (DESOBA) in RMSE.

Conclusion: Diffusion models with learned latent features and noise fusion effectively improve shadow removal, setting new benchmarks.

Abstract: Recovering textures under shadows has remained a challenging problem due to
the difficulty of inferring shadow-free scenes from shadow images. In this
paper, we propose the use of diffusion models as they offer a promising
approach to gradually refine the details of shadow regions during the diffusion
process. Our method improves this process by conditioning on a learned latent
feature space that inherits the characteristics of shadow-free images, thus
avoiding the limitation of conventional methods that condition on degraded
images only. Additionally, we propose to alleviate potential local optima
during training by fusing noise features with the diffusion network. We
demonstrate the effectiveness of our approach which outperforms the previous
best method by 13% in terms of RMSE on the AISTD dataset. Further, we explore
instance-level shadow removal, where our model outperforms the previous best
method by 82% in terms of RMSE on the DESOBA dataset.

</details>


### [244] [Leveraging Habitat Information for Fine-grained Bird Identification](https://arxiv.org/pdf/2312.14999)
*Tin Nguyen, Peijie Chen, Anh Totti Nguyen*

Main category: cs.CV

TL;DR: The paper explores integrating habitat information into bird classifiers, improving accuracy for CNNs, ViTs, and CLIP models.


<details>
  <summary>Details</summary>
Motivation: Traditional bird classifiers ignore habitat, a key cue for bird identification. This work aims to leverage habitat data for better performance.

Method: Augments CNNs, ViTs, and CLIP with habitat data via image augmentation and textual descriptors.

Result: Improvements of up to +0.83 (NABirds) and +0.23 (CUB-200) for CNNs/ViTs, and +0.99 (NABirds) and +1.1 (CUB-200) for CLIP.

Conclusion: Integrating habitat features consistently boosts classifier accuracy, validating its importance.

Abstract: Traditional bird classifiers mostly rely on the visual characteristics of
birds. Some prior works even train classifiers to be invariant to the
background, completely discarding the living environment of birds. Instead, we
are the first to explore integrating habitat information, one of the four major
cues for identifying birds by ornithologists, into modern bird classifiers. We
focus on two leading model types: (1) CNNs and ViTs trained on the downstream
bird datasets; and (2) original, multi-modal CLIP. Training CNNs and ViTs with
habitat-augmented data results in an improvement of up to +0.83 and +0.23
points on NABirds and CUB-200, respectively. Similarly, adding habitat
descriptors to the prompts for CLIP yields a substantial accuracy boost of up
to +0.99 and +1.1 points on NABirds and CUB-200, respectively. We find
consistent accuracy improvement after integrating habitat features into the
image augmentation process and into the textual descriptors of vision-language
CLIP classifiers. Code is available at:
https://anonymous.4open.science/r/reasoning-8B7E/.

</details>


### [245] [Towards Complementary Knowledge Distillation for Efficient Dense Image Prediction](https://arxiv.org/pdf/2401.13174)
*Dong Zhang, Pingcheng Dong, Long Chen, Kwang-Ting Cheng*

Main category: cs.CV

TL;DR: The paper proposes a BCD method within the KD framework to improve boundary and context distillation for efficient dense image prediction models, enhancing mask quality and connectivity without extra costs.


<details>
  <summary>Details</summary>
Motivation: Small EDIP models struggle with boundary completeness and target region connectivity despite recognizing main object regions.

Method: BCD method includes boundary distillation (extracting explicit boundaries) and context distillation (transferring implicit pixel-level contexts).

Result: BCD outperforms existing methods in semantic segmentation, object detection, and instance segmentation, improving boundaries and connectivity.

Conclusion: BCD is simple, efficient, and effective for EDIP tasks, requiring no extra supervision or inference costs.

Abstract: It has been revealed that small efficient dense image prediction (EDIP)
models, trained using the knowledge distillation (KD) framework, encounter two
key challenges, including maintaining boundary region completeness and
preserving target region connectivity, despite their favorable capacity to
recognize main object regions. In this work, we propose a complementary
boundary and context distillation (BCD) method within the KD framework for
EDIPs, which facilitates the targeted knowledge transfer from large accurate
teacher models to compact efficient student models. Specifically, the boundary
distillation component focuses on extracting explicit object-level semantic
boundaries from the hierarchical feature maps of the backbone network to
enhance the student model's mask quality in boundary regions. Concurrently, the
context distillation component leverages self-relations as a bridge to transfer
implicit pixel-level contexts from the teacher model to the student model,
ensuring strong connectivity in target regions. Our proposed BCD method is
specifically designed for EDIP tasks and is characterized by its simplicity and
efficiency. Extensive experimental results across semantic segmentation, object
detection, and instance segmentation on various representative datasets
demonstrate that our method can outperform existing methods without requiring
extra supervisions or incurring increased inference costs, resulting in
well-defined object boundaries and smooth connecting regions.

</details>


### [246] [Msmsfnet: a multi-stream and multi-scale fusion net for edge detection](https://arxiv.org/pdf/2404.04856)
*Chenguang Liu, Chisheng Wang, Feifei Dong, Xiayang Xiao, Xin Su, Chuanhua Zhu, Dejin Zhang, Qingquan Li*

Main category: cs.CV

TL;DR: The paper introduces a new network architecture (msmsfnet) for edge detection, trained from scratch, outperforming state-of-the-art methods without relying on pre-trained weights.


<details>
  <summary>Details</summary>
Motivation: Existing edge detection methods depend heavily on pre-trained ImageNet weights, which are costly and not always applicable (e.g., for SAR images). This limits model design and fairness in comparisons.

Method: The authors propose the multi-stream and multi-scale fusion net (msmsfnet) and evaluate its performance when trained from scratch on public datasets.

Result: msmsfnet outperforms state-of-the-art edge detectors in three datasets and works efficiently for SAR images, where pre-trained weights are unavailable. It also achieves competitive performance with pre-trained weights on BSDS500.

Conclusion: Training edge detectors from scratch is viable and effective, especially for domains like SAR images, reducing dependency on pre-trained models.

Abstract: Edge detection is a long-standing problem in computer vision. Despite the
efficiency of existing algorithms, their performance, however, rely heavily on
the pre-trained weights of the backbone network on the ImageNet dataset. The
use of pre-trained weights in previous methods significantly increases the
difficulty to design new models for edge detection without relying on existing
well-trained ImageNet models, as pre-training the model on the ImageNet dataset
is expensive and becomes compulsory to ensure the fairness of comparison.
Besides, the pre-training and fine-tuning strategy is not always useful and
sometimes even inaccessible. For instance, the pre-trained weights on the
ImageNet dataset are unlikely to be helpful for edge detection in Synthetic
Aperture Radar (SAR) images due to strong differences in the statistics between
optical images and SAR images. Moreover, no dataset has comparable size to the
ImageNet dataset for SAR image processing. In this work, we study the
performance achievable by state-of-the-art deep learning based edge detectors
in publicly available datasets when they are trained from scratch, and devise a
new network architecture, the multi-stream and multi-scale fusion net
(msmsfnet), for edge detection. We show in our experiments that by training all
models from scratch, our model outperforms state-of-the-art edge detectors in
three publicly available datasets. We also demonstrate the efficiency of our
model for edge detection in SAR images, where no useful pre-trained weight is
available. Finally, We show that our model is able to achieve competitive
performance on the BSDS500 dataset when the pre-trained weights are used.

</details>


### [247] [A Lightweight UDF Learning Framework for 3D Reconstruction Based on Local Shape Functions](https://arxiv.org/pdf/2407.01330)
*Jiangbei Hu, Yanggeng Li, Fei Hou, Junhui Hou, Zhebin Zhang, Shengfa Wang, Na Lei, Ying He*

Main category: cs.CV

TL;DR: LoSF-UDF is a lightweight neural framework for surface reconstruction from 3D point clouds using local shape functions to learn UDFs, avoiding costly large-scale training.


<details>
  <summary>Details</summary>
Motivation: Traditional UDF learning methods are costly and require re-training for new datasets. LoSF-UDF addresses this by leveraging local shape patterns and a small training dataset.

Method: The approach learns features within a specific radius around query points using an attention mechanism, focusing on key features for UDF estimation.

Result: The method is efficient, robust to noise, and outperforms existing methods without shape-specific training. It also aids other unsupervised approaches.

Conclusion: LoSF-UDF provides a lightweight, efficient, and noise-resilient solution for surface reconstruction, validated across diverse datasets.

Abstract: Unsigned distance fields (UDFs) provide a versatile framework for
representing a diverse array of 3D shapes, encompassing both watertight and
non-watertight geometries. Traditional UDF learning methods typically require
extensive training on large 3D shape datasets, which is costly and necessitates
re-training for new datasets. This paper presents a novel neural framework,
LoSF-UDF, for reconstructing surfaces from 3D point clouds by leveraging local
shape functions to learn UDFs. We observe that 3D shapes manifest simple
patterns in localized regions, prompting us to develop a training dataset of
point cloud patches characterized by mathematical functions that represent a
continuum from smooth surfaces to sharp edges and corners. Our approach learns
features within a specific radius around each query point and utilizes an
attention mechanism to focus on the crucial features for UDF estimation.
Despite being highly lightweight, with only 653 KB of trainable parameters and
a modest-sized training dataset with 0.5 GB storage, our method enables
efficient and robust surface reconstruction from point clouds without requiring
for shape-specific training. Furthermore, our method exhibits enhanced
resilience to noise and outliers in point clouds compared to existing methods.
We conduct comprehensive experiments and comparisons across various datasets,
including synthetic and real-scanned point clouds, to validate our method's
efficacy. Notably, our lightweight framework offers rapid and reliable
initialization for other unsupervised iterative approaches, improving both the
efficiency and accuracy of their reconstructions. Our project and code are
available at https://jbhu67.github.io/LoSF-UDF.github.io.

</details>


### [248] [HPPP: Halpern-type Preconditioned Proximal Point Algorithms and Applications to Image Restoration](https://arxiv.org/pdf/2407.13120)
*Shuchang Zhang, Hui Zhang, Hongxia Wang*

Main category: cs.CV

TL;DR: The paper introduces a Halpern-type Preconditioned Proximal Point (HPPP) algorithm to address weak convergence and lack of acceleration in degenerate PPP methods, and combines it with denoiser priors for image restoration.


<details>
  <summary>Details</summary>
Motivation: The degenerate PPP method has limitations like weak convergence and no accelerated variants, prompting the need for an improved approach.

Method: Proposes HPPP by integrating Halpern's iteration for strong convergence and acceleration, and combines it with denoiser priors (e.g., PnP) for image restoration.

Result: Numerical experiments on toy examples and image restoration confirm the effectiveness of the proposed HPPP and accelerated PnP methods.

Conclusion: The HPPP algorithm successfully overcomes the limitations of degenerate PPP, offering strong convergence and acceleration, with practical applications in image restoration.

Abstract: Recently, the degenerate preconditioned proximal point (PPP) method provides
a unified and flexible framework for designing and analyzing operator-splitting
algorithms such as Douglas-Rachford (DR). However, the degenerate PPP method
exhibits weak convergence in the infinite-dimensional Hilbert space and lacks
accelerated variants. To address these issues, we propose a Halpern-type PPP
(HPPP) algorithm, which leverages the strong convergence and acceleration
properties of Halpern's iteration method. Moreover, we propose a novel
algorithm for image restoration by combining HPPP with denoiser priors such as
Plug-and-Play (PnP) prior, which can be viewed as an accelerated PnP method.
Finally, numerical experiments including several toy examples and image
restoration validate the effectiveness of our proposed algorithms.

</details>


### [249] [BCTR: Bidirectional Conditioning Transformer for Scene Graph Generation](https://arxiv.org/pdf/2407.18715)
*Peng Hao, Weilong Wang, Xiaobing Wang, Yingying Jiang, Hanchao Jia, Shaowei Cui, Junhang Wei, Xiaoshuai Hao*

Main category: cs.CV

TL;DR: A novel bidirectional conditioning factorization method, BCTR, improves Scene Graph Generation by enabling efficient interaction between entities and predicates, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing SGG methods assume unidirectional conditioning, limiting performance due to restricted information interaction.

Method: Proposes BCTR with Bidirectional Conditioning Generator (BCG) for feature augmentation and Random Feature Alignment (RFA) for regularization.

Result: BCTR achieves state-of-the-art performance on Visual Genome and Open Image V6 benchmarks.

Conclusion: Bidirectional conditioning in a semantic-aligned space enhances SGG performance and generalizability.

Abstract: Scene Graph Generation (SGG) remains a challenging task due to its
compositional property. Previous approaches improve prediction efficiency
through end-to-end learning. However, these methods exhibit limited performance
as they assume unidirectional conditioning between entities and predicates,
which restricts effective information interaction. To address this limitation,
we propose a novel bidirectional conditioning factorization in a
semantic-aligned space for SGG, enabling efficient and generalizable
interaction between entities and predicates. Specifically, we introduce an
end-to-end scene graph generation model, the Bidirectional Conditioning
Transformer (BCTR), to implement this factorization. BCTR consists of two key
modules. First, the Bidirectional Conditioning Generator (BCG) performs
multi-stage interactive feature augmentation between entities and predicates,
enabling mutual enhancement between these predictions. Second, Random Feature
Alignment (RFA) is present to regularize feature space by distilling
multi-modal knowledge from pre-trained models. Within this regularized feature
space, BCG is feasible to capture interaction patterns across diverse
relationships during training, and the learned interaction patterns can
generalize to unseen but semantically related relationships during inference.
Extensive experiments on Visual Genome and Open Image V6 show that BCTR
achieves state-of-the-art performance on both benchmarks.

</details>


### [250] [BihoT: A Large-Scale Dataset and Benchmark for Hyperspectral Camouflaged Object Tracking](https://arxiv.org/pdf/2408.12232)
*Hanzheng Wang, Wei Li, Xiang-Gen Xia, Qian Du*

Main category: cs.CV

TL;DR: The paper introduces a new task, hyperspectral camouflaged object tracking (HCOT), and a dataset (BihoT) to address biases in existing hyperspectral tracking datasets. A baseline model (SPDAN) is proposed, leveraging spectral features for improved tracking performance.


<details>
  <summary>Details</summary>
Motivation: Existing hyperspectral object tracking (HOT) datasets are biased toward visual appearances, neglecting spectral features. This limits performance in camouflaged scenarios where spectral information is crucial.

Method: The paper proposes the SPDAN model, which includes a spectral embedding network (SEN), a spectral prompt-based backbone network (SPBN), and a distractor-aware module (DAM). SEN extracts spectral-spatial features, SPBN fine-tunes RGB trackers with spectral prompts, and DAM handles occlusion-related distractors.

Result: SPDAN achieves state-of-the-art performance on the new BihoT dataset and other HOT datasets, demonstrating its effectiveness in leveraging spectral features.

Conclusion: The HCOT task and BihoT dataset address biases in HOT, and the SPDAN model provides a robust baseline for spectral-based tracking in challenging camouflaged scenarios.

Abstract: Hyperspectral object tracking (HOT) has exhibited potential in various
applications, particularly in scenes where objects are camouflaged. Existing
trackers can effectively retrieve objects via band regrouping because of the
bias in existing HOT datasets, where most objects tend to have distinguishing
visual appearances rather than spectral characteristics. This bias allows the
tracker to directly use the visual features obtained from the false-color
images generated by hyperspectral images without the need to extract spectral
features. To tackle this bias, we find that the tracker should focus on the
spectral information when object appearance is unreliable. Thus, we provide a
new task called hyperspectral camouflaged object tracking (HCOT) and
meticulously construct a large-scale HCOT dataset, termed BihoT, which consists
of 41,912 hyperspectral images covering 49 video sequences. The dataset covers
various artificial camouflage scenes where objects have similar appearances,
diverse spectrums, and frequent occlusion, making it a very challenging dataset
for HCOT. Besides, a simple but effective baseline model, named spectral
prompt-based distractor-aware network (SPDAN), is proposed, comprising a
spectral embedding network (SEN), a spectral prompt-based backbone network
(SPBN), and a distractor-aware module (DAM). Specifically, the SEN extracts
spectral-spatial features via 3-D and 2-D convolutions. Then, the SPBN
fine-tunes powerful RGB trackers with spectral prompts and alleviates the
insufficiency of training samples. Moreover, the DAM utilizes a novel statistic
to capture the distractor caused by occlusion from objects and background.
Extensive experiments demonstrate that our proposed SPDAN achieves
state-of-the-art performance on the proposed BihoT and other HOT datasets.

</details>


### [251] [Camouflaged Object Tracking: A Benchmark](https://arxiv.org/pdf/2408.13877)
*Xiaoyu Guo, Pengzhi Zhong, Hao Zhang, Defeng Huang, Huikai Shao, Qijun Zhao, Shuiwang Li*

Main category: cs.CV

TL;DR: The paper introduces the Camouflaged Object Tracking Dataset (COTD) and a novel tracking framework, HiPTrack-MLS, to address the challenge of tracking camouflaged objects, where existing methods perform poorly.


<details>
  <summary>Details</summary>
Motivation: Current tracking algorithms struggle with camouflaged objects, which are critical in applications like military, security, and agriculture.

Method: The authors create COTD, a dataset with 200 sequences and 80,000 annotated frames, and propose HiPTrack-MLS, a new tracking framework.

Result: Evaluation of 20 existing trackers shows poor performance on camouflaged objects, while HiPTrack-MLS demonstrates improved results.

Conclusion: The COTD dataset and HiPTrack-MLS framework address a critical gap in tracking camouflaged objects, with promising performance.

Abstract: Visual tracking has seen remarkable advancements, largely driven by the
availability of large-scale training datasets that have enabled the development
of highly accurate and robust algorithms. While significant progress has been
made in tracking general objects, research on more challenging scenarios, such
as tracking camouflaged objects, remains limited. Camouflaged objects, which
blend seamlessly with their surroundings or other objects, present unique
challenges for detection and tracking in complex environments. This challenge
is particularly critical in applications such as military, security,
agriculture, and marine monitoring, where precise tracking of camouflaged
objects is essential. To address this gap, we introduce the Camouflaged Object
Tracking Dataset (COTD), a specialized benchmark designed specifically for
evaluating camouflaged object tracking methods. The COTD dataset comprises 200
sequences and approximately 80,000 frames, each annotated with detailed
bounding boxes. Our evaluation of 20 existing tracking algorithms reveals
significant deficiencies in their performance with camouflaged objects. To
address these issues, we propose a novel tracking framework, HiPTrack-MLS,
which demonstrates promising results in improving tracking performance for
camouflaged objects. COTD and code are avialable at
https://github.com/openat25/HIPTrack-MLS.

</details>


### [252] [A novel fusion of Sentinel-1 and Sentinel-2 with climate data for crop phenology estimation using Machine Learning](https://arxiv.org/pdf/2409.00020)
*Shahab Aldin Shojaeezadeh, Abdelrazek Elnashar, Tobias Karl David Weber*

Main category: cs.CV

TL;DR: The paper presents a Machine Learning (LightGBM) model to predict 13 phenological stages for eight major crops in Germany using Sentinel-1, Sentinel-2, and high-resolution climate data, achieving reasonable accuracy (R2 > 0.43) and low error (6 days MAE).


<details>
  <summary>Details</summary>
Motivation: Accurate crop phenology detection is crucial for agricultural management, but existing methods lack integration of Sentinel data and climate data for crop modelers.

Method: A LightGBM model was trained with feature selection to combine RS and climate data, validated using observed phenology data from 2017-2021.

Result: The model achieved R2 > 0.43 and 6 days MAE, showing transferability across Germany. Combining radar and climate data improved performance.

Conclusion: The framework benefits crop model calibration, agricultural decisions, and sustainable food production, addressing global food demand.

Abstract: Crop phenology describes the physiological development stages of crops from
planting to harvest which is valuable information for decision makers to plan
and adapt agricultural management strategies. In the era of big Earth
observation data ubiquity, attempts have been made to accurately detect crop
phenology using Remote Sensing (RS) and high resolution weather data. However,
most studies have focused on large scale predictions of phenology or developed
methods which are not adequate to help crop modeler communities on leveraging
Sentinel-1 and Sentinal-2 data and fusing them with high resolution climate
data, using a novel framework. For this, we trained a Machine Learning (ML)
LightGBM model to predict 13 phenological stages for eight major crops across
Germany at 20 m scale. Observed phonologies were taken from German national
phenology network (German Meteorological Service; DWD) between 2017 and 2021.
We proposed a thorough feature selection analysis to find the best combination
of RS and climate data to detect phenological stages. At national scale,
predicted phenology resulted in a reasonable precision of R2 > 0.43 and a low
Mean Absolute Error of 6 days, averaged over all phenological stages and crops.
The spatio-temporal analysis of the model predictions demonstrates its
transferability across different spatial and temporal context of Germany. The
results indicated that combining radar sensors with climate data yields a very
promising performance for a multitude of practical applications. Moreover,
these improvements are expected to be useful to generate highly valuable input
for crop model calibrations and evaluations, facilitate informed agricultural
decisions, and contribute to sustainable food production to address the
increasing global food demand.

</details>


### [253] [IGEV++: Iterative Multi-range Geometry Encoding Volumes for Stereo Matching](https://arxiv.org/pdf/2409.00638)
*Gangwei Xu, Xianqi Wang, Zhaoxing Zhang, Junda Cheng, Chunyuan Liao, Xin Yang*

Main category: cs.CV

TL;DR: IGEV++ is a new deep network for stereo matching, addressing ambiguities in ill-posed regions and large disparities using Multi-range Geometry Encoding Volumes (MGEV). It achieves top performance on benchmarks like Scene Flow, Middlebury, and KITTI.


<details>
  <summary>Details</summary>
Motivation: Handling matching ambiguities in ill-posed regions and large disparities remains a challenge in stereo matching.

Method: IGEV++ constructs MGEV for multi-range geometry encoding, uses adaptive patch matching, selective geometry feature fusion, and ConvGRUs for iterative disparity updates.

Result: Achieves best performance on Scene Flow (up to 768px) and state-of-the-art on Middlebury, ETH3D, KITTI 2012/2015, with a 3.23% Bad 2.0 rate on Middlebury.

Conclusion: IGEV++ efficiently handles large disparities and ill-posed regions, outperforming existing methods and offering a real-time variant.

Abstract: Stereo matching is a core component in many computer vision and robotics
systems. Despite significant advances over the last decade, handling matching
ambiguities in ill-posed regions and large disparities remains an open
challenge. In this paper, we propose a new deep network architecture, called
IGEV++, for stereo matching. The proposed IGEV++ constructs Multi-range
Geometry Encoding Volumes (MGEV), which encode coarse-grained geometry
information for ill-posed regions and large disparities, while preserving
fine-grained geometry information for details and small disparities. To
construct MGEV, we introduce an adaptive patch matching module that efficiently
and effectively computes matching costs for large disparity ranges and/or
ill-posed regions. We further propose a selective geometry feature fusion
module to adaptively fuse multi-range and multi-granularity geometry features
in MGEV. Then, we input the fused geometry features into ConvGRUs to
iteratively update the disparity map. MGEV allows to efficiently handle large
disparities and ill-posed regions, such as occlusions and textureless regions,
and enjoys rapid convergence during iterations. Our IGEV++ achieves the best
performance on the Scene Flow test set across all disparity ranges, up to
768px. Our IGEV++ also achieves state-of-the-art accuracy on the Middlebury,
ETH3D, KITTI 2012, and 2015 benchmarks. Specifically, IGEV++ achieves a 3.23\%
2-pixel outlier rate (Bad 2.0) on the large disparity benchmark, Middlebury,
representing error reductions of 31.9\% and 54.8\% compared to RAFT-Stereo and
GMStereo, respectively. We also present a real-time version of IGEV++ that
achieves the best performance among all published real-time methods on the
KITTI benchmarks. The code is publicly available at
https://github.com/gangweix/IGEV and https://github.com/gangweix/IGEV-plusplus.

</details>


### [254] [Enhanced Generative Data Augmentation for Semantic Segmentation via Stronger Guidance](https://arxiv.org/pdf/2409.06002)
*Quang-Huy Che, Duc-Tri Le, Bich-Nga Pham, Duc-Khai Lam, Vinh-Tiep Nguyen*

Main category: cs.CV

TL;DR: The paper introduces a data augmentation pipeline for semantic segmentation using a Controllable Diffusion model, addressing limitations of traditional methods and generative models by improving prompt generation and visual references.


<details>
  <summary>Details</summary>
Motivation: Traditional data augmentation methods lack diversity in semantic dimensions, and generative models struggle with accurate content reflection. This work aims to enhance synthetic image generation for semantic segmentation.

Method: The proposed method uses Class-Prompt Appending and Visual Prior Blending for efficient prompt generation and attention enhancement. A class balancing algorithm ensures dataset balance.

Result: Evaluated on PASCAL VOC datasets, the pipeline effectively generates high-quality synthetic images for semantic segmentation.

Conclusion: The method improves data augmentation for semantic segmentation by preserving segmentation structure and balancing the dataset, demonstrating effectiveness in synthetic image generation.

Abstract: Data augmentation is crucial for pixel-wise annotation tasks like semantic
segmentation, where labeling requires significant effort and intensive labor.
Traditional methods, involving simple transformations such as rotations and
flips, create new images but often lack diversity along key semantic dimensions
and fail to alter high-level semantic properties. To address this issue,
generative models have emerged as an effective solution for augmenting data by
generating synthetic images. Controllable Generative models offer data
augmentation methods for semantic segmentation tasks by using prompts and
visual references from the original image. However, these models face
challenges in generating synthetic images that accurately reflect the content
and structure of the original image due to difficulties in creating effective
prompts and visual references. In this work, we introduce an effective data
augmentation pipeline for semantic segmentation using Controllable Diffusion
model. Our proposed method includes efficient prompt generation using
\textit{Class-Prompt Appending} and \textit{Visual Prior Blending} to enhance
attention to labeled classes in real images, allowing the pipeline to generate
a precise number of augmented images while preserving the structure of
segmentation-labeled classes. In addition, we implement a \textit{class
balancing algorithm} to ensure a balanced training dataset when merging the
synthetic and original images. Evaluation on PASCAL VOC datasets, our pipeline
demonstrates its effectiveness in generating high-quality synthetic images for
semantic segmentation. Our code is available at
\href{https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance}{this
https URL}.

</details>


### [255] [On Synthetic Texture Datasets: Challenges, Creation, and Curation](https://arxiv.org/pdf/2409.10297)
*Blaine Hoak, Patrick McDaniel*

Main category: cs.CV

TL;DR: The paper introduces a method to generate high-quality, diverse texture images using text-to-image models, addressing the lack of large texture datasets. It also highlights biases in NSFW filters when applied to textures.


<details>
  <summary>Details</summary>
Motivation: Existing texture datasets are limited in size and diversity, hindering comprehensive evaluations in texture-related ML tasks. Generative models offer a solution but pose challenges in texture synthesis and validation.

Method: The pipeline involves (1) creating prompts from descriptors, (2) using Stable Diffusion for image generation and filtering, and (3) further quality filtering to produce the Prompted Textures Dataset (PTD) with 246,285 images across 56 textures.

Result: The PTD is high-quality and diverse, validated by metrics and human evaluation. NSFW filters were found overly sensitive to textures, flagging 60% of images, revealing model biases.

Conclusion: The work provides a scalable solution for texture data generation, uncovering biases in safety filters and enabling broader texture-based ML research.

Abstract: The influence of textures on machine learning models has been an ongoing
investigation, specifically in texture bias/learning, interpretability, and
robustness. However, due to the lack of large and diverse texture data
available, the findings in these works have been limited, as more comprehensive
evaluations have not been feasible. Image generative models are able to provide
data creation at scale, but utilizing these models for texture synthesis has
been unexplored and poses additional challenges both in creating accurate
texture images and validating those images. In this work, we introduce an
extensible methodology and corresponding new dataset for generating
high-quality, diverse texture images capable of supporting a broad set of
texture-based tasks. Our pipeline consists of: (1) developing prompts from a
range of descriptors to serve as input to text-to-image models, (2) adopting
and adapting Stable Diffusion pipelines to generate and filter the
corresponding images, and (3) further filtering down to the highest quality
images. Through this, we create the Prompted Textures Dataset (PTD), a dataset
of 246,285 texture images that span 56 textures. During the process of
generating images, we find that NSFW safety filters in image generation
pipelines are highly sensitive to texture (and flag up to 60\% of our texture
images), uncovering a potential bias in these models and presenting unique
challenges when working with texture data. Through both standard metrics and a
human evaluation, we find that our dataset is high quality and diverse. Our
dataset is available for download at https://zenodo.org/records/15359142.

</details>


### [256] [When SAM2 Meets Video Camouflaged Object Segmentation: A Comprehensive Evaluation and Adaptation](https://arxiv.org/pdf/2409.18653)
*Yuli Zhou, Guolei Sun, Yawei Li, Guo-Sen Xie, Luca Benini, Ender Konukoglu*

Main category: cs.CV

TL;DR: The study evaluates SAM2's performance in video camouflaged object segmentation (VCOS), showing its strong zero-shot ability and potential for improvement through fine-tuning and integration with MLLMs.


<details>
  <summary>Details</summary>
Motivation: VCOS is challenging due to objects blending into surroundings, and SAM2's effectiveness in dynamic camouflaged scenarios is under-explored.

Method: Assessed SAM2 on camouflaged video datasets with various prompts, integrated it with MLLMs and VCOS methods, and fine-tuned SAM2 for VCOS.

Result: SAM2 demonstrates excellent zero-shot ability for VCOS, with further improvements possible through parameter adjustments.

Conclusion: SAM2 is highly effective for VCOS, and its performance can be enhanced via fine-tuning and multimodal integration.

Abstract: This study investigates the application and performance of the Segment
Anything Model 2 (SAM2) in the challenging task of video camouflaged object
segmentation (VCOS). VCOS involves detecting objects that blend seamlessly in
the surroundings for videos, due to similar colors and textures, poor light
conditions, etc. Compared to the objects in normal scenes, camouflaged objects
are much more difficult to detect. SAM2, a video foundation model, has shown
potential in various tasks. But its effectiveness in dynamic camouflaged
scenarios remains under-explored. This study presents a comprehensive study on
SAM2's ability in VCOS. First, we assess SAM2's performance on camouflaged
video datasets using different models and prompts (click, box, and mask).
Second, we explore the integration of SAM2 with existing multimodal large
language models (MLLMs) and VCOS methods. Third, we specifically adapt SAM2 by
fine-tuning it on the video camouflaged dataset. Our comprehensive experiments
demonstrate that SAM2 has excellent zero-shot ability of detecting camouflaged
objects in videos. We also show that this ability could be further improved by
specifically adjusting SAM2's parameters for VCOS. The code is available at
https://github.com/zhoustan/SAM2-VCOS

</details>


### [257] [GTransPDM: A Graph-embedded Transformer with Positional Decoupling for Pedestrian Crossing Intention Prediction](https://arxiv.org/pdf/2409.20223)
*Chen Xie, Ciyun Lin, Xiaoyu Zheng, Bowen Gong, Antonio M. López*

Main category: cs.CV

TL;DR: GTransPDM, a Graph-embedded Transformer with Position Decoupling Module, improves pedestrian crossing intention prediction using multi-modal features, achieving high accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Pedestrian crossing intention prediction is vital for autonomous vehicle safety, but current methods face challenges like distorted positions and inefficient preprocessing.

Method: Proposes a positional decoupling module for motion decomposition and depth encoding, and a graph-embedded Transformer for spatio-temporal dynamics of pose skeletons.

Result: Achieves 92% accuracy on PIE and 87% on JAAD datasets, with 0.05ms processing speed, outperforming state-of-the-art methods.

Conclusion: GTransPDM effectively addresses preprocessing and accuracy issues, enhancing pedestrian intention prediction for autonomous driving.

Abstract: Understanding and predicting pedestrian crossing behavioral intention is
crucial for the driving safety of autonomous vehicles. Nonetheless, challenges
emerge when using promising images or environmental context masks to extract
various factors for time-series network modeling, causing pre-processing errors
or a loss of efficiency. Typically, pedestrian positions captured by onboard
cameras are often distorted and do not accurately reflect their actual
movements. To address these issues, GTransPDM -- a Graph-embedded Transformer
with a Position Decoupling Module -- was developed for pedestrian crossing
intention prediction by leveraging multi-modal features. First, a positional
decoupling module was proposed to decompose pedestrian lateral motion and
encode depth cues in the image view. Then, a graph-embedded Transformer was
designed to capture the spatio-temporal dynamics of human pose skeletons,
integrating essential factors such as position, skeleton, and ego-vehicle
motion. Experimental results indicate that the proposed method achieves 92%
accuracy on the PIE dataset and 87% accuracy on the JAAD dataset, with a
processing speed of 0.05ms. It outperforms the state-of-the-art in comparison.

</details>


### [258] [Scaling Large Motion Models with Million-Level Human Motions](https://arxiv.org/pdf/2410.03311)
*Ye Wang, Sipeng Zheng, Bin Cao, Qianshan Wei, Weishuai Zeng, Qin Jin, Zongqing Lu*

Main category: cs.CV

TL;DR: MotionLib introduces a million-level dataset for motion generation, enabling the training of a large motion model (Being-M0) that performs robustly across diverse activities. The work emphasizes scaling data and model size, and proposes Motionbook for better motion encoding.


<details>
  <summary>Details</summary>
Motivation: The lack of massive high-quality data hinders the development of generalist models in human motion understanding. MotionLib addresses this gap.

Method: MotionLib provides a large dataset with hierarchical text descriptions. Being-M0 is trained on this data. Motionbook introduces a compact motion feature and a 2D lookup-free tokenizer.

Result: Being-M0 demonstrates robust performance across diverse activities, including unseen ones. Motionbook enhances motion token representation.

Conclusion: This work advances motion generation by scaling data and model size, and introduces innovative encoding techniques, laying groundwork for future models.

Abstract: Inspired by the recent success of LLMs, the field of human motion
understanding has increasingly shifted toward developing large motion models.
Despite some progress, current efforts remain far from achieving truly
generalist models, primarily due to the lack of massive high-quality data. To
address this gap, we present MotionLib, the first million-level dataset for
motion generation, which is at least 15$\times$ larger than existing
counterparts and enriched with hierarchical text descriptions. Using MotionLib,
we train a large motion model named Being-M0, demonstrating robust performance
across a wide range of human activities, including unseen ones. Through
systematic investigation, for the first time, we highlight the importance of
scaling both data and model size for advancing motion generation, along with
key insights to achieve this goal. To better integrate the motion modality, we
propose Motionbook, an innovative motion encoding approach including (1) a
compact yet lossless feature to represent motions; (2) a novel 2D lookup-free
motion tokenizer that preserves fine-grained motion details while expanding
codebook capacity, significantly enhancing the representational power of motion
tokens. We believe this work lays the groundwork for developing more versatile
and powerful motion generation models in the future. For further details, visit
https://github.com/BeingBeyond/Being-M0.

</details>


### [259] [Rotating-star Pattern for Camera Calibration](https://arxiv.org/pdf/2410.13371)
*Zezhun Shi*

Main category: cs.CV

TL;DR: A novel camera calibration method using rotated checkerboard patterns improves accuracy and stability over star-shaped patterns.


<details>
  <summary>Details</summary>
Motivation: Addressing the aliasing artifacts and accuracy issues in traditional star-shaped calibration patterns.

Method: Employing a series of checkerboard patterns rotated around a central point and proposing a tailored feature extraction algorithm.

Result: Improved accuracy and high stability across varying exposure levels compared to star-shaped patterns.

Conclusion: The rotated checkerboard pattern is a superior alternative for camera calibration, offering better performance.

Abstract: Camera calibration is fundamental to 3D vision, and the choice of calibration
pattern greatly affects the accuracy. To address aberration issue, star-shaped
pattern has been proposed as alternatives to traditional checkerboard. However,
such pattern suffers from aliasing artifacts. In this paper, we present a novel
solution by employing a series of checkerboard patterns rotated around a
central point instead of a single star-shaped pattern. We further propose a
complete feature extraction algorithm tailored for this design. Experimental
results demonstrate that our approach offers improved accuracy over the
conventional star-shaped pattern and achieves high stability across varying
exposure levels.

</details>


### [260] [DiffGAN: A Test Generation Approach for Differential Testing of Deep Neural Networks](https://arxiv.org/pdf/2410.19794)
*Zohreh Aghababaeyan, Manel Abdellatif, Lionel Briand, Ramesh S*

Main category: cs.CV

TL;DR: DiffGAN is a black-box approach using GANs and genetic algorithms to generate diverse test inputs for differential testing of DNNs, outperforming baselines in revealing behavioral discrepancies.


<details>
  <summary>Details</summary>
Motivation: Ensuring DNN reliability is challenging, and traditional accuracy-based evaluations fail to capture behavioral differences between models, especially with limited test data.

Method: DiffGAN combines GANs and the Non-dominated Sorting Genetic Algorithm II with custom fitness functions for diversity and divergence to generate triggering inputs.

Result: DiffGAN outperforms baselines, generating four times more triggering inputs with greater diversity and validity, and improves model selection accuracy.

Conclusion: DiffGAN is an effective black-box solution for differential testing, enhancing model reliability and selection in practical applications.

Abstract: Deep Neural Networks (DNNs) are increasingly deployed across applications.
However, ensuring their reliability remains a challenge, and in many
situations, alternative models with similar functionality and accuracy are
available. Traditional accuracy-based evaluations often fail to capture
behavioral differences between models, especially with limited test datasets,
making it difficult to select or combine models effectively. Differential
testing addresses this by generating test inputs that expose discrepancies in
DNN model behavior. However, existing approaches face significant limitations:
many rely on model internals or are constrained by available seed inputs. To
address these challenges, we propose DiffGAN, a black-box test image generation
approach for differential testing of DNN models. DiffGAN leverages a Generative
Adversarial Network (GAN) and the Non-dominated Sorting Genetic Algorithm II to
generate diverse and valid triggering inputs that reveal behavioral
discrepancies between models. DiffGAN employs two custom fitness functions,
focusing on diversity and divergence, to guide the exploration of the GAN input
space and identify discrepancies between models' outputs. By strategically
searching this space, DiffGAN generates inputs with specific features that
trigger differences in model behavior. DiffGAN is black-box, making it
applicable in more situations. We evaluate DiffGAN on eight DNN model pairs
trained on widely used image datasets. Our results show DiffGAN significantly
outperforms a SOTA baseline, generating four times more triggering inputs, with
greater diversity and validity, within the same budget. Additionally, the
generated inputs improve the accuracy of a machine learning-based model
selection mechanism, which selects the best-performing model based on input
characteristics and can serve as a smart output voting mechanism when using
alternative models.

</details>


### [261] [Veri-Car: Towards Open-world Vehicle Information Retrieval](https://arxiv.org/pdf/2411.06864)
*Andrés Muñoz, Nancy Thomas, Annita Vapsi, Daniel Borrajo*

Main category: cs.CV

TL;DR: Veri-Car is an information retrieval system using supervised learning to extract vehicle details from images, handling open-world challenges with high accuracy.


<details>
  <summary>Details</summary>
Motivation: The need to extract vehicle characteristics from images is complex due to noise, many classes, and new models.

Method: Uses supervised learning, pre-trained models, hierarchical multi-similarity loss, and integrates license plate detection with OCR.

Result: Achieves high precision and accuracy in classifying seen and unseen data, with impressive license plate extraction.

Conclusion: Veri-Car effectively addresses vehicle identification challenges, including open-world problems.

Abstract: Many industrial and service sectors require tools to extract vehicle
characteristics from images. This is a complex task not only by the variety of
noise, and large number of classes, but also by the constant introduction of
new vehicle models to the market. In this paper, we present Veri-Car, an
information retrieval integrated approach designed to help on this task. It
leverages supervised learning techniques to accurately identify the make, type,
model, year, color, and license plate of cars. The approach also addresses the
challenge of handling open-world problems, where new car models and variations
frequently emerge, by employing a sophisticated combination of pre-trained
models, and a hierarchical multi-similarity loss. Veri-Car demonstrates robust
performance, achieving high precision and accuracy in classifying both seen and
unseen data. Additionally, it integrates an ensemble license plate detection,
and an OCR model to extract license plate numbers with impressive accuracy.

</details>


### [262] [Efficient 3D Perception on Multi-Sweep Point Cloud with Gumbel Spatial Pruning](https://arxiv.org/pdf/2411.07742)
*Tianyu Sun, Jianhao Li, Xueqian Zhang, Zhongdao Wang, Bailan Feng, Hengshuang Zhao*

Main category: cs.CV

TL;DR: The paper introduces Gumbel Spatial Pruning (GSP) to dynamically prune redundant points in accumulated point clouds, enabling the use of more sweeps (up to 40) without extra computation, significantly improving perception accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with distant or occluded objects in sparse outdoor point clouds. Accumulating sweeps helps but increases computation.

Method: Proposes GSP, a learnable pruning layer to remove redundant points dynamically, decoupled from other network components.

Result: Enables using up to 40 sweeps (vs. 10) without extra cost, improving performance in tasks like 3D object detection and BEV segmentation.

Conclusion: GSP effectively balances computation and accuracy, enhancing perception in sparse outdoor point clouds.

Abstract: This paper studies point cloud perception within outdoor environments.
Existing methods face limitations in recognizing objects located at a distance
or occluded, due to the sparse nature of outdoor point clouds. In this work, we
observe a significant mitigation of this problem by accumulating multiple
temporally consecutive point cloud sweeps, resulting in a remarkable
improvement in perception accuracy. However, the computation cost also
increases, hindering previous approaches from utilizing a large number of point
cloud sweeps. To tackle this challenge, we find that a considerable portion of
points in the accumulated point cloud is redundant, and discarding these points
has minimal impact on perception accuracy. We introduce a simple yet effective
Gumbel Spatial Pruning (GSP) layer that dynamically prunes points based on a
learned end-to-end sampling. The GSP layer is decoupled from other network
components and thus can be seamlessly integrated into existing point cloud
network architectures. Without incurring additional computational overhead, we
increase the number of point cloud sweeps from 10, a common practice, to as
many as 40. Consequently, there is a significant enhancement in perception
performance. For instance, in nuScenes 3D object detection and BEV map
segmentation tasks, our pruning strategy improves several 3D perception
baseline methods.

</details>


### [263] [CCi-YOLOv8n: Enhanced Fire Detection with CARAFE and Context-Guided Modules](https://arxiv.org/pdf/2411.11011)
*Kunwei Lv, Ruobing Wu, Suyang Chen, Ping Lan*

Main category: cs.CV

TL;DR: CCi-YOLOv8n improves YOLOv8 for fire/smoke detection by integrating CARAFE and context-guided modules, validated on the Web-Fire dataset.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for better fire detection technologies due to serious threats from urban and forest fires.

Method: Enhanced YOLOv8 model with CARAFE up-sampling, context-guided module, and inverted residual mobile block for small target detection.

Result: Outperforms YOLOv8n in precision, proving effectiveness for fire detection.

Conclusion: CCi-YOLOv8n is a robust solution for detecting small fires and smoke.

Abstract: Fire incidents in urban and forested areas pose serious threats,underscoring
the need for more effective detection technologies. To address these
challenges, we present CCi-YOLOv8n, an enhanced YOLOv8 model with targeted
improvements for detecting small fires and smoke. The model integrates the
CARAFE up-sampling operator and a context-guided module to reduce information
loss during up-sampling and down-sampling, thereby retaining richer feature
representations. Additionally, an inverted residual mobile block enhanced C2f
module captures small targets and fine smoke patterns, a critical improvement
over the original model's detection capacity.For validation, we introduce
Web-Fire, a dataset curated for fire and smoke detection across diverse
real-world scenarios. Experimental results indicate that CCi-YOLOv8n
outperforms YOLOv8n in detection precision, confirming its effectiveness for
robust fire detection tasks.

</details>


### [264] [Transmission Line Defect Detection Based on UAV Patrol Images and Vision-language Pretraining](https://arxiv.org/pdf/2411.11370)
*Ke Zhang, Zhaoye Zheng, Yurong Guo, Jiacun Wang, Jiyuan Yang, Yangjie Xiao*

Main category: cs.CV

TL;DR: A novel method using vision-language pretraining (VLP-TL) and progressive transfer strategy (PTS) improves defect detection in UAV patrol images by leveraging multimodal information.


<details>
  <summary>Details</summary>
Motivation: UAV patrol images often lack sufficient defect-related visual information due to imaging distance and angles, reducing detection accuracy.

Method: Proposes VLP-TL with tailored pretraining tasks and PTS to transfer knowledge from pretraining to defect detection, enhancing visual information.

Result: Experimental results show significant accuracy improvement in defect detection by utilizing multimodal data.

Conclusion: The method effectively addresses insufficient visual information in UAV images, enhancing defect detection performance.

Abstract: Unmanned aerial vehicle (UAV) patrol inspection has emerged as a predominant
approach in transmission line monitoring owing to its cost-effectiveness.
Detecting defects in transmission lines is a critical task during UAV patrol
inspection. However, due to imaging distance and shooting angles, UAV patrol
images often suffer from insufficient defect-related visual information, which
has an adverse effect on detection accuracy. In this article, we propose a
novel method for detecting defects in UAV patrol images, which is based on
vision-language pretraining for transmission line (VLP-TL) and a progressive
transfer strategy (PTS). Specifically, VLP-TL contains two novel pretraining
tasks tailored for the transmission line scenario, aimimg at pretraining an
image encoder with abundant knowledge acquired from both visual and linguistic
information. Transferring the pretrained image encoder to the defect detector
as its backbone can effectively alleviate the insufficient visual information
problem. In addition, the PTS further improves transfer performance by
progressively bridging the gap between pretraining and downstream defection
detection. Experimental results demonstrate that the proposed method
significantly improves defect detection accuracy by jointly utilizing
multimodal information, overcoming the limitations of insufficient
defect-related visual information provided by UAV patrol images.

</details>


### [265] [GeoGround: A Unified Large Vision-Language Model for Remote Sensing Visual Grounding](https://arxiv.org/pdf/2411.11904)
*Yue Zhou, Mengcheng Lan, Xiang Li, Litong Feng, Yiping Ke, Xue Jiang, Qingyun Li, Xue Yang, Wayne Zhang*

Main category: cs.CV

TL;DR: GeoGround is a unified framework for RS visual grounding, supporting HBB, OBB, and mask tasks with flexible output selection, outperforming specialized methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods are task-specific and lack generalization, while VLMs struggle with dense tasks like segmentation. GeoGround aims to unify and enhance RS visual grounding.

Method: Uses Text-Mask technique for pixel-level grounding, with prompt-assisted and geometry-guided learning for signal consistency.

Result: GeoGround matches specialized methods on benchmarks, showing strong performance across four RS visual grounding tasks.

Conclusion: GeoGround effectively unifies diverse RS visual grounding tasks, offering flexibility and high performance.

Abstract: Remote sensing (RS) visual grounding aims to use natural language expression
to locate specific objects (in the form of the bounding box or segmentation
mask) in RS images, enhancing human interaction with intelligent RS
interpretation systems. Early research in this area was primarily based on
horizontal bounding boxes (HBBs), but as more diverse RS datasets have become
available, tasks involving oriented bounding boxes (OBBs) and segmentation
masks have emerged. In practical applications, different targets require
different grounding types: HBB can localize an object's position, OBB provides
its orientation, and mask depicts its shape. However, existing specialized
methods are typically tailored to a single type of RS visual grounding task and
are hard to generalize across tasks. In contrast, large vision-language models
(VLMs) exhibit powerful multi-task learning capabilities but struggle to handle
dense prediction tasks like segmentation. This paper proposes GeoGround, a
novel framework that unifies support for HBB, OBB, and mask RS visual grounding
tasks, allowing flexible output selection. Rather than customizing the
architecture of VLM, our work aims to elegantly support pixel-level visual
grounding output through the Text-Mask technique. We define prompt-assisted and
geometry-guided learning to enhance consistency across different signals.
Experimental results show that GeoGround demonstrates strong performance across
four RS visual grounding tasks, matching the performance of specialized methods
on multiple benchmarks. Code available at https://github.com/zytx121/GeoGround

</details>


### [266] [dc-GAN: Dual-Conditioned GAN for Face Demorphing From a Single Morph](https://arxiv.org/pdf/2411.14494)
*Nitish Shukla, Arun Ross*

Main category: cs.CV

TL;DR: The paper proposes dc-GAN, a GAN-based method for high-quality face demorphing, overcoming limitations of existing techniques.


<details>
  <summary>Details</summary>
Motivation: Existing demorphing methods are restrictive or produce poor outputs, limiting their practical use.

Method: The authors introduce dc-GAN, a GAN-based demorphing method conditioned on morph images, ensuring high-quality reconstructions.

Result: Experiments on AMSL, FRLL-Morphs, and MorDiff datasets demonstrate the method's efficacy and generalizability.

Conclusion: dc-GAN effectively addresses demorphing challenges, producing superior results and generalizing across paradigms.

Abstract: A facial morph is an image created by combining two face images pertaining to
two distinct identities. Face demorphing inverts the process and tries to
recover the original images constituting a facial morph. While morph attack
detection (MAD) techniques can be used to flag morph images, they do not
divulge any visual information about the faces used to create them. Demorphing
helps address this problem. Existing demorphing techniques are either very
restrictive (assume identities during testing) or produce feeble outputs (both
outputs look very similar). In this paper, we overcome these issues by
proposing dc-GAN, a novel GAN-based demorphing method conditioned on the morph
images. Our method overcomes morph-replication and produces high quality
reconstructions of the bonafide images used to create the morphs. Moreover, our
method is highly generalizable across demorphing paradigms
(differential/reference-free). We conduct experiments on AMSL, FRLL-Morphs and
MorDiff datasets to showcase the efficacy of our method.

</details>


### [267] [Opt-In Art: Learning Art Styles Only from Few Examples](https://arxiv.org/pdf/2412.00176)
*Hui Ren, Joanna Materzynska, Rohit Gandikota, David Bau, Antonio Torralba*

Main category: cs.CV

TL;DR: A study shows that pre-training on paintings isn't necessary for artistic style adaptation; a model trained on photos can adapt to artistic styles with few examples, matching state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To investigate if pre-training on artistic datasets is essential for learning artistic styles, or if adaptation from non-artistic data is sufficient.

Method: Train a text-to-image model on photographs only, then adapt it to artistic styles using few examples. Evaluate with user studies and automatic metrics.

Result: The adapted model performs comparably to models pre-trained on large artistic datasets. Data attribution reveals artistic outputs can emerge without prior artistic exposure.

Conclusion: Artistic style generation can be achieved without pre-training on artistic data, using minimal, carefully selected examples.

Abstract: We explore whether pre-training on datasets with paintings is necessary for a
model to learn an artistic style with only a few examples. To investigate this,
we train a text-to-image model exclusively on photographs, without access to
any painting-related content. We show that it is possible to adapt a model that
is trained without paintings to an artistic style, given only few examples.
User studies and automatic evaluations confirm that our model (post-adaptation)
performs on par with state-of-the-art models trained on massive datasets that
contain artistic content like paintings, drawings or illustrations. Finally,
using data attribution techniques, we analyze how both artistic and
non-artistic datasets contribute to generating artistic-style images.
Surprisingly, our findings suggest that high-quality artistic outputs can be
achieved without prior exposure to artistic data, indicating that artistic
style generation can occur in a controlled, opt-in manner using only a limited,
carefully selected set of training examples.

</details>


### [268] [MambaNUT: Nighttime UAV Tracking via Mamba-based Adaptive Curriculum Learning](https://arxiv.org/pdf/2412.00626)
*You Wu, Xiangyang Yang, Xucheng Wang, Hengzhou Ye, Dan Zeng, Shuiwang Li*

Main category: cs.CV

TL;DR: MambaNUT is a novel pure Mamba-based tracking framework for nighttime UAV tracking, using linear-complexity state space models and adaptive curriculum learning to improve generalization and reduce computational costs.


<details>
  <summary>Details</summary>
Motivation: Over-reliance on image enhancement, limited nighttime data, and lack of integration between daytime and nighttime trackers hinder end-to-end trainable frameworks. Current ViT-based trackers are computationally heavy.

Method: Proposes MambaNUT with a Vision Mamba backbone, single-stream architecture, and adaptive curriculum learning (ACL) for dynamic sampling and loss weighting.

Result: Achieves state-of-the-art performance on nighttime UAV tracking benchmarks with lower computational costs.

Conclusion: MambaNUT offers an efficient, high-performance solution for nighttime UAV tracking, addressing key limitations of existing methods.

Abstract: Harnessing low-light enhancement and domain adaptation, nighttime UAV
tracking has made substantial strides. However, over-reliance on image
enhancement, limited high-quality nighttime data, and a lack of integration
between daytime and nighttime trackers hinder the development of an end-to-end
trainable framework. Additionally, current ViT-based trackers demand heavy
computational resources due to their reliance on the self-attention mechanism.
In this paper, we propose a novel pure Mamba-based tracking framework
(MambaNUT) that employs a state space model with linear complexity as its
backbone, incorporating a single-stream architecture that integrates feature
learning and template-search coupling within Vision Mamba. We introduce an
adaptive curriculum learning (ACL) approach that dynamically adjusts sampling
strategies and loss weights, thereby improving the model's ability of
generalization. Our ACL is composed of two levels of curriculum schedulers: (1)
sampling scheduler that transforms the data distribution from imbalanced to
balanced, as well as from easier (daytime) to harder (nighttime) samples; (2)
loss scheduler that dynamically assigns weights based on the size of the
training set and IoU of individual instances. Exhaustive experiments on
multiple nighttime UAV tracking benchmarks demonstrate that the proposed
MambaNUT achieves state-of-the-art performance while requiring lower
computational costs. The code will be available at
https://github.com/wuyou3474/MambaNUT.

</details>


### [269] [SerialGen: Personalized Image Generation by First Standardization Then Personalization](https://arxiv.org/pdf/2412.01485)
*Cong Xie, Han Zou, Ruiqi Yu, Yan Zhang, Zhenpeng Zhan*

Main category: cs.CV

TL;DR: SerialGen is a two-stage framework for generating personalized human characters with high text controllability and appearance consistency.


<details>
  <summary>Details</summary>
Motivation: To achieve both high text controllability and whole-body appearance consistency in personalized human character generation.

Method: A serial generation method with two stages: standardization of reference images and personalized generation based on standardized references, enhanced by two modules.

Result: Validated ability to produce personalized images with faithful appearance recovery and accurate text response.

Conclusion: The serial generation method and standardization model significantly improve appearance consistency and text controllability.

Abstract: In this work, we are interested in achieving both high text controllability
and whole-body appearance consistency in the generation of personalized human
characters. We propose a novel framework, named SerialGen, which is a serial
generation method consisting of two stages: first, a standardization stage that
standardizes reference images, and then a personalized generation stage based
on the standardized reference. Furthermore, we introduce two modules aimed at
enhancing the standardization process. Our experimental results validate the
proposed framework's ability to produce personalized images that faithfully
recover the reference image's whole-body appearance while accurately responding
to a wide range of text prompts. Through thorough analysis, we highlight the
critical contribution of the proposed serial generation method and
standardization model, evidencing enhancements in appearance consistency
between reference and output images and across serial outputs generated from
diverse text prompts. The term "Serial" in this work carries a double meaning:
it refers to the two-stage method and also underlines our ability to generate
serial images with consistent appearance throughout.

</details>


### [270] [Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs](https://arxiv.org/pdf/2412.01818)
*Qizhe Zhang, Aosong Cheng, Ming Lu, Renrui Zhang, Zhiyong Zhuo, Jiajun Cao, Shaobo Guo, Qi She, Shanghang Zhang*

Main category: cs.CV

TL;DR: VisPruner is a plug-and-play method for pruning visual tokens in LVLMs using visual cues, outperforming text-visual attention methods and reducing computational costs significantly.


<details>
  <summary>Details</summary>
Motivation: Existing methods use text-visual attention scores for pruning, but these scores are not ideal indicators, prompting the need for a better approach.

Method: VisPruner selects significant tokens using visual attention and removes duplicates based on similarity, retaining diverse tokens to preserve visual information.

Result: VisPruner reduces FLOPs by 91% and latency by 75% for LLaVA-1.5-7B while maintaining performance, outperforming existing methods.

Conclusion: VisPruner is an effective, training-free solution for efficient token pruning in LVLMs, validated across architectures and reduction ratios.

Abstract: Large vision-language models (LVLMs) generally contain significantly more
visual tokens than their textual counterparts, resulting in a considerable
computational burden. Recent efforts have been made to tackle this issue by
pruning visual tokens early within the language model. Most existing works use
attention scores between text and visual tokens to assess the importance of
visual tokens. However, in this study, we first analyze the text-visual
attention in the language model and find that this score is not an ideal
indicator for token pruning. Based on the analysis, We propose VisPruner, a
plug-and-play method that utilizes visual cues for more effective token pruning
in LVLMs. Specifically, we first use visual attention to select a limited
number of significant tokens. Then, we remove duplicate tokens from the
remaining ones based on their similarity. By retaining diverse tokens alongside
the initially selected important tokens, we maximally preserve the visual
information of the input image. Experimental results demonstrate that our
VisPruner sustains strong performance across various VLM architectures and
reduction ratios, significantly outperforming existing methods based on
text-visual attention. Notably, without any training, VisPruner can reduce the
FLOPs of LLaVA-1.5-7B by 91% and inference latency by 75%, while maintaining
comparable performance. Our code is available at
https://github.com/Theia-4869/VisPruner.

</details>


### [271] [Efficient and Comprehensive Feature Extraction in Large Vision-Language Model for Clinical Pathology Analysis](https://arxiv.org/pdf/2412.09521)
*Shengxuming Zhang, Weihan Li, Tianhong Gao, Jiacong Hu, Haoming Luo, Xiuming Zhang, Jing Zhang, Mingli Song, Zunlei Feng*

Main category: cs.CV

TL;DR: OmniPath, a pathology-specialized LVLM, improves diagnostic accuracy and efficiency by addressing feature redundancy and resolution constraints with mixed task-guided feature enhancement and prompt-guided detail feature completion.


<details>
  <summary>Details</summary>
Motivation: Traditional vision models and LVLMs struggle with redundant features and resolution limits, hindering pathology diagnosis.

Method: Proposes mixed task-guided feature enhancement and prompt-guided detail feature completion, leveraging 490,000 pathology samples.

Result: OmniPath outperforms existing methods in accuracy and efficiency across diverse pathology tasks.

Conclusion: OmniPath offers a clinically aligned, interactive solution for auxiliary pathology diagnosis.

Abstract: Pathological diagnosis is vital for determining disease characteristics,
guiding treatment, and assessing prognosis, relying heavily on detailed,
multi-scale analysis of high-resolution whole slide images (WSI). However,
traditional pure vision models face challenges of redundant feature extraction,
whereas existing large vision-language models (LVLMs) are limited by input
resolution constraints, hindering their efficiency and accuracy. To overcome
these issues, we propose two innovative strategies: the mixed task-guided
feature enhancement, which directs feature extraction toward lesion-related
details across scales, and the prompt-guided detail feature completion, which
integrates coarse- and fine-grained features from WSI based on specific prompts
without compromising inference speed. Leveraging a comprehensive dataset of
490,000 samples from diverse pathology tasks-including cancer detection,
grading, vascular and neural invasion identification, and so on-we trained the
pathology-specialized LVLM, OmniPath. Extensive experiments demonstrate that
this model significantly outperforms existing methods in diagnostic accuracy
and efficiency, offering an interactive, clinically aligned approach for
auxiliary diagnosis in a wide range of pathology applications.

</details>


### [272] [Multi-QuAD: Multi-Level Quality-Adaptive Dynamic Network for Reliable Multimodal Classification](https://arxiv.org/pdf/2412.14489)
*Shu Shen, C. L. Philip Chen, Tong Zhang*

Main category: cs.CV

TL;DR: Multi-QuAD improves multimodal classification by dynamically adapting network depth and parameters based on sample quality, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal classification methods lack robust data quality estimation and dynamic adaptation, undermining reliability.

Method: Multi-QuAD uses noise-free prototypes and classifier-free design for quality estimation, GCND for dynamic depth, and LGP for adaptive parameters.

Result: Outperforms state-of-the-art methods on four datasets in classification and reliability.

Conclusion: Multi-QuAD enhances reliability and adaptability in multimodal classification, addressing quality and dynamic network challenges.

Abstract: Multimodal machine learning has achieved remarkable progress in many
scenarios, but its reliability is undermined by varying sample quality. This
paper finds that existing reliable multimodal classification methods not only
fail to provide robust estimation of data quality, but also lack dynamic
networks for sample-specific depth and parameters to achieve reliable
inference. To this end, a novel framework for multimodal reliable
classification termed \textit{Multi-level Quality-Adaptive Dynamic multimodal
network} (Multi-QuAD) is proposed. Multi-QuAD first adopts a novel approach
based on noise-free prototypes and a classifier-free design to reliably
estimate the quality of each sample at both modality and feature levels. It
then achieves sample-specific network depth via the \textbf{\textit{Global
Confidence Normalized Depth (GCND)}} mechanism. By normalizing depth across
modalities and samples, \textit{\textbf{GCND}} effectively mitigates the impact
of challenging modality inputs on dynamic depth reliability. Furthermore,
Multi-QuAD provides sample-adaptive network parameters via the
\textbf{\textit{Layer-wise Greedy Parameter (LGP)}} mechanism driven by
feature-level quality. The cross-modality layer-wise greedy strategy in
\textbf{\textit{LGP}} designs a reliable parameter prediction paradigm for
multimodal networks with variable architecture for the first time. Experiments
conducted on four datasets demonstrate that Multi-QuAD significantly
outperforms state-of-the-art methods in classification performance and
reliability, exhibiting strong adaptability to data with diverse quality.

</details>


### [273] [FusionSORT: Fusion Methods for Online Multi-object Visual Tracking](https://arxiv.org/pdf/2501.00843)
*Nathanael L. Baisa*

Main category: cs.CV

TL;DR: The paper investigates four fusion methods for associating detections to tracklets in multi-object visual tracking, emphasizing the importance of choosing the right method.


<details>
  <summary>Details</summary>
Motivation: To improve data association in multi-object visual tracking by exploring and evaluating different fusion methods for combining strong and weak cues.

Method: Four fusion methods (minimum, weighted sum based on IoU, Kalman filter gating, and Hadamard product of costs) are tested on MOT17, MOT20, and DanceTrack datasets.

Result: The choice of fusion method significantly impacts data association performance in multi-object visual tracking.

Conclusion: The study highlights the critical role of selecting the appropriate fusion method for effective data association, aiding the computer vision community in making informed choices.

Abstract: In this work, we investigate four different fusion methods for associating
detections to tracklets in multi-object visual tracking. In addition to
considering strong cues such as motion and appearance information, we also
consider weak cues such as height intersection-over-union (height-IoU) and
tracklet confidence information in the data association using different fusion
methods. These fusion methods include minimum, weighted sum based on IoU,
Kalman filter (KF) gating, and hadamard product of costs due to the different
cues. We conduct extensive evaluations on validation sets of MOT17, MOT20 and
DanceTrack datasets, and find out that the choice of a fusion method is key for
data association in multi-object visual tracking. We hope that this
investigative work helps the computer vision research community to use the
right fusion method for data association in multi-object visual tracking.

</details>


### [274] [Improving Tropical Cyclone Forecasting With Video Diffusion Models](https://arxiv.org/pdf/2501.16003)
*Zhibo Ren, Pritthijit Nath, Pancham Shukla*

Main category: cs.CV

TL;DR: A novel video diffusion model for tropical cyclone forecasting improves long-term dynamics and extends the reliable forecasting horizon from 36 to 50 hours, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for TC forecasting treat evolution as independent frame-to-frame predictions, lacking long-term dependency modeling.

Method: Uses video diffusion models with temporal layers and a two-stage training strategy to generate multiple frames simultaneously, improving quality and low-data performance.

Result: Outperforms Nath et al. by 19.3% in MAE, 16.2% in PSNR, and 36.1% in SSIM, with a longer forecasting horizon.

Conclusion: The approach produces temporally coherent forecasts and maintains single-frame quality, validated by traditional metrics and FVD.

Abstract: Tropical cyclone (TC) forecasting is crucial for disaster preparedness and
mitigation. While recent deep learning approaches have shown promise, existing
methods often treat TC evolution as a series of independent frame-to-frame
predictions, limiting their ability to capture long-term dynamics. We present a
novel application of video diffusion models for TC forecasting that explicitly
models temporal dependencies through additional temporal layers. Our approach
enables the model to generate multiple frames simultaneously, better capturing
cyclone evolution patterns. We introduce a two-stage training strategy that
significantly improves individual-frame quality and performance in low-data
regimes. Experimental results show our method outperforms the previous approach
of Nath et al. by 19.3% in MAE, 16.2% in PSNR, and 36.1% in SSIM. Most notably,
we extend the reliable forecasting horizon from 36 to 50 hours. Through
comprehensive evaluation using both traditional metrics and Fr\'echet Video
Distance (FVD), we demonstrate that our approach produces more temporally
coherent forecasts while maintaining competitive single-frame quality. Code
accessible at https://github.com/Ren-creater/forecast-video-diffmodels.

</details>


### [275] [RealRAG: Retrieval-augmented Realistic Image Generation via Self-reflective Contrastive Learning](https://arxiv.org/pdf/2502.00848)
*Yuanhuiyi Lyu, Xu Zheng, Lutao Jiang, Yibo Yan, Xin Zou, Huiyu Zhou, Linfeng Zhang, Xuming Hu*

Main category: cs.CV

TL;DR: RealRAG is a retrieval-augmented framework that enhances text-to-image models by integrating real-world images to address hallucinations and distortions in fine-grained or novel object generation.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image models struggle with fine-grained or unseen objects due to fixed parameters and closed datasets, leading to hallucinations or distortions.

Method: RealRAG uses a reflective retriever trained via self-reflective contrastive learning to augment generation with real-world images, compensating for the model's knowledge gaps.

Result: The framework improves realism and reduces distortions, achieving a 16.18% FID score gain on the Stanford Car benchmark.

Conclusion: RealRAG effectively bridges knowledge gaps in generative models, enhancing performance for fine-grained and novel object generation.

Abstract: Recent text-to-image generative models, e.g., Stable Diffusion V3 and Flux,
have achieved notable progress. However, these models are strongly restricted
to their limited knowledge, a.k.a., their own fixed parameters, that are
trained with closed datasets. This leads to significant hallucinations or
distortions when facing fine-grained and unseen novel real-world objects, e.g.,
the appearance of the Tesla Cybertruck. To this end, we present the first
real-object-based retrieval-augmented generation framework (RealRAG), which
augments fine-grained and unseen novel object generation by learning and
retrieving real-world images to overcome the knowledge gaps of generative
models. Specifically, to integrate missing memory for unseen novel object
generation, we train a reflective retriever by self-reflective contrastive
learning, which injects the generator's knowledge into the sef-reflective
negatives, ensuring that the retrieved augmented images compensate for the
model's missing knowledge. Furthermore, the real-object-based framework
integrates fine-grained visual knowledge for the generative models, tackling
the distortion problem and improving the realism for fine-grained object
generation. Our Real-RAG is superior in its modular application to all types of
state-of-the-art text-to-image generative models and also delivers remarkable
performance boosts with all of them, such as a gain of 16.18% FID score with
the auto-regressive model on the Stanford Car benchmark.

</details>


### [276] [GP-GS: Gaussian Processes for Enhanced Gaussian Splatting](https://arxiv.org/pdf/2502.02283)
*Zhihao Guo, Jingxuan Su, Shenglin Wang, Jinlong Fan, Jing Zhang, Wei Zhou, Hadi Amirpour, Yunlong Zhao, Liangxiu Han, Peng Wang*

Main category: cs.CV

TL;DR: The paper introduces GP-GS, a framework enhancing 3D Gaussian Splatting by using Gaussian Processes to densify sparse SfM point clouds, improving reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: Sparse SfM point clouds limit 3D Gaussian Splatting's reconstruction quality, prompting the need for adaptive densification.

Method: A multi-output Gaussian Process model dynamically samples and filters to densify SfM point clouds, guided by uncertainty estimates.

Result: GP-GS generates dense point clouds, improving 3D Gaussian initialization and reconstruction performance.

Conclusion: The framework is validated on synthetic and real-world datasets, proving effective for high-quality 3D reconstruction.

Abstract: 3D Gaussian Splatting has emerged as an efficient photorealistic novel view
synthesis method. However, its reliance on sparse Structure-from-Motion (SfM)
point clouds often limits scene reconstruction quality. To address the
limitation, this paper proposes a novel 3D reconstruction framework, Gaussian
Processes enhanced Gaussian Splatting (GP-GS), in which a multi-output Gaussian
Process model is developed to enable adaptive and uncertainty-guided
densification of sparse SfM point clouds. Specifically, we propose a dynamic
sampling and filtering pipeline that adaptively expands the SfM point clouds by
leveraging GP-based predictions to infer new candidate points from the input 2D
pixels and depth maps. The pipeline utilizes uncertainty estimates to guide the
pruning of high-variance predictions, ensuring geometric consistency and
enabling the generation of dense point clouds. These densified point clouds
provide high-quality initial 3D Gaussians, enhancing reconstruction
performance. Extensive experiments conducted on synthetic and real-world
datasets across various scales validate the effectiveness and practicality of
the proposed framework.

</details>


### [277] [Articulate AnyMesh: Open-Vocabulary 3D Articulated Objects Modeling](https://arxiv.org/pdf/2502.02590)
*Xiaowen Qiu, Jincheng Yang, Yian Wang, Zhehuan Chen, Yufei Wang, Tsun-Hsuan Wang, Zhou Xian, Chuang Gan*

Main category: cs.CV

TL;DR: Articulate Anymesh automates converting rigid 3D meshes into articulated objects using Vision-Language Models, enabling open-vocabulary 3D modeling and expanding dataset coverage.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D articulated object modeling are limited by handcrafted categories, restricting their applicability to diverse objects.

Method: The framework uses Vision-Language Models and visual prompting to segment parts and construct functional joints from rigid 3D meshes.

Result: It generates high-quality articulated objects (tools, toys, devices, vehicles) and aids robotic skill acquisition in simulation.

Conclusion: Articulate Anymesh broadens 3D articulated object modeling and enhances robotic manipulation capabilities.

Abstract: 3D articulated objects modeling has long been a challenging problem, since it
requires to capture both accurate surface geometries and semantically
meaningful and spatially precise structures, parts, and joints. Existing
methods heavily depend on training data from a limited set of handcrafted
articulated object categories (e.g., cabinets and drawers), which restricts
their ability to model a wide range of articulated objects in an
open-vocabulary context. To address these limitations, we propose Articulate
Anymesh, an automated framework that is able to convert any rigid 3D mesh into
its articulated counterpart in an open-vocabulary manner. Given a 3D mesh, our
framework utilizes advanced Vision-Language Models and visual prompting
techniques to extract semantic information, allowing for both the segmentation
of object parts and the construction of functional joints. Our experiments show
that Articulate Anymesh can generate large-scale, high-quality 3D articulated
objects, including tools, toys, mechanical devices, and vehicles, significantly
expanding the coverage of existing 3D articulated object datasets.
Additionally, we show that these generated assets can facilitate the
acquisition of new articulated object manipulation skills in simulation, which
can then be transferred to a real robotic system. Our Github website is
https://articulate-anymesh.github.io.

</details>


### [278] [HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation](https://arxiv.org/pdf/2502.04847)
*Qijun Gan, Yi Ren, Chen Zhang, Zhenhui Ye, Pan Xie, Xiang Yin, Zehuan Yuan, Bingyue Peng, Jianke Zhu*

Main category: cs.CV

TL;DR: HumanDiT is a pose-guided Diffusion Transformer framework that generates high-fidelity human motion videos with fine-grained body rendering, supporting variable resolutions and sequence lengths.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with detailed body part rendering and visual consistency in long sequences. HumanDiT aims to overcome these limitations.

Method: Built on Diffusion Transformer (DiT), HumanDiT uses a prefix-latent reference strategy and Keypoint-DiT for pose sequence generation, along with a Pose Adapter for pose transfer.

Result: HumanDiT outperforms existing methods in generating long-form, pose-accurate videos across diverse scenarios.

Conclusion: HumanDiT advances human motion video generation by addressing key challenges in detail rendering and sequence consistency.

Abstract: Human motion video generation has advanced significantly, while existing
methods still struggle with accurately rendering detailed body parts like hands
and faces, especially in long sequences and intricate motions. Current
approaches also rely on fixed resolution and struggle to maintain visual
consistency. To address these limitations, we propose HumanDiT, a pose-guided
Diffusion Transformer (DiT)-based framework trained on a large and wild dataset
containing 14,000 hours of high-quality video to produce high-fidelity videos
with fine-grained body rendering. Specifically, (i) HumanDiT, built on DiT,
supports numerous video resolutions and variable sequence lengths, facilitating
learning for long-sequence video generation; (ii) we introduce a prefix-latent
reference strategy to maintain personalized characteristics across extended
sequences. Furthermore, during inference, HumanDiT leverages Keypoint-DiT to
generate subsequent pose sequences, facilitating video continuation from static
images or existing videos. It also utilizes a Pose Adapter to enable pose
transfer with given sequences. Extensive experiments demonstrate its superior
performance in generating long-form, pose-accurate videos across diverse
scenarios.

</details>


### [279] [Clinical Inspired MRI Lesion Segmentation](https://arxiv.org/pdf/2502.16032)
*Lijun Yan, Churan Wang, Fangwei Zhong, Yizhou Wang*

Main category: cs.CV

TL;DR: A residual fusion method for MRI lesion segmentation, inspired by clinical practice, achieves state-of-the-art results on brain tumor and breast lesion datasets.


<details>
  <summary>Details</summary>
Motivation: Accurate lesion segmentation in MRI is challenging due to varying contrast mechanisms. Clinical use of pre- and post-contrast sequence differences inspired the proposed method.

Method: Iteratively and adaptively fuses features from pre- and post-contrast sequences at multiple resolutions using dynamic weights.

Result: Achieves state-of-the-art performance on BraTS2023 (brain tumor) and an in-house breast MRI dataset.

Conclusion: The clinically inspired method effectively addresses diverse lesion enhancement patterns and has broad application potential.

Abstract: Magnetic resonance imaging (MRI) is a potent diagnostic tool for detecting
pathological tissues in various diseases. Different MRI sequences have
different contrast mechanisms and sensitivities for different types of lesions,
which pose challenges to accurate and consistent lesion segmentation. In
clinical practice, radiologists commonly use the sub-sequence feature, i.e. the
difference between post contrast-enhanced T1-weighted (post) and
pre-contrast-enhanced (pre) sequences, to locate lesions. Inspired by this, we
propose a residual fusion method to learn subsequence representation for MRI
lesion segmentation. Specifically, we iteratively and adaptively fuse features
from pre- and post-contrast sequences at multiple resolutions, using dynamic
weights to achieve optimal fusion and address diverse lesion enhancement
patterns. Our method achieves state-of-the-art performances on BraTS2023
dataset for brain tumor segmentation and our in-house breast MRI dataset for
breast lesion segmentation. Our method is clinically inspired and has the
potential to facilitate lesion segmentation in various applications.

</details>


### [280] [A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs](https://arxiv.org/pdf/2502.19159)
*Xuan Ding, Rui Sun, Yunjian Zhang, Xiu Yan, Yueqi Zhou, Kaihao Huang, Suzhong Fu, Chuanlong Xie, Yao Zhu*

Main category: cs.CV

TL;DR: Depth-wise pruning in Transformers can accelerate inference but may degrade performance by discarding entire layers. This paper introduces a sliding layer merging method to dynamically fuse similar layers, improving performance over traditional pruning.


<details>
  <summary>Details</summary>
Motivation: To address the performance degradation caused by indiscriminate layer pruning in Transformers by leveraging layer similarity for dynamic merging.

Method: Analyzes layer correlations in reproducing kernel Hilbert space and proposes a sliding layer merging method to dynamically fuse consecutive layers based on similarity.

Result: Outperforms existing pruning techniques, achieving a 1.654% improvement in zero-shot tasks on Vicuna-7B with 35% pruning.

Conclusion: The method simplifies model structure while maintaining performance and suggests combining depth and width pruning for enhanced results.

Abstract: Compared to width-wise pruning, depth-wise pruning can significantly
accelerate inference in resource-constrained scenarios. However, treating the
entire Transformer layer as the minimum pruning unit may degrade model
performance by indiscriminately discarding the entire information of the layer.
This paper reveals the ``Patch-like'' feature relationship between layers in
large language models by analyzing the correlation of the outputs of different
layers in the reproducing kernel Hilbert space. Building on this observation,
we propose a sliding layer merging method that dynamically selects and fuses
consecutive layers from top to bottom according to a pre-defined similarity
threshold, thereby simplifying the model structure while maintaining its
performance. Extensive experiments on LLMs with various architectures and
different parameter scales show that our method outperforms existing pruning
techniques in both zero-shot inference performance and retraining recovery
quality after pruning. In particular, in the experiment with 35% pruning on the
Vicuna-7B model, our method achieved a 1.654% improvement in average
performance on zero-shot tasks compared to the existing method. Moreover, we
further reveal the potential of combining depth pruning with width pruning to
enhance the pruning effect. Our codes are available at
https://github.com/920927/SLM-a-sliding-layer-merging-method.

</details>


### [281] [Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly a GAN Discriminator](https://arxiv.org/pdf/2503.01103)
*Kaiwen Zheng, Yongxin Chen, Huayu Chen, Guande He, Ming-Yu Liu, Jun Zhu, Qinsheng Zhang*

Main category: cs.CV

TL;DR: The paper introduces Direct Discriminative Optimization (DDO), a framework combining likelihood-based generative training and GAN-type discrimination to overcome mode-covering limitations of MLE, achieving SOTA results in visual generation.


<details>
  <summary>Details</summary>
Motivation: To address the mode-covering tendency of MLE in generative models, which limits generation quality under constrained model capacity.

Method: Proposes DDO, integrating likelihood-based training and GAN-type discrimination via reverse KL divergence and self-generated negative signals, eliminating joint training needs.

Result: DDO significantly improves SOTA diffusion and autoregressive models, reducing FID scores across multiple datasets without guidance mechanisms.

Conclusion: DDO offers an efficient, effective way to refine generative models beyond MLE limits, demonstrating superior performance in visual generation tasks.

Abstract: While likelihood-based generative models, particularly diffusion and
autoregressive models, have achieved remarkable fidelity in visual generation,
the maximum likelihood estimation (MLE) objective, which minimizes the forward
KL divergence, inherently suffers from a mode-covering tendency that limits the
generation quality under limited model capacity. In this work, we propose
Direct Discriminative Optimization (DDO) as a unified framework that integrates
likelihood-based generative training and GAN-type discrimination to bypass this
fundamental constraint by exploiting reverse KL and self-generated negative
signals. Our key insight is to parameterize a discriminator implicitly using
the likelihood ratio between a learnable target model and a fixed reference
model, drawing parallels with the philosophy of Direct Preference Optimization
(DPO). Unlike GANs, this parameterization eliminates the need for joint
training of generator and discriminator networks, allowing for direct,
efficient, and effective finetuning of a well-trained model to its full
potential beyond the limits of MLE. DDO can be performed iteratively in a
self-play manner for progressive model refinement, with each round requiring
less than 1% of pretraining epochs. Our experiments demonstrate the
effectiveness of DDO by significantly advancing the previous SOTA diffusion
model EDM, reducing FID scores from 1.79/1.58/1.96 to new records of
1.30/0.97/1.26 on CIFAR-10/ImageNet-64/ImageNet 512x512 datasets without any
guidance mechanisms, and by consistently improving both guidance-free and
CFG-enhanced FIDs of visual autoregressive models on ImageNet 256x256.

</details>


### [282] [Self-Adaptive Gamma Context-Aware SSM-based Model for Metal Defect Detection](https://arxiv.org/pdf/2503.01234)
*Sijin Sun, Ming Deng, Xingrui Yu, Xingyu Xi, Liangbin Zhao*

Main category: cs.CV

TL;DR: A Self-Adaptive Gamma Context-Aware SSM-based model (GCM-DET) is proposed for robust metal defect detection, addressing grayscale variations and complex defects, achieving significant performance gains on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with grayscale variations and complex defect states, limiting robustness in metal defect detection.

Method: The GCM-DET integrates Dynamic Gamma Correction (GC) for grayscale enhancement, State-Space Search Management (SSM) for multi-scale feature capture, and Focal Loss to handle class imbalance.

Result: The model achieves mAP@0.5 gains of 27.6%, 6.6%, and 2.6% on CD5-DET, NEU-DET, and GC10-DET datasets, respectively.

Conclusion: GCM-DET significantly improves metal defect detection, demonstrating robustness across varied datasets.

Abstract: Metal defect detection is critical in industrial quality assurance, yet
existing methods struggle with grayscale variations and complex defect states,
limiting its robustness. To address these challenges, this paper proposes a
Self-Adaptive Gamma Context-Aware SSM-based model(GCM-DET). This advanced
detection framework integrating a Dynamic Gamma Correction (GC) module to
enhance grayscale representation and optimize feature extraction for precise
defect reconstruction. A State-Space Search Management (SSM) architecture
captures robust multi-scale features, effectively handling defects of varying
shapes and scales. Focal Loss is employed to mitigate class imbalance and
refine detection accuracy. Additionally, the CD5-DET dataset is introduced,
specifically designed for port container maintenance, featuring significant
grayscale variations and intricate defect patterns. Experimental results
demonstrate that the proposed model achieves substantial improvements, with
mAP@0.5 gains of 27.6\%, 6.6\%, and 2.6\% on the CD5-DET, NEU-DET, and GC10-DET
datasets.

</details>


### [283] [Enhancing Layer Attention Efficiency through Pruning Redundant Retrievals](https://arxiv.org/pdf/2503.06473)
*Hanze Li, Xiande Huang*

Main category: cs.CV

TL;DR: Proposes a novel Efficient Layer Attention (ELA) architecture to reduce redundancy in layer attention mechanisms, improving training efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Existing layer attention methods suffer from redundancy, leading to similar features across layers, reduced representational capacity, and increased training time.

Method: Quantifies redundancy using KL divergence between adjacent layers and introduces Enhanced Beta Quantile Mapping (EBQM) to skip redundant layers.

Result: ELA achieves a 30% reduction in training time while enhancing performance in tasks like image classification and object detection.

Conclusion: The ELA architecture effectively addresses redundancy, improving both efficiency and performance in deep neural networks.

Abstract: Growing evidence suggests that layer attention mechanisms, which enhance
interaction among layers in deep neural networks, have significantly advanced
network architectures. However, existing layer attention methods suffer from
redundancy, as attention weights learned by adjacent layers often become highly
similar. This redundancy causes multiple layers to extract nearly identical
features, reducing the model's representational capacity and increasing
training time. To address this issue, we propose a novel approach to quantify
redundancy by leveraging the Kullback-Leibler (KL) divergence between adjacent
layers. Additionally, we introduce an Enhanced Beta Quantile Mapping (EBQM)
method that accurately identifies and skips redundant layers, thereby
maintaining model stability. Our proposed Efficient Layer Attention (ELA)
architecture, improves both training efficiency and overall performance,
achieving a 30% reduction in training time while enhancing performance in tasks
such as image classification and object detection.

</details>


### [284] [Introducing Unbiased Depth into 2D Gaussian Splatting for High-accuracy Surface Reconstruction](https://arxiv.org/pdf/2503.06587)
*Xiaoming Peng, Yixin Yang, Yang Zhou, Hui Huang*

Main category: cs.CV

TL;DR: The paper improves 2D Gaussian Splatting (2DGS) by addressing its failure on glossy surfaces through a novel depth convergence loss and rectified depth criterion, achieving better reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: 2DGS struggles with glossy surfaces, causing visible holes due to reflection discontinuity. The goal is to enhance reconstruction quality for such surfaces.

Method: Replaces depth distortion loss with a depth convergence loss and rectifies the depth criterion to account for all intersecting Gaussians along the ray.

Result: Significant improvement in reconstruction quality, producing more complete and accurate surfaces compared to 2DGS.

Conclusion: The proposed method effectively addresses the limitations of 2DGS on glossy surfaces, enhancing overall reconstruction quality.

Abstract: Recently, 2D Gaussian Splatting (2DGS) has demonstrated superior geometry
reconstruction quality than the popular 3DGS by using 2D surfels to approximate
thin surfaces. However, it falls short when dealing with glossy surfaces,
resulting in visible holes in these areas. We found the reflection
discontinuity causes the issue. To fit the jump from diffuse to specular
reflection at different viewing angles, depth bias is introduced in the
optimized Gaussian primitives. To address that, we first replace the depth
distortion loss in 2DGS with a novel depth convergence loss, which imposes a
strong constraint on depth continuity. Then, we rectified the depth criterion
in determining the actual surface, which fully accounts for all the
intersecting Gaussians along the ray. Qualitative and quantitative evaluations
across various datasets reveal that our method significantly improves
reconstruction quality, with more complete and accurate surfaces than 2DGS.

</details>


### [285] [Referring to Any Person](https://arxiv.org/pdf/2503.08507)
*Qing Jiang, Lin Wu, Zhaoyang Zeng, Tianhe Ren, Yuda Xiong, Yihao Chen, Qin Liu, Lei Zhang*

Main category: cs.CV

TL;DR: The paper addresses the challenge of detecting individuals in computer vision using natural language descriptions, introducing a new dataset (HumanRef) and model (RexSeek) to improve real-world usability.


<details>
  <summary>Details</summary>
Motivation: Existing models and benchmarks for referring to individuals in computer vision are limited, failing to achieve practical usability due to their focus on one-to-one referring.

Method: The authors redefine the task, introduce the HumanRef dataset, and propose RexSeek, a model combining a multimodal large language model with object detection.

Result: RexSeek outperforms state-of-the-art models on HumanRef and generalizes well to common object referring tasks.

Conclusion: The work advances the field by addressing key limitations in task definition, dataset design, and model architecture, offering a robust solution for real-world applications.

Abstract: Humans are undoubtedly the most important participants in computer vision,
and the ability to detect any individual given a natural language description,
a task we define as referring to any person, holds substantial practical value.
However, we find that existing models generally fail to achieve real-world
usability, and current benchmarks are limited by their focus on one-to-one
referring, that hinder progress in this area. In this work, we revisit this
task from three critical perspectives: task definition, dataset design, and
model architecture. We first identify five aspects of referable entities and
three distinctive characteristics of this task. Next, we introduce HumanRef, a
novel dataset designed to tackle these challenges and better reflect real-world
applications. From a model design perspective, we integrate a multimodal large
language model with an object detection framework, constructing a robust
referring model named RexSeek. Experimental results reveal that
state-of-the-art models, which perform well on commonly used benchmarks like
RefCOCO/+/g, struggle with HumanRef due to their inability to detect multiple
individuals. In contrast, RexSeek not only excels in human referring but also
generalizes effectively to common object referring, making it broadly
applicable across various perception tasks. Code is available at
https://github.com/IDEA-Research/RexSeek

</details>


### [286] [Multi-camera orientation tracking method for anisotropic particles in particle-laden flows](https://arxiv.org/pdf/2503.08694)
*Mees M. Flapper, Elian Bernard, Sander G. Huisman*

Main category: cs.CV

TL;DR: A method for tracking 3D location and orientation of anisotropic particles using multi-camera recordings is developed, tested for robustness, and demonstrated in various experiments.


<details>
  <summary>Details</summary>
Motivation: To enable detailed investigations of particle dynamics (location, orientation, rotation) in fluids, especially for anisotropic particles.

Method: Uses multi-camera recordings and a shape-based algorithm to reconstruct 3D location and orientation of particles. Robustness is tested with synthetic images.

Result: The method is robust, works for diverse particle shapes, tracks multiple particles simultaneously, and distinguishes particle types.

Conclusion: The tracking method is effective and broadly applicable, as demonstrated in quiescent and turbulent fluid experiments.

Abstract: A method for particle orientation tracking is developed and demonstrated
specifically for anisotropic particles. Using (high-speed) multi-camera
recordings of anisotropic particles from different viewpoints, we reconstruct
the 3D location and orientation of these particles using their known shape.
This paper describes an algorithm which tracks the location and orientation of
multiple anisotropic particles over time, enabling detailed investigations of
location, orientation, and rotation statistics. The robustness and error of
this method is quantified, and we explore the effects of noise, image size, the
number of used cameras, and the camera arrangement by applying the algorithm to
synthetic images. We showcase several use-cases of this method in several
experiments (in both quiescent and turbulent fluids), demonstrating the
effectiveness and broad applicability of the described tracking method. The
proposed method is shown to work for widely different particle shapes,
successfully tracks multiple particles simultaneously, and the method can
distinguish between different types of particles.

</details>


### [287] [A super-resolution reconstruction method for lightweight building images based on an expanding feature modulation network](https://arxiv.org/pdf/2503.13179)
*Yi Zhang, Ruonan Lin, Ang Ping*

Main category: cs.CV

TL;DR: A lightweight method for building image super-resolution using a Dilated Contextual Feature Modulation Network (DCFMN) is proposed, addressing challenges like long-range dependencies and improving efficiency without extra computational costs.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of existing lightweight super-resolution networks in modeling long-range dependencies and enhancing reconstruction quality for building images.

Method: The DCFMN integrates an expansion separable modulation unit (for multi-scale feature aggregation) and a local feature enhancement module (for channel mixing and reparameterization).

Result: The method achieves accurate global feature modeling, improves reconstruction quality, and maintains lightweight efficiency without added computational costs.

Conclusion: DCFMN effectively resolves challenges in building image super-resolution, offering a balance between accuracy and computational efficiency.

Abstract: This study proposes a lightweight method for building image super-resolution
using a Dilated Contextual Feature Modulation Network (DCFMN). The process
includes obtaining high-resolution images, down-sampling them to
low-resolution, enhancing the low-resolution images, constructing and training
a lightweight network model, and generating super-resolution outputs. To
address challenges such as regular textures and long-range dependencies in
building images, the DCFMN integrates an expansion separable modulation unit
and a local feature enhancement module. The former employs multiple expansion
convolutions equivalent to a large kernel to efficiently aggregate multi-scale
features while leveraging a simple attention mechanism for adaptivity. The
latter encodes local features, mixes channel information, and ensures no
additional computational burden during inference through reparameterization.
This approach effectively resolves the limitations of existing lightweight
super-resolution networks in modeling long-range dependencies, achieving
accurate and efficient global feature modeling without increasing computational
costs, and significantly improving both reconstruction quality and lightweight
efficiency for building image super-resolution models.

</details>


### [288] [Less is More: Improving Motion Diffusion Models with Sparse Keyframes](https://arxiv.org/pdf/2503.13859)
*Jinseok Bae, Inwoo Hwang, Young Yoon Lee, Ziyu Guo, Joseph Liu, Yizhak Ben-Shabat, Young Min Kim, Mubbasir Kapadia*

Main category: cs.CV

TL;DR: A novel diffusion framework using sparse keyframes improves motion generation efficiency and performance over dense frame methods.


<details>
  <summary>Details</summary>
Motivation: Existing motion diffusion models process dense frame sequences, which are computationally intensive and inefficient due to redundant frames.

Method: The proposed framework focuses on sparse, geometrically meaningful keyframes, masks non-keyframes, and dynamically refines the keyframe mask during inference.

Result: Outperforms state-of-the-art methods in text alignment and motion realism with fewer diffusion steps.

Conclusion: The sparse keyframe approach enhances efficiency and performance, proving robust for downstream tasks.

Abstract: Recent advances in motion diffusion models have led to remarkable progress in
diverse motion generation tasks, including text-to-motion synthesis. However,
existing approaches represent motions as dense frame sequences, requiring the
model to process redundant or less informative frames. The processing of dense
animation frames imposes significant training complexity, especially when
learning intricate distributions of large motion datasets even with modern
neural architectures. This severely limits the performance of generative motion
models for downstream tasks. Inspired by professional animators who mainly
focus on sparse keyframes, we propose a novel diffusion framework explicitly
designed around sparse and geometrically meaningful keyframes. Our method
reduces computation by masking non-keyframes and efficiently interpolating
missing frames. We dynamically refine the keyframe mask during inference to
prioritize informative frames in later diffusion steps. Extensive experiments
show that our approach consistently outperforms state-of-the-art methods in
text alignment and motion realism, while also effectively maintaining high
performance at significantly fewer diffusion steps. We further validate the
robustness of our framework by using it as a generative prior and adapting it
to different downstream tasks.

</details>


### [289] [TGBFormer: Transformer-GraphFormer Blender Network for Video Object Detection](https://arxiv.org/pdf/2503.13903)
*Qiang Qi, Xiao Wang*

Main category: cs.CV

TL;DR: The paper proposes TGBFormer, a network blending transformers and GraphFormer for video object detection, improving global and local feature aggregation.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely solely on CNNs or ViTs, limiting their ability to leverage both global and local features, which hampers detection performance.

Method: TGBFormer combines a spatial-temporal transformer for global context, a GraphFormer for local relationships, and a blender module to integrate both.

Result: TGBFormer achieves 86.5% mAP on ImageNet VID at 41.0 FPS, setting a new state-of-the-art.

Conclusion: The proposed TGBFormer effectively combines global and local features, outperforming existing methods in video object detection.

Abstract: Video object detection has made significant progress in recent years thanks
to convolutional neural networks (CNNs) and vision transformers (ViTs).
Typically, CNNs excel at capturing local features but struggle to model global
representations. Conversely, ViTs are adept at capturing long-range global
features but face challenges in representing local feature details.
Off-the-shelf video object detection methods solely rely on CNNs or ViTs to
conduct feature aggregation, which hampers their capability to simultaneously
leverage global and local information, thereby resulting in limited detection
performance. In this paper, we propose a Transformer-GraphFormer Blender
Network (TGBFormer) for video object detection, with three key technical
improvements to fully exploit the advantages of transformers and graph
convolutional networks while compensating for their limitations. First, we
develop a spatial-temporal transformer module to aggregate global contextual
information, constituting global representations with long-range feature
dependencies. Second, we introduce a spatial-temporal GraphFormer module that
utilizes local spatial and temporal relationships to aggregate features,
generating new local representations that are complementary to the transformer
outputs. Third, we design a global-local feature blender module to adaptively
couple transformer-based global representations and GraphFormer-based local
representations. Extensive experiments demonstrate that our TGBFormer
establishes new state-of-the-art results on the ImageNet VID dataset.
Particularly, our TGBFormer achieves 86.5% mAP while running at around 41.0 FPS
on a single Tesla A100 GPU.

</details>


### [290] [A Vision Centric Remote Sensing Benchmark](https://arxiv.org/pdf/2503.15816)
*Abduljaleel Adejumo, Faegheh Yeganli, Clifford Broni-bediako, Aoran Xiao, Naoto Yokoya, Mennatullah Siam*

Main category: cs.CV

TL;DR: The paper explores the limitations of CLIP-based MLLMs in remote sensing (RS) tasks, introduces the RSMMVP benchmark to evaluate these models, and highlights their struggles with visual grounding and spatial reasoning in RS imagery.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs excel in vision-language tasks but underperform in RS due to unique challenges like visual grounding and spatial reasoning. This study aims to address these gaps.

Method: The authors introduce the RSMMVP benchmark to identify CLIP-blind pairs and evaluate MLLMs using visual question answering (VQA) in RS tasks.

Result: The study reveals significant limitations of CLIP-based MLLMs in RS-specific representation learning, particularly in differentiating visually distinct yet semantically similar images.

Conclusion: The findings highlight weaknesses in CLIP-based visual encoding for RS and provide a foundation for developing more effective MLLMs tailored to remote sensing.

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable success in
vision-language tasks but their remote sensing (RS) counterpart are relatively
under explored. Unlike natural images, RS imagery presents unique challenges
that current MLLMs struggle to handle, particularly in visual grounding and
spatial reasoning. This study investigates the limitations of CLIP-based MLLMs
in RS, highlighting their failure to differentiate visually distinct yet
semantically similar RS images. To address this, we introduce a remote sensing
multimodal visual patterns (RSMMVP) benchmark. It is designed to evaluate MLLMs
in RS tasks by identifying the CLIP-blind pairs, where CLIP-based models
incorrectly assign high similarity scores to visually distinct RS images.
Through a visual question answering (VQA) evaluation, we analyze the
performance of state-of-the-art MLLMs, revealing significant limitations in RS
specific representation learning. The results provide valuable insights into
the weaknesses of CLIP-based visual encoding and offer a foundation for future
research to develop more effective MLLMs tailored for remote sensing
applications.

</details>


### [291] [EDEN: Enhanced Diffusion for High-quality Large-motion Video Frame Interpolation](https://arxiv.org/pdf/2503.15831)
*Zihao Zhang, Haoran Chen, Haoyu Zhao, Guansong Lu, Yanwei Fu, Hang Xu, Zuxuan Wu*

Main category: cs.CV

TL;DR: EDEN introduces an enhanced diffusion method for high-quality video frame interpolation, outperforming existing techniques in handling large motion.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generating sharp, temporally consistent frames in scenarios with large motion, where current diffusion-based methods fall short.

Method: Uses a transformer-based tokenizer for refined latent representations, enhances the diffusion transformer with temporal attention, and incorporates start-end frame difference embedding.

Result: Achieves state-of-the-art results, including a 10% LPIPS reduction on DAVIS and SNU-FILM, and an 8% improvement on DAIN-HD.

Conclusion: EDEN effectively handles large-motion video frame interpolation, setting new benchmarks in performance.

Abstract: Handling complex or nonlinear motion patterns has long posed challenges for
video frame interpolation. Although recent advances in diffusion-based methods
offer improvements over traditional optical flow-based approaches, they still
struggle to generate sharp, temporally consistent frames in scenarios with
large motion. To address this limitation, we introduce EDEN, an Enhanced
Diffusion for high-quality large-motion vidEo frame iNterpolation. Our approach
first utilizes a transformer-based tokenizer to produce refined latent
representations of the intermediate frames for diffusion models. We then
enhance the diffusion transformer with temporal attention across the process
and incorporate a start-end frame difference embedding to guide the generation
of dynamic motion. Extensive experiments demonstrate that EDEN achieves
state-of-the-art results across popular benchmarks, including nearly a 10%
LPIPS reduction on DAVIS and SNU-FILM, and an 8% improvement on DAIN-HD.

</details>


### [292] [V-NAW: Video-based Noise-aware Adaptive Weighting for Facial Expression Recognition](https://arxiv.org/pdf/2503.15970)
*JunGyu Lee, Kunyoung Lee, Haesol Park, Ig-Jae Kim, Gi Pyo Nam*

Main category: cs.CV

TL;DR: The paper proposes V-NAW to address label ambiguity and class imbalance in video-based FER, along with an augmentation strategy to reduce redundancy, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Label ambiguity and class imbalance in video-based FER degrade performance, necessitating solutions to improve accuracy.

Method: Proposes Video-based Noise-aware Adaptive Weighting (V-NAW) for adaptive frame importance and a redundancy-reducing augmentation strategy.

Result: Extensive experiments show significant improvements in video-based FER performance.

Conclusion: The proposed methods effectively address key challenges in FER, enhancing performance and robustness.

Abstract: Facial Expression Recognition (FER) plays a crucial role in human affective
analysis and has been widely applied in computer vision tasks such as
human-computer interaction and psychological assessment. The 8th Affective
Behavior Analysis in-the-Wild (ABAW) Challenge aims to assess human emotions
using the video-based Aff-Wild2 dataset. This challenge includes various tasks,
including the video-based EXPR recognition track, which is our primary focus.
In this paper, we demonstrate that addressing label ambiguity and class
imbalance, which are known to cause performance degradation, can lead to
meaningful performance improvements. Specifically, we propose Video-based
Noise-aware Adaptive Weighting (V-NAW), which adaptively assigns importance to
each frame in a clip to address label ambiguity and effectively capture
temporal variations in facial expressions. Furthermore, we introduce a simple
and effective augmentation strategy to reduce redundancy between consecutive
frames, which is a primary cause of overfitting. Through extensive experiments,
we validate the effectiveness of our approach, demonstrating significant
improvements in video-based FER performance.

</details>


### [293] [Think or Not Think: A Study of Explicit Thinking in Rule-Based Visual Reinforcement Fine-Tuning](https://arxiv.org/pdf/2503.16188)
*Ming Li, Jike Zhong, Shitian Zhao, Yuxiang Lai, Haoquan Zhang, Wang Bill Zhu, Kaipeng Zhang*

Main category: cs.CV

TL;DR: The paper explores the necessity of explicit thinking in rule-based reinforcement fine-tuning (RFT) for MLLMs, introducing methods like No-Thinking-RL and Think-After-Answer, and finds that thinking isn't always required, especially in visual tasks or for limited-capability models.


<details>
  <summary>Details</summary>
Motivation: To challenge the conventional belief that explicit thinking is crucial for RFT success in MLLMs and investigate whether thinking can be optional or adaptive.

Method: Proposes CLS-RL for image classification, No-Thinking-RL (RFT without thinking), Think-After-Answer (thinking after the answer), and Adaptive-Thinking (learning when to think).

Result: No-Thinking-RL outperforms thinking-based RFT in visual tasks; limited-capability models struggle with thinking; Think-After-Answer mitigates inconsistencies; Adaptive-Thinking adapts based on task and model.

Conclusion: Explicit thinking isn't always necessary in RFT; MLLMs can adaptively decide when to think, improving performance based on task complexity and model capability.

Abstract: This paper investigates the role of explicit thinking process in rule-based
reinforcement fine-tuning (RFT) for MLLMs. We first propose CLS-RL for MLLM
image classification, using verifiable rewards for fine-tuning. Experiments
show CLS-RL significantly outperforms SFT and yields a cross-dataset
generalization effect. We then rethink and question whether explicit thinking
in RFT is always necessary. Challenging the convention that explicit thinking
is crucial for the success of RFT, we introduce No-Thinking-RL, exploring RFT
without thinking by introducing a simple equality accuracy reward. We evaluate
No-Thinking-RL on 6 diverse tasks across different model sizes and types.
Experimental results reveal three key findings: 1). Visual perception tasks do
not require thinking during RFT, as No-Thinking-RL consistently outperforms or
matches Thinking-based RFT across model sizes. 2).} Models with limited
capabilities struggle to generate high-quality CoT for RFT, making
Thinking-based RFT less effective than No-Thinking-RL. 3). There are
inconsistencies between the answers in the thinking and answer tags for some
responses of thinking-based RFT, which show lower accuracy than the overall
accuracy. We hypothesize that explicit thinking before verifiable answers may
hinder reward convergence and reduce performance. To test this hypothesis, we
propose Think-After-Answer, which places thinking after the answer to mitigate
this effect for experimental verification. Lastly, we conduct a pilot study to
explore whether MLLMs can learn when to think during RFT, introducing an
Adaptive-Thinking method. Experiments show that it converges to a specific
prompt depending on model capability and task complexity, achieving comparable
or better performance than both Thinking and No-Thinking-RL. This suggests
MLLMs can adaptively decide to think or not based on their capabilities and
task complexity.

</details>


### [294] [SCAM: A Real-World Typographic Robustness Evaluation for Multimodal Foundation Models](https://arxiv.org/pdf/2504.04893)
*Justus Westerhoff, Erblina Purelku, Jakob Hackstein, Jonas Loos, Leo Pinetzki, Lorenz Hufe*

Main category: cs.CV

TL;DR: SCAM dataset introduces 1,162 real-world typographic attack images, revealing vulnerabilities in Vision-Language Models (VLMs) and Large Vision-Language Models (LVLMs).


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack size and diversity to study typographic attacks, which exploit text-visual interplay in multimodal models.

Method: Created SCAM dataset with diverse attack images, benchmarked VLMs, and analyzed model susceptibility based on training data and architecture.

Result: Typographic attacks degrade model performance; LVLMs remain vulnerable due to vision encoders, but larger LLMs reduce susceptibility. Synthetic attacks mimic real-world ones.

Conclusion: SCAM provides resources for robust multimodal AI research, with datasets and code publicly released.

Abstract: Typographic attacks exploit the interplay between text and visual content in
multimodal foundation models, causing misclassifications when misleading text
is embedded within images. However, existing datasets are limited in size and
diversity, making it difficult to study such vulnerabilities. In this paper, we
introduce SCAM, the largest and most diverse dataset of real-world typographic
attack images to date, containing 1,162 images across hundreds of object
categories and attack words. Through extensive benchmarking of Vision-Language
Models (VLMs) on SCAM, we demonstrate that typographic attacks significantly
degrade performance, and identify that training data and model architecture
influence the susceptibility to these attacks. Our findings reveal that
typographic attacks persist in state-of-the-art Large Vision-Language Models
(LVLMs) due to the choice of their vision encoder, though larger Large Language
Models (LLMs) backbones help mitigate their vulnerability. Additionally, we
demonstrate that synthetic attacks closely resemble real-world (handwritten)
attacks, validating their use in research. Our work provides a comprehensive
resource and empirical insights to facilitate future research toward robust and
trustworthy multimodal AI systems. We publicly release the datasets introduced
in this paper along with the code for evaluations at
www.bliss.berlin/research/scam.

</details>


### [295] [FANeRV: Frequency Separation and Augmentation based Neural Representation for Video](https://arxiv.org/pdf/2504.06755)
*Li Yu, Zhihui Li, Yao Zhao, Jimin Xiao, Moncef Gabbouj*

Main category: cs.CV

TL;DR: FANeRV improves video reconstruction by separating and enhancing high/low-frequency components using wavelet transforms and specialized modules, outperforming existing NeRV methods.


<details>
  <summary>Details</summary>
Motivation: Existing NeRV methods struggle with fine spatial details, leading to vague reconstructions.

Method: Uses Wavelet Frequency Upgrade Block for frequency separation, targeted enhancement, and gated network fusion. Integrates convolutional residual blocks for high-frequency detail restoration.

Result: Significantly improves reconstruction and excels in tasks like video compression, inpainting, and interpolation.

Conclusion: FANeRV addresses NeRV limitations, offering superior performance in video tasks.

Abstract: Neural representations for video (NeRV) have gained considerable attention
for their strong performance across various video tasks. However, existing NeRV
methods often struggle to capture fine spatial details, resulting in vague
reconstructions. In this paper, we present a Frequency Separation and
Augmentation based Neural Representation for video (FANeRV), which addresses
these limitations with its core Wavelet Frequency Upgrade Block. This block
explicitly separates input frames into high and low-frequency components using
discrete wavelet transform, followed by targeted enhancement using specialized
modules. Finally, a specially designed gated network effectively fuses these
frequency components for optimal reconstruction. Additionally, convolutional
residual enhancement blocks are integrated into the later stages of the network
to balance parameter distribution and improve the restoration of high-frequency
details. Experimental results demonstrate that FANeRV significantly improves
reconstruction performance and excels in multiple tasks, including video
compression, inpainting, and interpolation, outperforming existing NeRV
methods.

</details>


### [296] [FocusedAD: Character-centric Movie Audio Description](https://arxiv.org/pdf/2504.12157)
*Xiaojun Ye, Chun Wang, Yiren Song, Sheng Zhou, Liangcheng Li, Jiajun Bu*

Main category: cs.CV

TL;DR: FocusedAD is a character-centric framework for Movie Audio Description (AD) that improves narration by tracking characters, using contextual cues, and generating plot-relevant details. It achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To address the unique challenges of AD, such as plot-relevant narration and character name references, for blind and visually impaired audiences.

Method: Proposes FocusedAD with three modules: Character Perception Module (CPM) for tracking characters, Dynamic Prior Module (DPM) for contextual cues, and Focused Caption Module (FCM) for enriched narrations. Also introduces an automated pipeline for character query banks.

Result: Achieves state-of-the-art performance on benchmarks, including zero-shot results on MAD-eval-Named and Cinepile-AD.

Conclusion: FocusedAD effectively addresses AD challenges and improves narration quality, with code and data made available.

Abstract: Movie Audio Description (AD) aims to narrate visual content during
dialogue-free segments, particularly benefiting blind and visually impaired
(BVI) audiences. Compared with general video captioning, AD demands
plot-relevant narration with explicit character name references, posing unique
challenges in movie understanding.To identify active main characters and focus
on storyline-relevant regions, we propose FocusedAD, a novel framework that
delivers character-centric movie audio descriptions. It includes: (i) a
Character Perception Module(CPM) for tracking character regions and linking
them to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues
from prior ADs and subtitles via learnable soft prompts; and (iii) a Focused
Caption Module(FCM) that generates narrations enriched with plot-relevant
details and named characters. To overcome limitations in character
identification, we also introduce an automated pipeline for building character
query banks. FocusedAD achieves state-of-the-art performance on multiple
benchmarks, including strong zero-shot results on MAD-eval-Named and our newly
proposed Cinepile-AD dataset. Code and data will be released at
https://github.com/Thorin215/FocusedAD .

</details>


### [297] [ForgetMe: Evaluating Selective Forgetting in Generative Models](https://arxiv.org/pdf/2504.12574)
*Zhenyu Yu, Mohd Yamani Inda Idris, Pei Wang*

Main category: cs.CV

TL;DR: The paper proposes an Automatic Dataset Creation Framework (ForgetMe dataset) and an Entangled metric for evaluating selective unlearning in diffusion models, addressing privacy concerns.


<details>
  <summary>Details</summary>
Motivation: The challenge of removing sensitive information from diffusion models while preserving non-sensitive regions due to their high-dimensional and complex nature.

Method: Uses prompt-based layered editing and training-free local feature removal to create the ForgetMe dataset, with LoRA fine-tuning on Stable Diffusion for selective unlearning.

Result: The ForgetMe dataset and Entangled metric effectively benchmark selective unlearning, validated on diverse real and synthetic datasets.

Conclusion: Provides a scalable solution for privacy-preserving generative AI, advancing selective unlearning in diffusion models.

Abstract: The widespread adoption of diffusion models in image generation has increased
the demand for privacy-compliant unlearning. However, due to the
high-dimensional nature and complex feature representations of diffusion
models, achieving selective unlearning remains challenging, as existing methods
struggle to remove sensitive information while preserving the consistency of
non-sensitive regions. To address this, we propose an Automatic Dataset
Creation Framework based on prompt-based layered editing and training-free
local feature removal, constructing the ForgetMe dataset and introducing the
Entangled evaluation metric. The Entangled metric quantifies unlearning
effectiveness by assessing the similarity and consistency between the target
and background regions and supports both paired (Entangled-D) and unpaired
(Entangled-S) image data, enabling unsupervised evaluation. The ForgetMe
dataset encompasses a diverse set of real and synthetic scenarios, including
CUB-200-2011 (Birds), Stanford-Dogs, ImageNet, and a synthetic cat dataset. We
apply LoRA fine-tuning on Stable Diffusion to achieve selective unlearning on
this dataset and validate the effectiveness of both the ForgetMe dataset and
the Entangled metric, establishing them as benchmarks for selective unlearning.
Our work provides a scalable and adaptable solution for advancing
privacy-preserving generative AI.

</details>


### [298] [Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene Understanding](https://arxiv.org/pdf/2504.13580)
*Yuchen Rao, Stefan Ainetter, Sinisa Stekovic, Vincent Lepetit, Friedrich Fraundorfer*

Main category: cs.CV

TL;DR: Using synthetic CAD models for automatic 3D annotations improves deep learning model performance and reduces annotation costs.


<details>
  <summary>Details</summary>
Motivation: High-level 3D scene understanding is crucial but hindered by the difficulty of generating accurate 3D annotations.

Method: Employed a pipeline to automatically annotate ScanNet++ v1 dataset with CAD models and 9D poses, using synthetic data as ground truth.

Result: Models trained on automatic annotations outperform those using manual annotations in tasks like point cloud completion and CAD model retrieval.

Conclusion: Automatic 3D annotations are viable and superior, offering cost-effective solutions for 3D scene understanding.

Abstract: High-level 3D scene understanding is essential in many applications. However,
the challenges of generating accurate 3D annotations make development of deep
learning models difficult. We turn to recent advancements in automatic
retrieval of synthetic CAD models, and show that data generated by such methods
can be used as high-quality ground truth for training supervised deep learning
models. More exactly, we employ a pipeline akin to the one previously used to
automatically annotate objects in ScanNet scenes with their 9D poses and CAD
models. This time, we apply it to the recent ScanNet++ v1 dataset, which
previously lacked such annotations. Our findings demonstrate that it is not
only possible to train deep learning models on these automatically-obtained
annotations but that the resulting models outperform those trained on manually
annotated data. We validate this on two distinct tasks: point cloud completion
and single-view CAD model retrieval and alignment. Our results underscore the
potential of automatic 3D annotations to enhance model performance while
significantly reducing annotation costs. To support future research in 3D scene
understanding, we will release our annotations, which we call SCANnotate++,
along with our trained models.

</details>


### [299] [Compile Scene Graphs with Reinforcement Learning](https://arxiv.org/pdf/2504.13617)
*Zuyao Chen, Jinlin Wu, Zhen Lei, Marc Pollefeys, Chang Wen Chen*

Main category: cs.CV

TL;DR: R1-SGG is a multimodal LLM trained for end-to-end scene graph generation, combining supervised fine-tuning and reinforcement learning with graph-centric rewards.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored use of LLMs for structured visual representation extraction, specifically scene graphs, which require accurate object and relationship triplet generation.

Method: Initial supervised fine-tuning (SFT) on scene graph data, followed by reinforcement learning (RL) with designed graph-centric rewards (Hard Recall, Hard Recall+Relax, Soft Recall, and format consistency).

Result: R1-SGG reduces failure rates and outperforms traditional SGG models and existing M-LLMs in Recall and mean Recall on VG150 and PSG benchmarks.

Conclusion: R1-SGG demonstrates the effectiveness of combining SFT and RL for structured visual representation tasks, achieving strong performance in scene graph generation.

Abstract: Next-token prediction is the fundamental principle for training large
language models (LLMs), and reinforcement learning (RL) further enhances their
reasoning performance. As an effective way to model language, image, video, and
other modalities, the use of LLMs for end-to-end extraction of structured
visual representations, such as scene graphs, remains underexplored. It
requires the model to accurately produce a set of objects and relationship
triplets, rather than generating text token by token. To achieve this, we
introduce R1-SGG, a multimodal LLM (M-LLM) initially trained via supervised
fine-tuning (SFT) on the scene graph dataset and subsequently refined using
reinforcement learning to enhance its ability to generate scene graphs in an
end-to-end manner. The SFT follows a conventional prompt-response paradigm,
while RL requires the design of effective reward signals. We design a set of
graph-centric rewards, including three recall-based variants -- Hard Recall,
Hard Recall+Relax, and Soft Recall -- which evaluate semantic and spatial
alignment between predictions and ground truth at the object and relation
levels. A format consistency reward further ensures that outputs follow the
expected structural schema. Extensive experiments on the VG150 and PSG
benchmarks show that R1-SGG substantially reduces failure rates and achieves
strong performance in Recall and mean Recall, surpassing traditional SGG models
and existing multimodal language models. Our code is available at
https://github.com/gpt4vision/R1-SGG

</details>


### [300] [DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs](https://arxiv.org/pdf/2504.17040)
*Zhenhailong Wang, Senthil Purushwalkam, Caiming Xiong, Silvio Savarese, Heng Ji, Ran Xu*

Main category: cs.CV

TL;DR: DyMU is a training-free framework that dynamically reduces computational costs in vision-language models by merging and reconstructing tokens, maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Address inefficiency of fixed-length outputs in vision transformers and reduce computational burden without fine-tuning.

Method: Uses Dynamic Token Merging (DToMe) to merge similar visual tokens and Virtual Token Unmerging (VTU) to simulate full-sequence attention.

Result: Reduces visual token count by 32%-85% while matching full-length model performance.

Conclusion: DyMU is efficient, adaptable, and applicable to state-of-the-art VLM architectures without training.

Abstract: We present DyMU, an efficient, training-free framework that dynamically
reduces the computational burden of vision-language models (VLMs) while
maintaining high task performance. Our approach comprises two key components.
First, Dynamic Token Merging (DToMe) reduces the number of visual token
embeddings by merging similar tokens based on image complexity, addressing the
inherent inefficiency of fixed-length outputs in vision transformers. Second,
Virtual Token Unmerging (VTU) simulates the expected token sequence for large
language models (LLMs) by efficiently reconstructing the attention dynamics of
a full sequence, thus preserving the downstream performance without additional
fine-tuning. Unlike previous approaches, our method dynamically adapts token
compression to the content of the image and operates completely training-free,
making it readily applicable to most state-of-the-art VLM architectures.
Extensive experiments on image and video understanding tasks demonstrate that
DyMU can reduce the average visual token count by 32%-85% while achieving
comparable performance to full-length models across diverse VLM architectures,
including the recently popularized AnyRes-based visual encoders. Furthermore,
through qualitative analyses, we demonstrate that DToMe effectively adapts
token reduction based on image complexity and, unlike existing systems,
provides users more control over computational costs. Project page:
https://mikewangwzhl.github.io/dymu/.

</details>


### [301] [TableCenterNet: A one-stage network for table structure recognition](https://arxiv.org/pdf/2504.17522)
*Anyi Xiao, Cihui Yang*

Main category: cs.CV

TL;DR: TableCenterNet is a one-stage end-to-end network for table structure recognition, unifying spatial and logical structure prediction into parallel regression, outperforming existing methods in adaptability, robustness, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for table structure recognition are either multi-stage (time-consuming) or rely on complex post-processing, struggling with adaptability and efficiency.

Method: Proposes TableCenterNet, a one-stage network with shared feature extraction and task-specific decoding to predict table structures in parallel.

Result: Achieves state-of-the-art performance on the TableGraph-24k dataset, demonstrating cross-scenario adaptability and computational efficiency.

Conclusion: TableCenterNet offers a simpler, faster, and more effective solution for table structure recognition compared to existing approaches.

Abstract: Table structure recognition aims to parse tables in unstructured data into
machine-understandable formats. Recent methods address this problem through a
two-stage process or optimized one-stage approaches. However, these methods
either require multiple networks to be serially trained and perform more
time-consuming sequential decoding, or rely on complex post-processing
algorithms to parse the logical structure of tables. They struggle to balance
cross-scenario adaptability, robustness, and computational efficiency. In this
paper, we propose a one-stage end-to-end table structure parsing network called
TableCenterNet. This network unifies the prediction of table spatial and
logical structure into a parallel regression task for the first time, and
implicitly learns the spatial-logical location mapping laws of cells through a
synergistic architecture of shared feature extraction layers and task-specific
decoding. Compared with two-stage methods, our method is easier to train and
faster to infer. Experiments on benchmark datasets show that TableCenterNet can
effectively parse table structures in diverse scenarios and achieve
state-of-the-art performance on the TableGraph-24k dataset. Code is available
at https://github.com/dreamy-xay/TableCenterNet.

</details>


### [302] [Spike Imaging Velocimetry: Dense Motion Estimation of Fluids Using Spike Cameras](https://arxiv.org/pdf/2504.18864)
*Yunzhong Zhang, Bo Xiong, You Zhou, Changqing Su, Zhen Cheng, Zhaofei Yu, Xun Cao, Tiejun Huang*

Main category: cs.CV

TL;DR: The paper introduces Spike Imaging Velocimetry (SIV), a deep learning framework for Particle Image Velocimetry (PIV) using spike cameras, achieving superior performance in turbulent flow fields.


<details>
  <summary>Details</summary>
Motivation: Accurate, non-intrusive flow measurement methods are needed, and spike cameras offer potential for PIV due to their ultra-high-speed and high-dynamic-range capabilities.

Method: Proposes SIV with a Detail-Preserving Hierarchical Transform (DPHT) module and Graph Encoder (GE) for feature extraction, and introduces the PSSD dataset for validation.

Result: SIV outperforms baseline methods on the PSSD dataset, demonstrating its effectiveness in complex flow scenarios.

Conclusion: The study highlights the potential of spike cameras in PIV and provides open-sourced datasets and SIV implementation for further research.

Abstract: The need for accurate and non-intrusive flow measurement methods has led to
the widespread adoption of Particle Image Velocimetry (PIV), a powerful
diagnostic tool in fluid motion estimation. This study investigates the
tremendous potential of spike cameras (a type of ultra-high-speed,
high-dynamic-range camera) in PIV. We propose a deep learning framework, Spike
Imaging Velocimetry (SIV), designed specifically for highly turbulent and
intricate flow fields. To aggregate motion features from the spike stream while
minimizing information loss, we incorporate a Detail-Preserving Hierarchical
Transform (DPHT) module. Additionally, we introduce a Graph Encoder (GE) to
extract contextual features from highly complex fluid flows. Furthermore, we
present a spike-based PIV dataset, Particle Scenes with Spike and Displacement
(PSSD), which provides labeled data for three challenging fluid dynamics
scenarios. Our proposed method achieves superior performance compared to
existing baseline methods on PSSD. The datasets and our implementation of SIV
are open-sourced in the supplementary materials.

</details>


### [303] [Adapting In-Domain Few-Shot Segmentation to New Domains without Retraining](https://arxiv.org/pdf/2504.21414)
*Qi Fan, Kaiqi Liu, Nian Liu, Hisham Cholakkal, Rao Muhammad Anwer, Wenbin Li, Yang Gao*

Main category: cs.CV

TL;DR: Proposes Informative Structure Adaptation (ISA) for cross-domain few-shot segmentation (CD-FSS) without retraining, using domain-specific model structures and hierarchical training.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges in CD-FSS due to diverse target domains and limited support data, avoiding costly retraining.

Method: Adapts model structures using a novel Fisher score, then trains them hierarchically with support samples.

Result: Superior performance on CD-FSS benchmarks, eliminating the need for redesign or retraining.

Conclusion: ISA effectively handles domain shifts and enhances adaptation for new domains in CD-FSS.

Abstract: Cross-domain few-shot segmentation (CD-FSS) aims to segment objects of novel
classes in new domains, which is often challenging due to the diverse
characteristics of target domains and the limited availability of support data.
Most CD-FSS methods redesign and retrain in-domain FSS models using various
domain-generalization techniques, which are effective but costly to train. To
address these issues, we propose adapting informative model structures of the
well-trained FSS model for target domains by learning domain characteristics
from few-shot labeled support samples during inference, thereby eliminating the
need for retraining. Specifically, we first adaptively identify domain-specific
model structures by measuring parameter importance using a novel structure
Fisher score in a data-dependent manner. Then, we progressively train the
selected informative model structures with hierarchically constructed training
samples, progressing from fewer to more support shots. The resulting
Informative Structure Adaptation (ISA) method effectively addresses domain
shifts and equips existing well-trained in-domain FSS models with flexible
adaptation capabilities for new domains, eliminating the need to redesign or
retrain CD-FSS models on base data. Extensive experiments validate the
effectiveness of our method, demonstrating superior performance across multiple
CD-FSS benchmarks.

</details>


### [304] [GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal Diffusion Transformers](https://arxiv.org/pdf/2504.21476)
*Xinyu Li, Qi Yao, Yuanda Wang*

Main category: cs.CV

TL;DR: GarmentDiffusion is a generative model for creating precise 3D sewing patterns from multimodal inputs, improving efficiency and speed over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for sewing pattern generation are limited by input modality constraints and inefficiency.

Method: Uses a diffusion transformer to encode 3D sewing patterns into compact edge tokens, enabling faster and more efficient generation.

Result: Achieves 100x speedup over SewingGPT and state-of-the-art performance on DressCodeData and GarmentCodeData.

Conclusion: GarmentDiffusion sets a new benchmark for efficient and precise sewing pattern generation.

Abstract: Garment sewing patterns are fundamental design elements that bridge the gap
between design concepts and practical manufacturing. The generative modeling of
sewing patterns is crucial for creating diversified garments. However, existing
approaches are limited either by reliance on a single input modality or by
suboptimal generation efficiency. In this work, we present GarmentDiffusion, a
new generative model capable of producing centimeter-precise, vectorized 3D
sewing patterns from multimodal inputs (text, image, and incomplete sewing
pattern). Our method efficiently encodes 3D sewing pattern parameters into
compact edge token representations, achieving a sequence length that is 10
times shorter than that of the autoregressive SewingGPT in DressCode. By
employing a diffusion transformer, we simultaneously denoise all edge tokens
along the temporal axis, while maintaining a constant number of denoising steps
regardless of dataset-specific edge and panel statistics. With all combination
of designs of our model, the sewing pattern generation speed is accelerated by
100 times compared to SewingGPT. We achieve new state-of-the-art results on
DressCodeData, as well as on the largest sewing pattern dataset, namely
GarmentCodeData. The project website is available at
https://shenfu-research.github.io/Garment-Diffusion/.

</details>


### [305] [MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric Guidance](https://arxiv.org/pdf/2504.21497)
*Mengting Wei, Yante Li, Tuomas Varanka, Yan Jiang, Guoying Zhao*

Main category: cs.CV

TL;DR: A method for video face reenactment using a 3D face parametric model (FLAME) integrated into a latent diffusion framework, improving shape consistency and motion control.


<details>
  <summary>Details</summary>
Motivation: To enhance video-based face generation by ensuring shape consistency and precise motion control, addressing limitations in existing approaches.

Method: Uses FLAME for 3D face representation, incorporating depth, normal, and rendering maps into a latent diffusion model. A Geometric Guidance Encoder (GGE) and multi-layer feature fusion module combine appearance and motion features.

Result: Generates high-quality face animations with accurate expression and pose modeling, showing strong generalization on out-of-domain images.

Conclusion: The proposed method effectively integrates 3D face modeling with latent diffusion, achieving superior performance in face reenactment.

Abstract: In this study, we propose a method for video face reenactment that integrates
a 3D face parametric model into a latent diffusion framework, aiming to improve
shape consistency and motion control in existing video-based face generation
approaches. Our approach employs the FLAME (Faces Learned with an Articulated
Model and Expressions) model as the 3D face parametric representation,
providing a unified framework for modeling face expressions and head pose. This
not only enables precise extraction of motion features from driving videos, but
also contributes to the faithful preservation of face shape and geometry.
Specifically, we enhance the latent diffusion model with rich 3D expression and
detailed pose information by incorporating depth maps, normal maps, and
rendering maps derived from FLAME sequences. These maps serve as motion
guidance and are encoded into the denoising UNet through a specifically
designed Geometric Guidance Encoder (GGE). A multi-layer feature fusion module
with integrated self-attention mechanisms is used to combine facial appearance
and motion latent features within the spatial domain. By utilizing the 3D face
parametric model as motion guidance, our method enables parametric alignment of
face identity between the reference image and the motion captured from the
driving video. Experimental results on benchmark datasets show that our method
excels at generating high-quality face animations with precise expression and
head pose variation modeling. In addition, it demonstrates strong
generalization performance on out-of-domain images. Code is publicly available
at https://github.com/weimengting/MagicPortrait.

</details>


### [306] [AI-Assisted Decision-Making for Clinical Assessment of Auto-Segmented Contour Quality](https://arxiv.org/pdf/2505.00308)
*Biling Wang, Austen Maniscalco, Ti Bai, Siqiu Wang, Michael Dohopolski, Mu-Han Lin, Chenyang Shen, Dan Nguyen, Junzhou Huang, Steve Jiang, Xinlei Wang*

Main category: cs.CV

TL;DR: A Deep Learning-based QA method for auto-contours in radiotherapy uses Bayesian Ordinal Classification and uncertainty thresholds to reduce manual workload and improve accuracy.


<details>
  <summary>Details</summary>
Motivation: To enhance efficiency in Online Adaptive Radiotherapy by providing a reliable QA method for auto-contours without heavy reliance on ground truth or extensive labeling.

Method: Developed a Bayesian Ordinal Classification model to classify contour quality and quantify uncertainty, validated under three data scenarios (no labels, limited labels, extensive labels).

Result: Achieved over 90% accuracy with minimal manual labels (30) and calibration (34 subjects), accurately predicting 93% of auto-contour qualities in 98% of cases.

Conclusion: The method improves OART workflow by reducing manual reviews, enabling faster decisions, and ensuring safer radiotherapy through uncertainty quantification.

Abstract: Purpose: This study presents a Deep Learning (DL)-based quality assessment
(QA) approach for evaluating auto-generated contours (auto-contours) in
radiotherapy, with emphasis on Online Adaptive Radiotherapy (OART). Leveraging
Bayesian Ordinal Classification (BOC) and calibrated uncertainty thresholds,
the method enables confident QA predictions without relying on ground truth
contours or extensive manual labeling. Methods: We developed a BOC model to
classify auto-contour quality and quantify prediction uncertainty. A
calibration step was used to optimize uncertainty thresholds that meet clinical
accuracy needs. The method was validated under three data scenarios: no manual
labels, limited labels, and extensive labels. For rectum contours in prostate
cancer, we applied geometric surrogate labels when manual labels were absent,
transfer learning when limited, and direct supervision when ample labels were
available. Results: The BOC model delivered robust performance across all
scenarios. Fine-tuning with just 30 manual labels and calibrating with 34
subjects yielded over 90% accuracy on test data. Using the calibrated
threshold, over 93% of the auto-contours' qualities were accurately predicted
in over 98% of cases, reducing unnecessary manual reviews and highlighting
cases needing correction. Conclusion: The proposed QA model enhances contouring
efficiency in OART by reducing manual workload and enabling fast, informed
clinical decisions. Through uncertainty quantification, it ensures safer, more
reliable radiotherapy workflows.

</details>


### [307] [Image Recognition with Online Lightweight Vision Transformer: A Survey](https://arxiv.org/pdf/2505.03113)
*Zherui Zhang, Rongtao Xu, Jie Zhou, Changwei Wang, Xingtian Pei, Wenhao Xu, Jiguang Zhang, Li Guo, Longxiang Gao, Wenbo Xu, Shibiao Xu*

Main category: cs.CV

TL;DR: A survey of strategies for creating lightweight vision transformers for image recognition, focusing on efficiency, dynamic networks, and knowledge distillation, with evaluations on ImageNet-1K and future research directions.


<details>
  <summary>Details</summary>
Motivation: The Transformer architecture's success in NLP inspires its adaptation to computer vision, but it faces computational and memory challenges due to lack of inductive biases and efficiency benefits.

Method: The paper surveys three key areas: Efficient Component Design, Dynamic Network, and Knowledge Distillation, evaluating their trade-offs on ImageNet-1K.

Result: Analysis highlights advantages, disadvantages, and flexibility of each strategy in terms of precision, parameters, and throughput.

Conclusion: Future research directions and challenges in lightweighting vision transformers are proposed to guide further exploration.

Abstract: The Transformer architecture has achieved significant success in natural
language processing, motivating its adaptation to computer vision tasks. Unlike
convolutional neural networks, vision transformers inherently capture
long-range dependencies and enable parallel processing, yet lack inductive
biases and efficiency benefits, facing significant computational and memory
challenges that limit its real-world applicability. This paper surveys various
online strategies for generating lightweight vision transformers for image
recognition, focusing on three key areas: Efficient Component Design, Dynamic
Network, and Knowledge Distillation. We evaluate the relevant exploration for
each topic on the ImageNet-1K benchmark, analyzing trade-offs among precision,
parameters, throughput, and more to highlight their respective advantages,
disadvantages, and flexibility. Finally, we propose future research directions
and potential challenges in the lightweighting of vision transformers with the
aim of inspiring further exploration and providing practical guidance for the
community. Project Page: https://github.com/ajxklo/Lightweight-VIT

</details>


### [308] [SMMT: Siamese Motion Mamba with Self-attention for Thermal Infrared Target Tracking](https://arxiv.org/pdf/2505.04088)
*Shang Zhang, Huanbin Zhang, Dali Feng, Yujie Cui, Ruoyan Xiong, Cen He*

Main category: cs.CV

TL;DR: The paper proposes a Siamese Motion Mamba Tracker (SMMT) for thermal infrared (TIR) object tracking, addressing challenges like occlusion and motion blur with bidirectional state-space modeling and self-attention.


<details>
  <summary>Details</summary>
Motivation: TIR tracking faces issues like occlusion, motion blur, and clutter, degrading tracker performance.

Method: SMMT integrates bidirectional state-space modeling and self-attention in a Siamese architecture, with weight-sharing layers and a motion edge-aware loss.

Result: SMMT outperforms on benchmarks LSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR2017.

Conclusion: SMMT is effective for TIR tracking, improving accuracy and handling motion blur.

Abstract: Thermal infrared (TIR) object tracking often suffers from challenges such as
target occlusion, motion blur, and background clutter, which significantly
degrade the performance of trackers. To address these issues, this paper
pro-poses a novel Siamese Motion Mamba Tracker (SMMT), which integrates a
bidirectional state-space model and a self-attention mechanism. Specifically,
we introduce the Motion Mamba module into the Siamese architecture to ex-tract
motion features and recover overlooked edge details using bidirectional
modeling and self-attention. We propose a Siamese parameter-sharing strate-gy
that allows certain convolutional layers to share weights. This approach
reduces computational redundancy while preserving strong feature
represen-tation. In addition, we design a motion edge-aware regression loss to
improve tracking accuracy, especially for motion-blurred targets. Extensive
experi-ments are conducted on four TIR tracking benchmarks, including
LSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR 2017. The results show that SMMT
achieves superior performance in TIR target tracking.

</details>


### [309] [MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection](https://arxiv.org/pdf/2505.04594)
*Zhihao Zhang, Abhinav Kumar, Girish Chandar Ganesan, Xiaoming Liu*

Main category: cs.CV

TL;DR: MonoCoP improves monocular 3D object detection by sequentially predicting 3D attributes using a Chain-of-Prediction (CoP) approach, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook the inter-correlation of 3D attributes, limiting accuracy. MonoCoP addresses this by conditioning predictions on prior attributes.

Method: Uses AttributeNet for attribute-specific features, constructs a feature propagation chain, and employs residual connections to aggregate features.

Result: Achieves SoTA on KITTI and outperforms methods on Waymo and nuScenes.

Conclusion: MonoCoP's sequential and conditional prediction approach enhances accuracy and stability in monocular 3D object detection.

Abstract: Accurately predicting 3D attributes is crucial for monocular 3D object
detection (Mono3D), with depth estimation posing the greatest challenge due to
the inherent ambiguity in mapping 2D images to 3D space. While existing methods
leverage multiple depth cues (e.g., estimating depth uncertainty, modeling
depth error) to improve depth accuracy, they overlook that accurate depth
prediction requires conditioning on other 3D attributes, as these attributes
are intrinsically inter-correlated through the 3D to 2D projection, which
ultimately limits overall accuracy and stability. Inspired by Chain-of-Thought
(CoT) in large language models (LLMs), this paper proposes MonoCoP, which
leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and
conditionally via three key designs. First, it employs a lightweight
AttributeNet (AN) for each 3D attribute to learn attribute-specific features.
Next, MonoCoP constructs an explicit chain to propagate these learned features
from one attribute to the next. Finally, MonoCoP uses a residual connection to
aggregate features for each attribute along the chain, ensuring that later
attribute predictions are conditioned on all previously processed attributes
without forgetting the features of earlier ones. Experimental results show that
our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI
leaderboard without requiring additional data and further surpasses existing
methods on the Waymo and nuScenes frontal datasets.

</details>


### [310] [Federated Deconfounding and Debiasing Learning for Out-of-Distribution Generalization](https://arxiv.org/pdf/2505.04979)
*Zhuang Qi, Sijin Zhou, Lei Meng, Han Hu, Han Yu, Xiangxu Meng*

Main category: cs.CV

TL;DR: FedDDL addresses attribute bias in federated learning by deconfounding and debiasing, improving model focus on main objects and boosting accuracy.


<details>
  <summary>Details</summary>
Motivation: Attribute bias in FL causes inconsistent local model optimization due to non-causal associations, degrading performance. Existing methods lack comprehensive inference path analysis and are limited by confounding factors.

Method: FedDDL constructs a causal graph for inference analysis, uses backdoor adjustment, and includes intra-client deconfounding (decoupling background/objects) and inter-client debiasing (causal prototypes) modules.

Result: FedDDL achieves 4.5% higher Top-1 Accuracy on average over 9 state-of-the-art methods, enhancing focus on main objects in unseen data.

Conclusion: FedDDL effectively mitigates attribute bias in FL, improving model performance by addressing confounding and bias through causal analysis and regularization.

Abstract: Attribute bias in federated learning (FL) typically leads local models to
optimize inconsistently due to the learning of non-causal associations,
resulting degraded performance. Existing methods either use data augmentation
for increasing sample diversity or knowledge distillation for learning
invariant representations to address this problem. However, they lack a
comprehensive analysis of the inference paths, and the interference from
confounding factors limits their performance. To address these limitations, we
propose the \underline{Fed}erated \underline{D}econfounding and
\underline{D}ebiasing \underline{L}earning (FedDDL) method. It constructs a
structured causal graph to analyze the model inference process, and performs
backdoor adjustment to eliminate confounding paths. Specifically, we design an
intra-client deconfounding learning module for computer vision tasks to
decouple background and objects, generating counterfactual samples that
establish a connection between the background and any label, which stops the
model from using the background to infer the label. Moreover, we design an
inter-client debiasing learning module to construct causal prototypes to reduce
the proportion of the background in prototype components. Notably, it bridges
the gap between heterogeneous representations via causal prototypical
regularization. Extensive experiments on 2 benchmarking datasets demonstrate
that \methodname{} significantly enhances the model capability to focus on main
objects in unseen data, leading to 4.5\% higher Top-1 Accuracy on average over
9 state-of-the-art existing methods.

</details>


### [311] [Driving with Context: Online Map Matching for Complex Roads Using Lane Markings and Scenario Recognition](https://arxiv.org/pdf/2505.05007)
*Xin Bi, Zhichao Li, Yuxuan Xia, Panpan Tong, Lijuan Zhang, Yang Chen, Junsheng Fu*

Main category: cs.CV

TL;DR: Proposes an online SD map matching method using HMM with multiple probability factors for accurate navigation in complex road networks, validated by road tests in Europe and China.


<details>
  <summary>Details</summary>
Motivation: Current online map matching methods struggle with accuracy in complex road networks, especially multilevel roads, necessitating a more robust solution.

Method: Uses HMM with lane markings (via multi-lane tracking and ICP registration) and scenario recognition to design probability factors for improved map matching.

Result: Achieves high F1 scores (98.04% and 94.60%) on test datasets, outperforming existing methods in multilevel road areas.

Conclusion: The proposed method significantly enhances online map matching accuracy, particularly in complex road networks, as demonstrated by experimental results.

Abstract: Accurate online map matching is fundamental to vehicle navigation and the
activation of intelligent driving functions. Current online map matching
methods are prone to errors in complex road networks, especially in multilevel
road area. To address this challenge, we propose an online Standard Definition
(SD) map matching method by constructing a Hidden Markov Model (HMM) with
multiple probability factors. Our proposed method can achieve accurate map
matching even in complex road networks by carefully leveraging lane markings
and scenario recognition in the designing of the probability factors. First,
the lane markings are generated by a multi-lane tracking method and associated
with the SD map using HMM to build an enriched SD map. In areas covered by the
enriched SD map, the vehicle can re-localize itself by performing Iterative
Closest Point (ICP) registration for the lane markings. Then, the probability
factor accounting for the lane marking detection can be obtained using the
association probability between adjacent lanes and roads. Second, the driving
scenario recognition model is applied to generate the emission probability
factor of scenario recognition, which improves the performance of map matching
on elevated roads and ordinary urban roads underneath them. We validate our
method through extensive road tests in Europe and China, and the experimental
results show that our proposed method effectively improves the online map
matching accuracy as compared to other existing methods, especially in
multilevel road area. Specifically, the experiments show that our proposed
method achieves $F_1$ scores of 98.04% and 94.60% on the Zenseact Open Dataset
and test data of multilevel road areas in Shanghai respectively, significantly
outperforming benchmark methods. The implementation is available at
https://github.com/TRV-Lab/LMSR-OMM.

</details>


### [312] [PIDiff: Image Customization for Personalized Identities with Diffusion Models](https://arxiv.org/pdf/2505.05081)
*Jinyu Gu, Haipeng Liu, Meng Wang, Yang Wang*

Main category: cs.CV

TL;DR: PIDiff is a fine-tuning-based diffusion model for personalized text-to-image generation, using W+ space and identity-tailored fine-tuning to avoid semantic entanglement and improve identity feature extraction.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to disentangle identity and background information, leading to loss of key identity characteristics and reduced diversity in generated images.

Method: PIDiff leverages W+ space for multi-level identity feature extraction and employs an identity-tailored fine-tuning strategy to avoid semantic interference. It also uses a cross-attention block and parameter optimization for accurate localization.

Result: PIDiff achieves accurate identity feature extraction and localization while preserving generation capability for in-the-wild images. Style editing is also possible.

Conclusion: PIDiff effectively addresses the issue of semantic entanglement in personalized text-to-image generation, validating its superiority through experimental results.

Abstract: Text-to-image generation for personalized identities aims at incorporating
the specific identity into images using a text prompt and an identity image.
Based on the powerful generative capabilities of DDPMs, many previous works
adopt additional prompts, such as text embeddings and CLIP image embeddings, to
represent the identity information, while they fail to disentangle the identity
information and background information. As a result, the generated images not
only lose key identity characteristics but also suffer from significantly
reduced diversity. To address this issue, previous works have combined the W+
space from StyleGAN with diffusion models, leveraging this space to provide a
more accurate and comprehensive representation of identity features through
multi-level feature extraction. However, the entanglement of identity and
background information in in-the-wild images during training prevents accurate
identity localization, resulting in severe semantic interference between
identity and background. In this paper, we propose a novel fine-tuning-based
diffusion model for personalized identities text-to-image generation, named
PIDiff, which leverages the W+ space and an identity-tailored fine-tuning
strategy to avoid semantic entanglement and achieves accurate feature
extraction and localization. Style editing can also be achieved by PIDiff
through preserving the characteristics of identity features in the W+ space,
which vary from coarse to fine. Through the combination of the proposed
cross-attention block and parameter optimization strategy, PIDiff preserves the
identity information and maintains the generation capability for in-the-wild
images of the pre-trained model during inference. Our experimental results
validate the effectiveness of our method in this task.

</details>


### [313] [MDE-Edit: Masked Dual-Editing for Multi-Object Image Editing via Diffusion Models](https://arxiv.org/pdf/2505.05101)
*Hongyang Zhu, Haipeng Liu, Bo Fu, Yang Wang*

Main category: cs.CV

TL;DR: MDE-Edit is a training-free method for precise multi-object editing in complex scenes, addressing attention misalignment and attribute-object mismatch via dual-loss optimization.


<details>
  <summary>Details</summary>
Motivation: Challenges in multi-object editing include inaccurate localization and attribute-object mismatch due to attention issues in existing methods.

Method: MDE-Edit optimizes noise latent features in diffusion models using Object Alignment Loss (OAL) and Color Consistency Loss (CCL) for precise edits.

Result: MDE-Edit outperforms state-of-the-art methods in editing accuracy and visual quality.

Conclusion: MDE-Edit provides a robust solution for complex multi-object image manipulation.

Abstract: Multi-object editing aims to modify multiple objects or regions in complex
scenes while preserving structural coherence. This task faces significant
challenges in scenarios involving overlapping or interacting objects: (1)
Inaccurate localization of target objects due to attention misalignment,
leading to incomplete or misplaced edits; (2) Attribute-object mismatch, where
color or texture changes fail to align with intended regions due to
cross-attention leakage, creating semantic conflicts (\textit{e.g.}, color
bleeding into non-target areas). Existing methods struggle with these
challenges: approaches relying on global cross-attention mechanisms suffer from
attention dilution and spatial interference between objects, while mask-based
methods fail to bind attributes to geometrically accurate regions due to
feature entanglement in multi-object scenarios. To address these limitations,
we propose a training-free, inference-stage optimization approach that enables
precise localized image manipulation in complex multi-object scenes, named
MDE-Edit. MDE-Edit optimizes the noise latent feature in diffusion models via
two key losses: Object Alignment Loss (OAL) aligns multi-layer cross-attention
with segmentation masks for precise object positioning, and Color Consistency
Loss (CCL) amplifies target attribute attention within masks while suppressing
leakage to adjacent regions. This dual-loss design ensures localized and
coherent multi-object edits. Extensive experiments demonstrate that MDE-Edit
outperforms state-of-the-art methods in editing accuracy and visual quality,
offering a robust solution for complex multi-object image manipulation tasks.

</details>


### [314] [Flow-GRPO: Training Flow Matching Models via Online RL](https://arxiv.org/pdf/2505.05470)
*Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, Wanli Ouyang*

Main category: cs.CV

TL;DR: Flow-GRPO integrates RL into flow matching models via ODE-to-SDE conversion and Denoising Reduction, improving efficiency and performance in text-to-image tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance flow matching models by incorporating RL for better exploration and sampling efficiency without compromising performance.

Method: Uses ODE-to-SDE conversion for RL exploration and Denoising Reduction to improve training efficiency.

Result: Achieves significant accuracy improvements in text-to-image tasks (e.g., 63% to 95% in GenEval) and better human preference alignment.

Conclusion: Flow-GRPO effectively combines RL with flow matching, delivering high performance without reward hacking or quality loss.

Abstract: We propose Flow-GRPO, the first method integrating online reinforcement
learning (RL) into flow matching models. Our approach uses two key strategies:
(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary
Differential Equation (ODE) into an equivalent Stochastic Differential Equation
(SDE) that matches the original model's marginal distribution at all timesteps,
enabling statistical sampling for RL exploration; and (2) a Denoising Reduction
strategy that reduces training denoising steps while retaining the original
inference timestep number, significantly improving sampling efficiency without
performance degradation. Empirically, Flow-GRPO is effective across multiple
text-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly
perfect object counts, spatial relations, and fine-grained attributes, boosting
GenEval accuracy from 63% to 95%. In visual text rendering, its accuracy
improves from 59% to 92%, significantly enhancing text generation. Flow-GRPO
also achieves substantial gains in human preference alignment. Notably, very
little reward hacking occurred, meaning rewards did not increase at the cost of
appreciable image quality or diversity degradation.

</details>


### [315] [GeomHair: Reconstruction of Hair Strands from Colorless 3D Scans](https://arxiv.org/pdf/2505.05376)
*Rachmadio Noval Lazuardi, Artem Sevastopolsky, Egor Zakharov, Matthias Niessner, Vanessa Sklyarova*

Main category: cs.CV

TL;DR: A novel method reconstructs hair strands from colorless 3D scans using multi-modal orientation extraction, avoiding RGB reliance and leveraging a diffusion prior.


<details>
  <summary>Details</summary>
Motivation: Accurate hair strand reconstruction is vital for digital avatars, animation, and AR/VR, but existing RGB-based methods struggle with complex hairstyles and environmental sensitivity.

Method: Extracts sharp surface features from scans, uses a neural 2D line detector for orientation, and incorporates a diffusion prior refined with synthetic data.

Result: Accurate reconstruction of simple and intricate hairstyles without color information, validated by the Strands400 dataset.

Conclusion: The method advances hair strand reconstruction by eliminating RGB dependency and introducing a large real-world dataset for future research.

Abstract: We propose a novel method that reconstructs hair strands directly from
colorless 3D scans by leveraging multi-modal hair orientation extraction. Hair
strand reconstruction is a fundamental problem in computer vision and graphics
that can be used for high-fidelity digital avatar synthesis, animation, and
AR/VR applications. However, accurately recovering hair strands from raw scan
data remains challenging due to human hair's complex and fine-grained
structure. Existing methods typically rely on RGB captures, which can be
sensitive to the environment and can be a challenging domain for extracting the
orientation of guiding strands, especially in the case of challenging
hairstyles. To reconstruct the hair purely from the observed geometry, our
method finds sharp surface features directly on the scan and estimates strand
orientation through a neural 2D line detector applied to the renderings of scan
shading. Additionally, we incorporate a diffusion prior trained on a diverse
set of synthetic hair scans, refined with an improved noise schedule, and
adapted to the reconstructed contents via a scan-specific text prompt. We
demonstrate that this combination of supervision signals enables accurate
reconstruction of both simple and intricate hairstyles without relying on color
information. To facilitate further research, we introduce Strands400, the
largest publicly available dataset of hair strands with detailed surface
geometry extracted from real-world data, which contains reconstructed hair
strands from the scans of 400 subjects.

</details>


### [316] [Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation](https://arxiv.org/pdf/2505.05472)
*Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, Weilin Huang*

Main category: cs.CV

TL;DR: Mogao is a unified framework for interleaved multi-modal generation, combining autoregressive and diffusion models with key technical improvements, achieving state-of-the-art performance in multi-modal tasks.


<details>
  <summary>Details</summary>
Motivation: Existing unified models are limited to single-modal generation conditioned on multiple modalities, prompting the need for a more flexible and powerful framework.

Method: Mogao integrates deep-fusion design, dual vision encoders, interleaved rotary position embeddings, and multi-modal classifier-free guidance, trained on a large-scale dataset for joint text and image generation.

Result: Mogao excels in multi-modal understanding, text-to-image generation, and produces high-quality interleaved outputs, with emergent zero-shot capabilities.

Conclusion: Mogao serves as a practical omni-modal foundation model, advancing the development of unified multi-modal systems.

Abstract: Recent progress in unified models for image understanding and generation has
been impressive, yet most approaches remain limited to single-modal generation
conditioned on multiple modalities. In this paper, we present Mogao, a unified
framework that advances this paradigm by enabling interleaved multi-modal
generation through a causal approach. Mogao integrates a set of key technical
improvements in architecture design, including a deep-fusion design, dual
vision encoders, interleaved rotary position embeddings, and multi-modal
classifier-free guidance, which allow it to harness the strengths of both
autoregressive models for text generation and diffusion models for high-quality
image synthesis. These practical improvements also make Mogao particularly
effective to process interleaved sequences of text and images arbitrarily. To
further unlock the potential of unified models, we introduce an efficient
training strategy on a large-scale, in-house dataset specifically curated for
joint text and image generation. Extensive experiments show that Mogao not only
achieves state-of-the-art performance in multi-modal understanding and
text-to-image generation, but also excels in producing high-quality, coherent
interleaved outputs. Its emergent capabilities in zero-shot image editing and
compositional generation highlight Mogao as a practical omni-modal foundation
model, paving the way for future development and scaling the unified
multi-modal systems.

</details>


### [317] [Prompt to Polyp: Medical Text-Conditioned Image Synthesis with Diffusion Models](https://arxiv.org/pdf/2505.05573)
*Mikhail Chaichuk, Sushant Gautam, Steven Hicks, Elena Tutubalina*

Main category: cs.CV

TL;DR: The paper compares text-to-image synthesis methods for medical images, introducing MSDM, an optimized model, and finds it balances quality and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing data scarcity in healthcare AI while preserving privacy by generating realistic medical images from text descriptions.

Method: Compares fine-tuning large pre-trained models (FLUX, Kandinsky) with training compact domain-specific models (MSDM), which integrates a clinical text encoder, variational autoencoder, and cross-attention mechanisms.

Result: Large models achieve higher fidelity, but MSDM offers comparable quality with lower computational costs, validated on colonoscopy and radiology datasets.

Conclusion: MSDM is a viable alternative to large models for medical image synthesis, balancing performance and efficiency.

Abstract: The generation of realistic medical images from text descriptions has
significant potential to address data scarcity challenges in healthcare AI
while preserving patient privacy. This paper presents a comprehensive study of
text-to-image synthesis in the medical domain, comparing two distinct
approaches: (1) fine-tuning large pre-trained latent diffusion models and (2)
training small, domain-specific models. We introduce a novel model named MSDM,
an optimized architecture based on Stable Diffusion that integrates a clinical
text encoder, variational autoencoder, and cross-attention mechanisms to better
align medical text prompts with generated images. Our study compares two
approaches: fine-tuning large pre-trained models (FLUX, Kandinsky) versus
training compact domain-specific models (MSDM). Evaluation across colonoscopy
(MedVQA-GI) and radiology (ROCOv2) datasets reveals that while large models
achieve higher fidelity, our optimized MSDM delivers comparable quality with
lower computational costs. Quantitative metrics and qualitative evaluations by
medical experts reveal strengths and limitations of each approach.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [318] [BedreFlyt: Improving Patient Flows through Hospital Wards with Digital Twins](https://arxiv.org/pdf/2505.06287)
*Riccardo Sieve, Paul Kobialka, Laura Slaughter, Rudolf Schlatte, Einar Broch Johnsen, Silvia Lizeth Tapia Tarifa*

Main category: cs.AI

TL;DR: The paper discusses a digital twin framework for hospital resource planning, combining formal models, ontologies, and SMT solvers to optimize short-term and long-term decision-making.


<details>
  <summary>Details</summary>
Motivation: To enhance resource planning in hospitals by leveraging digital twins for scenario exploration and optimization.

Method: Uses executable formal models, ontologies for knowledge representation, and SMT solvers to model and solve optimization problems for patient hospitalization.

Result: Proposes a digital twin architecture that supports both short-term decision-making and long-term strategic planning by generating various resource scenarios.

Conclusion: The digital twin approach effectively addresses hospital resource planning challenges, demonstrating its utility through bed bay allocation in a ward.

Abstract: Digital twins are emerging as a valuable tool for short-term decision-making
as well as for long-term strategic planning across numerous domains, including
process industry, energy, space, transport, and healthcare. This paper reports
on our ongoing work on designing a digital twin to enhance resource planning,
e.g., for the in-patient ward needs in hospitals. By leveraging executable
formal models for system exploration, ontologies for knowledge representation
and an SMT solver for constraint satisfiability, our approach aims to explore
hypothetical "what-if" scenarios to improve strategic planning processes, as
well as to solve concrete, short-term decision-making tasks. Our proposed
solution uses the executable formal model to turn a stream of arriving
patients, that need to be hospitalized, into a stream of optimization problems,
e.g., capturing daily inpatient ward needs, that can be solved by SMT
techniques. The knowledge base, which formalizes domain knowledge, is used to
model the needed configuration in the digital twin, allowing the twin to
support both short-term decision-making and long-term strategic planning by
generating scenarios spanning average-case as well as worst-case resource
needs, depending on the expected treatment of patients, as well as ranging over
variations in available resources, e.g., bed distribution in different rooms.
We illustrate our digital twin architecture by considering the problem of bed
bay allocation in a hospital ward.

</details>


### [319] [A Grounded Memory System For Smart Personal Assistants](https://arxiv.org/pdf/2505.06328)
*Felix Ocker, Jörg Deigmöller, Pavel Smirnov, Julian Eggert*

Main category: cs.AI

TL;DR: A memory system for agentic AI applications combining Vision Language Models, Large Language Models, and a knowledge graph with vector embeddings for robust information extraction and retrieval.


<details>
  <summary>Details</summary>
Motivation: To address the need for a robust memory system in agentic AI applications like cognitive assistants and robotics.

Method: Combines Vision Language Models for image captioning, Large Language Models for information extraction, and a knowledge graph with vector embeddings for memory representation. Uses semantic search and graph query generation for question answering.

Result: Demonstrates a functional memory system with potential for real-world applications.

Conclusion: The proposed system effectively grounds memory in reality, showcasing promise for diverse AI applications.

Abstract: A wide variety of agentic AI applications - ranging from cognitive assistants
for dementia patients to robotics - demand a robust memory system grounded in
reality. In this paper, we propose such a memory system consisting of three
components. First, we combine Vision Language Models for image captioning and
entity disambiguation with Large Language Models for consistent information
extraction during perception. Second, the extracted information is represented
in a memory consisting of a knowledge graph enhanced by vector embeddings to
efficiently manage relational information. Third, we combine semantic search
and graph query generation for question answering via Retrieval Augmented
Generation. We illustrate the system's working and potential using a real-world
example.

</details>


### [320] [Reliable Collaborative Conversational Agent System Based on LLMs and Answer Set Programming](https://arxiv.org/pdf/2505.06438)
*Yankai Zeng, Gopal Gupta*

Main category: cs.AI

TL;DR: The paper proposes a dual-agent system using Answer Set Programming (ASP) for reliable and secure task-oriented dialogue, outperforming LLM-driven bots in a fast-food drive-through scenario.


<details>
  <summary>Details</summary>
Motivation: LLM-driven AI bots are unreliable in task-oriented dialogues, and collaboration among agents is challenging due to insecure knowledge transfer.

Method: An Administrator-Assistant Dual-Agent paradigm with ASP-driven bots sharing a knowledge base and using a Collaborative Rule Set (CRS) for secure communication.

Result: The system, AutoManager, was tested in a fast-food drive-through scenario and proved more reliable than a real-world LLM-driven solution.

Conclusion: ASP-driven dual-agent systems offer a secure and reliable alternative to LLM-driven bots for task-oriented dialogues.

Abstract: As the Large-Language-Model-driven (LLM-driven) Artificial Intelligence (AI)
bots became popular, people realized their strong potential in Task-Oriented
Dialogue (TOD). However, bots relying wholly on LLMs are unreliable in their
knowledge, and whether they can finally produce a correct result for the task
is not guaranteed. The collaboration among these agents also remains a
challenge, since the necessary information to convey is unclear, and the
information transfer is by prompts -- unreliable, and malicious knowledge is
easy to inject. With the help of logic programming tools such as Answer Set
Programming (ASP), conversational agents can be built safely and reliably, and
communication among the agents made more efficient and secure. We proposed an
Administrator-Assistant Dual-Agent paradigm, where the two ASP-driven bots
share the same knowledge base and complete their tasks independently, while the
information can be passed by a Collaborative Rule Set (CRS). The knowledge and
information conveyed are encapsulated and invisible to the users, ensuring the
security of information transmission. We have constructed AutoManager, a
dual-agent system for managing the drive-through window of a fast-food
restaurant such as Taco Bell in the US. In AutoManager, the assistant bot takes
the customer's order while the administrator bot manages the menu and food
supply. We evaluated our AutoManager and compared it with the real-world Taco
Bell Drive-Thru AI Order Taker, and the results show that our method is more
reliable.

</details>


### [321] [Opening the Scope of Openness in AI](https://arxiv.org/pdf/2505.06464)
*Tamara Paris, AJung Moon, Jin Guo*

Main category: cs.AI

TL;DR: The paper critiques the current framing of openness in AI, inspired by open source software, and proposes a tailored taxonomy of openness for AI to address its unique challenges and societal impacts.


<details>
  <summary>Details</summary>
Motivation: The motivation is to challenge the assumptions of openness in AI, derived from open source software, and to develop a more nuanced understanding tailored to AI's specific needs and risks.

Method: The authors qualitatively analyze 98 concepts of openness from topic modeling to create a taxonomy of openness, which is then used to evaluate current AI openness discussions.

Result: The study results in a taxonomy of openness for AI, identifying gaps and interdisciplinary links, and advocating for a broader, more holistic view of openness.

Conclusion: The paper concludes that openness in AI should be redefined beyond open source principles, incorporating actions, system properties, and ethical objectives for a more comprehensive approach.

Abstract: The concept of openness in AI has so far been heavily inspired by the
definition and community practice of open source software. This positions
openness in AI as having positive connotations; it introduces assumptions of
certain advantages, such as collaborative innovation and transparency. However,
the practices and benefits of open source software are not fully transferable
to AI, which has its own challenges. Framing a notion of openness tailored to
AI is crucial to addressing its growing societal implications, risks, and
capabilities. We argue that considering the fundamental scope of openness in
different disciplines will broaden discussions, introduce important
perspectives, and reflect on what openness in AI should mean. Toward this goal,
we qualitatively analyze 98 concepts of openness discovered from topic
modeling, through which we develop a taxonomy of openness. Using this taxonomy
as an instrument, we situate the current discussion on AI openness, identify
gaps and highlight links with other disciplines. Our work contributes to the
recent efforts in framing openness in AI by reflecting principles and practices
of openness beyond open source software and calls for a more holistic view of
openness in terms of actions, system properties, and ethical objectives.

</details>


### [322] [KCluster: An LLM-based Clustering Approach to Knowledge Component Discovery](https://arxiv.org/pdf/2505.06469)
*Yumou Wei, Paulo Carvalho, John Stamper*

Main category: cs.AI

TL;DR: KCluster uses LLM-induced similarity metrics and clustering to automate KC model creation, outperforming expert-designed models.


<details>
  <summary>Details</summary>
Motivation: Manual KC model design is slow and unsustainable with growing question banks and Generative AI use.

Method: KCluster employs an LLM to measure question similarity and clustering to group questions into KCs.

Result: KCluster's models predict student performance better than expert-designed ones and generate descriptive KC labels.

Conclusion: KCluster automates KC discovery, reduces human effort, and offers insights for improving instruction.

Abstract: Educators evaluate student knowledge using knowledge component (KC) models
that map assessment questions to KCs. Still, designing KC models for large
question banks remains an insurmountable challenge for instructors who need to
analyze each question by hand. The growing use of Generative AI in education is
expected only to aggravate this chronic deficiency of expert-designed KC
models, as course engineers designing KCs struggle to keep up with the pace at
which questions are generated. In this work, we propose KCluster, a novel KC
discovery algorithm based on identifying clusters of congruent questions
according to a new similarity metric induced by a large language model (LLM).
We demonstrate in three datasets that an LLM can create an effective metric of
question similarity, which a clustering algorithm can use to create KC models
from questions with minimal human effort. Combining the strengths of LLM and
clustering, KCluster generates descriptive KC labels and discovers KC models
that predict student performance better than the best expert-designed models
available. In anticipation of future work, we illustrate how KCluster can
reveal insights into difficult KCs and suggest improvements to instruction.

</details>


### [323] [SmartPilot: A Multiagent CoPilot for Adaptive and Intelligent Manufacturing](https://arxiv.org/pdf/2505.06492)
*Chathurangi Shyalika, Renjith Prasad, Alaa Al Ghazo, Darssan Eswaramoorthi, Harleen Kaur, Sara Shree Muthuselvam, Amit Sheth*

Main category: cs.AI

TL;DR: SmartPilot is a neurosymbolic, multiagent CoPilot for Industry 4.0, addressing anomaly prediction, production forecasting, and domain-specific QA to enhance manufacturing decision-making.


<details>
  <summary>Details</summary>
Motivation: Current AI models lack deeper insights into anomalies and struggle with complex sensor data, leading to inefficiencies in manufacturing.

Method: SmartPilot integrates neurosymbolic reasoning and multiagent systems to process multimodal sensor data on edge devices.

Result: SmartPilot provides intelligent decision-making for anomaly prediction, production forecasting, and domain-specific QA.

Conclusion: SmartPilot bridges AI capabilities with industrial needs, driving transformative innovation in manufacturing.

Abstract: In the dynamic landscape of Industry 4.0, achieving efficiency, precision,
and adaptability is essential to optimize manufacturing operations. Industries
suffer due to supply chain disruptions caused by anomalies, which are being
detected by current AI models but leaving domain experts uncertain without
deeper insights into these anomalies. Additionally, operational inefficiencies
persist due to inaccurate production forecasts and the limited effectiveness of
traditional AI models for processing complex sensor data. Despite these
advancements, existing systems lack the seamless integration of these
capabilities needed to create a truly unified solution for enhancing production
and decision-making. We propose SmartPilot, a neurosymbolic, multiagent CoPilot
designed for advanced reasoning and contextual decision-making to address these
challenges. SmartPilot processes multimodal sensor data and is compact to
deploy on edge devices. It focuses on three key tasks: anomaly prediction,
production forecasting, and domain-specific question answering. By bridging the
gap between AI capabilities and real-world industrial needs, SmartPilot
empowers industries with intelligent decision-making and drives transformative
innovation in manufacturing. The demonstration video, datasets, and
supplementary materials are available at
https://github.com/ChathurangiShyalika/SmartPilot.

</details>


### [324] [On Definite Iterated Belief Revision with Belief Algebras](https://arxiv.org/pdf/2505.06505)
*Hua Meng, Zhiguo Long, Michael Sioutis, Zhengchun Zhou*

Main category: cs.AI

TL;DR: A novel framework for iterated belief revision using preference relations and belief algebras ensures deterministic revision results, with a practical algorithm for implementation.


<details>
  <summary>Details</summary>
Motivation: Traditional frameworks for iterated belief revision are too loose, leading to non-deterministic outcomes, which is problematic for safety-critical applications.

Method: Characterizes belief information through preference relations and represents beliefs and evidence as belief algebras. Introduces additional postulates, including an upper-bound constraint, to uniquely determine revision results.

Result: Proves that the revision result is uniquely determined given the current belief state and new evidence. Develops a practical algorithm for the revision process.

Conclusion: The proposed framework offers a predictable and principled method for belief revision, making it suitable for real-world applications.

Abstract: Traditional logic-based belief revision research focuses on designing rules
to constrain the behavior of revision operators. Frameworks have been proposed
to characterize iterated revision rules, but they are often too loose, leading
to multiple revision operators that all satisfy the rules under the same belief
condition. In many practical applications, such as safety critical ones, it is
important to specify a definite revision operator to enable agents to
iteratively revise their beliefs in a deterministic way. In this paper, we
propose a novel framework for iterated belief revision by characterizing belief
information through preference relations. Semantically, both beliefs and new
evidence are represented as belief algebras, which provide a rich and
expressive foundation for belief revision. Building on traditional revision
rules, we introduce additional postulates for revision with belief algebra,
including an upper-bound constraint on the outcomes of revision. We prove that
the revision result is uniquely determined given the current belief state and
new evidence. Furthermore, to make the framework more useful in practice, we
develop a particular algorithm for performing the proposed revision process. We
argue that this approach may offer a more predictable and principled method for
belief revision, making it suitable for real-world applications.

</details>


### [325] [Text-to-CadQuery: A New Paradigm for CAD Generation with Scalable Large Model Capabilities](https://arxiv.org/pdf/2505.06507)
*Haoyang Xie, Feng Ju*

Main category: cs.AI

TL;DR: The paper proposes generating CadQuery code directly from text using pretrained LLMs, bypassing intermediate representations and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Current methods for generative CAD require converting task-specific command sequences into CAD representations, adding complexity and limiting pretrained model use.

Method: Fine-tune pretrained LLMs on a dataset augmented with 170,000 CadQuery annotations to generate CadQuery code from text.

Result: Fine-tuned models show consistent improvements, with the best model achieving 69.3% top-1 exact match and reducing Chamfer Distance by 48.6%.

Conclusion: Directly generating CadQuery code with LLMs is effective, scalable, and outperforms existing methods.

Abstract: Computer-aided design (CAD) is fundamental to modern engineering and
manufacturing, but creating CAD models still requires expert knowledge and
specialized software. Recent advances in large language models (LLMs) open up
the possibility of generative CAD, where natural language is directly
translated into parametric 3D models. However, most existing methods generate
task-specific command sequences that pretrained models cannot directly handle.
These sequences must be converted into CAD representations such as CAD vectors
before a 3D model can be produced, which requires training models from scratch
and adds unnecessary complexity. To tackle this issue, we propose generating
CadQuery code directly from text, leveraging the strengths of pretrained LLMs
to produce 3D models without intermediate representations, using this
Python-based scripting language. Since LLMs already excel at Python generation
and spatial reasoning, fine-tuning them on Text-to-CadQuery data proves highly
effective. Given that these capabilities typically improve with scale, we
hypothesize that larger models will perform better after fine-tuning. To enable
this, we augment the Text2CAD dataset with 170,000 CadQuery annotations. We
fine-tune six open-source LLMs of varying sizes and observe consistent
improvements. Our best model achieves a top-1 exact match of 69.3%, up from
58.8%, and reduces Chamfer Distance by 48.6%. Project page:
https://github.com/Text-to-CadQuery/Text-to-CadQuery.

</details>


### [326] [A Point-Based Algorithm for Distributional Reinforcement Learning in Partially Observable Domains](https://arxiv.org/pdf/2505.06518)
*Larry Preuett III*

Main category: cs.AI

TL;DR: The paper extends Distributional Reinforcement Learning (DistRL) to Partially Observable Markov Decision Processes (POMDPs), introducing new distributional Bellman operators and a finite representation of return distributions via psi-vectors. It proposes DPBVI for risk-sensitive control.


<details>
  <summary>Details</summary>
Motivation: Address uncertainty in environment state and policy outcomes for safer algorithms in partially observable settings.

Method: Extends DistRL to POMDPs with new distributional Bellman operators and psi-vectors, proposing DPBVI for planning.

Result: Convergence proven under supremum p-Wasserstein metric; DPBVI enables risk-sensitive control.

Conclusion: The work bridges DistRL and POMDP planning, fostering robust decision-making under partial observability.

Abstract: In many real-world planning tasks, agents must tackle uncertainty about the
environment's state and variability in the outcomes of any chosen policy. We
address both forms of uncertainty as a first step toward safer algorithms in
partially observable settings. Specifically, we extend Distributional
Reinforcement Learning (DistRL)-which models the entire return distribution for
fully observable domains-to Partially Observable Markov Decision Processes
(POMDPs), allowing an agent to learn the distribution of returns for each
conditional plan. Concretely, we introduce new distributional Bellman operators
for partial observability and prove their convergence under the supremum
p-Wasserstein metric. We also propose a finite representation of these return
distributions via psi-vectors, generalizing the classical alpha-vectors in
POMDP solvers. Building on this, we develop Distributional Point-Based Value
Iteration (DPBVI), which integrates psi-vectors into a standard point-based
backup procedure-bridging DistRL and POMDP planning. By tracking return
distributions, DPBVI naturally enables risk-sensitive control in domains where
rare, high-impact events must be carefully managed. We provide source code to
foster further research in robust decision-making under partial observability.

</details>


### [327] [Online Feedback Efficient Active Target Discovery in Partially Observable Environments](https://arxiv.org/pdf/2505.06535)
*Anindya Sarkar, Binglin Ji, Yevgeniy Vorobeychik*

Main category: cs.AI

TL;DR: DiffATD is a novel method for active target discovery in costly data acquisition domains, balancing exploration-exploitation without supervised training.


<details>
  <summary>Details</summary>
Motivation: Costly data acquisition in domains like medical imaging and remote sensing necessitates efficient sampling strategies to maximize target discovery within limited budgets.

Method: DiffATD uses diffusion dynamics to maintain belief distributions over unobserved states, dynamically balancing exploration (high entropy regions) and exploitation (high-likelihood target areas) with an incrementally trained reward model.

Result: DiffATD outperforms baselines and competes with supervised methods, even without prior training, while offering interpretability.

Conclusion: DiffATD provides an efficient, interpretable solution for target discovery in partially observable environments with limited sampling budgets.

Abstract: In various scientific and engineering domains, where data acquisition is
costly, such as in medical imaging, environmental monitoring, or remote
sensing, strategic sampling from unobserved regions, guided by prior
observations, is essential to maximize target discovery within a limited
sampling budget. In this work, we introduce Diffusion-guided Active Target
Discovery (DiffATD), a novel method that leverages diffusion dynamics for
active target discovery. DiffATD maintains a belief distribution over each
unobserved state in the environment, using this distribution to dynamically
balance exploration-exploitation. Exploration reduces uncertainty by sampling
regions with the highest expected entropy, while exploitation targets areas
with the highest likelihood of discovering the target, indicated by the belief
distribution and an incrementally trained reward model designed to learn the
characteristics of the target. DiffATD enables efficient target discovery in a
partially observable environment within a fixed sampling budget, all without
relying on any prior supervised training. Furthermore, DiffATD offers
interpretability, unlike existing black-box policies that require extensive
supervised training. Through extensive experiments and ablation studies across
diverse domains, including medical imaging and remote sensing, we show that
DiffATD performs significantly better than baselines and competitively with
supervised methods that operate under full environmental observability.

</details>


### [328] [TAROT: Towards Essentially Domain-Invariant Robustness with Theoretical Justification](https://arxiv.org/pdf/2505.06580)
*Dongyoon Yang, Jihu Lee, Yongdai Kim*

Main category: cs.AI

TL;DR: The paper introduces TAROT, a robust domain adaptation algorithm, and a new divergence measure for generalization bounds, achieving superior performance on DomainNet.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for models that maintain performance across diverse and adversarial domains.

Method: Proposes TAROT, a novel algorithm leveraging a new divergence measure for robust domain adaptation.

Result: TAROT outperforms state-of-the-art methods in accuracy, robustness, and domain generalization.

Conclusion: TAROT's success on DomainNet demonstrates its broader applicability in real-world domain adaptation.

Abstract: Robust domain adaptation against adversarial attacks is a critical research
area that aims to develop models capable of maintaining consistent performance
across diverse and challenging domains. In this paper, we derive a new
generalization bound for robust risk on the target domain using a novel
divergence measure specifically designed for robust domain adaptation. Building
upon this, we propose a new algorithm named TAROT, which is designed to enhance
both domain adaptability and robustness. Through extensive experiments, TAROT
not only surpasses state-of-the-art methods in accuracy and robustness but also
significantly enhances domain generalization and scalability by effectively
learning domain-invariant features. In particular, TAROT achieves superior
performance on the challenging DomainNet dataset, demonstrating its ability to
learn domain-invariant representations that generalize well across different
domains, including unseen ones. These results highlight the broader
applicability of our approach in real-world domain adaptation scenarios.

</details>


### [329] [Exploring Multimodal Foundation AI and Expert-in-the-Loop for Sustainable Management of Wild Salmon Fisheries in Indigenous Rivers](https://arxiv.org/pdf/2505.06637)
*Chi Xu, Yili Jin, Sami Ma, Rongsheng Qian, Hao Fang, Jiangchuan Liu, Xue Liu, Edith C. H. Ngai, William I. Atlas, Katrina M. Connors, Mark A. Spoljaric*

Main category: cs.AI

TL;DR: The paper explores using AI and expert collaboration to improve wild salmon monitoring and fisheries management in the Pacific Northwest, addressing ecological and societal challenges.


<details>
  <summary>Details</summary>
Motivation: Wild salmon are vital to the North Pacific Rim, but climate change, habitat loss, and data limitations hinder effective management.

Method: The project integrates multimodal AI and expert-in-the-loop frameworks, using video and sonar for automated species identification, counting, and measurement.

Result: AI tools reduce manual effort, speed up results, and improve accuracy, validated by experts to ensure ecological relevance.

Conclusion: The interdisciplinary approach fosters ethical AI, open data, and culturally informed fisheries management.

Abstract: Wild salmon are essential to the ecological, economic, and cultural
sustainability of the North Pacific Rim. Yet climate variability, habitat loss,
and data limitations in remote ecosystems that lack basic infrastructure
support pose significant challenges to effective fisheries management. This
project explores the integration of multimodal foundation AI and
expert-in-the-loop frameworks to enhance wild salmon monitoring and sustainable
fisheries management in Indigenous rivers across Pacific Northwest. By
leveraging video and sonar-based monitoring, we develop AI-powered tools for
automated species identification, counting, and length measurement, reducing
manual effort, expediting delivery of results, and improving decision-making
accuracy. Expert validation and active learning frameworks ensure ecological
relevance while reducing annotation burdens. To address unique technical and
societal challenges, we bring together a cross-domain, interdisciplinary team
of university researchers, fisheries biologists, Indigenous stewardship
practitioners, government agencies, and conservation organizations. Through
these collaborations, our research fosters ethical AI co-development, open data
sharing, and culturally informed fisheries management.

</details>


### [330] [A Survey on Data-Driven Modeling of Human Drivers' Lane-Changing Decisions](https://arxiv.org/pdf/2505.06680)
*Linxuan Huang, Dong-Fan Xie, Li Li, Zhengbing He*

Main category: cs.AI

TL;DR: This paper surveys data-driven lane-changing decision (LCD) models, focusing on human drivers, and reviews their frameworks, challenges, and opportunities.


<details>
  <summary>Details</summary>
Motivation: Traditional LCD models oversimplify behavior and interactions, while data-driven approaches leverage machine learning for adaptive modeling in dynamic environments.

Method: The paper systematically reviews data-driven LCD models, covering data sources, preprocessing, inputs, outputs, objectives, structures, and validation methods.

Result: The survey highlights the potential of data-driven LCD models but also identifies challenges like driving safety, uncertainty, and technical framework integration.

Conclusion: Data-driven LCD models offer promising advancements but require addressing challenges to fully realize their potential in connected and autonomous vehicles.

Abstract: Lane-changing (LC) behavior, a critical yet complex driving maneuver,
significantly influences driving safety and traffic dynamics. Traditional
analytical LC decision (LCD) models, while effective in specific environments,
often oversimplify behavioral heterogeneity and complex interactions, limiting
their capacity to capture real LCD. Data-driven approaches address these gaps
by leveraging rich empirical data and machine learning to decode latent
decision-making patterns, enabling adaptive LCD modeling in dynamic
environments. In light of the rapid development of artificial intelligence and
the demand for data-driven models oriented towards connected vehicles and
autonomous vehicles, this paper presents a comprehensive survey of data-driven
LCD models, with a particular focus on human drivers LC decision-making. It
systematically reviews the modeling framework, covering data sources and
preprocessing, model inputs and outputs, objectives, structures, and validation
methods. This survey further discusses the opportunities and challenges faced
by data-driven LCD models, including driving safety, uncertainty, as well as
the integration and improvement of technical frameworks.

</details>


### [331] [Bi-level Mean Field: Dynamic Grouping for Large-Scale MARL](https://arxiv.org/pdf/2505.06706)
*Yuxuan Zheng, Yihe Zhou, Feiyang Xu, Mingli Song, Shunyu Liu*

Main category: cs.AI

TL;DR: The paper proposes a Bi-level Mean Field (BMF) method to address the curse of dimensionality in large-scale MARL by dynamically grouping agents and modeling bi-level interactions, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing Mean Field (MF) methods simplify agent interactions but fail to account for individual differences, causing aggregation noise. This limits learning efficiency in large-scale MARL.

Method: BMF uses a Variational AutoEncoder (VAE) for dynamic agent grouping and models inter- and intra-group interactions to reduce aggregation noise.

Result: Experiments show BMF outperforms state-of-the-art methods in various tasks.

Conclusion: BMF effectively captures agent diversity and improves learning efficiency in large-scale MARL, offering a promising solution to the curse of dimensionality.

Abstract: Large-scale Multi-Agent Reinforcement Learning (MARL) often suffers from the
curse of dimensionality, as the exponential growth in agent interactions
significantly increases computational complexity and impedes learning
efficiency. To mitigate this, existing efforts that rely on Mean Field (MF)
simplify the interaction landscape by approximating neighboring agents as a
single mean agent, thus reducing overall complexity to pairwise interactions.
However, these MF methods inevitably fail to account for individual
differences, leading to aggregation noise caused by inaccurate iterative
updates during MF learning. In this paper, we propose a Bi-level Mean Field
(BMF) method to capture agent diversity with dynamic grouping in large-scale
MARL, which can alleviate aggregation noise via bi-level interaction.
Specifically, BMF introduces a dynamic group assignment module, which employs a
Variational AutoEncoder (VAE) to learn the representations of agents,
facilitating their dynamic grouping over time. Furthermore, we propose a
bi-level interaction module to model both inter- and intra-group interactions
for effective neighboring aggregation. Experiments across various tasks
demonstrate that the proposed BMF yields results superior to the
state-of-the-art methods. Our code will be made publicly available.

</details>


### [332] [Integrating Expert Knowledge into Logical Programs via LLMs](https://arxiv.org/pdf/2502.12275)
*Franciszek Górski, Oskar Wysocki, Marco Valentino, Andre Freitas*

Main category: cs.AI

TL;DR: ExKLoP is a framework for evaluating how LLMs integrate expert knowledge into logical reasoning, focusing on engineering applications. It assesses syntactic fluency, logical correctness, and self-correction via feedback loops, using a dataset of 130 premises and 950 prompts. Results show strong syntactic performance but varying logical accuracy and self-improvement capabilities.


<details>
  <summary>Details</summary>
Motivation: The need to ensure LLMs effectively embed expert knowledge (e.g., operational ranges) into logical reasoning for engineering tasks like safety monitoring.

Method: ExKLoP systematically evaluates LLM-generated rules for syntactic and logical correctness, using an iterative feedback loop based on code execution. It employs a dataset of 130 engineering premises and 950 prompts for benchmarking.

Result: Most LLMs generate syntactically correct code but vary in logical rule implementation and self-improvement. Models like Llama3, Gemma3, Codestral, and QwenCoder show strong performance in translating expert knowledge.

Conclusion: ExKLoP is a robust platform for selecting effective LLMs for self-correcting systems, highlighting error types and model capabilities.

Abstract: This paper introduces ExKLoP, a novel framework designed to evaluate how
effectively Large Language Models (LLMs) integrate expert knowledge into
logical reasoning systems. This capability is especially valuable in
engineering, where expert knowledge-such as manufacturer-recommended
operational ranges-can be directly embedded into automated monitoring systems.
By mirroring expert verification steps, tasks like range checking and
constraint validation help ensure system safety and reliability. Our approach
systematically evaluates LLM-generated logical rules, assessing both syntactic
fluency and logical correctness in these critical validation tasks. We also
explore the models' capacity for self-correction via an iterative feedback loop
based on code execution outcomes. ExKLoP presents an extensible dataset
comprising 130 engineering premises, 950 prompts, and corresponding validation
points. It enables comprehensive benchmarking while allowing control over task
complexity and scalability of experiments. We leverage the synthetic data
creation methodology to conduct extensive empirical evaluation on a diverse set
of LLMs including Llama3, Gemma3, Codestral and QwenCoder. The results reveal
that most models generate nearly perfect syntactically correct code and exhibit
strong performance in translating expert knowledge into correct code. At the
same time, while most LLMs produce nearly flawless syntactic output, their
ability to correctly implement logical rules varies, as does their capacity for
self-improvement. Overall, ExKLoP serves as a robust evaluation platform that
streamlines the selection of effective models for self-correcting systems while
clearly delineating the types of errors encountered.

</details>


### [333] [Value Iteration with Guessing for Markov Chains and Markov Decision Processes](https://arxiv.org/pdf/2505.06769)
*Krishnendu Chatterjee, Mahdi JafariRaviz, Raimundo Saona, Jakub Svoboda*

Main category: cs.AI

TL;DR: The paper introduces a new approach for Value Iteration (VI) in probabilistic systems, focusing on reducing the number of Bellman updates required after preprocessing. It provides theoretical and practical improvements for both Markov chains (MCs) and Markov decision processes (MDPs).


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiency of existing VI approaches, which require exponentially many Bellman updates in the worst case, by proposing a preprocessing step and guessing values to reduce computational complexity.

Method: The method involves a polynomial-time preprocessing algorithm for MCs, enabling subexponentially many Bellman updates. For MDPs, an improved analysis of VI convergence speed is presented, along with a practical algorithm.

Result: Theoretical results show almost-linear-time preprocessing for MCs and improved VI convergence for MDPs. Experiments demonstrate significant performance gains over existing VI-based methods.

Conclusion: The work advances VI efficiency for probabilistic systems, offering both theoretical and practical improvements, validated by experimental results.

Abstract: Two standard models for probabilistic systems are Markov chains (MCs) and
Markov decision processes (MDPs). Classic objectives for such probabilistic
models for control and planning problems are reachability and stochastic
shortest path. The widely studied algorithmic approach for these problems is
the Value Iteration (VI) algorithm which iteratively applies local updates
called Bellman updates. There are many practical approaches for VI in the
literature but they all require exponentially many Bellman updates for MCs in
the worst case. A preprocessing step is an algorithm that is discrete,
graph-theoretical, and requires linear space. An important open question is
whether, after a polynomial-time preprocessing, VI can be achieved with
sub-exponentially many Bellman updates. In this work, we present a new approach
for VI based on guessing values. Our theoretical contributions are twofold.
First, for MCs, we present an almost-linear-time preprocessing algorithm after
which, along with guessing values, VI requires only subexponentially many
Bellman updates. Second, we present an improved analysis of the speed of
convergence of VI for MDPs. Finally, we present a practical algorithm for MDPs
based on our new approach. Experimental results show that our approach provides
a considerable improvement over existing VI-based approaches on several
benchmark examples from the literature.

</details>


### [334] [Control Plane as a Tool: A Scalable Design Pattern for Agentic AI Systems](https://arxiv.org/pdf/2505.06817)
*Sivasathivel Kandasamy*

Main category: cs.AI

TL;DR: The paper reviews agentic AI systems, identifies gaps in tool orchestration, and proposes a 'Control Plane as a Tool' pattern for scalable, safe, and extensible agent design.


<details>
  <summary>Details</summary>
Motivation: Agentic AI systems, often based on LLMs, show promise but face immature architectures, especially in tool orchestration at scale.

Method: The paper reviews agent types, interaction modes, and infrastructural challenges, then proposes the 'Control Plane as a Tool' design abstraction.

Result: The proposed pattern addresses scaling, safety, and extensibility challenges by encapsulating tool routing logic behind a single interface.

Conclusion: The 'Control Plane as a Tool' pattern is a reusable solution for improving agentic AI system design, particularly in tool orchestration.

Abstract: Agentic AI systems represent a new frontier in artificial intelligence, where
agents often based on large language models(LLMs) interact with tools,
environments, and other agents to accomplish tasks with a degree of autonomy.
These systems show promise across a range of domains, but their architectural
underpinnings remain immature. This paper conducts a comprehensive review of
the types of agents, their modes of interaction with the environment, and the
infrastructural and architectural challenges that emerge. We identify a gap in
how these systems manage tool orchestration at scale and propose a reusable
design abstraction: the "Control Plane as a Tool" pattern. This pattern allows
developers to expose a single tool interface to an agent while encapsulating
modular tool routing logic behind it. We position this pattern within the
broader context of agent design and argue that it addresses several key
challenges in scaling, safety, and extensibility.

</details>


### [335] [Beyond the Tragedy of the Commons: Building A Reputation System for Generative Multi-agent Systems](https://arxiv.org/pdf/2505.05029)
*Siyue Ren, Wanli Fu, Xinkun Zou, Chen Shen, Yi Cai, Chen Chu, Zhen Wang, Shuyue Hu*

Main category: cs.AI

TL;DR: RepuNet, a dual-level reputation framework, mitigates the 'tragedy of the commons' in generative multi-agent systems by modeling agent and system-level reputations, fostering cooperation and emergent behaviors.


<details>
  <summary>Details</summary>
Motivation: Address the 'tragedy of the commons' in generative multi-agent systems, where individual self-interest harms collective outcomes.

Method: Propose RepuNet, a dynamic reputation framework combining agent-level reputation dynamics and system-level network evolution, driven by direct interactions and gossip.

Result: RepuNet effectively promotes cooperation, forms cooperative clusters, isolates exploitative agents, and favors positive gossip sharing.

Conclusion: Reputation systems like RepuNet can resolve collective challenges and induce emergent behaviors in generative MASs.

Abstract: The tragedy of the commons, where individual self-interest leads to
collectively disastrous outcomes, is a pervasive challenge in human society.
Recent studies have demonstrated that similar phenomena can arise in generative
multi-agent systems (MASs). To address this challenge, this paper explores the
use of reputation systems as a remedy. We propose RepuNet, a dynamic,
dual-level reputation framework that models both agent-level reputation
dynamics and system-level network evolution. Specifically, driven by direct
interactions and indirect gossip, agents form reputations for both themselves
and their peers, and decide whether to connect or disconnect other agents for
future interactions. Through two distinct scenarios, we show that RepuNet
effectively mitigates the 'tragedy of the commons', promoting and sustaining
cooperation in generative MASs. Moreover, we find that reputation systems can
give rise to rich emergent behaviors in generative MASs, such as the formation
of cooperative clusters, the social isolation of exploitative agents, and the
preference for sharing positive gossip rather than negative ones.

</details>


### [336] [Beyond Patterns: Harnessing Causal Logic for Autonomous Driving Trajectory Prediction](https://arxiv.org/pdf/2505.06856)
*Bonan Wang, Haicheng Liao, Chengyue Wang, Bin Rao, Yanchen Guan, Guyang Yu, Jiaxun Zhang, Songning Lai, Chengzhong Xu, Zhenning Li*

Main category: cs.AI

TL;DR: A novel trajectory prediction framework for autonomous driving uses causal inference to improve robustness, generalization, and accuracy by identifying genuine causal relationships and mitigating spurious correlations.


<details>
  <summary>Details</summary>
Motivation: Traditional data-driven models for trajectory prediction often overlook causal relationships, leading to less robust and generalizable predictions.

Method: The framework decomposes the environment into spatial and temporal components, employs causal inference, and uses a progressive fusion strategy for multimodal information integration.

Result: Evaluations on five datasets (ApolloScape, nuScenes, NGSIM, HighD, MoCAD) show superior performance over SOTA methods, with improvements in RMSE and FDE.

Conclusion: Causal reasoning can significantly enhance trajectory prediction, contributing to more robust autonomous driving systems.

Abstract: Accurate trajectory prediction has long been a major challenge for autonomous
driving (AD). Traditional data-driven models predominantly rely on statistical
correlations, often overlooking the causal relationships that govern traffic
behavior. In this paper, we introduce a novel trajectory prediction framework
that leverages causal inference to enhance predictive robustness,
generalization, and accuracy. By decomposing the environment into spatial and
temporal components, our approach identifies and mitigates spurious
correlations, uncovering genuine causal relationships. We also employ a
progressive fusion strategy to integrate multimodal information, simulating
human-like reasoning processes and enabling real-time inference. Evaluations on
five real-world datasets--ApolloScape, nuScenes, NGSIM, HighD, and
MoCAD--demonstrate our model's superiority over existing state-of-the-art
(SOTA) methods, with improvements in key metrics such as RMSE and FDE. Our
findings highlight the potential of causal reasoning to transform trajectory
prediction, paving the way for robust AD systems.

</details>


### [337] [Embodied Intelligence: The Key to Unblocking Generalized Artificial Intelligence](https://arxiv.org/pdf/2505.06897)
*Jinhao Jiang, Changlin Chen, Shile Feng, Wanru Geng, Zesheng Zhou, Ni Wang, Shuai Li, Feng-Qi Cui, Erbao Dong*

Main category: cs.AI

TL;DR: This paper explores Embodied Artificial Intelligence (EAI) as a foundational approach to achieving Artificial General Intelligence (AGI), analyzing its core modules and their contributions to AGI principles.


<details>
  <summary>Details</summary>
Motivation: The lack of systematic reviews connecting EAI to AGI motivates this study, aiming to highlight EAI's role in bridging narrow AI and AGI.

Method: The paper systematically analyzes EAI's four core modules (perception, decision-making, action, feedback) and their alignment with AGI principles.

Result: Findings suggest EAI's dynamic learning and real-world interaction are crucial for advancing AGI.

Conclusion: EAI is a promising cornerstone for AGI development, with future research needed to address challenges and trends.

Abstract: The ultimate goal of artificial intelligence (AI) is to achieve Artificial
General Intelligence (AGI). Embodied Artificial Intelligence (EAI), which
involves intelligent systems with physical presence and real-time interaction
with the environment, has emerged as a key research direction in pursuit of
AGI. While advancements in deep learning, reinforcement learning, large-scale
language models, and multimodal technologies have significantly contributed to
the progress of EAI, most existing reviews focus on specific technologies or
applications. A systematic overview, particularly one that explores the direct
connection between EAI and AGI, remains scarce. This paper examines EAI as a
foundational approach to AGI, systematically analyzing its four core modules:
perception, intelligent decision-making, action, and feedback. We provide a
detailed discussion of how each module contributes to the six core principles
of AGI. Additionally, we discuss future trends, challenges, and research
directions in EAI, emphasizing its potential as a cornerstone for AGI
development. Our findings suggest that EAI's integration of dynamic learning
and real-world interaction is essential for bridging the gap between narrow AI
and AGI.

</details>


### [338] [Towards Artificial General or Personalized Intelligence? A Survey on Foundation Models for Personalized Federated Intelligence](https://arxiv.org/pdf/2505.06907)
*Yu Qiao, Huy Q. Le, Avi Deb Raha, Phuong-Nam Tran, Apurba Adhikary, Mengchun Zhang, Loc X. Nguyen, Eui-Nam Huh, Dusit Niyato, Choong Seon Hong*

Main category: cs.AI

TL;DR: The paper introduces Artificial Personalized Intelligence (API) to adapt large language models (LLMs) for user-specific needs while addressing privacy and efficiency. It proposes Personalized Federated Intelligence (PFI), combining federated learning (FL) and foundational models (FMs) for edge deployment.


<details>
  <summary>Details</summary>
Motivation: To address challenges like privacy concerns and computational demands of LLMs, enabling personalized AI while maintaining efficiency and privacy.

Method: Proposes PFI, integrating FL's privacy benefits with FMs' generalization, and explores efficient, trustworthy, and RAG-empowered PFI.

Result: Identifies opportunities and challenges for deploying FM-powered FL systems at the edge, focusing on personalization, efficiency, and privacy.

Conclusion: PFI is a key technique for API development, complementing AGI, with future research needed for practical deployment.

Abstract: The rise of large language models (LLMs), such as ChatGPT, DeepSeek, and
Grok-3, has reshaped the artificial intelligence landscape. As prominent
examples of foundational models (FMs) built on LLMs, these models exhibit
remarkable capabilities in generating human-like content, bringing us closer to
achieving artificial general intelligence (AGI). However, their large-scale
nature, sensitivity to privacy concerns, and substantial computational demands
present significant challenges to personalized customization for end users. To
bridge this gap, this paper presents the vision of artificial personalized
intelligence (API), focusing on adapting these powerful models to meet the
specific needs and preferences of users while maintaining privacy and
efficiency. Specifically, this paper proposes personalized federated
intelligence (PFI), which integrates the privacy-preserving advantages of
federated learning (FL) with the zero-shot generalization capabilities of FMs,
enabling personalized, efficient, and privacy-protective deployment at the
edge. We first review recent advances in both FL and FMs, and discuss the
potential of leveraging FMs to enhance federated systems. We then present the
key motivations behind realizing PFI and explore promising opportunities in
this space, including efficient PFI, trustworthy PFI, and PFI empowered by
retrieval-augmented generation (RAG). Finally, we outline key challenges and
future research directions for deploying FM-powered FL systems at the edge with
improved personalization, computational efficiency, and privacy guarantees.
Overall, this survey aims to lay the groundwork for the development of API as a
complement to AGI, with a particular focus on PFI as a key enabling technique.

</details>


### [339] [Causal knowledge graph analysis identifies adverse drug effects](https://arxiv.org/pdf/2505.06949)
*Sumyyah Toonsi, Paul Schofield, Robert Hoehndorf*

Main category: cs.AI

TL;DR: The paper introduces Causal Knowledge Graphs (CKGs) to bridge the gap between knowledge graphs and causal models, enabling both deductive reasoning and causal inference. Applied to biomedical data, it successfully identifies known and novel drug effects.


<details>
  <summary>Details</summary>
Motivation: To integrate the qualitative strengths of knowledge graphs with the probabilistic semantics of causal models for improved biomedical causal inference.

Method: Developed CKGs with formal causal semantics, applied to a Drug-Disease CKG (DD-CKG) for mediation analysis using UK Biobank and MIMIC-IV data.

Result: Reproduced known adverse drug reactions and identified novel candidate effects, validated by improved prediction of shared drug indications.

Conclusion: CKGs provide a scalable, knowledge-driven framework for causal inference, validated by clinical relevance.

Abstract: Knowledge graphs and structural causal models have each proven valuable for
organizing biomedical knowledge and estimating causal effects, but remain
largely disconnected: knowledge graphs encode qualitative relationships
focusing on facts and deductive reasoning without formal probabilistic
semantics, while causal models lack integration with background knowledge in
knowledge graphs and have no access to the deductive reasoning capabilities
that knowledge graphs provide. To bridge this gap, we introduce a novel
formulation of Causal Knowledge Graphs (CKGs) which extend knowledge graphs
with formal causal semantics, preserving their deductive capabilities while
enabling principled causal inference. CKGs support deconfounding via explicitly
marked causal edges and facilitate hypothesis formulation aligned with both
encoded and entailed background knowledge. We constructed a Drug-Disease CKG
(DD-CKG) integrating disease progression pathways, drug indications,
side-effects, and hierarchical disease classification to enable automated
large-scale mediation analysis. Applied to UK Biobank and MIMIC-IV cohorts, we
tested whether drugs mediate effects between indications and downstream disease
progression, adjusting for confounders inferred from the DD-CKG. Our approach
successfully reproduced known adverse drug reactions with high precision while
identifying previously undocumented significant candidate adverse effects.
Further validation through side effect similarity analysis demonstrated that
combining our predicted drug effects with established databases significantly
improves the prediction of shared drug indications, supporting the clinical
relevance of our novel findings. These results demonstrate that our methodology
provides a generalizable, knowledge-driven framework for scalable causal
inference.

</details>


### [340] [From Knowledge to Reasoning: Evaluating LLMs for Ionic Liquids Research in Chemical and Biological Engineering](https://arxiv.org/pdf/2505.06964)
*Gaurab Sarkar, Sougata Saha*

Main category: cs.AI

TL;DR: The paper evaluates LLMs' reasoning in Chemical and Biological Engineering, focusing on Ionic Liquids for carbon sequestration, and finds smaller LLMs lack domain-specific reasoning despite general knowledge.


<details>
  <summary>Details</summary>
Motivation: Assess LLM performance in niche scientific domains like CBE, where benchmarks are lacking, to understand their utility in carbon capture research.

Method: Constructed an expert-curated dataset of 5,920 examples for benchmarking LLMs in Ionic Liquids, varying linguistic and domain-specific difficulty.

Result: Smaller general-purpose LLMs show knowledge of Ionic Liquids but lack domain-specific reasoning capabilities.

Conclusion: LLMs can be leveraged for carbon capture research with Ionic Liquids, benefiting both fields and aiding carbon neutrality goals by 2050.

Abstract: Although Large Language Models (LLMs) have achieved remarkable performance in
diverse general knowledge and reasoning tasks, their utility in the scientific
domain of Chemical and Biological Engineering (CBE) is unclear. Hence, it
necessitates challenging evaluation benchmarks that can measure LLM performance
in knowledge- and reasoning-based tasks, which is lacking. As a foundational
step, we empirically measure the reasoning capabilities of LLMs in CBE. We
construct and share an expert-curated dataset of 5,920 examples for
benchmarking LLMs' reasoning capabilities in the niche domain of Ionic Liquids
(ILs) for carbon sequestration, an emergent solution to reducing global
warming. The dataset presents different difficulty levels by varying along the
dimensions of linguistic and domain-specific knowledge. Benchmarking three less
than 10B parameter open-source LLMs on the dataset suggests that while smaller
general-purpose LLMs are knowledgeable about ILs, they lack domain-specific
reasoning capabilities. Based on our results, we further discuss considerations
for leveraging LLMs for carbon capture research using ILs. Since LLMs have a
high carbon footprint, gearing them for IL research can symbiotically benefit
both fields and help reach the ambitious carbon neutrality target by 2050.
Dataset link: https://github.com/sougata-ub/llms_for_ionic_liquids

</details>


### [341] [CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging](https://arxiv.org/pdf/2505.06977)
*Wenju Sun, Qingyong Li, Yangli-ao Geng, Boyang Li*

Main category: cs.AI

TL;DR: CAT Merging is a training-free framework that trims conflict-prone components in task vectors to improve multi-task model merging, outperforming existing methods by up to 2.5% in accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing task vector accumulation methods suffer from knowledge conflicts, leading to performance degradation in multi-task model merging.

Method: CAT Merging selectively trims conflict-prone components using projection for linear weights and masking for normalization layer parameters.

Result: Experiments show CAT Merging improves accuracy by up to 2.5% (ViT-B/32) and 2.0% (ViT-L/14) over state-of-the-art methods.

Conclusion: CAT Merging effectively addresses knowledge conflicts in multi-task model merging, enhancing performance without additional training.

Abstract: Multi-task model merging offers a promising paradigm for integrating multiple
expert models into a unified model without additional training. Existing
state-of-the-art techniques, such as Task Arithmetic and its variants, merge
models by accumulating task vectors -- the parameter differences between
pretrained and finetuned models. However, task vector accumulation is often
hindered by knowledge conflicts, leading to performance degradation. To address
this challenge, we propose Conflict-Aware Task Merging (CAT Merging), a novel
training-free framework that selectively trims conflict-prone components from
the task vectors. CAT Merging introduces several parameter-specific strategies,
including projection for linear weights and masking for scaling and shifting
parameters in normalization layers. Extensive experiments on vision, language,
and vision-language tasks demonstrate that CAT Merging effectively suppresses
knowledge conflicts, achieving average accuracy improvements of up to 2.5%
(ViT-B/32) and 2.0% (ViT-L/14) over state-of-the-art methods.

</details>


### [342] [A Multi-Agent Reinforcement Learning Approach for Cooperative Air-Ground-Human Crowdsensing in Emergency Rescue](https://arxiv.org/pdf/2505.06997)
*Wenhao Lu, Zhengqiu Zhu, Yong Zhao, Yonglin Tian, Junjie Zeng, Jun Zhang, Zhong Liu, Fei-Yue Wang*

Main category: cs.AI

TL;DR: The paper introduces HECTA4ER, a multi-agent reinforcement learning algorithm for optimizing task allocation among humans, UAVs, and UGVs in emergency rescue scenarios, achieving an 18.42% higher task completion rate.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of task allocation in heterogeneous mobile crowdsensing for emergency rescue, where complex environments, limited communication, and partial observability exist.

Method: Proposes HECTA4ER, a Dec-POMDP-based algorithm with specialized modules for feature extraction, action-observation history, and a mixing network for global-local information integration.

Result: HECTA4ER outperforms baselines with an 18.42% increase in task completion rate, validated by simulations and a real-world case study.

Conclusion: HECTA4ER is effective and robust for emergency response, demonstrating strong practical potential.

Abstract: Mobile crowdsensing is evolving beyond traditional human-centric models by
integrating heterogeneous entities like unmanned aerial vehicles (UAVs) and
unmanned ground vehicles (UGVs). Optimizing task allocation among these diverse
agents is critical, particularly in challenging emergency rescue scenarios
characterized by complex environments, limited communication, and partial
observability. This paper tackles the Heterogeneous-Entity
Collaborative-Sensing Task Allocation (HECTA) problem specifically for
emergency rescue, considering humans, UAVs, and UGVs. We introduce a novel
``Hard-Cooperative'' policy where UGVs prioritize recharging low-battery UAVs,
alongside performing their sensing tasks. The primary objective is maximizing
the task completion rate (TCR) under strict time constraints. We rigorously
formulate this NP-hard problem as a decentralized partially observable Markov
decision process (Dec-POMDP) to effectively handle sequential decision-making
under uncertainty. To solve this, we propose HECTA4ER, a novel multi-agent
reinforcement learning algorithm built upon a Centralized Training with
Decentralized Execution architecture. HECTA4ER incorporates tailored designs,
including specialized modules for complex feature extraction, utilization of
action-observation history via hidden states, and a mixing network integrating
global and local information, specifically addressing the challenges of partial
observability. Furthermore, theoretical analysis confirms the algorithm's
convergence properties. Extensive simulations demonstrate that HECTA4ER
significantly outperforms baseline algorithms, achieving an average 18.42%
increase in TCR. Crucially, a real-world case study validates the algorithm's
effectiveness and robustness in dynamic sensing scenarios, highlighting its
strong potential for practical application in emergency response.

</details>


### [343] [Explainable AI the Latest Advancements and New Trends](https://arxiv.org/pdf/2505.07005)
*Bowen Long, Enjie Liu, Renxi Qiu, Yanqing Duan*

Main category: cs.AI

TL;DR: The paper surveys ethical elements and interpretability in AI, focusing on explainable AI techniques and trends, highlighting the link between explainability and meta-reasoning.


<details>
  <summary>Details</summary>
Motivation: The opacity of AI decision-making necessitates trustworthy AI, aligning societal standards with technological advancements.

Method: Conducted a survey on ethical elements and state-of-the-art interpretability research, focusing on explainable AI techniques.

Result: Identified trends in explainable AI and emphasized the connection between AI explainability and meta-reasoning.

Conclusion: Integrating explainability and meta-reasoning could advance future interpretable AI systems.

Abstract: In recent years, Artificial Intelligence technology has excelled in various
applications across all domains and fields. However, the various algorithms in
neural networks make it difficult to understand the reasons behind decisions.
For this reason, trustworthy AI techniques have started gaining popularity. The
concept of trustworthiness is cross-disciplinary; it must meet societal
standards and principles, and technology is used to fulfill these requirements.
In this paper, we first surveyed developments from various countries and
regions on the ethical elements that make AI algorithms trustworthy; and then
focused our survey on the state of the art research into the interpretability
of AI. We have conducted an intensive survey on technologies and techniques
used in making AI explainable. Finally, we identified new trends in achieving
explainable AI. In particular, we elaborate on the strong link between the
explainability of AI and the meta-reasoning of autonomous systems. The concept
of meta-reasoning is 'reason the reasoning', which coincides with the intention
and goal of explainable Al. The integration of the approaches could pave the
way for future interpretable AI systems.

</details>


### [344] [LLM-Augmented Chemical Synthesis and Design Decision Programs](https://arxiv.org/pdf/2505.07027)
*Haorui Wang, Jeff Guo, Lingkai Kong, Rampi Ramprasad, Philippe Schwaller, Yuanqi Du, Chao Zhang*

Main category: cs.AI

TL;DR: LLMs are explored for multi-step retrosynthesis planning, introducing a new encoding scheme and route-level search strategy, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Retrosynthesis is vital in organic chemistry and drug development, but current ML methods are limited by combinatorial complexity. LLMs show potential for complex chemical decision-making.

Method: An efficient encoding scheme for reaction pathways and a route-level search strategy are introduced, moving beyond step-by-step prediction.

Result: The LLM-augmented approach excels in retrosynthesis planning and extends to synthesizable molecular design.

Conclusion: LLMs can successfully navigate multi-step retrosynthesis, offering a promising solution for complex chemical tasks.

Abstract: Retrosynthesis, the process of breaking down a target molecule into simpler
precursors through a series of valid reactions, stands at the core of organic
chemistry and drug development. Although recent machine learning (ML) research
has advanced single-step retrosynthetic modeling and subsequent route searches,
these solutions remain restricted by the extensive combinatorial space of
possible pathways. Concurrently, large language models (LLMs) have exhibited
remarkable chemical knowledge, hinting at their potential to tackle complex
decision-making tasks in chemistry. In this work, we explore whether LLMs can
successfully navigate the highly constrained, multi-step retrosynthesis
planning problem. We introduce an efficient scheme for encoding reaction
pathways and present a new route-level search strategy, moving beyond the
conventional step-by-step reactant prediction. Through comprehensive
evaluations, we show that our LLM-augmented approach excels at retrosynthesis
planning and extends naturally to the broader challenge of synthesizable
molecular design.

</details>


### [345] [Efficient Fault Detection in WSN Based on PCA-Optimized Deep Neural Network Slicing Trained with GOA](https://arxiv.org/pdf/2505.07030)
*Mahmood Mohassel Feghhi, Raya Majid Alsharfa, Majid Hameed Majeed*

Main category: cs.AI

TL;DR: A hybrid PCA-GOA-DNN method for fault detection in WSNs achieves 99.72% accuracy by combining dimensionality reduction with optimized neural network training.


<details>
  <summary>Details</summary>
Motivation: Traditional fault detection methods in WSNs struggle with optimizing DNNs for high-dimensional data and nonlinear relationships, leading to slow convergence and suboptimal architectures.

Method: The study uses PCA to reduce dimensionality from 12 to 4 features, then trains a six-layer DNN optimized by GOA to enhance efficiency and accuracy.

Result: The method achieves 99.72% classification accuracy, outperforming conventional approaches with high precision and recall.

Conclusion: The PCA-GOA-DNN framework is computationally efficient and effective for large-scale WSNs, advancing fault detection in resource-constrained environments.

Abstract: Fault detection in Wireless Sensor Networks (WSNs) is crucial for reliable
data transmission and network longevity. Traditional fault detection methods
often struggle with optimizing deep neural networks (DNNs) for efficient
performance, especially in handling high-dimensional data and capturing
nonlinear relationships. Additionally, these methods typically suffer from slow
convergence and difficulty in finding optimal network architectures using
gradient-based optimization. This study proposes a novel hybrid method
combining Principal Component Analysis (PCA) with a DNN optimized by the
Grasshopper Optimization Algorithm (GOA) to address these limitations. Our
approach begins by computing eigenvalues from the original 12-dimensional
dataset and sorting them in descending order. The cumulative sum of these
values is calculated, retaining principal components until 99.5% variance is
achieved, effectively reducing dimensionality to 4 features while preserving
critical information. This compressed representation trains a six-layer DNN
where GOA optimizes the network architecture, overcoming backpropagation's
limitations in discovering nonlinear relationships. This hybrid PCA-GOA-DNN
framework compresses the data and trains a six-layer DNN that is optimized by
GOA, enhancing both training efficiency and fault detection accuracy. The
dataset used in this study is a real-world WSNs dataset developed by the
University of North Carolina, which was used to evaluate the proposed method's
performance. Extensive simulations demonstrate that our approach achieves a
remarkable 99.72% classification accuracy, with exceptional precision and
recall, outperforming conventional methods. The method is computationally
efficient, making it suitable for large-scale WSN deployments, and represents a
significant advancement in fault detection for resource-constrained WSNs.

</details>


### [346] [DialogueReason: Rule-Based RL Sparks Dialogue Reasoning in LLMs](https://arxiv.org/pdf/2505.07049)
*Yubo Shu, Zhewei Huang, Xin Wu, Chen Hu, Shuchang Zhou, Daxin Jiang*

Main category: cs.AI

TL;DR: DialogueReason introduces dialogue-based reasoning to improve diversity and coherency in reasoning models, outperforming monologue-style models on complex tasks.


<details>
  <summary>Details</summary>
Motivation: Monologue-style reasoning models limit diversity and coherency, often recycling fixed strategies or showing attention shifts. DialogueReason aims to address these limitations.

Method: Analyzes monologue reasoning patterns, introduces Compound-QA task, and develops DialogueReason with agents, environment, and interactions. Uses PPO with rule-based rewards to train LLMs.

Result: DialogueReason outperforms monologue models on MATH, AIME, and GPQA datasets, especially in complex compound questions.

Conclusion: Dialogue-based reasoning enhances interpretability, human interaction, and inspires multi-agent system design.

Abstract: We propose DialogueReason, a reasoning paradigm that uncovers the lost roles
in monologue-style reasoning models, aiming to boost diversity and coherency of
the reasoning process. Recent advances in RL-based large reasoning models have
led to impressive long CoT capabilities and high performance on math and
science benchmarks. However, these reasoning models rely mainly on
monologue-style reasoning, which often limits reasoning diversity and
coherency, frequently recycling fixed strategies or exhibiting unnecessary
shifts in attention. Our work consists of an analysis of monologue reasoning
patterns and the development of a dialogue-based reasoning approach. We first
introduce the Compound-QA task, which concatenates multiple problems into a
single prompt to assess both diversity and coherency of reasoning. Our analysis
shows that Compound-QA exposes weaknesses in monologue reasoning, evidenced by
both quantitative metrics and qualitative reasoning traces. Building on the
analysis, we propose a dialogue-based reasoning, named DialogueReason,
structured around agents, environment, and interactions. Using PPO with
rule-based rewards, we train open-source LLMs (Qwen-QWQ and Qwen-Base) to adopt
dialogue reasoning. We evaluate trained models on MATH, AIME, and GPQA
datasets, showing that the dialogue reasoning model outperforms monologue
models under more complex compound questions. Additionally, we discuss how
dialogue-based reasoning helps enhance interpretability, facilitate more
intuitive human interaction, and inspire advances in multi-agent system design.

</details>


### [347] [Exploring Gen-AI applications in building research and industry: A review](https://arxiv.org/pdf/2410.01098)
*Hanlong Wan, Jian Zhang, Yan Chen, Weili Xu, Fan Feng*

Main category: cs.AI

TL;DR: The paper explores how Generative AI (Gen-AI) can revolutionize the building industry by automating tasks like compliance checking and design assistance, improving efficiency and reducing costs.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the potential of Gen-AI in transforming labor-intensive processes in the building industry.

Method: The study reviews foundational models (Transformer, Diffusion), customization techniques, and applications in text and image generation for building-related tasks.

Result: Gen-AI shows promise in automating compliance checking, design assistance, and other building processes, enhancing efficiency.

Conclusion: The paper highlights Gen-AI's current capabilities and suggests future research directions to advance smarter building practices.

Abstract: This paper investigates the transformative potential of Generative AI
(Gen-AI) technologies, particularly large language models, within the building
industry. By leveraging these advanced AI tools, the study explores their
application across key areas such as automated compliance checking and building
design assistance. The research highlights how Gen-AI can automate
labor-intensive processes, significantly improving efficiency and reducing
costs in building practices. The paper first discusses the two widely applied
fundamental models-Transformer and Diffusion model-and summarizes current
pathways for accessing Gen-AI models and the most common techniques for
customizing them. It then explores applications for text generation, such as
compliance checking, control support, data mining, and building simulation
input file editing. Additionally, it examines image generation, including
direct generation through diffusion models and indirect generation through
language model-supported template creation based on existing Computer-Aided
Design or other design tools with rendering. The paper concludes with a
comprehensive analysis of the current capabilities of Gen-AI in the building
industry, outlining future directions for research and development, with the
goal of paving the way for smarter, more effective, and responsive design,
construction, and operational practices.

</details>


### [348] [Unlocking Non-Block-Structured Decisions: Inductive Mining with Choice Graphs](https://arxiv.org/pdf/2505.07052)
*Humam Kourani, Gyunam Park, Wil M. P. van der Aalst*

Main category: cs.AI

TL;DR: The paper proposes an extension to POWL using choice graphs to model non-block-structured decisions in process discovery, improving accuracy while maintaining scalability.


<details>
  <summary>Details</summary>
Motivation: Existing inductive mining algorithms struggle with non-block-structured decision points, limiting their ability to accurately model real-world processes.

Method: The authors extend POWL with choice graphs and develop an inductive mining algorithm to discover models with complex decision logic.

Result: Experiments show the extended POWL with choice graphs more accurately captures real-world decision-making without sacrificing scalability.

Conclusion: The proposed extension bridges the gap in modeling non-block-structured decisions, enhancing the practical applicability of inductive mining.

Abstract: Process discovery aims to automatically derive process models from event
logs, enabling organizations to analyze and improve their operational
processes. Inductive mining algorithms, while prioritizing soundness and
efficiency through hierarchical modeling languages, often impose a strict
block-structured representation. This limits their ability to accurately
capture the complexities of real-world processes. While recent advancements
like the Partially Ordered Workflow Language (POWL) have addressed the
block-structure limitation for concurrency, a significant gap remains in
effectively modeling non-block-structured decision points. In this paper, we
bridge this gap by proposing an extension of POWL to handle
non-block-structured decisions through the introduction of choice graphs.
Choice graphs offer a structured yet flexible approach to model complex
decision logic within the hierarchical framework of POWL. We present an
inductive mining discovery algorithm that uses our extension and preserves the
quality guarantees of the inductive mining framework. Our experimental
evaluation demonstrates that the discovered models, enriched with choice
graphs, more precisely represent the complex decision-making behavior found in
real-world processes, without compromising the high scalability inherent in
inductive mining techniques.

</details>


### [349] [Arbitrarily Applicable Same/Opposite Relational Responding with NARS](https://arxiv.org/pdf/2505.07079)
*Robert Johansson, Patrick Hammer, Tony Lofthouse*

Main category: cs.AI

TL;DR: The study demonstrates how the Non-Axiomatic Reasoning System (NARS) can learn and generalize same/opposite relational responding, mimicking human symbolic cognition, through minimal training.


<details>
  <summary>Details</summary>
Motivation: To explore the integration of human-like relational learning mechanisms into artificial general intelligence, specifically within NARS.

Method: Extended NARS with acquired relations, enabling it to derive symmetric and novel relational combinations via a matching-to-sample procedure.

Result: NARS successfully internalized relational rules and demonstrated derived generalizations, mirroring human relational learning.

Conclusion: The findings highlight NARS's potential for modeling arbitrary and context-sensitive relational learning, advancing artificial general intelligence.

Abstract: Same/opposite relational responding, a fundamental aspect of human symbolic
cognition, allows the flexible generalization of stimulus relationships based
on minimal experience. In this study, we demonstrate the emergence of
\textit{arbitrarily applicable} same/opposite relational responding within the
Non-Axiomatic Reasoning System (NARS), a computational cognitive architecture
designed for adaptive reasoning under uncertainty. Specifically, we extend NARS
with an implementation of \textit{acquired relations}, enabling the system to
explicitly derive both symmetric (mutual entailment) and novel relational
combinations (combinatorial entailment) from minimal explicit training in a
contextually controlled matching-to-sample (MTS) procedure. Experimental
results show that NARS rapidly internalizes explicitly trained relational rules
and robustly demonstrates derived relational generalizations based on arbitrary
contextual cues. Importantly, derived relational responding in critical test
phases inherently combines both mutual and combinatorial entailments, such as
deriving same-relations from multiple explicitly trained opposite-relations.
Internal confidence metrics illustrate strong internalization of these
relational principles, closely paralleling phenomena observed in human
relational learning experiments. Our findings underscore the potential for
integrating nuanced relational learning mechanisms inspired by learning
psychology into artificial general intelligence frameworks, explicitly
highlighting the arbitrary and context-sensitive relational capabilities
modeled within NARS.

</details>


### [350] [Architectural Precedents for General Agents using Large Language Models](https://arxiv.org/pdf/2505.07087)
*Robert E. Wray, James R. Kirk, John E. Laird*

Main category: cs.AI

TL;DR: The paper explores recurring cognitive design patterns in pre-transformer AI architectures and examines their presence in LLM-based systems, identifying gaps for future AGI research.


<details>
  <summary>Details</summary>
Motivation: To understand mechanisms and representations for general intelligence by analyzing cognitive design patterns in AI architectures, including LLMs.

Method: Summarize recurring cognitive design patterns from pre-transformer architectures and analyze their manifestation in LLM-based systems, focusing on reasoning and agentic use cases.

Result: Identifies how these patterns appear in LLM systems and highlights gaps in current Agentic LLM Systems.

Conclusion: The study predicts future research directions for achieving general intelligence using LLMs and generative foundation models.

Abstract: One goal of AI (and AGI) is to identify and understand specific mechanisms
and representations sufficient for general intelligence. Often, this work
manifests in research focused on architectures and many cognitive architectures
have been explored in AI/AGI. However, different research groups and even
different research traditions have somewhat independently identified
similar/common patterns of processes and representations or cognitive design
patterns that are manifest in existing architectures. Today, AI systems
exploiting large language models (LLMs) offer a relatively new combination of
mechanism and representation available for exploring the possibilities of
general intelligence. In this paper, we summarize a few recurring cognitive
design patterns that have appeared in various pre-transformer AI architectures.
We then explore how these patterns are evident in systems using LLMs,
especially for reasoning and interactive ("agentic") use cases. By examining
and applying these recurring patterns, we can also predict gaps or deficiencies
in today's Agentic LLM Systems and identify likely subjects of future research
towards general intelligence using LLMs and other generative foundation models.

</details>


### [351] [RefPentester: A Knowledge-Informed Self-Reflective Penetration Testing Framework Based on Large Language Models](https://arxiv.org/pdf/2505.07089)
*Hanzheng Dai, Yuanliang Li, Zhibo Zhang, Jun Yan*

Main category: cs.AI

TL;DR: RefPentester, a knowledge-informed self-reflective AutoPT framework, outperforms GPT-4o by 16.7% in identifying vulnerabilities by learning from failures and improving PT strategies.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based AutoPT frameworks underperform due to imbalanced knowledge, short-sighted planning, and hallucinations. They also lack mechanisms to learn from failed operations.

Method: Proposed RefPentester integrates a seven-state Stage Machine to guide PT stages, select tactics, and learn from failures.

Result: RefPentester outperforms GPT-4o by 16.7% in revealing credentials and shows superior success rates in PT stage transitions.

Conclusion: RefPentester addresses key limitations of LLM-based AutoPT, enhancing adaptive learning and performance in penetration testing.

Abstract: Automated penetration testing (AutoPT) powered by large language models
(LLMs) has gained attention for its ability to automate ethical hacking
processes and identify vulnerabilities in target systems by leveraging the
intrinsic knowledge of LLMs. However, existing LLM-based AutoPT frameworks
often underperform compared to human experts in challenging tasks for several
reasons: the imbalanced knowledge used in LLM training, short-sighted planning
in the planning process, and hallucinations during command generation. In
addition, the penetration testing (PT) process, with its trial-and-error
nature, is limited by existing frameworks that lack mechanisms to learn from
previous failed operations, restricting adaptive improvement of PT strategies.
To address these limitations, we propose a knowledge-informed self-reflective
PT framework powered by LLMs, called RefPentester, which is an AutoPT framework
designed to assist human operators in identifying the current stage of the PT
process, selecting appropriate tactic and technique for the stage, choosing
suggested action, providing step-by-step operational guidance, and learning
from previous failed operations. We also modeled the PT process as a
seven-state Stage Machine to integrate the proposed framework effectively. The
evaluation shows that RefPentester can successfully reveal credentials on Hack
The Box's Sau machine, outperforming the baseline GPT-4o model by 16.7\%.
Across PT stages, RefPentester also demonstrates superior success rates on PT
stage transitions.

</details>


### [352] [ReCDAP: Relation-Based Conditional Diffusion with Attention Pooling for Few-Shot Knowledge Graph Completion](https://arxiv.org/pdf/2505.07171)
*Jeongho Kim, Chanyeong Heo, Jaehee Jung*

Main category: cs.AI

TL;DR: The paper proposes ReCDAP, a method for few-shot KG completion that leverages both positive and negative triples to improve performance.


<details>
  <summary>Details</summary>
Motivation: Existing KG completion methods focus only on positive triples or use negative triples minimally, limiting performance.

Method: ReCDAP generates negative triples, incorporates them conditionally in a diffusion process, and uses attention pooling to distinguish positive and negative relations.

Result: ReCDAP outperforms existing methods on two datasets, achieving state-of-the-art results.

Conclusion: The proposed method effectively improves KG completion by utilizing both positive and negative information.

Abstract: Knowledge Graphs (KGs), composed of triples in the form of (head, relation,
tail) and consisting of entities and relations, play a key role in information
retrieval systems such as question answering, entity search, and
recommendation. In real-world KGs, although many entities exist, the relations
exhibit a long-tail distribution, which can hinder information retrieval
performance. Previous few-shot knowledge graph completion studies focused
exclusively on the positive triple information that exists in the graph or,
when negative triples were incorporated, used them merely as a signal to
indicate incorrect triples. To overcome this limitation, we propose
Relation-Based Conditional Diffusion with Attention Pooling (ReCDAP). First,
negative triples are generated by randomly replacing the tail entity in the
support set. By conditionally incorporating positive information in the KG and
non-existent negative information into the diffusion process, the model
separately estimates the latent distributions for positive and negative
relations. Moreover, including an attention pooler enables the model to
leverage the differences between positive and negative cases explicitly.
Experiments on two widely used datasets demonstrate that our method outperforms
existing approaches, achieving state-of-the-art performance. The code is
available at https://github.com/hou27/ReCDAP-FKGC.

</details>


### [353] [Accountability of Generative AI: Exploring a Precautionary Approach for "Artificially Created Nature"](https://arxiv.org/pdf/2505.07178)
*Yuri Nakao*

Main category: cs.AI

TL;DR: The paper discusses the accountability challenges of generative AI, arguing transparency alone isn't enough for accountability. It suggests using the precautionary principle and citizen participation to address risks.


<details>
  <summary>Details</summary>
Motivation: Concerns about the accountability of generative AI systems due to their complexity and lack of transparency.

Method: Examines AI transparency research, argues its insufficiency for accountability, and proposes the precautionary principle and citizen participation.

Result: Transparency is insufficient for accountability; generative AI may become "artificially created nature," requiring precautionary measures and public involvement.

Conclusion: A platform for citizen participation is essential to mitigate generative AI risks, alongside the precautionary principle.

Abstract: The rapid development of generative artificial intelligence (AI) technologies
raises concerns about the accountability of sociotechnical systems. Current
generative AI systems rely on complex mechanisms that make it difficult for
even experts to fully trace the reasons behind the outputs. This paper first
examines existing research on AI transparency and accountability and argues
that transparency is not a sufficient condition for accountability but can
contribute to its improvement. We then discuss that if it is not possible to
make generative AI transparent, generative AI technology becomes ``artificially
created nature'' in a metaphorical sense, and suggest using the precautionary
principle approach to consider AI risks. Finally, we propose that a platform
for citizen participation is needed to address the risks of generative AI.

</details>


### [354] [Measuring General Intelligence with Generated Games](https://arxiv.org/pdf/2505.07215)
*Vivek Verma, David Huang, William Chen, Dan Klein, Nicholas Tomlin*

Main category: cs.AI

TL;DR: gg-bench is a dynamic benchmark for evaluating general reasoning in language models using synthetic game environments generated by LLMs and tested via RL agents.


<details>
  <summary>Details</summary>
Motivation: To create a flexible and challenging benchmark for assessing language models' reasoning abilities beyond static datasets.

Method: Generates novel games via LLMs, implements them as Gym environments, and evaluates models by their winrate against RL agents trained via self-play.

Result: State-of-the-art LLMs like GPT-4o achieve low winrates (7-9%), while reasoning models like o1 and DeepSeek-R1 perform better (31-36%).

Conclusion: gg-bench provides a scalable and challenging benchmark for future research, with released resources for expansion.

Abstract: We present gg-bench, a collection of game environments designed to evaluate
general reasoning capabilities in language models. Unlike most static
benchmarks, gg-bench is a data generating process where new evaluation
instances can be generated at will. In particular, gg-bench is synthetically
generated by (1) using a large language model (LLM) to generate natural
language descriptions of novel games, (2) using the LLM to implement each game
in code as a Gym environment, and (3) training reinforcement learning (RL)
agents via self-play on the generated games. We evaluate language models by
their winrate against these RL agents by prompting models with the game
description, current board state, and a list of valid moves, after which models
output the moves they wish to take. gg-bench is challenging: state-of-the-art
LLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench
using in-context learning, while reasoning models such as o1, o3-mini and
DeepSeek-R1 achieve average winrates of 31-36%. We release the generated games,
data generation process, and evaluation code in order to support future
modeling work and expansion of our benchmark.

</details>


### [355] [Interpretable Event Diagnosis in Water Distribution Networks](https://arxiv.org/pdf/2505.07299)
*André Artelt, Stelios G. Vrachimis, Demetrios G. Eliades, Ulrike Kuhl, Barbara Hammer, Marios M. Polycarpou*

Main category: cs.AI

TL;DR: A framework for interpretable event diagnosis in water systems using counterfactual explanations to bridge algorithmic results and operator intuition.


<details>
  <summary>Details</summary>
Motivation: Operators often distrust data-driven methodologies for event diagnosis, preferring their own judgment. The goal is to enhance trust and understanding by providing interpretable explanations.

Method: Proposes counterfactual event fingerprints to contrast algorithmic results with alternative explanations, presented graphically.

Result: Applied and evaluated on the L-Town benchmark, showing potential to improve operator decision-making.

Conclusion: The framework aids operators in combining algorithmic insights with their experience, fostering better trust and informed decisions.

Abstract: The increasing penetration of information and communication technologies in
the design, monitoring, and control of water systems enables the use of
algorithms for detecting and identifying unanticipated events (such as leakages
or water contamination) using sensor measurements. However, data-driven
methodologies do not always give accurate results and are often not trusted by
operators, who may prefer to use their engineering judgment and experience to
deal with such events.
  In this work, we propose a framework for interpretable event diagnosis -- an
approach that assists the operators in associating the results of algorithmic
event diagnosis methodologies with their own intuition and experience. This is
achieved by providing contrasting (i.e., counterfactual) explanations of the
results provided by fault diagnosis algorithms; their aim is to improve the
understanding of the algorithm's inner workings by the operators, thus enabling
them to take a more informed decision by combining the results with their
personal experiences. Specifically, we propose counterfactual event
fingerprints, a representation of the difference between the current event
diagnosis and the closest alternative explanation, which can be presented in a
graphical way. The proposed methodology is applied and evaluated on a realistic
use case using the L-Town benchmark.

</details>


### [356] [FedIFL: A federated cross-domain diagnostic framework for motor-driven systems with inconsistent fault modes](https://arxiv.org/pdf/2505.07315)
*Zexiao Wang, Yankai Wang, Xiaoqiang Liao, Xinguo Ming, Weiming Shen*

Main category: cs.AI

TL;DR: The paper proposes FedIFL, a federated learning framework, to address label space inconsistency in fault diagnosis by using prototype contrastive learning and feature disentanglement.


<details>
  <summary>Details</summary>
Motivation: Industrial data scarcity and label space inconsistency hinder fault diagnosis model training, especially for start-ups. Federated learning offers privacy but struggles with generalization due to inconsistent fault modes.

Method: FedIFL employs intra-client prototype contrastive learning and cross-client feature disentanglement with instance-level consistency and orthogonal losses.

Result: FedIFL improves generalization in global label spaces, enabling accurate fault diagnosis for Motor Driven Systems with inconsistent fault modes.

Conclusion: FedIFL effectively addresses label space inconsistency in federated learning, validated by real-world MDS experiments.

Abstract: Due to the scarcity of industrial data, individual equipment users,
particularly start-ups, struggle to independently train a comprehensive fault
diagnosis model; federated learning enables collaborative training while
ensuring data privacy, making it an ideal solution. However, the diversity of
working conditions leads to variations in fault modes, resulting in
inconsistent label spaces across different clients. In federated diagnostic
scenarios, label space inconsistency leads to local models focus on
client-specific fault modes and causes local models from different clients to
map different failure modes to similar feature representations, which weakens
the aggregated global model's generalization. To tackle this issue, this
article proposed a federated cross-domain diagnostic framework termed Federated
Invariant Features Learning (FedIFL). In intra-client training, prototype
contrastive learning mitigates intra-client domain shifts, subsequently,
feature generating ensures local models can access distributions of other
clients in a privacy-friendly manner. Besides, in cross-client training, a
feature disentanglement mechanism is introduced to mitigate cross-client domain
shifts, specifically, an instance-level federated instance consistency loss is
designed to ensure the instance-level consistency of invariant features between
different clients, furthermore, a federated instance personalization loss and
an orthogonal loss are constructed to distinguish specific features that from
the invariant features. Eventually, the aggregated model achieves promising
generalization among global label spaces, enabling accurate fault diagnosis for
target clients' Motor Driven Systems (MDSs) with inconsistent label spaces.
Experiments on real-world MDSs validate the effectiveness and superiority of
FedIFL in federated cross-domain diagnosis with inconsistent fault modes.

</details>


### [357] [AIS Data-Driven Maritime Monitoring Based on Transformer: A Comprehensive Review](https://arxiv.org/pdf/2505.07374)
*Zhiye Xie, Enmei Tu, Xianping Fu, Guoliang Yuan, Yi Han*

Main category: cs.AI

TL;DR: The paper reviews Transformer-based AIS data applications in maritime monitoring, focusing on trajectory prediction, behavior detection, and datasets.


<details>
  <summary>Details</summary>
Motivation: To address the untapped potential of AIS data for maritime safety, efficiency, and sustainability using Transformer models.

Method: Reviews Transformer-based AIS data applications, collects and analyzes public AIS datasets, and performs statistical analysis.

Result: Reveals vessel operational characteristics and provides datasets for future maritime monitoring research.

Conclusion: Identifies promising research directions and offers datasets for further study.

Abstract: With the increasing demands for safety, efficiency, and sustainability in
global shipping, Automatic Identification System (AIS) data plays an
increasingly important role in maritime monitoring. AIS data contains
spatial-temporal variation patterns of vessels that hold significant research
value in the marine domain. However, due to its massive scale, the full
potential of AIS data has long remained untapped. With its powerful sequence
modeling capabilities, particularly its ability to capture long-range
dependencies and complex temporal dynamics, the Transformer model has emerged
as an effective tool for processing AIS data. Therefore, this paper reviews the
research on Transformer-based AIS data-driven maritime monitoring, providing a
comprehensive overview of the current applications of Transformer models in the
marine field. The focus is on Transformer-based trajectory prediction methods,
behavior detection, and prediction techniques. Additionally, this paper
collects and organizes publicly available AIS datasets from the reviewed
papers, performing data filtering, cleaning, and statistical analysis. The
statistical results reveal the operational characteristics of different vessel
types, providing data support for further research on maritime monitoring
tasks. Finally, we offer valuable suggestions for future research, identifying
two promising research directions. Datasets are available at
https://github.com/eyesofworld/Maritime-Monitoring.

</details>


### [358] [How well do LLMs reason over tabular data, really?](https://arxiv.org/pdf/2505.07453)
*Cornelius Wolff, Madelon Hulsebos*

Main category: cs.AI

TL;DR: The paper investigates whether general-purpose LLMs can effectively reason over tabular data, highlighting their limitations in robustness and evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Prior evaluations of LLMs on tabular data are unrealistic, and their robustness to real-world tabular variations is poorly understood.

Method: The study critiques existing benchmarks and metrics, introduces an LLM-as-a-judge evaluation, and tests LLMs on tabular inputs with missing values, duplicates, and structural variations.

Result: LLMs show significant deficits in tabular reasoning, especially when faced with realistic tabular variations.

Conclusion: Improving LLM robustness for realistic tabular inputs is crucial, as current methods fall short.

Abstract: Large Language Models (LLMs) excel in natural language tasks, but less is
known about their reasoning capabilities over tabular data. Prior analyses
devise evaluation strategies that poorly reflect an LLM's realistic performance
on tabular queries. Moreover, we have a limited understanding of the robustness
of LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can
general-purpose LLMs reason over tabular data, really?, and focus on two
questions 1) are tabular reasoning capabilities of general-purpose LLMs robust
to real-world characteristics of tabular inputs, and 2) how can we
realistically evaluate an LLM's performance on analytical tabular queries?
Building on a recent tabular reasoning benchmark, we first surface shortcomings
of its multiple-choice prompt evaluation strategy, as well as commonly used
free-form text metrics such as SacreBleu and BERT-score. We show that an
LLM-as-a-judge procedure yields more reliable performance insights and unveil a
significant deficit in tabular reasoning performance of LLMs. We then extend
the tabular inputs reflecting three common characteristics in practice: 1)
missing values, 2) duplicate entities, and 3) structural variations.
Experiments show that the tabular reasoning capabilities of general-purpose
LLMs suffer from these variations, stressing the importance of improving their
robustness for realistic tabular inputs.

</details>


### [359] [A Survey on Collaborative Mechanisms Between Large and Small Language Models](https://arxiv.org/pdf/2505.07460)
*Yi Chen, JiaHao Zhao, HaoHao Han*

Main category: cs.AI

TL;DR: LLM-SLM collaboration balances performance and efficiency, enabling advanced AI on edge devices. This survey covers interaction mechanisms, technologies, applications, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: Address deployment challenges of LLMs (high resource costs, latency) and performance limitations of SLMs by leveraging their collaboration.

Method: Survey of LLM-SLM collaboration, detailing interaction mechanisms (pipeline, routing, auxiliary, distillation, fusion) and enabling technologies.

Result: Identifies key applications (low latency, privacy, personalization, offline operation) and challenges (system overhead, consistency, task allocation, evaluation, security).

Conclusion: LLM-SLM collaboration is pivotal for efficient, adaptable AI, with future directions including adaptive frameworks, deeper fusion, and multimodal expansion.

Abstract: Large Language Models (LLMs) deliver powerful AI capabilities but face
deployment challenges due to high resource costs and latency, whereas Small
Language Models (SLMs) offer efficiency and deployability at the cost of
reduced performance. Collaboration between LLMs and SLMs emerges as a crucial
paradigm to synergistically balance these trade-offs, enabling advanced AI
applications, especially on resource-constrained edge devices. This survey
provides a comprehensive overview of LLM-SLM collaboration, detailing various
interaction mechanisms (pipeline, routing, auxiliary, distillation, fusion),
key enabling technologies, and diverse application scenarios driven by
on-device needs like low latency, privacy, personalization, and offline
operation. While highlighting the significant potential for creating more
efficient, adaptable, and accessible AI, we also discuss persistent challenges
including system overhead, inter-model consistency, robust task allocation,
evaluation complexity, and security/privacy concerns. Future directions point
towards more intelligent adaptive frameworks, deeper model fusion, and
expansion into multimodal and embodied AI, positioning LLM-SLM collaboration as
a key driver for the next generation of practical and ubiquitous artificial
intelligence.

</details>


### [360] [Web-Bench: A LLM Code Benchmark Based on Web Standards and Frameworks](https://arxiv.org/pdf/2505.07473)
*Kai Xu, YiWei Mao, XinYi Guan, ZiLong Feng*

Main category: cs.AI

TL;DR: The paper introduces Web-Bench, a new benchmark for evaluating LLMs in coding, addressing saturation in existing benchmarks by simulating real-world web development workflows.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLM coding performance are becoming saturated, limiting their effectiveness in guiding LLM development.

Method: Proposed Web-Bench, a benchmark with 50 projects, each containing 20 sequentially dependent tasks, designed to simulate real-world web development.

Result: Current SOTA model (Claude 3.7 Sonnet) achieves only 25.1% Pass@1 on Web-Bench, highlighting its challenge compared to other benchmarks.

Conclusion: Standards and frameworks are foundational for LLM optimization in development, and Web-Bench provides a more realistic evaluation tool.

Abstract: The application of large language models (LLMs) in the field of coding is
evolving rapidly: from code assistants, to autonomous coding agents, and then
to generating complete projects through natural language. Early LLM code
benchmarks primarily focused on code generation accuracy, but these benchmarks
have gradually become saturated. Benchmark saturation weakens their guiding
role for LLMs. For example, HumanEval Pass@1 has reached 99.4% and MBPP 94.2%.
Among various attempts to address benchmark saturation, approaches based on
software engineering have stood out, but the saturation of existing software
engineering benchmarks is rapidly increasing. To address this, we propose a new
benchmark, Web-Bench, which contains 50 projects, each consisting of 20 tasks
with sequential dependencies. The tasks implement project features in sequence,
simulating real-world human development workflows. When designing Web-Bench, we
aim to cover the foundational elements of Web development: Web Standards and
Web Frameworks. Given the scale and complexity of these projects, which were
designed by engineers with 5 to 10 years of experience, each presents a
significant challenge. On average, a single project takes 4 to 8 hours for a
senior engineer to complete. On our given benchmark agent (Web-Agent), SOTA
(Claude 3.7 Sonnet) achieves only 25.1% Pass@1, significantly lower (better)
than SWE-Bench's Verified (65.4%) and Full (33.8%) scores. Finally, we discuss
that in any development field, Standards and Frameworks represent foundational
knowledge and efficiency tools, respectively, and LLMs require optimization
tailored to them.

</details>


### [361] [HALO: Half Life-Based Outdated Fact Filtering in Temporal Knowledge Graphs](https://arxiv.org/pdf/2505.07509)
*Feng Ding, Tingting Wang, Yupeng Gao, Shuo Yu, Jing Ren, Feng Xia*

Main category: cs.AI

TL;DR: HALO is a framework for filtering outdated facts in temporal knowledge graphs (TKGs) by quantifying their temporal validity using half-life theory, improving reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Existing TKG reasoning methods ignore the negative impact of outdated facts, leading to performance degradation and extra computational costs.

Method: HALO uses three modules: temporal fact attention to capture fact evolution, a dynamic relation-aware encoder to predict fact half-life, and a time decay function to filter outdated facts.

Result: HALO outperforms state-of-the-art TKG reasoning methods on three public datasets.

Conclusion: HALO effectively detects and filters outdated facts, enhancing TKG reasoning performance.

Abstract: Outdated facts in temporal knowledge graphs (TKGs) result from exceeding the
expiration date of facts, which negatively impact reasoning performance on
TKGs. However, existing reasoning methods primarily focus on positive
importance of historical facts, neglecting adverse effects of outdated facts.
Besides, training on these outdated facts yields extra computational cost. To
address these challenges, we propose an outdated fact filtering framework named
HALO, which quantifies the temporal validity of historical facts by exploring
the half-life theory to filter outdated facts in TKGs. HALO consists of three
modules: the temporal fact attention module, the dynamic relation-aware encoder
module, and the outdated fact filtering module. Firstly, the temporal fact
attention module captures the evolution of historical facts over time to
identify relevant facts. Secondly, the dynamic relation-aware encoder module is
designed for efficiently predicting the half life of each fact. Finally, we
construct a time decay function based on the half-life theory to quantify the
temporal validity of facts and filter outdated facts. Experimental results show
that HALO outperforms the state-of-the-art TKG reasoning methods on three
public datasets, demonstrating its effectiveness in detecting and filtering
outdated facts (Codes are available at
https://github.com/yushuowiki/K-Half/tree/main ).

</details>


### [362] [QuantX: A Framework for Hardware-Aware Quantization of Generative AI Workloads](https://arxiv.org/pdf/2505.07531)
*Khurram Mazher, Saad Bin Nasir*

Main category: cs.AI

TL;DR: QuantX is a suite for LLM/VLM quantization, achieving 3-bit resolution with minimal performance loss, optimized for hardware constraints and outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient quantization of LLMs/VLMs with minimal performance loss while considering hardware-specific constraints.

Method: QuantX employs tailored quantization strategies, including hardware-aware dequantization, to balance speed, memory, and accuracy.

Result: QuantX achieves performance within 6% of unquantized models (e.g., LlaVa-v1.6 at 3-bits) and outperforms recent SOTA methods.

Conclusion: QuantX provides effective quantization recipes, offering insights and options for LLM/VLM quantization with practical trade-offs.

Abstract: We present QuantX: a tailored suite of recipes for LLM and VLM quantization.
It is capable of quantizing down to 3-bit resolutions with minimal loss in
performance. The quantization strategies in QuantX take into account
hardware-specific constraints to achieve efficient dequantization during
inference ensuring flexible trade-off between runtime speed, memory requirement
and model accuracy. Our results demonstrate that QuantX achieves performance
within 6% of the unquantized model for LlaVa-v1.6 quantized down to 3-bits for
multiple end user tasks and outperforms recently published state-of-the-art
quantization techniques. This manuscript provides insights into the LLM
quantization process that motivated the range of recipes and options that are
incorporated in QuantX.

</details>


### [363] [YuLan-OneSim: Towards the Next Generation of Social Simulator with Large Language Models](https://arxiv.org/pdf/2505.07581)
*Lei Wang, Heyang Gao, Xiaohe Bo, Xu Chen, Ji-Rong Wen*

Main category: cs.AI

TL;DR: YuLan-OneSim is a novel LLM-based social simulator enabling code-free scenario construction, large-scale simulations, and an AI social researcher, validated through experiments.


<details>
  <summary>Details</summary>
Motivation: To simplify and enhance social behavior simulations by reducing programming needs and expanding accessibility for diverse researchers.

Method: Develops a code-free, evolvable, and scalable simulator with default scenarios and an AI researcher for automated social science research.

Result: Supports 100,000 agents, improves simulation quality via feedback, and automates research tasks like report generation.

Conclusion: YuLan-OneSim advances social simulation by combining ease of use, scalability, and AI-driven research capabilities.

Abstract: Leveraging large language model (LLM) based agents to simulate human social
behaviors has recently gained significant attention. In this paper, we
introduce a novel social simulator called YuLan-OneSim. Compared to previous
works, YuLan-OneSim distinguishes itself in five key aspects: (1) Code-free
scenario construction: Users can simply describe and refine their simulation
scenarios through natural language interactions with our simulator. All
simulation code is automatically generated, significantly reducing the need for
programming expertise. (2) Comprehensive default scenarios: We implement 50
default simulation scenarios spanning 8 domains, including economics,
sociology, politics, psychology, organization, demographics, law, and
communication, broadening access for a diverse range of social researchers. (3)
Evolvable simulation: Our simulator is capable of receiving external feedback
and automatically fine-tuning the backbone LLMs, significantly enhancing the
simulation quality. (4) Large-scale simulation: By developing a fully
responsive agent framework and a distributed simulation architecture, our
simulator can handle up to 100,000 agents, ensuring more stable and reliable
simulation results. (5) AI social researcher: Leveraging the above features, we
develop an AI social researcher. Users only need to propose a research topic,
and the AI researcher will automatically analyze the input, construct
simulation environments, summarize results, generate technical reports, review
and refine the reports--completing the social science research loop. To
demonstrate the advantages of YuLan-OneSim, we conduct experiments to evaluate
the quality of the automatically generated scenarios, the reliability,
efficiency, and scalability of the simulation process, as well as the
performance of the AI social researcher.

</details>


### [364] [S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models](https://arxiv.org/pdf/2505.07686)
*Muzhi Dai, Chenxu Yang, Qingyi Si*

Main category: cs.AI

TL;DR: The paper introduces S-GRPO, a reinforcement learning method to reduce redundant reasoning steps in chain-of-thought (CoT) generation, improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the overthinking problem in reasoning models, where excessive redundancy in CoT steps occurs due to neglected regulation of intermediate reasoning.

Method: Proposes Serial-Group Decaying-Reward Policy Optimization (S-GRPO), which allows models to exit CoT generation early by rewarding earlier, higher-quality answers with decaying rewards.

Result: Achieves 35.4%–61.1% sequence length reduction and 0.72%–6.08% accuracy improvements across multiple benchmarks.

Conclusion: S-GRPO effectively balances reasoning efficiency and accuracy, demonstrating compatibility with advanced models like Qwen3 and Deepseek-distill.

Abstract: As Test-Time Scaling emerges as an active research focus in the large
language model community, advanced post-training methods increasingly emphasize
extending chain-of-thought (CoT) generation length, thereby enhancing reasoning
capabilities to approach Deepseek R1-like reasoning models. However, recent
studies reveal that reasoning models (even Qwen3) consistently exhibit
excessive thought redundancy in CoT generation. This overthinking problem stems
from conventional outcome-reward reinforcement learning's systematic neglect in
regulating intermediate reasoning steps. This paper proposes Serial-Group
Decaying-Reward Policy Optimization (namely S-GRPO), a novel reinforcement
learning method that empowers models with the capability to determine the
sufficiency of reasoning steps, subsequently triggering early exit of CoT
generation. Specifically, unlike GRPO, which samples multiple possible
completions (parallel group) in parallel, we select multiple temporal positions
in the generation of one CoT to allow the model to exit thinking and instead
generate answers (serial group), respectively. For the correct answers in a
serial group, we assign rewards that decay according to positions, with lower
rewards towards the later ones, thereby reinforcing the model's behavior to
generate higher-quality answers at earlier phases with earlier exits of
thinking. Empirical evaluations demonstrate compatibility with state-of-the-art
reasoning models, including Qwen3 and Deepseek-distill models, achieving 35.4%
~ 61.1\% sequence length reduction with 0.72% ~ 6.08% accuracy improvements
across GSM8K, AIME 2024, AMC 2023, MATH-500, and GPQA Diamond benchmarks.

</details>


### [365] [Belief Injection for Epistemic Control in Linguistic State Space](https://arxiv.org/pdf/2505.07693)
*Sebastian Dumbrava*

Main category: cs.AI

TL;DR: Belief injection is a proactive method for influencing AI agents' cognitive states using linguistic beliefs, contrasting with reactive methods like belief filtering.


<details>
  <summary>Details</summary>
Motivation: To enhance AI reasoning and alignment by directly embedding targeted linguistic beliefs into agents' cognitive frameworks.

Method: Introduces belief injection strategies (direct, context-aware, goal-oriented, reflective) within the Semantic Manifold framework.

Result: Provides a framework for proactive epistemic control, with practical applications and ethical considerations.

Conclusion: Belief injection offers a promising direction for cognitive governance, with future research needed for implementation and ethical refinement.

Abstract: This work introduces belief injection, a proactive epistemic control
mechanism for artificial agents whose cognitive states are structured as
dynamic ensembles of linguistic belief fragments. Grounded in the Semantic
Manifold framework, belief injection directly incorporates targeted linguistic
beliefs into an agent's internal cognitive state, influencing reasoning and
alignment proactively rather than reactively. We delineate various injection
strategies, such as direct, context-aware, goal-oriented, and reflective
approaches, and contrast belief injection with related epistemic control
mechanisms, notably belief filtering. Additionally, this work discusses
practical applications, implementation considerations, ethical implications,
and outlines promising directions for future research into cognitive governance
using architecturally embedded belief injection.

</details>


### [366] [Emotion-Gradient Metacognitive RSI (Part I): Theoretical Foundations and Single-Agent Architecture](https://arxiv.org/pdf/2505.07757)
*Rintaro Ando*

Main category: cs.AI

TL;DR: The EG-MRSI framework integrates metacognition, emotion-based motivation, and recursive self-improvement for AGI, with provable safety and quantifiable learning metrics.


<details>
  <summary>Details</summary>
Motivation: To create a unified, safe, and open-ended AGI framework by combining introspective metacognition, emotion-driven rewards, and recursive self-modification.

Method: Introduces a differentiable intrinsic reward function, formalizes agent configuration and dynamics, and derives a reinforcement-compatible optimization objective.

Result: Defines quantifiable metrics (Meaning Density, Meaning Conversion Efficiency) and establishes theoretical foundations for safe, recursive self-improvement.

Conclusion: EG-MRSI provides a rigorous foundation for AGI, with future extensions planned for safety, collective intelligence, and feasibility constraints.

Abstract: We present the Emotion-Gradient Metacognitive Recursive Self-Improvement
(EG-MRSI) framework, a novel architecture that integrates introspective
metacognition, emotion-based intrinsic motivation, and recursive
self-modification into a unified theoretical system. The framework is
explicitly capable of overwriting its own learning algorithm under formally
bounded risk. Building upon the Noise-to-Meaning RSI (N2M-RSI) foundation,
EG-MRSI introduces a differentiable intrinsic reward function driven by
confidence, error, novelty, and cumulative success. This signal regulates both
a metacognitive mapping and a self-modification operator constrained by
provable safety mechanisms. We formally define the initial agent configuration,
emotion-gradient dynamics, and RSI trigger conditions, and derive a
reinforcement-compatible optimization objective that guides the agent's
development trajectory. Meaning Density and Meaning Conversion Efficiency are
introduced as quantifiable metrics of semantic learning, closing the gap
between internal structure and predictive informativeness. This Part I paper
establishes the single-agent theoretical foundations of EG-MRSI. Future parts
will extend this framework to include safety certificates and rollback
protocols (Part II), collective intelligence mechanisms (Part III), and
feasibility constraints including thermodynamic and computational limits (Part
IV). Together, the EG-MRSI series provides a rigorous, extensible foundation
for open-ended and safe AGI.

</details>


### [367] ["I Apologize For Not Understanding Your Policy": Exploring the Specification and Evaluation of User-Managed Access Control Policies by AI Virtual Assistants](https://arxiv.org/pdf/2505.07759)
*Jennifer Mondragon, Carlos Rubio-Medrano, Gael Cruz, Dvijesh Shastri*

Main category: cs.AI

TL;DR: The study evaluates AI-based Virtual Assistants (VAs) like ChatGPT and Google Gemini for managing User-Managed Access Control Policies (U-MAPs), finding limitations in their comprehension and suggesting improvements.


<details>
  <summary>Details</summary>
Motivation: To assess whether current VAs can effectively handle U-MAPs, crucial for security and privacy without compromising user experience.

Method: Conducted unstructured to structured tests to evaluate VAs' comprehension of U-MAPs across scenarios.

Result: VAs showed a lack of understanding in managing varying U-MAP approaches.

Conclusion: The study highlights limitations and provides insights for improving VAs in handling complex authorization rules and dynamic changes.

Abstract: The rapid evolution of Artificial Intelligence (AI)-based Virtual Assistants
(VAs) e.g., Google Gemini, ChatGPT, Microsoft Copilot, and High-Flyer Deepseek
has turned them into convenient interfaces for managing emerging technologies
such as Smart Homes, Smart Cars, Electronic Health Records, by means of
explicit commands,e.g., prompts, which can be even launched via voice, thus
providing a very convenient interface for end-users. However, the proper
specification and evaluation of User-Managed Access Control Policies (U-MAPs),
the rules issued and managed by end-users to govern access to sensitive data
and device functionality - within these VAs presents significant challenges,
since such a process is crucial for preventing security vulnerabilities and
privacy leaks without impacting user experience. This study provides an initial
exploratory investigation on whether current publicly-available VAs can manage
U-MAPs effectively across differing scenarios. By conducting unstructured to
structured tests, we evaluated the comprehension of such VAs, revealing a lack
of understanding in varying U-MAP approaches. Our research not only identifies
key limitations, but offers valuable insights into how VAs can be further
improved to manage complex authorization rules and adapt to dynamic changes.

</details>


### [368] [Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving](https://arxiv.org/pdf/2505.07773)
*Xinji Mai, Haotian Xu, Xing W, Weinong Wang, Yingying Zhang, Wenqiang Zhang*

Main category: cs.AI

TL;DR: ZeroTIR trains LLMs with RL to autonomously generate and execute Python code for math tasks, showing predictable scaling in performance metrics like code execution frequency and accuracy.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with precise mathematical reasoning, and understanding how they learn to use tools like code execution autonomously is crucial.

Method: Uses RL from outcome-based rewards to train LLMs (ZeroTIR) for tool-integrated reasoning without supervised examples, featuring a decoupled code execution environment.

Result: ZeroTIR outperforms non-tool baselines, with predictable scaling in metrics like code execution frequency and task accuracy as training progresses.

Conclusion: The study provides foundational insights into autonomous tool use in RL, offering a reproducible benchmark for future research.

Abstract: Large Language Models (LLMs) often struggle with mathematical reasoning tasks
requiring precise, verifiable computation. While Reinforcement Learning (RL)
from outcome-based rewards enhances text-based reasoning, understanding how
agents autonomously learn to leverage external tools like code execution
remains crucial. We investigate RL from outcome-based rewards for
Tool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously
generate and execute Python code for mathematical problems without supervised
tool-use examples. Our central contribution is we demonstrate that as RL
training progresses, key metrics scale predictably. Specifically, we observe
strong positive correlations where increased training steps lead to increases
in the spontaneous code execution frequency, the average response length, and,
critically, the final task accuracy. This suggests a quantifiable relationship
between computational effort invested in training and the emergence of
effective, tool-augmented reasoning strategies. We implement a robust framework
featuring a decoupled code execution environment and validate our findings
across standard RL algorithms and frameworks. Experiments show ZeroTIR
significantly surpasses non-tool ZeroRL baselines on challenging math
benchmarks. Our findings provide a foundational understanding of how autonomous
tool use is acquired and scales within Agent RL, offering a reproducible
benchmark for future studies. Code is released at
\href{https://github.com/Anonymize-Author/AgentRL}{https://github.com/Anonymize-Author/AgentRL}.

</details>


### [369] [CHARTOM: A Visual Theory-of-Mind Benchmark for Multimodal Large Language Models](https://arxiv.org/pdf/2408.14419)
*Shubham Bharti, Shiyun Cheng, Jihyun Rho, Jianrui Zhang, Mu Cai, Yong Jae Lee, Martina Rau, Xiaojin Zhu*

Main category: cs.AI

TL;DR: CHARTOM is a visual theory-of-mind benchmark for multimodal LLMs, testing chart comprehension and misleading potential, challenging top models like GPT and Claude.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs' ability to comprehend charts and assess their potential to mislead humans, addressing societal benefits.

Method: Constructed CHARTOM with calibrated human performance data, benchmarking top LLMs (GPT, Claude, Gemini, Qwen, Llama, Llava).

Result: All tested models struggled with CHARTOM, indicating room for improvement in multimodal LLMs.

Conclusion: CHARTOM highlights challenges for LLMs in visual theory-of-mind tasks, suggesting future research directions.

Abstract: We introduce CHARTOM, a visual theory-of-mind benchmark for multimodal large
language models. CHARTOM consists of specially designed data visualizing
charts. Given a chart, a language model needs to not only correctly comprehend
the chart (the FACT question) but also judge if the chart will be misleading to
a human reader (the MIND question). Both questions have significant societal
benefits. We detail the construction of the CHARTOM benchmark including its
calibration on human performance. We benchmark leading LLMs as of late 2024 -
including GPT, Claude, Gemini, Qwen, Llama, and Llava - on the CHARTOM dataset
and found that our benchmark was challenging to all of them, suggesting room
for future large language models to improve.

</details>


### [370] [A Statistical Case Against Empirical Human-AI Alignment](https://arxiv.org/pdf/2502.14581)
*Julian Rodemann, Esteban Garces Arias, Christoph Luther, Christoph Jansen, Thomas Augustin*

Main category: cs.AI

TL;DR: The paper critiques naive empirical human-AI alignment, proposing prescriptive and a posteriori alignment as better alternatives, supported by examples like language model decoding.


<details>
  <summary>Details</summary>
Motivation: To highlight the risks of statistical biases in empirical human-AI alignment and advocate for more principled approaches.

Method: Principled argumentation and tangible examples, such as human-centric decoding of language models.

Result: Demonstrates that naive empirical alignment can introduce biases, suggesting alternatives.

Conclusion: Advocates for prescriptive and a posteriori empirical alignment over naive approaches to avoid biases.

Abstract: Empirical human-AI alignment aims to make AI systems act in line with
observed human behavior. While noble in its goals, we argue that empirical
alignment can inadvertently introduce statistical biases that warrant caution.
This position paper thus advocates against naive empirical alignment, offering
prescriptive alignment and a posteriori empirical alignment as alternatives. We
substantiate our principled argument by tangible examples like human-centric
decoding of language models.

</details>


### [371] [An Illusion of Progress? Assessing the Current State of Web Agents](https://arxiv.org/pdf/2504.01382)
*Tianci Xue, Weijian Qi, Tianneng Shi, Chan Hee Song, Boyu Gou, Dawn Song, Huan Sun, Yu Su*

Main category: cs.AI

TL;DR: The paper evaluates web agents' capabilities, revealing over-optimism in prior results due to flawed benchmarks, and introduces Online-Mind2Web for realistic testing and an LLM-as-a-Judge method for scalable evaluation.


<details>
  <summary>Details</summary>
Motivation: To accurately measure and monitor the progression of autonomous web agents based on LLMs, given their growing importance in work automation.

Method: Conducts a comprehensive assessment using Online-Mind2Web, a benchmark with 300 diverse tasks, and introduces an LLM-as-a-Judge method for evaluation.

Result: Current web agents' competency is overestimated; the new benchmark and evaluation method achieve 85% agreement with human judgment.

Conclusion: The study provides a realistic evaluation framework and highlights gaps in current web agents, guiding future research.

Abstract: As digitalization and cloud technologies evolve, the web is becoming
increasingly important in the modern society. Autonomous web agents based on
large language models (LLMs) hold a great potential in work automation. It is
therefore important to accurately measure and monitor the progression of their
capabilities. In this work, we conduct a comprehensive and rigorous assessment
of the current state of web agents. Our results depict a very different picture
of the competency of current agents, suggesting over-optimism in previously
reported results. This gap can be attributed to shortcomings in existing
benchmarks. We introduce Online-Mind2Web, an online evaluation benchmark
consisting of 300 diverse and realistic tasks spanning 136 websites. It enables
us to evaluate web agents under a setting that approximates how real users use
these agents. To facilitate more scalable evaluation and development, we also
develop a novel LLM-as-a-Judge automatic evaluation method and show that it can
achieve around 85% agreement with human judgment, substantially higher than
existing methods. Finally, we present the first comprehensive comparative
analysis of current web agents, highlighting both their strengths and
limitations to inspire future research.

</details>


### [372] [The Leaderboard Illusion](https://arxiv.org/pdf/2504.20879)
*Shivalika Singh, Yiyang Nan, Alex Wang, Daniel D'Souza, Sayash Kapoor, Ahmet Üstün, Sanmi Koyejo, Yuntian Deng, Shayne Longpre, Noah A. Smith, Beyza Ermis, Marzieh Fadaee, Sara Hooker*

Main category: cs.AI

TL;DR: The paper highlights systematic biases in Chatbot Arena's leaderboard due to undisclosed private testing, selective disclosure, and data access asymmetries, favoring proprietary models over open-source ones.


<details>
  <summary>Details</summary>
Motivation: To address distortions in AI benchmarking, particularly in Chatbot Arena, where undisclosed practices and data access disparities skew results.

Method: Analysis of private testing practices, selective disclosure, and data distribution among model providers, including Meta, Google, and OpenAI.

Result: Proprietary models receive more data and fewer removals, leading to biased scores. Limited additional data can boost performance by up to 112%.

Conclusion: Reforms are needed for fairer, more transparent benchmarking to avoid overfitting to Arena-specific dynamics and ensure general model quality.

Abstract: Measuring progress is fundamental to the advancement of any scientific field.
As benchmarks play an increasingly central role, they also grow more
susceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard
for ranking the most capable AI systems. Yet, in this work we identify
systematic issues that have resulted in a distorted playing field. We find that
undisclosed private testing practices benefit a handful of providers who are
able to test multiple variants before public release and retract scores if
desired. We establish that the ability of these providers to choose the best
score leads to biased Arena scores due to selective disclosure of performance
results. At an extreme, we identify 27 private LLM variants tested by Meta in
the lead-up to the Llama-4 release. We also establish that proprietary closed
models are sampled at higher rates (number of battles) and have fewer models
removed from the arena than open-weight and open-source alternatives. Both
these policies lead to large data access asymmetries over time. Providers like
Google and OpenAI have received an estimated 19.2% and 20.4% of all data on the
arena, respectively. In contrast, a combined 83 open-weight models have only
received an estimated 29.7% of the total data. We show that access to Chatbot
Arena data yields substantial benefits; even limited additional data can result
in relative performance gains of up to 112% on the arena distribution, based on
our conservative estimates. Together, these dynamics result in overfitting to
Arena-specific dynamics rather than general model quality. The Arena builds on
the substantial efforts of both the organizers and an open community that
maintains this valuable evaluation platform. We offer actionable
recommendations to reform the Chatbot Arena's evaluation framework and promote
fairer, more transparent benchmarking for the field

</details>


### [373] [Lean Copilot: Large Language Models as Copilots for Theorem Proving in Lean](https://arxiv.org/pdf/2404.12534)
*Peiyang Song, Kaiyu Yang, Anima Anandkumar*

Main category: cs.AI

TL;DR: Lean Copilot integrates LLMs into Lean for theorem proving, outperforming rule-based methods by reducing manual steps and automating more proofs.


<details>
  <summary>Details</summary>
Motivation: Existing neural theorem provers struggle with fully autonomous novel theorem proving, requiring human insights. Lean Copilot aims to assist humans in theorem proving by leveraging LLMs.

Method: Introduces Lean Copilot, a framework for running LLM inference in Lean, enabling tools for proof suggestions, goal completion, and premise selection.

Result: Lean Copilot reduces manual proof steps to 2.08 (vs. 3.86 by aesop) and automates 74.2% of steps (vs. 40.1% by aesop).

Conclusion: Lean Copilot effectively enhances theorem proving with LLMs, offering a flexible and powerful tool for Lean users, with open-sourced code for further research.

Abstract: Neural theorem proving combines large language models (LLMs) with proof
assistants such as Lean, where the correctness of formal proofs can be
rigorously verified, leaving no room for hallucination. With existing neural
theorem provers pretrained on a fixed collection of data and offering valuable
suggestions at times, it is challenging for them to continually prove novel
theorems in a fully autonomous mode, where human insights may be critical. In
this paper, we explore LLMs as copilots that assist humans in proving theorems.
We introduce Lean Copilot, a general framework for running LLM inference
natively in Lean. It enables programmers to build various LLM-based proof
automation tools that integrate seamlessly into the workflow of Lean users.
Lean users can use our pretrained models or bring their own ones that run
either locally (with or without GPUs) or on the cloud. Using Lean Copilot, we
build LLM-based tools that suggest proof steps, complete proof goals, and
select relevant premises. Experimental results on the Mathematics in Lean
textbook demonstrate the effectiveness of our method compared to existing
rule-based proof automation in Lean (aesop). When assisting humans, Lean
Copilot requires only 2.08 manually-entered proof steps on average (3.86
required by aesop); when automating the theorem proving process, Lean Copilot
automates 74.2% proof steps on average, 85% better than aesop (40.1%). We open
source all code and artifacts under a permissive MIT license to facilitate
further research.

</details>


### [374] [Enhancing Monotonic Modeling with Spatio-Temporal Adaptive Awareness in Diverse Marketing](https://arxiv.org/pdf/2406.14132)
*Bin Li, Jiayan Pei, Feiyang Xiao, Yifan Zhao, Zhixing Zhang, Diwei Liu, HengXu He, Jia Jia*

Main category: cs.AI

TL;DR: The paper proposes CoMAN, a method for efficient budget allocation in OFOS marketing by predicting user sensitivity to incentives and ensuring spatio-temporal adaptability.


<details>
  <summary>Details</summary>
Motivation: Address challenges in OFOS marketing: efficient budget allocation and spatio-temporal robustness in diverse campaigns.

Method: Introduces CoMAN with spatio-temporal perception modules, learning convexity/concavity, and sensitivity functions.

Result: CoMAN improves incentive allocation, boosts conversion rates, and maintains budget efficiency, outperforming existing methods.

Conclusion: CoMAN effectively addresses OFOS marketing challenges, enhancing efficiency and adaptability in dynamic campaigns.

Abstract: In the mobile internet era, the Online Food Ordering Service (OFOS) emerges
as an integral component of inclusive finance owing to the convenience it
brings to people. OFOS platforms offer dynamic allocation incentives to users
and merchants through diverse marketing campaigns to encourage payments while
maintaining the platforms' budget efficiency. Despite significant progress, the
marketing domain continues to face two primary challenges: (i) how to allocate
a limited budget with greater efficiency, demanding precision in predicting
users' monotonic response (i.e. sensitivity) to incentives, and (ii) ensuring
spatio-temporal adaptability and robustness in diverse marketing campaigns
across different times and locations. To address these issues, we propose a
Constrained Monotonic Adaptive Network (CoMAN) method for spatio-temporal
perception within marketing pricing. Specifically, we capture spatio-temporal
preferences within attribute features through two foundational spatio-temporal
perception modules. To further enhance catching the user sensitivity
differentials to incentives across varied times and locations, we design
modules for learning spatio-temporal convexity and concavity as well as for
expressing sensitivity functions. CoMAN can achieve a more efficient allocation
of incentive investments during pricing, thus increasing the conversion rate
and orders while maintaining budget efficiency. Extensive offline and online
experimental results within our diverse marketing campaigns demonstrate the
effectiveness of the proposed approach while outperforming the monotonic
state-of-the-art method.

</details>


### [375] [Artificial Neural Networks on Graded Vector Spaces](https://arxiv.org/pdf/2407.19031)
*Tony Shaska*

Main category: cs.AI

TL;DR: A framework for graded neural networks, leveraging graded vector spaces to model hierarchical data, outperforming traditional networks in tasks like predicting invariants and modeling supersymmetric systems.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of classical neural networks in handling hierarchical and structured data, particularly in fields like algebraic geometry and physics, by incorporating graded algebraic properties.

Method: Extends neural networks with graded neurons, layers, and activation functions, grounded in group actions, representation theory, and graded algebra. Introduces graded architectures, loss functions, and equivariant extensions.

Result: Validated through case studies, the framework outperforms standard neural networks in tasks such as predicting invariants in weighted projective spaces and modeling supersymmetric systems.

Conclusion: Establishes a new frontier in machine learning by merging mathematical rigor with interdisciplinary applications, with future challenges in scalability and finite field extensions.

Abstract: This paper presents a transformative framework for artificial neural networks
over graded vector spaces, tailored to model hierarchical and structured data
in fields like algebraic geometry and physics. By exploiting the algebraic
properties of graded vector spaces, where features carry distinct weights, we
extend classical neural networks with graded neurons, layers, and activation
functions that preserve structural integrity. Grounded in group actions,
representation theory, and graded algebra, our approach combines theoretical
rigor with practical utility.
  We introduce graded neural architectures, loss functions prioritizing graded
components, and equivariant extensions adaptable to diverse gradings. Case
studies validate the framework's effectiveness, outperforming standard neural
networks in tasks such as predicting invariants in weighted projective spaces
and modeling supersymmetric systems.
  This work establishes a new frontier in machine learning, merging
mathematical sophistication with interdisciplinary applications. Future
challenges, including computational scalability and finite field extensions,
offer rich opportunities for advancing this paradigm.

</details>


### [376] [Long Term Memory: The Foundation of AI Self-Evolution](https://arxiv.org/pdf/2410.15665)
*Xun Jiang, Feng Li, Han Zhao, Jiahao Qiu, Jiaying Wang, Jun Shao, Shihao Xu, Shu Zhang, Weiling Chen, Xavier Tang, Yize Chen, Mengyue Wu, Weizhi Ma, Mengdi Wang, Tianqiao Chen*

Main category: cs.AI

TL;DR: The paper explores AI self-evolution through long-term memory (LTM) during inference, enabling models to improve iteratively with limited data, inspired by human cognitive processes.


<details>
  <summary>Details</summary>
Motivation: Current LLMs rely on large-scale training, but self-evolution during inference is crucial for lifelong learning and adaptability, inspired by human cognitive development.

Method: Proposes LTM to store and manage interaction data, enabling models to evolve iteratively. Tests a multi-agent framework (OMNE) on the GAIA benchmark.

Result: OMNE achieved first place on GAIA, demonstrating LTM's effectiveness for AI self-evolution.

Conclusion: LTM is vital for advancing AI self-evolution and practical applications, with future research needed to refine its implementation.

Abstract: Large language models (LLMs) like GPTs, trained on vast datasets, have
demonstrated impressive capabilities in language understanding, reasoning, and
planning, achieving human-level performance in various tasks. Most studies
focus on enhancing these models by training on ever-larger datasets to build
more powerful foundation models. While training stronger models is important,
enabling models to evolve during inference is equally crucial, a process we
refer to as AI self-evolution. Unlike large-scale training, self-evolution may
rely on limited data or interactions. Inspired by the columnar organization of
the human cerebral cortex, we hypothesize that AI models could develop
cognitive abilities and build internal representations through iterative
interactions with their environment. To achieve this, models need long-term
memory (LTM) to store and manage processed interaction data. LTM supports
self-evolution by representing diverse experiences across environments and
agents. In this report, we explore AI self-evolution and its potential to
enhance models during inference. We examine LTM's role in lifelong learning,
allowing models to evolve based on accumulated interactions. We outline the
structure of LTM and the systems needed for effective data retention and
representation. We also classify approaches for building personalized models
with LTM data and show how these models achieve self-evolution through
interaction. Using LTM, our multi-agent framework OMNE achieved first place on
the GAIA benchmark, demonstrating LTM's potential for AI self-evolution.
Finally, we present a roadmap for future research, emphasizing the importance
of LTM for advancing AI technology and its practical applications.

</details>


### [377] [Learning Fair and Preferable Allocations through Neural Network](https://arxiv.org/pdf/2410.17500)
*Ryota Maruo, Koh Takeuchi, Hisashi Kashima*

Main category: cs.AI

TL;DR: The paper proposes a neural round robin (NRR) method to learn implicit allocation mechanisms from examples while ensuring envy-freeness up to one good (EF1), outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing heuristic rules for resource allocation are hard to formalize mathematically, and human biases can lead to unfair outcomes. The goal is to design mechanisms that replicate expert knowledge while strictly satisfying fairness constraints.

Method: The authors developed NRR, a neural network parameterizing round robin (RR), using supervised learning on examples of valuations and allocations. NRR is trained to learn the agent ordering for RR.

Result: Experiments show NRR outperforms baselines in predicting allocations that are closer to the desired EF1 outcomes.

Conclusion: The NRR method effectively learns implicit allocation mechanisms while adhering to fairness constraints, offering a promising approach for fair resource allocation.

Abstract: The fair allocation of indivisible resources is a fundamental problem.
Existing research has developed various allocation mechanisms or algorithms to
satisfy different fairness notions. For example, round robin (RR) was proposed
to meet the fairness criterion known as envy-freeness up to one good (EF1).
Expert algorithms without mathematical formulations are used in real-world
resource allocation problems to find preferable outcomes for users. Therefore,
we aim to design mechanisms that strictly satisfy good properties with
replicating expert knowledge. However, this problem is challenging because such
heuristic rules are often difficult to formalize mathematically, complicating
their integration into theoretical frameworks. Additionally, formal algorithms
struggle to find preferable outcomes, and directly replicating these implicit
rules can result in unfair allocations because human decision-making can
introduce biases. In this paper, we aim to learn implicit allocation mechanisms
from examples while strictly satisfying fairness constraints, specifically
focusing on learning EF1 allocation mechanisms through supervised learning on
examples of reported valuations and corresponding allocation outcomes produced
by implicit rules. To address this, we developed a neural RR (NRR), a novel
neural network that parameterizes RR. NRR is built from a differentiable
relaxation of RR and can be trained to learn the agent ordering used for RR. We
conducted experiments to learn EF1 allocation mechanisms from examples,
demonstrating that our method outperforms baselines in terms of the proximity
of predicted allocations and other metrics.

</details>


### [378] [Hybrid Local Causal Discovery](https://arxiv.org/pdf/2412.19507)
*Zhaolong Ling, Honghui Peng, Yiwen Zhang, Debo Cheng, Xingyu Wu, Peng Zhou, Kui Yu*

Main category: cs.AI

TL;DR: HLCD is a hybrid algorithm combining constraint-based and score-based methods to improve local causal discovery by reducing errors and avoiding incorrect results from equivalence classes.


<details>
  <summary>Details</summary>
Motivation: Existing methods using AND or OR rules alone lead to cascading errors, while global methods may return incorrect results due to local equivalence classes.

Method: HLCD uses a constraint-based approach with the OR rule for a candidate skeleton, then a score-based method to refine it. It distinguishes V-structures from equivalence classes during orientation.

Result: HLCD outperforms seven competitors on 14 benchmark datasets.

Conclusion: HLCD effectively addresses issues in local causal discovery by combining constraint and score-based methods, achieving superior performance.

Abstract: Local causal discovery aims to learn and distinguish the direct causes and
effects of a target variable from observed data. Existing constraint-based
local causal discovery methods use AND or OR rules in constructing the local
causal skeleton, but using either rule alone is prone to produce cascading
errors in the learned local causal skeleton, and thus impacting the inference
of local causal relationships. On the other hand, directly applying score-based
global causal discovery methods to local causal discovery may randomly return
incorrect results due to the existence of local equivalence classes. To address
the above issues, we propose a Hybrid Local Causal Discovery algorithm, called
HLCD. Specifically, HLCD initially utilizes a constraint-based approach
combined with the OR rule to obtain a candidate skeleton and then employs a
score-based method to eliminate redundant portions in the candidate skeleton.
Furthermore, during the local causal orientation phase, HLCD distinguishes
between V-structures and equivalence classes by comparing the local structure
scores between the two, thereby avoiding orientation interference caused by
local equivalence classes. We conducted extensive experiments with seven
state-of-the-art competitors on 14 benchmark Bayesian network datasets, and the
experimental results demonstrate that HLCD significantly outperforms existing
local causal discovery algorithms.

</details>


### [379] [Large Language Models Think Too Fast To Explore Effectively](https://arxiv.org/pdf/2501.18009)
*Lan Pan, Hanbo Xie, Robert C. Wilson*

Main category: cs.AI

TL;DR: LLMs' exploration ability in open-ended tasks is studied using Little Alchemy 2. Most LLMs underperform humans, except the o1 model, due to fast, less detailed reasoning. DeepSeek shows more human-like exploration. SAE analysis reveals LLMs process uncertainty early and empowerment late, limiting exploration.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' exploration capacity, an essential but underexplored aspect of intelligence, and compare it to human performance in open-ended tasks.

Method: Using Little Alchemy 2, LLMs and humans combine elements to discover new ones. Performance and strategies (uncertainty-driven vs. empowerment-balanced) are analyzed. Sparse Autoencoders (SAE) examine model representations.

Result: Most LLMs underperform humans; o1 model is an exception. Traditional LLMs (e.g., GPT-4o) reason quickly but superficially, while DeepSeek exhibits iterative, human-like exploration. SAE shows LLMs process uncertainty early and empowerment late, hindering exploration.

Conclusion: LLMs' exploration is limited by fast, shallow reasoning and imbalanced processing of uncertainty and empowerment. Improvements should focus on more iterative, human-like strategies.

Abstract: Large Language Models (LLMs) have emerged with many intellectual capacities.
While numerous benchmarks assess their intelligence, limited attention has been
given to their ability to explore--an essential capacity for discovering new
information and adapting to novel environments in both natural and artificial
systems. The extent to which LLMs can effectively explore, particularly in
open-ended tasks, remains unclear. This study investigates whether LLMs can
surpass humans in exploration during an open-ended task, using Little Alchemy 2
as a paradigm, where agents combine elements to discover new ones. Results show
most LLMs underperform compared to humans, except for the o1 model, with
traditional LLMs relying primarily on uncertainty-driven strategies, unlike
humans who balance uncertainty and empowerment. Results indicate that
traditional reasoning-focused LLMs, such as GPT-4o, exhibit a significantly
faster and less detailed reasoning process, limiting their exploratory
performance. In contrast, the DeepSeek reasoning model demonstrates prolonged,
iterative thought processes marked by repetitive analysis of combinations and
past trials, reflecting a more thorough and human-like exploration strategy.
Representational analysis of the models with Sparse Autoencoders (SAE) revealed
that uncertainty and choices are represented at earlier transformer blocks,
while empowerment values are processed later, causing LLMs to think too fast
and make premature decisions, hindering effective exploration. These findings
shed light on the limitations of LLM exploration and suggest directions for
improving their adaptability.

</details>


### [380] [Logical Modalities within the European AI Act: An Analysis](https://arxiv.org/pdf/2501.19112)
*Lara Lawniczak, Christoph Benzmüller*

Main category: cs.AI

TL;DR: The paper analyzes the European AI Act's logical modalities for formal representation using LogiKEy, embedding selected logics in HOL for automated reasoning and identifying challenges.


<details>
  <summary>Details</summary>
Motivation: To prepare a formal representation of the European AI Act's logical modalities within the LogiKEy framework for normative reasoning.

Method: Uses Higher-Order Logic (HOL) and Isabelle/HOL to embed diverse logics, encoding sample paragraphs and evaluating embeddings for automated reasoning.

Result: Initial experiments assess the suitability of embeddings and highlight challenges for robust reasoning.

Conclusion: The study advances formal representation of the AI Act but identifies key challenges for future work.

Abstract: The paper presents a comprehensive analysis of the European AI Act in terms
of its logical modalities, with the aim of preparing its formal representation,
for example, within the logic-pluralistic Knowledge Engineering Framework and
Methodology (LogiKEy). LogiKEy develops computational tools for normative
reasoning based on formal methods, employing Higher-Order Logic (HOL) as a
unifying meta-logic to integrate diverse logics through shallow semantic
embeddings. This integration is facilitated by Isabelle/HOL, a proof assistant
tool equipped with several automated theorem provers. The modalities within the
AI Act and the logics suitable for their representation are discussed. For a
selection of these logics, embeddings in HOL are created, which are then used
to encode sample paragraphs. Initial experiments evaluate the suitability of
these embeddings for automated reasoning, and highlight key challenges on the
way to more robust reasoning capabilities.

</details>


### [381] [Unbiased Evaluation of Large Language Models from a Causal Perspective](https://arxiv.org/pdf/2502.06655)
*Meilin Chen, Jian Tian, Liang Ma, Di Xie, Weijie Chen, Jiang Zhu*

Main category: cs.AI

TL;DR: The paper addresses biases in Agents-as-an-Evaluator methods for LLM evaluation, proposes the Unbiased Evaluator protocol, and demonstrates its effectiveness in providing unbiased and interpretable assessments.


<details>
  <summary>Details</summary>
Motivation: Benchmark contamination and unexplored biases in current evaluation methods motivate the need for a more robust and unbiased evaluation protocol.

Method: The authors theoretically formulate evaluation bias, identify biases in Agents-as-an-Evaluator, and propose the Unbiased Evaluator protocol.

Result: Experiments show significant improvement in LLM evaluation, with the Unbiased Evaluator providing strong evidence of benchmark contamination and interpretable results.

Conclusion: The Unbiased Evaluator offers a comprehensive and unbiased solution for LLM evaluation, addressing current limitations.

Abstract: Benchmark contamination has become a significant concern in the LLM
evaluation community. Previous Agents-as-an-Evaluator address this issue by
involving agents in the generation of questions. Despite their success, the
biases in Agents-as-an-Evaluator methods remain largely unexplored. In this
paper, we present a theoretical formulation of evaluation bias, providing
valuable insights into designing unbiased evaluation protocols. Furthermore, we
identify two type of bias in Agents-as-an-Evaluator through carefully designed
probing tasks on a minimal Agents-as-an-Evaluator setup. To address these
issues, we propose the Unbiased Evaluator, an evaluation protocol that delivers
a more comprehensive, unbiased, and interpretable assessment of LLMs.Extensive
experiments reveal significant room for improvement in current LLMs.
Additionally, we demonstrate that the Unbiased Evaluator not only offers strong
evidence of benchmark contamination but also provides interpretable evaluation
results.

</details>


### [382] [D-CIPHER: Dynamic Collaborative Intelligent Multi-Agent System with Planner and Heterogeneous Executors for Offensive Security](https://arxiv.org/pdf/2502.10931)
*Meet Udeshi, Minghao Shao, Haoran Xi, Nanda Rani, Kimberly Milner, Venkata Sai Charan Putrevu, Brendan Dolan-Gavitt, Sandeep Kumar Shukla, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri, Muhammad Shafique*

Main category: cs.AI

TL;DR: D-CIPHER is a multi-agent LLM framework for collaborative CTF solving, outperforming single-agent systems with dynamic feedback and role specialization.


<details>
  <summary>Details</summary>
Motivation: Single-agent LLM systems are inadequate for complex CTF tasks, while real-world CTF competitions rely on team collaboration.

Method: D-CIPHER uses a Planner-Executor system with distinct roles, dynamic feedback loops, and an Auto-prompter for initial prompt generation.

Result: Achieves state-of-the-art performance on benchmarks (22.0%-44.0%) and solves 65% more MITRE ATT&CK techniques than prior work.

Conclusion: D-CIPHER demonstrates superior offensive security capability and efficiency in collaborative CTF solving.

Abstract: Large Language Models (LLMs) have been used in cybersecurity such as
autonomous security analysis or penetration testing. Capture the Flag (CTF)
challenges serve as benchmarks to assess automated task-planning abilities of
LLM agents for cybersecurity. Early attempts to apply LLMs for solving CTF
challenges used single-agent systems, where feedback was restricted to a single
reasoning-action loop. This approach was inadequate for complex CTF tasks.
Inspired by real-world CTF competitions, where teams of experts collaborate, we
introduce the D-CIPHER LLM multi-agent framework for collaborative CTF solving.
D-CIPHER integrates agents with distinct roles with dynamic feedback loops to
enhance reasoning on complex tasks. It introduces the Planner-Executor agent
system, consisting of a Planner agent for overall problem-solving along with
multiple heterogeneous Executor agents for individual tasks, facilitating
efficient allocation of responsibilities among the agents. Additionally,
D-CIPHER incorporates an Auto-prompter agent to improve problem-solving by
auto-generating a highly relevant initial prompt. We evaluate D-CIPHER on
multiple CTF benchmarks and LLM models via comprehensive studies to highlight
the impact of our enhancements. Additionally, we manually map the CTFs in NYU
CTF Bench to MITRE ATT&CK techniques that apply for a comprehensive evaluation
of D-CIPHER's offensive security capability. D-CIPHER achieves state-of-the-art
performance on three benchmarks: 22.0% on NYU CTF Bench, 22.5% on Cybench, and
44.0% on HackTheBox, which is 2.5% to 8.5% better than previous work. D-CIPHER
solves 65% more ATT&CK techniques compared to previous work, demonstrating
stronger offensive capability.

</details>


### [383] [Using Language Models to Decipher the Motivation Behind Human Behaviors](https://arxiv.org/pdf/2503.15752)
*Yutong Xie, Qiaozhu Mei, Walter Yuan, Matthew O. Jackson*

Main category: cs.AI

TL;DR: AI uses language model prompts to analyze human behavior motivations in economic games, revealing insights into behavioral tendencies and scenario framing.


<details>
  <summary>Details</summary>
Motivation: To decipher the motivations behind human behaviors and understand differences in behavioral tendencies across populations using AI.

Method: Varying prompts to a large language model to elicit human behaviors in classic economic games and analyzing the results.

Result: AI provides insights into the motivations behind behaviors, relationships between economic scenarios, and differences in population tendencies.

Conclusion: AI offers a novel tool for examining the thinking and framing that drive human behaviors in economic contexts.

Abstract: AI presents a novel tool for deciphering the motivations behind human
behaviors. By varying prompts to a large language model, we can elicit the full
range of human behaviors in a variety of different scenarios in classic
economic games. By analyzing which prompts elicit which behaviors, we infer
(decipher) the motivations behind the human behaviors. We also show how one can
analyze the prompts to reveal relationships between the classic economic games,
providing insight into what different economic scenarios induce people to think
about. We also show how this deciphering process can be used to understand
differences in the behavioral tendencies of different populations. We show how
AI offers a new way to examine the thinking and framing that produce different
behaviors.

</details>


### [384] [AdaWorld: Learning Adaptable World Models with Latent Actions](https://arxiv.org/pdf/2503.18938)
*Shenyuan Gao, Siyuan Zhou, Yilun Du, Jun Zhang, Chuang Gan*

Main category: cs.AI

TL;DR: AdaWorld is a novel world model learning approach that uses self-supervised latent actions for efficient adaptation to new environments with limited data.


<details>
  <summary>Details</summary>
Motivation: Existing world models require extensive action-labeled data and costly training, limiting adaptability to novel environments.

Method: AdaWorld extracts latent actions from videos self-supervisedly and trains an autoregressive world model conditioned on these actions.

Result: AdaWorld outperforms in simulation quality and visual planning across multiple environments.

Conclusion: AdaWorld enables efficient adaptation and transfer of world models with minimal interaction and finetuning.

Abstract: World models aim to learn action-controlled future prediction and have proven
essential for the development of intelligent agents. However, most existing
world models rely heavily on substantial action-labeled data and costly
training, making it challenging to adapt to novel environments with
heterogeneous actions through limited interactions. This limitation can hinder
their applicability across broader domains. To overcome this limitation, we
propose AdaWorld, an innovative world model learning approach that enables
efficient adaptation. The key idea is to incorporate action information during
the pretraining of world models. This is achieved by extracting latent actions
from videos in a self-supervised manner, capturing the most critical
transitions between frames. We then develop an autoregressive world model that
conditions on these latent actions. This learning paradigm enables highly
adaptable world models, facilitating efficient transfer and learning of new
actions even with limited interactions and finetuning. Our comprehensive
experiments across multiple environments demonstrate that AdaWorld achieves
superior performance in both simulation quality and visual planning.

</details>


### [385] [A Computational Theory for Efficient Mini Agent Evaluation with Causal Guarantees](https://arxiv.org/pdf/2503.21138)
*Hedong Yan*

Main category: cs.AI

TL;DR: A computational theory for evaluating mini agents reduces evaluation costs by building models, proving error bounds, and using meta-learning for heterogeneous agents. It cuts errors by 24.1-99.0% and speeds up evaluation significantly.


<details>
  <summary>Details</summary>
Motivation: To reduce the high cost of experimental evaluation for agents by developing a computational evaluation model.

Method: Proposes a meta-learner for heterogeneous agents, proves error bounds, and uses prediction for efficiency and consistency.

Result: Reduced evaluation errors by 24.1-99.0% across 12 scenes and sped up evaluation by 3-7 orders of magnitude.

Conclusion: The method offers a scalable, efficient alternative to traditional evaluation, with proven error bounds and significant time savings.

Abstract: In order to reduce the cost of experimental evaluation for agents, we
introduce a computational theory of evaluation for mini agents: build
evaluation model to accelerate the evaluation procedures. We prove upper bounds
of generalized error and generalized causal effect error of given evaluation
models for infinite agents. We also prove efficiency, and consistency to
estimated causal effect from deployed agents to evaluation metric by
prediction. To learn evaluation models, we propose a meta-learner to handle
heterogeneous agents space problem. Comparing with existed evaluation
approaches, our (conditional) evaluation model reduced 24.1\% to 99.0\%
evaluation errors across 12 scenes, including individual medicine, scientific
simulation, social experiment, business activity, and quantum trade. The
evaluation time is reduced 3 to 7 order of magnitude per subject comparing with
experiments or simulations.

</details>


### [386] [A Survey of WebAgents: Towards Next-Generation AI Agents for Web Automation with Large Foundation Models](https://arxiv.org/pdf/2503.23350)
*Liangbo Ning, Ziran Liang, Zhuohang Jiang, Haohao Qu, Yujuan Ding, Wenqi Fan, Xiao-yong Wei, Shanru Lin, Hui Liu, Philip S. Yu, Qing Li*

Main category: cs.AI

TL;DR: The paper surveys WebAgents, AI-driven autonomous agents for automating repetitive web tasks, leveraging Large Foundation Models (LFMs) for enhanced productivity and efficiency.


<details>
  <summary>Details</summary>
Motivation: Repetitive web tasks negatively impact quality of life; AI Agents (WebAgents) can automate these tasks, improving convenience and productivity.

Method: The survey reviews existing research on WebAgents, focusing on architectures, training, and trustworthiness.

Result: LFMs show promise in developing powerful WebAgents capable of handling complex web tasks.

Conclusion: The paper highlights the potential of WebAgents and suggests future research directions to further explore their capabilities.

Abstract: With the advancement of web techniques, they have significantly
revolutionized various aspects of people's lives. Despite the importance of the
web, many tasks performed on it are repetitive and time-consuming, negatively
impacting overall quality of life. To efficiently handle these tedious daily
tasks, one of the most promising approaches is to advance autonomous agents
based on Artificial Intelligence (AI) techniques, referred to as AI Agents, as
they can operate continuously without fatigue or performance degradation. In
the context of the web, leveraging AI Agents -- termed WebAgents -- to
automatically assist people in handling tedious daily tasks can dramatically
enhance productivity and efficiency. Recently, Large Foundation Models (LFMs)
containing billions of parameters have exhibited human-like language
understanding and reasoning capabilities, showing proficiency in performing
various complex tasks. This naturally raises the question: `Can LFMs be
utilized to develop powerful AI Agents that automatically handle web tasks,
providing significant convenience to users?' To fully explore the potential of
LFMs, extensive research has emerged on WebAgents designed to complete daily
web tasks according to user instructions, significantly enhancing the
convenience of daily human life. In this survey, we comprehensively review
existing research studies on WebAgents across three key aspects: architectures,
training, and trustworthiness. Additionally, several promising directions for
future research are explored to provide deeper insights.

</details>


### [387] [The Geometry of Self-Verification in a Task-Specific Reasoning Model](https://arxiv.org/pdf/2504.14379)
*Andrew Lee, Lihao Sun, Chris Wendler, Fernanda Viégas, Martin Wattenberg*

Main category: cs.AI

TL;DR: The paper investigates how reasoning models self-verify answers, using a model trained on the CountDown task. It identifies key components like GLU weights and attention heads involved in verification.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms behind self-verification in reasoning models, leveraging preference tuning and structured outputs.

Method: Top-down and bottom-up analyses are conducted, focusing on GLU weights and attention heads to reverse-engineer verification processes.

Result: GLU weights encode verification tokens, and specific attention heads (e.g., 'previous-token heads') are crucial for self-verification. A minimal circuit of three heads is identified.

Conclusion: The study reveals a verification circuit in reasoning models, with findings applicable to base and general reasoning models like DeepSeek-R1.

Abstract: How do reasoning models verify their own answers? We study this question by
training a model using DeepSeek R1's recipe on the CountDown task. We leverage
the fact that preference tuning leads to mode collapse, yielding a model that
always produces highly structured chain-of-thought sequences. With this setup,
we do top-down and bottom-up analyses to reverse-engineer how the model
verifies its outputs. Top-down, we find Gated Linear Unit (GLU) weights
encoding verification-related tokens, such as ``success'' or ``incorrect''.
Bottom-up, we find that ``previous-token heads'' are mainly responsible for
self-verification in our setup. Our analyses meet in the middle: drawing
inspiration from inter-layer communication channels, we use the identified GLU
weights to localize as few as three attention heads that can disable
self-verification, pointing to a necessary component of a potentially larger
verification circuit. Finally, we verify that similar verification components
exist in our base model and a general reasoning DeepSeek-R1 model.

</details>


### [388] [ApproXAI: Energy-Efficient Hardware Acceleration of Explainable AI using Approximate Computing](https://arxiv.org/pdf/2504.17929)
*Ayesha Siddique, Khurram Khalil, Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: XAIedge improves energy efficiency in real-time XAI by using approximate computing, achieving 2× better efficiency than existing methods.


<details>
  <summary>Details</summary>
Motivation: Current XAI methods are computationally intensive and lack energy efficiency for real-time applications.

Method: XAIedge integrates approximate computing into XAI algorithms (e.g., integrated gradients, Shapley analysis) and optimizes matrix computations for TPU-based edge devices.

Result: XAIedge achieves 2× better energy efficiency while maintaining accuracy.

Conclusion: XAIedge advances real-time XAI deployment in energy-constrained settings.

Abstract: Explainable artificial intelligence (XAI) enhances AI system transparency by
framing interpretability as an optimization problem. However, this approach
often necessitates numerous iterations of computationally intensive operations,
limiting its applicability in real-time scenarios. While recent research has
focused on XAI hardware acceleration on FPGAs and TPU, these methods do not
fully address energy efficiency in real-time settings. To address this
limitation, we propose XAIedge, a novel framework that leverages approximate
computing techniques into XAI algorithms, including integrated gradients, model
distillation, and Shapley analysis. XAIedge translates these algorithms into
approximate matrix computations and exploits the synergy between convolution,
Fourier transform, and approximate computing paradigms. This approach enables
efficient hardware acceleration on TPU-based edge devices, facilitating faster
real-time outcome interpretations. Our comprehensive evaluation demonstrates
that XAIedge achieves a $2\times$ improvement in energy efficiency compared to
existing accurate XAI hardware acceleration techniques while maintaining
comparable accuracy. These results highlight the potential of XAIedge to
significantly advance the deployment of explainable AI in energy-constrained
real-time applications.

</details>


### [389] [Emotions in Artificial Intelligence](https://arxiv.org/pdf/2505.01462)
*Hermann Borotschnig*

Main category: cs.AI

TL;DR: The paper explores how AI might emulate human/animal emotions, proposing affective tags in episodic memory for decision-making, and debates the moral status of such AI.


<details>
  <summary>Details</summary>
Motivation: To understand if AI can benefit from emotion-like heuristics for adaptive behavior, similar to biological systems.

Method: A thought experiment where AI uses affective tags in episodic memory to project emotions onto current contexts, aiding decision-making.

Result: Proposes that AI can emulate emotions without consciousness, raising questions about moral standing based on self-awareness.

Conclusion: Argues that mere emotion emulation or consciousness isn't enough for moral status; self-awareness of emotions is key.

Abstract: This conceptual contribution offers a speculative account of how AI systems
might emulate emotions as experienced by humans and animals. It presents a
thought experiment grounded in the hypothesis that natural emotions evolved as
heuristics for rapid situational appraisal and action selection, enabling
biologically adaptive behaviour without requiring full deliberative modeling.
The text examines whether artificial systems operating in complex action spaces
could similarly benefit from these principles. It is proposed that affect be
interwoven with episodic memory by storing corresponding affective tags
alongside all events. This allows AIs to establish whether present situations
resemble past events and project the associated emotional labels onto the
current context. These emotional cues are then combined with need-driven
emotional hints. The combined emotional state facilitates decision-making in
the present by modulating action selection. The low complexity and experiential
inertness of the proposed architecture are emphasized as evidence that
emotional expression and consciousness are, in principle, orthogonal-permitting
the theoretical possibility of affective zombies. On this basis, the moral
status of AIs emulating affective states is critically examined. It is argued
that neither the mere presence of internal representations of emotion nor
consciousness alone suffices for moral standing; rather, the capacity for
self-awareness of inner emotional states is posited as a necessary condition. A
complexity-based criterion is proposed to exclude such awareness in the
presented model. Additional thought experiments are presented to test the
conceptual boundaries of this framework.

</details>


### [390] [Adversarial Cooperative Rationalization: The Risk of Spurious Correlations in Even Clean Datasets](https://arxiv.org/pdf/2505.02118)
*Wei Liu, Zhongyu Niu, Lang Gao, Zhiying Deng, Jun Wang, Haozhao Wang, Ruixuan Li*

Main category: cs.AI

TL;DR: The paper explores a self-rationalization framework with a cooperative game, identifies a sampling bias issue, and proposes a solution to improve performance.


<details>
  <summary>Details</summary>
Motivation: To address the unintentional sampling bias in rationale extraction within cooperative games between generators and predictors.

Method: Theoretical analysis and empirical evidence are used to identify bias origins, followed by introducing an instruction to prevent incorrect correlations. Experiments are conducted on multiple datasets and architectures.

Result: The proposed method outperforms recent rationalization techniques and matches or exceeds the performance of a leading LLM.

Conclusion: The study provides a solution to mitigate bias in rationale extraction, enhancing prediction accuracy and performance.

Abstract: This study investigates the self-rationalization framework constructed with a
cooperative game, where a generator initially extracts the most informative
segment from raw input, and a subsequent predictor utilizes the selected subset
for its input. The generator and predictor are trained collaboratively to
maximize prediction accuracy. In this paper, we first uncover a potential
caveat: such a cooperative game could unintentionally introduce a sampling bias
during rationale extraction. Specifically, the generator might inadvertently
create an incorrect correlation between the selected rationale candidate and
the label, even when they are semantically unrelated in the original dataset.
Subsequently, we elucidate the origins of this bias using both detailed
theoretical analysis and empirical evidence. Our findings suggest a direction
for inspecting these correlations through attacks, based on which we further
introduce an instruction to prevent the predictor from learning the
correlations. Through experiments on six text classification datasets and two
graph classification datasets using three network architectures (GRUs, BERT,
and GCN), we show that our method not only significantly outperforms recent
rationalization methods, but also achieves comparable or even better results
than a representative LLM (llama3.1-8b-instruct).

</details>


### [391] [LLM-Guided Probabilistic Program Induction for POMDP Model Estimation](https://arxiv.org/pdf/2505.02216)
*Aidan Curtis, Hao Tang, Thiago Veloso, Kevin Ellis, Joshua Tenenbaum, Tomás Lozano-Pérez, Leslie Pack Kaelbling*

Main category: cs.AI

TL;DR: Using LLMs to learn low-complexity POMDP models outperforms traditional methods like tabular learning, behavior cloning, or direct LLM planning.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of learning POMDP models, especially those with components modeled as low-complexity probabilistic graphical programs.

Method: Leveraging LLMs as priors to generate candidate probabilistic programs, which are refined via empirical testing and feedback.

Result: Effective learning of POMDP models in toy problems, MiniGrid domains, and real robotics tasks, surpassing traditional methods.

Conclusion: LLM-guided POMDP model construction is a promising approach for learning under uncertainty.

Abstract: Partially Observable Markov Decision Processes (POMDPs) model decision making
under uncertainty. While there are many approaches to approximately solving
POMDPs, we aim to address the problem of learning such models. In particular,
we are interested in a subclass of POMDPs wherein the components of the model,
including the observation function, reward function, transition function, and
initial state distribution function, can be modeled as low-complexity
probabilistic graphical models in the form of a short probabilistic program.
Our strategy to learn these programs uses an LLM as a prior, generating
candidate probabilistic programs that are then tested against the empirical
distribution and adjusted through feedback. We experiment on a number of
classical toy POMDP problems, simulated MiniGrid domains, and two real
mobile-base robotics search domains involving partial observability. Our
results show that using an LLM to guide in the construction of a low-complexity
POMDP model can be more effective than tabular POMDP learning, behavior
cloning, or direct LLM planning.

</details>


### [392] [A Pain Assessment Framework based on multimodal data and Deep Machine Learning methods](https://arxiv.org/pdf/2505.05396)
*Stefanos Gkikas*

Main category: cs.AI

TL;DR: The thesis explores pain assessment from clinical and computational perspectives, aiming to develop high-performing, clinically applicable methods. It investigates demographic impacts on pain perception and proposes unimodal/multimodal pipelines, achieving state-of-the-art results and advancing AI research.


<details>
  <summary>Details</summary>
Motivation: To bridge clinical pain assessment with computational methods, addressing gaps in existing approaches and incorporating demographic factors.

Method: Developed computational pipelines for automatic pain assessment, tested in unimodal and multimodal configurations.

Result: Achieved state-of-the-art performance, demonstrating effectiveness in clinical settings.

Conclusion: The proposed methods advance pain assessment and open new directions in AI, foundation models, and generative AI.

Abstract: From the original abstract: This thesis initially aims to study the pain
assessment process from a clinical-theoretical perspective while exploring and
examining existing automatic approaches. Building on this foundation, the
primary objective of this Ph.D. project is to develop innovative computational
methods for automatic pain assessment that achieve high performance and are
applicable in real clinical settings. A primary goal is to thoroughly
investigate and assess significant factors, including demographic elements that
impact pain perception, as recognized in pain research, through a computational
standpoint. Within the limits of the available data in this research area, our
goal was to design, develop, propose, and offer automatic pain assessment
pipelines for unimodal and multimodal configurations that are applicable to the
specific requirements of different scenarios. The studies published in this
Ph.D. thesis showcased the effectiveness of the proposed methods, achieving
state-of-the-art results. Additionally, they paved the way for exploring new
approaches in artificial intelligence, foundation models, and generative
artificial intelligence.

</details>


### [393] [APOLLO: Automated LLM and Lean Collaboration for Advanced Formal Reasoning](https://arxiv.org/pdf/2505.05758)
*Azim Ospanov, Farzan Farnia, Roozbeh Yousefzadeh*

Main category: cs.AI

TL;DR: APOLLO is a pipeline combining Lean and LLMs to improve automated theorem proving, achieving high accuracy with low sampling budgets.


<details>
  <summary>Details</summary>
Motivation: Generating correct formal proofs with LLMs is challenging; APOLLO aims to enhance efficiency and correctness by integrating Lean's verification with LLM reasoning.

Method: APOLLO automates proof generation, error analysis, and repair using Lean and LLMs, iterating with a low sampling budget.

Result: Achieves 75.0% accuracy on miniF2F and improves Goedel-Prover-SFT to 65.6%, reducing sample complexity significantly.

Conclusion: Compiler-guided repair of LLM outputs boosts efficiency and correctness, offering a scalable approach for automated theorem proving.

Abstract: Formal reasoning and automated theorem proving constitute a challenging
subfield of machine learning, in which machines are tasked with proving
mathematical theorems using formal languages like Lean. A formal verification
system can check whether a formal proof is correct or not almost
instantaneously, but generating a completely correct formal proof with large
language models (LLMs) remains a formidable task. The usual approach in the
literature is to prompt the LLM many times (up to several thousands) until one
of the generated proofs passes the verification system. In this work, we
present APOLLO (Automated PrOof repair via LLM and Lean cOllaboration), a
modular, model-agnostic pipeline that combines the strengths of the Lean
compiler with an LLM's reasoning abilities to achieve better proof-generation
results at a low sampling budget. Apollo directs a fully automated process in
which the LLM generates proofs for theorems, a set of agents analyze the
proofs, fix the syntax errors, identify the mistakes in the proofs using Lean,
isolate failing sub-lemmas, utilize automated solvers, and invoke an LLM on
each remaining goal with a low top-K budget. The repaired sub-proofs are
recombined and reverified, iterating up to a user-controlled maximum number of
attempts. On the miniF2F benchmark, we establish a new state-of-the-art
accuracy of 75.0% among 7B-parameter models while keeping the sampling budget
below one thousand. Moreover, Apollo raises the state-of-the-art accuracy for
Goedel-Prover-SFT to 65.6% while cutting sample complexity from 25,600 to a few
hundred. General-purpose models (o3-mini, o4-mini) jump from 3-7% to over 40%
accuracy. Our results demonstrate that targeted, compiler-guided repair of LLM
outputs yields dramatic gains in both efficiency and correctness, suggesting a
general paradigm for scalable automated theorem proving.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [394] [Bridging Ears and Eyes: Analyzing Audio and Visual Large Language Models to Humans in Visible Sound Recognition and Reducing Their Sensory Gap via Cross-Modal Distillation](https://arxiv.org/pdf/2505.06803)
*Xilin Jiang, Junkai Wu, Vishal Choudhari, Nima Mesgarani*

Main category: cs.SD

TL;DR: Audio LLMs lag behind visual and audio-visual LLMs in recognizing sound objects, similar to human sensory discrepancies. Cross-modal distillation improves performance, especially in challenging classes.


<details>
  <summary>Details</summary>
Motivation: To explore the performance gap between audio, visual, and audio-visual LLMs compared to humans and propose a method to bridge this gap.

Method: Systematic evaluation of Qwen2-Audio, Qwen2-VL, and Qwen2.5-Omni against humans, followed by cross-modal distillation between LLMs to transfer knowledge.

Result: Audio LLMs underperform visual and audio-visual LLMs, but distillation improves their performance, particularly in difficult sound classes.

Conclusion: The study reveals sensory gaps in LLMs and offers a framework to enhance modality-specific perception through cross-modal distillation.

Abstract: Audio large language models (LLMs) are considered experts at recognizing
sound objects, yet their performance relative to LLMs in other sensory
modalities, such as visual or audio-visual LLMs, and to humans using their
ears, eyes, or both remains unexplored. To investigate this, we systematically
evaluate audio, visual, and audio-visual LLMs, specifically Qwen2-Audio,
Qwen2-VL, and Qwen2.5-Omni, against humans in recognizing sound objects of
different classes from audio-only, silent video, or sounded video inputs. We
uncover a performance gap between Qwen2-Audio and Qwen2-VL that parallels the
sensory discrepancy between human ears and eyes. To reduce this gap, we
introduce a cross-modal distillation framework, where an LLM in one modality
serves as the teacher and another as the student, with knowledge transfer in
sound classes predicted as more challenging to the student by a heuristic
model. Distillation in both directions, from Qwen2-VL to Qwen2-Audio and vice
versa, leads to notable improvements, particularly in challenging classes. This
work highlights the sensory gap in LLMs from a human-aligned perspective and
proposes a principled approach to enhancing modality-specific perception in
multimodal LLMs.

</details>


### [395] [Multi-Domain Audio Question Answering Toward Acoustic Content Reasoning in The DCASE 2025 Challenge](https://arxiv.org/pdf/2505.07365)
*Chao-Han Huck Yang, Sreyan Ghosh, Qing Wang, Jaeyeon Kim, Hengyi Hong, Sonal Kumar, Guirui Zhong, Zhifeng Kong, S Sakshi, Vaibhavi Lokegaonkar, Oriol Nieto, Ramani Duraiswami, Dinesh Manocha, Gunhee Kim, Jun Du, Rafael Valle, Bryan Catanzaro*

Main category: cs.SD

TL;DR: The paper introduces Task 5 of the DCASE 2025 Challenge, focusing on Audio Question Answering (AQA) across diverse sound domains, with three QA subsets and baseline models evaluated for robustness and accuracy.


<details>
  <summary>Details</summary>
Motivation: To advance audio-language models' understanding and reasoning capabilities toward human-level acuity, enabling AI agents to perceive and interact effectively with the world.

Method: The task defines three QA subsets (Bioacoustics, Temporal Soundscapes, Complex QA) and evaluates models using top-1 accuracy with answer-shuffling robustness. Baseline systems include Qwen2-Audio-7B, AudioFlamingo 2, and Gemini-2-Flash.

Result: Preliminary results show strong variation in performance across models and subsets on the development set.

Conclusion: The challenge aims to push the boundaries of audio-language models, enhancing their ability to understand and reason about diverse acoustic scenes.

Abstract: We present Task 5 of the DCASE 2025 Challenge: an Audio Question Answering
(AQA) benchmark spanning multiple domains of sound understanding. This task
defines three QA subsets (Bioacoustics, Temporal Soundscapes, and Complex QA)
to test audio-language models on interactive question-answering over diverse
acoustic scenes. We describe the dataset composition (from marine mammal calls
to soundscapes and complex real-world clips), the evaluation protocol (top-1
accuracy with answer-shuffling robustness), and baseline systems
(Qwen2-Audio-7B, AudioFlamingo 2, Gemini-2-Flash). Preliminary results on the
development set are compared, showing strong variation across models and
subsets. This challenge aims to advance the audio understanding and reasoning
capabilities of audio-language models toward human-level acuity, which are
crucial for enabling AI agents to perceive and interact about the world
effectively.

</details>


### [396] [Beyond Identity: A Generalizable Approach for Deepfake Audio Detection](https://arxiv.org/pdf/2505.06766)
*Yasaman Ahmadiadli, Xiao-Ping Zhang, Naimul Khan*

Main category: cs.SD

TL;DR: The paper proposes an identity-independent audio deepfake detection framework to address identity leakage, using Artifact Detection Modules (ADMs) and dynamic artifact generation techniques, achieving superior performance across datasets.


<details>
  <summary>Details</summary>
Motivation: Deepfake audio poses significant threats like fraud and identity misuse, but current detection models generalize poorly due to identity leakage, where models learn speaker-specific features instead of manipulation artifacts.

Method: The framework employs ADMs to isolate synthetic artifacts in time and frequency domains, alongside novel dynamic artifact generation techniques (e.g., frequency swaps, time manipulations, noise augmentation) to enforce dataset-invariant feature learning.

Result: Experiments on ASVspoof2019, ADD 2022, FoR, and In-The-Wild datasets show F1 scores of 0.230 (ADD 2022), 0.604 (FoR), and 0.813 (In-The-Wild), outperforming baselines, with Dynamic Frequency Swap being the most effective.

Conclusion: Artifact-based learning effectively mitigates identity leakage, enhancing generalization in audio deepfake detection.

Abstract: Deepfake audio presents a growing threat to digital security, due to its
potential for social engineering, fraud, and identity misuse. However, existing
detection models suffer from poor generalization across datasets, due to
implicit identity leakage, where models inadvertently learn speaker-specific
features instead of manipulation artifacts. To the best of our knowledge, this
is the first study to explicitly analyze and address identity leakage in the
audio deepfake detection domain. This work proposes an identity-independent
audio deepfake detection framework that mitigates identity leakage by
encouraging the model to focus on forgery-specific artifacts instead of
overfitting to speaker traits. Our approach leverages Artifact Detection
Modules (ADMs) to isolate synthetic artifacts in both time and frequency
domains, enhancing cross-dataset generalization. We introduce novel dynamic
artifact generation techniques, including frequency domain swaps, time domain
manipulations, and background noise augmentation, to enforce learning of
dataset-invariant features. Extensive experiments conducted on ASVspoof2019,
ADD 2022, FoR, and In-The-Wild datasets demonstrate that the proposed
ADM-enhanced models achieve F1 scores of 0.230 (ADD 2022), 0.604 (FoR), and
0.813 (In-The-Wild), consistently outperforming the baseline. Dynamic Frequency
Swap proves to be the most effective strategy across diverse conditions. These
findings emphasize the value of artifact-based learning in mitigating implicit
identity leakage for more generalizable audio deepfake detection.

</details>


### [397] [Multi-band Frequency Reconstruction for Neural Psychoacoustic Coding](https://arxiv.org/pdf/2505.07235)
*Dianwen Ng, Kun Zhou, Yi-Wen Chao, Zhiwei Xiong, Bin Ma, Eng Siong Chng*

Main category: cs.SD

TL;DR: MUFFIN is a neural audio coding framework that uses psychoacoustic multi-band frequency reconstruction for high-fidelity compression, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The challenge of preserving perceptual quality in neural audio compression across diverse content drives the need for advanced frameworks like MUFFIN.

Method: MUFFIN employs a Multi-Band Spectral Residual Vector Quantization (MBS-RVQ) module, a transformer-inspired convolutional backbone, and modified snake activation for efficient compression and disentanglement of speaker identity.

Result: MUFFIN achieves superior reconstruction quality and a state-of-the-art 12.5 Hz compression rate with minimal loss, also excelling in downstream generative tasks.

Conclusion: MUFFIN shows promise as a high-fidelity audio compression tool and a token representation for integration with language models.

Abstract: Achieving high-fidelity audio compression while preserving perceptual quality
across diverse content remains a key challenge in Neural Audio Coding (NAC). We
introduce MUFFIN, a fully convolutional Neural Psychoacoustic Coding (NPC)
framework that leverages psychoacoustically guided multi-band frequency
reconstruction. At its core is a Multi-Band Spectral Residual Vector
Quantization (MBS-RVQ) module that allocates bitrate across frequency bands
based on perceptual salience. This design enables efficient compression while
disentangling speaker identity from content using distinct codebooks. MUFFIN
incorporates a transformer-inspired convolutional backbone and a modified snake
activation to enhance resolution in fine-grained spectral regions. Experimental
results on multiple benchmarks demonstrate that MUFFIN consistently outperforms
existing approaches in reconstruction quality. A high-compression variant
achieves a state-of-the-art 12.5 Hz rate with minimal loss. MUFFIN also proves
effective in downstream generative tasks, highlighting its promise as a token
representation for integration with language models. Audio samples and code are
available.

</details>


### [398] [Predicting Music Track Popularity by Convolutional Neural Networks on Spotify Features and Spectrogram of Audio Waveform](https://arxiv.org/pdf/2505.07280)
*Navid Falah, Behnam Yousefimehr, Mehdi Ghatee*

Main category: cs.SD

TL;DR: A CNN-based model using Spotify data predicts music track popularity with 97% F1 score.


<details>
  <summary>Details</summary>
Motivation: Predicting music track success is challenging; this study aims to provide a data-driven solution using Spotify features.

Method: Uses Convolutional Neural Networks (CNNs) to analyze Spotify data, including acoustic attributes, metadata, and user engagement metrics.

Result: The model achieves a 97% F1 score in predicting music popularity across genres and time periods.

Conclusion: The study offers insights into digital music trends and provides the industry with a predictive tool for track success.

Abstract: In the digital streaming landscape, it's becoming increasingly challenging
for artists and industry experts to predict the success of music tracks. This
study introduces a pioneering methodology that uses Convolutional Neural
Networks (CNNs) and Spotify data analysis to forecast the popularity of music
tracks. Our approach takes advantage of Spotify's wide range of features,
including acoustic attributes based on the spectrogram of audio waveform,
metadata, and user engagement metrics, to capture the complex patterns and
relationships that influence a track's popularity. Using a large dataset
covering various genres and demographics, our CNN-based model shows impressive
effectiveness in predicting the popularity of music tracks. Additionally, we've
conducted extensive experiments to assess the strength and adaptability of our
model across different musical styles and time periods, with promising results
yielding a 97\% F1 score. Our study not only offers valuable insights into the
dynamic landscape of digital music consumption but also provides the music
industry with advanced predictive tools for assessing and predicting the
success of music tracks.

</details>


### [399] [Audio Transformers](https://arxiv.org/pdf/2105.00335)
*Prateek Verma, Jonathan Berger*

Main category: cs.SD

TL;DR: Transformers outperform CNNs in audio classification on the Free Sound 50K dataset without unsupervised pre-training, using pooling and multi-rate signal processing techniques.


<details>
  <summary>Details</summary>
Motivation: To explore Transformer architectures for raw audio signals, challenging the dominance of CNNs in audio feature classification.

Method: Applied Transformer architectures directly to raw audio, incorporating pooling and multi-rate signal processing inspired by wavelets.

Result: Achieved state-of-the-art results on Free Sound 50K, surpassing convolutional models in mean average precision.

Conclusion: Transformers can effectively replace CNNs in audio understanding, learning adaptable time-frequency representations.

Abstract: Over the past two decades, CNN architectures have produced compelling models
of sound perception and cognition, learning hierarchical organizations of
features. Analogous to successes in computer vision, audio feature
classification can be optimized for a particular task of interest, over a wide
variety of datasets and labels. In fact similar architectures designed for
image understanding have proven effective for acoustic scene analysis. Here we
propose applying Transformer based architectures without convolutional layers
to raw audio signals. On a standard dataset of Free Sound 50K,comprising of 200
categories, our model outperforms convolutional models to produce state of the
art results. This is significant as unlike in natural language processing and
computer vision, we do not perform unsupervised pre-training for outperforming
convolutional architectures. On the same training set, with respect mean
aver-age precision benchmarks, we show a significant improvement. We further
improve the performance of Transformer architectures by using techniques such
as pooling inspired from convolutional net-work designed in the past few years.
In addition, we also show how multi-rate signal processing ideas inspired from
wavelets, can be applied to the Transformer embeddings to improve the results.
We also show how our models learns a non-linear non constant band-width
filter-bank, which shows an adaptable time frequency front end representation
for the task of audio understanding, different from other tasks e.g. pitch
estimation.

</details>


### [400] [Lightweight End-to-end Text-to-speech Synthesis for low resource on-device applications](https://arxiv.org/pdf/2505.07701)
*Biel Tura Vecino, Adam Gabryś, Daniel Mątwicki, Andrzej Pomirski, Tom Iddon, Marius Cotescu, Jaime Lorenzo-Trueba*

Main category: cs.SD

TL;DR: The paper proposes a lightweight end-to-end (LE2E) text-to-speech (TTS) model that reduces computational complexity and memory usage while maintaining high-quality speech generation, making it suitable for low-resource, on-device applications.


<details>
  <summary>Details</summary>
Motivation: Current end-to-end TTS models are computationally heavy and memory-intensive, limiting their use in real-time, low-resource scenarios.

Method: The authors introduce a lightweight E2E-TTS model (LE2E) and evaluate it on the LJSpeech dataset, comparing it to traditional two-stage approaches.

Result: LE2E achieves state-of-the-art performance with 90% fewer parameters and 10x faster real-time factor, outperforming equivalent two-stage models in quality.

Conclusion: LE2E is a promising solution for real-time, high-quality, low-resource TTS applications on devices.

Abstract: Recent works have shown that modelling raw waveform directly from text in an
end-to-end (E2E) fashion produces more natural-sounding speech than traditional
neural text-to-speech (TTS) systems based on a cascade or two-stage approach.
However, current E2E state-of-the-art models are computationally complex and
memory-consuming, making them unsuitable for real-time offline on-device
applications in low-resource scenarios. To address this issue, we propose a
Lightweight E2E-TTS (LE2E) model that generates high-quality speech requiring
minimal computational resources. We evaluate the proposed model on the LJSpeech
dataset and show that it achieves state-of-the-art performance while being up
to $90\%$ smaller in terms of model parameters and $10\times$ faster in
real-time-factor. Furthermore, we demonstrate that the proposed E2E training
paradigm achieves better quality compared to an equivalent architecture trained
in a two-stage approach. Our results suggest that LE2E is a promising approach
for developing real-time, high quality, low-resource TTS applications for
on-device applications.

</details>


### [401] [ISAC: An Invertible and Stable Auditory Filter Bank with Customizable Kernels for ML Integration](https://arxiv.org/pdf/2505.07709)
*Daniel Haider, Felix Perfler, Peter Balazs, Clara Hollomey, Nicki Holighaus*

Main category: cs.SD

TL;DR: ISAC is an invertible, stable filter bank designed for machine learning, mimicking auditory frequency scales and enabling perfect reconstruction.


<details>
  <summary>Details</summary>
Motivation: To create a perceptually-motivated audio front-end adaptable for machine learning and analysis-synthesis tasks.

Method: Uses non-linear auditory frequency scales, user-defined temporal support, and learnable convolutional kernels for perfect reconstruction.

Result: ISAC offers a versatile and powerful audio front-end for various applications.

Conclusion: ISAC is a robust, user-friendly solution for audio processing in machine learning.

Abstract: This paper introduces ISAC, an invertible and stable, perceptually-motivated
filter bank that is specifically designed to be integrated into machine
learning paradigms. More precisely, the center frequencies and bandwidths of
the filters are chosen to follow a non-linear, auditory frequency scale, the
filter kernels have user-defined maximum temporal support and may serve as
learnable convolutional kernels, and there exists a corresponding filter bank
such that both form a perfect reconstruction pair. ISAC provides a powerful and
user-friendly audio front-end suitable for any application, including
analysis-synthesis schemes.

</details>


### [402] [Psychophysiology-aided Perceptually Fluent Speech Analysis of Children Who Stutter](https://arxiv.org/pdf/2211.09089)
*Yi Xiao, Harshit Sharma, Victoria Tumanova, Asif Salekin*

Main category: cs.SD

TL;DR: PASAD is a novel approach for detecting changes in perceptually fluent speech acoustics in young children, leveraging physiological responses to analyze speech-motor-control factors linked to stuttering.


<details>
  <summary>Details</summary>
Motivation: To understand how situational physiological arousal affects speech production in children who stutter (CWS) and identify speech-motor-control factors underlying stuttering disfluencies.

Method: Uses a Hyper-Network structure to extract temporal speech importance information from physiological parameters, with data from 73 preschool-age children (CWS and CWNS).

Result: PASAD outperforms state-of-the-art multi-modal baselines, is expressive, adaptive, generalizable, robust, and real-time executable.

Conclusion: PASAD enhances understanding of speech-motor-control and stuttering development in children, offering a real-time, effective analysis tool.

Abstract: This paper presents a novel approach named PASAD that detects changes in
perceptually fluent speech acoustics of young children. Particularly, analysis
of perceptually fluent speech enables identifying the speech-motor-control
factors that are considered as the underlying cause of stuttering disfluencies.
Recent studies indicate that the speech production of young children,
especially those who stutter, may get adversely affected by situational
physiological arousal. A major contribution of this paper is leveraging the
speaker's situational physiological responses in real-time to analyze the
speech signal effectively. The presented PASAD approach adapts a Hyper-Network
structure to extract temporal speech importance information leveraging
physiological parameters. Moreover, we collected speech and physiological
sensing data from 73 preschool-age children who stutter (CWS) and who do not
stutter (CWNS) in different conditions. PASAD's unique architecture enables
identifying speech attributes distinct to a CWS's fluent speech and mapping
them to the speaker's respective speech-motor-control factors. Extracted
knowledge can enhance understanding of children's speech-motor-control and
stuttering development. Our comprehensive evaluation shows that PASAD
outperforms state-of-the-art multi-modal baseline approaches in different
conditions, is expressive and adaptive to the speaker's speech and physiology,
generalizable, robust, and is real-time executable.

</details>


### [403] [DGSNA: prompt-based Dynamic Generative Scene-based Noise Addition method](https://arxiv.org/pdf/2411.12363)
*Zihao Chen, Zhentao Lin, Bi Zeng, Linyi Huang, Zhi Li, Jia Cai*

Main category: cs.SD

TL;DR: DGSNA is a novel noise addition method combining dynamic scene generation and noise addition, improving speech system robustness by up to 11.21%.


<details>
  <summary>Details</summary>
Motivation: Existing noise addition methods lack coverage of real-world noisy scenes and rely on pre-existing data.

Method: Integrates Dynamic Generation of Scene-based Information (DGSI) with Scene-based Noise Addition for Speech (SNAS) to automate noise addition.

Result: Enhances speech recognition and keyword spotting robustness by up to 11.21%.

Conclusion: DGSNA offers a comprehensive and realistic noise simulation, compatible with other methods for further improvement.

Abstract: To ensure the reliable operation of speech systems across diverse
environments, noise addition methods have emerged as the prevailing solution.
However, existing methods offer limited coverage of real-world noisy scenes and
depend on pre-existing scene-based information and noise. This paper presents
prompt-based Dynamic Generative Scene-based Noise Addition (DGSNA), a novel
noise addition methodology that integrates Dynamic Generation of Scene-based
Information (DGSI) with Scene-based Noise Addition for Speech (SNAS). This
integration facilitates automated scene-based noise addition by transforming
clean speech into various noise environments, thereby providing a more
comprehensive and realistic simulation of diverse noise conditions.
Experimental results demonstrate that DGSNA significantly enhances the
robustness of speech recognition and keyword spotting models across various
noise conditions, achieving a relative improvement of up to 11.21%.
Furthermore, DGSNA can be effectively integrated with other noise addition
methods to enhance performance. Our implementation and demonstrations are
available at https://dgsna.github.io.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [404] [Neural Network Operator-Based Fractal Approximation: Smoothness Preservation and Convergence Analysis](https://arxiv.org/pdf/2505.06229)
*Aaqib Ayoub Bhat, Asif Khan, M. Mursaleen*

Main category: cs.LG

TL;DR: A neural network-based method for constructing α-fractal interpolation functions (FIFs) is introduced, preserving smoothness and using only node values, with convergence analysis.


<details>
  <summary>Details</summary>
Motivation: To develop a novel approach for generating FIFs using neural networks, avoiding reliance on the entire original function and ensuring smoothness preservation.

Method: Constructs α-fractals using neural network operators, focusing on node values and employing a four-layered network to maintain smoothness.

Result: FIFs are generated with interpolation properties, smoothness preservation under constraints, and convergence to the original function under suitable conditions.

Conclusion: The method successfully integrates neural networks with approximation theory for FIF construction, offering efficiency and theoretical guarantees.

Abstract: This paper presents a new approach of constructing $\alpha$-fractal
interpolation functions (FIFs) using neural network operators, integrating
concepts from approximation theory. Initially, we construct $\alpha$-fractals
utilizing neural network-based operators, providing an approach to generating
fractal functions with interpolation properties. Based on the same foundation,
we have developed fractal interpolation functions that utilize only the values
of the original function at the nodes or partition points, unlike traditional
methods that rely on the entire original function.
  Further, we have constructed \(\alpha\)-fractals that preserve the smoothness
of functions under certain constraints by employing a four-layered neural
network operator, ensuring that if \(f \in C^{r}[a,b]\), then the corresponding
fractal \(f^{\alpha} \in C^{r}[a,b]\). Furthermore, we analyze the convergence
of these $\alpha$-fractals to the original function under suitable conditions.
The work uses key approximation theory tools, such as the modulus of continuity
and interpolation operators, to develop convergence results and uniform
approximation error bounds.

</details>


### [405] [Beyond Attention: Toward Machines with Intrinsic Higher Mental States](https://arxiv.org/pdf/2505.06257)
*Ahsan Adeel*

Main category: cs.LG

TL;DR: The paper proposes a method inspired by neurobiology to pre-select relevant information in models like Transformers, enabling faster learning with reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: The challenge of determining relevance in attention mechanisms, traditionally relying on learning algorithms like backpropagation, is addressed by emulating high-level perceptual processing and mental states.

Method: The approach uses triadic neuronal-level modulation loops (Q, K, V) to enable parallel reasoning and rapid refinement of understanding, reducing computational demands.

Result: The method achieves orders-of-magnitude faster learning with fewer resources (heads, layers, tokens) and scales linearly with input tokens (O(N)).

Conclusion: The neurobiologically inspired approach enhances efficiency and performance in tasks like reinforcement learning, computer vision, and NLP.

Abstract: Attending to what is relevant is fundamental to both the mammalian brain and
modern machine learning models such as Transformers. Yet, determining relevance
remains a core challenge, traditionally offloaded to learning algorithms like
backpropagation. Inspired by recent cellular neurobiological evidence linking
neocortical pyramidal cells to distinct mental states, this work shows how
models (e.g., Transformers) can emulate high-level perceptual processing and
awake thought (imagination) states to pre-select relevant information before
applying attention. Triadic neuronal-level modulation loops among questions
($Q$), clues (keys, $K$), and hypotheses (values, $V$) enable diverse, deep,
parallel reasoning chains at the representation level and allow a rapid shift
from initial biases to refined understanding. This leads to orders-of-magnitude
faster learning with significantly reduced computational demand (e.g., fewer
heads, layers, and tokens), at an approximate cost of $\mathcal{O}(N)$, where
$N$ is the number of input tokens. Results span reinforcement learning (e.g.,
CarRacing in a high-dimensional visual setup), computer vision, and natural
language question answering.

</details>


### [406] [ABE: A Unified Framework for Robust and Faithful Attribution-Based Explainability](https://arxiv.org/pdf/2505.06258)
*Zhiyu Zhu, Jiayu Zhang, Zhibo Jin, Fang Chen, Jianlong Zhou*

Main category: cs.LG

TL;DR: ABE is a unified framework for attribution-based explainability, addressing scalability, coupling, and usability issues in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing attribution frameworks like InterpretDL and OmniXAI have scalability, theoretical, and usability limitations, hindering neural network transparency.

Method: ABE formalizes Fundamental Attribution Methods, integrates state-of-the-art algorithms, and offers four customizable modules: Robustness, Interpretability, Validation, and Data & Model.

Result: ABE provides a scalable, extensible foundation for developing novel attribution techniques and enhancing interpretability.

Conclusion: ABE advances attribution-based explainability, fostering transparent AI systems, with code available for public use.

Abstract: Attribution algorithms are essential for enhancing the interpretability and
trustworthiness of deep learning models by identifying key features driving
model decisions. Existing frameworks, such as InterpretDL and OmniXAI,
integrate multiple attribution methods but suffer from scalability limitations,
high coupling, theoretical constraints, and lack of user-friendly
implementations, hindering neural network transparency and interoperability. To
address these challenges, we propose Attribution-Based Explainability (ABE), a
unified framework that formalizes Fundamental Attribution Methods and
integrates state-of-the-art attribution algorithms while ensuring compliance
with attribution axioms. ABE enables researchers to develop novel attribution
techniques and enhances interpretability through four customizable modules:
Robustness, Interpretability, Validation, and Data & Model. This framework
provides a scalable, extensible foundation for advancing attribution-based
explainability and fostering transparent AI systems. Our code is available at:
https://github.com/LMBTough/ABE-XAI.

</details>


### [407] [Fair Clustering with Clusterlets](https://arxiv.org/pdf/2505.06259)
*Mattia Setzu, Riccardo Guidotti*

Main category: cs.LG

TL;DR: The paper proposes simple fuzzy clustering algorithms using "clusterlets" to optimize fairness in clustering, achieving high fairness and cohesion with proper tuning.


<details>
  <summary>Details</summary>
Motivation: Fairness in clustering is crucial due to its real-world applications, but existing methods for fair clustering can be computationally expensive or arbitrary.

Method: The authors introduce clusterlet-based fuzzy clustering algorithms that match single-class clusters, optimizing fairness by leveraging clusterlet distance and regularizing for fairness.

Result: Empirical results show the proposed methods achieve high fairness, cohesion, and low overlap with appropriate parameter tuning.

Conclusion: Simple matching strategies using clusterlets effectively optimize fair clustering, balancing fairness and clustering quality.

Abstract: Given their widespread usage in the real world, the fairness of clustering
methods has become of major interest. Theoretical results on fair clustering
show that fairness enjoys transitivity: given a set of small and fair clusters,
a trivial centroid-based clustering algorithm yields a fair clustering.
Unfortunately, discovering a suitable starting clustering can be
computationally expensive, rather complex or arbitrary.
  In this paper, we propose a set of simple \emph{clusterlet}-based fuzzy
clustering algorithms that match single-class clusters, optimizing fair
clustering. Matching leverages clusterlet distance, optimizing for classic
clustering objectives, while also regularizing for fairness. Empirical results
show that simple matching strategies are able to achieve high fairness, and
that appropriate parameter tuning allows to achieve high cohesion and low
overlap.

</details>


### [408] [Dialz: A Python Toolkit for Steering Vectors](https://arxiv.org/pdf/2505.06262)
*Zara Siddique, Liam D. Turner, Luis Espinosa-Anke*

Main category: cs.LG

TL;DR: Dialz is a Python framework for steering vectors in LLMs, enabling concept modification (e.g., honesty) at inference time. It supports tasks like dataset creation, vector application, and visualization, focusing on modularity and usability. Dialz reduces harmful outputs and aids model interpretability, promoting safer AI.


<details>
  <summary>Details</summary>
Motivation: To provide a powerful, modular tool for steering LLM activations, improving controllability and safety beyond prompting or fine-tuning.

Method: Dialz offers tools for creating contrastive datasets, computing/applying steering vectors, and visualizations, emphasizing usability and rapid prototyping.

Result: Demonstrated effectiveness in reducing harmful outputs (e.g., stereotypes) and enhancing model interpretability across layers.

Conclusion: Dialz accelerates research, supports safer AI development, and fosters transparency in language generation.

Abstract: We introduce Dialz, a framework for advancing research on steering vectors
for open-source LLMs, implemented in Python. Steering vectors allow users to
modify activations at inference time to amplify or weaken a 'concept', e.g.
honesty or positivity, providing a more powerful alternative to prompting or
fine-tuning. Dialz supports a diverse set of tasks, including creating
contrastive pair datasets, computing and applying steering vectors, and
visualizations. Unlike existing libraries, Dialz emphasizes modularity and
usability, enabling both rapid prototyping and in-depth analysis. We
demonstrate how Dialz can be used to reduce harmful outputs such as
stereotypes, while also providing insights into model behaviour across
different layers. We release Dialz with full documentation, tutorials, and
support for popular open-source models to encourage further research in safe
and controllable language generation. Dialz enables faster research cycles and
facilitates insights into model interpretability, paving the way for safer,
more transparent, and more reliable AI systems.

</details>


### [409] [Learning Graph Representation of Agent Diffuser](https://arxiv.org/pdf/2505.06761)
*Youcef Djenouri, Nassim Belmecheri, Tomasz Michalak, Jan Dubiński, Ahmed Nabil Belbachir, Anis Yazidi*

Main category: cs.LG

TL;DR: LGR-AD is a multi-agent system improving adaptability in text-to-image synthesis by modeling generation as interacting agents, outperforming traditional diffusion models.


<details>
  <summary>Details</summary>
Motivation: Static model parameters may not optimally handle distinct phases of image generation, prompting the need for adaptable solutions.

Method: LGR-AD uses a distributed system of agents, a graph neural network for collaboration, and a top-$k$ maximum spanning tree for coordination.

Result: LGR-AD outperforms traditional diffusion models in benchmarks, balancing accuracy and diversity.

Conclusion: LGR-AD offers scalable, flexible solutions for complex image generation tasks.

Abstract: Diffusion-based generative models have significantly advanced text-to-image
synthesis, demonstrating impressive text comprehension and zero-shot
generalization. These models refine images from random noise based on textual
prompts, with initial reliance on text input shifting towards enhanced visual
fidelity over time. This transition suggests that static model parameters might
not optimally address the distinct phases of generation. We introduce LGR-AD
(Learning Graph Representation of Agent Diffusers), a novel multi-agent system
designed to improve adaptability in dynamic computer vision tasks. LGR-AD
models the generation process as a distributed system of interacting agents,
each representing an expert sub-model. These agents dynamically adapt to
varying conditions and collaborate through a graph neural network that encodes
their relationships and performance metrics. Our approach employs a
coordination mechanism based on top-$k$ maximum spanning trees, optimizing the
generation process. Each agent's decision-making is guided by a meta-model that
minimizes a novel loss function, balancing accuracy and diversity. Theoretical
analysis and extensive empirical evaluations show that LGR-AD outperforms
traditional diffusion models across various benchmarks, highlighting its
potential for scalable and flexible solutions in complex image generation
tasks. Code is available at: https://github.com/YousIA/LGR_AD

</details>


### [410] [ONERA's CRM WBPN database for machine learning activities, related regression challenge and first results](https://arxiv.org/pdf/2505.06265)
*Jacques Peter, Quentin Bennehard, Sébastien Heib, Jean-Luc Hantrais-Gervois, Frédéric Moëns*

Main category: cs.LG

TL;DR: A new CFD database for machine learning in aerodynamic prediction is introduced, featuring 468 simulations. It includes a regression challenge to predict wall distributions of pressure and friction coefficients, with results from various ML methods evaluated.


<details>
  <summary>Details</summary>
Motivation: To support machine learning advancements in aerodynamic field prediction by providing a comprehensive CFD database and defining a regression challenge.

Method: 468 Reynolds-Averaged Navier-Stokes simulations using the Spalart-Allmaras turbulence model, split into training and testing sets. Various ML regressors (e.g., MLPs, Decision Trees, k-NN) are evaluated.

Result: Initial performance results using R² scores and worst relative mean absolute error metrics are presented, showcasing the capabilities of tested ML techniques.

Conclusion: The database and challenge offer valuable insights and references for future work in ML-based aerodynamic prediction.

Abstract: This paper presents a new Computational Fluid Dynamics database, developed at
ONERA, to support the advancement of machine learning techniques for
aerodynamic field prediction. It contains 468 Reynolds-Averaged Navier-Stokes
simulations using the Spalart-Allmaras turbulence model, performed on the
NASA/Boeing Common Research Model wing-body-pylon-nacelle configuration. The
database spans a wide range of flow conditions, varying Mach number (including
transonic regimes), angle of attack (capturing flow separation), and Reynolds
number (based on three stagnation pressures, with one setting matching wind
tunnel experiments). The quality of the database is assessed, through checking
the convergence level of each computation.
  Based on these data, a regression challenge is defined. It consists in
predicting the wall distributions of pressure and friction coefficients for
unseen aerodynamic conditions. The 468 simulations are split into training and
testing sets, with the training data made available publicly on the Codabench
platform. The paper further evaluates several classical machine learning
regressors on this task. Tested pointwise methods include Multi-Layer
Perceptrons, $\lambda$-DNNs, and Decision Trees, while global methods include
Multi-Layer Perceptron, k-Nearest Neighbors, Proper Orthogonal Decomposition
and IsoMap. Initial performance results, using $R^2$ scores and worst relative
mean absolute error metrics, are presented, offering insights into the
capabilities of these techniques for the challenge and references for future
work.

</details>


### [411] [Knowledge Guided Encoder-Decoder Framework Integrating Multiple Physical Models for Agricultural Ecosystem Modeling](https://arxiv.org/pdf/2505.06266)
*Qi Cheng, Licheng Liu, Zhang Yao, Hong Mu, Shiyuan Luo, Zhenong Jin, Yiqun Xie, Xiaowei Jia*

Main category: cs.LG

TL;DR: A knowledge-guided encoder-decoder model is proposed for agricultural monitoring, combining physical models and a language model to improve predictions of crop variables under diverse conditions.


<details>
  <summary>Details</summary>
Motivation: Address limitations of traditional physical models (specificity, uncertainty) and data-driven models (lack of generalizability, black-box nature) in agricultural monitoring.

Method: Proposes a knowledge-guided encoder-decoder model integrating multiple physical models and a language model for processing inconsistent inputs and model selection.

Result: Effective and robust predictions of carbon and nitrogen fluxes across multiple sites under various scenarios.

Conclusion: The proposed model offers a universal solution for agricultural monitoring, balancing physical knowledge and data-driven flexibility.

Abstract: Agricultural monitoring is critical for ensuring food security, maintaining
sustainable farming practices, informing policies on mitigating food shortage,
and managing greenhouse gas emissions. Traditional process-based physical
models are often designed and implemented for specific situations, and their
parameters could also be highly uncertain. In contrast, data-driven models
often use black-box structures and does not explicitly model the
inter-dependence between different ecological variables. As a result, they
require extensive training data and lack generalizability to different tasks
with data distribution shifts and inconsistent observed variables. To address
the need for more universal models, we propose a knowledge-guided
encoder-decoder model, which can predict key crop variables by leveraging
knowledge of underlying processes from multiple physical models. The proposed
method also integrates a language model to process complex and inconsistent
inputs and also utilizes it to implement a model selection mechanism for
selectively combining the knowledge from different physical models. Our
evaluations on predicting carbon and nitrogen fluxes for multiple sites
demonstrate the effectiveness and robustness of the proposed model under
various scenarios.

</details>


### [412] [Tri-MTL: A Triple Multitask Learning Approach for Respiratory Disease Diagnosis](https://arxiv.org/pdf/2505.06271)
*June-Woo Kim, Sanghoon Lee, Miika Toikkanen, Daehwan Hwang, Kyunghoon Kim*

Main category: cs.LG

TL;DR: The paper explores using multitask learning (MTL) with deep learning to improve respiratory sound classification and disease diagnosis by integrating sound patterns, disease manifestations, and patient metadata.


<details>
  <summary>Details</summary>
Motivation: Auscultation is vital in clinical practice, but understanding the interplay between respiratory sounds, diseases, and metadata remains a gap. MTL offers a promising solution.

Method: The study extends MTL with deep learning, incorporating stethoscope data and metadata to model relationships between sounds and diseases.

Result: Experiments show significant improvements in lung sound classification and diagnostic performance when metadata is included in the MTL framework.

Conclusion: Integrating MTL with metadata enhances respiratory sound analysis and disease diagnosis, demonstrating its potential in clinical applications.

Abstract: Auscultation remains a cornerstone of clinical practice, essential for both
initial evaluation and continuous monitoring. Clinicians listen to the lung
sounds and make a diagnosis by combining the patient's medical history and test
results. Given this strong association, multitask learning (MTL) can offer a
compelling framework to simultaneously model these relationships, integrating
respiratory sound patterns with disease manifestations. While MTL has shown
considerable promise in medical applications, a significant research gap
remains in understanding the complex interplay between respiratory sounds,
disease manifestations, and patient metadata attributes. This study
investigates how integrating MTL with cutting-edge deep learning architectures
can enhance both respiratory sound classification and disease diagnosis.
Specifically, we extend recent findings regarding the beneficial impact of
metadata on respiratory sound classification by evaluating its effectiveness
within an MTL framework. Our comprehensive experiments reveal significant
improvements in both lung sound classification and diagnostic performance when
the stethoscope information is incorporated into the MTL architecture.

</details>


### [413] [Cluster-Aware Multi-Round Update for Wireless Federated Learning in Heterogeneous Environments](https://arxiv.org/pdf/2505.06268)
*Pengcheng Sun, Erwu Liu, Wei Ni, Kanglei Yu, Rui Wang, Abbas Jamalipour*

Main category: cs.LG

TL;DR: A clustering strategy (CAMU) improves FL efficiency in heterogeneous environments by grouping devices with similar data and communication traits, optimizing updates and resource use.


<details>
  <summary>Details</summary>
Motivation: Resource constraints and device heterogeneity degrade FL performance, necessitating strategies to mitigate these issues.

Method: Proposes CAMU, clustering devices by similarity and adjusting local update frequency based on contribution thresholds, optimizing resource use.

Result: CAMU reduces update bias, enhances accuracy, and balances computation-communication resources, improving FL convergence.

Conclusion: CAMU effectively boosts FL performance in heterogeneous settings, optimizing resource efficiency and model accuracy.

Abstract: The aggregation efficiency and accuracy of wireless Federated Learning (FL)
are significantly affected by resource constraints, especially in heterogeneous
environments where devices exhibit distinct data distributions and
communication capabilities. This paper proposes a clustering strategy that
leverages prior knowledge similarity to group devices with similar data and
communication characteristics, mitigating performance degradation from
heterogeneity. On this basis, a novel Cluster- Aware Multi-round Update (CAMU)
strategy is proposed, which treats clusters as the basic units and adjusts the
local update frequency based on the clustered contribution threshold,
effectively reducing update bias and enhancing aggregation accuracy. The
theoretical convergence of the CAMU strategy is rigorously validated.
Meanwhile, based on the convergence upper bound, the local update frequency and
transmission power of each cluster are jointly optimized to achieve an optimal
balance between computation and communication resources under constrained
conditions, significantly improving the convergence efficiency of FL.
Experimental results demonstrate that the proposed method effectively improves
the model performance of FL in heterogeneous environments and achieves a better
balance between communication cost and computational load under limited
resources.

</details>


### [414] [Interpretable SHAP-bounded Bayesian Optimization for Underwater Acoustic Metamaterial Coating Design](https://arxiv.org/pdf/2505.06519)
*Hansani Weeratunge, Dominic Robe, Elnaz Hajizadeh*

Main category: cs.LG

TL;DR: An interpretability-informed Bayesian optimization framework was developed to optimize underwater acoustic coatings using SHAP for insights, improving efficiency without extra simulations.


<details>
  <summary>Details</summary>
Motivation: To enhance the design of underwater acoustic coatings by leveraging interpretability tools for more efficient optimization under computational constraints.

Method: Combined SHAP analysis with Bayesian optimization to identify key parameters and refine design bounds, applied to polyurethane elastomers with metamaterial features.

Result: Improved optimal solutions for two polyurethane materials without additional simulation iterations, demonstrating SHAP's effectiveness.

Conclusion: The approach efficiently combines interpretability and optimization, applicable to other materials and engineering problems.

Abstract: We developed an interpretability informed Bayesian optimization framework to
optimize underwater acoustic coatings based on polyurethane elastomers with
embedded metamaterial features. A data driven model was employed to analyze the
relationship between acoustic performance, specifically sound absorption and
the corresponding design variables. By leveraging SHapley Additive exPlanations
(SHAP), a machine learning interpretability tool, we identified the key
parameters influencing the objective function and gained insights into how
these parameters affect sound absorption. The insights derived from the SHAP
analysis were subsequently used to automatically refine the bounds of the
optimization problem automatically, enabling a more targeted and efficient
exploration of the design space.
  The proposed approach was applied to two polyurethane materials with distinct
hardness levels, resulting in improved optimal solutions compared to those
obtained without SHAP-informed guidance. Notably, these enhancements were
achieved without increasing the number of simulation iterations. Our findings
demonstrate the potential of SHAP to streamline optimization processes by
uncovering hidden parameter relationships and guiding the search toward
promising regions of the design space. This work underscores the effectiveness
of combining interpretability techniques with Bayesian optimization for the
efficient and cost-effective design of underwater acoustic metamaterials under
strict computational constraints and can be generalized towards other materials
and engineering optimization problems.

</details>


### [415] [A machine learning model for skillful climate system prediction](https://arxiv.org/pdf/2505.06269)
*Chenguang Zhou, Lei Chen, Xiaohui Zhong, Bo Lu, Hao Li, Libo Wu, Jie Wu, Jiahui Hu, Zesheng Dou, Pang-Chi Hsu, Xiaoye Zhang*

Main category: cs.LG

TL;DR: FengShun-CSM, an AI-based climate system model, outperforms ECMWF's S2S model in 60-day global forecasts for 29 variables, excelling in precipitation, land, and ocean predictions due to better intra-seasonal variability representation.


<details>
  <summary>Details</summary>
Motivation: To address the unresolved challenge of developing a fully coupled AI-based climate system model integrating atmosphere, ocean, land, and sea ice interactions.

Method: Introduces FengShun-CSM, leveraging AI and machine learning to provide 60-day global daily forecasts for 29 key variables.

Result: Significantly outperforms ECMWF's S2S model, especially in predicting precipitation, land surface, and oceanic components, with improved intra-seasonal variability representation (e.g., MJO).

Conclusion: Demonstrates the feasibility of AI-powered CSMs, offering transformative potential for Earth system modeling and applications in disaster mitigation, conservation, and agriculture.

Abstract: Climate system models (CSMs), through integrating cross-sphere interactions
among the atmosphere, ocean, land, and cryosphere, have emerged as pivotal
tools for deciphering climate dynamics and improving forecasting capabilities.
Recent breakthroughs in artificial intelligence (AI)-driven meteorological
modeling have demonstrated remarkable success in single-sphere systems and
partially spheres coupled systems. However, the development of a fully coupled
AI-based climate system model encompassing atmosphere-ocean-land-sea ice
interactions has remained an unresolved challenge. This paper introduces
FengShun-CSM, an AI-based CSM model that provides 60-day global daily forecasts
for 29 critical variables across atmospheric, oceanic, terrestrial, and
cryospheric domains. The model significantly outperforms the European Centre
for Medium-Range Weather Forecasts (ECMWF) subseasonal-to-seasonal (S2S) model
in predicting most variables, particularly precipitation, land surface, and
oceanic components. This enhanced capability is primarily attributed to its
improved representation of intra-seasonal variability modes, most notably the
Madden-Julian Oscillation (MJO). Remarkably, FengShun-CSM exhibits substantial
potential in predicting subseasonal extreme events. Such breakthroughs will
advance its applications in meteorological disaster mitigation, marine
ecosystem conservation, and agricultural productivity enhancement. Furthermore,
it validates the feasibility of developing AI-powered CSMs through machine
learning technologies, establishing a transformative paradigm for
next-generation Earth system modeling.

</details>


### [416] [Importance Analysis for Dynamic Control of Balancing Parameter in a Simple Knowledge Distillation Setting](https://arxiv.org/pdf/2505.06270)
*Seongmin Kim, Kwanho Kim, Minseung Kim, Kanghyun Jo*

Main category: cs.LG

TL;DR: The paper explains why dynamically adjusting the balancing parameter in knowledge distillation (KD) improves performance, based on a mathematical rationale.


<details>
  <summary>Details</summary>
Motivation: Deep learning models are complex and slow, so KD is used to compress them. However, the balance between distillation and downstream-task losses is critical for effectiveness.

Method: The study analyzes a simple KD setting, showing mathematically that the balancing parameter should change dynamically as the loss decreases.

Result: Dynamically adjusting the balancing parameter enhances KD performance.

Conclusion: The paper concludes that dynamic adjustment of the balancing parameter is mathematically justified and improves KD outcomes.

Abstract: Although deep learning models owe their remarkable success to deep and
complex architectures, this very complexity typically comes at the expense of
real-time performance. To address this issue, a variety of model compression
techniques have been proposed, among which knowledge distillation (KD) stands
out for its strong empirical performance. The KD contains two concurrent
processes: (i) matching the outputs of a large, pre-trained teacher network and
a lightweight student network, and (ii) training the student to solve its
designated downstream task. The associated loss functions are termed the
distillation loss and the downsteam-task loss, respectively. Numerous prior
studies report that KD is most effective when the influence of the distillation
loss outweighs that of the downstream-task loss. The influence(or importance)
is typically regulated by a balancing parameter. This paper provides a
mathematical rationale showing that in a simple KD setting when the loss is
decreasing, the balancing parameter should be dynamically adjusted

</details>


### [417] [A Sensitivity-Driven Expert Allocation Method in LoRA-MoE for Efficient Fine-Tuning](https://arxiv.org/pdf/2505.06272)
*Junzhou Xu, Boyu Diao*

Main category: cs.LG

TL;DR: The paper introduces LoRA-SMoE, a method for efficient fine-tuning of deep learning models by adaptively allocating experts based on parameter sensitivity, reducing redundancy and improving performance in resource-constrained settings.


<details>
  <summary>Details</summary>
Motivation: The pre-training-fine-tuning paradigm struggles with shared parameters in complex multi-task datasets, and existing MoE methods increase parameter redundancy and training time.

Method: LoRA-SMoE uses gradient information from sampled data to assess parameter sensitivity and adaptively allocates expert numbers within a budget, maintaining memory efficiency.

Result: LoRA-SMoE outperforms SOTA methods by improving model performance while reducing trainable parameters, especially in resource-limited scenarios.

Conclusion: LoRA-SMoE offers an efficient, resource-friendly fine-tuning solution with minimal computational overhead, making it ideal for constrained environments.

Abstract: As deep learning models expand, the pre-training-fine-tuning paradigm has
become the standard approach for handling various downstream tasks. However,
shared parameters can lead to diminished performance when dealing with complex
datasets involving multiple tasks. While introducing Mixture-of-Experts (MoE)
methods has alleviated this issue to some extent, it also significantly
increases the number of parameters required for fine-tuning and training time,
introducing greater parameter redundancy. To address these challenges, we
propose a method for allocating expert numbers based on parameter sensitivity
LoRA-SMoE (A Sensitivity-Driven Expert Allocation Method in LoRA-MoE for
Efficient Fine-Tuning). This method rapidly assesses the sensitivity of
different tasks to parameters by sampling a small amount of data and using
gradient information. It then adaptively allocates expert numbers within a
given budget. The process maintains comparable memory consumption to LoRA
(Low-Rank Adaptation) while ensuring an efficient and resource-friendly
fine-tuning procedure. Experimental results demonstrate that compared to SOTA
fine-tuning methods, our LoRA-SMoE approach can enhance model performance while
reducing the number of trainable parameters. This significantly improves model
performance in resource-constrained environments. Additionally, due to its
efficient parameter sensitivity evaluation mechanism, LoRA-SMoE requires
minimal computational overhead to optimize expert allocation, making it
particularly suitable for scenarios with limited computational resources. All
the code in this study will be made publicly available following the acceptance
of the paper for publication. Source code is at
https://github.com/EMLS-ICTCAS/LoRA-SMoE

</details>


### [418] [Policy-labeled Preference Learning: Is Preference Enough for RLHF?](https://arxiv.org/pdf/2505.06273)
*Taehyun Cho, Seokhun Ju, Seungyub Han, Dohyeong Kim, Kyungjae Lee, Jungwoo Lee*

Main category: cs.LG

TL;DR: PPL improves RLHF by addressing likelihood mismatch using regret-based modeling and KL regularization, showing better performance in offline and online RL tasks.


<details>
  <summary>Details</summary>
Motivation: Existing RLHF methods misinterpret trajectories as optimal, leading to inaccurate likelihood estimation and suboptimal learning.

Method: Proposes Policy-Labeled Preference Learning (PPL) to model human preferences with regret and introduces contrastive KL regularization.

Result: PPL significantly improves offline RLHF performance and works effectively in online settings.

Conclusion: PPL resolves likelihood mismatch in RLHF, enhancing policy optimization through regret-based modeling and KL regularization.

Abstract: To design rewards that align with human goals, Reinforcement Learning from
Human Feedback (RLHF) has emerged as a prominent technique for learning reward
functions from human preferences and optimizing policies via reinforcement
learning algorithms. However, existing RLHF methods often misinterpret
trajectories as being generated by an optimal policy, causing inaccurate
likelihood estimation and suboptimal learning. Inspired by Direct Preference
Optimization framework which directly learns optimal policy without explicit
reward, we propose policy-labeled preference learning (PPL), to resolve
likelihood mismatch issues by modeling human preferences with regret, which
reflects behavior policy information. We also provide a contrastive KL
regularization, derived from regret-based principles, to enhance RLHF in
sequential decision making. Experiments in high-dimensional continuous control
tasks demonstrate PPL's significant improvements in offline RLHF performance
and its effectiveness in online settings.

</details>


### [419] [PARM: Multi-Objective Test-Time Alignment via Preference-Aware Autoregressive Reward Model](https://arxiv.org/pdf/2505.06274)
*Baijiong Lin, Weisen Jiang, Yuancheng Xu, Hao Chen, Ying-Cong Chen*

Main category: cs.LG

TL;DR: PARM introduces a unified Autoregressive Reward Model (ARM) trained across all preference dimensions, reducing inference costs and improving alignment with user preferences compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of GenARM, which requires multiple ARMs and suffers from misalignment due to separate training.

Method: Proposes Preference-aware ARM (PARM) with Preference-Aware Bilinear Low-Rank Adaptation (PBLoRA) for precise control over preference trade-offs.

Result: PARM reduces inference costs, achieves better alignment, and enables weak-to-strong guidance with limited resources.

Conclusion: PARM offers a cost-effective and efficient solution for multi-objective test-time alignment in LLMs.

Abstract: Multi-objective test-time alignment aims to adapt large language models
(LLMs) to diverse multi-dimensional user preferences during inference while
keeping LLMs frozen. Recently, GenARM (Xu et al., 2025) first independently
trains Autoregressive Reward Models (ARMs) for each preference dimension
without awareness of each other, then combines their outputs based on
user-specific preference vectors during inference to achieve multi-objective
test-time alignment, leading to two key limitations: the need for
\textit{multiple} ARMs increases the inference cost, and the separate training
of ARMs causes the misalignment between the guided generation and the user
preferences. To address these issues, we propose Preference-aware ARM (PARM), a
single unified ARM trained across all preference dimensions. PARM uses our
proposed Preference-Aware Bilinear Low-Rank Adaptation (PBLoRA), which employs
a bilinear form to condition the ARM on preference vectors, enabling it to
achieve precise control over preference trade-offs during inference.
Experiments demonstrate that PARM reduces inference costs and achieves better
alignment with preference vectors compared with existing methods. Additionally,
PARM enables weak-to-strong guidance, allowing a smaller PARM to guide a larger
frozen LLM without expensive training, making multi-objective alignment
accessible with limited computing resources. The code is available at
https://github.com/Baijiong-Lin/PARM.

</details>


### [420] [Attonsecond Streaking Phase Retrieval Via Deep Learning Methods](https://arxiv.org/pdf/2505.06275)
*Yuzhou Zhu, Zheng Zhang, Ruyi Zhang, Liang Zhou*

Main category: cs.LG

TL;DR: Neural networks outperform traditional methods in attosecond streaking phase retrieval, with capsule networks achieving the highest accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for attosecond streaking phase retrieval degrade accuracy for broadband pulses, prompting the need for more robust solutions.

Method: Four neural architectures (CNN, ViT, Hybrid CNN-ViT, and Capsule Network) are compared for phase retrieval, analyzed theoretically, and tested on synthetic data.

Result: Capsule networks outperform others, with a strict accuracy hierarchy: CNN < ViT < Hybrid < Capsule.

Conclusion: Neural networks, especially capsule networks, offer superior phase retrieval, with potential for real-time applications via physics-informed networks and photonic hardware.

Abstract: Attosecond streaking phase retrieval is essential for resolving electron
dynamics on sub-femtosecond time scales yet traditional algorithms rely on
iterative minimization and central momentum approximations that degrade
accuracy for broadband pulses. In this work phase retrieval is reformulated as
a supervised computer-vision problem and four neural architectures are
systematically compared. A convolutional network demonstrates strong
sensitivity to local streak edges but lacks global context; a vision
transformer captures long-range delay-energy correlations at the expense of
local inductive bias; a hybrid CNN-ViT model unites local feature extraction
and full-graph attention; and a capsule network further enforces spatial pose
agreement through dynamic routing. A theoretical analysis introduces local,
global and positional sensitivity measures and derives surrogate error bounds
that predict the strict ordering $CNN<ViT<Hybrid<Capsule$. Controlled
experiments on synthetic streaking spectrograms confirm this hierarchy, with
the capsule network achieving the highest retrieval fidelity. Looking forward,
embedding the strong-field integral into physics-informed neural networks and
exploring photonic hardware implementations promise pathways toward real-time
attosecond pulse characterization under demanding experimental conditions.

</details>


### [421] [Interpretable Learning Dynamics in Unsupervised Reinforcement Learning](https://arxiv.org/pdf/2505.06279)
*Shashwat Pandey*

Main category: cs.LG

TL;DR: The paper introduces an interpretability framework for unsupervised RL agents, analyzing how intrinsic motivation affects attention, behavior, and representation learning. It evaluates five agents using metrics like attention diversity and change rate, finding curiosity-driven agents exhibit broader, dynamic attention and exploration. Transformer-RND stands out for its structured latent representations.


<details>
  <summary>Details</summary>
Motivation: To understand how intrinsic motivation influences RL agents' attention, behavior, and representation learning, and to provide diagnostic tools for evaluating perception and abstraction beyond reward-centric metrics.

Method: Analyzed five RL agents (DQN, RND, ICM, PPO, Transformer-RND) using Grad-CAM, LRP, exploration metrics, and latent space clustering. Introduced attention diversity and change rate metrics to measure spatial and temporal attention shifts.

Result: Curiosity-driven agents showed broader, dynamic attention and exploratory behavior. Transformer-RND excelled with wide attention, high exploration, and structured latent representations. Architectural biases and training signals significantly impacted agent dynamics.

Conclusion: The framework provides interpretability tools for RL agents, revealing how intrinsic motivation shapes behavior and representation. It enables more generalizable and interpretable agent designs beyond traditional reward-based evaluation.

Abstract: We present an interpretability framework for unsupervised reinforcement
learning (URL) agents, aimed at understanding how intrinsic motivation shapes
attention, behavior, and representation learning. We analyze five agents DQN,
RND, ICM, PPO, and a Transformer-RND variant trained on procedurally generated
environments, using Grad-CAM, Layer-wise Relevance Propagation (LRP),
exploration metrics, and latent space clustering. To capture how agents
perceive and adapt over time, we introduce two metrics: attention diversity,
which measures the spatial breadth of focus, and attention change rate, which
quantifies temporal shifts in attention. Our findings show that
curiosity-driven agents display broader, more dynamic attention and exploratory
behavior than their extrinsically motivated counterparts. Among them,
TransformerRND combines wide attention, high exploration coverage, and compact,
structured latent representations. Our results highlight the influence of
architectural inductive biases and training signals on internal agent dynamics.
Beyond reward-centric evaluation, the proposed framework offers diagnostic
tools to probe perception and abstraction in RL agents, enabling more
interpretable and generalizable behavior.

</details>


### [422] [Show or Tell? A Benchmark To Evaluate Visual and Textual Prompts in Semantic Segmentation](https://arxiv.org/pdf/2505.06280)
*Gabriele Rosi, Fabio Cermelli*

Main category: cs.LG

TL;DR: The paper introduces 'Show or Tell' (SoT), a benchmark for evaluating visual and textual prompts in semantic segmentation, comparing their performance across diverse domains.


<details>
  <summary>Details</summary>
Motivation: Systematic exploration of prompt engineering in computer vision is limited, especially for semantic segmentation, where textual and visual prompts offer distinct but untested advantages.

Method: SoT evaluates 5 open-vocabulary (textual) and 4 visual reference prompt methods across 14 datasets in 7 domains, using a confidence-based mask merging strategy for multi-class segmentation.

Result: Open-vocabulary methods perform well with common concepts but struggle in complex domains, while visual reference methods show high variability based on input prompts.

Conclusion: The study highlights strengths and weaknesses of both prompting modalities, offering insights for future research in vision foundation models for segmentation.

Abstract: Prompt engineering has shown remarkable success with large language models,
yet its systematic exploration in computer vision remains limited. In semantic
segmentation, both textual and visual prompts offer distinct advantages:
textual prompts through open-vocabulary methods allow segmentation of arbitrary
categories, while visual reference prompts provide intuitive reference
examples. However, existing benchmarks evaluate these modalities in isolation,
without direct comparison under identical conditions. We present Show or Tell
(SoT), a novel benchmark specifically designed to evaluate both visual and
textual prompts for semantic segmentation across 14 datasets spanning 7 diverse
domains (common scenes, urban, food, waste, parts, tools, and land-cover). We
evaluate 5 open-vocabulary methods and 4 visual reference prompt approaches,
adapting the latter to handle multi-class segmentation through a
confidence-based mask merging strategy. Our extensive experiments reveal that
open-vocabulary methods excel with common concepts easily described by text but
struggle with complex domains like tools, while visual reference prompt methods
achieve good average results but exhibit high variability depending on the
input prompt. Through comprehensive quantitative and qualitative analysis, we
identify the strengths and weaknesses of both prompting modalities, providing
valuable insights to guide future research in vision foundation models for
segmentation tasks.

</details>


### [423] [A Data-Driven Probabilistic Framework for Cascading Urban Risk Analysis Using Bayesian Networks](https://arxiv.org/pdf/2505.06281)
*Chunduru Rohith Kumar, PHD Surya Shanmuk, Prabhala Naga Srinivas, Sri Venkatesh Lankalapalli, Debasis Dwibedy*

Main category: cs.LG

TL;DR: A Bayesian network-based framework models cascading risks in urban systems, using hybrid data and interpretable probabilistic reasoning to identify key risk factors for resilience planning.


<details>
  <summary>Details</summary>
Motivation: Addressing the complexity of cascading risks in urban systems requires robust, data-driven methods to understand interdependencies across domains like air, water, and infrastructure.

Method: Bayesian Belief Networks (BBNs) with Hill-Climbing search and BIC/K2 scoring are used to construct DAGs. Hybrid data (real-world and GAN-generated) is balanced with SMOTE, and CPTs enable probabilistic reasoning.

Result: Key intra- and inter-domain risk factors are identified, showcasing the framework's utility for proactive urban resilience planning.

Conclusion: The study provides a scalable, interpretable foundation for cascading risk assessment, paving the way for future empirical research.

Abstract: The increasing complexity of cascading risks in urban systems necessitates
robust, data-driven frameworks to model interdependencies across multiple
domains. This study presents a foundational Bayesian network-based approach for
analyzing cross-domain risk propagation across key urban domains, including
air, water, electricity, agriculture, health, infrastructure, weather, and
climate. Directed Acyclic Graphs (DAGs) are constructed using Bayesian Belief
Networks (BBNs), with structure learning guided by Hill-Climbing search
optimized through Bayesian Information Criterion (BIC) and K2 scoring. The
framework is trained on a hybrid dataset that combines real-world urban
indicators with synthetically generated data from Generative Adversarial
Networks (GANs), and is further balanced using the Synthetic Minority
Over-sampling Technique (SMOTE). Conditional Probability Tables (CPTs) derived
from the learned structures enable interpretable probabilistic reasoning and
quantify the likelihood of cascading failures. The results identify key intra-
and inter-domain risk factors and demonstrate the framework's utility for
proactive urban resilience planning. This work establishes a scalable,
interpretable foundation for cascading risk assessment and serves as a basis
for future empirical research in this emerging interdisciplinary field.

</details>


### [424] [Image Classification Using a Diffusion Model as a Pre-Training Model](https://arxiv.org/pdf/2505.06890)
*Kosuke Ukita, Ye Xiaolong, Tsuyoshi Okita*

Main category: cs.LG

TL;DR: A diffusion model with ViT-derived representation conditioning improves zero-shot classification for hematoma detection, outperforming DINOv2.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of large-scale labeled datasets by leveraging self-supervised learning on unlabeled data.

Method: Integrates a representation-conditioning mechanism using ViT-derived representations in a Transformer-based diffusion model.

Result: Achieves +6.15% accuracy and +13.60% F1-score improvement over DINOv2 in hematoma detection.

Conclusion: The method is effective for image classification, particularly in zero-shot tasks.

Abstract: In this paper, we propose a diffusion model that integrates a
representation-conditioning mechanism, where the representations derived from a
Vision Transformer (ViT) are used to condition the internal process of a
Transformer-based diffusion model. This approach enables
representation-conditioned data generation, addressing the challenge of
requiring large-scale labeled datasets by leveraging self-supervised learning
on unlabeled data. We evaluate our method through a zero-shot classification
task for hematoma detection in brain imaging. Compared to the strong
contrastive learning baseline, DINOv2, our method achieves a notable
improvement of +6.15% in accuracy and +13.60% in F1-score, demonstrating its
effectiveness in image classification.

</details>


### [425] [InfoNCE is a Free Lunch for Semantically guided Graph Contrastive Learning](https://arxiv.org/pdf/2505.06282)
*Zixu Wang, Bingbing Xu, Yige Yuan, Huawei Shen, Xueqi Cheng*

Main category: cs.LG

TL;DR: The paper introduces IFL-GCL, a method addressing sampling bias in Graph Contrastive Learning (GCL) by treating it as a Positive-Unlabeled (PU) problem and using InfoNCE to extract semantic information, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Traditional GCL methods classify semantically similar pairs as negative due to augmentation-based sampling, leading to bias and performance limitations. The paper redefines GCL as a PU problem to address this.

Method: Proposes IFL-GCL, leveraging InfoNCE to align representation similarity with positive sample probability and redefining the maximum likelihood objective for corrected samples.

Result: IFL-GCL shows up to 9.05% improvement in both IID and OOD scenarios, validating its effectiveness.

Conclusion: Semantically guided GCL via IFL-GCL significantly enhances performance, with publicly available code for implementation.

Abstract: As an important graph pre-training method, Graph Contrastive Learning (GCL)
continues to play a crucial role in the ongoing surge of research on graph
foundation models or LLM as enhancer for graphs. Traditional GCL optimizes
InfoNCE by using augmentations to define self-supervised tasks, treating
augmented pairs as positive samples and others as negative. However, this leads
to semantically similar pairs being classified as negative, causing significant
sampling bias and limiting performance. In this paper, we argue that GCL is
essentially a Positive-Unlabeled (PU) learning problem, where the definition of
self-supervised tasks should be semantically guided, i.e., augmented samples
with similar semantics are considered positive, while others, with unknown
semantics, are treated as unlabeled. From this perspective, the key lies in how
to extract semantic information. To achieve this, we propose IFL-GCL, using
InfoNCE as a "free lunch" to extract semantic information. Specifically, We
first prove that under InfoNCE, the representation similarity of node pairs
aligns with the probability that the corresponding contrastive sample is
positive. Then we redefine the maximum likelihood objective based on the
corrected samples, leading to a new InfoNCE loss function. Extensive
experiments on both the graph pretraining framework and LLM as an enhancer show
significantly improvements of IFL-GCL in both IID and OOD scenarios, achieving
up to a 9.05% improvement, validating the effectiveness of semantically guided.
Code for IFL-GCL is publicly available at:
https://github.com/Camel-Prince/IFL-GCL.

</details>


### [426] [Soft causal learning for generalized molecule property prediction: An environment perspective](https://arxiv.org/pdf/2505.06283)
*Limin Li, Kuo Yang, Wenjie Du, Pengkun Wang, Zhengyang Zhou, Yang Wang*

Main category: cs.LG

TL;DR: A soft causal learning framework is proposed to address OOD challenges in molecular science by modeling molecule environments and bypassing invariant subgraphs.


<details>
  <summary>Details</summary>
Motivation: Existing GNN-based models fail to adapt to OOD samples and ignore key issues like expanding atom patterns, complex associations, and environment-invariance interactions.

Method: Incorporates chemistry theories into a graph growth generator, uses a GIB-based objective to disentangle environments, and introduces cross-attention for dynamic interactions.

Result: Experiments on seven datasets show superior generalization in OOD scenarios, supported by comparisons and case studies.

Conclusion: The proposed framework effectively addresses OOD challenges in molecular science by modeling environments and enabling dynamic interactions.

Abstract: Learning on molecule graphs has become an increasingly important topic in AI
for science, which takes full advantage of AI to facilitate scientific
discovery. Existing solutions on modeling molecules utilize Graph Neural
Networks (GNNs) to achieve representations but they mostly fail to adapt models
to out-of-distribution (OOD) samples. Although recent advances on OOD-oriented
graph learning have discovered the invariant rationale on graphs, they still
ignore three important issues, i.e., 1) the expanding atom patterns regarding
environments on graphs lead to failures of invariant rationale based models, 2)
the associations between discovered molecular subgraphs and corresponding
properties are complex where causal substructures cannot fully interpret the
labels. 3) the interactions between environments and invariances can influence
with each other thus are challenging to be modeled. To this end, we propose a
soft causal learning framework, to tackle the unresolved OOD challenge in
molecular science, from the perspective of fully modeling the molecule
environments and bypassing the invariant subgraphs. Specifically, we first
incorporate chemistry theories into our graph growth generator to imitate
expaned environments, and then devise an GIB-based objective to disentangle
environment from whole graphs and finally introduce a cross-attention based
soft causal interaction, which allows dynamic interactions between environments
and invariances. We perform experiments on seven datasets by imitating
different kinds of OOD generalization scenarios. Extensive comparison, ablation
experiments as well as visualized case studies demonstrate well generalization
ability of our proposal.

</details>


### [427] [DMRL: Data- and Model-aware Reward Learning for Data Extraction](https://arxiv.org/pdf/2505.06284)
*Zhiqiang Wang, Ruoxi Cheng*

Main category: cs.LG

TL;DR: DMRL, a Data- and Model-aware Reward Learning approach, improves data extraction from LLMs by addressing limitations of current methods.


<details>
  <summary>Details</summary>
Motivation: Current data extraction methods for LLMs are limited by reliance on duplicates, prompt engineering, and random-search adversarial generation.

Method: DMRL uses inverse reinforcement learning, introspective reasoning datasets, and Group Relative Policy Optimization (GRPO) for dynamic tuning.

Result: DMRL outperforms baseline methods in data extraction across various LLMs.

Conclusion: DMRL provides a robust solution for systematic red-teaming and privacy breach prevention in LLMs.

Abstract: Large language models (LLMs) are inherently vulnerable to unintended privacy
breaches. Consequently, systematic red-teaming research is essential for
developing robust defense mechanisms. However, current data extraction methods
suffer from several limitations: (1) rely on dataset duplicates (addressable
via deduplication), (2) depend on prompt engineering (now countered by
detection and defense), and (3) rely on random-search adversarial generation.
To address these challenges, we propose DMRL, a Data- and Model-aware Reward
Learning approach for data extraction. This technique leverages inverse
reinforcement learning to extract sensitive data from LLMs. Our method consists
of two main components: (1) constructing an introspective reasoning dataset
that captures leakage mindsets to guide model behavior, and (2) training reward
models with Group Relative Policy Optimization (GRPO), dynamically tuning
optimization based on task difficulty at both the data and model levels.
Comprehensive experiments across various LLMs demonstrate that DMRL outperforms
all baseline methods in data extraction performance.

</details>


### [428] [IIKL: Isometric Immersion Kernel Learning with Riemannian Manifold for Geometric Preservation](https://arxiv.org/pdf/2505.06288)
*Zihao Chen, Wenyong Wang, Jiachen Yang, Yu Xiang*

Main category: cs.LG

TL;DR: The paper introduces Isometric Immersion Kernel Learning (IIKL) to preserve geometric properties of non-Euclidean data by mapping it into a Riemannian manifold, improving downstream task accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods lose critical geometric information by mapping non-Euclidean data into Euclidean space. The goal is to preserve intrinsic geometric and topological properties.

Method: Proposes IIKL to build a Riemannian manifold and induce its metric from data, ensuring inner product invariance. Uses MLE for efficient training.

Result: IIKL reduces inner product invariant loss by 90% and improves reconstruction accuracy by 40% compared to SOTA methods.

Conclusion: IIKL successfully preserves geometric structures and enhances performance in downstream tasks like reconstruction and classification.

Abstract: Geometric representation learning in preserving the intrinsic geometric and
topological properties for discrete non-Euclidean data is crucial in scientific
applications. Previous research generally mapped non-Euclidean discrete data
into Euclidean space during representation learning, which may lead to the loss
of some critical geometric information. In this paper, we propose a novel
Isometric Immersion Kernel Learning (IIKL) method to build Riemannian manifold
and isometrically induce Riemannian metric from discrete non-Euclidean data. We
prove that Isometric immersion is equivalent to the kernel function in the
tangent bundle on the manifold, which explicitly guarantees the invariance of
the inner product between vectors in the arbitrary tangent space throughout the
learning process, thus maintaining the geometric structure of the original
data. Moreover, a novel parameterized learning model based on IIKL is
introduced, and an alternating training method for this model is derived using
Maximum Likelihood Estimation (MLE), ensuring efficient convergence.
Experimental results proved that using the learned Riemannian manifold and its
metric, our model preserved the intrinsic geometric representation of data in
both 3D and high-dimensional datasets successfully, and significantly improved
the accuracy of downstream tasks, such as data reconstruction and
classification. It is showed that our method could reduce the inner product
invariant loss by more than 90% compared to state-of-the-art (SOTA) methods,
also achieved an average 40% improvement in downstream reconstruction accuracy
and a 90% reduction in error for geometric metrics involving isometric and
conformal.

</details>


### [429] [Edge-Optimized Deep Learning & Pattern Recognition Techniques for Non-Intrusive Load Monitoring of Energy Time Series](https://arxiv.org/pdf/2505.06289)
*Sotirios Athanasoulias*

Main category: cs.LG

TL;DR: The paper addresses challenges in Non-Intrusive Load Monitoring (NILM) for energy efficiency, focusing on underrepresented regions and edge deployment.


<details>
  <summary>Details</summary>
Motivation: The need for sustainable energy practices and user engagement drives the research, highlighting gaps in regional data and computational inefficiencies in NILM.

Method: The study introduces an interoperable data collection framework (Plegma Dataset) and explores deep neural networks with model compression for edge deployment.

Result: The work provides a region-specific dataset and efficient NILM solutions, enhancing scalability and privacy.

Conclusion: This research advances NILM by addressing real-world challenges, making it more adaptable for global energy sustainability.

Abstract: The growing global energy demand and the urgent need for sustainability call
for innovative ways to boost energy efficiency. While advanced energy-saving
systems exist, they often fall short without user engagement. Providing
feedback on energy consumption behavior is key to promoting sustainable
practices. Non-Intrusive Load Monitoring (NILM) offers a promising solution by
disaggregating total household energy usage, recorded by a central smart meter,
into appliance-level data. This empowers users to optimize consumption.
Advances in AI, IoT, and smart meter adoption have further enhanced NILM's
potential.
  Despite this promise, real-world NILM deployment faces major challenges.
First, existing datasets mainly represent regions like the USA and UK, leaving
places like the Mediterranean underrepresented. This limits understanding of
regional consumption patterns, such as heavy use of air conditioners and
electric water heaters. Second, deep learning models used in NILM require high
computational power, often relying on cloud services. This increases costs,
raises privacy concerns, and limits scalability, especially for households with
poor connectivity. This thesis tackles these issues with key contributions. It
presents an interoperable data collection framework and introduces the Plegma
Dataset, focused on underrepresented Mediterranean energy patterns. It also
explores advanced deep neural networks and model compression techniques for
efficient edge deployment. By bridging theoretical advances with practical
needs, this work aims to make NILM scalable, efficient, and adaptable for
global energy sustainability.

</details>


### [430] [UniCO: Towards a Unified Model for Combinatorial Optimization Problems](https://arxiv.org/pdf/2505.06290)
*Zefang Zong, Xiaochen Wei, Guozhen Zhang, Chen Gao, Huandong Wang, Yong Li*

Main category: cs.LG

TL;DR: UniCO is a unified model for solving diverse Combinatorial Optimization (CO) problems using a transformer backbone and a two-stage self-supervised learning approach, achieving generalization with minimal fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a unified model for diverse CO problems, aiming for efficiency and convenience.

Method: Frames CO problems as Markov Decision Processes (MDPs), tokenizes trajectory data, and uses a transformer backbone with a CO-prefix design and two-stage self-supervised learning.

Result: UniCO generalizes to new CO problems with minimal fine-tuning, achieving few-shot or zero-shot performance across 10 problems.

Conclusion: UniCO complements existing neural CO methods by offering a versatile, unified solution for diverse problems.

Abstract: Combinatorial Optimization (CO) encompasses a wide range of problems that
arise in many real-world scenarios. While significant progress has been made in
developing learning-based methods for specialized CO problems, a unified model
with a single architecture and parameter set for diverse CO problems remains
elusive. Such a model would offer substantial advantages in terms of efficiency
and convenience. In this paper, we introduce UniCO, a unified model for solving
various CO problems. Inspired by the success of next-token prediction, we frame
each problem-solving process as a Markov Decision Process (MDP), tokenize the
corresponding sequential trajectory data, and train the model using a
transformer backbone. To reduce token length in the trajectory data, we propose
a CO-prefix design that aggregates static problem features. To address the
heterogeneity of state and action tokens within the MDP, we employ a two-stage
self-supervised learning approach. In this approach, a dynamic prediction model
is first trained and then serves as a pre-trained model for subsequent policy
generation. Experiments across 10 CO problems showcase the versatility of
UniCO, emphasizing its ability to generalize to new, unseen problems with
minimal fine-tuning, achieving even few-shot or zero-shot performance. Our
framework offers a valuable complement to existing neural CO methods that focus
on optimizing performance for individual problems.

</details>


### [431] [Spatio-Temporal Graph Neural Network for Urban Spaces: Interpolating Citywide Traffic Volume](https://arxiv.org/pdf/2505.06292)
*Silke K. Kaiser, Filipe Rodrigues, Carlos Lima Azevedo, Lynn H. Kaack*

Main category: cs.LG

TL;DR: GNNUI, a Graph Neural Network for urban traffic volume interpolation, outperforms existing methods under sparse sensor coverage, validated on cycling and taxi datasets.


<details>
  <summary>Details</summary>
Motivation: Urban traffic volume data is sparse due to high sensor costs, necessitating accurate interpolation methods for urban planning.

Method: GNNUI uses masking, node features, and a zero-inflated loss function to estimate traffic volumes in diverse urban networks.

Result: GNNUI shows robust performance (low MAE, RMSE) even at 1% sensor coverage, outperforming baselines on cycling and taxi datasets.

Conclusion: GNNUI is effective for urban traffic interpolation, handling sparse data and diverse network structures.

Abstract: Reliable street-level traffic volume data, covering multiple modes of
transportation, helps urban planning by informing decisions on infrastructure
improvements, traffic management, and public transportation. Yet, traffic
sensors measuring traffic volume are typically scarcely located, due to their
high deployment and maintenance costs. To address this, interpolation methods
can estimate traffic volumes at unobserved locations using available data.
Graph Neural Networks have shown strong performance in traffic volume
forecasting, particularly on highways and major arterial networks. Applying
them to urban settings, however, presents unique challenges: urban networks
exhibit greater structural diversity, traffic volumes are highly overdispersed
with many zeros, the best way to account for spatial dependencies remains
unclear, and sensor coverage is often very sparse. We introduce the Graph
Neural Network for Urban Interpolation (GNNUI), a novel urban traffic volume
estimation approach. GNNUI employs a masking algorithm to learn interpolation,
integrates node features to capture functional roles, and uses a loss function
tailored to zero-inflated traffic distributions. In addition to the model, we
introduce two new open, large-scale urban traffic volume benchmarks, covering
different transportation modes: Strava cycling data from Berlin and New York
City taxi data. GNNUI outperforms recent, some graph-based, interpolation
methods across metrics (MAE, RMSE, true-zero rate, Kullback-Leibler divergence)
and remains robust from 90% to 1% sensor coverage. On Strava, for instance, MAE
rises only from 7.1 to 10.5, on Taxi from 23.0 to 40.4, demonstrating strong
performance under extreme data scarcity, common in real-world urban settings.
We also examine how graph connectivity choices influence model accuracy.

</details>


### [432] [Benchmarking Traditional Machine Learning and Deep Learning Models for Fault Detection in Power Transformers](https://arxiv.org/pdf/2505.06295)
*Bhuvan Saravanan, Pasanth Kumar M D, Aarnesh Vengateson*

Main category: cs.LG

TL;DR: Comparative analysis of ML and DL for power transformer fault diagnosis shows comparable performance, with RF and 1D-CNN leading in accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate fault diagnosis is crucial for power system stability and safety.

Method: Used normalized gas concentration data to train five ML classifiers (SVM, KNN, RF, XGBoost, ANN) and four DL models (LSTM, GRU, 1D-CNN, TabNet).

Result: RF achieved the highest ML accuracy (86.82%), while 1D-CNN closely followed (86.30%).

Conclusion: Both ML and DL methods are effective for transformer fault classification, with RF and 1D-CNN performing best.

Abstract: Accurate diagnosis of power transformer faults is essential for ensuring the
stability and safety of electrical power systems. This study presents a
comparative analysis of conventional machine learning (ML) algorithms and deep
learning (DL) algorithms for fault classification of power transformers. Using
a condition-monitored dataset spanning 10 months, various gas concentration
features were normalized and used to train five ML classifiers: Support Vector
Machine (SVM), k-Nearest Neighbors (KNN), Random Forest (RF), XGBoost, and
Artificial Neural Network (ANN). In addition, four DL models were evaluated:
Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), One-Dimensional
Convolutional Neural Network (1D-CNN), and TabNet. Experimental results show
that both ML and DL approaches performed comparably. The RF model achieved the
highest ML accuracy at 86.82%, while the 1D-CNN model attained a close 86.30%.

</details>


### [433] [Lossless Compression of Large Language Model-Generated Text via Next-Token Prediction](https://arxiv.org/pdf/2505.06297)
*Yu Mao, Holger Pirk, Chun Jason Xue*

Main category: cs.LG

TL;DR: The paper investigates lossless compression for LLM-generated data, showing LLMs can compress their own outputs more effectively than traditional methods like Gzip.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of LLM-generated data necessitates efficient compression methods, as traditional techniques are inadequate for its complexity.

Method: The study systematically evaluates LLM-based prediction methods for compression, testing 14 LLMs and 8 datasets.

Result: LLM-based methods achieve over 20x compression rates, outperforming Gzip's 3x, with consistent performance across LLM sizes and datasets.

Conclusion: LLMs are robust and practical for lossless compression of their own outputs, offering significant advantages over conventional methods.

Abstract: As large language models (LLMs) continue to be deployed and utilized across
domains, the volume of LLM-generated data is growing rapidly. This trend
highlights the increasing importance of effective and lossless compression for
such data in modern text management systems. However, compressing LLM-generated
data presents unique challenges compared to traditional human- or
machine-generated content. Traditional machine-generated data is typically
derived from computational processes or device outputs, often highly structured
and limited to low-level elements like labels or numerical values. This
structure enables conventional lossless compressors to perform efficiently. In
contrast, LLM-generated data is more complex and diverse, requiring new
approaches for effective compression. In this work, we conduct the first
systematic investigation of lossless compression techniques tailored
specifically to LLM-generated data. Notably, because LLMs are trained via
next-token prediction, we find that LLM-generated data is highly predictable
for the models themselves. This predictability enables LLMs to serve as
efficient compressors of their own outputs. Through extensive experiments with
14 representative LLMs and 8 LLM-generated datasets from diverse domains, we
show that LLM-based prediction methods achieve remarkable compression rates,
exceeding 20x, far surpassing the 3x rate achieved by Gzip, a widely used
general-purpose compressor. Furthermore, this advantage holds across different
LLM sizes and dataset types, demonstrating the robustness and practicality of
LLM-based methods in lossless text compression under generative AI workloads.

</details>


### [434] [ARDNS-FN-Quantum: A Quantum-Enhanced Reinforcement Learning Framework with Cognitive-Inspired Adaptive Exploration for Dynamic Environments](https://arxiv.org/pdf/2505.06300)
*Umberto Gonçalves de Sousa*

Main category: cs.LG

TL;DR: ARDNS-FN-Quantum, a novel RL framework with quantum enhancement, outperforms DQN and PPO in efficiency, stability, and adaptability, achieving a 99.5% success rate and lower reward variance.


<details>
  <summary>Details</summary>
Motivation: Traditional RL algorithms like DQN and PPO struggle with exploration, stability, and adaptability in dynamic environments. This study aims to address these limitations by integrating quantum computing and cognitive science.

Method: The framework combines a 2-qubit quantum circuit for action selection, a dual-memory system, and adaptive exploration strategies. It was tested in a 10X10 grid-world over 20,000 episodes.

Result: ARDNS-FN-Quantum achieved a 99.5% success rate, higher mean rewards, and fewer steps to goal compared to DQN and PPO. It also showed superior stability with lower reward variance.

Conclusion: By integrating quantum computing and cognitive science, ARDNS-FN-Quantum offers a scalable, human-like approach for adaptive learning, with applications in robotics and autonomous systems.

Abstract: Reinforcement learning (RL) has transformed sequential decision making, yet
traditional algorithms like Deep Q-Networks (DQNs) and Proximal Policy
Optimization (PPO) often struggle with efficient exploration, stability, and
adaptability in dynamic environments. This study presents ARDNS-FN-Quantum
(Adaptive Reward-Driven Neural Simulator with Quantum enhancement), a novel
framework that integrates a 2-qubit quantum circuit for action selection, a
dual-memory system inspired by human cognition, and adaptive exploration
strategies modulated by reward variance and curiosity. Evaluated in a 10X10
grid-world over 20,000 episodes, ARDNS-FN-Quantum achieves a 99.5% success rate
(versus 81.3% for DQN and 97.0% for PPO), a mean reward of 9.0528 across all
episodes (versus 1.2941 for DQN and 7.6196 for PPO), and an average of 46.7
steps to goal (versus 135.9 for DQN and 62.5 for PPO). In the last 100
episodes, it records a mean reward of 9.1652 (versus 7.0916 for DQN and 9.0310
for PPO) and 37.2 steps to goal (versus 52.7 for DQN and 53.4 for PPO).
Graphical analyses, including learning curves, steps-to-goal trends, reward
variance, and reward distributions, demonstrate ARDNS-FN-Quantum's superior
stability (reward variance 5.424 across all episodes versus 252.262 for DQN and
76.583 for PPO) and efficiency. By bridging quantum computing, cognitive
science, and RL, ARDNS-FN-Quantum offers a scalable, human-like approach to
adaptive learning in uncertain environments, with potential applications in
robotics, autonomous systems, and decision-making under uncertainty.

</details>


### [435] [Domain-Adversarial Anatomical Graph Networks for Cross-User Human Activity Recognition](https://arxiv.org/pdf/2505.06301)
*Xiaozhou Ye, Kevin I-Kai Wang*

Main category: cs.LG

TL;DR: The paper introduces EEG-ADG, a graph-based adversarial framework for cross-user HAR, leveraging biomechanical invariants and adversarial learning to improve generalization.


<details>
  <summary>Details</summary>
Motivation: Cross-user variability in HAR, caused by differences in sensor placement and behavior, limits traditional methods' generalization. The paper aims to address this by integrating biomechanical principles.

Method: Proposes EEG-ADG, a GNN-based framework with Variational Edge Feature Extractor and Gradient Reversal Layer to model biomechanical relationships and enforce domain generalization.

Result: Achieves state-of-the-art performance on OPPORTUNITY and DSADS datasets, demonstrating robustness to unseen users.

Conclusion: The work successfully combines biomechanical principles with adversarial learning, offering a unified and generalized model for cross-user HAR.

Abstract: Cross-user variability in Human Activity Recognition (HAR) remains a critical
challenge due to differences in sensor placement, body dynamics, and behavioral
patterns. Traditional methods often fail to capture biomechanical invariants
that persist across users, limiting their generalization capability. We propose
an Edge-Enhanced Graph-Based Adversarial Domain Generalization (EEG-ADG)
framework that integrates anatomical correlation knowledge into a unified graph
neural network (GNN) architecture. By modeling three biomechanically motivated
relationships together-Interconnected Units, Analogous Units, and Lateral
Units-our method encodes domain-invariant features while addressing
user-specific variability through Variational Edge Feature Extractor. A
Gradient Reversal Layer (GRL) enforces adversarial domain generalization,
ensuring robustness to unseen users. Extensive experiments on OPPORTUNITY and
DSADS datasets demonstrate state-of-the-art performance. Our work bridges
biomechanical principles with graph-based adversarial learning by integrating
information fusion techniques. This fusion of information underpins our unified
and generalized model for cross-user HAR.

</details>


### [436] [QiMeng-TensorOp: Automatically Generating High-Performance Tensor Operators with Hardware Primitives](https://arxiv.org/pdf/2505.06302)
*Xuzhi Zhang, Shaohui Peng, Qirui Zhou, Yuanbo Wen, Qi Guo, Ruizhi Chen, Xinguo Zhu, Weiqiang Xiong, Haixin Chen, Congying Ma, Ke Gao, Chen Zhao, Yanjun Wu, Yunji Chen, Ling Li*

Main category: cs.LG

TL;DR: QiMeng-TensorOp is a framework that auto-generates high-performance tensor operators for diverse hardware using LLMs, achieving significant performance gains and cost reductions.


<details>
  <summary>Details</summary>
Motivation: Manually optimizing tensor operators for varying hardware architectures is time-consuming and lacks portability, while LLMs alone struggle to leverage hardware characteristics effectively.

Method: The framework uses a one-line user prompt to guide LLMs in generating and tuning tensor operators with hardware primitives for optimal performance.

Result: QiMeng-TensorOp outperforms vanilla LLMs by up to 1291x, matches or exceeds human expert performance (e.g., 251% of OpenBLAS on RISC-V), and reduces development costs by 200x.

Conclusion: QiMeng-TensorOp effectively automates high-performance tensor operator generation, bridging the gap between LLMs and hardware optimization.

Abstract: Computation-intensive tensor operators constitute over 90\% of the
computations in Large Language Models (LLMs) and Deep Neural
Networks.Automatically and efficiently generating high-performance tensor
operators with hardware primitives is crucial for diverse and ever-evolving
hardware architectures like RISC-V, ARM, and GPUs, as manually optimized
implementation takes at least months and lacks portability.LLMs excel at
generating high-level language codes, but they struggle to fully comprehend
hardware characteristics and produce high-performance tensor operators. We
introduce a tensor-operator auto-generation framework with a one-line user
prompt (QiMeng-TensorOp), which enables LLMs to automatically exploit hardware
characteristics to generate tensor operators with hardware primitives, and tune
parameters for optimal performance across diverse hardware. Experimental
results on various hardware platforms, SOTA LLMs, and typical tensor operators
demonstrate that QiMeng-TensorOp effectively unleashes the computing capability
of various hardware platforms, and automatically generates tensor operators of
superior performance. Compared with vanilla LLMs, QiMeng-TensorOp achieves up
to $1291 \times$ performance improvement. Even compared with human experts,
QiMeng-TensorOp could reach $251 \%$ of OpenBLAS on RISC-V CPUs, and $124 \%$
of cuBLAS on NVIDIA GPUs. Additionally, QiMeng-TensorOp also significantly
reduces development costs by $200 \times$ compared with human experts.

</details>


### [437] [Collaborative Multi-LoRA Experts with Achievement-based Multi-Tasks Loss for Unified Multimodal Information Extraction](https://arxiv.org/pdf/2505.06303)
*Li Yuan, Yi Cai, Xudong Shen, Qing Li, Qingbao Huang, Zikun Deng, Tao Wang*

Main category: cs.LG

TL;DR: C-LoRAE, a collaborative multi-LoRA experts method with achievement-based multi-task loss, improves Multimodal Information Extraction by balancing task-specific and shared knowledge, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional MIE methods lack knowledge sharing across tasks, and recent unified approaches face computational inefficiency and gradient conflicts.

Method: C-LoRAE extends LoRA with universal and task-specific experts, plus an achievement-based loss to balance training.

Result: Outperforms traditional fine-tuning and LoRA on seven datasets across three MIE tasks.

Conclusion: C-LoRAE enhances generalization and mitigates gradient conflicts, offering a scalable solution for MIE.

Abstract: Multimodal Information Extraction (MIE) has gained attention for extracting
structured information from multimedia sources. Traditional methods tackle MIE
tasks separately, missing opportunities to share knowledge across tasks. Recent
approaches unify these tasks into a generation problem using instruction-based
T5 models with visual adaptors, optimized through full-parameter fine-tuning.
However, this method is computationally intensive, and multi-task fine-tuning
often faces gradient conflicts, limiting performance. To address these
challenges, we propose collaborative multi-LoRA experts with achievement-based
multi-task loss (C-LoRAE) for MIE tasks. C-LoRAE extends the low-rank
adaptation (LoRA) method by incorporating a universal expert to learn shared
multimodal knowledge from cross-MIE tasks and task-specific experts to learn
specialized instructional task features. This configuration enhances the
model's generalization ability across multiple tasks while maintaining the
independence of various instruction tasks and mitigating gradient conflicts.
Additionally, we propose an achievement-based multi-task loss to balance
training progress across tasks, addressing the imbalance caused by varying
numbers of training samples in MIE tasks. Experimental results on seven
benchmark datasets across three key MIE tasks demonstrate that C-LoRAE achieves
superior overall performance compared to traditional fine-tuning methods and
LoRA methods while utilizing a comparable number of training parameters to
LoRA.

</details>


### [438] [GraphComp: Extreme Error-bounded Compression of Scientific Data via Temporal Graph Autoencoders](https://arxiv.org/pdf/2505.06316)
*Guozhong Li, Muhannad Alhumaidi, Spiros Skiadopoulos, Ibrahim Hoteit, Panos Kalnis*

Main category: cs.LG

TL;DR: GRAPHCOMP, a graph-based error-bounded lossy compression method, leverages spatial and temporal correlations in scientific data for higher compression ratios, outperforming state-of-the-art methods by 22-50%.


<details>
  <summary>Details</summary>
Motivation: Existing error-bounded lossy compression methods often ignore spatial and temporal correlations in scientific data, limiting compression efficiency.

Method: GRAPHCOMP uses irregular segmentation and a graph representation, combined with a temporal graph autoencoder inspired by GNNs, to compress data while preserving correlations.

Result: GRAPHCOMP achieves the highest compression ratios, outperforming other methods by 22-50% on real and synthetic datasets.

Conclusion: GRAPHCOMP effectively addresses the limitations of existing methods by leveraging graph-based techniques, offering superior compression for scientific data.

Abstract: The generation of voluminous scientific data poses significant challenges for
efficient storage, transfer, and analysis. Recently, error-bounded lossy
compression methods emerged due to their ability to achieve high compression
ratios while controlling data distortion. However, they often overlook the
inherent spatial and temporal correlations within scientific data, thus missing
opportunities for higher compression. In this paper we propose GRAPHCOMP, a
novel graph-based method for error-bounded lossy compression of scientific
data. We perform irregular segmentation of the original grid data and generate
a graph representation that preserves the spatial and temporal correlations.
Inspired by Graph Neural Networks (GNNs), we then propose a temporal graph
autoencoder to learn latent representations that significantly reduce the size
of the graph, effectively compressing the original data. Decompression reverses
the process and utilizes the learnt graph model together with the latent
representation to reconstruct an approximation of the original data. The
decompressed data are guaranteed to satisfy a user-defined point-wise error
bound. We compare our method against the state-of-the-art error-bounded lossy
methods (i.e., HPEZ, SZ3.1, SPERR, and ZFP) on large-scale real and synthetic
data. GRAPHCOMP consistently achieves the highest compression ratio across most
datasets, outperforming the second-best method by margins ranging from 22% to
50%.

</details>


### [439] [Reinforcement Learning for Game-Theoretic Resource Allocation on Graphs](https://arxiv.org/pdf/2505.06319)
*Zijian An, Lifeng Zhou*

Main category: cs.LG

TL;DR: The paper applies RL methods (DQN and PPO) to solve the multi-step Colonel Blotto Game on graphs, introducing an action-displacement matrix to handle constraints. RL outperforms baselines, achieving balanced win rates and exploiting graph asymmetries.


<details>
  <summary>Details</summary>
Motivation: The challenge of finding optimal strategies in dynamic, graph-constrained resource allocation games motivates the use of RL to handle complex action spaces and structural constraints.

Method: Formulates the problem as an MDP, uses DQN and PPO with an action-displacement adjacency matrix to enforce graph constraints, and compares RL policies against random, greedy, and learned baselines.

Result: RL methods (DQN and PPO) consistently outperform baselines, achieving a 50% win rate against learned policies and exploiting graph asymmetries effectively.

Conclusion: RL is effective for graph-based resource allocation games, adapting to structural advantages and initial resource imbalances.

Abstract: Game-theoretic resource allocation on graphs (GRAG) involves two players
competing over multiple steps to control nodes of interest on a graph, a
problem modeled as a multi-step Colonel Blotto Game (MCBG). Finding optimal
strategies is challenging due to the dynamic action space and structural
constraints imposed by the graph. To address this, we formulate the MCBG as a
Markov Decision Process (MDP) and apply Reinforcement Learning (RL) methods,
specifically Deep Q-Network (DQN) and Proximal Policy Optimization (PPO). To
enforce graph constraints, we introduce an action-displacement adjacency matrix
that dynamically generates valid action sets at each step. We evaluate RL
performance across a variety of graph structures and initial resource
distributions, comparing against random, greedy, and learned RL policies.
Experimental results show that both DQN and PPO consistently outperform
baseline strategies and converge to a balanced $50\%$ win rate when competing
against the learned RL policy. Particularly, on asymmetric graphs, RL agents
successfully exploit structural advantages and adapt their allocation
strategies, even under disadvantageous initial resource distributions.

</details>


### [440] [Divide (Text) and Conquer (Sentiment): Improved Sentiment Classification by Constituent Conflict Resolution](https://arxiv.org/pdf/2505.06320)
*Jan Kościałkowski, Paweł Marcinkowski*

Main category: cs.LG

TL;DR: The paper introduces new methods for handling conflicting sentiments in long passages, using an MLP model that outperforms baselines at a fraction of the cost.


<details>
  <summary>Details</summary>
Motivation: Sentiment classification is challenging for passages with conflicting tones, especially longer ones, leading to poor model performance.

Method: Novel methodologies for isolating and aggregating conflicting sentiments, including an MLP model.

Result: The MLP model outperforms baseline models on datasets like Amazon, Twitter, and SST, costing ~1/100 of fine-tuning baselines.

Conclusion: The proposed MLP-based approach effectively predicts overall sentiment in complex passages while being cost-efficient.

Abstract: Sentiment classification, a complex task in natural language processing,
becomes even more challenging when analyzing passages with multiple conflicting
tones. Typically, longer passages exacerbate this issue, leading to decreased
model performance. The aim of this paper is to introduce novel methodologies
for isolating conflicting sentiments and aggregating them to effectively
predict the overall sentiment of such passages. One of the aggregation
strategies involves a Multi-Layer Perceptron (MLP) model which outperforms
baseline models across various datasets, including Amazon, Twitter, and SST
while costing $\sim$1/100 of what fine-tuning the baseline would take.

</details>


### [441] [Learn to Think: Bootstrapping LLM Reasoning Capability Through Graph Learning](https://arxiv.org/pdf/2505.06321)
*Hang Gao, Chenhao Zhang, Tie Wang, Junsuo Zhao, Fengge Wu, Changwen Zheng, Huaping Liu*

Main category: cs.LG

TL;DR: A novel framework using graph learning and GNNs enhances LLMs' reasoning flexibility and performance without extra training or task-specific prompts.


<details>
  <summary>Details</summary>
Motivation: Addressing LLMs' limitations in computational cost and complex reasoning by improving flexibility and generalizability.

Method: Models reasoning as a graph, uses LLM-based graph learning, and integrates a GNN for adaptive representation learning.

Result: Significant improvement in reasoning performance across tasks without additional training or prompt design.

Conclusion: The framework effectively enhances LLMs' reasoning capabilities adaptively and flexibly.

Abstract: Large Language Models (LLMs) have achieved remarkable success across various
domains. However, they still face significant challenges, including high
computational costs for training and limitations in solving complex reasoning
problems. Although existing methods have extended the reasoning capabilities of
LLMs through structured paradigms, these approaches often rely on task-specific
prompts and predefined reasoning processes, which constrain their flexibility
and generalizability. To address these limitations, we propose a novel
framework that leverages graph learning to enable more flexible and adaptive
reasoning capabilities for LLMs. Specifically, this approach models the
reasoning process of a problem as a graph and employs LLM-based graph learning
to guide the adaptive generation of each reasoning step. To further enhance the
adaptability of the model, we introduce a Graph Neural Network (GNN) module to
perform representation learning on the generated reasoning process, enabling
real-time adjustments to both the model and the prompt. Experimental results
demonstrate that this method significantly improves reasoning performance
across multiple tasks without requiring additional training or task-specific
prompt design. Code can be found in https://github.com/zch65458525/L2T.

</details>


### [442] [Human in the Latent Loop (HILL): Interactively Guiding Model Training Through Human Intuition](https://arxiv.org/pdf/2505.06325)
*Daniel Geissler, Lars Krupp, Vishal Banwari, David Habusch, Bo Zhou, Paul Lukowicz, Jakob Karolus*

Main category: cs.LG

TL;DR: HILL is an interactive framework that integrates human intuition into model training by reshaping latent space representations, improving performance while revealing potential biases.


<details>
  <summary>Details</summary>
Motivation: Latent spaces are often complex and obscure; incorporating human intuition can enhance model understanding and training efficiency.

Method: HILL uses a knowledge distillation-inspired approach to integrate user modifications into the training loop, treating human input as a teacher.

Result: Human-guided modifications improved model performance and generalization, but also highlighted risks of user biases.

Conclusion: HILL introduces a novel human-AI interaction paradigm, balancing the benefits of human intuition with the need to mitigate biases.

Abstract: Latent space representations are critical for understanding and improving the
behavior of machine learning models, yet they often remain obscure and
intricate. Understanding and exploring the latent space has the potential to
contribute valuable human intuition and expertise about respective domains. In
this work, we present HILL, an interactive framework allowing users to
incorporate human intuition into the model training by interactively reshaping
latent space representations. The modifications are infused into the model
training loop via a novel approach inspired by knowledge distillation, treating
the user's modifications as a teacher to guide the model in reshaping its
intrinsic latent representation. The process allows the model to converge more
effectively and overcome inefficiencies, as well as provide beneficial insights
to the user. We evaluated HILL in a user study tasking participants to train an
optimal model, closely observing the employed strategies. The results
demonstrated that human-guided latent space modifications enhance model
performance while maintaining generalization, yet also revealing the risks of
including user biases. Our work introduces a novel human-AI interaction
paradigm that infuses human intuition into model training and critically
examines the impact of human intervention on training strategies and potential
biases.

</details>


### [443] [Prompting Large Language Models for Training-Free Non-Intrusive Load Monitoring](https://arxiv.org/pdf/2505.06330)
*Junyu Xue, Xudong Wang, Xiaoling He, Shicheng Liu, Yi Wang, Guoming Tang*

Main category: cs.LG

TL;DR: A prompt-based NILM framework using LLMs achieves competitive accuracy, robust generalization, and interpretability without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of deep learning in NILM, such as labeled data dependency, poor generalization, and lack of interpretability.

Method: Introduce a prompt-based framework leveraging LLMs with in-context learning, integrating appliance features, timestamps, and examples.

Result: Achieves an average F1-score of 0.676 on unseen households, with robust generalization and clear explanations.

Conclusion: LLMs reduce data needs, improve adaptability, and offer transparent energy disaggregation in NILM.

Abstract: Non-intrusive Load Monitoring (NILM) aims to disaggregate aggregate household
electricity consumption into individual appliance usage, enabling more
effective energy management. While deep learning has advanced NILM, it remains
limited by its dependence on labeled data, restricted generalization, and lack
of interpretability. In this paper, we introduce the first prompt-based NILM
framework that leverages Large Language Models (LLMs) with in-context learning.
We design and evaluate prompt strategies that integrate appliance features,
timestamps and contextual information, as well as representative time-series
examples, using the REDD dataset. With optimized prompts, LLMs achieve
competitive state detection accuracy, reaching an average F1-score of 0.676 on
unseen households, and demonstrate robust generalization without the need for
fine-tuning. LLMs also enhance interpretability by providing clear,
human-readable explanations for their predictions. Our results show that LLMs
can reduce data requirements, improve adaptability, and provide transparent
energy disaggregation in NILM applications.

</details>


### [444] [Mask-PINNs: Regulating Feature Distributions in Physics-Informed Neural Networks](https://arxiv.org/pdf/2505.06331)
*Feilong Jiang, Xiaonan Hou, Jianqiao Ye, Min Xia*

Main category: cs.LG

TL;DR: Mask-PINNs improve PINNs by addressing internal covariate shift with a learnable mask, enhancing stability, accuracy, and robustness.


<details>
  <summary>Details</summary>
Motivation: Internal covariate shift in PINNs limits neural network capacity, which Mask-PINNs aim to resolve.

Method: Introduces a learnable, nonlinear mask function to stabilize feature distributions without violating physics.

Result: Improves feature distribution stability, accuracy, and robustness across PDE benchmarks.

Conclusion: Mask-PINNs enable stable, efficient training of wider networks, addressing a key limitation in PINNs.

Abstract: Physics-Informed Neural Networks (PINNs) are a class of deep learning models
designed to solve partial differential equations by incorporating physical laws
directly into the loss function. However, the internal covariate shift, which
has been largely overlooked, hinders the effective utilization of neural
network capacity in PINNs. To this end, we propose Mask-PINNs, a novel
architecture designed to address this issue in PINNs. Unlike traditional
normalization methods such as BatchNorm or LayerNorm, we introduce a learnable,
nonlinear mask function that constrains the feature distributions without
violating underlying physics. The experimental results show that the proposed
method significantly improves feature distribution stability, accuracy, and
robustness across various activation functions and PDE benchmarks. Furthermore,
it enables the stable and efficient training of wider networks a capability
that has been largely overlooked in PINNs.

</details>


### [445] [NSF-MAP: Neurosymbolic Multimodal Fusion for Robust and Interpretable Anomaly Prediction in Assembly Pipelines](https://arxiv.org/pdf/2505.06333)
*Chathurangi Shyalika, Renjith Prasad, Fadi El Kalach, Revathy Venkataramanan, Ramtin Zand, Ramy Harik, Amit Sheth*

Main category: cs.LG

TL;DR: A neurosymbolic AI and fusion-based approach for multimodal anomaly prediction in assembly pipelines, combining time series and image data with transfer learning for improved performance.


<details>
  <summary>Details</summary>
Motivation: Conventional single-modality methods are inadequate for complex predictive environments with abundant data and multiple modalities, necessitating a more robust solution.

Method: Proposes a time series and image-based fusion model using decision-level fusion techniques, transfer learning, and knowledge-infused learning.

Result: The approach effectively harnesses complementary strengths of time series and image data, offering robust and interpretable anomaly prediction with enhanced performance.

Conclusion: The neurosymbolic AI-based fusion method outperforms traditional baselines, providing a scalable and interpretable solution for anomaly prediction in assembly pipelines.

Abstract: In modern assembly pipelines, identifying anomalies is crucial in ensuring
product quality and operational efficiency. Conventional single-modality
methods fail to capture the intricate relationships required for precise
anomaly prediction in complex predictive environments with abundant data and
multiple modalities. This paper proposes a neurosymbolic AI and fusion-based
approach for multimodal anomaly prediction in assembly pipelines. We introduce
a time series and image-based fusion model that leverages decision-level fusion
techniques. Our research builds upon three primary novel approaches in
multimodal learning: time series and image-based decision-level fusion
modeling, transfer learning for fusion, and knowledge-infused learning. We
evaluate the novel method using our derived and publicly available multimodal
dataset and conduct comprehensive ablation studies to assess the impact of our
preprocessing techniques and fusion model compared to traditional baselines.
The results demonstrate that a neurosymbolic AI-based fusion approach that uses
transfer learning can effectively harness the complementary strengths of time
series and image data, offering a robust and interpretable approach for anomaly
prediction in assembly pipelines with enhanced performance. \noindent The
datasets, codes to reproduce the results, supplementary materials, and demo are
available at https://github.com/ChathurangiShyalika/NSF-MAP.

</details>


### [446] [Remote Rowhammer Attack using Adversarial Observations on Federated Learning Clients](https://arxiv.org/pdf/2505.06335)
*Jinsheng Yuan, Yuhang Hao, Weisi Guo, Yun Wu, Chongyan Gu*

Main category: cs.LG

TL;DR: The paper demonstrates a novel attack on Federated Learning (FL) servers by exploiting repetitive memory updates to induce bit flips via remote rowhammer, without requiring backdoor access.


<details>
  <summary>Details</summary>
Motivation: To investigate under-explored server-side vulnerabilities in FL, where client-facing attacks are more commonly studied, and to highlight the risks of remote rowhammer attacks.

Method: Using reinforcement learning (RL), the attacker manipulates client sensor observations to maximize repetitive memory updates on the server, inducing bit flips in DRAM.

Result: The attack achieves a 70% repeated update rate (RUR) in a large-scale FL ASR system, successfully corrupting server memory.

Conclusion: This reveals a new security threat in FL, necessitating further research into mitigation strategies and hardware design.

Abstract: Federated Learning (FL) has the potential for simultaneous global learning
amongst a large number of parallel agents, enabling emerging AI such as LLMs to
be trained across demographically diverse data. Central to this being efficient
is the ability for FL to perform sparse gradient updates and remote direct
memory access at the central server. Most of the research in FL security
focuses on protecting data privacy at the edge client or in the communication
channels between the client and server. Client-facing attacks on the server are
less well investigated as the assumption is that a large collective of clients
offer resilience.
  Here, we show that by attacking certain clients that lead to a high frequency
repetitive memory update in the server, we can remote initiate a rowhammer
attack on the server memory. For the first time, we do not need backdoor access
to the server, and a reinforcement learning (RL) attacker can learn how to
maximize server repetitive memory updates by manipulating the client's sensor
observation. The consequence of the remote rowhammer attack is that we are able
to achieve bit flips, which can corrupt the server memory. We demonstrate the
feasibility of our attack using a large-scale FL automatic speech recognition
(ASR) systems with sparse updates, our adversarial attacking agent can achieve
around 70\% repeated update rate (RUR) in the targeted server model,
effectively inducing bit flips on server DRAM. The security implications are
that can cause disruptions to learning or may inadvertently cause elevated
privilege. This paves the way for further research on practical mitigation
strategies in FL and hardware design.

</details>


### [447] [Latent Diffeomorphic Dynamic Mode Decomposition](https://arxiv.org/pdf/2505.06351)
*Willem Diepeveen, Jon Schwenk, Andrea Bertozzi*

Main category: cs.LG

TL;DR: LDDMD combines DMD's interpretability with RNNs' predictive power for non-linear systems, demonstrated in streamflow prediction.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between interpretability (DMD) and predictive power (RNNs) in non-linear system analysis.

Method: Latent Diffeomorphic Dynamic Mode Decomposition (LDDMD), integrating DMD and RNNs.

Result: Effective modeling of complex non-linear systems with memory, enabling accurate predictions (e.g., streamflow).

Conclusion: LDDMD successfully balances simplicity, interpretability, and predictive power for non-linear systems.

Abstract: We present Latent Diffeomorphic Dynamic Mode Decomposition (LDDMD), a new
data reduction approach for the analysis of non-linear systems that combines
the interpretability of Dynamic Mode Decomposition (DMD) with the predictive
power of Recurrent Neural Networks (RNNs). Notably, LDDMD maintains simplicity,
which enhances interpretability, while effectively modeling and learning
complex non-linear systems with memory, enabling accurate predictions. This is
exemplified by its successful application in streamflow prediction.

</details>


### [448] [CAST: Time-Varying Treatment Effects with Application to Chemotherapy and Radiotherapy on Head and Neck Squamous Cell Carcinoma](https://arxiv.org/pdf/2505.06367)
*Everest Yang, Ria Vasishtha, Luqman K. Dad, Lisa A. Kachnic, Andrew Hope, Eric Wang, Xiao Wu, Yading Yuan, David J. Brenner, Igor Shuryak*

Main category: cs.LG

TL;DR: CAST introduces a continuous-time framework for estimating dynamic treatment effects in survival data, overcoming limitations of fixed-time methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for causal survival analysis estimate effects at fixed time points, missing dynamic changes over time. CAST aims to capture these temporal dynamics.

Method: CAST combines parametric and non-parametric methods to model treatment effects as continuous functions of time, applied to the RADCURE dataset of HNSCC patients.

Result: CAST reveals how treatment effects evolve (rise, peak, decline) over time, aiding clinicians in optimizing treatment timing and selection.

Conclusion: CAST advances causal machine learning for personalized care by enabling continuous-time effect estimation in survival data.

Abstract: Causal machine learning (CML) enables individualized estimation of treatment
effects, offering critical advantages over traditional correlation-based
methods. However, existing approaches for medical survival data with censoring
such as causal survival forests estimate effects at fixed time points, limiting
their ability to capture dynamic changes over time. We introduce Causal
Analysis for Survival Trajectories (CAST), a novel framework that models
treatment effects as continuous functions of time following treatment. By
combining parametric and non-parametric methods, CAST overcomes the limitations
of discrete time-point analysis to estimate continuous effect trajectories.
Using the RADCURE dataset [1] of 2,651 patients with head and neck squamous
cell carcinoma (HNSCC) as a clinically relevant example, CAST models how
chemotherapy and radiotherapy effects evolve over time at the population and
individual levels. By capturing the temporal dynamics of treatment response,
CAST reveals how treatment effects rise, peak, and decline over the follow-up
period, helping clinicians determine when and for whom treatment benefits are
maximized. This framework advances the application of CML to personalized care
in HNSCC and other life-threatening medical conditions. Source code/data
available at: https://github.com/CAST-FW/HNSCC

</details>


### [449] [The ML.ENERGY Benchmark: Toward Automated Inference Energy Measurement and Optimization](https://arxiv.org/pdf/2505.06371)
*Jae-Won Chung, Jiachen Liu, Jeff J. Ma, Ruofan Wu, Oh Jun Kweon, Yuxuan Xia, Zhiyu Wu, Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: The paper introduces the ML.ENERGY Benchmark, a tool for measuring and optimizing energy consumption in generative AI services, highlighting its design principles, implementation, and significant energy-saving results.


<details>
  <summary>Details</summary>
Motivation: Energy is a critical but often overlooked bottleneck in ML systems, especially with the rapid adoption of generative AI in real-world services.

Method: The ML.ENERGY Benchmark suite measures inference energy consumption in realistic service environments, with a leaderboard for tracking performance and optimization.

Result: The benchmark includes energy measurements of 40 model architectures across 6 tasks, showing how design choices impact energy and automated optimizations can save over 40% energy.

Conclusion: The ML.ENERGY Benchmark is a valuable, open-source tool for understanding and reducing energy consumption in generative AI services, adaptable to various models and scenarios.

Abstract: As the adoption of Generative AI in real-world services grow explosively,
energy has emerged as a critical bottleneck resource. However, energy remains a
metric that is often overlooked, under-explored, or poorly understood in the
context of building ML systems. We present the ML.ENERGY Benchmark, a benchmark
suite and tool for measuring inference energy consumption under realistic
service environments, and the corresponding ML.ENERGY Leaderboard, which have
served as a valuable resource for those hoping to understand and optimize the
energy consumption of their generative AI services. In this paper, we explain
four key design principles for benchmarking ML energy we have acquired over
time, and then describe how they are implemented in the ML.ENERGY Benchmark. We
then highlight results from the latest iteration of the benchmark, including
energy measurements of 40 widely used model architectures across 6 different
tasks, case studies of how ML design choices impact energy consumption, and how
automated optimization recommendations can lead to significant (sometimes more
than 40%) energy savings without changing what is being computed by the model.
The ML.ENERGY Benchmark is open-source and can be easily extended to various
customized models and application scenarios.

</details>


### [450] [RiM: Record, Improve and Maintain Physical Well-being using Federated Learning](https://arxiv.org/pdf/2505.06384)
*Aditya Mishra, Haroon Lone*

Main category: cs.LG

TL;DR: RiM is a mobile app using federated learning to improve students' physical well-being while ensuring privacy.


<details>
  <summary>Details</summary>
Motivation: Addressing students' neglect of physical health due to academic pressures and privacy concerns in traditional ML methods.

Method: Uses a pre-trained MLP model on simulated data, fine-tuned via federated learning with student data, ensuring privacy by sharing only model weights.

Result: FedAvg-based RiM achieves 60.71% accuracy and 0.91 MAE, outperforming FedPer.

Conclusion: RiM effectively predicts lifestyle deficits while preserving privacy, proving its real-world applicability.

Abstract: In academic settings, the demanding environment often forces students to
prioritize academic performance over their physical well-being. Moreover,
privacy concerns and the inherent risk of data breaches hinder the deployment
of traditional machine learning techniques for addressing these health
challenges. In this study, we introduce RiM: Record, Improve, and Maintain, a
mobile application which incorporates a novel personalized machine learning
framework that leverages federated learning to enhance students' physical
well-being by analyzing their lifestyle habits. Our approach involves
pre-training a multilayer perceptron (MLP) model on a large-scale simulated
dataset to generate personalized recommendations. Subsequently, we employ
federated learning to fine-tune the model using data from IISER Bhopal
students, thereby ensuring its applicability in real-world scenarios. The
federated learning approach guarantees differential privacy by exclusively
sharing model weights rather than raw data. Experimental results show that the
FedAvg-based RiM model achieves an average accuracy of 60.71% and a mean
absolute error of 0.91--outperforming the FedPer variant (average accuracy
46.34%, MAE 1.19)--thereby demonstrating its efficacy in predicting lifestyle
deficits under privacy-preserving constraints.

</details>


### [451] [Tweedie Regression for Video Recommendation System](https://arxiv.org/pdf/2505.06445)
*Yan Zheng, Qiang Chen, Chenglei Niu*

Main category: cs.LG

TL;DR: The paper redefines video recommendation from CTR classification to regression, using Tweedie Loss to maximize revenue via user watch time, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Businesses aim for more than clicks; they seek increased watch time for higher ad revenue, which CTR-focused methods don't address.

Method: Switches from classification to regression, introducing Tweedie Loss for better handling of sparse positive labels and diverse user interests.

Result: Offline and online tests show improved user engagement (watch time) and revenue, with Tweedie Loss outperforming traditional metrics.

Conclusion: Tweedie Regression is more effective for revenue-focused video recommendations, offering a framework for single-objective loss function design.

Abstract: Modern recommendation systems aim to increase click-through rates (CTR) for
better user experience, through commonly treating ranking as a classification
task focused on predicting CTR. However, there is a gap between this method and
the actual objectives of businesses across different sectors. In video
recommendation services, the objective of video on demand (VOD) extends beyond
merely encouraging clicks, but also guiding users to discover their true
interests, leading to increased watch time. And longer users watch time will
leads to more revenue through increased chances of presenting online display
advertisements. This research addresses the issue by redefining the problem
from classification to regression, with a focus on maximizing revenue through
user viewing time. Due to the lack of positive labels on recommendation, the
study introduces Tweedie Loss Function, which is better suited in this scenario
than the traditional mean square error loss. The paper also provides insights
on how Tweedie process capture users diverse interests. Our offline simulation
and online A/B test revealed that we can substantially enhance our core
business objectives: user engagement in terms of viewing time and,
consequently, revenue. Additionally, we provide a theoretical comparison
between the Tweedie Loss and the commonly employed viewing time weighted
Logloss, highlighting why Tweedie Regression stands out as an efficient
solution. We further outline a framework for designing a loss function that
focuses on a singular objective.

</details>


### [452] [Structured Prediction with Abstention via the Lovász Hinge](https://arxiv.org/pdf/2505.06446)
*Jessie Finocchiaro, Rafael Frongillo, Enrique Nueve*

Main category: cs.LG

TL;DR: The Lovász hinge is inconsistent for binary structured classification unless the evaluation function is modular. A consistent target loss, the structured abstain problem, is identified, along with link functions for polymatroids. Tight embedding conditions and experimental validation are provided, with extensions to multiclass settings.


<details>
  <summary>Details</summary>
Motivation: To address the inconsistency of the Lovász hinge in binary structured classification and identify a consistent target loss for improved interpretability and performance.

Method: Leveraging the embedding framework, the structured abstain problem is introduced, with consistent link functions for polymatroids. Tight embedding conditions are derived and validated experimentally.

Result: The Lovász hinge is consistent only for modular functions. The structured abstain problem and link functions provide consistency for polymatroids, with experimental validation.

Conclusion: The structured abstain problem and derived link functions offer a consistent solution for binary and multiclass structured classification, enhancing interpretability and performance.

Abstract: The Lov\'asz hinge is a convex loss function proposed for binary structured
classification, in which k related binary predictions jointly evaluated by a
submodular function. Despite its prevalence in image segmentation and related
tasks, the consistency of the Lov\'asz hinge has remained open. We show that
the Lov\'asz hinge is inconsistent with its desired target unless the set
function used for evaluation is modular. Leveraging the embedding framework of
Finocchiaro et al. (2024), we find the target loss for which the Lov\'asz hinge
is consistent. This target, which we call the structured abstain problem, is a
variant of selective classification for structured prediction that allows one
to abstain on any subset of the k binary predictions. We derive a family of
link functions, each of which is simultaneously consistent for all
polymatroids, a subset of submodular set functions. We then give sufficient
conditions on the polymatroid for the structured abstain problem to be tightly
embedded by the Lov\'asz hinge, meaning no target prediction is redundant. We
experimentally demonstrate the potential of the structured abstain problem for
interpretability in structured classification tasks. Finally, for the
multiclass setting, we show that one can combine the binary encoding
construction of Ramaswamy et al. (2018) with our link construction to achieve
an efficient consistent surrogate for a natural multiclass generalization of
the structured abstain problem.

</details>


### [453] [Sponge Attacks on Sensing AI: Energy-Latency Vulnerabilities and Defense via Model Pruning](https://arxiv.org/pdf/2505.06454)
*Syed Mhamudul Hasan, Hussein Zangoti, Iraklis Anagnostopoulos, Abdur R. Shahid*

Main category: cs.LG

TL;DR: The paper explores energy-latency sponge attacks on sensing-based AI models in IoT, demonstrates their impact, and proposes pruning as a defense, balancing efficiency and resilience.


<details>
  <summary>Details</summary>
Motivation: Address the overlooked threat of sponge attacks on lightweight AI models in IoT, where energy and latency are critical.

Method: Systematically study sponge attacks on sensing-based AI, using wearable sensing as a case study, and evaluate pruning as a defense.

Result: Sponge attacks degrade performance by increasing energy use and latency; pruning improves resilience but involves efficiency trade-offs.

Conclusion: Pruning enhances attack resilience in sensing-based AI, but designers must weigh efficiency against security in IoT deployments.

Abstract: Recent studies have shown that sponge attacks can significantly increase the
energy consumption and inference latency of deep neural networks (DNNs).
However, prior work has focused primarily on computer vision and natural
language processing tasks, overlooking the growing use of lightweight AI models
in sensing-based applications on resource-constrained devices, such as those in
Internet of Things (IoT) environments. These attacks pose serious threats of
energy depletion and latency degradation in systems where limited battery
capacity and real-time responsiveness are critical for reliable operation. This
paper makes two key contributions. First, we present the first systematic
exploration of energy-latency sponge attacks targeting sensing-based AI models.
Using wearable sensing-based AI as a case study, we demonstrate that sponge
attacks can substantially degrade performance by increasing energy consumption,
leading to faster battery drain, and by prolonging inference latency. Second,
to mitigate such attacks, we investigate model pruning, a widely adopted
compression technique for resource-constrained AI, as a potential defense. Our
experiments show that pruning-induced sparsity significantly improves model
resilience against sponge poisoning. We also quantify the trade-offs between
model efficiency and attack resilience, offering insights into the security
implications of model compression in sensing-based AI systems deployed in IoT
environments.

</details>


### [454] [Improved Uncertainty Quantification in Physics-Informed Neural Networks Using Error Bounds and Solution Bundles](https://arxiv.org/pdf/2505.06459)
*Pablo Flores, Olga Graf, Pavlos Protopapas, Karim Pichara*

Main category: cs.LG

TL;DR: The paper proposes a two-step method to train Bayesian Neural Networks for uncertainty quantification in Physics-Informed Neural Networks (PINNs), improving uncertainty estimation using error bounds and applying it to cosmology problems.


<details>
  <summary>Details</summary>
Motivation: PINNs lack built-in uncertainty quantification, which is crucial for reliable solutions in differential equation systems, especially in fields like cosmology.

Method: A two-step procedure trains Bayesian Neural Networks, incorporating heteroscedastic variance derived from PINN error bounds for better uncertainty estimation.

Result: Improved uncertainty quantification for PINN solutions, demonstrated in forward problems and parameter estimation in cosmology.

Conclusion: The method enhances PINN reliability by providing robust uncertainty estimates, applicable to both forward and inverse problems in physics.

Abstract: Physics-Informed Neural Networks (PINNs) have been widely used to obtain
solutions to various physical phenomena modeled as Differential Equations. As
PINNs are not naturally equipped with mechanisms for Uncertainty
Quantification, some work has been done to quantify the different uncertainties
that arise when dealing with PINNs. In this paper, we use a two-step procedure
to train Bayesian Neural Networks that provide uncertainties over the solutions
to differential equation systems provided by PINNs. We use available error
bounds over PINNs to formulate a heteroscedastic variance that improves the
uncertainty estimation. Furthermore, we solve forward problems and utilize the
obtained uncertainties when doing parameter estimation in inverse problems in
cosmology.

</details>


### [455] [Probing In-Context Learning: Impact of Task Complexity and Model Architecture on Generalization and Efficiency](https://arxiv.org/pdf/2505.06475)
*Binwen Liu, Peiyu Xu, Quan Yuan, Yihong Chen*

Main category: cs.LG

TL;DR: The paper examines in-context learning (ICL) by testing various model architectures on tasks of differing complexity, revealing how architecture impacts performance.


<details>
  <summary>Details</summary>
Motivation: To understand how different model architectures influence in-context learning (ICL) across tasks with varying complexity, including temporal and recursive reasoning.

Method: Four models (GPT2-style Transformer, Transformer with FlashAttention, Hyena-based convolutional model, Mamba state-space model) are trained on synthetic datasets and evaluated for generalization. Tasks include Gaussian kernel regression and nonlinear dynamical systems.

Result: Model architecture significantly affects ICL performance: standard Transformer is robust, Mamba excels in temporal tasks, Hyena handles long-range dependencies but with early variance, and FlashAttention is efficient but data-sensitive.

Conclusion: Architecture choice is crucial for ICL, with each model excelling in specific scenarios. Curriculum learning and input scaling are key for handling complex tasks.

Abstract: We investigate in-context learning (ICL) through a meticulous experimental
framework that systematically varies task complexity and model architecture.
Extending beyond the linear regression baseline, we introduce Gaussian kernel
regression and nonlinear dynamical system tasks, which emphasize temporal and
recursive reasoning. We evaluate four distinct models: a GPT2-style
Transformer, a Transformer with FlashAttention mechanism, a convolutional
Hyena-based model, and the Mamba state-space model. Each model is trained from
scratch on synthetic datasets and assessed for generalization during testing.
Our findings highlight that model architecture significantly shapes ICL
performance. The standard Transformer demonstrates robust performance across
diverse tasks, while Mamba excels in temporally structured dynamics. Hyena
effectively captures long-range dependencies but shows higher variance early in
training, and FlashAttention offers computational efficiency but is more
sensitive in low-data regimes. Further analysis uncovers locality-induced
shortcuts in Gaussian kernel tasks, enhanced nonlinear separability through
input range scaling, and the critical role of curriculum learning in mastering
high-dimensional tasks.

</details>


### [456] [QoS-Efficient Serving of Multiple Mixture-of-Expert LLMs Using Partial Runtime Reconfiguration](https://arxiv.org/pdf/2505.06481)
*HamidReza Imani, Jiaxin Peng, Peiman Mohseni, Abdolah Amirany, Tarek El-Ghazawi*

Main category: cs.LG

TL;DR: A serving system for efficient multi-tenant deployment of MoE-LLMs on a single GPU, using similarity-based expert consolidation and runtime partial reconfiguration to reduce memory and maintain performance.


<details>
  <summary>Details</summary>
Motivation: Addressing high memory demands and inefficiencies in multi-tenant environments for MoE-LLMs, where conventional virtualization falls short.

Method: Proposes similarity-based expert consolidation to share experts across models and runtime partial reconfiguration to dynamically adjust layers, ensuring quality and throughput.

Result: Achieves 85% reduction in turnaround time on Mixtral-8x7B models and maintains output quality with Google's Switch Transformer Base-8, outperforming baselines.

Conclusion: The approach effectively balances memory efficiency and performance, making it viable for multi-tenant MoE-LLM deployment on single-GPU systems.

Abstract: The deployment of mixture-of-experts (MoE) large language models (LLMs)
presents significant challenges due to their high memory demands. These
challenges become even more pronounced in multi-tenant environments, where
shared resources must accommodate multiple models, limiting the effectiveness
of conventional virtualization techniques. This paper addresses the problem of
efficiently serving multiple fine-tuned MoE-LLMs on a single-GPU. We propose a
serving system that employs \textit{similarity-based expert consolidation} to
reduce the overall memory footprint by sharing similar experts across models.
To ensure output quality, we introduce \textit{runtime partial
reconfiguration}, dynamically replacing non-expert layers when processing
requests from different models. As a result, our approach achieves a
competitive output quality while maintaining throughput comparable to serving a
single model while incurring a negligible increase in time-to-first-token
(TTFT). Experiments on a server with a single NVIDIA A100 GPU (80GB) using
Mixtral-8x7B models demonstrate an 85\% average reduction in turnaround time
compared to NVIDIA's multi-instance GPU (MIG). Furthermore, experiments on
Google's Switch Transformer Base-8 model with up to four variants demonstrate
the scalability and resilience of our approach in maintaining output quality
compared to other model merging baselines, highlighting its effectiveness.

</details>


### [457] [Video-Enhanced Offline Reinforcement Learning: A Model-Based Approach](https://arxiv.org/pdf/2505.06482)
*Minting Pan, Yitao Zheng, Jiajian Li, Yunbo Wang, Xiaokang Yang*

Main category: cs.LG

TL;DR: VeoRL enhances offline RL by using video data to build a world model, improving performance in visuomotor tasks.


<details>
  <summary>Details</summary>
Motivation: Offline RL struggles with suboptimal behavior and inaccurate value estimation due to lack of interaction. VeoRL addresses this by leveraging video data.

Method: VeoRL constructs an interactive world model from unlabeled video data, transferring commonsense knowledge to the RL agent.

Result: Achieves over 100% performance gains in robotic manipulation, autonomous driving, and video games.

Conclusion: VeoRL effectively bridges the gap in offline RL by utilizing video data for improved policy learning.

Abstract: Offline reinforcement learning (RL) enables policy optimization in static
datasets, avoiding the risks and costs of real-world exploration. However, it
struggles with suboptimal behavior learning and inaccurate value estimation due
to the lack of environmental interaction. In this paper, we present
Video-Enhanced Offline RL (VeoRL), a model-based approach that constructs an
interactive world model from diverse, unlabeled video data readily available
online. Leveraging model-based behavior guidance, VeoRL transfers commonsense
knowledge of control policy and physical dynamics from natural videos to the RL
agent within the target domain. Our method achieves substantial performance
gains (exceeding 100% in some cases) across visuomotor control tasks in robotic
manipulation, autonomous driving, and open-world video games.

</details>


### [458] [FedADP: Unified Model Aggregation for Federated Learning with Heterogeneous Model Architectures](https://arxiv.org/pdf/2505.06497)
*Jiacheng Wang, Hongtao Lv, Lei Liu*

Main category: cs.LG

TL;DR: FedADP is a federated learning framework that dynamically adjusts model architectures to address heterogeneity, improving efficiency and accuracy by up to 23.30% over existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional FL struggles with efficiency and accuracy in heterogeneous environments due to diverse client architectures and resources.

Method: FedADP dynamically adjusts model architectures during aggregation to accommodate client heterogeneity.

Result: FedADP outperforms methods like FlexiFed, achieving up to 23.30% higher accuracy.

Conclusion: FedADP enhances model adaptability and training efficiency in heterogeneous settings.

Abstract: Traditional Federated Learning (FL) faces significant challenges in terms of
efficiency and accuracy, particularly in heterogeneous environments where
clients employ diverse model architectures and have varying computational
resources. Such heterogeneity complicates the aggregation process, leading to
performance bottlenecks and reduced model generalizability. To address these
issues, we propose FedADP, a federated learning framework designed to adapt to
client heterogeneity by dynamically adjusting model architectures during
aggregation. FedADP enables effective collaboration among clients with
differing capabilities, maximizing resource utilization and ensuring model
quality. Our experimental results demonstrate that FedADP significantly
outperforms existing methods, such as FlexiFed, achieving an accuracy
improvement of up to 23.30%, thereby enhancing model adaptability and training
efficiency in heterogeneous real-world settings.

</details>


### [459] [PRUNE: A Patching Based Repair Framework for Certiffable Unlearning of Neural Networks](https://arxiv.org/pdf/2505.06520)
*Xuran Li, Jingyi Wang, Xiaohan Yuan, Peixin Zhang, Zhan Qin, Zhibo Wang, Kui Ren*

Main category: cs.LG

TL;DR: A novel neural network unlearning method uses lightweight patches to selectively forget data, offering efficiency and verifiability.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for data removal (right to be forgotten) without costly retraining or verification challenges.

Method: Strategic application of minimal patches to the network for targeted forgetting, with iterative selection for larger datasets.

Result: Effective unlearning with preserved model performance, competitive efficiency, and low memory use.

Conclusion: The approach provides a practical, certifiable solution for data removal in neural networks.

Abstract: It is often desirable to remove (a.k.a. unlearn) a speciffc part of the
training data from a trained neural network model. A typical application
scenario is to protect the data holder's right to be forgotten, which has been
promoted by many recent regulation rules. Existing unlearning methods involve
training alternative models with remaining data, which may be costly and
challenging to verify from the data holder or a thirdparty auditor's
perspective. In this work, we provide a new angle and propose a novel
unlearning approach by imposing carefully crafted "patch" on the original
neural network to achieve targeted "forgetting" of the requested data to
delete. Speciffcally, inspired by the research line of neural network repair,
we propose to strategically seek a lightweight minimum "patch" for unlearning a
given data point with certiffable guarantee. Furthermore, to unlearn a
considerable amount of data points (or an entire class), we propose to
iteratively select a small subset of representative data points to unlearn,
which achieves the effect of unlearning the whole set. Extensive experiments on
multiple categorical datasets demonstrates our approach's effectiveness,
achieving measurable unlearning while preserving the model's performance and
being competitive in efffciency and memory consumption compared to various
baseline methods.

</details>


### [460] [GBDTSVM: Combined Support Vector Machine and Gradient Boosting Decision Tree Framework for efficient snoRNA-disease association prediction](https://arxiv.org/pdf/2505.06534)
*Ummay Maria Muna, Fahim Hafiz, Shanta Biswas, Riasat Azim*

Main category: cs.LG

TL;DR: The paper introduces 'GBDTSVM', a machine learning model combining GBDT and SVM to predict snoRNA-disease associations, outperforming existing methods with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Precise identification of snoRNA-disease associations is crucial for disease understanding and treatment, but traditional methods are costly and slow. Machine learning offers a viable alternative.

Method: The 'GBDTSVM' model integrates GBDT for feature extraction and SVM for classification, enhanced by Gaussian kernel profile similarity for snoRNAs and diseases.

Result: GBDTSVM achieved AUROC of 0.96 and AUPRC of 0.95, outperforming state-of-the-art methods on multiple datasets (MDRF, LSGT, PsnoD). A case study validated top predictions.

Conclusion: GBDTSVM is a robust tool for snoRNA-disease research, offering high accuracy and efficiency, with potential to advance disease understanding and treatment strategies.

Abstract: Small nucleolar RNAs (snoRNAs) are increasingly recognized for their critical
role in the pathogenesis and characterization of various human diseases.
Consequently, the precise identification of snoRNA-disease associations (SDAs)
is essential for the progression of diseases and the advancement of treatment
strategies. However, conventional biological experimental approaches are
costly, time-consuming, and resource-intensive; therefore, machine
learning-based computational methods offer a promising solution to mitigate
these limitations. This paper proposes a model called 'GBDTSVM', representing a
novel and efficient machine learning approach for predicting snoRNA-disease
associations by leveraging a Gradient Boosting Decision Tree (GBDT) and Support
Vector Machine (SVM). 'GBDTSVM' effectively extracts integrated snoRNA-disease
feature representations utilizing GBDT and SVM is subsequently utilized to
classify and identify potential associations. Furthermore, the method enhances
the accuracy of these predictions by incorporating Gaussian kernel profile
similarity for both snoRNAs and diseases. Experimental evaluation of the
GBDTSVM model demonstrated superior performance compared to state-of-the-art
methods in the field, achieving an area under the receiver operating
characteristic (AUROC) of 0.96 and an area under the precision-recall curve
(AUPRC) of 0.95 on MDRF dataset. Moreover, our model shows superior performance
on two more datasets named LSGT and PsnoD. Additionally, a case study on the
predicted snoRNA-disease associations verified the top 10 predicted snoRNAs
across nine prevalent diseases, further validating the efficacy of the GBDTSVM
approach. These results underscore the model's potential as a robust tool for
advancing snoRNA-related disease research. Source codes and datasets our
proposed framework can be obtained from: https://github.com/mariamuna04/gbdtsvm

</details>


### [461] [dcFCI: Robust Causal Discovery Under Latent Confounding, Unfaithfulness, and Mixed Data](https://arxiv.org/pdf/2505.06542)
*Adèle H. Ribeiro, Dominik Heider*

Main category: cs.LG

TL;DR: The paper introduces a nonparametric score to assess PAG compatibility with data and proposes dcFCI, a hybrid algorithm addressing latent confounding, empirical unfaithfulness, and mixed data types, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing causal discovery methods like FCI rely on empirical faithfulness, which often fails due to limited sample sizes, leading to incorrect inferences.

Method: The authors develop a nonparametric score for PAG compatibility and integrate it into dcFCI, a hybrid algorithm combining (Anytime)FCI-guided search to explore and validate candidate PAGs.

Result: dcFCI outperforms state-of-the-art methods, recovering true PAGs even in small or heterogeneous datasets, and provides insights into structural uncertainty.

Conclusion: dcFCI enhances causal discovery by addressing key limitations, enabling more robust causal reasoning and decision-making.

Abstract: Causal discovery is central to inferring causal relationships from
observational data. In the presence of latent confounding, algorithms such as
Fast Causal Inference (FCI) learn a Partial Ancestral Graph (PAG) representing
the true model's Markov Equivalence Class. However, their correctness
critically depends on empirical faithfulness, the assumption that observed
(in)dependencies perfectly reflect those of the underlying causal model, which
often fails in practice due to limited sample sizes. To address this, we
introduce the first nonparametric score to assess a PAG's compatibility with
observed data, even with mixed variable types. This score is both necessary and
sufficient to characterize structural uncertainty and distinguish between
distinct PAGs. We then propose data-compatible FCI (dcFCI), the first hybrid
causal discovery algorithm to jointly address latent confounding, empirical
unfaithfulness, and mixed data types. dcFCI integrates our score into an
(Anytime)FCI-guided search that systematically explores, ranks, and validates
candidate PAGs. Experiments on synthetic and real-world scenarios demonstrate
that dcFCI significantly outperforms state-of-the-art methods, often recovering
the true PAG even in small and heterogeneous datasets. Examining top-ranked
PAGs further provides valuable insights into structural uncertainty, supporting
more robust and informed causal reasoning and decision-making.

</details>


### [462] [Good Things Come in Pairs: Paired Autoencoders for Inverse Problems](https://arxiv.org/pdf/2505.06549)
*Matthias Chung, Bas Peters, Michael Solomon*

Main category: cs.LG

TL;DR: The paper discusses the paired autoencoder framework for solving inverse problems, combining data-driven and model-based methods, and demonstrates its effectiveness through numerical experiments.


<details>
  <summary>Details</summary>
Motivation: To address inverse problems in scientific computing by leveraging the strengths of both data-driven and model-based approaches.

Method: The paired autoencoder framework projects data and quantities of interest into latent spaces, creating surrogate forward and inverse mappings. It includes latent-space refinement and novel variational variants for uncertainty analysis.

Result: Numerical experiments in seismic imaging and inpainting show high-quality estimates, even with noisy data. The framework provides reconstruction metrics and enables latent-space refinement.

Conclusion: The paired autoencoder framework is effective for inverse problems, offering robust solutions and uncertainty analysis capabilities.

Abstract: In this book chapter, we discuss recent advances in data-driven approaches
for inverse problems. In particular, we focus on the \emph{paired autoencoder}
framework, which has proven to be a powerful tool for solving inverse problems
in scientific computing. The paired autoencoder framework is a novel approach
that leverages the strengths of both data-driven and model-based methods by
projecting both the data and the quantity of interest into a latent space and
mapping these latent spaces to provide surrogate forward and inverse mappings.
We illustrate the advantages of this approach through numerical experiments,
including seismic imaging and classical inpainting: nonlinear and linear
inverse problems, respectively. Although the paired autoencoder framework is
likelihood-free, it generates multiple data- and model-based reconstruction
metrics that help assess whether examples are in or out of distribution. In
addition to direct model estimates from data, the paired autoencoder enables
latent-space refinement to fit the observed data accurately. Numerical
experiments show that this procedure, combined with the latent-space initial
guess, is essential for high-quality estimates, even when data noise exceeds
the training regime. We also introduce two novel variants that combine
variational and paired autoencoder ideas, maintaining the original benefits
while enabling sampling for uncertainty analysis.

</details>


### [463] [An \tilde{O}ptimal Differentially Private Learner for Concept Classes with VC Dimension 1](https://arxiv.org/pdf/2505.06581)
*Chao Yan*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present the first nearly optimal differentially private PAC learner for
any concept class with VC dimension 1 and Littlestone dimension $d$. Our
algorithm achieves the sample complexity of
$\tilde{O}_{\varepsilon,\delta,\alpha,\delta}(\log^* d)$, nearly matching the
lower bound of $\Omega(\log^* d)$ proved by Alon et al. [STOC19]. Prior to our
work, the best known upper bound is $\tilde{O}(VC\cdot d^5)$ for general VC
classes, as shown by Ghazi et al. [STOC21].

</details>


### [464] [Geometry of Learning -- L2 Phase Transitions in Deep and Shallow Neural Networks](https://arxiv.org/pdf/2505.06597)
*Ibrahim Talha Ersoy, Karoline Wiesner*

Main category: cs.LG

TL;DR: The paper explores how L2 regularization in neural networks leads to phase transitions, linking these to curvature changes in the loss landscape, and extends findings to complex datasets and non-L2 settings.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between regularization strength, phase transitions, and the geometric properties of the loss landscape in neural networks.

Method: Integrates Ricci curvature of the loss landscape with regularizer-driven deep learning, analyzing phase transitions and curvature change points in single and multi-hidden-layer NNs, including experiments on MNIST using a Variational Autoencoder.

Result: Identifies that curvature change points correspond to phase transitions in model accuracy, applicable even beyond L2 regularization, and provides insights for optimizing model performance.

Conclusion: The framework connects geometric features of the error landscape to phase transitions, offering practical regularization strategies and new methods for probing neural network structures.

Abstract: When neural networks (NNs) are subject to L2 regularization, increasing the
regularization strength beyond a certain threshold pushes the model into an
under-parameterization regime. This transition manifests as a first-order phase
transition in single-hidden-layer NNs and a second-order phase transition in
NNs with two or more hidden layers. This paper establishes a unified framework
for such transitions by integrating the Ricci curvature of the loss landscape
with regularizer-driven deep learning. First, we show that a curvature
change-point separates the model-accuracy regimes in the onset of learning and
that it is identical to the critical point of the phase transition driven by
regularization. Second, we show that for more complex data sets additional
phase transitions exist between model accuracies, and that they are again
identical to curvature change points in the error landscape. Third, by studying
the MNIST data set using a Variational Autoencoder, we demonstrate that the
curvature change points identify phase transitions in model accuracy outside
the L2 setting. Our framework also offers practical insights for optimizing
model performance across various architectures and datasets. By linking
geometric features of the error landscape to observable phase transitions, our
work paves the way for more informed regularization strategies and potentially
new methods for probing the intrinsic structure of neural networks beyond the
L2 context.

</details>


### [465] [Minimizing Risk Through Minimizing Model-Data Interaction: A Protocol For Relying on Proxy Tasks When Designing Child Sexual Abuse Imagery Detection Models](https://arxiv.org/pdf/2505.06621)
*Thamiris Coelho, Leo S. F. Ribeiro, João Macedo, Jefersson A. dos Santos, Sandra Avila*

Main category: cs.LG

TL;DR: The paper addresses the challenge of automating CSAI detection without direct access to sensitive data, introducing 'Proxy Tasks' as a solution and demonstrating their effectiveness in a real-world application.


<details>
  <summary>Details</summary>
Motivation: The growing issue of CSAI distribution overwhelms LEAs, necessitating automated solutions that avoid direct interaction with sensitive data to prevent leaks.

Method: The authors formalize 'Proxy Tasks' for training models without CSAI data, review existing literature, and propose a protocol combining Proxy Tasks with LEA input. They apply this to Few-shot Indoor Scene Classification on CSAI.

Result: The proposed model achieves promising results on a real-world CSAI dataset without training on sensitive data.

Conclusion: Proxy Tasks offer a viable approach for automating CSAI detection while adhering to data sensitivity constraints, with potential for further refinement.

Abstract: The distribution of child sexual abuse imagery (CSAI) is an ever-growing
concern of our modern world; children who suffered from this heinous crime are
revictimized, and the growing amount of illegal imagery distributed overwhelms
law enforcement agents (LEAs) with the manual labor of categorization. To ease
this burden researchers have explored methods for automating data triage and
detection of CSAI, but the sensitive nature of the data imposes restricted
access and minimal interaction between real data and learning algorithms,
avoiding leaks at all costs. In observing how these restrictions have shaped
the literature we formalize a definition of "Proxy Tasks", i.e., the substitute
tasks used for training models for CSAI without making use of CSA data. Under
this new terminology we review current literature and present a protocol for
making conscious use of Proxy Tasks together with consistent input from LEAs to
design better automation in this field. Finally, we apply this protocol to
study -- for the first time -- the task of Few-shot Indoor Scene Classification
on CSAI, showing a final model that achieves promising results on a real-world
CSAI dataset whilst having no weights actually trained on sensitive data.

</details>


### [466] [Dyn-D$^2$P: Dynamic Differentially Private Decentralized Learning with Provable Utility Guarantee](https://arxiv.org/pdf/2505.06651)
*Zehan Zhu, Yan Huang, Xin Wang, Shouling Ji, Jinming Xu*

Main category: cs.LG

TL;DR: Dyn-D$^2$P is a dynamic differentially private decentralized learning method for time-varying networks, improving accuracy by adjusting gradient clipping and noise levels while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: Existing methods use fixed noise and clipping, degrading accuracy. Dyn-D$^2$P aims to enhance accuracy under strong privacy guarantees.

Method: Dyn-D$^2$P dynamically adjusts gradient clipping and noise levels using the GDP framework, based on gradient convergence.

Result: Experiments show Dyn-D$^2$P outperforms fixed-noise methods, especially under strong privacy. A utility bound is provided, scaling as $1/\sqrt{n}$.

Conclusion: Dyn-D$^2$P is the first to analyze utility for dynamic private decentralized non-convex optimization, improving accuracy and privacy.

Abstract: Most existing decentralized learning methods with differential privacy (DP)
guarantee rely on constant gradient clipping bounds and fixed-level DP Gaussian
noises for each node throughout the training process, leading to a significant
accuracy degradation compared to non-private counterparts. In this paper, we
propose a new Dynamic Differentially Private Decentralized learning approach
(termed Dyn-D$^2$P) tailored for general time-varying directed networks.
Leveraging the Gaussian DP (GDP) framework for privacy accounting, Dyn-D$^2$P
dynamically adjusts gradient clipping bounds and noise levels based on gradient
convergence. This proposed dynamic noise strategy enables us to enhance model
accuracy while preserving the total privacy budget. Extensive experiments on
benchmark datasets demonstrate the superiority of Dyn-D$^2$P over its
counterparts employing fixed-level noises, especially under strong privacy
guarantees. Furthermore, we provide a provable utility bound for Dyn-D$^2$P
that establishes an explicit dependency on network-related parameters, with a
scaling factor of $1/\sqrt{n}$ in terms of the number of nodes $n$ up to a bias
error term induced by gradient clipping. To our knowledge, this is the first
model utility analysis for differentially private decentralized non-convex
optimization with dynamic gradient clipping bounds and noise levels.

</details>


### [467] [Improving Block-Wise LLM Quantization by 4-bit Block-Wise Optimal Float (BOF4): Analysis and Variations](https://arxiv.org/pdf/2505.06653)
*Patrick Blumenberg, Thomas Graave, Tim Fingscheidt*

Main category: cs.LG

TL;DR: The paper introduces BOF4, a 4-bit block-wise optimal float quantizer, reducing quantization errors in LLMs. It also proposes BOF4-S and OPQ for improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing quantization methods like NF4 and AF4 for LLMs incur suboptimal errors, prompting the need for optimized solutions.

Method: Proposes BOF4 for optimized block-wise quantization, BOF4-S for error reduction, and OPQ for handling outlier weights.

Result: BOF4-S and OPQ achieve top performance in 4-bit quantization, minimizing perplexity degradation.

Conclusion: The proposed methods outperform baselines, offering efficient and accurate quantization for LLMs.

Abstract: Large language models (LLMs) demand extensive memory capacity during both
fine-tuning and inference. To enable memory-efficient fine-tuning, existing
methods apply block-wise quantization techniques, such as NF4 and AF4, to the
network weights. We show that these quantization techniques incur suboptimal
quantization errors. Therefore, as a first novelty, we propose an optimization
approach for block-wise quantization. Using this method, we design a family of
quantizers named 4-bit block-wise optimal float (BOF4), which consistently
reduces the quantization error compared to both baseline methods. We provide
both a theoretical and a data-driven solution for the optimization process and
prove their practical equivalence. Secondly, we propose a modification to the
employed normalization method based on the signed absolute block maximum
(BOF4-S), enabling further reduction of the quantization error and empirically
achieving less degradation in language modeling performance. Thirdly, we
explore additional variations of block-wise quantization methods applied to
LLMs through an experimental study on the importance of accurately representing
zero and large-amplitude weights on the one hand, and optimization towards
various error metrics on the other hand. Lastly, we introduce a mixed-precision
quantization strategy dubbed outlier-preserving quantization (OPQ) to address
the distributional mismatch induced by outlier weights in block-wise
quantization. By storing outlier weights in 16-bit precision (OPQ) while
applying BOF4-S, we achieve top performance among 4-bit block-wise quantization
techniques w.r.t. perplexity.

</details>


### [468] [A Novel Framework for Significant Wave Height Prediction based on Adaptive Feature Extraction Time-Frequency Network](https://arxiv.org/pdf/2505.06688)
*Jianxin Zhang, Lianzi Jiang, Xinyu Han, Xiangrong Wang*

Main category: cs.LG

TL;DR: AFE-TFNet, a novel framework combining feature extraction and fusion, improves wave height prediction accuracy and avoids data leakage.


<details>
  <summary>Details</summary>
Motivation: Accurate wave height (Hs) prediction is crucial for wave energy use, but Hs's non-linear, non-stationary nature makes it challenging. Existing methods risk data leakage.

Method: AFE-TFNet uses an encoder-decoder framework: encoder extracts global/local frequency features (WT, FT, Inception blocks) and fuses time/frequency features (DHSEW); decoder uses LSTM.

Result: AFE-TFNet outperforms benchmarks, improves accuracy, and handles medium/long-term forecasting better. Feature extraction and DHSEW boost performance.

Conclusion: AFE-TFNet is effective for complex signal forecasting, with stable accuracy across rolling window sizes.

Abstract: Precise forecasting of significant wave height (Hs) is essential for the
development and utilization of wave energy. The challenges in predicting Hs
arise from its non-linear and non-stationary characteristics. The combination
of decomposition preprocessing and machine learning models have demonstrated
significant effectiveness in Hs prediction by extracting data features.
However, decomposing the unknown data in the test set can lead to data leakage
issues. To simultaneously achieve data feature extraction and prevent data
leakage, a novel Adaptive Feature Extraction Time-Frequency Network (AFE-TFNet)
is proposed to improve prediction accuracy and stability. It is encoder-decoder
rolling framework. The encoder consists of two stages: feature extraction and
feature fusion. In the feature extraction stage, global and local frequency
domain features are extracted by combining Wavelet Transform (WT) and Fourier
Transform (FT), and multi-scale frequency analysis is performed using Inception
blocks. In the feature fusion stage, time-domain and frequency-domain features
are integrated through dominant harmonic sequence energy weighting (DHSEW). The
decoder employed an advanced long short-term memory (LSTM) model. Hourly
measured wind speed (Ws), dominant wave period (DPD), average wave period (APD)
and Hs from three stations are used as the dataset, and the four metrics are
employed to evaluate the forecasting performance. Results show that AFE-TFNet
significantly outperforms benchmark methods in terms of prediction accuracy.
Feature extraction can significantly improve the prediction accuracy. DHSEW has
substantially increased the accuracy of medium-term to long-term forecasting.
The prediction accuracy of AFE-TFNet does not demonstrate significant
variability with changes of rolling time window size. Overall, AFE-TFNet shows
strong potential for handling complex signal forecasting.

</details>


### [469] [Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety](https://arxiv.org/pdf/2505.06843)
*Zihan Guan, Mengxuan Hu, Ronghang Zhu, Sheng Li, Anil Vullikanti*

Main category: cs.LG

TL;DR: Fine-tuning LLMs on benign datasets can increase harmfulness. A new attack, Self-Inf-N, identifies harmful outliers in benign data, severely compromising safety alignment. Existing defenses often fail.


<details>
  <summary>Details</summary>
Motivation: To expose vulnerabilities in LLM fine-tuning by showing how benign datasets can degrade safety, and to develop a more effective attack method.

Method: Proposed Self-Inf-N for outlier detection in benign datasets, fine-tuned LLMs on these outliers, and tested across seven models.

Result: Fine-tuning on 100 outliers significantly harms safety alignment; attack is highly transferable and bypasses most defenses.

Conclusion: Urgent need for stronger safeguards against such attacks, as current mitigation strategies are insufficient.

Abstract: Recent studies have uncovered a troubling vulnerability in the fine-tuning
stage of large language models (LLMs): even fine-tuning on entirely benign
datasets can lead to a significant increase in the harmfulness of LLM outputs.
Building on this finding, our red teaming study takes this threat one step
further by developing a more effective attack. Specifically, we analyze and
identify samples within benign datasets that contribute most to safety
degradation, then fine-tune LLMs exclusively on these samples. We approach this
problem from an outlier detection perspective and propose Self-Inf-N, to detect
and extract outliers for fine-tuning. Our findings reveal that fine-tuning LLMs
on 100 outlier samples selected by Self-Inf-N in the benign datasets severely
compromises LLM safety alignment. Extensive experiments across seven mainstream
LLMs demonstrate that our attack exhibits high transferability across different
architectures and remains effective in practical scenarios. Alarmingly, our
results indicate that most existing mitigation strategies fail to defend
against this attack, underscoring the urgent need for more robust alignment
safeguards. Codes are available at
https://github.com/GuanZihan/Benign-Samples-Matter.

</details>


### [470] [E2E-FANet: A Highly Generalizable Framework for Waves prediction Behind Floating Breakwaters via Exogenous-to-Endogenous Variable Attention](https://arxiv.org/pdf/2505.06690)
*Jianxin Zhang, Lianzi Jiang, Xinyu Han, Xiangrong Wang, Weinan Huang*

Main category: cs.LG

TL;DR: The paper introduces E2E-FANet, a neural network for predicting waves behind floating breakwaters, addressing limitations of existing methods in nonlinear interactions and frequency-domain modeling.


<details>
  <summary>Details</summary>
Motivation: Accurate wave prediction is crucial for optimizing coastal engineering, enhancing safety, and improving design efficiency, but existing methods fail to capture nonlinear interactions and frequency-domain relationships effectively.

Method: E2E-FANet uses a Dual-Basis Frequency Mapping module for time-frequency feature extraction, an Exogenous-to-Endogenous Cross-Attention module for interaction modeling, and a Temporal-wise Attention mechanism for dependency capture.

Result: E2E-FANet achieves accurate wave predictions and generalizes well across diverse wave and relative water density conditions.

Conclusion: E2E-FANet effectively addresses the limitations of existing methods, providing a robust solution for wave-structure interaction modeling in coastal engineering.

Abstract: Accurate prediction of waves behind floating breakwaters (FB) is crucial for
optimizing coastal engineering structures, enhancing safety, and improving
design efficiency. Existing methods demonstrate limitations in capturing
nonlinear interactions between waves and structures, while exhibiting
insufficient capability in modeling the complex frequency-domain relationships
among elevations of different wave gauges. To address these challenges, this
study introduces the Exogenous-to-Endogenous Frequency-Aware Network
(E2E-FANet), a novel end-to-end neural network designed to model relationships
between waves and structures. The E2E-FANetarchitecture incorporates a
Dual-Basis Frequency Mapping (DBFM) module that leverages orthogonal cosine and
sine bases to extract wave features from the frequency domain while preserving
temporal information. Additionally, we introduce the Exogenous-to-Endogenous
Cross-Attention (E2ECA) module, which employs cross attention to model the
interactions between endogenous and exogenous variables. We incorporate a
Temporal-wise Attention (TA) mechanism that adaptively captures complex
dependencies in endogenous variables. These integrated modules function
synergistically, enabling E2E-FANet to achieve both comprehensive feature
perception in the time-frequency domain and precise modeling of wave-structure
interactions. To comprehensively evaluate the performance of E2E-FANet, we
constructed a multi-level validation framework comprising three distinct
testing scenarios: internal validation under identical wave conditions,
generalization testing across different wave conditions, and adaptability
testing with varying relative water density (RW) conditions. These
comprehensive tests demonstrate that E2E-FANet provides accurate waves behind
FB predictions while successfully generalizing diverse wave conditions.

</details>


### [471] [Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws](https://arxiv.org/pdf/2505.06699)
*Xiyuan Wei, Ming Lin, Fanjiang Ye, Fengguang Song, Liangliang Cao, My T. That, Tianbao Yang*

Main category: cs.LG

TL;DR: The paper introduces a theory-driven framework for model steering, called DRRho risk minimization, rooted in DRO, and applies it to CLIP, showing improved performance over heuristic methods.


<details>
  <summary>Details</summary>
Motivation: To formalize and enhance the understanding of model steering, a learning paradigm where a trained model guides the training of another, addressing sub-optimal performance in ad-hoc methods.

Method: Proposes DRRho risk minimization, a DRO-based framework, and applies it to CLIP with a reference model (DRRho-CLIP), supported by theoretical generalization analysis.

Result: Theoretical insights confirm improved generalization and data efficiency. Experiments show superior scaling and performance over heuristic approaches.

Conclusion: The work provides the first theoretical foundation for model steering, enhancing its practice and demonstrating effectiveness in CLIP.

Abstract: This paper formalizes an emerging learning paradigm that uses a trained model
as a reference to guide and enhance the training of a target model through
strategic data selection or weighting, named $\textbf{model steering}$. While
ad-hoc methods have been used in various contexts, including the training of
large foundation models, its underlying principles remain insufficiently
understood, leading to sub-optimal performance. In this work, we propose a
theory-driven framework for model steering called $\textbf{DRRho risk
minimization}$, which is rooted in Distributionally Robust Optimization (DRO).
Through a generalization analysis, we provide theoretical insights into why
this approach improves generalization and data efficiency compared to training
without a reference model. To the best of our knowledge, this is the first time
such theoretical insights are provided for the new learning paradigm, which
significantly enhance our understanding and practice of model steering.
Building on these insights and the connection between contrastive learning and
DRO, we introduce a novel method for Contrastive Language-Image Pretraining
(CLIP) with a reference model, termed DRRho-CLIP. Extensive experiments
validate the theoretical insights, reveal a superior scaling law compared to
CLIP without a reference model, and demonstrate its strength over existing
heuristic approaches.

</details>


### [472] [Beyond $\tilde{O}(\sqrt{T})$ Constraint Violation for Online Convex Optimization with Adversarial Constraints](https://arxiv.org/pdf/2505.06709)
*Abhishek Sinha, Rahul Vaze*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We revisit the Online Convex Optimization problem with adversarial
constraints (COCO) where, in each round, a learner is presented with a convex
cost function and a convex constraint function, both of which may be chosen
adversarially. The learner selects actions from a convex decision set in an
online fashion, with the goal of minimizing both regret and the cumulative
constraint violation (CCV) over a horizon of $T$ rounds. The best-known policy
for this problem achieves $O(\sqrt{T})$ regret and $\tilde{O}(\sqrt{T})$ CCV.
In this paper, we present a surprising improvement that achieves a
significantly smaller CCV by trading it off with regret. Specifically, for any
bounded convex cost and constraint functions, we propose an online policy that
achieves $\tilde{O}(\sqrt{dT}+ T^\beta)$ regret and $\tilde{O}(dT^{1-\beta})$
CCV, where $d$ is the dimension of the decision set and $\beta \in [0,1]$ is a
tunable parameter. We achieve this result by first considering the special case
of $\textsf{Constrained Expert}$ problem where the decision set is a
probability simplex and the cost and constraint functions are linear.
Leveraging a new adaptive small-loss regret bound, we propose an efficient
policy for the $\textsf{Constrained Expert}$ problem, that attains
$O(\sqrt{T\ln N}+T^{\beta})$ regret and $\tilde{O}(T^{1-\beta} \ln N)$ CCV,
where $N$ is the number of experts. The original problem is then reduced to the
$\textsf{Constrained Expert}$ problem via a covering argument. Finally, with an
additional smoothness assumption, we propose an efficient gradient-based policy
attaining $O(T^{\max(\frac{1}{2},\beta)})$ regret and $\tilde{O}(T^{1-\beta})$
CCV.

</details>


### [473] [Towards the Three-Phase Dynamics of Generalization Power of a DNN](https://arxiv.org/pdf/2505.06993)
*Yuxuan He, Junpeng Zhang, Hongyuan Zhang, Quanshi Zhang*

Main category: cs.LG

TL;DR: The paper introduces a method to analyze DNN generalization by disentangling generalizable and non-generalizable interactions during training, revealing a three-phase dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify the generalization power of DNNs by analyzing their interaction patterns.

Method: Proposes an efficient method to quantify generalization power of interactions, building on explainable AI theory.

Result: Discovers a three-phase training dynamics: early phase removes noise, later phases capture complex but less generalizable interactions.

Conclusion: Non-generalizable interactions cause the gap between training and testing losses, verified experimentally.

Abstract: This paper proposes a new perspective for analyzing the generalization power
of deep neural networks (DNNs), i.e., directly disentangling and analyzing the
dynamics of generalizable and non-generalizable interaction encoded by a DNN
through the training process. Specifically, this work builds upon the recent
theoretical achievement in explainble AI, which proves that the detailed
inference logic of DNNs can be can be strictly rewritten as a small number of
AND-OR interaction patterns. Based on this, we propose an efficient method to
quantify the generalization power of each interaction, and we discover a
distinct three-phase dynamics of the generalization power of interactions
during training. In particular, the early phase of training typically removes
noisy and non-generalizable interactions and learns simple and generalizable
ones. The second and the third phases tend to capture increasingly complex
interactions that are harder to generalize. Experimental results verify that
the learning of non-generalizable interactions is the the direct cause for the
gap between the training and testing losses.

</details>


### [474] [Activity and Subject Detection for UCI HAR Dataset with & without missing Sensor Data](https://arxiv.org/pdf/2505.06730)
*Debashish Saha, Piyush Malik, Adrika Saha*

Main category: cs.LG

TL;DR: A lightweight LSTM model is proposed for HAR, addressing activity and subject classification, and handling missing sensor data with KNN imputation.


<details>
  <summary>Details</summary>
Motivation: To fill gaps in HAR by recognizing individuals and addressing missing sensor data for personalized applications.

Method: Uses an LSTM model for activity and subject classification, tests KNN imputation with/without PCA for missing data.

Result: Achieved 93.89% activity and 80.19% subject recognition accuracy; KNN imputation without PCA performed best.

Conclusion: The framework advances HAR by reliably handling missing data and setting new benchmarks for classification.

Abstract: Current studies in Human Activity Recognition (HAR) primarily focus on the
classification of activities through sensor data, while there is not much
emphasis placed on recognizing the individuals performing these activities.
This type of classification is very important for developing personalized and
context-sensitive applications. Additionally, the issue of missing sensor data,
which often occurs in practical situations due to hardware malfunctions, has
not been explored yet. This paper seeks to fill these voids by introducing a
lightweight LSTM-based model that can be used to classify both activities and
subjects. The proposed model was used to classify the HAR dataset by UCI [1],
achieving an accuracy of 93.89% in activity recognition (across six
activities), nearing the 96.67% benchmark, and an accuracy of 80.19% in subject
recognition (involving 30 subjects), thereby establishing a new baseline for
this area of research. We then simulate the absence of sensor data to mirror
real-world scenarios and incorporate imputation techniques, both with and
without Principal Component Analysis (PCA), to restore incomplete datasets. We
found that K-Nearest Neighbors (KNN) imputation performs the best for filling
the missing sensor data without PCA because the use of PCA resulted in slightly
lower accuracy. These results demonstrate how well the framework handles
missing sensor data, which is a major step forward in using the Human Activity
Recognition dataset for reliable classification tasks.

</details>


### [475] [Deeply Explainable Artificial Neural Network](https://arxiv.org/pdf/2505.06731)
*David Zucker*

Main category: cs.LG

TL;DR: DxANN is a new deep learning architecture that embeds explainability directly into training, offering transparent decision-making without post hoc methods.


<details>
  <summary>Details</summary>
Motivation: The black-box nature of deep learning models limits their use in critical fields like medical imaging, where trust and accountability are essential.

Method: DxANN integrates explainability ante hoc using a flow-based framework, producing per-sample, per-feature explanations during the forward pass.

Result: DxANN enables accurate predictions and transparent decision-making, especially for image-based tasks, and is adaptable to other data types.

Conclusion: DxANN advances intrinsically interpretable deep learning, providing a practical solution for trust-critical applications.

Abstract: While deep learning models have demonstrated remarkable success in numerous
domains, their black-box nature remains a significant limitation, especially in
critical fields such as medical image analysis and inference. Existing
explainability methods, such as SHAP, LIME, and Grad-CAM, are typically applied
post hoc, adding computational overhead and sometimes producing inconsistent or
ambiguous results. In this paper, we present the Deeply Explainable Artificial
Neural Network (DxANN), a novel deep learning architecture that embeds
explainability ante hoc, directly into the training process. Unlike
conventional models that require external interpretation methods, DxANN is
designed to produce per-sample, per-feature explanations as part of the forward
pass. Built on a flow-based framework, it enables both accurate predictions and
transparent decision-making, and is particularly well-suited for image-based
tasks. While our focus is on medical imaging, the DxANN architecture is readily
adaptable to other data modalities, including tabular and sequential data.
DxANN marks a step forward toward intrinsically interpretable deep learning,
offering a practical solution for applications where trust and accountability
are essential.

</details>


### [476] [LineFlow: A Framework to Learn Active Control of Production Lines](https://arxiv.org/pdf/2505.06744)
*Kai Müller, Martin Wenzel, Tobias Windisch*

Main category: cs.LG

TL;DR: LineFlow is an open-source Python framework for simulating production lines and training RL agents, showing promise but facing challenges in complex scenarios.


<details>
  <summary>Details</summary>
Motivation: Designing control systems for production lines is challenging, and while RL shows potential, a standardized framework is lacking.

Method: Introduces LineFlow, an extensible framework for simulating production lines and training RL agents, with optimal solutions provided for core subproblems.

Result: RL policies approach optimal performance in simple scenarios but struggle with industrial-scale complexity, indicating areas for further research.

Conclusion: LineFlow demonstrates RL's potential for production line control but highlights challenges in complex settings, calling for more research.

Abstract: Many production lines require active control mechanisms, such as adaptive
routing, worker reallocation, and rescheduling, to maintain optimal
performance. However, designing these control systems is challenging for
various reasons, and while reinforcement learning (RL) has shown promise in
addressing these challenges, a standardized and general framework is still
lacking. In this work, we introduce LineFlow, an extensible, open-source Python
framework for simulating production lines of arbitrary complexity and training
RL agents to control them. To demonstrate the capabilities and to validate the
underlying theoretical assumptions of LineFlow, we formulate core subproblems
of active line control in ways that facilitate mathematical analysis. For each
problem, we provide optimal solutions for comparison. We benchmark
state-of-the-art RL algorithms and show that the learned policies approach
optimal performance in well-understood scenarios. However, for more complex,
industrial-scale production lines, RL still faces significant challenges,
highlighting the need for further research in areas such as reward shaping,
curriculum learning, and hierarchical control.

</details>


### [477] [Boltzmann Classifier: A Thermodynamic-Inspired Approach to Supervised Learning](https://arxiv.org/pdf/2505.06753)
*Muhamed Amin, Bernard R. Brooks*

Main category: cs.LG

TL;DR: The paper introduces the Boltzmann Classifier, a novel algorithm inspired by thermodynamics, offering probabilistic class estimates without iterative optimization.


<details>
  <summary>Details</summary>
Motivation: To leverage thermodynamic principles for interpretable, energy-based classification, avoiding complex optimization.

Method: Uses an energy function from feature deviations to compute class probabilities, normalized like the Boltzmann distribution, with tunable KT variable.

Result: Achieves high accuracy, competitive with logistic regression and k-nearest neighbors, while being computationally efficient.

Conclusion: Demonstrates physics-inspired machine learning for interpretable, energy-based systems without iterative optimization.

Abstract: We propose a novel classification algorithm, the Boltzmann Classifier,
inspired by the thermodynamic principles underlying the Boltzmann distribution.
Our method computes a probabilistic estimate for each class based on an energy
function derived from feature-wise deviations between input samples and
class-specific centroids. The resulting probabilities are proportional to the
exponential negative energies, normalized across classes, analogous to the
Boltzmann distribution used in statistical mechanics. In addition, the KT
variable can be used to allow the high energy states to be more accessible,
which allows the tuning of their probabilities as needed. We evaluate the model
performance on several datasets from different applications. The model achieves
a high accuracy, which indicates that the Boltzmann Classifier is competitive
with standard models like logistic regression and k-nearest neighbors while
offering a thermodynamically motivated probabilistic interpretation. our
classifier does not require iterative optimization or backpropagation and is
thus computationally efficient and easy to integrate into existing workflows.
This work demonstrates how ideas from physics can inform new directions in
machine learning, providing a foundation for interpretable, energy-based
decision-making systems.

</details>


### [478] [Privacy-aware Berrut Approximated Coded Computing applied to general distributed learning](https://arxiv.org/pdf/2505.06759)
*Xavier Martínez-Luaña, Manuel Fernández-Veiga, Rebeca P. Díaz-Redondo, Ana Fernández-Vilas*

Main category: cs.LG

TL;DR: PBACC enhances privacy in federated learning by generalizing coded computing, supporting various models with minimal impact on learning quality and bounded privacy leakage.


<details>
  <summary>Details</summary>
Motivation: Existing coded computing methods for privacy in federated learning are limited to exact computations and specific functions. PBACC aims to provide a more flexible and robust solution.

Method: PBACC algorithms are adapted for centralized aggregation, secure distributed training, and decentralized training, expanding its applicability.

Result: PBACC minimally affects learning quality (tested on CNNs, VAEs, Cox regression) and bounds privacy leakage to less than a fraction of one bit per participant.

Conclusion: PBACC is a versatile and efficient privacy solution for federated learning, with computational costs scaling with data decentralization.

Abstract: Coded computing is one of the techniques that can be used for privacy
protection in Federated Learning. However, most of the constructions used for
coded computing work only under the assumption that the computations involved
are exact, generally restricted to special classes of functions, and require
quantized inputs. This paper considers the use of Private Berrut Approximate
Coded Computing (PBACC) as a general solution to add strong but non-perfect
privacy to federated learning. We derive new adapted PBACC algorithms for
centralized aggregation, secure distributed training with centralized data, and
secure decentralized training with decentralized data, thus enlarging
significantly the applications of the method and the existing privacy
protection tools available for these paradigms. Particularly, PBACC can be used
robustly to attain privacy guarantees in decentralized federated learning for a
variety of models. Our numerical results show that the achievable quality of
different learning models (convolutional neural networks, variational
autoencoders, and Cox regression) is minimally altered by using these new
computing schemes, and that the privacy leakage can be bounded strictly to less
than a fraction of one bit per participant. Additionally, the computational
cost of the encoding and decoding processes depends only of the degree of
decentralization of the data.

</details>


### [479] [Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models](https://arxiv.org/pdf/2505.07558)
*Rei Higuchi, Taiji Suzuki*

Main category: cs.LG

TL;DR: DDRO is a novel alignment method for LLMs that avoids explicit preference modeling, ensuring statistical consistency and superior performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods assume specific preference models, leading to statistical inconsistency in aligning LLMs with human preferences.

Method: DDRO directly estimates the density ratio between preferred and unpreferred output distributions.

Result: DDRO is theoretically proven statistically consistent and outperforms existing methods on benchmarks.

Conclusion: DDRO enables reliable, data-driven alignment of LLMs with human preferences.

Abstract: Aligning large language models (LLMs) with human preferences is crucial for
safe deployment, yet existing methods assume specific preference models like
Bradley-Terry model. This assumption leads to statistical inconsistency, where
more data doesn't guarantee convergence to true human preferences. To address
this critical gap, we introduce a novel alignment method Direct Density Ratio
Optimization (DDRO). DDRO directly estimates the density ratio between
preferred and unpreferred output distributions, circumventing the need for
explicit human preference modeling. We theoretically prove that DDRO is
statistically consistent, ensuring convergence to the true preferred
distribution as the data size grows, regardless of the underlying preference
structure. Experiments demonstrate that DDRO achieves superior performance
compared to existing methods on many major benchmarks. DDRO unlocks the
potential for truly data-driven alignment, paving the way for more reliable and
human-aligned LLMs.

</details>


### [480] [Investigating Robotaxi Crash Severity Using Geographical Random Forest](https://arxiv.org/pdf/2505.06762)
*Junfeng Jiao, Seung Gyu Baik, Seung Jun Choi, Yiming Xu*

Main category: cs.LG

TL;DR: The paper uses a Geographical Random Forest model to analyze AV crash severity in San Francisco, finding localized machine learning outperforms regular methods, land use is the most critical built environment factor, and city centers have lower crash severity than residential areas.


<details>
  <summary>Details</summary>
Motivation: To address spatial heterogeneity and autocorrelation in AV crash severity by examining land use patterns and human behavior, aiding commercial AV operations.

Method: Employed a Geographical Random Forest (GRF) model with spatial localization, analyzing urban built environment measures like land use, intersections, and POIs.

Result: Localized machine learning performed better; land use was the most important factor; city centers had lower crash severity than residential areas.

Conclusion: Recommends geographic-specific safety measures for AV training, especially in residential neighborhoods, to mitigate crash severity.

Abstract: This paper quantitatively investigates the crash severity of Autonomous
Vehicles (AVs) with spatially localized machine learning and macroscopic
measures of the urban built environment. We address spatial heterogeneity and
spatial autocorrelation, while focusing on land use patterns and human
behavior. Our Geographical Random Forest (GRF) model, accompanied with a crash
severity risk map of San Francisco, presents three findings that are useful for
commercial operations of AVs and robotaxis. First, spatially localized machine
learning performed better than regular machine learning, when predicting AV
crash severity. Bias-variance tradeoff was evident as we adjust the
localization weight hyperparameter. Second, land use was the most important
built environment measure, compared to intersections, building footprints,
public transit stops, and Points Of Interests (POIs). Third, it was predicted
that city center areas with greater diversity and commercial activities were
more likely to result in low-severity AV crashes, than residential
neighborhoods. Residential land use may be associated with higher severity due
to human behavior and less restrictive environment. This paper recommends to
explicitly consider geographic locations, and to design safety measures
specific to residential neighborhoods, when robotaxi operators train their AV
systems.

</details>


### [481] [Decoding Futures Price Dynamics: A Regularized Sparse Autoencoder for Interpretable Multi-Horizon Forecasting and Factor Discovery](https://arxiv.org/pdf/2505.06795)
*Abhijit Gupta*

Main category: cs.LG

TL;DR: The paper introduces a Regularized Sparse Autoencoder (RSAE) for multi-horizon commodity price forecasting, combining accuracy with interpretability by learning sparse latent market drivers.


<details>
  <summary>Details</summary>
Motivation: Commodity price volatility and the lack of transparency in current forecasting models necessitate a method that provides accurate predictions and interpretable insights.

Method: The RSAE uses L1 regularization on latent vectors to enforce sparsity, enabling multi-horizon forecasting (e.g., 1-day, 1-week, 1-month) and discovery of interpretable market drivers.

Result: The RSAE achieves competitive forecasting accuracy on Copper and Crude Oil data while offering interpretable latent representations of market dynamics.

Conclusion: The RSAE outperforms traditional black-box models by providing both accurate forecasts and transparent insights into commodity price drivers.

Abstract: Commodity price volatility creates economic challenges, necessitating
accurate multi-horizon forecasting. Predicting prices for commodities like
copper and crude oil is complicated by diverse interacting factors
(macroeconomic, supply/demand, geopolitical, etc.). Current models often lack
transparency, limiting strategic use. This paper presents a Regularized Sparse
Autoencoder (RSAE), a deep learning framework for simultaneous multi-horizon
commodity price prediction and discovery of interpretable latent market
drivers. The RSAE forecasts prices at multiple horizons (e.g., 1-day, 1-week,
1-month) using multivariate time series. Crucially, L1 regularization
($\|\mathbf{z}\|_1$) on its latent vector $\mathbf{z}$ enforces sparsity,
promoting parsimonious explanations of market dynamics through learned factors
representing underlying drivers (e.g., demand, supply shocks). Drawing from
energy-based models and sparse coding, the RSAE optimizes predictive accuracy
while learning sparse representations. Evaluated on historical Copper and Crude
Oil data with numerous indicators, our findings indicate the RSAE offers
competitive multi-horizon forecasting accuracy and data-driven insights into
price dynamics via its interpretable latent space, a key advantage over
traditional black-box approaches.

</details>


### [482] [Topology Guidance: Controlling the Outputs of Generative Models via Vector Field Topology](https://arxiv.org/pdf/2505.06804)
*Xiaohan Wang, Matthew Berger*

Main category: cs.LG

TL;DR: The paper introduces topology guidance for generative models to control field synthesis, ensuring specified topological features are accurately represented in outputs.


<details>
  <summary>Details</summary>
Motivation: Generative models lack controllability for visual analysis, making it hard to predict or specify desired features like topology in synthesized fields.

Method: Proposes coupling a coordinate-based neural network with a diffusion model, using topologically-relevant signals to guide the denoising process.

Result: Demonstrates accurate adherence to specified critical points in 2D vector fields, aiding ensemble comparison by exploring topological commonalities.

Conclusion: Topology guidance enhances generative models' controllability, enabling precise synthesis of fields with user-defined topological features.

Abstract: For domains that involve numerical simulation, it can be computationally
expensive to run an ensemble of simulations spanning a parameter space of
interest to a user. To this end, an attractive surrogate for simulation is the
generative modeling of fields produced by an ensemble, allowing one to
synthesize fields in a computationally cheap, yet accurate, manner. However,
for the purposes of visual analysis, a limitation of generative models is their
lack of control, as it is unclear what one should expect when sampling a field
from a model. In this paper we study how to make generative models of fields
more controllable, so that users can specify features of interest, in
particular topological features, that they wish to see in the output. We
propose topology guidance, a method for guiding the sampling process of a
generative model, specifically a diffusion model, such that a topological
description specified as input is satisfied in the generated output. Central to
our method, we couple a coordinate-based neural network used to represent
fields, with a diffusion model used for generation. We show how to use
topologically-relevant signals provided by the coordinate-based network to help
guide the denoising process of a diffusion model. This enables us to faithfully
represent a user's specified topology, while ensuring that the output field
remains within the generative data distribution. Specifically, we study 2D
vector field topology, evaluating our method over an ensemble of fluid flows,
where we show that generated vector fields faithfully adhere to the location,
and type, of critical points over the spatial domain. We further show the
benefits of our method in aiding the comparison of ensembles, allowing one to
explore commonalities and differences in distributions along prescribed
topological features.

</details>


### [483] [Deep Learning for On-Street Parking Violation Prediction](https://arxiv.org/pdf/2505.06818)
*Thien Nhan Vo*

Main category: cs.LG

TL;DR: The paper proposes a DL-based model to predict parking violations in on-street parking systems, addressing issues of illegal parking and inaccurate parking space availability.


<details>
  <summary>Details</summary>
Motivation: Illegal parking and lack of parking spaces degrade urban quality of life. Existing systems are flawed due to illegal parking and high sensor costs.

Method: A Deep Learning model is used for fine-grained parking violation prediction, enhanced by data augmentation and smoothing techniques for noisy/missing data.

Result: Experiments on real data from Thessaloniki show the system provides accurate parking violation predictions.

Conclusion: The DL-based approach effectively improves parking system accuracy, offering a cost-effective solution to illegal parking challenges.

Abstract: Illegal parking along with the lack of available parking spaces are among the
biggest issues faced in many large cities. These issues can have a significant
impact on the quality of life of citizens. On-street parking systems have been
designed to this end aiming at ensuring that parking spaces will be available
for the local population, while also providing easy access to parking for
people visiting the city center. However, these systems are often affected by
illegal parking, providing incorrect information regarding the availability of
parking spaces. Even though this can be mitigated using sensors for detecting
the presence of cars in various parking sectors, the cost of these
implementations is usually prohibiting large. In this paper, we investigate an
indirect way of predicting parking violations at a fine-grained level,
equipping such parking systems with a valuable tool for providing more accurate
information to citizens. To this end, we employed a Deep Learning (DL)-based
model to predict fine-grained parking violation rates for on-street parking
systems. Moreover, we developed a data augmentation and smoothing technique for
further improving the accuracy of DL models under the presence of missing and
noisy data. We demonstrate, using experiments on real data collected in
Thessaloniki, Greece, that the developed system can indeed provide accurate
parking violation predictions.

</details>


### [484] [Streaming Sliced Optimal Transport](https://arxiv.org/pdf/2505.06835)
*Khai Nguyen*

Main category: cs.LG

TL;DR: Stream-SW is a method for computing sliced Wasserstein distance from sample streams, offering low memory complexity and accurate approximations.


<details>
  <summary>Details</summary>
Motivation: To enhance computational scalability of sliced Wasserstein distance for streaming data.

Method: Leverages quantile approximation for streaming one-dimensional Wasserstein distance, extending it to sliced Wasserstein via projections.

Result: Stream-SW outperforms random subsampling in accuracy and memory efficiency for Gaussian distributions and mixtures.

Conclusion: Stream-SW is effective for applications like point cloud classification and change point detection, with theoretical guarantees.

Abstract: Sliced optimal transport (SOT) or sliced Wasserstein (SW) distance is widely
recognized for its statistical and computational scalability. In this work, we
further enhance the computational scalability by proposing the first method for
computing SW from sample streams, called \emph{streaming sliced Wasserstein}
(Stream-SW). To define Stream-SW, we first introduce the streaming computation
of the one-dimensional Wasserstein distance. Since the one-dimensional
Wasserstein (1DW) distance has a closed-form expression, given by the absolute
difference between the quantile functions of the compared distributions, we
leverage quantile approximation techniques for sample streams to define the
streaming 1DW distance. By applying streaming 1DW to all projections, we obtain
Stream-SW. The key advantage of Stream-SW is its low memory complexity while
providing theoretical guarantees on the approximation error. We demonstrate
that Stream-SW achieves a more accurate approximation of SW than random
subsampling, with lower memory consumption, in comparing Gaussian distributions
and mixtures of Gaussians from streaming samples. Additionally, we conduct
experiments on point cloud classification, point cloud gradient flows, and
streaming change point detection to further highlight the favorable performance
of Stream-SW.

</details>


### [485] [The power of fine-grained experts: Granularity boosts expressivity in Mixture of Experts](https://arxiv.org/pdf/2505.06839)
*Enric Boix-Adsera, Philippe Rigollet*

Main category: cs.LG

TL;DR: MoE layers' granularity (number of active experts) impacts model expressivity, with higher granularity (e.g., 8 experts) showing exponential benefits over lower (e.g., 1 expert).


<details>
  <summary>Details</summary>
Motivation: To understand how the number of active experts in MoE layers affects model performance and expressivity.

Method: Theoretical analysis and experiments comparing architectures with varying expert granularity (e.g., 8 vs. 1 expert per layer).

Result: Higher granularity (more active experts) leads to exponentially greater network expressivity, supported by experimental evidence.

Conclusion: Models benefit from higher granularity in MoE layers, as it enhances expressivity without proportional computational cost.

Abstract: Mixture-of-Experts (MoE) layers are increasingly central to frontier model
architectures. By selectively activating parameters, they reduce computational
cost while scaling total parameter count. This paper investigates the impact of
the number of active experts, termed granularity, comparing architectures with
many (e.g., 8 per layer in DeepSeek) to those with fewer (e.g., 1 per layer in
Llama-4 models). We prove an exponential separation in network expressivity
based on this design parameter, suggesting that models benefit from higher
granularity. Experimental results corroborate our theoretical findings and
illustrate this separation.

</details>


### [486] [Predictive Digital Twins for Thermal Management Using Machine Learning and Reduced-Order Models](https://arxiv.org/pdf/2505.06849)
*Tamilselvan Subramani, Sebastian Bartscher*

Main category: cs.LG

TL;DR: A novel framework combines physics-based reduced-order models (ROMs) and machine learning for predictive digital twins of a headlamp heatsink, achieving high accuracy with Neural Networks.


<details>
  <summary>Details</summary>
Motivation: To enhance thermal management in automotive systems through scalable and interpretable digital twins.

Method: Integrates physics-based ROMs (derived via POD) with supervised machine learning (Decision Trees, k-NN, SVR, Neural Networks) to predict optimal ROM configurations.

Result: Neural Networks outperform other models with a mean absolute error (MAE) of 54.240, demonstrating high accuracy in predictions.

Conclusion: The framework advances thermal management, supporting robust design and predictive maintenance in automotive systems.

Abstract: Digital twins enable real-time simulation and prediction in engineering
systems. This paper presents a novel framework for predictive digital twins of
a headlamp heatsink, integrating physics-based reduced-order models (ROMs) from
computational fluid dynamics (CFD) with supervised machine learning. A
component-based ROM library, derived via proper orthogonal decomposition (POD),
captures thermal dynamics efficiently. Machine learning models, including
Decision Trees, k-Nearest Neighbors, Support Vector Regression (SVR), and
Neural Networks, predict optimal ROM configurations, enabling rapid digital
twin updates. The Neural Network achieves a mean absolute error (MAE) of
54.240, outperforming other models. Quantitative comparisons of predicted and
original values demonstrate high accuracy. This scalable, interpretable
framework advances thermal management in automotive systems, supporting robust
design and predictive maintenance.

</details>


### [487] [Improving Random Forests by Smoothing](https://arxiv.org/pdf/2505.06852)
*Ziyi Liu, Phuc Luong, Mario Boley, Daniel F. Schmidt*

Main category: cs.LG

TL;DR: The paper proposes combining Gaussian process regression and random forest regression to improve predictive performance by smoothing piecewise constant functions, addressing non-homogeneous smoothness and uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Gaussian processes struggle with non-homogeneous smoothness, while random forests lack smoothness and perform poorly with small data. Combining both aims to leverage their strengths.

Method: A kernel-based smoothing mechanism is applied to learned random forests or piecewise constant functions to enhance performance.

Result: The model improves predictive performance and log loss of uncertainty quantification, leveraging uncertainty over tree-splitting locations.

Conclusion: The hybrid approach outperforms standalone random forests and enhances uncertainty quantification, making it effective for non-homogeneous smoothness scenarios.

Abstract: Gaussian process regression is a popular model in the small data regime due
to its sound uncertainty quantification and the exploitation of the smoothness
of the regression function that is encountered in a wide range of practical
problems. However, Gaussian processes perform sub-optimally when the degree of
smoothness is non-homogeneous across the input domain. Random forest regression
partially addresses this issue by providing local basis functions of variable
support set sizes that are chosen in a data-driven way. However, they do so at
the expense of forgoing any degree of smoothness, which often results in poor
performance in the small data regime. Here, we aim to combine the advantages of
both models by applying a kernel-based smoothing mechanism to a learned random
forest or any other piecewise constant prediction function. As we demonstrate
empirically, the resulting model consistently improves the predictive
performance of the underlying random forests and, in almost all test cases,
also improves the log loss of the usual uncertainty quantification based on
inter-tree variance. The latter advantage can be attributed to the ability of
the smoothing model to take into account the uncertainty over the exact
tree-splitting locations.

</details>


### [488] [FreqMoE: Dynamic Frequency Enhancement for Neural PDE Solvers](https://arxiv.org/pdf/2505.06858)
*Tianyu Chen, Haoyi Zhou, Ying Li, Hao Wang, Zhenzhe Zhang, Tianchen Zhu, Shanghang Zhang, Jianxin Li*

Main category: cs.LG

TL;DR: FreqMoE improves Fourier Neural Operators (FNO) by addressing high-frequency signal sparsity and loss, using a progressive training framework for better efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: High-frequency signal sparsity and fixed-pattern truncation in FNO limit computational efficiency and performance, especially for high-resolution inputs or long-term predictions.

Method: FreqMoE first learns low-frequency weights, then uses a sparse upward-cycling strategy to extend these to high-frequency regions via a mixture of experts (MoE) in the frequency domain.

Result: FreqMoE achieves up to 16.6% accuracy improvement with 47.32x fewer parameters than dense FNO, showing stability in long-term predictions and generalization across FNO variants and grid structures.

Conclusion: FreqMoE introduces a 'Low frequency Pretraining, High frequency Fine-tuning' paradigm, significantly enhancing PDE-solving efficiency and accuracy.

Abstract: Fourier Neural Operators (FNO) have emerged as promising solutions for
efficiently solving partial differential equations (PDEs) by learning
infinite-dimensional function mappings through frequency domain
transformations. However, the sparsity of high-frequency signals limits
computational efficiency for high-dimensional inputs, and fixed-pattern
truncation often causes high-frequency signal loss, reducing performance in
scenarios such as high-resolution inputs or long-term predictions. To address
these challenges, we propose FreqMoE, an efficient and progressive training
framework that exploits the dependency of high-frequency signals on
low-frequency components. The model first learns low-frequency weights and then
applies a sparse upward-cycling strategy to construct a mixture of experts
(MoE) in the frequency domain, effectively extending the learned weights to
high-frequency regions. Experiments on both regular and irregular grid PDEs
demonstrate that FreqMoE achieves up to 16.6% accuracy improvement while using
merely 2.1% parameters (47.32x reduction) compared to dense FNO. Furthermore,
the approach demonstrates remarkable stability in long-term predictions and
generalizes seamlessly to various FNO variants and grid structures,
establishing a new ``Low frequency Pretraining, High frequency Fine-tuning''
paradigm for solving PDEs.

</details>


### [489] [Masked Subspace Clustering Methods](https://arxiv.org/pdf/2505.06863)
*Jiebo Song, Huaming Ling*

Main category: cs.LG

TL;DR: A Bilevel Clustering Optimization (BCO) framework is proposed to enhance clustering by leveraging unsupervised features and pairwise information, with three subspace clustering variants introduced.


<details>
  <summary>Details</summary>
Motivation: To improve clustering performance by better utilizing unsupervised features and pairwise information.

Method: Proposes BCO framework and three subspace clustering methods: Basic Masked Subspace Clustering (BMSC), General Masked Subspace Clustering (GMSC), and Recursive Masked Subspace Clustering (RMSC).

Result: Significant improvement over baselines on datasets like MNIST, USPS, ORL, COIL20, and COIL100.

Conclusion: The BCO framework and its subspace clustering variants effectively enhance clustering performance.

Abstract: To further utilize the unsupervised features and pairwise information, we
propose a general Bilevel Clustering Optimization (BCO) framework to improve
the performance of clustering. And then we introduce three special cases on
subspace clustering with two different types of masks. At first, we reformulate
the original subspace clustering as a Basic Masked Subspace Clustering (BMSC),
which reformulate the diagonal constraints to a hard mask. Then, we provide a
General Masked Subspace Clustering (GMSC) method to integrate different
clustering via a soft mask. Furthermore, based on BCO and GMSC, we induce a
learnable soft mask and design a Recursive Masked Subspace Clustering (RMSC)
method that can alternately update the affinity matrix and the soft mask.
Numerical experiments show that our models obtain significant improvement
compared with the baselines on several commonly used datasets, such as MNIST,
USPS, ORL, COIL20 and COIL100.

</details>


### [490] [Enhancing Time Series Forecasting via a Parallel Hybridization of ARIMA and Polynomial Classifiers](https://arxiv.org/pdf/2505.06874)
*Thanh Son Nguyen, Van Thanh Nguyen, Dang Minh Duc Nguyen*

Main category: cs.LG

TL;DR: A hybrid model combining ARIMA and polynomial classifiers is proposed for time series forecasting, outperforming individual models in accuracy with a slight trade-off in computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To leverage the strengths of both linear (ARIMA) and non-linear (polynomial classifiers) models for improved time series forecasting.

Method: Integration of ARIMA with polynomial classifiers, evaluated on diverse real-world datasets.

Result: The hybrid model consistently achieves higher prediction accuracy than standalone ARIMA or polynomial classifiers, though with slightly increased execution time.

Conclusion: The hybrid approach effectively combines linear and non-linear techniques, enhancing forecasting performance across various domains.

Abstract: Time series forecasting has attracted significant attention, leading to the
de-velopment of a wide range of approaches, from traditional statistical
meth-ods to advanced deep learning models. Among them, the Auto-Regressive
Integrated Moving Average (ARIMA) model remains a widely adopted linear
technique due to its effectiveness in modeling temporal dependencies in
economic, industrial, and social data. On the other hand, polynomial
classifi-ers offer a robust framework for capturing non-linear relationships
and have demonstrated competitive performance in domains such as stock price
pre-diction. In this study, we propose a hybrid forecasting approach that
inte-grates the ARIMA model with a polynomial classifier to leverage the
com-plementary strengths of both models. The hybrid method is evaluated on
multiple real-world time series datasets spanning diverse domains. Perfor-mance
is assessed based on forecasting accuracy and computational effi-ciency.
Experimental results reveal that the proposed hybrid model consist-ently
outperforms the individual models in terms of prediction accuracy, al-beit with
a modest increase in execution time.

</details>


### [491] [Learning Soft Sparse Shapes for Efficient Time-Series Classification](https://arxiv.org/pdf/2505.06892)
*Zhen Liu, Yicheng Luo, Boyuan Li, Emadeldeen Eldele, Min Wu, Qianli Ma*

Main category: cs.LG

TL;DR: SoftShape introduces soft sparsification and learning blocks for efficient time series classification, outperforming existing methods while retaining interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing shapelet-based methods discard non-discriminative shapes, potentially losing beneficial ones and ignoring varying contributions of shapelets.

Method: Proposes soft shape sparsification (merging lower-scored shapes) and learning blocks (intra- and inter-shape pattern learning via expert networks).

Result: Outperforms state-of-the-art methods and provides interpretable results.

Conclusion: SoftShape effectively balances efficiency and interpretability in time series classification.

Abstract: Shapelets are discriminative subsequences (or shapes) with high
interpretability in time series classification. Due to the time-intensive
nature of shapelet discovery, existing shapelet-based methods mainly focus on
selecting discriminative shapes while discarding others to achieve candidate
subsequence sparsification. However, this approach may exclude beneficial
shapes and overlook the varying contributions of shapelets to classification
performance. To this end, we propose a \textbf{Soft} sparse \textbf{Shape}s
(\textbf{SoftShape}) model for efficient time series classification. Our
approach mainly introduces soft shape sparsification and soft shape learning
blocks. The former transforms shapes into soft representations based on
classification contribution scores, merging lower-scored ones into a single
shape to retain and differentiate all subsequence information. The latter
facilitates intra- and inter-shape temporal pattern learning, improving model
efficiency by using sparsified soft shapes as inputs. Specifically, we employ a
learnable router to activate a subset of class-specific expert networks for
intra-shape pattern learning. Meanwhile, a shared expert network learns
inter-shape patterns by converting sparsified shapes into sequences. Extensive
experiments show that SoftShape outperforms state-of-the-art methods and
produces interpretable results.

</details>


### [492] [MMiC: Mitigating Modality Incompleteness in Clustered Federated Learning](https://arxiv.org/pdf/2505.06911)
*Lishan Yang, Wei Zhang, Quan Z. Sheng, Weitong Chen, Lina Yao, Weitong Chen, Ali Shakeri*

Main category: cs.LG

TL;DR: MMiC is a framework for mitigating missing modalities in Multimodal Federated Learning (MFL) by replacing partial parameters and optimizing client selection, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Missing modalities in MFL due to data quality or privacy policies hinder performance, necessitating a solution like MMiC.

Method: MMiC replaces partial parameters, uses the Banzhaf Power Index for client selection, and employs Markovitz Portfolio Optimization for global aggregation.

Result: MMiC outperforms existing federated learning architectures in global and personalized performance on multimodal datasets with missing modalities.

Conclusion: MMiC effectively addresses modality incompleteness in MFL, enhancing performance and collaboration.

Abstract: In the era of big data, data mining has become indispensable for uncovering
hidden patterns and insights from vast and complex datasets. The integration of
multimodal data sources further enhances its potential. Multimodal Federated
Learning (MFL) is a distributed approach that enhances the efficiency and
quality of multimodal learning, ensuring collaborative work and privacy
protection. However, missing modalities pose a significant challenge in MFL,
often due to data quality issues or privacy policies across the clients. In
this work, we present MMiC, a framework for Mitigating Modality incompleteness
in MFL within the Clusters. MMiC replaces partial parameters within client
models inside clusters to mitigate the impact of missing modalities.
Furthermore, it leverages the Banzhaf Power Index to optimize client selection
under these conditions. Finally, MMiC employs an innovative approach to
dynamically control global aggregation by utilizing Markovitz Portfolio
Optimization. Extensive experiments demonstrate that MMiC consistently
outperforms existing federated learning architectures in both global and
personalized performance on multimodal datasets with missing modalities,
confirming the effectiveness of our proposed solution.

</details>


### [493] [Non-Stationary Time Series Forecasting Based on Fourier Analysis and Cross Attention Mechanism](https://arxiv.org/pdf/2505.06917)
*Yuqi Xiong, Yang Wen*

Main category: cs.LG

TL;DR: AEFIN, a new framework for non-stationary time series forecasting, uses cross-attention and Fourier-MLP to improve accuracy and robustness, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models struggle with non-stationary time series due to changing statistical characteristics.

Method: AEFIN combines cross-attention, Fourier analysis, and MLP, with a novel loss function for stability constraints.

Result: AEFIN achieves better MSE and MAE, especially for non-stationary data.

Conclusion: AEFIN offers an innovative solution for non-stationary time series forecasting, advancing deep learning research.

Abstract: Time series forecasting has important applications in financial analysis,
weather forecasting, and traffic management. However, existing deep learning
models are limited in processing non-stationary time series data because they
cannot effectively capture the statistical characteristics that change over
time. To address this problem, this paper proposes a new framework, AEFIN,
which enhances the information sharing ability between stable and unstable
components by introducing a cross-attention mechanism, and combines Fourier
analysis networks with MLP to deeply explore the seasonal patterns and trend
characteristics in unstable components. In addition, we design a new loss
function that combines time-domain stability constraints, time-domain
instability constraints, and frequency-domain stability constraints to improve
the accuracy and robustness of forecasting. Experimental results show that
AEFIN outperforms the most common models in terms of mean square error and mean
absolute error, especially under non-stationary data conditions, and shows
excellent forecasting capabilities. This paper provides an innovative solution
for the modeling and forecasting of non-stationary time series data, and
contributes to the research of deep learning for complex time series.

</details>


### [494] [AI-Powered Inverse Design of Ku-Band SIW Resonant Structures by Iterative Residual Correction Network](https://arxiv.org/pdf/2505.06936)
*Mohammad Mashayekhi, Kamran Salehian*

Main category: cs.LG

TL;DR: Proposes IRC-Net for inverse design of Ku-band SIW components, improving accuracy over traditional methods.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy and efficiency of inverse electromagnetic modeling for designing complex microwave structures.

Method: Uses IRC-Net, combining residual neural networks and iterative correction, starting with FIM for initial estimates.

Result: IRC-Net outperforms single-stage networks, validated by simulations and measurements.

Conclusion: IRC-Net is effective and practical for inverse design of SIW components.

Abstract: Inverse electromagnetic modeling has emerged as a powerful approach for
designing complex microwave structures with high accuracy and efficiency. In
this study, we propose an Iterative Residual Correction Network (IRC-Net) for
the inverse design of Ku-band Substrate Integrated Waveguide (SIW) components
based on multimode resonators. We use a multimode resonance structure to
demonstrate that it is possible to control the resonances of the structure.
Therefore, these structures can be used for resonant components and smart
filter design. The proposed deep learning architecture leverages residual
neural networks to overcome the limitations of traditional inverse design
techniques, such as the Feedforward Inverse Model (FIM), offering improved
generalization and prediction accuracy. The approach begins with a FIM to
generate initial design estimates, followed by an iterative correction strategy
inspired by the Hybrid Inverse-Forward Residual Refinement Network
(HiFR\textsuperscript{2}-Net), which we call IRC-Net. Experiments demonstrate
that the IRC-Net achieves substantial improvements in prediction accuracy
compared to traditional single-stage networks, validated through statistical
metrics, full-wave electromagnetic simulations, and measurements. To validate
the proposed framework, we first design and fabricate a three-resonance SIW
structure. Next, we apply the trained IRC-Net model to predict the geometry of
a four-resonance structure based on its desired frequency response. Both
designs are fabricated and tested, showing strong agreement between the
simulated, predicted, and measured results, confirming the effectiveness and
practicality of the proposed method.

</details>


### [495] [A systematic review of challenges and proposed solutions in modeling multimodal data](https://arxiv.org/pdf/2505.06945)
*Maryam Farhadizadeh, Maria Weymann, Michael Blaß, Johann Kraus, Christopher Gundler, Sebastian Walter, Noah Hempen, Harald Binde, Nadine Binder*

Main category: cs.LG

TL;DR: A systematic review of 69 studies on multimodal data modeling in clinical research, identifying challenges and recent methodological advances to improve diagnostic accuracy and personalized care.


<details>
  <summary>Details</summary>
Motivation: To address the technical challenges of integrating diverse data types (e.g., imaging, genomics) in clinical research and improve diagnostic and personalized care outcomes.

Method: Systematic review of 69 studies to identify common obstacles (e.g., missing modalities, dimensionality imbalance) and highlight methodological advances (e.g., transfer learning, generative models).

Result: Identified key challenges and promising solutions, such as attention mechanisms and neural architecture search, for multimodal data modeling in medical applications.

Conclusion: The review provides a comprehensive overview and practical insights to guide future research in multimodal modeling for clinical use.

Abstract: Multimodal data modeling has emerged as a powerful approach in clinical
research, enabling the integration of diverse data types such as imaging,
genomics, wearable sensors, and electronic health records. Despite its
potential to improve diagnostic accuracy and support personalized care,
modeling such heterogeneous data presents significant technical challenges.
This systematic review synthesizes findings from 69 studies to identify common
obstacles, including missing modalities, limited sample sizes, dimensionality
imbalance, interpretability issues, and finding the optimal fusion techniques.
We highlight recent methodological advances, such as transfer learning,
generative models, attention mechanisms, and neural architecture search that
offer promising solutions. By mapping current trends and innovations, this
review provides a comprehensive overview of the field and offers practical
insights to guide future research and development in multimodal modeling for
medical applications.

</details>


### [496] [Learning Value of Information towards Joint Communication and Control in 6G V2X](https://arxiv.org/pdf/2505.06978)
*Lei Lei, Kan Zheng, Xuemin, Shen*

Main category: cs.LG

TL;DR: The paper introduces Sequential Stochastic Decision Process (SSDP) models to define and assess Value of Information (VoI) for optimizing communication systems in Connected Autonomous Vehicles (CAVs), bridging vehicle control and V2X communication.


<details>
  <summary>Details</summary>
Motivation: To enhance CAV decision-making under uncertainty by leveraging data-driven ML, particularly DRL, and addressing the fragmented research on VoI.

Method: Proposes SSDP models to define VoI, demonstrates MDP as a special case, and introduces a systematic VoI modeling framework based on MDP, RL, and Optimal Control theories.

Result: SSDP models explicitly represent information sets for better decision-making, and a structured approach for optimizing communication problems ("When", "What", "How") is presented.

Conclusion: The methodology, illustrated with a vehicle-following control problem, shows potential for joint optimization of control and communication in networked systems.

Abstract: As Cellular Vehicle-to-Everything (C-V2X) evolves towards future
sixth-generation (6G) networks, Connected Autonomous Vehicles (CAVs) are
emerging to become a key application. Leveraging data-driven Machine Learning
(ML), especially Deep Reinforcement Learning (DRL), is expected to
significantly enhance CAV decision-making in both vehicle control and V2X
communication under uncertainty. These two decision-making processes are
closely intertwined, with the value of information (VoI) acting as a crucial
bridge between them. In this paper, we introduce Sequential Stochastic Decision
Process (SSDP) models to define and assess VoI, demonstrating their application
in optimizing communication systems for CAVs. Specifically, we formally define
the SSDP model and demonstrate that the MDP model is a special case of it. The
SSDP model offers a key advantage by explicitly representing the set of
information that can enhance decision-making when available. Furthermore, as
current research on VoI remains fragmented, we propose a systematic VoI
modeling framework grounded in the MDP, Reinforcement Learning (RL) and Optimal
Control theories. We define different categories of VoI and discuss their
corresponding estimation methods. Finally, we present a structured approach to
leverage the various VoI metrics for optimizing the ``When", ``What", and
``How" to communicate problems. For this purpose, SSDP models are formulated
with VoI-associated reward functions derived from VoI-based optimization
objectives. While we use a simple vehicle-following control problem to
illustrate the proposed methodology, it holds significant potential to
facilitate the joint optimization of stochastic, sequential control and
communication decisions in a wide range of networked control systems.

</details>


### [497] [GuidedQuant: Large Language Model Quantization via Exploiting End Loss Guidance](https://arxiv.org/pdf/2505.07004)
*Jinuk Kim, Marwa El Halabi, Wonpyo Park, Clemens JS Schaefer, Deokjae Lee, Yeonhong Park, Jae W. Lee, Hyun Oh Song*

Main category: cs.LG

TL;DR: GuidedQuant improves post-training quantization by integrating gradient information and preserving weight dependencies, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing quantization methods either ignore feature importance or neglect weight interactions, limiting performance.

Method: GuidedQuant uses gradient information and preserves cross-weight dependencies, with a novel non-uniform scalar quantization algorithm.

Result: GuidedQuant enhances performance across quantization types and introduces a superior non-uniform scalar method.

Conclusion: GuidedQuant advances quantization by addressing key limitations, with code publicly available.

Abstract: Post-training quantization is a key technique for reducing the memory and
inference latency of large language models by quantizing weights and
activations without requiring retraining. However, existing methods either (1)
fail to account for the varying importance of hidden features to the end loss
or, when incorporating end loss, (2) neglect the critical interactions between
model weights. To address these limitations, we propose GuidedQuant, a novel
quantization approach that integrates gradient information from the end loss
into the quantization objective while preserving cross-weight dependencies
within output channels. GuidedQuant consistently boosts the performance of
state-of-the-art quantization methods across weight-only scalar, weight-only
vector, and weight-and-activation quantization. Additionally, we introduce a
novel non-uniform scalar quantization algorithm, which is guaranteed to
monotonically decrease the quantization objective value, and outperforms
existing methods in this category. We release the code at
https://github.com/snu-mllab/GuidedQuant.

</details>


### [498] [Incremental Uncertainty-aware Performance Monitoring with Active Labeling Intervention](https://arxiv.org/pdf/2505.07023)
*Alexander Koebler, Thomas Decker, Ingo Thon, Volker Tresp, Florian Buettner*

Main category: cs.LG

TL;DR: IUPM is a label-free method for monitoring ML models under gradual distribution shifts, using optimal transport and active labeling to estimate performance changes and manage uncertainty.


<details>
  <summary>Details</summary>
Motivation: Address unnoticed accuracy declines in ML models due to slow, gradual distribution shifts.

Method: Proposes IUPM, which models shifts using optimal transport, quantifies uncertainty, and uses active labeling under budget constraints.

Result: IUPM outperforms baselines in gradual shift scenarios and improves label acquisition efficiency.

Conclusion: IUPM effectively monitors performance and manages uncertainty in gradual distribution shifts.

Abstract: We study the problem of monitoring machine learning models under gradual
distribution shifts, where circumstances change slowly over time, often leading
to unnoticed yet significant declines in accuracy. To address this, we propose
Incremental Uncertainty-aware Performance Monitoring (IUPM), a novel label-free
method that estimates performance changes by modeling gradual shifts using
optimal transport. In addition, IUPM quantifies the uncertainty in the
performance prediction and introduces an active labeling procedure to restore a
reliable estimate under a limited labeling budget. Our experiments show that
IUPM outperforms existing performance estimation baselines in various gradual
shift scenarios and that its uncertainty awareness guides label acquisition
more effectively compared to other strategies.

</details>


### [499] [Efficient Machine Unlearning by Model Splitting and Core Sample Selection](https://arxiv.org/pdf/2505.07026)
*Maximilian Egger, Rawad Bitar, Rüdiger Urbanke*

Main category: cs.LG

TL;DR: MaxRR introduces a generalized unlearning metric and an unlearning-aware training procedure for efficient and precise machine unlearning, addressing challenges in verification and weak guarantees.


<details>
  <summary>Details</summary>
Motivation: To meet legal requirements like the right to be forgotten, efficient and verifiable machine unlearning is needed, but current methods lack efficiency and strong verification.

Method: Develops a generalized unlearning metric and an unlearning-aware training procedure (MaxRR) to enable exact or near-exact unlearning.

Result: MaxRR achieves efficient unlearning with properties close to full retraining, even when exact unlearning isn't feasible.

Conclusion: MaxRR provides a robust solution for machine unlearning, improving efficiency and verification, and is adaptable to cases where exact unlearning is impossible.

Abstract: Machine unlearning is essential for meeting legal obligations such as the
right to be forgotten, which requires the removal of specific data from machine
learning models upon request. While several approaches to unlearning have been
proposed, existing solutions often struggle with efficiency and, more
critically, with the verification of unlearning - particularly in the case of
weak unlearning guarantees, where verification remains an open challenge. We
introduce a generalized variant of the standard unlearning metric that enables
more efficient and precise unlearning strategies. We also present an
unlearning-aware training procedure that, in many cases, allows for exact
unlearning. We term our approach MaxRR. When exact unlearning is not feasible,
MaxRR still supports efficient unlearning with properties closely matching
those achieved through full retraining.

</details>


### [500] [Predicting Diabetes Using Machine Learning: A Comparative Study of Classifiers](https://arxiv.org/pdf/2505.07036)
*Mahade Hasan, Farhana Yasmin*

Main category: cs.LG

TL;DR: A novel hybrid model (DNet) combining CNN and LSTM outperforms traditional and ensemble ML methods in diabetes prediction, achieving 99.79% accuracy and 99.98% AUC-ROC.


<details>
  <summary>Details</summary>
Motivation: Diabetes causes severe complications, and ML can enable early intervention. The study aims to improve prediction accuracy with a hybrid model.

Method: Developed DNet, a CNN-LSTM hybrid with residual blocks, batch normalization, and dropout. Compared it with traditional and ensemble ML methods using a Kaggle dataset.

Result: DNet achieved 99.79% accuracy and 99.98% AUC-ROC, outperforming other models.

Conclusion: The hybrid CNN-LSTM architecture is highly effective for diabetes prediction, highlighting its potential in medical diagnostics.

Abstract: Diabetes remains a significant health challenge globally, contributing to
severe complications like kidney disease, vision loss, and heart issues. The
application of machine learning (ML) in healthcare enables efficient and
accurate disease prediction, offering avenues for early intervention and
patient support. Our study introduces an innovative diabetes prediction
framework, leveraging both traditional ML techniques such as Logistic
Regression, SVM, Na\"ive Bayes, and Random Forest and advanced ensemble methods
like AdaBoost, Gradient Boosting, Extra Trees, and XGBoost. Central to our
approach is the development of a novel model, DNet, a hybrid architecture
combining Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM)
layers for effective feature extraction and sequential learning. The DNet model
comprises an initial convolutional block for capturing essential features,
followed by a residual block with skip connections to facilitate efficient
information flow. Batch Normalization and Dropout are employed for robust
regularization, and an LSTM layer captures temporal dependencies within the
data. Using a Kaggle-sourced real-world diabetes dataset, our model evaluation
spans cross-validation accuracy, precision, recall, F1 score, and ROC-AUC.
Among the models, DNet demonstrates the highest efficacy with an accuracy of
99.79% and an AUC-ROC of 99.98%, establishing its potential for superior
diabetes prediction. This robust hybrid architecture showcases the value of
combining CNN and LSTM layers, emphasizing its applicability in medical
diagnostics and disease prediction tasks.

</details>


### [501] [Reinforcement Learning (RL) Meets Urban Climate Modeling: Investigating the Efficacy and Impacts of RL-Based HVAC Control](https://arxiv.org/pdf/2505.07045)
*Junjie Yu, John S. Schreck, David John Gagne, Keith W. Oleson, Jie Li, Yongtu Liang, Qi Liao, Mingfei Sun, David O. Topping, Zhonghua Zheng*

Main category: cs.LG

TL;DR: RL-based HVAC control reduces energy use but varies by climate. An integrated framework evaluates its efficacy, impacts, and transferability across cities, showing climate-dependent rewards and strategy transferability.


<details>
  <summary>Details</summary>
Motivation: To assess how RL-based HVAC control performs in different climates and its broader impacts on indoor and urban climates, as well as strategy transferability.

Method: Combines RL with an urban climate model and building energy model to evaluate HVAC control across cities with varying climates.

Result: Rewards and impacts vary by climate; hot climates achieve higher rewards, and cities with temperature variations show better strategy transferability.

Conclusion: Climate context is crucial for RL-based HVAC control evaluation, and city-to-city learning can aid deployment.

Abstract: Reinforcement learning (RL)-based heating, ventilation, and air conditioning
(HVAC) control has emerged as a promising technology for reducing building
energy consumption while maintaining indoor thermal comfort. However, the
efficacy of such strategies is influenced by the background climate and their
implementation may potentially alter both the indoor climate and local urban
climate. This study proposes an integrated framework combining RL with an urban
climate model that incorporates a building energy model, aiming to evaluate the
efficacy of RL-based HVAC control across different background climates, impacts
of RL strategies on indoor climate and local urban climate, and the
transferability of RL strategies across cities. Our findings reveal that the
reward (defined as a weighted combination of energy consumption and thermal
comfort) and the impacts of RL strategies on indoor climate and local urban
climate exhibit marked variability across cities with different background
climates. The sensitivity of reward weights and the transferability of RL
strategies are also strongly influenced by the background climate. Cities in
hot climates tend to achieve higher rewards across most reward weight
configurations that balance energy consumption and thermal comfort, and those
cities with more varying atmospheric temperatures demonstrate greater RL
strategy transferability. These findings underscore the importance of
thoroughly evaluating RL-based HVAC control strategies in diverse climatic
contexts. This study also provides a new insight that city-to-city learning
will potentially aid the deployment of RL-based HVAC control.

</details>


### [502] [Scaling Laws and Representation Learning in Simple Hierarchical Languages: Transformers vs. Convolutional Architectures](https://arxiv.org/pdf/2505.07070)
*Francesco Cagnetta, Alessandro Favero, Antonio Sclocchi, Matthieu Wyart*

Main category: cs.LG

TL;DR: The paper explores how neural language models learn hierarchical structure through next-token prediction, using the Random Hierarchy Model (RHM) to derive scaling laws. It compares convolutional networks and transformers, showing that convolutional models scale faster due to architectural alignment with data structure.


<details>
  <summary>Details</summary>
Motivation: To understand how neural models acquire hierarchical language structure and how architectural choices impact scaling laws.

Method: Theoretical scaling laws are derived using the RHM, a synthetic dataset mimicking natural language hierarchy. The framework is extended to compare convolutional networks and transformers.

Result: Convolutional networks outperform transformers in scaling due to their alignment with hierarchical data structure.

Conclusion: Model architecture and data statistics interact to shape representation learning, with convolutional networks benefiting from structural alignment.

Abstract: How do neural language models acquire a language's structure when trained for
next-token prediction? We address this question by deriving theoretical scaling
laws for neural network performance on synthetic datasets generated by the
Random Hierarchy Model (RHM) -- an ensemble of probabilistic context-free
grammars designed to capture the hierarchical structure of natural language
while remaining analytically tractable. Previously, we developed a theory of
representation learning based on data correlations that explains how deep
learning models capture the hierarchical structure of the data sequentially,
one layer at a time. Here, we extend our theoretical framework to account for
architectural differences. In particular, we predict and empirically validate
that convolutional networks, whose structure aligns with that of the generative
process through locality and weight sharing, enjoy a faster scaling of
performance compared to transformer models, which rely on global self-attention
mechanisms. This finding clarifies the architectural biases underlying neural
scaling laws and highlights how representation learning is shaped by the
interaction between model architecture and the statistical properties of data.

</details>


### [503] [COMRECGC: Global Graph Counterfactual Explainer through Common Recourse](https://arxiv.org/pdf/2505.07081)
*Gregoire Fournier, Sourav Medya*

Main category: cs.LG

TL;DR: The paper introduces COMRECGC, an algorithm for generating common recourse explanations in GNNs, addressing a gap in global counterfactual explanations.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored problem of finding common recourse for global counterfactual explanations in GNNs, particularly for applications like drug discovery.

Method: The authors formalize the common recourse explanation problem and design the COMRECGC algorithm to solve it.

Result: COMRECGC outperforms baselines on four real-world datasets and shows comparable or superior performance to graph counterfactual explanations.

Conclusion: Common recourse explanations are valuable for GNN applications, offering a promising alternative to traditional counterfactual methods.

Abstract: Graph neural networks (GNNs) have been widely used in various domains such as
social networks, molecular biology, or recommendation systems. Concurrently,
different explanations methods of GNNs have arisen to complement its black-box
nature. Explanations of the GNNs' predictions can be categorized into two
types--factual and counterfactual. Given a GNN trained on binary classification
into ''accept'' and ''reject'' classes, a global counterfactual explanation
consists in generating a small set of ''accept'' graphs relevant to all of the
input ''reject'' graphs. The transformation of a ''reject'' graph into an
''accept'' graph is called a recourse. A common recourse explanation is a small
set of recourse, from which every ''reject'' graph can be turned into an
''accept'' graph. Although local counterfactual explanations have been studied
extensively, the problem of finding common recourse for global counterfactual
explanation remains unexplored, particularly for GNNs. In this paper, we
formalize the common recourse explanation problem, and design an effective
algorithm, COMRECGC, to solve it. We benchmark our algorithm against strong
baselines on four different real-world graphs datasets and demonstrate the
superior performance of COMRECGC against the competitors. We also compare the
common recourse explanations to the graph counterfactual explanation, showing
that common recourse explanations are either comparable or superior, making
them worth considering for applications such as drug discovery or computational
biology.

</details>


### [504] [Multi-Objective-Guided Discrete Flow Matching for Controllable Biological Sequence Design](https://arxiv.org/pdf/2505.07086)
*Tong Chen, Yinuo Zhang, Sophia Tang, Pranam Chatterjee*

Main category: cs.LG

TL;DR: MOG-DFM is a framework for multi-objective-guided discrete flow matching, enabling Pareto-efficient trade-offs in biomolecule sequence design.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of designing biological sequences with conflicting functional and biophysical criteria.

Method: Uses hybrid rank-directional scoring and adaptive hypercone filtering to guide pretrained discrete flow matching models.

Result: Demonstrated effectiveness in optimizing peptide binders and DNA sequences for specific properties and shapes.

Conclusion: MOG-DFM is a powerful tool for multi-property biomolecule sequence design.

Abstract: Designing biological sequences that satisfy multiple, often conflicting,
functional and biophysical criteria remains a central challenge in biomolecule
engineering. While discrete flow matching models have recently shown promise
for efficient sampling in high-dimensional sequence spaces, existing approaches
address only single objectives or require continuous embeddings that can
distort discrete distributions. We present Multi-Objective-Guided Discrete Flow
Matching (MOG-DFM), a general framework to steer any pretrained discrete-time
flow matching generator toward Pareto-efficient trade-offs across multiple
scalar objectives. At each sampling step, MOG-DFM computes a hybrid
rank-directional score for candidate transitions and applies an adaptive
hypercone filter to enforce consistent multi-objective progression. We also
trained two unconditional discrete flow matching models, PepDFM for diverse
peptide generation and EnhancerDFM for functional enhancer DNA generation, as
base generation models for MOG-DFM. We demonstrate MOG-DFM's effectiveness in
generating peptide binders optimized across five properties (hemolysis,
non-fouling, solubility, half-life, and binding affinity), and in designing DNA
sequences with specific enhancer classes and DNA shapes. In total, MOG-DFM
proves to be a powerful tool for multi-property-guided biomolecule sequence
design.

</details>


### [505] [Physics-informed Multiple-Input Operators for efficient dynamic response prediction of structures](https://arxiv.org/pdf/2505.07090)
*Bilal Ahmed, Yuqing Qiu, Diab W. Abueidda, Waleed El-Sekelly, Tarek Abdoun, Mostafa E. Mobasher*

Main category: cs.LG

TL;DR: MIONet improves dynamic structural response prediction by encoding temporal dynamics, achieving FEM-level accuracy faster than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Dynamic structural analysis is computationally intensive, and existing methods like DeepONet with RNNs struggle with continuous dynamics.

Method: MIONet uses a second trunk network for temporal dynamics, physics-informed loss, and Schur complement for efficiency.

Result: Validated on a beam and bridge, MIONet matches FEM accuracy in seconds, outperforming GRU-based DeepONet in speed and continuity.

Conclusion: MIONet is efficient and accurate for real-time structural monitoring and digital twins.

Abstract: Finite element (FE) modeling is essential for structural analysis but remains
computationally intensive, especially under dynamic loading. While operator
learning models have shown promise in replicating static structural responses
at FEM level accuracy, modeling dynamic behavior remains more challenging. This
work presents a Multiple Input Operator Network (MIONet) that incorporates a
second trunk network to explicitly encode temporal dynamics, enabling accurate
prediction of structural responses under moving loads. Traditional DeepONet
architectures using recurrent neural networks (RNNs) are limited by fixed time
discretization and struggle to capture continuous dynamics. In contrast, MIONet
predicts responses continuously over both space and time, removing the need for
step wise modeling. It maps scalar inputs including load type, velocity,
spatial mesh, and time steps to full field structural responses. To improve
efficiency and enforce physical consistency, we introduce a physics informed
loss based on dynamic equilibrium using precomputed mass, damping, and
stiffness matrices, without solving the governing PDEs directly. Further, a
Schur complement formulation reduces the training domain, significantly cutting
computational costs while preserving global accuracy. The model is validated on
both a simple beam and the KW-51 bridge, achieving FEM level accuracy within
seconds. Compared to GRU based DeepONet, our model offers comparable accuracy
with improved temporal continuity and over 100 times faster inference, making
it well suited for real-time structural monitoring and digital twin
applications.

</details>


### [506] [Navigating the Rashomon Effect: How Personalization Can Help Adjust Interpretable Machine Learning Models to Individual Users](https://arxiv.org/pdf/2505.07100)
*Julian Rosenberger, Philipp Schröppel, Sven Kruschel, Mathias Kraus, Patrick Zschech, Maximilian Förster*

Main category: cs.LG

TL;DR: The paper explores personalizing interpretable ML models (like GAMs) using contextual bandits, showing individualized configurations without sacrificing interpretability.


<details>
  <summary>Details</summary>
Motivation: The Rashomon effect in ML shows multiple models can perform similarly but explain data differently, prompting the need for personalized interpretable models.

Method: Developed a personalization approach using contextual bandits, tested in an online experiment with 108 users (personalized vs. non-personalized groups).

Result: Personalization led to individualized model configurations while maintaining high interpretability, with users understanding the models well.

Conclusion: The study highlights the potential for personalizing interpretable ML, offering tailored solutions without compromising clarity.

Abstract: The Rashomon effect describes the observation that in machine learning (ML)
multiple models often achieve similar predictive performance while explaining
the underlying relationships in different ways. This observation holds even for
intrinsically interpretable models, such as Generalized Additive Models (GAMs),
which offer users valuable insights into the model's behavior. Given the
existence of multiple GAM configurations with similar predictive performance, a
natural question is whether we can personalize these configurations based on
users' needs for interpretability. In our study, we developed an approach to
personalize models based on contextual bandits. In an online experiment with
108 users in a personalized treatment and a non-personalized control group, we
found that personalization led to individualized rather than one-size-fits-all
configurations. Despite these individual adjustments, the interpretability
remained high across both groups, with users reporting a strong understanding
of the models. Our research offers initial insights into the potential for
personalizing interpretable ML.

</details>


### [507] [Learning from Samples: Inverse Problems over measures via Sharpened Fenchel-Young Losses](https://arxiv.org/pdf/2505.07124)
*Francisco Andrade, Gabriel Peyré, Clarice Poon*

Main category: cs.LG

TL;DR: The paper introduces sharpened Fenchel-Young losses for estimating parameters of optimal probability distributions, with applications in socio-economic and biological systems. It provides stability guarantees for inverse unbalanced optimal transport (iUOT) and inverse gradient flow (iJKO), validated through numerical experiments.


<details>
  <summary>Details</summary>
Motivation: Parameter estimation in optimal probability distributions is crucial for socio-economic and biological modeling. The paper addresses the challenge of stability in such estimations with limited samples.

Method: The approach minimizes sharpened Fenchel-Young losses, leveraging strong convexity and sample complexity. It focuses on iUOT for cost function estimation and iJKO for potential function recovery.

Result: The method offers explicit stability guarantees for iUOT and iJKO, with practical validation on Gaussian distributions.

Conclusion: The proposed framework effectively estimates parameters in optimal distributions, demonstrating stability and applicability in real-world scenarios.

Abstract: Estimating parameters from samples of an optimal probability distribution is
essential in applications ranging from socio-economic modeling to biological
system analysis. In these settings, the probability distribution arises as the
solution to an optimization problem that captures either static interactions
among agents or the dynamic evolution of a system over time. Our approach
relies on minimizing a new class of loss functions, called sharpened
Fenchel-Young losses, which measure the sub-optimality gap of the optimization
problem over the space of measures. We study the stability of this estimation
method when only a finite number of sample is available. The parameters to be
estimated typically correspond to a cost function in static problems and to a
potential function in dynamic problems. To analyze stability, we introduce a
general methodology that leverages the strong convexity of the loss function
together with the sample complexity of the forward optimization problem. Our
analysis emphasizes two specific settings in the context of optimal transport,
where our method provides explicit stability guarantees: The first is inverse
unbalanced optimal transport (iUOT) with entropic regularization, where the
parameters to estimate are cost functions that govern transport computations;
this method has applications such as link prediction in machine learning. The
second is inverse gradient flow (iJKO), where the objective is to recover a
potential function that drives the evolution of a probability distribution via
the Jordan-Kinderlehrer-Otto (JKO) time-discretization scheme; this is
particularly relevant for understanding cell population dynamics in single-cell
genomics. Finally, we validate our approach through numerical experiments on
Gaussian distributions, where closed-form solutions are available, to
demonstrate the practical performance of our methods

</details>


### [508] [Triangulating PL functions and the existence of efficient ReLU DNNs](https://arxiv.org/pdf/2505.07137)
*Danny Calegari*

Main category: cs.LG

TL;DR: The paper proves that piecewise linear functions with compact support can be represented as sums of simplex functions, leading to efficient universal ReLU networks.


<details>
  <summary>Details</summary>
Motivation: To provide a simple and elementary proof for the existence of efficient universal ReLU networks capable of computing piecewise linear functions of bounded complexity.

Method: Uses degree 1 triangulations of relative homology classes in $R^{d+1}$ to represent piecewise linear functions as sums of simplex functions.

Result: Demonstrates that such representations enable the construction of efficient universal ReLU networks for these functions.

Conclusion: The findings offer a concise proof and practical implications for neural network design.

Abstract: We show that every piecewise linear function $f:R^d \to R$ with compact
support a polyhedron $P$ has a representation as a sum of so-called `simplex
functions'. Such representations arise from degree 1 triangulations of the
relative homology class (in $R^{d+1}$) bounded by $P$ and the graph of $f$, and
give a short elementary proof of the existence of efficient universal ReLU
neural networks that simultaneously compute all such functions $f$ of bounded
complexity.

</details>


### [509] [AugMixCloak: A Defense against Membership Inference Attacks via Image Transformation](https://arxiv.org/pdf/2505.07149)
*Heqing Ren, Chao Feng, Alberto Huertas, Burkhard Stiller*

Main category: cs.LG

TL;DR: AugMixCloak defends against membership inference attacks in federated learning using data augmentation and PCA-based fusion, outperforming regularization and confidence masking methods.


<details>
  <summary>Details</summary>
Motivation: Federated learning (FL) reduces data leakage risks but remains vulnerable to membership inference attacks (MIAs), necessitating stronger defenses.

Method: AugMixCloak combines data augmentation and PCA-based information fusion on query images detected by perceptual hashing (pHash) to protect against MIAs.

Result: AugMixCloak effectively defends against binary classifier-based and metric-based MIAs across five datasets and various FL topologies.

Conclusion: AugMixCloak offers superior protection and generalization compared to regularization-based defenses and confidence score masking.

Abstract: Traditional machine learning (ML) raises serious privacy concerns, while
federated learning (FL) mitigates the risk of data leakage by keeping data on
local devices. However, the training process of FL can still leak sensitive
information, which adversaries may exploit to infer private data. One of the
most prominent threats is the membership inference attack (MIA), where the
adversary aims to determine whether a particular data record was part of the
training set.
  This paper addresses this problem through a two-stage defense called
AugMixCloak. The core idea is to apply data augmentation and principal
component analysis (PCA)-based information fusion to query images, which are
detected by perceptual hashing (pHash) as either identical to or highly similar
to images in the training set. Experimental results show that AugMixCloak
successfully defends against both binary classifier-based MIA and metric-based
MIA across five datasets and various decentralized FL (DFL) topologies.
Compared with regularization-based defenses, AugMixCloak demonstrates stronger
protection. Compared with confidence score masking, AugMixCloak exhibits better
generalization.

</details>


### [510] [Causal View of Time Series Imputation: Some Identification Results on Missing Mechanism](https://arxiv.org/pdf/2505.07180)
*Ruichu Cai, Kaitao Zheng, Junxian Huang, Zijian Li, Zhengming Chen, Boyan Xu, Zhifeng Hao*

Main category: cs.LG

TL;DR: The paper proposes a framework for time series imputation that addresses different missing mechanisms (MAR and MNAR) separately, improving accuracy by modeling data generation processes with variational inference and normalizing flow-based neural architecture.


<details>
  <summary>Details</summary>
Motivation: Existing time series imputation methods often ignore the differences between missing mechanisms (MAR and MNAR), leading to misleading results. This paper aims to address this gap by tailoring solutions for each mechanism.

Method: The framework analyzes data generation processes with temporal latent states and missing cause variables, models these via variational inference, and estimates prior distributions of latent variables using normalizing flow-based neural architecture. Identifiability of latent variables is proven under nonlinear independent component analysis.

Result: Experimental results show the proposed method outperforms existing techniques across datasets with different missing mechanisms, proving its effectiveness in real-world applications.

Conclusion: The paper successfully addresses the challenge of mechanism mismatching in time series imputation by proposing a tailored framework, demonstrating superior performance over existing methods.

Abstract: Time series imputation is one of the most challenge problems and has broad
applications in various fields like health care and the Internet of Things.
Existing methods mainly aim to model the temporally latent dependencies and the
generation process from the observed time series data. In real-world scenarios,
different types of missing mechanisms, like MAR (Missing At Random), and MNAR
(Missing Not At Random) can occur in time series data. However, existing
methods often overlook the difference among the aforementioned missing
mechanisms and use a single model for time series imputation, which can easily
lead to misleading results due to mechanism mismatching. In this paper, we
propose a framework for time series imputation problem by exploring Different
Missing Mechanisms (DMM in short) and tailoring solutions accordingly.
Specifically, we first analyze the data generation processes with temporal
latent states and missing cause variables for different mechanisms.
Sequentially, we model these generation processes via variational inference and
estimate prior distributions of latent variables via normalizing flow-based
neural architecture. Furthermore, we establish identifiability results under
the nonlinear independent component analysis framework to show that latent
variables are identifiable. Experimental results show that our method surpasses
existing time series imputation techniques across various datasets with
different missing mechanisms, demonstrating its effectiveness in real-world
applications.

</details>


### [511] [Compression, Regularity, Randomness and Emergent Structure: Rethinking Physical Complexity in the Data-Driven Era](https://arxiv.org/pdf/2505.07222)
*Nima Dehghani*

Main category: cs.LG

TL;DR: A unified framework organizes statistical, algorithmic, and dynamical complexity measures along three axes (regularity, randomness, complexity) and discusses their computational challenges and modern data-driven approximations.


<details>
  <summary>Details</summary>
Motivation: To systematically organize and conceptualize diverse complexity measures, addressing the lack of a unified framework.

Method: Proposes a taxonomy mapping measures into a conceptual space, evaluating computational accessibility and approximability, and highlighting modern data-driven methods.

Result: Reveals computational challenges (e.g., uncomputability) and the role of data-driven methods (e.g., autoencoders, neural networks) as practical approximations.

Conclusion: Classical complexity questions remain vital for next-gen scientific modeling, with implications for physics-informed AI and AI-guided discovery.

Abstract: Complexity science offers a wide range of measures for quantifying
unpredictability, structure, and information. Yet, a systematic conceptual
organization of these measures is still missing.
  We present a unified framework that locates statistical, algorithmic, and
dynamical measures along three axes (regularity, randomness, and complexity)
and situates them in a common conceptual space. We map statistical,
algorithmic, and dynamical measures into this conceptual space, discussing
their computational accessibility and approximability.
  This taxonomy reveals the deep challenges posed by uncomputability and
highlights the emergence of modern data-driven methods (including autoencoders,
latent dynamical models, symbolic regression, and physics-informed neural
networks) as pragmatic approximations to classical complexity ideals. Latent
spaces emerge as operational arenas where regularity extraction, noise
management, and structured compression converge, bridging theoretical
foundations with practical modeling in high-dimensional systems.
  We close by outlining implications for physics-informed AI and AI-guided
discovery in complex physical systems, arguing that classical questions of
complexity remain central to next-generation scientific modeling.

</details>


### [512] [REMEDI: Relative Feature Enhanced Meta-Learning with Distillation for Imbalanced Prediction](https://arxiv.org/pdf/2505.07245)
*Fei Liu, Huanhuan Ren, Yu Guan, Xiuxu Wang, Wang Lv, Zhiqiang Hu, Yaxi Chen*

Main category: cs.LG

TL;DR: REMEDI is a multi-stage framework for predicting future vehicle purchases in imbalanced datasets, combining diverse base models, meta-features, and distillation for efficient deployment.


<details>
  <summary>Details</summary>
Motivation: Predicting vehicle purchases is challenging due to extreme class imbalance (<0.5% positive rate) and complex behavioral patterns.

Method: REMEDI trains diverse base models, uses relative performance meta-features for model fusion, and distills knowledge into a single efficient model via supervised fine-tuning.

Result: REMEDI outperforms baselines, identifying ~50% of actual buyers within the top 60,000 recommendations at ~10% precision.

Conclusion: REMEDI effectively addresses imbalanced prediction challenges while maintaining deployment efficiency.

Abstract: Predicting future vehicle purchases among existing owners presents a critical
challenge due to extreme class imbalance (<0.5% positive rate) and complex
behavioral patterns. We propose REMEDI (Relative feature Enhanced Meta-learning
with Distillation for Imbalanced prediction), a novel multi-stage framework
addressing these challenges. REMEDI first trains diverse base models to capture
complementary aspects of user behavior. Second, inspired by comparative
op-timization techniques, we introduce relative performance meta-features
(deviation from ensemble mean, rank among peers) for effective model fusion
through a hybrid-expert architecture. Third, we distill the ensemble's
knowledge into a single efficient model via supervised fine-tuning with MSE
loss, enabling practical deployment. Evaluated on approximately 800,000 vehicle
owners, REMEDI significantly outperforms baseline approaches, achieving the
business target of identifying ~50% of actual buyers within the top 60,000
recommendations at ~10% precision. The distilled model preserves the ensemble's
predictive power while maintaining deployment efficiency, demonstrating
REMEDI's effectiveness for imbalanced prediction in industry settings.

</details>


### [513] [UMoE: Unifying Attention and FFN with Shared Experts](https://arxiv.org/pdf/2505.07260)
*Yuanhang Yang, Chaozheng Wang, Jing Li*

Main category: cs.LG

TL;DR: UMoE unifies MoE designs in attention and FFN layers by reformulating attention as FFN-like, enabling efficient parameter sharing and superior performance.


<details>
  <summary>Details</summary>
Motivation: Existing attention-based MoE layers require specialized implementations and underperform compared to FFN-based MoE layers.

Method: Introduces UMoE, a novel reformulation of attention to reveal its FFN-like structure, enabling unified MoE designs.

Result: UMoE achieves superior performance with attention-based MoE layers and efficient parameter sharing.

Conclusion: UMoE successfully unifies MoE in attention and FFN layers, improving performance and efficiency.

Abstract: Sparse Mixture of Experts (MoE) architectures have emerged as a promising
approach for scaling Transformer models. While initial works primarily
incorporated MoE into feed-forward network (FFN) layers, recent studies have
explored extending the MoE paradigm to attention layers to enhance model
performance. However, existing attention-based MoE layers require specialized
implementations and demonstrate suboptimal performance compared to their
FFN-based counterparts. In this paper, we aim to unify the MoE designs in
attention and FFN layers by introducing a novel reformulation of the attention
mechanism, revealing an underlying FFN-like structure within attention modules.
Our proposed architecture, UMoE, achieves superior performance through
attention-based MoE layers while enabling efficient parameter sharing between
FFN and attention components.

</details>


### [514] [Cache-Efficient Posterior Sampling for Reinforcement Learning with LLM-Derived Priors Across Discrete and Continuous Domains](https://arxiv.org/pdf/2505.07274)
*Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma*

Main category: cs.LG

TL;DR: A framework for integrating LLMs as priors in RL with adaptive caching reduces computational costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Leveraging LLMs in RL is computationally expensive; this work aims to reduce costs without sacrificing performance.

Method: Uses an adaptive caching mechanism with meta-optimized parameters and surrogate gradients for efficient inference in discrete and continuous domains.

Result: Achieves 3.8--4.7x fewer LLM queries, 4.0--12.0x lower latency, and retains 96--98% performance. Offline RL variant improves performance by 14--29% and reduces training time by 38--40%.

Conclusion: The framework is generalizable and practical for LLM-guided RL in resource-constrained settings.

Abstract: Integrating large language models (LLMs) as priors in reinforcement learning
(RL) offers significant advantages but comes with substantial computational
costs. We present a principled cache-efficient framework for posterior sampling
with LLM-derived priors that dramatically reduces these costs while maintaining
high performance. At the core of our approach is an adaptive caching mechanism,
where cache parameters are meta-optimized using surrogate gradients derived
from policy performance. This design enables efficient inference across both
discrete text environments (e.g., TextWorld, ALFWorld) and continuous control
domains (e.g., MuJoCo), achieving a 3.8--4.7$\times$ reduction in LLM queries
and 4.0--12.0$\times$ lower median latencies (85--93\,ms on a consumer GPU)
while retaining 96--98\% of uncached performance. Our theoretical analysis
provides KL divergence bounds on approximation quality, validated empirically.
The framework extends to offline RL, where our CQL-Prior variant improves
performance by 14--29\% and reduces training time by 38--40\%. Extensive
evaluations across a diverse suite of eight tasks demonstrate the
generalizability and practical viability of LLM-guided RL in
resource-constrained settings.

</details>


### [515] [INTELLECT-2: A Reasoning Model Trained Through Globally Decentralized Reinforcement Learning](https://arxiv.org/pdf/2505.07291)
*Prime Intellect Team, Sami Jaghouar, Justus Mattern, Jack Min Ong, Jannik Straube, Manveer Basra, Aaron Pazdera, Kushal Thaman, Matthew Di Ferrante, Felix Gabriel, Fares Obeid, Kemal Erdem, Michael Keiblinger, Johannes Hagemann*

Main category: cs.LG

TL;DR: INTELLECT-2 is a 32B parameter language model trained via globally distributed, asynchronous RL using a permissionless compute swarm. It introduces novel infrastructure (PRIME-RL, TOPLOC, SHARDCAST) and training modifications to achieve stability and outperform QwQ-32B.


<details>
  <summary>Details</summary>
Motivation: To pioneer decentralized, large-scale RL training by leveraging heterogeneous, permissionless compute resources, advancing open research in this field.

Method: Uses fully asynchronous RL across a dynamic compute swarm, with custom frameworks (PRIME-RL, TOPLOC, SHARDCAST) and modified GRPO training recipe for stability.

Result: Successfully trained INTELLECT-2, outperforming QwQ-32B, the state-of-the-art reasoning model in its parameter range.

Conclusion: INTELLECT-2 demonstrates the feasibility of decentralized RL training and opens avenues for open research, with all code and data released publicly.

Abstract: We introduce INTELLECT-2, the first globally distributed reinforcement
learning (RL) training run of a 32 billion parameter language model. Unlike
traditional centralized training efforts, INTELLECT-2 trains a reasoning model
using fully asynchronous RL across a dynamic, heterogeneous swarm of
permissionless compute contributors.
  To enable a training run with this unique infrastructure, we built various
components from scratch: we introduce PRIME-RL, our training framework
purpose-built for distributed asynchronous reinforcement learning, based on top
of novel components such as TOPLOC, which verifies rollouts from untrusted
inference workers, and SHARDCAST, which efficiently broadcasts policy weights
from training nodes to inference workers.
  Beyond infrastructure components, we propose modifications to the standard
GRPO training recipe and data filtering techniques that were crucial to achieve
training stability and ensure that our model successfully learned its training
objective, thus improving upon QwQ-32B, the state of the art reasoning model in
the 32B parameter range.
  We open-source INTELLECT-2 along with all of our code and data, hoping to
encourage and enable more open research in the field of decentralized training.

</details>


### [516] [Online Episodic Convex Reinforcement Learning](https://arxiv.org/pdf/2505.07303)
*Bianca Marin Moreno, Khaled Eldowa, Pierre Gaillard, Margaux Brégère, Nadia Oudjane*

Main category: cs.LG

TL;DR: The paper introduces algorithms for online concave utility reinforcement learning (CURL) in MDPs, achieving near-optimal regret bounds without prior knowledge of transitions. It also addresses a bandit version of CURL with sub-linear regret.


<details>
  <summary>Details</summary>
Motivation: To generalize reinforcement learning from linear to convex losses on state-action distributions, requiring new approaches due to non-linearity.

Method: Uses online mirror descent with varying constraints and exploration bonuses for CURL, and adapts bandit convex optimization techniques for the bandit version.

Result: Achieves near-optimal regret bounds for CURL and sub-linear regret for the bandit version.

Conclusion: The proposed methods successfully address CURL and its bandit variant, advancing online learning in MDPs with convex objectives.

Abstract: We study online learning in episodic finite-horizon Markov decision processes
(MDPs) with convex objective functions, known as the concave utility
reinforcement learning (CURL) problem. This setting generalizes RL from linear
to convex losses on the state-action distribution induced by the agent's
policy. The non-linearity of CURL invalidates classical Bellman equations and
requires new algorithmic approaches. We introduce the first algorithm achieving
near-optimal regret bounds for online CURL without any prior knowledge on the
transition function. To achieve this, we use an online mirror descent algorithm
with varying constraint sets and a carefully designed exploration bonus. We
then address for the first time a bandit version of CURL, where the only
feedback is the value of the objective function on the state-action
distribution induced by the agent's policy. We achieve a sub-linear regret
bound for this more challenging problem by adapting techniques from bandit
convex optimization to the MDP setting.

</details>


### [517] [Uncertainty Profiles for LLMs: Uncertainty Source Decomposition and Adaptive Model-Metric Selection](https://arxiv.org/pdf/2505.07309)
*Pei-Fu Guo, Yun-Da Tsai, Shou-De Lin*

Main category: cs.LG

TL;DR: The paper presents a framework to decompose LLM uncertainty into four sources, evaluates existing metrics, and proposes a task-specific selection method for models/metrics, improving reliability.


<details>
  <summary>Details</summary>
Motivation: Addressing the issue of hallucinations in LLM outputs by enhancing interpretability and clarity in uncertainty estimation.

Method: Decomposes LLM uncertainty into four sources, develops a source-specific estimation pipeline, and evaluates metrics across tasks/models.

Result: Metrics, tasks, and models show systematic variation in uncertainty characteristics; the proposed selection method outperforms baselines.

Conclusion: The uncertainty-aware selection strategy improves model/metric selection, aiding reliable deployment in uncertainty estimation.

Abstract: Large language models (LLMs) often generate fluent but factually incorrect
outputs, known as hallucinations, which undermine their reliability in
real-world applications. While uncertainty estimation has emerged as a
promising strategy for detecting such errors, current metrics offer limited
interpretability and lack clarity about the types of uncertainty they capture.
In this paper, we present a systematic framework for decomposing LLM
uncertainty into four distinct sources, inspired by previous research. We
develop a source-specific estimation pipeline to quantify these uncertainty
types and evaluate how existing metrics relate to each source across tasks and
models. Our results show that metrics, task, and model exhibit systematic
variation in uncertainty characteristic. Building on this, we propose a method
for task specific metric/model selection guided by the alignment or divergence
between their uncertainty characteristics and that of a given task. Our
experiments across datasets and models demonstrate that our uncertainty-aware
selection strategy consistently outperforms baseline strategies, helping us
select appropriate models or uncertainty metrics, and contributing to more
reliable and efficient deployment in uncertainty estimation.

</details>


### [518] [Dynamical Label Augmentation and Calibration for Noisy Electronic Health Records](https://arxiv.org/pdf/2505.07320)
*Yuhao Li, Ling Luo, Uwe Aickelin*

Main category: cs.LG

TL;DR: ACTLL is an attention-based framework for handling noisy labels in medical time series data, improving prediction accuracy by dynamically calibrating uncertain labels and augmenting confident instances.


<details>
  <summary>Details</summary>
Motivation: Labeling errors in EHR data hinder accurate patient outcome predictions, necessitating a robust method to address noise.

Method: ACTLL uses a Beta mixture model to classify instances into certain/uncertain sets, then dynamically calibrates or augments labels while capturing temporal dynamics.

Result: ACTLL outperforms others on EHR datasets (eICU, MIMIC-IV-ED) and benchmarks (UCR, UEA), especially with high noise.

Conclusion: ACTLL effectively mitigates label noise in medical time series, enhancing prediction reliability.

Abstract: Medical research, particularly in predicting patient outcomes, heavily relies
on medical time series data extracted from Electronic Health Records (EHR),
which provide extensive information on patient histories. Despite rigorous
examination, labeling errors are inevitable and can significantly impede
accurate predictions of patient outcome. To address this challenge, we propose
an \textbf{A}ttention-based Learning Framework with Dynamic
\textbf{C}alibration and Augmentation for \textbf{T}ime series Noisy
\textbf{L}abel \textbf{L}earning (ACTLL). This framework leverages a
two-component Beta mixture model to identify the certain and uncertain sets of
instances based on the fitness distribution of each class, and it captures
global temporal dynamics while dynamically calibrating labels from the
uncertain set or augmenting confident instances from the certain set.
Experimental results on large-scale EHR datasets eICU and MIMIC-IV-ED, and
several benchmark datasets from the UCR and UEA repositories, demonstrate that
our model ACTLL has achieved state-of-the-art performance, especially under
high noise levels.

</details>


### [519] [From Search To Sampling: Generative Models For Robust Algorithmic Recourse](https://arxiv.org/pdf/2505.07351)
*Prateek Garg, Lokesh Nagalapatti, Sunita Sarawagi*

Main category: cs.LG

TL;DR: GenRe is a generative recourse model that jointly trains proximity, plausibility, and validity objectives, outperforming existing methods by avoiding separate optimization and using forward sampling for efficient recourse recommendations.


<details>
  <summary>Details</summary>
Motivation: Existing recourse methods optimize objectives separately during inference, leading to poor recommendations. GenRe addresses this by jointly training the objectives.

Method: GenRe uses a generative model trained with synthesized supervision to jointly optimize proximity, plausibility, and validity. It employs forward sampling for efficient recourse generation.

Result: GenRe outperforms state-of-the-art baselines, providing the best trade-off between cost, plausibility, and validity.

Conclusion: GenRe offers a robust and efficient solution for algorithmic recourse, improving performance and practicality over existing methods.

Abstract: Algorithmic Recourse provides recommendations to individuals who are
adversely impacted by automated model decisions, on how to alter their profiles
to achieve a favorable outcome. Effective recourse methods must balance three
conflicting goals: proximity to the original profile to minimize cost,
plausibility for realistic recourse, and validity to ensure the desired
outcome. We show that existing methods train for these objectives separately
and then search for recourse through a joint optimization over the recourse
goals during inference, leading to poor recourse recommendations. We introduce
GenRe, a generative recourse model designed to train the three recourse
objectives jointly. Training such generative models is non-trivial due to lack
of direct recourse supervision. We propose efficient ways to synthesize such
supervision and further show that GenRe's training leads to a consistent
estimator. Unlike most prior methods, that employ non-robust gradient descent
based search during inference, GenRe simply performs a forward sampling over
the generative model to produce minimum cost recourse, leading to superior
performance across multiple metrics. We also demonstrate GenRe provides the
best trade-off between cost, plausibility and validity, compared to
state-of-art baselines. Our code is available at:
https://github.com/prateekgargx/genre.

</details>


### [520] [Generalization Bounds and Stopping Rules for Learning with Self-Selected Data](https://arxiv.org/pdf/2505.07367)
*Julian Rodemann, James Bailie*

Main category: cs.LG

TL;DR: The paper introduces universal generalization bounds for reciprocal learning, covering various paradigms like active learning and semi-supervised learning, with no assumptions on data distribution.


<details>
  <summary>Details</summary>
Motivation: To understand how self-selected training data methods generalize, unifying paradigms like active learning under reciprocal learning.

Method: Uses covering numbers and Wasserstein ambiguity sets to prove generalization bounds, applicable to both convergent and finite iteration solutions.

Result: Provides anytime valid bounds, enabling stopping rules for practitioners to ensure out-of-sample performance.

Conclusion: Demonstrates the bounds' practicality through semi-supervised learning, a special case of reciprocal learning.

Abstract: Many learning paradigms self-select training data in light of previously
learned parameters. Examples include active learning, semi-supervised learning,
bandits, or boosting. Rodemann et al. (2024) unify them under the framework of
"reciprocal learning". In this article, we address the question of how well
these methods can generalize from their self-selected samples. In particular,
we prove universal generalization bounds for reciprocal learning using covering
numbers and Wasserstein ambiguity sets. Our results require no assumptions on
the distribution of self-selected data, only verifiable conditions on the
algorithms. We prove results for both convergent and finite iteration
solutions. The latter are anytime valid, thereby giving rise to stopping rules
for a practitioner seeking to guarantee the out-of-sample performance of their
reciprocal learning algorithm. Finally, we illustrate our bounds and stopping
rules for reciprocal learning's special case of semi-supervised learning.

</details>


### [521] [ICE-Pruning: An Iterative Cost-Efficient Pruning Pipeline for Deep Neural Networks](https://arxiv.org/pdf/2505.07411)
*Wenhao Hu, Paul Henderson, José Cano*

Main category: cs.LG

TL;DR: ICE-Pruning is a novel iterative pruning pipeline for DNNs that reduces fine-tuning time while maintaining accuracy, using automatic fine-tuning triggers, freezing strategies, and a pruning-aware learning rate scheduler.


<details>
  <summary>Details</summary>
Motivation: Existing pruning pipelines are computationally expensive due to repeated fine-tuning, prompting the need for a faster method without sacrificing accuracy.

Method: ICE-Pruning introduces three components: automatic fine-tuning triggers, freezing strategies for faster fine-tuning, and a pruning-aware learning rate scheduler, along with hyperparameter auto-tuning.

Result: ICE-Pruning accelerates pruning by up to 9.61x while maintaining similar accuracy to traditional methods.

Conclusion: ICE-Pruning offers a computationally efficient alternative to traditional pruning pipelines, significantly reducing time without compromising model accuracy.

Abstract: Pruning is a widely used method for compressing Deep Neural Networks (DNNs),
where less relevant parameters are removed from a DNN model to reduce its size.
However, removing parameters reduces model accuracy, so pruning is typically
combined with fine-tuning, and sometimes other operations such as rewinding
weights, to recover accuracy. A common approach is to repeatedly prune and then
fine-tune, with increasing amounts of model parameters being removed in each
step. While straightforward to implement, pruning pipelines that follow this
approach are computationally expensive due to the need for repeated
fine-tuning.
  In this paper we propose ICE-Pruning, an iterative pruning pipeline for DNNs
that significantly decreases the time required for pruning by reducing the
overall cost of fine-tuning, while maintaining a similar accuracy to existing
pruning pipelines. ICE-Pruning is based on three main components: i) an
automatic mechanism to determine after which pruning steps fine-tuning should
be performed; ii) a freezing strategy for faster fine-tuning in each pruning
step; and iii) a custom pruning-aware learning rate scheduler to further
improve the accuracy of each pruning step and reduce the overall time
consumption. We also propose an efficient auto-tuning stage for the
hyperparameters (e.g., freezing percentage) introduced by the three components.
We evaluate ICE-Pruning on several DNN models and datasets, showing that it can
accelerate pruning by up to 9.61x. Code is available at
https://github.com/gicLAB/ICE-Pruning

</details>


### [522] [Learning Penalty for Optimal Partitioning via Automatic Feature Extraction](https://arxiv.org/pdf/2505.07413)
*Tung L Nguyen, Toby Hocking*

Main category: cs.LG

TL;DR: A novel method using recurrent neural networks to learn penalty parameters for changepoint detection outperforms traditional manual feature extraction on genomic datasets.


<details>
  <summary>Details</summary>
Motivation: The challenge of determining the appropriate penalty parameter for changepoint detection in sequences, traditionally done manually, motivates the need for an automated and more accurate approach.

Method: The study employs recurrent neural networks to automatically learn the penalty parameter from raw sequences, eliminating manual feature extraction.

Result: Experiments on 20 genomic datasets show the proposed method achieves higher partitioning accuracy than traditional methods.

Conclusion: The novel approach of using neural networks for penalty parameter learning improves changepoint detection accuracy, offering a promising alternative to manual methods.

Abstract: Changepoint detection identifies significant shifts in data sequences, making
it important in areas like finance, genetics, and healthcare. The Optimal
Partitioning algorithms efficiently detect these changes, using a penalty
parameter to limit the changepoints number. Determining the appropriate value
for this penalty can be challenging. Traditionally, this process involved
manually extracting statistical features, such as sequence length or variance
to make the prediction. This study proposes a novel approach that uses
recurrent neural networks to learn this penalty directly from raw sequences by
automatically extracting features. Experiments conducted on 20 benchmark
genomic datasets show that this novel method surpasses traditional methods in
partitioning accuracy in most cases.

</details>


### [523] [LEAD: Iterative Data Selection for Efficient LLM Instruction Tuning](https://arxiv.org/pdf/2505.07437)
*Xiaotian Lin, Yanlin Qi, Yizhang Zhu, Themis Palpanas, Chengliang Chai, Nan Tang, Yuyu Luo*

Main category: cs.LG

TL;DR: LEAD is an efficient iterative data selection framework for instruction tuning, eliminating costly model inference by using Instance-Level Dynamic Uncertainty (IDU) and a two-stage selection strategy. It improves model performance and reduces training time.


<details>
  <summary>Details</summary>
Motivation: Existing iterative data selection methods for LLMs are computationally expensive due to repeated full-dataset model inference, creating efficiency bottlenecks.

Method: LEAD introduces IDU, combining training loss, gradient-based loss change approximation, and historical loss smoothing. It uses a two-stage coarse-to-fine selection strategy with a multi-armed bandit mechanism.

Result: LEAD outperforms state-of-the-art methods, improving model performance by 6.1%-10.8% with only 2.5% of training data and reducing training time by 5-10x.

Conclusion: LEAD offers a computationally efficient and effective solution for iterative data selection in instruction tuning, enhancing LLM performance and scalability.

Abstract: Instruction tuning has emerged as a critical paradigm for improving the
capabilities and alignment of large language models (LLMs). However, existing
iterative model-aware data selection methods incur significant computational
overhead, as they rely on repeatedly performing full-dataset model inference to
estimate sample utility for subsequent training iterations, creating a
fundamental efficiency bottleneck. In this paper, we propose LEAD, an efficient
iterative data selection framework that accurately estimates sample utility
entirely within the standard training loop, eliminating the need for costly
additional model inference. At its core, LEAD introduces Instance-Level Dynamic
Uncertainty (IDU), a theoretically grounded utility function combining
instantaneous training loss, gradient-based approximation of loss changes, and
exponential smoothing of historical loss signals. To further scale efficiently
to large datasets, LEAD employs a two-stage, coarse-to-fine selection strategy,
adaptively prioritizing informative clusters through a multi-armed bandit
mechanism, followed by precise fine-grained selection of high-utility samples
using IDU. Extensive experiments across four diverse benchmarks show that LEAD
significantly outperforms state-of-the-art methods, improving average model
performance by 6.1%-10.8% while using only 2.5% of the training data and
reducing overall training time by 5-10x.

</details>


### [524] [Unified Continuous Generative Models](https://arxiv.org/pdf/2505.07447)
*Peng Sun, Yi Jiang, Tao Lin*

Main category: cs.LG

TL;DR: A unified framework (UCGM) for training and sampling continuous generative models achieves SOTA performance, improving efficiency and results for both multi-step and few-step methods.


<details>
  <summary>Details</summary>
Motivation: Existing work treats multi-step and few-step generative models as separate paradigms, leading to fragmented methodologies.

Method: Introduces UCGM, a unified framework for training, sampling, and analyzing continuous generative models.

Result: UCGM achieves 1.30 FID in 20 steps (multi-step) and 1.42 FID in 2 steps (few-step) on ImageNet 256x256. Pre-trained model performance improves from 1.26 FID (250 steps) to 1.06 FID (40 steps).

Conclusion: UCGM unifies and optimizes generative model training and sampling, demonstrating superior performance and efficiency.

Abstract: Recent advances in continuous generative models, including multi-step
approaches like diffusion and flow-matching (typically requiring 8-1000
sampling steps) and few-step methods such as consistency models (typically 1-8
steps), have demonstrated impressive generative performance. However, existing
work often treats these approaches as distinct paradigms, resulting in separate
training and sampling methodologies. We introduce a unified framework for
training, sampling, and analyzing these models. Our implementation, the Unified
Continuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves
state-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a
675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID
in 20 steps and a few-step model reaching 1.42 FID in just 2 steps.
Additionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at
250 steps) improves performance to 1.06 FID in only 40 steps. Code is available
at: https://github.com/LINs-lab/UCGM.

</details>


### [525] [Prototype Augmented Hypernetworks for Continual Learning](https://arxiv.org/pdf/2505.07450)
*Neil De La Fuente, Maria Pilligua, Daniel Vidal, Albin Soutiff, Cecilia Curreli, Daniel Cremers, Andrey Barsky*

Main category: cs.LG

TL;DR: PAH is a framework using hypernetworks and task prototypes to prevent catastrophic forgetting in continual learning, achieving high accuracy with minimal forgetting.


<details>
  <summary>Details</summary>
Motivation: Address catastrophic forgetting in continual learning by dynamically generating task-specific classifiers and stabilizing feature representations.

Method: Uses Prototype-Augmented Hypernetworks (PAH) with cross-entropy and dual distillation losses (logit and prototype alignment).

Result: Achieves 74.5% and 63.7% accuracy on Split-CIFAR100 and TinyImageNet with only 1.7% and 4.4% forgetting, respectively.

Conclusion: PAH outperforms prior methods without needing stored samples or heads, demonstrating effective continual learning.

Abstract: Continual learning (CL) aims to learn a sequence of tasks without forgetting
prior knowledge, but gradient updates for a new task often overwrite the
weights learned earlier, causing catastrophic forgetting (CF). We propose
Prototype-Augmented Hypernetworks (PAH), a framework where a single
hypernetwork, conditioned on learnable task prototypes, dynamically generates
task-specific classifier heads on demand. To mitigate forgetting, PAH combines
cross-entropy with dual distillation losses, one to align logits and another to
align prototypes, ensuring stable feature representations across tasks.
Evaluations on Split-CIFAR100 and TinyImageNet demonstrate that PAH achieves
state-of-the-art performance, reaching 74.5 % and 63.7 % accuracy with only 1.7
% and 4.4 % forgetting, respectively, surpassing prior methods without storing
samples or heads.

</details>


### [526] [You Only Look One Step: Accelerating Backpropagation in Diffusion Sampling with Gradient Shortcuts](https://arxiv.org/pdf/2505.07477)
*Hongkun Dou, Zeyu Li, Xingyu Jiang, Hongjue Li, Lijun Yang, Wen Yao, Yue Deng*

Main category: cs.LG

TL;DR: The paper introduces Shortcut Diffusion Optimization (SDO), a method to efficiently guide diffusion models using differentiable metrics without full backpropagation, reducing computational costs by ~90%.


<details>
  <summary>Details</summary>
Motivation: Downstream tasks often require guiding generated content with differentiable metrics, but full backpropagation during generation is computationally expensive.

Method: SDO optimizes metrics by retaining the computational graph of only one step during generation, avoiding full backpropagation.

Result: SDO reduces computational costs by ~90% while maintaining performance, demonstrated on tasks like latent optimization and network fine-tuning.

Conclusion: SDO is a lightweight, high-performance alternative to full backpropagation for optimizing diffusion models.

Abstract: Diffusion models (DMs) have recently demonstrated remarkable success in
modeling large-scale data distributions. However, many downstream tasks require
guiding the generated content based on specific differentiable metrics,
typically necessitating backpropagation during the generation process. This
approach is computationally expensive, as generating with DMs often demands
tens to hundreds of recursive network calls, resulting in high memory usage and
significant time consumption. In this paper, we propose a more efficient
alternative that approaches the problem from the perspective of parallel
denoising. We show that full backpropagation throughout the entire generation
process is unnecessary. The downstream metrics can be optimized by retaining
the computational graph of only one step during generation, thus providing a
shortcut for gradient propagation. The resulting method, which we call Shortcut
Diffusion Optimization (SDO), is generic, high-performance, and computationally
lightweight, capable of optimizing all parameter types in diffusion sampling.
We demonstrate the effectiveness of SDO on several real-world tasks, including
controlling generation by optimizing latent and aligning the DMs by fine-tuning
network parameters. Compared to full backpropagation, our approach reduces
computational costs by $\sim 90\%$ while maintaining superior performance. Code
is available at https://github.com/deng-ai-lab/SDO.

</details>


### [527] [Identifying Causal Direction via Variational Bayesian Compression](https://arxiv.org/pdf/2505.07503)
*Quang-Duy Tran, Bao Duong, Phuoc Nguyen, Thin Nguyen*

Main category: cs.LG

TL;DR: The paper proposes using variational Bayesian learning of neural networks to improve cause-effect identification by enhancing model fitness and codelength succinctness, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of distinguishing cause and effect from observational data, leveraging the algorithmic Markov condition for better codelength approximation.

Method: Uses variational Bayesian learning of neural networks to interpret codelengths, improving fitness and succinctness without high computational complexity.

Result: Outperforms complexity-based and structural causal model regression-based approaches in synthetic and real-world benchmarks.

Conclusion: The proposed method effectively enhances cause-effect identification by balancing model fitness and computational efficiency.

Abstract: Telling apart the cause and effect between two random variables with purely
observational data is a challenging problem that finds applications in various
scientific disciplines. A key principle utilized in this task is the
algorithmic Markov condition, which postulates that the joint distribution,
when factorized according to the causal direction, yields a more succinct
codelength compared to the anti-causal direction. Previous approaches
approximate these codelengths by relying on simple functions or Gaussian
processes (GPs) with easily evaluable complexity, compromising between model
fitness and computational complexity. To overcome these limitations, we propose
leveraging the variational Bayesian learning of neural networks as an
interpretation of the codelengths. Consequently, we can enhance the model
fitness while promoting the succinctness of the codelengths, while avoiding the
significant computational complexity of the GP-based approaches. Extensive
experiments on both synthetic and real-world benchmarks in cause-effect
identification demonstrate the effectiveness of our proposed method, surpassing
the overall performance of related complexity-based and structural causal model
regression-based approaches.

</details>


### [528] [MedualTime: A Dual-Adapter Language Model for Medical Time Series-Text Multimodal Learning](https://arxiv.org/pdf/2406.06620)
*Jiexia Ye, Weiqi Zhang, Ziyue Li, Jia Li, Meng Zhao, Fugee Tsung*

Main category: cs.LG

TL;DR: The paper introduces MedualTime, a novel textual-temporal multimodal learning paradigm to address biases in existing LM approaches by allowing either modality to serve as primary, enhancing cross-modal interaction.


<details>
  <summary>Details</summary>
Motivation: Existing contrastive and prompt-based LM approaches in medical time series-text multimodal learning are biased, favoring time series over text, potentially missing critical text-based information.

Method: Proposes MedualTime, a dual-adapter LM model with lightweight adaptation tokens for high-level modality fusion, enabling efficient fine-tuning and adapter alignment.

Result: MedualTime achieves 8% accuracy and 12% F1 improvements in supervised settings and shows strong transferability in few-shot experiments.

Conclusion: The proposed paradigm effectively captures modality-specific information and fosters cross-modal interaction, outperforming existing methods.

Abstract: The recent rapid advancements in language models (LMs) have garnered
attention in medical time series-text multimodal learning. However, existing
contrastive learning-based and prompt-based LM approaches tend to be biased,
often assigning a primary role to time series modality while treating text
modality as secondary. We classify these approaches under a temporal-primary
paradigm, which may overlook the unique and critical task-relevant information
embedded in text modality like clinical reports, thus failing to fully leverage
mutual benefits and complementarity of different modalities. To fill this gap,
we propose a novel textual-temporal multimodal learning paradigm that enables
either modality to serve as the primary while being enhanced by the other,
thereby effectively capturing modality-specific information and fostering
cross-modal interaction. In specific, we design MedualTime, a language model
composed of dual adapters to implement temporal-primary and textual-primary
modeling simultaneously. Within each adapter, lightweight adaptation tokens are
injected into the top layers of LM to encourage high-level modality fusion. The
shared LM pipeline by dual adapters not only achieves adapter alignment but
also enables efficient fine-tuning, reducing computational resources.
Empirically, MedualTime demonstrates superior performance on medical data,
achieving notable improvements of 8% accuracy and 12% F1 in supervised
settings. Furthermore, MedualTime's transferability is validated by few-shot
label transfer experiments from coarse-grained to fine-grained medical data.
https://github.com/start2020/MedualTime

</details>


### [529] [EAGLE: Contrastive Learning for Efficient Graph Anomaly Detection](https://arxiv.org/pdf/2505.07508)
*Jing Ren, Mingliang Hou, Zhixuan Liu, Xiaomei Bai*

Main category: cs.LG

TL;DR: EAGLE is a deep learning-based model for efficient graph anomaly detection on heterogeneous graphs using contrastive learning, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for graph anomaly detection lack efficiency, especially for embedded devices, prompting the need for a more efficient solution.

Method: EAGLE uses contrastive learning to compare abnormal and normal nodes by their distances to local context, sampling instance pairs on meta path-level and employing a graph autoencoder for unsupervised node embedding learning, combined with a discriminator for anomaly scoring.

Result: EAGLE achieves superior performance over state-of-the-art methods on three heterogeneous network datasets.

Conclusion: EAGLE provides an efficient and effective solution for graph anomaly detection on heterogeneous graphs, addressing the limitations of existing methods.

Abstract: Graph anomaly detection is a popular and vital task in various real-world
scenarios, which has been studied for several decades. Recently, many studies
extending deep learning-based methods have shown preferable performance on
graph anomaly detection. However, existing methods are lack of efficiency that
is definitely necessary for embedded devices. Towards this end, we propose an
Efficient Anomaly detection model on heterogeneous Graphs via contrastive
LEarning (EAGLE) by contrasting abnormal nodes with normal ones in terms of
their distances to the local context. The proposed method first samples
instance pairs on meta path-level for contrastive learning. Then, a graph
autoencoder-based model is applied to learn informative node embeddings in an
unsupervised way, which will be further combined with the discriminator to
predict the anomaly scores of nodes. Experimental results show that EAGLE
outperforms the state-of-the-art methods on three heterogeneous network
datasets.

</details>


### [530] [Adaptive Latent-Space Constraints in Personalized FL](https://arxiv.org/pdf/2505.07525)
*Sana Ayromlou, D. B. Emerson*

Main category: cs.LG

TL;DR: The paper explores adaptive MMD measures in the Ditto framework for personalized federated learning (pFL), improving model performance, especially in tasks with feature heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Addressing statistical heterogeneity in federated learning (FL) by enhancing personalized FL (pFL) methods to combine global learning with local modeling.

Method: Investigates theoretically supported, adaptive MMD measures within the Ditto framework to improve pFL.

Result: Adaptive MMD measures significantly boost model performance, particularly in tasks with feature heterogeneity.

Conclusion: The findings advocate for tailored constraints in FL systems to handle various heterogeneity types, with broader applicability beyond Ditto.

Abstract: Federated learning (FL) has become an effective and widely used approach to
training deep learning models on decentralized datasets held by distinct
clients. FL also strengthens both security and privacy protections for training
data. Common challenges associated with statistical heterogeneity between
distributed datasets have spurred significant interest in personalized FL (pFL)
methods, where models combine aspects of global learning with local modeling
specific to each client's unique characteristics. In this work, the efficacy of
theoretically supported, adaptive MMD measures within the Ditto framework, a
state-of-the-art technique in pFL, are investigated. The use of such measures
significantly improves model performance across a variety of tasks, especially
those with pronounced feature heterogeneity. While the Ditto algorithm is
specifically considered, such measures are directly applicable to a number of
other pFL settings, and the results motivate the use of constraints tailored to
the various kinds of heterogeneity expected in FL systems.

</details>


### [531] [Kalman Filter Enhanced GRPO for Reinforcement Learning-Based Language Model Reasoning](https://arxiv.org/pdf/2505.07527)
*Hu Wang, Congbo Ma, Ian Reid, Mohammad Yaqub*

Main category: cs.LG

TL;DR: KRPO enhances GRPO by using Kalman filtering for adaptive advantage estimation, improving stability and performance in noisy reward environments.


<details>
  <summary>Details</summary>
Motivation: GRPO's naive batch mean baseline can introduce bias in noisy reward settings, prompting the need for a more adaptive method.

Method: KRPO employs lightweight Kalman filtering to dynamically estimate reward mean and variance, replacing GRPO's batch mean baseline.

Result: KRPO improves stability and performance over GRPO, especially in environments with highly dynamic rewards.

Conclusion: KRPO offers a simple, effective enhancement to GRPO, requiring no additional parameters while improving advantage estimation.

Abstract: Reward baseline is important for Reinforcement Learning (RL) algorithms to
reduce variance in policy gradient estimates. Recently, for language modeling,
Group Relative Policy Optimization (GRPO) is proposed to compute the advantage
for each output by subtracting the mean reward, as the baseline, for all
outputs in the group. However, it can lead to inaccurate advantage estimates in
environments with highly noisy rewards, potentially introducing bias. In this
work, we propose a model, called Kalman Filter Enhanced Group Relative Policy
Optimization (KRPO), by using lightweight Kalman filtering to dynamically
estimate the latent reward mean and variance. This filtering technique replaces
the naive batch mean baseline, enabling more adaptive advantage normalization.
Our method does not require additional learned parameters over GRPO. This
approach offers a simple yet effective way to incorporate multiple outputs of
GRPO into advantage estimation, improving policy optimization in settings where
highly dynamic reward signals are difficult to model for language models.
Through experiments and analyses, we show that using a more adaptive advantage
estimation model, KRPO can improve the stability and performance of GRPO. The
code is available at https://github.com/billhhh/KRPO_LLMs_RL

</details>


### [532] [Self-Data Distillation for Recovering Quality in Pruned Large Language Models](https://arxiv.org/pdf/2410.09982)
*Vithursan Thangarasa, Ganesh Venkatesh, Mike Lasby, Nish Sinnadurai, Sean Lie*

Main category: cs.LG

TL;DR: The paper proposes self-data distilled fine-tuning to mitigate quality degradation in pruned large language models, outperforming standard supervised fine-tuning and improving accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Large language models require compression for efficiency, but pruning and fine-tuning degrade quality. Addressing this degradation is crucial.

Method: Uses self-data distilled fine-tuning, leveraging the original model to generate a dataset preserving semantic richness and avoiding catastrophic forgetting.

Result: Improves accuracy by up to 8%, retains 91.2% of original accuracy when pruning, and reduces FLOPs by 16.3%. Enhances inference efficiency via speculative decoding.

Conclusion: Self-data distillation effectively balances model compression and quality retention, offering practical benefits for deployment.

Abstract: Large language models have driven significant progress in natural language
processing, but their deployment requires substantial compute and memory
resources. As models scale, compression techniques become essential for
balancing model quality with computational efficiency. Structured pruning,
which removes less critical components of the model, is a promising strategy
for reducing complexity. However, one-shot pruning often results in significant
quality degradation, particularly in tasks requiring multi-step reasoning. To
recover lost quality, supervised fine-tuning (SFT) is commonly applied, but it
can lead to catastrophic forgetting by shifting the model's learned data
distribution. Therefore, addressing the degradation from both pruning and SFT
is essential to preserve the original model's quality. In this work, we utilize
self-data distilled fine-tuning to address these challenges. Our approach
leverages the original, unpruned model to generate a distilled dataset that
preserves semantic richness and mitigates catastrophic forgetting by
maintaining alignment with the base model's knowledge. Empirically, we
demonstrate that self-data distillation consistently outperforms standard SFT,
improving average accuracy by up to 8% on the HuggingFace OpenLLM Leaderboard
v1. Specifically, when pruning six decoder blocks on Llama3.1-8B Instruct
(i.e., 32 to 26 layers, reducing the model size from 8.03B to 6.72B
parameters), our method retains 91.2% of the original model's accuracy compared
to 81.7% with SFT, while reducing real-world FLOPs by 16.3%. Furthermore,
combining self-data distilled models through model merging yields enhanced
quality retention. Additionally, leveraging these pruned models in speculative
decoding increases token acceptance rates, thereby improving inference
efficiency in applied settings.

</details>


### [533] [Noise Optimized Conditional Diffusion for Domain Adaptation](https://arxiv.org/pdf/2505.07548)
*Lingkun Luo, Shiqiang Hu, Liming Chen*

Main category: cs.LG

TL;DR: NOCDDA integrates conditional diffusion models with domain adaptation to improve pseudo-labeling by optimizing noise and enhancing cross-domain alignment.


<details>
  <summary>Details</summary>
Motivation: The scarcity of high-confidence pseudo-labeled target domain samples in UDA leads to inaccurate cross-domain alignment, causing adaptation failures.

Method: NOCDDA combines conditional diffusion models with DA, optimizing noise and refining sampling regions for better pseudo-label generation.

Result: Extensive experiments show NOCDDA outperforms 31 state-of-the-art methods across 5 datasets and 29 DA tasks.

Conclusion: NOCDDA effectively enhances cross-domain alignment and robustness in domain adaptation tasks.

Abstract: Pseudo-labeling is a cornerstone of Unsupervised Domain Adaptation (UDA), yet
the scarcity of High-Confidence Pseudo-Labeled Target Domain Samples
(\textbf{hcpl-tds}) often leads to inaccurate cross-domain statistical
alignment, causing DA failures. To address this challenge, we propose
\textbf{N}oise \textbf{O}ptimized \textbf{C}onditional \textbf{D}iffusion for
\textbf{D}omain \textbf{A}daptation (\textbf{NOCDDA}), which seamlessly
integrates the generative capabilities of conditional diffusion models with the
decision-making requirements of DA to achieve task-coupled optimization for
efficient adaptation. For robust cross-domain consistency, we modify the DA
classifier to align with the conditional diffusion classifier within a unified
optimization framework, enabling forward training on noise-varying cross-domain
samples. Furthermore, we argue that the conventional \( \mathcal{N}(\mathbf{0},
\mathbf{I}) \) initialization in diffusion models often generates
class-confused hcpl-tds, compromising discriminative DA. To resolve this, we
introduce a class-aware noise optimization strategy that refines sampling
regions for reverse class-specific hcpl-tds generation, effectively enhancing
cross-domain alignment. Extensive experiments across 5 benchmark datasets and
29 DA tasks demonstrate significant performance gains of \textbf{NOCDDA} over
31 state-of-the-art methods, validating its robustness and effectiveness.

</details>


### [534] [Similarity-Dissimilarity Loss for Multi-label Supervised Contrastive Learning](https://arxiv.org/pdf/2410.13439)
*Guangming Huang, Yunfei Long, Cunjin Luo*

Main category: cs.LG

TL;DR: The paper introduces a method for multi-label supervised contrastive learning (MSCL) by defining multi-label relations, proposing a dynamic loss function, and validating its effectiveness across domains.


<details>
  <summary>Details</summary>
Motivation: Addressing the ambiguity in identifying positive samples and formulating contrastive loss functions in multi-label scenarios.

Method: Defines five multi-label relations, introduces a Similarity-Dissimilarity Loss for dynamic re-weighting, and provides theoretical proofs.

Result: Outperforms baselines in image, text, and medical domains, demonstrating effectiveness and robustness.

Conclusion: The proposed method successfully addresses challenges in MSCL and shows consistent performance improvements.

Abstract: Supervised contrastive learning has achieved remarkable success by leveraging
label information; however, determining positive samples in multi-label
scenarios remains a critical challenge. In multi-label supervised contrastive
learning (MSCL), multi-label relations are not yet fully defined, leading to
ambiguity in identifying positive samples and formulating contrastive loss
functions to construct the representation space. To address these challenges,
we: (i) first define five distinct multi-label relations in MSCL to
systematically identify positive samples, (ii) introduce a novel
Similarity-Dissimilarity Loss that dynamically re-weights samples through
computing the similarity and dissimilarity factors between positive samples and
given anchors based on multi-label relations, and (iii) further provide
theoretical grounded proofs for our method through rigorous mathematical
analysis that supports the formulation and effectiveness of the proposed loss
function. We conduct the experiments across both image and text modalities, and
extend the evaluation to medical domain. The results demonstrate that our
method consistently outperforms baselines in a comprehensive evaluation,
confirming its effectiveness and robustness. Code is available at:
https://github.com/guangminghuang/similarity-dissimilarity-loss.

</details>


### [535] [Injecting Knowledge Graphs into Large Language Models](https://arxiv.org/pdf/2505.07554)
*Erica Coppolillo*

Main category: cs.LG

TL;DR: A method to integrate Knowledge Graphs (KGs) into Large Language Models (LLMs) using Knowledge Graph Embedding (KGE) models, improving reasoning performance efficiently.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of integrating structured KG knowledge into LLMs without losing structural fidelity or incurring high computational costs.

Method: Leverages KGE models to integrate graph embeddings into LLM inputs, enabling graph-aware reasoning in a model-agnostic and resource-efficient way.

Result: Improves reasoning performance over baselines and achieves the best accuracy-efficiency trade-off compared to state-of-the-art LLMs.

Conclusion: The approach successfully integrates KGs into LLMs, enhancing reasoning while maintaining efficiency and compatibility.

Abstract: Integrating structured knowledge from Knowledge Graphs (KGs) into Large
Language Models (LLMs) remains a key challenge for symbolic reasoning. Existing
methods mainly rely on prompt engineering or fine-tuning, which lose structural
fidelity or incur high computational costs. Building on recent encoding
techniques which integrate graph embeddings within the LLM input as tokens, we
extend this paradigm to the KG domain by leveraging Knowledge Graph Embedding
(KGE) models, thus enabling graph-aware reasoning. Our approach is
model-agnostic, resource-efficient, and compatible with any LLMs. Extensive
experimentation on synthetic and real-world datasets shows that our method
improves reasoning performance over established baselines, further achieving
the best trade-off in terms of accuracy and efficiency against state-of-the-art
LLMs.

</details>


### [536] [Personalized Federated Learning under Model Dissimilarity Constraints](https://arxiv.org/pdf/2505.07575)
*Samuel Erickson, Mikael Johansson*

Main category: cs.LG

TL;DR: KARULA is a regularized strategy for personalized federated learning that addresses statistical heterogeneity by constraining pairwise model dissimilarities based on distribution differences.


<details>
  <summary>Details</summary>
Motivation: The challenge of statistical heterogeneity among clients in federated learning motivates the need for a strategy that adapts to complex interrelations, which existing clustered approaches fail to capture.

Method: KARULA uses a surrogate for the 1-Wasserstein distance to constrain pairwise model dissimilarities and proposes an inexact projected stochastic gradient algorithm for solving the constrained problem.

Result: Theoretical analysis shows convergence to a neighborhood of a stationary point with rate O(1/K) for smooth, possibly non-convex losses. Empirical results demonstrate effectiveness on synthetic and real federated datasets.

Conclusion: KARULA effectively addresses statistical heterogeneity in federated learning, offering a scalable and adaptive solution with theoretical guarantees.

Abstract: One of the defining challenges in federated learning is that of statistical
heterogeneity among clients. We address this problem with KARULA, a regularized
strategy for personalized federated learning, which constrains the pairwise
model dissimilarities between clients based on the difference in their
distributions, as measured by a surrogate for the 1-Wasserstein distance
adapted for the federated setting. This allows the strategy to adapt to highly
complex interrelations between clients, that e.g., clustered approaches fail to
capture. We propose an inexact projected stochastic gradient algorithm to solve
the constrained problem that the strategy defines, and show theoretically that
it converges with smooth, possibly non-convex losses to a neighborhood of a
stationary point with rate O(1/K). We demonstrate the effectiveness of KARULA
on synthetic and real federated data sets.

</details>


### [537] [Training and Evaluating with Human Label Variation: An Empirical Study](https://arxiv.org/pdf/2502.01891)
*Kemal Kurniawan, Meladel Mistica, Timothy Baldwin, Jey Han Lau*

Main category: cs.LG

TL;DR: The paper addresses human label variation (HLV) by proposing new fuzzy set-based evaluation metrics and testing them as training objectives. It finds disaggregated annotations or soft labels perform best, and the proposed soft metric aligns well with human preference.


<details>
  <summary>Details</summary>
Motivation: HLV challenges the single ground truth assumption, but existing methods and metrics lack clarity on performance in different settings.

Method: Proposes new differentiable metrics using fuzzy set theory, tests them as training objectives, and evaluates 14 training methods and 6 metrics across 6 HLV datasets.

Result: Disaggregated annotations or soft labels outperform training with differentiable metrics. The proposed soft metric is more interpretable and correlates best with human preference.

Conclusion: Soft labels or disaggregated annotations are optimal for HLV, and the proposed soft metric offers interpretability and human alignment.

Abstract: Human label variation (HLV) challenges the standard assumption that a
labelled instance has a single ground truth, instead embracing the natural
variation in human annotation to train and evaluate models. While various
training methods and metrics for HLV have been proposed, it is still unclear
which methods and metrics perform best in what settings. We propose new
evaluation metrics for HLV leveraging fuzzy set theory. Since these new
proposed metrics are differentiable, we then in turn experiment with employing
these metrics as training objectives. We conduct an extensive study over 6 HLV
datasets testing 14 training methods and 6 evaluation metrics. We find that
training on either disaggregated annotations or soft labels performs best
across metrics, outperforming training using the proposed training objectives
with differentiable metrics. We also show that our proposed soft metric is more
interpretable and correlates best with human preference.

</details>


### [538] [Trial and Trust: Addressing Byzantine Attacks with Comprehensive Defense Strategy](https://arxiv.org/pdf/2505.07614)
*Gleb Molodtsov, Daniil Medyakov, Sergey Skorik, Nikolas Khachaturov, Shahane Tigranyan, Vladimir Aletov, Aram Avetisyan, Martin Takáč, Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: The paper proposes a method to counter Byzantine attacks in federated/distributed ML by combining trust scores and trial functions, ensuring robustness even with majority malicious nodes.


<details>
  <summary>Details</summary>
Motivation: Addressing vulnerabilities in federated/distributed ML setups to Byzantine attacks, where adversarial updates disrupt convergence.

Method: Combines trust scores and trial functions to dynamically filter outliers, adaptable to scaled methods (Adam, RMSProp) and practical scenarios (local training, partial participation).

Result: Validated robustness via experiments on synthetic and real ECG data; theoretical analysis shows convergence guarantees comparable to non-Byzantine settings.

Conclusion: The method effectively mitigates Byzantine attacks, maintaining performance in adversarial settings and practical scenarios.

Abstract: Recent advancements in machine learning have improved performance while also
increasing computational demands. While federated and distributed setups
address these issues, their structure is vulnerable to malicious influences. In
this paper, we address a specific threat, Byzantine attacks, where compromised
clients inject adversarial updates to derail global convergence. We combine the
trust scores concept with trial function methodology to dynamically filter
outliers. Our methods address the critical limitations of previous approaches,
allowing functionality even when Byzantine nodes are in the majority. Moreover,
our algorithms adapt to widely used scaled methods like Adam and RMSProp, as
well as practical scenarios, including local training and partial
participation. We validate the robustness of our methods by conducting
extensive experiments on both synthetic and real ECG data collected from
medical institutions. Furthermore, we provide a broad theoretical analysis of
our algorithms and their extensions to aforementioned practical setups. The
convergence guarantees of our methods are comparable to those of classical
algorithms developed without Byzantine interference.

</details>


### [539] [Enhancing Federated Learning with Kolmogorov-Arnold Networks: A Comparative Study Across Diverse Aggregation Strategies](https://arxiv.org/pdf/2505.07629)
*Yizhou Ma, Zhuoqin Yang, Luis-Daniel Ibáñez*

Main category: cs.LG

TL;DR: KANs outperform MLPs in federated learning, offering better accuracy, stability, and efficiency, especially in non-IID settings.


<details>
  <summary>Details</summary>
Motivation: Traditional MLPs struggle with complex nonlinear relationships in federated learning, prompting exploration of KANs as a superior alternative.

Method: Comparison of KANs and MLPs in FL frameworks across diverse datasets, evaluating accuracy, stability, convergence, and robustness under varying conditions.

Result: KANs consistently outperform MLPs in accuracy, stability, and convergence efficiency, requiring fewer communication rounds and handling non-IID data better.

Conclusion: KANs are a robust, scalable alternative to MLPs in FL, suitable for decentralized and privacy-preserving environments.

Abstract: Multilayer Perceptron (MLP), as a simple yet powerful model, continues to be
widely used in classification and regression tasks. However, traditional MLPs
often struggle to efficiently capture nonlinear relationships in load data when
dealing with complex datasets. Kolmogorov-Arnold Networks (KAN), inspired by
the Kolmogorov-Arnold representation theorem, have shown promising capabilities
in modeling complex nonlinear relationships. In this study, we explore the
performance of KANs within federated learning (FL) frameworks and compare them
to traditional Multilayer Perceptrons. Our experiments, conducted across four
diverse datasets demonstrate that KANs consistently outperform MLPs in terms of
accuracy, stability, and convergence efficiency. KANs exhibit remarkable
robustness under varying client numbers and non-IID data distributions,
maintaining superior performance even as client heterogeneity increases.
Notably, KANs require fewer communication rounds to converge compared to MLPs,
highlighting their efficiency in FL scenarios. Additionally, we evaluate
multiple parameter aggregation strategies, with trimmed mean and FedProx
emerging as the most effective for optimizing KAN performance. These findings
establish KANs as a robust and scalable alternative to MLPs for federated
learning tasks, paving the way for their application in decentralized and
privacy-preserving environments.

</details>


### [540] [Generating Skyline Explanations for Graph Neural Networks](https://arxiv.org/pdf/2505.07635)
*Dazhuo Qiu, Haolai Che, Arijit Khan, Yinghui Wu*

Main category: cs.LG

TL;DR: The paper introduces 'skyline explanation,' a novel GNN explanation method that optimizes multiple explainability measures simultaneously, addressing biases in single-measure approaches.


<details>
  <summary>Details</summary>
Motivation: Existing GNN explanation methods focus on single measures (e.g., fidelity), leading to biased explanations. The paper aims to provide comprehensive explanations by optimizing multiple measures.

Method: The approach formulates skyline explanation as a multi-objective optimization problem, designs efficient algorithms (onion-peeling and diversification), and ensures provable quality guarantees.

Result: Empirical tests on real-world graphs confirm the method's effectiveness, efficiency, and scalability.

Conclusion: Skyline explanation offers a more balanced and comprehensive way to explain GNN outputs by leveraging multi-objective optimization.

Abstract: This paper proposes a novel approach to generate subgraph explanations for
graph neural networks GNNs that simultaneously optimize multiple measures for
explainability. Existing GNN explanation methods often compute subgraphs
(called ``explanatory subgraphs'') that optimize a pre-defined, single
explainability measure, such as fidelity or conciseness. This can lead to
biased explanations that cannot provide a comprehensive explanation to clarify
the output of GNN models. We introduce skyline explanation, a GNN explanation
paradigm that aims to identify k explanatory subgraphs by simultaneously
optimizing multiple explainability measures. (1) We formulate skyline
explanation generation as a multi-objective optimization problem, and pursue
explanations that approximate a skyline set of explanatory subgraphs. We show
the hardness for skyline explanation generation. (2) We design efficient
algorithms with an onion-peeling approach that strategically removes edges from
neighbors of nodes of interests, and incrementally improves explanations as it
explores an interpretation domain, with provable quality guarantees. (3) We
further develop an algorithm to diversify explanations to provide more
comprehensive perspectives. Using real-world graphs, we empirically verify the
effectiveness, efficiency, and scalability of our algorithms.

</details>


### [541] [I Predict Therefore I Am: Is Next Token Prediction Enough to Learn Human-Interpretable Concepts from Data?](https://arxiv.org/pdf/2503.08980)
*Yuhang Liu, Dong Gong, Yichao Cai, Erdun Gao, Zhen Zhang, Biwei Huang, Mingming Gong, Anton van den Hengel, Javen Qinfeng Shi*

Main category: cs.LG

TL;DR: The paper introduces a generative model to analyze LLMs, showing their representations align with latent concepts, and proposes a structured sparse autoencoder for evaluation.


<details>
  <summary>Details</summary>
Motivation: To clarify whether LLMs exhibit intelligence or just manipulate data, and to understand their underlying generative factors.

Method: A novel generative model using latent discrete variables, validated with identifiability results and empirical tests on LLMs like Pythia, Llama, and DeepSeek.

Result: LLMs' representations approximate latent concept probabilities, supporting the linear representation hypothesis. The structured sparse autoencoder proves effective.

Conclusion: The study provides theoretical and empirical evidence that LLMs capture generative factors, offering a framework for understanding and evaluating their representations.

Abstract: The remarkable achievements of large language models (LLMs) have led many to
conclude that they exhibit a form of intelligence. This is as opposed to
explanations of their capabilities based on their ability to perform relatively
simple manipulations of vast volumes of data. To illuminate the distinction
between these explanations, we introduce a novel generative model that
generates tokens on the basis of human-interpretable concepts represented as
latent discrete variables. Under mild conditions, even when the mapping from
the latent space to the observed space is non-invertible, we establish an
identifiability result, i.e., the representations learned by LLMs through
next-token prediction can be approximately modeled as the logarithm of the
posterior probabilities of these latent discrete concepts given input context,
up to an invertible linear transformation. This theoretical finding not only
provides evidence that LLMs capture underlying generative factors, but also
provide a unified prospective for understanding of the linear representation
hypothesis. Taking this a step further, our finding motivates a reliable
evaluation of sparse autoencoders by treating the performance of supervised
concept extractors as an upper bound. Pushing this idea even further, it
inspires a structural variant that enforces dependence among latent concepts in
addition to promoting sparsity. Empirically, we validate our theoretical
results through evaluations on both simulation data and the Pythia, Llama, and
DeepSeek model families, and demonstrate the effectiveness of our structured
sparse autoencoder.

</details>


### [542] [Simple Semi-supervised Knowledge Distillation from Vision-Language Models via $\mathbf{\texttt{D}}$ual-$\mathbf{\texttt{H}}$ead $\mathbf{\texttt{O}}$ptimization](https://arxiv.org/pdf/2505.07675)
*Seongjae Kang, Dong Bok Lee, Hyungjoon Jang, Sung Ju Hwang*

Main category: cs.LG

TL;DR: DHO is a simple yet effective KD framework for transferring VLM knowledge to compact models, outperforming baselines with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Deploying large VLMs in resource-constrained environments is challenging; existing KD methods are complex and computationally heavy.

Method: Introduces dual prediction heads for learning from labeled data and teacher predictions, combining outputs linearly during inference.

Result: DHO outperforms baselines, achieving SOTA on ImageNet with 3% and 0.1% accuracy improvements for 1% and 10% labeled data.

Conclusion: DHO effectively mitigates gradient conflicts, enabling efficient knowledge transfer and superior performance.

Abstract: Vision-language models (VLMs) have achieved remarkable success across diverse
tasks by leveraging rich textual information with minimal labeled data.
However, deploying such large models remains challenging, particularly in
resource-constrained environments. Knowledge distillation (KD) offers a
well-established solution to this problem; however, recent KD approaches from
VLMs often involve multi-stage training or additional tuning, increasing
computational overhead and optimization complexity. In this paper, we propose
$\mathbf{\texttt{D}}$ual-$\mathbf{\texttt{H}}$ead
$\mathbf{\texttt{O}}$ptimization ($\mathbf{\texttt{DHO}}$) -- a simple yet
effective KD framework that transfers knowledge from VLMs to compact,
task-specific models in semi-supervised settings. Specifically, we introduce
dual prediction heads that independently learn from labeled data and teacher
predictions, and propose to linearly combine their outputs during inference. We
observe that $\texttt{DHO}$ mitigates gradient conflicts between supervised and
distillation signals, enabling more effective feature learning than single-head
KD baselines. As a result, extensive experiments show that $\texttt{DHO}$
consistently outperforms baselines across multiple domains and fine-grained
datasets. Notably, on ImageNet, it achieves state-of-the-art performance,
improving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively,
while using fewer parameters.

</details>


### [543] [Joint Graph Convolution and Sequential Modeling for Scalable Network Traffic Estimation](https://arxiv.org/pdf/2505.07674)
*Nan Jiang, Wenxuan Zhu, Xu Han, Weiqiang Huang, Yumeng Sun*

Main category: cs.LG

TL;DR: A spatiotemporal model combining GCN and GRU for network traffic prediction, validated on the Abilene dataset, outperforms other methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of predicting network traffic in complex topological environments.

Method: Integrates Graph Convolutional Networks (GCN) for spatial dependencies and Gated Recurrent Units (GRU) for temporal evolution.

Result: Superior performance in traffic forecasting, validated through experiments and ablation studies.

Conclusion: The model is robust, stable, and generalizes well in complex scenarios.

Abstract: This study focuses on the challenge of predicting network traffic within
complex topological environments. It introduces a spatiotemporal modeling
approach that integrates Graph Convolutional Networks (GCN) with Gated
Recurrent Units (GRU). The GCN component captures spatial dependencies among
network nodes, while the GRU component models the temporal evolution of traffic
data. This combination allows for precise forecasting of future traffic
patterns. The effectiveness of the proposed model is validated through
comprehensive experiments on the real-world Abilene network traffic dataset.
The model is benchmarked against several popular deep learning methods.
Furthermore, a set of ablation experiments is conducted to examine the
influence of various components on performance, including changes in the number
of graph convolution layers, different temporal modeling strategies, and
methods for constructing the adjacency matrix. Results indicate that the
proposed approach achieves superior performance across multiple metrics,
demonstrating robust stability and strong generalization capabilities in
complex network traffic forecasting scenarios.

</details>


### [544] [Multimodal Survival Modeling in the Age of Foundation Models](https://arxiv.org/pdf/2505.07683)
*Steven Song, Morgan Borjigin-Wang, Irene Madejski, Robert L. Grossman*

Main category: cs.LG

TL;DR: The paper explores using foundation models (FMs) for zero-shot embeddings in multimodal survival prediction, outperforming unimodal models by leveraging TCGA's pathology reports.


<details>
  <summary>Details</summary>
Motivation: To modernize survival modeling by utilizing underused free-text pathology reports in TCGA and leveraging FMs for feature extraction.

Method: Training classical, multimodal survival models using zero-shot embeddings from FMs, including pathology reports, and evaluating text summarization and hallucination effects.

Result: Multimodal fusion with FM-derived embeddings outperforms unimodal models, demonstrating the additive benefit of pathology report text.

Conclusion: The study successfully modernizes survival modeling by integrating FMs and pathology report data, highlighting their untapped potential.

Abstract: The Cancer Genome Atlas (TCGA) has enabled novel discoveries and served as a
large-scale reference through its harmonized genomics, clinical, and image
data. Prior studies have trained bespoke cancer survival prediction models from
unimodal or multimodal TCGA data. A modern paradigm in biomedical deep learning
is the development of foundation models (FMs) to derive meaningful feature
embeddings, agnostic to a specific modeling task. Biomedical text especially
has seen growing development of FMs. While TCGA contains free-text data as
pathology reports, these have been historically underutilized. Here, we
investigate the feasibility of training classical, multimodal survival models
over zero-shot embeddings extracted by FMs. We show the ease and additive
effect of multimodal fusion, outperforming unimodal models. We demonstrate the
benefit of including pathology report text and rigorously evaluate the effect
of model-based text summarization and hallucination. Overall, we modernize
survival modeling by leveraging FMs and information extraction from pathology
reports.

</details>


### [545] [SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in Large Language Models](https://arxiv.org/pdf/2505.07680)
*Hang Wu, Jianian Zhu, Yinghui Li, Haojie Wang, Biao Hou, Jidong Zhai*

Main category: cs.LG

TL;DR: The paper introduces \systemname{}, a framework for adaptive routing in LLM inference using multi-level speculative decoding to balance quality and cost dynamically.


<details>
  <summary>Details</summary>
Motivation: Existing LLM serving strategies lack adaptability to varying request complexities and system performance, leading to inefficiencies.

Method: \systemname{} employs adaptive model chain scheduling, multi-level collaborative verification, and synchronized state management to optimize inference paths.

Result: Preliminary experiments show the framework's effectiveness in dynamically optimizing LLM inference.

Conclusion: \systemname{} addresses the limitations of static approaches, offering a scalable and efficient solution for LLM serving.

Abstract: Large Language Models (LLMs) present a critical trade-off between inference
quality and computational cost: larger models offer superior capabilities but
incur significant latency, while smaller models are faster but less powerful.
Existing serving strategies often employ fixed model scales or static two-stage
speculative decoding, failing to dynamically adapt to the varying complexities
of user requests or fluctuations in system performance. This paper introduces
\systemname{}, a novel framework that reimagines LLM inference as an adaptive
routing problem solved through multi-level speculative decoding. \systemname{}
dynamically constructs and optimizes inference "paths" (chains of models) based
on real-time feedback, addressing the limitations of static approaches. Our
contributions are threefold: (1) An \textbf{adaptive model chain scheduling}
mechanism that leverages performance profiling (execution times) and predictive
similarity metrics (derived from token distribution divergence) to continuously
select the optimal sequence of draft and verifier models, minimizing predicted
latency per generated token. (2) A \textbf{multi-level collaborative
verification} framework where intermediate models within the selected chain can
validate speculative tokens, reducing the verification burden on the final,
most powerful target model. (3) A \textbf{synchronized state management} system
providing efficient, consistent KV cache handling across heterogeneous models
in the chain, including precise, low-overhead rollbacks tailored for
asynchronous batch processing inherent in multi-level speculation. Preliminary
experiments demonstrate the validity of our method.

</details>


### [546] [4TaStiC: Time and trend traveling time series clustering for classifying long-term type 2 diabetes patients](https://arxiv.org/pdf/2505.07702)
*Onthada Preedasawakul, Nathakhun Wiroonsri*

Main category: cs.LG

TL;DR: A new clustering algorithm, 4TaStiC, is introduced to group diabetes patients based on time series lab data, addressing challenges like irregular visits and combining level and trend differences. It outperforms existing methods and aids clinical decisions.


<details>
  <summary>Details</summary>
Motivation: Diabetes patients' lab data is irregular and complex, making clustering challenging. Existing methods fail to account for both level and trend differences in time series data.

Method: 4TaStiC combines Euclidean and Pearson correlation metrics for dissimilarity measurement, tested on artificial datasets and applied to 1,989 diabetes patients.

Result: 4TaStiC outperformed seven existing methods on targeted datasets and effectively clustered patients into groups with clear clinical characteristics.

Conclusion: 4TaStiC is effective for clustering irregular time series data, aiding clinical decisions, and has potential applications beyond medicine.

Abstract: Diabetes is one of the most prevalent diseases worldwide, characterized by
persistently high blood sugar levels, capable of damaging various internal
organs and systems. Diabetes patients require routine check-ups, resulting in a
time series of laboratory records, such as hemoglobin A1c, which reflects each
patient's health behavior over time and informs their doctor's recommendations.
Clustering patients into groups based on their entire time series data assists
doctors in making recommendations and choosing treatments without the need to
review all records. However, time series clustering of this type of dataset
introduces some challenges; patients visit their doctors at different time
points, making it difficult to capture and match trends, peaks, and patterns.
Additionally, two aspects must be considered: differences in the levels of
laboratory results and differences in trends and patterns. To address these
challenges, we introduce a new clustering algorithm called Time and Trend
Traveling Time Series Clustering (4TaStiC), using a base dissimilarity measure
combined with Euclidean and Pearson correlation metrics. We evaluated this
algorithm on artificial datasets, comparing its performance with that of seven
existing methods. The results show that 4TaStiC outperformed the other methods
on the targeted datasets. Finally, we applied 4TaStiC to cluster a cohort of
1,989 type 2 diabetes patients at Siriraj Hospital. Each group of patients
exhibits clear characteristics that will benefit doctors in making efficient
clinical decisions. Furthermore, the proposed algorithm can be applied to
contexts outside the medical field.

</details>


### [547] [Assessing the Chemical Intelligence of Large Language Models](https://arxiv.org/pdf/2505.07735)
*Nicholas T. Runcie, Charlotte M. Deane, Fergus Imrie*

Main category: cs.LG

TL;DR: Reasoning models like OpenAI's o3-mini outperform non-reasoning models (e.g., GPT-4o) in chemistry tasks, achieving 28%-59% accuracy on the ChemIQ benchmark, which tests organic chemistry concepts through short-answer responses.


<details>
  <summary>Details</summary>
Motivation: To evaluate the capability of reasoning models in performing advanced chemistry tasks without external tools, addressing a gap in benchmarks that rely on multiple-choice formats.

Method: Created the ChemIQ benchmark with 796 questions on organic chemistry, requiring short-answer responses. Tested reasoning models (e.g., o3-mini) and non-reasoning models (e.g., GPT-4o).

Result: Reasoning models achieved 28%-59% accuracy, significantly outperforming GPT-4o (7%). They excelled in tasks like SMILES-to-IUPAC conversion and NMR data interpretation.

Conclusion: Latest reasoning models exhibit advanced chemical reasoning abilities, mirroring human chemist processes, and show promise for real-world applications.

Abstract: Large Language Models are versatile, general-purpose tools with a wide range
of applications. Recently, the advent of "reasoning models" has led to
substantial improvements in their abilities in advanced problem-solving domains
such as mathematics and software engineering. In this work, we assessed the
ability of reasoning models to directly perform chemistry tasks, without any
assistance from external tools. We created a novel benchmark, called ChemIQ,
which consists of 796 questions assessing core concepts in organic chemistry,
focused on molecular comprehension and chemical reasoning. Unlike previous
benchmarks, which primarily use multiple choice formats, our approach requires
models to construct short-answer responses, more closely reflecting real-world
applications. The reasoning models, exemplified by OpenAI's o3-mini, correctly
answered 28%-59% of questions depending on the reasoning level used, with
higher reasoning levels significantly increasing performance on all tasks.
These models substantially outperformed the non-reasoning model, GPT-4o, which
achieved only 7% accuracy. We found that Large Language Models can now convert
SMILES strings to IUPAC names, a task earlier models were unable to perform.
Additionally, we show that the latest reasoning models can elucidate structures
from 1H and 13C NMR data, correctly generating SMILES strings for 74% of
molecules containing up to 10 heavy atoms, and in one case solving a structure
comprising 21 heavy atoms. For each task, we found evidence that the reasoning
process mirrors that of a human chemist. Our results demonstrate that the
latest reasoning models have the ability to perform advanced chemical
reasoning.

</details>


### [548] [The Pitfalls of Benchmarking in Algorithm Selection: What We Are Getting Wrong](https://arxiv.org/pdf/2505.07750)
*Gašper Petelin, Gjorgjina Cenikj*

Main category: cs.LG

TL;DR: The paper critiques common evaluation methods for algorithm selection meta-models in black-box optimization, highlighting flaws in 'leave-instance-out' techniques and scale-sensitive metrics.


<details>
  <summary>Details</summary>
Motivation: To address methodological issues in evaluating algorithm selection approaches, which can mislead researchers and skew results.

Method: Identifies flaws in 'leave-instance-out' evaluation and analyzes the impact of scale-sensitive metrics on meta-model performance.

Result: Non-informative features and meta-models can achieve high accuracy, and scale-sensitive metrics may falsely inflate performance assessments.

Conclusion: Careful evaluation methodologies are crucial to avoid misleading results and ensure valid assessments of algorithm selection approaches.

Abstract: Algorithm selection, aiming to identify the best algorithm for a given
problem, plays a pivotal role in continuous black-box optimization. A common
approach involves representing optimization functions using a set of features,
which are then used to train a machine learning meta-model for selecting
suitable algorithms. Various approaches have demonstrated the effectiveness of
these algorithm selection meta-models. However, not all evaluation approaches
are equally valid for assessing the performance of meta-models. We highlight
methodological issues that frequently occur in the community and should be
addressed when evaluating algorithm selection approaches. First, we identify
flaws with the "leave-instance-out" evaluation technique. We show that
non-informative features and meta-models can achieve high accuracy, which
should not be the case with a well-designed evaluation framework. Second, we
demonstrate that measuring the performance of optimization algorithms with
metrics sensitive to the scale of the objective function requires careful
consideration of how this impacts the construction of the meta-model, its
predictions, and the model's error. Such metrics can falsely present overly
optimistic performance assessments of the meta-models. This paper emphasizes
the importance of careful evaluation, as loosely defined methodologies can
mislead researchers, divert efforts, and introduce noise into the field

</details>


### [549] [Overflow Prevention Enhances Long-Context Recurrent LLMs](https://arxiv.org/pdf/2505.07793)
*Assaf Ben-Kish, Itamar Zimerman, M. Jehanzeb Mirza, James Glass, Leonid Karlinsky, Raja Giryes*

Main category: cs.LG

TL;DR: A study on recurrent sub-quadratic LLMs reveals underutilization of long contexts despite training. A chunk-based inference method improves performance significantly, even outperforming Transformers in some cases.


<details>
  <summary>Details</summary>
Motivation: To investigate how fixed-size recurrent memory in large long-context models affects performance and whether they truly exploit long-range dependencies.

Method: A chunk-based inference procedure that processes only the most relevant input portions, tested on models like Falcon3-Mamba-Inst-7B and RecurrentGemma-IT-9B.

Result: Performance improvements of 14% to 51% on various models, with state-of-the-art results on LongBench v2. The method outperforms even in tasks assumed to need cross-context relations.

Conclusion: Recurrent models may not fully exploit long-range dependencies, as a simple chunk-based approach yields better performance, raising questions about their design.

Abstract: A recent trend in LLMs is developing recurrent sub-quadratic models that
improve long-context processing efficiency. We investigate leading large
long-context models, focusing on how their fixed-size recurrent memory affects
their performance. Our experiments reveal that, even when these models are
trained for extended contexts, their use of long contexts remains
underutilized. Specifically, we demonstrate that a chunk-based inference
procedure, which identifies and processes only the most relevant portion of the
input can mitigate recurrent memory failures and be effective for many
long-context tasks: On LongBench, our method improves the overall performance
of Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%,
RecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this
simple approach also leads to state-of-the-art results in the challenging
LongBench v2 benchmark, showing competitive performance with equivalent size
Transformers. Furthermore, our findings raise questions about whether recurrent
models genuinely exploit long-range dependencies, as our single-chunk strategy
delivers stronger performance - even in tasks that presumably require
cross-context relations.

</details>


### [550] [Synthesizing Diverse Network Flow Datasets with Scalable Dynamic Multigraph Generation](https://arxiv.org/pdf/2505.07777)
*Arya Grayeli, Vipin Swarup, Steven E. Noel*

Main category: cs.LG

TL;DR: A novel ML model for generating high-fidelity synthetic network flow datasets using dynamic multigraphs, stochastic Kronecker graphs, and tabular GANs, with XGBoost for alignment. Evaluated with new metrics, it improves accuracy while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: Challenges in obtaining real-world network datasets due to privacy, security, and computational constraints necessitate synthetic data generation.

Method: Combines stochastic Kronecker graph generator for structure, tabular GAN for features, and XGBoost for alignment. Introduces new evaluation metrics.

Result: Improved accuracy over previous methods with similar efficiency. Explores accuracy-diversity trade-off in synthetic graphs.

Conclusion: Contributes synthetic netflow datasets and new evaluation metrics, addressing gaps in graph generative models.

Abstract: Obtaining real-world network datasets is often challenging because of
privacy, security, and computational constraints. In the absence of such
datasets, graph generative models become essential tools for creating synthetic
datasets. In this paper, we introduce a novel machine learning model for
generating high-fidelity synthetic network flow datasets that are
representative of real-world networks. Our approach involves the generation of
dynamic multigraphs using a stochastic Kronecker graph generator for structure
generation and a tabular generative adversarial network for feature generation.
We further employ an XGBoost (eXtreme Gradient Boosting) model for graph
alignment, ensuring accurate overlay of features onto the generated graph
structure. We evaluate our model using new metrics that assess both the
accuracy and diversity of the synthetic graphs. Our results demonstrate
improvements in accuracy over previous large-scale graph generation methods
while maintaining similar efficiency. We also explore the trade-off between
accuracy and diversity in synthetic graph dataset creation, a topic not
extensively covered in related works. Our contributions include the synthesis
and evaluation of large real-world netflow datasets and the definition of new
metrics for evaluating synthetic graph generative models.

</details>


### [551] [Revealing Weaknesses in Text Watermarking Through Self-Information Rewrite Attacks](https://arxiv.org/pdf/2505.05190)
*Yixin Cheng, Hongcheng Guo, Yangming Li, Leonid Sigal*

Main category: cs.LG

TL;DR: The paper reveals a vulnerability in current text watermarking algorithms, introduces the SIRA attack exploiting this flaw, and demonstrates its high success rate and low cost, urging the need for more robust watermarking.


<details>
  <summary>Details</summary>
Motivation: To expose a critical vulnerability in existing text watermarking algorithms that embed watermarks in high-entropy tokens, which attackers can exploit.

Method: Introduces the Self-Information Rewrite Attack (SIRA), a paraphrasing attack that identifies and targets pattern tokens using self-information calculations.

Result: SIRA achieves nearly 100% success rates on seven watermarking methods with minimal cost (0.88 USD per million tokens) and requires no access to the watermark algorithms or LLM.

Conclusion: The findings emphasize the urgent need for more robust watermarking techniques to counter such vulnerabilities.

Abstract: Text watermarking aims to subtly embed statistical signals into text by
controlling the Large Language Model (LLM)'s sampling process, enabling
watermark detectors to verify that the output was generated by the specified
model. The robustness of these watermarking algorithms has become a key factor
in evaluating their effectiveness. Current text watermarking algorithms embed
watermarks in high-entropy tokens to ensure text quality. In this paper, we
reveal that this seemingly benign design can be exploited by attackers, posing
a significant risk to the robustness of the watermark. We introduce a generic
efficient paraphrasing attack, the Self-Information Rewrite Attack (SIRA),
which leverages the vulnerability by calculating the self-information of each
token to identify potential pattern tokens and perform targeted attack. Our
work exposes a widely prevalent vulnerability in current watermarking
algorithms. The experimental results show SIRA achieves nearly 100% attack
success rates on seven recent watermarking methods with only 0.88 USD per
million tokens cost. Our approach does not require any access to the watermark
algorithms or the watermarked LLM and can seamlessly transfer to any LLM as the
attack model, even mobile-level models. Our findings highlight the urgent need
for more robust watermarking.

</details>


### [552] [MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine Learning Engineering](https://arxiv.org/pdf/2505.07782)
*Rushi Qiang, Yuchen Zhuang, Yinghao Li, Dingu Sagar V K, Rongzhi Zhang, Changhao Li, Ian Shu-Hei Wong, Sherry Yang, Percy Liang, Chao Zhang, Bo Dai*

Main category: cs.LG

TL;DR: MLE-Dojo is a Gym-style framework for iterative reinforcement learning of LLM agents, offering interactive environments and structured feedback for tasks like data processing and hyperparameter tuning. Evaluations show current models improve iteratively but struggle with long-horizon solutions.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack interactivity and iterative feedback, limiting the evaluation and improvement of autonomous LLM agents in realistic MLE workflows.

Method: Built on 200+ Kaggle challenges, MLE-Dojo provides an executable environment for supervised fine-tuning and reinforcement learning, supporting iterative experimentation and real-time verification.

Result: Evaluations of eight LLMs show iterative improvements but highlight limitations in autonomous long-horizon problem-solving and error resolution.

Conclusion: MLE-Dojo's flexible architecture fosters interoperability and scalability, advancing community-driven innovation in MLE agents.

Abstract: We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement
learning, evaluating, and improving autonomous large language model (LLM)
agents in iterative machine learning engineering (MLE) workflows. Unlike
existing benchmarks that primarily rely on static datasets or single-attempt
evaluations, MLE-Dojo provides an interactive environment enabling agents to
iteratively experiment, debug, and refine solutions through structured feedback
loops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse,
open-ended MLE tasks carefully curated to reflect realistic engineering
scenarios such as data processing, architecture search, hyperparameter tuning,
and code debugging. Its fully executable environment supports comprehensive
agent training via both supervised fine-tuning and reinforcement learning,
facilitating iterative experimentation, realistic data sampling, and real-time
outcome verification. Extensive evaluations of eight frontier LLMs reveal that
while current models achieve meaningful iterative improvements, they still
exhibit significant limitations in autonomously generating long-horizon
solutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's
flexible and extensible architecture seamlessly integrates diverse data
sources, tools, and evaluation protocols, uniquely enabling model-based agent
tuning and promoting interoperability, scalability, and reproducibility. We
open-source our framework and benchmarks to foster community-driven innovation
towards next-generation MLE agents.

</details>


### [553] [Relative Overfitting and Accept-Reject Framework](https://arxiv.org/pdf/2505.07783)
*Yanxin Liu, Yunqi Zhang*

Main category: cs.LG

TL;DR: The paper addresses scaling law challenges in LLMs by attributing them to noise effects and introduces the 'relative overfitting' concept and the AR framework to improve performance efficiently.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome bottlenecks in LLM scaling laws caused by noise effects under diminishing marginal returns.

Method: The paper investigates performance differences between models, introduces 'relative overfitting,' and proposes the AR framework, validated using self-built and pre-trained models across diverse NLP tasks.

Result: The AR framework achieves better performance improvements than increasing LLM parameters, with lower costs, and shows universal, stable effectiveness.

Conclusion: The approach has potential beyond NLP, in CV and AI for science, offering a way to overcome scaling law bottlenecks.

Abstract: Currently, the scaling law of Large Language Models (LLMs) faces challenges
and bottlenecks. This paper posits that noise effects, stemming from changes in
the signal-to-noise ratio under diminishing marginal returns, are the root
cause of these issues. To control this noise, we investigated the differences
between models with performance advantages and disadvantages, introducing the
concept of "relative overfitting." Based on their complementary strengths, we
have proposed an application framework, Accept-Reject (AR). In Natural Language
Processing (NLP), we use LLMs and Small Language Models (SLMs) as the medium
for discussion. This framework enables SLMs to exert a universal positive
influence on LLM decision outputs, rather than the intuitively expected
negative influence. We validated our approach using self-built models based on
mainstream architectures and pre-trained mainstream models across multiple
datasets, including basic language modeling, long-context tasks, subject
examination, and question-answering (QA) benchmarks. The results demonstrate
that through our structure, compared to increasing the LLM's parameters, we can
achieve better performance improvements with significantly lower parameter and
computational costs in many scenarios. These improvements are universal,
stable, and effective. Furthermore, we explore the potential of "relative
overfitting" and the AR framework in other machine learning domains, such as
computer vision (CV) and AI for science. We hope the proposed approach can help
scale laws overcome existing bottlenecks.

</details>


### [554] [A Theoretical Framework for Explaining Reinforcement Learning with Shapley Values](https://arxiv.org/pdf/2505.07797)
*Daniel Beechey, Thomas M. S. Smith, Özgür Şimşek*

Main category: cs.LG

TL;DR: A framework for explaining reinforcement learning decisions using Shapley values to analyze state feature influence, enhancing interpretability and trust.


<details>
  <summary>Details</summary>
Motivation: The lack of transparency in reinforcement learning decisions limits deployment in safety-critical settings, necessitating interpretable explanations.

Method: Develops a theoretical framework using Shapley values to quantify the influence of state features on agent behavior, performance, and value estimation.

Result: Produces mathematically grounded explanations that align with human intuition and reveal new insights.

Conclusion: The framework provides a principled foundation for interpretable and trustworthy reinforcement learning, unifying prior work.

Abstract: Reinforcement learning agents can achieve superhuman performance, but their
decisions are often difficult to interpret. This lack of transparency limits
deployment, especially in safety-critical settings where human trust and
accountability are essential. In this work, we develop a theoretical framework
for explaining reinforcement learning through the influence of state features,
which represent what the agent observes in its environment. We identify three
core elements of the agent-environment interaction that benefit from
explanation: behaviour (what the agent does), performance (what the agent
achieves), and value estimation (what the agent expects to achieve). We treat
state features as players cooperating to produce each element and apply Shapley
values, a principled method from cooperative game theory, to identify the
influence of each feature. This approach yields a family of mathematically
grounded explanations with clear semantics and theoretical guarantees. We use
illustrative examples to show how these explanations align with human intuition
and reveal novel insights. Our framework unifies and extends prior work, making
explicit the assumptions behind existing approaches, and offers a principled
foundation for more interpretable and trustworthy reinforcement learning.

</details>


### [555] [The Pump Scheduling Problem: A Real-World Scenario for Reinforcement Learning](https://arxiv.org/pdf/2210.11111)
*Henrique Donâncio, Laurent Vercouter, Harald Roclawski*

Main category: cs.LG

TL;DR: A testbed for Deep Reinforcement Learning (DRL) is introduced, focusing on the pump scheduling problem in a real-world water distribution facility to address challenges like safety constraints and partial observability.


<details>
  <summary>Details</summary>
Motivation: Existing DRL benchmarks lack grounding in complex, real-world environments with safety constraints and partial observability.

Method: The testbed includes a realistic simulator, three years of high-resolution operational data, and a baseline RL task formulation for pump scheduling.

Result: The testbed supports research in offline RL, safe exploration, inverse RL, and multi-objective optimization.

Conclusion: This work bridges the gap between DRL and real-world applications by providing a practical and challenging benchmark.

Abstract: Deep Reinforcement Learning (DRL) has demonstrated impressive results in
domains such as games and robotics, where task formulations are well-defined.
However, few DRL benchmarks are grounded in complex, real-world environments,
where safety constraints, partial observability, and the need for
hand-engineered task representations pose significant challenges. To help
bridge this gap, we introduce a testbed based on the pump scheduling problem in
a real-world water distribution facility. The task involves controlling pumps
to ensure a reliable water supply while minimizing energy consumption and
respecting the constraints of the system. Our testbed includes a realistic
simulator, three years of high-resolution (1-minute) operational data from
human-led control, and a baseline RL task formulation. This testbed supports a
wide range of research directions, including offline RL, safe exploration,
inverse RL, and multi-objective optimization.

</details>


### [556] [Decentralized Adversarial Training over Graphs](https://arxiv.org/pdf/2303.13326)
*Ying Cao, Elsa Rizk, Stefan Vlaski, Ali H. Sayed*

Main category: cs.LG

TL;DR: The paper studies adversarial training in multi-agent systems over graphs, proposing decentralized algorithms to enhance robustness against varied adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Existing research focuses on single-agent learners, but this work explores how multi-agent interactions and heterogeneous attack models over graphs can improve robustness.

Method: A min-max formulation of distributed learning is used to develop two decentralized adversarial training algorithms based on diffusion and consensus strategies.

Result: The framework's convergence is analyzed for strongly-convex, convex, and non-convex environments, demonstrating enhanced robustness to adversarial attacks.

Conclusion: Decentralized adversarial training over graphs improves robustness by leveraging multi-agent coordination and heterogeneous attack models.

Abstract: The vulnerability of machine learning models to adversarial attacks has been
attracting considerable attention in recent years. Most existing studies focus
on the behavior of stand-alone single-agent learners. In comparison, this work
studies adversarial training over graphs, where individual agents are subjected
to perturbations of varied strength levels across space. It is expected that
interactions by linked agents, and the heterogeneity of the attack models that
are possible over the graph, can help enhance robustness in view of the
coordination power of the group. Using a min-max formulation of distributed
learning, we develop a decentralized adversarial training framework for
multi-agent systems. Specifically, we devise two decentralized adversarial
training algorithms by relying on two popular decentralized learning
strategies--diffusion and consensus. We analyze the convergence properties of
the proposed framework for strongly-convex, convex, and non-convex
environments, and illustrate the enhanced robustness to adversarial attacks.

</details>


### [557] [Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach](https://arxiv.org/pdf/2402.01454)
*Masayuki Takayama, Tadahisa Okuda, Thong Pham, Tatsuyoshi Ikenoue, Shingo Fukuma, Shohei Shimizu, Akiyoshi Sannai*

Main category: cs.LG

TL;DR: The paper proposes a method combining statistical causal discovery (SCD) and knowledge-based causal inference (KBCI) using large language models (LLMs) to improve causal inference by integrating expert knowledge. Experiments show improved accuracy over SCD alone, with potential applications in healthcare and other domains.


<details>
  <summary>Details</summary>
Motivation: To address challenges in systematically acquiring domain expert knowledge for causal discovery, the paper aims to synthesize SCD and LLM-based KBCI for more accurate causal models.

Method: The method involves statistical causal prompting (SCP) for LLMs and prior knowledge augmentation for SCD, integrating LLM-KBCI with SCD.

Result: Experiments show LLM-KBCI and SCD augmented with LLM-KBCI outperform SCD without prior knowledge, approaching ground truths. LLM background knowledge also improves SCD on unseen real-world data.

Conclusion: The proposed method, with domain-specific improvements, can mitigate dataset biases and enhance causal inference, though limitations and risks require careful application and expert validation.

Abstract: In practical statistical causal discovery (SCD), embedding domain expert
knowledge as constraints into the algorithm is important for reasonable causal
models reflecting the broad knowledge of domain experts, despite the challenges
in the systematic acquisition of background knowledge. To overcome these
challenges, this paper proposes a novel method for causal inference, in which
SCD and knowledge-based causal inference (KBCI) with a large language model
(LLM) are synthesized through ``statistical causal prompting (SCP)'' for LLMs
and prior knowledge augmentation for SCD. The experiments in this work have
revealed that the results of LLM-KBCI and SCD augmented with LLM-KBCI approach
the ground truths, more than the SCD result without prior knowledge. These
experiments have also revealed that the SCD result can be further improved if
the LLM undergoes SCP. Furthermore, with an unpublished real-world dataset, we
have demonstrated that the background knowledge provided by the LLM can improve
the SCD on this dataset, even if this dataset has never been included in the
training data of the LLM. For future practical application of this proposed
method across important domains such as healthcare, we also thoroughly discuss
the limitations, risks of critical errors, expected improvement of techniques
around LLMs, and realistic integration of expert checks of the results into
this automatic process, with SCP simulations under various conditions both in
successful and failure scenarios. The careful and appropriate application of
the proposed approach in this work, with improvement and customization for each
domain, can thus address challenges such as dataset biases and limitations,
illustrating the potential of LLMs to improve data-driven causal inference
across diverse scientific domains.
  The code used in this work is publicly available at:
www.github.com/mas-takayama/LLM-and-SCD

</details>


### [558] [Poly2Vec: Polymorphic Fourier-Based Encoding of Geospatial Objects for GeoAI Applications](https://arxiv.org/pdf/2408.14806)
*Maria Despoina Siampou, Jialiang Li, John Krumm, Cyrus Shahabi, Hua Lu*

Main category: cs.LG

TL;DR: Poly2Vec is a polymorphic Fourier-based encoding method for geospatial objects, preserving spatial properties and outperforming object-specific baselines in key tasks.


<details>
  <summary>Details</summary>
Motivation: Existing encoding methods for geospatial objects often discard spatial information or are limited to specific data types, reducing effectiveness in GeoAI applications.

Method: Poly2Vec uses a polymorphic Fourier-based approach with a learned fusion module to adaptively integrate magnitude and phase for different tasks and geometries.

Result: Poly2Vec outperforms baselines in preserving spatial relationships (topology, direction, distance) and enhances performance in population prediction and land use inference.

Conclusion: Poly2Vec provides a unified and effective encoding solution for diverse geospatial objects, improving GeoAI task performance.

Abstract: Encoding geospatial objects is fundamental for geospatial artificial
intelligence (GeoAI) applications, which leverage machine learning (ML) models
to analyze spatial information. Common approaches transform each object into
known formats, like image and text, for compatibility with ML models. However,
this process often discards crucial spatial information, such as the object's
position relative to the entire space, reducing downstream task effectiveness.
Alternative encoding methods that preserve some spatial properties are often
devised for specific data objects (e.g., point encoders), making them
unsuitable for tasks that involve different data types (i.e., points,
polylines, and polygons). To this end, we propose Poly2Vec, a polymorphic
Fourier-based encoding approach that unifies the representation of geospatial
objects, while preserving the essential spatial properties. Poly2Vec
incorporates a learned fusion module that adaptively integrates the magnitude
and phase of the Fourier transform for different tasks and geometries. We
evaluate Poly2Vec on five diverse tasks, organized into two categories. The
first empirically demonstrates that Poly2Vec consistently outperforms
object-specific baselines in preserving three key spatial relationships:
topology, direction, and distance. The second shows that integrating Poly2Vec
into a state-of-the-art GeoAI workflow improves the performance in two popular
tasks: population prediction and land use inference.

</details>


### [559] [Neural Algorithmic Reasoning with Multiple Correct Solutions](https://arxiv.org/pdf/2409.06953)
*Zeno Kujawa, John Poole, Dobrik Georgiev, Danilo Numeroso, Henry Fleischmann, Pietro Liò*

Main category: cs.LG

TL;DR: The paper introduces a method for Neural Algorithmic Reasoning (NAR) to handle multiple correct solutions, demonstrated on Bellman-Ford and Depth-First Search algorithms.


<details>
  <summary>Details</summary>
Motivation: Classical NAR implementations return only one solution, but some problems have multiple correct solutions, necessitating a method to recover more.

Method: The method involves generating training data, sampling solutions from model output, and validating them, serving as a framework for broader NAR tasks.

Result: Demonstrated successfully on Bellman-Ford and Depth-First Search algorithms, providing deeper insights into these specific cases.

Conclusion: This work is the first in NAR literature to address multiple solutions, offering a reusable framework for future research.

Abstract: Neural Algorithmic Reasoning (NAR) extends classical algorithms to higher
dimensional data. However, canonical implementations of NAR train neural
networks to return only a single solution, even when there are multiple correct
solutions to a problem, such as single-source shortest paths. For some
applications, it is desirable to recover more than one correct solution. To
that end, we give the first method for NAR with multiple solutions. We
demonstrate our method on two classical algorithms: Bellman-Ford (BF) and
Depth-First Search (DFS), favouring deeper insight into two algorithms over a
broader survey of algorithms. This method involves generating appropriate
training data as well as sampling and validating solutions from model output.
Each step of our method, which can serve as a framework for neural algorithmic
reasoning beyond the tasks presented in this paper, might be of independent
interest to the field and our results represent the first attempt at this task
in the NAR literature.

</details>


### [560] [BNEM: A Boltzmann Sampler Based on Bootstrapped Noised Energy Matching](https://arxiv.org/pdf/2409.09787)
*RuiKang OuYang, Bo Qiang, José Miguel Hernández-Lobato*

Main category: cs.LG

TL;DR: The paper introduces Noised Energy Matching (NEM) and its bootstrapped variant (BNEM) for efficient Boltzmann distribution sampling, showing improved performance and robustness.


<details>
  <summary>Details</summary>
Motivation: Efficiently sampling from Boltzmann distributions is critical in fields like molecular dynamics, but existing methods face challenges. This work aims to learn neural samplers directly from energy functions rather than pre-sampled data.

Method: Proposes NEM, a diffusion-based sampler, and BNEM, which uses bootstrapping to balance bias and variance. Both are evaluated on a 2D 40-GMM and a 4-particle double-well potential.

Result: BNEM achieves state-of-the-art performance with lower variance and higher robustness compared to related works.

Conclusion: BNEM offers a promising solution for efficient Boltzmann sampling, combining theoretical advantages with practical robustness.

Abstract: Developing an efficient sampler capable of generating independent and
identically distributed (IID) samples from a Boltzmann distribution is a
crucial challenge in scientific research, e.g. molecular dynamics. In this
work, we intend to learn neural samplers given energy functions instead of data
sampled from the Boltzmann distribution. By learning the energies of the noised
data, we propose a diffusion-based sampler, Noised Energy Matching, which
theoretically has lower variance and more complexity compared to related works.
Furthermore, a novel bootstrapping technique is applied to NEM to balance
between bias and variance. We evaluate NEM and BNEM on a 2-dimensional 40
Gaussian Mixture Model (GMM) and a 4-particle double-well potential (DW-4). The
experimental results demonstrate that BNEM can achieve state-of-the-art
performance while being more robust.

</details>


### [561] [Moral Alignment for LLM Agents](https://arxiv.org/pdf/2410.01639)
*Elizaveta Tennant, Stephen Hailes, Mirco Musolesi*

Main category: cs.LG

TL;DR: The paper proposes using intrinsic rewards for moral alignment of LLM agents, offering a transparent and cost-effective alternative to human feedback-based methods like RLHF or DPO.


<details>
  <summary>Details</summary>
Motivation: As LLM-based agents become more influential and less transparent, aligning them to human values is crucial. Current methods rely on opaque human preference data, prompting the need for explicit, value-driven reward functions.

Method: The authors design reward functions encoding human values (Deontological Ethics and Utilitarianism) for Reinforcement Learning-based fine-tuning. They test this on the Iterated Prisoner's Dilemma and other matrix games.

Result: Moral fine-tuning successfully aligns agents, unlearns selfish strategies, and generalizes learned strategies to other environments.

Conclusion: Intrinsic reward-based fine-tuning is a promising, transparent, and cost-effective solution for aligning LLM agents to human values.

Abstract: Decision-making agents based on pre-trained Large Language Models (LLMs) are
increasingly being deployed across various domains of human activity. While
their applications are currently rather specialized, several research efforts
are underway to develop more generalist agents. As LLM-based systems become
more agentic, their influence on human activity will grow and their
transparency will decrease. Consequently, developing effective methods for
aligning them to human values is vital.
  The prevailing practice in alignment often relies on human preference data
(e.g., in RLHF or DPO), in which values are implicit, opaque and are
essentially deduced from relative preferences over different model outputs. In
this work, instead of relying on human feedback, we introduce the design of
reward functions that explicitly and transparently encode core human values for
Reinforcement Learning-based fine-tuning of foundation agent models.
Specifically, we use intrinsic rewards for the moral alignment of LLM agents.
  We evaluate our approach using the traditional philosophical frameworks of
Deontological Ethics and Utilitarianism, quantifying moral rewards for agents
in terms of actions and consequences on the Iterated Prisoner's Dilemma (IPD)
environment. We also show how moral fine-tuning can be deployed to enable an
agent to unlearn a previously developed selfish strategy. Finally, we find that
certain moral strategies learned on the IPD game generalize to several other
matrix game environments. In summary, we demonstrate that fine-tuning with
intrinsic rewards is a promising general solution for aligning LLM agents to
human values, and it might represent a more transparent and cost-effective
alternative to currently predominant alignment techniques.

</details>


### [562] [Flow Matching with Gaussian Process Priors for Probabilistic Time Series Forecasting](https://arxiv.org/pdf/2410.03024)
*Marcel Kollovieh, Marten Lienen, David Lüdke, Leo Schwinn, Stephan Günnemann*

Main category: cs.LG

TL;DR: TSFlow, a conditional flow matching model for time series, improves generative modeling by aligning prior distributions with data structure using Gaussian processes and optimal transport paths.


<details>
  <summary>Details</summary>
Motivation: Diffusion models for time series rely on fixed priors, which mismatch data distributions, complicating generation. TSFlow addresses this by integrating data-dependent priors.

Method: TSFlow combines Gaussian processes, optimal transport paths, and data-dependent priors. It also introduces conditional prior sampling for probabilistic forecasting.

Result: TSFlow produces high-quality unconditional samples and achieves competitive results in forecasting benchmarks.

Conclusion: TSFlow enhances time series modeling by better aligning priors with data, improving both generation and forecasting.

Abstract: Recent advancements in generative modeling, particularly diffusion models,
have opened new directions for time series modeling, achieving state-of-the-art
performance in forecasting and synthesis. However, the reliance of
diffusion-based models on a simple, fixed prior complicates the generative
process since the data and prior distributions differ significantly. We
introduce TSFlow, a conditional flow matching (CFM) model for time series
combining Gaussian processes, optimal transport paths, and data-dependent prior
distributions. By incorporating (conditional) Gaussian processes, TSFlow aligns
the prior distribution more closely with the temporal structure of the data,
enhancing both unconditional and conditional generation. Furthermore, we
propose conditional prior sampling to enable probabilistic forecasting with an
unconditionally trained model. In our experimental evaluation on eight
real-world datasets, we demonstrate the generative capabilities of TSFlow,
producing high-quality unconditional samples. Finally, we show that both
conditionally and unconditionally trained models achieve competitive results
across multiple forecasting benchmarks.

</details>


### [563] [A Comparative Study on Dynamic Graph Embedding based on Mamba and Transformers](https://arxiv.org/pdf/2412.11293)
*Ashish Parmanand Pandey, Alan John Varghese, Sarang Patil, Mengjia Xu*

Main category: cs.LG

TL;DR: Comparative analysis of transformer and Mamba-based models for dynamic graph embedding, showing Mamba's efficiency and performance gains.


<details>
  <summary>Details</summary>
Motivation: Address scalability challenges of transformer-based models in temporal graph data by exploring Mamba architecture's linear complexity.

Method: Introduces three models: TransformerG2G, DG-Mamba, and GDG-Mamba, combining graph neural networks with state-space models.

Result: Mamba-based models match or outperform transformers in link prediction, especially in high temporal variability datasets, with computational efficiency.

Conclusion: Mamba models offer scalable, efficient solutions for dynamic graph embedding, enabling broader applications in real-world networks.

Abstract: Dynamic graph embedding has emerged as an important technique for modeling
complex time-evolving networks across diverse domains. While transformer-based
models have shown promise in capturing long-range dependencies in temporal
graph data, they face scalability challenges due to quadratic computational
complexity. This study presents a comparative analysis of dynamic graph
embedding approaches using transformers and the recently proposed Mamba
architecture, a state-space model with linear complexity. We introduce three
novel models: TransformerG2G augment with graph convolutional networks,
\mathcal{DG}-Mamba, and \mathcal{GDG}-Mamba with graph isomorphism network edge
convolutions. Our experiments on multiple benchmark datasets demonstrate that
Mamba-based models achieve comparable or superior performance to
transformer-based approaches in link prediction tasks while offering
significant computational efficiency gains on longer sequences. Notably,
\mathcal{DG}-Mamba variants consistently outperform transformer-based models on
datasets with high temporal variability, such as UCI, Bitcoin, and Reality
Mining, while maintaining competitive performance on more stable graphs like
SBM. We provide insights into the learned temporal dependencies through
analysis of attention weights and state matrices, revealing the models' ability
to capture complex temporal patterns. By effectively combining state-space
models with graph neural networks, our work addresses key limitations of
previous approaches and contributes to the growing body of research on
efficient temporal graph representation learning. These findings offer
promising directions for scaling dynamic graph embedding to larger, more
complex real-world networks, potentially enabling new applications in areas
such as social network analysis, financial modeling, and biological system
dynamics.

</details>


### [564] [Formal Verification of Markov Processes with Learned Parameters](https://arxiv.org/pdf/2501.15767)
*Muhammad Maaz, Timothy C. Y. Chan*

Main category: cs.LG

TL;DR: The paper introduces a method for verifying properties of Markov processes with ML model parameters, formulating it as a bilinear program and solving it efficiently.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of formally verifying properties of Markov processes where parameters are derived from ML models, ensuring reliability and correctness in applications like healthcare.

Method: A decomposition and bound propagation scheme is developed to solve the bilinear program, applicable to linear models, tree-based models, and neural networks.

Result: The method achieves global optimality up to 100x faster than state-of-the-art solvers, demonstrated through computational experiments and a healthcare case study.

Conclusion: The approach is practical and scalable, supported by the open-source tool markovml for building and verifying Markov processes with ML models.

Abstract: We introduce the problem of formally verifying properties of Markov processes
where the parameters are given by the output of machine learning models. For a
broad class of machine learning models, including linear models, tree-based
models, and neural networks, verifying properties of Markov chains like
reachability, hitting time, and total reward can be formulated as a bilinear
program. We develop a decomposition and bound propagation scheme for solving
the bilinear program and show through computational experiments that our method
solves the problem to global optimality up to 100x faster than state-of-the-art
solvers. To demonstrate the practical utility of our approach, we apply it to a
real-world healthcare case study. Along with the paper, we release markovml, an
open-source tool for building Markov processes, integrating pretrained machine
learning models, and verifying their properties, available at
https://github.com/mmaaz-git/markovml.

</details>


### [565] [Adaptive Width Neural Networks](https://arxiv.org/pdf/2501.15889)
*Federico Errica, Henrik Christiansen, Viktor Zaverkin, Mathias Niepert, Francesco Alesiani*

Main category: cs.LG

TL;DR: A novel technique for learning neural network layer widths during training, avoiding hyper-parameter tuning and enabling dynamic adaptation and compression.


<details>
  <summary>Details</summary>
Motivation: Challenges the traditional reliance on hyper-parameter tuning for network width, offering a scalable solution for modern large-scale models.

Method: Jointly optimizes layer widths and parameters via simple backpropagation, applying it across diverse data domains.

Result: Demonstrates adaptive width tuning, soft neuron importance ordering, and efficient network truncation or compression.

Conclusion: Provides a scalable alternative to hyper-parameter tuning, especially for large foundation models.

Abstract: For almost 70 years, researchers have mostly relied on hyper-parameter tuning
to select the width of neural networks' layers. This paper challenges the
status quo by introducing an easy-to-use technique to learn an unbounded width
of a neural network's layer during training. The technique does not rely on
alternate optimization nor hand-crafted gradient heuristics; rather, it jointly
optimizes the width and the parameters of each layer via simple
backpropagation. We apply the technique to a broad range of data domains such
as tables, images, text, sequences, and graphs, showing how the width adapts to
the task's difficulty. The method imposes a soft ordering of importance among
neurons, by which it also is possible to truncate the trained network at
virtually zero cost, achieving a smooth trade-off between performance and
compute resources in a structured way. Alternatively, one can dynamically
compress the network with no performance degradation. In light of recent
foundation models trained on large datasets, believed to require billions of
parameters and where hyper-parameter tuning is unfeasible due to humongous
training costs, our approach stands as a viable alternative for width learning.

</details>


### [566] [Clustering Properties of Self-Supervised Learning](https://arxiv.org/pdf/2501.18452)
*Xi Weng, Jianing An, Xudong Ma, Binhang Qi, Jie Luo, Xi Yang, Jin Song Dong, Lei Huang*

Main category: cs.LG

TL;DR: The paper introduces ReSA, a self-supervised learning method that leverages clustering properties of encoder outputs to improve representation learning, outperforming state-of-the-art SSL methods.


<details>
  <summary>Details</summary>
Motivation: Despite SSL methods' success in learning rich representations, few exploit their clustering properties for self-improvement. This paper aims to bridge this gap.

Method: Proposes Representation Self-Assignment (ReSA), a positive-feedback SSL method that uses the encoder's clustering properties to guide learning.

Result: ReSA outperforms other SSL methods on benchmarks, enhancing clustering at fine and coarse levels for more structured representations.

Conclusion: ReSA effectively leverages clustering properties to improve SSL, yielding semantically meaningful and structured representations.

Abstract: Self-supervised learning (SSL) methods via joint embedding architectures have
proven remarkably effective at capturing semantically rich representations with
strong clustering properties, magically in the absence of label supervision.
Despite this, few of them have explored leveraging these untapped properties to
improve themselves. In this paper, we provide an evidence through various
metrics that the encoder's output $encoding$ exhibits superior and more stable
clustering properties compared to other components. Building on this insight,
we propose a novel positive-feedback SSL method, termed Representation
Self-Assignment (ReSA), which leverages the model's clustering properties to
promote learning in a self-guided manner. Extensive experiments on standard SSL
benchmarks reveal that models pretrained with ReSA outperform other
state-of-the-art SSL methods by a significant margin. Finally, we analyze how
ReSA facilitates better clustering properties, demonstrating that it
effectively enhances clustering performance at both fine-grained and
coarse-grained levels, shaping representations that are inherently more
structured and semantically meaningful.

</details>


### [567] [Dual Alignment Maximin Optimization for Offline Model-based RL](https://arxiv.org/pdf/2502.00850)
*Chi Zhou, Wang Luo, Haoran Li, Congying Han, Tiande Guo, Zicheng Zhang*

Main category: cs.LG

TL;DR: DAMO is a novel actor-critic framework addressing offline RL challenges by aligning policies and trajectories while ensuring synthetic and offline data compatibility.


<details>
  <summary>Details</summary>
Motivation: Addressing synthetic-to-real distribution mismatch and policy discrepancies in offline RL by focusing on policy consistency and data compatibility.

Method: Proposes Dual Alignment Maximin Optimization (DAMO), combining inner minimization for dual conservative value estimation and outer maximization for policy improvement.

Result: DAMO ensures model-policy alignment and achieves competitive performance across benchmark tasks.

Conclusion: DAMO effectively addresses offline RL challenges by unifying policy and data alignment, demonstrating robust performance.

Abstract: Offline reinforcement learning agents face significant deployment challenges
due to the synthetic-to-real distribution mismatch. While most prior research
has focused on improving the fidelity of synthetic sampling and incorporating
off-policy mechanisms, the directly integrated paradigm often fails to ensure
consistent policy behavior in biased models and underlying environmental
dynamics, which inherently arise from discrepancies between behavior and
learning policies. In this paper, we first shift the focus from model
reliability to policy discrepancies while optimizing for expected returns, and
then self-consistently incorporate synthetic data, deriving a novel
actor-critic paradigm, Dual Alignment Maximin Optimization (DAMO). It is a
unified framework to ensure both model-environment policy consistency and
synthetic and offline data compatibility. The inner minimization performs dual
conservative value estimation, aligning policies and trajectories to avoid
out-of-distribution states and actions, while the outer maximization ensures
that policy improvements remain consistent with inner value estimates.
Empirical evaluations demonstrate that DAMO effectively ensures model and
policy alignments, achieving competitive performance across diverse benchmark
tasks.

</details>


### [568] [Effective Regularization Through Loss-Function Metalearning](https://arxiv.org/pdf/2010.00788)
*Santiago Gonzalez, Risto Miikkulainen*

Main category: cs.LG

TL;DR: TaylorGLO uses evolutionary computation to optimize neural network loss functions, improving performance, training speed, and data use by balancing error reduction and overfitting avoidance.


<details>
  <summary>Details</summary>
Motivation: To theoretically validate that evolved loss functions in TaylorGLO prevent overfitting and improve regularization, and to generalize this principle to other techniques like label smoothing.

Method: Theoretical analysis of TaylorGLO's evolved loss functions, decomposing learning rules to reveal a balance between minimizing error and avoiding overfitting.

Result: Evolved loss functions effectively regularize networks, leading to better performance and robustness against adversarial inputs.

Conclusion: The study provides insights into regularization and highlights the potential of evolutionary neural architecture search.

Abstract: Evolutionary computation can be used to optimize several different aspects of
neural network architectures. For instance, the TaylorGLO method discovers
novel, customized loss functions, resulting in improved performance, faster
training, and improved data utilization. A likely reason is that such functions
discourage overfitting, leading to effective regularization. This paper
demonstrates theoretically that this is indeed the case for TaylorGLO. Learning
rule decomposition reveals that evolved loss functions balance two factors: the
pull toward zero error, and a push away from it to avoid overfitting. This is a
general principle that may be used to understand other regularization
techniques as well (as demonstrated in this paper for label smoothing). The
theoretical analysis leads to a constraint that can be utilized to find more
effective loss functions in practice; the mechanism also results in networks
that are more robust (as demonstrated in this paper with adversarial inputs).
The analysis in this paper thus constitutes a first step towards understanding
regularization, and demonstrates the power of evolutionary neural architecture
search in general.

</details>


### [569] [Towards Optimal Branching of Linear and Semidefinite Relaxations for Neural Network Robustness Certification](https://arxiv.org/pdf/2101.09306)
*Brendon G. Anderson, Ziye Ma, Jingqi Li, Somayeh Sojoudi*

Main category: cs.LG

TL;DR: A branch-and-bound approach is proposed to certify ReLU neural network robustness against adversarial inputs, reducing relaxation errors in LP and SDP methods.


<details>
  <summary>Details</summary>
Motivation: To address the relaxation errors in existing LP and SDP certification methods for ReLU networks.

Method: Partition the input uncertainty set and solve relaxations on each part, with intelligent partitioning for LP and SDP.

Result: Significant increases in certified test samples on MNIST, CIFAR-10, and Wisconsin datasets.

Conclusion: The branched LP and SDP methods are effective, with a multi-layer heuristic matching state-of-the-art performance.

Abstract: In this paper, we study certifying the robustness of ReLU neural networks
against adversarial input perturbations. To diminish the relaxation error
suffered by the popular linear programming (LP) and semidefinite programming
(SDP) certification methods, we take a branch-and-bound approach to propose
partitioning the input uncertainty set and solving the relaxations on each part
separately. We show that this approach reduces relaxation error, and that the
error is eliminated entirely upon performing an LP relaxation with a partition
intelligently designed to exploit the nature of the ReLU activations. To scale
this approach to large networks, we consider using a coarser partition whereby
the number of parts in the partition is reduced. We prove that computing such a
coarse partition that directly minimizes the LP relaxation error is NP-hard. By
instead minimizing the worst-case LP relaxation error, we develop a closed-form
branching scheme in the single-hidden layer case. We extend the analysis to the
SDP, where the feasible set geometry is exploited to design a branching scheme
that minimizes the worst-case SDP relaxation error. Experiments on MNIST,
CIFAR-10, and Wisconsin breast cancer diagnosis classifiers demonstrate
significant increases in the percentages of test samples certified. By
independently increasing the input size and the number of layers, we
empirically illustrate under which regimes the branched LP and branched SDP are
best applied. Finally, we extend our LP branching method into a multi-layer
branching heuristic, which attains comparable performance to prior
state-of-the-art heuristics on large-scale, deep neural network certification
benchmarks.

</details>


### [570] [Diffusion Approximations for Thompson Sampling](https://arxiv.org/pdf/2105.09232)
*Lin Fan, Peter W. Glynn*

Main category: cs.LG

TL;DR: The paper analyzes Thompson sampling's behavior using weak convergence, showing its dynamics evolve like discrete SDEs/ODEs and converge to continuous versions as a parameter γ approaches zero.


<details>
  <summary>Details</summary>
Motivation: To understand Thompson sampling's dynamics in small-γ regimes and generalize findings to other sampling-based bandit algorithms.

Method: Develops weak convergence theory from first principles using the Continuous Mapping Theorem, analyzing dynamics in regimes with small γ.

Result: Shows Thompson sampling's dynamics converge to SDE/ODE solutions, and many sampling-based algorithms share the same weak limits as Gaussian Thompson sampling.

Conclusion: Sampling-based algorithms, including Thompson sampling, are robust to model mis-specification in the small-γ regime.

Abstract: We study the behavior of Thompson sampling from the perspective of weak
convergence. In the regime with small $\gamma > 0$, where the gaps between arm
means scale as $\sqrt{\gamma}$ and over time horizons that scale as $1/\gamma$,
we show that the dynamics of Thompson sampling evolve according to discrete
versions of SDE's and stochastic ODE's. As $\gamma \downarrow 0$, we show that
the dynamics converge weakly to solutions of the corresponding SDE's and
stochastic ODE's. Our weak convergence theory is developed from first
principles using the Continuous Mapping Theorem, and can be easily adapted to
analyze other sampling-based bandit algorithms. In this regime, we also show
that the weak limits of the dynamics of many sampling-based algorithms --
including Thompson sampling designed for single-parameter exponential family
rewards, and algorithms using bootstrap-based sampling to balance exploration
and exploitation -- coincide with those of Gaussian Thompson sampling.
Moreover, in this regime, these algorithms are generally robust to model
mis-specification.

</details>


### [571] [Guided Exploration for Efficient Relational Model Learning](https://arxiv.org/pdf/2502.06146)
*Annie Feng, Nishanth Kumar, Tomas Lozano-Perez, Leslie Pack-Kaelbling*

Main category: cs.LG

TL;DR: The paper proposes efficient exploration principles for learning relational models in complex environments, introducing Baking-Large as a test domain and showing improved sample efficiency with oracle demonstrations and precondition-targeting guidance.


<details>
  <summary>Details</summary>
Motivation: Random exploration methods are inefficient for learning relational models in large-scale environments, and goal-literal babbling (GLIB) lacks scalability. The paper aims to identify and demonstrate principles for efficient exploration.

Method: The authors propose two principles: operator initialization with demonstrations covering lifted effects and refining preconditions by selecting informative goal-action pairs. They test these in the Baking-Large domain using oracle-driven demonstrations and precondition-targeting guidance.

Result: Experiments show oracle demonstrations and precondition-targeting guidance significantly improve sample efficiency and generalization in learning relational models.

Conclusion: The principles of efficient exploration demonstrated in this work can guide future methods for learning accurate relational models in complex domains.

Abstract: Efficient exploration is critical for learning relational models in
large-scale environments with complex, long-horizon tasks. Random exploration
methods often collect redundant or irrelevant data, limiting their ability to
learn accurate relational models of the environment. Goal-literal babbling
(GLIB) improves upon random exploration by setting and planning to novel goals,
but its reliance on random actions and random novel goal selection limits its
scalability to larger domains. In this work, we identify the principles
underlying efficient exploration in relational domains: (1) operator
initialization with demonstrations that cover the distinct lifted effects
necessary for planning and (2) refining preconditions to collect maximally
informative transitions by selecting informative goal-action pairs and
executing plans to them. To demonstrate these principles, we introduce
Baking-Large, a challenging domain with extensive state-action spaces and
long-horizon tasks. We evaluate methods using oracle-driven demonstrations for
operator initialization and precondition-targeting guidance to efficiently
gather critical transitions. Experiments show that both the oracle
demonstrations and precondition-targeting oracle guidance significantly improve
sample efficiency and generalization, paving the way for future methods to use
these principles to efficiently learn accurate relational models in complex
domains.

</details>


### [572] [Entropy-driven Fair and Effective Federated Learning](https://arxiv.org/pdf/2301.12407)
*Lin Wang, Zhichao Wang, Ye Shi, Sai Praneeth Karimireddy, Xiaoying Tang*

Main category: cs.LG

TL;DR: A novel federated learning algorithm improves fairness and global model performance using entropy-based aggregation and alignment techniques.


<details>
  <summary>Details</summary>
Motivation: Addressing unfair outcomes in federated learning due to device heterogeneity, while maintaining global model performance.

Method: Uses entropy-based aggregation, model and gradient alignments, and a bi-level optimization framework with analytic solutions.

Result: Theoretical convergence guarantees and empirical improvements in fairness and global model performance.

Conclusion: The proposed algorithm outperforms existing methods in balancing fairness and performance.

Abstract: Federated Learning (FL) enables collaborative model training across
distributed devices while preserving data privacy. Nonetheless, the
heterogeneity of edge devices often leads to inconsistent performance of the
globally trained models, resulting in unfair outcomes among users. Existing
federated fairness algorithms strive to enhance fairness but often fall short
in maintaining the overall performance of the global model, typically measured
by the average accuracy across all clients. To address this issue, we propose a
novel algorithm that leverages entropy-based aggregation combined with model
and gradient alignments to simultaneously optimize fairness and global model
performance. Our method employs a bi-level optimization framework, where we
derive an analytic solution to the aggregation probability in the inner loop,
making the optimization process computationally efficient. Additionally, we
introduce an innovative alignment update and an adaptive strategy in the outer
loop to further balance global model's performance and fairness. Theoretical
analysis indicates that our approach guarantees convergence even in non-convex
FL settings and demonstrates significant fairness improvements in generalized
regression and strongly convex models. Empirically, our approach surpasses
state-of-the-art federated fairness algorithms, ensuring consistent performance
among clients while improving the overall performance of the global model.

</details>


### [573] [Deep Learning Models for Flood Predictions in South Florida](https://arxiv.org/pdf/2306.15907)
*Jimeng Shi, Zeda Yin, Rukmangadh Myana, Khandker Ishtiaq, Anupama John, Jayantha Obeysekera, Arturo Leon, Giri Narasimhan*

Main category: cs.LG

TL;DR: Deep learning models outperform physics-based tools like HEC-RAS in predicting river water stages, offering 500x speedups and better accuracy, even during extreme weather.


<details>
  <summary>Details</summary>
Motivation: Physics-based models are computationally intensive for large watersheds and long simulations, necessitating faster, efficient alternatives.

Method: Several DL models (MLP, RNN, CNN, LSTM, RCNN) were trained as surrogate models, using past river data and future covariates.

Result: DL models significantly outperformed HEC-RAS in accuracy and speed (500x faster), even under extreme conditions like tropical storms.

Conclusion: DL models are effective, efficient alternatives to traditional physics-based methods for water stage prediction.

Abstract: Simulating and predicting the water level/stage in river systems is essential
for flood warnings, hydraulic operations, and flood mitigations. Physics-based
detailed hydrological and hydraulic computational tools, such as HEC-RAS, MIKE,
and SWMM, can be used to simulate a complete watershed and compute the water
stage at any point in the river system. However, these physics-based models are
computationally intensive, especially for large watersheds and for longer
simulations, since they use detailed grid representations of terrain elevation
maps of the entire watershed and solve complex partial differential equations
(PDEs) for each grid cell. To overcome this problem, we train several deep
learning (DL) models for use as surrogate models to rapidly predict the water
stage. A portion of the Miami River in South Florida was chosen as a case study
for this paper. Extensive experiments show that the performance of various DL
models (MLP, RNN, CNN, LSTM, and RCNN) is significantly better than that of the
physics-based model, HEC-RAS, even during extreme precipitation conditions
(i.e., tropical storms), and with speedups exceeding 500x. To predict the water
stages more accurately, our DL models use both measured variables of the river
system from the recent past and covariates for which predictions are typically
available for the near future.

</details>


### [574] [Individualised Treatment Effects Estimation with Composite Treatments and Composite Outcomes](https://arxiv.org/pdf/2502.08282)
*Vinod Kumar Chauhan, Lei Clifton, Gaurav Nigam, David A. Clifton*

Main category: cs.LG

TL;DR: The paper introduces H-Learner, a hypernetwork-based method for estimating individualised treatment effects (ITE) under composite treatments and outcomes, addressing data scarcity by sharing information dynamically.


<details>
  <summary>Details</summary>
Motivation: Existing ITE estimation methods are limited to single treatments and outcomes, hindering their application in complex real-world scenarios like healthcare.

Method: Proposes H-Learner, a hypernetwork-based approach that dynamically shares information across treatments and outcomes to tackle data scarcity.

Result: Empirical analysis shows H-Learner's effectiveness with binary and arbitrary composite treatments and outcomes compared to existing methods.

Conclusion: H-Learner provides a scalable solution for ITE estimation in complex scenarios with composite treatments and outcomes.

Abstract: Estimating individualised treatment effect (ITE) -- that is the causal effect
of a set of variables (also called exposures, treatments, actions, policies, or
interventions), referred to as \textit{composite treatments}, on a set of
outcome variables of interest, referred to as \textit{composite outcomes}, for
a unit from observational data -- remains a fundamental problem in causal
inference with applications across disciplines, such as healthcare, economics,
education, social science, marketing, and computer science. Previous work in
causal machine learning for ITE estimation is limited to simple settings, like
single treatments and single outcomes. This hinders their use in complex
real-world scenarios; for example, consider studying the effect of different
ICU interventions, such as beta-blockers and statins for a patient admitted for
heart surgery, on different outcomes of interest such as atrial fibrillation
and in-hospital mortality. The limited research into composite treatments and
outcomes is primarily due to data scarcity for all treatments and outcomes. To
address the above challenges, we propose a novel and innovative
hypernetwork-based approach, called \emph{H-Learner}, to solve ITE estimation
under composite treatments and composite outcomes, which tackles the data
scarcity issue by dynamically sharing information across treatments and
outcomes. Our empirical analysis with binary and arbitrary composite treatments
and outcomes demonstrates the effectiveness of the proposed approach compared
to existing methods.

</details>


### [575] [Contaminated Multivariate Time-Series Anomaly Detection with Spatio-Temporal Graph Conditional Diffusion Models](https://arxiv.org/pdf/2308.12563)
*Thi Kieu Khanh Ho, Narges Armanfard*

Main category: cs.LG

TL;DR: TSAD-C is a novel unsupervised time-series anomaly detection method that handles noisy training data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Real-world anomaly detection often involves noisy training data, a challenge overlooked by mainstream algorithms.

Method: TSAD-C uses a Decontaminator, Long-range Variable Dependency Modeling, and Anomaly Scoring to detect anomalies without labeled training data.

Result: TSAD-C outperforms existing methods on four diverse datasets, setting a new state-of-the-art.

Conclusion: TSAD-C effectively addresses noisy training data in anomaly detection, advancing the field.

Abstract: Mainstream unsupervised anomaly detection algorithms often excel in academic
datasets, yet their real-world performance is restricted due to the controlled
experimental conditions involving clean training data. Addressing the challenge
of training with noise, a prevalent issue in practical anomaly detection, is
frequently overlooked. In a pioneering endeavor, this study delves into the
realm of label-level noise within sensory time-series anomaly detection (TSAD).
This paper presents a novel and practical end-to-end unsupervised TSAD when the
training data is contaminated with anomalies. The introduced approach, called
TSAD-C, is devoid of access to abnormality labels during the training phase.
TSAD-C encompasses three core modules: a Decontaminator to rectify anomalies
(aka noise) present during training, a Long-range Variable Dependency Modeling
module to capture long-term intra- and inter-variable dependencies within the
decontaminated data that is considered as a surrogate of the pure normal data,
and an Anomaly Scoring module to detect anomalies from all types. Our extensive
experiments conducted on four reliable and diverse datasets conclusively
demonstrate that TSAD-C surpasses existing methodologies, thus establishing a
new state-of-the-art in the TSAD field.

</details>


### [576] [Collaborative Deterministic-Diffusion Model for Probabilistic Spatiotemporal Prediction](https://arxiv.org/pdf/2502.11013)
*Zhi Sheng, Yuan Yuan, Yudi Zhang, Depeng Jin, Yong Li*

Main category: cs.LG

TL;DR: CoST combines deterministic and probabilistic models for urban spatiotemporal prediction, improving accuracy and uncertainty handling.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with multi-modal distributions and computational inefficiency, highlighting the need for a hybrid approach.

Method: CoST uses a mean-residual decomposition framework: deterministic models handle mean values, while diffusion models learn residuals. A scale-aware diffusion process addresses spatial heterogeneity.

Result: CoST outperforms existing methods by 20% in accuracy and efficiency across eight datasets.

Conclusion: CoST bridges deterministic precision and probabilistic uncertainty, advancing urban spatiotemporal prediction.

Abstract: Accurate prediction of urban spatiotemporal dynamics is essential for
enhancing urban management and decision-making. Existing spatiotemporal
prediction models are predominantly deterministic, focusing on primary
spatiotemporal patterns. However, those dynamics are highly complex, exhibiting
multi-modal distributions that are challenging for deterministic models to
capture. In this paper, we highlight the critical role of probabilistic
prediction in capturing the uncertainties and complexities inherent in
spatiotemporal data. While mainstream probabilistic models can capture
uncertainty, they struggle with accurately learning primary patterns and often
suffer from computational inefficiency. To address these challenges, we propose
CoST, which collaborates deterministic and probabilistic models to improve both
predictive accuracy and the ability to handle uncertainty. To achieve this, we
design a mean-residual decomposition framework, where the mean value is modeled
by a deterministic model, and the residual variations are learned by a
probabilistic model, specifically diffusion models. Moreover, we introduce a
scale-aware diffusion process, which better accounts for spatially
heterogeneous dynamics across different regions. Extensive experiments on eight
real-world datasets demonstrate that CoST significantly outperforms existing
methods in both deterministic and probabilistic metrics, achieving a 20%
improvement with low computational cost. CoST bridges the gap between
deterministic precision and probabilistic uncertainty, making a significant
advancement in the field of urban spatiotemporal prediction.

</details>


### [577] [Jointly spatial-temporal representation learning for individual trajectories](https://arxiv.org/pdf/2312.04055)
*Fei Huang, Jianrong Lv, Yang Yue*

Main category: cs.LG

TL;DR: The paper proposes ST-GraphRL, a method to learn spatial-temporal dependencies in trajectories for geospatial foundation models, outperforming baselines in mobility prediction and similarity preservation.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to encode implicit spatial-temporal dependencies in trajectories, limiting general-purpose representation learning.

Method: ST-GraphRL uses a weighted directed spatial-temporal graph, a two-stage encoder (decoupling and fusion), and a decoder to learn and simulate spatial-temporal patterns.

Result: ST-GraphRL outperforms baselines in predicting spatial-temporal distributions and preserving trajectory similarity, with validated spatial-temporal pattern understanding.

Conclusion: The method advances geospatial data representation learning and supports GeoFMs development.

Abstract: Individual trajectories, rich in human-environment interaction information
across space and time, serve as vital inputs for geospatial foundation models
(GeoFMs). However, existing attempts at learning trajectory representations
have overlooked the implicit spatial-temporal dependency within trajectories,
failing to encode such dependency in a deep learning-friendly format. That
poses a challenge in obtaining general-purpose trajectory representations.
Therefore, this paper proposes a spatial-temporal joint representation learning
method (ST-GraphRL) to formalize learnable spatial-temporal dependencies into
trajectory representations. The proposed ST-GraphRL consists of three
compositions: (i) a weighted directed spatial-temporal graph to explicitly
construct mobility interactions in both space and time dimensions; (ii) a
two-stage jointly encoder (i.e., decoupling and fusion), to learn entangled
spatial-temporal dependencies by independently decomposing and jointly
aggregating space and time information; (iii) a decoder guides ST-GraphRL to
learn explicit mobility regularities by simulating the spatial-temporal
distributions of trajectories. Tested on three real-world human mobility
datasets, the proposed ST-GraphRL outperformed all the baseline models in
predicting movement spatial-temporal distributions and preserving trajectory
similarity with high spatial-temporal correlations. Analyzing spatial-temporal
features presented in latent space validates that ST-GraphRL understands
spatial-temporal patterns. This study may also benefit representation learnings
of other geospatial data to achieve general-purpose data representations and
advance GeoFMs development.

</details>


### [578] [Tiled Flash Linear Attention: More Efficient Linear RNN and xLSTM Kernels](https://arxiv.org/pdf/2503.14376)
*Maximilian Beck, Korbinian Pöppel, Phillip Lippe, Sepp Hochreiter*

Main category: cs.LG

TL;DR: TFLA introduces a tiled kernel algorithm for linear RNNs, enabling large chunk sizes and high arithmetic intensity, outperforming existing methods in speed and efficiency for long-context modeling.


<details>
  <summary>Details</summary>
Motivation: To address the memory and IO inefficiencies of Flash Linear Attention (FLA) in linear RNNs, especially for long-context pre-training.

Method: Proposes Tiled Flash Linear Attention (TFLA), a kernel algorithm with sequence parallelization within chunks, and applies it to mLSTM with optimizations.

Result: TFLA-based mLSTM kernels outperform Flash Attention, Linear Attention, and Mamba kernels in speed benchmarks.

Conclusion: TFLA sets a new state of the art for efficient long-context sequence modeling, combining high performance with reduced computational overhead.

Abstract: Linear RNNs with gating recently demonstrated competitive performance
compared to Transformers in language modeling. Although their linear compute
scaling in sequence length offers theoretical runtime advantages over
Transformers, realizing these benefits in practice requires optimized custom
kernels, as Transformers rely on the highly efficient Flash Attention kernels
(Dao, 2024). Leveraging the chunkwise-parallel formulation of linear RNNs,
Flash Linear Attention (FLA) (Yang & Zhang, 2024) shows that linear RNN kernels
are faster than Flash Attention, by parallelizing over chunks of the input
sequence. However, since the chunk size of FLA is limited, many intermediate
states must be materialized in GPU memory. This leads to low arithmetic
intensity and causes high memory consumption and IO cost, especially for
long-context pre-training. In this work, we present Tiled Flash Linear
Attention (TFLA), a novel kernel algorithm for linear RNNs, that enables
arbitrary large chunk sizes and high arithmetic intensity by introducing an
additional level of sequence parallelization within each chunk. First, we apply
TFLA to the xLSTM with matrix memory, the mLSTM (Beck et al., 2024). Second, we
propose an mLSTM variant with sigmoid input gate and reduced computation for
even faster kernel runtimes at equal language modeling performance. In our
speed benchmarks, we show that our new mLSTM kernels based on TFLA outperform
highly optimized Flash Attention, Linear Attention and Mamba kernels, setting a
new state of the art for efficient long-context sequence modeling primitives.

</details>


### [579] [Identifying Drivers of Predictive Aleatoric Uncertainty](https://arxiv.org/pdf/2312.07252)
*Pascal Iversen, Simon Witzke, Katharina Baum, Bernhard Y. Renard*

Main category: cs.LG

TL;DR: A simple method to explain predictive aleatoric uncertainties in AI, outperforming complex approaches with minimal model changes.


<details>
  <summary>Details</summary>
Motivation: To enhance trust in AI by explaining uncertainty estimates, which are often left unexplained, using straightforward methods.

Method: Adapts a neural network with a Gaussian output distribution to estimate uncertainty as predictive variance, then applies standard explainers to the variance output.

Result: The approach reliably explains uncertainty influences, outperforming complex methods in synthetic and real datasets.

Conclusion: The proposed method effectively explains uncertainty estimates with minimal architectural changes and superior performance in most cases.

Abstract: Explainability and uncertainty quantification are key to trustable artificial
intelligence. However, the reasoning behind uncertainty estimates is generally
left unexplained. Identifying the drivers of uncertainty complements
explanations of point predictions in recognizing model limitations and
enhancing transparent decision-making. So far, explanations of uncertainties
have been rarely studied. The few exceptions rely on Bayesian neural networks
or technically intricate approaches, such as auxiliary generative models,
thereby hindering their broad adoption. We propose a straightforward approach
to explain predictive aleatoric uncertainties. We estimate uncertainty in
regression as predictive variance by adapting a neural network with a Gaussian
output distribution. Subsequently, we apply out-of-the-box explainers to the
model's variance output. This approach can explain uncertainty influences more
reliably than complex published approaches, which we demonstrate in a synthetic
setting with a known data-generating process. We substantiate our findings with
a nuanced, quantitative benchmark including synthetic and real, tabular and
image datasets. For this, we adapt metrics from conventional XAI research to
uncertainty explanations. Overall, the proposed method explains uncertainty
estimates with little modifications to the model architecture and outperforms
more intricate methods in most settings.

</details>


### [580] [Adaptive Message Passing: A General Framework to Mitigate Oversmoothing, Oversquashing, and Underreaching](https://arxiv.org/pdf/2312.16560)
*Federico Errica, Henrik Christiansen, Viktor Zaverkin, Takashi Maruyama, Mathias Niepert, Francesco Alesiani*

Main category: cs.LG

TL;DR: A framework is proposed to improve deep graph networks' ability to capture long-range interactions by adapting message passing depth and filtering messages, addressing limitations like oversmoothing and oversquashing.


<details>
  <summary>Details</summary>
Motivation: Long-range interactions are crucial for complex systems but computationally expensive. Deep graph networks struggle to model these due to message passing limitations.

Method: A variational inference framework adapts message passing depth and filters messages to mitigate oversmoothing, oversquashing, and underreaching.

Result: The framework competes with state-of-the-art on five node and graph prediction datasets, better capturing long-range interactions.

Conclusion: The proposed method effectively addresses limitations of deep graph networks, enhancing their ability to model long-range dependencies.

Abstract: Long-range interactions are essential for the correct description of complex
systems in many scientific fields. The price to pay for including them in the
calculations, however, is a dramatic increase in the overall computational
costs. Recently, deep graph networks have been employed as efficient,
data-driven models for predicting properties of complex systems represented as
graphs. These models rely on a message passing strategy that should, in
principle, capture long-range information without explicitly modeling the
corresponding interactions. In practice, most deep graph networks cannot really
model long-range dependencies due to the intrinsic limitations of (synchronous)
message passing, namely oversmoothing, oversquashing, and underreaching. This
work proposes a general framework that learns to mitigate these limitations:
within a variational inference framework, we endow message passing
architectures with the ability to adapt their depth and filter messages along
the way. With theoretical and empirical arguments, we show that this strategy
better captures long-range interactions, by competing with the state of the art
on five node and graph prediction datasets.

</details>


### [581] [Tight Finite Time Bounds of Two-Time-Scale Linear Stochastic Approximation with Markovian Noise](https://arxiv.org/pdf/2401.00364)
*Shaan Ul Haque, Sajad Khodadadian, Siva Theja Maguluri*

Main category: cs.LG

TL;DR: The paper derives tight finite-time error bounds for linear two-time-scale stochastic approximation (SA) with Markovian noise, showing mean squared error decreases as $trace(\Sigma^y)/k + o(1/k)$. The bounds match the Central Limit Theorem covariance and are applied to RL algorithms like TDC, GTD, and GTD2.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address theoretical challenges in analyzing two-time-scale SA with Markovian noise, particularly in RL applications like GTD methods.

Method: The method involves deriving upper bounds on error for linear two-time-scale SA with Markovian noise, focusing on mean squared error and covariance matching the CLT.

Result: The result shows the mean squared error decreases as $trace(\Sigma^y)/k + o(1/k)$, with tight bounds matching the CLT covariance. Applications include sample complexity for RL algorithms.

Conclusion: The paper concludes with tight finite-time bounds for linear SA, applicable to RL algorithms, and extends to Polyak-Ruppert averaging in TD-learning.

Abstract: Stochastic approximation (SA) is an iterative algorithm for finding the fixed
point of an operator using noisy samples and widely used in optimization and
Reinforcement Learning (RL). The noise in RL exhibits a Markovian structure,
and in some cases, such as gradient temporal difference (GTD) methods, SA is
employed in a two-time-scale framework. This combination introduces significant
theoretical challenges for analysis.
  We derive an upper bound on the error for the iterations of linear
two-time-scale SA with Markovian noise. We demonstrate that the mean squared
error decreases as $trace (\Sigma^y)/k + o(1/k)$ where $k$ is the number of
iterates, and $\Sigma^y$ is an appropriately defined covariance matrix. A key
feature of our bounds is that the leading term, $\Sigma^y$, exactly matches
with the covariance in the Central Limit Theorem (CLT) for the two-time-scale
SA, and we call them tight finite-time bounds. We illustrate their use in RL by
establishing sample complexity for off-policy algorithms, TDC, GTD, and GTD2.
  A special case of linear two-time-scale SA that is extensively studied is
linear SA with Polyak-Ruppert averaging. We present tight finite time bounds
corresponding to the covariance matrix of the CLT. Such bounds can be used to
study TD-learning with Polyak-Ruppert averaging.

</details>


### [582] [Leveraging Gradients for Unsupervised Accuracy Estimation under Distribution Shift](https://arxiv.org/pdf/2401.08909)
*Renchunzi Xie, Ambroise Odonnat, Vasilii Feofanov, Ievgen Redko, Jianfeng Zhang, Bo An*

Main category: cs.LG

TL;DR: The paper proposes a method to estimate model test performance under distribution shifts using gradients, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Estimating test performance without ground-truth labels is crucial for safe ML deployment, especially under distribution shifts.

Method: Uses the norm of classification-layer gradients from one gradient step on test data, correlating higher gradient magnitudes with poor generalization.

Result: Extensive experiments show the method significantly outperforms state-of-the-art approaches across diverse distribution shifts.

Conclusion: Gradient-based estimation is effective for predicting test accuracy under distribution shifts, offering theoretical and empirical support.

Abstract: Estimating the test performance of a model, possibly under distribution
shift, without having access to the ground-truth labels is a challenging, yet
very important problem for the safe deployment of machine learning algorithms
in the wild. Existing works mostly rely on information from either the outputs
or the extracted features of neural networks to estimate a score that
correlates with the ground-truth test accuracy. In this paper, we investigate
-- both empirically and theoretically -- how the information provided by the
gradients can be predictive of the ground-truth test accuracy even under
distribution shifts. More specifically, we use the norm of classification-layer
gradients, backpropagated from the cross-entropy loss after only one gradient
step over test data. Our intuition is that these gradients should be of higher
magnitude when the model generalizes poorly. We provide the theoretical
insights behind our approach and the key ingredients that ensure its empirical
success. Extensive experiments conducted with various architectures on diverse
distribution shifts demonstrate that our method significantly outperforms
current state-of-the-art approaches. The code is available at
https://github.com/Renchunzi-Xie/GdScore

</details>


### [583] [Enhancing stroke disease classification through machine learning models by feature selection techniques](https://arxiv.org/pdf/2504.00485)
*Mahade Hasan, Farhana Yasmin, Xue Yu*

Main category: cs.LG

TL;DR: The paper evaluates nine ML algorithms for heart disease prediction, with XGBoost achieving 99% accuracy and 100% ROC AUC, highlighting its potential for early diagnosis.


<details>
  <summary>Details</summary>
Motivation: Heart disease is a major global health issue, but existing ML models lack high accuracy. This study aims to improve predictive performance.

Method: Nine ML algorithms (e.g., XGBoost, SVM) were applied with feature selection, hyperparameter tuning, and a novel voting system. Performance was evaluated using metrics like accuracy and ROC AUC.

Result: XGBoost outperformed others with 99% accuracy, 98% recall, and 100% ROC AUC.

Conclusion: The study presents a highly accurate approach for heart disease prediction, emphasizing XGBoost's effectiveness for early diagnosis.

Abstract: Heart disease remains a leading cause of mortality and morbidity worldwide,
necessitating the development of accurate and reliable predictive models to
facilitate early detection and intervention. While state of the art work has
focused on various machine learning approaches for predicting heart disease,
but they could not able to achieve remarkable accuracy. In response to this
need, we applied nine machine learning algorithms XGBoost, logistic regression,
decision tree, random forest, k-nearest neighbors (KNN), support vector machine
(SVM), gaussian na\"ive bayes (NB gaussian), adaptive boosting, and linear
regression to predict heart disease based on a range of physiological
indicators. Our approach involved feature selection techniques to identify the
most relevant predictors, aimed at refining the models to enhance both
performance and interpretability. The models were trained, incorporating
processes such as grid search hyperparameter tuning, and cross-validation to
minimize overfitting. Additionally, we have developed a novel voting system
with feature selection techniques to advance heart disease classification.
Furthermore, we have evaluated the models using key performance metrics
including accuracy, precision, recall, F1-score, and the area under the
receiver operating characteristic curve (ROC AUC). Among the models, XGBoost
demonstrated exceptional performance, achieving 99% accuracy, precision,
F1-Score, 98% recall, and 100% ROC AUC. This study offers a promising approach
to early heart disease diagnosis and preventive healthcare.

</details>


### [584] [Beyond DAGs: A Latent Partial Causal Model for Multimodal Learning](https://arxiv.org/pdf/2402.06223)
*Yuhang Liu, Zhen Zhang, Dong Gong, Erdun Gao, Biwei Huang, Mingming Gong, Anton van den Hengel, Kun Zhang, Javen Qinfeng Shi*

Main category: cs.LG

TL;DR: The paper proposes a latent partial causal model for multimodal data, relaxing the DAG assumption, and shows identifiability of latent variables via multimodal contrastive learning, validated by synthetic and real-world experiments.


<details>
  <summary>Details</summary>
Motivation: Traditional DAG-based causal models require strong assumptions, which may not hold for latent causal models and complex multimodal data. The work aims to relax these assumptions while maintaining practical utility.

Method: A novel latent partial causal model with two latent coupled variables connected by an undirected edge is proposed. Identifiability is established under specific statistical assumptions, linking it to multimodal contrastive learning.

Result: The model demonstrates identifiability of latent variables, robustness in synthetic experiments, and practical utility in pre-trained models like CLIP, improving few-shot learning and domain generalization.

Conclusion: The work advances multimodal contrastive learning theoretically and practically, showing its potential for disentanglement and broader applications.

Abstract: Directed acyclic graphs (DAGs) are fundamental graph structures in causal
modeling, but identifying the desired DAG from observational data often
requires strong assumptions that may not hold in real-world scenarios,
especially for latent causal models and complex multimodal data. This raises
the question of whether we can relax or bypass the DAG assumption while
maintaining practical utility. In this work, we propose a novel latent partial
causal model for multimodal data, featuring two latent coupled variables,
connected by an undirected edge, to represent the transfer of knowledge across
modalities. Under specific statistical assumptions, we establish an
identifiability result, demonstrating that representations learned by
multimodal contrastive learning correspond to the latent coupled variables up
to a trivial transformation. This result deepens our understanding of the why
multimodal contrastive learning works, highlights its potential for
disentanglement, and expands the utility of pre-trained models like CLIP.
Synthetic experiments confirm the robustness of our findings, even when the
assumptions are partially violated. Most importantly, experiments on a
pre-trained CLIP model embodies disentangled representations, enabling few-shot
learning and improving domain generalization across diverse real-world
datasets. Together, these contributions push the boundaries of multimodal
contrastive learning, both theoretically and, crucially, in practical
applications.

</details>


### [585] [Order-Optimal Regret with Novel Policy Gradient Approaches in Infinite-Horizon Average Reward MDPs](https://arxiv.org/pdf/2404.02108)
*Swetha Ganesh, Washim Uddin Mondal, Vaneet Aggarwal*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present two Policy Gradient-based algorithms with general parametrization
in the context of infinite-horizon average reward Markov Decision Process
(MDP). The first one employs Implicit Gradient Transport for variance
reduction, ensuring an expected regret of the order
$\tilde{\mathcal{O}}(T^{2/3})$. The second approach, rooted in Hessian-based
techniques, ensures an expected regret of the order
$\tilde{\mathcal{O}}(\sqrt{T})$. These results significantly improve the
state-of-the-art $\tilde{\mathcal{O}}(T^{3/4})$ regret and achieve the
theoretical lower bound. We also show that the average-reward function is
approximately $L$-smooth, a result that was previously assumed in earlier
works.

</details>


### [586] [MetaMolGen: A Neural Graph Motif Generation Model for De Novo Molecular Design](https://arxiv.org/pdf/2504.15587)
*Zimo Yan, Jie Zhang, Zheng Xie, Chang Liu, Yizhen Liu, Yiping Song*

Main category: cs.LG

TL;DR: MetaMolGen is a meta-learning-based molecular generator for few-shot and property-conditioned molecular generation, outperforming traditional models in low-data scenarios.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of conditional molecular generation in data-scarce settings where traditional models struggle.

Method: Uses a normalized latent space for graph motifs and a lightweight autoregressive model for SMILES generation, integrating a learnable property projector for conditional generation.

Result: Generates valid and diverse SMILES sequences in low-data regimes, outperforming conventional baselines.

Conclusion: MetaMolGen excels in fast adaptation and efficient conditional generation for practical molecular design.

Abstract: Molecular generation plays an important role in drug discovery and materials
science, especially in data-scarce scenarios where traditional generative
models often struggle to achieve satisfactory conditional generalization. To
address this challenge, we propose MetaMolGen, a first-order
meta-learning-based molecular generator designed for few-shot and
property-conditioned molecular generation. MetaMolGen standardizes the
distribution of graph motifs by mapping them to a normalized latent space, and
employs a lightweight autoregressive sequence model to generate SMILES
sequences that faithfully reflect the underlying molecular structure. In
addition, it supports conditional generation of molecules with target
properties through a learnable property projector integrated into the
generative process.Experimental results demonstrate that MetaMolGen
consistently generates valid and diverse SMILES sequences under low-data
regimes, outperforming conventional baselines. This highlights its advantage in
fast adaptation and efficient conditional generation for practical molecular
design.

</details>


### [587] [MD-NOMAD: Mixture density nonlinear manifold decoder for emulating stochastic differential equations and uncertainty propagation](https://arxiv.org/pdf/2404.15731)
*Akshay Thakur, Souvik Chakraborty*

Main category: cs.LG

TL;DR: MD-NOMAD combines NOMAD with mixture density methods to estimate conditional probability distributions for stochastic simulators, showing strong performance in empirical tests.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of estimating complex probability distributions for stochastic output functions in simulators.

Method: Leverages NOMAD's neural architecture and mixture density-based methods to model conditional probability distributions.

Result: Demonstrates strong performance in empirical assessments on stochastic ODEs and PDEs.

Conclusion: MD-NOMAD effectively combines probabilistic modeling and scalability for stochastic simulators.

Abstract: We propose a neural operator framework, termed mixture density nonlinear
manifold decoder (MD-NOMAD), for stochastic simulators. Our approach leverages
an amalgamation of the pointwise operator learning neural architecture
nonlinear manifold decoder (NOMAD) with mixture density-based methods to
estimate conditional probability distributions for stochastic output functions.
MD-NOMAD harnesses the ability of probabilistic mixture models to estimate
complex probability and the high-dimensional scalability of pointwise neural
operator NOMAD. We conduct empirical assessments on a wide array of stochastic
ordinary and partial differential equations and present the corresponding
results, which highlight the performance of the proposed framework.

</details>


### [588] [Statistical Guarantees in Synthetic Data through Conformal Adversarial Generation](https://arxiv.org/pdf/2504.17058)
*Rahul Vishwakarma, Shrey Dharmendra Modi, Vishwanath Seshagiri*

Main category: cs.LG

TL;DR: The paper introduces Conformalized GAN (cGAN), a framework integrating conformal prediction into GANs to provide statistical guarantees for synthetic data generation.


<details>
  <summary>Details</summary>
Motivation: Existing generative models lack rigorous statistical guarantees, limiting their use in critical domains. The paper aims to address this by ensuring uncertainty quantification in synthetic data.

Method: The framework incorporates multiple conformal prediction paradigms (ICP, Mondrian, Cross-Conformal, Venn-Abers) into GANs to achieve distribution-free uncertainty quantification.

Result: cGAN shows improved calibration and maintains generative power, offering provable statistical guarantees for synthetic data.

Conclusion: The approach enables reliable synthetic data use in high-stakes domains like healthcare and finance, with proven validity and efficiency.

Abstract: The generation of high-quality synthetic data presents significant challenges
in machine learning research, particularly regarding statistical fidelity and
uncertainty quantification. Existing generative models produce compelling
synthetic samples but lack rigorous statistical guarantees about their relation
to the underlying data distribution, limiting their applicability in critical
domains requiring robust error bounds. We address this fundamental limitation
by presenting a novel framework that incorporates conformal prediction
methodologies into Generative Adversarial Networks (GANs). By integrating
multiple conformal prediction paradigms including Inductive Conformal
Prediction (ICP), Mondrian Conformal Prediction, Cross-Conformal Prediction,
and Venn-Abers Predictors, we establish distribution-free uncertainty
quantification in generated samples. This approach, termed Conformalized GAN
(cGAN), demonstrates enhanced calibration properties while maintaining the
generative power of traditional GANs, producing synthetic data with provable
statistical guarantees. We provide rigorous mathematical proofs establishing
finite-sample validity guarantees and asymptotic efficiency properties,
enabling the reliable application of synthetic data in high-stakes domains
including healthcare, finance, and autonomous systems.

</details>


### [589] [AttackBench: Evaluating Gradient-based Attacks for Adversarial Examples](https://arxiv.org/pdf/2404.19460)
*Antonio Emanuele Cinà, Jérôme Rony, Maura Pintor, Luca Demetrio, Ambra Demontis, Battista Biggio, Ismail Ben Ayed, Fabio Roli*

Main category: cs.LG

TL;DR: AttackBench is a framework for fair comparison of gradient-based adversarial attacks, addressing biased evaluations in existing methods.


<details>
  <summary>Details</summary>
Motivation: Current adversarial attack evaluations are overly optimistic and biased due to inconsistent experimental setups, favoring certain attacks unfairly.

Method: Proposes AttackBench, categorizing attacks and evaluating effectiveness and efficiency via an optimality metric and query budget constraints.

Result: Extensive experiments on 100+ attack implementations show few outperform others, revealing implementation issues.

Conclusion: AttackBench is released as a public benchmark to standardize and update evaluations of gradient-based attacks.

Abstract: Adversarial examples are typically optimized with gradient-based attacks.
While novel attacks are continuously proposed, each is shown to outperform its
predecessors using different experimental setups, hyperparameter settings, and
number of forward and backward calls to the target models. This provides
overly-optimistic and even biased evaluations that may unfairly favor one
particular attack over the others. In this work, we aim to overcome these
limitations by proposing AttackBench, i.e., the first evaluation framework that
enables a fair comparison among different attacks. To this end, we first
propose a categorization of gradient-based attacks, identifying their main
components and differences. We then introduce our framework, which evaluates
their effectiveness and efficiency. We measure these characteristics by (i)
defining an optimality metric that quantifies how close an attack is to the
optimal solution, and (ii) limiting the number of forward and backward queries
to the model, such that all attacks are compared within a given maximum query
budget. Our extensive experimental analysis compares more than $100$ attack
implementations with a total of over $800$ different configurations against
CIFAR-10 and ImageNet models, highlighting that only very few attacks
outperform all the competing approaches. Within this analysis, we shed light on
several implementation issues that prevent many attacks from finding better
solutions or running at all. We release AttackBench as a publicly-available
benchmark, aiming to continuously update it to include and evaluate novel
gradient-based attacks for optimizing adversarial examples.

</details>


### [590] [A constraints-based approach to fully interpretable neural networks for detecting learner behaviors](https://arxiv.org/pdf/2504.20055)
*Juan D. Pinto, Luc Paquette*

Main category: cs.LG

TL;DR: A novel interpretable neural-network-based model for detecting gaming-the-system behavior in education, designed to provide faithful and intelligible explanations.


<details>
  <summary>Details</summary>
Motivation: Address concerns about the interpretability of complex machine learning models in education by developing a model that is interpretable by design.

Method: Implement constraints to simplify inference and align with human task conception, training the model to detect gaming-the-system behavior.

Result: The model successfully learns patterns indicative of gaming-the-system behavior and provides fully interpretable explanations.

Conclusion: The approach demonstrates the feasibility of interpretable models in education, suggesting human-grounded evaluation for explainability.

Abstract: The increasing use of complex machine learning models in education has led to
concerns about their interpretability, which in turn has spurred interest in
developing explainability techniques that are both faithful to the model's
inner workings and intelligible to human end-users. In this paper, we describe
a novel approach to creating a neural-network-based behavior detection model
that is interpretable by design. Our model is fully interpretable, meaning that
the parameters we extract for our explanations have a clear interpretation,
fully capture the model's learned knowledge about the learner behavior of
interest, and can be used to create explanations that are both faithful and
intelligible. We achieve this by implementing a series of constraints to the
model that both simplify its inference process and bring it closer to a human
conception of the task at hand. We train the model to detect gaming-the-system
behavior, evaluate its performance on this task, and compare its learned
patterns to those identified by human experts. Our results show that the model
is successfully able to learn patterns indicative of gaming-the-system behavior
while providing evidence for fully interpretable explanations. We discuss the
implications of our approach and suggest ways to evaluate explainability using
a human-grounded approach.

</details>


### [591] [Scaling up the Banded Matrix Factorization Mechanism for Differentially Private ML](https://arxiv.org/pdf/2405.15913)
*Ryan McKenna*

Main category: cs.LG

TL;DR: Techniques to scale DP-BandMF for large-scale training, overcoming limitations in iterations and parameters.


<details>
  <summary>Details</summary>
Motivation: DP-BandMF's scalability issues hinder its use in large-scale scenarios with many iterations or parameters.

Method: Developed techniques to extend DP-BandMF's scalability for high iteration counts and large parameter sets.

Result: Enabled DP-BandMF to handle virtually any number of parameters and iterations with minimal utility loss.

Conclusion: Scaled DP-BandMF effectively broadens its applicability in large-scale training without sacrificing utility.

Abstract: Correlated noise mechanisms such as DP Matrix Factorization (DP-MF) have
proven to be effective alternatives to DP-SGD in large-epsilon few-epoch
training regimes. Significant work has been done to find the best correlated
noise strategies, and the current state-of-the-art approach is DP-BandMF, which
optimally balances the benefits of privacy amplification and noise correlation.
Despite it's utility advantages, severe scalability limitations prevent this
mechanism from handling large-scale training scenarios where the number of
training iterations may exceed $10^4$ and the number of model parameters may
exceed $10^7$. In this work, we present techniques to scale up DP-BandMF along
these two dimensions, significantly extending it's reach and enabling it to
handle settings with virtually any number of model parameters and training
iterations, with negligible utility degradation.

</details>


### [592] [Token-Efficient RL for LLM Reasoning](https://arxiv.org/pdf/2504.20834)
*Alan Lee, Harry Tong*

Main category: cs.LG

TL;DR: Proposes RL strategies for LLMs under memory/compute limits, focusing on LoRA compatibility. Introduces S-GRPO and T-SPMO, improving SVAMP accuracy from 46% to 70%.


<details>
  <summary>Details</summary>
Motivation: Address memory and compute constraints in LLMs while maintaining compatibility with LoRA fine-tuning.

Method: Uses critic-free RL methods on token subsets (S-GRPO, T-SPMO) for efficiency and stability.

Result: Achieves 70% accuracy on SVAMP, strong multi-digit multiplication performance. Full-token GRPO under LoRA fails.

Conclusion: Selective token-level optimization may act as an implicit regularizer in low-parameter training.

Abstract: We propose reinforcement learning (RL) strategies tailored for reasoning in
large language models (LLMs) under strict memory and compute limits, with a
particular focus on compatibility with LoRA fine-tuning. Building on early
policy gradient methods with baseline subtraction, we design critic-free
methods that operate on a small, informative subset of output tokens to reduce
memory usage and stabilize training. We introduce S-GRPO, a stochastic variant
of Group Relative Policy Optimization, and T-SPMO, a token-level prefix
matching approach for fine-grained credit assignment. Applied to Qwen2-1.5B,
our methods raise accuracy on the SVAMP benchmark from 46% to over 70% and show
strong performance on multi-digit multiplication. Surprisingly, full-token GRPO
under LoRA fails to improve over the base model, suggesting that selective
token-level optimization may act as an implicit regularizer in low-parameter
training regimes.

</details>


### [593] [DEFT: Efficient Fine-Tuning of Diffusion Models by Learning the Generalised $h$-transform](https://arxiv.org/pdf/2406.01781)
*Alexander Denker, Francisco Vargas, Shreyas Padhy, Kieran Didi, Simon Mathis, Vincent Dutordoir, Riccardo Barbano, Emile Mathieu, Urszula Julia Komorowska, Pietro Lio*

Main category: cs.LG

TL;DR: The paper introduces DEFT, a method for conditional generation using Doob's h-transform, unifying existing approaches and achieving faster, high-performance results.


<details>
  <summary>Details</summary>
Motivation: To address issues like hyperparameter sensitivity and inefficiency in existing conditional sampling methods for diffusion models.

Method: Proposes DEFT, which fine-tunes a small network to learn the conditional h-transform while keeping the unconditional model unchanged.

Result: DEFT achieves state-of-the-art performance, with speedups up to 1.6x and superior perceptual quality on tasks like image reconstruction and protein motif scaffolding.

Conclusion: DEFT provides a unified, efficient, and high-performing framework for conditional generation in diffusion models.

Abstract: Generative modelling paradigms based on denoising diffusion processes have
emerged as a leading candidate for conditional sampling in inverse problems. In
many real-world applications, we often have access to large, expensively
trained unconditional diffusion models, which we aim to exploit for improving
conditional sampling. Most recent approaches are motivated heuristically and
lack a unifying framework, obscuring connections between them. Further, they
often suffer from issues such as being very sensitive to hyperparameters, being
expensive to train or needing access to weights hidden behind a closed API. In
this work, we unify conditional training and sampling using the mathematically
well-understood Doob's h-transform. This new perspective allows us to unify
many existing methods under a common umbrella. Under this framework, we propose
DEFT (Doob's h-transform Efficient FineTuning), a new approach for conditional
generation that simply fine-tunes a very small network to quickly learn the
conditional $h$-transform, while keeping the larger unconditional network
unchanged. DEFT is much faster than existing baselines while achieving
state-of-the-art performance across a variety of linear and non-linear
benchmarks. On image reconstruction tasks, we achieve speedups of up to
1.6$\times$, while having the best perceptual quality on natural images and
reconstruction performance on medical images. Further, we also provide initial
experiments on protein motif scaffolding and outperform reconstruction guidance
methods.

</details>


### [594] [Branches: Efficiently Seeking Optimal Sparse Decision Trees with AO*](https://arxiv.org/pdf/2406.02175)
*Ayman Chaouki, Jesse Read, Albert Bifet*

Main category: cs.LG

TL;DR: The paper introduces Branches, an AO*-type algorithm for Decision Tree Learning, addressing inefficiencies in Depth-First-Search and memory issues in Best-First-Search, with proven optimality and support for non-binary features.


<details>
  <summary>Details</summary>
Motivation: Current Decision Tree Learning methods face inefficiencies in high-depth searches and memory consumption, prompting the need for a more efficient approach.

Method: The problem is framed within an AND/OR graph search framework and solved using Branches, a novel AO*-type algorithm.

Result: Branches outperforms state-of-the-art methods in efficiency and supports non-binary features, enhancing computational gains.

Conclusion: Branches offers a theoretically and practically superior solution for Decision Tree Learning, with broader applicability due to non-binary feature support.

Abstract: Decision Tree (DT) Learning is a fundamental problem in Interpretable Machine
Learning, yet it poses a formidable optimisation challenge. Practical
algorithms have recently emerged, primarily leveraging Dynamic Programming and
Branch & Bound. However, most of these approaches rely on a Depth-First-Search
strategy, which is inefficient when searching for DTs at high depths and
requires the definition of a maximum depth hyperparameter. Best-First-Search
was also employed by other methods to circumvent these issues. The downside of
this strategy is its higher memory consumption, as such, it has to be designed
in a fully efficient manner that takes full advantage of the problem's
structure. We formulate the problem within an AND/OR graph search framework and
we solve it with a novel AO*-type algorithm called Branches. We prove both
optimality and complexity guarantees for Branches and we show that it is more
efficient than the state of the art theoretically and on a variety of
experiments. Furthermore, Branches supports non-binary features unlike the
other methods, we show that this property can further induce larger gains in
computational efficiency.

</details>


### [595] [Marginalization Consistent Probabilistic Forecasting of Irregular Time Series via Mixture of Separable flows](https://arxiv.org/pdf/2406.07246)
*Vijaya Krishna Yalavarthi, Randolf Scholz, Christian Kloetergens, Kiran Madhusudhanan, Stefan Born, Lars Schmidt-Thieme*

Main category: cs.LG

TL;DR: MOSES is a new model for probabilistic forecasting in irregular time series with missing values, addressing marginalization inconsistency in existing methods like ProFITi. It combines Gaussian Processes and Normalizing Flows for accurate joint and marginal predictions.


<details>
  <summary>Details</summary>
Motivation: Existing models like ProFITi lack marginalization consistency, leading to inaccurate marginal predictions. MOSES aims to solve this issue while maintaining strong joint prediction performance.

Method: MOSES uses a mixture of latent multivariate Gaussian Processes and separable univariate Normalizing Flows, enabling analytical marginalization for broader probabilistic queries.

Result: MOSES outperforms other marginalization-consistent baselines in joint and marginal predictions, nearly matching ProFITi in joint prediction but excelling in marginal accuracy.

Conclusion: MOSES provides a robust solution for probabilistic forecasting, balancing joint and marginal prediction accuracy, and addressing a critical gap in existing models.

Abstract: Probabilistic forecasting models for joint distributions of targets in
irregular time series with missing values are a heavily under-researched area
in machine learning, with, to the best of our knowledge, only two Models have
been researched so far: The Gaussian Process Regression model, and ProFITi.
While ProFITi, thanks to using multivariate normalizing flows, is very
expressive, leading to better predictive performance, it suffers from
marginalization inconsistency: It does not guarantee that the marginal
distributions of a subset of variables in its predictive distributions coincide
with the directly predicted distributions of these variables. When asked to
directly predict marginal distributions, they are often vastly inaccurate. We
propose MOSES (Marginalization Consistent Mixture of Separable Flows), a model
that parametrizes a stochastic process through a mixture of several latent
multivariate Gaussian Processes combined with separable univariate Normalizing
Flows. In particular, MOSES can be analytically marginalized, allowing it to
directly answer a wider range of probabilistic queries than most competitors.
Experiments on four datasets show that MOSES achieves both accurate joint and
marginal predictions, surpassing all other marginalization consistent
baselines, while only trailing slightly behind ProFITi in joint prediction, but
vastly superior when predicting marginal distributions.

</details>


### [596] [Rewriting Pre-Training Data Boosts LLM Performance in Math and Code](https://arxiv.org/pdf/2505.02881)
*Kazuki Fujii, Yukito Tajima, Sakae Mizuki, Hinari Shimada, Taihei Shiotani, Koshiro Saito, Masanari Ohi, Masaki Kawamura, Taishi Nakamura, Takumi Okamoto, Shigeki Ishida, Kakeru Hattori, Youmi Ma, Hiroya Takamura, Rio Yokota, Naoaki Okazaki*

Main category: cs.LG

TL;DR: Two datasets, SwallowCode and SwallowMath, enhance LLM performance by refining public data through systematic rewriting, improving code generation and mathematical reasoning.


<details>
  <summary>Details</summary>
Motivation: The quality of pre-training corpora limits LLM performance in program synthesis and mathematical reasoning. The paper aims to address this by improving data quality through systematic rewriting.

Method: Introduces SwallowCode (16.1B tokens) and SwallowMath (2.3B tokens) datasets. SwallowCode uses a four-stage pipeline (syntax validation, style filtering, LLM rewriting) to refine Python snippets. SwallowMath enhances Finemath-4+ by removing boilerplate and reformatting solutions.

Result: Continual pre-training with SwallowCode boosts pass@1 by +17.0 on HumanEval and +17.7 on HumanEval+. SwallowMath improves accuracy by +12.4 on GSM8K and +7.6 on MATH.

Conclusion: The datasets significantly enhance LLM performance, with each pipeline stage contributing incrementally. All resources are publicly available for reproducible research.

Abstract: The performance of large language models (LLMs) in program synthesis and
mathematical reasoning is fundamentally limited by the quality of their
pre-training corpora. We introduce two openly licensed datasets, released under
the Llama 3.3 Community License, that significantly enhance LLM performance by
systematically rewriting public data. SwallowCode (approximately 16.1 billion
tokens) refines Python snippets from The-Stack-v2 through a novel four-stage
pipeline: syntax validation, pylint-based style filtering, and a two-stage LLM
rewriting process that enforces style conformity and transforms snippets into
self-contained, algorithmically efficient examples. Unlike prior methods that
rely on exclusionary filtering or limited transformations, our
transform-and-retain approach upgrades low-quality code, maximizing data
utility. SwallowMath (approximately 2.3 billion tokens) enhances Finemath-4+ by
removing boilerplate, restoring context, and reformatting solutions into
concise, step-by-step explanations. Within a fixed 50 billion token training
budget, continual pre-training of Llama-3.1-8B with SwallowCode boosts pass@1
by +17.0 on HumanEval and +17.7 on HumanEval+ compared to Stack-Edu, surpassing
the baseline model's code generation capabilities. Similarly, substituting
SwallowMath yields +12.4 accuracy on GSM8K and +7.6 on MATH. Ablation studies
confirm that each pipeline stage contributes incrementally, with rewriting
delivering the largest gains. All datasets, prompts, and checkpoints are
publicly available, enabling reproducible research and advancing LLM
pre-training for specialized domains.

</details>


### [597] [RobGC: Towards Robust Graph Condensation](https://arxiv.org/pdf/2406.13200)
*Xinyi Gao, Hongzhi Yin, Tong Chen, Guanhua Ye, Wentao Zhang, Bin Cui*

Main category: cs.LG

TL;DR: Robust Graph Condensation (RobGC) improves GNN training efficiency by generating noise-resistant condensed graphs, enhancing robustness in noisy environments.


<details>
  <summary>Details</summary>
Motivation: Large-scale graphs challenge GNN training due to computational demands and noise susceptibility in existing graph condensation methods.

Method: RobGC uses condensed graphs as feedback for denoising training graphs, employing label propagation-based alternating optimization for mutual purification.

Result: RobGC enhances robustness of condensed graphs under various noise types and levels, compatible with existing GC methods.

Conclusion: RobGC is a plug-and-play solution that extends the applicability of graph condensation in noisy real-world scenarios.

Abstract: Graph neural networks (GNNs) have attracted widespread attention for their
impressive capability of graph representation learning. However, the increasing
prevalence of large-scale graphs presents a significant challenge for GNN
training due to their computational demands, limiting the applicability of GNNs
in various scenarios. In response to this challenge, graph condensation (GC) is
proposed as a promising acceleration solution, focusing on generating an
informative compact graph that enables efficient training of GNNs while
retaining performance. Despite the potential to accelerate GNN training,
existing GC methods overlook the quality of large training graphs during both
the training and inference stages. They indiscriminately emulate the training
graph distributions, making the condensed graphs susceptible to noises within
the training graph and significantly impeding the application of GC in
intricate real-world scenarios. To address this issue, we propose robust graph
condensation (RobGC), a plug-and-play approach for GC to extend the robustness
and applicability of condensed graphs in noisy graph structure environments.
Specifically, RobGC leverages the condensed graph as a feedback signal to guide
the denoising process on the original training graph. A label propagation-based
alternating optimization strategy is in place for the condensation and
denoising processes, contributing to the mutual purification of the condensed
graph and training graph. Additionally, as a GC method designed for inductive
graph inference, RobGC facilitates test-time graph denoising by leveraging the
noise-free condensed graph to calibrate the structure of the test graph.
Extensive experiments show that RobGC is compatible with various GC methods,
significantly boosting their robustness under different types and levels of
graph structural noises.

</details>


### [598] [Purity Law for Generalizable Neural TSP Solvers](https://arxiv.org/pdf/2505.04558)
*Wenzhao Liu, Haoran Li, Congying Han, Zicheng Zhang, Anqi Li, Tiande Guo*

Main category: cs.LG

TL;DR: The paper introduces Purity Law (PuLa), a structural principle for optimal TSP solutions, and Purity Policy Optimization (PUPO), a training paradigm to enhance neural solvers' generalization.


<details>
  <summary>Details</summary>
Motivation: Generalization in neural approaches for TSP is challenging due to the lack of robust principles for universal patterns.

Method: Uncover PuLa, a principle linking edge prevalence to vertex sparsity, and propose PUPO to align neural solutions with PuLa.

Result: PUPO improves generalization of neural solvers without extra computational cost.

Conclusion: PuLa and PUPO offer a principled way to enhance neural TSP solvers' generalization.

Abstract: Achieving generalization in neural approaches across different scales and
distributions remains a significant challenge for the Traveling Salesman
Problem~(TSP). A key obstacle is that neural networks often fail to learn
robust principles for identifying universal patterns and deriving optimal
solutions from diverse instances. In this paper, we first uncover Purity Law
(PuLa), a fundamental structural principle for optimal TSP solutions, defining
that edge prevalence grows exponentially with the sparsity of surrounding
vertices. Statistically validated across diverse instances, PuLa reveals a
consistent bias toward local sparsity in global optima. Building on this
insight, we propose Purity Policy Optimization~(PUPO), a novel training
paradigm that explicitly aligns characteristics of neural solutions with PuLa
during the solution construction process to enhance generalization. Extensive
experiments demonstrate that PUPO can be seamlessly integrated with popular
neural solvers, significantly enhancing their generalization performance
without incurring additional computational overhead during inference.

</details>


### [599] [LLMEasyQuant: Scalable Quantization for Parallel and Distributed LLM Inference](https://arxiv.org/pdf/2406.19657)
*Dong Liu, Yanxuan Yu*

Main category: cs.LG

TL;DR: LLMEasyQuant is a modular, system-aware quantization framework for efficient low-bit inference of large language models (LLMs) across various hardware setups, offering flexibility, transparency, and scalability.


<details>
  <summary>Details</summary>
Motivation: Existing quantization toolkits lack transparency, flexibility, and scalability for LLMs, necessitating a better solution for efficient inference.

Method: LLMEasyQuant supports multiple quantization methods (e.g., Symmetric Quantization, ZeroQuant) with unified interfaces for calibration, bitwidth assignment, and runtime adaptation. It integrates fused CUDA kernels and NCCL-based distributed synchronization.

Result: Empirical results show speedups in GEMM execution, HBM load time, and near-linear multi-GPU scaling, with balanced latency, memory, and accuracy.

Conclusion: LLMEasyQuant provides a practical, scalable, and hardware-optimized solution for LLM inference.

Abstract: As large language models (LLMs) grow in size and deployment scale,
quantization has become an essential technique for reducing memory footprint
and improving inference efficiency. However, existing quantization toolkits
often lack transparency, flexibility, and system-level scalability across GPUs
and distributed environments. We present \textbf{LLMEasyQuant}, a modular,
system-aware quantization framework designed for efficient, low-bit inference
of LLMs on single-node multi-GPU, multi-node, and edge hardware. LLMEasyQuant
supports a wide range of quantization methods -- including Symmetric
Quantization, ZeroQuant, SmoothQuant, and SimQuant -- with unified interfaces
for per-layer calibration, bitwidth assignment, and runtime adaptation. It
integrates fused CUDA kernels with NCCL-based distributed synchronization and
supports both static and online quantization. Empirical results show that
LLMEasyQuant can achieve substantial speedups in GEMM execution, HBM load time,
and near-linear multi-GPU scaling. Ablation studies further validate its
ability to balance latency, memory, and accuracy under diverse deployment
conditions. LLMEasyQuant offers a practical quantization serving system for
scalable, hardware-optimized LLM inference.

</details>


### [600] [WATCH: Adaptive Monitoring for AI Deployments via Weighted-Conformal Martingales](https://arxiv.org/pdf/2505.04608)
*Drew Prinster, Xing Han, Anqi Liu, Suchi Saria*

Main category: cs.LG

TL;DR: Proposes weighted conformal test martingales (WCTMs) for post-deployment AI monitoring, enabling online adaptation and root-cause analysis while controlling false alarms.


<details>
  <summary>Details</summary>
Motivation: Ensuring AI/ML system reliability in high-stakes settings requires continual monitoring to detect unsafe behavior, but existing methods are limited in scope and adaptability.

Method: Introduces WCTMs, a generalization of conformal test martingales, for online monitoring of unexpected changepoints, with algorithms for adapting to mild shifts and detecting severe ones.

Result: Demonstrates improved performance over state-of-the-art baselines on real-world datasets.

Conclusion: WCTMs expand monitoring capabilities, offering better adaptability and detection of data shifts, enhancing AI system reliability post-deployment.

Abstract: Responsibly deploying artificial intelligence (AI) / machine learning (ML)
systems in high-stakes settings arguably requires not only proof of system
reliability, but moreover continual, post-deployment monitoring to quickly
detect and address any unsafe behavior. Statistical methods for nonparametric
change-point detection -- especially the tools of conformal test martingales
(CTMs) and anytime-valid inference -- offer promising approaches to this
monitoring task. However, existing methods are restricted to monitoring limited
hypothesis classes or ``alarm criteria'' (such as data shifts that violate
certain exchangeability assumptions), do not allow for online adaptation in
response to shifts, and/or do not enable root-cause analysis of any
degradation. In this paper, we expand the scope of these monitoring methods by
proposing a weighted generalization of conformal test martingales (WCTMs),
which lay a theoretical foundation for online monitoring for any unexpected
changepoints in the data distribution while controlling false-alarms. For
practical applications, we propose specific WCTM algorithms that adapt online
to mild covariate shifts (in the marginal input distribution) while quickly
detecting and diagnosing more severe shifts, such as concept shifts (in the
conditional label distribution) or extreme (out-of-support) covariate shifts
that cannot be easily adapted to. On real-world datasets, we demonstrate
improved performance relative to state-of-the-art baselines.

</details>


### [601] [DC is all you need: describing ReLU from a signal processing standpoint](https://arxiv.org/pdf/2407.16556)
*Christodoulos Kechris, Jonathan Dan, Jose Miranda, David Atienza*

Main category: cs.LG

TL;DR: The paper analyzes ReLU's spectral behavior in CNNs, showing it introduces higher frequencies and a DC component, which aids feature extraction.


<details>
  <summary>Details</summary>
Motivation: Non-linear activation functions like ReLU are key in CNNs but poorly understood in the frequency domain.

Method: Uses Taylor expansion to derive ReLU's frequency behavior, validates numerically, and tests on models.

Result: ReLU adds higher frequencies and a DC component, which helps converge to initial weight configurations.

Conclusion: The DC component from ReLU is beneficial for feature extraction and model convergence.

Abstract: Non-linear activation functions are crucial in Convolutional Neural Networks.
However, until now they have not been well described in the frequency domain.
In this work, we study the spectral behavior of ReLU, a popular activation
function. We use the ReLU's Taylor expansion to derive its frequency domain
behavior. We demonstrate that ReLU introduces higher frequency oscillations in
the signal and a constant DC component. Furthermore, we investigate the
importance of this DC component, where we demonstrate that it helps the model
extract meaningful features related to the input frequency content. We
accompany our theoretical derivations with experiments and real-world examples.
First, we numerically validate our frequency response model. Then we observe
ReLU's spectral behavior on two example models and a real-world one. Finally,
we experimentally investigate the role of the DC component introduced by ReLU
in the CNN's representations. Our results indicate that the DC helps to
converge to a weight configuration that is close to the initial random weights.

</details>


### [602] [Efficient LLM Context Distillation](https://arxiv.org/pdf/2409.01930)
*Rajesh Upadhayayaya, Manish Raj Osti, Zachary Smith, Chritopher Kottmyer*

Main category: cs.LG

TL;DR: Context distillation is an efficient method for adapting LLMs to specific tasks, performing comparably to in-context learning and offering advantages over few-shot fine-tuning in resource efficiency.


<details>
  <summary>Details</summary>
Motivation: To explore and validate context distillation as a method for adapting LLMs to specific tasks with minimal in-context examples, comparing it to in-context learning and few-shot fine-tuning.

Method: Comparative analysis of context distillation, in-context learning, and few-shot fine-tuning using OPT models and matched datasets from Mobach.

Result: Context distillation matches in-context learning in accuracy and outperforms it in out-of-domain generalization, though it falls short of few-shot fine-tuning. It requires fewer resources.

Conclusion: Context distillation is a viable and efficient alternative for adapting LLMs, especially with smaller datasets, balancing performance and computational demands.

Abstract: Large Language Models (LLMs) demonstrate proficiency across diverse tasks but
often require targeted adaptations for specific applications. Various methods
have been proposed to facilitate this adaptation, including fewshot
fine-tuning, in-context learning, and context distillation. This paper
specifically investigates context distillation a method that extends the
utility of task-specific examples by internalizing them, thus augmenting the
example set accessible for model inference. We conduct a comparative analysis
of context distillation with in-context learning (ICL) and few-shot fine-tuning
(FT), aiming to ascertain the efficacy of context distillation in adapting
models using minimal in-context examples. Employing matched datasets from
Mobach, our experiments leverage OPT models of various sizes. The results
indicate that context distillation effectively adapts models, with student
models attaining comparable in-domain and out-of-domain accuracies to
in-context learning. Although context distillation surpasses ICL in
out-of-domain generalization, it does not achieve the performance levels of FT.
However, the reduced dataset size and computational demands position context
distillation as a viable alternative, especially for smaller datasets. Overall,
this study presents context distillation as an efficient and potent method for
customizing LLMs to specific tasks.

</details>


### [603] [Stochastic Variational Propagation: Local, Scalable and Efficient Alternative to Backpropagation](https://arxiv.org/pdf/2505.05181)
*Bojian Yin, Federico Corradi*

Main category: cs.LG

TL;DR: SVP replaces backpropagation with hierarchical variational inference for scalable, memory-efficient deep learning, avoiding representation collapse via random projections and alignment loss.


<details>
  <summary>Details</summary>
Motivation: Backpropagation's scalability and memory limitations hinder deep learning efficiency, prompting the need for alternatives like SVP.

Method: SVP uses local ELBOs with random projections to prevent collapse and a feature alignment loss for consistency, enabling independent layer updates.

Result: SVP matches BP's accuracy, reduces memory by 4x, and improves scalability across architectures and datasets.

Conclusion: SVP offers a probabilistic, scalable alternative to BP, enhancing modularity and interpretability in neural networks.

Abstract: Backpropagation (BP) is the cornerstone of deep learning, but its reliance on
global gradient synchronization limits scalability and imposes significant
memory overhead. We propose Stochastic Variational Propagation (SVP), a
scalable alternative that reframes training as hierarchical variational
inference. SVP treats layer activations as latent variables and optimizes local
Evidence Lower Bounds (ELBOs), enabling independent, local updates while
preserving global coherence. However, directly applying KL divergence in
layer-wise ELBOs risks inter-layer's representation collapse due to excessive
compression. To prevent this, SVP projects activations into low-dimensional
spaces via fixed random matrices, ensuring information preservation and
representational diversity. Combined with a feature alignment loss for
inter-layer consistency, SVP achieves competitive accuracy with BP across
diverse architectures (MLPs, CNNs, Transformers) and datasets (MNIST to
ImageNet), reduces memory usage by up to 4x, and significantly improves
scalability. More broadly, SVP introduces a probabilistic perspective to deep
representation learning, opening pathways toward more modular and interpretable
neural network design.

</details>


### [604] [Diffusion Models Learn Low-Dimensional Distributions via Subspace Clustering](https://arxiv.org/pdf/2409.02426)
*Peng Wang, Huijie Zhang, Zekai Zhang, Siyi Chen, Yi Ma, Qing Qu*

Main category: cs.LG

TL;DR: Diffusion models learn image distributions efficiently with few samples due to low intrinsic dimensionality, manifold structure, and low-rank denoising autoencoders. Theoretical analysis links training to subspace clustering, showing linear sample scaling with intrinsic dimensions.


<details>
  <summary>Details</summary>
Motivation: To explain why diffusion models avoid the curse of dimensionality and perform well with limited training samples.

Method: Assume image data as a mixture of low-rank Gaussians and parameterize the denoising autoencoder as low-rank. Link training loss optimization to subspace clustering.

Result: Minimal sample requirement scales linearly with intrinsic dimensions, explaining diffusion models' efficiency. Subspaces correspond to semantic image representations.

Conclusion: Theoretical and empirical results validate that diffusion models break dimensionality constraints and enable semantic image editing.

Abstract: Recent empirical studies have demonstrated that diffusion models can
effectively learn the image distribution and generate new samples. Remarkably,
these models can achieve this even with a small number of training samples
despite a large image dimension, circumventing the curse of dimensionality. In
this work, we provide theoretical insights into this phenomenon by leveraging
key empirical observations: (i) the low intrinsic dimensionality of image data,
(ii) a union of manifold structure of image data, and (iii) the low-rank
property of the denoising autoencoder in trained diffusion models. These
observations motivate us to assume the underlying data distribution of image
data as a mixture of low-rank Gaussians and to parameterize the denoising
autoencoder as a low-rank model according to the score function of the assumed
distribution. With these setups, we rigorously show that optimizing the
training loss of diffusion models is equivalent to solving the canonical
subspace clustering problem over the training samples. Based on this
equivalence, we further show that the minimal number of samples required to
learn the underlying distribution scales linearly with the intrinsic dimensions
under the above data and model assumptions. This insight sheds light on why
diffusion models can break the curse of dimensionality and exhibit the phase
transition in learning distributions. Moreover, we empirically establish a
correspondence between the subspaces and the semantic representations of image
data, facilitating image editing. We validate these results with corroborated
experimental results on both simulated distributions and image datasets.

</details>


### [605] [Looped Transformers for Length Generalization](https://arxiv.org/pdf/2409.15647)
*Ying Fan, Yilun Du, Kannan Ramchandran, Kangwook Lee*

Main category: cs.LG

TL;DR: Looped Transformers with adaptive steps improve length generalization for arithmetic tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of Transformers struggling with length generalization on unseen input lengths.

Method: Using looped Transformers with adaptive steps and a learning algorithm for tasks with iterative solutions.

Result: Achieved highly length-generalizable solutions for various tasks.

Conclusion: Looped Transformers enhance length generalization in arithmetic and algorithmic tasks.

Abstract: Recent work has shown that Transformers trained from scratch can successfully
solve various arithmetic and algorithmic tasks, such as adding numbers and
computing parity. While these Transformers generalize well on unseen inputs of
the same length, they struggle with length generalization, i.e., handling
inputs of unseen lengths. In this work, we demonstrate that looped Transformers
with an adaptive number of steps significantly improve length generalization.
We focus on tasks with a known iterative solution, involving multiple
iterations of a RASP-L operation - a length-generalizable operation that can be
expressed by a finite-sized Transformer. We train looped Transformers using our
proposed learning algorithm and observe that they learn highly
length-generalizable solutions for various tasks.

</details>


### [606] [Continuous Thought Machines](https://arxiv.org/pdf/2505.05522)
*Luke Darlow, Ciaran Regan, Sebastian Risi, Jeffrey Seely, Llion Jones*

Main category: cs.LG

TL;DR: The paper introduces the Continuous Thought Machine (CTM), a model incorporating neuron-level temporal processing and synchronization to enhance biological realism in deep learning.


<details>
  <summary>Details</summary>
Motivation: To challenge the oversimplification of neural activity in deep learning by reintroducing temporal dynamics and synchronization as core elements.

Method: CTM uses neuron-level temporal processing and neural synchronization as latent representations, balancing biological realism and computational efficiency.

Result: CTM demonstrates strong performance in tasks like ImageNet-1K classification, maze-solving, and question-answering, with adaptive compute capabilities.

Conclusion: CTM advances biologically plausible AI, offering interpretability and versatility without focusing on state-of-the-art benchmarks.

Abstract: Biological brains demonstrate complex neural activity, where the timing and
interplay between neurons is critical to how brains process information. Most
deep learning architectures simplify neural activity by abstracting away
temporal dynamics. In this paper we challenge that paradigm. By incorporating
neuron-level processing and synchronization, we can effectively reintroduce
neural timing as a foundational element. We present the Continuous Thought
Machine (CTM), a model designed to leverage neural dynamics as its core
representation. The CTM has two core innovations: (1) neuron-level temporal
processing, where each neuron uses unique weight parameters to process a
history of incoming signals; and (2) neural synchronization employed as a
latent representation. The CTM aims to strike a balance between oversimplified
neuron abstractions that improve computational efficiency, and biological
realism. It operates at a level of abstraction that effectively captures
essential temporal dynamics while remaining computationally tractable for deep
learning. We demonstrate the CTM's strong performance and versatility across a
range of challenging tasks, including ImageNet-1K classification, solving 2D
mazes, sorting, parity computation, question-answering, and RL tasks. Beyond
displaying rich internal representations and offering a natural avenue for
interpretation owing to its internal process, the CTM is able to perform tasks
that require complex sequential reasoning. The CTM can also leverage adaptive
compute, where it can stop earlier for simpler tasks, or keep computing when
faced with more challenging instances. The goal of this work is to share the
CTM and its associated innovations, rather than pushing for new
state-of-the-art results. To that end, we believe the CTM represents a
significant step toward developing more biologically plausible and powerful
artificial intelligence systems.

</details>


### [607] [Differentially Private Bilevel Optimization](https://arxiv.org/pdf/2409.19800)
*Guy Kornowski*

Main category: cs.LG

TL;DR: First DP algorithms for bilevel optimization, avoiding Hessian computations, with gradient-based privacy guarantees.


<details>
  <summary>Details</summary>
Motivation: Address the lack of differentially private solutions for bilevel optimization in machine learning, especially in large-scale settings.

Method: Proposes a gradient-based $\epsilon,\delta$-DP algorithm for non-convex upper-level and strongly-convex lower-level problems.

Result: Achieves hypergradient norm bounds depending on dataset size and dimensions, applicable to constrained/unconstrained problems.

Conclusion: Provides a practical DP solution for bilevel optimization, demonstrated with a hyperparameter tuning application.

Abstract: We present differentially private (DP) algorithms for bilevel optimization, a
problem class that received significant attention lately in various machine
learning applications. These are the first algorithms for such problems under
standard DP constraints, and are also the first to avoid Hessian computations
which are prohibitive in large-scale settings. Under the well-studied setting
in which the upper-level is not necessarily convex and the lower-level problem
is strongly-convex, our proposed gradient-based $(\epsilon,\delta)$-DP
algorithm returns a point with hypergradient norm at most
$\widetilde{\mathcal{O}}\left((\sqrt{d_\mathrm{up}}/\epsilon
n)^{1/2}+(\sqrt{d_\mathrm{low}}/\epsilon n)^{1/3}\right)$ where $n$ is the
dataset size, and $d_\mathrm{up}/d_\mathrm{low}$ are the upper/lower level
dimensions. Our analysis covers constrained and unconstrained problems alike,
accounts for mini-batch gradients, and applies to both empirical and population
losses. As an application, we specialize our analysis to derive a simple
private rule for tuning a regularization hyperparameter.

</details>


### [608] [Rethinking Graph Contrastive Learning through Relative Similarity Preservation](https://arxiv.org/pdf/2505.05533)
*Zhiyuan Ning, Pengfei Wang, Ziyue Qiao, Pengyang Wang, Yuanchun Zhou*

Main category: cs.LG

TL;DR: The paper introduces RELGCL, a graph contrastive learning framework that leverages natural relative similarity patterns in graphs, outperforming existing methods by avoiding unreliable absolute similarity measures.


<details>
  <summary>Details</summary>
Motivation: Traditional graph contrastive learning (GCL) struggles with semantic validity and similarity verification due to the discrete, non-Euclidean nature of graphs. The authors aim to address this by identifying and utilizing inherent relative similarity patterns.

Method: The authors analyze 11 real-world graphs to discover universal label consistency patterns. They propose RELGCL, a GCL framework with pairwise and listwise implementations that preserve these patterns through collective similarity objectives.

Result: Extensive experiments show RELGCL consistently outperforms 20 existing methods across homophily and heterophily graphs, validating its effectiveness.

Conclusion: The paper concludes that leveraging natural relative similarity patterns in graphs is more effective than relying on artificial absolute similarity, as demonstrated by RELGCL's superior performance.

Abstract: Graph contrastive learning (GCL) has achieved remarkable success by following
the computer vision paradigm of preserving absolute similarity between
augmented views. However, this approach faces fundamental challenges in graphs
due to their discrete, non-Euclidean nature -- view generation often breaks
semantic validity and similarity verification becomes unreliable. Through
analyzing 11 real-world graphs, we discover a universal pattern transcending
the homophily-heterophily dichotomy: label consistency systematically
diminishes as structural distance increases, manifesting as smooth decay in
homophily graphs and oscillatory decay in heterophily graphs. We establish
theoretical guarantees for this pattern through random walk theory, proving
label distribution convergence and characterizing the mechanisms behind
different decay behaviors. This discovery reveals that graphs naturally encode
relative similarity patterns, where structurally closer nodes exhibit
collectively stronger semantic relationships. Leveraging this insight, we
propose RELGCL, a novel GCL framework with complementary pairwise and listwise
implementations that preserve these inherent patterns through collective
similarity objectives. Extensive experiments demonstrate that our method
consistently outperforms 20 existing approaches across both homophily and
heterophily graphs, validating the effectiveness of leveraging natural relative
similarity over artificial absolute similarity.

</details>


### [609] [Characterizing and Efficiently Accelerating Multimodal Generation Model Inference](https://arxiv.org/pdf/2410.00215)
*Yejin Lee, Anna Sun, Basil Hosmer, Bilge Acun, Can Balioglu, Changhan Wang, Charles David Hernandez, Christian Puhrsch, Daniel Haziza, Driss Guessous, Francisco Massa, Jacob Kahn, Jeffrey Wan, Jeremy Reizenstein, Jiaqi Zhai, Joe Isaacson, Joel Schlosser, Juan Pino, Kaushik Ram Sadagopan, Leonid Shamis, Linjian Ma, Min-Jae Hwang, Mingda Chen, Mostafa Elhoushi, Pedro Rodriguez, Ram Pasunuru, Scott Yih, Sravya Popuri, Xing Liu, Carole-Jean Wu*

Main category: cs.LG

TL;DR: The paper highlights system design and optimization opportunities for scaling generative AI, focusing on latency bottlenecks like GPU idle time and memory-intensive operations.


<details>
  <summary>Details</summary>
Motivation: To sustainably scale generative AI for billions of users, efficient and fast inference is crucial, given the high resource demands of multi-modal models.

Method: Characterizes multi-modal generation models on real systems, identifying bottlenecks like auto-regressive token generation and memory-intensive attention. Proposes optimization levers across applications, system software, and hardware.

Result: Demonstrates a 3.88x improvement in baseline performance using state-of-the-art optimizations.

Conclusion: Optimizing system design and leveraging multi-level optimizations can significantly enhance the efficiency and scalability of generative AI.

Abstract: Generative artificial intelligence (AI) technology is revolutionizing the
computing industry. Not only its applications have broadened to various sectors
but also poses new system design and optimization opportunities. The technology
is capable of understanding and responding in multiple modalities. However, the
advanced capability currently comes with significant system resource demands.
To sustainably scale generative AI capabilities to billions of users in the
world, inference must be fast and efficient. This paper pinpoints key system
design and optimization opportunities by characterizing a family of emerging
multi-modal generation models on real systems. Auto-regressive token generation
is a critical latency performance bottleneck, typically dominated by GPU idle
time. In addition to memory-intensive attention across the generative AI
models, linear operations constitute significant inference latency due to the
feed forward networks in Transformer-based models. We demonstrate that
state-of-the-art optimization levers, spanning from applications to system
software and hardware, set a 3.88x better baseline.

</details>


### [610] [Multi-Modal Molecular Representation Learning via Structure Awareness](https://arxiv.org/pdf/2505.05877)
*Rong Yin, Ruyue Liu, Xiaoshuai Hao, Xingrui Zhou, Yong Liu, Can Ma, Weiping Wang*

Main category: cs.LG

TL;DR: Proposes MMSA, a self-supervised framework for multi-modal molecular representation learning, enhancing intermodal interactions and higher-order relationships.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal methods overlook intermodal interactions and higher-order relationships in molecular representations.

Method: MMSA combines multi-modal representation learning and structure-awareness modules, using hypergraphs and memory mechanisms.

Result: Achieves state-of-the-art performance on MoleculeNet, with ROC-AUC improvements of 1.8%-9.6%.

Conclusion: MMSA effectively captures complex molecular relationships and invariant features, outperforming baselines.

Abstract: Accurate extraction of molecular representations is a critical step in the
drug discovery process. In recent years, significant progress has been made in
molecular representation learning methods, among which multi-modal molecular
representation methods based on images, and 2D/3D topologies have become
increasingly mainstream. However, existing these multi-modal approaches often
directly fuse information from different modalities, overlooking the potential
of intermodal interactions and failing to adequately capture the complex
higher-order relationships and invariant features between molecules. To
overcome these challenges, we propose a structure-awareness-based multi-modal
self-supervised molecular representation pre-training framework (MMSA) designed
to enhance molecular graph representations by leveraging invariant knowledge
between molecules. The framework consists of two main modules: the multi-modal
molecular representation learning module and the structure-awareness module.
The multi-modal molecular representation learning module collaboratively
processes information from different modalities of the same molecule to
overcome intermodal differences and generate a unified molecular embedding.
Subsequently, the structure-awareness module enhances the molecular
representation by constructing a hypergraph structure to model higher-order
correlations between molecules. This module also introduces a memory mechanism
for storing typical molecular representations, aligning them with memory
anchors in the memory bank to integrate invariant knowledge, thereby improving
the model generalization ability. Extensive experiments have demonstrated the
effectiveness of MMSA, which achieves state-of-the-art performance on the
MoleculeNet benchmark, with average ROC-AUC improvements ranging from 1.8% to
9.6% over baseline methods.

</details>


### [611] [SHAP values via sparse Fourier representation](https://arxiv.org/pdf/2410.06300)
*Ali Gorji, Andisheh Amrollahi, Andreas Krause*

Main category: cs.LG

TL;DR: Proposes a two-stage algorithm for efficient SHAP value computation using Fourier approximations, achieving speedups and tunable precision.


<details>
  <summary>Details</summary>
Motivation: Addresses spectral bias in real-world predictors and the need for efficient SHAP value computation in both black-box and tree-based models.

Method: Uses compact Fourier representations to approximate models, then introduces a closed-form formula for exact SHAP value computation, enabling parallelization.

Result: Achieves significant speedups over existing methods with a tunable trade-off between efficiency and precision.

Conclusion: The method enables amortized SHAP value computation, improving efficiency while maintaining precision.

Abstract: SHAP (SHapley Additive exPlanations) values are a widely used method for
local feature attribution in interpretable and explainable AI. We propose an
efficient two-stage algorithm for computing SHAP values in both black-box
setting and tree-based models. Motivated by spectral bias in real-world
predictors, we first approximate models using compact Fourier representations,
exactly for trees and approximately for black-box models. In the second stage,
we introduce a closed-form formula for {\em exactly} computing SHAP values
using the Fourier representation, that ``linearizes'' the computation into a
simple summation and is amenable to parallelization. As the Fourier
approximation is computed only once, our method enables amortized SHAP value
computation, achieving significant speedups over existing methods and a tunable
trade-off between efficiency and precision.

</details>


### [612] [LLMs Outperform Experts on Challenging Biology Benchmarks](https://arxiv.org/pdf/2505.06108)
*Lennart Justen*

Main category: cs.LG

TL;DR: The study evaluates 27 Large Language Models (LLMs) on eight biology benchmarks, showing significant performance improvements, with some models surpassing expert-level capabilities. Chain-of-thought reasoning had minimal impact, while extended reasoning features improved results. Benchmark saturation and data errors were noted, calling for better evaluation methods.


<details>
  <summary>Details</summary>
Motivation: To assess the biological capabilities of frontier LLMs and track their performance improvements over time, identifying strengths, limitations, and the need for advanced evaluation methodologies.

Method: Systematic evaluation of 27 LLMs on eight biology benchmarks, with ten independent runs per benchmark, covering molecular biology, genetics, cloning, virology, and biosecurity.

Result: Top models showed a 4-fold improvement on challenging tasks, with some surpassing expert performance. Extended reasoning features improved results, while chain-of-thought did not. Benchmark saturation and data errors were observed.

Conclusion: LLMs have advanced significantly in biological tasks, but benchmark limitations and evaluation methods need refinement to keep pace with AI progress.

Abstract: This study systematically evaluates 27 frontier Large Language Models on
eight biology benchmarks spanning molecular biology, genetics, cloning,
virology, and biosecurity. Models from major AI developers released between
November 2022 and April 2025 were assessed through ten independent runs per
benchmark. The findings reveal dramatic improvements in biological
capabilities. Top model performance increased more than 4-fold on the
challenging text-only subset of the Virology Capabilities Test over the study
period, with OpenAI's o3 now performing twice as well as expert virologists.
Several models now match or exceed expert-level performance on other
challenging benchmarks, including the biology subsets of GPQA and WMDP and
LAB-Bench CloningScenarios. Contrary to expectations, chain-of-thought did not
substantially improve performance over zero-shot evaluation, while extended
reasoning features in o3-mini and Claude 3.7 Sonnet typically improved
performance as predicted by inference scaling. Benchmarks such as PubMedQA and
the MMLU and WMDP biology subsets exhibited performance plateaus well below
100%, suggesting benchmark saturation and errors in the underlying benchmark
data. The analysis highlights the need for more sophisticated evaluation
methodologies as AI systems continue to advance.

</details>


### [613] [Transfer Learning with Foundational Models for Time Series Forecasting using Low-Rank Adaptations](https://arxiv.org/pdf/2410.11539)
*M. Germán-Morales, A. J. Rivera-Rivas, M. J. del Jesus Díaz, C. J. Carmona*

Main category: cs.LG

TL;DR: LLIAM adapts Large Language Models for Time Series Forecasting using prompting and Low-Rank Adaptations, achieving competitive results without complex modifications.


<details>
  <summary>Details</summary>
Motivation: To leverage scalable Foundational Models (FMs) for Time Series Forecasting, reducing the need for costly training and promoting Green AI.

Method: Proposes LLIAM, combining time-series prompting and Low-Rank Adaptations for fine-tuning on diverse datasets.

Result: LLIAM outperforms state-of-the-art DL algorithms and shows strong generalization in zero-shot studies.

Conclusion: LLIAM is an effective, simple approach for Time Series Forecasting, encouraging resource-efficient AI practices.

Abstract: Foundational Models are an emerging widely used technique of GenAI. These
models are distinguished by their scalability and the ease with which they can
be adapted through the exploitation of Transfer Learning. The availability of
high computational power and large datasets have supported their development,
achieving a high generalization capacity due to the enormous and heterogeneous
amounts of data used in their initial training. These characteristics
contribute to a solid base that can be adapted or adjusted to a wide range of
tasks, increasing their applicability. This study proposes the methodology
LLIAM, a straightforward adaptation of a kind of FM, Large Language Models, for
the Time Series Forecasting task. An adequate time-series prompting schema and
Low-Rank Adaptations are used to enhance the knowledge of the model with
diverse time series datasets, known as the fine-tuning phase. A study divided
in two stages has been performed for evaluating the effectiveness of the
proposed methodology. Initially, a comparison was made between the performance
of LLIAM and different state-of-the-art DL algorithms, including Recurrent
Neural Networks and Temporal Convolutional Networks, as well as a LLM-based
method, TimeLLM. Following this, a zero-shot study is presented in order to
evaluate the generalization capacity of the proposed methodology with time
series datasets from unknown domains not considered in the model training. The
outcomes of this investigation demonstrate the efficacy of LLIAM, highlighting
that this straightforward and general approach can attain competent results
without the necessity for applying complex modifications. This work also
encourages the use of available resources (such as these pre-trained models)
and efficient fine-tuning techniques to avoid unnecessary and costly training,
narrowing the gap between the goals of traditional AI and Green AI.

</details>


### [614] [Reward-free World Models for Online Imitation Learning](https://arxiv.org/pdf/2410.14081)
*Shangzhe Li, Zhiao Huang, Hao Su*

Main category: cs.LG

TL;DR: A novel online imitation learning method using reward-free world models in latent spaces achieves expert-level performance in complex tasks.


<details>
  <summary>Details</summary>
Motivation: Prior online IL methods struggle with high-dimensional inputs and complex dynamics, necessitating a more efficient and stable approach.

Method: Leverages reward-free world models in latent spaces, adopts inverse soft-Q learning, and uses latent dynamics for planning.

Result: Achieves stable, expert-level performance in high-dimensional tasks, outperforming existing methods on benchmarks like DMControl and MyoSuite.

Conclusion: The proposed method effectively addresses instability and inefficiency in online IL for complex tasks.

Abstract: Imitation learning (IL) enables agents to acquire skills directly from expert
demonstrations, providing a compelling alternative to reinforcement learning.
However, prior online IL approaches struggle with complex tasks characterized
by high-dimensional inputs and complex dynamics. In this work, we propose a
novel approach to online imitation learning that leverages reward-free world
models. Our method learns environmental dynamics entirely in latent spaces
without reconstruction, enabling efficient and accurate modeling. We adopt the
inverse soft-Q learning objective, reformulating the optimization process in
the Q-policy space to mitigate the instability associated with traditional
optimization in the reward-policy space. By employing a learned latent dynamics
model and planning for control, our approach consistently achieves stable,
expert-level performance in tasks with high-dimensional observation or action
spaces and intricate dynamics. We evaluate our method on a diverse set of
benchmarks, including DMControl, MyoSuite, and ManiSkill2, demonstrating
superior empirical performance compared to existing approaches.

</details>


### [615] [RAM: Replace Attention with MLP for Efficient Multivariate Time Series Forecasting](https://arxiv.org/pdf/2410.24023)
*Suhan Guo, Jiahong Deng, Yi Wei, Hui Dou, Furao Shen, Jian Zhao*

Main category: cs.LG

TL;DR: The paper introduces RAM, a pruning strategy that replaces attention mechanisms with MLPs in time series forecasting, reducing computational costs (FLOPs) significantly with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Despite the widespread use of attention-based architectures in time series forecasting, their effectiveness is not well understood. The authors aim to simplify these models by replacing attention with MLPs.

Method: RAM approximates attention using feedforward layers, residual connections, and layer normalization, removing key attention components (Q, K, V projections, score calculation, etc.) without major performance loss.

Result: RAM reduces FLOPs by 62.579% for spatio-temporal models (performance drop <2.5%) and 42.233% for LTSF models (performance drop <2%), maintaining top-tier performance.

Conclusion: RAM demonstrates that attention mechanisms can be effectively replaced with simpler MLP-based structures, offering computational efficiency without sacrificing forecasting accuracy.

Abstract: Attention-based architectures have become ubiquitous in time series
forecasting tasks, including spatio-temporal (STF) and long-term time series
forecasting (LTSF). Yet, our understanding of the reasons for their
effectiveness remains limited. In this work, we propose a novel pruning
strategy, $\textbf{R}$eplace $\textbf{A}$ttention with $\textbf{M}$LP (RAM),
that approximates the attention mechanism using only feedforward layers,
residual connections, and layer normalization for temporal and/or spatial
modeling in multivariate time series forecasting. Specifically, the Q, K, and V
projections, the attention score calculation, the dot-product between the
attention score and the V, and the final projection can be removed from the
attention-based networks without significantly degrading the performance, so
that the given network remains the top-tier compared to other SOTA methods. RAM
achieves a $62.579\%$ reduction in FLOPs for spatio-temporal models with less
than $2.5\%$ performance drop, and a $42.233\%$ FLOPs reduction for LTSF models
with less than $2\%$ performance drop.

</details>


### [616] [M2PDE: Compositional Generative Multiphysics and Multi-component PDE Simulation](https://arxiv.org/pdf/2412.04134)
*Tao Zhang, Zhenhai Liu, Feipeng Qi, Yongjun Jiao, Tailin Wu*

Main category: cs.LG

TL;DR: M2PDE uses diffusion models to improve multiphysics and multi-component simulations, outperforming traditional methods in accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in integrating specialized solvers and handling complex structures in multiphysics and multi-component simulations.

Method: Proposes M2PDE, a diffusion-based model that learns energy functions for conditional probabilities of physical processes/components and samples joint distributions for solutions.

Result: M2PDE achieves higher accuracy in multiphysics tasks and scales effectively in multi-component simulations, outperforming existing approaches.

Conclusion: M2PDE offers a robust solution for complex simulations, demonstrating superior performance and scalability.

Abstract: Multiphysics simulation, which models the interactions between multiple
physical processes, and multi-component simulation of complex structures are
critical in fields like nuclear and aerospace engineering. Previous studies use
numerical solvers or ML-based surrogate models for these simulations. However,
multiphysics simulations typically require integrating multiple specialized
solvers-each for a specific physical process-into a coupled program, which
introduces significant development challenges. Furthermore, existing numerical
algorithms struggle with highly complex large-scale structures in
multi-component simulations. Here we propose compositional Multiphysics and
Multi-component PDE Simulation with Diffusion models (M2PDE) to overcome these
challenges. During diffusion-based training, M2PDE learns energy functions
modeling the conditional probability of one physical process/component
conditioned on other processes/components. In inference, M2PDE generates
coupled multiphysics and multi-component solutions by sampling from the joint
probability distribution. We evaluate M2PDE on two multiphysics
tasks-reaction-diffusion and nuclear thermal coupling-where it achieves more
accurate predictions than surrogate models in challenging scenarios. We then
apply it to a multi-component prismatic fuel element problem, demonstrating
that M2PDE scales from single-component training to a 64-component structure
and outperforms existing domain-decomposition and graph-based approaches. The
code is available at https://github.com/AI4Science-WestlakeU/M2PDE.

</details>


### [617] [GradStop: Exploring Training Dynamics in Unsupervised Outlier Detection through Gradient](https://arxiv.org/pdf/2412.08501)
*Yuang Zhang, Liping Wang, Yihong Huang, Yuanxing Zheng, Fan Zhang, Xuemin Lin*

Main category: cs.LG

TL;DR: The paper proposes GradStop, an early stopping algorithm for deep Unsupervised Outlier Detection (UOD) to prevent overfitting and improve performance by leveraging training dynamics and gradient cohesion.


<details>
  <summary>Details</summary>
Motivation: Deep UOD methods often misalign optimization goals with performance goals due to lack of labels, leading to overfitting. The paper addresses this by focusing on training dynamics.

Method: GradStop uses a sampling-based approach to estimate real-time performance, dividing data into outlier-rich and inlier-rich sets, and applies gradient cohesion metrics to guide early stopping.

Result: Experiments on 4 algorithms and 47 datasets show GradStop enhances performance, with Auto Encoder (AE) outperforming SOTA methods and ensemble AEs.

Conclusion: GradStop effectively mitigates performance degradation in deep UOD training, improving anomaly detection potential.

Abstract: Unsupervised Outlier Detection (UOD) is a critical task in data mining and
machine learning, aiming to identify instances that significantly deviate from
the majority. Without any label, deep UOD methods struggle with the
misalignment between the model's direct optimization goal and the final
performance goal of Outlier Detection (OD) task. Through the perspective of
training dynamics, this paper proposes an early stopping algorithm to optimize
the training of deep UOD models, ensuring they perform optimally in OD rather
than overfitting the entire contaminated dataset.
  Inspired by UOD mechanism and inlier priority phenomenon, where intuitively
models fit inliers more quickly than outliers, we propose GradStop, a
sampling-based label-free algorithm to estimate model's real-time performance
during training. First, a sampling method generates two sets: one likely
containing more outliers and the other more inliers, then a metric based on
gradient cohesion is applied to probe into current training dynamics, which
reflects model's performance on OD task.
  Experimental results on 4 deep UOD algorithms and 47 real-world datasets and
theoretical proofs demonstrate the effectiveness of our proposed early stopping
algorithm in enhancing the performance of deep UOD models. Auto Encoder (AE)
enhanced by GradStop achieves better performance than itself, other SOTA UOD
methods, and even ensemble AEs. Our method provides a robust and effective
solution to the problem of performance degradation during training, enabling
deep UOD models to achieve better potential in anomaly detection tasks.

</details>


### [618] [Is Stochastic Gradient Descent Effective? A PDE Perspective on Machine Learning processes](https://arxiv.org/pdf/2501.08425)
*Davide Barbieri, Matteo Bonforte, Peio Ibarrondo*

Main category: cs.LG

TL;DR: The paper analyzes SGD in neural networks using Fokker-Planck PDEs, identifying drift and diffusion regimes, and providing bounds on Mean Exit Time (MET) and asymptotic convergence for non-convex cases.


<details>
  <summary>Details</summary>
Motivation: To understand SGD's behavior in non-convex optimization, addressing gaps in Fokker-Planck theory for degenerate diffusion and non-convex potentials.

Method: Uses parabolic PDEs (Fokker-Planck type) to model SGD, analyzing drift and diffusion regimes, MET bounds, and asymptotic convergence via duality and entropy methods.

Result: Quantitative estimates for weight concentration in the drift regime, MET bounds for escaping suboptimal minima, and asymptotic convergence results for non-convex cases.

Conclusion: The study bridges stochastic optimization and PDE theory, offering insights into SGD dynamics, escape times from minima, and parameter convergence in neural networks.

Abstract: In this paper we analyze the behaviour of the stochastic gradient descent
(SGD), a widely used method in supervised learning for optimizing neural
network weights via a minimization of non-convex loss functions. Since the
pioneering work of E, Li and Tai (2017), the underlying structure of such
processes can be understood via parabolic PDEs of Fokker-Planck type, which are
at the core of our analysis. Even if Fokker-Planck equations have a long
history and a extensive literature, almost nothing is known when the potential
is non-convex or when the diffusion matrix is degenerate, and this is the main
difficulty that we face in our analysis.
  We identify two different regimes: in the initial phase of SGD, the loss
function drives the weights to concentrate around the nearest local minimum. We
refer to this phase as the drift regime and we provide quantitative estimates
on this concentration phenomenon. Next, we introduce the diffusion regime,
where stochastic fluctuations help the learning process to escape suboptimal
local minima. We analyze the Mean Exit Time (MET) and prove upper and lower
bounds of the MET. Finally, we address the asymptotic convergence of SGD, for a
non-convex cost function and a degenerate diffusion matrix, that do not allow
to use the standard approaches, and require new techniques. For this purpose,
we exploit two different methods: duality and entropy methods.
  We provide new results about the dynamics and effectiveness of SGD, offering
a deep connection between stochastic optimization and PDE theory, and some
answers and insights to basic questions in the Machine Learning processes: How
long does SGD take to escape from a bad minimum? Do neural network parameters
converge using SGD? How do parameters evolve in the first stage of training
with SGD?

</details>


### [619] [Amortized Safe Active Learning for Real-Time Data Acquisition: Pretrained Neural Policies from Simulated Nonparametric Functions](https://arxiv.org/pdf/2501.15458)
*Cen-You Li, Marc Toussaint, Barbara Rakitsch, Christoph Zimmer*

Main category: cs.LG

TL;DR: An amortized safe active learning framework replaces costly online computations with a pretrained neural policy, improving speed while maintaining safety and learning quality.


<details>
  <summary>Details</summary>
Motivation: Existing safe active learning methods rely on Gaussian processes, which are computationally expensive for real-time decision-making.

Method: The proposed framework uses a pretrained neural policy trained on simulated nonparametric functions, eliminating the need for GP inference or constrained optimization during deployment.

Result: The framework achieves significant speed improvements while preserving safety and learning quality.

Conclusion: The modular framework is adaptable to unconstrained, time-sensitive active learning tasks by omitting safety requirements.

Abstract: Safe active learning (AL) is a sequential scheme for learning unknown systems
while respecting safety constraints during data acquisition. Existing methods
often rely on Gaussian processes (GPs) to model the task and safety
constraints, requiring repeated GP updates and constrained acquisition
optimization-incurring in significant computations which are challenging for
real-time decision-making. We propose an amortized safe AL framework that
replaces expensive online computations with a pretrained neural policy.
Inspired by recent advances in amortized Bayesian experimental design, we turn
GPs into a pretraining simulator. We train our policy prior to the AL
deployment on simulated nonparametric functions, using Fourier feature-based GP
sampling and a differentiable, safety-aware acquisition objective. At
deployment, our policy selects safe and informative queries via a single
forward pass, eliminating the need for GP inference or constrained
optimization. This leads to substantial speed improvements while preserving
safety and learning quality. Our framework is modular and can be adapted to
unconstrained, time-sensitive AL tasks by omitting the safety requirement.

</details>


### [620] [Mamba-Based Graph Convolutional Networks: Tackling Over-smoothing with Selective State Space](https://arxiv.org/pdf/2501.15461)
*Xin He, Yili Wang, Wenqi Fan, Xu Shen, Xin Juan, Rui Miao, Xin Wang*

Main category: cs.LG

TL;DR: MbaGCN, a new GNN architecture inspired by the Mamba paradigm, addresses over-smoothing by adaptively aggregating neighborhood information through three key layers.


<details>
  <summary>Details</summary>
Motivation: Over-smoothing in deep GNNs causes indistinguishable node representations, limiting their effectiveness. MbaGCN aims to solve this by integrating the Mamba paradigm for better information aggregation.

Method: MbaGCN introduces three layers: Message Aggregation, Selective State Space Transition, and Node State Prediction, to adaptively process neighborhood information.

Result: MbaGCN shows promise as a foundational framework for deep GNNs, though it doesn't always outperform existing methods.

Conclusion: MbaGCN effectively integrates the Mamba paradigm into GNNs, offering a scalable solution for future research in graph representation learning.

Abstract: Graph Neural Networks (GNNs) have shown great success in various graph-based
learning tasks. However, it often faces the issue of over-smoothing as the
model depth increases, which causes all node representations to converge to a
single value and become indistinguishable. This issue stems from the inherent
limitations of GNNs, which struggle to distinguish the importance of
information from different neighborhoods. In this paper, we introduce MbaGCN, a
novel graph convolutional architecture that draws inspiration from the Mamba
paradigm-originally designed for sequence modeling. MbaGCN presents a new
backbone for GNNs, consisting of three key components: the Message Aggregation
Layer, the Selective State Space Transition Layer, and the Node State
Prediction Layer. These components work in tandem to adaptively aggregate
neighborhood information, providing greater flexibility and scalability for
deep GNN models. While MbaGCN may not consistently outperform all existing
methods on each dataset, it provides a foundational framework that demonstrates
the effective integration of the Mamba paradigm into graph representation
learning. Through extensive experiments on benchmark datasets, we demonstrate
that MbaGCN paves the way for future advancements in graph neural network
research.

</details>


### [621] [Enhancing Sample Selection Against Label Noise by Cutting Mislabeled Easy Examples](https://arxiv.org/pdf/2502.08227)
*Suqin Yuan, Lei Feng, Bo Han, Tongliang Liu*

Main category: cs.LG

TL;DR: The paper introduces Early Cutting, a method to filter out harmful mislabeled easy examples (MEEs) during training, improving model performance.


<details>
  <summary>Details</summary>
Motivation: Existing sample selection methods overlook the varying harm of mislabeled examples, especially MEEs, which degrade model performance.

Method: Proposes Early Cutting, a recalibration step using later training states to re-select confident samples and filter MEEs.

Result: Experiments on CIFAR, WebVision, and ImageNet-1k show improved sample selection and model performance.

Conclusion: Early Cutting effectively reduces MEEs, enhancing model robustness and performance.

Abstract: Sample selection is a prevalent approach in learning with noisy labels,
aiming to identify confident samples for training. Although existing sample
selection methods have achieved decent results by reducing the noise rate of
the selected subset, they often overlook that not all mislabeled examples harm
the model's performance equally. In this paper, we demonstrate that mislabeled
examples correctly predicted by the model early in the training process are
particularly harmful to model performance. We refer to these examples as
Mislabeled Easy Examples (MEEs). To address this, we propose Early Cutting,
which introduces a recalibration step that employs the model's later training
state to re-select the confident subset identified early in training, thereby
avoiding misleading confidence from early learning and effectively filtering
out MEEs. Experiments on the CIFAR, WebVision, and full ImageNet-1k datasets
demonstrate that our method effectively improves sample selection and model
performance by reducing MEEs.

</details>


### [622] [Keep your distance: learning dispersed embeddings on $\mathbb{S}_m$](https://arxiv.org/pdf/2502.08231)
*Evgeniia Tokarchuk, Hua Chang Bakker, Vlad Niculae*

Main category: cs.LG

TL;DR: The paper explores methods to achieve well-separated features in high-dimensional spaces, focusing on dispersion of embeddings. It connects dispersion to mathematical problems, proposes new methods including an online variant of Lloyd's algorithm, and demonstrates their effectiveness in tasks like image classification and NLP.


<details>
  <summary>Details</summary>
Motivation: Learning well-separated features is crucial for machine learning applications, but existing theoretical and numerical solutions are often inapplicable due to high dimensionality and trade-offs with task-oriented objectives.

Method: The paper reviews existing methods, proposes a reinterpretation of pairwise dispersion using MMD, introduces an online variant of Lloyd's algorithm, and derives a novel hypersphere-based dispersion method.

Result: Experiments show the importance of dispersion in tasks like image classification and NLP, with different algorithms exhibiting trade-offs in various regimes.

Conclusion: The study highlights the significance of dispersion in representation learning, offering new methods and insights for achieving it effectively.

Abstract: Learning well-separated features in high-dimensional spaces, such as text or
image embeddings, is crucial for many machine learning applications. Achieving
such separation can be effectively accomplished through the dispersion of
embeddings, where unrelated vectors are pushed apart as much as possible. By
constraining features to be on a hypersphere, we can connect dispersion to
well-studied problems in mathematics and physics, where optimal solutions are
known for limited low-dimensional cases. However, in representation learning we
typically deal with a large number of features in high-dimensional space, and
moreover, dispersion is usually traded off with some other task-oriented
training objective, making existing theoretical and numerical solutions
inapplicable. Therefore, it is common to rely on gradient-based methods to
encourage dispersion, usually by minimizing some function of the pairwise
distances. In this work, we first give an overview of existing methods from
disconnected literature, making new connections and highlighting similarities.
Next, we introduce some new angles. We propose to reinterpret pairwise
dispersion using a maximum mean discrepancy (MMD) motivation. We then propose
an online variant of the celebrated Lloyd's algorithm, of K-Means fame, as an
effective alternative regularizer for dispersion on generic domains. Finally,
we derive a novel dispersion method that directly exploits properties of the
hypersphere. Our experiments show the importance of dispersion in image
classification and natural language processing tasks, and how algorithms
exhibit different trade-offs in different regimes.

</details>


### [623] [InFL-UX: A Toolkit for Web-Based Interactive Federated Learning](https://arxiv.org/pdf/2503.04318)
*Tim Maurer, Abdulrahman Mohamed Selim, Hasan Md Tusfiqur Alam, Matthias Eiletz, Michael Barz, Daniel Sonntag*

Main category: cs.LG

TL;DR: InFL-UX is a browser-based Federated Learning toolkit that integrates user contributions into ML workflows, enabling collaborative model training with a focus on usability and real-world interaction.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between Federated Learning and Interactive Machine Learning by involving non-technical users in ML tasks through a simple, interactive interface.

Method: InFL-UX allows users to upload datasets, define classes, and train classification models collaboratively in the browser using modern web technologies.

Result: The toolkit facilitates real-world, interactive FL settings, prioritizing usability and decentralized training.

Conclusion: InFL-UX empowers non-technical users to actively participate in ML classification, merging FL and IML effectively.

Abstract: This paper presents InFL-UX, an interactive, proof-of-concept browser-based
Federated Learning (FL) toolkit designed to integrate user contributions
seamlessly into the machine learning (ML) workflow. InFL-UX enables users
across multiple devices to upload datasets, define classes, and collaboratively
train classification models directly in the browser using modern web
technologies. Unlike traditional FL toolkits, which often focus on backend
simulations, InFL-UX provides a simple user interface for researchers to
explore how users interact with and contribute to FL systems in real-world,
interactive settings. By prioritising usability and decentralised model
training, InFL-UX bridges the gap between FL and Interactive Machine Learning
(IML), empowering non-technical users to actively participate in ML
classification tasks.

</details>


### [624] [Predicting Human Choice Between Textually Described Lotteries](https://arxiv.org/pdf/2503.14004)
*Eyal Marantz, Ori Plonsky*

Main category: cs.LG

TL;DR: Fine-tuned LLMs like GPT-4o outperform hybrid models in predicting human decisions from text-based lotteries, revealing differences in textual vs. numerical decision-making.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding human decision-making from textual descriptions, as prior work focused on numerical lotteries.

Method: Evaluated computational approaches (fine-tuned LLMs, embeddings, behavioral theories) on a large dataset of text-based binary choices.

Result: Fine-tuned LLMs, especially GPT-4o, surpassed hybrid models incorporating behavioral theories.

Conclusion: Textual and numerical decision-making differ significantly, necessitating new modeling strategies.

Abstract: Predicting human decision-making under risk and uncertainty is a
long-standing challenge in cognitive science, economics, and AI. While prior
research has focused on numerically described lotteries, real-world decisions
often rely on textual descriptions. This study conducts the first large-scale
exploration of human decision-making in such tasks using a large dataset of
one-shot binary choices between textually described lotteries. We evaluate
multiple computational approaches, including fine-tuning Large Language Models
(LLMs), leveraging embeddings, and integrating behavioral theories of choice
under risk. Our results show that fine-tuned LLMs, specifically GPT-4o,
outperform hybrid models that incorporate behavioral theory, challenging
established methods in numerical settings. These findings highlight fundamental
differences in how textual and numerical information influence decision-making
and underscore the need for new modeling strategies to bridge this gap.

</details>


### [625] [UniMoMo: Unified Generative Modeling of 3D Molecules for De Novo Binder Design](https://arxiv.org/pdf/2503.19300)
*Xiangzhe Kong, Zishen Zhang, Ziting Zhang, Rui Jiao, Jianzhu Ma, Wenbing Huang, Kai Liu, Yang Liu*

Main category: cs.LG

TL;DR: UniMoMo is a unified framework for designing multi-domain molecules using a single model, outperforming domain-specific methods.


<details>
  <summary>Details</summary>
Motivation: Existing generative methods are limited to single-domain molecules, failing to meet versatile therapeutic needs or leverage cross-domain transferability.

Method: UniMoMo unifies molecular representations as graphs of blocks, using a geometric latent diffusion model for 3D generation, with an iterative full-atom autoencoder and E(3)-equivariant diffusion.

Result: Benchmarks show UniMoMo's superiority over domain-specific models in generating peptides, antibodies, and small molecules.

Conclusion: Multi-domain training enhances model performance, making UniMoMo a versatile tool for molecular design.

Abstract: The design of target-specific molecules such as small molecules, peptides,
and antibodies is vital for biological research and drug discovery. Existing
generative methods are restricted to single-domain molecules, failing to
address versatile therapeutic needs or utilize cross-domain transferability to
enhance model performance. In this paper, we introduce Unified generative
Modeling of 3D Molecules (UniMoMo), the first framework capable of designing
binders of multiple molecular domains using a single model. In particular,
UniMoMo unifies the representations of different molecules as graphs of blocks,
where each block corresponds to either a standard amino acid or a molecular
fragment. Subsequently, UniMoMo utilizes a geometric latent diffusion model for
3D molecular generation, featuring an iterative full-atom autoencoder to
compress blocks into latent space points, followed by an E(3)-equivariant
diffusion process. Extensive benchmarks across peptides, antibodies, and small
molecules demonstrate the superiority of our unified framework over existing
domain-specific models, highlighting the benefits of multi-domain training.

</details>


### [626] [Semi-supervised Node Importance Estimation with Informative Distribution Modeling for Uncertainty Regularization](https://arxiv.org/pdf/2503.20697)
*Yankai Chen, Taotao Wang, Yixiang Fang, Yunyu Xiao*

Main category: cs.LG

TL;DR: EASING is a semi-supervised framework for node importance estimation in heterogeneous graphs, incorporating uncertainty modeling and pseudo-labeling to improve learning quality for unlabeled data.


<details>
  <summary>Details</summary>
Motivation: Ground-truth node-importance data are often partially labeled, and existing supervised methods overlook this limitation.

Method: EASING uses DJE, a deep encoder-decoder architecture, to model node distributions for importance and uncertainty estimation, and employs pseudo-labeling for semi-supervised learning.

Result: EASING outperforms competing methods on three real-world datasets.

Conclusion: EASING effectively addresses the challenge of partially labeled data in node importance estimation, offering superior performance and uncertainty-aware predictions.

Abstract: Node importance estimation, a classical problem in network analysis,
underpins various web applications. Previous methods either exploit intrinsic
topological characteristics, e.g., graph centrality, or leverage additional
information, e.g., data heterogeneity, for node feature enhancement. However,
these methods follow the supervised learning setting, overlooking the fact that
ground-truth node-importance data are usually partially labeled in practice. In
this work, we propose the first semi-supervised node importance estimation
framework, i.e., EASING, to improve learning quality for unlabeled data in
heterogeneous graphs. Different from previous approaches, EASING explicitly
captures uncertainty to reflect the confidence of model predictions. To jointly
estimate the importance values and uncertainties, EASING incorporates DJE, a
deep encoder-decoder neural architecture. DJE introduces distribution modeling
for graph nodes, where the distribution representations derive both importance
and uncertainty estimates. Additionally, DJE facilitates effective pseudo-label
generation for the unlabeled data to enrich the training samples. Based on
labeled and pseudo-labeled data, EASING develops effective semi-supervised
heteroscedastic learning with varying node uncertainty regularization.
Extensive experiments on three real-world datasets highlight the superior
performance of EASING compared to competing methods. Codes are available via
https://github.com/yankai-chen/EASING.

</details>


### [627] [Topological Schrödinger Bridge Matching](https://arxiv.org/pdf/2504.04799)
*Maosheng Yang*

Main category: cs.LG

TL;DR: The paper introduces the Topological Schrödinger Bridge problem (TSBP) for matching signal distributions on topological domains, extending traditional SB methods to graphs and simplicial complexes.


<details>
  <summary>Details</summary>
Motivation: Existing SB methods are limited to Euclidean domains, but many real-world data (e.g., node signals, edge flows) are defined over topological domains like graphs. The paper aims to bridge this gap.

Method: The authors propose TSBP, using topology-aware stochastic dynamics (e.g., topological heat diffusion) as a reference process. For Gaussian boundary distributions, they derive a closed-form solution; for general cases, they parameterize unknowns in the optimal process as neural networks and train them via likelihood.

Result: Theoretical results are validated on synthetic and real-world networks, demonstrating the effectiveness of TSB-based models for topological signal matching.

Conclusion: The work successfully extends SB to topological domains, offering practical applications and connections to emerging models, with future directions outlined for further research.

Abstract: Given two boundary distributions, the Schr\"odinger Bridge (SB) problem seeks
the ``most likely`` random evolution between them with respect to a reference
process. It has revealed rich connections to recent machine learning methods
for generative modeling and distribution matching. While these methods perform
well in Euclidean domains, they are not directly applicable to topological
domains such as graphs and simplicial complexes, which are crucial for data
defined over network entities, such as node signals and edge flows. In this
work, we propose the Topological Schr\"odinger Bridge problem (TSBP) for
matching signal distributions on a topological domain. We set the reference
process to follow some linear tractable topology-aware stochastic dynamics such
as topological heat diffusion. For the case of Gaussian boundary distributions,
we derive a closed-form topological SB (TSB) in terms of its time-marginal and
stochastic differential. In the general case, leveraging the well-known result,
we show that the optimal process follows the forward-backward topological
dynamics governed by some unknowns. Building on these results, we develop
TSB-based models for matching topological signals by parameterizing the
unknowns in the optimal process as (topological) neural networks and learning
them through likelihood training. We validate the theoretical results and
demonstrate the practical applications of TSB-based models on both synthetic
and real-world networks, emphasizing the role of topology. Additionally, we
discuss the connections of TSB-based models to other emerging models, and
outline future directions for topological signal matching.

</details>


### [628] [Proofs as Explanations: Short Certificates for Reliable Predictions](https://arxiv.org/pdf/2504.08377)
*Avrim Blum, Steve Hanneke, Chirag Pabbaraju, Donya Saless*

Main category: cs.LG

TL;DR: The paper proposes a model for explainable AI where explanations are subsets of training data proving predictions under certain assumptions. It generalizes this for any hypothesis class and analyzes the size and bounds of such proofs.


<details>
  <summary>Details</summary>
Motivation: To provide a framework for explainable AI by certifying predictions using subsets of training data, ensuring robustness and generalizability across hypothesis classes.

Method: Defines the robust hollow star number for hypothesis classes, analyzes worst-case and distributional bounds on certificate size, and introduces the certificate coefficient for sample size analysis.

Result: The robust hollow star number characterizes the smallest certificate size, and matching bounds on sample size are derived based on the certificate coefficient, VC dimension, and corruption tolerance.

Conclusion: The work generalizes explainable AI proofs, providing theoretical guarantees on certificate size and sample complexity, applicable to various hypothesis classes.

Abstract: We consider a model for explainable AI in which an explanation for a
prediction $h(x)=y$ consists of a subset $S'$ of the training data (if it
exists) such that all classifiers $h' \in H$ that make at most $b$ mistakes on
$S'$ predict $h'(x)=y$. Such a set $S'$ serves as a proof that $x$ indeed has
label $y$ under the assumption that (1) the target function $h^\star$ belongs
to $H$, and (2) the set $S$ contains at most $b$ corrupted points. For example,
if $b=0$ and $H$ is the family of linear classifiers in $\mathbb{R}^d$, and if
$x$ lies inside the convex hull of the positive data points in $S$ (and hence
every consistent linear classifier labels $x$ as positive), then
Carath\'eodory's theorem states that $x$ lies inside the convex hull of $d+1$
of those points. So, a set $S'$ of size $d+1$ could be released as an
explanation for a positive prediction, and would serve as a short proof of
correctness of the prediction under the assumption of realizability.
  In this work, we consider this problem more generally, for general hypothesis
classes $H$ and general values $b\geq 0$. We define the notion of the robust
hollow star number of $H$ (which generalizes the standard hollow star number),
and show that it precisely characterizes the worst-case size of the smallest
certificate achievable, and analyze its size for natural classes. We also
consider worst-case distributional bounds on certificate size, as well as
distribution-dependent bounds that we show tightly control the sample size
needed to get a certificate for any given test example. In particular, we
define a notion of the certificate coefficient $\varepsilon_x$ of an example
$x$ with respect to a data distribution $D$ and target function $h^\star$, and
prove matching upper and lower bounds on sample size as a function of
$\varepsilon_x$, $b$, and the VC dimension $d$ of $H$.

</details>


### [629] [Inference-friendly Graph Compression for Graph Neural Networks](https://arxiv.org/pdf/2504.13034)
*Yangxin Fan, Haolai Che, Yinghui Wu*

Main category: cs.LG

TL;DR: The paper proposes IFGC, a graph compression method to accelerate GNN inference by preserving inference results while minimizing decompression costs.


<details>
  <summary>Details</summary>
Motivation: GNN inference is costly for large graphs, limiting their practical applications. IFGC aims to address this by compressing graphs without compromising inference quality.

Method: IFGC introduces inference equivalence relations and three practical schemes: SPGC, (α, r)-compression, and anchored compression, each with tailored algorithms.

Result: Experiments on large-scale graphs confirm IFGC's effectiveness and efficiency in accelerating GNN inference.

Conclusion: IFGC provides a scalable solution for efficient GNN inference on large graphs, balancing compression and accuracy.

Abstract: Graph Neural Networks (GNNs) have demonstrated promising performance in graph
analysis. Nevertheless, the inference process of GNNs remains costly, hindering
their applications for large graphs. This paper proposes inference-friendly
graph compression (IFGC), a graph compression scheme to accelerate GNNs
inference. Given a graph $G$ and a GNN $M$, an IFGC computes a small compressed
graph $G_c$, to best preserve the inference results of $M$ over $G$, such that
the result can be directly inferred by accessing $G_c$ with no or little
decompression cost. (1) We characterize IFGC with a class of inference
equivalence relation. The relation captures the node pairs in $G$ that are not
distinguishable for GNN inference. (2) We introduce three practical
specifications of IFGC for representative GNNs: structural preserving
compression (SPGC), which computes $G_c$ that can be directly processed by GNN
inference without decompression; ($\alpha$, $r$)-compression, that allows for a
configurable trade-off between compression ratio and inference quality, and
anchored compression that preserves inference results for specific nodes of
interest. For each scheme, we introduce compression and inference algorithms
with guarantees of efficiency and quality of the inferred results. We conduct
extensive experiments on diverse sets of large-scale graphs, which verifies the
effectiveness and efficiency of our graph compression approaches.

</details>


### [630] [A Comprehensive Survey of Synthetic Tabular Data Generation](https://arxiv.org/pdf/2504.16506)
*Ruxue Shi, Yili Wang, Mengnan Du, Xu Shen, Xin Wang*

Main category: cs.LG

TL;DR: A survey on synthetic tabular data generation, addressing gaps in existing reviews by unifying diverse methods (traditional, diffusion-based, LLM-based) and detailing pipelines, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: Challenges like data scarcity, privacy, and class imbalance limit ML use of tabular data. Existing surveys are fragmented, missing recent advances (e.g., LLMs, diffusion models).

Method: Proposes a taxonomy of methods, details the synthetic data pipeline (synthesis, post-processing, evaluation), and compares approaches.

Result: Provides a unified review, comparative analysis, and identifies challenges and applications.

Conclusion: Highlights open research questions and future directions to advance synthetic tabular data generation.

Abstract: Tabular data remains one of the most prevalent and critical data formats
across diverse real-world applications. However, its effective use in machine
learning (ML) is often constrained by challenges such as data scarcity, privacy
concerns, and class imbalance. Synthetic data generation has emerged as a
promising solution, leveraging generative models to learn the distribution of
real datasets and produce high-fidelity, privacy-preserving samples. Various
generative paradigms have been explored, including energy-based models (EBMs),
variational autoencoders (VAEs), generative adversarial networks (GANs), large
language models (LLMs), and diffusion models. While several surveys have
investigated synthetic tabular data generation, most focus on narrow subdomains
or specific generative methods, such as GANs, diffusion models, or
privacy-preserving techniques. This limited scope often results in fragmented
insights, lacking a comprehensive synthesis that bridges diverse approaches. In
particular, recent advances driven by LLMs and diffusion-based models remain
underexplored. This gap hinders a holistic understanding of the field`s
evolution, methodological interplay, and open challenges. To address this, our
survey provides a unified and systematic review of synthetic tabular data
generation. Our contributions are threefold: (1) we propose a comprehensive
taxonomy that organizes existing methods into traditional approaches,
diffusion-based methods, and LLM-based models, and provide an in-depth
comparative analysis; (2) we detail the complete pipeline for synthetic tabular
data generation, including data synthesis, post-processing, and evaluation; (3)
we identify major challenges, explore real-world applications, and outline open
research questions and future directions to guide future work in this rapidly
evolving area.

</details>


### [631] [Low-Rank Matrix Approximation for Neural Network Compression](https://arxiv.org/pdf/2504.20078)
*Kalyan Cherukuri, Aarav Lala*

Main category: cs.LG

TL;DR: ARSVD method adaptively compresses DNN weight matrices using spectral entropy, outperforming fixed-rank SVD by reducing redundancy and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the high memory and computation costs of DNNs by optimizing layer-wise compression.

Method: Uses spectral entropy to adaptively select rank per layer, ensuring retention of informational content.

Result: Achieves better performance with reduced space and time complexity compared to fixed-rank methods.

Conclusion: ARSVD offers a more efficient and adaptive approach to DNN compression.

Abstract: Deep Neural Networks (DNNs) have encountered an emerging deployment challenge
due to large and expensive memory and computation requirements. In this paper,
we present a new Adaptive-Rank Singular Value Decomposition (ARSVD) method that
approximates the optimal rank for compressing weight matrices in neural
networks using spectral entropy. Unlike conventional SVD-based methods that
apply a fixed-rank truncation across all layers, ARSVD uses an adaptive
selection of the rank per layer through the entropy distribution of its
singular values. This approach ensures that each layer will retain a certain
amount of its informational content, thereby reducing redundancy. Our method
enables efficient, layer-wise compression, yielding improved performance with
reduced space and time complexity compared to static-rank reduction techniques.

</details>


### [632] [FreCT: Frequency-augmented Convolutional Transformer for Robust Time Series Anomaly Detection](https://arxiv.org/pdf/2505.00941)
*Wenxin Zhang, Ding Xu, Guangzhen Yao, Xiaojian Lin, Renxiang Guan, Chengze Du, Renda Han, Xi Xuan, Cuicui Luo*

Main category: cs.LG

TL;DR: FreCT, a novel Frequency-augmented Convolutional Transformer, improves time series anomaly detection by integrating frequency analysis and robust training techniques, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing reconstruction-based anomaly detection methods struggle with sequential pattern complexity, computational deviations from anomalies, and lack of frequency-domain alignment.

Method: FreCT combines patch operations for contrastive views, a Transformer with convolution for long-term and local dependencies, and Fourier-based frequency analysis. It uses stop-gradient KL divergence and absolute error for robust training.

Result: FreCT outperforms existing methods in anomaly detection on four public datasets.

Conclusion: FreCT effectively addresses challenges in time series anomaly detection by leveraging frequency augmentation and robust optimization, demonstrating superior performance.

Abstract: Time series anomaly detection is critical for system monitoring and risk
identification, across various domains, such as finance and healthcare.
However, for most reconstruction-based approaches, detecting anomalies remains
a challenge due to the complexity of sequential patterns in time series data.
On the one hand, reconstruction-based techniques are susceptible to
computational deviation stemming from anomalies, which can lead to impure
representations of normal sequence patterns. On the other hand, they often
focus on the time-domain dependencies of time series, while ignoring the
alignment of frequency information beyond the time domain. To address these
challenges, we propose a novel Frequency-augmented Convolutional Transformer
(FreCT). FreCT utilizes patch operations to generate contrastive views and
employs an improved Transformer architecture integrated with a convolution
module to capture long-term dependencies while preserving local topology
information. The introduced frequency analysis based on Fourier transformation
could enhance the model's ability to capture crucial characteristics beyond the
time domain. To protect the training quality from anomalies and improve the
robustness, FreCT deploys stop-gradient Kullback-Leibler (KL) divergence and
absolute error to optimize consistency information in both time and frequency
domains. Extensive experiments on four public datasets demonstrate that FreCT
outperforms existing methods in identifying anomalies.

</details>


### [633] [Practical Efficiency of Muon for Pretraining](https://arxiv.org/pdf/2505.02222)
*Essential AI, :, Ishaan Shah, Anthony M. Polloreno, Karl Stratos, Philip Monk, Adarsh Chaluvaraju, Andrew Hojel, Andrew Ma, Anil Thomas, Ashish Tanwer, Darsh J Shah, Khoi Nguyen, Kurt Smith, Michael Callahan, Michael Pust, Mohit Parmar, Peter Rushton, Platon Mazarakis, Ritvik Kapila, Saurabh Srivastava, Somanshu Singla, Tim Romanski, Yash Vanjani, Ashish Vaswani*

Main category: cs.LG

TL;DR: Muon, a second-order optimizer, outperforms AdamW in compute-time tradeoff and data efficiency at large batch sizes, enabling economical training. Combined with muP, it offers efficient hyperparameter transfer with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: To improve optimization efficiency and data retention at large batch sizes beyond the critical batch size, while maintaining computational efficiency.

Method: Muon is used as a second-order optimizer, combined with maximal update parameterization (muP) and a telescoping algorithm to account for errors in muP.

Result: Muon retains data efficiency at large batch sizes and is computationally efficient, validated on models up to four billion parameters.

Conclusion: Muon expands the Pareto frontier over AdamW, offering a more economical and efficient training solution, especially when combined with muP.

Abstract: We demonstrate that Muon, the simplest instantiation of a second-order
optimizer, explicitly expands the Pareto frontier over AdamW on the
compute-time tradeoff. We find that Muon is more effective than AdamW in
retaining data efficiency at large batch sizes, far beyond the so-called
critical batch size, while remaining computationally efficient, thus enabling
more economical training. We study the combination of Muon and the maximal
update parameterization (muP) for efficient hyperparameter transfer and present
a simple telescoping algorithm that accounts for all sources of error in muP
while introducing only a modest overhead in resources. We validate our findings
through extensive experiments with model sizes up to four billion parameters
and ablations on the data distribution and architecture.

</details>


### [634] [Geospatial Mechanistic Interpretability of Large Language Models](https://arxiv.org/pdf/2505.03368)
*Stef De Sabbata, Stefano Mizzaro, Kevin Roitero*

Main category: cs.LG

TL;DR: The paper introduces a framework for studying how LLMs process geographical information using spatial analysis and mechanistic interpretability.


<details>
  <summary>Details</summary>
Motivation: To understand the internal representations of LLMs when handling geographical data, addressing a gap in knowledge about their spatial reasoning.

Method: Uses probing, mechanistic interpretability, and spatial autocorrelation to analyze LLMs' internal structures and geographical feature representations.

Result: Features for placenames show spatial patterns, revealing how LLMs process geographical information.

Conclusion: The framework advances the study of foundation models in geography, offering insights into LLMs' spatial reasoning.

Abstract: Large Language Models (LLMs) have demonstrated unprecedented capabilities
across various natural language processing tasks. Their ability to process and
generate viable text and code has made them ubiquitous in many fields, while
their deployment as knowledge bases and "reasoning" tools remains an area of
ongoing research. In geography, a growing body of literature has been focusing
on evaluating LLMs' geographical knowledge and their ability to perform spatial
reasoning. However, very little is still known about the internal functioning
of these models, especially about how they process geographical information.
  In this chapter, we establish a novel framework for the study of geospatial
mechanistic interpretability - using spatial analysis to reverse engineer how
LLMs handle geographical information. Our aim is to advance our understanding
of the internal representations that these complex models generate while
processing geographical information - what one might call "how LLMs think about
geographic information" if such phrasing was not an undue anthropomorphism.
  We first outline the use of probing in revealing internal structures within
LLMs. We then introduce the field of mechanistic interpretability, discussing
the superposition hypothesis and the role of sparse autoencoders in
disentangling polysemantic internal representations of LLMs into more
interpretable, monosemantic features. In our experiments, we use spatial
autocorrelation to show how features obtained for placenames display spatial
patterns related to their geographic location and can thus be interpreted
geospatially, providing insights into how these models process geographical
information. We conclude by discussing how our framework can help shape the
study and use of foundation models in geography.

</details>


### [635] [ItDPDM: Information-Theoretic Discrete Poisson Diffusion Model](https://arxiv.org/pdf/2505.05082)
*Sagnik Bhattacharya, Abhiram Gorle, Ahmed Mohsin, Ahsan Bilal, Connor Ding, Amit Kumar Singh Yadav, Tsachy Weissman*

Main category: cs.LG

TL;DR: ItDPDM introduces a discrete Poisson diffusion model for generative modeling, addressing limitations of existing methods by operating directly in discrete state-spaces and using a novel Poisson Reconstruction Loss.


<details>
  <summary>Details</summary>
Motivation: Existing methods for discrete data modeling either use continuous embeddings or approximate losses, leading to suboptimal performance. ItDPDM aims to overcome these issues.

Method: ItDPDM uses a Poisson diffusion process in discrete state-spaces and introduces a Poisson Reconstruction Loss (PRL) with an exact relationship to true negative log-likelihood.

Result: Experiments on Lakh MIDI and CIFAR-10 show ItDPDM reduces test NLL by up to 80% and achieves faster convergence.

Conclusion: ItDPDM effectively addresses limitations of prior methods, offering improved performance and efficiency in generative modeling of discrete data.

Abstract: Existing methods for generative modeling of discrete data, such as symbolic
music tokens, face two primary challenges: (1) they either embed discrete
inputs into continuous state-spaces or (2) rely on variational losses that only
approximate the true negative log-likelihood. Previous efforts have
individually targeted these limitations. While information-theoretic Gaussian
diffusion models alleviate the suboptimality of variational losses, they still
perform modeling in continuous domains. In this work, we introduce the
Information-Theoretic Discrete Poisson Diffusion Model (ItDPDM), which
simultaneously addresses both limitations by directly operating in a discrete
state-space via a Poisson diffusion process inspired by photon arrival
processes in camera sensors. We introduce a novel Poisson Reconstruction Loss
(PRL) and derive an exact relationship between PRL and the true negative
log-likelihood, thereby eliminating the need for approximate evidence lower
bounds. Experiments conducted on the Lakh MIDI symbolic music dataset and the
CIFAR-10 image benchmark demonstrate that ItDPDM delivers significant
improvements, reducing test NLL by up to 80% compared to prior baselines, while
also achieving faster convergence.

</details>


### [636] [FloE: On-the-Fly MoE Inference on Memory-constrained GPU](https://arxiv.org/pdf/2505.05950)
*Yuxin Zhou, Zheng Li, Jun Zhang, Jue Wang, Yiping Wang, Zhongle Xie, Ke Chen, Lidan Shou*

Main category: cs.LG

TL;DR: FloE is an efficient MoE inference system for memory-constrained GPUs, reducing data movement and enabling faster inference with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency of offloading expert parameters to CPU memory due to PCIe bandwidth limitations in latency-sensitive scenarios.

Method: Uses compression techniques on expert parameter matrices and low-cost sparse prediction to reduce data movement.

Result: Achieves 9.3x parameter compression, 8.5x memory reduction, and 48.7x speedup on a GeForce RTX 3090 with minimal performance degradation.

Conclusion: FloE effectively balances efficiency and performance for MoE inference on resource-constrained devices.

Abstract: With the widespread adoption of Mixture-of-Experts (MoE) models, there is a
growing demand for efficient inference on memory-constrained devices. While
offloading expert parameters to CPU memory and loading activated experts on
demand has emerged as a potential solution, the large size of activated experts
overburdens the limited PCIe bandwidth, hindering the effectiveness in
latency-sensitive scenarios. To mitigate this, we propose FloE, an on-the-fly
MoE inference system on memory-constrained GPUs. FloE is built on the insight
that there exists substantial untapped redundancy within sparsely activated
experts. It employs various compression techniques on the expert's internal
parameter matrices to reduce the data movement load, combined with low-cost
sparse prediction, achieving perceptible inference acceleration in wall-clock
time on resource-constrained devices. Empirically, FloE achieves a 9.3x
compression of parameters per expert in Mixtral-8x7B; enables deployment on a
GPU with only 11GB VRAM, reducing the memory footprint by up to 8.5x; and
delivers a 48.7x inference speedup compared to DeepSpeed-MII on a single
GeForce RTX 3090 - all with only a 4.4$\%$ - 7.6$\%$ average performance
degradation.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [637] [Internet of Agents: Fundamentals, Applications, and Challenges](https://arxiv.org/pdf/2505.07176)
*Yuntao Wang, Shaolong Guo, Yanghe Pan, Zhou Su, Fahao Chen, Tom H. Luan, Peng Li, Jiawen Kang, Dusit Niyato*

Main category: cs.MA

TL;DR: The paper introduces the Internet of Agents (IoA) as a framework for interconnecting and orchestrating autonomous AI agents, detailing its architecture, enablers, and open research challenges.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of AI agents necessitates a unified infrastructure for seamless interaction and collaboration among heterogeneous agents.

Method: The paper presents a hierarchical IoA architecture, analyzes key operational enablers, and identifies emerging applications.

Result: The framework enables dynamic discovery, adaptive communication, task matching, and conflict resolution among agents.

Conclusion: Open research directions are identified to build resilient and trustworthy IoA ecosystems.

Abstract: With the rapid proliferation of large language models and vision-language
models, AI agents have evolved from isolated, task-specific systems into
autonomous, interactive entities capable of perceiving, reasoning, and acting
without human intervention. As these agents proliferate across virtual and
physical environments, from virtual assistants to embodied robots, the need for
a unified, agent-centric infrastructure becomes paramount. In this survey, we
introduce the Internet of Agents (IoA) as a foundational framework that enables
seamless interconnection, dynamic discovery, and collaborative orchestration
among heterogeneous agents at scale. We begin by presenting a general IoA
architecture, highlighting its hierarchical organization, distinguishing
features relative to the traditional Internet, and emerging applications. Next,
we analyze the key operational enablers of IoA, including capability
notification and discovery, adaptive communication protocols, dynamic task
matching, consensus and conflict-resolution mechanisms, and incentive models.
Finally, we identify open research directions toward building resilient and
trustworthy IoA ecosystems.

</details>


### [638] [Hypergraph Coordination Networks with Dynamic Grouping for Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2505.07207)
*Chiqiang Liu, Dazi Li*

Main category: cs.MA

TL;DR: A novel framework combining dynamic spectral clustering and hypergraph neural networks improves adaptive group formation and information exchange in multi-agent reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in organizing agent relationships and dynamic coordination in cooperative multi-agent systems.

Method: Integrates dynamic spectral clustering with hypergraph neural networks, using attention mechanisms for selective information processing.

Result: Outperforms state-of-the-art methods in sample efficiency and final performance on cooperative tasks.

Conclusion: The framework effectively models complex agent relationships and enhances coordination in multi-agent systems.

Abstract: Cooperative multi-agent reinforcement learning faces significant challenges
in effectively organizing agent relationships and facilitating information
exchange, particularly when agents need to adapt their coordination patterns
dynamically. This paper presents a novel framework that integrates dynamic
spectral clustering with hypergraph neural networks to enable adaptive group
formation and efficient information processing in multi-agent systems. The
proposed framework dynamically constructs and updates hypergraph structures
through spectral clustering on agents' state histories, enabling higher-order
relationships to emerge naturally from agent interactions. The hypergraph
structure is enhanced with attention mechanisms for selective information
processing, providing an expressive and efficient way to model complex agent
relationships. This architecture can be implemented in both value-based and
policy-based paradigms through a unified objective combining task performance
with structural regularization. Extensive experiments on challenging
cooperative tasks demonstrate that our method significantly outperforms
state-of-the-art approaches in both sample efficiency and final performance.

</details>


### [639] [RAI: Flexible Agent Framework for Embodied AI](https://arxiv.org/pdf/2505.07532)
*Kajetan Rachwał, Maciej Majek, Bartłomiej Boczek, Kacper Dąbrowski, Paweł Liberadzki, Adam Dąbrowski, Maria Ganzha*

Main category: cs.MA

TL;DR: RAI is a framework for embodied Multi Agent Systems in robotics, integrating robotic stacks, LLMs, and simulations, tested on physical and digital robots with successful deployments and evaluations.


<details>
  <summary>Details</summary>
Motivation: To address the growing interest in embodied AI by providing a robust framework for integrating agents with robotics and simulations.

Method: RAI implements tools for agent integration with robotic stacks, LLMs, and simulations, including ROS 2 and embodiment mechanisms, tested on a physical robot and digital twin.

Result: Successful deployments in simulations (robot arm manipulator, tractor controller) and physical robot, demonstrating control, embodiment, and perception effectiveness.

Conclusion: RAI effectively builds multi-agent systems, identifies generative model shortcomings, and proves versatile in embodied AI tasks.

Abstract: With an increase in the capabilities of generative language models, a growing
interest in embodied AI has followed. This contribution introduces RAI - a
framework for creating embodied Multi Agent Systems for robotics. The proposed
framework implements tools for Agents' integration with robotic stacks, Large
Language Models, and simulations. It provides out-of-the-box integration with
state-of-the-art systems like ROS 2. It also comes with dedicated mechanisms
for the embodiment of Agents. These mechanisms have been tested on a physical
robot, Husarion ROSBot XL, which was coupled with its digital twin, for rapid
prototyping. Furthermore, these mechanisms have been deployed in two
simulations: (1) robot arm manipulator and (2) tractor controller. All of these
deployments have been evaluated in terms of their control capabilities,
effectiveness of embodiment, and perception ability. The proposed framework has
been used successfully to build systems with multiple agents. It has
demonstrated effectiveness in all the aforementioned tasks. It also enabled
identifying and addressing the shortcomings of the generative models used for
embodied AI.

</details>


### [640] [The evolutionary advantage of guilt: co-evolution of social and non-social guilt in structured populations](https://arxiv.org/pdf/2302.09859)
*Theodor Cimpeanu, Luis Moniz Pereira, The Anh Han*

Main category: cs.MA

TL;DR: The paper explores how guilt evolves in machines, distinguishing between social and non-social guilt, and its impact on cooperation in structured vs. unstructured populations.


<details>
  <summary>Details</summary>
Motivation: To understand the evolution of guilt as a behavioral trait in ethical machines and its role in fostering cooperation.

Method: Uses evolutionary game theory, analytical models, and agent-based simulations to study guilt in lattice, scale-free, and well-mixed networks.

Result: In structured networks, guilt strategies dominate and enhance cooperation, with non-social guilt thriving due to lower costs and protection from exploiters.

Conclusion: Structured environments favor the evolution of guilt, offering insights for designing ethical AI systems.

Abstract: Building ethical machines may involve bestowing upon them the emotional
capacity to self-evaluate and repent on their actions. While apologies
represent potential strategic interactions, the explicit evolution of guilt as
a behavioural trait remains poorly understood. Our study delves into the
co-evolution of two forms of emotional guilt: social guilt entails a cost,
requiring agents to exert efforts to understand others' internal states and
behaviours; and non-social guilt, which only involves awareness of one's own
state, incurs no social cost. Resorting to methods from evolutionary game
theory, we study analytically, and through extensive numerical and agent-based
simulations, whether and how guilt can evolve and deploy, depending on the
underlying structure of the systems of agents. Our findings reveal that in
lattice and scale-free networks, strategies favouring emotional guilt dominate
a broader range of guilt and social costs compared to non-structured well-mixed
populations, so leading to higher levels of cooperation. In structured
populations, both social and non-social guilt can thrive through clustering
with emotionally inclined strategies, thereby providing protection against
exploiters, particularly for less costly non-social strategies. These insights
shed light on the complex interplay of guilt and cooperation, enhancing our
understanding of ethical artificial intelligence.

</details>


### [641] [A Multi-Agent Rollout Approach for Highway Bottleneck Decongestion in Mixed Autonomy](https://arxiv.org/pdf/2405.03132)
*Lu Liu, Maonan Wang, Man-On Pun, Xi Xiong*

Main category: cs.MA

TL;DR: A multi-agent rollout approach optimizes traffic flow in mixed autonomy environments by controlling AVs to coordinate human-driven vehicles, reducing congestion by 9.42% with 10% AV penetration.


<details>
  <summary>Details</summary>
Motivation: To alleviate congestion and enhance mobility by integrating AVs into existing infrastructure, focusing on real-time traffic flow optimization at highway bottlenecks.

Method: Uses a decentralized partially observable Markov decision process (Dec-POMDP) and an improved multi-agent rollout algorithm with agent-by-agent policy iterations.

Result: Simulations show a 9.42% reduction in average travel time at bottlenecks with 10% AV penetration.

Conclusion: The multi-agent rollout algorithm effectively optimizes traffic flow in dynamic, mixed autonomy environments.

Abstract: The integration of autonomous vehicles (AVs) into the existing transportation
infrastructure offers a promising solution to alleviate congestion and enhance
mobility. This research explores a novel approach to traffic optimization by
employing a multi-agent rollout approach within a mixed autonomy environment.
The study concentrates on coordinating the speed of human-driven vehicles by
longitudinally controlling AVs, aiming to dynamically optimize traffic flow and
alleviate congestion at highway bottlenecks in real-time. We model the problem
as a decentralized partially observable Markov decision process (Dec-POMDP) and
propose an improved multi-agent rollout algorithm. By employing agent-by-agent
policy iterations, our approach implicitly considers cooperation among multiple
agents and seamlessly adapts to complex scenarios where the number of agents
dynamically varies. Validated in a real-world network with varying AV
penetration rates and traffic flow, the simulations demonstrate that the
multi-agent rollout algorithm significantly enhances performance, reducing
average travel time on bottleneck segments by 9.42% with a 10% AV penetration
rate.

</details>


### [642] [Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation](https://arxiv.org/pdf/2505.03586)
*Songchen Fu, Siang Chen, Shaojing Zhao, Letian Bai, Ta Li, Yonghong Yan*

Main category: cs.MA

TL;DR: The paper introduces the DSID-POMDP framework to address stochastic individual delays in multi-agent systems and proposes the RDC training framework to mitigate performance degradation caused by delays.


<details>
  <summary>Details</summary>
Motivation: Observation delays in multi-agent systems hinder agents from making decisions based on the true state of the environment, posing challenges for MARL.

Method: The authors extend Dec-POMDP to formulate DSID-POMDP and propose the RDC framework with recommended module implementations. They test it using MARL benchmarks (MPE and SMAC).

Result: Baseline MARL methods perform poorly under delays, while RDC achieves near delay-free performance in some scenarios and maintains generalizability.

Conclusion: The work offers a novel solution for delayed observation problems in MASs, with practical applicability demonstrated through experiments.

Abstract: In real-world multi-agent systems (MASs), observation delays are ubiquitous,
preventing agents from making decisions based on the environment's true state.
An individual agent's local observation often consists of multiple components
from other agents or dynamic entities in the environment. These discrete
observation components with varying delay characteristics pose significant
challenges for multi-agent reinforcement learning (MARL). In this paper, we
first formulate the decentralized stochastic individual delay partially
observable Markov decision process (DSID-POMDP) by extending the standard
Dec-POMDP. We then propose the Rainbow Delay Compensation (RDC), a MARL
training framework for addressing stochastic individual delays, along with
recommended implementations for its constituent modules. We implement the
DSID-POMDP's observation generation pattern using standard MARL benchmarks,
including MPE and SMAC. Experiments demonstrate that baseline MARL methods
suffer severe performance degradation under fixed and unfixed delays. The
RDC-enhanced approach mitigates this issue, remarkably achieving ideal
delay-free performance in certain delay scenarios while maintaining
generalizability. Our work provides a novel perspective on multi-agent delayed
observation problems and offers an effective solution framework. The source
code is available at https://anonymous.4open.science/r/RDC-pymarl-4512/.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [643] [Emotion-Qwen: Training Hybrid Experts for Unified Emotion and General Vision-Language Understanding](https://arxiv.org/pdf/2505.06685)
*Dawei Huang, Qing Li, Chuan Yan, Zebang Cheng, Yurong Huang, Xiang Li, Bin Li, Xiaohui Wang, Zheng Lian, Xiaojiang Peng*

Main category: cs.MM

TL;DR: Emotion-Qwen is a multimodal framework for emotion understanding in videos, addressing limitations of LMMs by using a Hybrid Compressor and pre-training on large datasets. It achieves top performance on emotion benchmarks while maintaining general VL task capabilities.


<details>
  <summary>Details</summary>
Motivation: Current LMMs underperform in emotion-specific tasks and suffer from catastrophic forgetting when fine-tuned. Emotion-Qwen aims to enhance emotion understanding without sacrificing general VL reasoning.

Method: The framework uses a Hybrid Compressor (MoE-based) to dynamically route inputs. It is pre-trained in three stages on general and emotional datasets and evaluated on the VER dataset (40K bilingual video clips).

Result: Emotion-Qwen achieves state-of-the-art performance on emotion benchmarks and remains competitive on general VL tasks.

Conclusion: Emotion-Qwen effectively balances emotion-specific and general-purpose processing, advancing multimodal emotion understanding.

Abstract: Emotion understanding in videos aims to accurately recognize and interpret
individuals' emotional states by integrating contextual, visual, textual, and
auditory cues. While Large Multimodal Models (LMMs) have demonstrated
significant progress in general vision-language (VL) tasks, their performance
in emotion-specific scenarios remains limited. Moreover, fine-tuning LMMs on
emotion-related tasks often leads to catastrophic forgetting, hindering their
ability to generalize across diverse tasks. To address these challenges, we
present Emotion-Qwen, a tailored multimodal framework designed to enhance both
emotion understanding and general VL reasoning. Emotion-Qwen incorporates a
sophisticated Hybrid Compressor based on the Mixture of Experts (MoE) paradigm,
which dynamically routes inputs to balance emotion-specific and general-purpose
processing. The model is pre-trained in a three-stage pipeline on large-scale
general and emotional image datasets to support robust multimodal
representations. Furthermore, we construct the Video Emotion Reasoning (VER)
dataset, comprising more than 40K bilingual video clips with fine-grained
descriptive annotations, to further enrich Emotion-Qwen's emotional reasoning
capability. Experimental results demonstrate that Emotion-Qwen achieves
state-of-the-art performance on multiple emotion recognition benchmarks, while
maintaining competitive results on general VL tasks. Code and models are
available at https://anonymous.4open.science/r/Emotion-Qwen-Anonymous.

</details>


### [644] [EmoVLM-KD: Fusing Distilled Expertise with Vision-Language Models for Visual Emotion Analysis](https://arxiv.org/pdf/2505.07164)
*SangEun Lee, Yubeen Lee, Eunil Park*

Main category: cs.MM

TL;DR: EmoVLM-KD integrates instruction-tuned vision-language models and conventional vision models via knowledge distillation to enhance visual emotion analysis, achieving state-of-the-art performance efficiently.


<details>
  <summary>Details</summary>
Motivation: Vision-language models and conventional vision models show complementary strengths in visual emotion analysis, but deploying both is computationally expensive.

Method: Proposes EmoVLM-KD, which distills knowledge from a vision model into a vision-language model using a lightweight module and a gate for balanced predictions.

Result: EmoVLM-KD outperforms existing methods on benchmark datasets while maintaining computational efficiency.

Conclusion: The integration of complementary models via distillation improves visual emotion analysis performance without high computational costs.

Abstract: Visual emotion analysis, which has gained considerable attention in the field
of affective computing, aims to predict the dominant emotions conveyed by an
image. Despite advancements in visual emotion analysis with the emergence of
vision-language models, we observed that instruction-tuned vision-language
models and conventional vision models exhibit complementary strengths in visual
emotion analysis, as vision-language models excel in certain cases, whereas
vision models perform better in others. This finding highlights the need to
integrate these capabilities to enhance the performance of visual emotion
analysis. To bridge this gap, we propose EmoVLM-KD, an instruction-tuned
vision-language model augmented with a lightweight module distilled from
conventional vision models. Instead of deploying both models simultaneously,
which incurs high computational costs, we transfer the predictive patterns of a
conventional vision model into the vision-language model using a knowledge
distillation framework. Our approach first fine-tunes a vision-language model
on emotion-specific instruction data and then attaches a distilled module to
its visual encoder while keeping the vision-language model frozen. Predictions
from the vision language model and the distillation module are effectively
balanced by a gate module, which subsequently generates the final outcome.
Extensive experiments show that EmoVLM-KD achieves state-of-the-art performance
on multiple visual emotion analysis benchmark datasets, outperforming the
existing methods while maintaining computational efficiency. The code is
available in https://github.com/sange1104/EmoVLM-KD.

</details>


### [645] [Bridging Discrete and Continuous: A Multimodal Strategy for Complex Emotion Detection](https://arxiv.org/pdf/2409.07901)
*Jiehui Jia, Huan Zhang, Jinhua Liang*

Main category: cs.MM

TL;DR: The paper introduces a multimodal framework for emotion recognition using facial expressions, voice tones, and video transcripts, mapping emotions in a 3D VAD space. It transitions from discrete to continuous emotion labeling with K-means clustering and achieves strong accuracy on the MER2024 dataset.


<details>
  <summary>Details</summary>
Motivation: Accurate emotion recognition in human-computer interaction is challenging due to the complexity of emotional expressions. This study aims to improve emotion detection by integrating multiple modalities and representing emotions continuously.

Method: A multimodal approach combines facial expressions, voice tones, and video transcripts. Emotions are mapped in a 3D Valence-Arousal-Dominance (VAD) space. K-means clustering transitions from discrete to continuous labeling, and a classifier is built for emotion recognition.

Result: The model successfully transforms discrete emotions into a continuous system and achieves diverse, comprehensive emotion vocabulary with strong accuracy on the MER2024 dataset.

Conclusion: The proposed framework enhances emotion recognition by enabling continuous and diverse emotional representations, validated by high accuracy on culturally consistent data.

Abstract: In the domain of human-computer interaction, accurately recognizing and
interpreting human emotions is crucial yet challenging due to the complexity
and subtlety of emotional expressions. This study explores the potential for
detecting a rich and flexible range of emotions through a multimodal approach
which integrates facial expressions, voice tones, and transcript from video
clips. We propose a novel framework that maps variety of emotions in a
three-dimensional Valence-Arousal-Dominance (VAD) space, which could reflect
the fluctuations and positivity/negativity of emotions to enable a more variety
and comprehensive representation of emotional states. We employed K-means
clustering to transit emotions from traditional discrete categorization to a
continuous labeling system and built a classifier for emotion recognition upon
this system. The effectiveness of the proposed model is evaluated using the
MER2024 dataset, which contains culturally consistent video clips from Chinese
movies and TV series, annotated with both discrete and open-vocabulary emotion
labels. Our experiment successfully achieved the transformation between
discrete and continuous models, and the proposed model generated a more diverse
and comprehensive set of emotion vocabulary while maintaining strong accuracy.

</details>


### [646] [Mitigating Image Captioning Hallucinations in Vision-Language Models](https://arxiv.org/pdf/2505.03420)
*Fei Zhao, Chengcui Zhang, Runlin Zhang, Tianyang Wang, Xi Li*

Main category: cs.MM

TL;DR: A reinforcement learning-based test-time adaptation framework reduces hallucinations in VLMs by updating minimal parameters, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in VLMs due to distribution shifts hinder reliability; current solutions are resource-intensive.

Method: Proposes a test-time adaptation framework using reinforcement learning, updating only 0.003% of parameters in layer normalization.

Result: Achieves 15.4% and 17.3% hallucination reduction on LLaVA and InstructBLIP, with a 68.3% improvement over baselines.

Conclusion: The method effectively mitigates hallucinations without retraining or auxiliary models, enhancing VLM reliability.

Abstract: Hallucinations in vision-language models (VLMs) hinder reliability and
real-world applicability, usually stemming from distribution shifts between
pretraining data and test samples. Existing solutions, such as retraining or
fine-tuning on additional data, demand significant computational resources and
labor-intensive data collection, while ensemble-based methods incur additional
costs by introducing auxiliary VLMs. To address these challenges, we propose a
novel test-time adaptation framework using reinforcement learning to mitigate
hallucinations during inference without retraining or any auxiliary VLMs. By
updating only the learnable parameters in the layer normalization of the
language model (approximately 0.003% of the model parameters), our method
reduces distribution shifts between test samples and pretraining samples. A
CLIP-based hallucination evaluation model is proposed to provide dual rewards
to VLMs. Experimental results demonstrate a 15.4% and 17.3% reduction in
hallucination rates on LLaVA and InstructBLIP, respectively. Our approach
outperforms state-of-the-art baselines with a 68.3% improvement in
hallucination mitigation, demonstrating its effectiveness.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [647] [RADE: A Neural Codec for Transmitting Speech over HF Radio Channels](https://arxiv.org/pdf/2505.06671)
*David Rowe, Jean-Marc Valin*

Main category: eess.AS

TL;DR: An autoencoder replaces traditional speech compression and radio signal processing, achieving higher intelligibility over HF channels.


<details>
  <summary>Details</summary>
Motivation: To improve speech intelligibility in radio communications by replacing classical signal processing with neural networks.

Method: An autoencoder processes vocoder features into QAM symbols, transmitted via OFDM over HF radio, and decodes back to speech features.

Result: Outperforms analog and digital systems in intelligibility across various SNRs, with low PAPR.

Conclusion: Neural networks can effectively replace traditional methods in radio speech compression, enhancing performance.

Abstract: Speech compression is commonly used to send voice over radio channels in
applications such as mobile telephony and two-way push-to-talk (PTT) radio. In
classical systems, the speech codec is combined with forward error correction,
modulation and radio hardware. In this paper we describe an autoencoder that
replaces many of the traditional signal processing elements with a neural
network. The encoder takes a vocoder feature set (short term spectrum, pitch,
voicing), and produces discrete time, but continuously valued quadrature
amplitude modulation (QAM) symbols. We use orthogonal frequency domain
multiplexing (OFDM) to send and receive these symbols over high frequency (HF)
radio channels. The decoder converts received QAM symbols to vocoder features
suitable for synthesis. The autoencoder has been trained to be robust to
additive Gaussian noise and multipath channel impairments while simultaneously
maintaining a Peak To Average Power Ratio (PAPR) of less than 1~dB. Over
simulated and real world HF radio channels we have achieved output speech
intelligibility that clearly surpasses existing analog and digital radio
systems over a range of SNRs.

</details>


### [648] [TACOS: Temporally-aligned Audio CaptiOnS for Language-Audio Pretraining](https://arxiv.org/pdf/2505.07609)
*Paul Primus, Florian Schmid, Gerhard Widmer*

Main category: eess.AS

TL;DR: The paper proposes a method to improve audio-text alignment by using temporal supervision and a novel dataset, showing better performance than models trained on global captions.


<details>
  <summary>Details</summary>
Motivation: Existing contrastive language-audio models lack strong temporal supervision, limiting their effectiveness for frame-level tasks.

Method: A dataset of 12,000 annotated audio recordings is curated, cleaned using large language models, and a frame-wise contrastive training strategy is introduced.

Result: The model achieves better temporal text-audio alignment compared to global caption-trained models on the AudioSet Strong benchmark.

Conclusion: Temporal supervision enhances language-audio models, with the dataset and code made publicly available.

Abstract: Learning to associate audio with textual descriptions is valuable for a range
of tasks, including pretraining, zero-shot classification, audio retrieval,
audio captioning, and text-conditioned audio generation. Existing contrastive
language-audio pretrained models are typically trained using global, clip-level
descriptions, which provide only weak temporal supervision. We hypothesize that
CLAP-like language-audio models - particularly, if they are expected to produce
frame-level embeddings - can benefit from a stronger temporal supervision. To
confirm our hypothesis, we curate a novel dataset of approximately 12,000 audio
recordings from Freesound, each annotated with single-sentence free-text
descriptions linked to a specific temporal segment in an audio recording. We
use large language models to clean these annotations by removing references to
non-audible events, transcribed speech, typos, and annotator language bias. We
further propose a frame-wise contrastive training strategy that learns to align
text descriptions with temporal regions in an audio recording and demonstrate
that our model has better temporal text-audio alignment abilities compared to
models trained only on global captions when evaluated on the AudioSet Strong
benchmark. The dataset and our source code are available on Zenodo and GitHub,
respectively.

</details>


### [649] [Diffused Responsibility: Analyzing the Energy Consumption of Generative Text-to-Audio Diffusion Models](https://arxiv.org/pdf/2505.07615)
*Riccardo Passoni, Francesca Ronchini, Luca Comanducci, Romain Serizel, Fabio Antonacci*

Main category: eess.AS

TL;DR: Analysis of energy usage in 7 text-to-audio diffusion models, exploring trade-offs between audio quality and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: Address concerns about high computational demands and environmental impact of text-to-audio models.

Method: Evaluate energy consumption of 7 state-of-the-art models, analyzing parameter variations and Pareto-optimal solutions.

Result: Identified trade-offs between audio quality and energy efficiency, providing insights for efficient model development.

Conclusion: Highlights the need for balancing performance and environmental impact in generative audio models.

Abstract: Text-to-audio models have recently emerged as a powerful technology for
generating sound from textual descriptions. However, their high computational
demands raise concerns about energy consumption and environmental impact. In
this paper, we conduct an analysis of the energy usage of 7 state-of-the-art
text-to-audio diffusion-based generative models, evaluating to what extent
variations in generation parameters affect energy consumption at inference
time. We also aim to identify an optimal balance between audio quality and
energy consumption by considering Pareto-optimal solutions across all selected
models. Our findings provide insights into the trade-offs between performance
and environmental impact, contributing to the development of more efficient
generative audio models.

</details>


### [650] [Is MixIT Really Unsuitable for Correlated Sources? Exploring MixIT for Unsupervised Pre-training in Music Source Separation](https://arxiv.org/pdf/2505.07631)
*Kohei Saijo, Yoshiaki Bando*

Main category: eess.AS

TL;DR: MixIT-based pre-training improves music source separation (MSS) performance, even though MixIT was overlooked in MSS due to source independence assumptions.


<details>
  <summary>Details</summary>
Motivation: High cost of obtaining isolated music sources makes unsupervised pre-training appealing. MixIT, though not designed for MSS, shows potential for separating instruments.

Method: Pre-train a model on unlabeled Free Music Archive data using MixIT, then fine-tune on MUSDB18 with supervision using the band-split TF-Locoformer model.

Result: MixIT-based pre-training outperforms training from scratch, enhancing MSS performance.

Conclusion: MixIT is a viable approach for unsupervised pre-training in MSS, despite its challenges, offering performance gains.

Abstract: In music source separation (MSS), obtaining isolated sources or stems is
highly costly, making pre-training on unlabeled data a promising approach.
Although source-agnostic unsupervised learning like mixture-invariant training
(MixIT) has been explored in general sound separation, they have been largely
overlooked in MSS due to its implicit assumption of source independence. We
hypothesize, however, that the difficulty of applying MixIT to MSS arises from
the ill-posed nature of MSS itself, where stem definitions are
application-dependent and models lack explicit knowledge of what should or
should not be separated, rather than from high inter-source correlation. While
MixIT does not assume any source model and struggles with such ambiguities, our
preliminary experiments show that it can still separate instruments to some
extent, suggesting its potential for unsupervised pre-training. Motivated by
these insights, this study investigates MixIT-based pre-training for MSS. We
first pre-train a model on in-the-wild, unlabeled data from the Free Music
Archive using MixIT, and then fine-tune it on MUSDB18 with supervision. Using
the band-split TF-Locoformer, one of the state-of-the-art MSS models, we
demonstrate that MixIT-based pre-training improves the performance over
training from scratch.

</details>


### [651] [Adaptive Mixture of Low-Rank Experts for Robust Audio Spoofing Detection](https://arxiv.org/pdf/2503.12010)
*Qixian Chen, Yuxiong Xu, Sara Mandelli, Sheng Li, Bin Li*

Main category: eess.AS

TL;DR: AMULET framework improves audio spoofing detection by using attack-specific experts and adaptive fusion, enhancing robustness against real-world post-processing attacks.


<details>
  <summary>Details</summary>
Motivation: Existing models fail under real-world post-processing attacks due to reliance on clean datasets.

Method: AMULET employs Attack-Specific Experts (ASEs) with Low-Rank Adaptation (LoRA) and Adaptive Expert Fusion (AEF) to dynamically adapt to varied attacks.

Result: AMULET outperforms full fine-tuning and other strategies, showing better noise resilience and adaptability to unseen attacks.

Conclusion: AMULET is a robust and adaptable solution for audio spoofing detection in complex real-world scenarios.

Abstract: In audio spoofing detection, most studies rely on clean datasets, making
models susceptible to real-world post-processing attacks, such as channel
compression and noise. To overcome this challenge, we propose the Adaptive
MixtUre Low-rank ExperTs (AMULET) framework, which enhances resilience by
leveraging attack-specific knowledge and dynamically adapting to varied attack
conditions. Specifically, AMULET employs Attack-Specific Experts (ASEs)
fine-tuned with Low-Rank Adaptation (LoRA), allowing each expert to focus on
distinct post-processing patterns using just 1.13\% of the parameters required
for full fine-tuning. Furthermore, we introduce Adaptive Expert Fusion (AEF),
which adaptively selects and integrates expert knowledge to enhance the
robustness of spoofing detection. Experimental results demonstrate that AMULET
significantly enhances robustness by improving noise resilience and exhibiting
greater adaptability to unseen post-processing methods compared to models
trained with full fine-tuning. Additionally, our framework outperforms both
single expert and other expert aggregation strategies under various mixed
attacks, demonstrating its superior robustness and adaptability in managing
complex real-world scenarios.

</details>


### [652] [OmniAudio: Generating Spatial Audio from 360-Degree Video](https://arxiv.org/pdf/2504.14906)
*Huadai Liu, Tianyi Luo, Qikai Jiang, Kaicheng Luo, Peiwen Sun, Jialei Wan, Rongjie Huang, Qian Chen, Wen Wang, Xiangtai Li, Shiliang Zhang, Zhijie Yan, Zhou Zhao, Wei Xue*

Main category: eess.AS

TL;DR: The paper introduces 360V2SA, a task to generate spatial audio (FOA format) from 360-degree videos, addressing limitations of traditional methods. It presents Sphere360 dataset and OmniAudio framework, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Traditional video-to-audio methods lack spatial cues for 3D environments. This work aims to generate accurate spatial audio from 360-degree videos.

Method: The paper proposes OmniAudio, a dual-branch framework leveraging self-supervised pre-training with FOA and non-spatial data, and uses panoramic/FoV video inputs.

Result: OmniAudio achieves state-of-the-art performance on the Sphere360 dataset, validated by objective and subjective metrics.

Conclusion: The work advances spatial audio generation from 360-degree videos, with potential applications in immersive audio experiences.

Abstract: Traditional video-to-audio generation techniques primarily focus on
field-of-view (FoV) video and non-spatial audio, often missing the spatial cues
necessary for accurately representing sound sources in 3D environments. To
address this limitation, we introduce a novel task, 360V2SA, to generate
spatial audio from 360-degree videos, specifically producing First-order
Ambisonics (FOA) audio - a standard format for representing 3D spatial audio
that captures sound directionality and enables realistic 3D audio reproduction.
We first create Sphere360, a novel dataset tailored for this task that is
curated from real-world data. We also design an efficient semi-automated
pipeline for collecting and cleaning paired video-audio data. To generate
spatial audio from 360-degree video, we propose a novel framework OmniAudio,
which leverages self-supervised pre-training using both spatial audio data (in
FOA format) and large-scale non-spatial data. Furthermore, OmniAudio features a
dual-branch framework that utilizes both panoramic and FoV video inputs to
capture comprehensive local and global information from 360-degree videos.
Experimental results demonstrate that OmniAudio achieves state-of-the-art
performance across both objective and subjective metrics on Sphere360. Code and
datasets will be released at https://github.com/liuhuadai/OmniAudio. The demo
page is available at https://OmniAudio-360V2SA.github.io.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [653] [PC-SRGAN: Physically Consistent Super-Resolution Generative Adversarial Network for General Transient Simulations](https://arxiv.org/pdf/2505.06502)
*Md Rakibul Hasan, Pouria Behnoudfar, Dan MacKinlay, Thomas Poulet*

Main category: eess.IV

TL;DR: PC-SRGAN improves Super Resolution with physical consistency, outperforming traditional methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the lack of physical meaningfulness in GAN-generated images for scientific applications.

Method: PC-SRGAN ensures physical consistency while enhancing resolution, using numerically justified time integrators and advanced metrics.

Result: Significant improvements in PSNR and SSIM, even with limited training data (13% of SRGAN's requirement).

Conclusion: PC-SRGAN advances scientific machine learning with reliable, causal models, offering broader applications and better process understanding.

Abstract: Machine Learning, particularly Generative Adversarial Networks (GANs), has
revolutionised Super Resolution (SR). However, generated images often lack
physical meaningfulness, which is essential for scientific applications. Our
approach, PC-SRGAN, enhances image resolution while ensuring physical
consistency for interpretable simulations. PC-SRGAN significantly improves both
the Peak Signal-to-Noise Ratio and the Structural Similarity Index Measure
compared to conventional methods, even with limited training data (e.g., only
13% of training data required for SRGAN). Beyond SR, PC-SRGAN augments
physically meaningful machine learning, incorporating numerically justified
time integrators and advanced quality metrics. These advancements promise
reliable and causal machine-learning models in scientific domains. A
significant advantage of PC-SRGAN over conventional SR techniques is its
physical consistency, which makes it a viable surrogate model for
time-dependent problems. PC-SRGAN advances scientific machine learning,
offering improved accuracy and efficiency for image processing, enhanced
process understanding, and broader applications to scientific research. The
source codes and data will be made publicly available at
https://github.com/hasan-rakibul/PC-SRGAN upon acceptance of this paper.

</details>


### [654] [LMLCC-Net: A Semi-Supervised Deep Learning Model for Lung Nodule Malignancy Prediction from CT Scans using a Novel Hounsfield Unit-Based Intensity Filtering](https://arxiv.org/pdf/2505.06370)
*Adhora Madhuri, Nusaiba Sobir, Tasnia Binte Mamun, Taufiq Hasan*

Main category: eess.IV

TL;DR: LMLCC-Net, a 3D CNN framework, improves lung nodule classification in CT scans by leveraging HU-based intensity filtering and texture analysis, achieving high accuracy (91.96%) and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Early diagnosis of lung cancer via CT scans can reduce mortality. Existing methods overlook HU intensity differences between benign and malignant nodules.

Method: LMLCC-Net uses multi-branch 3D CNNs with learnable HU-based filters and semi-supervised learning for ambiguous cases. Evaluated on LUNA16 dataset.

Result: Achieves 91.96% ACC, 92.04% SEN, and 91.87% AUC, outperforming prior methods.

Conclusion: LMLCC-Net aids radiologists in nodule classification, enhancing patient care.

Abstract: Lung cancer is the leading cause of patient mortality in the world. Early
diagnosis of malignant pulmonary nodules in CT images can have a significant
impact on reducing disease mortality and morbidity. In this work, we propose
LMLCC-Net, a novel deep learning framework for classifying nodules from CT scan
images using a 3D CNN, considering Hounsfield Unit (HU)-based intensity
filtering. Benign and malignant nodules have significant differences in their
intensity profile of HU, which was not exploited in the literature. Our method
considers the intensity pattern as well as the texture for the prediction of
malignancies. LMLCC-Net extracts features from multiple branches that each use
a separate learnable HU-based intensity filtering stage. Various combinations
of branches and learnable ranges of filters were explored to finally produce
the best-performing model. In addition, we propose a semi-supervised learning
scheme for labeling ambiguous cases and also developed a lightweight model to
classify the nodules. The experimental evaluations are carried out on the
LUNA16 dataset. Our proposed method achieves a classification accuracy (ACC) of
91.96%, a sensitivity (SEN) of 92.04%, and an area under the curve (AUC) of
91.87%, showing improved performance compared to existing methods. The proposed
method can have a significant impact in helping radiologists in the
classification of pulmonary nodules and improving patient care.

</details>


### [655] [Reproducing and Improving CheXNet: Deep Learning for Chest X-ray Disease Classification](https://arxiv.org/pdf/2505.06646)
*Daniel Strick, Carlos Garcia, Anthony Huang*

Main category: eess.IV

TL;DR: Reproduced CheXNet and explored better algorithms for classifying 14 diseases in X-ray images, achieving an average AUC-ROC of 0.85 and F1 score of 0.39.


<details>
  <summary>Details</summary>
Motivation: Advance deep learning for radiologic image analysis to improve disease classification in medical imaging.

Method: Reproduced CheXNet and tested other algorithms on the NIH ChestX-ray14 dataset, evaluating performance with F1 score and AUC-ROC.

Result: Best model achieved an average AUC-ROC of 0.85 and F1 score of 0.39 across 14 diseases.

Conclusion: Deep learning shows promise for standardizing disease classification in radiologic images, with potential for further improvement.

Abstract: Deep learning for radiologic image analysis is a rapidly growing field in
biomedical research and is likely to become a standard practice in modern
medicine. On the publicly available NIH ChestX-ray14 dataset, containing X-ray
images that are classified by the presence or absence of 14 different diseases,
we reproduced an algorithm known as CheXNet, as well as explored other
algorithms that outperform CheXNet's baseline metrics. Model performance was
primarily evaluated using the F1 score and AUC-ROC, both of which are critical
metrics for imbalanced, multi-label classification tasks in medical imaging.
The best model achieved an average AUC-ROC score of 0.85 and an average F1
score of 0.39 across all 14 disease classifications present in the dataset.

</details>


### [656] [HistDiST: Histopathological Diffusion-based Stain Transfer](https://arxiv.org/pdf/2505.06793)
*Erik Großkopf, Valay Bundele, Mehran Hossienzadeh, Hendrik P. A. Lensch*

Main category: eess.IV

TL;DR: HistDiST, a Latent Diffusion Model, improves H&E-to-IHC translation with dual-conditioning and novel metrics, outperforming GAN-based methods by 28% in molecular relevance.


<details>
  <summary>Details</summary>
Motivation: H&E staining lacks molecular specificity, and IHC is costly. Existing GAN-based methods face instability and fidelity issues, while diffusion models are underexplored.

Method: HistDiST uses a Latent Diffusion Model with dual-conditioning (morphological embeddings and VAE-encoded H&E), rescaled noise schedule, and DDIM inversion for structural consistency.

Result: HistDiST achieves a 28% improvement in Molecular Retrieval Accuracy (MRA) on H&E-to-Ki67 translation, outperforming existing methods.

Conclusion: HistDiST effectively bridges H&E and IHC with high fidelity, offering a cost-effective alternative to IHC.

Abstract: Hematoxylin and Eosin (H&E) staining is the cornerstone of histopathology but
lacks molecular specificity. While Immunohistochemistry (IHC) provides
molecular insights, it is costly and complex, motivating H&E-to-IHC translation
as a cost-effective alternative. Existing translation methods are mainly
GAN-based, often struggling with training instability and limited structural
fidelity, while diffusion-based approaches remain underexplored. We propose
HistDiST, a Latent Diffusion Model (LDM) based framework for high-fidelity
H&E-to-IHC translation. HistDiST introduces a dual-conditioning strategy,
utilizing Phikon-extracted morphological embeddings alongside VAE-encoded H&E
representations to ensure pathology-relevant context and structural
consistency. To overcome brightness biases, we incorporate a rescaled noise
schedule, v-prediction, and trailing timesteps, enforcing a zero-SNR condition
at the final timestep. During inference, DDIM inversion preserves the
morphological structure, while an eta-cosine noise schedule introduces
controlled stochasticity, balancing structural consistency and molecular
fidelity. Moreover, we propose Molecular Retrieval Accuracy (MRA), a novel
pathology-aware metric leveraging GigaPath embeddings to assess molecular
relevance. Extensive evaluations on MIST and BCI datasets demonstrate that
HistDiST significantly outperforms existing methods, achieving a 28%
improvement in MRA on the H&E-to-Ki67 translation task, highlighting its
effectiveness in capturing true IHC semantics.

</details>


### [657] [Missing Data Estimation for MR Spectroscopic Imaging via Mask-Free Deep Learning Methods](https://arxiv.org/pdf/2505.06811)
*Tan-Hanh Pham, Ovidiu C. Andronesi, Xianqi Li, Kim-Doang Nguyen*

Main category: eess.IV

TL;DR: A deep learning-based, mask-free framework for estimating missing data in MRSI metabolic maps outperforms traditional methods, achieving high accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: MRSI's utility is limited by missing or corrupted data due to motion artifacts, field inhomogeneities, or spectral fitting failures, especially in high-resolution 3D acquisitions.

Method: The proposed framework uses 2D and 3D U-Net architectures to implicitly detect and estimate missing data, along with a progressive training strategy for robustness.

Result: The 2D model achieves an MSE of 0.002 and SSIM of 0.97 with 20% missing voxels; the 3D model reaches an MSE of 0.001 and SSIM of 0.98 with 15% missing voxels.

Conclusion: The mask-free deep learning approach is effective and broadly applicable for MRSI restoration, with strong potential for clinical and research use.

Abstract: Magnetic Resonance Spectroscopic Imaging (MRSI) is a powerful tool for
non-invasive mapping of brain metabolites, providing critical insights into
neurological conditions. However, its utility is often limited by missing or
corrupted data due to motion artifacts, magnetic field inhomogeneities, or
failed spectral fitting-especially in high resolution 3D acquisitions. To
address this, we propose the first deep learning-based, mask-free framework for
estimating missing data in MRSI metabolic maps. Unlike conventional restoration
methods that rely on explicit masks to identify missing regions, our approach
implicitly detects and estimates these areas using contextual spatial features
through 2D and 3D U-Net architectures. We also introduce a progressive training
strategy to enhance robustness under varying levels of data degradation. Our
method is evaluated on both simulated and real patient datasets and
consistently outperforms traditional interpolation techniques such as cubic and
linear interpolation. The 2D model achieves an MSE of 0.002 and an SSIM of 0.97
with 20% missing voxels, while the 3D model reaches an MSE of 0.001 and an SSIM
of 0.98 with 15% missing voxels. Qualitative results show improved fidelity in
estimating missing data, particularly in metabolically heterogeneous regions
and ventricular regions. Importantly, our model generalizes well to real-world
datasets without requiring retraining or mask input. These findings demonstrate
the effectiveness and broad applicability of mask-free deep learning for MRSI
restoration, with strong potential for clinical and research integration.

</details>


### [658] [Uni-AIMS: AI-Powered Microscopy Image Analysis](https://arxiv.org/pdf/2505.06918)
*Yanhui Hong, Nan Wang, Zhiyi Xia, Haoyi Tao, Xi Fang, Yiming Li, Jiankun Wang, Peng Jin, Xiaochen Cai, Shengyu Li, Ziqi Chen, Zezhong Zhang, Guolin Ke, Linfeng Zhang*

Main category: eess.IV

TL;DR: A systematic solution for intelligent microscopy image recognition and analysis, featuring a data engine for high-quality datasets, a robust segmentation model, and an intelligent analysis platform.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of microscopy image analysis, including diverse datasets, cluttered environments, and precise scale bar recognition.

Method: Combines a data engine (real and synthetic data with human-in-the-loop annotation) and a segmentation model for robust object detection.

Result: Effective detection of small/large objects and scale bars, validated in real-world applications.

Conclusion: Advances automated microscopy analysis with scalability and generalizability for interdisciplinary research.

Abstract: This paper presents a systematic solution for the intelligent recognition and
automatic analysis of microscopy images. We developed a data engine that
generates high-quality annotated datasets through a combination of the
collection of diverse microscopy images from experiments, synthetic data
generation and a human-in-the-loop annotation process. To address the unique
challenges of microscopy images, we propose a segmentation model capable of
robustly detecting both small and large objects. The model effectively
identifies and separates thousands of closely situated targets, even in
cluttered visual environments. Furthermore, our solution supports the precise
automatic recognition of image scale bars, an essential feature in quantitative
microscopic analysis. Building upon these components, we have constructed a
comprehensive intelligent analysis platform and validated its effectiveness and
practicality in real-world applications. This study not only advances automatic
recognition in microscopy imaging but also ensures scalability and
generalizability across multiple application domains, offering a powerful tool
for automated microscopic analysis in interdisciplinary research.

</details>


### [659] [Whitened CLIP as a Likelihood Surrogate of Images and Captions](https://arxiv.org/pdf/2505.06934)
*Roy Betser, Meir Yossef Levi, Guy Gilboa*

Main category: eess.IV

TL;DR: Whitened CLIP transforms CLIP embeddings to have zero mean, unit variance, and no correlation, enabling fast log-likelihood estimation via Euclidean norm.


<details>
  <summary>Details</summary>
Motivation: Likelihood approximations for images are challenging but useful; CLIP's potential for this is explored.

Method: Introduces Whitened CLIP, an invertible linear transformation of CLIP embeddings to standard normal distribution.

Result: Whitened embeddings approximate standard normal, allowing simple log-likelihood estimation.

Conclusion: Whitened CLIP provides a fast, training-free method for likelihood estimation in images and captions.

Abstract: Likelihood approximations for images are not trivial to compute and can be
useful in many applications. We examine the use of Contrastive Language-Image
Pre-training (CLIP) to assess the likelihood of images and captions. We
introduce \textit{Whitened CLIP}, a novel transformation of the CLIP latent
space via an invertible linear operation. This transformation ensures that each
feature in the embedding space has zero mean, unit standard deviation, and no
correlation with all other features, resulting in an identity covariance
matrix. We show that the whitened embeddings statistics can be well
approximated as a standard normal distribution, thus, the log-likelihood is
estimated simply by the square Euclidean norm in the whitened embedding space.
The whitening procedure is completely training-free and performed using a
pre-computed whitening matrix, hence, is very fast. We present several
preliminary experiments demonstrating the properties and applicability of these
likelihood scores to images and captions.

</details>


### [660] [Skull stripping with purely synthetic data](https://arxiv.org/pdf/2505.07159)
*Jong Sung Park, Juhyung Ha, Siddhesh Thakur, Alexandra Badea, Spyridon Bakas, Eleftherios Garyfallidis*

Main category: eess.IV

TL;DR: PUMBA trains a brain extraction model without real images or labels, achieving comparable accuracy in multi-modal, multi-species, and pathological cases.


<details>
  <summary>Details</summary>
Motivation: Address the lack of a generalizable skull stripping approach for multi-modal and multi-species cases.

Method: PUMBA uses purely synthetic data to train the model, avoiding real brain images or labels.

Result: The model performs comparably in multi-modal, multi-species, and pathological scenarios.

Conclusion: PUMBA offers a novel direction for generalizable medical image segmentation tasks.

Abstract: While many skull stripping algorithms have been developed for multi-modal and
multi-species cases, there is still a lack of a fundamentally generalizable
approach. We present PUMBA(PUrely synthetic Multimodal/species invariant Brain
extrAction), a strategy to train a model for brain extraction with no real
brain images or labels. Our results show that even without any real images or
anatomical priors, the model achieves comparable accuracy in multi-modal,
multi-species and pathological cases. This work presents a new direction of
research for any generalizable medical image segmentation task.

</details>


### [661] [Metrics that matter: Evaluating image quality metrics for medical image generation](https://arxiv.org/pdf/2505.07175)
*Yash Deo, Yan Jia, Toni Lassila, William A. P. Smith, Tom Lawton, Siyuan Kang, Alejandro F. Frangi, Ibrahim Habli*

Main category: eess.IV

TL;DR: The study evaluates no-reference image quality metrics for synthetic medical imaging, revealing their limitations in clinical validity and downstream task suitability.


<details>
  <summary>Details</summary>
Motivation: Assessing generative models for medical imaging is critical but challenging due to high standards for fidelity and safety. Current no-reference metrics lack reliability in this domain.

Method: The study systematically evaluates no-reference metrics using brain MRI data, testing sensitivity to noise, distribution shifts, and morphological alterations. It compares metric scores with downstream segmentation task performance.

Result: Findings show poor correlation between no-reference metrics and downstream task suitability, with insensitivity to clinically crucial anatomical details and misleading scores for distribution shifts.

Conclusion: A multifaceted validation framework, combining downstream task performance and cautious metric interpretation, is needed to ensure generative models are clinically fit.

Abstract: Evaluating generative models for synthetic medical imaging is crucial yet
challenging, especially given the high standards of fidelity, anatomical
accuracy, and safety required for clinical applications. Standard evaluation of
generated images often relies on no-reference image quality metrics when ground
truth images are unavailable, but their reliability in this complex domain is
not well established. This study comprehensively assesses commonly used
no-reference image quality metrics using brain MRI data, including tumour and
vascular images, providing a representative exemplar for the field. We
systematically evaluate metric sensitivity to a range of challenges, including
noise, distribution shifts, and, critically, localised morphological
alterations designed to mimic clinically relevant inaccuracies. We then compare
these metric scores against model performance on a relevant downstream
segmentation task, analysing results across both controlled image perturbations
and outputs from different generative model architectures. Our findings reveal
significant limitations: many widely-used no-reference image quality metrics
correlate poorly with downstream task suitability and exhibit a profound
insensitivity to localised anatomical details crucial for clinical validity.
Furthermore, these metrics can yield misleading scores regarding distribution
shifts, e.g. data memorisation. This reveals the risk of misjudging model
readiness, potentially leading to the deployment of flawed tools that could
compromise patient safety. We conclude that ensuring generative models are
truly fit for clinical purpose requires a multifaceted validation framework,
integrating performance on relevant downstream tasks with the cautious
interpretation of carefully selected no-reference image quality metrics.

</details>


### [662] [Multi-Plane Vision Transformer for Hemorrhage Classification Using Axial and Sagittal MRI Data](https://arxiv.org/pdf/2505.07349)
*Badhan Kumar Das, Gengyan Zhao, Boris Mailhe, Thomas J. Re, Dorin Comaniciu, Eli Gibson, Andreas Maier*

Main category: eess.IV

TL;DR: A 3D multi-plane vision transformer (MP-ViT) is proposed for brain hemorrhage classification in MRI with varying orientations, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: The diverse nature of MRI acquisitions with varying contrasts and orientations complicates hemorrhage identification, and resampling images to fixed planes can cause information loss.

Method: MP-ViT uses two transformer encoders for axial and sagittal contrasts, integrating information via cross-attention and a modality indication vector for missing contrast data.

Result: MP-ViT achieved a 5.5% improvement in AUC over ViT and 1.8% over CNN-based models on a clinical dataset of 10,084 training subjects.

Conclusion: MP-ViT shows promise for improving hemorrhage detection in MRI with varying orientations.

Abstract: Identifying brain hemorrhages from magnetic resonance imaging (MRI) is a
critical task for healthcare professionals. The diverse nature of MRI
acquisitions with varying contrasts and orientation introduce complexity in
identifying hemorrhage using neural networks. For acquisitions with varying
orientations, traditional methods often involve resampling images to a fixed
plane, which can lead to information loss. To address this, we propose a 3D
multi-plane vision transformer (MP-ViT) for hemorrhage classification with
varying orientation data. It employs two separate transformer encoders for
axial and sagittal contrasts, using cross-attention to integrate information
across orientations. MP-ViT also includes a modality indication vector to
provide missing contrast information to the model. The effectiveness of the
proposed model is demonstrated with extensive experiments on real world
clinical dataset consists of 10,084 training, 1,289 validation and 1,496 test
subjects. MP-ViT achieved substantial improvement in area under the curve
(AUC), outperforming the vision transformer (ViT) by 5.5% and CNN-based
architectures by 1.8%. These results highlight the potential of MP-ViT in
improving performance for hemorrhage detection when different orientation
contrasts are needed.

</details>


### [663] [GAN-based synthetic FDG PET images from T1 brain MRI can serve to improve performance of deep unsupervised anomaly detection models](https://arxiv.org/pdf/2505.07364)
*Daria Zotova, Nicolas Pinon, Robin Trombetta, Romain Bouet, Julien Jung, Carole Lartizien*

Main category: eess.IV

TL;DR: GAN-based models effectively generate synthetic FDG PET images from T1 MRI, aiding unsupervised anomaly detection in epilepsy cases.


<details>
  <summary>Details</summary>
Motivation: Address the scarcity of large curated multimodal datasets and evaluate synthetic data's impact on deep model training.

Method: Compare GAN frameworks for PET image generation, assess visual quality, and train a UAD model using synthetic data.

Result: Best GAN models achieve SSIM 0.9 and PSNR 23.8; UAD model trained on synthetic data reaches 74% sensitivity.

Conclusion: GANs outperform transformers/diffusion models for MR-to-PET translation, proving synthetic data's diagnostic value.

Abstract: Background and Objective. Research in the cross-modal medical image
translation domain has been very productive over the past few years in tackling
the scarce availability of large curated multimodality datasets with the
promising performance of GAN-based architectures. However, only a few of these
studies assessed task-based related performance of these synthetic data,
especially for the training of deep models. Method. We design and compare
different GAN-based frameworks for generating synthetic brain
[18F]fluorodeoxyglucose (FDG) PET images from T1 weighted MRI data. We first
perform standard qualitative and quantitative visual quality evaluation. Then,
we explore further impact of using these fake PET data in the training of a
deep unsupervised anomaly detection (UAD) model designed to detect subtle
epilepsy lesions in T1 MRI and FDG PET images. We introduce novel diagnostic
task-oriented quality metrics of the synthetic FDG PET data tailored to our
unsupervised detection task, then use these fake data to train a use case UAD
model combining a deep representation learning based on siamese autoencoders
with a OC-SVM density support estimation model. This model is trained on normal
subjects only and allows the detection of any variation from the pattern of the
normal population. We compare the detection performance of models trained on 35
paired real MR T1 of normal subjects paired either on 35 true PET images or on
35 synthetic PET images generated from the best performing generative models.
Performance analysis is conducted on 17 exams of epilepsy patients undergoing
surgery. Results. The best performing GAN-based models allow generating
realistic fake PET images of control subject with SSIM and PSNR values around
0.9 and 23.8, respectively and in distribution (ID) with regard to the true
control dataset. The best UAD model trained on these synthetic normative PET
data allows reaching 74% sensitivity. Conclusion. Our results confirm that
GAN-based models are the best suited for MR T1 to FDG PET translation,
outperforming transformer or diffusion models. We also demonstrate the
diagnostic value of these synthetic data for the training of UAD models and
evaluation on clinical exams of epilepsy patients. Our code and the normative
image dataset are available.

</details>


### [664] [Towards a physically realistic computationally efficient DVS pixel model](https://arxiv.org/pdf/2505.07386)
*Rui Graca, Tobi Delbruck*

Main category: eess.IV

TL;DR: A new DVS event camera model aims for high realism and computational efficiency by combining circuit-derived equations with stochastic event generation.


<details>
  <summary>Details</summary>
Motivation: Existing DVS models lack realism for HDR scenes and computational efficiency for large-scale simulations.

Method: Uses large-signal differential equations from circuit analysis and a stochastic event generation mechanism based on first-passage-time theory.

Result: Achieves accurate noise generation with timesteps 1000x longer than previous methods.

Conclusion: The proposed model advances DVS simulation by balancing physical realism and computational efficiency.

Abstract: Dynamic Vision Sensor (DVS) event camera models are important tools for
predicting camera response, optimizing biases, and generating realistic
simulated datasets. Existing DVS models have been useful, but have not
demonstrated high realism for challenging HDR scenes combined with adequate
computational efficiency for array-level scene simulation. This paper reports
progress towards a physically realistic and computationally efficient DVS model
based on large-signal differential equations derived from circuit analysis,
with parameters fitted from pixel measurements and circuit simulation. These
are combined with an efficient stochastic event generation mechanism based on
first-passage-time theory, allowing accurate noise generation with timesteps
greater than 1000x longer than previous methods

</details>


### [665] [Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video Generation Model](https://arxiv.org/pdf/2505.07449)
*Wei Li, Ming Hu, Guoan Wang, Lihao Liu, Kaijin Zhou, Junzhi Ning, Xin Guo, Zongyuan Ge, Lixu Gu, Junjun He*

Main category: eess.IV

TL;DR: Ophora is an AI model that generates ophthalmic surgical videos from natural language instructions, addressing data scarcity and privacy issues.


<details>
  <summary>Details</summary>
Motivation: The difficulty in collecting annotated ophthalmic surgical videos due to privacy and labor constraints motivates the use of text-guided video generation (T2V).

Method: Ophora uses a Comprehensive Data Curation pipeline to create a dataset (Ophora-160K) and a Progressive Video-Instruction Tuning scheme to adapt a T2V model for surgical video generation.

Result: Ophora produces realistic and reliable surgical videos, validated by quantitative analysis and ophthalmologist feedback, and aids in surgical workflow understanding.

Conclusion: Ophora effectively addresses data scarcity in ophthalmic surgery through AI-generated videos, demonstrating practical utility for downstream tasks.

Abstract: In ophthalmic surgery, developing an AI system capable of interpreting
surgical videos and predicting subsequent operations requires numerous
ophthalmic surgical videos with high-quality annotations, which are difficult
to collect due to privacy concerns and labor consumption. Text-guided video
generation (T2V) emerges as a promising solution to overcome this issue by
generating ophthalmic surgical videos based on surgeon instructions. In this
paper, we present Ophora, a pioneering model that can generate ophthalmic
surgical videos following natural language instructions. To construct Ophora,
we first propose a Comprehensive Data Curation pipeline to convert narrative
ophthalmic surgical videos into a large-scale, high-quality dataset comprising
over 160K video-instruction pairs, Ophora-160K. Then, we propose a Progressive
Video-Instruction Tuning scheme to transfer rich spatial-temporal knowledge
from a T2V model pre-trained on natural video-text datasets for
privacy-preserved ophthalmic surgical video generation based on Ophora-160K.
Experiments on video quality evaluation via quantitative analysis and
ophthalmologist feedback demonstrate that Ophora can generate realistic and
reliable ophthalmic surgical videos based on surgeon instructions. We also
validate the capability of Ophora for empowering downstream tasks of ophthalmic
surgical workflow understanding. Code is available at
https://github.com/mar-cry/Ophora.

</details>


### [666] [Breast Cancer Classification in Deep Ultraviolet Fluorescence Images Using a Patch-Level Vision Transformer Framework](https://arxiv.org/pdf/2505.07654)
*Pouya Afshin, David Helminiak, Tongtong Lu, Tina Yen, Julie M. Jorns, Mollie Patton, Bing Yu, Dong Hye Ye*

Main category: eess.IV

TL;DR: A patch-level vision transformer (ViT) model with Grad-CAM++ improves breast cancer classification in DUV-FSM images, achieving 98.33% accuracy.


<details>
  <summary>Details</summary>
Motivation: To enhance intraoperative margin assessment in breast-conserving surgery by improving the classification of malignant and normal tissues in DUV-FSM images.

Method: A patch-level ViT model with Grad-CAM++ saliency weighting is used to classify DUV whole surface images (WSIs), capturing local and global features.

Result: The proposed framework achieves 98.33% classification accuracy, outperforming conventional deep learning methods.

Conclusion: The ViT-based approach with Grad-CAM++ provides accurate and interpretable classification for breast cancer detection in DUV-FSM images.

Abstract: Breast-conserving surgery (BCS) aims to completely remove malignant lesions
while maximizing healthy tissue preservation. Intraoperative margin assessment
is essential to achieve a balance between thorough cancer resection and tissue
conservation. A deep ultraviolet fluorescence scanning microscope (DUV-FSM)
enables rapid acquisition of whole surface images (WSIs) for excised tissue,
providing contrast between malignant and normal tissues. However, breast cancer
classification with DUV WSIs is challenged by high resolutions and complex
histopathological features. This study introduces a DUV WSI classification
framework using a patch-level vision transformer (ViT) model, capturing local
and global features. Grad-CAM++ saliency weighting highlights relevant spatial
regions, enhances result interpretability, and improves diagnostic accuracy for
benign and malignant tissue classification. A comprehensive 5-fold
cross-validation demonstrates the proposed approach significantly outperforms
conventional deep learning methods, achieving a classification accuracy of
98.33%.

</details>


### [667] [Hierarchical Sparse Attention Framework for Computationally Efficient Classification of Biological Cells](https://arxiv.org/pdf/2505.07661)
*Elad Yoshai, Dana Yagoda-Aharoni, Eden Dotan, Natan T. Shaked*

Main category: eess.IV

TL;DR: SparseAttnNet is a hierarchical attention framework for efficient image classification by processing only the most informative pixels, reducing computational costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional CNNs process entire images inefficiently, focusing on irrelevant features. SparseAttnNet aims to improve efficiency and relevance by dynamically selecting salient pixels.

Method: Uses coarse and fine multi-head attention to select top-k pixels, processes them like words in a language model, and incorporates global context.

Result: Achieves competitive accuracy with 15% pixel processing, reducing computational demands compared to CNNs and Vision Transformers.

Conclusion: SparseAttnNet is efficient, lightweight, and explainable, ideal for resource-constrained settings like imaging flow cytometry.

Abstract: We present SparseAttnNet, a new hierarchical attention-driven framework for
efficient image classification that adaptively selects and processes only the
most informative pixels from images. Traditional convolutional neural networks
typically process the entire images regardless of information density, leading
to computational inefficiency and potential focus on irrelevant features. Our
approach leverages a dynamic selection mechanism that uses coarse attention
distilled by fine multi-head attention from the downstream layers of the model,
allowing the model to identify and extract the most salient k pixels, where k
is adaptively learned during training based on loss convergence trends. Once
the top-k pixels are selected, the model processes only these pixels, embedding
them as words in a language model to capture their semantics, followed by
multi-head attention to incorporate global context. For biological cell images,
we demonstrate that SparseAttnNet can process approximately 15% of the pixels
instead of the full image. Applied to cell classification tasks using white
blood cells images from the following modalities: optical path difference (OPD)
images from digital holography for stain-free cells, images from
motion-sensitive (event) camera from stain-free cells, and brightfield
microscopy images of stained cells, For all three imaging modalities,
SparseAttnNet achieves competitive accuracy while drastically reducing
computational requirements in terms of both parameters and floating-point
operations per second, compared to traditional CNNs and Vision Transformers.
Since the model focuses on biologically relevant regions, it also offers
improved explainability. The adaptive and lightweight nature of SparseAttnNet
makes it ideal for deployment in resource-constrained and high-throughput
settings, including imaging flow cytometry.

</details>


### [668] [ABS-Mamba: SAM2-Driven Bidirectional Spiral Mamba Network for Medical Image Translation](https://arxiv.org/pdf/2505.07687)
*Feng Yuan, Yifan Gao, Wenbin Wu, Keqing Wu, Xiaotong Guo, Jie Jiang, Xin Gao*

Main category: eess.IV

TL;DR: ABS-Mamba is a novel architecture for multi-modal medical image translation, combining SAM2 for organ-aware semantics, CNNs for local details, and Mamba for feature dependencies, achieving high-fidelity results.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges of global anatomical semantics and local structural fidelity in multi-modal medical image translation, complicated by intermodality information loss and distortion.

Method: Dual-resolution framework with SAM2 for organ-scale semantics, CNNs for local features, RFFN for fusion, BMRN for spatial dependencies, and a skip fusion decoder. Fine-tuned with LoRA+.

Result: Outperforms state-of-the-art methods on SynthRAD2023 and BraTS2019 datasets, preserving anatomical semantics and structural details.

Conclusion: ABS-Mamba enhances diagnostic accuracy in clinical applications, offering high-fidelity cross-modal synthesis.

Abstract: Accurate multi-modal medical image translation requires ha-rmonizing global
anatomical semantics and local structural fidelity, a challenge complicated by
intermodality information loss and structural distortion. We propose ABS-Mamba,
a novel architecture integrating the Segment Anything Model 2 (SAM2) for
organ-aware semantic representation, specialized convolutional neural networks
(CNNs) for preserving modality-specific edge and texture details, and Mamba's
selective state-space modeling for efficient long- and short-range feature
dependencies. Structurally, our dual-resolution framework leverages SAM2's
image encoder to capture organ-scale semantics from high-resolution inputs,
while a parallel CNNs branch extracts fine-grained local features. The Robust
Feature Fusion Network (RFFN) integrates these epresentations, and the
Bidirectional Mamba Residual Network (BMRN) models spatial dependencies using
spiral scanning and bidirectional state-space dynamics. A three-stage skip
fusion decoder enhances edge and texture fidelity. We employ Efficient Low-Rank
Adaptation (LoRA+) fine-tuning to enable precise domain specialization while
maintaining the foundational capabilities of the pre-trained components.
Extensive experimental validation on the SynthRAD2023 and BraTS2019 datasets
demonstrates that ABS-Mamba outperforms state-of-the-art methods, delivering
high-fidelity cross-modal synthesis that preserves anatomical semantics and
structural details to enhance diagnostic accuracy in clinical applications. The
code is available at https://github.com/gatina-yone/ABS-Mamba

</details>


### [669] [Efficient Bilinear Attention-based Fusion for Medical Visual Question Answering](https://arxiv.org/pdf/2410.21000)
*Zhilin Zhang, Jie Wang, Zhanghao Qin, Ruiqi Zhu, Xiaoliang Gong*

Main category: eess.IV

TL;DR: OMniBAN, a fusion model for MedVQA, combines Orthogonality loss, Multi-head attention, and Bilinear Attention Network for efficiency and performance, outperforming larger models with fewer resources.


<details>
  <summary>Details</summary>
Motivation: To address the limited research on efficient fusion mechanisms in MedVQA, aiming to support clinical decision-making with reduced computational demands.

Method: Introduces OMniBAN, integrating Orthogonality loss, Multi-head attention, and Bilinear Attention Network for efficient fusion.

Result: OMniBAN achieves comparable performance to larger models with fewer parameters (2/3) and lower FLOPs (1/4), with slight improvements on closed-ended questions.

Conclusion: OMniBAN offers a viable, efficient solution for real-world MedVQA applications where computational resources are constrained.

Abstract: Medical Visual Question Answering (MedVQA) has attracted growing interest at
the intersection of medical image understanding and natural language processing
for clinical applications. By interpreting medical images and providing precise
answers to relevant clinical inquiries, MedVQA has the potential to support
diagnostic decision-making and reduce workload across various fields like
radiology. While recent approaches rely heavily on unified large pre-trained
Visual-Language Models, research on more efficient fusion mechanisms remains
relatively limited in this domain. In this paper, we introduce a fusion model,
OMniBAN, that integrates Orthogonality loss, Multi-head attention, and a
Bilinear Attention Network to achieve high computational efficiency as well as
solid performance. We conduct comprehensive experiments and demonstrate how
bilinear attention fusion can approximate the performance of larger fusion
models like cross-modal Transformer. Our results show that OMniBAN requires
fewer parameters (approximately 2/3 of Transformer-based Co-Attention) and
substantially lower FLOPs (approximately 1/4), while achieving comparable
overall performance and even slight improvements on closed-ended questions on
two key MedVQA benchmarks. This balance between efficiency and accuracy
suggests that OMniBAN could be a viable option for real-world medical image
question answering, where computational resources are often constrained.

</details>


### [670] [ReXGradient-160K: A Large-Scale Publicly Available Dataset of Chest Radiographs with Free-text Reports](https://arxiv.org/pdf/2505.00228)
*Xiaoman Zhang, Julián N. Acosta, Josh Miller, Ouwen Huang, Pranav Rajpurkar*

Main category: eess.IV

TL;DR: ReXGradient-160K is the largest public chest X-ray dataset with 160,000 studies from 109,487 patients, designed for AI in medical imaging and automated report generation.


<details>
  <summary>Details</summary>
Motivation: To accelerate research in medical imaging AI and improve automated radiological analysis by providing a comprehensive, large-scale dataset.

Method: The dataset includes 160,000 chest X-ray studies with paired radiology reports, divided into training, validation, and test sets.

Result: The dataset is publicly available and structured for AI development, with a private test set for benchmarking.

Conclusion: ReXGradient-160K aims to advance AI in medical imaging and automated report generation, fostering further research.

Abstract: We present ReXGradient-160K, representing the largest publicly available
chest X-ray dataset to date in terms of the number of patients. This dataset
contains 160,000 chest X-ray studies with paired radiological reports from
109,487 unique patients across 3 U.S. health systems (79 medical sites). This
comprehensive dataset includes multiple images per study and detailed radiology
reports, making it particularly valuable for the development and evaluation of
AI systems for medical imaging and automated report generation models. The
dataset is divided into training (140,000 studies), validation (10,000
studies), and public test (10,000 studies) sets, with an additional private
test set (10,000 studies) reserved for model evaluation on the ReXrank
benchmark. By providing this extensive dataset, we aim to accelerate research
in medical imaging AI and advance the state-of-the-art in automated
radiological analysis. Our dataset will be open-sourced at
https://huggingface.co/datasets/rajpurkarlab/ReXGradient-160K.

</details>


### [671] [From Spaceborne to Airborne: SAR Image Synthesis Using Foundation Models for Multi-Scale Adaptation](https://arxiv.org/pdf/2505.03844)
*Solene Debuysere, Nicolas Trouve, Nathan Letheule, Olivier Leveque, Elise Colin*

Main category: eess.IV

TL;DR: The paper addresses the scarcity of high-resolution SAR datasets by creating a synthetic dataset and using a pre-trained latent diffusion model to transform satellite SAR imagery into airborne representations.


<details>
  <summary>Details</summary>
Motivation: The lack of open-source, labeled SAR datasets hinders the use of foundation models in remote sensing. Synthetic image generation is proposed to augment scarce data.

Method: A 3.5 billion parameter pre-trained latent diffusion model is used, leveraging 15 years of ONERA's airborne SAR data. Spatial conditioning techniques are applied to transform satellite SAR imagery into airborne representations.

Result: The method effectively bridges the realism gap between simulated and real SAR images, demonstrating a novel application of AI in SAR imaging.

Conclusion: This work pioneers a new approach in SAR imaging technology, showcasing the potential of synthetic data and foundation models for remote sensing applications.

Abstract: The availability of Synthetic Aperture Radar (SAR) satellite imagery has
increased considerably in recent years, with datasets commercially available.
However, the acquisition of high-resolution SAR images in airborne
configurations, remains costly and limited. Thus, the lack of open source,
well-labeled, or easily exploitable SAR text-image datasets is a barrier to the
use of existing foundation models in remote sensing applications. In this
context, synthetic image generation is a promising solution to augment this
scarce data, enabling a broader range of applications. Leveraging over 15 years
of ONERA's extensive archival airborn data from acquisition campaigns, we
created a comprehensive training dataset of 110 thousands SAR images to exploit
a 3.5 billion parameters pre-trained latent diffusion model
\cite{Baqu2019SethiR}. In this work, we present a novel approach utilizing
spatial conditioning techniques within a foundation model to transform
satellite SAR imagery into airborne SAR representations. Additionally, we
demonstrate that our pipeline is effective for bridging the realism of
simulated images generated by ONERA's physics-based simulator EMPRISE
\cite{empriseem_ai_images}. Our method explores a key application of AI in
advancing SAR imaging technology. To the best of our knowledge, we are the
first to introduce this approach in the literature.

</details>
