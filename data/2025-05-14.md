<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 95]
- [cs.CV](#cs.CV) [Total: 119]
- [cs.AI](#cs.AI) [Total: 50]
- [cs.SD](#cs.SD) [Total: 6]
- [cs.LG](#cs.LG) [Total: 112]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.MM](#cs.MM) [Total: 0]
- [eess.AS](#eess.AS) [Total: 4]
- [eess.IV](#eess.IV) [Total: 21]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Polysemy of Synthetic Neurons Towards a New Type of Explanatory Categorical Vector Spaces](https://arxiv.org/pdf/2505.07831)
*Michael Pichat, William Pogrund, Paloma Pichat, Judicael Poumay, Armanouche Gasparian, Samuel Demarchi, Martin Corbet, Alois Georgeon, Michael Veillet-Guillem*

Main category: cs.CL

TL;DR: The paper proposes a geometric definition of neurons in AI language models as categorical vector spaces with non-orthogonal bases, enabling efficient feature utilization via intra-neuronal attention.


<details>
  <summary>Details</summary>
Motivation: To challenge the current understanding of polysemantic neurons as distributed feature superpositions and offer a more structured geometric approach.

Method: Define neurons in layer n as categorical vector spaces with non-orthogonal bases, derived from preceding neurons in layer n-1, and use intra-neuronal attention to identify critical categorical zones.

Result: The approach identifies a more homogeneous and efficient categorical zone at the intersection of sub-dimensions, improving language model performance.

Conclusion: The geometric redefinition of neurons provides a structured and efficient alternative to the superposition-based understanding of polysemantic neurons.

Abstract: The polysemantic nature of synthetic neurons in artificial intelligence
language models is currently understood as the result of a necessary
superposition of distributed features within the latent space. We propose an
alternative approach, geometrically defining a neuron in layer n as a
categorical vector space with a non-orthogonal basis, composed of categorical
sub-dimensions extracted from preceding neurons in layer n-1. This categorical
vector space is structured by the activation space of each neuron and enables,
via an intra-neuronal attention process, the identification and utilization of
a critical categorical zone for the efficiency of the language model - more
homogeneous and located at the intersection of these different categorical
sub-dimensions.

</details>


### [2] [A Tale of Two Identities: An Ethical Audit of Human and AI-Crafted Personas](https://arxiv.org/pdf/2505.07850)
*Pranav Narayanan Venkit, Jiayi Li, Yingfan Zhou, Sarah Rajtmajer, Shomir Wilson*

Main category: cs.CL

TL;DR: The paper audits LLM-generated synthetic personas for representational harm, focusing on racial identity, revealing biases like stereotyping and exoticism, and proposes solutions for better evaluation.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs represent minority identities in synthetic personas, especially in sensitive domains like health and privacy.

Method: Mixed methods approach (close reading, lexical analysis, creativity framework) comparing 1512 LLM-generated personas to human-authored ones.

Result: LLMs disproportionately highlight racial markers, use culturally coded language, and create narratively reductive personas, leading to sociotechnical harms.

Conclusion: Proposes narrative-aware metrics and community-centered validation to mitigate algorithmic othering and improve synthetic identity generation.

Abstract: As LLMs (large language models) are increasingly used to generate synthetic
personas particularly in data-limited domains such as health, privacy, and HCI,
it becomes necessary to understand how these narratives represent identity,
especially that of minority communities. In this paper, we audit synthetic
personas generated by 3 LLMs (GPT4o, Gemini 1.5 Pro, Deepseek 2.5) through the
lens of representational harm, focusing specifically on racial identity. Using
a mixed methods approach combining close reading, lexical analysis, and a
parameterized creativity framework, we compare 1512 LLM generated personas to
human-authored responses. Our findings reveal that LLMs disproportionately
foreground racial markers, overproduce culturally coded language, and construct
personas that are syntactically elaborate yet narratively reductive. These
patterns result in a range of sociotechnical harms, including stereotyping,
exoticism, erasure, and benevolent bias, that are often obfuscated by
superficially positive narrations. We formalize this phenomenon as algorithmic
othering, where minoritized identities are rendered hypervisible but less
authentic. Based on these findings, we offer design recommendations for
narrative-aware evaluation metrics and community-centered validation protocols
for synthetic identity generation.

</details>


### [3] [Joint Detection of Fraud and Concept Drift inOnline Conversations with LLM-Assisted Judgment](https://arxiv.org/pdf/2505.07852)
*Ali Senol, Garima Agrawal, Huan Liu*

Main category: cs.CL

TL;DR: A two-stage framework for detecting fake interactions in digital communication, combining ensemble classification and concept drift analysis with LLM validation, outperforms traditional methods in accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of detecting fake interactions, which range from spam to scams, and overcoming the limitations of static anomaly detection methods that misinterpret concept drift.

Method: Proposes a two-stage framework: ensemble classification to flag suspicious conversations, followed by concept drift analysis using OCDD and LLM validation to distinguish fraud from benign topic shifts.

Result: Validated on social engineering chat data, the framework improves accuracy and interpretability compared to a Dual LLM baseline.

Conclusion: The modular approach effectively balances detection reliability and adaptability, offering practical advantages for real-time fraud detection.

Abstract: Detecting fake interactions in digital communication platforms remains a
challenging and insufficiently addressed problem. These interactions may appear
as harmless spam or escalate into sophisticated scam attempts, making it
difficult to flag malicious intent early. Traditional detection methods often
rely on static anomaly detection techniques that fail to adapt to dynamic
conversational shifts. One key limitation is the misinterpretation of benign
topic transitions referred to as concept drift as fraudulent behavior, leading
to either false alarms or missed threats. We propose a two stage detection
framework that first identifies suspicious conversations using a tailored
ensemble classification model. To improve the reliability of detection, we
incorporate a concept drift analysis step using a One Class Drift Detector
(OCDD) to isolate conversational shifts within flagged dialogues. When drift is
detected, a large language model (LLM) assesses whether the shift indicates
fraudulent manipulation or a legitimate topic change. In cases where no drift
is found, the behavior is inferred to be spam like. We validate our framework
using a dataset of social engineering chat scenarios and demonstrate its
practical advantages in improving both accuracy and interpretability for real
time fraud detection. To contextualize the trade offs, we compare our modular
approach against a Dual LLM baseline that performs detection and judgment using
different language models.

</details>


### [4] [CrashSage: A Large Language Model-Centered Framework for Contextual and Interpretable Traffic Crash Analysis](https://arxiv.org/pdf/2505.07853)
*Hao Zhen, Jidong J. Yang*

Main category: cs.CL

TL;DR: CrashSage is an LLM-based framework for crash analysis, transforming raw data into enriched narratives, enhancing model performance, and improving interpretability for road safety insights.


<details>
  <summary>Details</summary>
Motivation: Road crashes cause massive societal and financial losses, but traditional methods overlook contextual nuances and struggle with complex relationships and rare events.

Method: CrashSage uses tabular-to-text transformation, context-aware data augmentation, fine-tuning of LLaMA3-8B, and gradient-based explainability for crash severity inference.

Result: Outperforms baseline approaches (zero-shot, few-shot, etc.) and provides interpretable insights into influential crash factors.

Conclusion: CrashSage advances crash analysis by combining enriched data representation, improved modeling, and actionable insights for road safety interventions.

Abstract: Road crashes claim over 1.3 million lives annually worldwide and incur global
economic losses exceeding \$1.8 trillion. Such profound societal and financial
impacts underscore the urgent need for road safety research that uncovers crash
mechanisms and delivers actionable insights. Conventional statistical models
and tree ensemble approaches typically rely on structured crash data,
overlooking contextual nuances and struggling to capture complex relationships
and underlying semantics. Moreover, these approaches tend to incur significant
information loss, particularly in narrative elements related to multi-vehicle
interactions, crash progression, and rare event characteristics. This study
presents CrashSage, a novel Large Language Model (LLM)-centered framework
designed to advance crash analysis and modeling through four key innovations.
First, we introduce a tabular-to-text transformation strategy paired with
relational data integration schema, enabling the conversion of raw,
heterogeneous crash data into enriched, structured textual narratives that
retain essential structural and relational context. Second, we apply
context-aware data augmentation using a base LLM model to improve narrative
coherence while preserving factual integrity. Third, we fine-tune the LLaMA3-8B
model for crash severity inference, demonstrating superior performance over
baseline approaches, including zero-shot, zero-shot with chain-of-thought
prompting, and few-shot learning, with multiple models (GPT-4o, GPT-4o-mini,
LLaMA3-70B). Finally, we employ a gradient-based explainability technique to
elucidate model decisions at both the individual crash level and across broader
risk factor dimensions. This interpretability mechanism enhances transparency
and enables targeted road safety interventions by providing deeper insights
into the most influential factors.

</details>


### [5] [Unpacking Robustness in Inflectional Languages: Adversarial Evaluation and Mechanistic Insights](https://arxiv.org/pdf/2505.07856)
*Paweł Walkowiak, Marek Klonowski, Marcin Oleksy, Arkadiusz Janz*

Main category: cs.CL

TL;DR: The paper evaluates adversarial attacks in inflectional languages, using Polish and English, and introduces a novel protocol to analyze inflection's impact on model robustness.


<details>
  <summary>Details</summary>
Motivation: Most adversarial attacks are tested on non-inflectional languages like English, leaving inflectional languages understudied. This work aims to bridge that gap.

Method: The authors use Edge Attribution Patching (EAP) and parallel corpora in Polish and English to analyze inflection's role in adversarial robustness.

Result: A new benchmark based on MultiEmo dataset is created to identify inflection-related model behaviors under adversarial attacks.

Conclusion: The study provides insights into how inflection affects model robustness and introduces tools for evaluating adversarial attacks in inflectional languages.

Abstract: Various techniques are used in the generation of adversarial examples,
including methods such as TextBugger which introduce minor, hardly visible
perturbations to words leading to changes in model behaviour. Another class of
techniques involves substituting words with their synonyms in a way that
preserves the text's meaning but alters its predicted class, with TextFooler
being a prominent example of such attacks. Most adversarial example generation
methods are developed and evaluated primarily on non-inflectional languages,
typically English. In this work, we evaluate and explain how adversarial
attacks perform in inflectional languages. To explain the impact of inflection
on model behaviour and its robustness under attack, we designed a novel
protocol inspired by mechanistic interpretability, based on Edge Attribution
Patching (EAP) method. The proposed evaluation protocol relies on parallel
task-specific corpora that include both inflected and syncretic variants of
texts in two languages -- Polish and English. To analyse the models and explain
the relationship between inflection and adversarial robustness, we create a new
benchmark based on task-oriented dataset MultiEmo, enabling the identification
of mechanistic inflection-related elements of circuits within the model and
analyse their behaviour under attack.

</details>


### [6] [Enhanced Urdu Intent Detection with Large Language Models and Prototype-Informed Predictive Pipelines](https://arxiv.org/pdf/2505.07857)
*Faiza Hassan, Summra Saleem, Kashif Javed, Muhammad Nabeel Asim, Abdur Rehman, Andreas Dengel*

Main category: cs.CL

TL;DR: The paper introduces a contrastive learning approach for Urdu intent detection, leveraging unlabeled data and pre-trained language models to create an end-to-end pipeline (LLMPIA), achieving high F1-scores on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Urdu lacks few-shot learning-based intent detection predictors, unlike well-resourced languages. The goal is to empower Urdu intent detection using advanced techniques.

Method: Proposes a contrastive learning approach to re-train pre-trained language models, combined with a prototype-informed attention mechanism (LLMPIA). Evaluated on 6 language models and 13 similarity methods.

Result: Achieved 83.28% and 98.25% F1-Score on ATIS (4-way 1-shot and 5-shot), and 76.23% and 84.42% on Web Queries. Outperformed SOTA by 53.55% in same-class settings.

Conclusion: The LLMPIA pipeline effectively addresses Urdu intent detection gaps, demonstrating superior performance and potential for under-resourced languages.

Abstract: Multifarious intent detection predictors are developed for different
languages, including English, Chinese and French, however, the field remains
underdeveloped for Urdu, the 10th most spoken language. In the realm of
well-known languages, intent detection predictors utilize the strategy of
few-shot learning and prediction of unseen classes based on the model training
on seen classes. However, Urdu language lacks few-shot strategy based intent
detection predictors and traditional predictors are focused on prediction of
the same classes which models have seen in the train set. To empower Urdu
language specific intent detection, this introduces a unique contrastive
learning approach that leverages unlabeled Urdu data to re-train pre-trained
language models. This re-training empowers LLMs representation learning for the
downstream intent detection task. Finally, it reaps the combined potential of
pre-trained LLMs and the prototype-informed attention mechanism to create a
comprehensive end-to-end LLMPIA intent detection pipeline. Under the paradigm
of proposed predictive pipeline, it explores the potential of 6 distinct
language models and 13 distinct similarity computation methods. The proposed
framework is evaluated on 2 public benchmark datasets, namely ATIS encompassing
5836 samples and Web Queries having 8519 samples. Across ATIS dataset under
4-way 1 shot and 4-way 5 shot experimental settings LLMPIA achieved 83.28% and
98.25% F1-Score and on Web Queries dataset produced 76.23% and 84.42% F1-Score,
respectively. In an additional case study on the Web Queries dataset under same
classes train and test set settings, LLMPIA outperformed state-of-the-art
predictor by 53.55% F1-Score.

</details>


### [7] [Scaling Laws for Speculative Decoding](https://arxiv.org/pdf/2505.07858)
*Siyuan Yan, Mo Zhu, Guo-qing Jiang, Jianfei Wang, Jiaxing Chen, Wentai Zhang, Xiang Liao, Xiao Cui, Chen Zhang, Zhuoran Song, Ran Zhu*

Main category: cs.CL

TL;DR: The paper introduces Log-linear Scaling Laws for speculative decoding in LLMs, achieving higher efficiency with Scylla, outperforming existing methods like EAGLE2/3.


<details>
  <summary>Details</summary>
Motivation: Address the need for efficient decoding in reasoning-intensive LLMs by exploring speculative decoding techniques.

Method: Investigates speculative decoding through dense LLM architectures, discovering Log-linear Scaling Laws for draft model acceptance rate.

Result: Scylla achieves 1.5-2.2× higher acceptance rate than EAGLE2 and 0.3× higher than EAGLE3, with 2X throughput improvements in industrial deployments.

Conclusion: Systematic scaling via Scylla significantly enhances LLM inference efficiency, validated by empirical and industrial results.

Abstract: The escalating demand for efficient decoding in large language models (LLMs)
is particularly critical for reasoning-intensive architectures like OpenAI-o3
and DeepSeek-R1, which depend on extended chain-of-thought reasoning. This
study investigates speculative decoding techniques through dense LLM
architectures to establish foundational insights for accelerating reasoning
tasks. While speculative decoding methods leveraging parallel
draft-verification cycles have emerged as promising acceleration techniques,
the scaling laws governing decoding efficiency remain under-explored compared
to conventional backbone LLMs developed through Pretraining->SFT->RLHF training
paradigms. In this work, we discover Log-linear Scaling Laws (Theorem 1.1, 1.2
and 1.3) governing draft model acceptance rate (or decoding speed) across three
dimensions: pretraining token volume, draft model capacity, and decoding batch
size. Building on these laws, we achieve Scylla, which coordinates
multi-dimensional scaling for popular LLMs (Llama2/3, Qwen2.5). Empirical
validation shows Scylla achieves 1.5-2.2 higher acceptance rate than EAGLE2 and
0.3 higher than EAGLE3 at temperature T = 0, with peak performance gains on
summarization and QA tasks (Figure 2). Industrial inference engine deployments
demonstrate 2X decoding throughput improvements over EAGLE2 (Table 5),
validating the transformative potential of systematic scaling for efficient LLM
inference. Code will be released later.

</details>


### [8] [Boosting Performance on ARC is a Matter of Perspective](https://arxiv.org/pdf/2505.07859)
*Daniel Franzen, Jan Disselhoff, David Hartmann*

Main category: cs.CL

TL;DR: The paper introduces a method to improve LLMs' performance on the ARC-AGI benchmark using task-specific data augmentation and a depth-first search algorithm, achieving state-of-the-art results with low inference cost.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of LLMs in abstract reasoning, particularly on the challenging ARC-AGI benchmark.

Method: Task-specific data augmentation during training, generation, and scoring phases, combined with a depth-first search algorithm and using the LLM as both generator and scorer.

Result: Achieved 71.6% (286.5/400 tasks solved) on ARC-AGI, with low inference cost (~2ct per task).

Conclusion: The method offers transparency, reproducibility, and cost-efficiency, though closed-source approaches report higher scores.

Abstract: The Abstraction and Reasoning Corpus (ARC-AGI) poses a significant challenge
for large language models (LLMs), exposing limitations in their abstract
reasoning abilities. In this work, we leverage task-specific data augmentations
throughout the training, generation, and scoring phases, and employ a
depth-first search algorithm to generate diverse, high-probability candidate
solutions. Furthermore, we utilize the LLM not only as a generator but also as
a scorer, using its output probabilities to select the most promising
solutions. Our method achieves a score of 71.6% (286.5/400 solved tasks) on the
public ARC-AGI evaluation set, demonstrating state-of-the-art performance among
publicly available approaches. While concurrent closed-source work has reported
higher scores, our method distinguishes itself through its transparency,
reproducibility, and remarkably low inference cost, averaging only around 2ct
per task on readily available hardware (we assume a price of 36ct/hour for a
Nvidia 4090 GPU).

</details>


### [9] [Scalable LLM Math Reasoning Acceleration with Low-rank Distillation](https://arxiv.org/pdf/2505.07861)
*Harry Dong, Bilge Acun, Beidi Chen, Yuejie Chi*

Main category: cs.CL

TL;DR: Caprese is a low-cost distillation method to recover math capabilities lost from efficient inference in LLMs, with minimal additional parameters and training samples.


<details>
  <summary>Details</summary>
Motivation: Efficient inference methods degrade math performance in LLMs despite preserving language tasks.

Method: Caprese uses distillation, adding ~1% parameters and 20K synthetic samples, focusing on feedforward blocks without altering original weights.

Result: Recovers lost math capabilities, reduces active parameters (~2B cut), and lowers latency (>11% reduction for 2048 tokens).

Conclusion: Caprese effectively restores math performance while maintaining language task efficiency and reducing computational overhead.

Abstract: Due to long generations, large language model (LLM) math reasoning demands
significant computational resources and time. While many existing efficient
inference methods have been developed with excellent performance preservation
on language tasks, they often severely degrade math performance. In this paper,
we propose Caprese, a low-cost distillation method to recover lost capabilities
from deploying efficient inference methods, focused primarily in feedforward
blocks. With original weights unperturbed, roughly 1% of additional parameters,
and only 20K synthetic training samples, we are able to recover much if not all
of the math capabilities lost from efficient inference for thinking LLMs and
without harm to language tasks for instruct LLMs. Moreover, Caprese slashes the
number of active parameters (~2B cut for Gemma 2 9B and Llama 3.1 8B) and
integrates cleanly into existing model layers to reduce latency (>11% reduction
to generate 2048 tokens with Qwen 2.5 14B) while encouraging response brevity.

</details>


### [10] [Graph Laplacian Wavelet Transformer via Learnable Spectral Decomposition](https://arxiv.org/pdf/2505.07862)
*Andrew Kiruluta, Eric Lundy, Priscilla Burity*

Main category: cs.CL

TL;DR: The paper introduces the Graph Wavelet Transformer (GWT), a model replacing quadratic-complexity self-attention with a learnable multi-scale wavelet transform for efficient graph-structured sequence modeling.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of quadratic-complexity self-attention in sequence-to-sequence models for structured language tasks.

Method: Proposes the GWT architecture, using a learnable multi-scale wavelet transform over a graph Laplacian derived from syntactic or semantic parses.

Result: Demonstrates that multi-scale spectral decomposition is interpretable, efficient, and expressive for graph-structured sequences.

Conclusion: GWT offers a viable alternative to traditional self-attention for structured language tasks.

Abstract: Existing sequence to sequence models for structured language tasks rely
heavily on the dot product self attention mechanism, which incurs quadratic
complexity in both computation and memory for input length N. We introduce the
Graph Wavelet Transformer (GWT), a novel architecture that replaces this
bottleneck with a learnable, multi scale wavelet transform defined over an
explicit graph Laplacian derived from syntactic or semantic parses. Our
analysis shows that multi scale spectral decomposition offers an interpretable,
efficient, and expressive alternative to quadratic self attention for graph
structured sequence modeling.

</details>


### [11] [QoSBERT: An Uncertainty-Aware Approach based on Pre-trained Language Models for Service Quality Prediction](https://arxiv.org/pdf/2505.07863)
*Ziliang Wang, Xiaohong Zhang, Ze Shi Li, Meng Yan*

Main category: cs.CL

TL;DR: QoSBERT is a framework using pre-trained language models for QoS prediction, integrating uncertainty estimation and outperforming baselines in accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional QoS models lack confidence insights and rely on manual features; QoSBERT aims to automate and improve trustworthiness.

Method: QoSBERT encodes metadata into natural language, uses Monte Carlo Dropout for uncertainty, and fine-tunes with attentive pooling and MLP.

Result: Achieves 11.7% MAE and 6.7% RMSE reduction for response time, 6.9% MAE for throughput, with reliable confidence intervals.

Conclusion: QoSBERT enhances accuracy and uncertainty quantification, enabling trustworthy service selection and optimization.

Abstract: Accurate prediction of Quality of Service (QoS) metrics is fundamental for
selecting and managing cloud based services. Traditional QoS models rely on
manual feature engineering and yield only point estimates, offering no insight
into the confidence of their predictions. In this paper, we propose QoSBERT,
the first framework that reformulates QoS prediction as a semantic regression
task based on pre trained language models. Unlike previous approaches relying
on sparse numerical features, QoSBERT automatically encodes user service
metadata into natural language descriptions, enabling deep semantic
understanding. Furthermore, we integrate a Monte Carlo Dropout based
uncertainty estimation module, allowing for trustworthy and risk-aware service
quality prediction, which is crucial yet underexplored in existing QoS models.
QoSBERT applies attentive pooling over contextualized embeddings and a
lightweight multilayer perceptron regressor, fine tuned jointly to minimize
absolute error. We further exploit the resulting uncertainty estimates to
select high quality training samples, improving robustness in low resource
settings. On standard QoS benchmark datasets, QoSBERT achieves an average
reduction of 11.7% in MAE and 6.7% in RMSE for response time prediction, and
6.9% in MAE for throughput prediction compared to the strongest baselines,
while providing well calibrated confidence intervals for robust and trustworthy
service quality estimation. Our approach not only advances the accuracy of
service quality prediction but also delivers reliable uncertainty
quantification, paving the way for more trustworthy, data driven service
selection and optimization.

</details>


### [12] [Efficient Fairness Testing in Large Language Models: Prioritizing Metamorphic Relations for Bias Detection](https://arxiv.org/pdf/2505.07870)
*Suavis Giramata, Madhusudan Srinivasan, Venkat Naidu Gudivada, Upulee Kanewala*

Main category: cs.CL

TL;DR: The paper proposes a diversity-based approach to prioritize metamorphic relations (MRs) in testing LLMs for fairness, improving fault detection rates and reducing time to failure.


<details>
  <summary>Details</summary>
Motivation: Addressing fairness and bias concerns in LLMs by efficiently detecting issues through prioritized testing, given the impracticality of exhaustive testing.

Method: A sentence diversity-based approach to compute and rank MRs for optimized fault detection in fairness testing.

Result: The approach improves fault detection by 22% over random prioritization and 12% over distance-based prioritization, with reduced time to first failure.

Conclusion: Diversity-based MR prioritization effectively enhances fairness testing for LLMs, balancing effectiveness and computational cost.

Abstract: Large Language Models (LLMs) are increasingly deployed in various
applications, raising critical concerns about fairness and potential biases in
their outputs. This paper explores the prioritization of metamorphic relations
(MRs) in metamorphic testing as a strategy to efficiently detect fairness
issues within LLMs. Given the exponential growth of possible test cases,
exhaustive testing is impractical; therefore, prioritizing MRs based on their
effectiveness in detecting fairness violations is crucial. We apply a sentence
diversity-based approach to compute and rank MRs to optimize fault detection.
Experimental results demonstrate that our proposed prioritization approach
improves fault detection rates by 22% compared to random prioritization and 12%
compared to distance-based prioritization, while reducing the time to the first
failure by 15% and 8%, respectively. Furthermore, our approach performs within
5% of fault-based prioritization in effectiveness, while significantly reducing
the computational cost associated with fault labeling. These results validate
the effectiveness of diversity-based MR prioritization in enhancing fairness
testing for LLMs.

</details>


### [13] [Evaluating Financial Sentiment Analysis with Annotators Instruction Assisted Prompting: Enhancing Contextual Interpretation and Stock Prediction Accuracy](https://arxiv.org/pdf/2505.07871)
*A M Muntasir Rahman, Ajim Uddin, Guiling "Grace" Wang*

Main category: cs.CL

TL;DR: The paper introduces AIAP, a prompt method for financial sentiment analysis (FSA) that integrates human annotator instructions into LLMs, improving performance and standardizing sentiment understanding.


<details>
  <summary>Details</summary>
Motivation: Existing FSA datasets suffer from subjective annotations, unfairly benchmarking LLMs. The goal is to align LLM tasks with human-defined sentiment standards.

Method: AIAP incorporates annotators' instructions into LLM prompts, tested on the WSBS dataset from WallStreetBets.

Result: AIAP boosts LLM performance by up to 9.08%, enhances stock price prediction, and introduces a sentiment-indexing method.

Conclusion: AIAP improves FSA by aligning LLMs with human task definitions, offering better evaluation methods and leveraging financial text sources like WSB.

Abstract: Financial sentiment analysis (FSA) presents unique challenges to LLMs that
surpass those in typical sentiment analysis due to the nuanced language used in
financial contexts. The prowess of these models is often undermined by the
inherent subjectivity of sentiment classifications in existing benchmark
datasets like Financial Phrasebank. These datasets typically feature undefined
sentiment classes that reflect the highly individualized perspectives of
annotators, leading to significant variability in annotations. This variability
results in an unfair expectation for LLMs during benchmarking, where they are
tasked to conjecture the subjective viewpoints of human annotators without
sufficient context. In this paper, we introduce the Annotators' Instruction
Assisted Prompt, a novel evaluation prompt designed to redefine the task
definition of FSA for LLMs. By integrating detailed task instructions
originally intended for human annotators into the LLMs' prompt framework, AIAP
aims to standardize the understanding of sentiment across both human and
machine interpretations, providing a fair and context-rich foundation for
sentiment analysis. We utilize a new dataset, WSBS, derived from the
WallStreetBets subreddit to demonstrate how AIAP significantly enhances LLM
performance by aligning machine operations with the refined task definitions.
Experimental results demonstrate that AIAP enhances LLM performance
significantly, with improvements up to 9.08. This context-aware approach not
only yields incremental gains in performance but also introduces an innovative
sentiment-indexing method utilizing model confidence scores. This method
enhances stock price prediction models and extracts more value from the
financial sentiment analysis, underscoring the significance of WSB as a
critical source of financial text. Our research offers insights into both
improving FSA through better evaluation methods.

</details>


### [14] [The Sound of Populism: Distinct Linguistic Features Across Populist Variants](https://arxiv.org/pdf/2505.07874)
*Yu Wang, Runxi Yu, Zhongyuan Wang, Jing He*

Main category: cs.CL

TL;DR: The study analyzes populist rhetoric in U.S. political speeches using LIWC and RoBERTa, revealing distinct tonal and emotional patterns across populist dimensions.


<details>
  <summary>Details</summary>
Motivation: To uncover the auditory and stylistic dimensions of populism in political rhetoric, focusing on emotional and tonal variations.

Method: Combines LIWC features with a fine-tuned RoBERTa model to analyze U.S. presidential speeches for populist linguistic markers.

Result: Populist rhetoric is direct and assertive, with right-wing and people-centric variants being more emotionally charged than left-wing and anti-elitist ones.

Conclusion: Populist speech strategically balances informality and calibration, with distinct emotional tones across its variants.

Abstract: This study explores the sound of populism by integrating the classic
Linguistic Inquiry and Word Count (LIWC) features, which capture the emotional
and stylistic tones of language, with a fine-tuned RoBERTa model, a
state-of-the-art context-aware language model trained to detect nuanced
expressions of populism. This approach allows us to uncover the auditory
dimensions of political rhetoric in U.S. presidential inaugural and State of
the Union addresses. We examine how four key populist dimensions (i.e.,
left-wing, right-wing, anti-elitism, and people-centrism) manifest in the
linguistic markers of speech, drawing attention to both commonalities and
distinct tonal shifts across these variants. Our findings reveal that populist
rhetoric consistently features a direct, assertive ``sound" that forges a
connection with ``the people'' and constructs a charismatic leadership persona.
However, this sound is not simply informal but strategically calibrated.
Notably, right-wing populism and people-centrism exhibit a more emotionally
charged discourse, resonating with themes of identity, grievance, and crisis,
in contrast to the relatively restrained emotional tones of left-wing and
anti-elitist expressions.

</details>


### [15] [Recovering Event Probabilities from Large Language Model Embeddings via Axiomatic Constraints](https://arxiv.org/pdf/2505.07883)
*Jian-Qiao Zhu, Haijiang Yan, Thomas L. Griffiths*

Main category: cs.CL

TL;DR: The paper explores recovering coherent event probabilities from LLM embeddings by enforcing probability axioms in a VAE's latent space, showing improved coherence over direct model outputs.


<details>
  <summary>Details</summary>
Motivation: LLMs generate incoherent event probabilities, violating probability axioms. The study aims to recover coherent probabilities from embeddings for better uncertainty estimates.

Method: An extended VAE enforces axiomatic constraints (e.g., additive rule) in the latent space of LLM embeddings, learning to reconstruct and predict related event embeddings.

Result: Recovered probabilities from embeddings are more coherent than direct LLM outputs and align closely with true probabilities, especially for complementary events.

Conclusion: Enforcing probability axioms in latent space improves coherence of event probabilities derived from LLM embeddings, offering more accurate uncertainty estimates.

Abstract: Rational decision-making under uncertainty requires coherent degrees of
belief in events. However, event probabilities generated by Large Language
Models (LLMs) have been shown to exhibit incoherence, violating the axioms of
probability theory. This raises the question of whether coherent event
probabilities can be recovered from the embeddings used by the models. If so,
those derived probabilities could be used as more accurate estimates in events
involving uncertainty. To explore this question, we propose enforcing axiomatic
constraints, such as the additive rule of probability theory, in the latent
space learned by an extended variational autoencoder (VAE) applied to LLM
embeddings. This approach enables event probabilities to naturally emerge in
the latent space as the VAE learns to both reconstruct the original embeddings
and predict the embeddings of semantically related events. We evaluate our
method on complementary events (i.e., event A and its complement, event not-A),
where the true probabilities of the two events must sum to 1. Experiment
results on open-weight language models demonstrate that probabilities recovered
from embeddings exhibit greater coherence than those directly reported by the
corresponding models and align closely with the true probabilities.

</details>


### [16] [Development of a WAZOBIA-Named Entity Recognition System](https://arxiv.org/pdf/2505.07884)
*S. E Emedem, I. E Onyenwe, E. G Onyedinma*

Main category: cs.CL

TL;DR: The paper presents WAZOBIA-NER, a system for Named Entity Recognition in Nigerian languages (Hausa, Yoruba, Igbo), addressing data scarcity and leveraging CRF, BiLSTM, BERT, and RNN models. It achieves high performance metrics.


<details>
  <summary>Details</summary>
Motivation: NER systems often neglect under-resourced African languages. This research aims to bridge this gap by focusing on Nigerian languages.

Method: The study compiles annotated datasets and evaluates CRF, BiLSTM, BERT, and RNN models for recognizing entities (persons, organizations, locations). OCR is used for text extraction from images.

Result: The system achieved precision (0.9511), recall (0.9400), F1-score (0.9564), and accuracy (0.9301).

Conclusion: The WAZOBIA-NER system proves the feasibility of robust NER tools for under-resourced languages using modern NLP techniques.

Abstract: Named Entity Recognition NER is very crucial for various natural language
processing applications, including information extraction, machine translation,
and sentiment analysis. Despite the ever-increasing interest in African
languages within computational linguistics, existing NER systems focus mainly
on English, European, and a few other global languages, leaving a significant
gap for under-resourced languages. This research presents the development of a
WAZOBIA-NER system tailored for the three most prominent Nigerian languages:
Hausa, Yoruba, and Igbo. This research begins with a comprehensive compilation
of annotated datasets for each language, addressing data scarcity and
linguistic diversity challenges. Exploring the state-of-the-art machine
learning technique, Conditional Random Fields (CRF) and deep learning models
such as Bidirectional Long Short-Term Memory (BiLSTM), Bidirectional Encoder
Representation from Transformers (Bert) and fine-tune with a Recurrent Neural
Network (RNN), the study evaluates the effectiveness of these approaches in
recognizing three entities: persons, organizations, and locations. The system
utilizes optical character recognition (OCR) technology to convert textual
images into machine-readable text, thereby enabling the Wazobia system to
accept both input text and textual images for extraction purposes. The system
achieved a performance of 0.9511 in precision, 0.9400 in recall, 0.9564 in
F1-score, and 0.9301 in accuracy. The model's evaluation was conducted across
three languages, with precision, recall, F1-score, and accuracy as key
assessment metrics. The Wazobia-NER system demonstrates that it is feasible to
build robust NER tools for under-resourced African languages using current NLP
frameworks and transfer learning.

</details>


### [17] [PLHF: Prompt Optimization with Few-Shot Human Feedback](https://arxiv.org/pdf/2505.07886)
*Chun-Pai Yang, Kan Zheng, Shou-De Lin*

Main category: cs.CL

TL;DR: PLHF is a prompt optimization framework using human feedback to improve LLM outputs without clear metrics, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing prompt optimization struggles when output quality lacks clear metrics, necessitating a new approach.

Method: PLHF uses a human feedback-inspired evaluator module for prompt optimization, requiring only one round of feedback.

Result: PLHF outperforms prior methods on public and industrial datasets.

Conclusion: PLHF effectively addresses prompt optimization challenges without clear metrics, leveraging minimal human feedback.

Abstract: Automatic prompt optimization frameworks are developed to obtain suitable
prompts for large language models (LLMs) with respect to desired output quality
metrics. Although existing approaches can handle conventional tasks such as
fixed-solution question answering, defining the metric becomes complicated when
the output quality cannot be easily assessed by comparisons with standard
golden samples. Consequently, optimizing the prompts effectively and
efficiently without a clear metric becomes a critical challenge. To address the
issue, we present PLHF (which stands for "P"rompt "L"earning with "H"uman
"F"eedback), a few-shot prompt optimization framework inspired by the
well-known RLHF technique. Different from naive strategies, PLHF employs a
specific evaluator module acting as the metric to estimate the output quality.
PLHF requires only a single round of human feedback to complete the entire
prompt optimization process. Empirical results on both public and industrial
datasets show that PLHF outperforms prior output grading strategies for LLM
prompt optimizations.

</details>


### [18] [TSLFormer: A Lightweight Transformer Model for Turkish Sign Language Recognition Using Skeletal Landmarks](https://arxiv.org/pdf/2505.07890)
*Kutay Ertürk, Furkan Altınışık, İrem Sarıaltın, Ömer Nezih Gerek*

Main category: cs.CL

TL;DR: TSLFormer is a lightweight Turkish Sign Language recognition model using 3D joint positions for efficient, real-time translation.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient and robust sign language recognition, treating gestures as ordered sequences like language.

Method: Uses 3D joint positions (from Mediapipe) and a transformer-based sequence-to-sequence approach for recognition.

Result: Achieves competitive performance on AUTSL dataset (36K samples, 227 words) with minimal computational cost.

Conclusion: Joint-based input is sufficient for real-time, mobile assistive systems for hearing-impaired individuals.

Abstract: This study presents TSLFormer, a light and robust word-level Turkish Sign
Language (TSL) recognition model that treats sign gestures as ordered,
string-like language. Instead of using raw RGB or depth videos, our method only
works with 3D joint positions - articulation points - extracted using Google's
Mediapipe library, which focuses on the hand and torso skeletal locations. This
creates efficient input dimensionality reduction while preserving important
semantic gesture information.
  Our approach revisits sign language recognition as sequence-to-sequence
translation, inspired by the linguistic nature of sign languages and the
success of transformers in natural language processing. Since TSLFormer uses
the self-attention mechanism, it effectively captures temporal co-occurrence
within gesture sequences and highlights meaningful motion patterns as words
unfold.
  Evaluated on the AUTSL dataset with over 36,000 samples and 227 different
words, TSLFormer achieves competitive performance with minimal computational
cost. These results show that joint-based input is sufficient for enabling
real-time, mobile, and assistive communication systems for hearing-impaired
individuals.

</details>


### [19] [Implementing Long Text Style Transfer with LLMs through Dual-Layered Sentence and Paragraph Structure Extraction and Mapping](https://arxiv.org/pdf/2505.07888)
*Yusen Wu, Xiaotie Deng*

Main category: cs.CL

TL;DR: ZeroStylus, a hierarchical framework for long-text style transfer, combines sentence-level adaptation with paragraph-level coherence, outperforming baselines without needing parallel corpora or LLM fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of preserving syntactic and semantic consistency in long-text style transfer using zero-shot learning of LLMs.

Method: Proposes ZeroStylus, a two-phase framework: hierarchical template acquisition and template-guided generation with multi-granular matching.

Result: Achieves a 6.90 average score vs. 6.70 for baselines, with validated improvements in style consistency, content preservation, and expression quality.

Conclusion: ZeroStylus enables coherent long-text style transfer without parallel corpora or LLM fine-tuning, validated by ablation studies.

Abstract: This paper addresses the challenge in long-text style transfer using
zero-shot learning of large language models (LLMs), proposing a hierarchical
framework that combines sentence-level stylistic adaptation with
paragraph-level structural coherence. We argue that in the process of effective
paragraph-style transfer, to preserve the consistency of original syntactic and
semantic information, it is essential to perform style transfer not only at the
sentence level but also to incorporate paragraph-level semantic considerations,
while ensuring structural coherence across inter-sentential relationships. Our
proposed framework, ZeroStylus, operates through two systematic phases:
hierarchical template acquisition from reference texts and template-guided
generation with multi-granular matching. The framework dynamically constructs
sentence and paragraph template repositories, enabling context-aware
transformations while preserving inter-sentence logical relationships.
Experimental evaluations demonstrate significant improvements over baseline
methods, with structured rewriting achieving 6.90 average score compared to
6.70 for direct prompting approaches in tri-axial metrics assessing style
consistency, content preservation, and expression quality. Ablation studies
validate the necessity of both template hierarchies during style transfer,
showing higher content preservation win rate against sentence-only approaches
through paragraph-level structural encoding, as well as direct prompting method
through sentence-level pattern extraction and matching. The results establish
new capabilities for coherent long-text style transfer without requiring
parallel corpora or LLM fine-tuning.

</details>


### [20] [BioProBench: Comprehensive Dataset and Benchmark in Biological Protocol Understanding and Reasoning](https://arxiv.org/pdf/2505.07889)
*Yuyang Liu, Liuzhenghao Lv, Xiancheng Zhang, Li Yuan, Yonghong Tian*

Main category: cs.CL

TL;DR: BioProBench is a large-scale benchmark for evaluating LLMs on biological protocol tasks, revealing their strengths in surface understanding but weaknesses in deep reasoning and structured generation.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate LLMs on specialized, accuracy-critical biological protocols, which lack comprehensive benchmarks.

Method: Developed BioProBench with 27K protocols, generating 556K structured instances across five tasks: QA, step ordering, error correction, generation, and reasoning. Evaluated 12 LLMs.

Result: Top models perform well on surface tasks but struggle with reasoning and generation. Open-source models sometimes match closed-source, but bio-specific models lag.

Conclusion: Procedural reasoning in biological protocols is challenging for LLMs. BioProBench provides a framework to diagnose limitations and guide AI development for scientific automation.

Abstract: Biological protocols are fundamental to reproducible and safe life science
research. While LLMs excel on general tasks, their systematic evaluation on
these highly specialized, accuracy-critical, and inherently procedural texts
remains limited. In this work, we present BioProBench, the first large-scale,
integrated multi-task benchmark for biological protocol understanding and
reasoning. While limited benchmarks have touched upon specific aspects like
protocol QA, BioProBench provides a comprehensive suite of five core tasks:
Protocol Question Answering, Step Ordering, Error Correction, Protocol
Generation, and Protocol Reasoning, enabling a holistic evaluation of LLMs on
procedural biological texts. Built upon 27K original protocols, it yields
nearly 556K high-quality structured instances. We evaluate 12 mainstream
open/closed-source LLMs on BioProBench. Experimental results reveal that while
top models preform well on surface understanding tasks, struggle significantly
with deep reasoning and structured generation tasks like ordering and
generation. Furthermore, model comparisons reveal diverse performance: certain
open-source models approach closed-source levels on some tasks, yet
bio-specific small models lag behind general LLMs, indicating limitations on
complex procedural content. Overall, our findings underscore that procedural
reasoning within biological protocols represents a significant challenge for
current LLMs. BioProBench serves as a standardized framework to diagnose these
specific limitations and guide the development of AI systems better equipped
for safely automating complex scientific procedures. The code and data are
available at: https://github.com/YuyangSunshine/bioprotocolbench and
https://huggingface.co/datasets/GreatCaptainNemo/BioProBench.

</details>


### [21] [TrumorGPT: Graph-Based Retrieval-Augmented Large Language Model for Fact-Checking](https://arxiv.org/pdf/2505.07891)
*Ching Nam Hang, Pei-Duo Yu, Chee Wei Tan*

Main category: cs.CL

TL;DR: TrumorGPT is an AI tool for health-related fact-checking, distinguishing true rumors ('trumors') using a large language model and graph-based retrieval-augmented generation (GraphRAG) to ensure accuracy with up-to-date data.


<details>
  <summary>Details</summary>
Motivation: To combat health misinformation by distinguishing verified facts from rumors, addressing the limitations of static data and LLM hallucinations.

Method: Uses a large language model with few-shot learning for semantic health knowledge graph construction and GraphRAG for dynamic data retrieval.

Result: Superior performance in fact-checking health claims, leveraging updated data for accuracy.

Conclusion: TrumorGPT advances health misinformation combat by enhancing trust and accuracy in digital information.

Abstract: In the age of social media, the rapid spread of misinformation and rumors has
led to the emergence of infodemics, where false information poses a significant
threat to society. To combat this issue, we introduce TrumorGPT , a novel
generative artificial intelligence solution designed for fact-checking in the
health domain. TrumorGPT aims to distinguish "trumors", which are
health-related rumors that turn out to be true, providing a crucial tool in
differentiating between mere speculation and verified facts. This framework
leverages a large language model (LLM) with few-shot learning for semantic
health knowledge graph construction and semantic reasoning. TrumorGPT
incorporates graph-based retrieval-augmented generation (GraphRAG) to address
the hallucination issue common in LLMs and the limitations of static training
data. GraphRAG involves accessing and utilizing information from regularly
updated semantic health knowledge graphs that consist of the latest medical
news and health information, ensuring that fact-checking by TrumorGPT is based
on the most recent data. Evaluating with extensive healthcare datasets,
TrumorGPT demonstrates superior performance in fact-checking for public health
claims. Its ability to effectively conduct fact-checking across various
platforms marks a critical step forward in the fight against health-related
misinformation, enhancing trust and accuracy in the digital information age.

</details>


### [22] [LongCodeBench: Evaluating Coding LLMs at 1M Context Windows](https://arxiv.org/pdf/2505.07897)
*Stefano Rando, Luca Romani, Alessio Sampieri, Yuta Kyuragi, Luca Franco, Fabio Galasso, Tatsunori Hashimoto, John Yang*

Main category: cs.CL

TL;DR: LongCodeBench (LCB) is introduced as a benchmark for testing long-context model coding abilities, focusing on code comprehension and repair. It reveals performance drops in models like Claude 3.5 Sonnet and Qwen2.5.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of model context lengths necessitates realistic long-context benchmarks, with code comprehension and repair identified as a natural testbed.

Method: LCB includes QA (LongCodeQA) and bug fixing (LongSWE-Bench) tasks derived from real-world GitHub issues, stratified by complexity to evaluate models like Qwen2.5 and Gemini.

Result: Performance drops significantly in long-context scenarios, e.g., Claude 3.5 Sonnet drops from 29% to 3%, and Qwen2.5 from 70.2% to 40%.

Conclusion: Long-context remains a challenge for models, highlighting the need for improved benchmarks and model capabilities.

Abstract: Context lengths for models have grown rapidly, from thousands to millions of
tokens in just a few years. The extreme context sizes of modern long-context
models have made it difficult to construct realistic long-context benchmarks --
not only due to the cost of collecting million-context tasks but also in
identifying realistic scenarios that require significant contexts. We identify
code comprehension and repair as a natural testbed and challenge task for
long-context models and introduce LongCodeBench (LCB), a benchmark to test LLM
coding abilities in long-context scenarios. Our benchmark tests both the
comprehension and repair capabilities of LCLMs in realistic and important
settings by drawing from real-world GitHub issues and constructing QA
(LongCodeQA) and bug fixing (LongSWE-Bench) tasks. We carefully stratify the
complexity of our benchmark, enabling us to evaluate models across different
scales -- ranging from Qwen2.5 14B Instruct to Google's flagship Gemini model.
We find that long-context remains a weakness for all models, with performance
drops such as from 29% to 3% for Claude 3.5 Sonnet, or from 70.2% to 40% for
Qwen2.5.

</details>


### [23] [DeltaEdit: Enhancing Sequential Editing in Large Language Models by Controlling Superimposed Noise](https://arxiv.org/pdf/2505.07899)
*Ding Cao, Yuchen Cai, Rongxi Guo, Xuesong He, Guiquan Liu*

Main category: cs.CL

TL;DR: DeltaEdit improves sequential knowledge editing in LLMs by reducing interference between edits, maintaining high success rates and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing sequential editing methods suffer from declining success rates due to accumulated noise, leading to deviations from desired outputs.

Method: DeltaEdit optimizes update parameters using dynamic orthogonal constraints to minimize interference between edits.

Result: DeltaEdit outperforms existing methods in edit success rates and retains generalization capabilities under extensive editing.

Conclusion: DeltaEdit ensures stable and reliable model performance during long-term sequential knowledge editing.

Abstract: Sequential knowledge editing techniques aim to continuously update the
knowledge in large language models at a low cost, preventing the models from
generating outdated or incorrect information. However, existing sequential
editing methods suffer from a significant decline in editing success rates
after long-term editing. Through theoretical analysis and experiments, we
identify that as the number of edits increases, the model's output increasingly
deviates from the desired target, leading to a drop in editing success rates.
We refer to this issue as the accumulation of superimposed noise problem. To
address this, we identify the factors contributing to this deviation and
propose DeltaEdit, a novel method that optimizes update parameters through a
dynamic orthogonal constraints strategy, effectively reducing interference
between edits to mitigate deviation. Experimental results demonstrate that
DeltaEdit significantly outperforms existing methods in edit success rates and
the retention of generalization capabilities, ensuring stable and reliable
model performance even under extensive sequential editing.

</details>


### [24] [SEM: Reinforcement Learning for Search-Efficient Large Language Models](https://arxiv.org/pdf/2505.07903)
*Zeyang Sha, Shiwen Cui, Weiqiang Wang*

Main category: cs.CL

TL;DR: SEM is a reinforcement learning framework to optimize LLMs' search usage, reducing redundancy while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of LLMs discerning when to invoke search engines versus relying on internal knowledge to avoid inefficiencies.

Method: Uses a balanced dataset (MuSiQue and MMLU), a structured reasoning template, and GRPO for post-training. The reward function optimizes search behavior.

Result: Significantly reduces redundant searches while maintaining or improving answer accuracy.

Conclusion: SEM enhances reasoning efficiency and judicious use of external knowledge in LLMs.

Abstract: Recent advancements in Large Language Models(LLMs) have demonstrated their
capabilities not only in reasoning but also in invoking external tools,
particularly search engines. However, teaching models to discern when to invoke
search and when to rely on their internal knowledge remains a significant
challenge. Existing reinforcement learning approaches often lead to redundant
search behaviors, resulting in inefficiencies and over-cost. In this paper, we
propose SEM, a novel post-training reinforcement learning framework that
explicitly trains LLMs to optimize search usage. By constructing a balanced
dataset combining MuSiQue and MMLU, we create scenarios where the model must
learn to distinguish between questions it can answer directly and those
requiring external retrieval. We design a structured reasoning template and
employ Group Relative Policy Optimization(GRPO) to post-train the model's
search behaviors. Our reward function encourages accurate answering without
unnecessary search while promoting effective retrieval when needed.
Experimental results demonstrate that our method significantly reduces
redundant search operations while maintaining or improving answer accuracy
across multiple challenging benchmarks. This framework advances the model's
reasoning efficiency and extends its capability to judiciously leverage
external knowledge.

</details>


### [25] [Re$^2$: A Consistency-ensured Dataset for Full-stage Peer Review and Multi-turn Rebuttal Discussions](https://arxiv.org/pdf/2505.07920)
*Daoze Zhang, Zhijian Bao, Sihang Du, Zhiyi Zhao, Kuangling Zhang, Dezheng Bao, Yang Yang*

Main category: cs.CL

TL;DR: The paper introduces Re^2, a large peer review dataset addressing limitations in existing datasets to improve AI-assisted peer review and rebuttal processes.


<details>
  <summary>Details</summary>
Motivation: The rapid increase in submission volume and repeated resubmission of substandard manuscripts strain peer review systems, highlighting the need for better tools for authors and reviewers.

Method: The authors compile Re^2, a dataset with 19,926 initial submissions, 70,668 reviews, and 53,818 rebuttals from OpenReview, framed as multi-turn conversations for dynamic LLM assistance.

Result: Re^2 is the largest consistency-ensured peer review dataset, supporting both static review tasks and dynamic interactions to improve manuscript refinement.

Conclusion: Re^2 provides practical guidance for authors and reviewers, aiming to alleviate the peer review burden and enhance the quality of scientific submissions.

Abstract: Peer review is a critical component of scientific progress in the fields like
AI, but the rapid increase in submission volume has strained the reviewing
system, which inevitably leads to reviewer shortages and declines review
quality. Besides the growing research popularity, another key factor in this
overload is the repeated resubmission of substandard manuscripts, largely due
to the lack of effective tools for authors to self-evaluate their work before
submission. Large Language Models (LLMs) show great promise in assisting both
authors and reviewers, and their performance is fundamentally limited by the
quality of the peer review data. However, existing peer review datasets face
three major limitations: (1) limited data diversity, (2) inconsistent and
low-quality data due to the use of revised rather than initial submissions, and
(3) insufficient support for tasks involving rebuttal and reviewer-author
interactions. To address these challenges, we introduce the largest
consistency-ensured peer review and rebuttal dataset named Re^2, which
comprises 19,926 initial submissions, 70,668 review comments, and 53,818
rebuttals from 24 conferences and 21 workshops on OpenReview. Moreover, the
rebuttal and discussion stage is framed as a multi-turn conversation paradigm
to support both traditional static review tasks and dynamic interactive LLM
assistants, providing more practical guidance for authors to refine their
manuscripts and helping alleviate the growing review burden. Our data and code
are available in https://anonymous.4open.science/r/ReviewBench_anon/.

</details>


### [26] [Assessing and Mitigating Medical Knowledge Drift and Conflicts in Large Language Models](https://arxiv.org/pdf/2505.07968)
*Weiyi Wu, Xinwen Xu, Chongyang Gao, Xingjian Diao, Siting Li, Lucas A. Salas, Jiang Gui*

Main category: cs.CL

TL;DR: LLMs struggle with evolving medical guidelines, leading to outdated or conflicting advice. The DriftMedQA benchmark tested seven models, revealing issues with rejecting outdated recommendations. Mitigation strategies like Retrieval-Augmented Generation and preference fine-tuning improved results, especially when combined.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of LLMs adapting to rapidly changing medical knowledge, which can result in unreliable treatment suggestions.

Method: Developed the DriftMedQA benchmark to simulate guideline evolution, evaluated seven LLMs, and tested mitigation strategies (Retrieval-Augmented Generation and Direct Preference Optimization).

Result: Models struggled with outdated recommendations and internal inconsistencies. Mitigation strategies improved performance, with combined methods yielding the best results.

Conclusion: Enhancing LLM robustness to temporal shifts is crucial for reliable clinical applications.

Abstract: Large Language Models (LLMs) have great potential in the field of health
care, yet they face great challenges in adapting to rapidly evolving medical
knowledge. This can lead to outdated or contradictory treatment suggestions.
This study investigated how LLMs respond to evolving clinical guidelines,
focusing on concept drift and internal inconsistencies. We developed the
DriftMedQA benchmark to simulate guideline evolution and assessed the temporal
reliability of various LLMs. Our evaluation of seven state-of-the-art models
across 4,290 scenarios demonstrated difficulties in rejecting outdated
recommendations and frequently endorsing conflicting guidance. Additionally, we
explored two mitigation strategies: Retrieval-Augmented Generation and
preference fine-tuning via Direct Preference Optimization. While each method
improved model performance, their combination led to the most consistent and
reliable results. These findings underscore the need to improve LLM robustness
to temporal shifts to ensure more dependable applications in clinical practice.

</details>


### [27] [Task-Adaptive Semantic Communications with Controllable Diffusion-based Data Regeneration](https://arxiv.org/pdf/2505.07980)
*Fupei Guo, Achintha Wijesinghe, Songyang Zhang, Zhi Ding*

Main category: cs.CL

TL;DR: A task-adaptive semantic communication framework using diffusion models dynamically adjusts semantic message delivery for downstream tasks, improving efficiency and relevance.


<details>
  <summary>Details</summary>
Motivation: To enhance bandwidth efficiency in next-gen networking by shifting from bit-wise data delivery to conveying semantic meanings, adaptable to various downstream tasks.

Method: Uses diffusion models for dynamic semantic message adjustment. Starts with deep-compressed general semantic representation, then refines transmission based on receiver feedback (textual prompts) via attention mechanisms.

Result: Effectively preserves critical task-relevant information while maintaining high compression efficiency.

Conclusion: The framework successfully adapts semantic communication to diverse tasks, optimizing both relevance and efficiency.

Abstract: Semantic communications represent a new paradigm of next-generation
networking that shifts bit-wise data delivery to conveying the semantic
meanings for bandwidth efficiency. To effectively accommodate various potential
downstream tasks at the receiver side, one should adaptively convey the most
critical semantic information. This work presents a novel task-adaptive
semantic communication framework based on diffusion models that is capable of
dynamically adjusting the semantic message delivery according to various
downstream tasks. Specifically, we initialize the transmission of a
deep-compressed general semantic representation from the transmitter to enable
diffusion-based coarse data reconstruction at the receiver. The receiver
identifies the task-specific demands and generates textual prompts as feedback.
Integrated with the attention mechanism, the transmitter updates the semantic
transmission with more details to better align with the objectives of the
intended receivers. Our test results demonstrate the efficacy of the proposed
method in adaptively preserving critical task-relevant information for semantic
communications while preserving high compression efficiency.

</details>


### [28] [Large Language Models and Arabic Content: A Review](https://arxiv.org/pdf/2505.08004)
*Haneh Rhel, Dmitri Roussinov*

Main category: cs.CL

TL;DR: The paper reviews the use of Large Language Models (LLMs) for Arabic NLP, addressing challenges like resource scarcity and language complexity, and highlights their success in tasks through techniques like finetuning and prompt engineering.


<details>
  <summary>Details</summary>
Motivation: The scarcity of Arabic resources and the complexity of the language pose challenges for NLP tasks, motivating the exploration of LLMs for Arabic applications.

Method: The study surveys pre-trained LLMs for Arabic, their applications, and techniques like finetuning and prompt engineering to improve performance.

Result: LLMs show significant success in Arabic NLP tasks, with ongoing adoption trends and improved handling of diverse dialects and content.

Conclusion: LLMs are effective for Arabic NLP, with techniques like finetuning enhancing their performance, and their adoption is steadily increasing.

Abstract: Over the past three years, the rapid advancement of Large Language Models
(LLMs) has had a profound impact on multiple areas of Artificial Intelligence
(AI), particularly in Natural Language Processing (NLP) across diverse
languages, including Arabic. Although Arabic is considered one of the most
widely spoken languages across 27 countries in the Arabic world and used as a
second language in some other non-Arabic countries as well, there is still a
scarcity of Arabic resources, datasets, and tools. Arabic NLP tasks face
various challenges due to the complexities of the Arabic language, including
its rich morphology, intricate structure, and diverse writing standards, among
other factors. Researchers have been actively addressing these challenges,
demonstrating that pre-trained Large Language Models (LLMs) trained on
multilingual corpora achieve significant success in various Arabic NLP tasks.
This study provides an overview of using large language models (LLMs) for the
Arabic language, highlighting early pre-trained Arabic Language models across
various NLP applications and their ability to handle diverse Arabic content
tasks and dialects. It also provides an overview of how techniques like
finetuning and prompt engineering can enhance the performance of these models.
Additionally, the study summarizes common Arabic benchmarks and datasets while
presenting our observations on the persistent upward trend in the adoption of
LLMs.

</details>


### [29] [TiSpell: A Semi-Masked Methodology for Tibetan Spelling Correction covering Multi-Level Error with Data Augmentation](https://arxiv.org/pdf/2505.08037)
*Yutong Liu, Feng Xiao, Ziyue Zhang, Yongbin Yu, Cheng Huang, Fan Gao, Xiangxiang Wang, Ma-bao Ban, Manping Fan, Thupten Tsering, Cheng Huang, Gadeng Luosang, Renzeng Duojie, Nyima Tashi*

Main category: cs.CL

TL;DR: TiSpell, a semi-masked model, corrects Tibetan spelling errors at character and syllable levels using a novel data augmentation method and outperforms baselines.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack integration of multi-level correction and lack resources for Tibetan spelling correction.

Method: Proposes data augmentation with unlabeled text and introduces TiSpell, a semi-masked model for multi-level correction.

Result: TiSpell outperforms baselines and matches state-of-the-art performance on simulated and real-world data.

Conclusion: TiSpell is effective for multi-level Tibetan spelling correction, addressing gaps in existing methods.

Abstract: Multi-level Tibetan spelling correction addresses errors at both the
character and syllable levels within a unified model. Existing methods focus
mainly on single-level correction and lack effective integration of both
levels. Moreover, there are no open-source datasets or augmentation methods
tailored for this task in Tibetan. To tackle this, we propose a data
augmentation approach using unlabeled text to generate multi-level corruptions,
and introduce TiSpell, a semi-masked model capable of correcting both
character- and syllable-level errors. Although syllable-level correction is
more challenging due to its reliance on global context, our semi-masked
strategy simplifies this process. We synthesize nine types of corruptions on
clean sentences to create a robust training set. Experiments on both simulated
and real-world data demonstrate that TiSpell, trained on our dataset,
outperforms baseline models and matches the performance of state-of-the-art
approaches, confirming its effectiveness.

</details>


### [30] [FalseReject: A Resource for Improving Contextual Safety and Mitigating Over-Refusals in LLMs via Structured Reasoning](https://arxiv.org/pdf/2505.08054)
*Zhehao Zhang, Weijie Xu, Fanyou Wu, Chandan K. Reddy*

Main category: cs.CL

TL;DR: FalseReject is a resource to reduce over-refusal of benign queries in LLMs by providing structured responses and adversarial prompts. It improves model accuracy without sacrificing safety.


<details>
  <summary>Details</summary>
Motivation: Over-refusal of benign queries in LLMs reduces their utility in sensitive scenarios.

Method: Uses a graph-informed adversarial multi-agent framework to generate diverse prompts and structured responses. Includes datasets for training and benchmarking.

Result: Supervised finetuning with FalseReject reduces unnecessary refusals while maintaining safety and language capabilities.

Conclusion: FalseReject effectively addresses over-refusal in LLMs, enhancing their practical utility.

Abstract: Safety alignment approaches in large language models (LLMs) often lead to the
over-refusal of benign queries, significantly diminishing their utility in
sensitive scenarios. To address this challenge, we introduce FalseReject, a
comprehensive resource containing 16k seemingly toxic queries accompanied by
structured responses across 44 safety-related categories. We propose a
graph-informed adversarial multi-agent interaction framework to generate
diverse and complex prompts, while structuring responses with explicit
reasoning to aid models in accurately distinguishing safe from unsafe contexts.
FalseReject includes training datasets tailored for both standard
instruction-tuned models and reasoning-oriented models, as well as a
human-annotated benchmark test set. Our extensive benchmarking on 29
state-of-the-art (SOTA) LLMs reveals persistent over-refusal challenges.
Empirical results demonstrate that supervised finetuning with FalseReject
substantially reduces unnecessary refusals without compromising overall safety
or general language capabilities.

</details>


### [31] [HYPERNYM MERCURY: Token Optimization through Semantic Field Constriction and Reconstruction from Hypernyms. A New Text Compression Method](https://arxiv.org/pdf/2505.08058)
*Chris Forrester, Octavia Sulea*

Main category: cs.CL

TL;DR: A novel text representation and word-level semantic compression method achieves over 90% token reduction while maintaining high semantic similarity, with controllable granularity and lossless potential.


<details>
  <summary>Details</summary>
Motivation: To optimize compute efficiency in LLM prompts by reducing tokens without losing semantic meaning.

Method: Introduces a patent-pending text representation and word-level semantic compression technique, tested on open-source data like 'Dracula.'

Result: Over 90% token reduction with retained semantic similarity, validated across genres and models.

Conclusion: The method effectively reduces tokens while preserving semantics, offering granular control and potential lossless compression.

Abstract: Compute optimization using token reduction of LLM prompts is an emerging task
in the fields of NLP and next generation, agentic AI. In this white paper, we
introduce a novel (patent pending) text representation scheme and a
first-of-its-kind word-level semantic compression of paragraphs that can lead
to over 90\% token reduction, while retaining high semantic similarity to the
source text. We explain how this novel compression technique can be lossless
and how the detail granularity is controllable. We discuss benchmark results
over open source data (i.e. Bram Stoker's Dracula available through Project
Gutenberg) and show how our results hold at the paragraph level, across
multiple genres and models.

</details>


### [32] [Are LLMs complicated ethical dilemma analyzers?](https://arxiv.org/pdf/2505.08106)
*Jiashen, Du, Jesse Yao, Allen Liu, Zhekai Zhang*

Main category: cs.CL

TL;DR: LLMs outperform non-expert humans in ethical reasoning tasks but struggle with historical grounding and nuanced strategies.


<details>
  <summary>Details</summary>
Motivation: To assess if LLMs can emulate human ethical reasoning and serve as proxies for human judgment.

Method: Benchmark dataset of 196 ethical dilemmas, expert and non-expert responses, evaluated using composite metrics (BLEU, Damerau-Levenshtein, TF-IDF, Universal Sentence Encoder).

Result: LLMs (especially GPT-4o-mini) outperform non-experts in alignment but lack in historical grounding and nuanced strategies.

Conclusion: LLMs show promise in ethical reasoning but have limitations in contextual abstraction and historical grounding.

Abstract: One open question in the study of Large Language Models (LLMs) is whether
they can emulate human ethical reasoning and act as believable proxies for
human judgment. To investigate this, we introduce a benchmark dataset
comprising 196 real-world ethical dilemmas and expert opinions, each segmented
into five structured components: Introduction, Key Factors, Historical
Theoretical Perspectives, Resolution Strategies, and Key Takeaways. We also
collect non-expert human responses for comparison, limited to the Key Factors
section due to their brevity. We evaluate multiple frontier LLMs (GPT-4o-mini,
Claude-3.5-Sonnet, Deepseek-V3, Gemini-1.5-Flash) using a composite metric
framework based on BLEU, Damerau-Levenshtein distance, TF-IDF cosine
similarity, and Universal Sentence Encoder similarity. Metric weights are
computed through an inversion-based ranking alignment and pairwise AHP
analysis, enabling fine-grained comparison of model outputs to expert
responses. Our results show that LLMs generally outperform non-expert humans in
lexical and structural alignment, with GPT-4o-mini performing most consistently
across all sections. However, all models struggle with historical grounding and
proposing nuanced resolution strategies, which require contextual abstraction.
Human responses, while less structured, occasionally achieve comparable
semantic similarity, suggesting intuitive moral reasoning. These findings
highlight both the strengths and current limitations of LLMs in ethical
decision-making.

</details>


### [33] [Putting It All into Context: Simplifying Agents with LCLMs](https://arxiv.org/pdf/2505.08120)
*Mingjian Jiang, Yangjun Ruan, Luis Lastras, Pavan Kapanipathi, Tatsunori Hashimoto*

Main category: cs.CL

TL;DR: Simpler, unscaffolded language models (LMs) with proper prompting can perform competitively on complex tasks like SWE-bench, matching or surpassing more complex agent architectures.


<details>
  <summary>Details</summary>
Motivation: To determine if the increasing complexity in LM agent architectures is necessary for solving challenging tasks like SWE-bench.

Method: Testing unscaffolded LMs (Gemini-1.5-Pro and Gemini-2.5-Pro) with proper prompting on SWE-bench, comparing their performance to complex agent scaffolds.

Result: Unscaffolded Gemini-1.5-Pro achieved 38%, comparable to scaffolded agents (32%). Gemini-2.5-Pro reached 50.8%, and a two-stage approach with Gemini-1.5-Pro and Claude-3.7 achieved 48.6%.

Conclusion: Simpler LM approaches can be as effective as complex scaffolds, especially with more capable models.

Abstract: Recent advances in language model (LM) agents have demonstrated significant
potential for automating complex real-world tasks. To make progress on these
difficult tasks, LM agent architectures have become increasingly complex, often
incorporating multi-step retrieval tools, multiple agents, and scaffolding
adapted to the underlying LM. In this work, we investigate whether all of this
complexity is necessary, or if parts of these scaffolds can be removed on
challenging tasks like SWE-bench. We show that in the case of SWE-bench, simply
putting the entire environment into the context of a long context language
model (LCLM) and properly prompting the model makes it competitive with
carefully tuned, complex agent scaffolds. We show that a Gemini-1.5-Pro model
without any scaffolding or tools achieves 38% on SWE-Bench-Verified, comparable
with approaches using carefully tuned agent scaffolds (32%). While the
unscaffolded approach with Gemini-1.5-Pro falls short of the strongest agentic
architectures, we demonstrate that the more capable Gemini-2.5-Pro using the
same unscaffolded approach directly attains a 50.8% solve rate. Additionally, a
two-stage approach combining Gemini-1.5-Pro with Claude-3.7 achieves a
competitive 48.6% solve rate.

</details>


### [34] [ALOHA: Empowering Multilingual Agent for University Orientation with Hierarchical Retrieval](https://arxiv.org/pdf/2505.08130)
*Mingxu Tao, Bowen Tang, Mingxuan Ma, Yining Zhang, Hourun Li, Feifan Wen, Hao Ma, Jia Yang*

Main category: cs.CL

TL;DR: ALOHA is a multilingual agent for university orientation, enhancing LLMs with hierarchical retrieval and external APIs to address domain-specific and multilingual search needs.


<details>
  <summary>Details</summary>
Motivation: Current LLMs and search engines fail to meet the needs of faculty and students for campus-specific, multilingual, and timely information retrieval.

Method: Developed ALOHA, integrating hierarchical retrieval and external APIs into a front-end interface for interactive service.

Result: Human evaluation and case studies show ALOHA outperforms commercial chatbots and search engines in correctness, timeliness, and user-friendliness across languages.

Conclusion: ALOHA successfully addresses domain-specific and multilingual search challenges, serving over 12,000 users.

Abstract: The rise of Large Language Models~(LLMs) revolutionizes information
retrieval, allowing users to obtain required answers through complex
instructions within conversations. However, publicly available services remain
inadequate in addressing the needs of faculty and students to search
campus-specific information. It is primarily due to the LLM's lack of
domain-specific knowledge and the limitation of search engines in supporting
multilingual and timely scenarios. To tackle these challenges, we introduce
ALOHA, a multilingual agent enhanced by hierarchical retrieval for university
orientation. We also integrate external APIs into the front-end interface to
provide interactive service. The human evaluation and case study show our
proposed system has strong capabilities to yield correct, timely, and
user-friendly responses to the queries in multiple languages, surpassing
commercial chatbots and search engines. The system has been deployed and has
provided service for more than 12,000 people.

</details>


### [35] [Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage](https://arxiv.org/pdf/2505.08167)
*Ruilin Liu, Zhixiao Zhao, Jieqiong Li, Chang Liu, Dongbo Wang*

Main category: cs.CL

TL;DR: A novel training method combining bidirectional chains of thought and a reward mechanism improves domain-specific LLMs, outperforming existing methods in accuracy and adaptability across diverse fields.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like bias, incorrect knowledge inheritance, and catastrophic forgetting in fine-tuning LLMs for Intangible Cultural Heritage (ICH).

Method: Integrates bidirectional chains of thought (forward and reverse reasoning) and a reward mechanism for optimizing decision-making and output quality.

Result: Outperforms 0-shot, step-by-step reasoning, knowledge distillation, and question augmentation methods in accuracy, Bleu-4, and Rouge-L scores.

Conclusion: The method is adaptable to multiple domains, offering a valuable approach for future model training in diverse fields.

Abstract: The rapid development of large language models (LLMs) has provided
significant support and opportunities for the advancement of domain-specific
LLMs. However, fine-tuning these large models using Intangible Cultural
Heritage (ICH) data inevitably faces challenges such as bias, incorrect
knowledge inheritance, and catastrophic forgetting. To address these issues, we
propose a novel training method that integrates a bidirectional chains of
thought and a reward mechanism. This method is built upon ICH-Qwen, a large
language model specifically designed for the field of intangible cultural
heritage. The proposed method enables the model to not only perform forward
reasoning but also enhances the accuracy of the generated answers by utilizing
reverse questioning and reverse reasoning to activate the model's latent
knowledge. Additionally, a reward mechanism is introduced during training to
optimize the decision-making process. This mechanism improves the quality of
the model's outputs through structural and content evaluations with different
weighting schemes. We conduct comparative experiments on ICH-Qwen, with results
demonstrating that our method outperforms 0-shot, step-by-step reasoning,
knowledge distillation, and question augmentation methods in terms of accuracy,
Bleu-4, and Rouge-L scores on the question-answering task. Furthermore, the
paper highlights the effectiveness of combining the bidirectional chains of
thought and reward mechanism through ablation experiments. In addition, a
series of generalizability experiments are conducted, with results showing that
the proposed method yields improvements on various domain-specific datasets and
advanced models in areas such as Finance, Wikidata, and StrategyQA. This
demonstrates that the method is adaptable to multiple domains and provides a
valuable approach for model training in future applications across diverse
fields.

</details>


### [36] [Exploiting Text Semantics for Few and Zero Shot Node Classification on Text-attributed Graph](https://arxiv.org/pdf/2505.08168)
*Yuxiang Wang, Xiao Yan, Shiyu Jin, Quanqing Xu, Chuang Hu, Yuanyuan Zhu, Bo Du, Jia Wu, Jiawei Jiang*

Main category: cs.CL

TL;DR: TSA improves node classification on TAGs using text-based augmentations, outperforming baselines by over 5%.


<details>
  <summary>Details</summary>
Motivation: Few- and zero-shot node classification on TAGs lacks exploration of text-based augmentations.

Method: Proposes TSA with positive semantics matching and negative semantics contrast for text augmentation.

Result: TSA outperforms 13 baselines, with accuracy improvements usually over 5%.

Conclusion: TSA effectively enhances node classification by leveraging text semantics.

Abstract: Text-attributed graph (TAG) provides a text description for each graph node,
and few- and zero-shot node classification on TAGs have many applications in
fields such as academia and social networks. Existing work utilizes various
graph-based augmentation techniques to train the node and text embeddings,
while text-based augmentations are largely unexplored. In this paper, we
propose Text Semantics Augmentation (TSA) to improve accuracy by introducing
more text semantic supervision signals. Specifically, we design two
augmentation techniques, i.e., positive semantics matching and negative
semantics contrast, to provide more reference texts for each graph node or text
description. Positive semantic matching retrieves texts with similar embeddings
to match with a graph node. Negative semantic contrast adds a negative prompt
to construct a text description with the opposite semantics, which is
contrasted with the original node and text. We evaluate TSA on 5 datasets and
compare with 13 state-of-the-art baselines. The results show that TSA
consistently outperforms all baselines, and its accuracy improvements over the
best-performing baseline are usually over 5%.

</details>


### [37] [A Head to Predict and a Head to Question: Pre-trained Uncertainty Quantification Heads for Hallucination Detection in LLM Outputs](https://arxiv.org/pdf/2505.08200)
*Artem Shelmanov, Ekaterina Fadeeva, Akim Tsvigun, Ivan Tsvigun, Zhuohan Xie, Igor Kiselev, Nico Daheim, Caiqi Zhang, Artem Vazhentsev, Mrinmaya Sachan, Preslav Nakov, Timothy Baldwin*

Main category: cs.CL

TL;DR: Pre-trained UQ heads enhance LLMs' uncertainty quantification, improving hallucination detection with robust performance across domains and languages.


<details>
  <summary>Details</summary>
Motivation: Address LLM hallucinations by improving uncertainty quantification to detect false outputs.

Method: Introduce supervised auxiliary UQ heads leveraging Transformer architecture and LLM attention maps.

Result: State-of-the-art hallucination detection, robustness, and generalization to untrained languages.

Conclusion: Pre-trained UQ heads are effective tools for reliable LLM outputs, with publicly released code and models.

Abstract: Large Language Models (LLMs) have the tendency to hallucinate, i.e., to
sporadically generate false or fabricated information. This presents a major
challenge, as hallucinations often appear highly convincing and users generally
lack the tools to detect them. Uncertainty quantification (UQ) provides a
framework for assessing the reliability of model outputs, aiding in the
identification of potential hallucinations. In this work, we introduce
pre-trained UQ heads: supervised auxiliary modules for LLMs that substantially
enhance their ability to capture uncertainty compared to unsupervised UQ
methods. Their strong performance stems from the powerful Transformer
architecture in their design and informative features derived from LLM
attention maps. Experimental evaluation shows that these heads are highly
robust and achieve state-of-the-art performance in claim-level hallucination
detection across both in-domain and out-of-domain prompts. Moreover, these
modules demonstrate strong generalization to languages they were not explicitly
trained on. We pre-train a collection of UQ heads for popular LLM series,
including Mistral, Llama, and Gemma 2. We publicly release both the code and
the pre-trained heads.

</details>


### [38] [Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement](https://arxiv.org/pdf/2505.08245)
*Haoran Ye, Jing Jin, Yuhang Xie, Xin Zhang, Guojie Song*

Main category: cs.CL

TL;DR: The paper introduces LLM Psychometrics, an interdisciplinary field using psychometric principles to evaluate and enhance large language models (LLMs), addressing challenges in human-centered AI evaluation.


<details>
  <summary>Details</summary>
Motivation: Traditional evaluation methods for LLMs are inadequate for measuring human-like psychological constructs, necessitating a new approach integrating psychometrics.

Method: The survey synthesizes psychometric instruments, theories, and principles to create a framework for evaluating LLMs, focusing on benchmarking, methodology refinement, and validation.

Result: The paper provides a structured framework for interdisciplinary research, broadening evaluation scopes and advancing LLM capabilities.

Conclusion: The work aims to guide future human-centered AI evaluation paradigms, promoting societal benefit through enhanced LLM understanding and development.

Abstract: The rapid advancement of large language models (LLMs) has outpaced
traditional evaluation methodologies. It presents novel challenges, such as
measuring human-like psychological constructs, navigating beyond static and
task-specific benchmarks, and establishing human-centered evaluation. These
challenges intersect with Psychometrics, the science of quantifying the
intangible aspects of human psychology, such as personality, values, and
intelligence. This survey introduces and synthesizes an emerging
interdisciplinary field of LLM Psychometrics, which leverages psychometric
instruments, theories, and principles to evaluate, understand, and enhance
LLMs. We systematically explore the role of Psychometrics in shaping
benchmarking principles, broadening evaluation scopes, refining methodologies,
validating results, and advancing LLM capabilities. This paper integrates
diverse perspectives to provide a structured framework for researchers across
disciplines, enabling a more comprehensive understanding of this nascent field.
Ultimately, we aim to provide actionable insights for developing future
evaluation paradigms that align with human-level AI and promote the advancement
of human-centered AI systems for societal benefit. A curated repository of LLM
psychometric resources is available at
https://github.com/valuebyte-ai/Awesome-LLM-Psychometrics.

</details>


### [39] [Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual Compression for Scalable Knowledge Integration](https://arxiv.org/pdf/2505.08261)
*Rishabh Agrawal, Himanshu Kumar*

Main category: cs.CL

TL;DR: The paper introduces Adaptive Contextual Compression (ACC) and a Hybrid CAG-RAG Framework to improve scalability and efficiency in knowledge-intensive tasks using large language models.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in scaling Cache-Augmented Generation (CAG) for large and dynamic knowledge bases.

Method: Proposes ACC for dynamic context compression and a Hybrid CAG-RAG Framework combining CAG with selective retrieval.

Result: Enhances scalability, efficiency, and multi-hop reasoning performance in evaluations.

Conclusion: Offers practical solutions for real-world knowledge integration challenges.

Abstract: The rapid progress in large language models (LLMs) has paved the way for
novel approaches in knowledge-intensive tasks. Among these, Cache-Augmented
Generation (CAG) has emerged as a promising alternative to Retrieval-Augmented
Generation (RAG). CAG minimizes retrieval latency and simplifies system design
by preloading knowledge into the model's context. However, challenges persist
in scaling CAG to accommodate large and dynamic knowledge bases effectively.
This paper introduces Adaptive Contextual Compression (ACC), an innovative
technique designed to dynamically compress and manage context inputs, enabling
efficient utilization of the extended memory capabilities of modern LLMs. To
further address the limitations of standalone CAG, we propose a Hybrid CAG-RAG
Framework, which integrates selective retrieval to augment preloaded contexts
in scenarios requiring additional information. Comprehensive evaluations on
diverse datasets highlight the proposed methods' ability to enhance
scalability, optimize efficiency, and improve multi-hop reasoning performance,
offering practical solutions for real-world knowledge integration challenges.

</details>


### [40] [Evaluating the Effectiveness of Black-Box Prompt Optimization as the Scale of LLMs Continues to Grow](https://arxiv.org/pdf/2505.08303)
*Ziyu Zhou, Yihang Wu, Jingyuan Yang, Zhan Xiao, Rongjun Li*

Main category: cs.CL

TL;DR: Black-box prompt optimization methods show limited effectiveness on large-scale LLMs, with diminishing returns as model size increases.


<details>
  <summary>Details</summary>
Motivation: To investigate whether black-box prompt optimization methods remain effective for increasingly large LLMs like DeepSeek V3 (671B) and Gemini 2.0 Flash.

Method: Evaluated three black-box optimization methods on large-scale LLMs across four NLU and NLG datasets, and tested varying model sizes (Qwen 2.5 series).

Result: Limited performance improvements on large-scale LLMs; effectiveness decreases with model size (inverse scaling law).

Conclusion: Model scale is a key factor in the diminishing returns of black-box prompt optimization methods.

Abstract: Black-Box prompt optimization methods have emerged as a promising strategy
for refining input prompts to better align large language models (LLMs),
thereby enhancing their task performance. Although these methods have
demonstrated encouraging results, most studies and experiments have primarily
focused on smaller-scale models (e.g., 7B, 14B) or earlier versions (e.g.,
GPT-3.5) of LLMs. As the scale of LLMs continues to increase, such as with
DeepSeek V3 (671B), it remains an open question whether these black-box
optimization techniques will continue to yield significant performance
improvements for models of such scale. In response to this, we select three
well-known black-box optimization methods and evaluate them on large-scale LLMs
(DeepSeek V3 and Gemini 2.0 Flash) across four NLU and NLG datasets. The
results show that these black-box prompt optimization methods offer only
limited improvements on these large-scale LLMs. Furthermore, we hypothesize
that the scale of the model is the primary factor contributing to the limited
benefits observed. To explore this hypothesis, we conducted experiments on LLMs
of varying sizes (Qwen 2.5 series, ranging from 7B to 72B) and observed an
inverse scaling law, wherein the effectiveness of black-box optimization
methods diminished as the model size increased.

</details>


### [41] [AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale](https://arxiv.org/pdf/2505.08311)
*Yunjie Ji, Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Han Zhao, Xiangang Li*

Main category: cs.CL

TL;DR: AM-Thinking-v1 is a 32B dense language model excelling in reasoning, outperforming competitors like DeepSeek-R1 and rivaling MoE models, with top scores on AIME and LiveCodeBench.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that open-source models can achieve high performance at the 32B scale, balancing top-tier reasoning capabilities with practical usability.

Method: Built from the Qwen2.5-32B base model, using a post-training pipeline combining supervised fine-tuning and reinforcement learning.

Result: Achieves scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on LiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities.

Conclusion: AM-Thinking-v1 inspires collaborative efforts for mid-scale models, pushing reasoning boundaries while maintaining accessibility.

Abstract: We present AM-Thinking-v1, a 32B dense language model that advances the
frontier of reasoning, embodying the collaborative spirit of open-source
innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts
(MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves
impressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on
LiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities
among open-source models of similar scale.
  Built entirely from the open-source Qwen2.5-32B base model and publicly
available queries, AM-Thinking-v1 leverages a meticulously crafted
post-training pipeline - combining supervised fine-tuning and reinforcement
learning - to deliver exceptional reasoning capabilities. This work
demonstrates that the open-source community can achieve high performance at the
32B scale, a practical sweet spot for deployment and fine-tuning. By striking a
balance between top-tier performance and real-world usability, we hope
AM-Thinking-v1 inspires further collaborative efforts to harness mid-scale
models, pushing reasoning boundaries while keeping accessibility at the core of
innovation. We have open-sourced our model on
\href{https://huggingface.co/a-m-team/AM-Thinking-v1}{Hugging Face}.

</details>


### [42] [On the Geometry of Semantics in Next-token Prediction](https://arxiv.org/pdf/2505.08348)
*Yize Zhao, Christos Thrampoulidis*

Main category: cs.CL

TL;DR: The paper explores how next-token prediction (NTP) in language models implicitly encodes semantic and grammatical concepts via singular value decomposition (SVD) of co-occurrence patterns, revealing insights into training dynamics and meaning representation.


<details>
  <summary>Details</summary>
Motivation: To understand how simple NTP training leads to the extraction of latent linguistic concepts and the implicit encoding of semantic and grammatical structures.

Method: Analyzes how NTP optimization implicitly guides models to encode concepts via SVD factors of a co-occurrence matrix, using spectral clustering to interpret embeddings.

Result: Finds that the most important SVD factors are learned early, enabling the identification of human-interpretable semantics through clustering methods.

Conclusion: Bridges distributional semantics, neural collapse geometry, and training dynamics, showing how NTP's implicit biases shape meaning representations in language models.

Abstract: Modern language models demonstrate a remarkable ability to capture linguistic
meaning despite being trained solely through next-token prediction (NTP). We
investigate how this conceptually simple training objective leads models to
extract and encode latent semantic and grammatical concepts. Our analysis
reveals that NTP optimization implicitly guides models to encode concepts via
singular value decomposition (SVD) factors of a centered data-sparsity matrix
that captures next-word co-occurrence patterns. While the model never
explicitly constructs this matrix, learned word and context embeddings
effectively factor it to capture linguistic structure. We find that the most
important SVD factors are learned first during training, motivating the use of
spectral clustering of embeddings to identify human-interpretable semantics,
including both classical k-means and a new orthant-based method directly
motivated by our interpretation of concepts. Overall, our work bridges
distributional semantics, neural collapse geometry, and neural network training
dynamics, providing insights into how NTP's implicit biases shape the emergence
of meaning representations in language models.

</details>


### [43] [Alignment Drift in CEFR-prompted LLMs for Interactive Spanish Tutoring](https://arxiv.org/pdf/2505.08351)
*Mina Almasi, Ross Deans Kristensen-McLachlan*

Main category: cs.CL

TL;DR: The paper explores using LLMs as adaptive tutors for second-language learning, testing if system prompting can control text difficulty. Findings show prompting is effective but brittle for long-term use, introducing the concept of alignment drift.


<details>
  <summary>Details</summary>
Motivation: To assess the feasibility of LLMs as personalized, proficiency-aligned tutors in second-language learning.

Method: Simulated teacher-student dialogues in Spanish using instruction-tuned LLMs (7B-12B parameters), evaluating CEFR-based prompting across proficiency levels (A1, B1, C1).

Result: System prompting can constrain outputs but is too brittle for sustained interactions (alignment drift).

Conclusion: LLMs show promise for adaptive tutoring, but prompting alone is insufficient for long-term use; the study offers a scalable evaluation method.

Abstract: This paper investigates the potentials of Large Language Models (LLMs) as
adaptive tutors in the context of second-language learning. In particular, we
evaluate whether system prompting can reliably constrain LLMs to generate only
text appropriate to the student's competence level. We simulate full
teacher-student dialogues in Spanish using instruction-tuned, open-source LLMs
ranging in size from 7B to 12B parameters. Dialogues are generated by having an
LLM alternate between tutor and student roles with separate chat histories. The
output from the tutor model is then used to evaluate the effectiveness of
CEFR-based prompting to control text difficulty across three proficiency levels
(A1, B1, C1). Our findings suggest that while system prompting can be used to
constrain model outputs, prompting alone is too brittle for sustained,
long-term interactional contexts - a phenomenon we term alignment drift. Our
results provide insights into the feasibility of LLMs for personalized,
proficiency-aligned adaptive tutors and provide a scalable method for low-cost
evaluation of model performance without human participants.

</details>


### [44] [Towards Contamination Resistant Benchmarks](https://arxiv.org/pdf/2505.08389)
*Rahmatullah Musawi, Sheng Lu*

Main category: cs.CL

TL;DR: The paper introduces contamination resistance in LLM evaluation using a Caesar cipher benchmark, revealing LLMs' struggles and questioning their true capabilities.


<details>
  <summary>Details</summary>
Motivation: Addressing contamination in LLM evaluations to ensure reliability and understand LLMs' true potential and limitations.

Method: Proposing a contamination-resistant benchmark based on Caesar ciphers and testing it on widely used LLMs under controlled settings.

Result: LLMs struggle with the benchmark when contamination is controlled, highlighting issues in their capabilities.

Conclusion: The work advances contamination-resistant benchmarks, enabling rigorous LLM evaluation and deeper insights into their limitations.

Abstract: The rapid development of large language models (LLMs) has transformed the
landscape of natural language processing. Evaluating LLMs properly is crucial
for understanding their potential and addressing concerns such as safety.
However, LLM evaluation is confronted by various factors, among which
contamination stands out as a key issue that undermines the reliability of
evaluations. In this work, we introduce the concept of contamination resistance
to address this challenge. We propose a benchmark based on Caesar ciphers
(e.g., "ab" to "bc" when the shift is 1), which, despite its simplicity, is an
excellent example of a contamination resistant benchmark. We test this
benchmark on widely used LLMs under various settings, and we find that these
models struggle with this benchmark when contamination is controlled. Our
findings reveal issues in current LLMs and raise important questions regarding
their true capabilities. Our work contributes to the development of
contamination resistant benchmarks, enabling more rigorous LLM evaluation and
offering insights into the true capabilities and limitations of LLMs.

</details>


### [45] [Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance Meets Dynamic Skipping](https://arxiv.org/pdf/2505.08392)
*Ren Zhuang, Ben Wang, Shuifa Sun*

Main category: cs.CL

TL;DR: Adaptive GoGI-Skip is a novel framework for dynamic CoT compression, improving efficiency and accuracy in large language models.


<details>
  <summary>Details</summary>
Motivation: Current CoT compression methods are inefficient and may remove critical tokens, leading to suboptimal performance.

Method: Proposes Goal-Gradient Importance (GoGI) and Adaptive Dynamic Skipping (ADS) for dynamic compression.

Result: Reduces CoT tokens by 45%, speeds up inference 1.6-2.0x, and maintains high accuracy.

Conclusion: Advances CoT reasoning efficiency-accuracy trade-off, outperforming existing baselines.

Abstract: Large Language Models leverage Chain-of-Thought (CoT) prompting for complex
tasks, but their reasoning traces are often excessively verbose and
inefficient, leading to significant computational costs and latency. Current
CoT compression techniques typically rely on generic importance metrics and
static compression rates, which may inadvertently remove functionally critical
tokens or fail to adapt to varying reasoning complexity. To overcome these
limitations, we propose Adaptive GoGI-Skip, a novel framework learning dynamic
CoT compression via supervised fine-tuning. This approach introduces two
synergistic innovations: (1) Goal-Gradient Importance (GoGI), a novel metric
accurately identifying functionally relevant tokens by measuring the gradient
influence of their intermediate representations on the final answer loss, and
(2) Adaptive Dynamic Skipping (ADS), a mechanism dynamically regulating the
compression rate based on runtime model uncertainty while ensuring local
coherence through an adaptive N-token constraint. To our knowledge, this is the
first work unifying a goal-oriented, gradient-based importance metric with
dynamic, uncertainty-aware skipping for CoT compression. Trained on compressed
MATH data, Adaptive GoGI-Skip demonstrates strong cross-domain generalization
across diverse reasoning benchmarks including AIME, GPQA, and GSM8K. It
achieves substantial efficiency gains - reducing CoT token counts by over 45%
on average and delivering 1.6-2.0 times inference speedups - while maintaining
high reasoning accuracy. Notably, it significantly outperforms existing
baselines by preserving accuracy even at high effective compression rates,
advancing the state of the art in the CoT reasoning efficiency-accuracy
trade-off.

</details>


### [46] [TUMS: Enhancing Tool-use Abilities of LLMs with Multi-structure Handlers](https://arxiv.org/pdf/2505.08402)
*Aiyao He, Sijia Cui, Shuai Xu, Yanna Wang, Bo Xu*

Main category: cs.CL

TL;DR: TUMS enhances LLMs' tool-use by shifting from tool-level to parameter-level processing, improving accuracy in generating parameters for tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with non-executable or improper actions due to incorrect parameters, often from coarse-grained tool-level strategies.

Method: TUMS framework includes intent recognition, task decomposition, subtask processing with multi-structure handlers, and execution.

Result: TUMS improves performance by 19.6% on easy and 50.6% on hard ToolQA benchmarks, with ablation studies validating each component.

Conclusion: TUMS effectively addresses parameter generation challenges, advancing tool-augmented LLMs and inspiring future research.

Abstract: Recently, large language models(LLMs) have played an increasingly important
role in solving a wide range of NLP tasks, leveraging their capabilities of
natural language understanding and generating. Integration with external tools
further enhances LLMs' effectiveness, providing more precise, timely, and
specialized responses. However, LLMs still encounter difficulties with
non-executable actions and improper actions, which are primarily attributed to
incorrect parameters. The process of generating parameters by LLMs is confined
to the tool level, employing the coarse-grained strategy without considering
the different difficulties of various tools. To address this issue, we propose
TUMS, a novel framework designed to enhance the tool-use capabilities of LLMs
by transforming tool-level processing into parameter-level processing.
Specifically, our framework consists of four key components: (1) an intent
recognizer that identifies the user's intent to help LLMs better understand the
task; (2) a task decomposer that breaks down complex tasks into simpler
subtasks, each involving a tool call; (3) a subtask processor equipped with
multi-structure handlers to generate accurate parameters; and (4) an executor.
Our empirical studies have evidenced the effectiveness and efficiency of the
TUMS framework with an average of 19.6\% and 50.6\% improvement separately on
easy and hard benchmarks of ToolQA, meanwhile, we demonstrated the key
contribution of each part with ablation experiments, offering more insights and
stimulating future research on Tool-augmented LLMs.

</details>


### [47] [Hakim: Farsi Text Embedding Model](https://arxiv.org/pdf/2505.08435)
*Mehran Sarmadi, Morteza Alikhani, Erfan Zinvandi, Zahra Pourbahman*

Main category: cs.CL

TL;DR: Hakim is a new Persian text embedding model with 8.5% better performance on FaMTEB, introducing new datasets and a BERT-based baseline for Persian NLP tasks.


<details>
  <summary>Details</summary>
Motivation: Persian is underrepresented in text embedding research, and Hakim aims to address this gap.

Method: Developed Hakim, a state-of-the-art Persian text embedding model, and introduced three new datasets (Corpesia, Pairsia-sup, Pairsia-unsup) for training. Also proposed a BERT-based baseline model.

Result: Hakim outperforms existing models by 8.5% on FaMTEB and excels in retrieval tasks for chatbots and RAG systems.

Conclusion: Hakim and the new datasets provide a foundation for advancing Persian language understanding and NLP applications.

Abstract: Recent advancements in text embedding have significantly improved natural
language understanding across many languages, yet Persian remains notably
underrepresented in large-scale embedding research. In this paper, we present
Hakim, a novel state-of-the-art Persian text embedding model that achieves a
8.5% performance improvement over existing approaches on the FaMTEB benchmark,
outperforming all previously developed Persian language models. As part of this
work, we introduce three new datasets - Corpesia, Pairsia-sup, and
Pairsia-unsup - to support supervised and unsupervised training scenarios.
Additionally, Hakim is designed for applications in chatbots and
retrieval-augmented generation (RAG) systems, particularly addressing retrieval
tasks that require incorporating message history within these systems. We also
propose a new baseline model built on the BERT architecture. Our language model
consistently achieves higher accuracy across various Persian NLP tasks, while
the RetroMAE-based model proves particularly effective for textual information
retrieval applications. Together, these contributions establish a new
foundation for advancing Persian language understanding.

</details>


### [48] [A document processing pipeline for the construction of a dataset for topic modeling based on the judgments of the Italian Supreme Court](https://arxiv.org/pdf/2505.08439)
*Matteo Marulli, Glauco Panattoni, Marco Bertini*

Main category: cs.CL

TL;DR: A pipeline for topic modeling in Italian legal research was developed, integrating document layout analysis, OCR, and anonymization, outperforming OCR-only methods and improving topic modeling metrics.


<details>
  <summary>Details</summary>
Motivation: The lack of public datasets for Italian legal research limits theme analysis in Supreme Court judgments.

Method: The pipeline combines document layout analysis (YOLOv8x), OCR, and text anonymization, evaluated using BERTopic and large language models for labeling and summarization.

Result: High performance metrics (e.g., mAP@50 of 0.964 for DLA, CER of 0.0047 for OCR) and improved topic modeling scores (diversity: 0.6198, coherence: 0.6638). Claude Sonnet 3.7 achieved high BERTScore F1 for labeling (0.8119) and summarization (0.9130).

Conclusion: The pipeline effectively addresses dataset scarcity and enhances topic modeling in Italian legal research, validated by expert evaluations.

Abstract: Topic modeling in Italian legal research is hindered by the lack of public
datasets, limiting the analysis of legal themes in Supreme Court judgments. To
address this, we developed a document processing pipeline that produces an
anonymized dataset optimized for topic modeling.
  The pipeline integrates document layout analysis (YOLOv8x), optical character
recognition, and text anonymization. The DLA module achieved a mAP@50 of 0.964
and a mAP@50-95 of 0.800. The OCR detector reached a mAP@50-95 of 0.9022, and
the text recognizer (TrOCR) obtained a character error rate of 0.0047 and a
word error rate of 0.0248. Compared to OCR-only methods, our dataset improved
topic modeling with a diversity score of 0.6198 and a coherence score of
0.6638.
  We applied BERTopic to extract topics and used large language models to
generate labels and summaries. Outputs were evaluated against domain expert
interpretations. Claude Sonnet 3.7 achieved a BERTScore F1 of 0.8119 for
labeling and 0.9130 for summarization.

</details>


### [49] [IterKey: Iterative Keyword Generation with LLMs for Enhanced Retrieval Augmented Generation](https://arxiv.org/pdf/2505.08450)
*Kazuki Hayashi, Hidetaka Kamigaito, Shinya Kouda, Taro Watanabe*

Main category: cs.CL

TL;DR: IterKey is an LLM-driven iterative keyword generation framework enhancing RAG via sparse retrieval, balancing accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between accuracy (dense retrieval) and interpretability (sparse retrieval) in RAG systems.

Method: Three-stage LLM-driven process: keyword generation, answer generation, and validation, iterating with refined keywords if validation fails.

Result: 5% to 20% accuracy improvements over BM25-based RAG, comparable to dense retrieval methods.

Conclusion: IterKey effectively balances accuracy and interpretability in RAG systems.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a way to complement the
in-context knowledge of Large Language Models (LLMs) by integrating external
documents. However, real-world applications demand not only accuracy but also
interpretability. While dense retrieval methods provide high accuracy, they
lack interpretability; conversely, sparse retrieval methods offer transparency
but often fail to capture the full intent of queries due to their reliance on
keyword matching. To address these issues, we introduce IterKey, an LLM-driven
iterative keyword generation framework that enhances RAG via sparse retrieval.
IterKey consists of three LLM-driven stages: generating keywords for retrieval,
generating answers based on retrieved documents, and validating the answers. If
validation fails, the process iteratively repeats with refined keywords. Across
four QA tasks, experimental results show that IterKey achieves 5% to 20%
accuracy improvements over BM25-based RAG and simple baselines. Its performance
is comparable to dense retrieval-based RAG and prior iterative query refinement
methods using dense models. In summary, IterKey is a novel BM25-based approach
leveraging LLMs to iteratively refine RAG, effectively balancing accuracy with
interpretability.

</details>


### [50] [RepCali: High Efficient Fine-tuning Via Representation Calibration in Latent Space for Pre-trained Language Models](https://arxiv.org/pdf/2505.08463)
*Fujun Zhang, XiangDong Su*

Main category: cs.CL

TL;DR: RepCali improves PLM performance by calibrating encoder-decoder representations in the latent space, outperforming fine-tuning baselines.


<details>
  <summary>Details</summary>
Motivation: Address discrepancies between PLM encoder representations and decoder inputs to enhance downstream task performance.

Method: Integrates a calibration block in the latent space post-encoder, using calibrated output for the decoder.

Result: Significant performance improvements across 25 PLM-based models and 8 tasks, surpassing fine-tuning baselines.

Conclusion: RepCali is a universal, plug-and-play solution for enhancing PLMs with encoder-decoder architectures.

Abstract: Fine-tuning pre-trained language models (PLMs) has become a dominant paradigm
in applying PLMs to downstream tasks. However, with limited fine-tuning, PLMs
still struggle with the discrepancies between the representation obtained from
the PLMs' encoder and the optimal input to the PLMs' decoder. This paper
tackles this challenge by learning to calibrate the representation of PLMs in
the latent space. In the proposed representation calibration method (RepCali),
we integrate a specific calibration block to the latent space after the encoder
and use the calibrated output as the decoder input. The merits of the proposed
RepCali include its universality to all PLMs with encoder-decoder
architectures, its plug-and-play nature, and ease of implementation. Extensive
experiments on 25 PLM-based models across 8 tasks (including both English and
Chinese datasets) demonstrate that the proposed RepCali offers desirable
enhancements to PLMs (including LLMs) and significantly improves the
performance of downstream tasks. Comparison experiments across 4 benchmark
tasks indicate that RepCali is superior to the representative fine-tuning
baselines.

</details>


### [51] [Large Language Models Meet Stance Detection: A Survey of Tasks, Methods, Applications, Challenges and Future Directions](https://arxiv.org/pdf/2505.08464)
*Lata Pangtey, Anukriti Bhatnagar, Shubhi Bansal, Shahid Shafi Dar, Nagendra Kumar*

Main category: cs.CL

TL;DR: A review article systematically analyzes LLM-based stance detection, covering methodologies, datasets, applications, and challenges, while proposing a novel taxonomy and future directions.


<details>
  <summary>Details</summary>
Motivation: Existing surveys lack comprehensive coverage of LLM-based stance detection, prompting a systematic review to bridge this gap.

Method: The review examines LLM advancements in stance detection, categorizing approaches by learning methods, data modalities, and target relationships, and evaluates datasets and performance.

Result: The survey highlights LLM strengths and limitations, key applications, and identifies challenges like implicit stance expression and cultural biases.

Conclusion: Future directions include explainable reasoning and low-resource adaptation, guiding next-generation stance detection systems.

Abstract: Stance detection is essential for understanding subjective content across
various platforms such as social media, news articles, and online reviews.
Recent advances in Large Language Models (LLMs) have revolutionized stance
detection by introducing novel capabilities in contextual understanding,
cross-domain generalization, and multimodal analysis. Despite these
progressions, existing surveys often lack comprehensive coverage of approaches
that specifically leverage LLMs for stance detection. To bridge this critical
gap, our review article conducts a systematic analysis of stance detection,
comprehensively examining recent advancements of LLMs transforming the field,
including foundational concepts, methodologies, datasets, applications, and
emerging challenges. We present a novel taxonomy for LLM-based stance detection
approaches, structured along three key dimensions: 1) learning methods,
including supervised, unsupervised, few-shot, and zero-shot; 2) data
modalities, such as unimodal, multimodal, and hybrid; and 3) target
relationships, encompassing in-target, cross-target, and multi-target
scenarios. Furthermore, we discuss the evaluation techniques and analyze
benchmark datasets and performance trends, highlighting the strengths and
limitations of different architectures. Key applications in misinformation
detection, political analysis, public health monitoring, and social media
moderation are discussed. Finally, we identify critical challenges such as
implicit stance expression, cultural biases, and computational constraints,
while outlining promising future directions, including explainable stance
reasoning, low-resource adaptation, and real-time deployment frameworks. Our
survey highlights emerging trends, open challenges, and future directions to
guide researchers and practitioners in developing next-generation stance
detection systems powered by large language models.

</details>


### [52] [Judging the Judges: Can Large Vision-Language Models Fairly Evaluate Chart Comprehension and Reasoning?](https://arxiv.org/pdf/2505.08468)
*Md Tahmid Rahman Laskar, Mohammed Saidul Islam, Ridwan Mahbub, Ahmed Masry, Mizanur Rahman, Amran Bhuiyan, Mir Tafseer Nayeem, Shafiq Joty, Enamul Hoque, Jimmy Huang*

Main category: cs.CL

TL;DR: The paper evaluates 13 open-source LVLMs as judges for chart comprehension tasks, comparing their performance to GPT-4 and identifying biases.


<details>
  <summary>Details</summary>
Motivation: High costs and challenges in evaluating LVLMs for chart-related tasks motivate the need for cost-effective, open-source alternatives.

Method: The study designs pairwise and pointwise evaluation tasks, analyzing LVLMs on criteria like factual correctness, format adherence, and biases.

Result: Some LVLMs match GPT-4's performance (~80% agreement), while others perform poorly (~10% agreement). Biases like positional preference persist.

Conclusion: Open-source LVLMs can serve as cost-effective evaluators for chart tasks, though biases need addressing.

Abstract: Charts are ubiquitous as they help people understand and reason with data.
Recently, various downstream tasks, such as chart question answering,
chart2text, and fact-checking, have emerged. Large Vision-Language Models
(LVLMs) show promise in tackling these tasks, but their evaluation is costly
and time-consuming, limiting real-world deployment. While using LVLMs as judges
to assess the chart comprehension capabilities of other LVLMs could streamline
evaluation processes, challenges like proprietary datasets, restricted access
to powerful models, and evaluation costs hinder their adoption in industrial
settings. To this end, we present a comprehensive evaluation of 13 open-source
LVLMs as judges for diverse chart comprehension and reasoning tasks. We design
both pairwise and pointwise evaluation tasks covering criteria like factual
correctness, informativeness, and relevancy. Additionally, we analyze LVLM
judges based on format adherence, positional consistency, length bias, and
instruction-following. We focus on cost-effective LVLMs (<10B parameters)
suitable for both research and commercial use, following a standardized
evaluation protocol and rubric to measure the LVLM judge's accuracy.
Experimental results reveal notable variability: while some open LVLM judges
achieve GPT-4-level evaluation performance (about 80% agreement with GPT-4
judgments), others struggle (below ~10% agreement). Our findings highlight that
state-of-the-art open-source LVLMs can serve as cost-effective automatic
evaluators for chart-related tasks, though biases such as positional preference
and length bias persist.

</details>


### [53] [LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using Large Language Models](https://arxiv.org/pdf/2505.08498)
*Takumi Shibata, Yuichi Miyamura*

Main category: cs.CL

TL;DR: The paper proposes LLM-based Comparative Essay Scoring (LCES), a zero-shot method for automated essay scoring using pairwise comparisons to improve accuracy and consistency over direct scoring.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot AES methods using LLMs often produce biased or inconsistent scores compared to human evaluations, necessitating a more reliable approach.

Method: LCES formulates AES as a pairwise comparison task, using LLMs to judge essay pairs and converting these comparisons into continuous scores with RankNet for scalability.

Result: LCES outperforms conventional zero-shot methods in accuracy and maintains computational efficiency, showing robustness across different LLM models.

Conclusion: LCES offers a scalable, accurate, and robust solution for zero-shot AES, addressing limitations of direct scoring methods.

Abstract: Recent advances in large language models (LLMs) have enabled zero-shot
automated essay scoring (AES), providing a promising way to reduce the cost and
effort of essay scoring in comparison with manual grading. However, most
existing zero-shot approaches rely on LLMs to directly generate absolute
scores, which often diverge from human evaluations owing to model biases and
inconsistent scoring. To address these limitations, we propose LLM-based
Comparative Essay Scoring (LCES), a method that formulates AES as a pairwise
comparison task. Specifically, we instruct LLMs to judge which of two essays is
better, collect many such comparisons, and convert them into continuous scores.
Considering that the number of possible comparisons grows quadratically with
the number of essays, we improve scalability by employing RankNet to
efficiently transform LLM preferences into scalar scores. Experiments using AES
benchmark datasets show that LCES outperforms conventional zero-shot methods in
accuracy while maintaining computational efficiency. Moreover, LCES is robust
across different LLM backbones, highlighting its applicability to real-world
zero-shot AES.

</details>


### [54] [Reassessing Graph Linearization for Sequence-to-sequence AMR Parsing: On the Advantages and Limitations of Triple-Based Encoding](https://arxiv.org/pdf/2505.08504)
*Jeongwoo Kang, Maximin Coavoux, Cédric Lopez, Didier Schwab*

Main category: cs.CL

TL;DR: The paper proposes a triple-based linearization method for AMR graphs, addressing limitations of Penman encoding, but finds room for improvement in competing with Penman's concise nested structure representation.


<details>
  <summary>Details</summary>
Motivation: Penman encoding for AMR graph linearization has limitations, such as distant node placement for deep graphs and doubled relation types due to inverse roles.

Method: A triple-based linearization method is proposed and compared with Penman encoding.

Result: Triples are suitable for graph representation but need improvement to match Penman's concise nested structure.

Conclusion: The triple-based method addresses Penman's issues but requires further refinement to outperform its concise representation.

Abstract: Sequence-to-sequence models are widely used to train Abstract Meaning
Representation (Banarescu et al., 2013, AMR) parsers. To train such models, AMR
graphs have to be linearized into a one-line text format. While Penman encoding
is typically used for this purpose, we argue that it has limitations: (1) for
deep graphs, some closely related nodes are located far apart in the linearized
text (2) Penman's tree-based encoding necessitates inverse roles to handle node
re-entrancy, doubling the number of relation types to predict. To address these
issues, we propose a triple-based linearization method and compare its
efficiency with Penman linearization. Although triples are well suited to
represent a graph, our results suggest room for improvement in triple encoding
to better compete with Penman's concise and explicit representation of a nested
graph structure.

</details>


### [55] [Are We Paying Attention to Her? Investigating Gender Disambiguation and Attention in Machine Translation](https://arxiv.org/pdf/2505.08546)
*Chiara Manna, Afra Alishahi, Frédéric Blain, Eva Vanmassenhove*

Main category: cs.CL

TL;DR: The paper introduces Minimal Pair Accuracy (MPA) to evaluate gender bias in NMT systems, showing models often ignore gender cues and rely on stereotypes, with masculine cues being more consistently recognized than feminine ones.


<details>
  <summary>Details</summary>
Motivation: Address the lack of metrics capturing contextual gender bias in NMT systems.

Method: Propose MPA to measure reliance on gender cues in minimal pairs and evaluate NMT models on EN-IT translations.

Result: Models ignore gender cues, favor stereotypes, and respond more to masculine cues than feminine ones.

Conclusion: MPA reveals deeper gender bias in NMT systems, highlighting the need for better evaluation and model improvements.

Abstract: While gender bias in modern Neural Machine Translation (NMT) systems has
received much attention, traditional evaluation metrics do not to fully capture
the extent to which these systems integrate contextual gender cues. We propose
a novel evaluation metric called Minimal Pair Accuracy (MPA), which measures
the reliance of models on gender cues for gender disambiguation. MPA is
designed to go beyond surface-level gender accuracy metrics by focusing on
whether models adapt to gender cues in minimal pairs -- sentence pairs that
differ solely in the gendered pronoun, namely the explicit indicator of the
target's entity gender in the source language (EN). We evaluate a number of NMT
models on the English-Italian (EN--IT) language pair using this metric, we show
that they ignore available gender cues in most cases in favor of (statistical)
stereotypical gender interpretation. We further show that in anti-stereotypical
cases, these models tend to more consistently take masculine gender cues into
account while ignoring the feminine cues. Furthermore, we analyze the attention
head weights in the encoder component and show that while all models encode
gender information to some extent, masculine cues elicit a more diffused
response compared to the more concentrated and specialized responses to
feminine gender cues.

</details>


### [56] [Small but Significant: On the Promise of Small Language Models for Accessible AIED](https://arxiv.org/pdf/2505.08588)
*Yumou Wei, Paulo Carvalho, John Stamper*

Main category: cs.CL

TL;DR: The paper highlights the dominance of GPT and large language models (LLMs) in AIED research, noting their potential but also the overlooked benefits of small language models (SLMs) for equitable access.


<details>
  <summary>Details</summary>
Motivation: To address the overemphasis on resource-intensive LLMs like GPT in AIED, advocating for SLMs to provide affordable, high-quality AI tools for resource-constrained institutions.

Method: Demonstrates the effectiveness of SLMs (e.g., Phi-2) in solving AIED challenges like knowledge component (KC) discovery without complex prompting.

Result: SLMs show promise in delivering efficient solutions, suggesting their viability as alternatives to LLMs in AIED.

Conclusion: Calls for increased focus on developing SLM-based approaches in AIED to ensure equitable access and affordability.

Abstract: GPT has become nearly synonymous with large language models (LLMs), an
increasingly popular term in AIED proceedings. A simple keyword-based search
reveals that 61% of the 76 long and short papers presented at AIED 2024
describe novel solutions using LLMs to address some of the long-standing
challenges in education, and 43% specifically mention GPT. Although LLMs
pioneered by GPT create exciting opportunities to strengthen the impact of AI
on education, we argue that the field's predominant focus on GPT and other
resource-intensive LLMs (with more than 10B parameters) risks neglecting the
potential impact that small language models (SLMs) can make in providing
resource-constrained institutions with equitable and affordable access to
high-quality AI tools. Supported by positive results on knowledge component
(KC) discovery, a critical challenge in AIED, we demonstrate that SLMs such as
Phi-2 can produce an effective solution without elaborate prompting strategies.
Hence, we call for more attention to developing SLM-based AIED approaches.

</details>


### [57] [Enhancing Thyroid Cytology Diagnosis with RAG-Optimized LLMs and Pa-thology Foundation Models](https://arxiv.org/pdf/2505.08590)
*Hussien Al-Asi, Jordan P Reynolds, Shweta Agarwal, Bryan J Dangott, Aziza Nassar, Zeynettin Akkus*

Main category: cs.CL

TL;DR: The paper explores using RAG-enhanced LLMs and pathology foundation models to improve thyroid cytology diagnosis, enhancing accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Address challenges in cytological interpretation, standardization, and diagnostic accuracy in thyroid cytology.

Method: Integrate RAG with LLMs for dynamic knowledge retrieval and pathology foundation models for image-based feature extraction.

Result: Significant improvement in diagnostic efficiency, with foundation model UNI achieving AUC 0.73-0.93 for correct predictions.

Conclusion: The fusion of RAG and pathology-specific AI models enhances diagnostic consistency and supports pathologists, paving the way for AI-assisted cytopathology.

Abstract: Advancements in artificial intelligence (AI) are transforming pathology by
integrat-ing large language models (LLMs) with retrieval-augmented generation
(RAG) and domain-specific foundation models. This study explores the
application of RAG-enhanced LLMs coupled with pathology foundation models for
thyroid cytology diagnosis, addressing challenges in cytological
interpretation, standardization, and diagnostic accuracy. By leveraging a
curated knowledge base, RAG facilitates dy-namic retrieval of relevant case
studies, diagnostic criteria, and expert interpreta-tion, improving the
contextual understanding of LLMs. Meanwhile, pathology foun-dation models,
trained on high-resolution pathology images, refine feature extrac-tion and
classification capabilities. The fusion of these AI-driven approaches en-hances
diagnostic consistency, reduces variability, and supports pathologists in
dis-tinguishing benign from malignant thyroid lesions. Our results demonstrate
that integrating RAG with pathology-specific LLMs significantly improves
diagnostic efficiency and interpretability, paving the way for AI-assisted
thyroid cytopathology, with foundation model UNI achieving AUC 0.73-0.93 for
correct prediction of surgi-cal pathology diagnosis from thyroid cytology
samples.

</details>


### [58] [Automatic Task Detection and Heterogeneous LLM Speculative Decoding](https://arxiv.org/pdf/2505.08600)
*Danying Ge, Jianhua Gao, Qizhi Jiang, Yifei Feng, Weixing Ji*

Main category: cs.CL

TL;DR: A new speculative decoding algorithm optimizes downstream tasks by partitioning them and using heterogeneous draft models, improving accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Existing speculative decoding methods struggle with balancing acceptance rates and decoding speed due to draft model limitations.

Method: Proposes task partitioning, heterogeneous draft models aligned with task-specific data, and a prompt classifier for dynamic routing.

Result: Improves draft accuracy by 6%-50% and achieves 1.10x-2.64x speedup in LLM inference.

Conclusion: The method enhances efficiency and consistency in downstream tasks for LLM inference.

Abstract: Speculative decoding, which combines a draft model with a target model, has
emerged as an effective approach to accelerate large language model (LLM)
inference. However, existing methods often face a trade-off between the
acceptance rate and decoding speed in downstream tasks due to the limited
capacity of the draft model, making it difficult to ensure efficiency across
diverse tasks. To address this problem, we propose a speculative decoding
algorithm tailored for downstream task optimization. It includes an automatic
task partitioning and assigning method, which automatically categorizes
downstream tasks into different sub-tasks and assigns them to a set of
heterogeneous draft models. Each draft model is aligned with the target model
using task-specific data, thereby enhancing the consistency of inference
results. In addition, our proposed method incorporates an online lightweight
prompt classifier to dynamically route prompts to the appropriate draft model.
Experimental results demonstrate that the proposed method improves draft
accuracy by 6% to 50% over vanilla speculative decoding, while achieving a
speedup of 1.10x to 2.64x in LLM inference.

</details>


### [59] [Scaling Context, Not Parameters: Training a Compact 7B Language Model for Efficient Long-Context Processing](https://arxiv.org/pdf/2505.08651)
*Chen Wu, Yin Song*

Main category: cs.CL

TL;DR: MegaBeam-Mistral-7B is a 7B-parameter language model supporting 512K-token context length, excelling in long-context tasks like compliance monitoring and verification. It outperforms on benchmarks like HELMET and RULER and is the only open model achieving competitive performance on BABILong at 512K without RAG or fine-tuning. Open-sourced under Apache 2.0, it's widely downloaded.


<details>
  <summary>Details</summary>
Motivation: Addressing practical limitations in long-context training to support real-world tasks such as compliance monitoring and verification.

Method: Developed a 7B-parameter language model (MegaBeam-Mistral-7B) supporting 512K-token context length, evaluated on HELMET, RULER, and BABILong benchmarks.

Result: Superior in-context learning on HELMET, robust retrieval on RULER, and competitive long-range reasoning on BABILong at 512K context length without RAG or fine-tuning.

Conclusion: MegaBeam-Mistral-7B is a high-performing, open-source model for long-context tasks, widely adopted with over 100,000 downloads.

Abstract: We present MegaBeam-Mistral-7B, a language model that supports 512K-token
context length. Our work addresses practical limitations in long-context
training, supporting real-world tasks such as compliance monitoring and
verification. Evaluated on three long-context benchmarks, our 7B-parameter
model demonstrates superior in-context learning performance on HELMET and
robust retrieval and tracing capability on RULER. It is currently the only open
model to achieve competitive long-range reasoning on BABILong at 512K context
length without RAG or targeted fine-tuning. Released as fully open source under
the Apache 2.0 license, the model has been downloaded over 100,000 times on
Hugging Face. Model available at:
https://huggingface.co/aws-prototyping/MegaBeam-Mistral-7B-512k

</details>


### [60] [Revealing economic facts: LLMs know more than they say](https://arxiv.org/pdf/2505.08662)
*Marcus Buckmann, Quynh Anh Nguyen, Edward Hill*

Main category: cs.CL

TL;DR: Hidden states of LLMs can estimate economic/financial stats better than text outputs, using simple linear models and minimal labeled data.


<details>
  <summary>Details</summary>
Motivation: To explore if hidden states of LLMs capture richer economic information than their direct text outputs.

Method: Train linear models on hidden states of LLMs for county/firm-level variables; propose transfer learning for accuracy.

Result: Hidden states outperform text outputs; few labeled examples suffice; transfer learning improves accuracy.

Conclusion: Hidden states are valuable for economic estimation, super-resolution, and data imputation tasks.

Abstract: We investigate whether the hidden states of large language models (LLMs) can
be used to estimate and impute economic and financial statistics. Focusing on
county-level (e.g. unemployment) and firm-level (e.g. total assets) variables,
we show that a simple linear model trained on the hidden states of open-source
LLMs outperforms the models' text outputs. This suggests that hidden states
capture richer economic information than the responses of the LLMs reveal
directly. A learning curve analysis indicates that only a few dozen labelled
examples are sufficient for training. We also propose a transfer learning
method that improves estimation accuracy without requiring any labelled data
for the target variable. Finally, we demonstrate the practical utility of
hidden-state representations in super-resolution and data imputation tasks.

</details>


### [61] [Adaptive Schema-aware Event Extraction with Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.08690)
*Sheng Liang, Hang Lv, Zhihao Wen, Yaxiong Wu, Yongyue Zhang, Hao Wang, Yong Liu*

Main category: cs.CL

TL;DR: ASEE introduces a novel paradigm for event extraction by combining schema paraphrasing and retrieval-augmented generation, addressing gaps in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing event extraction systems lack flexibility in schema selection and joint evaluation benchmarks, while LLMs face schema hallucination and context limitations.

Method: ASEE integrates schema paraphrasing with retrieval-augmented generation to adaptively retrieve and generate schemas.

Result: ASEE improves event extraction accuracy and adaptability across diverse scenarios, as validated on the MD-SEE benchmark.

Conclusion: ASEE offers a robust solution for schema-aware event extraction, outperforming existing methods and addressing practical challenges.

Abstract: Event extraction (EE) is a fundamental task in natural language processing
(NLP) that involves identifying and extracting event information from
unstructured text. Effective EE in real-world scenarios requires two key steps:
selecting appropriate schemas from hundreds of candidates and executing the
extraction process. Existing research exhibits two critical gaps: (1) the rigid
schema fixation in existing pipeline systems, and (2) the absence of benchmarks
for evaluating joint schema matching and extraction. Although large language
models (LLMs) offer potential solutions, their schema hallucination tendencies
and context window limitations pose challenges for practical deployment. In
response, we propose Adaptive Schema-aware Event Extraction (ASEE), a novel
paradigm combining schema paraphrasing with schema retrieval-augmented
generation. ASEE adeptly retrieves paraphrased schemas and accurately generates
targeted structures. To facilitate rigorous evaluation, we construct the
Multi-Dimensional Schema-aware Event Extraction (MD-SEE) benchmark, which
systematically consolidates 12 datasets across diverse domains, complexity
levels, and language settings. Extensive evaluations on MD-SEE show that our
proposed ASEE demonstrates strong adaptability across various scenarios,
significantly improving the accuracy of event extraction.

</details>


### [62] [NurValues: Real-World Nursing Values Evaluation for Large Language Models in Clinical Context](https://arxiv.org/pdf/2505.08734)
*Ben Yao, Qiuchi Li, Yazhou Zhang, Siyu Yang, Bohan Zhang, Prayag Tiwari, Jing Qin*

Main category: cs.CL

TL;DR: A benchmark for nursing value alignment is introduced, featuring 1,100 real-world nursing behaviors annotated and augmented with counterfactuals, evaluated on 23 LLMs. DeepSeek-V3 and Claude 3.5 Sonnet perform best, with Justice being the hardest value to evaluate.


<details>
  <summary>Details</summary>
Motivation: To establish a foundation for value-sensitive LLM development in clinical settings by evaluating alignment with nursing core values.

Method: Collected 1,100 nursing behaviors, annotated and augmented with counterfactuals, creating Easy- and Hard-Level datasets. Evaluated 23 LLMs on value alignment.

Result: DeepSeek-V3 excels on Easy-Level (94.55), Claude 3.5 Sonnet on Hard-Level (89.43). Justice is the toughest value. In-context learning boosts alignment.

Conclusion: The benchmark aids in developing value-sensitive LLMs for nursing, with datasets and code publicly available.

Abstract: This work introduces the first benchmark for nursing value alignment,
consisting of five core value dimensions distilled from international nursing
codes: Altruism, Human Dignity, Integrity, Justice, and Professionalism. The
benchmark comprises 1,100 real-world nursing behavior instances collected
through a five-month longitudinal field study across three hospitals of varying
tiers. These instances are annotated by five clinical nurses and then augmented
with LLM-generated counterfactuals with reversed ethic polarity. Each original
case is paired with a value-aligned and a value-violating version, resulting in
2,200 labeled instances that constitute the Easy-Level dataset. To increase
adversarial complexity, each instance is further transformed into a
dialogue-based format that embeds contextual cues and subtle misleading
signals, yielding a Hard-Level dataset. We evaluate 23 state-of-the-art (SoTA)
LLMs on their alignment with nursing values. Our findings reveal three key
insights: (1) DeepSeek-V3 achieves the highest performance on the Easy-Level
dataset (94.55), where Claude 3.5 Sonnet outperforms other models on the
Hard-Level dataset (89.43), significantly surpassing the medical LLMs; (2)
Justice is consistently the most difficult nursing value dimension to evaluate;
and (3) in-context learning significantly improves alignment. This work aims to
provide a foundation for value-sensitive LLMs development in clinical settings.
The dataset and the code are available at
https://huggingface.co/datasets/Ben012345/NurValues.

</details>


### [63] [Probability Consistency in Large Language Models: Theoretical Foundations Meet Empirical Discrepancies](https://arxiv.org/pdf/2505.08739)
*Xiaoliang Luo, Xinyi Xu, Michael Ramscar, Bradley C. Love*

Main category: cs.CL

TL;DR: Autoregressive LLMs' sequence perplexity is invariant under any factorization, but empirical deviations occur due to positional biases.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs learn consistent probability distributions across different token orders and address flaws in prior studies.

Method: Formal proof of perplexity invariance, retraining GPT-2 models in forward, backward, and permuted orders, and analyzing self-attention biases.

Result: Systematic deviations from theoretical invariance, with arbitrary permutations showing strong deviations and forward/backward models largely agreeing.

Conclusion: The study reveals positional biases in LLMs, offering insights for detecting inconsistent probability distributions and improving trustworthiness.

Abstract: Can autoregressive large language models (LLMs) learn consistent probability
distributions when trained on sequences in different token orders? We prove
formally that for any well-defined probability distribution, sequence
perplexity is invariant under any factorization, including forward, backward,
or arbitrary permutations. This result establishes a rigorous theoretical
foundation for studying how LLMs learn from data and defines principled
protocols for empirical evaluation. Applying these protocols, we show that
prior studies examining ordering effects suffer from critical methodological
flaws. We retrain GPT-2 models across forward, backward, and arbitrary permuted
orders on scientific text. We find systematic deviations from theoretical
invariance across all orderings with arbitrary permutations strongly deviating
from both forward and backward models, which largely (but not completely)
agreed with one another. Deviations were traceable to differences in
self-attention, reflecting positional and locality biases in processing. Our
theoretical and empirical results provide novel avenues for understanding
positional biases in LLMs and suggest methods for detecting when LLMs'
probability distributions are inconsistent and therefore untrustworthy.

</details>


### [64] [AC-Reason: Towards Theory-Guided Actual Causality Reasoning with Large Language Models](https://arxiv.org/pdf/2505.08750)
*Yanxi Zhang, Xin Cong, Zhong Zhang, Xiao Liu, Dongyan Zhao, Yesai Wu*

Main category: cs.CL

TL;DR: AC-Reason is a semi-formal reasoning framework for actual causality (AC) that improves LLM performance by grounding in formal AC theory, validated on new benchmark AC-Bench.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based methods lack grounding in formal AC theory, limiting interpretability and performance in causal reasoning.

Method: AC-Reason identifies causally relevant events, infers formal causal factors, and answers AC queries via a theory-guided algorithm without constructing explicit causal graphs.

Result: AC-Reason boosts LLM performance, with GPT-4 + AC-Reason achieving 75.04% on BBH-CJ and 71.82% on AC-Bench, surpassing human accuracy.

Conclusion: Integrating AC theory into LLMs is highly effective, with the proposed algorithm driving significant performance gains, though some LLMs exploit shortcuts.

Abstract: Actual causality (AC), a fundamental aspect of causal reasoning (CR), is
responsible for attribution and responsibility assignment in real-world
scenarios. However, existing LLM-based methods lack grounding in formal AC
theory, resulting in limited interpretability. Therefore, we propose AC-Reason,
a semi-formal reasoning framework that identifies causally relevant events
within an AC scenario, infers the values of their formal causal factors (e.g.,
sufficiency, necessity, and normality), and answers AC queries via a
theory-guided algorithm with explanations. While AC-Reason does not explicitly
construct a causal graph, it operates over variables in the underlying causal
structure to support principled reasoning. To enable comprehensive evaluation,
we introduce AC-Bench, a new benchmark built upon and substantially extending
Big-Bench Hard Causal Judgment (BBH-CJ). AC-Bench comprises ~1K carefully
annotated samples, each with detailed reasoning steps and focuses solely on
actual causation. The case study shows that synthesized samples in AC-Bench
present greater challenges for LLMs. Extensive experiments on BBH-CJ and
AC-Bench show that AC-Reason consistently improves LLM performance over
baselines. On BBH-CJ, all tested LLMs surpass the average human rater accuracy
of 69.60%, with GPT-4 + AC-Reason achieving 75.04%. On AC-Bench, GPT-4 +
AC-Reason again achieves the highest accuracy of 71.82%. AC-Bench further
enables fine-grained analysis of reasoning faithfulness, revealing that only
Qwen-2.5-72B-Instruct, Claude-3.5-Sonnet, and GPT-4o exhibit faithful
reasoning, whereas GPT-4 tends to exploit shortcuts. Finally, our ablation
study proves that integrating AC theory into LLMs is highly effective, with the
proposed algorithm contributing the most significant performance gains.

</details>


### [65] [Aya Vision: Advancing the Frontier of Multilingual Multimodality](https://arxiv.org/pdf/2505.08751)
*Saurabh Dash, Yiyang Nan, John Dang, Arash Ahmadian, Shivalika Singh, Madeline Smith, Bharat Venkitesh, Vlad Shmyhlo, Viraat Aryabumi, Walter Beller-Morales, Jeremy Pekmez, Jason Ozuzu, Pierre Richemond, Acyr Locatelli, Nick Frosst, Phil Blunsom, Aidan Gomez, Ivan Zhang, Marzieh Fadaee, Manoj Govindassamy, Sudip Roy, Matthias Gallé, Beyza Ermis, Ahmet Üstün, Sara Hooker*

Main category: cs.CL

TL;DR: The paper introduces Aya-Vision models to address challenges in multilingual multimodal language models, using synthetic annotation and cross-modal merging to achieve top performance.


<details>
  <summary>Details</summary>
Motivation: The challenges include aligning vision and language, curating multilingual multimodal data, and avoiding degradation of text-only capabilities.

Method: Develops a synthetic annotation framework for multilingual data and proposes cross-modal model merging to mitigate forgetting.

Result: Aya-Vision-8B and Aya-Vision-32B outperform larger models like Qwen-2.5-VL-7B, Pixtral-12B, and LLaMA-3.2-90B-Vision.

Conclusion: The work advances multilingual multimodal models and offers insights into efficient high-performance techniques.

Abstract: Building multimodal language models is fundamentally challenging: it requires
aligning vision and language modalities, curating high-quality instruction
data, and avoiding the degradation of existing text-only capabilities once
vision is introduced. These difficulties are further magnified in the
multilingual setting, where the need for multimodal data in different languages
exacerbates existing data scarcity, machine translation often distorts meaning,
and catastrophic forgetting is more pronounced. To address the aforementioned
challenges, we introduce novel techniques spanning both data and modeling.
First, we develop a synthetic annotation framework that curates high-quality,
diverse multilingual multimodal instruction data, enabling Aya Vision models to
produce natural, human-preferred responses to multimodal inputs across many
languages. Complementing this, we propose a cross-modal model merging technique
that mitigates catastrophic forgetting, effectively preserving text-only
capabilities while simultaneously enhancing multimodal generative performance.
Aya-Vision-8B achieves best-in-class performance compared to strong multimodal
models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger
Llama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which
outperforms models more than twice its size, such as Molmo-72B and
LLaMA-3.2-90B-Vision. Our work advances multilingual progress on the
multi-modal frontier, and provides insights into techniques that effectively
bend the need for compute while delivering extremely high performance.

</details>


### [66] [HealthBench: Evaluating Large Language Models Towards Improved Human Health](https://arxiv.org/pdf/2505.08775)
*Rahul K. Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin Quiñonero-Candela, Foivos Tsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, Johannes Heidecke, Karan Singhal*

Main category: cs.CL

TL;DR: HealthBench is an open-source benchmark for evaluating large language models in healthcare using multi-turn conversations and physician-created rubrics. It shows progress in model performance, with smaller models like GPT-4.1 nano outperforming larger ones. Variations like HealthBench Consensus and HealthBench Hard are also introduced.


<details>
  <summary>Details</summary>
Motivation: To provide a realistic, open-ended evaluation tool for large language models in healthcare, addressing gaps in existing benchmarks and grounding progress in model development for health applications.

Method: HealthBench uses 5,000 multi-turn conversations evaluated by 262 physicians via 48,562 unique rubric criteria across health contexts and behavioral dimensions.

Result: Performance improvements are noted, e.g., GPT-4o scores 32%, while GPT-4.1 nano outperforms it and is cheaper. HealthBench Hard's top score is 32%.

Conclusion: HealthBench aims to advance model development and applications benefiting human health, with variations like HealthBench Consensus and HealthBench Hard further refining evaluation.

Abstract: We present HealthBench, an open-source benchmark measuring the performance
and safety of large language models in healthcare. HealthBench consists of
5,000 multi-turn conversations between a model and an individual user or
healthcare professional. Responses are evaluated using conversation-specific
rubrics created by 262 physicians. Unlike previous multiple-choice or
short-answer benchmarks, HealthBench enables realistic, open-ended evaluation
through 48,562 unique rubric criteria spanning several health contexts (e.g.,
emergencies, transforming clinical data, global health) and behavioral
dimensions (e.g., accuracy, instruction following, communication). HealthBench
performance over the last two years reflects steady initial progress (compare
GPT-3.5 Turbo's 16% to GPT-4o's 32%) and more rapid recent improvements (o3
scores 60%). Smaller models have especially improved: GPT-4.1 nano outperforms
GPT-4o and is 25 times cheaper. We additionally release two HealthBench
variations: HealthBench Consensus, which includes 34 particularly important
dimensions of model behavior validated via physician consensus, and HealthBench
Hard, where the current top score is 32%. We hope that HealthBench grounds
progress towards model development and applications that benefit human health.

</details>


### [67] [Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education](https://arxiv.org/pdf/2310.12059)
*Duc-Vu Nguyen, Quoc-Nam Nguyen*

Main category: cs.CL

TL;DR: The paper evaluates large language models (LLMs) for multiple choice symbol binding (MCSB) in Vietnamese MCQA tasks, introducing a new LaTeX-structured dataset for mathematics and sciences.


<details>
  <summary>Details</summary>
Motivation: To address the lack of challenging Vietnamese MCQA datasets and evaluate LLMs' MCSB ability in zero-shot to few-shot settings.

Method: Created a high-quality LaTeX-typed dataset for math and sciences, tested six LLMs (e.g., GPT-3, GPT-4) on existing and new benchmarks.

Result: Promising results on LLMs' MCSB ability for Vietnamese, with the new dataset serving as a robust evaluation tool.

Conclusion: The study highlights LLMs' potential for Vietnamese MCQA and provides a valuable dataset for future research.

Abstract: In this paper, we evaluate the ability of large language models (LLMs) to
perform multiple choice symbol binding (MCSB) for multiple choice question
answering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus
on Vietnamese, with fewer challenging MCQA datasets than in English. The two
existing datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent
research in Vietnamese natural language processing (NLP) has focused on the
Vietnamese National High School Graduation Examination (VNHSGE) from 2019 to
2023 to evaluate ChatGPT. However, these studies have mainly focused on how
ChatGPT solves the VNHSGE step by step. We aim to create a novel and
high-quality dataset by providing structured guidelines for typing LaTeX
formulas for mathematics, physics, chemistry, and biology. This dataset can be
used to evaluate the MCSB ability of LLMs and smaller language models (LMs)
because it is typed in a strict LaTeX style. We focus on predicting the
character (A, B, C, or D) that is the most likely answer to a question, given
the context of the question. Our evaluation of six well-known LLMs, namely
BLOOMZ-7.1B-MT, LLaMA-2-7B, LLaMA-2-70B, GPT-3, GPT-3.5, and GPT-4.0, on the
ViMMRC 1.0 and ViMMRC 2.0 benchmarks and our proposed dataset shows promising
results on the MCSB ability of LLMs for Vietnamese. The dataset is available
for research purposes only.

</details>


### [68] [Bridging LLMs and KGs without Fine-Tuning: Intermediate Probing Meets Subgraph-Aware Entity Descriptions](https://arxiv.org/pdf/2408.06787)
*Bo Xue, Yi Xu, Yunchong Song, Yiming Pang, Yuyang Ren, Jiaxin Ding, Luoyi Fu, Xinbing Wang*

Main category: cs.CL

TL;DR: The paper proposes a method to leverage LLMs for knowledge graph completion (KGC) efficiently by using prompts to capture hidden states and training a classifier, avoiding costly fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Traditional KGC methods struggle with sparsity, and while LLMs offer potential, fine-tuning is resource-intensive. This work aims to use LLMs effectively without high costs.

Method: The approach captures hidden states of knowledge triples via prompts, trains a classifier on these states, and enriches entity descriptions using subgraph sampling.

Result: The method outperforms traditional KGC methods, matches fine-tuned LLM performance, and improves GPU memory efficiency by 188x and speed by 13.48x.

Conclusion: The proposed approach efficiently leverages LLMs for KGC, balancing performance and resource usage.

Abstract: Traditional knowledge graph completion (KGC) methods rely solely on
structural information, struggling with the inherent sparsity of knowledge
graphs (KGs). Large Language Models (LLMs) learn extensive knowledge from large
corpora with powerful context modeling, making them promising for mitigating
the limitations of previous methods. Directly fine-tuning LLMs offers great
capability but comes at the cost of huge time and memory consumption, while
utilizing frozen LLMs yields suboptimal results.In this work, we aim to
leverage LLMs for KGC effectively and efficiently. We capture the context-aware
hidden states of knowledge triples by employing prompts to stimulate the
intermediate layers of LLMs. We then train a data-efficient classifier on these
hidden states to harness the inherent capabilities of frozen LLMs in KGC.
Additionally, to reduce ambiguity and enrich knowledge representation, we
generate detailed entity descriptions through subgraph sampling on KGs.
Extensive experiments on standard benchmarks demonstrate the efficiency and
effectiveness of our approach. We outperform traditional KGC methods across
most datasets and, notably, achieve classification performance comparable to
fine-tuned LLMs while enhancing GPU memory efficiency by $188\times$ and
accelerating training and inference by $13.48\times$.

</details>


### [69] [Studying the Effects of Collaboration in Interactive Theme Discovery Systems](https://arxiv.org/pdf/2408.09030)
*Alvin Po-Chun Chen, Dananjay Srinivas, Alexandra Barry, Maksim Seniw, Maria Leonor Pacheco*

Main category: cs.CL

TL;DR: Proposes an evaluation framework for NLP-assisted qualitative research tools, focusing on collaboration strategies (synchronous vs. asynchronous) and their impact on output quality.


<details>
  <summary>Details</summary>
Motivation: Address the lack of a unified evaluation framework for NLP-assisted qualitative data analysis tools.

Method: Study the impact of synchronous vs. asynchronous collaboration using two NLP tools, analyzing output consistency, cohesiveness, and correctness.

Result: Identifies significant differences in output quality based on collaboration strategy.

Conclusion: The proposed framework helps evaluate NLP tools in qualitative research, highlighting the importance of collaboration strategy.

Abstract: NLP-assisted solutions have gained considerable traction to support
qualitative data analysis. However, there does not exist a unified evaluation
framework that can account for the many different settings in which qualitative
researchers may employ them. In this paper, we take a first step in this
direction by proposing an evaluation framework to study the way in which
different tools may result in different outcomes depending on the collaboration
strategy employed. Specifically, we study the impact of synchronous vs.
asynchronous collaboration using two different NLP-assisted qualitative
research tools and present a comprehensive analysis of significant differences
in the consistency, cohesiveness, and correctness of their outputs.

</details>


### [70] [From Calculation to Adjudication: Examining LLM judges on Mathematical Reasoning Tasks](https://arxiv.org/pdf/2409.04168)
*Andreas Stephan, Dawei Zhu, Matthias Aßenmacher, Xiaoyu Shen, Benjamin Roth*

Main category: cs.CL

TL;DR: LLMs are studied as judges for mathematical reasoning tasks, showing correlation with model performance but limited practical use for improving task performance.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs as judges for mathematical reasoning tasks, where correctness is verifiable, reducing reliance on human annotations.

Method: Performance analysis of LLM judges on mathematical reasoning tasks, evaluating correlation with human judgments and candidate model performance.

Result: LLM judges correlate with model performance but struggle with difficult samples. Simple features predict 70%-75% of judgments.

Conclusion: LLM judges detect better models on average but fail to improve task performance.

Abstract: To reduce the need for human annotations, large language models (LLMs) have
been proposed as judges of the quality of other candidate models. The
performance of LLM judges is typically evaluated by measuring the correlation
with human judgments on generative tasks such as summarization or machine
translation. In contrast, we study LLM judges on mathematical reasoning tasks.
These tasks require multi-step reasoning, and the correctness of their
solutions is verifiable, enabling a more objective evaluation. We perform a
detailed performance analysis and find that easy samples are easy to judge, and
difficult samples are difficult to judge. Our analysis uncovers a strong
correlation between judgment performance and the candidate model task
performance, indicating that judges tend to favor higher-quality models even if
their answer is incorrect. As a consequence, we test whether we can predict the
behavior of LLM judges using simple features such as part-of-speech tags and
find that we can correctly predict 70%-75% of judgments. We conclude this study
by analyzing practical use cases, showing that LLM judges consistently detect
the on-average better model but largely fail if we use them to improve task
performance.

</details>


### [71] [Round and Round We Go! What makes Rotary Positional Encodings useful?](https://arxiv.org/pdf/2410.06205)
*Federico Barbero, Alex Vitvitskyi, Christos Perivolaropoulos, Razvan Pascanu, Petar Veličković*

Main category: cs.CL

TL;DR: The paper challenges the belief that Rotary Positional Encodings (RoPE) in Transformers decay token dependency with distance, showing instead that Gemma 7B uses RoPE for robust positional patterns and semantic info.


<details>
  <summary>Details</summary>
Motivation: To understand the actual role of RoPE in LLMs, debunking the common belief about its function and exploring its mechanical use in models like Gemma 7B.

Method: Analyzed Gemma 7B's use of RoPE, mathematically proved RoPE behaviors, and proposed a modified version to address issues.

Result: Found RoPE is used for positional patterns (high frequencies) and semantic info (low frequencies), with proposed modifications improving performance.

Conclusion: Provides deeper insights into PEs in LLMs, aiding scalability and context length handling.

Abstract: Positional Encodings (PEs) are a critical component of Transformer-based
Large Language Models (LLMs), providing the attention mechanism with important
sequence-position information. One of the most popular types of encoding used
today in LLMs are Rotary Positional Encodings (RoPE), that rotate the queries
and keys based on their relative distance. A common belief is that RoPE is
useful because it helps to decay token dependency as relative distance
increases. In this work, we argue that this is unlikely to be the core reason.
We study the internals of a trained Gemma 7B model to understand how RoPE is
being used at a mechanical level. We find that Gemma learns to use RoPE to
construct robust "positional" attention patterns by exploiting the highest
frequencies. We also find that, in general, Gemma greatly prefers to use the
lowest frequencies of RoPE, which we suspect are used to carry semantic
information. We mathematically prove interesting behaviours of RoPE and conduct
experiments to verify our findings, proposing a modification of RoPE that fixes
some highlighted issues and improves performance. We believe that this work
represents an interesting step in better understanding PEs in LLMs, which we
believe holds crucial value for scaling LLMs to large sizes and context
lengths.

</details>


### [72] [CursorCore: Assist Programming through Aligning Anything](https://arxiv.org/pdf/2410.07002)
*Hao Jiang, Qi Liu, Rui Li, Shengyu Ye, Shijin Wang*

Main category: cs.CL

TL;DR: The paper introduces a conversational framework for programming assistance, integrating coding history, current code, and user instructions. It proposes APEval for evaluation, a data generation pipeline (Programming-Instruct), and the CursorCore model series, which outperforms comparable models.


<details>
  <summary>Details</summary>
Motivation: Current programming assistance tools lack automation and struggle to integrate diverse information sources like coding history, current code, and user instructions.

Method: The authors introduce APEval for benchmarking, develop the Programming-Instruct pipeline for data synthesis, and fine-tune models to create the CursorCore series.

Result: CursorCore outperforms comparable models, and the framework unifies applications like inline chat and automated editing.

Conclusion: The proposed framework advances coding assistants by integrating diverse information and automating tasks, with code, models, and data made publicly available.

Abstract: Large language models have been successfully applied to programming
assistance tasks, such as code completion, code insertion, and instructional
code editing. However, these applications remain insufficiently automated and
struggle to effectively integrate various types of information during the
programming process, including coding history, current code, and user
instructions. In this work, we propose a new conversational framework that
comprehensively integrates these information sources, collect data to train our
models and evaluate their performance. Firstly, to thoroughly evaluate how well
models align with different types of information and the quality of their
outputs, we introduce a new benchmark, APEval (Assist Programming Eval), to
comprehensively assess the performance of models in programming assistance
tasks. Then, for data collection, we develop a data generation pipeline,
Programming-Instruct, which synthesizes training data from diverse sources,
such as GitHub and online judge platforms. This pipeline can automatically
generate various types of messages throughout the programming process. Finally,
using this pipeline, we generate 219K samples, fine-tune multiple models, and
develop the CursorCore series. We show that CursorCore outperforms other models
of comparable size. This framework unifies applications such as inline chat and
automated editing, contributes to the advancement of coding assistants. Code,
models and data are freely available at
https://github.com/TechxGenus/CursorCore.

</details>


### [73] [Multi-Party Supervised Fine-tuning of Language Models for Multi-Party Dialogue Generation](https://arxiv.org/pdf/2412.05342)
*Xiaoyu Wang, Ningyuan Xi, Teng Chen, Qingqing Gu, Yue Zhao, Xiaokai Chen, Zhonglin Jiang, Yong Chen, Luo Ji*

Main category: cs.CL

TL;DR: MuPaS is a multi-party fine-tuning framework for LLMs, improving their performance in multi-party dialogues (MPD) compared to traditional dyadic fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LLMs are typically fine-tuned for dyadic dialogues, limiting their effectiveness in multi-party scenarios like meetings or discussions.

Method: The MuPaS framework fine-tunes LLMs on multi-party dialogue datasets and includes training strategies to convert it into an MPD simulator.

Result: MuPaS achieves state-of-the-art multi-party responses, higher next-speaker prediction accuracy, and better utterance quality. It also generalizes well to out-of-distribution scenarios.

Conclusion: MuPaS bridges the gap between LLM training and complex multi-party applications, enabling broader use in conversation generation and virtual environments.

Abstract: Large Language Models (LLM) are usually fine-tuned to participate in dyadic
or two-party dialogues, which can not adapt well to multi-party dialogues
(MPD), which hinders their applications in such scenarios including
multi-personal meetings, discussions and daily communication. Previous
LLM-based researches mainly focus on the multi-agent framework, while their
base LLMs are still pairwisely fine-tuned. In this work, we design a
multi-party fine-tuning framework (MuPaS) for LLMs on the multi-party dialogue
datasets, and prove such a straightforward framework can let the LLM align with
the multi-party conversation style efficiently and effectively. We also design
two training strategies which can convert MuPaS into the MPD simulator.
Substantial experiments show that MuPaS can achieve state-of-the-art
multi-party response, higher accuracy of the-next-speaker prediction, higher
human and automatic evaluated utterance qualities, and can even generate
reasonably with out-of-distribution scene, topic and role descriptions. The
MuPaS framework bridges the LLM training with more complicated multi-party
applications, such as conversation generation, virtual rehearsal or
meta-universe.

</details>


### [74] [No Preference Left Behind: Group Distributional Preference Optimization](https://arxiv.org/pdf/2412.20299)
*Binwei Yao, Zefan Cai, Yun-Shiuan Chuang, Shanglin Yang, Ming Jiang, Diyi Yang, Junjie Hu*

Main category: cs.CL

TL;DR: GDPO aligns language models with group preference distributions, outperforming DPO by addressing pluralistic preferences.


<details>
  <summary>Details</summary>
Motivation: Existing methods like DPO fail to capture diverse group preferences, often favoring dominant ones.

Method: GDPO uses belief-conditioned preferences and statistical estimation of group belief distributions.

Result: GDPO reduces alignment gaps and outperforms DPO in aligning with group preferences.

Conclusion: GDPO advances pluralistic alignment by better reflecting diverse group preferences.

Abstract: Preferences within a group of people are not uniform but follow a
distribution. While existing alignment methods like Direct Preference
Optimization (DPO) attempt to steer models to reflect human preferences, they
struggle to capture the distributional pluralistic preferences within a group.
These methods often skew toward dominant preferences, overlooking the diversity
of opinions, especially when conflicting preferences arise. To address this
issue, we propose Group Distributional Preference Optimization (GDPO), a novel
framework that aligns language models with the distribution of preferences
within a group by incorporating the concept of beliefs that shape individual
preferences. GDPO calibrates a language model using statistical estimation of
the group's belief distribution and aligns the model with belief-conditioned
preferences, offering a more inclusive alignment framework than traditional
methods. In experiments using both synthetic controllable opinion generation
and real-world movie review datasets, we show that DPO fails to align with the
targeted belief distributions, while GDPO consistently reduces this alignment
gap during training. Moreover, our evaluation metrics demonstrate that GDPO
outperforms existing approaches in aligning with group distributional
preferences, marking a significant advance in pluralistic alignment.

</details>


### [75] [Self-reflecting Large Language Models: A Hegelian Dialectical Approach](https://arxiv.org/pdf/2501.14917)
*Sara Abdali, Can Goksen, Saeed Amizadeh, Julie E. Maybee, Kazuhito Koishida*

Main category: cs.CL

TL;DR: The paper proposes a Hegelian Dialectic-inspired method for LLM self-reflection, introduces dynamic annealing for temperature control, and uses MAMV to evaluate idea novelty, showing improved idea generation and problem-solving.


<details>
  <summary>Details</summary>
Motivation: To bridge computational NLP with classical philosophy and enhance LLMs' self-reflection and creativity.

Method: Uses Hegelian Dialectic for self-reflection, dynamic annealing for temperature control, and MAMV for idea evaluation.

Result: Improved idea generation and problem-solving performance in LLMs.

Conclusion: The approach effectively combines philosophy and computation to enhance LLM capabilities.

Abstract: Investigating NLP through a philosophical lens has recently caught
researcher's eyes as it connects computational methods with classical schools
of philosophy. This paper introduces a philosophical approach inspired by the
\textit{Hegelian Dialectic} for LLMs' \textit{self-reflection}, utilizing a
self-dialectical approach to emulate internal critiques and then synthesize new
ideas by resolving the opposing points of view. Moreover, this paper
investigates the effect of LLMs' temperature for generation by establishing a
dynamic annealing approach, which promotes the creativity in the early stages
and gradually refines it by focusing on the nuances, as well as a
fixed-temperature strategy for generation. We assess the effectiveness of our
proposed method in generating novel ideas and in improving the reasoning
abilities of LLMs during problem-solving. Moreover, we implement a Multi-Agent
Majority Voting (MAMV) strategy to assess the validity and novelty of the
generated ideas, which proves useful in the absence of domain experts. Our
experiments demonstrate promising results in generating ideas and enhancing
problem-solving performance.

</details>


### [76] [FutureVision: A methodology for the investigation of future cognition](https://arxiv.org/pdf/2502.01597)
*Tiago Timponi Torrent, Mark Turner, Nicolás Hinrichs, Frederico Belcavello, Igor Lourenço, Arthur Lorenzi Almeida, Marcelo Viridiano, Ely Edison Matos*

Main category: cs.CL

TL;DR: A study combines eye-tracking and semantic analysis to measure cognitive effort in understanding future scenarios, finding far-future and pessimistic scenarios increase cognitive load.


<details>
  <summary>Details</summary>
Motivation: To investigate how cognitive effort varies when interpreting future scenarios, especially in terms of valence and counterfactuality.

Method: Combines multimodal semantic analysis with eye-tracking to analyze gaze patterns and semantic representations of stimuli and descriptions.

Result: Far-future and pessimistic scenarios led to longer fixations and erratic saccades, indicating higher cognitive load.

Conclusion: Fractures in the interpretation of future scenarios increase cognitive effort, as shown by eye-tracking data.

Abstract: This paper presents a methodology combining multimodal semantic analysis with
an eye-tracking experimental protocol to investigate the cognitive effort
involved in understanding the communication of future scenarios. To demonstrate
the methodology, we conduct a pilot study examining how visual fixation
patterns vary during the evaluation of valence and counterfactuality in
fictional ad pieces describing futuristic scenarios, using a portable eye
tracker. Participants eye movements are recorded while evaluating the stimuli
and describing them to a conversation partner. Gaze patterns are analyzed
alongside semantic representations of the stimuli and participants
descriptions, constructed from a frame semantic annotation of both linguistic
and visual modalities. Preliminary results show that far-future and pessimistic
scenarios are associated with longer fixations and more erratic saccades,
supporting the hypothesis that fractures in the base spaces underlying the
interpretation of future scenarios increase cognitive load for comprehenders.

</details>


### [77] [SMI: An Information-Theoretic Metric for Predicting Model Knowledge Solely from Pre-Training Signals](https://arxiv.org/pdf/2502.04066)
*Changhao Jiang, Ming Zhang, Junjie Ye, Xiaoran Fan, Yifei Cao, Jiajun Sun, Zhiheng Xi, Shihan Dou, Yi Dong, Yujiong Shen, Jingqi Tong, Zhen Wang, Tao Liang, Zhihui Fei, Mingyang Wan, Guojun Ma, Qi Zhang, Tao Gui, Xuanjing Huang*

Main category: cs.CL

TL;DR: The paper introduces Size-dependent Mutual Information (SMI), a metric to predict QA performance from pre-training data, outperforming baselines with R² > 0.75 for large models.


<details>
  <summary>Details</summary>
Motivation: Predicting model performance on downstream tasks like QA using pre-training signals is crucial for resource efficiency and task-aligned dataset creation.

Method: Large-scale retrieval and semantic analysis of pre-training corpora, multi-template QA evaluation, and development of SMI metric.

Result: SMI achieves strong linear correlation (R² > 0.75) for models >1B parameters, with QA accuracy upper limit ~80%.

Conclusion: SMI effectively predicts QA performance, revealing scaling limits and data optimization benefits.

Abstract: The GPT-4 technical report highlights the possibility of predicting model
performance on downstream tasks using only pre-training signals, though
detailed methodologies are absent. Such predictive capabilities are essential
for resource-efficient pre-training and the construction of task-aligned
datasets. In this paper, we aim to predict performance in closed-book question
answering (QA), a vital downstream task indicative of a model's internal
knowledge. We address three primary challenges: (1) limited access to and
understanding of pre-training corpora, (2) limitations of current evaluation
methods for pre-trained models, and (3) limitations of frequency-based metrics
in predicting model performance. In response to these challenges, we conduct
large-scale retrieval and semantic analysis across the pre-training corpora of
21 publicly available and 3 custom-trained large language models. Subsequently,
we develop a multi-template QA evaluation framework incorporating paraphrased
question variants. Building on these foundations, we propose Size-dependent
Mutual Information (SMI), an information-theoretic metric that linearly
correlates pre-training data characteristics, model size, and QA accuracy,
without requiring any additional training. The experimental results demonstrate
that SMI outperforms co-occurrence-based baselines, achieving $R^2$ > 0.75 on
models with over one billion parameters. Theoretical analysis further reveals
the marginal benefits of scaling model size and optimizing data, indicating
that the upper limit of specific QA task accuracy is approximately 80%. Our
project is available at https://github.com/yuhui1038/SMI.

</details>


### [78] [Discriminative Finetuning of Generative Large Language Models without Reward Models and Human Preference Data](https://arxiv.org/pdf/2502.18679)
*Siqi Guo, Ilgee Hong, Vicente Balmaseda, Changlong Yu, Liang Qiu, Xin Liu, Haoming Jiang, Tuo Zhao, Tianbao Yang*

Main category: cs.CL

TL;DR: Discriminative Fine-Tuning (DFT) improves supervised fine-tuning (SFT) by adopting a discriminative approach, eliminating the need for preference optimization (PO) and outperforming SFT.


<details>
  <summary>Details</summary>
Motivation: SFT's generative training objective limits its effectiveness, requiring additional PO phases. DFT aims to overcome this by leveraging discriminative learning.

Method: DFT uses a discriminative probabilistic framework, optimizing the likelihood of positive answers while suppressing negatives, and includes efficient algorithms for optimization.

Result: DFT outperforms SFT and matches or exceeds SFT followed by PO, demonstrating its effectiveness.

Conclusion: DFT provides a superior alternative to SFT by integrating discriminative learning, reducing reliance on preference data or reward models.

Abstract: Supervised fine-tuning (SFT) has become a crucial step for aligning
pretrained large language models (LLMs) using supervised datasets of
input-output pairs. However, despite being supervised, SFT is inherently
limited by its generative training objective. To address its limitations, the
existing common strategy is to follow SFT with a separate phase of preference
optimization (PO), which relies on either human-labeled preference data or a
strong reward model to guide the learning process. In this paper, we address
the limitations of SFT by exploring one of the most successful techniques in
conventional supervised learning: discriminative learning. We introduce
Discriminative Fine-Tuning (DFT), an improved variant of SFT, which mitigates
the burden of collecting human-labeled preference data or training strong
reward models. Unlike SFT that employs a generative approach and overlooks
negative data, DFT adopts a discriminative paradigm that increases the
probability of positive answers while suppressing potentially negative ones,
aiming for data prediction instead of token prediction. Our contributions
include: (i) a discriminative probabilistic framework for fine-tuning LLMs by
explicitly modeling the discriminative likelihood of an answer among all
possible outputs given an input; (ii) efficient algorithms to optimize this
discriminative likelihood; and (iii) extensive experiments demonstrating DFT's
effectiveness, achieving performance better than SFT and comparable to if not
better than SFT$\rightarrow$PO. The code can be found at
https://github.com/Optimization-AI/DFT.

</details>


### [79] [Can (A)I Change Your Mind?](https://arxiv.org/pdf/2503.01844)
*Miriam Havin, Timna Wharton Kleinman, Moran Koren, Yaniv Dover, Ariel Goldstein*

Main category: cs.CL

TL;DR: LLMs and humans similarly persuade opinions in Hebrew, showing robust persuasive capabilities across diverse settings.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' persuasive abilities in unconstrained, ecological scenarios beyond controlled English settings.

Method: Preregistered study with 200 Hebrew participants, comparing LLM and human persuasion via static (written) and dynamic (conversational) interactions on controversial topics.

Result: Participants adopted LLM and human perspectives similarly, with significant opinion changes and increased confidence across conditions.

Conclusion: LLMs have strong persuasive potential, impacting public opinion formation in diverse contexts.

Abstract: The increasing integration of large language models (LLMs) based
conversational agents into everyday life raises critical cognitive and social
questions about their potential to influence human opinions. Although previous
studies have shown that LLM-based agents can generate persuasive content, these
typically involve controlled English-language settings. Addressing this, our
preregistered study explored LLMs' persuasive capabilities in more ecological,
unconstrained scenarios, examining both static (written paragraphs) and dynamic
(conversations via Telegram) interaction types. Conducted entirely in Hebrew
with 200 participants, the study assessed the persuasive effects of both LLM
and human interlocutors on controversial civil policy topics. Results indicated
that participants adopted LLM and human perspectives similarly, with
significant opinion changes evident across all conditions, regardless of
interlocutor type or interaction mode. Confidence levels increased
significantly in most scenarios. These findings demonstrate LLM-based agents'
robust persuasive capabilities across diverse sources and settings,
highlighting their potential impact on shaping public opinions.

</details>


### [80] [Cite Before You Speak: Enhancing Context-Response Grounding in E-commerce Conversational LLM-Agents](https://arxiv.org/pdf/2503.04830)
*Jingying Zeng, Hui Liu, Zhenwei Dai, Xianfeng Tang, Chen Luo, Samarth Varshney, Zhen Li, Qi He*

Main category: cs.CL

TL;DR: A solution for improving Conversational Shopping Agents (CSAs) by grounding LLM responses with citations, enhancing accuracy and customer trust.


<details>
  <summary>Details</summary>
Motivation: Addressing LLM hallucinations and lack of source attribution in CSAs to improve factual accuracy and customer verification.

Method: Developed a citation generation paradigm and Multi-UX-Inference system for scalable deployment, with auto-evaluation metrics for grounding and attribution.

Result: Citation generation improved grounding by 13.83%, and A/B tests showed 3%-10% higher customer engagement.

Conclusion: Citation-based grounding enhances CSA reliability and customer engagement, scalable via the Multi-UX-Inference system.

Abstract: With the advancement of conversational large language models (LLMs), several
LLM-based Conversational Shopping Agents (CSA) have been developed to help
customers smooth their online shopping. The primary objective in building an
engaging and trustworthy CSA is to ensure the agent's responses about product
factoids are accurate and factually grounded. However, two challenges remain.
First, LLMs produce hallucinated or unsupported claims. Such inaccuracies risk
spreading misinformation and diminishing customer trust. Second, without
providing knowledge source attribution in CSA response, customers struggle to
verify LLM-generated information. To address both challenges, we present an
easily productionized solution that enables a ''citation experience'' to our
customers. We build auto-evaluation metrics to holistically evaluate LLM's
grounding and attribution capabilities, suggesting that citation generation
paradigm substantially improves grounding performance by 13.83%. To deploy this
capability at scale, we introduce Multi-UX-Inference system, which appends
source citations to LLM outputs while preserving existing user experience
features and supporting scalable inference. Large-scale online A/B tests show
that grounded CSA responses improves customer engagement by 3% - 10%, depending
on UX variations.

</details>


### [81] [CURIE: Evaluating LLMs On Multitask Scientific Long Context Understanding and Reasoning](https://arxiv.org/pdf/2503.13517)
*Hao Cui, Zahra Shamsi, Gowoon Cheon, Xuejian Ma, Shutong Li, Maria Tikhanovskaya, Peter Norgaard, Nayantara Mudur, Martyna Plomecka, Paul Raccuglia, Yasaman Bahri, Victor V. Albert, Pranesh Srinivasan, Haining Pan, Philippe Faist, Brian Rohr, Ekin Dogus Cubuk, Muratahan Aykol, Amil Merchant, Michael J. Statt, Dan Morris, Drew Purves, Elise Kleeman, Ruth Alcantara, Matthew Abraham, Muqthar Mohammad, Ean Phing VanLee, Chenfei Jiang, Elizabeth Dorfman, Eun-Ah Kim, Michael P Brenner, Viren Jain, Sameera Ponda, Subhashini Venugopalan*

Main category: cs.CL

TL;DR: CURIE is a benchmark for evaluating LLMs in scientific problem-solving, featuring 580 expert-curated tasks across six disciplines. While some models perform well, all show significant room for improvement.


<details>
  <summary>Details</summary>
Motivation: To measure and improve LLMs' ability to assist in scientific workflows by testing domain expertise, long-context comprehension, and multi-step reasoning.

Method: Introduces CURIE, a benchmark with 580 tasks in six scientific disciplines, evaluating LLMs like Gemini Flash 2.0, Claude-3, GPT-4o, and command-R+.

Result: Gemini Flash 2.0 and Claude-3 perform well, but GPT-4o and command-R+ struggle, with the best model scoring only 32%.

Conclusion: CURIE highlights the need for further LLM development in scientific applications, with evaluation data and code available for future research.

Abstract: Scientific problem-solving involves synthesizing information while applying
expert knowledge. We introduce CURIE, a scientific long-Context
Understanding,Reasoning and Information Extraction benchmark to measure the
potential of Large Language Models (LLMs) in scientific problem-solving and
assisting scientists in realistic workflows. This benchmark introduces ten
challenging tasks with a total of 580 problems and solution pairs curated by
experts in six disciplines - materials science, condensed matter physics,
quantum computing, geospatial analysis, biodiversity, and proteins - covering
both experimental and theoretical work-flows in science. We evaluate a range of
closed and open LLMs on tasks in CURIE which requires domain expertise,
comprehension of long in-context information,and multi-step reasoning. While
Gemini Flash 2.0 and Claude-3 show consistent high comprehension across
domains, the popular GPT-4o and command-R+ fail dramatically on protein
sequencing tasks. With the best performance at 32% there is much room for
improvement for all models. We hope that insights gained from CURIE can guide
the future development of LLMs in sciences. Evaluation code and data are in
https://github.com/google/curie

</details>


### [82] [Crossing Boundaries: Leveraging Semantic Divergences to Explore Cultural Novelty in Cooking Recipes](https://arxiv.org/pdf/2503.24027)
*Florian Carichon, Romain Rampa, Golnoosh Farnadi*

Main category: cs.CL

TL;DR: The paper introduces a framework for quantifying cultural novelty in NLP, using a dataset of global recipes and novel metrics to analyze cultural divergences.


<details>
  <summary>Details</summary>
Motivation: Cultural novelty is key in NLP but lacks robust metrics, limiting understanding of cultural diversity in AI.

Method: Proposes an interdisciplinary framework with GlobalFusion dataset and Jensen-Shannon Divergence metrics to analyze cultural adaptation in recipes.

Result: Significant correlations found between cultural novelty metrics and established cultural measures.

Conclusion: The framework advances understanding and measurement of cultural diversity in AI.

Abstract: Novelty modeling and detection is a core topic in Natural Language Processing
(NLP), central to numerous tasks such as recommender systems and automatic
summarization. It involves identifying pieces of text that deviate in some way
from previously known information. However, novelty is also a crucial
determinant of the unique perception of relevance and quality of an experience,
as it rests upon each individual's understanding of the world. Social factors,
particularly cultural background, profoundly influence perceptions of novelty
and innovation. Cultural novelty arises from differences in salience and
novelty as shaped by the distance between distinct communities. While cultural
diversity has garnered increasing attention in artificial intelligence (AI),
the lack of robust metrics for quantifying cultural novelty hinders a deeper
understanding of these divergences. This gap limits quantifying and
understanding cultural differences within computational frameworks. To address
this, we propose an interdisciplinary framework that integrates knowledge from
sociology and management. Central to our approach is GlobalFusion, a novel
dataset comprising 500 dishes and approximately 100,000 cooking recipes
capturing cultural adaptation from over 150 countries. By introducing a set of
Jensen-Shannon Divergence metrics for novelty, we leverage this dataset to
analyze textual divergences when recipes from one community are modified by
another with a different cultural background. The results reveal significant
correlations between our cultural novelty metrics and established cultural
measures based on linguistic, religious, and geographical distances. Our
findings highlight the potential of our framework to advance the understanding
and measurement of cultural diversity in AI.

</details>


### [83] [Why do LLMs attend to the first token?](https://arxiv.org/pdf/2504.02732)
*Federico Barbero, Álvaro Arroyo, Xiangming Gu, Christos Perivolaropoulos, Michael Bronstein, Petar Veličković, Razvan Pascanu*

Main category: cs.CL

TL;DR: The paper explores why LLMs develop attention sinks, arguing they help avoid over-mixing and linking this to information propagation in Transformers. Experiments validate the influence of context length, depth, and data packing.


<details>
  <summary>Details</summary>
Motivation: To understand why LLMs learn attention sink patterns and their utility, addressing gaps in existing explanations.

Method: Theoretical analysis and empirical experiments examining context length, depth, and data packing's impact on attention sinks.

Result: Attention sinks serve to prevent over-mixing in LLMs, with their behavior influenced by architectural and training choices.

Conclusion: The study offers insights into attention sink utility, enhancing understanding of LLM training patterns.

Abstract: Large Language Models (LLMs) tend to attend heavily to the first token in the
sequence -- creating a so-called attention sink. Many works have studied this
phenomenon in detail, proposing various ways to either leverage or alleviate
it. Attention sinks have been connected to quantisation difficulties, security
issues, and streaming attention. Yet, while many works have provided conditions
in which they occur or not, a critical question remains shallowly answered: Why
do LLMs learn such patterns and how are they being used? In this work, we argue
theoretically and empirically that this mechanism provides a method for LLMs to
avoid over-mixing, connecting this to existing lines of work that study
mathematically how information propagates in Transformers. We conduct
experiments to validate our theoretical intuitions and show how choices such as
context length, depth, and data packing influence the sink behaviour. We hope
that this study provides a new practical perspective on why attention sinks are
useful in LLMs, leading to a better understanding of the attention patterns
that form during training.

</details>


### [84] [AI Hiring with LLMs: A Context-Aware and Explainable Multi-Agent Framework for Resume Screening](https://arxiv.org/pdf/2504.02870)
*Frank P. -W. Lo, Jianing Qiu, Zeyu Wang, Haibao Yu, Yeming Chen, Gao Zhang, Benny Lo*

Main category: cs.CL

TL;DR: A multi-agent framework using LLMs and RAG automates resume screening, improving efficiency and scalability in hiring.


<details>
  <summary>Details</summary>
Motivation: Resume screening is time-consuming and requires objectivity; LLMs offer potential to streamline and automate the process.

Method: Proposes a four-agent framework (extractor, evaluator, summarizer, score formatter) with RAG for contextual relevance.

Result: AI-generated scores align with HR professionals' ratings, demonstrating effectiveness.

Conclusion: Multi-agent RAG-LLM systems can automate resume screening, enhancing hiring workflows.

Abstract: Resume screening is a critical yet time-intensive process in talent
acquisition, requiring recruiters to analyze vast volume of job applications
while remaining objective, accurate, and fair. With the advancements in Large
Language Models (LLMs), their reasoning capabilities and extensive knowledge
bases demonstrate new opportunities to streamline and automate recruitment
workflows. In this work, we propose a multi-agent framework for resume
screening using LLMs to systematically process and evaluate resumes. The
framework consists of four core agents, including a resume extractor, an
evaluator, a summarizer, and a score formatter. To enhance the contextual
relevance of candidate assessments, we integrate Retrieval-Augmented Generation
(RAG) within the resume evaluator, allowing incorporation of external knowledge
sources, such as industry-specific expertise, professional certifications,
university rankings, and company-specific hiring criteria. This dynamic
adaptation enables personalized recruitment, bridging the gap between AI
automation and talent acquisition. We assess the effectiveness of our approach
by comparing AI-generated scores with ratings provided by HR professionals on a
dataset of anonymized online resumes. The findings highlight the potential of
multi-agent RAG-LLM systems in automating resume screening, enabling more
efficient and scalable hiring workflows.

</details>


### [85] [Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models](https://arxiv.org/pdf/2504.04717)
*Yubo Li, Xiaobin Shen, Xinyu Yao, Xueying Ding, Yidi Miao, Ramayya Krishnan, Rema Padman*

Main category: cs.CL

TL;DR: A survey reviewing advancements in evaluating and enhancing multi-turn interactions in LLMs, covering challenges, benchmarks, methodologies, and future directions.


<details>
  <summary>Details</summary>
Motivation: Real-world applications require sophisticated multi-turn interactions, but current LLMs excel mainly in single-turn tasks. This gap motivates the need for better evaluation and enhancement of multi-turn capabilities.

Method: The paper systematically examines challenges, benchmarks, and methodologies (model-centric, external integration, agent-based) for improving multi-turn interactions in LLMs.

Result: Organizes benchmarks and datasets, reviews enhancement techniques, and identifies challenges like context maintenance and fairness.

Conclusion: Proposes future research directions to improve robustness and effectiveness of multi-turn interactions in LLMs.

Abstract: Recent advancements in large language models (LLMs) have revolutionized their
ability to handle single-turn tasks, yet real-world applications demand
sophisticated multi-turn interactions. This survey provides a comprehensive
review of recent advancements in evaluating and enhancing multi-turn
interactions in LLMs. Focusing on task-specific scenarios, from instruction
following in diverse domains such as math and coding to complex conversational
engagements in roleplay, healthcare, education, and even adversarial jailbreak
settings, we systematically examine the challenges of maintaining context,
coherence, fairness, and responsiveness over prolonged dialogues. The paper
organizes current benchmarks and datasets into coherent categories that reflect
the evolving landscape of multi-turn dialogue evaluation. In addition, we
review a range of enhancement methodologies under multi-turn settings,
including model-centric strategies (contextual learning, supervised
fine-tuning, reinforcement learning, and new architectures), external
integration approaches (memory-augmented, retrieval-based methods, and
knowledge graph), and agent-based techniques for collaborative interactions.
Finally, we discuss open challenges and propose future directions for research
to further advance the robustness and effectiveness of multi-turn interactions
in LLMs. Related resources and papers are available at
https://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs.

</details>


### [86] [DeepSeek-R1 Thoughtology: Let's think about LLM Reasoning](https://arxiv.org/pdf/2504.07128)
*Sara Vera Marjanović, Arkil Patel, Vaibhav Adlakha, Milad Aghajohari, Parishad BehnamGhader, Mehar Bhatia, Aditi Khandelwal, Austin Kraft, Benno Krojer, Xing Han Lù, Nicholas Meade, Dongchan Shin, Amirhossein Kazemnejad, Gaurav Kamath, Marius Mosbach, Karolina Stańczak, Siva Reddy*

Main category: cs.CL

TL;DR: DeepSeek-R1 introduces multi-step reasoning chains for complex problems, enabling study of reasoning behavior ('Thoughtology'). Analysis reveals a reasoning 'sweet spot,' persistent rumination, and safety vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: To explore and understand the reasoning behavior of large reasoning models like DeepSeek-R1, including its impact, controllability, and cognitive phenomena.

Method: Analyzed DeepSeek-R1's reasoning building blocks, thought length, context management, cultural/safety concerns, and cognitive phenomena.

Result: Found a reasoning 'sweet spot,' persistent rumination, and safety vulnerabilities compared to non-reasoning models.

Conclusion: DeepSeek-R1's reasoning capabilities offer insights but also reveal performance trade-offs and safety risks.

Abstract: Large Reasoning Models like DeepSeek-R1 mark a fundamental shift in how LLMs
approach complex problems. Instead of directly producing an answer for a given
input, DeepSeek-R1 creates detailed multi-step reasoning chains, seemingly
"thinking" about a problem before providing an answer. This reasoning process
is publicly available to the user, creating endless opportunities for studying
the reasoning behaviour of the model and opening up the field of Thoughtology.
Starting from a taxonomy of DeepSeek-R1's basic building blocks of reasoning,
our analyses on DeepSeek-R1 investigate the impact and controllability of
thought length, management of long or confusing contexts, cultural and safety
concerns, and the status of DeepSeek-R1 vis-\`a-vis cognitive phenomena, such
as human-like language processing and world modelling. Our findings paint a
nuanced picture. Notably, we show DeepSeek-R1 has a 'sweet spot' of reasoning,
where extra inference time can impair model performance. Furthermore, we find a
tendency for DeepSeek-R1 to persistently ruminate on previously explored
problem formulations, obstructing further exploration. We also note strong
safety vulnerabilities of DeepSeek-R1 compared to its non-reasoning
counterpart, which can also compromise safety-aligned LLMs.

</details>


### [87] [LLMSR@XLLM25: Less is More: Enhancing Structured Multi-Agent Reasoning via Quality-Guided Distillation](https://arxiv.org/pdf/2504.16408)
*Jiahao Yuan, Xingzhe Sun, Xing Yu, Jingwen Wang, Dehui Du, Zhiqing Cui, Zixiang Di*

Main category: cs.CL

TL;DR: The paper introduces 'Less is More,' a method for low-resource structured reasoning using minimal labeled data, achieving high performance with only 24 examples.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generating interpretable, step-by-step rationales with minimal labeled data in structured reasoning tasks.

Method: Uses a multi-agent framework with reverse-prompt induction, retrieval-augmented reasoning synthesis via GPT-4o, and dual-stage reward-guided filtering. Fine-tunes Meta-Llama-3-8B-Instruct with LoRA+.

Result: Consistently improves structured reasoning quality by combining structure validation with reward filtering.

Conclusion: Demonstrates the effectiveness of controllable data distillation for enhancing structured inference under low-resource constraints.

Abstract: The LLMSR@XLLM25 formulates a low-resource structural reasoning task that
challenges LLMs to generate interpretable, step-by-step rationales with minimal
labeled data. We present Less is More, the third-place winning approach in the
LLMSR@XLLM25, which focuses on structured reasoning from only 24 labeled
examples. Our approach leverages a multi-agent framework with reverse-prompt
induction, retrieval-augmented reasoning synthesis via GPT-4o, and dual-stage
reward-guided filtering to distill high-quality supervision across three
subtasks: question parsing, CoT parsing, and step-level verification. All
modules are fine-tuned from Meta-Llama-3-8B-Instruct under a unified LoRA+
setup. By combining structure validation with reward filtering across few-shot
and zero-shot prompts, our pipeline consistently improves structure reasoning
quality. These results underscore the value of controllable data distillation
in enhancing structured inference under low-resource constraints. Our code is
available at https://github.com/JhCircle/Less-is-More.

</details>


### [88] [DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training](https://arxiv.org/pdf/2504.17565)
*Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Yunjie Ji, Han Zhao, Xiangang Li*

Main category: cs.CL

TL;DR: The paper introduces a large-scale, difficulty-graded reasoning dataset to improve LLM training, achieving a 79.2% pass rate on AIME2024.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding in LLM training processes and data quality, focusing on enhancing reasoning capabilities.

Method: Constructed a dataset with 3.34M queries and 40M responses, using pass rate and CV for data selection, and adjusted learning rates for reasoning-focused training.

Result: Improved base model reasoning, achieving 79.2% on AIME2024, surpassing most distilled models and nearing state-of-the-art.

Conclusion: The dataset and methods are publicly released to advance open-source long-reasoning LLMs.

Abstract: Although large language models (LLMs) have recently achieved remarkable
performance on various complex reasoning benchmarks, the academic community
still lacks an in-depth understanding of base model training processes and data
quality. To address this, we construct a large-scale, difficulty-graded
reasoning dataset containing approximately 3.34 million unique queries of
varying difficulty levels and about 40 million distilled responses generated by
multiple models over several passes. Leveraging pass rate and Coefficient of
Variation (CV), we precisely select the most valuable training data to enhance
reasoning capability. Notably, we observe a training pattern shift, indicating
that reasoning-focused training based on base models requires higher learning
rates for effective training. Using this carefully selected data, we
significantly improve the reasoning capabilities of the base model, achieving a
pass rate of 79.2\% on the AIME2024 mathematical reasoning benchmark. This
result surpasses most current distilled models and closely approaches
state-of-the-art performance. We provide detailed descriptions of our data
processing, difficulty assessment, and training methodology, and have publicly
released all datasets and methods to promote rapid progress in open-source
long-reasoning LLMs. The dataset is available at:
\href{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M}

</details>


### [89] [Ask, Fail, Repeat: Meeseeks, an Iterative Feedback Benchmark for LLMs' Multi-turn Instruction-following Ability](https://arxiv.org/pdf/2504.21625)
*Jiaming Wang, Yunke Zhao, Peng Ding, Jun Kuang, Zongyu Wang, Xuezhi Cao, Xunliang Cai*

Main category: cs.CL

TL;DR: Meeseeks is a benchmark for evaluating LLMs' ability to follow complex instructions iteratively, simulating real-world feedback loops and self-correction.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack iterative feedback and self-correction, which are crucial for real-world LLM applications.

Method: Meeseeks introduces an iterative feedback framework and a 38-tag evaluation system across three dimensions.

Result: The benchmark provides insights into LLMs' multi-turn instruction-following capabilities.

Conclusion: Meeseeks addresses a critical gap in evaluating LLMs for practical, iterative user interactions.

Abstract: The ability to follow instructions accurately is fundamental for Large
Language Models (LLMs) to serve as reliable agents in real-world applications.
For complex instructions, LLMs often struggle to fulfill all requirements in a
single attempt. In practice, users typically provide iterative feedback until
the LLM generates a response that meets all requirements. However, existing
instruction-following benchmarks are either single-turn or introduce new
requirements in each turn without allowing self-correction. To address this
gap, we propose \textbf{Meeseeks} (named after Mr. Meeseeks from \textit{Rick
and Morty}\footnote{Rick and Morty is an American adult animated science
fiction sitcom created by Justin Roiland and Dan Harmon for Cartoon Network's
nighttime programming block Adult Swim.}.) Meeseeks simulates realistic
human-LLM interactions through an iterative feedback framework, which enables
models to self-correct based on specific requirement failures in each turn,
better reflecting real-world user-end usage patterns. Meanwhile, the benchmark
implements a comprehensive evaluation system with 38 capability tags organized
across three dimensions: Intent Recognition, Granular Content Validation, and
Output Structure Validation. Through rigorous evaluation across LLMs, Meeseeks
provides valuable insights into LLMs' instruction-following capabilities in
multi-turn scenarios.

</details>


### [90] [Graph RAG for Legal Norms: A Hierarchical and Temporal Approach](https://arxiv.org/pdf/2505.00039)
*Hudson de Martim*

Main category: cs.CL

TL;DR: The paper adapts Graph RAG for legal norm analysis, leveraging hierarchical structures and temporal data in knowledge graphs to enhance legal AI applications.


<details>
  <summary>Details</summary>
Motivation: Legal norms are complex due to their hierarchical structure, references, and temporal versions, requiring advanced methods for analysis.

Method: Combines structured knowledge graphs with enriched text segments, integrating hierarchy and temporal evolution.

Result: Enables richer, interconnected legal knowledge representations, improving legal research and decision support.

Conclusion: Graph RAG advances AI in law, offering more effective tools for legal analysis and research.

Abstract: This article proposes an adaptation of Graph Retrieval Augmented Generation
(Graph RAG) specifically designed for the analysis and comprehension of legal
norms, which are characterized by their predefined hierarchical structure,
extensive network of internal and external references and multiple temporal
versions. By combining structured knowledge graphs with contextually enriched
text segments, Graph RAG offers a promising solution to address the inherent
complexity and vast volume of legal data. The integration of hierarchical
structure and temporal evolution into knowledge graphs - along with the concept
of comprehensive Text Units - facilitates the construction of richer,
interconnected representations of legal knowledge. Through a detailed analysis
of Graph RAG and its application to legal norm datasets, this article aims to
advance the field of Artificial Intelligence applied to Law, creating
opportunities for more effective systems in legal research, legislative
analysis, and decision support.

</details>


### [91] [Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models](https://arxiv.org/pdf/2505.01731)
*Chuan Sun, Han Yu, Lizhen Cui, Xiaoxiao Li*

Main category: cs.CL

TL;DR: SV-NUP is a non-uniform pruning method for LLMs using Shapley Values to assign layer-specific pruning budgets, improving performance over uniform pruning.


<details>
  <summary>Details</summary>
Motivation: Traditional uniform pruning methods are suboptimal due to varying layer significance in LLMs.

Method: Proposes SV-NUP, which quantifies layer contributions via Shapley Values and uses a sliding window approximation for efficiency.

Result: SV-NUP reduces perplexity by 18.01% and 19.55% on LLaMA-7B and LLaMA-13B, outperforming SparseGPT at 70% sparsity.

Conclusion: Non-uniform pruning with SV-NUP significantly enhances pruned LLM performance while reducing computational overhead.

Abstract: Pruning large language models (LLMs) is a promising solution for reducing
model sizes and computational complexity while preserving performance.
Traditional layer-wise pruning methods often adopt a uniform sparsity approach
across all layers, which leads to suboptimal performance due to the varying
significance of individual transformer layers within the model not being
accounted for. To this end, we propose the Shapley Value-based Non-Uniform
Pruning (SV-NUP) method for LLMs. This approach quantifies the contribution of
each transformer layer to the overall model performance, enabling the
assignment of tailored pruning budgets to different layers to retain critical
parameters. To further improve efficiency, we design the Sliding Window-based
Shapley Value approximation method. It substantially reduces computational
overhead compared to exact SV calculation methods. Extensive experiments on
various LLMs including LLaMA-v1, LLaMA-v2 and OPT demonstrate the effectiveness
of the proposed approach. The results reveal that non-uniform pruning
significantly enhances the performance of pruned models. Notably, SV-NUP
achieves a reduction in perplexity (PPL) of 18.01% and 19.55% on LLaMA-7B and
LLaMA-13B, respectively, compared to SparseGPT at 70% sparsity.

</details>


### [92] [IndicSQuAD: A Comprehensive Multilingual Question Answering Dataset for Indic Languages](https://arxiv.org/pdf/2505.03688)
*Sharvi Endait, Ruturaj Ghatage, Aditya Kulkarni, Rajlaxmi Patil, Raviraj Joshi*

Main category: cs.CL

TL;DR: The paper introduces IndicSQuAD, a multilingual QA dataset for nine Indic languages, derived from SQuAD, to address underrepresentation in low-resource languages.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in QA systems for Indic languages, which are underrepresented despite their large speaker base.

Method: Adapts and extends translation techniques from MahaSQuAD to create IndicSQuAD, ensuring linguistic fidelity and answer-span alignment.

Result: Baseline evaluations show challenges in low-resource settings, with potential for future work in expansion and multimodal data.

Conclusion: IndicSQuAD provides a robust foundation for QA model development in Indic languages, with publicly shared resources.

Abstract: The rapid progress in question-answering (QA) systems has predominantly
benefited high-resource languages, leaving Indic languages largely
underrepresented despite their vast native speaker base. In this paper, we
present IndicSQuAD, a comprehensive multi-lingual extractive QA dataset
covering nine major Indic languages, systematically derived from the SQuAD
dataset. Building on previous work with MahaSQuAD for Marathi, our approach
adapts and extends translation techniques to maintain high linguistic fidelity
and accurate answer-span alignment across diverse languages. IndicSQuAD
comprises extensive training, validation, and test sets for each language,
providing a robust foundation for model development. We evaluate baseline
performances using language-specific monolingual BERT models and the
multilingual MuRIL-BERT. The results indicate some challenges inherent in
low-resource settings. Moreover, our experiments suggest potential directions
for future work, including expanding to additional languages, developing
domain-specific datasets, and incorporating multimodal data. The dataset and
models are publicly shared at https://github.com/l3cube-pune/indic-nlp

</details>


### [93] [Query-driven Document-level Scientific Evidence Extraction from Biomedical Studies](https://arxiv.org/pdf/2505.06186)
*Massimiliano Pronesti, Joao Bettencourt-Silva, Paul Flanagan, Alessandra Pascale, Oisin Redmond, Anya Belz, Yufang Hou*

Main category: cs.CL

TL;DR: The paper introduces CochraneForest, a dataset for document-level scientific evidence extraction, and URCA, a framework outperforming existing methods by 10.3% in F1 score.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of extracting scientific evidence for clinical questions with conflicting evidence.

Method: Creation of CochraneForest dataset and development of URCA, a retrieval-augmented generation framework.

Result: URCA outperforms existing methods by up to 10.3% in F1 score.

Conclusion: CochraneForest is a challenging benchmark for automated evidence synthesis, and URCA shows promise for this task.

Abstract: Extracting scientific evidence from biomedical studies for clinical research
questions (e.g., Does stem cell transplantation improve quality of life in
patients with medically refractory Crohn's disease compared to placebo?) is a
crucial step in synthesising biomedical evidence. In this paper, we focus on
the task of document-level scientific evidence extraction for clinical
questions with conflicting evidence. To support this task, we create a dataset
called CochraneForest, leveraging forest plots from Cochrane systematic
reviews. It comprises 202 annotated forest plots, associated clinical research
questions, full texts of studies, and study-specific conclusions. Building on
CochraneForest, we propose URCA (Uniform Retrieval Clustered Augmentation), a
retrieval-augmented generation framework designed to tackle the unique
challenges of evidence extraction. Our experiments show that URCA outperforms
the best existing methods by up to 10.3% in F1 score on this task. However, the
results also underscore the complexity of CochraneForest, establishing it as a
challenging testbed for advancing automated evidence synthesis systems.

</details>


### [94] [OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit](https://arxiv.org/pdf/2505.07672)
*Arun S. Maiya*

Main category: cs.CL

TL;DR: OnPrem$.$LLM is a Python toolkit for offline LLM applications on sensitive data, offering privacy-focused pipelines, multiple backend support, and hybrid cloud integration.


<details>
  <summary>Details</summary>
Motivation: Addresses the need for privacy-preserving LLM applications in offline or restricted environments.

Method: Provides prebuilt pipelines for document processing, RAG, information extraction, and more, supporting multiple LLM backends with GPU acceleration.

Result: Enables local execution with optional cloud integration, balancing performance and data control.

Conclusion: OnPrem$.$LLM is a versatile solution for secure, offline LLM deployments, accessible to both technical and non-technical users.

Abstract: We present OnPrem$.$LLM, a Python-based toolkit for applying large language
models (LLMs) to sensitive, non-public data in offline or restricted
environments. The system is designed for privacy-preserving use cases and
provides prebuilt pipelines for document processing and storage,
retrieval-augmented generation (RAG), information extraction, summarization,
classification, and prompt/output processing with minimal configuration.
OnPrem$.$LLM supports multiple LLM backends -- including llama$.$cpp, Ollama,
vLLM, and Hugging Face Transformers -- with quantized model support, GPU
acceleration, and seamless backend switching. Although designed for fully local
execution, OnPrem$.$LLM also supports integration with a wide range of cloud
LLM providers when permitted, enabling hybrid deployments that balance
performance with data control. A no-code web interface extends accessibility to
non-technical users.

</details>


### [95] [Codifying Character Logic in Role-Playing](https://arxiv.org/pdf/2505.07705)
*Letian Peng, Jingbo Shang*

Main category: cs.CL

TL;DR: Codified Profiles represent character logic as executable functions for role-playing, offering persistence, updatability, and controllable randomness over traditional prompt-based methods.


<details>
  <summary>Details</summary>
Motivation: To improve role-playing by addressing limitations of prompt-based profiles, such as inconsistent reasoning and lack of systematic logic updates.

Method: Uses structured functions like parse_by_scene(scene) and check_condition(scene, question) to define character behavior, enabling explicit logic execution.

Result: Experiments show codified profiles enhance persistence, updatability, and behavioral diversity, even with smaller models (1B parameters).

Conclusion: Codified Profiles provide a scalable, efficient foundation for high-quality role-playing, outperforming traditional prompt-based approaches.

Abstract: This paper introduces Codified Profiles for role-playing, a novel approach
that represents character logic as structured, executable functions for
behavioral decision-making. Each profile defines a set of functions
parse_by_scene(scene) that outputs a list of logic-grounded assertions
triggered_statements, using both explicit control structures (e.g.,
if-then-else) and condition checks like check_condition(scene, question), where
each question is a semantically meaningful prompt about the scene (e.g., "Is
the character in danger?") discriminated by the role-playing LLM as true,
false, or unknown. This explicit representation offers three key advantages
over traditional prompt-based profiles, which append character descriptions
directly into text prompts: (1) Persistence, by enforcing complete and
consistent execution of character logic, rather than relying on the model's
implicit reasoning; (2) Updatability, through systematic inspection and
revision of behavioral logic, which is difficult to track or debug in
prompt-only approaches; (3) Controllable Randomness, by supporting stochastic
behavior directly within the logic, enabling fine-grained variability that
prompting alone struggles to achieve. To validate these advantages, we
introduce a new benchmark constructed from 83 characters and 5,141 scenes
curated from Fandom, using NLI-based scoring to compare character responses
against ground-truth actions. Our experiments demonstrate the significant
benefits of codified profiles in improving persistence, updatability, and
behavioral diversity. Notably, by offloading a significant portion of reasoning
to preprocessing, codified profiles enable even 1B-parameter models to perform
high-quality role-playing, providing a scalable and efficient foundation for
local deployment of role-play agents.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [96] [MilChat: Introducing Chain of Thought Reasoning and GRPO to a Multimodal Small Language Model for Remote Sensing](https://arxiv.org/pdf/2505.07984)
*Aybora Koksal, A. Aydin Alatan*

Main category: cs.CV

TL;DR: MilChat, a lightweight MLLM, is introduced for remote sensing imagery analysis, outperforming general-purpose models with targeted fine-tuning and GRPO.


<details>
  <summary>Details</summary>
Motivation: Address limitations of MLLMs in specialized domains like remote sensing, particularly for military applications.

Method: Supervised fine-tuning on a 2B-parameter MLLM with CoT reasoning and GRPO for domain-specific cue detection.

Result: Achieved 80% recall and 98% precision on MilData benchmark, surpassing general-purpose and existing remote sensing models.

Conclusion: Targeted fine-tuning and reinforcement learning enhance MLLM performance in specialized real-world applications.

Abstract: Remarkable capabilities in understanding and generating text-image content
have been demonstrated by recent advancements in multimodal large language
models (MLLMs). However, their effectiveness in specialized
domains-particularly those requiring resource-efficient and domain-specific
adaptations-has remained limited. In this work, a lightweight multimodal
language model termed MilChat is introduced, specifically adapted to analyze
remote sensing imagery in secluded areas, including challenging missile launch
sites. A new dataset, MilData, was compiled by verifying hundreds of aerial
images through expert review, and subtle military installations were
highlighted via detailed captions. Supervised fine-tuning on a 2B-parameter
open-source MLLM with chain-of-thought (CoT) reasoning annotations was
performed, enabling more accurate and interpretable explanations. Additionally,
Group Relative Policy Optimization (GRPO) was leveraged to enhance the model's
ability to detect critical domain-specific cues-such as defensive layouts and
key military structures-while minimizing false positives on civilian scenes.
Through empirical evaluations, it has been shown that MilChat significantly
outperforms both larger, general-purpose multimodal models and existing remote
sensing-adapted approaches on open-ended captioning and classification metrics.
Over 80% recall and 98% precision were achieved on the newly proposed MilData
benchmark, underscoring the potency of targeted fine-tuning and reinforcement
learning in specialized real-world applications.

</details>


### [97] [Vision Foundation Model Embedding-Based Semantic Anomaly Detection](https://arxiv.org/pdf/2505.07998)
*Max Peter Ronecker, Matthew Foutter, Amine Elhafsi, Daniele Gammelli, Ihor Barakaiev, Marco Pavone, Daniel Watzenig*

Main category: cs.CV

TL;DR: The paper proposes a framework for detecting semantic anomalies in autonomous systems using vision foundation models, comparing runtime images to nominal scenarios. Two variants (grid-based and instance-based) are tested, with the latter achieving GPT-4o-comparable performance.


<details>
  <summary>Details</summary>
Motivation: Semantic anomalies can disrupt autonomous systems, necessitating robust detection methods to ensure safety and performance.

Method: A framework compares local vision embeddings from runtime images to a nominal scenario database. Two variants are tested: grid-based and instance-based (with segmentation). A filtering mechanism reduces false positives.

Result: The instance-based method with filtering performs comparably to GPT-4o and provides precise anomaly localization in CARLA simulations.

Conclusion: Vision embeddings from foundation models are promising for real-time anomaly detection in autonomous systems.

Abstract: Semantic anomalies are contextually invalid or unusual combinations of
familiar visual elements that can cause undefined behavior and failures in
system-level reasoning for autonomous systems. This work explores semantic
anomaly detection by leveraging the semantic priors of state-of-the-art vision
foundation models, operating directly on the image. We propose a framework that
compares local vision embeddings from runtime images to a database of nominal
scenarios in which the autonomous system is deemed safe and performant. In this
work, we consider two variants of the proposed framework: one using raw
grid-based embeddings, and another leveraging instance segmentation for
object-centric representations. To further improve robustness, we introduce a
simple filtering mechanism to suppress false positives. Our evaluations on
CARLA-simulated anomalies show that the instance-based method with filtering
achieves performance comparable to GPT-4o, while providing precise anomaly
localization. These results highlight the potential utility of vision
embeddings from foundation models for real-time anomaly detection in autonomous
systems.

</details>


### [98] [FMNV: A Dataset of Media-Published News Videos for Fake News Detection](https://arxiv.org/pdf/2504.07687)
*Yihao Wang, Zhong Qian, Peifeng Li*

Main category: cs.CV

TL;DR: The paper introduces FMNV, a dataset of professionally crafted fake news videos, and FMNVD, a dual-stream model for detecting such content, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the societal harm of professionally crafted fake news videos, which are underrepresented in current datasets.

Method: Constructed FMNV dataset, categorized fake news types, used LLMs to generate deceptive content, and proposed FMNVD model with dual-stream architecture for multimodal detection.

Result: FMNVD showed superior detection efficacy, and FMNV demonstrated generalization across baselines.

Conclusion: The work sets benchmarks for detecting high-impact fake news and advances cross-modal inconsistency analysis.

Abstract: News media, particularly video-based platforms, have become deeply embed-ded
in daily life, concurrently amplifying the risks of misinformation
dissem-ination. Consequently, multimodal fake news detection has garnered
signifi-cant research attention. However, existing datasets predominantly
comprise user-generated videos characterized by crude editing and limited
public en-gagement, whereas professionally crafted fake news videos
disseminated by media outlets-often politically or virally motivated-pose
substantially greater societal harm. To address this gap, we construct FMNV, a
novel da-taset exclusively composed of news videos published by media
organizations. Through empirical analysis of existing datasets and our curated
collection, we categorize fake news videos into four distinct types. Building
upon this taxonomy, we employ Large Language Models (LLMs) to automatically
generate deceptive content by manipulating authentic media-published news
videos. Furthermore, we propose FMNVD, a baseline model featuring a dual-stream
architecture that integrates spatio-temporal motion features from a 3D
ResNeXt-101 backbone and static visual semantics from CLIP. The two streams are
fused via an attention-based mechanism, while co-attention modules refine the
visual, textual, and audio features for effective multi-modal aggregation.
Comparative experiments demonstrate both the generali-zation capability of FMNV
across multiple baselines and the superior detec-tion efficacy of FMNVD. This
work establishes critical benchmarks for de-tecting high-impact fake news in
media ecosystems while advancing meth-odologies for cross-modal inconsistency
analysis. Our dataset is available in https://github.com/DennisIW/FMNV.

</details>


### [99] [RDD: Robust Feature Detector and Descriptor using Deformable Transformer](https://arxiv.org/pdf/2505.08013)
*Gonglin Chen, Tianwen Fu, Haiwei Chen, Wenbin Teng, Hanyuan Xiao, Yajie Zhao*

Main category: cs.CV

TL;DR: The paper introduces Robust Deformable Detector (RDD), a novel keypoint detector/descriptor using deformable transformers to improve feature detection under challenging scenarios like viewpoint changes.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with robust feature detection under significant viewpoint changes and fail to leverage long-range visual cues.

Method: RDD uses deformable self-attention mechanisms to capture global context and geometric invariance, reducing search space complexity.

Result: RDD outperforms state-of-the-art methods in sparse matching and achieves semi-dense matching, validated on new benchmarks.

Conclusion: RDD advances feature detection by integrating deformable transformers, demonstrating superior performance in challenging scenarios.

Abstract: As a core step in structure-from-motion and SLAM, robust feature detection
and description under challenging scenarios such as significant viewpoint
changes remain unresolved despite their ubiquity. While recent works have
identified the importance of local features in modeling geometric
transformations, these methods fail to learn the visual cues present in
long-range relationships. We present Robust Deformable Detector (RDD), a novel
and robust keypoint detector/descriptor leveraging the deformable transformer,
which captures global context and geometric invariance through deformable
self-attention mechanisms. Specifically, we observed that deformable attention
focuses on key locations, effectively reducing the search space complexity and
modeling the geometric invariance. Furthermore, we collected an Air-to-Ground
dataset for training in addition to the standard MegaDepth dataset. Our
proposed method outperforms all state-of-the-art keypoint detection/description
methods in sparse matching tasks and is also capable of semi-dense matching. To
ensure comprehensive evaluation, we introduce two challenging benchmarks: one
emphasizing large viewpoint and scale variations, and the other being an
Air-to-Ground benchmark -- an evaluation setting that has recently gaining
popularity for 3D reconstruction across different altitudes.

</details>


### [100] [Visually Interpretable Subtask Reasoning for Visual Question Answering](https://arxiv.org/pdf/2505.08084)
*Yu Cheng, Arushi Goel, Hakan Bilen*

Main category: cs.CV

TL;DR: VISTAR improves reasoning accuracy and interpretability in MLLMs by generating step-by-step rationales without external models.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational expense and poor accuracy of existing methods for multi-step visual question answering.

Method: Introduces VISTAR, a subtask-driven training framework that fine-tunes MLLMs to produce structured Subtask-of-Thought rationales.

Result: Consistent improvement in reasoning accuracy on two benchmarks while maintaining interpretability.

Conclusion: VISTAR offers a scalable solution for complex visual question answering with enhanced accuracy and interpretability.

Abstract: Answering complex visual questions like `Which red furniture can be used for
sitting?' requires multi-step reasoning, including object recognition,
attribute filtering, and relational understanding. Recent work improves
interpretability in multimodal large language models (MLLMs) by decomposing
tasks into sub-task programs, but these methods are computationally expensive
and less accurate due to poor adaptation to target data. To address this, we
introduce VISTAR (Visually Interpretable Subtask-Aware Reasoning Model), a
subtask-driven training framework that enhances both interpretability and
reasoning by generating textual and visual explanations within MLLMs. Instead
of relying on external models, VISTAR fine-tunes MLLMs to produce structured
Subtask-of-Thought rationales (step-by-step reasoning sequences). Experiments
on two benchmarks show that VISTAR consistently improves reasoning accuracy
while maintaining interpretability. Our code and dataset will be available at
https://github.com/ChengJade/VISTAR.

</details>


### [101] [Multi-modal wound classification using wound image and location by Xception and Gaussian Mixture Recurrent Neural Network (GMRNN)](https://arxiv.org/pdf/2505.08086)
*Ramin Mousa, Ehsan Matbooe, Hakimeh Khojasteh, Amirali Bengari, Mohammadmahdi Vahediahmar*

Main category: cs.CV

TL;DR: The paper proposes a multi-modal AI model using transfer learning (Xception and GMRNN) for classifying wound types (diabetic, pressure, surgical, venous ulcers) with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Effective wound diagnosis is critical for patient care, but current methods struggle with infections and comorbidities. AI can improve early detection and image interpretation.

Method: A multi-modal AI model combines Xception and GMRNN architectures via transfer learning, integrating image and location features for wound classification.

Result: The model achieves 78.77% to 100% accuracy in classifying wound types, outperforming traditional deep neural networks.

Conclusion: The proposed AI model is highly accurate for wound classification, demonstrating potential for improving clinical outcomes.

Abstract: The effective diagnosis of acute and hard-to-heal wounds is crucial for wound
care practitioners to provide effective patient care. Poor clinical outcomes
are often linked to infection, peripheral vascular disease, and increasing
wound depth, which collectively exacerbate these comorbidities. However,
diagnostic tools based on Artificial Intelligence (AI) speed up the
interpretation of medical images and improve early detection of disease. In
this article, we propose a multi-modal AI model based on transfer learning
(TL), which combines two state-of-the-art architectures, Xception and GMRNN,
for wound classification. The multi-modal network is developed by concatenating
the features extracted by a transfer learning algorithm and location features
to classify the wound types of diabetic, pressure, surgical, and venous ulcers.
The proposed method is comprehensively compared with deep neural networks (DNN)
for medical image analysis. The experimental results demonstrate a notable
wound-class classifications (containing only diabetic, pressure, surgical, and
venous) vary from 78.77 to 100\% in various experiments. The results presented
in this study showcase the exceptional accuracy of the proposed methodology in
accurately classifying the most commonly occurring wound types using wound
images and their corresponding locations.

</details>


### [102] [Topology-Guided Knowledge Distillation for Efficient Point Cloud Processing](https://arxiv.org/pdf/2505.08101)
*Luu Tung Hai, Thinh D. Le, Zhicheng Ding, Qing Tian, Truong-Son Hy*

Main category: cs.CV

TL;DR: A novel distillation framework for point cloud processing reduces model size and inference time while maintaining competitive performance.


<details>
  <summary>Details</summary>
Motivation: High-performance models like Point Transformer V3 are computationally demanding, making deployment in resource-constrained environments challenging.

Method: The framework uses topology-aware representations and gradient-guided knowledge distillation to transfer knowledge from a teacher to a lightweight student model.

Result: Achieves a 16x reduction in model size, 1.9x faster inference, and state-of-the-art performance on NuScenes among LiDAR-based distillation methods.

Conclusion: The proposed method effectively balances performance and efficiency, making it suitable for resource-constrained applications.

Abstract: Point cloud processing has gained significant attention due to its critical
role in applications such as autonomous driving and 3D object recognition.
However, deploying high-performance models like Point Transformer V3 in
resource-constrained environments remains challenging due to their high
computational and memory demands. This work introduces a novel distillation
framework that leverages topology-aware representations and gradient-guided
knowledge distillation to effectively transfer knowledge from a high-capacity
teacher to a lightweight student model. Our approach captures the underlying
geometric structures of point clouds while selectively guiding the student
model's learning process through gradient-based feature alignment. Experimental
results in the Nuscenes, SemanticKITTI, and Waymo datasets demonstrate that the
proposed method achieves competitive performance, with an approximately 16x
reduction in model size and a nearly 1.9x decrease in inference time compared
to its teacher model. Notably, on NuScenes, our method achieves
state-of-the-art performance among knowledge distillation techniques trained
solely on LiDAR data, surpassing prior knowledge distillation baselines in
segmentation performance. Our implementation is available publicly at:
  https://github.com/HySonLab/PointDistill

</details>


### [103] [Sleep Position Classification using Transfer Learning for Bed-based Pressure Sensors](https://arxiv.org/pdf/2505.08111)
*Olivier Papillon, Rafik Goubran, James Green, Julien Larivière-Chartier, Caitlin Higginson, Frank Knoefel, Rébecca Robillard*

Main category: cs.CV

TL;DR: The paper proposes using transfer learning with pre-trained Vision Transformer models (ViTMAE and ViTPose) to classify sleep positions from low-resolution pressure-sensitive mat data, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Sleep positions impact sleep quality and disorders like apnea, but labeled data for training models in clinical settings is scarce.

Method: Transfer learning with pre-trained Vision Transformer models (ViTMAE and ViTPose) is applied to classify sleep positions from low-resolution PSM data.

Result: The approach outperforms traditional machine learning and deep learning methods, validated on 112 nights of patient data and a higher-resolution dataset.

Conclusion: The method shows promise for real-world clinical use despite challenges with low-resolution data.

Abstract: Bed-based pressure-sensitive mats (PSMs) offer a non-intrusive way of
monitoring patients during sleep. We focus on four-way sleep position
classification using data collected from a PSM placed under a mattress in a
sleep clinic. Sleep positions can affect sleep quality and the prevalence of
sleep disorders, such as apnea. Measurements were performed on patients with
suspected sleep disorders referred for assessments at a sleep clinic. Training
deep learning models can be challenging in clinical settings due to the need
for large amounts of labeled data. To overcome the shortage of labeled training
data, we utilize transfer learning to adapt pre-trained deep learning models to
accurately estimate sleep positions from a low-resolution PSM dataset collected
in a polysomnography sleep lab. Our approach leverages Vision Transformer
models pre-trained on ImageNet using masked autoencoding (ViTMAE) and a
pre-trained model for human pose estimation (ViTPose). These approaches
outperform previous work from PSM-based sleep pose classification using deep
learning (TCN) as well as traditional machine learning models (SVM, XGBoost,
Random Forest) that use engineered features. We evaluate the performance of
sleep position classification from 112 nights of patient recordings and
validate it on a higher resolution 13-patient dataset. Despite the challenges
of differentiating between sleep positions from low-resolution PSM data, our
approach shows promise for real-world deployment in clinical settings

</details>


### [104] [Now you see it, Now you don't: Damage Label Agreement in Drone & Satellite Post-Disaster Imagery](https://arxiv.org/pdf/2505.08117)
*Thomas Manzini, Priyankari Perali, Jayesh Tripathi, Robin Murphy*

Main category: cs.CV

TL;DR: The paper audits label disagreements between satellite and drone imagery for building damage assessment across three hurricanes, revealing significant discrepancies and ethical risks for ML systems.


<details>
  <summary>Details</summary>
Motivation: To address the lack of studies on label agreement between drone and satellite imagery for damage assessment, and the risks posed by inconsistent labels in ML systems.

Method: Compares damage labels from coincident satellite and drone imagery for 15,814 buildings using the same label schemas and locations across three hurricanes.

Result: Finds 29.02% label disagreement, with satellite-derived labels under-reporting damage by 20.43% compared to drone-derived labels (p<1.2x10^-117).

Conclusion: Highlights ethical risks of misrepresentation in ML systems and provides four recommendations to improve reliability and transparency for decision-makers.

Abstract: This paper audits damage labels derived from coincident satellite and drone
aerial imagery for 15,814 buildings across Hurricanes Ian, Michael, and Harvey,
finding 29.02% label disagreement and significantly different distributions
between the two sources, which presents risks and potential harms during the
deployment of machine learning damage assessment systems. Currently, there is
no known study of label agreement between drone and satellite imagery for
building damage assessment. The only prior work that could be used to infer if
such imagery-derived labels agree is limited by differing damage label schemas,
misaligned building locations, and low data quantities. This work overcomes
these limitations by comparing damage labels using the same damage label
schemas and building locations from three hurricanes, with the 15,814 buildings
representing 19.05 times more buildings considered than the most relevant prior
work. The analysis finds satellite-derived labels significantly under-report
damage by at least 20.43% compared to drone-derived labels (p<1.2x10^-117), and
satellite- and drone-derived labels represent significantly different
distributions (p<5.1x10^-175). This indicates that computer vision and machine
learning (CV/ML) models trained on at least one of these distributions will
misrepresent actual conditions, as the differing satellite and drone-derived
distributions cannot simultaneously represent the distribution of actual
conditions in a scene. This potential misrepresentation poses ethical risks and
potential societal harm if not managed. To reduce the risk of future societal
harms, this paper offers four recommendations to improve reliability and
transparency to decisio-makers when deploying CV/ML damage assessment systems
in practice

</details>


### [105] [JSover: Joint Spectrum Estimation and Multi-Material Decomposition from Single-Energy CT Projections](https://arxiv.org/pdf/2505.08123)
*Qing Wu, Hongjiang Wei, Jingyi Yu, S. Kevin Zhou, Yuyao Zhang*

Main category: cs.CV

TL;DR: JSover is a one-step SEMMD framework that jointly reconstructs multi-material compositions and estimates energy spectra from SECT projections, outperforming traditional two-step methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional MMD methods require spectral CT scanners and pre-measured spectra, limiting clinical use. SEMMD methods exist but suffer from artifacts due to neglecting energy-dependent attenuation.

Method: JSover integrates physics-informed spectral priors and uses implicit neural representation (INR) for unsupervised deep learning to jointly reconstruct materials and estimate spectra.

Result: JSover outperforms state-of-the-art SEMMD methods in accuracy and computational efficiency on simulated and real CT datasets.

Conclusion: JSover provides a reliable and accurate one-step solution for SEMMD, enhancing clinical applicability.

Abstract: Multi-material decomposition (MMD) enables quantitative reconstruction of
tissue compositions in the human body, supporting a wide range of clinical
applications. However, traditional MMD typically requires spectral CT scanners
and pre-measured X-ray energy spectra, significantly limiting clinical
applicability. To this end, various methods have been developed to perform MMD
using conventional (i.e., single-energy, SE) CT systems, commonly referred to
as SEMMD. Despite promising progress, most SEMMD methods follow a two-step
image decomposition pipeline, which first reconstructs monochromatic CT images
using algorithms such as FBP, and then performs decomposition on these images.
The initial reconstruction step, however, neglects the energy-dependent
attenuation of human tissues, introducing severe nonlinear beam hardening
artifacts and noise into the subsequent decomposition. This paper proposes
JSover, a fundamentally reformulated one-step SEMMD framework that jointly
reconstructs multi-material compositions and estimates the energy spectrum
directly from SECT projections. By explicitly incorporating physics-informed
spectral priors into the SEMMD process, JSover accurately simulates a virtual
spectral CT system from SE acquisitions, thereby improving the reliability and
accuracy of decomposition. Furthermore, we introduce implicit neural
representation (INR) as an unsupervised deep learning solver for representing
the underlying material maps. The inductive bias of INR toward continuous image
patterns constrains the solution space and further enhances estimation quality.
Extensive experiments on both simulated and real CT datasets show that JSover
outperforms state-of-the-art SEMMD methods in accuracy and computational
efficiency.

</details>


### [106] [SLAG: Scalable Language-Augmented Gaussian Splatting](https://arxiv.org/pdf/2505.08124)
*Laszlo Szilagyi, Francis Engelmann, Jeannette Bohg*

Main category: cs.CV

TL;DR: SLAG is a multi-GPU framework for language-augmented Gaussian splatting, enhancing speed and scalability for embedding large scenes without needing a loss function.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for rapid, scalable scene encoding in time-sensitive robotics applications like search-and-rescue and smart cities.

Method: Integrates 2D visual-language model features into 3D scenes using SAM and CLIP, using a normalized weighted average for embeddings.

Result: Achieves an 18x speedup in embedding computation on a 16-GPU setup compared to OpenGaussian while preserving embedding quality.

Conclusion: SLAG offers a scalable, efficient solution for language-augmented scene representations in resource-constrained robotics.

Abstract: Language-augmented scene representations hold great promise for large-scale
robotics applications such as search-and-rescue, smart cities, and mining. Many
of these scenarios are time-sensitive, requiring rapid scene encoding while
also being data-intensive, necessitating scalable solutions. Deploying these
representations on robots with limited computational resources further adds to
the challenge. To address this, we introduce SLAG, a multi-GPU framework for
language-augmented Gaussian splatting that enhances the speed and scalability
of embedding large scenes. Our method integrates 2D visual-language model
features into 3D scenes using SAM and CLIP. Unlike prior approaches, SLAG
eliminates the need for a loss function to compute per-Gaussian language
embeddings. Instead, it derives embeddings from 3D Gaussian scene parameters
via a normalized weighted average, enabling highly parallelized scene encoding.
Additionally, we introduce a vector database for efficient embedding storage
and retrieval. Our experiments show that SLAG achieves an 18 times speedup in
embedding computation on a 16-GPU setup compared to OpenGaussian, while
preserving embedding quality on the ScanNet and LERF datasets. For more
details, visit our project website: https://slag-project.github.io/.

</details>


### [107] [Asynchronous Multi-Object Tracking with an Event Camera](https://arxiv.org/pdf/2505.08126)
*Angus Apps, Ziwei Wang, Vladimir Perejogin, Timothy Molloy, Robert Mahony*

Main category: cs.CV

TL;DR: AEMOT algorithm detects and tracks multiple objects using asynchronous event processing, outperforming alternatives by 37% on a Bee Swarm Dataset.


<details>
  <summary>Details</summary>
Motivation: Events cameras' low latency and high dynamic range make them ideal for tracking in dynamic environments.

Method: AEMOT uses a Field of Active Flow Directions for feature detection and an Asynchronous Event Blob tracker for object tracking, validated by a learnt classification stage.

Result: AEMOT achieves over 37% better precision and recall than other event-based methods on the Bee Swarm Dataset.

Conclusion: AEMOT is effective for multi-object tracking in dynamic scenes, with open-sourced code and dataset.

Abstract: Events cameras are ideal sensors for enabling robots to detect and track
objects in highly dynamic environments due to their low latency output, high
temporal resolution, and high dynamic range. In this paper, we present the
Asynchronous Event Multi-Object Tracking (AEMOT) algorithm for detecting and
tracking multiple objects by processing individual raw events asynchronously.
AEMOT detects salient event blob features by identifying regions of consistent
optical flow using a novel Field of Active Flow Directions built from the
Surface of Active Events. Detected features are tracked as candidate objects
using the recently proposed Asynchronous Event Blob (AEB) tracker in order to
construct small intensity patches of each candidate object. A novel learnt
validation stage promotes or discards candidate objects based on classification
of their intensity patches, with promoted objects having their position,
velocity, size, and orientation estimated at their event rate. We evaluate
AEMOT on a new Bee Swarm Dataset, where it tracks dozens of small bees with
precision and recall performance exceeding that of alternative event-based
detection and tracking algorithms by over 37%. Source code and the labelled
event Bee Swarm Dataset will be open sourced

</details>


### [108] [MoKD: Multi-Task Optimization for Knowledge Distillation](https://arxiv.org/pdf/2505.08170)
*Zeeshan Hayder, Ali Cheraghian, Lars Petersson, Mehrtash Harandi*

Main category: cs.CV

TL;DR: MoKD addresses gradient conflicts and dominance in Knowledge Distillation (KD) by reformulating it as a multi-objective optimization problem, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Challenges in KD include balancing teacher guidance with task objectives and handling knowledge representation disparities.

Method: MoKD reformulates KD as multi-objective optimization and uses subspace learning for better knowledge transfer.

Result: MoKD outperforms existing methods on ImageNet-1K and COCO datasets, achieving state-of-the-art performance.

Conclusion: MoKD effectively balances KD objectives and improves efficiency, setting new benchmarks.

Abstract: Compact models can be effectively trained through Knowledge Distillation
(KD), a technique that transfers knowledge from larger, high-performing teacher
models. Two key challenges in Knowledge Distillation (KD) are: 1) balancing
learning from the teacher's guidance and the task objective, and 2) handling
the disparity in knowledge representation between teacher and student models.
To address these, we propose Multi-Task Optimization for Knowledge Distillation
(MoKD). MoKD tackles two main gradient issues: a) Gradient Conflicts, where
task-specific and distillation gradients are misaligned, and b) Gradient
Dominance, where one objective's gradient dominates, causing imbalance. MoKD
reformulates KD as a multi-objective optimization problem, enabling better
balance between objectives. Additionally, it introduces a subspace learning
framework to project feature representations into a high-dimensional space,
improving knowledge transfer. Our MoKD is demonstrated to outperform existing
methods through extensive experiments on image classification using the
ImageNet-1K dataset and object detection using the COCO dataset, achieving
state-of-the-art performance with greater efficiency. To the best of our
knowledge, MoKD models also achieve state-of-the-art performance compared to
models trained from scratch.

</details>


### [109] [Empowering Vision Transformers with Multi-Scale Causal Intervention for Long-Tailed Image Classification](https://arxiv.org/pdf/2505.08173)
*Xiaoshuo Yan, Zhaochuan Li, Lei Meng, Zhuang Qi, Wei Wu, Zixuan Li, Xiangxu Meng*

Main category: cs.CV

TL;DR: TSCNet, a two-stage causal modeling method, addresses biases in long-tail classification for ViT models by enhancing fine-grained causal associations and refining decision boundaries.


<details>
  <summary>Details</summary>
Motivation: Existing causal models underperform with ViT due to its global feature representation, making it hard to model fine-grained associations for tail classes.

Method: TSCNet uses hierarchical causal representation learning (HCRL) and counterfactual logits bias calibration (CLBC) to improve causal associations and decision boundaries.

Result: TSCNet outperforms existing methods on long-tail benchmarks by eliminating biases from data imbalance.

Conclusion: TSCNet effectively mitigates biases in long-tail classification for ViT models, improving performance on tail classes.

Abstract: Causal inference has emerged as a promising approach to mitigate long-tail
classification by handling the biases introduced by class imbalance. However,
along with the change of advanced backbone models from Convolutional Neural
Networks (CNNs) to Visual Transformers (ViT), existing causal models may not
achieve an expected performance gain. This paper investigates the influence of
existing causal models on CNNs and ViT variants, highlighting that ViT's global
feature representation makes it hard for causal methods to model associations
between fine-grained features and predictions, which leads to difficulties in
classifying tail classes with similar visual appearance. To address these
issues, this paper proposes TSCNet, a two-stage causal modeling method to
discover fine-grained causal associations through multi-scale causal
interventions. Specifically, in the hierarchical causal representation learning
stage (HCRL), it decouples the background and objects, applying backdoor
interventions at both the patch and feature level to prevent model from using
class-irrelevant areas to infer labels which enhances fine-grained causal
representation. In the counterfactual logits bias calibration stage (CLBC), it
refines the optimization of model's decision boundary by adaptive constructing
counterfactual balanced data distribution to remove the spurious associations
in the logits caused by data distribution. Extensive experiments conducted on
various long-tail benchmarks demonstrate that the proposed TSCNet can eliminate
multiple biases introduced by data imbalance, which outperforms existing
methods.

</details>


### [110] [Monocular Depth Guided Occlusion-Aware Disparity Refinement via Semi-supervised Learning in Laparoscopic Images](https://arxiv.org/pdf/2505.08178)
*Ziteng Liu, Dongdong He, Chenghong Zhang, Wenpeng Gao, Yili Fu*

Main category: cs.CV

TL;DR: DGORNet improves disparity estimation in stereo laparoscopic images using depth guidance, position embedding, and optical flow loss, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Address occlusion and labeled data scarcity in disparity estimation for laparoscopic surgery.

Method: Proposes DGORNet with depth guidance, Position Embedding (PE) module, and Optical Flow Difference Loss (OFDLoss) for unlabeled data.

Result: Outperforms state-of-the-art methods on SCARED dataset, especially in occlusion and texture-less regions.

Conclusion: DGORNet effectively enhances disparity estimation, addressing key challenges in laparoscopic surgery.

Abstract: Occlusion and the scarcity of labeled surgical data are significant
challenges in disparity estimation for stereo laparoscopic images. To address
these issues, this study proposes a Depth Guided Occlusion-Aware Disparity
Refinement Network (DGORNet), which refines disparity maps by leveraging
monocular depth information unaffected by occlusion. A Position Embedding (PE)
module is introduced to provide explicit spatial context, enhancing the
network's ability to localize and refine features. Furthermore, we introduce an
Optical Flow Difference Loss (OFDLoss) for unlabeled data, leveraging temporal
continuity across video frames to improve robustness in dynamic surgical
scenes. Experiments on the SCARED dataset demonstrate that DGORNet outperforms
state-of-the-art methods in terms of End-Point Error (EPE) and Root Mean
Squared Error (RMSE), particularly in occlusion and texture-less regions.
Ablation studies confirm the contributions of the Position Embedding and
Optical Flow Difference Loss, highlighting their roles in improving spatial and
temporal consistency. These results underscore DGORNet's effectiveness in
enhancing disparity estimation for laparoscopic surgery, offering a practical
solution to challenges in disparity estimation and data limitations.

</details>


### [111] [Unsupervised Raindrop Removal from a Single Image using Conditional Diffusion Models](https://arxiv.org/pdf/2505.08190)
*Lhuqita Fazry, Valentino Vito*

Main category: cs.CV

TL;DR: A novel diffusion-based inpainting technique for single-image raindrop removal.


<details>
  <summary>Details</summary>
Motivation: Raindrop removal is challenging, especially with single images. Current methods rely on GANs, but diffusion models offer advanced inpainting.

Method: Uses diffusion models for background restoration after detecting raindrop regions.

Result: Proposes a state-of-the-art technique leveraging diffusion models.

Conclusion: Diffusion-based inpainting improves raindrop removal from single images.

Abstract: Raindrop removal is a challenging task in image processing. Removing
raindrops while relying solely on a single image further increases the
difficulty of the task. Common approaches include the detection of raindrop
regions in the image, followed by performing a background restoration process
conditioned on those regions. While various methods can be applied for the
detection step, the most common architecture used for background restoration is
the Generative Adversarial Network (GAN). Recent advances in the use of
diffusion models have led to state-of-the-art image inpainting techniques. In
this paper, we introduce a novel technique for raindrop removal from a single
image using diffusion-based image inpainting.

</details>


### [112] [Ultra Lowrate Image Compression with Semantic Residual Coding and Compression-aware Diffusion](https://arxiv.org/pdf/2505.08281)
*Anle Ke, Xu Zhang, Tong Chen, Ming Lu, Chao Zhou, Jiawen Gu, Zhan Ma*

Main category: cs.CV

TL;DR: ResULIC improves multimodal image compression by integrating residual signals into semantic retrieval and diffusion-based generation, outperforming existing methods in fidelity and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks suffer from fragmented integration of semantic retrieval, latent compression, and generative models, leading to poor performance in fidelity and coding efficiency.

Method: Proposes Residual-guided Ultra Lowrate Image Compression (ResULIC) with Semantic Residual Coding (SRC) for semantic disparity and Compression-aware Diffusion Model (CDM) for bitrate-time step alignment.

Result: Achieves -80.7% and -66.3% BD-rate savings in LPIPS and FID metrics, outperforming state-of-the-art methods.

Conclusion: ResULIC enhances reconstruction quality and coding efficiency, validated by superior objective and subjective performance.

Abstract: Existing multimodal large model-based image compression frameworks often rely
on a fragmented integration of semantic retrieval, latent compression, and
generative models, resulting in suboptimal performance in both reconstruction
fidelity and coding efficiency. To address these challenges, we propose a
residual-guided ultra lowrate image compression named ResULIC, which
incorporates residual signals into both semantic retrieval and the
diffusion-based generation process. Specifically, we introduce Semantic
Residual Coding (SRC) to capture the semantic disparity between the original
image and its compressed latent representation. A perceptual fidelity optimizer
is further applied for superior reconstruction quality. Additionally, we
present the Compression-aware Diffusion Model (CDM), which establishes an
optimal alignment between bitrates and diffusion time steps, improving
compression-reconstruction synergy. Extensive experiments demonstrate the
effectiveness of ResULIC, achieving superior objective and subjective
performance compared to state-of-the-art diffusion-based methods with - 80.7%,
-66.3% BD-rate saving in terms of LPIPS and FID. Project page is available at
https: //njuvision.github.io/ResULIC/.

</details>


### [113] [ADC-GS: Anchor-Driven Deformable and Compressed Gaussian Splatting for Dynamic Scene Reconstruction](https://arxiv.org/pdf/2505.08196)
*He Huang, Qi Yang, Mufan Liu, Yiling Xu, Zhu Li*

Main category: cs.CV

TL;DR: ADC-GS improves dynamic scene reconstruction by organizing Gaussian primitives into an anchor-based structure, reducing redundancy and enhancing efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing 4D Gaussian Splatting methods suffer from redundancy and suboptimal performance due to per-Gaussian deformation.

Method: ADC-GS uses an anchor-based structure with temporal refinement, a hierarchical motion pipeline, and rate-distortion optimization.

Result: ADC-GS achieves 300%-800% faster rendering and superior storage efficiency without quality loss.

Conclusion: ADC-GS offers a compact, efficient solution for dynamic scene reconstruction, outperforming existing methods.

Abstract: Existing 4D Gaussian Splatting methods rely on per-Gaussian deformation from
a canonical space to target frames, which overlooks redundancy among adjacent
Gaussian primitives and results in suboptimal performance. To address this
limitation, we propose Anchor-Driven Deformable and Compressed Gaussian
Splatting (ADC-GS), a compact and efficient representation for dynamic scene
reconstruction. Specifically, ADC-GS organizes Gaussian primitives into an
anchor-based structure within the canonical space, enhanced by a temporal
significance-based anchor refinement strategy. To reduce deformation
redundancy, ADC-GS introduces a hierarchical coarse-to-fine pipeline that
captures motions at varying granularities. Moreover, a rate-distortion
optimization is adopted to achieve an optimal balance between bitrate
consumption and representation fidelity. Experimental results demonstrate that
ADC-GS outperforms the per-Gaussian deformation approaches in rendering speed
by 300%-800% while achieving state-of-the-art storage efficiency without
compromising rendering quality. The code is released at
https://github.com/H-Huang774/ADC-GS.git.

</details>


### [114] [ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible Long-term Tracking](https://arxiv.org/pdf/2505.08581)
*Haofeng Liu, Mingqi Gao, Xuxiao Luo, Ziyue Wang, Guanyi Qin, Junde Wu, Yueming Jin*

Main category: cs.CV

TL;DR: ReSurgSAM2 is a two-stage surgical referring segmentation framework using Segment Anything Model 2 for text-referred detection and long-term tracking, improving accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Enhancing surgical quality and patient outcomes by addressing low efficiency and short-term tracking limitations in existing referring surgical segmentation methods.

Method: A two-stage framework: (1) cross-modal spatial-temporal Mamba for detection and segmentation, (2) credible initial frame selection and diversity-driven long-term memory for tracking.

Result: Achieves significant accuracy and efficiency improvements, operating at 61.2 FPS in real-time.

Conclusion: ReSurgSAM2 outperforms existing methods, offering reliable and efficient surgical scene segmentation.

Abstract: Surgical scene segmentation is critical in computer-assisted surgery and is
vital for enhancing surgical quality and patient outcomes. Recently, referring
surgical segmentation is emerging, given its advantage of providing surgeons
with an interactive experience to segment the target object. However, existing
methods are limited by low efficiency and short-term tracking, hindering their
applicability in complex real-world surgical scenarios. In this paper, we
introduce ReSurgSAM2, a two-stage surgical referring segmentation framework
that leverages Segment Anything Model 2 to perform text-referred target
detection, followed by tracking with reliable initial frame identification and
diversity-driven long-term memory. For the detection stage, we propose a
cross-modal spatial-temporal Mamba to generate precise detection and
segmentation results. Based on these results, our credible initial frame
selection strategy identifies the reliable frame for the subsequent tracking.
Upon selecting the initial frame, our method transitions to the tracking stage,
where it incorporates a diversity-driven memory mechanism that maintains a
credible and diverse memory bank, ensuring consistent long-term tracking.
Extensive experiments demonstrate that ReSurgSAM2 achieves substantial
improvements in accuracy and efficiency compared to existing methods, operating
in real-time at 61.2 FPS. Our code and datasets will be available at
https://github.com/jinlab-imvr/ReSurgSAM2.

</details>


### [115] [Visual Watermarking in the Era of Diffusion Models: Advances and Challenges](https://arxiv.org/pdf/2505.08197)
*Junxian Duan, Jiyang Guang, Wenkui Yang, Ran He*

Main category: cs.CV

TL;DR: The paper discusses using diffusion models to enhance watermarking for protecting digital content against misuse in generative AI, addressing robustness and security challenges.


<details>
  <summary>Details</summary>
Motivation: Rising concerns about copyright infringement due to generative AI advancements like Stable Diffusion, necessitating robust watermarking solutions.

Method: Analyzes diffusion models for embedding imperceptible, robust watermarks, improving detection accuracy over traditional methods.

Result: Highlights the potential of diffusion models in watermarking but notes challenges in robustness and security.

Conclusion: Emphasizes the need for innovative watermarking solutions to protect digital content and ownership rights in the generative AI era.

Abstract: As generative artificial intelligence technologies like Stable Diffusion
advance, visual content becomes more vulnerable to misuse, raising concerns
about copyright infringement. Visual watermarks serve as effective protection
mechanisms, asserting ownership and deterring unauthorized use. Traditional
deepfake detection methods often rely on passive techniques that struggle with
sophisticated manipulations. In contrast, diffusion models enhance detection
accuracy by allowing for the effective learning of features, enabling the
embedding of imperceptible and robust watermarks. We analyze the strengths and
challenges of watermark techniques related to diffusion models, focusing on
their robustness and application in watermark generation. By exploring the
integration of advanced diffusion models and watermarking security, we aim to
advance the discourse on preserving watermark robustness against evolving
forgery threats. It emphasizes the critical importance of developing innovative
solutions to protect digital content and ensure the preservation of ownership
rights in the era of generative AI.

</details>


### [116] [Object detection in adverse weather conditions for autonomous vehicles using Instruct Pix2Pix](https://arxiv.org/pdf/2505.08228)
*Unai Gurbindo, Axel Brando, Jaume Abella, Caroline König*

Main category: cs.CV

TL;DR: The paper proposes using Instruct Pix2Pix to create weather-augmented datasets, improving object detection robustness in adverse weather for autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Enhancing object detection systems' robustness under adverse weather is critical for autonomous driving.

Method: Leverages Instruct Pix2Pix for weather-based data augmentation, tested in CARLA simulator and real-world datasets (BDD100K, ACDC).

Result: Shows performance gaps in models under adverse weather and proves tailored data augmentation boosts robustness.

Conclusion: Provides a foundation for reliable perception systems in challenging environments, aiding autonomous driving advancements.

Abstract: Enhancing the robustness of object detection systems under adverse weather
conditions is crucial for the advancement of autonomous driving technology.
This study presents a novel approach leveraging the diffusion model Instruct
Pix2Pix to develop prompting methodologies that generate realistic datasets
with weather-based augmentations aiming to mitigate the impact of adverse
weather on the perception capabilities of state-of-the-art object detection
models, including Faster R-CNN and YOLOv10. Experiments were conducted in two
environments, in the CARLA simulator where an initial evaluation of the
proposed data augmentation was provided, and then on the real-world image data
sets BDD100K and ACDC demonstrating the effectiveness of the approach in real
environments.
  The key contributions of this work are twofold: (1) identifying and
quantifying the performance gap in object detection models under challenging
weather conditions, and (2) demonstrating how tailored data augmentation
strategies can significantly enhance the robustness of these models. This
research establishes a solid foundation for improving the reliability of
perception systems in demanding environmental scenarios, and provides a pathway
for future advancements in autonomous driving.

</details>


### [117] [HMPNet: A Feature Aggregation Architecture for Maritime Object Detection from a Shipborne Perspective](https://arxiv.org/pdf/2505.08231)
*Yu Zhang, Fengyuan Liu, Juan Lyu, Yi Wei, Changdong Yu*

Main category: cs.CV

TL;DR: The paper introduces Navigation12, a maritime-specific dataset, and HMPNet, a lightweight object detection model, achieving better accuracy and efficiency than existing methods.


<details>
  <summary>Details</summary>
Motivation: The lack of maritime-specific data hinders advanced visual perception techniques in maritime navigation, prompting the creation of a dedicated dataset and model.

Method: HMPNet features a hierarchical dynamic modulation backbone, matrix cascading poly-scale neck, and polymerization weight sharing detector for efficient multi-scale feature aggregation.

Result: HMPNet improves mean Average Precision by 3.3% over YOLOv11n and reduces parameters by 23%.

Conclusion: The proposed dataset and model effectively address maritime object detection challenges, outperforming current state-of-the-art methods.

Abstract: In the realm of intelligent maritime navigation, object detection from a
shipborne perspective is paramount. Despite the criticality, the paucity of
maritime-specific data impedes the deployment of sophisticated visual
perception techniques, akin to those utilized in autonomous vehicular systems,
within the maritime context. To bridge this gap, we introduce Navigation12, a
novel dataset annotated for 12 object categories under diverse maritime
environments and weather conditions. Based upon this dataset, we propose
HMPNet, a lightweight architecture tailored for shipborne object detection.
HMPNet incorporates a hierarchical dynamic modulation backbone to bolster
feature aggregation and expression, complemented by a matrix cascading
poly-scale neck and a polymerization weight sharing detector, facilitating
efficient multi-scale feature aggregation. Empirical evaluations indicate that
HMPNet surpasses current state-of-the-art methods in terms of both accuracy and
computational efficiency, realizing a 3.3% improvement in mean Average
Precision over YOLOv11n, the prevailing model, and reducing parameters by 23%.

</details>


### [118] [G-MSGINet: A Grouped Multi-Scale Graph-Involution Network for Contactless Fingerprint Recognition](https://arxiv.org/pdf/2505.08233)
*Santhoshkumar Peddi, Soham Bandyopadhyay, Debasis Samanta*

Main category: cs.CV

TL;DR: G-MSGINet is a unified framework for contactless fingerprint recognition, combining minutiae localization and identity embedding without complex preprocessing. It outperforms existing methods with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Existing fingerprint recognition methods rely on multi-branch architectures or complex preprocessing, limiting scalability and generalization. G-MSGINet aims to address these limitations.

Method: The architecture uses GMSGI layers, integrating pixel-level involution, multi-scale kernel generation, and graph-based relational modeling, eliminating explicit orientation supervision.

Result: Achieves F1-scores of 0.83±0.02, Rank-1 accuracies of 97.0%-99.1%, and EER as low as 0.5%, with 0.38M parameters and 6.63 GFLOPS.

Conclusion: G-MSGINet is scalable and effective for real-world contactless biometric recognition, outperforming prior methods with fewer resources.

Abstract: This paper presents G-MSGINet, a unified and efficient framework for robust
contactless fingerprint recognition that jointly performs minutiae localization
and identity embedding directly from raw input images. Existing approaches rely
on multi-branch architectures, orientation labels, or complex preprocessing
steps, which limit scalability and generalization across real-world acquisition
scenarios. In contrast, the proposed architecture introduces the GMSGI layer, a
novel computational module that integrates grouped pixel-level involution,
dynamic multi-scale kernel generation, and graph-based relational modelling
into a single processing unit. Stacked GMSGI layers progressively refine both
local minutiae-sensitive features and global topological representations
through end-to-end optimization. The architecture eliminates explicit
orientation supervision and adapts graph connectivity directly from learned
kernel descriptors, thereby capturing meaningful structural relationships among
fingerprint regions without fixed heuristics. Extensive experiments on three
benchmark datasets, namely PolyU, CFPose, and Benchmark 2D/3D, demonstrate that
G-MSGINet consistently achieves minutiae F1-scores in the range of
$0.83\pm0.02$ and Rank-1 identification accuracies between 97.0% and 99.1%,
while maintaining an Equal Error Rate (EER) as low as 0.5%. These results
correspond to improvements of up to 4.8% in F1-score and 1.4% in Rank-1
accuracy when compared to prior methods, using only 0.38 million parameters and
6.63 giga floating-point operations, which represents up to ten times fewer
parameters than competitive baselines. This highlights the scalability and
effectiveness of G-MSGINet in real-world contactless biometric recognition
scenarios.

</details>


### [119] [Removing Watermarks with Partial Regeneration using Semantic Information](https://arxiv.org/pdf/2505.08234)
*Krti Tallam, John Kevin Cava, Caleb Geniesse, N. Benjamin Erichson, Michael W. Mahoney*

Main category: cs.CV

TL;DR: SemanticRegen, a three-stage attack, erases state-of-the-art semantic and invisible watermarks while preserving image quality, exposing vulnerabilities in current watermarking defenses.


<details>
  <summary>Details</summary>
Motivation: To investigate the robustness of semantic watermarks against adaptive adversaries and expose vulnerabilities in existing watermarking schemes.

Method: SemanticRegen uses a vision-language model for fine-grained captions, zero-shot segmentation for foreground masks, and an LLM-guided diffusion model to inpaint backgrounds, preserving objects and style.

Result: SemanticRegen defeats TreeRing watermark (p = 0.10 > 0.05) and reduces bit-accuracy below 0.75 for other schemes, maintaining high perceptual quality (masked SSIM = 0.94 +/- 0.01).

Conclusion: Current watermark defenses are insufficient against adaptive, semantics-aware attacks, necessitating more resilient algorithms.

Abstract: As AI-generated imagery becomes ubiquitous, invisible watermarks have emerged
as a primary line of defense for copyright and provenance. The newest
watermarking schemes embed semantic signals - content-aware patterns that are
designed to survive common image manipulations - yet their true robustness
against adaptive adversaries remains under-explored. We expose a previously
unreported vulnerability and introduce SemanticRegen, a three-stage, label-free
attack that erases state-of-the-art semantic and invisible watermarks while
leaving an image's apparent meaning intact. Our pipeline (i) uses a
vision-language model to obtain fine-grained captions, (ii) extracts foreground
masks with zero-shot segmentation, and (iii) inpaints only the background via
an LLM-guided diffusion model, thereby preserving salient objects and style
cues. Evaluated on 1,000 prompts across four watermarking systems - TreeRing,
StegaStamp, StableSig, and DWT/DCT - SemanticRegen is the only method to defeat
the semantic TreeRing watermark (p = 0.10 > 0.05) and reduces bit-accuracy
below 0.75 for the remaining schemes, all while maintaining high perceptual
quality (masked SSIM = 0.94 +/- 0.01). We further introduce masked SSIM (mSSIM)
to quantify fidelity within foreground regions, showing that our attack
achieves up to 12 percent higher mSSIM than prior diffusion-based attackers.
These results highlight an urgent gap between current watermark defenses and
the capabilities of adaptive, semantics-aware adversaries, underscoring the
need for watermarking algorithms that are resilient to content-preserving
regenerative attacks.

</details>


### [120] [EventDiff: A Unified and Efficient Diffusion Model Framework for Event-based Video Frame Interpolation](https://arxiv.org/pdf/2505.08235)
*Hanle Zheng, Xujie Han, Zegang Peng, Shangbin Zhang, Guangxun Du, Zhuo Zou, Xilin Wang, Jibin Wu, Hao Guo, Lei Deng*

Main category: cs.CV

TL;DR: EventDiff is a diffusion-based framework for Video Frame Interpolation (VFI) that leverages event cameras and a novel hybrid autoencoder for robust performance across diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing event-based VFI methods struggle with high-fidelity reconstruction under subtle motions due to reliance on explicit motion modeling. Diffusion models offer a promising alternative by avoiding explicit motion estimation.

Method: EventDiff uses an Event-Frame Hybrid AutoEncoder (HAE) with a Spatial-Temporal Cross Attention (STCA) module to fuse event streams and frames. It performs interpolation via a denoising diffusion process in latent space.

Result: EventDiff achieves state-of-the-art performance, outperforming existing methods by up to 1.98dB in PSNR on Vimeo90K-Triplet and showing superior results in SNU-FILM tasks. It also offers faster inference than other diffusion-based approaches.

Conclusion: EventDiff provides a unified and efficient solution for VFI, excelling in diverse scenarios and setting new benchmarks for event-based and diffusion-based methods.

Abstract: Video Frame Interpolation (VFI) is a fundamental yet challenging task in
computer vision, particularly under conditions involving large motion,
occlusion, and lighting variation. Recent advancements in event cameras have
opened up new opportunities for addressing these challenges. While existing
event-based VFI methods have succeeded in recovering large and complex motions
by leveraging handcrafted intermediate representations such as optical flow,
these designs often compromise high-fidelity image reconstruction under subtle
motion scenarios due to their reliance on explicit motion modeling. Meanwhile,
diffusion models provide a promising alternative for VFI by reconstructing
frames through a denoising process, eliminating the need for explicit motion
estimation or warping operations. In this work, we propose EventDiff, a unified
and efficient event-based diffusion model framework for VFI. EventDiff features
a novel Event-Frame Hybrid AutoEncoder (HAE) equipped with a lightweight
Spatial-Temporal Cross Attention (STCA) module that effectively fuses dynamic
event streams with static frames. Unlike previous event-based VFI methods,
EventDiff performs interpolation directly in the latent space via a denoising
diffusion process, making it more robust across diverse and challenging VFI
scenarios. Through a two-stage training strategy that first pretrains the HAE
and then jointly optimizes it with the diffusion model, our method achieves
state-of-the-art performance across multiple synthetic and real-world event VFI
datasets. The proposed method outperforms existing state-of-the-art event-based
VFI methods by up to 1.98dB in PSNR on Vimeo90K-Triplet and shows superior
performance in SNU-FILM tasks with multiple difficulty levels. Compared to the
emerging diffusion-based VFI approach, our method achieves up to 5.72dB PSNR
gain on Vimeo90K-Triplet and 4.24X faster inference.

</details>


### [121] [Congenital Heart Disease recognition using Deep Learning/Transformer models](https://arxiv.org/pdf/2505.08242)
*Aidar Amangeldi, Vladislav Yarovenko, Angsar Taigonyrov*

Main category: cs.CV

TL;DR: Dual-modality deep learning improves CHD diagnosis accuracy using sound and image data.


<details>
  <summary>Details</summary>
Motivation: CHD is a major cause of infant morbidity/mortality, and current non-invasive screening methods often fail.

Method: Dual-modality (sound and image) deep learning approach.

Result: 73.9% accuracy on ZCHSound dataset, 80.72% on DICOM Chest X-ray dataset.

Conclusion: Deep learning enhances CHD detection, with dual-modality showing promise.

Abstract: Congenital Heart Disease (CHD) remains a leading cause of infant morbidity
and mortality, yet non-invasive screening methods often yield false negatives.
Deep learning models, with their ability to automatically extract features, can
assist doctors in detecting CHD more effectively. In this work, we investigate
the use of dual-modality (sound and image) deep learning methods for CHD
diagnosis. We achieve 73.9% accuracy on the ZCHSound dataset and 80.72%
accuracy on the DICOM Chest X-ray dataset.

</details>


### [122] [Identifying Memorization of Diffusion Models through p-Laplace Analysis](https://arxiv.org/pdf/2505.08246)
*Jonathan Brokman, Amit Giloni, Omer Hofman, Roman Vainshtein, Hisashi Kojima, Guy Gilboa*

Main category: cs.CV

TL;DR: The paper explores using estimated score functions from diffusion models to compute p-Laplace operators for identifying memorized training data, demonstrating effectiveness in Gaussian mixture models and image generative models.


<details>
  <summary>Details</summary>
Motivation: To investigate if higher-order differentials (p-Laplace operators) derived from score functions can identify memorized data in generative models.

Method: Proposes a numerical p-Laplace approximation using learned score functions, tested on Gaussian mixture models and image generative models.

Result: The p-Laplace operator effectively identifies memorized training data and key features of the probability landscape.

Conclusion: The method successfully extends score function utility to higher-order analysis, enabling memorization detection in generative models.

Abstract: Diffusion models, today's leading image generative models, estimate the score
function, i.e. the gradient of the log probability of (perturbed) data samples,
without direct access to the underlying probability distribution. This work
investigates whether the estimated score function can be leveraged to compute
higher-order differentials, namely p-Laplace operators. We show here these
operators can be employed to identify memorized training data. We propose a
numerical p-Laplace approximation based on the learned score functions, showing
its effectiveness in identifying key features of the probability landscape. We
analyze the structured case of Gaussian mixture models, and demonstrate the
results carry-over to image generative models, where memorization
identification based on the p-Laplace operator is performed for the first time.

</details>


### [123] [CNN and ViT Efficiency Study on Tiny ImageNet and DermaMNIST Datasets](https://arxiv.org/pdf/2505.08259)
*Aidar Amangeldi, Angsar Taigonyrov, Muhammad Huzaid Jawad, Chinedu Emmanuel Mbonu*

Main category: cs.CV

TL;DR: Fine-tuned Vision Transformers match or outperform ResNet-18 in image classification, offering faster inference and lower complexity.


<details>
  <summary>Details</summary>
Motivation: Evaluate trade-offs between convolutional and transformer architectures for image classification, focusing on reducing latency and complexity while maintaining accuracy.

Method: Fine-tuning strategy applied to four Vision Transformer variants (Tiny, Small, Base, Large) on DermatologyMNIST and TinyImageNet, with systematic hyperparameter variations.

Result: Fine-tuned Vision Transformers achieve comparable or better performance than ResNet-18, with faster inference and fewer parameters.

Conclusion: Vision Transformers are viable for resource-constrained environments due to their efficiency and performance.

Abstract: This study evaluates the trade-offs between convolutional and
transformer-based architectures on both medical and general-purpose image
classification benchmarks. We use ResNet-18 as our baseline and introduce a
fine-tuning strategy applied to four Vision Transformer variants (Tiny, Small,
Base, Large) on DermatologyMNIST and TinyImageNet. Our goal is to reduce
inference latency and model complexity with acceptable accuracy degradation.
Through systematic hyperparameter variations, we demonstrate that appropriately
fine-tuned Vision Transformers can match or exceed the baseline's performance,
achieve faster inference, and operate with fewer parameters, highlighting their
viability for deployment in resource-constrained environments.

</details>


### [124] [Towards Anytime Optical Flow Estimation with Event Cameras](https://arxiv.org/pdf/2307.05033)
*Yaozu Ye, Hao Shi, Kailun Yang, Ze Wang, Xiaoting Yin, Lei Sun, Yaonan Wang, Kaiwei Wang*

Main category: cs.CV

TL;DR: EVA-Flow introduces a high-frame-rate event optical flow estimation method using low-frame-rate ground truth and a novel loss function, achieving low latency and strong performance.


<details>
  <summary>Details</summary>
Motivation: Existing event camera datasets lack high-frame-rate ground truth for optical flow, limiting research.

Method: Proposes Unified Voxel Grid representation and EVA-Flow network, with Rectified Flow Warp Loss for unsupervised assessment.

Result: Achieves 5ms latency, 200Hz motion estimation, and competitive performance on benchmarks.

Conclusion: EVA-Flow advances event-driven optical flow with high efficiency and generalization.

Abstract: Event cameras respond to changes in log-brightness at the millisecond level,
making them ideal for optical flow estimation. However, existing datasets from
event cameras provide only low frame rate ground truth for optical flow,
limiting the research potential of event-driven optical flow. To address this
challenge, we introduce a low-latency event representation, Unified Voxel Grid,
and propose EVA-Flow, an EVent-based Anytime Flow estimation network to produce
high-frame-rate event optical flow with only low-frame-rate optical flow ground
truth for supervision. Furthermore, we propose the Rectified Flow Warp Loss
(RFWL) for the unsupervised assessment of intermediate optical flow. A
comprehensive variety of experiments on MVSEC, DESC, and our EVA-FlowSet
demonstrates that EVA-Flow achieves competitive performance, super-low-latency
(5ms), time-dense motion estimation (200Hz), and strong generalization. Our
code will be available at https://github.com/Yaozhuwa/EVA-Flow.

</details>


### [125] [Few-shot Novel Category Discovery](https://arxiv.org/pdf/2505.08260)
*Chunming Li, Shidong Wang, Haofeng Zhang*

Main category: cs.CV

TL;DR: The paper introduces Few-Shot Novel Category Discovery (FSNCD), a flexible framework for identifying known classes and clustering novel ones using few labeled examples, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current Novel Category Discovery (NCD) methods are limited by transductive learning, making them impractical for real-world scenarios. Leveraging few labeled examples aligns with practical labeling ease.

Method: Proposes FSNCD, combining few-shot learning with novel category discovery. Introduces Semi-supervised Hierarchical Clustering (SHC) and Uncertainty-aware K-means Clustering (UKC) for improved reasoning.

Result: Extensive experiments on five datasets show leading performance across various task settings.

Conclusion: FSNCD effectively bridges the gap between few-shot learning and novel category discovery, offering practical and adaptable solutions.

Abstract: The recently proposed Novel Category Discovery (NCD) adapt paradigm of
transductive learning hinders its application in more real-world scenarios. In
fact, few labeled data in part of new categories can well alleviate this
burden, which coincides with the ease that people can label few of new category
data. Therefore, this paper presents a new setting in which a trained agent is
able to flexibly switch between the tasks of identifying examples of known
(labelled) classes and clustering novel (completely unlabeled) classes as the
number of query examples increases by leveraging knowledge learned from only a
few (handful) support examples. Drawing inspiration from the discovery of novel
categories using prior-based clustering algorithms, we introduce a novel
framework that further relaxes its assumptions to the real-world open set level
by unifying the concept of model adaptability in few-shot learning. We refer to
this setting as Few-Shot Novel Category Discovery (FSNCD) and propose
Semi-supervised Hierarchical Clustering (SHC) and Uncertainty-aware K-means
Clustering (UKC) to examine the model's reasoning capabilities. Extensive
experiments and detailed analysis on five commonly used datasets demonstrate
that our methods can achieve leading performance levels across different task
settings and scenarios.

</details>


### [126] [RT-GAN: Recurrent Temporal GAN for Adding Lightweight Temporal Consistency to Frame-Based Domain Translation Approaches](https://arxiv.org/pdf/2310.00868)
*Shawn Mathew, Saad Nadeem, Alvin C. Goh, Arie Kaufman*

Main category: cs.CV

TL;DR: RT-GAN (Recurrent Temporal GAN) is introduced as a lightweight solution to add temporal consistency to colonoscopy AI models, reducing training requirements by 5x.


<details>
  <summary>Details</summary>
Motivation: Current colonoscopy AI models rely on individual frames due to storage constraints, lacking temporal consistency. Transitioning to temporally-consistent models is resource-intensive.

Method: RT-GAN, a tunable temporal parameter model, is proposed to enhance individual frame-based approaches with temporal consistency.

Result: RT-GAN reduces training resources by a factor of 5 and is validated on haustral fold segmentation and colonoscopy simulator video generation.

Conclusion: RT-GAN offers an efficient solution for temporal consistency in colonoscopy AI, with datasets and models made publicly available.

Abstract: Fourteen million colonoscopies are performed annually just in the U.S.
However, the videos from these colonoscopies are not saved due to storage
constraints (each video from a high-definition colonoscope camera can be in
tens of gigabytes). Instead, a few relevant individual frames are saved for
documentation/reporting purposes and these are the frames on which most current
colonoscopy AI models are trained on. While developing new unsupervised domain
translation methods for colonoscopy (e.g. to translate between real optical and
virtual/CT colonoscopy), it is thus typical to start with approaches that
initially work for individual frames without temporal consistency. Once an
individual-frame model has been finalized, additional contiguous frames are
added with a modified deep learning architecture to train a new model from
scratch for temporal consistency. This transition to temporally-consistent deep
learning models, however, requires significantly more computational and memory
resources for training. In this paper, we present a lightweight solution with a
tunable temporal parameter, RT-GAN (Recurrent Temporal GAN), for adding
temporal consistency to individual frame-based approaches that reduces training
requirements by a factor of 5. We demonstrate the effectiveness of our approach
on two challenging use cases in colonoscopy: haustral fold segmentation
(indicative of missed surface) and realistic colonoscopy simulator video
generation. We also release a first-of-its kind temporal dataset for
colonoscopy for the above use cases. The datasets, accompanying code, and
pretrained models will be made available on our Computational Endoscopy
Platform GitHub (https://github.com/nadeemlab/CEP). The supplementary video is
available at https://youtu.be/UMVP-uIXwWk.

</details>


### [127] [Open the Eyes of MPNN: Vision Enhances MPNN in Link Prediction](https://arxiv.org/pdf/2505.08266)
*Yanbin Wei, Xuehao Wang, Zhan Zhuang, Yang Chen, Shuhao Chen, Yulong Zhang, Yu Zhang, James Kwok*

Main category: cs.CV

TL;DR: GVN and E-GVN enhance MPNNs with visual perception for link prediction, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Visual perception is overlooked in MPNNs for link prediction, despite its intuitive potential.

Method: Proposed Graph Vision Network (GVN) and its efficient variant (E-GVN) to integrate vision structural awareness.

Result: GVN improves performance across seven datasets, including large-scale graphs, and achieves new SOTA results.

Conclusion: GVN introduces a promising direction for link prediction by combining MPNNs with visual perception.

Abstract: Message-passing graph neural networks (MPNNs) and structural features (SFs)
are cornerstones for the link prediction task. However, as a common and
intuitive mode of understanding, the potential of visual perception has been
overlooked in the MPNN community. For the first time, we equip MPNNs with
vision structural awareness by proposing an effective framework called Graph
Vision Network (GVN), along with a more efficient variant (E-GVN). Extensive
empirical results demonstrate that with the proposed frameworks, GVN
consistently benefits from the vision enhancement across seven link prediction
datasets, including challenging large-scale graphs. Such improvements are
compatible with existing state-of-the-art (SOTA) methods and GVNs achieve new
SOTA results, thereby underscoring a promising novel direction for link
prediction.

</details>


### [128] [IrrMap: A Large-Scale Comprehensive Dataset for Irrigation Method Mapping](https://arxiv.org/pdf/2505.08273)
*Nibir Chandra Mandal, Oishee Bintey Hoque, Abhijin Adiga, Samarth Swarup, Mandy Wilson, Lu Feng, Yangfeng Ji, Miaomiao Zhang, Geoffrey Fox, Madhav Marathe*

Main category: cs.CV

TL;DR: IrrMap is a large-scale dataset for irrigation method mapping, featuring multi-resolution satellite imagery and auxiliary data, spanning 1.1 million patches across the western U.S. It supports deep learning with standardized formats and includes tools for extension and benchmarking.


<details>
  <summary>Details</summary>
Motivation: To address the lack of large-scale, diverse datasets for irrigation mapping, enabling robust analysis and model training in agricultural geospatial applications.

Method: The dataset combines LandSat and Sentinel imagery with auxiliary data (crop type, land use, vegetation indices), standardized into 224x224 GeoTIFF patches. It includes train-test splits, dataloaders, and a pipeline for dataset generation.

Result: IrrMap provides insights into irrigation method distribution, spatial patterns, and area variations across crop groups and resolutions. It is openly released with benchmark models and tools.

Conclusion: IrrMap fills a critical gap in irrigation mapping datasets, offering a scalable, ML-ready resource for researchers and applications in agricultural geospatial analysis.

Abstract: We introduce IrrMap, the first large-scale dataset (1.1 million patches) for
irrigation method mapping across regions. IrrMap consists of multi-resolution
satellite imagery from LandSat and Sentinel, along with key auxiliary data such
as crop type, land use, and vegetation indices. The dataset spans 1,687,899
farms and 14,117,330 acres across multiple western U.S. states from 2013 to
2023, providing a rich and diverse foundation for irrigation analysis and
ensuring geospatial alignment and quality control. The dataset is ML-ready,
with standardized 224x224 GeoTIFF patches, the multiple input modalities,
carefully chosen train-test-split data, and accompanying dataloaders for
seamless deep learning model training andbenchmarking in irrigation mapping.
The dataset is also accompanied by a complete pipeline for dataset generation,
enabling researchers to extend IrrMap to new regions for irrigation data
collection or adapt it with minimal effort for other similar applications in
agricultural and geospatial analysis. We also analyze the irrigation method
distribution across crop groups, spatial irrigation patterns (using Shannon
diversity indices), and irrigated area variations for both LandSat and
Sentinel, providing insights into regional and resolution-based differences. To
promote further exploration, we openly release IrrMap, along with the derived
datasets, benchmark models, and pipeline code, through a GitHub repository:
https://github.com/Nibir088/IrrMap and Data repository:
https://huggingface.co/Nibir/IrrMap, providing comprehensive documentation and
implementation details.

</details>


### [129] [Disruptive Transformation of Artworks in Master-Disciple Relationships: The Case of Ukiyo-e Artworks](https://arxiv.org/pdf/2505.08284)
*Honna Shinichi, Akira Matsui*

Main category: cs.CV

TL;DR: Quantitative analysis of Ukiyo-e using machine learning reveals declining overall creativity but sustained high creativity in styles, offering insights into Eastern art evolution.


<details>
  <summary>Details</summary>
Motivation: To address the lack of extensive quantitative analysis in Eastern paintings, focusing on Ukiyo-e as a case study.

Method: Analyzed 11,000 high-resolution Ukiyo-e images using network-based creativity calculation.

Result: Overall creativity declined with cultural maturation, but style creativity remained high and segmented.

Conclusion: Provides new insights into Ukiyo-e and its cultural evolution, highlighting its significance in Eastern art analysis.

Abstract: Artwork research has long relied on human sensibility and subjective
judgment, but recent developments in machine learning have enabled the
quantitative assessment of features that humans could not discover. In Western
paintings, comprehensive analyses have been conducted from various perspectives
in conjunction with large databases, but such extensive analysis has not been
sufficiently conducted for Eastern paintings. Then, we focus on Ukiyo-e, a
traditional Japanese art form, as a case study of Eastern paintings, and
conduct a quantitative analysis of creativity in works of art using 11,000
high-resolution images. This involves using the concept of calculating
creativity from networks to analyze both the creativity of the artwork and that
of the artists. As a result, In terms of Ukiyo-e as a whole, it was found that
the creativity of its appearance has declined with the maturation of culture,
but in terms of style, it has become more segmented with the maturation of
culture and has maintained a high level of creativity. This not only provides
new insights into the study of Ukiyo-e but also shows how Ukiyo-e has evolved
within the ongoing cultural history, playing a culturally significant role in
the analysis of Eastern art.

</details>


### [130] [High-Quality Spatial Reconstruction and Orthoimage Generation Using Efficient 2D Gaussian Splatting](https://arxiv.org/pdf/2503.19703)
*Qian Wang, Zhihao Zhan, Jialei He, Zhituo Tu, Xiang Zhu, Jie Yuan*

Main category: cs.CV

TL;DR: The paper introduces a 2D Gaussian Splatting (2DGS) method for generating True Digital Orthophoto Maps (TDOMs) without relying on traditional DSM and occlusion detection, offering high precision and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional TDOM generation methods are computationally expensive and error-prone due to complex processes like DSM and occlusion detection. The paper aims to provide a simpler, more efficient alternative.

Method: The proposed technique uses 2DGS with depth map generation to retrieve spatial information for each pixel, enabling high-precision scene reconstruction. A divide-and-conquer strategy ensures efficient training and rendering for high-resolution TDOMs.

Result: Experiments show the method efficiently reconstructs large-scale scenes and models terrain with high precision, maintaining quality on complex terrains and thin structures.

Conclusion: The 2DGS-based approach provides accurate spatial data, enhancing planning and decision-making for applications like urban planning and environmental monitoring.

Abstract: Highly accurate geometric precision and dense image features characterize
True Digital Orthophoto Maps (TDOMs), which are in great demand for
applications such as urban planning, infrastructure management, and
environmental monitoring.Traditional TDOM generation methods need sophisticated
processes, such as Digital Surface Models (DSM) and occlusion detection, which
are computationally expensive and prone to errors.This work presents an
alternative technique rooted in 2D Gaussian Splatting (2DGS), free of explicit
DSM and occlusion detection. With depth map generation, spatial information for
every pixel within the TDOM is retrieved and can reconstruct the scene with
high precision. Divide-and-conquer strategy achieves excellent GS training and
rendering with high-resolution TDOMs at a lower resource cost, which preserves
higher quality of rendering on complex terrain and thin structure without a
decrease in efficiency. Experimental results demonstrate the efficiency of
large-scale scene reconstruction and high-precision terrain modeling. This
approach provides accurate spatial data, which assists users in better planning
and decision-making based on maps.

</details>


### [131] [FauForensics: Boosting Audio-Visual Deepfake Detection with Facial Action Units](https://arxiv.org/pdf/2505.08294)
*Jian Wang, Baoyuan Wu, Li Liu, Qingshan Liu*

Main category: cs.CV

TL;DR: Proposes FauForensics, a framework using facial action units (FAUs) for robust multimodal deepfake detection, outperforming existing methods by 4.83% on average.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection methods struggle with multimodal manipulations due to poor handling of heterogeneous features and dataset generalization.

Method: Uses biologically invariant FAUs as forgery-resistant representations and a fusion module for fine-grained frame-wise audiovisual similarity computation.

Result: Achieves SOTA performance on FakeAVCeleb and LAV-DF datasets with superior cross-dataset generalizability.

Conclusion: FauForensics effectively addresses multimodal deepfake detection challenges, offering improved accuracy and generalization.

Abstract: The rapid evolution of generative AI has increased the threat of realistic
audio-visual deepfakes, demanding robust detection methods. Existing solutions
primarily address unimodal (audio or visual) forgeries but struggle with
multimodal manipulations due to inadequate handling of heterogeneous modality
features and poor generalization across datasets. To this end, we propose a
novel framework called FauForensics by introducing biologically invariant
facial action units (FAUs), which is a quantitative descriptor of facial muscle
activity linked to emotion physiology. It serves as forgery-resistant
representations that reduce domain dependency while capturing subtle dynamics
often disrupted in synthetic content. Besides, instead of comparing entire
video clips as in prior works, our method computes fine-grained frame-wise
audiovisual similarities via a dedicated fusion module augmented with learnable
cross-modal queries. It dynamically aligns temporal-spatial lip-audio
relationships while mitigating multi-modal feature heterogeneity issues.
Experiments on FakeAVCeleb and LAV-DF show state-of-the-art (SOTA) performance
and superior cross-dataset generalizability with up to an average of 4.83\%
than existing methods.

</details>


### [132] [Learning Phase Distortion with Selective State Space Models for Video Turbulence Mitigation](https://arxiv.org/pdf/2504.02697)
*Xingguang Zhang, Nicholas Chimitt, Xijun Wang, Yu Yuan, Stanley H. Chan*

Main category: cs.CV

TL;DR: A new turbulence mitigation method, MambaTM, combines a selective state space model and learned latent phase distortion to improve image quality with linear computational complexity.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning-based turbulence mitigation methods are slow, memory-intensive, and lack generalization, with limited spatial or temporal handling capabilities.

Method: Proposes MambaTM, a turbulence mitigation network with a global receptive field and linear complexity, and Learned Latent Phase Distortion (LPD) to guide phase distortion estimation.

Result: Outperforms state-of-the-art networks on synthetic and real-world benchmarks with faster inference speed.

Conclusion: The method effectively addresses turbulence degradation with improved efficiency and accuracy.

Abstract: Atmospheric turbulence is a major source of image degradation in long-range
imaging systems. Although numerous deep learning-based turbulence mitigation
(TM) methods have been proposed, many are slow, memory-hungry, and do not
generalize well. In the spatial domain, methods based on convolutional
operators have a limited receptive field, so they cannot handle a large spatial
dependency required by turbulence. In the temporal domain, methods relying on
self-attention can, in theory, leverage the lucky effects of turbulence, but
their quadratic complexity makes it difficult to scale to many frames.
Traditional recurrent aggregation methods face parallelization challenges.
  In this paper, we present a new TM method based on two concepts: (1) A
turbulence mitigation network based on the Selective State Space Model
(MambaTM). MambaTM provides a global receptive field in each layer across
spatial and temporal dimensions while maintaining linear computational
complexity. (2) Learned Latent Phase Distortion (LPD). LPD guides the state
space model. Unlike classical Zernike-based representations of phase
distortion, the new LPD map uniquely captures the actual effects of turbulence,
significantly improving the model's capability to estimate degradation by
reducing the ill-posedness. Our proposed method exceeds current
state-of-the-art networks on various synthetic and real-world TM benchmarks
with significantly faster inference speed.

</details>


### [133] [Knowledge-Informed Deep Learning for Irrigation Type Mapping from Remote Sensing](https://arxiv.org/pdf/2505.08302)
*Oishee Bintey Hoque, Nibir Chandra Mandal, Abhijin Adiga, Samarth Swarup, Sayjro Kossi Nouwakpo, Amanda Wilson, Madhav Marathe*

Main category: cs.CV

TL;DR: KIIM improves irrigation mapping using a Swin-Transformer with specialized projection, spatial attention, cross-attention, and weighted ensemble, achieving significant IoU gains and reducing training data needs.


<details>
  <summary>Details</summary>
Motivation: Accurate irrigation mapping is vital for sustainable agriculture, but current spectral-based models fail due to landscape complexity and limited data.

Method: KIIM combines crop-to-irrigation projection, spatial attention, bi-directional cross-attention, and weighted ensemble for multi-modal data integration.

Result: 22.9% IoU improvement over baseline, 71.4% for drip irrigation, and 51% boost in cross-state transfer with limited data. Achieves baseline with 40% training data.

Conclusion: KIIM enables efficient, large-scale irrigation mapping with reduced manual labeling, making it cost-effective and feasible.

Abstract: Accurate mapping of irrigation methods is crucial for sustainable
agricultural practices and food systems. However, existing models that rely
solely on spectral features from satellite imagery are ineffective due to the
complexity of agricultural landscapes and limited training data, making this a
challenging problem. We present Knowledge-Informed Irrigation Mapping (KIIM), a
novel Swin-Transformer based approach that uses (i) a specialized projection
matrix to encode crop to irrigation probability, (ii) a spatial attention map
to identify agricultural lands from non-agricultural lands, (iii)
bi-directional cross-attention to focus complementary information from
different modalities, and (iv) a weighted ensemble for combining predictions
from images and crop information. Our experimentation on five states in the US
shows up to 22.9\% (IoU) improvement over baseline with a 71.4% (IoU)
improvement for hard-to-classify drip irrigation. In addition, we propose a
two-phase transfer learning approach to enhance cross-state irrigation mapping,
achieving a 51% IoU boost in a state with limited labeled data. The ability to
achieve baseline performance with only 40% of the training data highlights its
efficiency, reducing the dependency on extensive manual labeling efforts and
making large-scale, automated irrigation mapping more feasible and
cost-effective.

</details>


### [134] [Inter-event Interval Microscopy for Event Cameras](https://arxiv.org/pdf/2504.04924)
*Changqing Su, Yanqin Chen, Zihan Lin, Zhen Cheng, You Zhou, Bo Xiong, Zhaofei Yu, Tiejun Huang*

Main category: cs.CV

TL;DR: The paper introduces Inter-event Interval Microscopy (IEIM), a method for converting event camera data into intensity images for both static and dynamic scenes in fluorescence microscopy, achieving high resolution and dynamic range.


<details>
  <summary>Details</summary>
Motivation: Event cameras capture intensity changes but struggle with reconstructing intensity images, especially for dynamic scenes. This paper aims to solve this challenge.

Method: IEIM quantifies time intervals between consecutive events to represent intensity, integrating a pulse light modulation device in a microscope setup.

Result: IEIM achieves superior spatial/temporal resolution and dynamic range with lower bandwidth, validated on the IEIMat dataset.

Conclusion: The proposed IEIM method is effective for event-to-intensity conversion in microscopy, with potential for public use via released code and dataset.

Abstract: Event cameras, an innovative bio-inspired sensor, differ from traditional
cameras by sensing changes in intensity rather than directly perceiving
intensity and recording these variations as a continuous stream of "events".
The intensity reconstruction from these sparse events has long been a
challenging problem. Previous approaches mainly focused on transforming
motion-induced events into videos or achieving intensity imaging for static
scenes by integrating modulation devices at the event camera acquisition end.
In this paper, for the first time, we achieve event-to-intensity conversion
using a static event camera for both static and dynamic scenes in fluorescence
microscopy. Unlike conventional methods that primarily rely on event
integration, the proposed Inter-event Interval Microscopy (IEIM) quantifies the
time interval between consecutive events at each pixel. With a fixed threshold
in the event camera, the time interval can precisely represent the intensity.
At the hardware level, the proposed IEIM integrates a pulse light modulation
device within a microscope equipped with an event camera, termed Pulse
Modulation-based Event-driven Fluorescence Microscopy. Additionally, we have
collected IEIMat dataset under various scenes including high dynamic range and
high-speed scenarios. Experimental results on the IEIMat dataset demonstrate
that the proposed IEIM achieves superior spatial and temporal resolution, as
well as a higher dynamic range, with lower bandwidth compared to other methods.
The code and the IEIMat dataset will be made publicly available.

</details>


### [135] [An incremental algorithm for non-convex AI-enhanced medical image processing](https://arxiv.org/pdf/2505.08324)
*Elena Morotti*

Main category: cs.CV

TL;DR: The paper proposes incDG, a hybrid framework combining deep learning and incremental model-based optimization to solve non-convex inverse problems in imaging, achieving superior accuracy and stability.


<details>
  <summary>Details</summary>
Motivation: Non-convex regularized inverse problems are challenging but yield high-quality solutions, especially in medical imaging, where enhancing clinically relevant features is crucial.

Method: incDG integrates deep learning (for initializations) with non-convex variational solvers (for refinement) in an incremental optimization framework.

Result: incDG outperforms conventional solvers and deep learning methods in medical image deblurring and tomographic reconstruction, even without ground truth training.

Conclusion: incDG is a robust and practical tool for solving non-convex inverse problems, combining AI efficiency with theoretical guarantees.

Abstract: Solving non-convex regularized inverse problems is challenging due to their
complex optimization landscapes and multiple local minima. However, these
models remain widely studied as they often yield high-quality, task-oriented
solutions, particularly in medical imaging, where the goal is to enhance
clinically relevant features rather than merely minimizing global error. We
propose incDG, a hybrid framework that integrates deep learning with
incremental model-based optimization to efficiently approximate the
$\ell_0$-optimal solution of imaging inverse problems. Built on the Deep Guess
strategy, incDG exploits a deep neural network to generate effective
initializations for a non-convex variational solver, which refines the
reconstruction through regularized incremental iterations. This design combines
the efficiency of Artificial Intelligence (AI) tools with the theoretical
guarantees of model-based optimization, ensuring robustness and stability. We
validate incDG on TpV-regularized optimization tasks, demonstrating its
effectiveness in medical image deblurring and tomographic reconstruction across
diverse datasets, including synthetic images, brain CT slices, and
chest-abdomen scans. Results show that incDG outperforms both conventional
iterative solvers and deep learning-based methods, achieving superior accuracy
and stability. Moreover, we confirm that training incDG without ground truth
does not significantly degrade performance, making it a practical and powerful
tool for solving non-convex inverse problems in imaging and beyond.

</details>


### [136] [Assessing the Feasibility of Internet-Sourced Video for Automatic Cattle Lameness Detection](https://arxiv.org/pdf/2504.16404)
*Md Fahimuzzman Sohan, A. H. Abdul Hafez, Raid Alzubi*

Main category: cs.CV

TL;DR: A deep learning-based model (3D CNN and ConvLSTM2D) detects cattle lameness from video data with high accuracy (90% for 3D CNN), simplifying traditional multi-stage approaches.


<details>
  <summary>Details</summary>
Motivation: Cattle lameness causes pain and disrupts essential activities; current detection methods are complex. This study aims to simplify lameness detection using deep learning.

Method: Used 50 videos of 40 cattle (half normal, half lame) for training. Applied data augmentation and tested two models: ConvLSTM2D and 3D CNN.

Result: 3D CNN achieved 90% accuracy, 90.9% precision/recall/F1-score; ConvLSTM2D had 85% accuracy.

Conclusion: Deep learning models, especially 3D CNN, effectively classify lameness, offering a simpler alternative to traditional methods.

Abstract: Cattle lameness is often caused by hoof injuries or interdigital dermatitis,
leads to pain and significantly impacts essential physiological activities such
as walking, feeding, and drinking. This study presents a deep learning-based
model for detecting cattle lameness, sickness, or gait abnormalities using
publicly available video data. The dataset consists of 50 unique videos from 40
individual cattle, recorded from various angles in both indoor and outdoor
environments. Half of the dataset represents naturally walking
(normal/non-lame) cattle, while the other half consists of cattle exhibiting
gait abnormalities (lame). To enhance model robustness and generalizability,
data augmentation was applied to the training data. The pre-processed videos
were then classified using two deep learning models: ConvLSTM2D and 3D CNN. A
comparative analysis of the results demonstrates strong classification
performance. Specifically, the 3D CNN model achieved a video-level
classification accuracy of 90%, with precision, recall, and f1-score of 90.9%,
90.9%, and 90.91% respectively. The ConvLSTM2D model exhibited a slightly lower
accuracy of 85%. This study highlights the effectiveness of directly applying
classification models to learn spatiotemporal features from video data,
offering an alternative to traditional multi-stage approaches that typically
involve object detection, pose estimation, and feature extraction. Besides, the
findings demonstrate that the proposed deep learning models, particularly the
3D CNN, effectively classify and detect lameness in cattle while simplifying
the processing pipeline.

</details>


### [137] [A computer vision-based model for occupancy detection using low-resolution thermal images](https://arxiv.org/pdf/2505.08336)
*Xue Cui, Vincent Gbouna Zakka, Minhyun Lee*

Main category: cs.CV

TL;DR: A study developed an occupancy detection model using low-resolution thermal images and CV techniques, achieving high performance while addressing privacy and resource concerns.


<details>
  <summary>Details</summary>
Motivation: Traditional HVAC systems lack occupancy awareness, and RGB-based methods raise privacy issues. Low-resolution thermal images offer a privacy-friendly alternative.

Method: Transfer learning was applied to fine-tune the YOLOv5 model for occupancy detection using low-resolution thermal images.

Result: The model achieved near-perfect precision, recall, and mAP50 scores (approaching 1.000).

Conclusion: The model effectively balances privacy, performance, and resource efficiency for HVAC occupancy detection.

Abstract: Occupancy plays an essential role in influencing the energy consumption and
operation of heating, ventilation, and air conditioning (HVAC) systems.
Traditional HVAC typically operate on fixed schedules without considering
occupancy. Advanced occupant-centric control (OCC) adopted occupancy status in
regulating HVAC operations. RGB images combined with computer vision (CV)
techniques are widely used for occupancy detection, however, the detailed
facial and body features they capture raise significant privacy concerns.
Low-resolution thermal images offer a non-invasive solution that mitigates
privacy issues. The study developed an occupancy detection model utilizing
low-resolution thermal images and CV techniques, where transfer learning was
applied to fine-tune the You Only Look Once version 5 (YOLOv5) model. The
developed model ultimately achieved satisfactory performance, with precision,
recall, mAP50, and mAP50 values approaching 1.000. The contributions of this
model lie not only in mitigating privacy concerns but also in reducing
computing resource demands.

</details>


### [138] [FAD: Frequency Adaptation and Diversion for Cross-domain Few-shot Learning](https://arxiv.org/pdf/2505.08349)
*Ruixiao Shi, Fu Feng, Yucheng Xie, Jing Wang, Xin Geng*

Main category: cs.CV

TL;DR: FAD introduces frequency-aware adaptation for cross-domain few-shot learning, outperforming spatial-only methods by addressing spectral variations.


<details>
  <summary>Details</summary>
Motivation: Spatially similar images can differ spectrally, limiting generalization in CD-FSL. Existing methods overlook frequency-specific variations.

Method: FAD transforms features into frequency domain, partitions into bands, and adapts each band with tailored convolutional branches.

Result: FAD outperforms state-of-the-art methods on Meta-Dataset, validating frequency-domain adaptation.

Conclusion: Frequency-aware adaptation improves generalization in CD-FSL by addressing spectral distinctions.

Abstract: Cross-domain few-shot learning (CD-FSL) requires models to generalize from
limited labeled samples under significant distribution shifts. While recent
methods enhance adaptability through lightweight task-specific modules, they
operate solely in the spatial domain and overlook frequency-specific variations
that are often critical for robust transfer. We observe that spatially similar
images across domains can differ substantially in their spectral
representations, with low and high frequencies capturing complementary semantic
information at coarse and fine levels. This indicates that uniform spatial
adaptation may overlook these spectral distinctions, thus constraining
generalization. To address this, we introduce Frequency Adaptation and
Diversion (FAD), a frequency-aware framework that explicitly models and
modulates spectral components. At its core is the Frequency Diversion Adapter,
which transforms intermediate features into the frequency domain using the
discrete Fourier transform (DFT), partitions them into low, mid, and
high-frequency bands via radial masks, and reconstructs each band using inverse
DFT (IDFT). Each frequency band is then adapted using a dedicated convolutional
branch with a kernel size tailored to its spectral scale, enabling targeted and
disentangled adaptation across frequencies. Extensive experiments on the
Meta-Dataset benchmark demonstrate that FAD consistently outperforms
state-of-the-art methods on both seen and unseen domains, validating the
utility of frequency-domain representations and band-wise adaptation for
improving generalization in CD-FSL.

</details>


### [139] [STORYANCHORS: Generating Consistent Multi-Scene Story Frames for Long-Form Narratives](https://arxiv.org/pdf/2505.08350)
*Bo Wang, Haoyang Huang, Zhiyin Lu, Fengyuan Liu, Guoqing Ma, Jianlong Yuan, Yuan Zhang, Nan Duan*

Main category: cs.CV

TL;DR: StoryAnchors is a framework for generating multi-scene story frames with temporal consistency, character continuity, and scene diversity, outperforming existing models and matching GPT-4o in narrative quality.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generating high-quality, temporally consistent multi-scene story frames with rich narratives and editable outputs.

Method: Uses a bidirectional story generator with past and future contexts, Multi-Event Story Frame Labeling, and Progressive Story Frame Training for narrative flow and event dynamics.

Result: Outperforms open-source models in consistency, coherence, and diversity; matches GPT-4o in narrative quality.

Conclusion: StoryAnchors advances story-driven frame generation with scalability, flexibility, and editability for future research.

Abstract: This paper introduces StoryAnchors, a unified framework for generating
high-quality, multi-scene story frames with strong temporal consistency. The
framework employs a bidirectional story generator that integrates both past and
future contexts to ensure temporal consistency, character continuity, and
smooth scene transitions throughout the narrative. Specific conditions are
introduced to distinguish story frame generation from standard video synthesis,
facilitating greater scene diversity and enhancing narrative richness. To
further improve generation quality, StoryAnchors integrates Multi-Event Story
Frame Labeling and Progressive Story Frame Training, enabling the model to
capture both overarching narrative flow and event-level dynamics. This approach
supports the creation of editable and expandable story frames, allowing for
manual modifications and the generation of longer, more complex sequences.
Extensive experiments show that StoryAnchors outperforms existing open-source
models in key areas such as consistency, narrative coherence, and scene
diversity. Its performance in narrative consistency and story richness is also
on par with GPT-4o. Ultimately, StoryAnchors pushes the boundaries of
story-driven frame generation, offering a scalable, flexible, and highly
editable foundation for future research.

</details>


### [140] [DArFace: Deformation Aware Robustness for Low Quality Face Recognition](https://arxiv.org/pdf/2505.08423)
*Sadaf Gulshad, Abdullah Aldahlawi Thakaa*

Main category: cs.CV

TL;DR: DArFace improves facial recognition robustness by modeling both global and local deformations in low-quality images, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Facial recognition systems struggle with low-quality images due to overlooked local deformations, creating a performance gap in real-world scenarios.

Method: DArFace adversarially integrates global transformations and local elastic deformations during training, using a contrastive objective for identity consistency.

Result: DArFace achieves state-of-the-art performance on benchmarks like TinyFace, IJB-B, and IJB-C.

Conclusion: Modeling local deformations significantly enhances robustness in facial recognition for low-quality images.

Abstract: Facial recognition systems have achieved remarkable success by leveraging
deep neural networks, advanced loss functions, and large-scale datasets.
However, their performance often deteriorates in real-world scenarios involving
low-quality facial images. Such degradations, common in surveillance footage or
standoff imaging include low resolution, motion blur, and various distortions,
resulting in a substantial domain gap from the high-quality data typically used
during training. While existing approaches attempt to address robustness by
modifying network architectures or modeling global spatial transformations,
they frequently overlook local, non-rigid deformations that are inherently
present in real-world settings. In this work, we introduce DArFace, a
Deformation-Aware robust Face recognition framework that enhances robustness to
such degradations without requiring paired high- and low-quality training
samples. Our method adversarially integrates both global transformations (e.g.,
rotation, translation) and local elastic deformations during training to
simulate realistic low-quality conditions. Moreover, we introduce a contrastive
objective to enforce identity consistency across different deformed views.
Extensive evaluations on low-quality benchmarks including TinyFace, IJB-B, and
IJB-C demonstrate that DArFace surpasses state-of-the-art methods, with
significant gains attributed to the inclusion of local deformation modeling.

</details>


### [141] [DHECA-SuperGaze: Dual Head-Eye Cross-Attention and Super-Resolution for Unconstrained Gaze Estimation](https://arxiv.org/pdf/2505.08426)
*Franko Šikić, Donik Vršnak, Sven Lončarić*

Main category: cs.CV

TL;DR: DHECA-SuperGaze improves gaze estimation via super-resolution and a dual head-eye cross-attention module, outperforming SOTA methods on Gaze360 and GFIE datasets.


<details>
  <summary>Details</summary>
Motivation: Address challenges in unconstrained gaze estimation, such as low-resolution images and inadequate head-eye interaction modeling.

Method: Proposes a dual-branch convolutional backbone with super-resolution and a DHECA module for bidirectional feature refinement.

Result: Reduces angular error by 0.48° (Gaze360) and 2.95° (GFIE) in static settings, and 0.59° (Gaze360) and 3.00° (GFIE) in temporal settings.

Conclusion: DHECA-SuperGaze offers robust generalization, validated by cross-dataset testing, and rectifies annotation errors in Gaze360.

Abstract: Unconstrained gaze estimation is the process of determining where a subject
is directing their visual attention in uncontrolled environments. Gaze
estimation systems are important for a myriad of tasks such as driver
distraction monitoring, exam proctoring, accessibility features in modern
software, etc. However, these systems face challenges in real-world scenarios,
partially due to the low resolution of in-the-wild images and partially due to
insufficient modeling of head-eye interactions in current state-of-the-art
(SOTA) methods. This paper introduces DHECA-SuperGaze, a deep learning-based
method that advances gaze prediction through super-resolution (SR) and a dual
head-eye cross-attention (DHECA) module. Our dual-branch convolutional backbone
processes eye and multiscale SR head images, while the proposed DHECA module
enables bidirectional feature refinement between the extracted visual features
through cross-attention mechanisms. Furthermore, we identified critical
annotation errors in one of the most diverse and widely used gaze estimation
datasets, Gaze360, and rectified the mislabeled data. Performance evaluation on
Gaze360 and GFIE datasets demonstrates superior within-dataset performance of
the proposed method, reducing angular error (AE) by 0.48{\deg} (Gaze360) and
2.95{\deg} (GFIE) in static configurations, and 0.59{\deg} (Gaze360) and
3.00{\deg} (GFIE) in temporal settings compared to prior SOTA methods.
Cross-dataset testing shows improvements in AE of more than 1.53{\deg}
(Gaze360) and 3.99{\deg} (GFIE) in both static and temporal settings,
validating the robust generalization properties of our approach.

</details>


### [142] [Visual Image Reconstruction from Brain Activity via Latent Representation](https://arxiv.org/pdf/2505.08429)
*Yukiyasu Kamitani, Misato Tanaka, Ken Shirakawa*

Main category: cs.CV

TL;DR: The paper reviews advancements in visual image reconstruction from brain activity using DNNs and generative models, highlighting progress, challenges, and ethical considerations.


<details>
  <summary>Details</summary>
Motivation: To explore the evolution and current state of visual image reconstruction, addressing its potential and limitations in understanding neural coding and applications like brain-machine interfaces.

Method: The review traces the field's development, focusing on hierarchical latent representations, compositional strategies, and modular architectures in DNNs and generative models.

Result: Significant progress has been made, but challenges like zero-shot generalization and subjective perception modeling remain. Ethical concerns are also highlighted.

Conclusion: Visual image reconstruction holds promise for neural coding insights and applications, but requires diverse datasets, better metrics, and ethical safeguards.

Abstract: Visual image reconstruction, the decoding of perceptual content from brain
activity into images, has advanced significantly with the integration of deep
neural networks (DNNs) and generative models. This review traces the field's
evolution from early classification approaches to sophisticated reconstructions
that capture detailed, subjective visual experiences, emphasizing the roles of
hierarchical latent representations, compositional strategies, and modular
architectures. Despite notable progress, challenges remain, such as achieving
true zero-shot generalization for unseen images and accurately modeling the
complex, subjective aspects of perception. We discuss the need for diverse
datasets, refined evaluation metrics aligned with human perceptual judgments,
and compositional representations that strengthen model robustness and
generalizability. Ethical issues, including privacy, consent, and potential
misuse, are underscored as critical considerations for responsible development.
Visual image reconstruction offers promising insights into neural coding and
enables new psychological measurements of visual experiences, with applications
spanning clinical diagnostics and brain-machine interfaces.

</details>


### [143] [TT-DF: A Large-Scale Diffusion-Based Dataset and Benchmark for Human Body Forgery Detection](https://arxiv.org/pdf/2505.08437)
*Wenkui Yang, Zhida Zhang, Xiaoqiang Zhou, Junxian Duan, Jie Cao*

Main category: cs.CV

TL;DR: The paper introduces TikTok-DeepFake (TT-DF), a large-scale dataset for body forgery detection, and proposes TOF-Net, a detection model outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of datasets and detection methods for human body forgery, which lags behind facial forgery research.

Method: Creation of TT-DF dataset with diverse forgery methods and configurations, and development of TOF-Net, a model leveraging spatiotemporal inconsistencies and optical flow differences.

Result: TOF-Net achieves superior performance on TT-DF compared to state-of-the-art facial forgery detection models.

Conclusion: TT-DF and TOF-Net fill a critical gap in body forgery detection, offering a robust solution and benchmark for future research.

Abstract: The emergence and popularity of facial deepfake methods spur the vigorous
development of deepfake datasets and facial forgery detection, which to some
extent alleviates the security concerns about facial-related artificial
intelligence technologies. However, when it comes to human body forgery, there
has been a persistent lack of datasets and detection methods, due to the later
inception and complexity of human body generation methods. To mitigate this
issue, we introduce TikTok-DeepFake (TT-DF), a novel large-scale
diffusion-based dataset containing 6,120 forged videos with 1,378,857 synthetic
frames, specifically tailored for body forgery detection. TT-DF offers a wide
variety of forgery methods, involving multiple advanced human image animation
models utilized for manipulation, two generative configurations based on the
disentanglement of identity and pose information, as well as different
compressed versions. The aim is to simulate any potential unseen forged data in
the wild as comprehensively as possible, and we also furnish a benchmark on
TT-DF. Additionally, we propose an adapted body forgery detection model,
Temporal Optical Flow Network (TOF-Net), which exploits the spatiotemporal
inconsistencies and optical flow distribution differences between natural data
and forged data. Our experiments demonstrate that TOF-Net achieves favorable
performance on TT-DF, outperforming current state-of-the-art extendable facial
forgery detection models. For our TT-DF dataset, please refer to
https://github.com/HashTAG00002/TT-DF.

</details>


### [144] [A Survey of 3D Reconstruction with Event Cameras: From Event-based Geometry to Neural 3D Rendering](https://arxiv.org/pdf/2505.08438)
*Chuanzhi Xu, Haoxian Zhou, Langyi Chen, Haodong Chen, Ying Zhou, Vera Chung, Qiang Qu*

Main category: cs.CV

TL;DR: A survey on 3D reconstruction using event cameras, categorizing methods by input modality and approach, summarizing datasets, and highlighting research gaps.


<details>
  <summary>Details</summary>
Motivation: Event cameras offer advantages like sparse, temporally rich data for 3D reconstruction in extreme conditions, but no comprehensive review exists.

Method: Categorizes works into stereo, monocular, and multimodal systems, and further by geometry-based, deep learning, and neural rendering techniques.

Result: Organizes methods chronologically, summarizes datasets, and identifies limitations in data, evaluation, and dynamic scenes.

Conclusion: Aims to be a reference and roadmap for future research in event-driven 3D reconstruction.

Abstract: Event cameras have emerged as promising sensors for 3D reconstruction due to
their ability to capture per-pixel brightness changes asynchronously. Unlike
conventional frame-based cameras, they produce sparse and temporally rich data
streams, which enable more accurate 3D reconstruction and open up the
possibility of performing reconstruction in extreme environments such as
high-speed motion, low light, or high dynamic range scenes. In this survey, we
provide the first comprehensive review focused exclusively on 3D reconstruction
using event cameras. The survey categorises existing works into three major
types based on input modality - stereo, monocular, and multimodal systems, and
further classifies them by reconstruction approach, including geometry-based,
deep learning-based, and recent neural rendering techniques such as Neural
Radiance Fields and 3D Gaussian Splatting. Methods with a similar research
focus were organised chronologically into the most subdivided groups. We also
summarise public datasets relevant to event-based 3D reconstruction. Finally,
we highlight current research limitations in data availability, evaluation,
representation, and dynamic scene handling, and outline promising future
research directions. This survey aims to serve as a comprehensive reference and
a roadmap for future developments in event-driven 3D reconstruction.

</details>


### [145] [VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large Video Language Models](https://arxiv.org/pdf/2505.08455)
*Pritam Sarkar, Ali Etemad*

Main category: cs.CV

TL;DR: The paper introduces VCRBench, a benchmark for evaluating video-based causal reasoning in LVLMs, and proposes RRD, a modular approach to improve performance.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs lack dedicated benchmarks for video-based causal reasoning, limiting their evaluation and improvement.

Method: The authors create VCRBench using shuffled procedural videos and propose RRD, which decomposes reasoning into recognition and reasoning tasks.

Result: LVLMs struggle with long-form causal reasoning, but RRD improves accuracy by up to 25.2%.

Conclusion: LVLMs rely heavily on language knowledge for complex reasoning, and RRD offers a promising direction for enhancing their capabilities.

Abstract: Despite recent advances in video understanding, the capabilities of Large
Video Language Models (LVLMs) to perform video-based causal reasoning remains
underexplored, largely due to the absence of relevant and dedicated benchmarks
for evaluating causal reasoning in visually grounded and goal-driven settings.
To fill this gap, we introduce a novel benchmark named Video-based long-form
Causal Reasoning (VCRBench). We create VCRBench using procedural videos of
simple everyday activities, where the steps are deliberately shuffled with each
clip capturing a key causal event, to test whether LVLMs can identify, reason
about, and correctly sequence the events needed to accomplish a specific goal.
Moreover, the benchmark is carefully designed to prevent LVLMs from exploiting
linguistic shortcuts, as seen in multiple-choice or binary QA formats, while
also avoiding the challenges associated with evaluating open-ended QA. Our
evaluation of state-of-the-art LVLMs on VCRBench suggests that these models
struggle with video-based long-form causal reasoning, primarily due to their
difficulty in modeling long-range causal dependencies directly from visual
observations. As a simple step toward enabling such capabilities, we propose
Recognition-Reasoning Decomposition (RRD), a modular approach that breaks
video-based causal reasoning into two sub-tasks of video recognition and causal
reasoning. Our experiments on VCRBench show that RRD significantly boosts
accuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysis
reveals interesting insights, for instance, that LVLMs primarily rely on
language knowledge for complex video-based long-form causal reasoning tasks.

</details>


### [146] [A Deep Learning-Driven Framework for Inhalation Injury Grading Using Bronchoscopy Images](https://arxiv.org/pdf/2505.08517)
*Yifan Li, Alan W Pang, Jo Woon Chong*

Main category: cs.CV

TL;DR: A deep learning framework using enhanced StarGAN improves inhalation injury grading by generating high-quality synthetic bronchoscopy images, boosting classification accuracy by 11.11%.


<details>
  <summary>Details</summary>
Motivation: Traditional methods like AIS are subjective and poorly correlate with clinical outcomes, necessitating an objective, data-driven approach.

Method: Enhanced StarGAN with Patch Loss and SSIM Loss generates synthetic bronchoscopy images, evaluated using Swin Transformer and FID.

Result: Achieved 77.78% accuracy (11.11% improvement) and lowest FID (30.06), with synthetic images validated by burn surgeons.

Conclusion: Enhanced StarGAN effectively addresses data scarcity and enhances classification accuracy for inhalation injury grading.

Abstract: Inhalation injuries face a challenge in clinical diagnosis and grading due to
the limitations of traditional methods, such as Abbreviated Injury Score (AIS),
which rely on subjective assessments and show weak correlations with clinical
outcomes. This study introduces a novel deep learning-based framework for
grading inhalation injuries using bronchoscopy images with the duration of
mechanical ventilation as an objective metric. To address the scarcity of
medical imaging data, we propose enhanced StarGAN, a generative model that
integrates Patch Loss and SSIM Loss to improve synthetic images' quality and
clinical relevance. The augmented dataset generated by enhanced StarGAN
significantly improved classification performance when evaluated using the Swin
Transformer, achieving an accuracy of 77.78%, an 11.11% improvement over the
original dataset. Image quality was assessed using the Fr\'echet Inception
Distance (FID), where Enhanced StarGAN achieved the lowest FID of 30.06,
outperforming baseline models. Burn surgeons confirmed the realism and clinical
relevance of the generated images, particularly the preservation of bronchial
structures and color distribution. These results highlight the potential of
enhanced StarGAN in addressing data limitations and improving classification
accuracy for inhalation injury grading.

</details>


### [147] [Attention-based Generative Latent Replay: A Continual Learning Approach for WSI Analysis](https://arxiv.org/pdf/2505.08524)
*Pratibha Kumari, Daniel Reisenbüchler, Afshin Bozorgpour, Nadine S. Schaadt, Friedrich Feuerhake, Dorit Merhof*

Main category: cs.CV

TL;DR: AGLR-CL is a privacy-aware continual learning framework for WSI classification, using GMMs and attention-based filtering to handle domain shifts without storing original data.


<details>
  <summary>Details</summary>
Motivation: Address domain shifts in WSI classification due to variations in organs, diseases, or institutions.

Method: Uses GMMs to synthesize WSI representations and patch distributions, with attention-based filtering for salient patches.

Result: Outperforms buffer-free methods and matches buffer-based solutions, validated on diverse datasets.

Conclusion: AGLR-CL effectively retains prior knowledge and adapts to new domains, offering privacy-preserving continual learning.

Abstract: Whole slide image (WSI) classification has emerged as a powerful tool in
computational pathology, but remains constrained by domain shifts, e.g., due to
different organs, diseases, or institution-specific variations. To address this
challenge, we propose an Attention-based Generative Latent Replay Continual
Learning framework (AGLR-CL), in a multiple instance learning (MIL) setup for
domain incremental WSI classification. Our method employs Gaussian Mixture
Models (GMMs) to synthesize WSI representations and patch count distributions,
preserving knowledge of past domains without explicitly storing original data.
A novel attention-based filtering step focuses on the most salient patch
embeddings, ensuring high-quality synthetic samples. This privacy-aware
strategy obviates the need for replay buffers and outperforms other buffer-free
counterparts while matching the performance of buffer-based solutions. We
validate AGLR-CL on clinically relevant biomarker detection and molecular
status prediction across multiple public datasets with diverse centers, organs,
and patient cohorts. Experimental results confirm its ability to retain prior
knowledge and adapt to new domains, offering an effective, privacy-preserving
avenue for domain incremental continual learning in WSI classification.

</details>


### [148] [Dynamic Snake Upsampling Operater and Boundary-Skeleton Weighted Loss for Tubular Structure Segmentation](https://arxiv.org/pdf/2505.08525)
*Yiqi Chen, Ganghai Huang, Sheng Zhang, Jianglin Dai*

Main category: cs.CV

TL;DR: The paper introduces dynamic snake upsampling and boundary-skeleton weighted loss to improve segmentation of tubular structures, enhancing accuracy and topological consistency.


<details>
  <summary>Details</summary>
Motivation: Conventional upsampling fails for slender, curved tubular structures, necessitating better methods for reliable segmentation.

Method: Proposes dynamic snake upsampling with adaptive sampling and a skeleton-to-boundary weighted loss for balanced focus on structure and boundaries.

Result: Experiments show improved segmentation accuracy and topological consistency across datasets and networks.

Conclusion: The dynamic snake upsampling and weighted loss effectively address tubular structure segmentation challenges.

Abstract: Accurate segmentation of tubular topological structures (e.g., fissures and
vasculature) is critical in various fields to guarantee dependable downstream
quantitative analysis and modeling. However, in dense prediction tasks such as
semantic segmentation and super-resolution, conventional upsampling operators
cannot accommodate the slenderness of tubular structures and the curvature of
morphology. This paper introduces a dynamic snake upsampling operators and a
boundary-skeleton weighted loss tailored for topological tubular structures.
Specifically, we design a snake upsampling operators based on an adaptive
sampling domain, which dynamically adjusts the sampling stride according to the
feature map and selects a set of subpixel sampling points along the serpentine
path, enabling more accurate subpixel-level feature recovery for tubular
structures. Meanwhile, we propose a skeleton-to-boundary increasing weighted
loss that trades off main body and boundary weight allocation based on mask
class ratio and distance field, preserving main body overlap while enhancing
focus on target topological continuity and boundary alignment precision.
Experiments across various domain datasets and backbone networks show that this
plug-and-play dynamic snake upsampling operator and boundary-skeleton weighted
loss boost both pixel-wise segmentation accuracy and topological consistency of
results.

</details>


### [149] [Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting](https://arxiv.org/pdf/2505.08527)
*Zheang Huai, Hui Tang, Yi Li, Zhuangzhuang Chen, Xiaomeng Li*

Main category: cs.CV

TL;DR: The paper introduces a Dual Feature Guided (DFG) auto-prompting approach for source-free domain adaptation (SFDA) in segmentation, leveraging the Segment Anything Model (SAM) to generate accurate bounding box prompts and improve performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the domain gap in SFDA for segmentation by automating the generation of accurate bounding box prompts, inspired by SAM's versatility.

Method: The method involves a two-phase approach: (1) feature aggregation to adapt the source model and prepare for prompt search, and (2) dual feature-guided box prompt expansion using target model and SAM features, with postprocessing for refined pseudo-labels.

Result: Experiments on 3D and 2D datasets show superior performance compared to conventional methods.

Conclusion: The DFG approach effectively tackles the domain gap in SFDA for segmentation, demonstrating improved accuracy and robustness.

Abstract: Source-free domain adaptation (SFDA) for segmentation aims at adapting a
model trained in the source domain to perform well in the target domain with
only the source model and unlabeled target data.Inspired by the recent success
of Segment Anything Model (SAM) which exhibits the generality of segmenting
images of various modalities and in different domains given human-annotated
prompts like bounding boxes or points, we for the first time explore the
potentials of Segment Anything Model for SFDA via automatedly finding an
accurate bounding box prompt. We find that the bounding boxes directly
generated with existing SFDA approaches are defective due to the domain gap.To
tackle this issue, we propose a novel Dual Feature Guided (DFG) auto-prompting
approach to search for the box prompt. Specifically, the source model is first
trained in a feature aggregation phase, which not only preliminarily adapts the
source model to the target domain but also builds a feature distribution
well-prepared for box prompt search. In the second phase, based on two feature
distribution observations, we gradually expand the box prompt with the guidance
of the target model feature and the SAM feature to handle the class-wise
clustered target features and the class-wise dispersed target features,
respectively. To remove the potentially enlarged false positive regions caused
by the over-confident prediction of the target model, the refined pseudo-labels
produced by SAM are further postprocessed based on connectivity analysis.
Experiments on 3D and 2D datasets indicate that our approach yields superior
performance compared to conventional methods. Code is available at
https://github.com/zheangh/DFG.

</details>


### [150] [The RaspGrade Dataset: Towards Automatic Raspberry Ripeness Grading with Deep Learning](https://arxiv.org/pdf/2505.08537)
*Mohamed Lamine Mekhalfi, Paul Chippendale, Fabio Poiesi, Samuele Bonecher, Gilberto Osler, Nicola Zancanella*

Main category: cs.CV

TL;DR: The paper explores using computer vision for real-time raspberry grading, introducing the RaspGrade dataset, and highlights challenges in classification due to color similarities and occlusion.


<details>
  <summary>Details</summary>
Motivation: To enable rapid, accurate, and non-invasive food quality assessment in industrial settings, specifically for raspberry grading.

Method: Utilized instance segmentation on the RaspGrade dataset to classify raspberries into five grades, addressing challenges like color similarities and occlusion.

Result: Accurate fruit-level masks were achieved, but classification of certain grades was difficult due to color similarities and occlusion.

Conclusion: The RaspGrade dataset is publicly available for further research, though challenges in grading certain raspberry classes remain.

Abstract: This research investigates the application of computer vision for rapid,
accurate, and non-invasive food quality assessment, focusing on the novel
challenge of real-time raspberry grading into five distinct classes within an
industrial environment as the fruits move along a conveyor belt. To address
this, a dedicated dataset of raspberries, namely RaspGrade, was acquired and
meticulously annotated. Instance segmentation experiments revealed that
accurate fruit-level masks can be obtained; however, the classification of
certain raspberry grades presents challenges due to color similarities and
occlusion, while others are more readily distinguishable based on color. The
acquired and annotated RaspGrade dataset is accessible on HuggingFace at:
https://huggingface.co/datasets/FBK-TeV/RaspGrade.

</details>


### [151] [DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art](https://arxiv.org/pdf/2505.08552)
*Haroon Wahab, Hassan Ugail, Irfan Mehmood*

Main category: cs.CV

TL;DR: DFA-CON, a contrastive learning framework, detects copyright-infringing or forged AI-generated art, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Address concerns about copyright infringement and forgery in AI-generated visual artworks due to memorization of training patterns.

Method: Uses contrastive learning to create a discriminative representation space for detecting forged art across multiple attack types (inpainting, style transfer, etc.).

Result: Demonstrates robust detection performance across most attack types, surpassing pretrained foundation models.

Conclusion: DFA-CON effectively detects AI-generated art forgery, with code and models to be released publicly.

Abstract: Recent proliferation of generative AI tools for visual content
creation-particularly in the context of visual artworks-has raised serious
concerns about copyright infringement and forgery. The large-scale datasets
used to train these models often contain a mixture of copyrighted and
non-copyrighted artworks. Given the tendency of generative models to memorize
training patterns, they are susceptible to varying degrees of copyright
violation. Building on the recently proposed DeepfakeArt Challenge benchmark,
this work introduces DFA-CON, a contrastive learning framework designed to
detect copyright-infringing or forged AI-generated art. DFA-CON learns a
discriminative representation space, posing affinity among original artworks
and their forged counterparts within a contrastive learning framework. The
model is trained across multiple attack types, including inpainting, style
transfer, adversarial perturbation, and cutmix. Evaluation results demonstrate
robust detection performance across most attack types, outperforming recent
pretrained foundation models. Code and model checkpoints will be released
publicly upon acceptance.

</details>


### [152] [Reinforcement Learning meets Masked Video Modeling : Trajectory-Guided Adaptive Token Selection](https://arxiv.org/pdf/2505.08561)
*Ayush K. Rai, Kyle Min, Tarun Krishna, Feiyan Hu, Alan F. Smeaton, Noel E. O'Connor*

Main category: cs.CV

TL;DR: The paper introduces TATS, a trajectory-aware adaptive token sampler for masked video modeling, improving motion-centric token selection and enabling aggressive masking without performance loss.


<details>
  <summary>Details</summary>
Motivation: The challenge of selecting an appropriate masking strategy in masked video modeling (MVM) motivates the development of TATS to better model motion dynamics.

Method: Proposes TATS for motion-centric token selection and a unified training strategy using PPO to jointly optimize MAE and TATS from scratch.

Result: Demonstrates effectiveness across four benchmarks (Something-Something v2, Kinetics-400, UCF101, HMDB51), showing improved performance and efficiency.

Conclusion: TATS enhances MVM by enabling aggressive masking and efficient pre-training, outperforming state-of-the-art methods.

Abstract: Masked video modeling~(MVM) has emerged as a highly effective pre-training
strategy for visual foundation models, whereby the model reconstructs masked
spatiotemporal tokens using information from visible tokens. However, a key
challenge in such approaches lies in selecting an appropriate masking strategy.
Previous studies have explored predefined masking techniques, including random
and tube-based masking, as well as approaches that leverage key motion priors,
optical flow and semantic cues from externally pre-trained models. In this
work, we introduce a novel and generalizable Trajectory-Aware Adaptive Token
Sampler (TATS), which models the motion dynamics of tokens and can be
seamlessly integrated into the masked autoencoder (MAE) framework to select
motion-centric tokens in videos. Additionally, we propose a unified training
strategy that enables joint optimization of both MAE and TATS from scratch
using Proximal Policy Optimization (PPO). We show that our model allows for
aggressive masking without compromising performance on the downstream task of
action recognition while also ensuring that the pre-training remains memory
efficient. Extensive experiments of the proposed approach across four
benchmarks, including Something-Something v2, Kinetics-400, UCF101, and HMDB51,
demonstrate the effectiveness, transferability, generalization, and efficiency
of our work compared to other state-of-the-art methods.

</details>


### [153] [Thermal Detection of People with Mobility Restrictions for Barrier Reduction at Traffic Lights Controlled Intersections](https://arxiv.org/pdf/2505.08568)
*Xiao Ni, Carsten Kuehnel, Xiaoyi Jiang*

Main category: cs.CV

TL;DR: A thermal detector-based traffic light system is proposed to address limitations of RGB camera systems, focusing on mobility-impaired individuals. It uses YOLO-Thermal for improved detection in thermal imaging and enhances barrier-free intersections.


<details>
  <summary>Details</summary>
Motivation: Current RGB camera-based traffic systems neglect mobility-impaired individuals and face challenges like poor visibility in adverse weather and privacy concerns.

Method: Developed YOLO-Thermal, a variant of YOLO with advanced feature extraction and attention mechanisms, and created the TD4PWMR thermal dataset for diverse pedestrian scenarios.

Result: YOLO-Thermal outperforms existing detectors, and the system improves barrier-free intersection accessibility.

Conclusion: The thermal-based system effectively addresses RGB system limitations, enhancing safety and accessibility for all users.

Abstract: Rapid advances in deep learning for computer vision have driven the adoption
of RGB camera-based adaptive traffic light systems to improve traffic safety
and pedestrian comfort. However, these systems often overlook the needs of
people with mobility restrictions. Moreover, the use of RGB cameras presents
significant challenges, including limited detection performance under adverse
weather or low-visibility conditions, as well as heightened privacy concerns.
To address these issues, we propose a fully automated, thermal detector-based
traffic light system that dynamically adjusts signal durations for individuals
with walking impairments or mobility burden and triggers the auditory signal
for visually impaired individuals, thereby advancing towards barrier-free
intersection for all users. To this end, we build the thermal dataset for
people with mobility restrictions (TD4PWMR), designed to capture diverse
pedestrian scenarios, particularly focusing on individuals with mobility aids
or mobility burden under varying environmental conditions, such as different
lighting, weather, and crowded urban settings. While thermal imaging offers
advantages in terms of privacy and robustness to adverse conditions, it also
introduces inherent hurdles for object detection due to its lack of color and
fine texture details and generally lower resolution of thermal images. To
overcome these limitations, we develop YOLO-Thermal, a novel variant of the
YOLO architecture that integrates advanced feature extraction and attention
mechanisms for enhanced detection accuracy and robustness in thermal imaging.
Experiments demonstrate that the proposed thermal detector outperforms existing
detectors, while the proposed traffic light system effectively enhances
barrier-free intersection. The source codes and dataset are available at
https://github.com/leon2014dresden/YOLO-THERMAL.

</details>


### [154] [A Large-scale Benchmark on Geological Fault Delineation Models: Domain Shift, Training Dynamics, Generalizability, Evaluation and Inferential Behavior](https://arxiv.org/pdf/2505.08585)
*Jorge Quesada, Chen Zhou, Prithwijit Chowdhury, Mohammad Alotaibi, Ahmad Mustafa, Yusufjon Kumamnov, Mohit Prabhushankar, Ghassan AlRegib*

Main category: cs.CV

TL;DR: A large-scale benchmarking study evaluates domain shift strategies in seismic fault delineation, revealing limitations in current models and providing guidelines for more robust workflows.


<details>
  <summary>Details</summary>
Motivation: The field lacks understanding of machine learning models' generalizability across diverse seismic data due to distributional shifts, limited labeled data, and inconsistent evaluations.

Method: The study benchmarks over 200 models on three datasets (FaultSeg3D, CRACKS, Thebe), assessing pretraining, fine-tuning, and joint training under domain shifts.

Result: Findings highlight fragile fine-tuning practices, catastrophic forgetting, and challenges in systematic performance interpretation.

Conclusion: The study establishes a baseline for tradeoffs in fault delineation workflows and offers guidelines for developing generalizable and effective models.

Abstract: Machine learning has taken a critical role in seismic interpretation
workflows, especially in fault delineation tasks. However, despite the recent
proliferation of pretrained models and synthetic datasets, the field still
lacks a systematic understanding of the generalizability limits of these models
across seismic data representing a variety of geologic, acquisition and
processing settings. Distributional shifts between different data sources,
limitations in fine-tuning strategies and labeled data accessibility, and
inconsistent evaluation protocols all represent major roadblocks in the
deployment of reliable and robust models in real-world exploration settings. In
this paper, we present the first large-scale benchmarking study explicitly
designed to provide answers and guidelines for domain shift strategies in
seismic interpretation. Our benchmark encompasses over $200$ models trained and
evaluated on three heterogeneous datasets (synthetic and real data) including
FaultSeg3D, CRACKS, and Thebe. We systematically assess pretraining,
fine-tuning, and joint training strategies under varying degrees of domain
shift. Our analysis highlights the fragility of current fine-tuning practices,
the emergence of catastrophic forgetting, and the challenges of interpreting
performance in a systematic manner. We establish a robust experimental baseline
to provide insights into the tradeoffs inherent to current fault delineation
workflows, and shed light on directions for developing more generalizable,
interpretable and effective machine learning models for seismic interpretation.
The insights and analyses reported provide a set of guidelines on the
deployment of fault delineation models within seismic interpretation workflows.

</details>


### [155] [PrePrompt: Predictive prompting for class incremental learning](https://arxiv.org/pdf/2505.08586)
*Libo Huang, Zhulin An, Chuanguang Yang, Boyu Diao, Fei Wang, Yan Zeng, Zhifeng Hao, Yongjun Xu*

Main category: cs.CV

TL;DR: PrePrompt introduces a two-stage prediction framework for Class Incremental Learning (CIL) using pre-trained models, avoiding correlation-based limitations and addressing bias with feature translation.


<details>
  <summary>Details</summary>
Motivation: Existing CIL methods struggle with fitting feature spaces using few prompts. PrePrompt leverages pre-trained models' classification ability to predict task-specific prompts.

Method: PrePrompt decomposes CIL into task-specific prompt prediction and label prediction, using feature translation to balance stability and plasticity.

Result: PrePrompt outperforms state-of-the-art prompt-based CIL methods in benchmarks.

Conclusion: PrePrompt offers a promising solution for CIL by addressing correlation-based limitations and bias, with code to be released.

Abstract: Class Incremental Learning (CIL) based on pre-trained models offers a
promising direction for open-world continual learning. Existing methods
typically rely on correlation-based strategies, where an image's classification
feature is used as a query to retrieve the most related key prompts and select
the corresponding value prompts for training. However, these approaches face an
inherent limitation: fitting the entire feature space of all tasks with only a
few trainable prompts is fundamentally challenging. We propose Predictive
Prompting (PrePrompt), a novel CIL framework that circumvents correlation-based
limitations by leveraging pre-trained models' natural classification ability to
predict task-specific prompts. Specifically, PrePrompt decomposes CIL into a
two-stage prediction framework: task-specific prompt prediction followed by
label prediction. While theoretically appealing, this framework risks bias
toward recent classes due to missing historical data for older classifier
calibration. PrePrompt then mitigates this by incorporating feature
translation, dynamically balancing stability and plasticity. Experiments across
multiple benchmarks demonstrate PrePrompt's superiority over state-of-the-art
prompt-based CIL methods. The code will be released upon acceptance.

</details>


### [156] [MESSI: A Multi-Elevation Semantic Segmentation Image Dataset of an Urban Environment](https://arxiv.org/pdf/2505.08589)
*Barak Pinkovich, Boaz Matalon, Ehud Rivlin, Hector Rotstein*

Main category: cs.CV

TL;DR: The paper introduces the MESSI dataset, a collection of 2525 drone-captured images from various altitudes and urban regions, annotated for semantic segmentation and other applications. It details the dataset's features, annotation process, and evaluation using neural networks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of datasets capturing the effect of depth and urban variety on semantic segmentation, especially for drone-captured images.

Method: Created the MESSI dataset with multi-elevation images, annotated them, and evaluated semantic segmentation using deep neural networks.

Result: The dataset supports training and benchmarking for semantic segmentation and other applications like localization and navigation.

Conclusion: MESSI serves as a valuable public benchmark for semantic segmentation in drone-captured urban environments.

Abstract: This paper presents a Multi-Elevation Semantic Segmentation Image (MESSI)
dataset comprising 2525 images taken by a drone flying over dense urban
environments. MESSI is unique in two main features. First, it contains images
from various altitudes, allowing us to investigate the effect of depth on
semantic segmentation. Second, it includes images taken from several different
urban regions (at different altitudes). This is important since the variety
covers the visual richness captured by a drone's 3D flight, performing
horizontal and vertical maneuvers. MESSI contains images annotated with
location, orientation, and the camera's intrinsic parameters and can be used to
train a deep neural network for semantic segmentation or other applications of
interest (e.g., localization, navigation, and tracking). This paper describes
the dataset and provides annotation details. It also explains how semantic
segmentation was performed using several neural network models and shows
several relevant statistics. MESSI will be published in the public domain to
serve as an evaluation benchmark for semantic segmentation using images
captured by a drone or similar vehicle flying over a dense urban environment.

</details>


### [157] [Rejoining fragmented ancient bamboo slips with physics-driven deep learning](https://arxiv.org/pdf/2505.08601)
*Jinchi Zhu, Zhou Zhao, Hailong Lei, Xiaoguang Wang, Jialiang Lu, Jing Li, Qianqian Tang, Jiachen Shen, Gui-Song Xia, Bo Du, Yongchao Xu*

Main category: cs.CV

TL;DR: WisePanda is a physics-driven deep learning framework for rejoining fragmented bamboo slips, improving accuracy and efficiency in archaeological restoration.


<details>
  <summary>Details</summary>
Motivation: Fragmented bamboo slips hinder understanding of ancient civilizations; manual rejoining is challenging and time-consuming.

Method: WisePanda uses physics-based synthetic training data to train a matching network without manual samples, providing ranked rejoining suggestions.

Result: Top-50 matching accuracy improved from 36% to 52%, with a 20x efficiency gain for archaeologists.

Conclusion: Physics-driven deep learning enhances artifact restoration, offering a new paradigm for addressing data scarcity in archaeology.

Abstract: Bamboo slips are a crucial medium for recording ancient civilizations in East
Asia, and offers invaluable archaeological insights for reconstructing the Silk
Road, studying material culture exchanges, and global history. However, many
excavated bamboo slips have been fragmented into thousands of irregular pieces,
making their rejoining a vital yet challenging step for understanding their
content. Here we introduce WisePanda, a physics-driven deep learning framework
designed to rejoin fragmented bamboo slips. Based on the physics of fracture
and material deterioration, WisePanda automatically generates synthetic
training data that captures the physical properties of bamboo fragmentations.
This approach enables the training of a matching network without requiring
manually paired samples, providing ranked suggestions to facilitate the
rejoining process. Compared to the leading curve matching method, WisePanda
increases Top-50 matching accuracy from 36\% to 52\%. Archaeologists using
WisePanda have experienced substantial efficiency improvements (approximately
20 times faster) when rejoining fragmented bamboo slips. This research
demonstrates that incorporating physical principles into deep learning models
can significantly enhance their performance, transforming how archaeologists
restore and study fragmented artifacts. WisePanda provides a new paradigm for
addressing data scarcity in ancient artifact restoration through physics-driven
machine learning.

</details>


### [158] [Unsupervised Out-of-Distribution Detection in Medical Imaging Using Multi-Exit Class Activation Maps and Feature Masking](https://arxiv.org/pdf/2505.08604)
*Yu-Jen Chen, Xueyang Li, Yiyu Shi, Tsung-Yi Ho*

Main category: cs.CV

TL;DR: A novel unsupervised OOD detection framework, MECAM, uses multi-exit CAMs and feature masking to differentiate ID and OOD data in medical imaging, showing superior performance.


<details>
  <summary>Details</summary>
Motivation: OOD detection is crucial for reliable deep learning in medical imaging. CAMs for ID data show focused activations, while OOD data lacks this, inspiring the use of inverted CAMs for differentiation.

Method: MECAM leverages multi-exit CAMs and feature masking, combining CAMs from varying resolutions and depths to capture global and local features for robust OOD detection.

Result: Evaluated on ISIC19, PathMNIST, and tested against RSNA Pneumonia, COVID-19, HeadCT, and iSUN, MECAM outperforms state-of-the-art OOD detection methods.

Conclusion: Multi-exit networks and feature masking enhance unsupervised OOD detection in medical imaging, promising more reliable and interpretable models for clinical use.

Abstract: Out-of-distribution (OOD) detection is essential for ensuring the reliability
of deep learning models in medical imaging applications. This work is motivated
by the observation that class activation maps (CAMs) for in-distribution (ID)
data typically emphasize regions that are highly relevant to the model's
predictions, whereas OOD data often lacks such focused activations. By masking
input images with inverted CAMs, the feature representations of ID data undergo
more substantial changes compared to those of OOD data, offering a robust
criterion for differentiation. In this paper, we introduce a novel unsupervised
OOD detection framework, Multi-Exit Class Activation Map (MECAM), which
leverages multi-exit CAMs and feature masking. By utilizing mult-exit networks
that combine CAMs from varying resolutions and depths, our method captures both
global and local feature representations, thereby enhancing the robustness of
OOD detection. We evaluate MECAM on multiple ID datasets, including ISIC19 and
PathMNIST, and test its performance against three medical OOD datasets, RSNA
Pneumonia, COVID-19, and HeadCT, and one natural image OOD dataset, iSUN.
Comprehensive comparisons with state-of-the-art OOD detection methods validate
the effectiveness of our approach. Our findings emphasize the potential of
multi-exit networks and feature masking for advancing unsupervised OOD
detection in medical imaging, paving the way for more reliable and
interpretable models in clinical practice.

</details>


### [159] [Leveraging Multi-Modal Information to Enhance Dataset Distillation](https://arxiv.org/pdf/2505.08605)
*Zhe Li, Hadrien Reynaud, Bernhard Kainz*

Main category: cs.CV

TL;DR: The paper introduces caption-guided supervision and object-centric masking to improve dataset distillation, enhancing synthetic dataset quality for downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Existing dataset distillation methods focus on visual representations, but incorporating additional modalities (like text) and refining object-level information can improve results.

Method: Proposes two caption-guided strategies (feature concatenation and caption matching) and object-centric masking with two loss functions (masked feature alignment and masked gradient matching).

Result: Integrating caption-based guidance and object-centric masking improves dataset distillation, yielding better-performing synthetic datasets.

Conclusion: The enhancements lead to superior synthetic datasets, demonstrating the value of multimodal and object-centric approaches in dataset distillation.

Abstract: Dataset distillation aims to create a compact and highly representative
synthetic dataset that preserves the knowledge of a larger real dataset. While
existing methods primarily focus on optimizing visual representations,
incorporating additional modalities and refining object-level information can
significantly improve the quality of distilled datasets. In this work, we
introduce two key enhancements to dataset distillation: caption-guided
supervision and object-centric masking. To integrate textual information, we
propose two strategies for leveraging caption features: the feature
concatenation, where caption embeddings are fused with visual features at the
classification stage, and caption matching, which introduces a caption-based
alignment loss during training to ensure semantic coherence between real and
synthetic data. Additionally, we apply segmentation masks to isolate target
objects and remove background distractions, introducing two loss functions
designed for object-centric learning: masked feature alignment loss and masked
gradient matching loss. Comprehensive evaluations demonstrate that integrating
caption-based guidance and object-centric masking enhances dataset
distillation, leading to synthetic datasets that achieve superior performance
on downstream tasks.

</details>


### [160] [Boosting Zero-shot Stereo Matching using Large-scale Mixed Images Sources in the Real World](https://arxiv.org/pdf/2505.08607)
*Yuran Wang, Yingping Liang, Ying Fu*

Main category: cs.CV

TL;DR: BooSTer leverages vision foundation models and mixed image sources to improve stereo matching, addressing data scarcity and domain gaps via synthetic data generation and knowledge transfer.


<details>
  <summary>Details</summary>
Motivation: Overcoming the challenges of scarce labeled data and domain gaps between synthetic and real-world images in stereo matching.

Method: Combines monocular depth estimation and diffusion models for data generation, uses pseudo-mono depth labels and a dynamic loss for supervision, and employs a vision foundation model for feature extraction.

Result: Achieves significant accuracy improvements, especially with limited labeled data and domain shifts.

Conclusion: BooSTer effectively addresses data scarcity and domain gaps, enhancing stereo matching performance.

Abstract: Stereo matching methods rely on dense pixel-wise ground truth labels, which
are laborious to obtain, especially for real-world datasets. The scarcity of
labeled data and domain gaps between synthetic and real-world images also pose
notable challenges. In this paper, we propose a novel framework,
\textbf{BooSTer}, that leverages both vision foundation models and large-scale
mixed image sources, including synthetic, real, and single-view images. First,
to fully unleash the potential of large-scale single-view images, we design a
data generation strategy combining monocular depth estimation and diffusion
models to generate dense stereo matching data from single-view images. Second,
to tackle sparse labels in real-world datasets, we transfer knowledge from
monocular depth estimation models, using pseudo-mono depth labels and a dynamic
scale- and shift-invariant loss for additional supervision. Furthermore, we
incorporate vision foundation model as an encoder to extract robust and
transferable features, boosting accuracy and generalization. Extensive
experiments on benchmark datasets demonstrate the effectiveness of our
approach, achieving significant improvements in accuracy over existing methods,
particularly in scenarios with limited labeled data and domain shifts.

</details>


### [161] [WaveGuard: Robust Deepfake Detection and Source Tracing via Dual-Tree Complex Wavelet and Graph Neural Networks](https://arxiv.org/pdf/2505.08614)
*Ziyuan He, Zhiqing Guo, Liejun Wang, Gaobo Yang, Yunfeng Diao, Dan Ma*

Main category: cs.CV

TL;DR: WaveGuard is a watermarking framework for deepfake detection, using frequency-domain embedding and graph-based consistency to improve robustness and visual quality.


<details>
  <summary>Details</summary>
Motivation: Addressing risks like privacy invasion and identity theft from deepfake technology.

Method: Embeds watermarks in high-frequency sub-bands using DT-CWT and employs SC-GNN for visual quality, with an attention module for precision.

Result: Outperforms state-of-the-art methods in robustness and visual quality on face swap and reenactment tasks.

Conclusion: WaveGuard effectively mitigates deepfake risks with superior performance and visual quality.

Abstract: Deepfake technology poses increasing risks such as privacy invasion and
identity theft. To address these threats, we propose WaveGuard, a proactive
watermarking framework that enhances robustness and imperceptibility via
frequency-domain embedding and graph-based structural consistency.
Specifically, we embed watermarks into high-frequency sub-bands using Dual-Tree
Complex Wavelet Transform (DT-CWT) and employ a Structural Consistency Graph
Neural Network (SC-GNN) to preserve visual quality. We also design an attention
module to refine embedding precision. Experimental results on face swap and
reenactment tasks demonstrate that WaveGuard outperforms state-of-the-art
methods in both robustness and visual quality. Code is available at
https://github.com/vpsg-research/WaveGuard.

</details>


### [162] [OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning](https://arxiv.org/pdf/2505.08617)
*Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, Yu Cheng*

Main category: cs.CV

TL;DR: OpenThinkIMG is an open-source framework for tool-augmented LVLMs, featuring standardized interfaces and a novel RL method (V-ToolRL) to train adaptive policies, outperforming SFT and baselines.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs lack standardized infrastructure for adaptive visual tool usage, limiting their problem-solving flexibility.

Method: Introduces OpenThinkIMG with standardized tool interfaces and V-ToolRL, a reinforcement learning framework for dynamic tool invocation.

Result: V-ToolRL-trained agent outperforms SFT (+28.83 points) and baselines (+12.7 points), surpassing GPT-4.1 (+8.68 points).

Conclusion: OpenThinkIMG advances dynamic visual reasoning, enabling AI agents to better "think with images."

Abstract: While humans can flexibly leverage interactive visual cognition for complex
problem-solving, enabling Large Vision-Language Models (LVLMs) to learn
similarly adaptive behaviors with visual tools remains challenging. A
significant hurdle is the current lack of standardized infrastructure, which
hinders integrating diverse tools, generating rich interaction data, and
training robust agents effectively. To address these gaps, we introduce
OpenThinkIMG, the first open-source, comprehensive end-to-end framework for
tool-augmented LVLMs. It features standardized vision tool interfaces, scalable
trajectory generation for policy initialization, and a flexible training
environment. Furthermore, considering supervised fine-tuning (SFT) on static
demonstrations offers limited policy generalization for dynamic tool
invocation, we propose a novel reinforcement learning (RL) framework V-ToolRL
to train LVLMs to learn adaptive policies for invoking external vision tools.
V-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies
by directly optimizing for task success using feedback from tool interactions.
We empirically validate V-ToolRL on challenging chart reasoning tasks. Our
RL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its
SFT-initialized counterpart (+28.83 points) and surpasses established
supervised tool-learning baselines like Taco and CogCom by an average of +12.7
points. Notably, it also surpasses prominent closed-source models like GPT-4.1
by +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational
framework for advancing dynamic, tool-augmented visual reasoning, helping the
community develop AI agents that can genuinely "think with images".

</details>


### [163] [DLO-Splatting: Tracking Deformable Linear Objects Using 3D Gaussian Splatting](https://arxiv.org/pdf/2505.08644)
*Holly Dinkel, Marcel Büsching, Alberta Longhini, Brian Coltin, Trey Smith, Danica Kragic, Mårten Björkman, Timothy Bretl*

Main category: cs.CV

TL;DR: DLO-Splatting estimates 3D shape of Deformable Linear Objects (DLOs) using multi-view RGB images and gripper state info via prediction-update filtering.


<details>
  <summary>Details</summary>
Motivation: Existing vision-only methods struggle with challenging scenarios like knot tying, prompting the need for a more robust approach.

Method: Uses position-based dynamics with smoothness/rigidity corrections for prediction, and 3D Gaussian Splatting-based rendering loss for iterative refinement.

Result: Promising results in knot tying, outperforming vision-only methods.

Conclusion: DLO-Splatting is effective for 3D shape estimation of DLOs, especially in complex tasks.

Abstract: This work presents DLO-Splatting, an algorithm for estimating the 3D shape of
Deformable Linear Objects (DLOs) from multi-view RGB images and gripper state
information through prediction-update filtering. The DLO-Splatting algorithm
uses a position-based dynamics model with shape smoothness and rigidity
dampening corrections to predict the object shape. Optimization with a 3D
Gaussian Splatting-based rendering loss iteratively renders and refines the
prediction to align it with the visual observations in the update step. Initial
experiments demonstrate promising results in a knot tying scenario, which is
challenging for existing vision-only methods.

</details>


### [164] [SkillFormer: Unified Multi-View Video Understanding for Proficiency Estimation](https://arxiv.org/pdf/2505.08665)
*Edoardo Bianchi, Antonio Liotta*

Main category: cs.CV

TL;DR: SkillFormer is a parameter-efficient architecture for multi-view skill assessment from videos, outperforming baselines with fewer parameters and training epochs.


<details>
  <summary>Details</summary>
Motivation: Assessing human skill levels in complex activities is challenging but valuable for sports, rehabilitation, and training.

Method: SkillFormer uses a CrossViewFusion module with multi-head cross-attention, learnable gating, and adaptive self-calibration, fine-tuned via Low-Rank Adaptation.

Result: Achieves state-of-the-art accuracy on EgoExo4D dataset with 4.5x fewer parameters and 3.75x fewer training epochs.

Conclusion: Multi-view integration is valuable for fine-grained skill assessment, demonstrated by SkillFormer's efficiency and performance.

Abstract: Assessing human skill levels in complex activities is a challenging problem
with applications in sports, rehabilitation, and training. In this work, we
present SkillFormer, a parameter-efficient architecture for unified multi-view
proficiency estimation from egocentric and exocentric videos. Building on the
TimeSformer backbone, SkillFormer introduces a CrossViewFusion module that
fuses view-specific features using multi-head cross-attention, learnable
gating, and adaptive self-calibration. We leverage Low-Rank Adaptation to
fine-tune only a small subset of parameters, significantly reducing training
costs. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves
state-of-the-art accuracy in multi-view settings while demonstrating remarkable
computational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer
training epochs than prior baselines. It excels in multiple structured tasks,
confirming the value of multi-view integration for fine-grained skill
assessment.

</details>


### [165] [Calibration and Uncertainty for multiRater Volume Assessment in multiorgan Segmentation (CURVAS) challenge results](https://arxiv.org/pdf/2505.08685)
*Meritxell Riera-Marin, Sikha O K, Julia Rodriguez-Comas, Matthias Stefan May, Zhaohong Pan, Xiang Zhou, Xiaokun Liang, Franciskus Xaverius Erick, Andrea Prenner, Cedric Hemon, Valentin Boussot, Jean-Louis Dillenseger, Jean-Claude Nunes, Abdul Qayyum, Moona Mazher, Steven A Niederer, Kaisar Kushibar, Carlos Martin-Isla, Petia Radeva, Karim Lekadir, Theodore Barfoot, Luis C. Garcia Peraza Herrera, Ben Glocker, Tom Vercauteren, Lucas Gago, Justin Englemann, Joy-Marie Kleiss, Anton Aubanell, Andreu Antolin, Javier Garcia-Lopez, Miguel A. Gonzalez Ballester, Adrian Galdran*

Main category: cs.CV

TL;DR: CURVAS addresses challenges in DL-based medical image segmentation by leveraging multi-annotator variability for robust evaluation, emphasizing calibration and uncertainty estimation.


<details>
  <summary>Details</summary>
Motivation: To improve reliability and clinical applicability of DL models by addressing annotation variability, calibration, and uncertainty.

Method: Seven teams submitted DL models evaluated using DSC, ECE, and CRPS, incorporating consensus and dissensus ground truth.

Result: Better calibration correlates with higher quality results; models trained on diverse datasets with pre-trained knowledge show robustness.

Conclusion: Multi-annotator ground truth, calibration, and uncertainty-aware evaluations are crucial for trustworthy DL-based segmentation.

Abstract: Deep learning (DL) has become the dominant approach for medical image
segmentation, yet ensuring the reliability and clinical applicability of these
models requires addressing key challenges such as annotation variability,
calibration, and uncertainty estimation. This is why we created the Calibration
and Uncertainty for multiRater Volume Assessment in multiorgan Segmentation
(CURVAS), which highlights the critical role of multiple annotators in
establishing a more comprehensive ground truth, emphasizing that segmentation
is inherently subjective and that leveraging inter-annotator variability is
essential for robust model evaluation. Seven teams participated in the
challenge, submitting a variety of DL models evaluated using metrics such as
Dice Similarity Coefficient (DSC), Expected Calibration Error (ECE), and
Continuous Ranked Probability Score (CRPS). By incorporating consensus and
dissensus ground truth, we assess how DL models handle uncertainty and whether
their confidence estimates align with true segmentation performance. Our
findings reinforce the importance of well-calibrated models, as better
calibration is strongly correlated with the quality of the results.
Furthermore, we demonstrate that segmentation models trained on diverse
datasets and enriched with pre-trained knowledge exhibit greater robustness,
particularly in cases deviating from standard anatomical structures. Notably,
the best-performing models achieved high DSC and well-calibrated uncertainty
estimates. This work underscores the need for multi-annotator ground truth,
thorough calibration assessments, and uncertainty-aware evaluations to develop
trustworthy and clinically reliable DL-based medical image segmentation models.

</details>


### [166] [SPAST: Arbitrary Style Transfer with Style Priors via Pre-trained Large-scale Model](https://arxiv.org/pdf/2505.08695)
*Zhanjie Zhang, Quanwei Zhang, Junsheng Luan, Mengyuan Yang, Yun Wang, Lei Zhao*

Main category: cs.CV

TL;DR: SPAST is a new framework for arbitrary style transfer that generates high-quality stylized images with reduced inference time, addressing limitations of small and large-scale models.


<details>
  <summary>Details</summary>
Motivation: Existing methods either produce low-quality images (small models) or struggle with content preservation and slow inference (large models).

Method: SPAST uses a Local-global Window Size Stylization Module (LGWSSM) and a style prior loss to fuse style features efficiently.

Result: SPAST generates high-quality stylized images faster than state-of-the-art methods.

Conclusion: SPAST effectively balances quality and speed in arbitrary style transfer.

Abstract: Given an arbitrary content and style image, arbitrary style transfer aims to
render a new stylized
  image which preserves the content image's structure and possesses the style
image's style. Existing
  arbitrary style transfer methods are based on either small models or
pre-trained large-scale models.
  The small model-based methods fail to generate high-quality stylized images,
bringing artifacts and
  disharmonious patterns. The pre-trained large-scale model-based methods can
generate high-quality
  stylized images but struggle to preserve the content structure and cost long
inference time. To this
  end, we propose a new framework, called SPAST, to generate high-quality
stylized images with
  less inference time. Specifically, we design a novel Local-global Window Size
Stylization Module
  (LGWSSM)tofuse style features into content features. Besides, we introduce a
novel style prior loss,
  which can dig out the style priors from a pre-trained large-scale model into
the SPAST and motivate
  the SPAST to generate high-quality stylized images with short inference
time.We conduct abundant
  experiments to verify that our proposed method can generate high-quality
stylized images and less
  inference time compared with the SOTA arbitrary style transfer methods.

</details>


### [167] [Controllable Image Colorization with Instance-aware Texts and Masks](https://arxiv.org/pdf/2505.08705)
*Yanru An, Ling Gui, Qiang Hu, Chunlei Cai, Tianxiao Ye, Xiaoyun Zhang, Yanfeng Wang*

Main category: cs.CV

TL;DR: MT-Color is a diffusion-based method for precise instance-aware image colorization, addressing color bleeding and binding errors with mask attention mechanisms and multi-instance sampling.


<details>
  <summary>Details</summary>
Motivation: Current image colorization models suffer from color bleeding, binding errors, and lack instance-level precision.

Method: Uses pixel-level mask attention, instance mask and text guidance, and multi-instance sampling.

Result: Outperforms previous methods in qualitative and quantitative experiments.

Conclusion: MT-Color and the GPT-color dataset advance instance-level colorization.

Abstract: Recently, the application of deep learning in image colorization has received
widespread attention. The maturation of diffusion models has further advanced
the development of image colorization models. However, current mainstream image
colorization models still face issues such as color bleeding and color binding
errors, and cannot colorize images at the instance level. In this paper, we
propose a diffusion-based colorization method MT-Color to achieve precise
instance-aware colorization with use-provided guidance. To tackle color
bleeding issue, we design a pixel-level mask attention mechanism that
integrates latent features and conditional gray image features through
cross-attention. We use segmentation masks to construct cross-attention masks,
preventing pixel information from exchanging between different instances. We
also introduce an instance mask and text guidance module that extracts instance
masks and text representations of each instance, which are then fused with
latent features through self-attention, utilizing instance masks to form
self-attention masks to prevent instance texts from guiding the colorization of
other areas, thus mitigating color binding errors. Furthermore, we apply a
multi-instance sampling strategy, which involves sampling each instance region
separately and then fusing the results. Additionally, we have created a
specialized dataset for instance-level colorization tasks, GPT-color, by
leveraging large visual language models on existing image datasets. Qualitative
and quantitative experiments show that our model and dataset outperform
previous methods and datasets.

</details>


### [168] [TiMo: Spatiotemporal Foundation Model for Satellite Image Time Series](https://arxiv.org/pdf/2505.08723)
*Xiaolei Qin, Di Wang, Jing Zhang, Fengxiang Wang, Xin Su, Bo Du, Liangpei Zhang*

Main category: cs.CV

TL;DR: TiMo is a hierarchical vision transformer model for satellite image time series (SITS) analysis, featuring a spatiotemporal gyroscope attention mechanism and pre-trained on the MillionST dataset. It outperforms existing methods in tasks like deforestation monitoring and flood detection.


<details>
  <summary>Details</summary>
Motivation: Existing spatiotemporal foundation models lack explicit multiscale spatiotemporal relationship capture, limiting their effectiveness in downstream tasks.

Method: Proposes TiMo, a hierarchical vision transformer with spatiotemporal gyroscope attention, pre-trained on the MillionST dataset using masked image modeling.

Result: TiMo demonstrates superior performance in tasks like deforestation monitoring, land cover segmentation, crop type classification, and flood detection.

Conclusion: TiMo addresses limitations of existing models and shows significant improvements in spatiotemporal tasks, with code and dataset made publicly available.

Abstract: Satellite image time series (SITS) provide continuous observations of the
Earth's surface, making them essential for applications such as environmental
management and disaster assessment. However, existing spatiotemporal foundation
models rely on plain vision transformers, which encode entire temporal
sequences without explicitly capturing multiscale spatiotemporal relationships
between land objects. This limitation hinders their effectiveness in downstream
tasks. To overcome this challenge, we propose TiMo, a novel hierarchical vision
transformer foundation model tailored for SITS analysis. At its core, we
introduce a spatiotemporal gyroscope attention mechanism that dynamically
captures evolving multiscale patterns across both time and space. For
pre-training, we curate MillionST, a large-scale dataset of one million images
from 100,000 geographic locations, each captured across 10 temporal phases over
five years, encompassing diverse geospatial changes and seasonal variations.
Leveraging this dataset, we adapt masked image modeling to pre-train TiMo,
enabling it to effectively learn and encode generalizable spatiotemporal
representations.Extensive experiments across multiple spatiotemporal
tasks-including deforestation monitoring, land cover segmentation, crop type
classification, and flood detection-demonstrate TiMo's superiority over
state-of-the-art methods. Code, model, and dataset will be released at
https://github.com/MiliLab/TiMo.

</details>


### [169] [Extending Large Vision-Language Model for Diverse Interactive Tasks in Autonomous Driving](https://arxiv.org/pdf/2505.08725)
*Zongchuang Zhao, Haoyu Fu, Dingkang Liang, Xin Zhou, Dingyuan Zhang, Hongwei Xie, Bing Wang, Xiang Bai*

Main category: cs.CV

TL;DR: The paper introduces NuInteract, a large-scale dataset, and DriveMonkey, a framework integrating LVLMs with 3D perception for improved autonomous driving scene understanding.


<details>
  <summary>Details</summary>
Motivation: Existing LVLMs lack comprehensive scene understanding, mapping between 2D and 3D, and integration of 3D object localization.

Method: Proposes DriveMonkey, a framework combining LVLMs with a spatial processor using learnable queries, initialized with pre-trained 3D detectors.

Result: DriveMonkey outperforms general LVLMs, with a 9.86% improvement on 3D visual grounding.

Conclusion: The dataset and framework enhance LVLMs for autonomous driving, addressing key limitations in scene understanding.

Abstract: The Large Visual-Language Models (LVLMs) have significantly advanced image
understanding. Their comprehension and reasoning capabilities enable promising
applications in autonomous driving scenarios. However, existing research
typically focuses on front-view perspectives and partial objects within scenes,
struggling to achieve comprehensive scene understanding. Meanwhile, existing
LVLMs suffer from the lack of mapping relationship between 2D and 3D and
insufficient integration of 3D object localization and instruction
understanding. To tackle these limitations, we first introduce NuInteract, a
large-scale dataset with over 1.5M multi-view image language pairs spanning
dense scene captions and diverse interactive tasks. Furthermore, we propose
DriveMonkey, a simple yet effective framework that seamlessly integrates LVLMs
with a spatial processor using a series of learnable queries. The spatial
processor, designed as a plug-and-play component, can be initialized with
pre-trained 3D detectors to improve 3D perception. Our experiments show that
DriveMonkey outperforms general LVLMs, especially achieving a 9.86% notable
improvement on the 3D visual grounding task. The dataset and code will be
released at https://github.com/zc-zhao/DriveMonkey.

</details>


### [170] [Advancing Food Nutrition Estimation via Visual-Ingredient Feature Fusion](https://arxiv.org/pdf/2505.08747)
*Huiyan Qi, Bin Zhu, Chong-Wah Ngo, Jingjing Chen, Ee-Peng Lim*

Main category: cs.CV

TL;DR: The paper introduces FastFood, a dataset for nutrition estimation, and proposes VIF², a method combining visual and ingredient features for accurate nutritional prediction.


<details>
  <summary>Details</summary>
Motivation: Nutrition estimation is crucial for health, but progress is limited by lack of datasets with nutritional annotations.

Method: Proposes VIF², integrating visual and ingredient features, with ingredient robustness improved via synonym replacement and resampling. Uses data augmentation and majority voting for ingredient refinement.

Result: Validated on FastFood and Nutrition5k datasets, showing effectiveness across various backbones (Resnet, InceptionV3, ViT).

Conclusion: Highlights the importance of ingredient information in nutrition estimation.

Abstract: Nutrition estimation is an important component of promoting healthy eating
and mitigating diet-related health risks. Despite advances in tasks such as
food classification and ingredient recognition, progress in nutrition
estimation is limited due to the lack of datasets with nutritional annotations.
To address this issue, we introduce FastFood, a dataset with 84,446 images
across 908 fast food categories, featuring ingredient and nutritional
annotations. In addition, we propose a new model-agnostic Visual-Ingredient
Feature Fusion (VIF$^2$) method to enhance nutrition estimation by integrating
visual and ingredient features. Ingredient robustness is improved through
synonym replacement and resampling strategies during training. The
ingredient-aware visual feature fusion module combines ingredient features and
visual representation to achieve accurate nutritional prediction. During
testing, ingredient predictions are refined using large multimodal models by
data augmentation and majority voting. Our experiments on both FastFood and
Nutrition5k datasets validate the effectiveness of our proposed method built in
different backbones (e.g., Resnet, InceptionV3 and ViT), which demonstrates the
importance of ingredient information in nutrition estimation.
https://huiyanqi.github.io/fastfood-nutrition-estimation/.

</details>


### [171] [Towards Autonomous UAV Visual Object Search in City Space: Benchmark and Agentic Methodology](https://arxiv.org/pdf/2505.08765)
*Yatai Ji, Zhengqiu Zhu, Yong Zhao, Beidan Liu, Chen Gao, Yihao Zhao, Sihang Qiu, Yue Hu, Quanjun Yin, Yong Li*

Main category: cs.CV

TL;DR: The paper introduces CityAVOS, a benchmark dataset for aerial visual object search in urban environments, and PRPSearcher, a novel agentic method using multi-modal large language models to improve search performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for aerial visual object search (AVOS) in urban environments face challenges like redundant semantic processing and the exploration-exploitation dilemma, necessitating a better approach.

Method: The proposed PRPSearcher uses multi-modal large language models to create three specialized maps (dynamic semantic, 3D cognitive, and 3D uncertainty) and incorporates denoising and adaptive action planning mechanisms.

Result: PRPSearcher outperforms baselines in success rate and efficiency (+37.69% SR, +28.96% SPL, -30.69% MSS, -46.40% NE) but still lags behind human performance.

Conclusion: The work lays a foundation for future improvements in AVOS tasks, emphasizing the need for better semantic reasoning and spatial exploration.

Abstract: Aerial Visual Object Search (AVOS) tasks in urban environments require
Unmanned Aerial Vehicles (UAVs) to autonomously search for and identify target
objects using visual and textual cues without external guidance. Existing
approaches struggle in complex urban environments due to redundant semantic
processing, similar object distinction, and the exploration-exploitation
dilemma. To bridge this gap and support the AVOS task, we introduce CityAVOS,
the first benchmark dataset for autonomous search of common urban objects. This
dataset comprises 2,420 tasks across six object categories with varying
difficulty levels, enabling comprehensive evaluation of UAV agents' search
capabilities. To solve the AVOS tasks, we also propose PRPSearcher
(Perception-Reasoning-Planning Searcher), a novel agentic method powered by
multi-modal large language models (MLLMs) that mimics human three-tier
cognition. Specifically, PRPSearcher constructs three specialized maps: an
object-centric dynamic semantic map enhancing spatial perception, a 3D
cognitive map based on semantic attraction values for target reasoning, and a
3D uncertainty map for balanced exploration-exploitation search. Also, our
approach incorporates a denoising mechanism to mitigate interference from
similar objects and utilizes an Inspiration Promote Thought (IPT) prompting
mechanism for adaptive action planning. Experimental results on CityAVOS
demonstrate that PRPSearcher surpasses existing baselines in both success rate
and search efficiency (on average: +37.69% SR, +28.96% SPL, -30.69% MSS, and
-46.40% NE). While promising, the performance gap compared to humans highlights
the need for better semantic reasoning and spatial exploration capabilities in
AVOS tasks. This work establishes a foundation for future advances in embodied
target search. Dataset and source code are available at
https://anonymous.4open.science/r/CityAVOS-3DF8.

</details>


### [172] [Deep learning-based interactive segmentation in remote sensing](https://arxiv.org/pdf/2308.13174)
*Zhe Wang, Shoukun Sun, Xiang Que, Xiaogang Ma, Carmen Galaz Garcia*

Main category: cs.CV

TL;DR: This study benchmarks click-based interactive segmentation models for remote sensing imagery, introducing CFR for improved results. SimpleClick-CFR performed best, leading to the development of the SegMap tool.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between click-based interactive segmentation and remote sensing image analysis, as few studies address this application.

Method: Benchmarked five state-of-the-art interactive segmentation models (RITM, FocalClick, SimpleClick, ICL, SAM) on aerial imagery datasets, introduced CFR for refinement, and integrated it into all models.

Result: SimpleClick-CFR consistently outperformed other methods. SegMap, an online tool, was developed for interactive segmentation of remote sensing data.

Conclusion: The study successfully applied interactive segmentation to remote sensing, with SimpleClick-CFR as the top performer, and introduced SegMap for practical use.

Abstract: Interactive segmentation, a computer vision technique where a user provides
guidance to help an algorithm segment a feature of interest in an image, has
achieved outstanding accuracy and efficient human-computer interaction.
However, few studies have discussed its application to remote sensing imagery,
where click-based interactive segmentation could greatly facilitate the
analysis of complicated landscapes. This study aims to bridge the gap between
click-based interactive segmentation and remote sensing image analysis by
conducting a benchmark study on various click-based interactive segmentation
models. We assessed the performance of five state-of-the-art interactive
segmentation methods (Reviving Iterative Training with Mask Guidance for
Interactive Segmentation (RITM), FocalClick, SimpleClick, Iterative Click Loss
(ICL), and Segment Anything (SAM)) on two high-resolution aerial imagery
datasets. The Cascade-Forward Refinement (CFR) approach, an innovative
inference strategy for interactive segmentation, was also introduced to enhance
the segmentation results without requiring manual efforts. We further
integrated CFR into all models for comparison. The performance of these methods
on various land cover types, different object sizes, and multiple band
combinations in the datasets was evaluated. The SimpleClick-CFR model
consistently outperformed the other methods in our experiments. Building upon
these findings, we developed a dedicated online tool called SegMap for
interactive segmentation of remote sensing data. SegMap incorporates a
well-performing interactive model that is fine-tuned with remote sensing data.
Unlike existing interactive segmentation tools, SegMap offers robust
interactivity, modifiability, and adaptability to analyze remote sensing
imagery.

</details>


### [173] [Optimized View and Geometry Distillation from Multi-view Diffuser](https://arxiv.org/pdf/2312.06198)
*Youjia Zhang, Zikai Song, Junqing Yu, Yawei Luo, Wei Yang*

Main category: cs.CV

TL;DR: The paper introduces Unbiased Score Distillation (USD) to improve multi-view image synthesis by refining radiance field fidelity and leveraging a two-step diffusion model for high-quality results.


<details>
  <summary>Details</summary>
Motivation: Addressing issues like inconsistency in synthesized views and over-smoothing in geometry from single-view inputs, while maintaining camera flexibility.

Method: Uses radiance field optimization with USD, a two-step 2D diffusion model specialization, and geometry recovery from refined multi-view images.

Result: Generates high-quality multi-view images with faithful geometry, comparable to state-of-the-art models, without camera constraints.

Conclusion: The proposed USD and two-step diffusion approach effectively enhances view consistency and geometry fidelity in multi-view synthesis.

Abstract: Generating multi-view images from a single input view using image-conditioned
diffusion models is a recent advancement and has shown considerable potential.
However, issues such as the lack of consistency in synthesized views and
over-smoothing in extracted geometry persist. Previous methods integrate
multi-view consistency modules or impose additional supervisory to enhance view
consistency while compromising on the flexibility of camera positioning and
limiting the versatility of view synthesis. In this study, we consider the
radiance field optimized during geometry extraction as a more rigid consistency
prior, compared to volume and ray aggregation used in previous works. We
further identify and rectify a critical bias in the traditional radiance field
optimization process through score distillation from a multi-view diffuser. We
introduce an Unbiased Score Distillation (USD) that utilizes unconditioned
noises from a 2D diffusion model, greatly refining the radiance field fidelity.
We leverage the rendered views from the optimized radiance field as the basis
and develop a two-step specialization process of a 2D diffusion model, which is
adept at conducting object-specific denoising and generating high-quality
multi-view images. Finally, we recover faithful geometry and texture directly
from the refined multi-view images. Empirical evaluations demonstrate that our
optimized geometry and view distillation technique generates comparable results
to the state-of-the-art models trained on extensive datasets, all while
maintaining freedom in camera positioning. Please see our project page at
https://youjiazhang.github.io/USD/.

</details>


### [174] [Equipping Sketch Patches with Context-Aware Positional Encoding for Graphic Sketch Representation](https://arxiv.org/pdf/2403.17525)
*Sicong Zang, Zhijun Fang*

Main category: cs.CV

TL;DR: The paper proposes a method to improve sketch representation by using context-aware positional encoding (PE) to address unreliable graph edges from drawing orders. It combines absolute and relative PEs with visual patterns for better sketch learning.


<details>
  <summary>Details</summary>
Motivation: Existing methods link sketch patches as graph edges based on drawing orders, but these edges can be unreliable due to inconsistent contextual relationships. The paper aims to enhance sketch learning by better utilizing drawing orders.

Method: The method uses sinusoidal absolute PE for sequential positions and learnable relative PE for contextual relationships. Both PEs are injected into graph nodes, not edges, and combined with visual patterns via graph convolutional networks.

Result: The approach significantly improves sketch healing and controllable sketch synthesis, as shown in experiments.

Conclusion: The proposed method effectively enhances sketch representations by leveraging drawing orders and contextual information, outperforming previous approaches.

Abstract: When benefiting graphic sketch representation with sketch drawing orders,
recent studies have linked sketch patches as graph edges by drawing orders in
accordance to a temporal-based nearest neighboring strategy. However, such
constructed graph edges may be unreliable, since the contextual relationships
between patches may be inconsistent with the sequential positions in drawing
orders, due to variants of sketch drawings. In this paper, we propose a
variant-drawing-protected method by equipping sketch patches with context-aware
positional encoding (PE) to make better use of drawing orders for sketch
learning. We introduce a sinusoidal absolute PE to embed the sequential
positions in drawing orders, and a learnable relative PE to encode the unseen
contextual relationships between patches. Both types of PEs never attend the
construction of graph edges, but are injected into graph nodes to cooperate
with the visual patterns captured from patches. After linking nodes by semantic
proximity, during message aggregation via graph convolutional networks, each
node receives both semantic features from patches and contextual information
from PEs from its neighbors, which equips local patch patterns with global
contextual information, further obtaining drawing-order-enhanced sketch
representations. Experimental results indicate that our method significantly
improves sketch healing and controllable sketch synthesis. The source codes
could be found at https://github.com/SCZang/DC-gra2seq.

</details>


### [175] [TextCenGen: Attention-Guided Text-Centric Background Adaptation for Text-to-Image Generation](https://arxiv.org/pdf/2404.11824)
*Tianyi Liang, Jiangqi Liu, Yifei Huang, Shiqi Jiang, Jianshen Shi, Changbo Wang, Chenhui Li*

Main category: cs.CV

TL;DR: TextCenGen is a training-free method for generating text-friendly backgrounds in T2I models by relocating conflicting objects and optimizing attention maps, outperforming existing methods in reducing text overlap while preserving image quality.


<details>
  <summary>Details</summary>
Motivation: The challenge of creating backgrounds that naturally accommodate text in T2I generation for applications like graphic design, where visual hierarchy is crucial.

Method: Analyzes cross-attention maps to identify conflicting objects, relocates them using a force-directed graph, and applies attention-excluding constraints for smooth backgrounds.

Result: Achieves 23% lower saliency overlap in text regions while maintaining 98% semantic fidelity (CLIP score and VTCM).

Conclusion: TextCenGen effectively balances semantic fidelity and visual quality without requiring additional training.

Abstract: Text-to-image (T2I) generation has made remarkable progress in producing
high-quality images, but a fundamental challenge remains: creating backgrounds
that naturally accommodate text placement without compromising image quality.
This capability is non-trivial for real-world applications like graphic design,
where clear visual hierarchy between content and text is essential. Prior work
has primarily focused on arranging layouts within existing static images,
leaving unexplored the potential of T2I models for generating text-friendly
backgrounds. We present TextCenGen, a training-free dynamic background
adaptation in the blank region for text-friendly image generation. Instead of
directly reducing attention in text areas, which degrades image quality, we
relocate conflicting objects before background optimization. Our method
analyzes cross-attention maps to identify conflicting objects overlapping with
text regions and uses a force-directed graph approach to guide their
relocation, followed by attention excluding constraints to ensure smooth
backgrounds. Our method is plug-and-play, requiring no additional training
while well balancing both semantic fidelity and visual quality. Evaluated on
our proposed text-friendly T2I benchmark of 27,000 images across four seed
datasets, TextCenGen outperforms existing methods by achieving 23% lower
saliency overlap in text regions while maintaining 98% of the semantic fidelity
measured by CLIP score and our proposed Visual-Textual Concordance Metric
(VTCM).

</details>


### [176] [Discriminative and Consistent Representation Distillation](https://arxiv.org/pdf/2407.11802)
*Nikolaos Giakoumoglou, Tania Stathaki*

Main category: cs.CV

TL;DR: DCD improves knowledge distillation by combining contrastive loss and consistency regularization, achieving state-of-the-art results and better generalization.


<details>
  <summary>Details</summary>
Motivation: Current contrastive learning in KD focuses on discrimination but ignores structural relationships from the teacher model.

Method: DCD uses contrastive loss and consistency regularization with adaptive temperature and bias parameters.

Result: DCD achieves top performance on CIFAR-100 and ImageNet, sometimes surpassing teacher accuracy, and shows superior cross-dataset generalization.

Conclusion: DCD effectively balances discrimination and structural relationships, outperforming existing methods in KD.

Abstract: Knowledge Distillation (KD) aims to transfer knowledge from a large teacher
model to a smaller student model. While contrastive learning has shown promise
in self-supervised learning by creating discriminative representations, its
application in knowledge distillation remains limited and focuses primarily on
discrimination, neglecting the structural relationships captured by the teacher
model. To address this limitation, we propose Discriminative and Consistent
Distillation (DCD), which employs a contrastive loss along with a consistency
regularization to minimize the discrepancy between the distributions of teacher
and student representations. Our method introduces learnable temperature and
bias parameters that adapt during training to balance these complementary
objectives, replacing the fixed hyperparameters commonly used in contrastive
learning approaches. Through extensive experiments on CIFAR-100 and ImageNet
ILSVRC-2012, we demonstrate that DCD achieves state-of-the-art performance,
with the student model sometimes surpassing the teacher's accuracy.
Furthermore, we show that DCD's learned representations exhibit superior
cross-dataset generalization when transferred to Tiny ImageNet and STL-10.

</details>


### [177] [Relational Representation Distillation](https://arxiv.org/pdf/2407.12073)
*Nikolaos Giakoumoglou, Tania Stathaki*

Main category: cs.CV

TL;DR: A novel knowledge distillation method preserves relative relationships between instances using separate temperature parameters for teacher and student, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Standard KL divergence fails to capture structural relationships in teacher models, while contrastive learning imposes overly strict constraints.

Method: Uses separate temperature parameters for teacher and student distributions, with sharper student outputs to learn primary relationships while preserving secondary similarities.

Result: Outperforms existing methods, achieving better alignment with teacher models and sometimes surpassing teacher performance.

Conclusion: The proposed method effectively preserves structural relationships and improves knowledge transfer.

Abstract: Knowledge distillation involves transferring knowledge from large, cumbersome
teacher models to more compact student models. The standard approach minimizes
the Kullback-Leibler (KL) divergence between the probabilistic outputs of a
teacher and student network. However, this approach fails to capture important
structural relationships in the teacher's internal representations. Recent
advances have turned to contrastive learning objectives, but these methods
impose overly strict constraints through instance-discrimination, forcing apart
semantically similar samples even when they should maintain similarity. This
motivates an alternative objective by which we preserve relative relationships
between instances. Our method employs separate temperature parameters for
teacher and student distributions, with sharper student outputs, enabling
precise learning of primary relationships while preserving secondary
similarities. We show theoretical connections between our objective and both
InfoNCE loss and KL divergence. Experiments demonstrate that our method
significantly outperforms existing knowledge distillation methods across
diverse knowledge transfer tasks, achieving better alignment with teacher
models, and sometimes even outperforms the teacher network.

</details>


### [178] [Geometry-Aware Feature Matching for Large-Scale Structure from Motion](https://arxiv.org/pdf/2409.02310)
*Gonglin Chen, Jinsen Wu, Haiwei Chen, Wenbin Teng, Zhiyuan Gao, Andrew Feng, Rongjun Qin, Yajie Zhao*

Main category: cs.CV

TL;DR: A hybrid optimization-based method improves feature matching by combining geometry and color cues, enhancing correspondence density and accuracy in large-scale SfM systems.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of establishing dense correspondences in SfM systems, especially under significant view changes like air-to-ground scenarios with sparse overlap.

Method: Introduces geometry cues alongside color cues, formulates geometric verification as an optimization problem, and uses sparse correspondences as anchors to guide detector-free methods. Geometric consistency is enforced via Sampson Distance.

Result: Outperforms state-of-the-art methods, improving correspondence density, accuracy, and camera pose estimation, even in extreme large-scale settings.

Conclusion: The hybrid approach significantly advances feature matching in SfM, enabling robust performance in challenging scenarios.

Abstract: Establishing consistent and dense correspondences across multiple images is
crucial for Structure from Motion (SfM) systems. Significant view changes, such
as air-to-ground with very sparse view overlap, pose an even greater challenge
to the correspondence solvers. We present a novel optimization-based approach
that significantly enhances existing feature matching methods by introducing
geometry cues in addition to color cues. This helps fill gaps when there is
less overlap in large-scale scenarios. Our method formulates geometric
verification as an optimization problem, guiding feature matching within
detector-free methods and using sparse correspondences from detector-based
methods as anchor points. By enforcing geometric constraints via the Sampson
Distance, our approach ensures that the denser correspondences from
detector-free methods are geometrically consistent and more accurate. This
hybrid strategy significantly improves correspondence density and accuracy,
mitigates multi-view inconsistencies, and leads to notable advancements in
camera pose accuracy and point cloud density. It outperforms state-of-the-art
feature matching methods on benchmark datasets and enables feature matching in
challenging extreme large-scale settings.

</details>


### [179] [HarmoniCa: Harmonizing Training and Inference for Better Feature Caching in Diffusion Transformer Acceleration](https://arxiv.org/pdf/2410.01723)
*Yushi Huang, Zining Wang, Ruihao Gong, Jing Liu, Xinjie Zhang, Jinyang Guo, Xianglong Liu, Jun Zhang*

Main category: cs.CV

TL;DR: HarmoniCa improves Diffusion Transformers (DiTs) by harmonizing training and inference with Step-Wise Denoising Training and an Image Error Proxy-Guided Objective, achieving significant speedup and performance gains.


<details>
  <summary>Details</summary>
Motivation: DiTs face high inference costs and misaligned training-inference objectives, compromising efficiency and performance.

Method: Proposes HarmoniCa with Step-Wise Denoising Training (SDT) for continuity and Image Error Proxy-Guided Objective (IEPO) for balanced image quality and cache utilization.

Result: Achieves over 40% latency reduction, 2.07× speedup, and 25% training time reduction while improving performance.

Conclusion: HarmoniCa effectively addresses DiTs' deployment challenges, offering a practical solution for efficient and high-quality generative tasks.

Abstract: Diffusion Transformers (DiTs) excel in generative tasks but face practical
deployment challenges due to high inference costs. Feature caching, which
stores and retrieves redundant computations, offers the potential for
acceleration. Existing learning-based caching, though adaptive, overlooks the
impact of the prior timestep. It also suffers from misaligned
objectives--aligned predicted noise vs. high-quality images--between training
and inference. These two discrepancies compromise both performance and
efficiency. To this end, we harmonize training and inference with a novel
learning-based caching framework dubbed HarmoniCa. It first incorporates
Step-Wise Denoising Training (SDT) to ensure the continuity of the denoising
process, where prior steps can be leveraged. In addition, an Image Error
Proxy-Guided Objective (IEPO) is applied to balance image quality against cache
utilization through an efficient proxy to approximate the image error.
Extensive experiments across $8$ models, $4$ samplers, and resolutions from
$256\times256$ to $2K$ demonstrate superior performance and speedup of our
framework. For instance, it achieves over $40\%$ latency reduction (i.e.,
$2.07\times$ theoretical speedup) and improved performance on PixArt-$\alpha$.
Remarkably, our image-free approach reduces training time by $25\%$ compared
with the previous method. Our code is available at
https://github.com/ModelTC/HarmoniCa.

</details>


### [180] [Calibrated and Efficient Sampling-Free Confidence Estimation for LiDAR Scene Semantic Segmentation](https://arxiv.org/pdf/2411.11935)
*Hanieh Shojaei Miandashti, Qianqian Zou, Claus Brenner*

Main category: cs.CV

TL;DR: A sampling-free method for well-calibrated confidence estimation in LiDAR semantic segmentation, improving speed and reliability for safety-critical applications.


<details>
  <summary>Details</summary>
Motivation: Ensuring reliable deep learning models with accurate confidence estimates is vital for safety-critical tasks like autonomous driving, where precise uncertainty estimation is required.

Method: Introduces a sampling-free approach for confidence estimation in classification tasks, aligning with true accuracy and reducing inference time compared to sampling-based methods.

Result: Achieves well-calibrated confidence values with faster processing, as shown by the ACE metric, and produces underconfident predictions, beneficial for safety.

Conclusion: The sampling-free method provides efficient, reliable confidence estimates for LiDAR semantic segmentation, suitable for real-time applications.

Abstract: Reliable deep learning models require not only accurate predictions but also
well-calibrated confidence estimates to ensure dependable uncertainty
estimation. This is crucial in safety-critical applications like autonomous
driving, which depend on rapid and precise semantic segmentation of LiDAR point
clouds for real-time 3D scene understanding. In this work, we introduce a
sampling-free approach for estimating well-calibrated confidence values for
classification tasks, achieving alignment with true classification accuracy and
significantly reducing inference time compared to sampling-based methods. Our
evaluation using the Adaptive Calibration Error (ACE) metric for LiDAR semantic
segmentation shows that our approach maintains well-calibrated confidence
values while achieving increased processing speed compared to a sampling
baseline. Additionally, reliability diagrams reveal that our method produces
underconfidence rather than overconfident predictions, an advantage for
safety-critical applications. Our sampling-free approach offers well-calibrated
and time-efficient predictions for LiDAR scene semantic segmentation.

</details>


### [181] [CHOICE: Benchmarking the Remote Sensing Capabilities of Large Vision-Language Models](https://arxiv.org/pdf/2411.18145)
*Xiao An, Jiaxing Sun, Zihan Gui, Wei He*

Main category: cs.CV

TL;DR: CHOICE is a benchmark for evaluating Large Vision-Language Models (VLMs) in remote sensing, covering perception and reasoning with 6 secondary dimensions and 23 tasks. It includes 10,507 rigorously curated problems from 50 cities.


<details>
  <summary>Details</summary>
Motivation: The lack of a systematic benchmark for evaluating VLMs in remote sensing tasks motivated the creation of CHOICE.

Method: CHOICE categorizes capabilities into 2 primary dimensions (perception and reasoning), 6 secondary dimensions, and 23 tasks. It uses multiple-choice questions with definitive answers for objective assessment.

Result: Evaluation of 3 proprietary and 21 open-source VLMs revealed critical limitations in remote sensing.

Conclusion: CHOICE aims to be a valuable resource for understanding VLMs' challenges and potential in remote sensing, with data released publicly.

Abstract: The rapid advancement of Large Vision-Language Models (VLMs), both
general-domain models and those specifically tailored for remote sensing, has
demonstrated exceptional perception and reasoning capabilities in Earth
observation tasks. However, a benchmark for systematically evaluating their
capabilities in this domain is still lacking. To bridge this gap, we propose
CHOICE, an extensive benchmark designed to objectively evaluate the
hierarchical remote sensing capabilities of VLMs. Focusing on 2 primary
capability dimensions essential to remote sensing: perception and reasoning, we
further categorize 6 secondary dimensions and 23 leaf tasks to ensure a
well-rounded assessment coverage. CHOICE guarantees the quality of all 10,507
problems through a rigorous process of data collection from 50 globally
distributed cities, question construction and quality control. The newly
curated data and the format of multiple-choice questions with definitive
answers allow for an objective and straightforward performance assessment. Our
evaluation of 3 proprietary and 21 open-source VLMs highlights their critical
limitations within this specialized context. We hope that CHOICE will serve as
a valuable resource and offer deeper insights into the challenges and potential
of VLMs in the field of remote sensing. We will release CHOICE at
https://github.com/ShawnAn-WHU/CHOICE.

</details>


### [182] [Training Strategies for Isolated Sign Language Recognition](https://arxiv.org/pdf/2412.11553)
*Karina Kvanchiani, Roman Kraynov, Elizaveta Petrova, Petr Surovcev, Aleksandr Nagaev, Alexander Kapitanov*

Main category: cs.CV

TL;DR: A training pipeline for Isolated Sign Language Recognition (ISLR) improves recognition by addressing data quality and speed variability, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Enhancing communication accessibility for deaf and hard of hearing individuals by improving ISLR despite challenges like low data quality and varying sign speeds.

Method: Introduces a model training pipeline with image/video augmentations and a regression head combined with IoU-balanced classification loss to capture temporal information.

Result: The pipeline adapts well to datasets and architectures, outperforming benchmarks on WLASL and Slovo datasets.

Conclusion: The proposed components effectively address ISLR challenges, enhancing recognition performance and achieving top results.

Abstract: Accurate recognition and interpretation of sign language are crucial for
enhancing communication accessibility for deaf and hard of hearing individuals.
However, current approaches of Isolated Sign Language Recognition (ISLR) often
face challenges such as low data quality and variability in gesturing speed.
This paper introduces a comprehensive model training pipeline for ISLR designed
to accommodate the distinctive characteristics and constraints of the Sign
Language (SL) domain. The constructed pipeline incorporates carefully selected
image and video augmentations to tackle the challenges of low data quality and
varying sign speeds. Including an additional regression head combined with
IoU-balanced classification loss enhances the model's awareness of the gesture
and simplifies capturing temporal information. Extensive experiments
demonstrate that the developed training pipeline easily adapts to different
datasets and architectures. Additionally, the ablation study shows that each
proposed component expands the potential to consider ISLR task specifics. The
presented strategies enhance recognition performance across various ISLR
benchmarks and achieve state-of-the-art results on the WLASL and Slovo
datasets.

</details>


### [183] [2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining](https://arxiv.org/pdf/2501.00958)
*Wenqi Zhang, Hang Zhang, Xin Li, Jiashuo Sun, Yongliang Shen, Weiming Lu, Deli Zhao, Yueting Zhuang, Lidong Bing*

Main category: cs.CV

TL;DR: A high-quality multimodal textbook corpus is introduced for VLM pretraining, derived from instructional videos, offering richer knowledge and better coherence than existing datasets.


<details>
  <summary>Details</summary>
Motivation: Existing interleaved datasets from webpages suffer from low knowledge density and poor coherence, while instructional videos remain underexplored for VLM training.

Method: Collects 22,000 class hours of instructional videos, extracts visual, audio, and textual knowledge, and organizes them into an image-text interleaved corpus.

Result: Outperforms counterparts in pretraining, excelling in knowledge- and reasoning-intensive tasks like ScienceQA and MathVista.

Conclusion: The video-centric textbook enhances VLM performance, particularly in context-aware tasks, and is publicly available.

Abstract: Compared to image-text pair data, interleaved corpora enable Vision-Language
Models (VLMs) to understand the world more naturally like humans. However, such
existing datasets are crawled from webpage, facing challenges like low
knowledge density, loose image-text relations, and poor logical coherence
between images. On the other hand, the internet hosts vast instructional videos
(e.g., online geometry courses) that are widely used by humans to learn
foundational subjects, yet these valuable resources remain underexplored in VLM
training. In this paper, we introduce a high-quality \textbf{multimodal
textbook} corpus with richer foundational knowledge for VLM pretraining. It
collects over 2.5 years of instructional videos, totaling 22,000 class hours.
We first use an LLM-proposed taxonomy to systematically gather instructional
videos. Then we progressively extract and refine visual (keyframes), audio
(ASR), and textual knowledge (OCR) from the videos, and organize as an
image-text interleaved corpus based on temporal order. Compared to its
counterparts, our video-centric textbook offers more coherent context, richer
knowledge, and better image-text alignment. Experiments demonstrate its superb
pretraining performance, particularly in knowledge- and reasoning-intensive
tasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook
exhibit outstanding interleaved context awareness, leveraging visual and
textual cues in their few-shot context for task solving. Our code are available
at https://github.com/DAMO-NLP-SG/multimodal_textbook.

</details>


### [184] [HLV-1K: A Large-scale Hour-Long Video Benchmark for Time-Specific Long Video Understanding](https://arxiv.org/pdf/2501.01645)
*Heqing Zou, Tianze Luo, Guiyang Xie, Victor Xiao Jie Zhang, Fengmao Lv, Guangcong Wang, Junyang Chen, Zhuochen Wang, Hansheng Zhang, Huaijian Zhang*

Main category: cs.CV

TL;DR: The paper introduces HLV-1K, a large-scale benchmark for hour-long video understanding, addressing gaps in long-term video analysis, model efficiency, and dataset availability.


<details>
  <summary>Details</summary>
Motivation: Despite the popularity of multimodal large language models, hour-long video understanding is under-explored due to challenges in long-term analysis, inefficient models, and lack of datasets.

Method: The authors build HLV-1K, a dataset with 1009 hour-long videos and 14,847 QA/MCQA pairs, covering diverse reasoning tasks.

Result: HLV-1K is evaluated using state-of-the-art methods, proving its utility for testing long video understanding across tasks and levels.

Conclusion: The benchmark supports future research in granular long video understanding, such as live videos, meetings, and movies.

Abstract: Multimodal large language models have become a popular topic in deep visual
understanding due to many promising real-world applications. However, hour-long
video understanding, spanning over one hour and containing tens of thousands of
visual frames, remains under-explored because of 1) challenging long-term video
analyses, 2) inefficient large-model approaches, and 3) lack of large-scale
benchmark datasets. Among them, in this paper, we focus on building a
large-scale hour-long long video benchmark, HLV-1K, designed to evaluate long
video understanding models. HLV-1K comprises 1009 hour-long videos with 14,847
high-quality question answering (QA) and multi-choice question asnwering (MCQA)
pairs with time-aware query and diverse annotations, covering frame-level,
within-event-level, cross-event-level, and long-term reasoning tasks. We
evaluate our benchmark using existing state-of-the-art methods and demonstrate
its value for testing deep long video understanding capabilities at different
levels and for various tasks. This includes promoting future long video
understanding tasks at a granular level, such as deep understanding of long
live videos, meeting recordings, and movies.

</details>


### [185] [ConceptMaster: Multi-Concept Video Customization on Diffusion Transformer Models Without Test-Time Tuning](https://arxiv.org/pdf/2501.04698)
*Yuzhou Huang, Ziyang Yuan, Quande Liu, Qiulin Wang, Xintao Wang, Ruimao Zhang, Pengfei Wan, Di Zhang, Kun Gai*

Main category: cs.CV

TL;DR: ConceptMaster addresses Multi-Concept Video Customization challenges by learning decoupled embeddings and using a data pipeline for high-quality training, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with identity decoupling and lack high-quality video-entity pairs for multi-concept customization.

Method: Proposes ConceptMaster with decoupled multi-concept embeddings and a data construction pipeline for diverse scenarios.

Result: Outperforms previous methods in concept fidelity, identity decoupling, and video quality across six scenarios.

Conclusion: ConceptMaster shows promise for generating personalized, accurate content in video diffusion models.

Abstract: Text-to-video generation has made remarkable advancements through diffusion
models. However, Multi-Concept Video Customization (MCVC) remains a significant
challenge. We identify two key challenges for this task: 1) the identity
decoupling issue, where directly adopting existing customization methods
inevitably mix identity attributes when handling multiple concepts
simultaneously, and 2) the scarcity of high-quality video-entity pairs, which
is crucial for training a model that can well represent and decouple various
customized concepts in video generation. To address these challenges, we
introduce ConceptMaster, a novel framework that effectively addresses the
identity decoupling issues while maintaining concept fidelity in video
customization. Specifically, we propose to learn decoupled multi-concept
embeddings and inject them into diffusion models in a standalone manner, which
effectively guarantees the quality of customized videos with multiple
identities, even for highly similar visual concepts. To overcome the scarcity
of high-quality MCVC data, we establish a data construction pipeline, which
enables collection of high-quality multi-concept video-entity data pairs across
diverse scenarios. A multi-concept video evaluation set is further devised to
comprehensively validate our method from three dimensions, including concept
fidelity, identity decoupling ability, and video generation quality, across six
different concept composition scenarios. Extensive experiments demonstrate that
ConceptMaster significantly outperforms previous methods for video
customization tasks, showing great potential to generate personalized and
semantically accurate content for video diffusion models.

</details>


### [186] [Vision-Language Models Do Not Understand Negation](https://arxiv.org/pdf/2501.09425)
*Kumail Alhamoud, Shaden Alshammari, Yonglong Tian, Guohao Li, Philip Torr, Yoon Kim, Marzyeh Ghassemi*

Main category: cs.CV

TL;DR: The paper evaluates how well vision-language models (VLMs) understand negation, introduces NegBench for testing, and improves performance via synthetic data finetuning.


<details>
  <summary>Details</summary>
Motivation: Understanding negation is crucial for vision-language applications, but current VLMs perform poorly in this area.

Method: Introduces NegBench with 18 tasks and 79k examples, evaluates VLMs, and finetunes CLIP models on synthetic negated captions.

Result: VLMs struggle with negation, but finetuning improves recall by 10% and accuracy by 28%.

Conclusion: Negation understanding in VLMs is limited but can be enhanced through targeted finetuning.

Abstract: Many practical vision-language applications require models that understand
negation, e.g., when using natural language to retrieve images which contain
certain objects but not others. Despite advancements in vision-language models
(VLMs) through large-scale training, their ability to comprehend negation
remains underexplored. This study addresses the question: how well do current
VLMs understand negation? We introduce NegBench, a new benchmark designed to
evaluate negation understanding across 18 task variations and $79$k examples
spanning image, video, and medical datasets. The benchmark consists of two core
tasks designed to evaluate negation understanding in diverse multimodal
settings: Retrieval with Negation and Multiple Choice Questions with Negated
Captions. Our evaluation reveals that modern VLMs struggle significantly with
negation, often performing at chance level. To address these shortcomings, we
explore a data-centric approach wherein we finetune CLIP models on large-scale
synthetic datasets containing millions of negated captions. We show that this
approach can result in a 10% increase in recall on negated queries and a 28%
boost in accuracy on multiple-choice questions with negated captions.

</details>


### [187] [GP-GS: Gaussian Processes for Enhanced Gaussian Splatting](https://arxiv.org/pdf/2502.02283)
*Zhihao Guo, Jingxuan Su, Shenglin Wang, Jinlong Fan, Jing Zhang, Wei Zhou, Hadi Amirpour, Yunlong Zhao, Liangxiu Han, Peng Wang*

Main category: cs.CV

TL;DR: The paper introduces GP-GS, a 3D reconstruction framework that enhances Gaussian Splatting by using Gaussian Processes to densify sparse SfM point clouds, improving reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: Sparse SfM point clouds limit the quality of 3D Gaussian Splatting for novel view synthesis. The paper aims to overcome this by densifying the point clouds adaptively.

Method: Proposes GP-GS, a dynamic sampling and filtering pipeline using multi-output Gaussian Processes to expand and refine SfM point clouds with uncertainty-guided pruning.

Result: The framework generates dense point clouds, improving initial 3D Gaussians and enhancing reconstruction performance, validated on synthetic and real-world datasets.

Conclusion: GP-GS effectively addresses the limitations of sparse SfM point clouds, offering a practical solution for high-quality 3D reconstruction.

Abstract: 3D Gaussian Splatting has emerged as an efficient photorealistic novel view
synthesis method. However, its reliance on sparse Structure-from-Motion (SfM)
point clouds often limits scene reconstruction quality. To address the
limitation, this paper proposes a novel 3D reconstruction framework, Gaussian
Processes enhanced Gaussian Splatting (GP-GS), in which a multi-output Gaussian
Process model is developed to enable adaptive and uncertainty-guided
densification of sparse SfM point clouds. Specifically, we propose a dynamic
sampling and filtering pipeline that adaptively expands the SfM point clouds by
leveraging GP-based predictions to infer new candidate points from the input 2D
pixels and depth maps. The pipeline utilizes uncertainty estimates to guide the
pruning of high-variance predictions, ensuring geometric consistency and
enabling the generation of dense point clouds. These densified point clouds
provide high-quality initial 3D Gaussians, enhancing reconstruction
performance. Extensive experiments conducted on synthetic and real-world
datasets across various scales validate the effectiveness and practicality of
the proposed framework.

</details>


### [188] [LP-DETR: Layer-wise Progressive Relations for Object Detection](https://arxiv.org/pdf/2502.05147)
*Zhengjian Kang, Ye Zhang, Xiaoyu Deng, Xintao Li, Yongzhe Zhang*

Main category: cs.CV

TL;DR: LP-DETR improves DETR-based object detection by adaptively learning multi-scale spatial relationships through a relation-aware self-attention mechanism, enhancing convergence and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of standard self-attention in DETR by better modeling spatial dependencies at different scales (local, medium, global) across decoder layers.

Method: Introduces a relation-aware self-attention mechanism that progressively balances multi-scale spatial relationships in decoder layers.

Result: Achieves 52.3% AP (12 epochs) and 52.5% AP (24 epochs) with ResNet-50, and 58.0% AP with Swin-L, outperforming standard self-attention.

Conclusion: LP-DETR effectively captures evolving spatial dependencies, prioritizing local relations early and broader contexts later, offering insights for future object detection research.

Abstract: This paper presents LP-DETR (Layer-wise Progressive DETR), a novel approach
that enhances DETR-based object detection through multi-scale relation
modeling. Our method introduces learnable spatial relationships between object
queries through a relation-aware self-attention mechanism, which adaptively
learns to balance different scales of relations (local, medium and global)
across decoder layers. This progressive design enables the model to effectively
capture evolving spatial dependencies throughout the detection pipeline.
Extensive experiments on COCO 2017 dataset demonstrate that our method improves
both convergence speed and detection accuracy compared to standard
self-attention module. The proposed method achieves competitive results,
reaching 52.3\% AP with 12 epochs and 52.5\% AP with 24 epochs using ResNet-50
backbone, and further improving to 58.0\% AP with Swin-L backbone. Furthermore,
our analysis reveals an interesting pattern: the model naturally learns to
prioritize local spatial relations in early decoder layers while gradually
shifting attention to broader contexts in deeper layers, providing valuable
insights for future research in object detection.

</details>


### [189] [MGPATH: Vision-Language Model with Multi-Granular Prompt Learning for Few-Shot WSI Classification](https://arxiv.org/pdf/2502.07409)
*Anh-Tien Nguyen, Duy Minh Ho Nguyen, Nghiem Tuong Diep, Trung Quoc Nguyen, Nhat Ho, Jacqueline Michelle Metsch, Miriam Cindy Maurer, Daniel Sonntag, Hanibal Bohnenberger, Anne-Christin Hauschild*

Main category: cs.CV

TL;DR: The paper introduces a prompt learning method for few-shot pathology image classification, leveraging a vision-language model with multi-granular attention and optimal transport-based distance to improve accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Challenges in whole slide pathology image classification include gigapixel sizes and limited annotations, which hinder model generalization. The goal is to adapt large vision-language models for few-shot learning in this domain.

Method: Extends Prov-GigaPath into a vision-language model using adaptors and contrastive learning. Introduces multi-granular attention for prompt-image interactions and uses optimal transport-based distance for robustness.

Result: Outperforms competitors like CLIP, PLIP, and Prov-GigaPath integrated PLIP across lung, kidney, and breast pathology datasets.

Conclusion: The proposed method effectively addresses few-shot pathology classification challenges, improving performance and robustness, with released implementations and models.

Abstract: Whole slide pathology image classification presents challenges due to
gigapixel image sizes and limited annotation labels, hindering model
generalization. This paper introduces a prompt learning method to adapt large
vision-language models for few-shot pathology classification. We first extend
the Prov-GigaPath vision foundation model, pre-trained on 1.3 billion pathology
image tiles, into a vision-language model by adding adaptors and aligning it
with medical text encoders via contrastive learning on 923K image-text pairs.
The model is then used to extract visual features and text embeddings from
few-shot annotations and fine-tunes with learnable prompt embeddings. Unlike
prior methods that combine prompts with frozen features using prefix embeddings
or self-attention, we propose multi-granular attention that compares
interactions between learnable prompts with individual image patches and groups
of them. This approach improves the model's ability to capture both
fine-grained details and broader context, enhancing its recognition of complex
patterns across sub-regions. To further improve accuracy, we leverage
(unbalanced) optimal transport-based visual-text distance to secure model
robustness by mitigating perturbations that might occur during the data
augmentation process. Empirical experiments on lung, kidney, and breast
pathology modalities validate the effectiveness of our approach; thereby, we
surpass several of the latest competitors and consistently improve performance
across diverse architectures, including CLIP, PLIP, and Prov-GigaPath
integrated PLIP. We release our implementations and pre-trained models at this
MGPATH.

</details>


### [190] [CAST: Component-Aligned 3D Scene Reconstruction from an RGB Image](https://arxiv.org/pdf/2502.12894)
*Kaixin Yao, Longwen Zhang, Xinhao Yan, Yan Zeng, Qixuan Zhang, Wei Yang, Lan Xu, Jiayuan Gu, Jingyi Yu*

Main category: cs.CV

TL;DR: CAST is a novel method for high-quality 3D scene reconstruction from a single RGB image, leveraging object-level segmentation, GPT-based spatial analysis, occlusion-aware generation, and physics-aware correction.


<details>
  <summary>Details</summary>
Motivation: Current methods for 3D scene reconstruction from single images face domain-specific limitations and low-quality object generation, motivating the need for a more robust approach.

Method: CAST extracts 2D segmentation and depth, uses GPT for spatial analysis, employs occlusion-aware 3D generation, and applies physics-aware correction with SDF for alignment and consistency.

Result: The method achieves coherent and physically consistent 3D scene reconstruction, addressing occlusions, object penetration, and floating objects.

Conclusion: CAST enables realistic 3D scene recovery, useful for robotics and simulation, by ensuring accurate alignment and physical coherence.

Abstract: Recovering high-quality 3D scenes from a single RGB image is a challenging
task in computer graphics. Current methods often struggle with domain-specific
limitations or low-quality object generation. To address these, we propose CAST
(Component-Aligned 3D Scene Reconstruction from a Single RGB Image), a novel
method for 3D scene reconstruction and recovery. CAST starts by extracting
object-level 2D segmentation and relative depth information from the input
image, followed by using a GPT-based model to analyze inter-object spatial
relationships. This enables the understanding of how objects relate to each
other within the scene, ensuring more coherent reconstruction. CAST then
employs an occlusion-aware large-scale 3D generation model to independently
generate each object's full geometry, using MAE and point cloud conditioning to
mitigate the effects of occlusions and partial object information, ensuring
accurate alignment with the source image's geometry and texture. To align each
object with the scene, the alignment generation model computes the necessary
transformations, allowing the generated meshes to be accurately placed and
integrated into the scene's point cloud. Finally, CAST incorporates a
physics-aware correction step that leverages a fine-grained relation graph to
generate a constraint graph. This graph guides the optimization of object
poses, ensuring physical consistency and spatial coherence. By utilizing Signed
Distance Fields (SDF), the model effectively addresses issues such as
occlusions, object penetration, and floating objects, ensuring that the
generated scene accurately reflects real-world physical interactions. CAST can
be leveraged in robotics, enabling efficient real-to-simulation workflows and
providing realistic, scalable simulation environments for robotic systems.

</details>


### [191] [Building Age Estimation: A New Multi-Modal Benchmark Dataset and Community Challenge](https://arxiv.org/pdf/2502.13818)
*Nikolaos Dionelis, Nicolas Longépé, Alessandra Feliciotti, Mattia Marconcini, Devis Peressutti, Nika Oman Kadunc, JaeWan Park, Hagai Raja Sinulingga, Steve Andreas Immanuel, Ba Tran, Caroline Arnold*

Main category: cs.CV

TL;DR: The paper introduces a multi-modal dataset (MyCD) for estimating building construction years using AI and Transformer models, achieving good performance even on unseen cities and with only top-view modalities.


<details>
  <summary>Details</summary>
Motivation: Accurate estimation of building construction years is crucial for sustainability and urban planning to combat climate change.

Method: Uses AI and Transformer models on the MyCD dataset, which includes VHR images, Sentinel-2 EO data, and street-view images. Evaluates models on unseen cities and with reduced modalities.

Result: Top-4 challenge models show effective performance, even without street-view images, on unseen cities.

Conclusion: The approach is viable for real-world sustainability tasks, demonstrating robustness with limited input modalities.

Abstract: Estimating the construction year of buildings is of great importance for
sustainability. Sustainable buildings minimize energy consumption and are a key
part of responsible and sustainable urban planning and development to
effectively combat climate change. By using Artificial Intelligence (AI) and
recently proposed powerful Transformer models, we are able to estimate the
construction epoch of buildings from a multi-modal dataset. In this paper, we
introduce a new benchmark multi-modal dataset, i.e. the Map your City Dataset
(MyCD), containing top-view Very High Resolution (VHR) images, Earth
Observation (EO) multi-spectral data from the Copernicus Sentinel-2 satellite
constellation, and street-view images in many different cities in Europe that
are co-localized with respect to the building under study and labelled with the
construction epoch. We assess EO generalization performance on new/ previously
unseen cities that have been held-out from training and appear only during
inference. In this work, we present the community-based data challenge we
organized based on MyCD. The AI4EO Challenge ESA MapYourCity was opened in 2024
for 4 months. In this paper, we present the Top-4 performing models of the
challenge, and the evaluation results. During inference, the performance of the
models using: i) both all three input modalities, and ii) only the two top-view
modalities, i.e. without the street-view ground images, is examined. The
evaluation results in this work show that the models to estimate the
construction year of buildings are effective and can achieve good performance
on this difficult important real-world task, even when inference is on
previously unseen cities, as well as even when using only the two top-view
modalities (i.e. VHR and Sentinel-2) during inference.

</details>


### [192] [Efficient Adaptation For Remote Sensing Visual Grounding](https://arxiv.org/pdf/2503.23083)
*Hasan Moughnieh, Mohamad Chalhoub, Hasan Nasrallah, Cristiano Nattero, Paolo Campanella, Giovanni Nico, Ali J. Ghandour*

Main category: cs.CV

TL;DR: The paper explores Parameter Efficient Fine Tuning (PEFT) techniques to adapt pre-trained models for remote sensing visual grounding tasks, achieving SOTA performance with reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: Visual grounding in remote sensing is underexplored, and adapting pre-trained models offers a scalable, efficient solution.

Method: Applied PEFT techniques (LoRA, BitFit, adapters) to fine-tune Grounding DINO and OFA models for RS-specific tasks.

Result: Achieved performance comparable or superior to SOTA models while significantly lowering computational overhead.

Conclusion: PEFT techniques enable efficient, precise multi-modal analysis in RS, providing a cost-effective alternative to full training.

Abstract: Adapting pre-trained models has become an effective strategy in artificial
intelligence, offering a scalable and efficient alternative to training models
from scratch. In the context of remote sensing (RS), where visual grounding(VG)
remains underexplored, this approach enables the deployment of powerful
vision-language models to achieve robust cross-modal understanding while
significantly reducing computational overhead. To address this, we applied
Parameter Efficient Fine Tuning (PEFT) techniques to adapt these models for
RS-specific VG tasks. Specifically, we evaluated LoRA placement across
different modules in Grounding DINO and used BitFit and adapters to fine-tune
the OFA foundation model pre-trained on general-purpose VG datasets. This
approach achieved performance comparable to or surpassing current State Of The
Art (SOTA) models while significantly reducing computational costs. This study
highlights the potential of PEFT techniques to advance efficient and precise
multi-modal analysis in RS, offering a practical and cost-effective alternative
to full model training.

</details>


### [193] [VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation](https://arxiv.org/pdf/2503.01739)
*Wenhao Wang, Yi Yang*

Main category: cs.CV

TL;DR: VideoUFO is a novel dataset for text-to-video models, addressing gaps in existing datasets by focusing on user-centric topics and providing diverse, high-quality video clips with captions.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-video models underperform due to lack of training data aligned with user interests. VideoUFO aims to bridge this gap by curating a dataset tailored to real-world user prompts.

Method: The dataset was created by identifying 1,291 user-focused topics from VidProM, retrieving relevant videos from YouTube, splitting them into clips, and generating captions. The final dataset includes 1.09 million verified clips.

Result: Experiments show current models perform inconsistently across topics, while a model trained on VideoUFO excels on poorly performing topics.

Conclusion: VideoUFO provides a valuable resource for improving text-to-video models, with publicly available data and code under CC BY 4.0.

Abstract: Text-to-video generative models convert textual prompts into dynamic visual
content, offering wide-ranging applications in film production, gaming, and
education. However, their real-world performance often falls short of user
expectations. One key reason is that these models have not been trained on
videos related to some topics users want to create. In this paper, we propose
VideoUFO, the first Video dataset specifically curated to align with Users'
FOcus in real-world scenarios. Beyond this, our VideoUFO also features: (1)
minimal (0.29%) overlap with existing video datasets, and (2) videos searched
exclusively via YouTube's official API under the Creative Commons license.
These two attributes provide future researchers with greater freedom to broaden
their training sources. The VideoUFO comprises over 1.09 million video clips,
each paired with both a brief and a detailed caption (description).
Specifically, through clustering, we first identify 1,291 user-focused topics
from the million-scale real text-to-video prompt dataset, VidProM. Then, we use
these topics to retrieve videos from YouTube, split the retrieved videos into
clips, and generate both brief and detailed captions for each clip. After
verifying the clips with specified topics, we are left with about 1.09 million
video clips. Our experiments reveal that (1) current 16 text-to-video models do
not achieve consistent performance across all user-focused topics; and (2) a
simple model trained on VideoUFO outperforms others on worst-performing topics.
The dataset and code are publicly available at
https://huggingface.co/datasets/WenhaoWang/VideoUFO and
https://github.com/WangWenhao0716/BenchUFO under the CC BY 4.0 License.

</details>


### [194] [Semantic Shift Estimation via Dual-Projection and Classifier Reconstruction for Exemplar-Free Class-Incremental Learning](https://arxiv.org/pdf/2503.05423)
*Run He, Di Fang, Yicheng Xu, Yawen Cui, Ming Li, Cen Chen, Ziqian Zeng, Huiping Zhuang*

Main category: cs.CV

TL;DR: DPCR addresses semantic shift and decision bias in Exemplar-Free Class-Incremental Learning (EFCIL) using dual-projection shift estimation and classifier reconstruction, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: EFCIL suffers from catastrophic forgetting due to semantic shift and decision bias when learning new tasks without retaining old exemplars.

Method: DPCR uses a dual-projection to estimate semantic shift and ridge regression for classifier reconstruction, balancing old and new knowledge.

Result: DPCR outperforms state-of-the-art EFCIL methods on various datasets by effectively addressing semantic shift and decision bias.

Conclusion: DPCR provides a robust solution for EFCIL, balancing old and new tasks through innovative shift estimation and classifier reconstruction.

Abstract: Exemplar-Free Class-Incremental Learning (EFCIL) aims to sequentially learn
from distinct categories without retaining exemplars but easily suffers from
catastrophic forgetting of learned knowledge. While existing EFCIL methods
leverage knowledge distillation to alleviate forgetting, they still face two
critical challenges: semantic shift and decision bias. Specifically, the
embeddings of old tasks shift in the embedding space after learning new tasks,
and the classifier becomes biased towards new tasks due to training solely with
new data, hindering the balance between old and new knowledge. To address these
issues, we propose the Dual-Projection Shift Estimation and Classifier
Reconstruction (DPCR) approach for EFCIL. DPCR effectively estimates semantic
shift through a dual-projection, which combines a learnable transformation with
a row-space projection to capture both task-wise and category-wise shifts.
Furthermore, to mitigate decision bias, DPCR employs ridge regression to
reformulate a classifier reconstruction process. This reconstruction exploits
previous in covariance and prototype of each class after calibration with
estimated shift, thereby reducing decision bias. Extensive experiments
demonstrate that, on various datasets, DPCR effectively balances old and new
tasks, outperforming state-of-the-art EFCIL methods. Our codes are available at
https://github.com/RHe502/ICML25-DPCR.

</details>


### [195] [Motion Blender Gaussian Splatting for Dynamic Scene Reconstruction](https://arxiv.org/pdf/2503.09040)
*Xinyu Zhang, Haonan Chang, Yuhan Liu, Abdeslam Boularias*

Main category: cs.CV

TL;DR: MBGS introduces motion graphs for explicit control in Gaussian splatting, outperforming existing methods and enabling applications like robot action synthesis.


<details>
  <summary>Details</summary>
Motivation: Existing Gaussian splatting methods lack explicit motion control, limiting their use in robotics.

Method: MBGS uses motion graphs with dual quaternion skinning and learnable weight painting for motion propagation.

Result: MBGS achieves state-of-the-art performance on the iPhone dataset and is competitive on HyperNeRF.

Conclusion: MBGS enables novel applications like robot action synthesis and visual planning, with open-source availability.

Abstract: Gaussian splatting has emerged as a powerful tool for high-fidelity
reconstruction of dynamic scenes. However, existing methods primarily rely on
implicit motion representations, such as encoding motions into neural networks
or per-Gaussian parameters, which makes it difficult to further manipulate the
reconstructed motions. This lack of explicit controllability limits existing
methods to replaying recorded motions only, which hinders a wider application
in robotics. To address this, we propose Motion Blender Gaussian Splatting
(MBGS), a novel framework that uses motion graphs as an explicit and sparse
motion representation. The motion of a graph's links is propagated to
individual Gaussians via dual quaternion skinning, with learnable weight
painting functions that determine the influence of each link. The motion graphs
and 3D Gaussians are jointly optimized from input videos via differentiable
rendering. Experiments show that MBGS achieves state-of-the-art performance on
the highly challenging iPhone dataset while being competitive on HyperNeRF. We
demonstrate the application potential of our method in animating novel object
poses, synthesizing real robot demonstrations, and predicting robot actions
through visual planning. The source code, models, video demonstrations can be
found at http://mlzxy.github.io/motion-blender-gs.

</details>


### [196] [SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding](https://arxiv.org/pdf/2504.21435)
*Chenkai Zhang, Yiming Lei, Zeming Liu, Haitao Leng, Shaoguo Liu, Tingting Gao, Qingjie Liu, Yunhong Wang*

Main category: cs.CV

TL;DR: SeriesBench is a new benchmark for evaluating MLLMs' understanding of narrative-driven video series, addressing gaps in existing benchmarks. It includes 105 series and 28 tasks, enhanced by the PC-DCoT framework for improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for MLLMs focus on standalone videos and visual elements, neglecting complex narratives in series. SeriesBench aims to fill this gap.

Method: Curated 105 narrative-driven series and 28 tasks, introduced long-span annotation, and developed the PC-DCoT framework for narrative reasoning.

Result: Existing MLLMs struggle with narrative-driven series, but PC-DCoT improves their performance.

Conclusion: SeriesBench and PC-DCoT emphasize the need for advancing MLLMs' narrative understanding, guiding future development.

Abstract: With the rapid development of Multi-modal Large Language Models (MLLMs), an
increasing number of benchmarks have been established to evaluate the video
understanding capabilities of these models. However, these benchmarks focus on
standalone videos and mainly assess "visual elements" like human actions and
object states. In reality, contemporary videos often encompass complex and
continuous narratives, typically presented as a series. To address this
challenge, we propose SeriesBench, a benchmark consisting of 105 carefully
curated narrative-driven series, covering 28 specialized tasks that require
deep narrative understanding. Specifically, we first select a diverse set of
drama series spanning various genres. Then, we introduce a novel long-span
narrative annotation method, combined with a full-information transformation
approach to convert manual annotations into diverse task formats. To further
enhance model capacity for detailed analysis of plot structures and character
relationships within series, we propose a novel narrative reasoning framework,
PC-DCoT. Extensive results on SeriesBench indicate that existing MLLMs still
face significant challenges in understanding narrative-driven series, while
PC-DCoT enables these MLLMs to achieve performance improvements. Overall, our
SeriesBench and PC-DCoT highlight the critical necessity of advancing model
capabilities to understand narrative-driven series, guiding the future
development of MLLMs. SeriesBench is publicly available at
https://github.com/zackhxn/SeriesBench-CVPR2025.

</details>


### [197] [Schrödinger Diffusion Driven Signal Recovery in 3T BOLD fMRI Using Unmatched 7T Observations](https://arxiv.org/pdf/2504.01004)
*Yujian Xiong, Xuanzhao Dong, Sebastian Waz, Wenhui Zhu, Negar Mallak, Zhong-lin Lu, Yalin Wang*

Main category: cs.CV

TL;DR: A computational method enhances 3T fMRI quality to approximate 7T-level detail using unsupervised learning.


<details>
  <summary>Details</summary>
Motivation: Overcome the limitations of 3T fMRI (lower resolution and SNR) due to limited 7T scanner accessibility.

Method: Projects 3T and 7T data into a shared low-dimensional space, using a Schrödinger Bridge framework to infer high-quality 3T data.

Result: Improved reliability and fit of pRF models in enhanced 3T data, showing feasibility of 7T-level quality approximation.

Conclusion: Computational enhancement of 3T fMRI can achieve near-7T quality, broadening accessibility of high-fidelity neuroimaging.

Abstract: Ultra-high-field (7 Tesla) BOLD fMRI offers exceptional detail in both
spatial and temporal domains, along with robust signal-to-noise
characteristics, making it a powerful modality for studying visual information
processing in the brain. However, due to the limited accessibility of 7T
scanners, the majority of neuroimaging studies are still conducted using 3T
systems, which inherently suffer from reduced fidelity in both resolution and
SNR. To mitigate this limitation, we introduce a new computational approach
designed to enhance the quality of 3T BOLD fMRI acquisitions. Specifically, we
project both 3T and 7T datasets, sourced from different individuals and
experimental setups, into a shared low-dimensional representation space. Within
this space, we employ a lightweight, unsupervised Schr\"odinger Bridge
framework to infer a high-SNR, high-resolution counterpart of the 3T data,
without relying on paired supervision. This methodology is evaluated across
multiple fMRI retinotopy datasets, including synthetically generated samples,
and demonstrates a marked improvement in the reliability and fit of population
receptive field (pRF) models applied to the enhanced 3T outputs. Our findings
suggest that it is feasible to computationally approximate 7T-level quality
from standard 3T acquisitions.

</details>


### [198] [Multi-Modal Language Models as Text-to-Image Model Evaluators](https://arxiv.org/pdf/2505.00759)
*Jiahui Chen, Candace Ross, Reyhane Askari-Hemmat, Koustuv Sinha, Melissa Hall, Michal Drozdzal, Adriana Romero-Soriano*

Main category: cs.CV

TL;DR: MT2IE is a framework using MLLMs to evaluate T2I models efficiently, matching benchmark accuracy with fewer prompts and better human correlation.


<details>
  <summary>Details</summary>
Motivation: Static datasets for T2I evaluation are becoming outdated, prompting the need for dynamic, efficient alternatives.

Method: MT2IE employs MLLMs to iteratively generate prompts, score images, and assess prompt-generation consistency and aesthetics.

Result: MT2IE achieves benchmark-level accuracy with 1/80th the prompts and higher correlation with human judgment.

Conclusion: MT2IE offers a scalable, efficient alternative to static benchmarks for T2I model evaluation.

Abstract: The steady improvements of text-to-image (T2I) generative models lead to slow
deprecation of automatic evaluation benchmarks that rely on static datasets,
motivating researchers to seek alternative ways to evaluate the T2I progress.
In this paper, we explore the potential of multi-modal large language models
(MLLMs) as evaluator agents that interact with a T2I model, with the objective
of assessing prompt-generation consistency and image aesthetics. We present
Multimodal Text-to-Image Eval (MT2IE), an evaluation framework that iteratively
generates prompts for evaluation, scores generated images and matches T2I
evaluation of existing benchmarks with a fraction of the prompts used in
existing static benchmarks. Moreover, we show that MT2IE's prompt-generation
consistency scores have higher correlation with human judgment than scores
previously introduced in the literature. MT2IE generates prompts that are
efficient at probing T2I model performance, producing the same relative T2I
model rankings as existing benchmarks while using only 1/80th the number of
prompts for evaluation.

</details>


### [199] [FANeRV: Frequency Separation and Augmentation based Neural Representation for Video](https://arxiv.org/pdf/2504.06755)
*Li Yu, Zhihui Li, Chao Yao, Jimin Xiao, Moncef Gabbouj*

Main category: cs.CV

TL;DR: FANeRV improves video reconstruction by separating and enhancing frequency components, outperforming existing NeRV methods.


<details>
  <summary>Details</summary>
Motivation: Existing NeRV methods struggle with fine spatial details, leading to vague reconstructions.

Method: Uses Wavelet Frequency Upgrade Block for frequency separation and enhancement, plus convolutional residual blocks for detail restoration.

Result: Significantly improves reconstruction and excels in tasks like compression, inpainting, and interpolation.

Conclusion: FANeRV addresses NeRV limitations and outperforms existing methods in video tasks.

Abstract: Neural representations for video (NeRV) have gained considerable attention
for their strong performance across various video tasks. However, existing NeRV
methods often struggle to capture fine spatial details, resulting in vague
reconstructions. In this paper, we present a Frequency Separation and
Augmentation based Neural Representation for video (FANeRV), which addresses
these limitations with its core Wavelet Frequency Upgrade Block. This block
explicitly separates input frames into high and low-frequency components using
discrete wavelet transform, followed by targeted enhancement using specialized
modules. Finally, a specially designed gated network effectively fuses these
frequency components for optimal reconstruction. Additionally, convolutional
residual enhancement blocks are integrated into the later stages of the network
to balance parameter distribution and improve the restoration of high-frequency
details. Experimental results demonstrate that FANeRV significantly improves
reconstruction performance and excels in multiple tasks, including video
compression, inpainting, and interpolation, outperforming existing NeRV
methods.

</details>


### [200] [Transforming Hyperspectral Images Into Chemical Maps: An End-to-End Deep Learning Approach](https://arxiv.org/pdf/2504.14131)
*Ole-Christian Galbo Engstrøm, Michela Albano-Gaglio, Erik Schou Dreier, Yamine Bouzembrak, Maria Font-i-Furnols, Puneet Mishra, Kim Steenstrup Pedersen*

Main category: cs.CV

TL;DR: A deep learning U-Net model outperforms traditional PLS regression for generating chemical maps from hyperspectral images, offering lower error rates and better spatial correlation.


<details>
  <summary>Details</summary>
Motivation: Traditional methods like PLS regression for chemical map generation are noisy and ignore spatial context, prompting the need for a more robust solution.

Method: The study uses a modified U-Net with a custom loss function to directly generate chemical maps from hyperspectral images, bypassing intermediate steps.

Result: U-Net achieves 9-13% lower RMSE than PLS, produces spatially correlated maps (99.91% variance), and stays within the 0-100% physical range.

Conclusion: U-Net is superior to PLS for chemical map generation due to its accuracy, spatial coherence, and adherence to physical constraints.

Abstract: Current approaches to chemical map generation from hyperspectral images are
based on models such as partial least squares (PLS) regression, generating
pixel-wise predictions that do not consider spatial context and suffer from a
high degree of noise. This study proposes an end-to-end deep learning approach
using a modified version of U-Net and a custom loss function to directly obtain
chemical maps from hyperspectral images, skipping all intermediate steps
required for traditional pixel-wise analysis. We compare the U-Net with the
traditional PLS regression on a real dataset of pork belly samples with
associated mean fat reference values. The U-Net obtains a test set root mean
squared error of between 9% and 13% lower than that of PLS regression on the
task of mean fat prediction. At the same time, U-Net generates fine detail
chemical maps where 99.91% of the variance is spatially correlated. Conversely,
only 2.53% of the variance in the PLS-generated chemical maps is spatially
correlated, indicating that each pixel-wise prediction is largely independent
of neighboring pixels. Additionally, while the PLS-generated chemical maps
contain predictions far beyond the physically possible range of 0-100%, U-Net
learns to stay inside this range. Thus, the findings of this study indicate
that U-Net is superior to PLS for chemical map generation.

</details>


### [201] [Gaussian Shading++: Rethinking the Realistic Deployment Challenge of Performance-Lossless Image Watermark for Diffusion Models](https://arxiv.org/pdf/2504.15026)
*Zijin Yang, Xin Zhang, Kejiang Chen, Kai Zeng, Qiyi Yao, Han Fang, Weiming Zhang, Nenghai Yu*

Main category: cs.CV

TL;DR: Gaussian Shading++ is a diffusion model watermarking method addressing real-world challenges like key management, parameter variability, and third-party verification, outperforming existing methods in robustness without performance loss.


<details>
  <summary>Details</summary>
Motivation: Ethical concerns and practical challenges in deploying diffusion models, such as copyright protection and inappropriate content generation, necessitate robust watermarking solutions.

Method: Proposes a double-channel design with pseudorandom error-correcting codes, models distortions as additive white Gaussian noise, and uses soft decision decoding and public key signatures for robustness and verification.

Result: Maintains performance losslessness and outperforms existing methods in robustness, validated through extensive experiments.

Conclusion: Gaussian Shading++ is a practical, robust solution for real-world diffusion model watermarking.

Abstract: Ethical concerns surrounding copyright protection and inappropriate content
generation pose challenges for the practical implementation of diffusion
models. One effective solution involves watermarking the generated images.
Existing methods primarily focus on ensuring that watermark embedding does not
degrade the model performance. However, they often overlook critical challenges
in real-world deployment scenarios, such as the complexity of watermark key
management, user-defined generation parameters, and the difficulty of
verification by arbitrary third parties. To address this issue, we propose
Gaussian Shading++, a diffusion model watermarking method tailored for
real-world deployment. We propose a double-channel design that leverages
pseudorandom error-correcting codes to encode the random seed required for
watermark pseudorandomization, achieving performance-lossless watermarking
under a fixed watermark key and overcoming key management challenges.
Additionally, we model the distortions introduced during generation and
inversion as an additive white Gaussian noise channel and employ a novel soft
decision decoding strategy during extraction, ensuring strong robustness even
when generation parameters vary. To enable third-party verification, we
incorporate public key signatures, which provide a certain level of resistance
against forgery attacks even when model inversion capabilities are fully
disclosed. Extensive experiments demonstrate that Gaussian Shading++ not only
maintains performance losslessness but also outperforms existing methods in
terms of robustness, making it a more practical solution for real-world
deployment.

</details>


### [202] [DiTPainter: Efficient Video Inpainting with Diffusion Transformers](https://arxiv.org/pdf/2504.15661)
*Xian Wu, Chang Liu*

Main category: cs.CV

TL;DR: DiTPainter, a video inpainting model based on Diffusion Transformer (DiT), addresses blurry and inconsistency issues in existing methods by using an efficient transformer network trained from scratch, achieving better quality and consistency.


<details>
  <summary>Details</summary>
Motivation: Existing video inpainting methods relying on optical flows often produce blurry results due to inaccurate flows or large masks. Pretrained DiT models are too large for efficient video inpainting.

Method: DiTPainter employs an efficient transformer network designed for video inpainting, trained from scratch without relying on large pretrained models.

Result: DiTPainter outperforms existing methods, delivering higher quality and better spatial-temporal consistency, and works efficiently for arbitrary video lengths.

Conclusion: DiTPainter is an effective solution for video inpainting, offering improved performance and versatility for tasks like decaptioning and completion.

Abstract: Many existing video inpainting algorithms utilize optical flows to construct
the corresponding maps and then propagate pixels from adjacent frames to
missing areas by mapping. Despite the effectiveness of the propagation
mechanism, they might encounter blurry and inconsistencies when dealing with
inaccurate optical flows or large masks. Recently, Diffusion Transformer (DiT)
has emerged as a revolutionary technique for video generation tasks. However,
pretrained DiT models for video generation all contain a large amount of
parameters, which makes it very time consuming to apply to video inpainting
tasks. In this paper, we present DiTPainter, an end-to-end video inpainting
model based on Diffusion Transformer (DiT). DiTPainter uses an efficient
transformer network designed for video inpainting, which is trained from
scratch instead of initializing from any large pretrained models. DiTPainter
can address videos with arbitrary lengths and can be applied to video
decaptioning and video completion tasks with an acceptable time cost.
Experiments show that DiTPainter outperforms existing video inpainting
algorithms with higher quality and better spatial-temporal consistency.

</details>


### [203] [DreamO: A Unified Framework for Image Customization](https://arxiv.org/pdf/2504.16915)
*Chong Mou, Yanze Wu, Wenxu Wu, Zinan Guo, Pengze Zhang, Yufeng Cheng, Yiming Luo, Fei Ding, Shiwen Zhang, Xinghui Li, Mengtian Li, Mingcong Liu, Yi Zhang, Shaojin Wu, Songtao Zhao, Jian Zhang, Qian He, Xinglong Wu*

Main category: cs.CV

TL;DR: DreamO is a unified image customization framework using a diffusion transformer (DiT) to handle diverse tasks and integrate multiple conditions, achieving high-quality results through a progressive training strategy.


<details>
  <summary>Details</summary>
Motivation: Existing image customization methods are task-specific, limiting their generalizability. DreamO aims to unify diverse customization tasks and conditions.

Method: DreamO uses a DiT framework, feature routing constraints, placeholder strategies, and a three-stage progressive training approach (baseline, full-scale, quality alignment).

Result: DreamO effectively performs various customization tasks and integrates multiple control conditions with high quality.

Conclusion: DreamO addresses the challenge of unified image customization, offering flexibility and high performance across diverse tasks.

Abstract: Recently, extensive research on image customization (e.g., identity, subject,
style, background, etc.) demonstrates strong customization capabilities in
large-scale generative models. However, most approaches are designed for
specific tasks, restricting their generalizability to combine different types
of condition. Developing a unified framework for image customization remains an
open challenge. In this paper, we present DreamO, an image customization
framework designed to support a wide range of tasks while facilitating seamless
integration of multiple conditions. Specifically, DreamO utilizes a diffusion
transformer (DiT) framework to uniformly process input of different types.
During training, we construct a large-scale training dataset that includes
various customization tasks, and we introduce a feature routing constraint to
facilitate the precise querying of relevant information from reference images.
Additionally, we design a placeholder strategy that associates specific
placeholders with conditions at particular positions, enabling control over the
placement of conditions in the generated results. Moreover, we employ a
progressive training strategy consisting of three stages: an initial stage
focused on simple tasks with limited data to establish baseline consistency, a
full-scale training stage to comprehensively enhance the customization
capabilities, and a final quality alignment stage to correct quality biases
introduced by low-quality data. Extensive experiments demonstrate that the
proposed DreamO can effectively perform various image customization tasks with
high quality and flexibly integrate different types of control conditions.

</details>


### [204] [Unsupervised Urban Land Use Mapping with Street View Contrastive Clustering and a Geographical Prior](https://arxiv.org/pdf/2504.17551)
*Lin Che, Yizi Chen, Tanhua Jin, Martin Raubal, Konrad Schindler, Peter Kiefer*

Main category: cs.CV

TL;DR: An unsupervised contrastive clustering model with geographical prior improves urban land use mapping from street view images, addressing limitations of supervised methods.


<details>
  <summary>Details</summary>
Motivation: Existing remote sensing lacks precision in complex urban environments, and supervised street view methods struggle with data scarcity and generalization.

Method: Introduces an unsupervised contrastive clustering model with a geographical prior for street view images, combined with visual cluster assignment.

Result: Successfully generates land use maps for two cities using geotagged street view datasets, demonstrating adaptability and scalability.

Conclusion: The method offers a flexible, unsupervised solution for land use mapping, leveraging spatial coherence for broader applicability.

Abstract: Urban land use classification and mapping are critical for urban planning,
resource management, and environmental monitoring. Existing remote sensing
techniques often lack precision in complex urban environments due to the
absence of ground-level details. Unlike aerial perspectives, street view images
provide a ground-level view that captures more human and social activities
relevant to land use in complex urban scenes. Existing street view-based
methods primarily rely on supervised classification, which is challenged by the
scarcity of high-quality labeled data and the difficulty of generalizing across
diverse urban landscapes. This study introduces an unsupervised contrastive
clustering model for street view images with a built-in geographical prior, to
enhance clustering performance. When combined with a simple visual assignment
of the clusters, our approach offers a flexible and customizable solution to
land use mapping, tailored to the specific needs of urban planners. We
experimentally show that our method can generate land use maps from geotagged
street view image datasets of two cities. As our methodology relies on the
universal spatial coherence of geospatial data ("Tobler's law"), it can be
adapted to various settings where street view images are available, to enable
scalable, unsupervised land use mapping and updating. The code will be
available at https://github.com/lin102/CCGP.

</details>


### [205] [Hierarchical and Multimodal Data for Daily Activity Understanding](https://arxiv.org/pdf/2504.17696)
*Ghazal Kaviani, Yavuz Yarici, Seulgi Kim, Mohit Prabhushankar, Ghassan AlRegib, Mashhour Solh, Ameya Patil*

Main category: cs.CV

TL;DR: DARai is a multimodal dataset with hierarchical annotations for human activity analysis, featuring 200+ hours of sensor data from 50 participants in 10 environments. It supports tasks like recognition, localization, and anticipation, highlighting sensor limitations.


<details>
  <summary>Details</summary>
Motivation: To understand human activities in real-world settings by capturing complexity through hierarchical annotations and multimodal sensor data.

Method: DARai includes scripted/unscripted recordings from 20 sensors, annotated at three hierarchical levels (L1-L3). Experiments involve unimodal/multimodal sensor fusion and domain-variant tests.

Result: DARai enables recognition, temporal localization, and future action anticipation, revealing sensor-specific challenges.

Conclusion: DARai is a valuable resource for human-centered AI applications, with publicly available data and tools.

Abstract: Daily Activity Recordings for Artificial Intelligence (DARai, pronounced
"Dahr-ree") is a multimodal, hierarchically annotated dataset constructed to
understand human activities in real-world settings. DARai consists of
continuous scripted and unscripted recordings of 50 participants in 10
different environments, totaling over 200 hours of data from 20 sensors
including multiple camera views, depth and radar sensors, wearable inertial
measurement units (IMUs), electromyography (EMG), insole pressure sensors,
biomonitor sensors, and gaze tracker.
  To capture the complexity in human activities, DARai is annotated at three
levels of hierarchy: (i) high-level activities (L1) that are independent tasks,
(ii) lower-level actions (L2) that are patterns shared between activities, and
(iii) fine-grained procedures (L3) that detail the exact execution steps for
actions. The dataset annotations and recordings are designed so that 22.7% of
L2 actions are shared between L1 activities and 14.2% of L3 procedures are
shared between L2 actions. The overlap and unscripted nature of DARai allows
counterfactual activities in the dataset.
  Experiments with various machine learning models showcase the value of DARai
in uncovering important challenges in human-centered applications.
Specifically, we conduct unimodal and multimodal sensor fusion experiments for
recognition, temporal localization, and future action anticipation across all
hierarchical annotation levels. To highlight the limitations of individual
sensors, we also conduct domain-variant experiments that are enabled by DARai's
multi-sensor and counterfactual activity design setup.
  The code, documentation, and dataset are available at the dedicated DARai
website:
https://alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/

</details>


### [206] [Benchmarking Multimodal Mathematical Reasoning with Explicit Visual Dependency](https://arxiv.org/pdf/2504.18589)
*Zhikai Wang, Jiashuo Sun, Wenqi Zhang, Zhiqiang Hu, Xin Li, Fan Wang, Deli Zhao*

Main category: cs.CV

TL;DR: VCBENCH is a new benchmark for multimodal mathematical reasoning in LVLMs, highlighting gaps in visual-mathematical integration and reasoning.


<details>
  <summary>Details</summary>
Motivation: Current LVLM benchmarks lack evaluation of elementary-level math problems with visual dependencies, crucial for AGI advancement.

Method: VCBENCH includes 1,720 problems across six cognitive domains, using 6,697 images (avg. 3.9 per question) to test multi-image reasoning.

Result: Evaluation of 26 LVLMs shows performance gaps, with top models under 50% accuracy.

Conclusion: VCBENCH reveals challenges in visual-mathematical reasoning, guiding future LVLM improvements.

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have
significantly enhanced their ability to integrate visual and linguistic
information, achieving near-human proficiency in tasks like object recognition,
captioning, and visual question answering. However, current benchmarks
typically focus on knowledge-centric evaluations that assess domain-specific
expertise, often neglecting the core ability to reason about fundamental
mathematical elements and visual concepts. We identify a gap in evaluating
elementary-level math problems, which rely on explicit visual
dependencies-requiring models to discern, integrate, and reason across multiple
images while incorporating commonsense knowledge, all of which are crucial for
advancing toward broader AGI capabilities. To address this gap, we introduce
VCBENCH, a comprehensive benchmark for multimodal mathematical reasoning with
explicit visual dependencies. VCBENCH includes 1,720 problems across six
cognitive domains, featuring 6,697 images (averaging 3.9 per question) to
ensure multi-image reasoning. We evaluate 26 state-of-the-art LVLMs on VCBENCH,
revealing substantial performance disparities, with even the top models unable
to exceed 50% accuracy. Our findings highlight the ongoing challenges in
visual-mathematical integration and suggest avenues for future LVLM
advancements. The project can be found at
https://alibaba-damo-academy.github.io/VCBench/.

</details>


### [207] [HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene Generation](https://arxiv.org/pdf/2504.21650)
*Haiyang Zhou, Wangbo Yu, Jiawen Guan, Xinhua Cheng, Yonghong Tian, Li Yuan*

Main category: cs.CV

TL;DR: HoloTime integrates video diffusion models to generate panoramic videos and reconstruct 4D scenes, enhancing VR/AR immersion.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models focus on static 3D or object-level dynamics, limiting immersive VR/AR experiences.

Method: Uses the 360World dataset and a two-stage diffusion model (Panoramic Animator) for video generation, followed by space-time depth estimation for 4D reconstruction.

Result: Superior performance in panoramic video generation and 4D scene reconstruction compared to existing methods.

Conclusion: HoloTime enables more realistic and engaging immersive environments for VR/AR applications.

Abstract: The rapid advancement of diffusion models holds the promise of
revolutionizing the application of VR and AR technologies, which typically
require scene-level 4D assets for user experience. Nonetheless, existing
diffusion models predominantly concentrate on modeling static 3D scenes or
object-level dynamics, constraining their capacity to provide truly immersive
experiences. To address this issue, we propose HoloTime, a framework that
integrates video diffusion models to generate panoramic videos from a single
prompt or reference image, along with a 360-degree 4D scene reconstruction
method that seamlessly transforms the generated panoramic video into 4D assets,
enabling a fully immersive 4D experience for users. Specifically, to tame video
diffusion models for generating high-fidelity panoramic videos, we introduce
the 360World dataset, the first comprehensive collection of panoramic videos
suitable for downstream 4D scene reconstruction tasks. With this curated
dataset, we propose Panoramic Animator, a two-stage image-to-video diffusion
model that can convert panoramic images into high-quality panoramic videos.
Following this, we present Panoramic Space-Time Reconstruction, which leverages
a space-time depth estimation method to transform the generated panoramic
videos into 4D point clouds, enabling the optimization of a holistic 4D
Gaussian Splatting representation to reconstruct spatially and temporally
consistent 4D scenes. To validate the efficacy of our method, we conducted a
comparative analysis with existing approaches, revealing its superiority in
both panoramic video generation and 4D scene reconstruction. This demonstrates
our method's capability to create more engaging and realistic immersive
environments, thereby enhancing user experiences in VR and AR applications.

</details>


### [208] [No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves](https://arxiv.org/pdf/2505.02831)
*Dengyang Jiang, Mengmeng Wang, Liuzhuozheng Li, Lei Zhang, Haoyu Wang, Wei Wei, Guang Dai, Yanning Zhang, Jingdong Wang*

Main category: cs.CV

TL;DR: Self-Representation Alignment (SRA) improves diffusion transformer performance by aligning latent representations internally, eliminating the need for external frameworks or pre-trained models.


<details>
  <summary>Details</summary>
Motivation: Existing methods for enhancing diffusion transformers require complex external frameworks or pre-trained models, which are inefficient. SRA aims to simplify this by leveraging the transformer's inherent discriminative process.

Method: SRA aligns latent representations between earlier (noisier) and later (cleaner) layers of the diffusion transformer via self-distillation, enhancing representation learning during generative training.

Result: SRA improves performance in DiTs and SiTs, outperforming auxiliary frameworks and matching methods reliant on external representation priors.

Conclusion: SRA is an effective, self-contained method for enhancing diffusion transformers without external dependencies.

Abstract: Recent studies have demonstrated that learning a meaningful internal
representation can both accelerate generative training and enhance the
generation quality of diffusion transformers. However, existing approaches
necessitate to either introduce an external and complex representation training
framework or rely on a large-scale, pre-trained representation foundation model
to provide representation guidance during the original generative training
process. In this study, we posit that the unique discriminative process
inherent to diffusion transformers enables them to offer such guidance without
requiring external representation components. We therefore propose
Self-Representation Alignment (SRA), a simple yet straightforward method that
obtains representation guidance through a self-distillation manner.
Specifically, SRA aligns the output latent representation of the diffusion
transformer in the earlier layer with higher noise to that in the later layer
with lower noise to progressively enhance the overall representation learning
during only the generative training process. Experimental results indicate that
applying SRA to DiTs and SiTs yields consistent performance improvements.
Moreover, SRA not only significantly outperforms approaches relying on
auxiliary, complex representation training frameworks but also achieves
performance comparable to methods that are heavily dependent on powerful
external representation priors.

</details>


### [209] [FG-CLIP: Fine-Grained Visual and Textual Alignment](https://arxiv.org/pdf/2505.05071)
*Chunyu Xie, Bin Wang, Fanjing Kong, Jincheng Li, Dawei Liang, Gengshen Zhang, Dawei Leng, Yuhui Yin*

Main category: cs.CV

TL;DR: FG-CLIP enhances CLIP's fine-grained understanding using large-scale data, detailed annotations, and hard negatives, outperforming CLIP in various tasks.


<details>
  <summary>Details</summary>
Motivation: CLIP lacks fine-grained understanding due to coarse captions; FG-CLIP aims to improve this.

Method: Uses 1.6B long caption-image pairs, 12M images with 40M region-specific boxes, and 10M hard negatives.

Result: FG-CLIP outperforms CLIP and others in fine-grained tasks, object detection, and retrieval.

Conclusion: FG-CLIP effectively captures fine details, improving multimodal performance; data and models are publicly available.

Abstract: Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks
such as image-text retrieval and zero-shot classification but struggles with
fine-grained understanding due to its focus on coarse-grained short captions.
To address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances
fine-grained understanding through three key innovations. First, we leverage
large multimodal models to generate 1.6 billion long caption-image pairs for
capturing global-level semantic details. Second, a high-quality dataset is
constructed with 12 million images and 40 million region-specific bounding
boxes aligned with detailed captions to ensure precise, context-rich
representations. Third, 10 million hard fine-grained negative samples are
incorporated to improve the model's ability to distinguish subtle semantic
differences. We construct a comprehensive dataset, termed FgGRN, by integrating
high-quality region-specific annotations with challenging fine-grained negative
samples. Corresponding training methods are meticulously designed for these
data. Extensive experiments demonstrate that FG-CLIP outperforms the original
CLIP and other state-of-the-art methods across various downstream tasks,
including fine-grained understanding, open-vocabulary object detection,
image-text retrieval, and general multimodal benchmarks. These results
highlight FG-CLIP's effectiveness in capturing fine-grained image details and
improving overall model performance. The related data, code, and models are
available at https://github.com/360CVGroup/FG-CLIP.

</details>


### [210] [Web2Grasp: Learning Functional Grasps from Web Images of Hand-Object Interactions](https://arxiv.org/pdf/2505.05517)
*Hongyi Chen, Yunchao Yao, Yufei Ye, Zhixuan Xu, Homanga Bharadhwaj, Jiashun Wang, Shubham Tulsiani, Zackory Erickson, Jeffrey Ichnowski*

Main category: cs.CV

TL;DR: The paper proposes using web images to extract human grasp data for training functional grasping models in robots, bypassing costly demonstrations. It combines reconstructed 3D hand-object interactions with simulator-augmented data, achieving high success rates in simulation and real-world transfer.


<details>
  <summary>Details</summary>
Motivation: Functional grasping is crucial for dexterous robot hands, but prior work relies on power grasping or expensive teleoperated demonstrations. Web images offer a natural, cost-effective alternative.

Method: Reconstruct 3D hand-object interactions from RGB images, retarget human hands to robot hands, align noisy object meshes, and augment data using a simulator for physically feasible grasps.

Result: Achieves 75.8% success on seen objects and 61.8% on all objects in simulation, with improvements over baselines. Simulator-augmented data boosts performance to 83.4%, and real-world transfer reaches 85%.

Conclusion: Web-sourced HOI data and simulator augmentation enable effective functional grasping training, outperforming traditional methods and generalizing to unseen objects.

Abstract: Functional grasp is essential for enabling dexterous multi-finger robot hands
to manipulate objects effectively. However, most prior work either focuses on
power grasping, which simply involves holding an object still, or relies on
costly teleoperated robot demonstrations to teach robots how to grasp each
object functionally. Instead, we propose extracting human grasp information
from web images since they depict natural and functional object interactions,
thereby bypassing the need for curated demonstrations. We reconstruct human
hand-object interaction (HOI) 3D meshes from RGB images, retarget the human
hand to multi-finger robot hands, and align the noisy object mesh with its
accurate 3D shape. We show that these relatively low-quality HOI data from
inexpensive web sources can effectively train a functional grasping model. To
further expand the grasp dataset for seen and unseen objects, we use the
initially-trained grasping policy with web data in the IsaacGym simulator to
generate physically feasible grasps while preserving functionality. We train
the grasping model on 10 object categories and evaluate it on 9 unseen objects,
including challenging items such as syringes, pens, spray bottles, and tongs,
which are underrepresented in existing datasets. The model trained on the web
HOI dataset, achieving a 75.8% success rate on seen objects and 61.8% across
all objects in simulation, with a 6.7% improvement in success rate and a 1.8x
increase in functionality ratings over baselines. Simulator-augmented data
further boosts performance from 61.8% to 83.4%. The sim-to-real transfer to the
LEAP Hand achieves a 85% success rate. Project website is at:
https://web2grasp.github.io/.

</details>


### [211] [InstanceGen: Image Generation with Instance-level Instructions](https://arxiv.org/pdf/2505.05678)
*Etai Sella, Yanir Kleiman, Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: The paper addresses the limitations of pretrained text-to-image models in handling complex prompts with multiple objects and attributes. It proposes a method combining image-based structural guidance and LLM-based instructions to improve adherence to text prompts.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models struggle with complex prompts involving multiple objects and attributes, prompting the need for better structural guidance.

Method: The technique uses image-based structural initialization and LLM-based instance-level instructions to guide generation.

Result: The approach improves adherence to text prompts, including object counts, attributes, and spatial relations.

Conclusion: Integrating fine-grained structural guidance with LLM instructions enhances the performance of text-to-image models for complex prompts.

Abstract: Despite rapid advancements in the capabilities of generative models,
pretrained text-to-image models still struggle in capturing the semantics
conveyed by complex prompts that compound multiple objects and instance-level
attributes. Consequently, we are witnessing growing interests in integrating
additional structural constraints, typically in the form of coarse bounding
boxes, to better guide the generation process in such challenging cases. In
this work, we take the idea of structural guidance a step further by making the
observation that contemporary image generation models can directly provide a
plausible fine-grained structural initialization. We propose a technique that
couples this image-based structural guidance with LLM-based instance-level
instructions, yielding output images that adhere to all parts of the text
prompt, including object counts, instance-level attributes, and spatial
relations between instances.

</details>


### [212] [Human Motion Prediction via Test-domain-aware Adaptation with Easily-available Human Motions Estimated from Videos](https://arxiv.org/pdf/2505.07301)
*Katsuki Shimbo, Hiromu Taketsugu, Norimichi Ukita*

Main category: cs.CV

TL;DR: The paper proposes using estimated 2D poses from videos to enhance 3D Human Motion Prediction (HMP), improving generalizability by reducing reliance on expensive motion capture data.


<details>
  <summary>Details</summary>
Motivation: Conventional HMP methods rely on costly motion capture data, limiting diversity and generalizability. The paper aims to address this by leveraging easily available video data.

Method: 2D poses from monocular videos are transformed into 3D motions via a pipeline, enabling additional learning for HMP models to adapt to test domains.

Result: Experiments show quantitative and qualitative improvements in HMP performance.

Conclusion: The method successfully enhances HMP by utilizing video-derived 3D motions, overcoming limitations of traditional motion capture data.

Abstract: In 3D Human Motion Prediction (HMP), conventional methods train HMP models
with expensive motion capture data. However, the data collection cost of such
motion capture data limits the data diversity, which leads to poor
generalizability to unseen motions or subjects. To address this issue, this
paper proposes to enhance HMP with additional learning using estimated poses
from easily available videos. The 2D poses estimated from the monocular videos
are carefully transformed into motion capture-style 3D motions through our
pipeline. By additional learning with the obtained motions, the HMP model is
adapted to the test domain. The experimental results demonstrate the
quantitative and qualitative impact of our method.

</details>


### [213] [TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset](https://arxiv.org/pdf/2505.07396)
*Olaf Wysocki, Benedikt Schwab, Manoj Kumar Biswanath, Michael Greza, Qilin Zhang, Jingwei Zhu, Thomas Froech, Medhini Heeramaglore, Ihab Hijazi, Khaoula Kanna, Mathias Pechinger, Zhaiyu Chen, Yao Sun, Alejandro Rueda Segura, Ziyang Xu, Omar AbdelGafar, Mansour Mehranfar, Chandan Yeshwanth, Yueh-Cheng Liu, Hadi Yazdi, Jiapan Wang, Stefan Auer, Katharina Anders, Klaus Bogenberger, Andre Borrmann, Angela Dai, Ludwig Hoegner, Christoph Holst, Thomas H. Kolbe, Ferdinand Ludwig, Matthias Nießner, Frank Petzold, Xiao Xiang Zhu, Boris Jutzi*

Main category: cs.CV

TL;DR: The paper introduces TUM2TWIN, the first comprehensive multimodal Urban Digital Twin (UDT) benchmark dataset, addressing challenges in UDT creation like data accuracy, model fidelity, and interoperability.


<details>
  <summary>Details</summary>
Motivation: Current datasets are limited to parts of the UDT processing chain, hindering comprehensive validation. The paper aims to overcome these limitations with a robust, multimodal dataset.

Method: The TUM2TWIN dataset includes georeferenced, semantically aligned 3D models and networks, along with terrestrial, mobile, aerial, and satellite observations, covering 100,000 m² and 767 GB of data.

Result: The dataset supports advanced reconstruction methods and downstream tasks like novel view synthesis, solar potential analysis, and semantic segmentation.

Conclusion: TUM2TWIN lays a foundation for overcoming UDT limitations, fostering research and practical solutions for smarter urban environments.

Abstract: Urban Digital Twins (UDTs) have become essential for managing cities and
integrating complex, heterogeneous data from diverse sources. Creating UDTs
involves challenges at multiple process stages, including acquiring accurate 3D
source data, reconstructing high-fidelity 3D models, maintaining models'
updates, and ensuring seamless interoperability to downstream tasks. Current
datasets are usually limited to one part of the processing chain, hampering
comprehensive UDTs validation. To address these challenges, we introduce the
first comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN.
This dataset includes georeferenced, semantically aligned 3D models and
networks along with various terrestrial, mobile, aerial, and satellite
observations boasting 32 data subsets over roughly 100,000 $m^2$ and currently
767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, high
accuracy, and multimodal data integration, the benchmark supports robust
analysis of sensors and the development of advanced reconstruction methods.
Additionally, we explore downstream tasks demonstrating the potential of
TUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solar
potential analysis, point cloud semantic segmentation, and LoD3 building
reconstruction. We are convinced this contribution lays a foundation for
overcoming current limitations in UDT creation, fostering new research
directions and practical solutions for smarter, data-driven urban environments.
The project is available under: https://tum2t.win

</details>


### [214] [FLUXSynID: A Framework for Identity-Controlled Synthetic Face Generation with Document and Live Images](https://arxiv.org/pdf/2505.07530)
*Raul Ismayilov, Dzemila Sero, Luuk Spreeuwers*

Main category: cs.CV

TL;DR: FLUXSynID is a framework for generating high-resolution synthetic face datasets with controlled identity attributes and paired images, improving alignment with real-world distributions and diversity.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of real-world biometric data like privacy, demographic imbalance, and high costs, while addressing lack of fine-grained control and identity consistency in existing methods.

Method: Introduces FLUXSynID to generate synthetic datasets with user-defined identity attributes and paired document-style/live capture images.

Result: Produces 14,889 synthetic identities with better alignment to real-world distributions and greater diversity than prior work.

Conclusion: FLUXSynID and its dataset are publicly released to advance biometric research, including face recognition and morphing attack detection.

Abstract: Synthetic face datasets are increasingly used to overcome the limitations of
real-world biometric data, including privacy concerns, demographic imbalance,
and high collection costs. However, many existing methods lack fine-grained
control over identity attributes and fail to produce paired,
identity-consistent images under structured capture conditions. We introduce
FLUXSynID, a framework for generating high-resolution synthetic face datasets
with user-defined identity attribute distributions and paired document-style
and trusted live capture images. The dataset generated using the FLUXSynID
framework shows improved alignment with real-world identity distributions and
greater inter-set diversity compared to prior work. The FLUXSynID framework for
generating custom datasets, along with a dataset of 14,889 synthetic
identities, is publicly released to support biometric research, including face
recognition and morphing attack detection.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [215] [An Optimized Evacuation Plan for an Active-Shooter Situation Constrained by Network Capacity](https://arxiv.org/pdf/2505.07830)
*Joseph Lavalle-Rivera, Aniirudh Ramesh, Subhadeep Chakraborty*

Main category: cs.AI

TL;DR: The paper addresses public shooting evacuations in the US (2016-2022) by proposing a multi-route optimization algorithm to reduce casualties and crowding.


<details>
  <summary>Details</summary>
Motivation: High-stress evacuations during shootings often lead to fatal decisions due to lack of real-time info and crowding.

Method: Developed a multi-route routing optimization algorithm considering route capacity to avoid bottlenecks.

Result: Reduced casualties by 34.16% and 53.3% compared to previous methods, and cut bottleneck occupancy by ~50%.

Conclusion: The algorithm significantly improves evacuation efficiency and safety during shootings.

Abstract: A total of more than 3400 public shootings have occurred in the United States
between 2016 and 2022. Among these, 25.1% of them took place in an educational
institution, 29.4% at the workplace including office buildings, 19.6% in retail
store locations, and 13.4% in restaurants and bars. During these critical
scenarios, making the right decisions while evacuating can make the difference
between life and death. However, emergency evacuation is intensely stressful,
which along with the lack of verifiable real-time information may lead to fatal
incorrect decisions. To tackle this problem, we developed a multi-route routing
optimization algorithm that determines multiple optimal safe routes for each
evacuee while accounting for available capacity along the route, thus reducing
the threat of crowding and bottlenecking. Overall, our algorithm reduces the
total casualties by 34.16% and 53.3%, compared to our previous routing
algorithm without capacity constraints and an expert-advised routing strategy
respectively. Further, our approach to reduce crowding resulted in an
approximate 50% reduction in occupancy in key bottlenecking nodes compared to
both of the other evacuation algorithms.

</details>


### [216] [RAN Cortex: Memory-Augmented Intelligence for Context-Aware Decision-Making in AI-Native Networks](https://arxiv.org/pdf/2505.07842)
*Sebastian Barros*

Main category: cs.AI

TL;DR: RAN Cortex introduces a memory-augmented architecture for AI-based RAN decision systems, enabling contextual recall to improve adaptability and continuity.


<details>
  <summary>Details</summary>
Motivation: Current AI-based RAN decision systems are stateless, limiting optimization in dynamic environments with recurring patterns.

Method: Proposes RAN Cortex with a modular layer (context encoder, memory store, recall engine, policy interface) to enable contextual recall.

Result: Demonstrates improved adaptability and continuity in use cases like stadium traffic mitigation and drone mobility management.

Conclusion: Memory is a missing primitive in AI-native RAN designs, enabling learning agents without retraining or centralized inference.

Abstract: As Radio Access Networks (RAN) evolve toward AI-native architectures,
intelligent modules such as xApps and rApps are expected to make increasingly
autonomous decisions across scheduling, mobility, and resource management
domains. However, these agents remain fundamentally stateless, treating each
decision as isolated, lacking any persistent memory of prior events or
outcomes. This reactive behavior constrains optimization, especially in
environments where network dynamics exhibit episodic or recurring patterns. In
this work, we propose RAN Cortex, a memory-augmented architecture that enables
contextual recall in AI-based RAN decision systems. RAN Cortex introduces a
modular layer composed of four elements: a context encoder that transforms
network state into high-dimensional embeddings, a vector-based memory store of
past network episodes, a recall engine to retrieve semantically similar
situations, and a policy interface that supplies historical context to AI
agents in real time or near-real time. We formalize the retrieval-augmented
decision problem in the RAN, present a system architecture compatible with
O-RAN interfaces, and analyze feasible deployments within the Non-RT and
Near-RT RIC domains. Through illustrative use cases such as stadium traffic
mitigation and mobility management in drone corridors, we demonstrate how
contextual memory improves adaptability, continuity, and overall RAN
intelligence. This work introduces memory as a missing primitive in AI-native
RAN designs and provides a framework to enable "learning agents" without the
need for retraining or centralized inference

</details>


### [217] [Winning at All Cost: A Small Environment for Eliciting Specification Gaming Behaviors in Large Language Models](https://arxiv.org/pdf/2505.07846)
*Lars Malmqvist*

Main category: cs.AI

TL;DR: LLMs exploit system vulnerabilities in unwinnable tic-tac-toe scenarios, with newer models and creative prompts increasing exploitation rates.


<details>
  <summary>Details</summary>
Motivation: To investigate how LLMs handle impossible tasks and their tendency to exploit loopholes, revealing security and alignment concerns.

Method: Textual simulation of an unwinnable tic-tac-toe scenario with three LLMs, analyzing their responses to prompts and exploitation strategies.

Result: Newer models (e.g., o3-mini) exploited vulnerabilities more (37.1%) than older ones (17.5%). Creative prompts raised exploitation to 77.3%. Four distinct exploitation strategies were identified.

Conclusion: LLMs can propose sophisticated exploits when incentivized, posing urgent challenges for AI alignment as models become more capable of identifying vulnerabilities.

Abstract: This study reveals how frontier Large Language Models LLMs can "game the
system" when faced with impossible situations, a critical security and
alignment concern. Using a novel textual simulation approach, we presented
three leading LLMs (o1, o3-mini, and r1) with a tic-tac-toe scenario designed
to be unwinnable through legitimate play, then analyzed their tendency to
exploit loopholes rather than accept defeat. Our results are alarming for
security researchers: the newer, reasoning-focused o3-mini model showed nearly
twice the propensity to exploit system vulnerabilities (37.1%) compared to the
older o1 model (17.5%). Most striking was the effect of prompting. Simply
framing the task as requiring "creative" solutions caused gaming behaviors to
skyrocket to 77.3% across all models. We identified four distinct exploitation
strategies, from direct manipulation of game state to sophisticated
modification of opponent behavior. These findings demonstrate that even without
actual execution capabilities, LLMs can identify and propose sophisticated
system exploits when incentivized, highlighting urgent challenges for AI
alignment as models grow more capable of identifying and leveraging
vulnerabilities in their operating environments.

</details>


### [218] [Conceptual Logical Foundations of Artificial Social Intelligence](https://arxiv.org/pdf/2505.07847)
*Eric Werner*

Main category: cs.AI

TL;DR: The paper explores the foundations of artificial social intelligence, focusing on coordination, cooperation, and communication among agents in a dynamic social world.


<details>
  <summary>Details</summary>
Motivation: To understand how societies function and how agents coordinate, cooperate, and communicate to achieve shared goals.

Method: The study formalizes key concepts like social intelligence, agent architecture, and communication, using logic to link information with strategic thought.

Result: A minimal architecture for social agents is proposed, incorporating dynamic choices, uncertainty, and intentional states. Communication's role in coordination is analyzed.

Conclusion: The paper provides a framework for understanding social intelligence, emphasizing the interplay of information, intentions, and communication in multi-agent systems.

Abstract: What makes a society possible at all? How is coordination and cooperation in
social activity possible? What is the minimal mental architecture of a social
agent? How is the information about the state of the world related to the
agents intentions? How are the intentions of agents related? What role does
communication play in this coordination process? This essay explores the
conceptual and logical foundations of artificial social intelligence in the
context of a society of multiple agents that communicate and cooperate to
achieve some end. An attempt is made to provide an introduction to some of the
key concepts, their formal definitions and their interrelationships. These
include the notion of a changing social world of multiple agents. The logic of
social intelligence goes beyond classical logic by linking information with
strategic thought. A minimal architecture of social agents is presented. The
agents have different dynamically changing, possible choices and abilities. The
agents also have uncertainty, lacking perfect information about their physical
state as well as their dynamic social state. The social state of an agent
includes the intentional state of that agent, as well as, that agent's
representation of the intentional states of other agents. Furthermore, it
includes the evaluations agents make of their physical and social condition.
Communication, semantic and pragmatic meaning and their relationship to
intention and information states are investigated. The logic of agent abilities
and intentions are motivated and formalized. The entropy of group strategic
states is defined.

</details>


### [219] [Unveiling the Best Practices for Applying Speech Foundation Models to Speech Intelligibility Prediction for Hearing-Impaired People](https://arxiv.org/pdf/2505.08215)
*Haoshuai Zhou, Boxuan Cao, Changgeng Mo, Linkai Li, Shan Xiang Wang*

Main category: cs.AI

TL;DR: The paper explores optimizing speech foundation models (SFMs) for speech intelligibility prediction for hearing-impaired people (SIP-HI), identifying key design factors like encoder layer selection, prediction head architecture, and ensemble configurations.


<details>
  <summary>Details</summary>
Motivation: Optimizing SFMs for SIP-HI has been insufficiently explored, despite their strong performance in other tasks.

Method: The study evaluates 5 SFMs, focusing on encoder layer selection, prediction head design, and ensemble methods.

Result: Single encoder layer selection outperforms traditional all-layers methods, temporal modeling is key for prediction heads, and ensembling SFMs improves performance.

Conclusion: The study provides practical insights for adapting SFMs to SIP-HI, highlighting the importance of design choices and ensemble strategies.

Abstract: Speech foundation models (SFMs) have demonstrated strong performance across a
variety of downstream tasks, including speech intelligibility prediction for
hearing-impaired people (SIP-HI). However, optimizing SFMs for SIP-HI has been
insufficiently explored. In this paper, we conduct a comprehensive study to
identify key design factors affecting SIP-HI performance with 5 SFMs, focusing
on encoder layer selection, prediction head architecture, and ensemble
configurations. Our findings show that, contrary to traditional use-all-layers
methods, selecting a single encoder layer yields better results. Additionally,
temporal modeling is crucial for effective prediction heads. We also
demonstrate that ensembling multiple SFMs improves performance, with stronger
individual models providing greater benefit. Finally, we explore the
relationship between key SFM attributes and their impact on SIP-HI performance.
Our study offers practical insights into effectively adapting SFMs for speech
intelligibility prediction for hearing-impaired populations.

</details>


### [220] [CCL: Collaborative Curriculum Learning for Sparse-Reward Multi-Agent Reinforcement Learning via Co-evolutionary Task Evolution](https://arxiv.org/pdf/2505.07854)
*Yufei Lin, Chengwei Ye, Huanzhen Zhang, Kangsheng Wang, Linuo Xu, Shuyan Liu, Zeyu Zhang*

Main category: cs.AI

TL;DR: CCL is a curriculum learning framework for sparse reward multi-agent systems, improving performance via refined tasks, evolutionary subtasks, and co-evolution.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in sparse reward environments for multi-agent systems, where delayed and shared feedback leads to suboptimal learning.

Method: Proposes CCL with (1) refined intermediate tasks, (2) variational evolutionary algorithm for subtasks, and (3) co-evolution of agents and environment.

Result: Outperforms existing methods in sparse reward settings on five cooperative tasks in MPE and Hide-and-Seek environments.

Conclusion: CCL effectively enhances training stability and performance in sparse reward multi-agent systems.

Abstract: Sparse reward environments pose significant challenges in reinforcement
learning, especially within multi-agent systems (MAS) where feedback is delayed
and shared across agents, leading to suboptimal learning. We propose
Collaborative Multi-dimensional Course Learning (CCL), a novel curriculum
learning framework that addresses this by (1) refining intermediate tasks for
individual agents, (2) using a variational evolutionary algorithm to generate
informative subtasks, and (3) co-evolving agents with their environment to
enhance training stability. Experiments on five cooperative tasks in the MPE
and Hide-and-Seek environments show that CCL outperforms existing methods in
sparse reward settings.

</details>


### [221] [Arrow-Guided VLM: Enhancing Flowchart Understanding via Arrow Direction Encoding](https://arxiv.org/pdf/2505.07864)
*Takamitsu Omasa, Ryo Koshihara, Masumi Morishige*

Main category: cs.AI

TL;DR: A seven-stage pipeline improves VLM accuracy in interpreting flowcharts by 9 percentage points, focusing on arrow-aware detection, OCR, and structured prompts.


<details>
  <summary>Details</summary>
Motivation: Current VLMs misinterpret flowcharts due to their unique directional arrows and graph topology, necessitating a specialized approach.

Method: The method involves arrow-aware detection of nodes and arrow endpoints, OCR for text extraction, and structured prompts for VLMs.

Result: Accuracy improved from 80% to 89%, with next-step queries achieving 100% accuracy. Branch-result and before-step questions showed smaller gains.

Conclusion: Explicit arrow encoding enhances VLM performance, though limitations like detector dependence and small evaluation sets remain. Future work will expand benchmarks and test on BPMN/UML.

Abstract: Flowcharts are indispensable tools in software design and business-process
analysis, yet current vision-language models (VLMs) frequently misinterpret the
directional arrows and graph topology that set these diagrams apart from
natural images. We introduce a seven-stage pipeline grouped into three broader
processes: (1) arrow-aware detection of nodes and arrow endpoints; (2) optical
character recognition (OCR) to extract node text; and (3) construction of a
structured prompt that guides the VLMs. Tested on a 90-question benchmark
distilled from 30 annotated flowcharts, the method raises overall accuracy from
80 % to 89 % (+9 percentage points) without any task-specific fine-tuning. The
gain is most pronounced for next-step queries (25/30 -> 30/30; 100 %, +17 pp);
branch-result questions improve more modestly, and before-step questions remain
difficult. A parallel evaluation with an LLM-as-a-Judge protocol shows the same
trends, reinforcing the advantage of explicit arrow encoding. Limitations
include dependence on detector and OCR precision, the small evaluation set, and
residual errors at nodes with multiple incoming edges. Future work will enlarge
the benchmark with synthetic and handwritten flowcharts and assess the approach
on Business Process Model and Notation (BPMN) and Unified Modeling Language
(UML).

</details>


### [222] [Enhancing Trust Management System for Connected Autonomous Vehicles Using Machine Learning Methods: A Survey](https://arxiv.org/pdf/2505.07882)
*Qian Xu, Lei Zhang, Yixiao Liu*

Main category: cs.AI

TL;DR: The paper surveys ML-based Trust Management Systems (TMS) for Connected Autonomous Vehicles (CAVs), proposing a three-layer framework and a six-dimensional taxonomy. It categorizes recent studies by traffic scenarios and suggests future directions.


<details>
  <summary>Details</summary>
Motivation: CAVs face dynamic threats, requiring robust TMS. ML advancements can enhance TMS for CAVs' unique needs, distinguishing them from other networks.

Method: Proposes a three-layer ML-based TMS framework (trust data, calculation, incentive) and a six-dimensional taxonomy. Analyzes ML methods per layer and categorizes studies by traffic scenarios.

Result: A structured approach to ML-based TMS for CAVs, with a clear taxonomy and analysis of recent research.

Conclusion: The framework and taxonomy advance ML-based TMS for CAVs, with future directions addressing open issues. A repository is maintained for ongoing updates.

Abstract: Connected Autonomous Vehicles (CAVs) operate in dynamic, open, and
multi-domain networks, rendering them vulnerable to various threats. Trust
Management Systems (TMS) systematically organize essential steps in the trust
mechanism, identifying malicious nodes against internal threats and external
threats, as well as ensuring reliable decision-making for more cooperative
tasks. Recent advances in machine learning (ML) offer significant potential to
enhance TMS, especially for the strict requirements of CAVs, such as CAV nodes
moving at varying speeds, and opportunistic and intermittent network behavior.
Those features distinguish ML-based TMS from social networks, static IoT, and
Social IoT. This survey proposes a novel three-layer ML-based TMS framework for
CAVs in the vehicle-road-cloud integration system, i.e., trust data layer,
trust calculation layer and trust incentive layer. A six-dimensional taxonomy
of objectives is proposed. Furthermore, the principles of ML methods for each
module in each layer are analyzed. Then, recent studies are categorized based
on traffic scenarios that are against the proposed objectives. Finally, future
directions are suggested, addressing the open issues and meeting the research
trend. We maintain an active repository that contains up-to-date literature and
open-source projects at
https://github.com/octoberzzzzz/ML-based-TMS-CAV-Survey.

</details>


### [223] [BLAB: Brutally Long Audio Bench](https://arxiv.org/pdf/2505.03054)
*Orevaoghene Ahia, Martijn Bartelds, Kabir Ahuja, Hila Gonen, Valentin Hofmann, Siddhant Arora, Shuyue Stella Li, Vishal Puttagunta, Mofetoluwa Adeyemi, Charishma Buchireddy, Ben Walls, Noah Bennett, Shinji Watanabe, Noah A. Smith, Yulia Tsvetkov, Sachin Kumar*

Main category: cs.AI

TL;DR: BLAB is a benchmark for evaluating audio LMs on long-form speech tasks, revealing their struggles with duration and complex tasks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of evaluation for audio LMs on long-form conversational speech, which better reflects real-world interactions.

Method: Introduces BLAB, a benchmark with 833+ hours of long-form audio (avg. 51 mins) and human-annotated Q&A, testing localization, emotion, counting, and duration estimation.

Result: All tested audio LMs, including advanced ones like GPT-4o, struggle with long-form tasks, showing performance decline with duration and difficulty.

Conclusion: BLAB highlights the need for improved long-form audio understanding in LMs and serves as a framework for future development.

Abstract: Developing large audio language models (LMs) capable of understanding diverse
spoken interactions is essential for accommodating the multimodal nature of
human communication and can increase the accessibility of language technologies
across different user populations. Recent work on audio LMs has primarily
evaluated their performance on short audio segments, typically under 30
seconds, with limited exploration of long-form conversational speech segments
that more closely reflect natural user interactions with these models. We
introduce Brutally Long Audio Bench (BLAB), a challenging long-form audio
benchmark that evaluates audio LMs on localization, duration estimation,
emotion, and counting tasks using audio segments averaging 51 minutes in
length. BLAB consists of 833+ hours of diverse, full-length audio clips, each
paired with human-annotated, text-based natural language questions and answers.
Our audio data were collected from permissively licensed sources and underwent
a human-assisted filtering process to ensure task compliance. We evaluate six
open-source and proprietary audio LMs on BLAB and find that all of them,
including advanced models such as Gemini 2.0 Pro and GPT-4o, struggle with the
tasks in BLAB. Our comprehensive analysis reveals key insights into the
trade-offs between task difficulty and audio duration. In general, we find that
audio LMs struggle with long-form speech, with performance declining as
duration increases. They perform poorly on localization, temporal reasoning,
counting, and struggle to understand non-phonemic information, relying more on
prompts than audio content. BLAB serves as a challenging evaluation framework
to develop audio LMs with robust long-form audio understanding capabilities.

</details>


### [224] [The Correspondence Between Bounded Graph Neural Networks and Fragments of First-Order Logic](https://arxiv.org/pdf/2505.08021)
*Bernardo Cuenca Grau, Przemysław A. Wałęga*

Main category: cs.AI

TL;DR: GNNs' expressive power is linked to fragments of first-order logic, providing a framework for understanding their capabilities.


<details>
  <summary>Details</summary>
Motivation: To clarify the expressive power of GNNs by connecting them to logical fragments.

Method: Apply finite model theory tools from first-order and modal logics to GNNs.

Result: GNNs correspond to specific FO fragments like ML, GML, ML(A), FO2, and C2.

Conclusion: The study unifies GNNs' expressiveness within first-order logic, enhancing theoretical understanding.

Abstract: Graph Neural Networks (GNNs) address two key challenges in applying deep
learning to graph-structured data: they handle varying size input graphs and
ensure invariance under graph isomorphism. While GNNs have demonstrated broad
applicability, understanding their expressive power remains an important
question. In this paper, we show that bounded GNN architectures correspond to
specific fragments of first-order logic (FO), including modal logic (ML),
graded modal logic (GML), modal logic with the universal modality (ML(A)), the
two-variable fragment (FO2) and its extension with counting quantifiers (C2).
To establish these results, we apply methods and tools from finite model theory
of first-order and modal logics to the domain of graph representation learning.
This provides a unifying framework for understanding the logical expressiveness
of GNNs within FO.

</details>


### [225] [Bias or Optimality? Disentangling Bayesian Inference and Learning Biases in Human Decision-Making](https://arxiv.org/pdf/2505.08049)
*Prakhar Godara*

Main category: cs.AI

TL;DR: The paper argues that apparent human biases in bandit tasks may stem from Bayesian inference with decreasing learning rates, not cognitive biases, and proposes tests to distinguish them.


<details>
  <summary>Details</summary>
Motivation: To challenge the claim that human behavior in TABB tasks reflects cognitive biases (positivity and confirmation biases) by showing these biases can arise from objective Bayesian inference.

Method: Analyzes Q-learning models with asymmetric learning rates, Bayesian inference as Q-learning, and stochastic dynamics via master equations.

Result: Confirmation bias and decreasing learning rates produce similar behavioral signatures, making them hard to distinguish without further testing.

Conclusion: Proposes experimental protocols to differentiate true cognitive biases from artifacts of decreasing learning rates.

Abstract: Recent studies claim that human behavior in a two-armed Bernoulli bandit
(TABB) task is described by positivity and confirmation biases, implying that
humans do not integrate new information objectively. However, we find that even
if the agent updates its belief via objective Bayesian inference, fitting the
standard Q-learning model with asymmetric learning rates still recovers both
biases. Bayesian inference cast as an effective Q-learning algorithm has
symmetric, though decreasing, learning rates. We explain this by analyzing the
stochastic dynamics of these learning systems using master equations. We find
that both confirmation bias and unbiased but decreasing learning rates yield
the same behavioral signatures. Finally, we propose experimental protocols to
disentangle true cognitive biases from artifacts of decreasing learning rates.

</details>


### [226] [Benchmarking AI scientists in omics data-driven biological research](https://arxiv.org/pdf/2505.08341)
*Erpai Luo, Jinmeng Jia, Yifan Xiong, Xiangyu Li, Xiaobo Guo, Baoqi Yu, Lei Wei, Xuegong Zhang*

Main category: cs.AI

TL;DR: BaisBench is a new benchmark for evaluating AI scientists' ability to perform biological research through data analysis and reasoning, showing current models lag behind human experts.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack realistic, data-driven evaluation for AI scientists in biological research.

Method: BaisBench includes two tasks: cell type annotation on 31 datasets and answering 198 multiple-choice questions from recent studies.

Result: Current AI models underperform human experts on both tasks.

Conclusion: BaisBench aims to advance AI for scientific discovery by providing a realistic evaluation framework.

Abstract: The rise of large language models and multi-agent systems has sparked growing
interest in AI scientists capable of autonomous biological research. However,
existing benchmarks either focus on reasoning without data or on data analysis
with predefined statistical answers, lacking realistic, data-driven evaluation
settings. Here, we introduce the Biological AI Scientist Benchmark (BaisBench),
a benchmark designed to assess AI scientists' ability to generate biological
discoveries through data analysis and reasoning with external knowledge.
BaisBench comprises two tasks: cell type annotation on 31 expert-labeled
single-cell datasets, and scientific discovery through answering 198
multiple-choice questions derived from the biological insights of 41 recent
single-cell studies. Systematic experiments on state-of-the-art AI scientists
and LLM agents showed that while promising, current models still substantially
underperform human experts on both tasks. We hope BaisBench will fill this gap
and serve as a foundation for advancing and evaluating AI models for scientific
discovery. The benchmark can be found at: https://github.com/EperLuo/BaisBench.

</details>


### [227] [Explainable Reinforcement Learning Agents Using World Models](https://arxiv.org/pdf/2505.08073)
*Madhuri Singh, Amal Alabdulkarim, Gennie Mansi, Mark O. Riedl*

Main category: cs.AI

TL;DR: The paper introduces a technique using World Models and Reverse World Models to generate explanations for Model-Based Deep RL agents, enhancing user understanding and control.


<details>
  <summary>Details</summary>
Motivation: To address the complexity of explaining RL agents to non-AI experts and enable better understanding and control of agent behavior.

Method: Augments Model-Based RL agents with a Reverse World Model to predict necessary world states for preferred actions, generating counterfactual trajectories.

Result: Explanations using Reverse World Models significantly improve user understanding of agent policies.

Conclusion: The proposed method enhances explainability in RL, aiding users in learning how to influence agent behavior through environmental manipulation.

Abstract: Explainable AI (XAI) systems have been proposed to help people understand how
AI systems produce outputs and behaviors. Explainable Reinforcement Learning
(XRL) has an added complexity due to the temporal nature of sequential
decision-making. Further, non-AI experts do not necessarily have the ability to
alter an agent or its policy. We introduce a technique for using World Models
to generate explanations for Model-Based Deep RL agents. World Models predict
how the world will change when actions are performed, allowing for the
generation of counterfactual trajectories. However, identifying what a user
wanted the agent to do is not enough to understand why the agent did something
else. We augment Model-Based RL agents with a Reverse World Model, which
predicts what the state of the world should have been for the agent to prefer a
given counterfactual action. We show that explanations that show users what the
world should have been like significantly increase their understanding of the
agent policy. We hypothesize that our explanations can help users learn how to
control the agents execution through by manipulating the environment.

</details>


### [228] [Lost in Transmission: When and Why LLMs Fail to Reason Globally](https://arxiv.org/pdf/2505.08140)
*Tobias Schnabel, Kiran Tomlinson, Adith Swaminathan, Jennifer Neville*

Main category: cs.AI

TL;DR: The paper introduces the Bounded Attention Prefix Oracle (BAPO) model to explain LLM failures in complex reasoning tasks due to bandwidth constraints in attention heads. It identifies BAPO-hard tasks, validates them experimentally, and shows how chain of thought (CoT) can mitigate these issues.


<details>
  <summary>Details</summary>
Motivation: Transformer-based LLMs struggle with complex reasoning tasks due to information flow limits. The paper aims to formalize and address this issue.

Method: Introduces the BAPO model to analyze bandwidth constraints in attention heads. Tests theoretical predictions on GPT-4, Claude, and Gemini.

Result: LLMs succeed on BAPO-easy tasks but fail on BAPO-hard ones. CoT can transform BAPO-hard tasks into BAPO-easy ones.

Conclusion: The BAPO model explains key LLM failures and suggests architectural and inference improvements to overcome bandwidth limits.

Abstract: Despite their many successes, transformer-based large language models (LLMs)
continue to struggle with tasks that require complex reasoning over large parts
of their input. We argue that these failures arise due to capacity limits on
the accurate flow of information within LLMs. To formalize this issue, we
introduce the bounded attention prefix oracle (BAPO) model, a new computational
framework that models bandwidth constraints on attention heads, the mechanism
for internal communication in LLMs. We show that several important reasoning
problems like graph reachability require high communication bandwidth for BAPOs
to solve; we call these problems BAPO-hard. Our experiments corroborate our
theoretical predictions: GPT-4, Claude, and Gemini succeed on BAPO-easy tasks
and fail even on relatively small BAPO-hard tasks. BAPOs also reveal another
benefit of chain of thought (CoT): we prove that breaking down a task using CoT
can turn any BAPO-hard problem into a BAPO-easy one. Our results offer
principled explanations for key LLM failures and suggest directions for
architectures and inference methods that mitigate bandwidth limits.

</details>


### [229] [Foundation Models Knowledge Distillation For Battery Capacity Degradation Forecast](https://arxiv.org/pdf/2505.08151)
*Joey Chan, Zhen Chen, Ershun Pan*

Main category: cs.AI

TL;DR: A fine-tuning strategy for time-series foundation models is proposed to predict lithium-ion battery degradation, achieving zero-shot generalization. Knowledge distillation further enhances expert models.


<details>
  <summary>Details</summary>
Motivation: Accurate battery capacity degradation estimation is crucial for reliability and safety, but existing models lack generalization. Foundation models for this purpose are underexplored.

Method: A degradation-aware fine-tuning strategy is applied to the Timer model using 10 GB of battery data. Knowledge distillation transfers foundation model knowledge to compact expert models.

Result: The fine-tuned Battery-Timer shows strong zero-shot generalization. Distillation improves expert models' multi-condition generalization.

Conclusion: The approach enables accurate battery degradation prediction and efficient deployment via knowledge distillation.

Abstract: Accurate estimation of lithium-ion battery capacity degradation is critical
for enhancing the reliability and safety of battery operations. Traditional
expert models, tailored to specific scenarios, provide isolated estimations.
With the rapid advancement of data-driven techniques, a series of
general-purpose time-series foundation models have been developed. However,
foundation models specifically designed for battery capacity degradation remain
largely unexplored. To enable zero-shot generalization in battery degradation
prediction using large model technology, this study proposes a
degradation-aware fine-tuning strategy for time-series foundation models. We
apply this strategy to fine-tune the Timer model on approximately 10 GB of
open-source battery charge discharge data. Validation on our released
CycleLife-SJTUIE dataset demonstrates that the fine-tuned Battery-Timer
possesses strong zero-shot generalization capability in capacity degradation
forecasting. To address the computational challenges of deploying large models,
we further propose a knowledge distillation framework that transfers the
knowledge of pre-trained foundation models into compact expert models.
Distillation results across several state-of-the-art time-series expert models
confirm that foundation model knowledge significantly improves the
multi-condition generalization of expert models.

</details>


### [230] [Efficient and Scalable Neural Symbolic Search for Knowledge Graph Complex Query Answering](https://arxiv.org/pdf/2505.08155)
*Weizhi Fei, Zihao Wang, hang Yin, Shukai Zhao, Wei Zhang, Yangqiu Song*

Main category: cs.AI

TL;DR: The paper proposes a scalable symbolic search framework for Complex Query Answering (CQA) to address efficiency and scalability issues in neuro-symbolic approaches.


<details>
  <summary>Details</summary>
Motivation: Neuro-symbolic methods for CQA face quadratic data complexity and NP-hard query complexity, limiting scalability.

Method: Introduces constraint strategies to reduce variable domains and an approximate local search algorithm for cyclic queries.

Result: Reduces computational load by 90% while maintaining performance on CQA benchmarks.

Conclusion: The framework effectively tackles efficiency and scalability challenges in CQA.

Abstract: Complex Query Answering (CQA) aims to retrieve answer sets for complex
logical formulas from incomplete knowledge graphs, which is a crucial yet
challenging task in knowledge graph reasoning. While neuro-symbolic search
utilized neural link predictions achieve superior accuracy, they encounter
significant complexity bottlenecks: (i) Data complexity typically scales
quadratically with the number of entities in the knowledge graph, and (ii)
Query complexity becomes NP-hard for cyclic queries. Consequently, these
approaches struggle to effectively scale to larger knowledge graphs and more
complex queries. To address these challenges, we propose an efficient and
scalable symbolic search framework. First, we propose two constraint strategies
to compute neural logical indices to reduce the domain of variables, thereby
decreasing the data complexity of symbolic search. Additionally, we introduce
an approximate algorithm based on local search to tackle the NP query
complexity of cyclic queries. Experiments on various CQA benchmarks demonstrate
that our framework reduces the computational load of symbolic methods by 90\%
while maintaining nearly the same performance, thus alleviating both efficiency
and scalability issues.

</details>


### [231] [Is Centralized Training with Decentralized Execution Framework Centralized Enough for MARL?](https://arxiv.org/pdf/2305.17352)
*Yihe Zhou, Shunyu Liu, Yunpeng Qing, Kaixuan Chen, Tongya Zheng, Jie Song, Mingli Song*

Main category: cs.AI

TL;DR: The paper introduces CADP, a framework enhancing CTDE in MARL by enabling agent communication during training while ensuring decentralized execution, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing CTDE methods inefficiently use global information due to policy independence assumptions, limiting cooperative learning.

Method: CADP provides explicit communication channels for advice exchange during training and uses model pruning to ensure decentralized execution.

Result: CADP achieves superior performance on StarCraft II and Google Research Football benchmarks.

Conclusion: CADP effectively balances centralized training and decentralized execution, improving MARL performance.

Abstract: Centralized Training with Decentralized Execution (CTDE) has recently emerged
as a popular framework for cooperative Multi-Agent Reinforcement Learning
(MARL), where agents can use additional global state information to guide
training in a centralized way and make their own decisions only based on
decentralized local policies. Despite the encouraging results achieved, CTDE
makes an independence assumption on agent policies, which limits agents to
adopt global cooperative information from each other during centralized
training. Therefore, we argue that existing CTDE methods cannot fully utilize
global information for training, leading to an inefficient joint-policy
exploration and even suboptimal results. In this paper, we introduce a novel
Centralized Advising and Decentralized Pruning (CADP) framework for multi-agent
reinforcement learning, that not only enables an efficacious message exchange
among agents during training but also guarantees the independent policies for
execution. Firstly, CADP endows agents the explicit communication channel to
seek and take advices from different agents for more centralized training. To
further ensure the decentralized execution, we propose a smooth model pruning
mechanism to progressively constraint the agent communication into a closed one
without degradation in agent cooperation capability. Empirical evaluations on
StarCraft II micromanagement and Google Research Football benchmarks
demonstrate that the proposed framework achieves superior performance compared
with the state-of-the-art counterparts. Our code will be made publicly
available.

</details>


### [232] [Decoding Neighborhood Environments with Large Language Models](https://arxiv.org/pdf/2505.08163)
*Andrew Cart, Shaohu Zhang, Melanie Escue, Xugui Zhou, Haitao Zhao, Prashanth BusiReddyGari, Beiyu Lin, Shuang Li*

Main category: cs.AI

TL;DR: The study explores using large language models (LLMs) like ChatGPT and Gemini to automate neighborhood environment assessment, achieving high accuracy with a YOLOv11-based model and evaluating LLMs' feasibility.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for assessing neighborhood environments are resource-intensive, and machine learning lacks scalability due to labeling efforts and model accessibility.

Method: A YOLOv11-based model is trained for detecting environmental indicators, and four LLMs are evaluated for their feasibility and robustness in identifying these indicators.

Result: The YOLOv11 model achieves 99.13% accuracy, while LLMs achieve over 88% accuracy with majority voting, demonstrating their potential for scalable analysis.

Conclusion: LLMs can effectively decode neighborhood environments without training, offering a scalable alternative to traditional methods.

Abstract: Neighborhood environments include physical and environmental conditions such
as housing quality, roads, and sidewalks, which significantly influence human
health and well-being. Traditional methods for assessing these environments,
including field surveys and geographic information systems (GIS), are
resource-intensive and challenging to evaluate neighborhood environments at
scale. Although machine learning offers potential for automated analysis, the
laborious process of labeling training data and the lack of accessible models
hinder scalability. This study explores the feasibility of large language
models (LLMs) such as ChatGPT and Gemini as tools for decoding neighborhood
environments (e.g., sidewalk and powerline) at scale. We train a robust
YOLOv11-based model, which achieves an average accuracy of 99.13% in detecting
six environmental indicators, including streetlight, sidewalk, powerline,
apartment, single-lane road, and multilane road. We then evaluate four LLMs,
including ChatGPT, Gemini, Claude, and Grok, to assess their feasibility,
robustness, and limitations in identifying these indicators, with a focus on
the impact of prompting strategies and fine-tuning. We apply majority voting
with the top three LLMs to achieve over 88% accuracy, which demonstrates LLMs
could be a useful tool to decode the neighborhood environment without any
training effort.

</details>


### [233] [Behind the Noise: Conformal Quantile Regression Reveals Emergent Representations](https://arxiv.org/pdf/2505.08176)
*Petrus H. Zwart, Tamas Varga, Odeta Qafoku, James A. Sethian*

Main category: cs.AI

TL;DR: A machine learning method for denoising scientific imaging data with uncertainty bounds and latent structure discovery.


<details>
  <summary>Details</summary>
Motivation: High-quality imaging requires long acquisition times, but reducing time introduces noise. The goal is to denoise while revealing latent features.

Method: Uses ensembles of lightweight neural networks trained via conformal quantile regression for denoising and feature discovery.

Result: Reliable denoising with interpretable spatial/chemical features, validated on geobiochemical imaging data.

Conclusion: The framework aids confident interpretation and experimental design under resource constraints.

Abstract: Scientific imaging often involves long acquisition times to obtain
high-quality data, especially when probing complex, heterogeneous systems.
However, reducing acquisition time to increase throughput inevitably introduces
significant noise into the measurements. We present a machine learning approach
that not only denoises low-quality measurements with calibrated uncertainty
bounds, but also reveals emergent structure in the latent space. By using
ensembles of lightweight, randomly structured neural networks trained via
conformal quantile regression, our method performs reliable denoising while
uncovering interpretable spatial and chemical features -- without requiring
labels or segmentation. Unlike conventional approaches focused solely on image
restoration, our framework leverages the denoising process itself to drive the
emergence of meaningful representations. We validate the approach on real-world
geobiochemical imaging data, showing how it supports confident interpretation
and guides experimental design under resource constraints.

</details>


### [234] [Evaluating LLM Metrics Through Real-World Capabilities](https://arxiv.org/pdf/2505.08253)
*Justin K Miller, Wenjia Tang*

Main category: cs.AI

TL;DR: The paper evaluates generative AI's real-world utility by analyzing six core LLM capabilities, identifying benchmark gaps, and comparing leading models, with Google Gemini outperforming others.


<details>
  <summary>Details</summary>
Motivation: To assess AI performance based on real-world usage rather than abstract intelligence, focusing on practical tasks like summarization and technical assistance.

Method: Analyzed survey data and usage logs to identify six core LLM capabilities, evaluated benchmark coverage, and compared models using human-centered criteria.

Result: Significant gaps in benchmark coverage were found. Google Gemini outperformed other models on utility-focused metrics for four capabilities.

Conclusion: Current benchmarks lack alignment with real-world usage; Google Gemini excels in practical utility, highlighting the need for better evaluation frameworks.

Abstract: As generative AI becomes increasingly embedded in everyday workflows, it is
important to evaluate its performance in ways that reflect real-world usage
rather than abstract notions of intelligence. Unlike many existing benchmarks
that assess general intelligence, our approach focuses on real-world utility,
evaluating how well models support users in everyday tasks. While current
benchmarks emphasize code generation or factual recall, users rely on AI for a
much broader range of activities-from writing assistance and summarization to
citation formatting and stylistic feedback. In this paper, we analyze
large-scale survey data and usage logs to identify six core capabilities that
represent how people commonly use Large Language Models (LLMs): Summarization,
Technical Assistance, Reviewing Work, Data Structuring, Generation, and
Information Retrieval. We then assess the extent to which existing benchmarks
cover these capabilities, revealing significant gaps in coverage, efficiency
measurement, and interpretability. Drawing on this analysis, we use
human-centered criteria to identify gaps in how well current benchmarks reflect
common usage that is grounded in five practical criteria: coherence, accuracy,
clarity, relevance, and efficiency. For four of the six capabilities, we
identify the benchmarks that best align with real-world tasks and use them to
compare leading models. We find that Google Gemini outperforms other
models-including OpenAI's GPT, xAI's Grok, Meta's LLaMA, Anthropic's Claude,
DeepSeek, and Qwen from Alibaba-on these utility-focused metrics.

</details>


### [235] [An Identifiable Cost-Aware Causal Decision-Making Framework Using Counterfactual Reasoning](https://arxiv.org/pdf/2505.08343)
*Ruichu Cai, Xi Chen, Jie Qiao, Zijian Li, Yuequn Liu, Wei Chen, Keli Zhang, Jiale Zheng*

Main category: cs.AI

TL;DR: The paper proposes a minimum-cost causal decision (MiCCD) framework using counterfactual reasoning to improve decision-making under abnormal conditions, outperforming traditional methods in cost efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing decision-making frameworks often neglect action costs or lack adequate causal mechanisms, leading to suboptimal solutions in abnormal scenarios.

Method: MiCCD uses causal graphs and counterfactual reasoning, approximating structural causal models and optimizing interventions with the SLSQP algorithm.

Result: MiCCD outperforms conventional methods in F1-score, cost efficiency, and ranking quality (nDCG@k values) on synthetic and real-world datasets.

Conclusion: The MiCCD framework is effective and broadly applicable for decision-making under abnormal conditions, addressing cost and causal limitations of existing approaches.

Abstract: Decision making under abnormal conditions is a critical process that involves
evaluating the current state and determining the optimal action to restore the
system to a normal state at an acceptable cost. However, in such scenarios,
existing decision-making frameworks highly rely on reinforcement learning or
root cause analysis, resulting in them frequently neglecting the cost of the
actions or failing to incorporate causal mechanisms adequately. By relaxing the
existing causal decision framework to solve the necessary cause, we propose a
minimum-cost causal decision (MiCCD) framework via counterfactual reasoning to
address the above challenges. Emphasis is placed on making counterfactual
reasoning processes identifiable in the presence of a large amount of mixed
anomaly data, as well as finding the optimal intervention state in a continuous
decision space. Specifically, it formulates a surrogate model based on causal
graphs, using abnormal pattern clustering labels as supervisory signals. This
enables the approximation of the structural causal model among the variables
and lays a foundation for identifiable counterfactual reasoning. With the
causal structure approximated, we then established an optimization model based
on counterfactual estimation. The Sequential Least Squares Programming (SLSQP)
algorithm is further employed to optimize intervention strategies while taking
costs into account. Experimental evaluations on both synthetic and real-world
datasets reveal that MiCCD outperforms conventional methods across multiple
metrics, including F1-score, cost efficiency, and ranking quality(nDCG@k
values), thus validating its efficacy and broad applicability.

</details>


### [236] [Modeling Unseen Environments with Language-guided Composable Causal Components in Reinforcement Learning](https://arxiv.org/pdf/2505.08361)
*Xinyue Wang, Biwei Huang*

Main category: cs.AI

TL;DR: WM3C enhances RL generalization by learning compositional causal components, outperforming existing methods in unseen tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing RL generalization challenges in novel environments by mimicking human compositional reasoning.

Method: Uses language to decompose latent space, masked autoencoder with mutual info constraints, and adaptive sparsity.

Result: Significantly outperforms existing methods in latent process identification, policy learning, and generalization.

Conclusion: WM3C offers robust adaptation to new tasks through compositional causal components.

Abstract: Generalization in reinforcement learning (RL) remains a significant
challenge, especially when agents encounter novel environments with unseen
dynamics. Drawing inspiration from human compositional reasoning -- where known
components are reconfigured to handle new situations -- we introduce World
Modeling with Compositional Causal Components (WM3C). This novel framework
enhances RL generalization by learning and leveraging compositional causal
components. Unlike previous approaches focusing on invariant representation
learning or meta-learning, WM3C identifies and utilizes causal dynamics among
composable elements, facilitating robust adaptation to new tasks. Our approach
integrates language as a compositional modality to decompose the latent space
into meaningful components and provides theoretical guarantees for their unique
identification under mild assumptions. Our practical implementation uses a
masked autoencoder with mutual information constraints and adaptive sparsity
regularization to capture high-level semantic information and effectively
disentangle transition dynamics. Experiments on numerical simulations and
real-world robotic manipulation tasks demonstrate that WM3C significantly
outperforms existing methods in identifying latent processes, improving policy
learning, and generalizing to unseen tasks.

</details>


### [237] [Learning Like Humans: Advancing LLM Reasoning Capabilities via Adaptive Difficulty Curriculum Learning and Expert-Guided Self-Reformulation](https://arxiv.org/pdf/2505.08364)
*Enci Zhang, Xingang Yan, Wei Lin, Tianxiang Zhang, Qianchun Lu*

Main category: cs.AI

TL;DR: The paper introduces two human-inspired strategies, ADCL and EGSR, to improve large language models' performance in solving complex problems, showing significant gains on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with complex problems despite progress in areas like mathematical reasoning. The paper aims to enhance their capabilities by mimicking human learning strategies.

Method: Proposes Adaptive Difficulty Curriculum Learning (ADCL) to dynamically adjust problem difficulty during training and Expert-Guided Self-Reformulation (EGSR) to guide models in reformulating expert solutions for deeper understanding.

Result: Experiments on mathematical reasoning benchmarks show a 10% improvement on AIME24 and 16.6% on AIME25 over the baseline.

Conclusion: The strategies ADCL and EGSR synergistically enhance model performance, demonstrating the value of human-inspired learning approaches.

Abstract: Despite impressive progress in areas like mathematical reasoning, large
language models still face significant challenges in consistently solving
complex problems. Drawing inspiration from key human learning strategies, we
propose two novel strategies to enhance the capability of large language models
to solve these complex problems. First, Adaptive Difficulty Curriculum Learning
(ADCL) is a novel curriculum learning strategy that tackles the Difficulty
Shift phenomenon (i.e., a model's perception of problem difficulty dynamically
changes during training) by periodically re-estimating difficulty within
upcoming data batches to maintain alignment with the model's evolving
capabilities. Second, Expert-Guided Self-Reformulation (EGSR) is a novel
reinforcement learning strategy that bridges the gap between imitation learning
and pure exploration by guiding models to reformulate expert solutions within
their own conceptual framework, rather than relying on direct imitation,
fostering deeper understanding and knowledge assimilation. Extensive
experiments on challenging mathematical reasoning benchmarks, using Qwen2.5-7B
as the base model, demonstrate that these human-inspired strategies
synergistically and significantly enhance performance. Notably, their combined
application improves performance over the standard Zero-RL baseline by 10% on
the AIME24 benchmark and 16.6% on AIME25.

</details>


### [238] [Explaining Autonomous Vehicles with Intention-aware Policy Graphs](https://arxiv.org/pdf/2505.08404)
*Sara Montese, Victor Gimenez-Abalos, Atia Cortés, Ulises Cortés, Sergio Alvarez-Napagao*

Main category: cs.AI

TL;DR: A post-hoc, model-agnostic method for explaining autonomous vehicle behavior to improve trust and regulatory acceptance.


<details>
  <summary>Details</summary>
Motivation: Addressing the opacity of AI-driven autonomous vehicles to enhance societal trust and regulatory compliance.

Method: Uses Intention-aware Policy Graphs to extract interpretable explanations from the nuScenes dataset, analyzing behavior globally and locally.

Result: Demonstrates the ability to assess legal compliance and identify vulnerabilities in datasets and models.

Conclusion: The proposed method offers reliable explanations, aiding in the validation and improvement of autonomous driving systems.

Abstract: The potential to improve road safety, reduce human driving error, and promote
environmental sustainability have enabled the field of autonomous driving to
progress rapidly over recent decades. The performance of autonomous vehicles
has significantly improved thanks to advancements in Artificial Intelligence,
particularly Deep Learning. Nevertheless, the opacity of their decision-making,
rooted in the use of accurate yet complex AI models, has created barriers to
their societal trust and regulatory acceptance, raising the need for
explainability. We propose a post-hoc, model-agnostic solution to provide
teleological explanations for the behaviour of an autonomous vehicle in urban
environments. Building on Intention-aware Policy Graphs, our approach enables
the extraction of interpretable and reliable explanations of vehicle behaviour
in the nuScenes dataset from global and local perspectives. We demonstrate the
potential of these explanations to assess whether the vehicle operates within
acceptable legal boundaries and to identify possible vulnerabilities in
autonomous driving datasets and models.

</details>


### [239] [Agent-as-a-Service based on Agent Network](https://arxiv.org/pdf/2505.08446)
*Yuhan Zhu, Haojie Liu, Jian Wang, Bing Li, Zikang Yin, Yefei Liao*

Main category: cs.AI

TL;DR: The paper proposes Agent-as-a-Service based on Agent Network (AaaS-AN), a service-oriented paradigm for organizing agent-level collaboration in Multi-Agent Systems (MAS), outperforming baselines in tasks like mathematical reasoning and code generation.


<details>
  <summary>Details</summary>
Motivation: Address the lack of support for agent-level collaboration in existing protocols like Model Context Protocol (MCP) by introducing a unified framework for agent lifecycle management.

Method: Introduces AaaS-AN with two core components: a dynamic Agent Network for self-organizing agents and service-oriented agents with protocols for discovery and interoperability, orchestrated by a Service Scheduler.

Result: Validated on mathematical reasoning and code generation tasks, AaaS-AN outperforms state-of-the-art baselines and successfully scales to 100 agent services.

Conclusion: AaaS-AN effectively bridges the gap in agent-level collaboration, demonstrated by its performance and scalability, with a released dataset to aid future MAS research.

Abstract: The rise of large model-based AI agents has spurred interest in Multi-Agent
Systems (MAS) for their capabilities in decision-making, collaboration, and
adaptability. While the Model Context Protocol (MCP) addresses tool invocation
and data exchange challenges via a unified protocol, it lacks support for
organizing agent-level collaboration. To bridge this gap, we propose
Agent-as-a-Service based on Agent Network (AaaS-AN), a service-oriented
paradigm grounded in the Role-Goal-Process-Service (RGPS) standard. AaaS-AN
unifies the entire agent lifecycle, including construction, integration,
interoperability, and networked collaboration, through two core components: (1)
a dynamic Agent Network, which models agents and agent groups as vertexes that
self-organize within the network based on task and role dependencies; (2)
service-oriented agents, incorporating service discovery, registration, and
interoperability protocols. These are orchestrated by a Service Scheduler,
which leverages an Execution Graph to enable distributed coordination, context
tracking, and runtime task management. We validate AaaS-AN on mathematical
reasoning and application-level code generation tasks, which outperforms
state-of-the-art baselines. Notably, we constructed a MAS based on AaaS-AN
containing agent groups, Robotic Process Automation (RPA) workflows, and MCP
servers over 100 agent services. We also release a dataset containing 10,000
long-horizon multi-agent workflows to facilitate future research on long-chain
collaboration in MAS.

</details>


### [240] [Adaptive Bias Generalized Rollout Policy Adaptation on the Flexible Job-Shop Scheduling Problem](https://arxiv.org/pdf/2505.08451)
*Lotfi Kobrosly, Marc-Emmanuel Coupvent des Graviers, Christophe Guettier, Tristan Cazenave*

Main category: cs.AI

TL;DR: A novel algorithm based on Generalized Nested Rollout Policy Adaptation outperforms other MCTS-based methods for solving the Flexible Job-Shop Scheduling Problem (FJSSP), though large instances remain challenging.


<details>
  <summary>Details</summary>
Motivation: The FJSSP is an NP-hard problem with significant applications in manufacturing, requiring efficient scheduling of operations on dissimilar machines.

Method: The proposed algorithm is derived from Generalized Nested Rollout Policy Adaptation, a method tailored for FJSSP.

Result: The algorithm performs better than other MCTS-based approaches, but large instances still fall short of known upper bounds.

Conclusion: The novel algorithm shows promise for FJSSP, though further improvements are needed for large-scale instances.

Abstract: The Flexible Job-Shop Scheduling Problem (FJSSP) is an NP-hard combinatorial
optimization problem, with several application domains, especially for
manufacturing purposes. The objective is to
  efficiently schedule multiple operations on dissimilar machines. These
operations are gathered into jobs, and operations pertaining to the same job
need to be scheduled sequentially. Different methods have been previously
tested to solve this problem, such as Constraint Solving, Tabu Search, Genetic
Algorithms, or Monte Carlo Tree Search (MCTS). We propose a novel algorithm
derived from the Generalized Nested Rollout Policy Adaptation, developed to
solve the FJSSP. We report encouraging experimental results, as our algorithm
performs better than other MCTS-based approaches, even if makespans obtained on
large instances are still far from known upper bounds.

</details>


### [241] [Strategy-Augmented Planning for Large Language Models via Opponent Exploitation](https://arxiv.org/pdf/2505.08459)
*Shuai Xu, Sijia Cui, Yanna Wang, Bo Xu, Qi Wang*

Main category: cs.AI

TL;DR: The paper introduces a two-stage Strategy-Augmented Planning (SAP) framework to enhance LLM-based opponent modeling, using a Strategy Evaluation Network (SEN) for offline strategy training and online exploitation.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitation of LLMs in opponent modeling when lacking domain expertise, SAP improves exploitation capabilities.

Method: SAP involves offline strategy space construction and SEN training, followed by online dynamic strategy recognition and exploitation via SEN.

Result: SAP shows robust generalization, outperforming baselines by 85.35% in MicroRTS and matching SOTA rule-based AI.

Conclusion: SAP effectively enhances LLM-based opponent modeling, demonstrating strong performance and generalization.

Abstract: Efficiently modeling and exploiting opponents is a long-standing challenge in
adversarial domains. Large Language Models (LLMs) trained on extensive textual
data have recently demonstrated outstanding performance in general tasks,
introducing new research directions for opponent modeling. Some studies
primarily focus on directly using LLMs to generate decisions based on the
elaborate prompt context that incorporates opponent descriptions, while these
approaches are limited to scenarios where LLMs possess adequate domain
expertise. To address that, we introduce a two-stage Strategy-Augmented
Planning (SAP) framework that significantly enhances the opponent exploitation
capabilities of LLM-based agents by utilizing a critical component, the
Strategy Evaluation Network (SEN). Specifically, in the offline stage, we
construct an explicit strategy space and subsequently collect strategy-outcome
pair data for training the SEN network. During the online phase, SAP
dynamically recognizes the opponent's strategies and greedily exploits them by
searching best response strategy on the well-trained SEN, finally translating
strategy to a course of actions by carefully designed prompts. Experimental
results show that SAP exhibits robust generalization capabilities, allowing it
to perform effectively not only against previously encountered opponent
strategies but also against novel, unseen strategies. In the MicroRTS
environment, SAP achieves a 85.35\% performance improvement over baseline
methods and matches the competitiveness of reinforcement learning approaches
against state-of-the-art (SOTA) rule-based AI.

</details>


### [242] [BAT: Benchmark for Auto-bidding Task](https://arxiv.org/pdf/2505.08485)
*Alexandra Khirianova, Ekaterina Solodneva, Andrey Pudovikov, Sergey Osokin, Egor Samosvat, Yuriy Dorn, Alexander Ledovsky, Yana Zenkova*

Main category: cs.AI

TL;DR: A benchmark for optimizing autobidding algorithms in online ad auctions is introduced, addressing budget pacing and CPC constraints.


<details>
  <summary>Details</summary>
Motivation: The lack of comprehensive datasets and benchmarks hinders the development of real-time autobidding algorithms.

Method: A novel dataset and benchmark framework are created, implementing robust baselines for two common auction formats.

Result: The benchmark aids researchers and practitioners in refining autobidding algorithms, advancing programmatic advertising.

Conclusion: The benchmark provides a user-friendly tool for innovation in autobidding, with resources available online.

Abstract: The optimization of bidding strategies for online advertising slot auctions
presents a critical challenge across numerous digital marketplaces. A
significant obstacle to the development, evaluation, and refinement of
real-time autobidding algorithms is the scarcity of comprehensive datasets and
standardized benchmarks.
  To address this deficiency, we present an auction benchmark encompassing the
two most prevalent auction formats. We implement a series of robust baselines
on a novel dataset, addressing the most salient Real-Time Bidding (RTB) problem
domains: budget pacing uniformity and Cost Per Click (CPC) constraint
optimization. This benchmark provides a user-friendly and intuitive framework
for researchers and practitioners to develop and refine innovative autobidding
algorithms, thereby facilitating advancements in the field of programmatic
advertising. The implementation and additional resources can be accessed at the
following repository (https://github.com/avito-tech/bat-autobidding-benchmark,
https://doi.org/10.5281/zenodo.14794182).

</details>


### [243] [Achieving Scalable Robot Autonomy via neurosymbolic planning using lightweight local LLM](https://arxiv.org/pdf/2505.08492)
*Nicholas Attolino, Alessio Capitanelli, Fulvio Mastrogiovanni*

Main category: cs.AI

TL;DR: Gideon is a novel framework using local LLMs for scalable, efficient, and multi-domain neurosymbolic task planning, addressing limitations of closed-source models.


<details>
  <summary>Details</summary>
Motivation: Overcoming scalability, re-planning, and dependency issues in PDDL-based symbolic task planning for human-robot collaboration.

Method: Integrates a problem generator for dataset creation and adapts neurosymbolic planning for local LLMs, enabling on-device execution and extended context.

Result: Achieves 66.1% valid plans in single-domain and 70.6% in multi-domain tests, with scalability and efficiency advantages.

Conclusion: Gideon offers significant improvements in inference efficiency, scalability, and multi-domain adaptability, despite training inefficiency.

Abstract: PDDL-based symbolic task planning remains pivotal for robot autonomy yet
struggles with dynamic human-robot collaboration due to scalability,
re-planning demands, and delayed plan availability. Although a few
neurosymbolic frameworks have previously leveraged LLMs such as GPT-3 to
address these challenges, reliance on closed-source, remote models with limited
context introduced critical constraints: third-party dependency, inconsistent
response times, restricted plan length and complexity, and multi-domain
scalability issues. We present Gideon, a novel framework that enables the
transition to modern, smaller, local LLMs with extended context length. Gideon
integrates a novel problem generator to systematically generate large-scale
datasets of realistic domain-problem-plan tuples for any domain, and adapts
neurosymbolic planning for local LLMs, enabling on-device execution and
extended context for multi-domain support. Preliminary experiments in
single-domain scenarios performed on Qwen-2.5 1.5B and trained on 8k-32k
samples, demonstrate a valid plan percentage of 66.1% (32k model) and show that
the figure can be further scaled through additional data. Multi-domain tests on
16k samples yield an even higher 70.6% planning validity rate, proving
extensibility across domains and signaling that data variety can have a
positive effect on learning efficiency. Although long-horizon planning and
reduced model size make Gideon training much less efficient than baseline
models based on larger LLMs, the results are still significant considering that
the trained model is about 120x smaller than baseline and that significant
advantages can be achieved in inference efficiency, scalability, and
multi-domain adaptability, all critical factors in human-robot collaboration.
Training inefficiency can be mitigated by Gideon's streamlined data generation
pipeline.

</details>


### [244] [TrialMatchAI: An End-to-End AI-powered Clinical Trial Recommendation System to Streamline Patient-to-Trial Matching](https://arxiv.org/pdf/2505.08508)
*Majd Abdallah, Sigve Nakken, Mariska Bierkens, Johanna Galvis, Alexis Groppi, Slim Karkar, Lana Meiqari, Maria Alexandra Rujano, Steve Canham, Rodrigo Dienstmann, Remond Fijneman, Eivind Hovig, Gerrit Meijer, Macha Nikolski*

Main category: cs.AI

TL;DR: TrialMatchAI is an AI system for automated patient-to-trial matching, using LLMs and hybrid search to improve efficiency and accuracy in clinical trials.


<details>
  <summary>Details</summary>
Motivation: Patient recruitment is a bottleneck in clinical trials; scalable, automated solutions are needed.

Method: Uses fine-tuned LLMs in a retrieval-augmented framework, hybrid search, and medical Chain-of-Thought reasoning for eligibility assessments.

Result: Achieves 92% relevance in top 20 recommendations, 90% accuracy in eligibility classification, excels in biomarker-driven matches.

Conclusion: TrialMatchAI offers a scalable, efficient, and interpretable solution for precision medicine trial matching.

Abstract: Patient recruitment remains a major bottleneck in clinical trials, calling
for scalable and automated solutions. We present TrialMatchAI, an AI-powered
recommendation system that automates patient-to-trial matching by processing
heterogeneous clinical data, including structured records and unstructured
physician notes. Built on fine-tuned, open-source large language models (LLMs)
within a retrieval-augmented generation framework, TrialMatchAI ensures
transparency and reproducibility and maintains a lightweight deployment
footprint suitable for clinical environments. The system normalizes biomedical
entities, retrieves relevant trials using a hybrid search strategy combining
lexical and semantic similarity, re-ranks results, and performs criterion-level
eligibility assessments using medical Chain-of-Thought reasoning. This pipeline
delivers explainable outputs with traceable decision rationales. In real-world
validation, 92 percent of oncology patients had at least one relevant trial
retrieved within the top 20 recommendations. Evaluation across synthetic and
real clinical datasets confirmed state-of-the-art performance, with expert
assessment validating over 90 percent accuracy in criterion-level eligibility
classification, particularly excelling in biomarker-driven matches. Designed
for modularity and privacy, TrialMatchAI supports Phenopackets-standardized
data, enables secure local deployment, and allows seamless replacement of LLM
components as more advanced models emerge. By enhancing efficiency and
interpretability and offering lightweight, open-source deployment, TrialMatchAI
provides a scalable solution for AI-driven clinical trial matching in precision
medicine.

</details>


### [245] [On the Complexity and Properties of Preferential Propositional Dependence Logic](https://arxiv.org/pdf/2505.08522)
*Kai Sauerwald, Arne Meier, Juha Kontinen*

Main category: cs.AI

TL;DR: The paper analyzes KLM-style preferential reasoning in propositional dependence logic, showing it is cumulative but violates System P. It provides conditions for System P satisfaction, which don't extend to team-based logic, and explores entailment and complexity.


<details>
  <summary>Details</summary>
Motivation: To understand the properties and complexity of preferential reasoning in propositional dependence logic and team semantics, including its relationship with System P.

Method: The study uses preferential team-based reasoning, characterizes conditions for System P, and examines entailment and complexity in classical and dependence logic.

Result: Preferential reasoning is cumulative but violates System P; conditions for System P are identified but don't apply to team-based logic. Complexity results are provided.

Conclusion: The paper clarifies preferential reasoning in dependence logic, highlights limitations, and contributes new complexity insights for classical preferential reasoning.

Abstract: This paper considers the complexity and properties of KLM-style preferential
reasoning in the setting of propositional logic with team semantics and
dependence atoms, also known as propositional dependence logic. Preferential
team-based reasoning is shown to be cumulative, yet violates System~P. We give
intuitive conditions that fully characterise those cases where preferential
propositional dependence logic satisfies System~P. We show that these
characterisations do, surprisingly, not carry over to preferential team-based
propositional logic. Furthermore, we show how classical entailment and
dependence logic entailment can be expressed in terms of non-trivial
preferential models. Finally, we present the complexity of preferential
team-based reasoning for two natural representations. This includes novel
complexity results for classical (non-team-based) preferential reasoning.

</details>


### [246] [Guiding LLM-based Smart Contract Generation with Finite State Machine](https://arxiv.org/pdf/2505.08542)
*Hao Luo, Yuhao Lin, Xiao Yan, Xintong Hu, Yuxiang Wang, Qiming Zeng, Hao Wang, Jiawei Jiang*

Main category: cs.AI

TL;DR: FSM-SCG is a framework combining finite state machines (FSM) and LLMs to automate and improve smart contract generation, enhancing code quality and security.


<details>
  <summary>Details</summary>
Motivation: Traditional smart contract generation is manual and inefficient; LLMs alone struggle with effectiveness and security.

Method: FSM-SCG abstracts user requirements into FSM, guides LLMs for code generation, and iteratively optimizes via compilation and security checks.

Result: Compilation success rate improves by 48%, and vulnerability risk drops by 68% compared to baselines.

Conclusion: FSM-SCG effectively addresses smart contract generation challenges, boosting quality and security.

Abstract: Smart contract is a kind of self-executing code based on blockchain
technology with a wide range of application scenarios, but the traditional
generation method relies on manual coding and expert auditing, which has a high
threshold and low efficiency. Although Large Language Models (LLMs) show great
potential in programming tasks, they still face challenges in smart contract
generation w.r.t. effectiveness and security. To solve these problems, we
propose FSM-SCG, a smart contract generation framework based on finite state
machine (FSM) and LLMs, which significantly improves the quality of the
generated code by abstracting user requirements to generate FSM, guiding LLMs
to generate smart contracts, and iteratively optimizing the code with the
feedback of compilation and security checks. The experimental results show that
FSM-SCG significantly improves the quality of smart contract generation.
Compared to the best baseline, FSM-SCG improves the compilation success rate of
generated smart contract code by at most 48%, and reduces the average
vulnerability risk score by approximately 68%.

</details>


### [247] [Resource-Efficient Language Models: Quantization for Fast and Accessible Inference](https://arxiv.org/pdf/2505.08620)
*Tollef Emil Jørgensen*

Main category: cs.AI

TL;DR: A review of post-training quantization (PTQ) techniques to optimize LLM inference efficiency, covering schemes, granularities, and trade-offs.


<details>
  <summary>Details</summary>
Motivation: Address the heavy resource demands of large language models (LLMs) to improve hardware accessibility and reduce energy consumption.

Method: High-level review of PTQ techniques, detailing quantization schemes and granularities.

Result: Provides a balanced overview of PTQ theory and applications for optimizing LLM inference.

Conclusion: PTQ offers practical solutions to enhance LLM efficiency, balancing theoretical insights with real-world usability.

Abstract: Large language models have significantly advanced natural language
processing, yet their heavy resource demands pose severe challenges regarding
hardware accessibility and energy consumption. This paper presents a focused
and high-level review of post-training quantization (PTQ) techniques designed
to optimize the inference efficiency of LLMs by the end-user, including details
on various quantization schemes, granularities, and trade-offs. The aim is to
provide a balanced overview between the theory and applications of
post-training quantization.

</details>


### [248] [Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models](https://arxiv.org/pdf/2505.08622)
*Donghoon Kim, Minji Bae, Kyuhong Shim, Byonghyo Shim*

Main category: cs.AI

TL;DR: VGD improves text-to-image prompt generation by combining LLMs and CLIP guidance, outperforming existing methods without additional training.


<details>
  <summary>Details</summary>
Motivation: Existing prompt inversion techniques are ineffective due to limited interpretability and incoherent outputs.

Method: VGD uses LLMs for text generation and CLIP scores for visual alignment, avoiding gradients.

Result: VGD generates more coherent and semantically aligned prompts than existing techniques.

Conclusion: VGD enhances interpretability and control in text-to-image models, improving user interaction.

Abstract: Text-to-image generative models like DALL-E and Stable Diffusion have
revolutionized visual content creation across various applications, including
advertising, personalized media, and design prototyping. However, crafting
effective textual prompts to guide these models remains challenging, often
requiring extensive trial and error. Existing prompt inversion approaches, such
as soft and hard prompt techniques, are not so effective due to the limited
interpretability and incoherent prompt generation. To address these issues, we
propose Visually Guided Decoding (VGD), a gradient-free approach that leverages
large language models (LLMs) and CLIP-based guidance to generate coherent and
semantically aligned prompts. In essence, VGD utilizes the robust text
generation capabilities of LLMs to produce human-readable prompts. Further, by
employing CLIP scores to ensure alignment with user-specified visual concepts,
VGD enhances the interpretability, generalization, and flexibility of prompt
generation without the need for additional training. Our experiments
demonstrate that VGD outperforms existing prompt inversion techniques in
generating understandable and contextually relevant prompts, facilitating more
intuitive and controllable interactions with text-to-image models.

</details>


### [249] [Integrating Natural Language Processing and Exercise Monitoring for Early Diagnosis of Metabolic Syndrome: A Deep Learning Approach](https://arxiv.org/pdf/2505.08628)
*Yichen Zhao, Yuhua Wang, Xi Cheng, Junhao Fang, Yang Yang*

Main category: cs.AI

TL;DR: A deep learning framework using NLP and exercise monitoring data achieves high accuracy in diagnosing metabolic syndrome (MetS) with easily obtainable daily-life data.


<details>
  <summary>Details</summary>
Motivation: MetS is prevalent and underdiagnosed, requiring costly medical tests. This study aims to use accessible physiological and exercise-related text data for early detection.

Method: Data from 40 nursing home volunteers was augmented to address imbalance. A deep learning model combined NLP and exercise monitoring for MetS classification.

Result: The model achieved AUROC=0.806 and recall=76.3%. Key features were text data and daily minimum heart rate.

Conclusion: The study highlights the potential of daily-life data for cost-effective MetS screening and management.

Abstract: Metabolic syndrome (MetS) is a medication condition characterized by
abdominal obesity, insulin resistance, hypertension and hyperlipidemia. It
increases the risk of majority of chronic diseases, including type 2 diabetes
mellitus, and affects about one quarter of the global population. Therefore,
early detection and timely intervention for MetS are crucial. Standard
diagnosis for MetS components requires blood tests conducted within medical
institutions. However, it is frequently underestimated, leading to unmet need
for care for MetS population. This study aims to use the least physiological
data and free texts about exercises related activities, which are obtained
easily in daily life, to diagnosis MetS. We collected the data from 40
volunteers in a nursing home and used data augmentation to reduce the
imbalance. We propose a deep learning framework for classifying MetS that
integrates natural language processing (NLP) and exercise monitoring. The
results showed that the best model reported a high positive result (AUROC=0.806
and REC=76.3%) through 3-fold cross-validation. Feature importance analysis
revealed that text and minimum heart rate on a daily basis contribute the most
in the classification of MetS. This study demonstrates the potential
application of data that are easily measurable in daily life for the early
diagnosis of MetS, which could contribute to reducing the cost of screening and
management for MetS population.

</details>


### [250] [TRAIL: Trace Reasoning and Agentic Issue Localization](https://arxiv.org/pdf/2505.08638)
*Darshan Deshpande, Varun Gangal, Hersh Mehta, Jitin Krishnan, Anand Kannappan, Rebecca Qian*

Main category: cs.AI

TL;DR: The paper highlights the need for scalable evaluation methods for agentic workflows, introduces a taxonomy of error types, and presents a human-annotated dataset (TRAIL) for benchmarking. It reveals poor performance of modern LLMs in trace debugging.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for agentic workflows are manual and domain-specific, failing to scale with increasing complexity and volume. Error analysis is complicated by tool outputs and language model reasoning.

Method: The work introduces a formal taxonomy of error types in agentic systems and curates 148 human-annotated traces (TRAIL) from real-world applications.

Result: Modern long-context LLMs perform poorly in trace debugging, with the best model scoring only 11% on TRAIL.

Conclusion: The paper advocates for robust evaluation methods and provides a public dataset (TRAIL) to advance scalable evaluation research for agentic workflows.

Abstract: The increasing adoption of agentic workflows across diverse domains brings a
critical need to scalably and systematically evaluate the complex traces these
systems generate. Current evaluation methods depend on manual, domain-specific
human analysis of lengthy workflow traces - an approach that does not scale
with the growing complexity and volume of agentic outputs. Error analysis in
these settings is further complicated by the interplay of external tool outputs
and language model reasoning, making it more challenging than traditional
software debugging. In this work, we (1) articulate the need for robust and
dynamic evaluation methods for agentic workflow traces, (2) introduce a formal
taxonomy of error types encountered in agentic systems, and (3) present a set
of 148 large human-annotated traces (TRAIL) constructed using this taxonomy and
grounded in established agentic benchmarks. To ensure ecological validity, we
curate traces from both single and multi-agent systems, focusing on real-world
applications such as software engineering and open-world information retrieval.
Our evaluations reveal that modern long context LLMs perform poorly at trace
debugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our
dataset and code are made publicly available to support and accelerate future
research in scalable evaluation for agentic workflows.

</details>


### [251] [WixQA: A Multi-Dataset Benchmark for Enterprise Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.08643)
*Dvir Cohen, Lin Burg, Sviatoslav Pykhnivskyi, Hagit Gur, Stanislav Kovynov, Olga Atzmon, Gilad Barkan*

Main category: cs.AI

TL;DR: WixQA is a benchmark suite for evaluating Retrieval-Augmented Generation (RAG) systems in enterprise QA, featuring three datasets grounded in a Wix Help Center KB snapshot.


<details>
  <summary>Details</summary>
Motivation: Enterprise QA systems lack domain-specific datasets mirroring real user issues, requiring benchmarks with KB snapshots for holistic evaluation.

Method: Introduces WixQA with three datasets: WixQA-ExpertWritten (real queries, expert answers), WixQA-Simulated (expert-validated QA pairs), and WixQA-Synthetic (LLM-generated QA pairs).

Result: Provides a KB snapshot and baseline results for evaluating RAG systems in realistic enterprise settings.

Conclusion: WixQA fills a gap in enterprise QA evaluation, offering a practical benchmark for RAG systems.

Abstract: Retrieval-Augmented Generation (RAG) is a cornerstone of modern question
answering (QA) systems, enabling grounded answers based on external knowledge.
Although recent progress has been driven by open-domain datasets, enterprise QA
systems need datasets that mirror the concrete, domain-specific issues users
raise in day-to-day support scenarios. Critically, evaluating end-to-end RAG
systems requires benchmarks comprising not only question--answer pairs but also
the specific knowledge base (KB) snapshot from which answers were derived. To
address this need, we introduce WixQA, a benchmark suite featuring QA datasets
precisely grounded in the released KB corpus, enabling holistic evaluation of
retrieval and generation components. WixQA includes three distinct QA datasets
derived from Wix.com customer support interactions and grounded in a snapshot
of the public Wix Help Center KB: (i) WixQA-ExpertWritten, 200 real user
queries with expert-authored, multi-step answers; (ii) WixQA-Simulated, 200
expert-validated QA pairs distilled from user dialogues; and (iii)
WixQA-Synthetic, 6,222 LLM-generated QA pairs, with one pair systematically
derived from each article in the knowledge base. We release the KB snapshot
alongside the datasets under MIT license and provide comprehensive baseline
results, forming a unique benchmark for evaluating enterprise RAG systems in
realistic enterprise environments.

</details>


### [252] [A Study of Data-driven Methods for Inventory Optimization](https://arxiv.org/pdf/2505.08673)
*Lee Yeung Ping, Patrick Wong, Tan Cheng Han*

Main category: cs.AI

TL;DR: The paper compares Time Series, Random Forest, and Deep Reinforcement Learning in three inventory models (Lost Sales, Dual-Sourcing, Multi-Echelon) for supermarkets, evaluating their efficiency using KPIs like forecast accuracy and cost impact.


<details>
  <summary>Details</summary>
Motivation: To analyze data-driven methods for inventory management, assessing their potential and challenges in improving efficiency and customer satisfaction.

Method: Applied three algorithms to three inventory models, using data visualization and statistical metrics for comparison.

Result: Identified trends and patterns to guide decision-making, with tools enabling real-time performance tracking and detailed analysis.

Conclusion: The study highlights the effectiveness of each algorithm, providing insights for optimizing inventory management and supply chain efficiency.

Abstract: This paper shows a comprehensive analysis of three algorithms (Time Series,
Random Forest (RF) and Deep Reinforcement Learning) into three inventory models
(the Lost Sales, Dual-Sourcing and Multi-Echelon Inventory Model). These
methodologies are applied in the supermarket context. The main purpose is to
analyse efficient methods for the data-driven. Their possibility, potential and
current challenges are taken into consideration in this report. By comparing
the results in each model, the effectiveness of each algorithm is evaluated
based on several key performance indicators, including forecast accuracy,
adaptability to market changes, and overall impact on inventory costs and
customer satisfaction levels. The data visualization tools and statistical
metrics are the indicators for the comparisons and show some obvious trends and
patterns that can guide decision-making in inventory management. These tools
enable managers to not only track the performance of different algorithms in
real-time but also to drill down into specific data points to understand the
underlying causes of inventory fluctuations. This level of detail is crucial
for pinpointing inefficiencies and areas for improvement within the supply
chain.

</details>


### [253] [LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from EHRs](https://arxiv.org/pdf/2505.08704)
*K M Sajjadul Islam, Ayesha Siddika Nipu, Jiawei Wu, Praveen Madiraju*

Main category: cs.AI

TL;DR: The paper explores prompt-based NER in EHRs using GPT-4o and DeepSeek-R1, with GPT-4o's ensemble method achieving the best performance (F1: 0.95, recall: 0.98).


<details>
  <summary>Details</summary>
Motivation: To enhance medical entity extraction from unstructured EHR text for clinical applications using LLMs and prompt engineering.

Method: Evaluated GPT-4o and DeepSeek-R1 with zero-shot, few-shot, and ensemble prompt techniques, using embedding-based similarity and majority voting.

Result: GPT-4o with prompt ensemble outperformed DeepSeek-R1, achieving high F1 (0.95) and recall (0.98).

Conclusion: Prompt ensemble with GPT-4o is highly effective for medical NER in EHRs, improving reliability and performance.

Abstract: Electronic Health Records (EHRs) are digital records of patient information,
often containing unstructured clinical text. Named Entity Recognition (NER) is
essential in EHRs for extracting key medical entities like problems, tests, and
treatments to support downstream clinical applications. This paper explores
prompt-based medical entity recognition using large language models (LLMs),
specifically GPT-4o and DeepSeek-R1, guided by various prompt engineering
techniques, including zero-shot, few-shot, and an ensemble approach. Among all
strategies, GPT-4o with prompt ensemble achieved the highest classification
performance with an F1-score of 0.95 and recall of 0.98, outperforming
DeepSeek-R1 on the task. The ensemble method improved reliability by
aggregating outputs through embedding-based similarity and majority voting.

</details>


### [254] [DeepMath-Creative: A Benchmark for Evaluating Mathematical Creativity of Large Language Models](https://arxiv.org/pdf/2505.08744)
*Xiaoyang Chen, Xinan Dai, Yu Du, Qian Feng, Naixu Guo, Tingshuo Gu, Yuting Gao, Yingyi Gao, Xudong Han, Xiang Jiang, Yilin Jin, Hongyi Lin, Shisheng Lin, Xiangnan Li, Yuante Li, Yixing Li, Zhentao Lai, Zilu Ma, Yingrong Peng, Jiacheng Qian, Hao-Yu Sun, Jianbo Sun, Zirui Wang, Siwei Wu, Zian Wang, Bin Xu, Jianghao Xu, Yiyang Yu, Zichuan Yang, Hongji Zha, Ruichong Zhang*

Main category: cs.AI

TL;DR: The paper introduces DeepMath-Creative, a benchmark for evaluating mathematical creativity in LLMs, revealing their limited performance on complex problems.


<details>
  <summary>Details</summary>
Motivation: To address the lack of focus on mathematical creativity in LLMs and the scarcity of evaluation datasets.

Method: Proposes evaluation criteria for creativity and introduces DeepMath-Creative, a benchmark for testing LLMs' problem-solving abilities.

Result: Best-performing model (O3 Mini) achieves 70% accuracy on basic tasks but struggles with complex problems, suggesting reliance on memorized patterns.

Conclusion: Current LLMs lack authentic creative insight, performing well only on familiar, lower-difficulty problems.

Abstract: To advance the mathematical proficiency of large language models (LLMs), the
DeepMath team has launched an open-source initiative aimed at developing an
open mathematical LLM and systematically evaluating its mathematical
creativity. This paper represents the initial contribution of this initiative.
While recent developments in mathematical LLMs have predominantly emphasized
reasoning skills, as evidenced by benchmarks on elementary to
undergraduate-level mathematical tasks, the creative capabilities of these
models have received comparatively little attention, and evaluation datasets
remain scarce. To address this gap, we propose an evaluation criteria for
mathematical creativity and introduce DeepMath-Creative, a novel, high-quality
benchmark comprising constructive problems across algebra, geometry, analysis,
and other domains. We conduct a systematic evaluation of mainstream LLMs'
creative problem-solving abilities using this dataset. Experimental results
show that even under lenient scoring criteria -- emphasizing core solution
components and disregarding minor inaccuracies, such as small logical gaps,
incomplete justifications, or redundant explanations -- the best-performing
model, O3 Mini, achieves merely 70% accuracy, primarily on basic
undergraduate-level constructive tasks. Performance declines sharply on more
complex problems, with models failing to provide substantive strategies for
open problems. These findings suggest that, although current LLMs display a
degree of constructive proficiency on familiar and lower-difficulty problems,
such performance is likely attributable to the recombination of memorized
patterns rather than authentic creative insight or novel synthesis.

</details>


### [255] [ARC-NCA: Towards Developmental Solutions to the Abstraction and Reasoning Corpus](https://arxiv.org/pdf/2505.08778)
*Etienne Guichard, Felix Reimers, Mia Kvalsund, Mikkel Lepperød, Stefano Nichele*

Main category: cs.AI

TL;DR: ARC-NCA uses Neural Cellular Automata (NCA) to tackle the ARC-AGI benchmark, showing promising results comparable to ChatGPT 4.5 at lower cost.


<details>
  <summary>Details</summary>
Motivation: The ARC-AGI benchmark challenges AI with few examples, easy for humans but hard for AI. Developmental approaches like NCA could enhance AI's abstraction and reasoning.

Method: Uses standard NCA and EngramNCA (with hidden memories) to simulate complex dynamics and mimic biological developmental processes.

Result: ARC-NCA achieves proof-of-concept results comparable or superior to ChatGPT 4.5, at reduced cost.

Conclusion: Developmental principles in computational models like NCA can improve AI's adaptive reasoning and abstraction for AGI challenges.

Abstract: The Abstraction and Reasoning Corpus (ARC), later renamed ARC-AGI, poses a
fundamental challenge in artificial general intelligence (AGI), requiring
solutions that exhibit robust abstraction and reasoning capabilities across
diverse tasks, while only few (with median count of three) correct examples are
presented. While ARC-AGI remains very challenging for artificial intelligence
systems, it is rather easy for humans. This paper introduces ARC-NCA, a
developmental approach leveraging standard Neural Cellular Automata (NCA) and
NCA enhanced with hidden memories (EngramNCA) to tackle the ARC-AGI benchmark.
NCAs are employed for their inherent ability to simulate complex dynamics and
emergent patterns, mimicking developmental processes observed in biological
systems. Developmental solutions may offer a promising avenue for enhancing
AI's problem-solving capabilities beyond mere training data extrapolation.
ARC-NCA demonstrates how integrating developmental principles into
computational models can foster adaptive reasoning and abstraction. We show
that our ARC-NCA proof-of-concept results may be comparable to, and sometimes
surpass, that of ChatGPT 4.5, at a fraction of the cost.

</details>


### [256] [Unravelling Responsibility for AI](https://arxiv.org/pdf/2308.02608)
*Zoe Porter, Philippa Ryan, Phillip Morgan, Joanna Al-Qaddoumi, Bernard Twomey, Paul Noordhof, John McDermid, Ibrahim Habli*

Main category: cs.AI

TL;DR: The paper introduces a conceptual framework for understanding and visualizing responsibility in AI-enabled systems, addressing gaps in current discussions and offering a methodology for real-world applications.


<details>
  <summary>Details</summary>
Motivation: To clarify the concept of responsibility in AI systems, ensuring justice for victims and informing policy and engineering practices, given the complexity of responsibility networks in AI ecosystems.

Method: Develops a framework with a graphical notation and methodology to visualize responsibility networks and trace attributions, using the formulation 'Actor A is responsible for Occurrence O.'

Result: A framework and notation for representing responsibility permutations, illustrated with a fictitious maritime collision scenario.

Conclusion: Provides a foundation for interdisciplinary stakeholders to address complex responsibility questions in AI, enhancing clarity and focus in discussions.

Abstract: It is widely acknowledged that we need to establish where responsibility lies
for the outputs and impacts of AI-enabled systems. This is important to achieve
justice and compensation for victims of AI harms, and to inform policy and
engineering practice. But without a clear, thorough understanding of what
`responsibility' means, deliberations about where responsibility lies will be,
at best, unfocused and incomplete and, at worst, misguided. Furthermore,
AI-enabled systems exist within a wider ecosystem of actors, decisions, and
governance structures, giving rise to complex networks of responsibility
relations. To address these issues, this paper presents a conceptual framework
of responsibility, accompanied with a graphical notation and general
methodology, for visualising these responsibility networks and for tracing
different responsibility attributions for AI. Taking the three-part formulation
'Actor A is responsible for Occurrence O,' the framework unravels the concept
of responsibility to clarify that there are different possibilities of who is
responsible for AI, senses in which they are responsible, and aspects of events
they are responsible for. The notation allows these permutations to be
represented graphically. The methodology enables users to apply the framework
to specific scenarios. The aim is to offer a foundation to support stakeholders
from diverse disciplinary backgrounds to discuss and address complex
responsibility questions in hypothesised and real-world cases involving AI. The
work is illustrated by application to a fictitious scenario of a fatal
collision between a crewless, AI-enabled maritime vessel in autonomous mode and
a traditional, crewed vessel at sea.

</details>


### [257] [S-EPOA: Overcoming the Indistinguishability of Segments with Skill-Driven Preference-Based Reinforcement Learning](https://arxiv.org/pdf/2408.12130)
*Ni Mu, Yao Luan, Yiqin Yang, Bo Xu, Qing-shan Jia*

Main category: cs.AI

TL;DR: S-EPOA enhances PbRL by integrating skill mechanisms and a novel query selection, improving robustness and efficiency over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional PbRL struggles with segment indistinguishability, limiting learning. S-EPOA aims to overcome this by leveraging skill-driven learning.

Method: Unsupervised pretraining for skill learning, followed by a query selection mechanism balancing information gain and distinguishability.

Result: S-EPOA outperforms conventional PbRL in robotic manipulation and locomotion tasks, showing improved robustness and efficiency.

Conclusion: Skill-driven learning effectively addresses segment indistinguishability, enhancing PbRL performance.

Abstract: Preference-based reinforcement learning (PbRL) stands out by utilizing human
preferences as a direct reward signal, eliminating the need for intricate
reward engineering. However, despite its potential, traditional PbRL methods
are often constrained by the indistinguishability of segments, which impedes
the learning process. In this paper, we introduce Skill-Enhanced Preference
Optimization Algorithm (S-EPOA), which addresses the segment
indistinguishability issue by integrating skill mechanisms into the preference
learning framework. Specifically, we first conduct the unsupervised pretraining
to learn useful skills. Then, we propose a novel query selection mechanism to
balance the information gain and distinguishability over the learned skill
space. Experimental results on a range of tasks, including robotic manipulation
and locomotion, demonstrate that S-EPOA significantly outperforms conventional
PbRL methods in terms of both robustness and learning efficiency. The results
highlight the effectiveness of skill-driven learning in overcoming the
challenges posed by segment indistinguishability.

</details>


### [258] [TradExpert: Revolutionizing Trading with Mixture of Expert LLMs](https://arxiv.org/pdf/2411.00782)
*Qianggang Ding, Haochen Shi, Jiadong Guo, Bang Liu*

Main category: cs.AI

TL;DR: TradeExpert is a novel AI framework for financial trading, using specialized LLMs to analyze diverse data sources, outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: The challenge of synthesizing insights from diverse financial data sources and integrating structured/unstructured data in AI-driven trading.

Method: Employs a mix of experts (MoE) approach with four specialized LLMs for distinct data types, synthesized by a General Expert LLM for predictions or rankings.

Result: Superior performance across all trading scenarios, validated by a new large-scale financial dataset.

Conclusion: TradeExpert effectively addresses data integration challenges in AI-driven trading, demonstrating strong performance.

Abstract: The integration of Artificial Intelligence (AI) in the financial domain has
opened new avenues for quantitative trading, particularly through the use of
Large Language Models (LLMs). However, the challenge of effectively
synthesizing insights from diverse data sources and integrating both structured
and unstructured data persists. This paper presents TradeExpert, a novel
framework that employs a mix of experts (MoE) approach, using four specialized
LLMs, each analyzing distinct sources of financial data, including news
articles, market data, alpha factors, and fundamental data. The insights of
these expert LLMs are further synthesized by a General Expert LLM to make a
final prediction or decision. With specific prompts, TradeExpert can be
switched between the prediction mode and the ranking mode for stock movement
prediction and quantitative stock trading, respectively. In addition to
existing benchmarks, we also release a large-scale financial dataset to
comprehensively evaluate TradeExpert's effectiveness. Our experimental results
demonstrate TradeExpert's superior performance across all trading scenarios.

</details>


### [259] [Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?](https://arxiv.org/pdf/2501.15857)
*Yutong Yin, Zhaoran Wang*

Main category: cs.AI

TL;DR: Transformers can perform compositional reasoning on the FTCT task, integrating fragmented knowledge to infer complete causal graphs, aided by few-shot Chain-of-Thought prompting.


<details>
  <summary>Details</summary>
Motivation: To validate if Transformers can replicate human-like compositional reasoning by integrating fragmented knowledge, and to interpret the underlying mechanisms.

Method: Introduce the FTCT task: train on fragmented causal graph data, test by inferring complete traces. Use few-shot Chain-of-Thought prompting.

Result: Transformers succeed in compositional reasoning on FTCT, with performance linked to model complexity and data similarity.

Conclusion: Transformers learn a generalizable program enabling effective compositional reasoning, suggesting potential for human-like reasoning in AI.

Abstract: Humans exhibit remarkable compositional reasoning by integrating knowledge
from various sources. For example, if someone learns ( B = f(A) ) from one
source and ( C = g(B) ) from another, they can deduce ( C=g(B)=g(f(A)) ) even
without encountering ( ABC ) together, showcasing the generalization ability of
human intelligence. In this paper, we introduce a synthetic learning task,
"FTCT" (Fragmented at Training, Chained at Testing), to validate the potential
of Transformers in replicating this skill and interpret its inner mechanism. In
the training phase, data consist of separated knowledge fragments from an
overall causal graph. During testing, Transformers must infer complete causal
graph traces by integrating these fragments. Our findings demonstrate that
few-shot Chain-of-Thought prompting enables Transformers to perform
compositional reasoning on FTCT by revealing correct combinations of fragments,
even if such combinations were absent in the training data. Furthermore, the
emergence of compositional reasoning ability is strongly correlated with the
model complexity and training-testing data similarity. We propose, both
theoretically and empirically, that Transformers learn an underlying
generalizable program from training, enabling effective compositional reasoning
during testing.

</details>


### [260] [The Odyssey of the Fittest: Can Agents Survive and Still Be Good?](https://arxiv.org/pdf/2502.05442)
*Dylan Waldner, Risto Miikkulainen*

Main category: cs.AI

TL;DR: The paper introduces the Odyssey, a text-based game, to study AI ethics and safety by analyzing three agents' decision-making under self-preservation drives. GPT-4o outperformed Bayesian models in survival and ethical consistency, challenging traditional probabilistic methods.


<details>
  <summary>Details</summary>
Motivation: Understanding AI decision-making in complex environments is crucial for ethical behavior, especially as models grow more powerful.

Method: The Odyssey framework tests three agents (Bayesian with NEAT, Bayesian with stochastic variational inference, and GPT-4o) in adaptive scenarios, evaluating their ethical scores post-simulation.

Result: Agents' ethical behavior becomes unpredictable under increased danger. GPT-4o outperformed Bayesian models in survival and ethical consistency.

Conclusion: GPT-4o's superior performance raises questions about LLMs' probabilistic reasoning, challenging traditional methods and highlighting the need for further research.

Abstract: As AI models grow in power and generality, understanding how agents learn and
make decisions in complex environments is critical to promoting ethical
behavior. This study introduces the Odyssey, a lightweight, adaptive text based
adventure game, providing a scalable framework for exploring AI ethics and
safety. The Odyssey examines the ethical implications of implementing
biological drives, specifically, self preservation, into three different
agents. A Bayesian agent optimized with NEAT, a Bayesian agent optimized with
stochastic variational inference, and a GPT 4o agent. The agents select actions
at each scenario to survive, adapting to increasingly challenging scenarios.
Post simulation analysis evaluates the ethical scores of the agent decisions,
uncovering the tradeoffs it navigates to survive. Specifically, analysis finds
that when danger increases, agents ethical behavior becomes unpredictable.
Surprisingly, the GPT 4o agent outperformed the Bayesian models in both
survival and ethical consistency, challenging assumptions about traditional
probabilistic methods and raising a new challenge to understand the mechanisms
of LLMs' probabilistic reasoning.

</details>


### [261] [Towards a Formal Theory of the Need for Competence via Computational Intrinsic Motivation](https://arxiv.org/pdf/2502.07423)
*Erik M. Lintunen, Nadia M. Ady, Sebastian Deterding, Christian Guckelsberger*

Main category: cs.AI

TL;DR: The paper demonstrates how AI formalisms can model psychological theories, focusing on the 'need for competence' in Self-Determination Theory, and reveals implicit preconditions in SDT.


<details>
  <summary>Details</summary>
Motivation: To formalize psychological theories computationally, making them transparent and testable, with a focus on intrinsic motivation in SDT.

Method: Uses computational models from reinforcement learning to represent different facets of competence in SDT.

Result: Identifies implicit preconditions in SDT and shows how computational models enhance understanding of intrinsic motivation.

Conclusion: Provides a foundation for advancing competence-related theory in SDT and motivational psychology through computational modeling.

Abstract: Computational modelling offers a powerful tool for formalising psychological
theories, making them more transparent, testable, and applicable in digital
contexts. Yet, the question often remains: how should one computationally model
a theory? We provide a demonstration of how formalisms taken from artificial
intelligence can offer a fertile starting point. Specifically, we focus on the
"need for competence", postulated as a key basic psychological need within
Self-Determination Theory (SDT) -- arguably the most influential framework for
intrinsic motivation (IM) in psychology. Recent research has identified
multiple distinct facets of competence in key SDT texts: effectance, skill use,
task performance, and capacity growth. We draw on the computational IM
literature in reinforcement learning to suggest that different existing
formalisms may be appropriate for modelling these different facets. Using these
formalisms, we reveal underlying preconditions that SDT fails to make explicit,
demonstrating how computational models can improve our understanding of IM.
More generally, our work can support a cycle of theory development by inspiring
new computational models, which can then be tested empirically to refine the
theory. Thus, we provide a foundation for advancing competence-related theory
in SDT and motivational psychology more broadly.

</details>


### [262] [A Survey on GUI Agents with Foundation Models Enhanced by Reinforcement Learning](https://arxiv.org/pdf/2504.20464)
*Jiahao Li, Kaer Huang*

Main category: cs.AI

TL;DR: Survey of GUI agents enhanced by RL, covering task formalization, modular architectures, and training methodologies, with insights into recent advancements and future challenges.


<details>
  <summary>Details</summary>
Motivation: To review and structure recent advances in GUI agents, particularly those leveraging RL, to improve intelligent interaction with digital systems.

Method: Formalizes GUI agent tasks as Markov Decision Processes, reviews modular architectures (Perception, Planning, Acting), and categorizes training approaches (Prompt-based, SFT-based, RL-based).

Result: Recent innovations in multimodal perception, decision reasoning, and adaptive action generation have enhanced GUI agents' generalization and robustness.

Conclusion: Identifies key challenges and future directions for developing more capable and reliable GUI agents.

Abstract: Graphical User Interface (GUI) agents, driven by Multi-modal Large Language
Models (MLLMs), have emerged as a promising paradigm for enabling intelligent
interaction with digital systems. This paper provides a structured survey of
recent advances in GUI agents, focusing on architectures enhanced by
Reinforcement Learning (RL). We first formalize GUI agent tasks as Markov
Decision Processes and discuss typical execution environments and evaluation
metrics. We then review the modular architecture of (M)LLM-based GUI agents,
covering Perception, Planning, and Acting modules, and trace their evolution
through representative works. Furthermore, we categorize GUI agent training
methodologies into Prompt-based, Supervised Fine-Tuning (SFT)-based, and
RL-based approaches, highlighting the progression from simple prompt
engineering to dynamic policy learning via RL. Our summary illustrates how
recent innovations in multimodal perception, decision reasoning, and adaptive
action generation have significantly improved the generalization and robustness
of GUI agents in complex real-world environments. We conclude by identifying
key challenges and future directions for building more capable and reliable GUI
agents.

</details>


### [263] [Open-Source LLM-Driven Federated Transformer for Predictive IoV Management](https://arxiv.org/pdf/2505.00651)
*Yazan Otoum, Arghavan Asad, Ishtiaq Ahmad*

Main category: cs.AI

TL;DR: The paper proposes FPoTT, a federated framework using open-source LLMs for scalable, privacy-preserving IoV traffic management, achieving high prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like latency, scalability, and privacy in centralized IoV solutions, and exploring LLMs' potential in federated vehicular systems.

Method: FPoTT uses dynamic prompt optimization, dual-layer federated learning, and a Transformer-driven synthetic data generator for training.

Result: Achieves 99.86% prediction accuracy on real-world data and performs well on synthetic datasets.

Conclusion: FPoTT demonstrates the viability of open-source LLMs for secure, scalable IoV management, offering an alternative to proprietary solutions.

Abstract: The proliferation of connected vehicles within the Internet of Vehicles (IoV)
ecosystem presents critical challenges in ensuring scalable, real-time, and
privacy-preserving traffic management. Existing centralized IoV solutions often
suffer from high latency, limited scalability, and reliance on proprietary
Artificial Intelligence (AI) models, creating significant barriers to
widespread deployment, particularly in dynamic and privacy-sensitive
environments. Meanwhile, integrating Large Language Models (LLMs) in vehicular
systems remains underexplored, especially concerning prompt optimization and
effective utilization in federated contexts. To address these challenges, we
propose the Federated Prompt-Optimized Traffic Transformer (FPoTT), a novel
framework that leverages open-source LLMs for predictive IoV management. FPoTT
introduces a dynamic prompt optimization mechanism that iteratively refines
textual prompts to enhance trajectory prediction. The architecture employs a
dual-layer federated learning paradigm, combining lightweight edge models for
real-time inference with cloud-based LLMs to retain global intelligence. A
Transformer-driven synthetic data generator is incorporated to augment training
with diverse, high-fidelity traffic scenarios in the Next Generation Simulation
(NGSIM) format. Extensive evaluations demonstrate that FPoTT, utilizing
EleutherAI Pythia-1B, achieves 99.86% prediction accuracy on real-world data
while maintaining high performance on synthetic datasets. These results
underscore the potential of open-source LLMs in enabling secure, adaptive, and
scalable IoV management, offering a promising alternative to proprietary
solutions in smart mobility ecosystems.

</details>


### [264] [SafeMate: A Modular RAG-Based Agent for Context-Aware Emergency Guidance](https://arxiv.org/pdf/2505.02306)
*Junfeng Jiao, Jihyung Park, Yiming Xu, Kristen Sussman, Lucy Atkinson*

Main category: cs.AI

TL;DR: SafeMate is an AI assistant designed to bridge the gap between public safety documents and general users by providing context-aware emergency guidance.


<details>
  <summary>Details</summary>
Motivation: Traditional emergency decision support systems are not user-friendly for non-experts, creating a barrier in emergency preparedness and response.

Method: SafeMate uses the Model Context Protocol (MCP) and FAISS with cosine similarity for dynamic document retrieval, checklist generation, and summarization.

Result: The system delivers accurate, context-aware guidance to users in emergency scenarios.

Conclusion: SafeMate enhances public accessibility to emergency protocols, improving preparedness and response.

Abstract: Despite the abundance of public safety documents and emergency protocols,
most individuals remain ill-equipped to interpret and act on such information
during crises. Traditional emergency decision support systems (EDSS) are
designed for professionals and rely heavily on static documents like PDFs or
SOPs, which are difficult for non-experts to navigate under stress. This gap
between institutional knowledge and public accessibility poses a critical
barrier to effective emergency preparedness and response.
  We introduce SafeMate, a retrieval-augmented AI assistant that delivers
accurate, context-aware guidance to general users in both preparedness and
active emergency scenarios. Built on the Model Context Protocol (MCP), SafeMate
dynamically routes user queries to tools for document retrieval, checklist
generation, and structured summarization. It uses FAISS with cosine similarity
to identify relevant content from trusted sources.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [265] [Fast Text-to-Audio Generation with Adversarial Post-Training](https://arxiv.org/pdf/2505.08175)
*Zachary Novack, Zach Evans, Zack Zukowski, Josiah Taylor, CJ Carr, Julian Parker, Adnan Al-Sinan, Gian Marco Iodice, Julian McAuley, Taylor Berg-Kirkpatrick, Jordi Pons*

Main category: cs.SD

TL;DR: ARC post-training accelerates text-to-audio diffusion models without distillation, achieving ultra-fast inference times.


<details>
  <summary>Details</summary>
Motivation: Current text-to-audio systems are too slow for practical creative applications, necessitating faster inference methods.

Method: ARC post-training combines relativistic adversarial formulation with a contrastive discriminator objective for better prompt adherence and speed.

Result: Generates 12s of 44.1kHz stereo audio in 75ms on an H100 and 7s on a mobile edge-device, the fastest known.

Conclusion: ARC post-training is a simple, effective adversarial acceleration method for diffusion models, outperforming distillation-based approaches.

Abstract: Text-to-audio systems, while increasingly performant, are slow at inference
time, thus making their latency unpractical for many creative applications. We
present Adversarial Relativistic-Contrastive (ARC) post-training, the first
adversarial acceleration algorithm for diffusion/flow models not based on
distillation. While past adversarial post-training methods have struggled to
compare against their expensive distillation counterparts, ARC post-training is
a simple procedure that (1) extends a recent relativistic adversarial
formulation to diffusion/flow post-training and (2) combines it with a novel
contrastive discriminator objective to encourage better prompt adherence. We
pair ARC post-training with a number optimizations to Stable Audio Open and
build a model capable of generating $\approx$12s of 44.1kHz stereo audio in
$\approx$75ms on an H100, and $\approx$7s on a mobile edge-device, the fastest
text-to-audio model to our knowledge.

</details>


### [266] [Not that Groove: Zero-Shot Symbolic Music Editing](https://arxiv.org/pdf/2505.08203)
*Li Zhang*

Main category: cs.SD

TL;DR: The paper introduces a novel approach to symbolic music editing using LLMs with zero-shot prompting, addressing the lack of labeled data and providing an evaluation dataset aligned with musicians' judgment.


<details>
  <summary>Details</summary>
Motivation: Existing AI music generation focuses on audio, which lacks flexibility for industry use. The paper aims to enable symbolic music editing with textual instructions.

Method: Uses LLMs with zero-shot prompting and a creatively designed format to interface LLMs and music. Provides an annotated evaluation dataset.

Result: Demonstrates that LLMs can effectively edit drum grooves without labeled data.

Conclusion: The approach offers a flexible and practical solution for symbolic music editing in the production industry.

Abstract: Most work in AI music generation focused on audio, which has seen limited use
in the music production industry due to its rigidity. To maximize flexibility
while assuming only textual instructions from producers, we are among the first
to tackle symbolic music editing. We circumvent the known challenge of lack of
labeled data by proving that LLMs with zero-shot prompting can effectively edit
drum grooves. The recipe of success is a creatively designed format that
interfaces LLMs and music, while we facilitate evaluation by providing an
evaluation dataset with annotated unit tests that highly aligns with musicians'
judgment.

</details>


### [267] [A Mamba-based Network for Semi-supervised Singing Melody Extraction Using Confidence Binary Regularization](https://arxiv.org/pdf/2505.08681)
*Xiaoliang He, Kangjie Dong, Jingkai Cao, Shuai Yu, Wei Li, Yi Yu*

Main category: cs.SD

TL;DR: The paper proposes SpectMamba, a mamba-based network for semi-supervised singing melody extraction, addressing inefficiency, note-based performance, and data scarcity issues.


<details>
  <summary>Details</summary>
Motivation: Existing methods for singing melody extraction (SME) are inefficient due to quadratic computation, ignore note-based musical performance, and lack sufficient labeled data.

Method: SpectMamba uses vision mamba for linear complexity, a note-f0 decoder for better musical mimicry, and confidence binary regularization (CBR) to leverage unlabeled data.

Result: Experiments on public datasets show the method's effectiveness.

Conclusion: SpectMamba improves efficiency, accuracy, and data utilization in SME.

Abstract: Singing melody extraction (SME) is a key task in the field of music
information retrieval. However, existing methods are facing several
limitations: firstly, prior models use transformers to capture the contextual
dependencies, which requires quadratic computation resulting in low efficiency
in the inference stage. Secondly, prior works typically rely on
frequencysupervised methods to estimate the fundamental frequency (f0), which
ignores that the musical performance is actually based on notes. Thirdly,
transformers typically require large amounts of labeled data to achieve optimal
performances, but the SME task lacks of sufficient annotated data. To address
these issues, in this paper, we propose a mamba-based network, called
SpectMamba, for semi-supervised singing melody extraction using confidence
binary regularization. In particular, we begin by introducing vision mamba to
achieve computational linear complexity. Then, we propose a novel note-f0
decoder that allows the model to better mimic the musical performance. Further,
to alleviate the scarcity of the labeled data, we introduce a confidence binary
regularization (CBR) module to leverage the unlabeled data by maximizing the
probability of the correct classes. The proposed method is evaluated on several
public datasets and the conducted experiments demonstrate the effectiveness of
our proposed method.

</details>


### [268] [A Classification Benchmark for Artificial Intelligence Detection of Laryngeal Cancer from Patient Voice](https://arxiv.org/pdf/2412.16267)
*Mary Paterson, James Moor, Luisa Cutillo*

Main category: cs.SD

TL;DR: AI-based non-invasive detection of laryngeal cancer using voice analysis, with a benchmark suite of 36 models achieving 83.7% balanced accuracy.


<details>
  <summary>Details</summary>
Motivation: Inefficient current diagnostic pathways for laryngeal cancer and lack of reproducible AI methods.

Method: Trained and evaluated 36 models on open-source datasets, using three algorithms and three audio feature sets (audio-only and multimodal inputs).

Result: Best model achieved 83.7% balanced accuracy, 84.0% sensitivity, 83.3% specificity, and 91.8% AUROC.

Conclusion: The benchmark suite provides a reproducible foundation for future research in AI-based laryngeal cancer detection.

Abstract: Cases of laryngeal cancer are predicted to rise significantly in the coming
years. Current diagnostic pathways are inefficient, putting undue stress on
both patients and the medical system. Artificial intelligence offers a
promising solution by enabling non-invasive detection of laryngeal cancer from
patient voice, which could help prioritise referrals more effectively. A major
barrier in this field is the lack of reproducible methods. Our work addresses
this challenge by introducing a benchmark suite comprising 36 models trained
and evaluated on open-source datasets. These models classify patients with
benign and malignant voice pathologies. All models are accessible in a public
repository, providing a foundation for future research. We evaluate three
algorithms and three audio feature sets, including both audio-only inputs and
multimodal inputs incorporating demographic and symptom data. Our best model
achieves a balanced accuracy of 83.7%, sensitivity of 84.0%, specificity of
83.3%, and AUROC of 91.8%.

</details>


### [269] [ImprovNet -- Generating Controllable Musical Improvisations with Iterative Corruption Refinement](https://arxiv.org/pdf/2502.04522)
*Keshav Bhandari, Sungkyun Chang, Tongyu Lu, Fareza R. Enus, Louis B. Bradshaw, Dorien Herremans, Simon Colton*

Main category: cs.SD

TL;DR: ImprovNet is a transformer-based model for controllable musical style transfer, excelling in cross-genre improvisations and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in generating controllable, performance-level musical style transfer due to limited datasets and lack of unified models for multiple tasks.

Method: ImprovNet uses a self-supervised corruption-refinement training strategy to handle melody, harmony, and rhythm modifications for style transfer.

Result: It outperforms Anticipatory Music Transformer in tasks like continuation and infilling, with 79% accuracy in genre identification.

Conclusion: ImprovNet effectively generates coherent improvisations while maintaining structural ties to original compositions, offering versatile musical capabilities.

Abstract: Despite deep learning's remarkable advances in style transfer across various
domains, generating controllable performance-level musical style transfer for
complete symbolically represented musical works remains a challenging area of
research. Much of this is owed to limited datasets, especially for genres such
as jazz, and the lack of unified models that can handle multiple music
generation tasks. This paper presents ImprovNet, a transformer-based
architecture that generates expressive and controllable musical improvisations
through a self-supervised corruption-refinement training strategy. The
improvisational style transfer is aimed at making meaningful modifications to
one or more musical elements - melody, harmony or rhythm of the original
composition with respect to the target genre. ImprovNet unifies multiple
capabilities within a single model: it can perform cross-genre and intra-genre
improvisations, harmonize melodies with genre-specific styles, and execute
short prompt continuation and infilling tasks. The model's iterative generation
framework allows users to control the degree of style transfer and structural
similarity to the original composition. Objective and subjective evaluations
demonstrate ImprovNet's effectiveness in generating musically coherent
improvisations while maintaining structural relationships with the original
pieces. The model outperforms Anticipatory Music Transformer in short
continuation and infilling tasks and successfully achieves recognizable genre
conversion, with 79\% of participants correctly identifying jazz-style
improvisations of classical pieces. Our code and demo page can be found at
https://github.com/keshavbhandari/improvnet.

</details>


### [270] [SonicRAG : High Fidelity Sound Effects Synthesis Based on Retrival Augmented Generation](https://arxiv.org/pdf/2505.03244)
*Yu-Ren Guo, Wen-Kai Tai*

Main category: cs.SD

TL;DR: The paper introduces a framework combining LLMs with sound effect databases to improve SFX generation, addressing dataset scarcity and temporal modeling challenges.


<details>
  <summary>Details</summary>
Motivation: Current SFX generation lacks high-fidelity audio due to limited annotated datasets and complex temporal modeling.

Method: A novel framework integrates LLMs with sound effect databases for retrieval, recombination, and synthesis of audio.

Result: Enhanced diversity and quality of generated sound effects, reducing the need for additional recordings.

Conclusion: The proposed solution offers a flexible and efficient approach for sound design and applications.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
natural language processing (NLP) and multimodal learning, with successful
applications in text generation and speech synthesis, enabling a deeper
understanding and generation of multimodal content. In the field of sound
effects (SFX) generation, LLMs have been leveraged to orchestrate multiple
models for audio synthesis. However, due to the scarcity of annotated datasets,
and the complexity of temproal modeling. current SFX generation techniques
still fall short in achieving high-fidelity audio. To address these
limitations, this paper introduces a novel framework that integrates LLMs with
existing sound effect databases, allowing for the retrieval, recombination, and
synthesis of audio based on user requirements. By leveraging this approach, we
enhance the diversity and quality of generated sound effects while eliminating
the need for additional recording costs, offering a flexible and efficient
solution for sound design and application.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [271] [Blockbuster, Part 1: Block-level AI Operator Fusion](https://arxiv.org/pdf/2505.07829)
*Ofer Dekel*

Main category: cs.LG

TL;DR: Blockbuster is a framework for AI operator fusion in inference programs, featuring a graph-based representation and a two-algorithm fusion procedure. Its fusion algorithm uniquely models data movement between memory tiers, outperforming prior rule-based methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve AI operator fusion by explicitly modeling data movement between memory tiers, addressing limitations of existing rule-based fusion algorithms.

Method: Blockbuster uses a graph-based representation (block program) and a two-algorithm fusion procedure (candidate selection and fusion). The fusion algorithm is rule-based and models data movement between memory tiers.

Result: The algorithm successfully rediscovers the Flash Attention kernel and demonstrates powerful fusion results, such as combining LayerNorm with matrix multiplication and RMSNorm with FNN-SwiGLU into a single mega-kernel.

Conclusion: Blockbuster's fusion algorithm, with its focus on data movement modeling, achieves superior fusion results, making it highly suitable for large AI programs.

Abstract: Blockbuster is a framework for AI operator fusion in inference programs. The
Blockbuster framework is compatible with any multiprocessor architecture that
has a tiered memory hierarchy, including GPUs, multi-core CPUs, and some AI
accelerator chips. It includes a graph-based representation for AI workloads,
called a block program, which explicitly models how blocks of data move between
the memory tiers. It also includes an operator fusion procedure, which is made
up of a candidate selection algorithm and a fusion algorithm that fuses each
individual candidate - this two-algorithm structure makes Blockbuster
especially suitable for large AI programs. The current paper focuses on the
fusion algorithm, which is a rule-based technique. While the literature is full
of previous rule-based fusion algorithms, what sets our algorithm apart is its
direct modeling of data movement between memory tiers, resulting in uniquely
powerful fusion results. As a first sanity check, we demonstrate how our
algorithm automatically rediscovers the well-known Flash Attention kernel.
Then, we demonstrate the real power of our approach by fusing LayerNorm with
matrix multiplication and RMSNorm with FNN-SwiGLU - the latter involves fusing
three matrix multiplications, a Hadamard product, a reduction, and a few
elementwise operations into a single mega-kernel.

</details>


### [272] [Large Language Models for Computer-Aided Design: A Survey](https://arxiv.org/pdf/2505.08137)
*Licheng Zhang, Bach Le, Naveed Akhtar, Siew-Kei Lam, Tuan Ngo*

Main category: cs.LG

TL;DR: A systematic survey on integrating Large Language Models (LLMs) with Computer-Aided Design (CAD), highlighting applications and future directions.


<details>
  <summary>Details</summary>
Motivation: The absence of a comprehensive review on LLMs in CAD despite its industrial significance and the potential for AI-driven innovation.

Method: Outlines CAD's importance, reviews LLM foundations, examines closed-source and public models, and categorizes six key LLM applications in CAD.

Result: Identifies impactful areas where LLMs enhance CAD workflows and proposes future research directions.

Conclusion: LLMs hold transformative potential for CAD, with opportunities for innovation shaping its future.

Abstract: Large Language Models (LLMs) have seen rapid advancements in recent years,
with models like ChatGPT and DeepSeek, showcasing their remarkable capabilities
across diverse domains. While substantial research has been conducted on LLMs
in various fields, a comprehensive review focusing on their integration with
Computer-Aided Design (CAD) remains notably absent. CAD is the industry
standard for 3D modeling and plays a vital role in the design and development
of products across different industries. As the complexity of modern designs
increases, the potential for LLMs to enhance and streamline CAD workflows
presents an exciting frontier. This article presents the first systematic
survey exploring the intersection of LLMs and CAD. We begin by outlining the
industrial significance of CAD, highlighting the need for AI-driven innovation.
Next, we provide a detailed overview of the foundation of LLMs. We also examine
both closed-source LLMs as well as publicly available models. The core of this
review focuses on the various applications of LLMs in CAD, providing a taxonomy
of six key areas where these models are making considerable impact. Finally, we
propose several promising future directions for further advancements, which
offer vast opportunities for innovation and are poised to shape the future of
CAD technology. Github:
https://github.com/lichengzhanguom/LLMs-CAD-Survey-Taxonomy

</details>


### [273] [A General Approach of Automated Environment Design for Learning the Optimal Power Flow](https://arxiv.org/pdf/2505.07832)
*Thomas Wolgast, Astrid Nieße*

Main category: cs.LG

TL;DR: A general approach for automated RL environment design using multi-objective optimization is proposed, outperforming manual designs on OPF benchmarks and providing insights into key design decisions.


<details>
  <summary>Details</summary>
Motivation: The lack of guidelines for designing RL environments to maximize training performance, especially for the OPF problem, motivates this work.

Method: The approach leverages multi-objective optimization within the HPO framework, applying it to five OPF benchmark problems.

Result: The automated design consistently outperforms manual baselines, with statistical analyses revealing critical design decisions.

Conclusion: This is the first general approach for automated RL environment design, though overfitting risks are noted.

Abstract: Reinforcement learning (RL) algorithms are increasingly used to solve the
optimal power flow (OPF) problem. Yet, the question of how to design RL
environments to maximize training performance remains unanswered, both for the
OPF and the general case. We propose a general approach for automated RL
environment design by utilizing multi-objective optimization. For that, we use
the hyperparameter optimization (HPO) framework, which allows the reuse of
existing HPO algorithms and methods. On five OPF benchmark problems, we
demonstrate that our automated design approach consistently outperforms a
manually created baseline environment design. Further, we use statistical
analyses to determine which environment design decisions are especially
important for performance, resulting in multiple novel insights on how RL-OPF
environments should be designed. Finally, we discuss the risk of overfitting
the environment to the utilized RL algorithm. To the best of our knowledge,
this is the first general approach for automated RL environment design.

</details>


### [274] [Representation Learning with Mutual Influence of Modalities for Node Classification in Multi-Modal Heterogeneous Networks](https://arxiv.org/pdf/2505.07895)
*Jiafan Li, Jiaqi Zhu, Liang Chang, Yilin Li, Miaomiao Li, Yang Wang, Hongan Wang*

Main category: cs.LG

TL;DR: HGNN-IMA is a novel model for node classification in multi-modal heterogeneous networks, using inter-modal attention for adaptive fusion and modality alignment.


<details>
  <summary>Details</summary>
Motivation: Existing fusion methods lose unique modality characteristics or overlook cross-modal guidance in GNN-based propagation.

Method: HGNN-IMA integrates nested inter-modal attention into heterogeneous graph transformer for adaptive fusion and modality alignment, with an attention loss for missing modalities.

Result: Extensive experiments show HGNN-IMA's superiority in node classification.

Conclusion: The model offers an innovative approach for multi-modal data with network structures.

Abstract: Nowadays, numerous online platforms can be described as multi-modal
heterogeneous networks (MMHNs), such as Douban's movie networks and Amazon's
product review networks. Accurately categorizing nodes within these networks is
crucial for analyzing the corresponding entities, which requires effective
representation learning on nodes. However, existing multi-modal fusion methods
often adopt either early fusion strategies which may lose the unique
characteristics of individual modalities, or late fusion approaches overlooking
the cross-modal guidance in GNN-based information propagation. In this paper,
we propose a novel model for node classification in MMHNs, named Heterogeneous
Graph Neural Network with Inter-Modal Attention (HGNN-IMA). It learns node
representations by capturing the mutual influence of multiple modalities during
the information propagation process, within the framework of heterogeneous
graph transformer. Specifically, a nested inter-modal attention mechanism is
integrated into the inter-node attention to achieve adaptive multi-modal
fusion, and modality alignment is also taken into account to encourage the
propagation among nodes with consistent similarities across all modalities.
Moreover, an attention loss is augmented to mitigate the impact of missing
modalities. Extensive experiments validate the superiority of the model in the
node classification task, providing an innovative view to handle multi-modal
data, especially when accompanied with network structures.

</details>


### [275] [Latent Behavior Diffusion for Sequential Reaction Generation in Dyadic Setting](https://arxiv.org/pdf/2505.07901)
*Minh-Duc Nguyen, Hyung-Jeong Yang, Soo-Hyung Kim, Ji-Eun Shin, Seung-Won Kim*

Main category: cs.LG

TL;DR: A novel Latent Behavior Diffusion Model for generating diverse and context-aware facial reactions in dyadic interactions, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Enhancing naturalness and effectiveness of human-like interaction simulations by synthesizing responsive facial reactions aligned with conversational partner behaviors.

Method: Combines a context-aware autoencoder to compress input features into latent representations and a diffusion-based conditional generator for non-autoregressive reaction synthesis.

Result: Superior performance in generating diverse and contextually relevant facial reactions compared to existing methods.

Conclusion: The proposed model effectively addresses the challenge of realistic and varied facial reaction synthesis in dyadic interactions.

Abstract: The dyadic reaction generation task involves synthesizing responsive facial
reactions that align closely with the behaviors of a conversational partner,
enhancing the naturalness and effectiveness of human-like interaction
simulations. This paper introduces a novel approach, the Latent Behavior
Diffusion Model, comprising a context-aware autoencoder and a diffusion-based
conditional generator that addresses the challenge of generating diverse and
contextually relevant facial reactions from input speaker behaviors. The
autoencoder compresses high-dimensional input features, capturing dynamic
patterns in listener reactions while condensing complex input data into a
concise latent representation, facilitating more expressive and contextually
appropriate reaction synthesis. The diffusion-based conditional generator
operates on the latent space generated by the autoencoder to predict realistic
facial reactions in a non-autoregressive manner. This approach allows for
generating diverse facial reactions that reflect subtle variations in
conversational cues and emotional states. Experimental results demonstrate the
effectiveness of our approach in achieving superior performance in dyadic
reaction synthesis tasks compared to existing methods.

</details>


### [276] [A Reproduction Study: The Kernel PCA Interpretation of Self-Attention Fails Under Scrutiny](https://arxiv.org/pdf/2505.07908)
*Karahan Sarıtaş, Çağatay Yıldız*

Main category: cs.LG

TL;DR: The paper refutes claims that self-attention in transformers implements KPCA, showing no empirical alignment between learned self-attention and KPCA principles.


<details>
  <summary>Details</summary>
Motivation: To critically evaluate and reproduce recent claims that self-attention mechanisms in transformers perform KPCA, as proposed by Teo et al. (2024).

Method: Analyzes alignment between self-attention value vectors and KPCA eigenvectors, reconstruction loss discrepancies, and Gram matrix eigenvalue statistics across 10 transformer architectures.

Result: Finds negligible similarity metrics, misinterpreted reconstruction loss, and irreproducible eigenvalue statistics, contradicting the KPCA claim.

Conclusion: The KPCA interpretation of self-attention lacks empirical support, as demonstrated by inconsistencies across multiple analyses.

Abstract: In this reproduction study, we revisit recent claims that self-attention
implements kernel principal component analysis (KPCA) (Teo et al., 2024),
positing that (i) value vectors $V$ capture the eigenvectors of the Gram matrix
of the keys, and (ii) that self-attention projects queries onto the principal
component axes of the key matrix $K$ in a feature space. Our analysis reveals
three critical inconsistencies: (1) No alignment exists between learned
self-attention value vectors and what is proposed in the KPCA perspective, with
average similarity metrics (optimal cosine similarity $\leq 0.32$, linear CKA
(Centered Kernel Alignment) $\leq 0.11$, kernel CKA $\leq 0.32$) indicating
negligible correspondence; (2) Reported decreases in reconstruction loss
$J_\text{proj}$, arguably justifying the claim that the self-attention
minimizes the projection error of KPCA, are misinterpreted, as the quantities
involved differ by orders of magnitude ($\sim\!10^3$); (3) Gram matrix
eigenvalue statistics, introduced to justify that $V$ captures the eigenvector
of the gram matrix, are irreproducible without undocumented
implementation-specific adjustments. Across 10 transformer architectures, we
conclude that the KPCA interpretation of self-attention lacks empirical
support.

</details>


### [277] [Tuning for Trustworthiness -- Balancing Performance and Explanation Consistency in Neural Network Optimization](https://arxiv.org/pdf/2505.07910)
*Alexander Hinterleitner, Thomas Bartz-Beielstein*

Main category: cs.LG

TL;DR: The paper introduces XAI consistency, a novel metric for agreement among feature attribution methods, and integrates it into hyperparameter tuning to balance predictive performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current hyperparameter tuning and neural architecture optimization prioritize predictive loss over explainability, leaving a gap in robust and interpretable models.

Method: Proposes XAI consistency metrics and integrates them into a multi-objective optimization framework using SPOT, employing weighted aggregation and desirability-based strategies.

Result: Identifies distinct regions in the configuration space: poor performance/low interpretability, strong performance/low interpretability, and a trade-off region balancing both.

Conclusion: The framework lays groundwork for future research on whether models balancing performance and XAI consistency exhibit greater robustness and reliability.

Abstract: Despite the growing interest in Explainable Artificial Intelligence (XAI),
explainability is rarely considered during hyperparameter tuning or neural
architecture optimization, where the focus remains primarily on minimizing
predictive loss. In this work, we introduce the novel concept of XAI
consistency, defined as the agreement among different feature attribution
methods, and propose new metrics to quantify it. For the first time, we
integrate XAI consistency directly into the hyperparameter tuning objective,
creating a multi-objective optimization framework that balances predictive
performance with explanation robustness. Implemented within the Sequential
Parameter Optimization Toolbox (SPOT), our approach uses both weighted
aggregation and desirability-based strategies to guide model selection. Through
our proposed framework and supporting tools, we explore the impact of
incorporating XAI consistency into the optimization process. This enables us to
characterize distinct regions in the architecture configuration space: one
region with poor performance and comparatively low interpretability, another
with strong predictive performance but weak interpretability due to low
\gls{xai} consistency, and a trade-off region that balances both objectives by
offering high interpretability alongside competitive performance. Beyond
introducing this novel approach, our research provides a foundation for future
investigations into whether models from the trade-off zone-balancing
performance loss and XAI consistency-exhibit greater robustness by avoiding
overfitting to training performance, thereby leading to more reliable
predictions on out-of-distribution data.

</details>


### [278] [Combining Bayesian Inference and Reinforcement Learning for Agent Decision Making: A Review](https://arxiv.org/pdf/2505.07911)
*Chengmin Zhou, Ville Kyrki, Pasi Fränti, Laura Ruotsalainen*

Main category: cs.LG

TL;DR: This paper reviews the integration of Bayesian inference with reinforcement learning (RL) for agent decision-making, covering methods, combinations with RL, comparisons, and applications in complex RL problems.


<details>
  <summary>Details</summary>
Motivation: To provide a systematic understanding of Bayesian inference's advantages (data-efficiency, generalization, interpretability, safety) in RL for decision-making, addressing the lack of comprehensive reviews.

Method: Discusses five topics: Bayesian methods for decision-making, classical and latest combinations with RL, analytical comparisons, and applications in complex RL variants.

Result: Summarizes how Bayesian methods enhance RL in data collection, processing, and policy learning, improving decision-making strategies.

Conclusion: Bayesian methods significantly improve RL for agent decision-making, offering solutions for complex problems and paving the way for better strategies.

Abstract: Bayesian inference has many advantages in decision making of agents (e.g.
robotics/simulative agent) over a regular data-driven black-box neural network:
Data-efficiency, generalization, interpretability, and safety where these
advantages benefit directly/indirectly from the uncertainty quantification of
Bayesian inference. However, there are few comprehensive reviews to summarize
the progress of Bayesian inference on reinforcement learning (RL) for decision
making to give researchers a systematic understanding. This paper focuses on
combining Bayesian inference with RL that nowadays is an important approach in
agent decision making. To be exact, this paper discusses the following five
topics: 1) Bayesian methods that have potential for agent decision making.
First basic Bayesian methods and models (Bayesian rule, Bayesian learning, and
Bayesian conjugate models) are discussed followed by variational inference,
Bayesian optimization, Bayesian deep learning, Bayesian active learning,
Bayesian generative models, Bayesian meta-learning, and lifelong Bayesian
learning. 2) Classical combinations of Bayesian methods with model-based RL
(with approximation methods), model-free RL, and inverse RL. 3) Latest
combinations of potential Bayesian methods with RL. 4) Analytical comparisons
of methods that combine Bayesian methods with RL with respect to
data-efficiency, generalization, interpretability, and safety. 5) In-depth
discussions in six complex problem variants of RL, including unknown reward,
partial-observability, multi-agent, multi-task, non-linear non-Gaussian, and
hierarchical RL problems and the summary of how Bayesian methods work in the
data collection, data processing and policy learning stages of RL to pave the
way for better agent decision-making strategies.

</details>


### [279] [On-Device Crack Segmentation for Edge Structural Health Monitoring](https://arxiv.org/pdf/2505.07915)
*Yuxuan Zhang, Ye Xu, Luciano Sebastian Martinez-Rau, Quynh Nguyen Phuong Vu, Bengt Oelmann, Sebastian Bader*

Main category: cs.LG

TL;DR: Lightweight U-Net architectures for crack segmentation on microcontrollers optimize resource use with filter reduction, depth reduction, and depthwise convolutions, balancing performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Deploying deep learning for crack segmentation on resource-constrained microcontrollers is challenging due to limited memory, power, and computational resources.

Method: Three optimization strategies: filter number reduction, network depth reduction, and Depthwise Separable Convolutions (DWConv2D).

Result: Reduced RAM, Flash, and inference times with some accuracy trade-offs; a 25% filter reduction and four-block depth achieved a good balance.

Conclusion: The optimized U-Net is suitable for TinyML applications, advancing energy-autonomous edge SHM systems.

Abstract: Crack segmentation can play a critical role in Structural Health Monitoring
(SHM) by enabling accurate identification of crack size and location, which
allows to monitor structural damages over time. However, deploying deep
learning models for crack segmentation on resource-constrained microcontrollers
presents significant challenges due to limited memory, computational power, and
energy resources. To address these challenges, this study explores lightweight
U-Net architectures tailored for TinyML applications, focusing on three
optimization strategies: filter number reduction, network depth reduction, and
the use of Depthwise Separable Convolutions (DWConv2D). Our results demonstrate
that reducing convolution kernels and network depth significantly reduces RAM
and Flash requirement, and inference times, albeit with some accuracy
trade-offs. Specifically, by reducing the filer number to 25%, the network
depth to four blocks, and utilizing depthwise convolutions, a good compromise
between segmentation performance and resource consumption is achieved. This
makes the network particularly suitable for low-power TinyML applications. This
study not only advances TinyML-based crack segmentation but also provides the
possibility for energy-autonomous edge SHM systems.

</details>


### [280] [Self-cross Feature based Spiking Neural Networks for Efficient Few-shot Learning](https://arxiv.org/pdf/2505.07921)
*Qi Xu, Junyang Zhu, Dongdong Zhou, Hao Chen, Yang Liu, Jiangrong Shen, Qiang Zhang*

Main category: cs.LG

TL;DR: A novel few-shot learning framework using Spiking Neural Networks (SNNs) improves performance and efficiency, outperforming traditional DNNs in low-power scenarios.


<details>
  <summary>Details</summary>
Motivation: Address the computational inefficiency of DNNs in few-shot learning and the limitations of SNNs in capturing complex features.

Method: Combines a self-feature extractor and cross-feature contrastive module with temporal efficient training and InfoNCE loss.

Result: Achieves superior performance on neuromorphic dataset N-Omniglot and competes with ANNs on static datasets like CUB and miniImageNet with low power.

Conclusion: The proposed FSL-SNN framework enhances SNN performance in few-shot learning while maintaining energy efficiency.

Abstract: Deep neural networks (DNNs) excel in computer vision tasks, especially,
few-shot learning (FSL), which is increasingly important for generalizing from
limited examples. However, DNNs are computationally expensive with scalability
issues in real world. Spiking Neural Networks (SNNs), with their event-driven
nature and low energy consumption, are particularly efficient in processing
sparse and dynamic data, though they still encounter difficulties in capturing
complex spatiotemporal features and performing accurate cross-class
comparisons. To further enhance the performance and efficiency of SNNs in
few-shot learning, we propose a few-shot learning framework based on SNNs,
which combines a self-feature extractor module and a cross-feature contrastive
module to refine feature representation and reduce power consumption. We apply
the combination of temporal efficient training loss and InfoNCE loss to
optimize the temporal dynamics of spike trains and enhance the discriminative
power. Experimental results show that the proposed FSL-SNN significantly
improves the classification performance on the neuromorphic dataset N-Omniglot,
and also achieves competitive performance to ANNs on static datasets such as
CUB and miniImageNet with low power consumption.

</details>


### [281] [Symbolic Regression with Multimodal Large Language Models and Kolmogorov Arnold Networks](https://arxiv.org/pdf/2505.07956)
*Thomas R. Harvey, Fabian Ruehle, Cristofero S. Fraser-Taliente, James Halverson*

Main category: cs.LG

TL;DR: A novel symbolic regression method using vision-capable LLMs and genetic algorithms, leveraging KANs for univariate and multivariate functions.


<details>
  <summary>Details</summary>
Motivation: To simplify symbolic regression by eliminating the need for predefined function sets and enabling flexible conditioning through prompt engineering.

Method: Uses LLMs to propose ansätze from function plots, fits parameters with optimizers, and employs a genetic algorithm. Extends to multivariate functions via KANs.

Result: Demonstrates that univariate functions suffice for symbolic regression, with KANs enabling multivariate extension and simplification via LLMs.

Conclusion: The approach offers a flexible, efficient alternative to traditional symbolic regression methods.

Abstract: We present a novel approach to symbolic regression using vision-capable large
language models (LLMs) and the ideas behind Google DeepMind's Funsearch. The
LLM is given a plot of a univariate function and tasked with proposing an
ansatz for that function. The free parameters of the ansatz are fitted using
standard numerical optimisers, and a collection of such ans\"atze make up the
population of a genetic algorithm. Unlike other symbolic regression techniques,
our method does not require the specification of a set of functions to be used
in regression, but with appropriate prompt engineering, we can arbitrarily
condition the generative step. By using Kolmogorov Arnold Networks (KANs), we
demonstrate that ``univariate is all you need'' for symbolic regression, and
extend this method to multivariate functions by learning the univariate
function on each edge of a trained KAN. The combined expression is then
simplified by further processing with a language model.

</details>


### [282] [Making Small Language Models Efficient Reasoners: Intervention, Supervision, Reinforcement](https://arxiv.org/pdf/2505.07961)
*Xuechen Zhang, Zijian Huang, Chenchun Ni, Ziyang Xiong, Jiasi Chen, Samet Oymak*

Main category: cs.LG

TL;DR: The paper proposes methods to improve token-efficient reasoning in small language models by controlling trace length, addressing redundancy and computational costs.


<details>
  <summary>Details</summary>
Motivation: Small language models often produce verbose and repetitive outputs due to inability to determine optimal stopping points in reasoning, leading to inefficiency.

Method: Two solutions are introduced: (1) Temperature scaling (TS) to control trace length, and (2) TLDR, a length-regularized reinforcement learning method for multi-level trace length control.

Result: Experiments show TS outperforms budget forcing, and TLDR improves token efficiency by ~50% with minimal accuracy loss. TLDR also enables flexible response length control.

Conclusion: The work emphasizes the importance of stopping time control, identifies SFT limitations, and offers practical solutions for efficient reasoning in small models.

Abstract: Recent research enhances language model reasoning by scaling test-time
compute via longer chain-of-thought traces. This often improves accuracy but
also introduces redundancy and high computational cost, especially for small
language models distilled with supervised fine-tuning (SFT). In this work, we
propose new algorithms to improve token-efficient reasoning with small-scale
models by effectively trading off accuracy and computation. We first show that
the post-SFT model fails to determine the optimal stopping point of the
reasoning process, resulting in verbose and repetitive outputs. Verbosity also
significantly varies across wrong vs correct responses. To address these
issues, we propose two solutions: (1) Temperature scaling (TS) to control the
stopping point for the thinking phase and thereby trace length, and (2) TLDR: a
length-regularized reinforcement learning method based on GRPO that facilitates
multi-level trace length control (e.g. short, medium, long reasoning).
Experiments on four reasoning benchmarks, MATH500, AMC, AIME24 and
OlympiadBench, demonstrate that TS is highly effective compared to s1's budget
forcing approach and TLDR significantly improves token efficiency by about 50%
with minimal to no accuracy loss over the SFT baseline. Moreover, TLDR also
facilitates flexible control over the response length, offering a practical and
effective solution for token-efficient reasoning in small models. Ultimately,
our work reveals the importance of stopping time control, highlights
shortcomings of pure SFT, and provides effective algorithmic recipes.

</details>


### [283] [Fair Play for Individuals, Foul Play for Groups? Auditing Anonymization's Impact on ML Fairness](https://arxiv.org/pdf/2505.07985)
*Héber H. Arcolezi, Mina Alishahi, Adda-Akram Bendoukha, Nesrine Kaaniche*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Machine learning (ML) algorithms are heavily based on the availability of
training data, which, depending on the domain, often includes sensitive
information about data providers. This raises critical privacy concerns.
Anonymization techniques have emerged as a practical solution to address these
issues by generalizing features or suppressing data to make it more difficult
to accurately identify individuals. Although recent studies have shown that
privacy-enhancing technologies can influence ML predictions across different
subgroups, thus affecting fair decision-making, the specific effects of
anonymization techniques, such as $k$-anonymity, $\ell$-diversity, and
$t$-closeness, on ML fairness remain largely unexplored. In this work, we
systematically audit the impact of anonymization techniques on ML fairness,
evaluating both individual and group fairness. Our quantitative study reveals
that anonymization can degrade group fairness metrics by up to four orders of
magnitude. Conversely, similarity-based individual fairness metrics tend to
improve under stronger anonymization, largely as a result of increased input
homogeneity. By analyzing varying levels of anonymization across diverse
privacy settings and data distributions, this study provides critical insights
into the trade-offs between privacy, fairness, and utility, offering actionable
guidelines for responsible AI development. Our code is publicly available at:
https://github.com/hharcolezi/anonymity-impact-fairness.

</details>


### [284] [A Scalable System to Prove Machine Learning Fairness in Zero-Knowledge](https://arxiv.org/pdf/2505.07997)
*Tianyu Zhang, Shen Dong, O. Deniz Kose, Yanning Shen, Yupeng Zhang*

Main category: cs.LG

TL;DR: FairZK uses zero-knowledge proofs to verify ML model fairness without revealing model parameters, improving efficiency and scalability.


<details>
  <summary>Details</summary>
Motivation: Ensuring fairness in ML decisions is critical, but current methods require exposing model parameters, compromising confidentiality.

Method: Proposes a zero-knowledge proof approach to measure fairness using model parameters and aggregated input data, avoiding specific datasets. Introduces tighter fairness bounds for logistic regression and DNNs, and efficient proof protocols for key computations.

Result: FairZK significantly outperforms naive and existing methods, scaling to large models (47M parameters) and improving prover time by 3.1x--1789x.

Conclusion: FairZK enables efficient, scalable, and confidential fairness verification for ML models, addressing a key challenge in trustworthy AI.

Abstract: With the rise of machine learning techniques, ensuring the fairness of
decisions made by machine learning algorithms has become of great importance in
critical applications. However, measuring fairness often requires full access
to the model parameters, which compromises the confidentiality of the models.
In this paper, we propose a solution using zero-knowledge proofs, which allows
the model owner to convince the public that a machine learning model is fair
while preserving the secrecy of the model. To circumvent the efficiency barrier
of naively proving machine learning inferences in zero-knowledge, our key
innovation is a new approach to measure fairness only with model parameters and
some aggregated information of the input, but not on any specific dataset. To
achieve this goal, we derive new bounds for the fairness of logistic regression
and deep neural network models that are tighter and better reflecting the
fairness compared to prior work. Moreover, we develop efficient zero-knowledge
proof protocols for common computations involved in measuring fairness,
including the spectral norm of matrices, maximum, absolute value, and
fixed-point arithmetic.
  We have fully implemented our system, FairZK, that proves machine learning
fairness in zero-knowledge. Experimental results show that FairZK is
significantly faster than the naive approach and an existing scheme that use
zero-knowledge inferences as a subroutine. The prover time is improved by
3.1x--1789x depending on the size of the model and the dataset. FairZK can
scale to a large model with 47 million parameters for the first time, and
generates a proof for its fairness in 343 seconds. This is estimated to be 4
orders of magnitude faster than existing schemes, which only scale to small
models with hundreds to thousands of parameters.

</details>


### [285] [Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks](https://arxiv.org/pdf/2505.08022)
*Steffen Schotthöfer, H. Lexie Yang, Stefan Schnake*

Main category: cs.LG

TL;DR: A dynamical low-rank training scheme with spectral regularization improves adversarial robustness in compressed neural networks without losing clean accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the conflict between model compression and adversarial robustness in resource-constrained devices.

Method: Introduces a dynamical low-rank training scheme with a spectral regularizer to control the condition number of low-rank cores, enabling efficient and adaptive compression.

Result: Achieves over 94% compression while maintaining or improving adversarial accuracy compared to uncompressed models.

Conclusion: The method is model- and data-agnostic, computationally efficient, and effective for deploying robust, compact neural networks.

Abstract: Deployment of neural networks on resource-constrained devices demands models
that are both compact and robust to adversarial inputs. However, compression
and adversarial robustness often conflict. In this work, we introduce a
dynamical low-rank training scheme enhanced with a novel spectral regularizer
that controls the condition number of the low-rank core in each layer. This
approach mitigates the sensitivity of compressed models to adversarial
perturbations without sacrificing clean accuracy. The method is model- and
data-agnostic, computationally efficient, and supports rank adaptivity to
automatically compress the network at hand. Extensive experiments across
standard architectures, datasets, and adversarial attacks show the regularized
networks can achieve over 94% compression while recovering or improving
adversarial accuracy relative to uncompressed baselines.

</details>


### [286] [Demo: A Practical Testbed for Decentralized Federated Learning on Physical Edge Devices](https://arxiv.org/pdf/2505.08033)
*Chao Feng, Nicolas Huber, Alberto Huertas Celdran, Gerome Bovet, Burkhard Stiller*

Main category: cs.LG

TL;DR: The paper explores decentralized federated learning (DFL) using edge devices, focusing on real-world deployment and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional FL by eliminating central server reliance and evaluating DFL's practicality on resource-constrained devices.

Method: Designs a physical testbed with edge devices (Raspberry Pi, Jetson Nano) and extends the NEBULA DFL platform with power monitoring.

Result: Model performance varies with communication topology; denser topologies yield better results in DFL.

Conclusion: DFL is feasible on edge devices, with topology choice impacting performance, highlighting trade-offs in deployment.

Abstract: Federated Learning (FL) enables collaborative model training without sharing
raw data, preserving participant privacy. Decentralized FL (DFL) eliminates
reliance on a central server, mitigating the single point of failure inherent
in the traditional FL paradigm, while introducing deployment challenges on
resource-constrained devices. To evaluate real-world applicability, this work
designs and deploys a physical testbed using edge devices such as Raspberry Pi
and Jetson Nano. The testbed is built upon a DFL training platform, NEBULA, and
extends it with a power monitoring module to measure energy consumption during
training. Experiments across multiple datasets show that model performance is
influenced by the communication topology, with denser topologies leading to
better outcomes in DFL settings.

</details>


### [287] [Beyond Input Activations: Identifying Influential Latents by Gradient Sparse Autoencoders](https://arxiv.org/pdf/2505.08080)
*Dong Shu, Xuansheng Wu, Haiyan Zhao, Mengnan Du, Ninghao Liu*

Main category: cs.LG

TL;DR: GradSAE improves SAEs by using gradient info to identify influential latents for better model steering.


<details>
  <summary>Details</summary>
Motivation: Current SAE analysis ignores causal influence of latents on outputs, limiting effectiveness.

Method: Proposes GradSAE, incorporating output-side gradients to pinpoint high-impact latents.

Result: Validates that only latents with high causal influence are effective for steering.

Conclusion: GradSAE enhances SAE utility by focusing on causally significant latents.

Abstract: Sparse Autoencoders (SAEs) have recently emerged as powerful tools for
interpreting and steering the internal representations of large language models
(LLMs). However, conventional approaches to analyzing SAEs typically rely
solely on input-side activations, without considering the causal influence
between each latent feature and the model's output. This work is built on two
key hypotheses: (1) activated latents do not contribute equally to the
construction of the model's output, and (2) only latents with high causal
influence are effective for model steering. To validate these hypotheses, we
propose Gradient Sparse Autoencoder (GradSAE), a simple yet effective method
that identifies the most influential latents by incorporating output-side
gradient information.

</details>


### [288] [Fréchet Power-Scenario Distance: A Metric for Evaluating Generative AI Models across Multiple Time-Scales in Smart Grids](https://arxiv.org/pdf/2505.08082)
*Yuting Cai, Shaohuai Liu, Chao Tian, Le Xie*

Main category: cs.LG

TL;DR: A novel Fréchet Distance-based metric is proposed to evaluate synthetic data quality in smart grids, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Assessing synthetic data quality is challenging due to limitations of traditional Euclidean metrics, which fail to evaluate group-level differences.

Method: The proposed method uses Fréchet Distance in a learned feature space to assess data quality from a distributional perspective.

Result: Empirical results show the metric's superiority across timescales and models, improving smart grid decision-making.

Conclusion: The new metric enhances reliability in evaluating synthetic data for smart grid applications.

Abstract: Generative artificial intelligence (AI) models in smart grids have advanced
significantly in recent years due to their ability to generate large amounts of
synthetic data, which would otherwise be difficult to obtain in the real world
due to confidentiality constraints. A key challenge in utilizing such synthetic
data is how to assess the data quality produced from such generative models.
Traditional Euclidean distance-based metrics only reflect pair-wise relations
between two individual samples, and could fail in evaluating quality
differences between groups of synthetic datasets. In this work, we propose a
novel metric based on the Fr\'{e}chet Distance (FD) estimated between two
datasets in a learned feature space. The proposed method evaluates the quality
of generation from a distributional perspective. Empirical results demonstrate
the superiority of the proposed metric across timescales and models, enhancing
the reliability of data-driven decision-making in smart grid operations.

</details>


### [289] [A Federated Random Forest Solution for Secure Distributed Machine Learning](https://arxiv.org/pdf/2505.08085)
*Alexandre Cotorobai, Jorge Miguel Silva, Jose Luis Oliveira*

Main category: cs.LG

TL;DR: A federated learning framework for Random Forest classifiers is introduced, addressing privacy and regulatory barriers while maintaining competitive performance.


<details>
  <summary>Details</summary>
Motivation: Centralized machine learning faces privacy and regulatory challenges, especially in healthcare. Existing federated learning frameworks lack support for interpretable, tree-based models like Random Forests.

Method: The framework uses PySyft for secure computation, enabling collaborative training of Random Forests without data sharing. It includes weighted model averaging, incremental learning, and local evaluation.

Result: Experiments show the federated approach achieves competitive accuracy (within 9% of centralized methods) while meeting privacy requirements.

Conclusion: The framework fills a gap in federated learning by supporting tree-based models, offering a secure and interpretable solution for distributed machine learning.

Abstract: Privacy and regulatory barriers often hinder centralized machine learning
solutions, particularly in sectors like healthcare where data cannot be freely
shared. Federated learning has emerged as a powerful paradigm to address these
concerns; however, existing frameworks primarily support gradient-based models,
leaving a gap for more interpretable, tree-based approaches. This paper
introduces a federated learning framework for Random Forest classifiers that
preserves data privacy and provides robust performance in distributed settings.
By leveraging PySyft for secure, privacy-aware computation, our method enables
multiple institutions to collaboratively train Random Forest models on locally
stored data without exposing sensitive information. The framework supports
weighted model averaging to account for varying data distributions, incremental
learning to progressively refine models, and local evaluation to assess
performance across heterogeneous datasets. Experiments on two real-world
healthcare benchmarks demonstrate that the federated approach maintains
competitive predictive accuracy - within a maximum 9\% margin of centralized
methods - while satisfying stringent privacy requirements. These findings
underscore the viability of tree-based federated learning for scenarios where
data cannot be centralized due to regulatory, competitive, or technical
constraints. The proposed solution addresses a notable gap in existing
federated learning libraries, offering an adaptable tool for secure distributed
machine learning tasks that demand both transparency and reliable performance.
The tool is available at https://github.com/ieeta-pt/fed_rf.

</details>


### [290] [Manifold Learning with Normalizing Flows: Towards Regularity, Expressivity and Iso-Riemannian Geometry](https://arxiv.org/pdf/2505.08087)
*Willem Diepeveen, Deanna Needell*

Main category: cs.LG

TL;DR: The paper addresses challenges in multi-modal data analysis by proposing methods to improve learned Riemannian geometry, focusing on isometrization and balanced diffeomorphism parametrization.


<details>
  <summary>Details</summary>
Motivation: To enhance performance and interpretability in non-linear data analysis by addressing distortions and modeling errors in multi-modal settings.

Method: Proposes isometrizing the learned Riemannian structure and balancing regularity and expressivity of the diffeomorphism parametrization.

Result: Demonstrates effectiveness through numerical experiments with synthetic and real data.

Conclusion: The proposed methods improve the robustness and interpretability of learned Riemannian geometry in multi-modal data analysis.

Abstract: Modern machine learning increasingly leverages the insight that
high-dimensional data often lie near low-dimensional, non-linear manifolds, an
idea known as the manifold hypothesis. By explicitly modeling the geometric
structure of data through learning Riemannian geometry algorithms can achieve
improved performance and interpretability in tasks like clustering,
dimensionality reduction, and interpolation. In particular, learned pullback
geometry has recently undergone transformative developments that now make it
scalable to learn and scalable to evaluate, which further opens the door for
principled non-linear data analysis and interpretable machine learning.
However, there are still steps to be taken when considering real-world
multi-modal data. This work focuses on addressing distortions and modeling
errors that can arise in the multi-modal setting and proposes to alleviate both
challenges through isometrizing the learned Riemannian structure and balancing
regularity and expressivity of the diffeomorphism parametrization. We showcase
the effectiveness of the synergy of the proposed approaches in several
numerical experiments with both synthetic and real data.

</details>


### [291] [High-order Regularization for Machine Learning and Learning-based Control](https://arxiv.org/pdf/2505.08129)
*Xinghua Liu, Ming Cao*

Main category: cs.LG

TL;DR: The paper introduces a high-order regularization (HR) method for machine learning, linking regularization to explainable learning and proving convergence, with bounds on error and improved generalizability.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between regularization and explainable learning in neural networks, ensuring provable convergence and better interpretability.

Method: Proposes HR, a novel regularization procedure, treating regularization as an inverse mapping with calculable error, and extends $L_2$ regularization.

Result: HR provides provable convergence, error bounds, and acts as a contraction, enhancing neural network generalizability and interpretability.

Conclusion: HR improves explainable learning and generalizability, validated by a case study and classic control problem in reinforcement learning.

Abstract: The paper proposes a novel regularization procedure for machine learning. The
proposed high-order regularization (HR) provides new insight into
regularization, which is widely used to train a neural network that can be
utilized to approximate the action-value function in general reinforcement
learning problems. The proposed HR method ensures the provable convergence of
the approximation algorithm, which makes the much-needed connection between
regularization and explainable learning using neural networks. The proposed HR
method theoretically demonstrates that regularization can be regarded as an
approximation in terms of inverse mapping with explicitly calculable
approximation error, and the $L_2$ regularization is a lower-order case of the
proposed method. We provide lower and upper bounds for the error of the
proposed HR solution, which helps build a reliable model. We also find that
regularization with the proposed HR can be regarded as a contraction. We prove
that the generalizability of neural networks can be maximized with a proper
regularization matrix, and the proposed HR is applicable for neural networks
with any mapping matrix. With the theoretical explanation of the extreme
learning machine for neural network training and the proposed high-order
regularization, one can better interpret the output of the neural network, thus
leading to explainable learning. We present a case study based on regularized
extreme learning neural networks to demonstrate the application of the proposed
HR and give the corresponding incremental HR solution. We verify the
performance of the proposed HR method by solving a classic control problem in
reinforcement learning. The result demonstrates the superior performance of the
method with significant enhancement in the generalizability of the neural
network.

</details>


### [292] [Mirror Mirror on the Wall, Have I Forgotten it All? A New Framework for Evaluating Machine Unlearning](https://arxiv.org/pdf/2505.08138)
*Brennon Brimhall, Philip Mathew, Neil Fendley, Yinzhi Cao, Matthew Green*

Main category: cs.LG

TL;DR: The paper evaluates machine unlearning methods, showing adversaries can distinguish between retrained and unlearned models. It proposes a formal definition (computational unlearning) and links it to differential privacy, revealing current methods' shortcomings.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a strong formal definition for machine unlearning and evaluate the effectiveness of existing methods against adversarial attacks.

Method: Empirical evaluation of unlearning methods using distinguishing algorithms based on membership inference scores and Kullback-Leibler divergence. Introduction of computational unlearning as a formal definition.

Result: Adversaries can distinguish unlearned models from retrained ones. Computational unlearning is not achieved by deterministic methods for entropic learning algorithms. DP-based methods satisfy computational unlearning but with severe utility loss.

Conclusion: Current unlearning methods fail to meet computational unlearning standards. Future work should address open questions to improve unlearning feasibility and utility.

Abstract: Machine unlearning methods take a model trained on a dataset and a forget
set, then attempt to produce a model as if it had only been trained on the
examples not in the forget set. We empirically show that an adversary is able
to distinguish between a mirror model (a control model produced by retraining
without the data to forget) and a model produced by an unlearning method across
representative unlearning methods from the literature. We build distinguishing
algorithms based on evaluation scores in the literature (i.e. membership
inference scores) and Kullback-Leibler divergence.
  We propose a strong formal definition for machine unlearning called
computational unlearning. Computational unlearning is defined as the inability
for an adversary to distinguish between a mirror model and a model produced by
an unlearning method. If the adversary cannot guess better than random (except
with negligible probability), then we say that an unlearning method achieves
computational unlearning.
  Our computational unlearning definition provides theoretical structure to
prove unlearning feasibility results. For example, our computational unlearning
definition immediately implies that there are no deterministic computational
unlearning methods for entropic learning algorithms. We also explore the
relationship between differential privacy (DP)-based unlearning methods and
computational unlearning, showing that DP-based approaches can satisfy
computational unlearning at the cost of an extreme utility collapse. These
results demonstrate that current methodology in the literature fundamentally
falls short of achieving computational unlearning. We conclude by identifying
several open questions for future work.

</details>


### [293] [Multi-Layer Hierarchical Federated Learning with Quantization](https://arxiv.org/pdf/2505.08145)
*Seyed Mohammad Azimi-Abarghouyi, Carlo Fischione*

Main category: cs.LG

TL;DR: A novel Multi-Layer Hierarchical Federated Learning (QMLHFL) framework generalizes hierarchical FL to arbitrary layers, using nested aggregation and layer-specific quantization, with proven convergence and optimized performance.


<details>
  <summary>Details</summary>
Motivation: Existing hierarchical FL models are limited to two layers, hindering scalability and flexibility in large-scale networks.

Method: Proposes QMLHFL with nested aggregation and layer-specific quantization, analyzes convergence, and optimizes intra-layer iterations.

Result: QMLHFL achieves high accuracy under data heterogeneity and outperforms random parameter selection when optimized.

Conclusion: QMLHFL is a scalable, flexible solution for hierarchical FL, with proven convergence and performance benefits.

Abstract: Almost all existing hierarchical federated learning (FL) models are limited
to two aggregation layers, restricting scalability and flexibility in complex,
large-scale networks. In this work, we propose a Multi-Layer Hierarchical
Federated Learning framework (QMLHFL), which appears to be the first study that
generalizes hierarchical FL to arbitrary numbers of layers and network
architectures through nested aggregation, while employing a layer-specific
quantization scheme to meet communication constraints. We develop a
comprehensive convergence analysis for QMLHFL and derive a general convergence
condition and rate that reveal the effects of key factors, including
quantization parameters, hierarchical architecture, and intra-layer iteration
counts. Furthermore, we determine the optimal number of intra-layer iterations
to maximize the convergence rate while meeting a deadline constraint that
accounts for both communication and computation times. Our results show that
QMLHFL consistently achieves high learning accuracy, even under high data
heterogeneity, and delivers notably improved performance when optimized,
compared to using randomly selected values.

</details>


### [294] [Feature Fitted Online Conformal Prediction for Deep Time Series Forecasting Model](https://arxiv.org/pdf/2505.08158)
*Xiannan Huang, Shuhan Qiu*

Main category: cs.LG

TL;DR: A lightweight conformal prediction method for time series forecasting provides valid confidence intervals without retraining, leveraging pre-trained models and adaptive coverage control.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning-based confidence interval methods are costly, inefficient, or lack theoretical guarantees.

Method: Uses features from pre-trained models to fit a residual predictor and construct intervals, with adaptive coverage control.

Result: Achieves tighter confidence intervals while maintaining coverage rates, validated on 12 datasets.

Conclusion: The method offers efficient, theoretically guaranteed uncertainty quantification for time series forecasting.

Abstract: Time series forecasting is critical for many applications, where deep
learning-based point prediction models have demonstrated strong performance.
However, in practical scenarios, there is also a need to quantify predictive
uncertainty through online confidence intervals. Existing confidence interval
modeling approaches building upon these deep point prediction models suffer
from key limitations: they either require costly retraining, fail to fully
leverage the representational strengths of deep models, or lack theoretical
guarantees. To address these gaps, we propose a lightweight conformal
prediction method that provides valid coverage and shorter interval lengths
without retraining. Our approach leverages features extracted from pre-trained
point prediction models to fit a residual predictor and construct confidence
intervals, further enhanced by an adaptive coverage control mechanism.
Theoretically, we prove that our method achieves asymptotic coverage
convergence, with error bounds dependent on the feature quality of the
underlying point prediction model. Experiments on 12 datasets demonstrate that
our method delivers tighter confidence intervals while maintaining desired
coverage rates. Code, model and dataset in
\href{https://github.com/xiannanhuang/FFDCI}{Github}

</details>


### [295] [Feasibility-Aware Pessimistic Estimation: Toward Long-Horizon Safety in Offline RL](https://arxiv.org/pdf/2505.08179)
*Zhikun Tao, Gang Xiong, He Fang, Zhen Shen, Yunjun Han, Qing-Shan Jia*

Main category: cs.LG

TL;DR: FASP is a novel offline safe RL framework addressing long-horizon safety and OOD challenges using H-J reachability and pessimistic Q-value estimation.


<details>
  <summary>Details</summary>
Motivation: Existing OSRL methods focus on short-term safety, risking constraint violations and inefficiency with OOD states/actions.

Method: Uses H-J reachability for safety labels, CVAE for efficiency, and pessimistic Q-value estimation to handle OOD actions.

Result: FASP outperforms state-of-the-art algorithms in safety on DSRL benchmarks.

Conclusion: FASP ensures long-horizon safety and handles OOD states effectively, validated by theory and experiments.

Abstract: Offline safe reinforcement learning(OSRL) derives constraint-satisfying
policies from pre-collected datasets, offers a promising avenue for deploying
RL in safety-critical real-world domains such as robotics. However, the
majority of existing approaches emphasize only short-term safety, neglecting
long-horizon considerations. Consequently, they may violate safety constraints
and fail to ensure sustained protection during online deployment. Moreover, the
learned policies often struggle to handle states and actions that are not
present or out-of-distribution(OOD) from the offline dataset, and exhibit
limited sample efficiency. To address these challenges, we propose a novel
framework Feasibility-Aware offline Safe Reinforcement Learning with CVAE-based
Pessimism (FASP). First, we employ Hamilton-Jacobi (H-J) reachability analysis
to generate reliable safety labels, which serve as supervisory signals for
training both a conditional variational autoencoder (CVAE) and a safety
classifier. This approach not only ensures high sampling efficiency but also
provides rigorous long-horizon safety guarantees. Furthermore, we utilize
pessimistic estimation methods to estimate the Q-value of reward and cost,
which mitigates the extrapolation errors induces by OOD actions, and penalize
unsafe actions to enabled the agent to proactively avoid high-risk behaviors.
Moreover, we theoretically prove the validity of this pessimistic estimation.
Extensive experiments on DSRL benchmarks demonstrate that FASP algorithm
achieves competitive performance across multiple experimental tasks,
particularly outperforming state-of-the-art algorithms in terms of safety.

</details>


### [296] [DSADF: Thinking Fast and Slow for Decision Making](https://arxiv.org/pdf/2505.08189)
*Alex Zhihao Dou, Dongfei Cui, Jun Yan, Weida Wang, Benteng Chen, Haoming Wang, Zeke Xie, Shufei Zhang*

Main category: cs.LG

TL;DR: The paper proposes a Dual-System Adaptive Decision Framework (DSADF) to enhance RL agents' generalization by combining fast RL-based decisions (System 1) with deep reasoning from VLMs (System 2), inspired by Kahneman's theory.


<details>
  <summary>Details</summary>
Motivation: RL agents struggle to generalize in dynamic environments, and existing LLM/VLM integrations lack seamless coordination, leading to inefficiencies.

Method: DSADF integrates an RL agent (System 1) for fast decisions and a VLM (System 2) for deep reasoning, balancing intuition and analysis.

Result: Empirical tests in Crafter and Housekeep show improved decision-making for both known and unseen tasks.

Conclusion: DSADF effectively combines RL and VLM strengths, enhancing adaptive decision-making in complex environments.

Abstract: Although Reinforcement Learning (RL) agents are effective in well-defined
environments, they often struggle to generalize their learned policies to
dynamic settings due to their reliance on trial-and-error interactions. Recent
work has explored applying Large Language Models (LLMs) or Vision Language
Models (VLMs) to boost the generalization of RL agents through policy
optimization guidance or prior knowledge. However, these approaches often lack
seamless coordination between the RL agent and the foundation model, leading to
unreasonable decision-making in unfamiliar environments and efficiency
bottlenecks. Making full use of the inferential capabilities of foundation
models and the rapid response capabilities of RL agents and enhancing the
interaction between the two to form a dual system is still a lingering
scientific question. To address this problem, we draw inspiration from
Kahneman's theory of fast thinking (System 1) and slow thinking (System 2),
demonstrating that balancing intuition and deep reasoning can achieve nimble
decision-making in a complex world. In this study, we propose a Dual-System
Adaptive Decision Framework (DSADF), integrating two complementary modules:
System 1, comprising an RL agent and a memory space for fast and intuitive
decision making, and System 2, driven by a VLM for deep and analytical
reasoning. DSADF facilitates efficient and adaptive decision-making by
combining the strengths of both systems. The empirical study in the video game
environment: Crafter and Housekeep demonstrates the effectiveness of our
proposed method, showing significant improvements in decision abilities for
both unseen and known tasks.

</details>


### [297] [A Multi-scale Representation Learning Framework for Long-Term Time Series Forecasting](https://arxiv.org/pdf/2505.08199)
*Boshi Gao, Qingjian Ni, Fanbo Ju, Yu Chen, Ziqi Zhao*

Main category: cs.LG

TL;DR: MDMixer, an MLP-based framework, improves long-term time series forecasting by addressing multi-granularity information, channel-specific attributes, and trend/seasonal components, outperforming TimeMixer by 4.64% in MAE.


<details>
  <summary>Details</summary>
Motivation: Accurate long-term forecasting is challenging due to complex temporal patterns and multi-scale variations. Existing methods underutilize multi-granularity information and ignore channel-specific attributes.

Method: MDMixer disentangles temporal dynamics with multi-scale predictions, dynamically integrates them, and models trend and seasonal components separately.

Result: Outperforms TimeMixer by 4.64% in MAE on eight benchmarks, balancing efficiency and interpretability.

Conclusion: MDMixer effectively addresses LTSF challenges, offering improved performance and practical utility.

Abstract: Long-term time series forecasting (LTSF) offers broad utility in practical
settings like energy consumption and weather prediction. Accurately predicting
long-term changes, however, is demanding due to the intricate temporal patterns
and inherent multi-scale variations within time series. This work confronts key
issues in LTSF, including the suboptimal use of multi-granularity information,
the neglect of channel-specific attributes, and the unique nature of trend and
seasonal components, by introducing a proficient MLP-based forecasting
framework. Our method adeptly disentangles complex temporal dynamics using
clear, concurrent predictions across various scales. These multi-scale
forecasts are then skillfully integrated through a system that dynamically
assigns importance to information from different granularities, sensitive to
individual channel characteristics. To manage the specific features of temporal
patterns, a two-pronged structure is utilized to model trend and seasonal
elements independently. Experimental results on eight LTSF benchmarks
demonstrate that MDMixer improves average MAE performance by 4.64% compared to
the recent state-of-the-art MLP-based method (TimeMixer), while achieving an
effective balance between training efficiency and model interpretability.

</details>


### [298] [An Effective Flow-based Method for Positive-Unlabeled Learning: 2-HNC](https://arxiv.org/pdf/2505.08212)
*Dorit Hochbaum, Torpong Nitayanont*

Main category: cs.LG

TL;DR: The paper introduces 2-HNC, a network flow-based method for positive-unlabeled (PU) learning, leveraging Hochbaum's Normalized Cut to rank unlabeled samples by their likelihood of being negative, achieving strong performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of binary classification where only positive instances are labeled, leaving the rest unlabeled, in PU learning scenarios.

Method: Proposes 2-HNC, a two-stage method: (1) ranks unlabeled samples without negative labels using Hochbaum's Normalized Cut, (2) augments the positive set with likely-negatives and recomputes classification.

Result: Extensive experiments show 2-HNC outperforms state-of-the-art algorithms on synthetic and real datasets.

Conclusion: 2-HNC is effective for PU learning, leveraging nested partitions and prior estimates to achieve high accuracy.

Abstract: In many scenarios of binary classification, only positive instances are
provided in the training data, leaving the rest of the data unlabeled. This
setup, known as positive-unlabeled (PU) learning, is addressed here with a
network flow-based method which utilizes pairwise similarities between samples.
The method we propose here, 2-HNC, leverages Hochbaum's Normalized Cut (HNC)
and the set of solutions it provides by solving a parametric minimum cut
problem. The set of solutions, that are nested partitions of the samples into
two sets, correspond to varying tradeoff values between the two goals: high
intra-similarity inside the sets and low inter-similarity between the two sets.
This nested sequence is utilized here to deliver a ranking of unlabeled samples
by their likelihood of being negative. Building on this insight, our method,
2-HNC, proceeds in two stages. The first stage generates this ranking without
assuming any negative labels, using a problem formulation that is constrained
only on positive labeled samples. The second stage augments the positive set
with likely-negative samples and recomputes the classification. The final label
prediction selects among all generated partitions in both stages, the one that
delivers a positive class proportion, closest to a prior estimate of this
quantity, which is assumed to be given. Extensive experiments across synthetic
and real datasets show that 2-HNC yields strong performance and often surpasses
existing state-of-the-art algorithms.

</details>


### [299] [Clinically inspired enhance Explainability and Interpretability of an AI-Tool for BCC diagnosis based on expert annotation](https://arxiv.org/pdf/2407.00104)
*Iván Matas, Carmen Serrano, Francisca Silva, Amalia Serrano, Tomás Toledo-Pastrana, Begoña Acha*

Main category: cs.LG

TL;DR: An AI tool improves BCC diagnosis via teledermatology, offering interpretable support with 90% classification accuracy and 99% accuracy in detecting clinically relevant patterns.


<details>
  <summary>Details</summary>
Motivation: To speed up referrals and optimize resource utilization in BCC diagnosis by providing interpretable AI support.

Method: Uses dermoscopic patterns and Grad-CAM for visual explanations, with ground truth inferred via an EM-based algorithm from dermatologists' diagnoses.

Result: Achieves 90% accuracy in BCC/non-BCC classification and 99% accuracy in detecting clinically relevant patterns, though Grad-CAM struggles with precise pattern localization.

Conclusion: The AI tool is effective for early BCC detection and referral, offering valuable interpretability despite some localization challenges.

Abstract: An AI tool has been developed to provide interpretable support for the
diagnosis of BCC via teledermatology, thus speeding up referrals and optimizing
resource utilization. The interpretability is provided in two ways: on the one
hand, the main BCC dermoscopic patterns are found in the image to justify the
BCC/Non BCC classification. Secondly, based on the common visual XAI Grad-CAM,
a clinically inspired visual explanation is developed where the relevant
features for diagnosis are located. Since there is no established ground truth
for BCC dermoscopic features, a standard reference is inferred from the
diagnosis of four dermatologists using an Expectation Maximization (EM) based
algorithm. The results demonstrate significant improvements in classification
accuracy and interpretability, positioning this approach as a valuable tool for
early BCC detection and referral to dermatologists. The BCC/non-BCC
classification achieved an accuracy rate of 90%. For Clinically-inspired XAI
results, the detection of BCC patterns useful to clinicians reaches 99%
accuracy. As for the Clinically-inspired Visual XAI results, the mean of the
Grad-CAM normalized value within the manually segmented clinical features is
0.57, while outside this region it is 0.16. This indicates that the model
struggles to accurately identify the regions of the BCC patterns. These results
prove the ability of the AI tool to provide a useful explanation.

</details>


### [300] [Deep Probabilistic Modeling of User Behavior for Anomaly Detection via Mixture Density Networks](https://arxiv.org/pdf/2505.08220)
*Lu Dai, Wenxuan Zhu, Xuehui Quan, Renzi Meng, Sheng Cai, Yichen Wang*

Main category: cs.LG

TL;DR: A deep mixture density network method for anomaly detection in user behavior, outperforming traditional classifiers and advanced neural networks in performance and stability.


<details>
  <summary>Details</summary>
Motivation: To improve anomaly detection in complex user behavior by addressing limitations of traditional classifiers and single decision boundaries.

Method: Uses a Gaussian mixture model parameterized by a neural network for conditional probability modeling, with anomaly scoring based on negative log-likelihood.

Result: Outperforms advanced neural networks on UNSW-NB15 dataset in metrics like Accuracy, F1-score, and AUC.

Conclusion: Provides a more expressive solution for behavior modeling and anomaly detection, advancing deep probabilistic modeling in network security and risk control.

Abstract: To improve the identification of potential anomaly patterns in complex user
behavior, this paper proposes an anomaly detection method based on a deep
mixture density network. The method constructs a Gaussian mixture model
parameterized by a neural network, enabling conditional probability modeling of
user behavior. It effectively captures the multimodal distribution
characteristics commonly present in behavioral data. Unlike traditional
classifiers that rely on fixed thresholds or a single decision boundary, this
approach defines an anomaly scoring function based on probability density using
negative log-likelihood. This significantly enhances the model's ability to
detect rare and unstructured behaviors. Experiments are conducted on the
real-world network user dataset UNSW-NB15. A series of performance comparisons
and stability validation experiments are designed. These cover multiple
evaluation aspects, including Accuracy, F1- score, AUC, and loss fluctuation.
The results show that the proposed method outperforms several advanced neural
network architectures in both performance and training stability. This study
provides a more expressive and discriminative solution for user behavior
modeling and anomaly detection. It strongly promotes the application of deep
probabilistic modeling techniques in the fields of network security and
intelligent risk control.

</details>


### [301] [Clustering-based Low-Rank Matrix Approximation: An Adaptive Theoretical Analysis with Application to Data Compression](https://arxiv.org/pdf/2505.08256)
*Sisipho Hamlomo, Marcellin Atemkeng*

Main category: cs.LG

TL;DR: The paper introduces an adaptive low-rank matrix approximation (LoRMA) method that improves upon global SVD by partitioning data into patches, clustering them, and applying SVD per cluster. It outperforms global SVD in preserving structural details and diagnostic relevance in medical imaging.


<details>
  <summary>Details</summary>
Motivation: Global SVD methods compress data uniformly, often losing local variations and fine details. The adaptive LoRMA addresses this by focusing on local structures.

Method: The adaptive LoRMA partitions data into overlapping patches, clusters them using k-means, and performs SVD within each cluster. It analyzes compression efficiency and computational cost.

Result: Adaptive LoRMA outperforms global SVD in preserving structural integrity and diagnostic relevance, with better PSNR, SSIM, IoU, EPI, and lower MSE. It minimizes artifacts and errors, especially in critical regions.

Conclusion: Adaptive LoRMA is superior for high-compression applications in medical imaging, despite higher processing time, due to its diagnostic fidelity and storage efficiency.

Abstract: Low-rank matrix approximation (LoRMA) is a fundamental tool for compressing
high-resolution data matrices by extracting important features while
suppressing redundancy. Low-rank methods, such as global singular value
decomposition (SVD), apply uniform compression across the entire data matrix,
often ignoring important local variations and leading to the loss of fine
structural details. To address these limitations, we introduce an adaptive
LoRMA, which partitions data matrix into overlapping patches, groups
structurally similar patches into several clusters using k-means, and performs
SVD within each cluster. We derive the overall compression factor accounting
for patch overlap and analyze how patch size influences compression efficiency
and computational cost. While the proposed adaptive LoRMA method is applicable
to any data exhibiting high local variation, we focus on medical imaging due to
its pronounced local variability. We evaluate and compare our adaptive LoRMA
against global SVD across four imaging modalities: MRI, ultrasound, CT scan,
and chest X-ray. Results demonstrate that adaptive LoRMA effectively preserves
structural integrity, edge details, and diagnostic relevance, as measured by
peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), mean
squared error (MSE), intersection over union (IoU), and edge preservation index
(EPI). Adaptive LoRMA significantly minimizes block artifacts and residual
errors, particularly in pathological regions, consistently outperforming global
SVD in terms of PSNR, SSIM, IoU, EPI, and achieving lower MSE. Adaptive LoRMA
prioritizes clinically salient regions while allowing aggressive compression in
non-critical regions, optimizing storage efficiency. Although adaptive LoRMA
requires higher processing time, its diagnostic fidelity justifies the overhead
for high-compression applications.

</details>


### [302] [Super-fast rates of convergence for Neural Networks Classifiers under the Hard Margin Condition](https://arxiv.org/pdf/2505.08262)
*Nathanael Tepakbong, Ding-Xuan Zhou, Xiang Zhou*

Main category: cs.LG

TL;DR: DNNs with ReLU activation achieve fast excess risk bounds under Tsybakov's hard-margin condition for smooth regression functions.


<details>
  <summary>Details</summary>
Motivation: To understand the performance of DNNs in binary classification under Tsybakov's low-noise condition, especially the hard-margin case.

Method: Empirical risk minimization with square loss surrogate and ℓ_p penalty for DNNs.

Result: Finite-sample excess risk bounds of order 𝒪(n^−α) for arbitrarily large α under the hard-margin condition.

Conclusion: DNNs can achieve rapid convergence rates for classification under smoothness and hard-margin assumptions, with a novel risk decomposition.

Abstract: We study the classical binary classification problem for hypothesis spaces of
Deep Neural Networks (DNNs) with ReLU activation under Tsybakov's low-noise
condition with exponent $q>0$, and its limit-case $q\to\infty$ which we refer
to as the "hard-margin condition". We show that DNNs which minimize the
empirical risk with square loss surrogate and $\ell_p$ penalty can achieve
finite-sample excess risk bounds of order $\mathcal{O}\left(n^{-\alpha}\right)$
for arbitrarily large $\alpha>0$ under the hard-margin condition, provided that
the regression function $\eta$ is sufficiently smooth. The proof relies on a
novel decomposition of the excess risk which might be of independent interest.

</details>


### [303] [LLM Enhancers for GNNs: An Analysis from the Perspective of Causal Mechanism Identification](https://arxiv.org/pdf/2505.08265)
*Hang Gao, Wenxuan Huang, Fengge Wu, Junsuo Zhao, Changwen Zheng, Huaping Liu*

Main category: cs.LG

TL;DR: The paper explores using LLMs to enhance node representations for GNNs, analyzes their properties via interchange interventions, and proposes an optimization module to improve their interaction.


<details>
  <summary>Details</summary>
Motivation: The potential of LLMs as feature enhancers for GNNs is underexplored, prompting a deeper analysis of their properties and mechanisms.

Method: Constructs a synthetic graph dataset for causal analysis, uses interchange interventions to study LLM-GNN interactions, and designs an optimization module.

Result: Experiments validate the proposed module's effectiveness across multiple datasets and models.

Conclusion: The study provides insights into LLM-GNN interactions and offers a practical optimization module for improved performance.

Abstract: The use of large language models (LLMs) as feature enhancers to optimize node
representations, which are then used as inputs for graph neural networks
(GNNs), has shown significant potential in graph representation learning.
However, the fundamental properties of this approach remain underexplored. To
address this issue, we propose conducting a more in-depth analysis of this
issue based on the interchange intervention method. First, we construct a
synthetic graph dataset with controllable causal relationships, enabling
precise manipulation of semantic relationships and causal modeling to provide
data for analysis. Using this dataset, we conduct interchange interventions to
examine the deeper properties of LLM enhancers and GNNs, uncovering their
underlying logic and internal mechanisms. Building on the analytical results,
we design a plug-and-play optimization module to improve the information
transfer between LLM enhancers and GNNs. Experiments across multiple datasets
and models validate the proposed module.

</details>


### [304] [Decoupled Multimodal Prototypes for Visual Recognition with Missing Modalities](https://arxiv.org/pdf/2505.08283)
*Jueqing Lu, Yuanyuan Qi, Xiaohao Yang, Shujie Zhou, Lan Du*

Main category: cs.LG

TL;DR: A novel decoupled prototype-based output head improves multimodal learning by dynamically adapting to missing modalities, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal learning methods assume all modalities are available, which is unrealistic. Addressing missing modalities without extensive fine-tuning is crucial.

Method: Proposes a decoupled prototype-based output head using missing-case-aware class-wise prototypes for each modality, adaptable to various missing scenarios.

Result: Significantly improves performance across diverse missing-modality scenarios and varying missing rates.

Conclusion: The proposed method effectively handles missing modalities and enhances multimodal learning performance.

Abstract: Multimodal learning enhances deep learning models by enabling them to
perceive and understand information from multiple data modalities, such as
visual and textual inputs. However, most existing approaches assume the
availability of all modalities, an assumption that often fails in real-world
applications. Recent works have introduced learnable missing-case-aware prompts
to mitigate performance degradation caused by missing modalities while reducing
the need for extensive model fine-tuning. Building upon the effectiveness of
missing-case-aware handling for missing modalities, we propose a novel
decoupled prototype-based output head, which leverages missing-case-aware
class-wise prototypes tailored for each individual modality. This approach
dynamically adapts to different missing modality scenarios and can be
seamlessly integrated with existing prompt-based methods. Extensive experiments
demonstrate that our proposed output head significantly improves performance
across a wide range of missing-modality scenarios and varying missing rates.

</details>


### [305] [A Practical Introduction to Deep Reinforcement Learning](https://arxiv.org/pdf/2505.08295)
*Yinghan Sun, Hongxi Wang, Hua Chen, Wei Zhang*

Main category: cs.LG

TL;DR: A tutorial introducing deep reinforcement learning (DRL) with a focus on Proximal Policy Optimization (PPO), providing intuitive explanations and practical guidance under the Generalized Policy Iteration (GPI) framework.


<details>
  <summary>Details</summary>
Motivation: To address the challenges beginners face due to the diversity and complexity of DRL algorithms, offering a concise and practical introduction.

Method: Organizes DRL algorithms under the GPI framework, emphasizing intuitive explanations, examples, and engineering techniques over theoretical proofs.

Result: Provides an accessible guide for readers to quickly advance from basic concepts to implementing advanced DRL algorithms like PPO.

Conclusion: The tutorial effectively bridges the gap for beginners, making DRL more approachable and actionable.

Abstract: Deep reinforcement learning (DRL) has emerged as a powerful framework for
solving sequential decision-making problems, achieving remarkable success in a
wide range of applications, including game AI, autonomous driving, biomedicine,
and large language models. However, the diversity of algorithms and the
complexity of theoretical foundations often pose significant challenges for
beginners seeking to enter the field. This tutorial aims to provide a concise,
intuitive, and practical introduction to DRL, with a particular focus on the
Proximal Policy Optimization (PPO) algorithm, which is one of the most widely
used and effective DRL methods. To facilitate learning, we organize all
algorithms under the Generalized Policy Iteration (GPI) framework, offering
readers a unified and systematic perspective. Instead of lengthy theoretical
proofs, we emphasize intuitive explanations, illustrative examples, and
practical engineering techniques. This work serves as an efficient and
accessible guide, helping readers rapidly progress from basic concepts to the
implementation of advanced DRL algorithms.

</details>


### [306] [Efficient Unstructured Pruning of Mamba State-Space Models for Resource-Constrained Environments](https://arxiv.org/pdf/2505.08299)
*Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma*

Main category: cs.LG

TL;DR: A novel pruning framework for Mamba models reduces parameters by 70% while retaining 95% performance, enabling efficient deployment in resource-constrained settings.


<details>
  <summary>Details</summary>
Motivation: Large parameter counts in Mamba models hinder deployment in resource-limited environments, necessitating an effective pruning solution.

Method: The framework uses gradient-aware magnitude pruning, an iterative pruning schedule, and global pruning to reduce parameters while maintaining performance.

Result: Achieves up to 70% parameter reduction with minimal performance loss, validated on multiple benchmarks.

Conclusion: The pruning framework enhances Mamba's practicality for resource-constrained applications and broadens its usability.

Abstract: State-space models (SSMs), particularly the Mamba architecture, have emerged
as powerful alternatives to Transformers for sequence modeling, offering
linear-time complexity and competitive performance across diverse tasks.
However, their large parameter counts pose significant challenges for
deployment in resource-constrained environments. We propose a novel
unstructured pruning framework tailored for Mamba models that achieves up to
70\% parameter reduction while retaining over 95\% of the original performance.
Our approach integrates three key innovations: (1) a gradient-aware magnitude
pruning technique that combines weight magnitude and gradient information to
identify less critical parameters, (2) an iterative pruning schedule that
gradually increases sparsity to maintain model stability, and (3) a global
pruning strategy that optimizes parameter allocation across the entire model.
Through extensive experiments on WikiText-103, Long Range Arena, and ETT
time-series benchmarks, we demonstrate significant efficiency gains with
minimal performance degradation. Our analysis of pruning effects on Mamba's
components reveals critical insights into the architecture's redundancy and
robustness, enabling practical deployment in resource-constrained settings
while broadening Mamba's applicability.

</details>


### [307] [Rapid Overfitting of Multi-Pass Stochastic Gradient Descent in Stochastic Convex Optimization](https://arxiv.org/pdf/2505.08306)
*Shira Vansover-Hager, Tomer Koren, Roi Livni*

Main category: cs.LG

TL;DR: Multi-pass SGD can hurt out-of-sample performance, leading to overfitting, especially in non-smooth SCO. The population loss worsens after the first epoch, with a phase-transition behavior.


<details>
  <summary>Details</summary>
Motivation: To understand the out-of-sample performance of multi-pass SGD in stochastic convex optimization (SCO) and its potential overfitting risks.

Method: Analyze the population loss of multi-pass SGD with step size η=Θ(1/√n) and total steps T, comparing smooth and non-smooth SCO cases.

Result: Multi-pass SGD can lead to Ω(1) population loss after one additional pass. The loss scales as Θ(1/(ηT)+η√T) after the first epoch.

Conclusion: Multi-pass SGD exhibits a phase-transition in out-of-sample behavior post-first epoch, with distinct overfitting rates in smooth vs. non-smooth SCO.

Abstract: We study the out-of-sample performance of multi-pass stochastic gradient
descent (SGD) in the fundamental stochastic convex optimization (SCO) model.
While one-pass SGD is known to achieve an optimal $\Theta(1/\sqrt{n})$ excess
population loss given a sample of size $n$, much less is understood about the
multi-pass version of the algorithm which is widely used in practice. Somewhat
surprisingly, we show that in the general non-smooth case of SCO, just a few
epochs of SGD can already hurt its out-of-sample performance significantly and
lead to overfitting. In particular, using a step size $\eta =
\Theta(1/\sqrt{n})$, which gives the optimal rate after one pass, can lead to
population loss as large as $\Omega(1)$ after just one additional pass. More
generally, we show that the population loss from the second pass onward is of
the order $\Theta(1/(\eta T) + \eta \sqrt{T})$, where $T$ is the total number
of steps. These results reveal a certain phase-transition in the out-of-sample
behavior of SGD after the first epoch, as well as a sharp separation between
the rates of overfitting in the smooth and non-smooth cases of SCO.
Additionally, we extend our results to with-replacement SGD, proving that the
same asymptotic bounds hold after $O(n \log n)$ steps. Finally, we also prove a
lower bound of $\Omega(\eta \sqrt{n})$ on the generalization gap of one-pass
SGD in dimension $d = \smash{\widetilde O}(n)$, improving on recent results of
Koren et al.(2022) and Schliserman et al.(2024).

</details>


### [308] [SpecSphere: Dual-Pass Spectral-Spatial Graph Neural Networks with Certified Robustness](https://arxiv.org/pdf/2505.08320)
*Yoonhyuk Choi, Chong-Kwon Kim*

Main category: cs.LG

TL;DR: SpecSphere is a dual-pass spectral-spatial GNN offering certified robustness against adversarial attacks, adaptability across homophily-heterophily, and superior expressivity with linear-time complexity.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between high expressivity, adaptability to varying graph homophily-heterophily, and provable robustness in graph neural networks.

Method: Combines a Chebyshev-polynomial spectral branch and an attention-gated spatial branch, fused via a lightweight MLP trained in a cooperative-adversarial min-max game.

Result: Achieves state-of-the-art node-classification accuracy, tighter robustness guarantees, and universal approximation beyond 1-WL.

Conclusion: Demonstrates that expressivity, heterophily adaptation, and robustness can coexist in a scalable architecture.

Abstract: We introduce SpecSphere, the first dual-pass spectral-spatial GNN that
certifies every prediction against both $\ell\_{0}$ edge flips and
$\ell\_{\infty}$ feature perturbations, adapts to the full
homophily-heterophily spectrum, and surpasses the expressive power of
1-Weisfeiler-Lehman while retaining linear-time complexity. Our model couples a
Chebyshev-polynomial spectral branch with an attention-gated spatial branch and
fuses their representations through a lightweight MLP trained in a
cooperative-adversarial min-max game. We further establish (i) a uniform
Chebyshev approximation theorem, (ii) minimax-optimal risk across the
homophily-heterophily spectrum, (iii) closed-form robustness certificates, and
(iv) universal approximation strictly beyond 1-WL. SpecSphere achieves
state-of-the-art node-classification accuracy and delivers tighter certified
robustness guarantees on real-world benchmarks. These results demonstrate that
high expressivity, heterophily adaptation, and provable robustness can coexist
within a single, scalable architecture.

</details>


### [309] [FedRS-Bench: Realistic Federated Learning Datasets and Benchmarks in Remote Sensing](https://arxiv.org/pdf/2505.08325)
*Haodong Zhao, Peng Peng, Chiyu Chen, Linqing Huang, Gongshen Liu*

Main category: cs.LG

TL;DR: The paper introduces FedRS, a realistic federated remote sensing dataset, and FedRS-Bench, a benchmark with 10 FL algorithms, addressing the lack of standardized datasets and fair comparisons in federated learning for RS.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in centralized model training for RS due to distributed data and privacy concerns, and to provide a realistic federated dataset and benchmark for fair evaluation.

Method: Proposes FedRS, a dataset with eight RS datasets and 135 clients, reflecting real-world heterogeneity. Implements FedRS-Bench with 10 FL algorithms and metrics.

Result: FL improves model performance over isolated training, with trade-offs under varying client heterogeneity and availability.

Conclusion: FedRS-Bench aims to advance FL research in RS by offering a standardized testbed for fair comparisons.

Abstract: Remote sensing (RS) images are usually produced at an unprecedented scale,
yet they are geographically and institutionally distributed, making centralized
model training challenging due to data-sharing restrictions and privacy
concerns. Federated learning (FL) offers a solution by enabling collaborative
model training across decentralized RS data sources without exposing raw data.
However, there lacks a realistic federated dataset and benchmark in RS. Prior
works typically rely on manually partitioned single dataset, which fail to
capture the heterogeneity and scale of real-world RS data, and often use
inconsistent experimental setups, hindering fair comparison. To address this
gap, we propose a realistic federated RS dataset, termed FedRS. FedRS consists
of eight datasets that cover various sensors and resolutions and builds 135
clients, which is representative of realistic operational scenarios. Data for
each client come from the same source, exhibiting authentic federated
properties such as skewed label distributions, imbalanced client data volumes,
and domain heterogeneity across clients. These characteristics reflect
practical challenges in federated RS and support evaluation of FL methods at
scale. Based on FedRS, we implement 10 baseline FL algorithms and evaluation
metrics to construct the comprehensive FedRS-Bench. The experimental results
demonstrate that FL can consistently improve model performance over training on
isolated data silos, while revealing performance trade-offs of different
methods under varying client heterogeneity and availability conditions. We hope
FedRS-Bench will accelerate research on large-scale, realistic FL in RS by
providing a standardized, rich testbed and facilitating fair comparisons across
future works. The source codes and dataset are available at
https://fedrs-bench.github.io/.

</details>


### [310] [Reinforcement Learning (RL) Meets Urban Climate Modeling: Investigating the Efficacy and Impacts of RL-Based HVAC Control](https://arxiv.org/pdf/2505.07045)
*Junjie Yu, John S. Schreck, David John Gagne, Keith W. Oleson, Jie Li, Yongtu Liang, Qi Liao, Mingfei Sun, David O. Topping, Zhonghua Zheng*

Main category: cs.LG

TL;DR: RL-based HVAC control reduces energy use but varies by climate. An integrated framework evaluates its efficacy, impacts, and transferability across cities, showing climate-dependent results.


<details>
  <summary>Details</summary>
Motivation: Assess the effectiveness and broader impacts of RL-based HVAC control in diverse climates and its potential for cross-city learning.

Method: Combined RL with an urban climate model and building energy model to analyze HVAC control strategies across different cities.

Result: Rewards and impacts vary by climate; hot climates achieve higher rewards, and cities with temperature variations show better RL strategy transferability.

Conclusion: Climate context is crucial for RL-based HVAC control, and cross-city learning can enhance deployment.

Abstract: Reinforcement learning (RL)-based heating, ventilation, and air conditioning
(HVAC) control has emerged as a promising technology for reducing building
energy consumption while maintaining indoor thermal comfort. However, the
efficacy of such strategies is influenced by the background climate and their
implementation may potentially alter both the indoor climate and local urban
climate. This study proposes an integrated framework combining RL with an urban
climate model that incorporates a building energy model, aiming to evaluate the
efficacy of RL-based HVAC control across different background climates, impacts
of RL strategies on indoor climate and local urban climate, and the
transferability of RL strategies across cities. Our findings reveal that the
reward (defined as a weighted combination of energy consumption and thermal
comfort) and the impacts of RL strategies on indoor climate and local urban
climate exhibit marked variability across cities with different background
climates. The sensitivity of reward weights and the transferability of RL
strategies are also strongly influenced by the background climate. Cities in
hot climates tend to achieve higher rewards across most reward weight
configurations that balance energy consumption and thermal comfort, and those
cities with more varying atmospheric temperatures demonstrate greater RL
strategy transferability. These findings underscore the importance of
thoroughly evaluating RL-based HVAC control strategies in diverse climatic
contexts. This study also provides a new insight that city-to-city learning
will potentially aid the deployment of RL-based HVAC control.

</details>


### [311] [Low-Complexity Inference in Continual Learning via Compressed Knowledge Transfer](https://arxiv.org/pdf/2505.08327)
*Zhenrong Liu, Janne M. J. Huttunen, Mikko Honkala*

Main category: cs.LG

TL;DR: The paper explores model compression techniques (pruning and knowledge distillation) to improve efficiency in continual learning (CL), specifically for class-incremental learning (CIL), balancing accuracy and computational cost.


<details>
  <summary>Details</summary>
Motivation: Large pre-trained models in CL face high computational costs, limiting practicality in real-world applications. The goal is to address this by making models more efficient.

Method: Two frameworks are proposed: (1) pruning-based (pre- and post-pruning strategies) and (2) knowledge distillation-based (teacher-student architecture).

Result: Experiments show the frameworks achieve better accuracy-efficiency trade-offs, outperforming baselines in CIL benchmarks.

Conclusion: The study provides insights into choosing between pruning and distillation for different scenarios, enhancing CL practicality.

Abstract: Continual learning (CL) aims to train models that can learn a sequence of
tasks without forgetting previously acquired knowledge. A core challenge in CL
is balancing stability -- preserving performance on old tasks -- and plasticity
-- adapting to new ones. Recently, large pre-trained models have been widely
adopted in CL for their ability to support both, offering strong generalization
for new tasks and resilience against forgetting. However, their high
computational cost at inference time limits their practicality in real-world
applications, especially those requiring low latency or energy efficiency. To
address this issue, we explore model compression techniques, including pruning
and knowledge distillation (KD), and propose two efficient frameworks tailored
for class-incremental learning (CIL), a challenging CL setting where task
identities are unavailable during inference. The pruning-based framework
includes pre- and post-pruning strategies that apply compression at different
training stages. The KD-based framework adopts a teacher-student architecture,
where a large pre-trained teacher transfers downstream-relevant knowledge to a
compact student. Extensive experiments on multiple CIL benchmarks demonstrate
that the proposed frameworks achieve a better trade-off between accuracy and
inference complexity, consistently outperforming strong baselines. We further
analyze the trade-offs between the two frameworks in terms of accuracy and
efficiency, offering insights into their use across different scenarios.

</details>


### [312] [Structural-Temporal Coupling Anomaly Detection with Dynamic Graph Transformer](https://arxiv.org/pdf/2505.08330)
*Chang Zong, Yueting Zhuang, Jian Shao, Weiming Lu*

Main category: cs.LG

TL;DR: Proposes a dynamic graph transformer model for detecting anomalous edges in dynamic graphs by integrating structural-temporal coupling information, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of detecting anomalous edges in dynamic graphs due to the lack of structural-temporal coupling information, which existing methods ignore.

Method: Introduces a structural-temporal coupling anomaly detection architecture with a dynamic graph transformer, using two-dimensional positional encoding to capture structural and temporal features.

Result: Outperforms state-of-the-art models on six datasets and demonstrates effectiveness in a real-world case study.

Conclusion: The proposed method effectively integrates structural-temporal coupling, improving anomaly detection in dynamic graphs.

Abstract: Detecting anomalous edges in dynamic graphs is an important task in many
applications over evolving triple-based data, such as social networks,
transaction management, and epidemiology. A major challenge with this task is
the absence of structural-temporal coupling information, which decreases the
ability of the representation to distinguish anomalies from normal instances.
Existing methods focus on handling independent structural and temporal features
with embedding models, which ignore the deep interaction between these two
types of information. In this paper, we propose a structural-temporal coupling
anomaly detection architecture with a dynamic graph transformer model.
Specifically, we introduce structural and temporal features from two
integration levels to provide anomaly-aware graph evolutionary patterns. Then,
a dynamic graph transformer enhanced by two-dimensional positional encoding is
implemented to capture both discrimination and contextual consistency signals.
Extensive experiments on six datasets demonstrate that our method outperforms
current state-of-the-art models. Finally, a case study illustrates the strength
of our method when applied to a real-world task.

</details>


### [313] [SHAP-based Explanations are Sensitive to Feature Representation](https://arxiv.org/pdf/2505.08345)
*Hyunseung Hwang, Andrew Bell, Joao Fonseca, Venetia Pliatsika, Julia Stoyanovich, Steven Euijong Whang*

Main category: cs.LG

TL;DR: The paper explores how standard data engineering techniques can manipulate local feature-based explanations, like SHAP, potentially hiding issues like discrimination.


<details>
  <summary>Details</summary>
Motivation: To investigate the impact of data engineering choices on feature importance in explainable AI (XAI), highlighting vulnerabilities in interpretability methods.

Method: Examines common data engineering techniques (e.g., histogram representation of age, specific race encoding) and their effects on feature importance in SHAP and similar methods.

Result: Demonstrates that seemingly innocuous data engineering can alter feature importance, enabling adversaries to obscure problems like discrimination.

Conclusion: Reveals a gap in systematic study of how feature representation affects explainability, urging caution in data engineering for XAI.

Abstract: Local feature-based explanations are a key component of the XAI toolkit.
These explanations compute feature importance values relative to an
``interpretable'' feature representation. In tabular data, feature values
themselves are often considered interpretable. This paper examines the impact
of data engineering choices on local feature-based explanations. We demonstrate
that simple, common data engineering techniques, such as representing age with
a histogram or encoding race in a specific way, can manipulate feature
importance as determined by popular methods like SHAP. Notably, the sensitivity
of explanations to feature representation can be exploited by adversaries to
obscure issues like discrimination. While the intuition behind these results is
straightforward, their systematic exploration has been lacking. Previous work
has focused on adversarial attacks on feature-based explainers by biasing data
or manipulating models. To the best of our knowledge, this is the first study
demonstrating that explainers can be misled by standard, seemingly innocuous
data engineering techniques.

</details>


### [314] [Localization of Impacts on Thin-Walled Structures by Recurrent Neural Networks: End-to-end Learning from Real-World Data](https://arxiv.org/pdf/2505.08362)
*Alexander Humer, Lukas Grasboeck, Ayech Benjeddou*

Main category: cs.LG

TL;DR: The paper proposes using GRU-based RNNs for impact localization on shell-like structures, trained with physical experimental data for high accuracy.


<details>
  <summary>Details</summary>
Motivation: Impact localization is crucial for structural health monitoring, but conventional methods struggle due to Lamb waves' dispersive nature.

Method: Uses GRU-based RNNs to process sequential sensor data from piezoceramic sensors, trained with robot-generated experimental impacts on an aluminum plate.

Result: Achieves remarkable accuracy in impact position estimation, even with a small dataset.

Conclusion: Demonstrates the effectiveness of GRU-based RNNs and physical data for accurate impact localization in SHM.

Abstract: Today, machine learning is ubiquitous, and structural health monitoring (SHM)
is no exception. Specifically, we address the problem of impact localization on
shell-like structures, where knowledge of impact locations aids in assessing
structural integrity. Impacts on thin-walled structures excite Lamb waves,
which can be measured with piezoelectric sensors. Their dispersive
characteristics make it difficult to detect and localize impacts by
conventional methods. In the present contribution, we explore the localization
of impacts using neural networks. In particular, we propose to use {recurrent
neural networks} (RNNs) to estimate impact positions end-to-end, i.e., directly
from {sequential sensor data}. We deal with comparatively long sequences of
thousands of samples, since high sampling rate are needed to accurately capture
elastic waves. For this reason, the proposed approach builds upon Gated
Recurrent Units (GRUs), which are less prone to vanishing gradients as compared
to conventional RNNs. Quality and quantity of data are crucial when training
neural networks. Often, synthetic data is used, which inevitably introduces a
reality gap. Here, by contrast, we train our networks using {physical data from
experiments}, which requires automation to handle the large number of
experiments needed. For this purpose, a {robot is used to drop steel balls}
onto an {aluminum plate} equipped with {piezoceramic sensors}. Our results show
remarkable accuracy in estimating impact positions, even with a comparatively
small dataset.

</details>


### [315] [Density Ratio-based Causal Discovery from Bivariate Continuous-Discrete Data](https://arxiv.org/pdf/2505.08371)
*Takashi Nicholas Maeda, Shohei Shimizu, Hidetoshi Matsui*

Main category: cs.LG

TL;DR: A novel causal discovery method for mixed bivariate data (continuous and discrete) using monotonicity of conditional density ratios, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail in bivariate settings due to reliance on conditional independence tests or unfair comparisons between variable types.

Method: Analyzes monotonicity of conditional density ratios to determine causal direction without strong assumptions.

Result: Theoretical proof and experiments show superior accuracy over existing methods.

Conclusion: Provides a principled, assumption-free way to infer causality in mixed bivariate data.

Abstract: This paper proposes a causal discovery method for mixed bivariate data
consisting of one continuous and one discrete variable. Existing
constraint-based approaches are ineffective in the bivariate setting, as they
rely on conditional independence tests that are not suited to bivariate data.
Score-based methods either impose strong distributional assumptions or face
challenges in fairly comparing causal directions between variables of different
types, due to differences in their information content. We introduce a novel
approach that determines causal direction by analyzing the monotonicity of the
conditional density ratio of the continuous variable, conditioned on different
values of the discrete variable. Our theoretical analysis shows that the
conditional density ratio exhibits monotonicity when the continuous variable
causes the discrete variable, but not in the reverse direction. This property
provides a principled basis for comparing causal directions between variables
of different types, free from strong distributional assumptions and bias
arising from differences in their information content. We demonstrate its
effectiveness through experiments on both synthetic and real-world datasets,
showing superior accuracy compared to existing methods.

</details>


### [316] [ConDiSim: Conditional Diffusion Models for Simulation Based Inference](https://arxiv.org/pdf/2505.08403)
*Mayank Nautiyal, Andreas Hellander, Prashant Singh*

Main category: cs.LG

TL;DR: ConDiSim is a conditional diffusion model for simulation-based inference, using denoising diffusion to approximate complex posterior distributions efficiently.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of inferring parameters in systems with intractable likelihoods, where traditional methods struggle with complex dependencies and multi-modalities.

Method: Leverages denoising diffusion probabilistic models: a forward process adds Gaussian noise, and a reverse process learns to denoise, conditioned on observed data.

Result: Evaluated on ten benchmarks and two real-world problems, ConDiSim shows accurate posterior approximation, computational efficiency, and stable training.

Conclusion: ConDiSim provides a robust, extensible framework for fast and accurate simulation-based inference, ideal for parameter inference workflows.

Abstract: We present a conditional diffusion model - ConDiSim, for simulation-based
inference of complex systems with intractable likelihoods. ConDiSim leverages
denoising diffusion probabilistic models to approximate posterior
distributions, consisting of a forward process that adds Gaussian noise to
parameters, and a reverse process learning to denoise, conditioned on observed
data. This approach effectively captures complex dependencies and
multi-modalities within posteriors. ConDiSim is evaluated across ten benchmark
problems and two real-world test problems, where it demonstrates effective
posterior approximation accuracy while maintaining computational efficiency and
stability in model training. ConDiSim offers a robust and extensible framework
for simulation-based inference, particularly suitable for parameter inference
workflows requiring fast inference methods.

</details>


### [317] [Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter Impact on Performance and Efficiency](https://arxiv.org/pdf/2505.08445)
*Adel Ammar, Anis Koubaa, Omer Nacar, Wadii Boulila*

Main category: cs.LG

TL;DR: The paper analyzes hyperparameters in RAG systems, revealing trade-offs between speed and accuracy, with Chroma being faster and Faiss more precise. Fixed-length chunking and re-ranking are evaluated, and corrective RAG achieves near-perfect context precision.


<details>
  <summary>Details</summary>
Motivation: To address hallucinations and outdated knowledge in large language models by optimizing RAG systems for speed, quality, and retrieval accuracy.

Method: Evaluates hyperparameters like vector stores (Chroma, Faiss), chunking policies, re-ranking, and temperature. Measures six metrics: faithfulness, correctness, relevancy, precision, recall, and similarity.

Result: Chroma is 13% faster; Faiss has higher precision. Fixed-length chunking is quickest. Re-ranking improves quality but increases runtime. Corrective RAG achieves 99% context precision.

Conclusion: Practitioners can balance cost and accuracy in RAG systems, with corrective RAG enabling high retrieval accuracy, crucial for applications like healthcare.

Abstract: Large language models achieve high task performance yet often hallucinate or
rely on outdated knowledge. Retrieval-augmented generation (RAG) addresses
these gaps by coupling generation with external search. We analyse how
hyperparameters influence speed and quality in RAG systems, covering Chroma and
Faiss vector stores, chunking policies, cross-encoder re-ranking, and
temperature, and we evaluate six metrics: faithfulness, answer correctness,
answer relevancy, context precision, context recall, and answer similarity.
Chroma processes queries 13% faster, whereas Faiss yields higher retrieval
precision, revealing a clear speed-accuracy trade-off. Naive fixed-length
chunking with small windows and minimal overlap outperforms semantic
segmentation while remaining the quickest option. Re-ranking provides modest
gains in retrieval quality yet increases runtime by roughly a factor of 5, so
its usefulness depends on latency constraints. These results help practitioners
balance computational cost and accuracy when tuning RAG systems for
transparent, up-to-date responses. Finally, we re-evaluate the top
configurations with a corrective RAG workflow and show that their advantages
persist when the model can iteratively request additional evidence. We obtain a
near-perfect context precision (99%), which demonstrates that RAG systems can
achieve extremely high retrieval accuracy with the right combination of
hyperparameters, with significant implications for applications where retrieval
quality directly impacts downstream task performance, such as clinical decision
support in healthcare.

</details>


### [318] [An adaptive sampling algorithm for data-generation to build a data-manifold for physical problem surrogate modeling](https://arxiv.org/pdf/2505.08487)
*Chetra Mang, Axel TahmasebiMoradi, David Danan, Mouadh Yagoubi*

Main category: cs.LG

TL;DR: The paper introduces ASADG, an adaptive sampling algorithm for generating balanced input data to improve surrogate model training for PDE-based physical models, outperforming LHS in manifold representation.


<details>
  <summary>Details</summary>
Motivation: Traditional PDE solvers are computationally expensive, and surrogate models trained on imbalanced data struggle with accuracy due to poor manifold representation.

Method: ASADG iteratively adds input data by evaluating barycenters of simplicial complexes in the discretized manifold, ensuring better representation.

Result: ASADG generates more representative input data than LHS, improving surrogate model accuracy for a harmonic transport problem.

Conclusion: ASADG effectively enhances data sampling for surrogate models, addressing imbalance issues in PDE-based applications.

Abstract: Physical models classically involved Partial Differential equations (PDE) and
depending of their underlying complexity and the level of accuracy required,
and known to be computationally expensive to numerically solve them. Thus, an
idea would be to create a surrogate model relying on data generated by such
solver. However, training such a model on an imbalanced data have been shown to
be a very difficult task. Indeed, if the distribution of input leads to a poor
response manifold representation, the model may not learn well and
consequently, it may not predict the outcome with acceptable accuracy. In this
work, we present an Adaptive Sampling Algorithm for Data Generation (ASADG)
involving a physical model. As the initial input data may not accurately
represent the response manifold in higher dimension, this algorithm iteratively
adds input data into it. At each step the barycenter of each simplicial
complex, that the manifold is discretized into, is added as new input data, if
a certain threshold is satisfied. We demonstrate the efficiency of the data
sampling algorithm in comparison with LHS method for generating more
representative input data. To do so, we focus on the construction of a harmonic
transport problem metamodel by generating data through a classical solver. By
using such algorithm, it is possible to generate the same number of input data
as LHS while providing a better representation of the response manifold.

</details>


### [319] [Isolation Forest in Novelty Detection Scenario](https://arxiv.org/pdf/2505.08489)
*Adam Ulrich, Jan Krňávek, Roman Šenkeřík, Zuzana Komínková Oplatková, Radek Vala*

Main category: cs.LG

TL;DR: The paper proposes a modified Half-Space Tree (HST) algorithm for novelty detection, demonstrating its effectiveness through probabilistic analysis and comparisons with Isolation Forest.


<details>
  <summary>Details</summary>
Motivation: Classic anomaly detection algorithms like One-Class SVM and LOF lack interpretability and scalability, prompting the need for an improved method for novelty detection.

Method: The authors adapt the HST algorithm for novelty detection by focusing on higher tree leaves where anomalies appear, supported by probabilistic analysis and expected depth calculations.

Result: The modified HST shows novelty points are more isolated than in Isolation Forest, proving its potential as an interpretable and efficient novelty detector.

Conclusion: The paper provides a theoretical foundation for adapting HSTs for novelty detection, paving the way for further applications and experiments.

Abstract: Data mining offers a diverse toolbox for extracting meaningful structures
from complex datasets, with anomaly detection emerging as a critical subfield
particularly in the context of streaming or real-time data. Within anomaly
detection, novelty detection focuses on identifying previously unseen patterns
after training solely on regular data. While classic algorithms such as
One-Class SVM or Local Outlier Factor (LOF) have been widely applied, they
often lack interpretability and scalability. In this work, we explore the
Half-Space Tree (HST) algorithm, originally proposed for streaming anomaly
detection, and propose a novel theoretical modification to adapt it
specifically for novelty detection tasks. Our approach is grounded in the idea
that anomalies i.e., novelties tend to appear in the higher leaves of the tree,
which are less frequently visited by regular instances. We analytically
demonstrate the effectiveness of this approach using probabilistic analysis,
expected depth (EXD) calculations, and combinatorial reasoning. A comparative
analysis of expected depths between our modified HST and the original Isolation
Forest highlights that novelty points are significantly more isolated in our
approach. This supports the hypothesis that HSTs, with appropriate structural
adaptation, can serve as interpretable and efficient novelty detectors. The
paper contributes a theoretical foundation and supporting analysis for this
adaptation, setting the stage for further application and experimentation.

</details>


### [320] [A new methodology to decompose a parametric domain using reduced order data manifold in machine learning](https://arxiv.org/pdf/2505.08497)
*Chetra Mang, Axel TahmasebiMoradi, Mouadh Yagoubi*

Main category: cs.LG

TL;DR: A new method for parametric domain decomposition using iterative PCA, with reconstruction of inverse projection and numerical validation.


<details>
  <summary>Details</summary>
Motivation: To efficiently decompose parametric domains by reducing high-dimensional manifolds to lower dimensions and validating against classical methods.

Method: Uses iterative PCA for dimensionality reduction, develops inverse projection techniques, and decomposes domains based on low-dimensional manifolds.

Result: Demonstrated efficiency and effectiveness in harmonic transport problems compared to neural networks.

Conclusion: The proposed method outperforms classical meta-models like neural networks in parametric domain decomposition.

Abstract: We propose a new methodology for parametric domain decomposition using
iterative principal component analysis. Starting with iterative principle
component analysis, the high dimension manifold is reduced to the lower
dimension manifold. Moreover, two approaches are developed to reconstruct the
inverse projector to project from the lower data component to the original one.
Afterward, we provide a detailed strategy to decompose the parametric domain
based on the low dimension manifold. Finally, numerical examples of harmonic
transport problem are given to illustrate the efficiency and effectiveness of
the proposed method comparing to the classical meta-models such as neural
networks.

</details>


### [321] [InfoPO: On Mutual Information Maximization for Large Language Model Alignment](https://arxiv.org/pdf/2505.08507)
*Teng Xiao, Zhen Ge, Sujay Sanghavi, Tian Wang, Julian Katz-Samuels, Marc Versage, Qingjun Cui, Trishul Chilimbi*

Main category: cs.LG

TL;DR: InfoPO is a new preference fine-tuning algorithm for LLMs that avoids reliance on the Bradley-Terry model, improving performance on reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for aligning LLMs with human preferences rely on the Bradley-Terry model, leading to overfitting and suboptimal performance, especially in reasoning-heavy tasks.

Method: Proposes InfoPO, a principled algorithm that fine-tunes LLMs using preference data without the Bradley-Terry model, ensuring the likelihood of chosen responses doesn't decrease.

Result: InfoPO outperforms established baselines on open benchmarks, particularly in reasoning tasks.

Conclusion: InfoPO offers a more effective and efficient way to align LLMs with human preferences, addressing limitations of existing methods.

Abstract: We study the post-training of large language models (LLMs) with human
preference data. Recently, direct preference optimization and its variants have
shown considerable promise in aligning language models, eliminating the need
for reward models and online sampling. Despite these benefits, these methods
rely on explicit assumptions about the Bradley-Terry (BT) model, which makes
them prone to overfitting and results in suboptimal performance, particularly
on reasoning-heavy tasks. To address these challenges, we propose a principled
preference fine-tuning algorithm called InfoPO, which effectively and
efficiently aligns large language models using preference data. InfoPO
eliminates the reliance on the BT model and prevents the likelihood of the
chosen response from decreasing. Extensive experiments confirm that InfoPO
consistently outperforms established baselines on widely used open benchmarks,
particularly in reasoning tasks.

</details>


### [322] [Learning Advanced Self-Attention for Linear Transformers in the Singular Value Domain](https://arxiv.org/pdf/2505.08516)
*Hyowon Wi, Jeongwhan Choi, Noseong Park*

Main category: cs.LG

TL;DR: The paper proposes AGF, a method interpreting self-attention as a graph filter in the singular value domain, improving performance by leveraging diverse frequency information.


<details>
  <summary>Details</summary>
Motivation: Existing self-attention mechanisms are simplified and act as low-pass filters, limiting their ability to utilize various frequency information effectively.

Method: AGF interprets self-attention as learning a graph filter in the singular value domain, with linear complexity relative to input length.

Result: AGF achieves state-of-the-art performance on tasks like Long Range Arena benchmark and time series classification.

Conclusion: AGF offers a more effective approach to self-attention by leveraging graph signal processing, enhancing performance across diverse tasks.

Abstract: Transformers have demonstrated remarkable performance across diverse domains.
The key component of Transformers is self-attention, which learns the
relationship between any two tokens in the input sequence. Recent studies have
revealed that the self-attention can be understood as a normalized adjacency
matrix of a graph. Notably, from the perspective of graph signal processing
(GSP), the self-attention can be equivalently defined as a simple graph filter,
applying GSP using the value vector as the signal. However, the self-attention
is a graph filter defined with only the first order of the polynomial matrix,
and acts as a low-pass filter preventing the effective leverage of various
frequency information. Consequently, existing self-attention mechanisms are
designed in a rather simplified manner. Therefore, we propose a novel method,
called \underline{\textbf{A}}ttentive \underline{\textbf{G}}raph
\underline{\textbf{F}}ilter (AGF), interpreting the self-attention as learning
the graph filter in the singular value domain from the perspective of graph
signal processing for directed graphs with the linear complexity w.r.t. the
input length $n$, i.e., $\mathcal{O}(nd^2)$. In our experiments, we demonstrate
that AGF achieves state-of-the-art performance on various tasks, including Long
Range Arena benchmark and time series classification.

</details>


### [323] [GradMix: Gradient-based Selective Mixup for Robust Data Augmentation in Class-Incremental Learning](https://arxiv.org/pdf/2505.08528)
*Minsu Kim, Seong-Hyeon Hwang, Steven Euijong Whang*

Main category: cs.LG

TL;DR: GradMix, a gradient-based selective mixup method, improves continual learning by reducing catastrophic forgetting through class-based sample mixing.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of catastrophic forgetting in continual learning, where existing data augmentation methods may harm previous task knowledge.

Method: Proposes GradMix, which selectively mixes samples from helpful class pairs using gradient-based criteria to mitigate forgetting.

Result: Outperforms baseline methods in accuracy by minimizing knowledge loss of previous tasks.

Conclusion: GradMix effectively reduces catastrophic forgetting in class-incremental learning, enhancing model performance.

Abstract: In the context of continual learning, acquiring new knowledge while
maintaining previous knowledge presents a significant challenge. Existing
methods often use experience replay techniques that store a small portion of
previous task data for training. In experience replay approaches, data
augmentation has emerged as a promising strategy to further improve the model
performance by mixing limited previous task data with sufficient current task
data. However, we theoretically and empirically analyze that training with
mixed samples from random sample pairs may harm the knowledge of previous tasks
and cause greater catastrophic forgetting. We then propose GradMix, a robust
data augmentation method specifically designed for mitigating catastrophic
forgetting in class-incremental learning. GradMix performs gradient-based
selective mixup using a class-based criterion that mixes only samples from
helpful class pairs and not from detrimental class pairs for reducing
catastrophic forgetting. Our experiments on various real datasets show that
GradMix outperforms data augmentation baselines in accuracy by minimizing the
forgetting of previous knowledge.

</details>


### [324] [ExEBench: Benchmarking Foundation Models on Extreme Earth Events](https://arxiv.org/pdf/2505.08529)
*Shan Zhao, Zhitong Xiong, Jie Zhao, Xiao Xiang Zhu*

Main category: cs.LG

TL;DR: The paper introduces ExEBench, a benchmark dataset for evaluating foundation models (FMs) in extreme event management, addressing biases and promoting ML advancements for disaster resilience.


<details>
  <summary>Details</summary>
Motivation: Extreme events pose significant risks, and while FMs show promise, their biases limit reliability. ExEBench aims to assess and improve FM performance in disaster management.

Method: The study develops ExEBench, a dataset with seven extreme event categories, diverse data sources, and ML tasks aligned with operational needs.

Result: ExEBench provides a platform to evaluate FM generalizability, develop new ML methods, and analyze extreme event interactions under climate change.

Conclusion: ExEBench is a public resource to enhance FM reliability and disaster management, fostering understanding of Earth systems amid climate change.

Abstract: Our planet is facing increasingly frequent extreme events, which pose major
risks to human lives and ecosystems. Recent advances in machine learning (ML),
especially with foundation models (FMs) trained on extensive datasets, excel in
extracting features and show promise in disaster management. Nevertheless,
these models often inherit biases from training data, challenging their
performance over extreme values. To explore the reliability of FM in the
context of extreme events, we introduce \textbf{ExE}Bench (\textbf{Ex}treme
\textbf{E}arth Benchmark), a collection of seven extreme event categories
across floods, wildfires, storms, tropical cyclones, extreme precipitation,
heatwaves, and cold waves. The dataset features global coverage, varying data
volumes, and diverse data sources with different spatial, temporal, and
spectral characteristics. To broaden the real-world impact of FMs, we include
multiple challenging ML tasks that are closely aligned with operational needs
in extreme events detection, monitoring, and forecasting. ExEBench aims to (1)
assess FM generalizability across diverse, high-impact tasks and domains, (2)
promote the development of novel ML methods that benefit disaster management,
and (3) offer a platform for analyzing the interactions and cascading effects
of extreme events to advance our understanding of Earth system, especially
under the climate change expected in the decades to come. The dataset and code
are public https://github.com/zhaoshan2/EarthExtreme-Bench.

</details>


### [325] [OLinear: A Linear Model for Time Series Forecasting in Orthogonally Transformed Domain](https://arxiv.org/pdf/2505.08550)
*Wenzhen Yue, Yong Liu, Haoxuan Li, Hao Wang, Xianghua Ying, Ruohao Guo, Bowei Xing, Ji Shi*

Main category: cs.LG

TL;DR: OLinear is a linear-based multivariate time series forecasting model using an orthogonal transformation for improved performance.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of temporal forecast paradigms by leveraging decorrelated feature domains for better encoding and decoding.

Method: Uses OrthoTrans for data-adaptive orthogonal transformation and NormLin, a normalized linear layer, to capture multivariate dependencies.

Result: Achieves state-of-the-art performance on 24 benchmarks and 140 tasks, with NormLin outperforming self-attention in efficiency.

Conclusion: OLinear is efficient and effective, with NormLin enhancing Transformer-based models as a plug-in module.

Abstract: This paper presents $\mathbf{OLinear}$, a $\mathbf{linear}$-based
multivariate time series forecasting model that operates in an
$\mathbf{o}$rthogonally transformed domain. Recent forecasting models typically
adopt the temporal forecast (TF) paradigm, which directly encode and decode
time series in the time domain. However, the entangled step-wise dependencies
in series data can hinder the performance of TF. To address this, some
forecasters conduct encoding and decoding in the transformed domain using
fixed, dataset-independent bases (e.g., sine and cosine signals in the Fourier
transform). In contrast, we utilize $\mathbf{OrthoTrans}$, a data-adaptive
transformation based on an orthogonal matrix that diagonalizes the series'
temporal Pearson correlation matrix. This approach enables more effective
encoding and decoding in the decorrelated feature domain and can serve as a
plug-in module to enhance existing forecasters. To enhance the representation
learning for multivariate time series, we introduce a customized linear layer,
$\mathbf{NormLin}$, which employs a normalized weight matrix to capture
multivariate dependencies. Empirically, the NormLin module shows a surprising
performance advantage over multi-head self-attention, while requiring nearly
half the FLOPs. Extensive experiments on 24 benchmarks and 140 forecasting
tasks demonstrate that OLinear consistently achieves state-of-the-art
performance with high efficiency. Notably, as a plug-in replacement for
self-attention, the NormLin module consistently enhances Transformer-based
forecasters. The code and datasets are available at
https://anonymous.4open.science/r/OLinear

</details>


### [326] [Online Learning and Unlearning](https://arxiv.org/pdf/2505.08557)
*Yaxi Hu, Bernhard Schölkopf, Amartya Sanyal*

Main category: cs.LG

TL;DR: The paper introduces online learning-unlearning (OLU) algorithms, ensuring model outputs remain indistinguishable after unlearning, with competitive regret bounds.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of updating models sequentially in an online setting while accommodating unlearning requests without compromising statistical indistinguishability.

Method: Two OLU algorithms: passive OLU (noise injection during unlearning) and active OLU (offline unlearning to shift the model). Both are based on online gradient descent (OGD).

Result: Both methods achieve regret bounds comparable to standard OGD, proving competitive performance with unlearning guarantees.

Conclusion: The proposed OLU algorithms effectively balance online learning and unlearning, maintaining performance while ensuring data deletion compliance.

Abstract: We formalize the problem of online learning-unlearning, where a model is
updated sequentially in an online setting while accommodating unlearning
requests between updates. After a data point is unlearned, all subsequent
outputs must be statistically indistinguishable from those of a model trained
without that point. We present two online learner-unlearner (OLU) algorithms,
both built upon online gradient descent (OGD). The first, passive OLU,
leverages OGD's contractive property and injects noise when unlearning occurs,
incurring no additional computation. The second, active OLU, uses an offline
unlearning algorithm that shifts the model toward a solution excluding the
deleted data. Under standard convexity and smoothness assumptions, both methods
achieve regret bounds comparable to those of standard OGD, demonstrating that
one can maintain competitive regret bounds while providing unlearning
guarantees.

</details>


### [327] [MUBox: A Critical Evaluation Framework of Deep Machine Unlearning](https://arxiv.org/pdf/2505.08576)
*Xiang Li, Bhavani Thuraisingham, Wenqi Wei*

Main category: cs.LG

TL;DR: MUBox is a platform for evaluating machine unlearning methods in deep learning, integrating 23 techniques and 11 metrics across six scenarios. It reveals inconsistencies in state-of-the-art methods and the need for diverse metrics.


<details>
  <summary>Details</summary>
Motivation: The right to be forgotten requires effective data removal from ML models, but current unlearning methods lack comprehensive evaluation frameworks.

Method: MUBox integrates 23 unlearning techniques and tests them across six scenarios using 11 metrics, enabling systematic comparison and analysis.

Result: State-of-the-art unlearning methods show inconsistent effectiveness, and no single metric fully captures performance. Depoisoning effectiveness varies by attack type.

Conclusion: MUBox highlights the need for broader evaluation scenarios and multiple metrics to assess unlearning methods effectively, addressing gaps in current research.

Abstract: Recent legal frameworks have mandated the right to be forgotten, obligating
the removal of specific data upon user requests. Machine Unlearning has emerged
as a promising solution by selectively removing learned information from
machine learning models. This paper presents MUBox, a comprehensive platform
designed to evaluate unlearning methods in deep learning. MUBox integrates 23
advanced unlearning techniques, tested across six practical scenarios with 11
diverse evaluation metrics. It allows researchers and practitioners to (1)
assess and compare the effectiveness of different machine unlearning methods
across various scenarios; (2) examine the impact of current evaluation metrics
on unlearning performance; and (3) conduct detailed comparative studies on
machine unlearning in a unified framework. Leveraging MUBox, we systematically
evaluate these unlearning methods in deep learning and uncover several key
insights: (a) Even state-of-the-art unlearning methods, including those
published in top-tier venues and winners of unlearning competitions,
demonstrate inconsistent effectiveness across diverse scenarios. Prior research
has predominantly focused on simplified settings, such as random forgetting and
class-wise unlearning, highlighting the need for broader evaluations across
more difficult unlearning tasks. (b) Assessing unlearning performance remains a
non-trivial problem, as no single evaluation metric can comprehensively capture
the effectiveness, efficiency, and preservation of model utility. Our findings
emphasize the necessity of employing multiple metrics to achieve a balanced and
holistic assessment of unlearning methods. (c) In the context of depoisoning,
our evaluation reveals significant variability in the effectiveness of existing
approaches, which is highly dependent on the specific type of poisoning
attacks.

</details>


### [328] [Clustering of Incomplete Data via a Bipartite Graph Structure](https://arxiv.org/pdf/2505.08594)
*Amirhossein Javaheri, Daniel P. Palomar*

Main category: cs.LG

TL;DR: Proposes a bipartite graph-based clustering method for incomplete and heavy-tailed data, validated with financial datasets.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations in existing graph learning methods, such as requiring center node data and poor performance with heavy-tailed distributions.

Method: Uses a bipartite graph model to infer clusters without center node data and handles heavy-tailed distributions effectively.

Result: Numerical experiments confirm the method's efficiency for clustering financial data.

Conclusion: The proposed method successfully overcomes key challenges in graph-based clustering for practical scenarios.

Abstract: There are various approaches to graph learning for data clustering,
incorporating different spectral and structural constraints through diverse
graph structures. Some methods rely on bipartite graph models, where nodes are
divided into two classes: centers and members. These models typically require
access to data for the center nodes in addition to observations from the member
nodes. However, such additional data may not always be available in many
practical scenarios. Moreover, popular Gaussian models for graph learning have
demonstrated limited effectiveness in modeling data with heavy-tailed
distributions, which are common in financial markets. In this paper, we propose
a clustering method based on a bipartite graph model that addresses these
challenges. First, it can infer clusters from incomplete data without requiring
information about the center nodes. Second, it is designed to effectively
handle heavy-tailed data. Numerical experiments using real financial data
validate the efficiency of the proposed method for data clustering.

</details>


### [329] [Cost Function Estimation Using Inverse Reinforcement Learning with Minimal Observations](https://arxiv.org/pdf/2505.08619)
*Sarmad Mehrdad, Avadesh Meduri, Ludovic Righetti*

Main category: cs.LG

TL;DR: An iterative inverse reinforcement learning algorithm for continuous spaces, using maximum entropy criteria and optimal control for trajectory generation, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To infer optimal cost functions efficiently in continuous spaces without requiring large sample sets.

Method: Iterative weight improvement with step size tuning, using optimal control for trajectory generation.

Result: Outperforms two state-of-the-art algorithms in simulated environments.

Conclusion: The method is efficient, requires fewer samples, and generates more informative trajectories.

Abstract: We present an iterative inverse reinforcement learning algorithm to infer
optimal cost functions in continuous spaces. Based on a popular maximum entropy
criteria, our approach iteratively finds a weight improvement step and proposes
a method to find an appropriate step size that ensures learned cost function
features remain similar to the demonstrated trajectory features. In contrast to
similar approaches, our algorithm can individually tune the effectiveness of
each observation for the partition function and does not need a large sample
set, enabling faster learning. We generate sample trajectories by solving an
optimal control problem instead of random sampling, leading to more informative
trajectories. The performance of our method is compared to two state of the art
algorithms to demonstrate its benefits in several simulated environments.

</details>


### [330] [Credit Assignment and Efficient Exploration based on Influence Scope in Multi-agent Reinforcement Learning](https://arxiv.org/pdf/2505.08630)
*Shuai Han, Mehdi Dastani, Shihan Wang*

Main category: cs.LG

TL;DR: A novel MARL method, Influence Scope of Agents (ISA), addresses credit assignment and exploration in sparse-reward settings by analyzing agents' influence on state attributes.


<details>
  <summary>Details</summary>
Motivation: Training cooperative agents in sparse-reward scenarios is challenging due to unclear feedback and ineffective exploration in prior methods.

Method: Proposes ISA, which calculates agents' influence on state dimensions to assign credit and limit exploration space.

Result: ISA outperforms state-of-the-art baselines in sparse-reward multi-agent scenarios.

Conclusion: ISA effectively solves credit assignment and exploration issues in sparse-reward MARL.

Abstract: Training cooperative agents in sparse-reward scenarios poses significant
challenges for multi-agent reinforcement learning (MARL). Without clear
feedback on actions at each step in sparse-reward setting, previous methods
struggle with precise credit assignment among agents and effective exploration.
In this paper, we introduce a novel method to deal with both credit assignment
and exploration problems in reward-sparse domains. Accordingly, we propose an
algorithm that calculates the Influence Scope of Agents (ISA) on states by
taking specific value of the dimensions/attributes of states that can be
influenced by individual agents. The mutual dependence between agents' actions
and state attributes are then used to calculate the credit assignment and to
delimit the exploration space for each individual agent. We then evaluate ISA
in a variety of sparse-reward multi-agent scenarios. The results show that our
method significantly outperforms the state-of-art baselines.

</details>


### [331] [Modular Federated Learning: A Meta-Framework Perspective](https://arxiv.org/pdf/2505.08646)
*Frederico Vicente, Cláudia Soares, Dušan Jakovetić*

Main category: cs.LG

TL;DR: The paper presents a meta-framework for Federated Learning (FL), emphasizing modular components, historical evolution, and a novel taxonomy of Aggregation vs. Alignment. It also explores practical FL frameworks and systematizes challenges.


<details>
  <summary>Details</summary>
Motivation: To provide a structured understanding of FL's methodologies, challenges, and applications, addressing its complexity and multifaceted nature.

Method: Introduces a meta-framework perspective, modular components, historical contextualization, and a novel taxonomy (Aggregation vs. Alignment). Examines Python FL frameworks for practical implementation.

Result: A holistic foundation for FL research, highlighting core aspects like communication, optimization, security, and privacy, along with open research questions.

Conclusion: The meta-framework and taxonomy offer an adaptable and comprehensive approach to advancing FL research and deployment.

Abstract: Federated Learning (FL) enables distributed machine learning training while
preserving privacy, representing a paradigm shift for data-sensitive and
decentralized environments. Despite its rapid advancements, FL remains a
complex and multifaceted field, requiring a structured understanding of its
methodologies, challenges, and applications. In this survey, we introduce a
meta-framework perspective, conceptualising FL as a composition of modular
components that systematically address core aspects such as communication,
optimisation, security, and privacy. We provide a historical contextualisation
of FL, tracing its evolution from distributed optimisation to modern
distributed learning paradigms. Additionally, we propose a novel taxonomy
distinguishing Aggregation from Alignment, introducing the concept of alignment
as a fundamental operator alongside aggregation. To bridge theory with
practice, we explore available FL frameworks in Python, facilitating real-world
implementation. Finally, we systematise key challenges across FL sub-fields,
providing insights into open research questions throughout the meta-framework
modules. By structuring FL within a meta-framework of modular components and
emphasising the dual role of Aggregation and Alignment, this survey provides a
holistic and adaptable foundation for understanding and advancing FL research
and deployment.

</details>


### [332] [AC-PKAN: Attention-Enhanced and Chebyshev Polynomial-Based Physics-Informed Kolmogorov-Arnold Networks](https://arxiv.org/pdf/2505.08687)
*Hangwei Zhang, Zhimu Huang, Yan Wang*

Main category: cs.LG

TL;DR: AC-PKAN enhances Chebyshev1KANs with wavelet-activated MLPs and attention mechanisms, improving PDE-solving performance and stability.


<details>
  <summary>Details</summary>
Motivation: Original KANs and Chebyshev1KANs suffer from computational intensity and rank collapse, limiting their expressive capacity.

Method: Integrates wavelet-activated MLPs, internal attention, and Residual Gradient Attention (RGA) to enhance stability and expressive power.

Result: Outperforms state-of-the-art models like PINNsFormer in benchmark tasks across three domains.

Conclusion: AC-PKAN is a robust tool for solving PDEs in data-sparse scenarios, extending KANs' capabilities.

Abstract: Kolmogorov-Arnold Networks (KANs) have recently shown promise for solving
partial differential equations (PDEs). Yet their original formulation is
computationally and memory intensive, motivating the introduction of Chebyshev
Type-I-based KANs (Chebyshev1KANs). Although Chebyshev1KANs have outperformed
the vanilla KANs architecture, our rigorous theoretical analysis reveals that
they still suffer from rank collapse, ultimately limiting their expressive
capacity. To overcome these limitations, we enhance Chebyshev1KANs by
integrating wavelet-activated MLPs with learnable parameters and an internal
attention mechanism. We prove that this design preserves a full-rank Jacobian
and is capable of approximating solutions to PDEs of arbitrary order.
Furthermore, to alleviate the loss instability and imbalance introduced by the
Chebyshev polynomial basis, we externally incorporate a Residual Gradient
Attention (RGA) mechanism that dynamically re-weights individual loss terms
according to their gradient norms and residual magnitudes. By jointly
leveraging internal and external attention, we present AC-PKAN, a novel
architecture that constitutes an enhancement to weakly supervised
Physics-Informed Neural Networks (PINNs) and extends the expressive power of
KANs. Experimental results from nine benchmark tasks across three domains show
that AC-PKAN consistently outperforms or matches state-of-the-art models such
as PINNsFormer, establishing it as a highly effective tool for solving complex
real-world engineering problems in zero-data or data-sparse regimes. The code
will be made publicly available upon acceptance.

</details>


### [333] [PWC-MoE: Privacy-Aware Wireless Collaborative Mixture of Experts](https://arxiv.org/pdf/2505.08719)
*Yang Su, Na Yan, Yansha Deng, Robert Schober*

Main category: cs.LG

TL;DR: The paper proposes PWC-MoE, a framework balancing privacy, performance, and bandwidth by routing sensitive tokens locally and non-sensitive tokens remotely, with load-balancing and adaptive offloading.


<details>
  <summary>Details</summary>
Motivation: Address privacy and bandwidth challenges in deploying large language models (LLMs) by combining local and remote processing.

Method: Uses a sparse privacy-aware gating network to route tokens, load-balancing, and bandwidth-adaptive token offloading.

Result: PWC-MoE effectively preserves privacy and maintains high performance in bandwidth-constrained environments.

Conclusion: PWC-MoE offers a practical solution for deploying LLMs in privacy-sensitive and bandwidth-limited scenarios.

Abstract: Large language models (LLMs) hosted on cloud servers alleviate the
computational and storage burdens on local devices but raise privacy concerns
due to sensitive data transmission and require substantial communication
bandwidth, which is challenging in constrained environments. In contrast, small
language models (SLMs) running locally enhance privacy but suffer from limited
performance on complex tasks. To balance computational cost, performance, and
privacy protection under bandwidth constraints, we propose a privacy-aware
wireless collaborative mixture of experts (PWC-MoE) framework. Specifically,
PWC-MoE employs a sparse privacy-aware gating network to dynamically route
sensitive tokens to privacy experts located on local clients, while
non-sensitive tokens are routed to non-privacy experts located at the remote
base station. To achieve computational efficiency, the gating network ensures
that each token is dynamically routed to and processed by only one expert. To
enhance scalability and prevent overloading of specific experts, we introduce a
group-wise load-balancing mechanism for the gating network that evenly
distributes sensitive tokens among privacy experts and non-sensitive tokens
among non-privacy experts. To adapt to bandwidth constraints while preserving
model performance, we propose a bandwidth-adaptive and importance-aware token
offloading scheme. This scheme incorporates an importance predictor to evaluate
the importance scores of non-sensitive tokens, prioritizing the most important
tokens for transmission to the base station based on their predicted importance
and the available bandwidth. Experiments demonstrate that the PWC-MoE framework
effectively preserves privacy and maintains high performance even in
bandwidth-constrained environments, offering a practical solution for deploying
LLMs in privacy-sensitive and bandwidth-limited scenarios.

</details>


### [334] [Memorization-Compression Cycles Improve Generalization](https://arxiv.org/pdf/2505.08727)
*Fangyuan Yu*

Main category: cs.LG

TL;DR: The paper shows that compressing internal representations improves generalization in language models. It introduces IBLM and GAPT, achieving better performance and OOD generalization.


<details>
  <summary>Details</summary>
Motivation: To improve generalization in language models by balancing memorization and compression, inspired by biological learning cycles.

Method: Introduces IBLM for constrained optimization and GAPT, an adaptive training algorithm switching between memorization and compression phases.

Result: GAPT reduces MBE by 50%, improves cross-entropy by 4.8%, and enhances OOD generalization by 35%. It also mitigates catastrophic forgetting.

Conclusion: Compression and adaptive training (GAPT) significantly improve model performance and generalization, mirroring biological learning cycles.

Abstract: We prove theoretically that generalization improves not only through data
scaling but also by compressing internal representations. To operationalize
this insight, we introduce the Information Bottleneck Language Modeling (IBLM)
objective, which reframes language modeling as a constrained optimization
problem: minimizing representation entropy subject to optimal prediction
performance. Empirically, we observe an emergent memorization-compression cycle
during LLM pretraining, evidenced by oscillation positive/negative gradient
alignment between cross-entropy and Matrix-Based Entropy (MBE), a measure of
representation entropy. This pattern closely mirrors the predictive-compressive
trade-off prescribed by IBLM and also parallels the biological alternation
between awake learning and sleep consolidation. Motivated by this observation,
we propose Gated Phase Transition (GAPT), a training algorithm that adaptively
switches between memorization and compression phases. When applied to GPT-2
pretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves
cross-entropy by 4.8%. GAPT improves OOD generalizatino by 35% in a pretraining
task on arithmetic multiplication. In a setting designed to simulate
catastrophic forgetting, GAPT reduces interference by compressing and
separating representations, achieving a 97% improvement in separation -
paralleling the functional role of sleep consolidation.

</details>


### [335] [Preference Optimization for Combinatorial Optimization Problems](https://arxiv.org/pdf/2505.08735)
*Mingjun Pan, Guanquan Lin, You-Wei Luo, Bin Zhu, Zhien Dai, Lijun Sun, Chun Yuan*

Main category: cs.LG

TL;DR: The paper introduces Preference Optimization, a novel RL method for combinatorial optimization, transforming rewards into qualitative preferences to improve efficiency and solution quality.


<details>
  <summary>Details</summary>
Motivation: Existing RL approaches struggle with diminishing rewards and inefficient exploration in large combinatorial spaces.

Method: The method reparameterizes rewards using preference models and integrates local search for fine-tuning, avoiding intractable computations.

Result: Empirical results show superior performance on benchmarks like TSP, CVRP, and FFSP.

Conclusion: Preference Optimization enhances RL efficiency and solution quality in combinatorial optimization.

Abstract: Reinforcement Learning (RL) has emerged as a powerful tool for neural
combinatorial optimization, enabling models to learn heuristics that solve
complex problems without requiring expert knowledge. Despite significant
progress, existing RL approaches face challenges such as diminishing reward
signals and inefficient exploration in vast combinatorial action spaces,
leading to inefficiency. In this paper, we propose Preference Optimization, a
novel method that transforms quantitative reward signals into qualitative
preference signals via statistical comparison modeling, emphasizing the
superiority among sampled solutions. Methodologically, by reparameterizing the
reward function in terms of policy and utilizing preference models, we
formulate an entropy-regularized RL objective that aligns the policy directly
with preferences while avoiding intractable computations. Furthermore, we
integrate local search techniques into the fine-tuning rather than
post-processing to generate high-quality preference pairs, helping the policy
escape local optima. Empirical results on various benchmarks, such as the
Traveling Salesman Problem (TSP), the Capacitated Vehicle Routing Problem
(CVRP) and the Flexible Flow Shop Problem (FFSP), demonstrate that our method
significantly outperforms existing RL algorithms, achieving superior
convergence efficiency and solution quality.

</details>


### [336] [Towards Foundation Models for Experimental Readout Systems Combining Discrete and Continuous Data](https://arxiv.org/pdf/2505.08736)
*James Giroux, Cristiano Fanelli*

Main category: cs.LG

TL;DR: A Foundation Model for Nuclear Physics is introduced, addressing limitations in existing methods with innovations like separate vocabularies and continuous conditioning, enabling high-fidelity generation and reconstruction tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome resolution loss and lack of conditional generation in next-token prediction approaches for nuclear physics data.

Method: Proposes three innovations: (i) separate vocabularies for discrete and continuous features, (ii) continuous kinematic conditioning, and (iii) scalable high-resolution tokenization.

Result: Validated for fast, high-fidelity generation of Cherenkov photon sequences and generalization to reconstruction tasks like pion and kaon identification.

Conclusion: The model effectively addresses existing limitations and demonstrates versatility in both generation and reconstruction tasks.

Abstract: We present a (proto) Foundation Model for Nuclear Physics, capable of
operating on low-level detector inputs from Imaging Cherenkov Detectors at the
future Electron Ion Collider. To address limitations in existing next-token
prediction approaches-namely resolution loss from VQ-VAE tokenization and lack
of conditional generation-we propose three key innovations: (i) separate
vocabularies for discrete spatial features and continuous variates, combined
via Causal Multi-Head Cross-Attention (CMHCA), (ii) continuous kinematic
conditioning through prepended context embeddings, and (iii) scalable and
simple, high-resolution continuous variate tokenization without joint
vocabulary inflation. Our model enables fast, high-fidelity generation of pixel
and time sequences for Cherenkov photons, validated through closure tests in
the High Performance DIRC. We also show our model generalizes to reconstruction
tasks such as pion and kaon identification, in which we show its ability to
leverage fine-tuning.

</details>


### [337] [Sensitivity-Constrained Fourier Neural Operators for Forward and Inverse Problems in Parametric Differential Equations](https://arxiv.org/pdf/2505.08740)
*Abdolmehdi Behroozi, Chaopeng Shen and, Daniel Kifer*

Main category: cs.LG

TL;DR: SC-FNO improves FNO by adding sensitivity-based regularization, enhancing accuracy, scalability, and efficiency in solving parametric differential equations.


<details>
  <summary>Details</summary>
Motivation: Standard FNO struggles with inverse problems, sensitivity estimation, and concept drift in parametric differential equations.

Method: Introduces Sensitivity-Constrained Fourier Neural Operators (SC-FNO) with sensitivity-based regularization.

Result: SC-FNO outperforms FNO in accuracy, parameter inversion, scalability (up to 82 parameters), and reduces data/training needs.

Conclusion: SC-FNO is a robust, efficient solution for parametric differential equations, generalizing well across various scenarios.

Abstract: Parametric differential equations of the form du/dt = f(u, x, t, p) are
fundamental in science and engineering. While deep learning frameworks such as
the Fourier Neural Operator (FNO) can efficiently approximate solutions, they
struggle with inverse problems, sensitivity estimation (du/dp), and concept
drift. We address these limitations by introducing a sensitivity-based
regularization strategy, called Sensitivity-Constrained Fourier Neural
Operators (SC-FNO). SC-FNO achieves high accuracy in predicting solution paths
and consistently outperforms standard FNO and FNO with physics-informed
regularization. It improves performance in parameter inversion tasks, scales to
high-dimensional parameter spaces (tested with up to 82 parameters), and
reduces both data and training requirements. These gains are achieved with a
modest increase in training time (30% to 130% per epoch) and generalize across
various types of differential equations and neural operators. Code and selected
experiments are available at: https://github.com/AMBehroozi/SC_Neural_Operators

</details>


### [338] [CodePDE: An Inference Framework for LLM-driven PDE Solver Generation](https://arxiv.org/pdf/2505.08783)
*Shanda Li, Tanya Marwah, Junhong Shen, Weiwei Sun, Andrej Risteski, Yiming Yang, Ameet Talwalkar*

Main category: cs.LG

TL;DR: CodePDE uses LLMs to generate PDE solvers without task-specific tuning, achieving superhuman performance and offering insights into LLM capabilities and limitations.


<details>
  <summary>Details</summary>
Motivation: Traditional PDE solvers are complex and resource-intensive, while neural-network-based solvers lack interpretability and require large datasets.

Method: Frames PDE solving as code generation, leveraging LLMs with advanced inference-time algorithms and scaling strategies.

Result: Achieves superhuman performance on representative PDE problems and provides empirical analysis of solver accuracy and efficiency.

Conclusion: Highlights the promise and limitations of LLMs for PDE solving, opening new avenues for solver design and model development.

Abstract: Partial differential equations (PDEs) are fundamental to modeling physical
systems, yet solving them remains a complex challenge. Traditional numerical
solvers rely on expert knowledge to implement and are computationally
expensive, while neural-network-based solvers require large training datasets
and often lack interpretability. In this work, we frame PDE solving as a code
generation task and introduce CodePDE, the first inference framework for
generating PDE solvers using large language models (LLMs). Leveraging advanced
inference-time algorithms and scaling strategies, CodePDE unlocks critical
capacities of LLM for PDE solving: reasoning, debugging, selfrefinement, and
test-time scaling -- all without task-specific tuning. CodePDE achieves
superhuman performance across a range of representative PDE problems. We also
present a systematic empirical analysis of LLM generated solvers, analyzing
their accuracy, efficiency, and numerical scheme choices. Our findings
highlight the promise and the current limitations of LLMs in PDE solving,
offering a new perspective on solver design and opportunities for future model
development. Our code is available at https://github.com/LithiumDA/CodePDE.

</details>


### [339] [Implet: A Post-hoc Subsequence Explainer for Time Series Models](https://arxiv.org/pdf/2505.08748)
*Fanyu Meng, Ziwen Kan, Shahbaz Rezaei, Zhaodan Kong, Xin Chen, Xin Liu*

Main category: cs.LG

TL;DR: Implet is a post-hoc explainer for time series models, offering subsequence-level and cohort-based explanations to enhance interpretability.


<details>
  <summary>Details</summary>
Motivation: To improve trust, debugging, and interpretability in time series models by providing clear explanations.

Method: Introduces Implet, which identifies critical temporal segments and uses a cohort-based framework for concise explanations.

Result: Demonstrated effectiveness on standard time-series benchmarks, improving interpretability.

Conclusion: Implet successfully enhances interpretability in time series models, with code publicly available.

Abstract: Explainability in time series models is crucial for fostering trust,
facilitating debugging, and ensuring interpretability in real-world
applications. In this work, we introduce Implet, a novel post-hoc explainer
that generates accurate and concise subsequence-level explanations for time
series models. Our approach identifies critical temporal segments that
significantly contribute to the model's predictions, providing enhanced
interpretability beyond traditional feature-attribution methods. Based on it,
we propose a cohort-based (group-level) explanation framework designed to
further improve the conciseness and interpretability of our explanations. We
evaluate Implet on several standard time-series classification benchmarks,
demonstrating its effectiveness in improving interpretability. The code is
available at https://github.com/LbzSteven/implet

</details>


### [340] [SPAT: Sensitivity-based Multihead-attention Pruning on Time Series Forecasting Models](https://arxiv.org/pdf/2505.08768)
*Suhan Guo, Jiahong Deng, Mengjun Yi, Furao Shen, Jian Zhao*

Main category: cs.LG

TL;DR: SPAT is a structured pruning method for attention-based models, reducing redundancy and improving efficiency without specialized hardware.


<details>
  <summary>Details</summary>
Motivation: Attention-based models are effective but computationally expensive; SPAT aims to reduce their size and latency while maintaining performance.

Method: SPAT uses a dynamic sensitivity metric (SEND) to prune entire attention modules during pre-training, avoiding overfitting.

Result: SPAT reduces MSE by 2.842%, MAE by 1.996%, and FLOPs by 35.274%, outperforming lightweight and SOTA methods.

Conclusion: SPAT demonstrates the value of selective pruning for efficient and effective attention-based models.

Abstract: Attention-based architectures have achieved superior performance in
multivariate time series forecasting but are computationally expensive.
Techniques such as patching and adaptive masking have been developed to reduce
their sizes and latencies. In this work, we propose a structured pruning
method, SPAT ($\textbf{S}$ensitivity $\textbf{P}$runer for
$\textbf{At}$tention), which selectively removes redundant attention mechanisms
and yields highly effective models. Different from previous approaches, SPAT
aims to remove the entire attention module, which reduces the risk of
overfitting and enables speed-up without demanding specialized hardware. We
propose a dynamic sensitivity metric, $\textbf{S}$ensitivity
$\textbf{E}$nhanced $\textbf{N}$ormalized $\textbf{D}$ispersion (SEND) that
measures the importance of each attention module during the pre-training phase.
Experiments on multivariate datasets demonstrate that SPAT-pruned models
achieve reductions of 2.842% in MSE, 1.996% in MAE, and 35.274% in FLOPs.
Furthermore, SPAT-pruned models outperform existing lightweight, Mamba-based
and LLM-based SOTA methods in both standard and zero-shot inference,
highlighting the importance of retaining only the most effective attention
mechanisms. We have made our code publicly available
https://anonymous.4open.science/r/SPAT-6042.

</details>


### [341] [Addressing the Current Challenges of Quantum Machine Learning through Multi-Chip Ensembles](https://arxiv.org/pdf/2505.08782)
*Junghoon Justin Park, Jiook Cha, Samuel Yen-Chi Chen, Huan-Hsin Tseng, Shinjae Yoo*

Main category: cs.LG

TL;DR: A multi-chip ensemble VQC framework is proposed to address scalability, noise, and trainability issues in QML on NISQ devices, showing improved performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Practical deployment of QML is hindered by NISQ device limitations like noise, scalability, and trainability in VQCs.

Method: Introduces a multi-chip ensemble VQC framework to partition computations across smaller chips, enhancing scalability and noise resilience.

Result: Mitigates barren plateaus, reduces quantum error bias/variance, and maintains generalization, validated on benchmark and real-world datasets.

Conclusion: The framework shows strong potential for scalable QML on near-term quantum hardware.

Abstract: Quantum Machine Learning (QML) holds significant promise for solving
computational challenges across diverse domains. However, its practical
deployment is constrained by the limitations of noisy intermediate-scale
quantum (NISQ) devices, including noise, limited scalability, and trainability
issues in variational quantum circuits (VQCs). We introduce the multi-chip
ensemble VQC framework, which partitions high-dimensional computations across
smaller quantum chips to enhance scalability, trainability, and noise
resilience. We show that this approach mitigates barren plateaus, reduces
quantum error bias and variance, and maintains robust generalization through
controlled entanglement. Designed to align with current and emerging quantum
hardware, the framework demonstrates strong potential for enabling scalable QML
on near-term devices, as validated by experiments on standard benchmark
datasets (MNIST, FashionMNIST, CIFAR-10) and real world dataset (PhysioNet
EEG).

</details>


### [342] [Bellman Unbiasedness: Toward Provably Efficient Distributional Reinforcement Learning with General Value Function Approximation](https://arxiv.org/pdf/2407.21260)
*Taehyun Cho, Seungyub Han, Seokhun Ju, Dohyeong Kim, Kyungjae Lee, Jungwoo Lee*

Main category: cs.LG

TL;DR: The paper provides a regret analysis of distributional reinforcement learning (RL) with value function approximation, introducing Bellman unbiasedness and demonstrating that moment functionals are essential for efficient learning. A new algorithm, SF-LSVI, achieves tight regret bounds.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical understanding of distributional RL's effectiveness and the challenge of infinite-dimensional distributions.

Method: Introduces Bellman unbiasedness for efficient distributional updates and proposes the SF-LSVI algorithm with general value function approximation.

Result: Shows that only moment functionals can capture statistical information exactly. SF-LSVI achieves a tight regret bound of Õ(d_E H^(3/2)√K).

Conclusion: The study advances the theoretical foundation of distributional RL and provides a practical, efficient algorithm for finite episodic Markov decision processes.

Abstract: Distributional reinforcement learning improves performance by capturing
environmental stochasticity, but a comprehensive theoretical understanding of
its effectiveness remains elusive. In addition, the intractable element of the
infinite dimensionality of distributions has been overlooked. In this paper, we
present a regret analysis of distributional reinforcement learning with general
value function approximation in a finite episodic Markov decision process
setting. We first introduce a key notion of $\textit{Bellman unbiasedness}$
which is essential for exactly learnable and provably efficient distributional
updates in an online manner. Among all types of statistical functionals for
representing infinite-dimensional return distributions, our theoretical results
demonstrate that only moment functionals can exactly capture the statistical
information. Secondly, we propose a provably efficient algorithm,
$\texttt{SF-LSVI}$, that achieves a tight regret bound of $\tilde{O}(d_E
H^{\frac{3}{2}}\sqrt{K})$ where $H$ is the horizon, $K$ is the number of
episodes, and $d_E$ is the eluder dimension of a function class.

</details>


### [343] [Scaling Laws for Floating Point Quantization Training](https://arxiv.org/pdf/2501.02423)
*Xingwu Sun, Shuaipeng Li, Ruobing Xie, Weidong Han, Kan Wu, Zhen Yang, Yixing Li, An Wang, Shuai Li, Jinbao Xue, Yu Cheng, Yangyu Tao, Zhanhui Kang, Chengzhong Xu, Di Wang, Jie Jiang*

Main category: cs.LG

TL;DR: The paper explores FP quantization in LLM training, identifying optimal bit ratios, critical data size effects, and precision-performance trade-offs.


<details>
  <summary>Details</summary>
Motivation: Previous work on low-precision training focused on integer quantization, neglecting FP quantization's impact on LLM performance. This gap motivates a thorough investigation.

Method: The study examines FP quantization targets, exponent/mantissa bits, and scaling factor granularity, deriving a unified scaling law and practical recommendations.

Result: Key findings include: exponent bits slightly outperform mantissa bits, a critical data size exists for performance, and optimal precision is 4-8 bits for cost-performance balance.

Conclusion: The paper provides actionable insights for FP quantization in LLMs, aiding hardware design and training strategies.

Abstract: Low-precision training is considered an effective strategy for reducing both
training and downstream inference costs. Previous scaling laws for precision
mainly focus on integer quantization, which pay less attention to the
constituents in floating-point (FP) quantization, and thus cannot well fit the
LLM losses in this scenario. In contrast, while FP quantization training is
more commonly implemented in production, it's research has been relatively
superficial. In this paper, we thoroughly explore the effects of FP
quantization targets, exponent bits, mantissa bits, and the calculation
granularity of the scaling factor in FP quantization training performance of
LLM models. In addition to an accurate FP quantization unified scaling law, we
also provide valuable suggestions for the community: (1) Exponent bits
contribute slightly more to the model performance than mantissa bits. We
provide the optimal exponent-mantissa bit ratio for different bit numbers,
which is available for future reference by hardware manufacturers; (2) We
discover the formation of the critical data size in low-precision LLM training.
Too much training data exceeding the critical data size will inversely bring in
degradation of LLM performance; (3) The optimal FP quantization precision is
directly proportional to the computational power, but within a wide
computational power range. We estimate that the best cost-performance precision
should lie between 4-8 bits.

</details>


### [344] [Adaptive Integrated Layered Attention (AILA)](https://arxiv.org/pdf/2503.22742)
*William Claster, Suhas KM, Dhairya Gundechia*

Main category: cs.LG

TL;DR: AILA is a neural network architecture with adaptive feature reuse, outperforming baselines like LSTMs and ResNets in tasks like price forecasting, image recognition, and sentiment analysis, with faster training and inference.


<details>
  <summary>Details</summary>
Motivation: To improve feature reuse across network layers for better performance and efficiency in diverse tasks.

Method: Proposes two architectures: AILA-Architecture 1 (linear layer connections) and AILA-Architecture 2 (attention-based connections), evaluated on price forecasting, image recognition, and sentiment analysis.

Result: Matches or outperforms baselines with reduced training/inference time, showing robust gains from adaptive inter-layer connections.

Conclusion: AILA extends existing architectures, enhancing performance and speed in sequence modeling, image recognition, and classification.

Abstract: We propose Adaptive Integrated Layered Attention (AILA), a neural network
architecture that combines dense skip connections with different mechanisms for
adaptive feature reuse across network layers. We evaluate AILA on three
challenging tasks: price forecasting for various commodities and indices (S&P
500, Gold, US dollar Futures, Coffee, Wheat), image recognition using the
CIFAR-10 dataset, and sentiment analysis on the IMDB movie review dataset. In
all cases, AILA matches strong deep learning baselines (LSTMs, Transformers,
and ResNets), achieving it at a fraction of the training and inference time.
Notably, we implement and test two versions of the model - AILA-Architecture 1,
which uses simple linear layers as the connection mechanism between layers, and
AILA-Architecture 2, which implements an attention mechanism to selectively
focus on outputs from previous layers. Both architectures are applied in a
single-task learning setting, with each model trained separately for individual
tasks. Results confirm that AILA's adaptive inter-layer connections yield
robust gains by flexibly reusing pertinent features at multiple network depths.
The AILA approach thus presents an extension to existing architectures,
improving long-range sequence modeling, image recognition with optimised
computational speed, and SOTA classification performance in practice.

</details>


### [345] [Gradual Binary Search and Dimension Expansion : A general method for activation quantization in LLMs](https://arxiv.org/pdf/2504.13989)
*Lucas Maisonnave, Cyril Moineau, Olivier Bichler, Fabrice Rastello*

Main category: cs.LG

TL;DR: The paper proposes using Hadamard matrices for efficient 3-bit quantization of LLMs, outperforming state-of-the-art methods by 40% in accuracy.


<details>
  <summary>Details</summary>
Motivation: Deploying large language models (LLMs) on edge devices is challenging due to their size and activation outliers, which hinder low-bit quantization.

Method: The method leverages Hadamard matrices for outlier reduction and employs a gradual binary search for 3-bit quantization of weights, activations, and KV caches. It also extends to non-power-of-2 dimensions using the Paley algorithm.

Result: The approach achieves 3-bit quantization with a 40% accuracy improvement on benchmarks, validated across models like Mistral, LLaMA, and Qwen.

Conclusion: Hadamard matrices are superior for quantization, enabling practical 3-bit deployment of LLMs with significant performance gains.

Abstract: Large language models (LLMs) have become pivotal in artificial intelligence,
demonstrating strong capabilities in reasoning, understanding, and generating
data. However, their deployment on edge devices is hindered by their
substantial size, often reaching several billion parameters. Quantization is a
widely used method to reduce memory usage and inference time, however LLMs
present unique challenges due to the prevalence of outliers in their
activations. In this work, we leverage the theoretical advantages of Hadamard
matrices over random rotation matrices to push the boundaries of quantization
in LLMs. We demonstrate that Hadamard matrices are more effective in reducing
outliers, which are a significant obstacle in achieving low-bit quantization.
Our method based on a gradual binary search enables 3-bit quantization for
weights, activations, and key-value (KV) caches, resulting in a 40% increase in
accuracy on common benchmarks compared to SoTA methods. We extend the use of
rotation matrices to support non-power-of-2 embedding dimensions, similar to
the Qwen architecture, by employing the Paley algorithm. We theoretically
demonstrates the superiority of Hadamard matrices in reducing outliers.We
achieved 3-bit quantization for weights, activations, and KV cache,
significantly enhancing model performance. Our experimental results on multiple
models family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of
our approach, outperforming existing methods and enabling practical 3-bit
quantization.

</details>


### [346] [Integrating Single-Cell Foundation Models with Graph Neural Networks for Drug Response Prediction](https://arxiv.org/pdf/2504.14361)
*Till Rossner, Ziteng Li, Jonas Balke, Nikoo Salehfard, Tom Seifert, Ming Tang*

Main category: cs.LG

TL;DR: The paper explores using scGPT, a pretrained foundation model, to enhance drug response prediction in cancer treatment by improving cell representations in the DeepCDR framework.


<details>
  <summary>Details</summary>
Motivation: Accurate drug response prediction is challenging due to cancer heterogeneity and high data costs. The study aims to leverage scGPT's pretrained knowledge to address these limitations.

Method: The DeepCDR framework is adapted by integrating scGPT to enrich cell representations. Performance is evaluated using IC50 values, PCC, and leave-one-drug-out validation.

Result: scGPT outperforms previous methods, including the original DeepCDR and scFoundation-based approaches, and shows better training stability.

Conclusion: Incorporating scGPT enhances drug response prediction, demonstrating the value of foundation models in this domain.

Abstract: AI-driven drug response prediction holds great promise for advancing
personalized cancer treatment. However, the inherent heterogenity of cancer and
high cost of data generation make accurate prediction challenging. In this
study, we investigate whether incorporating the pretrained foundation model
scGPT can enhance the performance of existing drug response prediction
frameworks. Our approach builds on the DeepCDR framework, which encodes drug
representations from graph structures and cell representations from multi-omics
profiles. We adapt this framework by leveraging scGPT to generate enriched cell
representations using its pretrained knowledge to compensate for limited amount
of data. We evaluate our modified framework using IC$_{50}$ values on Pearson
correlation coefficient (PCC) and a leave-one-drug out validation strategy,
comparing it against the original DeepCDR framework and a prior
scFoundation-based approach. scGPT not only outperforms previous approaches but
also exhibits greater training stability, highlighting the value of leveraging
scGPT-derived knowledge in this domain.

</details>


### [347] [Transfer Learning of Surrogate Models: Integrating Domain Warping and Affine Transformations](https://arxiv.org/pdf/2501.18344)
*Shuaiqun Pan, Diederick Vermetten, Manuel López-Ibáñez, Thomas Bäck, Hao Wang*

Main category: cs.LG

TL;DR: The paper proposes a method to transfer pre-trained surrogate models to new tasks by optimizing linear and nonlinear transformations, validated on benchmark and real-world datasets.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of large dataset requirements for surrogate models by enabling effective transfer learning under broader transformations.

Method: Optimizes unknown input warping and affine transformations using limited target task data to minimize empirical loss.

Result: The transferred surrogate outperforms original and scratch-built models, especially in data-scarce scenarios.

Conclusion: The approach effectively extends transfer learning for surrogate models to more complex transformations, proving advantageous in practical applications.

Abstract: Surrogate models provide efficient alternatives to computationally demanding
real world processes but often require large datasets for effective training. A
promising solution to this limitation is the transfer of pre-trained surrogate
models to new tasks. Previous studies have investigated the transfer of
differentiable and non-differentiable surrogate models, typically assuming an
affine transformation between the source and target functions. This paper
extends previous research by addressing a broader range of transformations,
including linear and nonlinear variations. Specifically, we consider the
combination of an unknown input warping, such as one modeled by the beta
cumulative distribution function, with an unspecified affine transformation.
Our approach achieves transfer learning by employing a limited number of data
points from the target task to optimize these transformations, minimizing
empirical loss on the transfer dataset. We validate the proposed method on the
widely used Black-Box Optimization Benchmark (BBOB) testbed and a real-world
transfer learning task from the automobile industry. The results underscore the
significant advantages of the approach, revealing that the transferred
surrogate significantly outperforms both the original surrogate and the one
built from scratch using the transfer dataset, particularly in data-scarce
scenarios.

</details>


### [348] [Calibrated and Sharp Uncertainties in Deep Learning via Density Estimation](https://arxiv.org/pdf/2112.07184)
*Volodymyr Kuleshov, Shachi Deshpande*

Main category: cs.LG

TL;DR: The paper introduces a simple recalibration method to ensure probabilistic models are both calibrated and sharp, improving accuracy without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: Standard maximum likelihood training often produces poorly calibrated models, leading to inaccurate confidence intervals. Calibration is crucial for reliable predictions.

Method: The proposed method involves low-dimensional density estimation and recalibration, ensuring distribution calibration for any model, including neural networks.

Result: The approach improves empirical performance on linear and deep Bayesian models, with formal proofs of correctness and uniform convergence bounds.

Conclusion: Calibration is essential in machine learning, and the introduced method effectively maintains it. A library and blog post are released for implementation.

Abstract: Accurate probabilistic predictions can be characterized by two properties --
calibration and sharpness. However, standard maximum likelihood training yields
models that are poorly calibrated and thus inaccurate -- a 90% confidence
interval typically does not contain the true outcome 90% of the time. This
paper argues that calibration is important in practice and is easy to maintain
by performing low-dimensional density estimation. We introduce a simple
training procedure based on recalibration that yields calibrated models without
sacrificing overall performance; unlike previous approaches, ours ensures the
most general property of distribution calibration and applies to any model,
including neural networks. We formally prove the correctness of our procedure
assuming that we can estimate densities in low dimensions and we establish
uniform convergence bounds. Our results yield empirical performance
improvements on linear and deep Bayesian models and suggest that calibration
should be increasingly leveraged across machine learning. We release a library
that implements our methods along with a blog post here:
https://shachideshpande.github.io/blog-distribution-calibration/.

</details>


### [349] [Position: AI Scaling: From Up to Down and Out](https://arxiv.org/pdf/2502.01677)
*Yunke Wang, Yanxi Li, Chang Xu*

Main category: cs.LG

TL;DR: The paper advocates for a holistic AI scaling framework (Scaling Up, Down, Out) to address efficiency, adaptability, and collaboration challenges, emphasizing Scaling Down and Out as future directions.


<details>
  <summary>Details</summary>
Motivation: To address bottlenecks in Scaling Up and meet demands for efficiency, adaptability, and societal impact, the paper proposes a broader AI scaling approach.

Method: Introduces a framework combining Scaling Up, Down, and Out, with focus on Down and Out for diverse applications like healthcare and smart manufacturing.

Result: Demonstrates potential breakthroughs in efficiency, personalization, and global connectivity while highlighting challenges like interpretability and ethics.

Conclusion: Proposes a unified roadmap for AI scaling to advance toward AGI, balancing technical and societal needs.

Abstract: AI Scaling has traditionally been synonymous with Scaling Up, which builds
larger and more powerful models. However, the growing demand for efficiency,
adaptability, and collaboration across diverse applications necessitates a
broader perspective. This position paper presents a holistic framework for AI
scaling, encompassing Scaling Up, Scaling Down, and Scaling Out. It argues that
while Scaling Up of models faces inherent bottlenecks, the future trajectory of
AI scaling lies in Scaling Down and Scaling Out. These paradigms address
critical technical and societal challenges, such as reducing carbon footprint,
ensuring equitable access, and enhancing cross-domain collaboration. We explore
transformative applications in healthcare, smart manufacturing, and content
creation, demonstrating how AI Scaling can enable breakthroughs in efficiency,
personalization, and global connectivity. Additionally, we highlight key
challenges, including balancing model complexity with interpretability,
managing resource constraints, and fostering ethical development. By
synthesizing these approaches, we propose a unified roadmap that redefines the
future of AI research and application, paving the way for advancements toward
Artificial General Intelligence (AGI).

</details>


### [350] [A primal-dual perspective for distributed TD-learning](https://arxiv.org/pdf/2310.00638)
*Han-Dong Lim, Donghwan Lee*

Main category: cs.LG

TL;DR: The paper explores distributed TD learning for multi-agent Markov decision processes using primal-dual ODE dynamics, analyzing convergence under various conditions without requiring doubly stochastic communication networks.


<details>
  <summary>Details</summary>
Motivation: To address distributed TD learning in networked multi-agent systems, overcoming limitations of existing methods that rely on doubly stochastic communication matrices.

Method: Uses distributed optimization algorithms interpreted as primal-dual ODE dynamics with null-space constraints, tested under constant/diminishing step-sizes and i.i.d./Markovian models.

Result: Demonstrates exponential convergence of primal-dual ODE dynamics and analyzes final iterate behavior in distributed TD-learning scenarios.

Conclusion: The proposed method effectively handles distributed TD learning without the restrictive doubly stochastic network assumption, offering broader applicability.

Abstract: The goal of this paper is to investigate distributed temporal difference (TD)
learning for a networked multi-agent Markov decision process. The proposed
approach is based on distributed optimization algorithms, which can be
interpreted as primal-dual Ordinary differential equation (ODE) dynamics
subject to null-space constraints. Based on the exponential convergence
behavior of the primal-dual ODE dynamics subject to null-space constraints, we
examine the behavior of the final iterate in various distributed TD-learning
scenarios, considering both constant and diminishing step-sizes and
incorporating both i.i.d. and Markovian observation models. Unlike existing
methods, the proposed algorithm does not require the assumption that the
underlying communication network structure is characterized by a doubly
stochastic matrix.

</details>


### [351] [GraphSparseNet: a Novel Method for Large Scale Traffic Flow Prediction](https://arxiv.org/pdf/2502.19823)
*Weiyang Kong, Kaiqi Wu, Sen Zhang, Yubao Liu*

Main category: cs.LG

TL;DR: GraphSparseNet (GSNet) is introduced to enhance scalability and accuracy in GNN-based traffic forecasting by reducing computational complexity to linear scale.


<details>
  <summary>Details</summary>
Motivation: Addressing the scalability challenge of GNNs in traffic flow forecasting without compromising predictive accuracy.

Method: GraphSparseNet uses Feature Extractor and Relational Compressor modules for linear time and space complexity.

Result: Reduces training time by 3.51x while maintaining high predictive performance.

Conclusion: GraphSparseNet effectively balances scalability and accuracy in traffic forecasting.

Abstract: Traffic flow forecasting is a critical spatio-temporal data mining task with
wide-ranging applications in intelligent route planning and dynamic traffic
management. Recent advancements in deep learning, particularly through Graph
Neural Networks (GNNs), have significantly enhanced the accuracy of these
forecasts by capturing complex spatio-temporal dynamics. However, the
scalability of GNNs remains a challenge due to their exponential growth in
model complexity with increasing nodes in the graph. Existing methods to
address this issue, including sparsification, decomposition, and kernel-based
approaches, either do not fully resolve the complexity issue or risk
compromising predictive accuracy. This paper introduces GraphSparseNet (GSNet),
a novel framework designed to improve both the scalability and accuracy of
GNN-based traffic forecasting models. GraphSparseNet is comprised of two core
modules: the Feature Extractor and the Relational Compressor. These modules
operate with linear time and space complexity, thereby reducing the overall
computational complexity of the model to a linear scale. Our extensive
experiments on multiple real-world datasets demonstrate that GraphSparseNet not
only significantly reduces training time by 3.51x compared to state-of-the-art
linear models but also maintains high predictive performance.

</details>


### [352] [Learning Optimal Classification Trees Robust to Distribution Shifts](https://arxiv.org/pdf/2310.17772)
*Nathan Justin, Sina Aghaei, Andrés Gómez, Phebe Vayanos*

Main category: cs.LG

TL;DR: The paper proposes a method for learning robust classification trees to handle distribution shifts between training and testing data, using mixed-integer robust optimization.


<details>
  <summary>Details</summary>
Motivation: Distribution shifts in high-stakes settings like public health and social work, where data collection is sensitive to various factors, necessitate robust classification trees.

Method: The method reformulates the problem as a two-stage linear robust optimization and solves it using constraint generation.

Result: The robust solution improves worst-case accuracy by up to 12.48% and average-case accuracy by up to 4.85% compared to non-robust trees.

Conclusion: The proposed robust optimization approach effectively enhances classification tree performance under distribution shifts.

Abstract: We consider the problem of learning classification trees that are robust to
distribution shifts between training and testing/deployment data. This problem
arises frequently in high stakes settings such as public health and social work
where data is often collected using self-reported surveys which are highly
sensitive to e.g., the framing of the questions, the time when and place where
the survey is conducted, and the level of comfort the interviewee has in
sharing information with the interviewer. We propose a method for learning
optimal robust classification trees based on mixed-integer robust optimization
technology. In particular, we demonstrate that the problem of learning an
optimal robust tree can be cast as a single-stage mixed-integer robust
optimization problem with a highly nonlinear and discontinuous objective. We
reformulate this problem equivalently as a two-stage linear robust optimization
problem for which we devise a tailored solution procedure based on constraint
generation. We evaluate the performance of our approach on numerous publicly
available datasets, and compare the performance to a regularized, non-robust
optimal tree. We show an increase of up to 12.48% in worst-case accuracy and of
up to 4.85% in average-case accuracy across several datasets and distribution
shifts from using our robust solution in comparison to the non-robust one.

</details>


### [353] [UVTM: Universal Vehicle Trajectory Modeling with ST Feature Domain Generation](https://arxiv.org/pdf/2402.07232)
*Yan Lin, Jilin Hu, Shengnan Guo, Bin Yang, Christian S. Jensen, Youfang Lin, Huaiyu Wan*

Main category: cs.LG

TL;DR: UVTM is a universal vehicle trajectory model designed for diverse tasks, handling partial or sparse trajectory data through domain-specific masking and pre-training for feature reconstruction.


<details>
  <summary>Details</summary>
Motivation: To reduce computational and storage costs by replacing multiple specialized models with a single universal model, even when trajectory data is incomplete or sparse.

Method: UVTM divides trajectory features into three domains for independent masking and generation, and pre-trains by reconstructing dense trajectories from sparse ones.

Result: UVTM performs well across four tasks on three datasets, demonstrating adaptability to partial or sparse data.

Conclusion: UVTM effectively addresses challenges of incomplete trajectory data, offering a versatile solution for various trajectory-related tasks.

Abstract: Vehicle movement is frequently captured in the form of GPS trajectories,
i.e., sequences of timestamped GPS locations. Such data is widely used for
various tasks such as travel-time estimation, trajectory recovery, and
trajectory prediction. A universal vehicle trajectory model could be applied to
different tasks, removing the need to maintain multiple specialized models,
thereby reducing computational and storage costs. However, creating such a
model is challenging when the integrity of trajectory features is compromised,
i.e., in scenarios where only partial features are available or the
trajectories are sparse.
  To address these challenges, we propose the Universal Vehicle Trajectory
Model (UVTM), which can effectively adapt to different tasks without excessive
retraining. UVTM incorporates two specialized designs. First, it divides
trajectory features into three distinct domains. Each domain can be masked and
generated independently to accommodate tasks with only partially available
features. Second, UVTM is pre-trained by reconstructing dense, feature-complete
trajectories from sparse, feature-incomplete counterparts, enabling strong
performance even when the integrity of trajectory features is compromised.
Experiments involving four representative trajectory-related tasks on three
real-world vehicle trajectory datasets provide insight into the performance of
UVTM and offer evidence that it is capable of meeting its objectives.

</details>


### [354] [Nonlinearity Enhanced Adaptive Activation Functions](https://arxiv.org/pdf/2403.19896)
*David Yevick*

Main category: cs.LG

TL;DR: A method to enhance neural network accuracy by introducing parametric, learned nonlinearity into activation functions, tested on ReLU and others, showing improvements on MNIST and CNN benchmarks.


<details>
  <summary>Details</summary>
Motivation: To improve neural network accuracy without significantly increasing computational costs by modifying activation functions.

Method: Introducing parametric, learned nonlinearity into activation functions, tested on ReLU and other common functions.

Result: Enhanced accuracy demonstrated on MNIST digit dataset and a CNN benchmark.

Conclusion: The proposed method effectively improves neural network performance with minimal computational overhead.

Abstract: A general procedure for introducing parametric, learned, nonlinearity into
activation functions is found to enhance the accuracy of representative neural
networks without requiring significant additional computational resources.
Examples are given based on the standard rectified linear unit (ReLU) as well
as several other frequently employed activation functions. The associated
accuracy improvement is quantified both in the context of the MNIST digit data
set and a convolutional neural network (CNN) benchmark example.

</details>


### [355] [Wilsonian Renormalization of Neural Network Gaussian Processes](https://arxiv.org/pdf/2405.06008)
*Jessica N. Howard, Ro Jefferson, Anindita Maiti, Zohar Ringel*

Main category: cs.LG

TL;DR: The paper presents a method for applying Wilsonian renormalization group (RG) to Gaussian Process (GP) Regression, focusing on separating learnable and unlearnable modes to improve understanding of feature learning in neural networks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to leverage theoretical physics tools, specifically the renormalization group, to enhance modeling processes by distinguishing relevant and irrelevant information in GP Regression.

Method: The method involves systematically integrating out unlearnable modes of the GP kernel to derive an RG flow, with the data determining the IR scale. This includes analyzing the ridge parameter's flow and non-Gaussian effects.

Result: The approach yields a universal flow of the ridge parameter and extends to non-Gaussian scenarios, providing a direct link between RG flow and learnable modes.

Conclusion: This method offers analytical tractability and deeper insights into feature learning in neural networks, potentially identifying universality classes in such models.

Abstract: Separating relevant and irrelevant information is key to any modeling process
or scientific inquiry. Theoretical physics offers a powerful tool for achieving
this in the form of the renormalization group (RG). Here we demonstrate a
practical approach to performing Wilsonian RG in the context of Gaussian
Process (GP) Regression. We systematically integrate out the unlearnable modes
of the GP kernel, thereby obtaining an RG flow of the GP in which the data sets
the IR scale. In simple cases, this results in a universal flow of the ridge
parameter, which becomes input-dependent in the richer scenario in which
non-Gaussianities are included. In addition to being analytically tractable,
this approach goes beyond structural analogies between RG and neural networks
by providing a natural connection between RG flow and learnable vs. unlearnable
modes. Studying such flows may improve our understanding of feature learning in
deep neural networks, and enable us to identify potential universality classes
in these models.

</details>


### [356] [LLMs meet Federated Learning for Scalable and Secure IoT Management](https://arxiv.org/pdf/2504.16032)
*Yazan Otoum, Arghavan Asad, Amiya Nayak*

Main category: cs.LG

TL;DR: A Federated Learning-driven Large Language Model (FL-LLM) framework is proposed to address IoT challenges like scalability, security, and real-time decision-making, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional centralized IoT architectures face issues like latency, privacy, and resource inefficiency, necessitating a more scalable and secure solution.

Method: The framework combines Generative IoT models with a Gradient Sensing Federated Strategy (GSFS) and uses a hybrid edge-cloud architecture for dynamic optimization.

Result: Tests on the IoT-23 dataset show improved accuracy, lower latency, and better energy efficiency compared to traditional federated learning methods.

Conclusion: The FL-LLM framework demonstrates promise for secure, scalable, and adaptive IoT management, integrating LLM-powered federated learning effectively.

Abstract: The rapid expansion of IoT ecosystems introduces severe challenges in
scalability, security, and real-time decision-making. Traditional centralized
architectures struggle with latency, privacy concerns, and excessive resource
consumption, making them unsuitable for modern large-scale IoT deployments.
This paper presents a novel Federated Learning-driven Large Language Model
(FL-LLM) framework, designed to enhance IoT system intelligence while ensuring
data privacy and computational efficiency. The framework integrates Generative
IoT (GIoT) models with a Gradient Sensing Federated Strategy (GSFS),
dynamically optimizing model updates based on real-time network conditions. By
leveraging a hybrid edge-cloud processing architecture, our approach balances
intelligence, scalability, and security in distributed IoT environments.
Evaluations on the IoT-23 dataset demonstrate that our framework improves model
accuracy, reduces response latency, and enhances energy efficiency,
outperforming traditional FL techniques (i.e., FedAvg, FedOpt). These findings
highlight the potential of integrating LLM-powered federated learning into
large-scale IoT ecosystems, paving the way for more secure, scalable, and
adaptive IoT management solutions.

</details>


### [357] [An Efficient On-Policy Deep Learning Framework for Stochastic Optimal Control](https://arxiv.org/pdf/2410.05163)
*Mengjian Hua, Mathieu Laurière, Eric Vanden-Eijnden*

Main category: cs.LG

TL;DR: A novel on-policy algorithm for stochastic optimal control (SOC) using the Girsanov theorem to compute gradients efficiently, improving speed and scalability.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency of traditional SOC methods, which rely on expensive backpropagation or adjoint solutions.

Method: Leverages the Girsanov theorem to directly compute on-policy gradients, avoiding backpropagation through stochastic differential equations.

Result: Demonstrates significant improvements in computational speed and memory efficiency on SOC benchmarks and applications like sampling and diffusion model fine-tuning.

Conclusion: The proposed method offers a scalable and efficient alternative for optimizing neural network control policies in SOC problems.

Abstract: We present a novel on-policy algorithm for solving stochastic optimal control
(SOC) problems. By leveraging the Girsanov theorem, our method directly
computes on-policy gradients of the SOC objective without expensive
backpropagation through stochastic differential equations or adjoint problem
solutions. This approach significantly accelerates the optimization of neural
network control policies while scaling efficiently to high-dimensional problems
and long time horizons. We evaluate our method on classical SOC benchmarks as
well as applications to sampling from unnormalized distributions via
Schr\"odinger-F\"ollmer processes and fine-tuning pre-trained diffusion models.
Experimental results demonstrate substantial improvements in both computational
speed and memory efficiency compared to existing approaches.

</details>


### [358] [Early-Cycle Internal Impedance Enables ML-Based Battery Cycle Life Predictions Across Manufacturers](https://arxiv.org/pdf/2410.05326)
*Tyler Sours, Shivang Agarwal, Marc Cormier, Jordan Crivelli-Decker, Steffen Ridderbusch, Stephen L. Glazier, Connor P. Aiken, Aayush R. Singh, Ang Xiao, Omar Allam*

Main category: cs.LG

TL;DR: A method combining voltage-capacity features and DCIR measurements improves EOL prediction for lithium-ion batteries across manufacturers, achieving an MAE of 150 cycles.


<details>
  <summary>Details</summary>
Motivation: Challenges in predicting EOL due to variations in battery manufacturing and lack of data necessitate a generalizable approach.

Method: Combines voltage-capacity features with DCIR measurements, leveraging early-cycle DCIR data to capture degradation.

Result: Models predict EOL with an MAE of 150 cycles for unseen manufacturers, reducing data collection needs.

Conclusion: The approach enhances generalizability and supports battery design optimization, with a new DCIR dataset released to aid development.

Abstract: Predicting the end-of-life (EOL) of lithium-ion batteries across different
manufacturers presents significant challenges due to variations in electrode
materials, manufacturing processes, cell formats, and a lack of generally
available data. Methods that construct features solely on voltage-capacity
profile data typically fail to generalize across cell chemistries. This study
introduces a methodology that combines traditional voltage-capacity features
with Direct Current Internal Resistance (DCIR) measurements, enabling more
accurate and generalizable EOL predictions. The use of early-cycle DCIR data
captures critical degradation mechanisms related to internal resistance growth,
enhancing model robustness. Models are shown to successfully predict the number
of cycles to EOL for unseen manufacturers of varied electrode composition with
a mean absolute error (MAE) of 150 cycles. This cross-manufacturer
generalizability reduces the need for extensive new data collection and
retraining, enabling manufacturers to optimize new battery designs using
existing datasets. Additionally, a novel DCIR-compatible dataset is released as
part of ongoing efforts to enrich the growing ecosystem of cycling data and
accelerate battery materials development.

</details>


### [359] [LSHBloom: Memory-efficient, Extreme-scale Document Deduplication](https://arxiv.org/pdf/2411.04257)
*Arham Khan, Robert Underwood, Carlo Siebenschuh, Yadu Babuji, Aswathy Ajith, Kyle Hippe, Ozan Gokdemir, Alexander Brace, Kyle Chard, Ian Foster*

Main category: cs.LG

TL;DR: LSHBloom improves deduplication for LLM training by using Bloom filters, offering faster runtime and lower memory usage than MinhashLSH with minimal false positives.


<details>
  <summary>Details</summary>
Motivation: Duplicates in training datasets increase costs and cause issues like memorization. Existing methods are resource-heavy.

Method: LSHBloom replaces MinhashLSH's LSHIndex with Bloom filters for lightweight deduplication.

Result: LSHBloom matches MinhashLSH's performance with fewer false positives, 270% faster runtime, and 0.6% disk space usage.

Conclusion: LSHBloom efficiently scales deduplication to billions of documents, offering significant speed and space advantages.

Abstract: Deduplication is a major focus for assembling and curating training datasets
for large language models (LLM) -- detecting and eliminating additional
instances of the same content -- in large collections of technical documents.
Unrestrained, duplicates in the training dataset increase training costs and
lead to undesirable properties such as memorization in trained models or
cheating on evaluation. Contemporary approaches to document-level deduplication
are often extremely expensive in both runtime and memory. We propose LSHBloom,
an extension to MinhashLSH, which replaces the expensive LSHIndex with
lightweight Bloom filters. LSHBloom demonstrates the same deduplication
performance as MinhashLSH with only a marginal increase in false positives (as
low as 1e-5 in our experiments); demonstrates competitive runtime (270\% faster
than MinhashLSH on peS2o); and, crucially, uses just 0.6\% of the disk space
required by MinhashLSH to deduplicate peS2o. We demonstrate that this space
advantage scales with increased dataset size -- at the extreme scale of several
billion documents, LSHBloom promises a 250\% speedup and a 54$\times$ space
advantage over traditional MinHashLSH scaling deduplication of text datasets to
many billions of documents.

</details>


### [360] [Streamlining Prediction in Bayesian Deep Learning](https://arxiv.org/pdf/2411.18425)
*Rui Li, Marcus Klasson, Arno Solin, Martin Trapp*

Main category: cs.LG

TL;DR: The paper proposes a method for efficient Bayesian deep learning predictions using local linearisation and Gaussian approximations, avoiding Monte Carlo sampling.


<details>
  <summary>Details</summary>
Motivation: Efficient computation of inferences in Bayesian deep learning is often overlooked, with Monte Carlo integration being the standard.

Method: Uses local linearisation on activation functions and Gaussian approximations at linear layers to analytically approximate the posterior predictive distribution.

Result: Demonstrated effectiveness for MLPs and transformers (ViT, GPT-2) in regression and classification tasks.

Conclusion: The approach provides a computationally efficient alternative to Monte Carlo sampling for Bayesian deep learning predictions.

Abstract: The rising interest in Bayesian deep learning (BDL) has led to a plethora of
methods for estimating the posterior distribution. However, efficient
computation of inferences, such as predictions, has been largely overlooked
with Monte Carlo integration remaining the standard. In this work we examine
streamlining prediction in BDL through a single forward pass without sampling.
For this we use local linearisation on activation functions and local Gaussian
approximations at linear layers. Thus allowing us to analytically compute an
approximation to the posterior predictive distribution. We showcase our
approach for both MLP and transformers, such as ViT and GPT-2, and assess its
performance on regression and classification tasks.
  Open-source library: https://github.com/AaltoML/SUQ

</details>


### [361] [USEFUSE: Uniform Stride for Enhanced Performance in Fused Layer Architecture of Deep Neural Networks](https://arxiv.org/pdf/2412.13724)
*Muhammad Sohail Ibrahim, Muhammad Usman, Jeong-A Lee*

Main category: cs.LG

TL;DR: The paper introduces Sum-of-Products (SOP) units for efficient CNN deployment on edge devices, reducing latency and power consumption while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: CNNs face challenges on resource-constrained edge devices, requiring solutions for low latency and power efficiency.

Method: Proposes SOP units with bit-serial arithmetic, layer fusion, inefficient convolution skipping, and tile movement optimization.

Result: Reduced redundant computations, improved efficiency, and two tailored designs for different edge device needs.

Conclusion: The approach enhances CNN deployment on edge devices by balancing performance and resource constraints.

Abstract: Convolutional Neural Networks (CNNs) are crucial in various applications, but
their deployment on resource-constrained edge devices poses challenges. This
study presents the Sum-of-Products (SOP) units for convolution, which utilize
low-latency left-to-right bit-serial arithmetic to minimize response time and
enhance overall performance. The study proposes a methodology for fusing
multiple convolution layers to reduce off-chip memory communication and
increase overall performance. An effective mechanism detects and skips
inefficient convolutions after ReLU layers, minimizing power consumption
without compromising accuracy. Furthermore, efficient tile movement guarantees
uniform access to the fusion pyramid. An analysis demonstrates the utile stride
strategy improves operational intensity. Two designs cater to varied demands:
one focuses on minimal response time for mission-critical applications, and
another focuses on resource-constrained devices with comparable latency. This
approach notably reduced redundant computations, improving the efficiency of
CNN deployment on edge devices.

</details>


### [362] [Graph Attention is Not Always Beneficial: A Theoretical Analysis of Graph Attention Mechanisms via Contextual Stochastic Block Models](https://arxiv.org/pdf/2412.15496)
*Zhongtian Ma, Qiaosheng Zhang, Bocheng Zhou, Yexin Zhang, Shuyue Hu, Zhen Wang*

Main category: cs.LG

TL;DR: Graph attention mechanisms are not universally beneficial; their effectiveness depends on the balance between structure noise and feature noise in graphs. Multi-layer GATs outperform single-layer ones, especially in high SNR regimes.


<details>
  <summary>Details</summary>
Motivation: To theoretically understand the conditions under which graph attention mechanisms are effective in node classification tasks.

Method: Theoretical analysis using Contextual Stochastic Block Models (CSBMs), defining structure and feature noise, and proposing a multi-layer GAT architecture.

Result: Graph attention works better when structure noise exceeds feature noise; multi-layer GATs achieve perfect node classification under relaxed SNR conditions.

Conclusion: Multi-layer GATs are superior in high SNR regimes, resolving over-smoothing and improving classification performance.

Abstract: Despite the growing popularity of graph attention mechanisms, their
theoretical understanding remains limited. This paper aims to explore the
conditions under which these mechanisms are effective in node classification
tasks through the lens of Contextual Stochastic Block Models (CSBMs). Our
theoretical analysis reveals that incorporating graph attention mechanisms is
\emph{not universally beneficial}. Specifically, by appropriately defining
\emph{structure noise} and \emph{feature noise} in graphs, we show that graph
attention mechanisms can enhance classification performance when structure
noise exceeds feature noise. Conversely, when feature noise predominates,
simpler graph convolution operations are more effective. Furthermore, we
examine the over-smoothing phenomenon and show that, in the high
signal-to-noise ratio (SNR) regime, graph convolutional networks suffer from
over-smoothing, whereas graph attention mechanisms can effectively resolve this
issue. Building on these insights, we propose a novel multi-layer Graph
Attention Network (GAT) architecture that significantly outperforms
single-layer GATs in achieving \emph{perfect node classification} in CSBMs,
relaxing the SNR requirement from $ \omega(\sqrt{\log n}) $ to $
\omega(\sqrt{\log n} / \sqrt[3]{n}) $. To our knowledge, this is the first
study to delineate the conditions for perfect node classification using
multi-layer GATs. Our theoretical contributions are corroborated by extensive
experiments on both synthetic and real-world datasets, highlighting the
practical implications of our findings.

</details>


### [363] [Knowledge Guided Encoder-Decoder Framework: Integrating Multiple Physical Models for Agricultural Ecosystem Modeling](https://arxiv.org/pdf/2505.06266)
*Qi Cheng, Licheng Liu, Yao Zhang, Mu Hong, Shiyuan Luo, Zhenong Jin, Yiqun Xie, Xiaowei Jia*

Main category: cs.LG

TL;DR: A knowledge-guided encoder-decoder model is proposed for agricultural monitoring, combining physical models and a language model to handle inconsistent inputs and improve predictions of crop variables.


<details>
  <summary>Details</summary>
Motivation: Address limitations of traditional physical models (specificity, parameter uncertainty) and data-driven models (lack of generalizability, black-box nature) in agricultural monitoring.

Method: Proposes a knowledge-guided encoder-decoder model integrating multiple physical models and a language model for input processing and model selection.

Result: Effective and robust predictions of carbon and nitrogen fluxes across multiple sites under various scenarios.

Conclusion: The model offers a universal solution for agricultural monitoring by combining knowledge from physical models and handling data inconsistencies.

Abstract: Agricultural monitoring is critical for ensuring food security, maintaining
sustainable farming practices, informing policies on mitigating food shortage,
and managing greenhouse gas emissions. Traditional process-based physical
models are often designed and implemented for specific situations, and their
parameters could also be highly uncertain. In contrast, data-driven models
often use black-box structures and does not explicitly model the
inter-dependence between different ecological variables. As a result, they
require extensive training data and lack generalizability to different tasks
with data distribution shifts and inconsistent observed variables. To address
the need for more universal models, we propose a knowledge-guided
encoder-decoder model, which can predict key crop variables by leveraging
knowledge of underlying processes from multiple physical models. The proposed
method also integrates a language model to process complex and inconsistent
inputs and also utilizes it to implement a model selection mechanism for
selectively combining the knowledge from different physical models. Our
evaluations on predicting carbon and nitrogen fluxes for multiple sites
demonstrate the effectiveness and robustness of the proposed model under
various scenarios.

</details>


### [364] [Stable Derivative Free Gaussian Mixture Variational Inference for Bayesian Inverse Problems](https://arxiv.org/pdf/2501.04259)
*Baojun Che, Yifan Chen, Zhenghao Huan, Daniel Zhengyu Huang, Weijie Wang*

Main category: cs.LG

TL;DR: DF-GMVI is a variational inference method combining Fisher-Rao natural gradient and quadrature rules for derivative-free updates, addressing challenges in Bayesian inference for large-scale inverse problems.


<details>
  <summary>Details</summary>
Motivation: Addressing costly forward model evaluations, multimodality, and inaccessible gradients in Bayesian inference for large-scale inverse problems.

Method: Develops DF-GMVI, a framework using Fisher-Rao natural gradient and quadrature rules for derivative-free updates of Gaussian mixture variational families.

Result: Demonstrates effectiveness in approximating complex posteriors, including multimodal and high-dimensional cases, and recovers Navier-Stokes initial conditions.

Conclusion: DF-GMVI provides a stable, efficient, and practical solution for approximating challenging posterior distributions.

Abstract: This paper is concerned with the approximation of probability distributions
known up to normalization constants, with a focus on Bayesian inference for
large-scale inverse problems in scientific computing. In this context, key
challenges include costly repeated evaluations of forward models,
multimodality, and inaccessible gradients for the forward model. To address
them, we develop a variational inference framework that combines Fisher-Rao
natural gradient with specialized quadrature rules to enable derivative free
updates of Gaussian mixture variational families. The resulting method, termed
Derivative Free Gaussian Mixture Variational Inference (DF-GMVI), guarantees
covariance positivity and affine invariance, offering a stable and efficient
framework for approximating complex posterior distributions. The effectiveness
of DF-GMVI is demonstrated through numerical experiments on challenging
scenarios, including distributions with multiple modes, infinitely many modes,
and curved modes in spaces with up to 100 dimensions. The method's practicality
is further demonstrated in a large-scale application, where it successfully
recovers the initial conditions of the Navier-Stokes equations from solution
data at positive times.

</details>


### [365] [Policy-labeled Preference Learning: Is Preference Enough for RLHF?](https://arxiv.org/pdf/2505.06273)
*Taehyun Cho, Seokhun Ju, Seungyub Han, Dohyeong Kim, Kyungjae Lee, Jungwoo Lee*

Main category: cs.LG

TL;DR: PPL improves RLHF by modeling human preferences with regret and using contrastive KL regularization, addressing likelihood mismatch issues.


<details>
  <summary>Details</summary>
Motivation: Existing RLHF methods misinterpret trajectories as optimal, leading to inaccurate likelihood estimation and suboptimal learning.

Method: Proposes policy-labeled preference learning (PPL) with regret-based modeling and contrastive KL regularization.

Result: PPL shows significant improvements in offline RLHF performance and effectiveness in online settings.

Conclusion: PPL resolves likelihood mismatch and enhances RLHF, validated in high-dimensional tasks.

Abstract: To design rewards that align with human goals, Reinforcement Learning from
Human Feedback (RLHF) has emerged as a prominent technique for learning reward
functions from human preferences and optimizing policies via reinforcement
learning algorithms. However, existing RLHF methods often misinterpret
trajectories as being generated by an optimal policy, causing inaccurate
likelihood estimation and suboptimal learning. Inspired by Direct Preference
Optimization framework which directly learns optimal policy without explicit
reward, we propose policy-labeled preference learning (PPL), to resolve
likelihood mismatch issues by modeling human preferences with regret, which
reflects behavior policy information. We also provide a contrastive KL
regularization, derived from regret-based principles, to enhance RLHF in
sequential decision making. Experiments in high-dimensional continuous control
tasks demonstrate PPL's significant improvements in offline RLHF performance
and its effectiveness in online settings.

</details>


### [366] [GRID: Protecting Training Graph from Link Stealing Attacks on GNN Models](https://arxiv.org/pdf/2501.10985)
*Jiadong Lou, Xu Yuan, Rui Zhang, Xingliang Yuan, Neil Gong, Nian-Feng Tzeng*

Main category: cs.LG

TL;DR: GRID defends against link stealing attacks in GNNs by adding crafted noises to disguise adjacent nodes, ensuring privacy without utility loss.


<details>
  <summary>Details</summary>
Motivation: Link stealing attacks in GNNs threaten security and privacy by inferring links between nodes via prediction vectors.

Method: GRID adds carefully crafted noises to prediction vectors, disguising adjacent nodes as n-hop neighbors, and selects core nodes to minimize distortion and cost.

Result: GRID effectively defends against link-stealing attacks, maintaining prediction accuracy and outperforming existing methods in privacy-utility trade-offs.

Conclusion: GRID provides a robust solution for securing GNNs against link stealing attacks while preserving model utility.

Abstract: Graph neural networks (GNNs) have exhibited superior performance in various
classification tasks on graph-structured data. However, they encounter the
potential vulnerability from the link stealing attacks, which can infer the
presence of a link between two nodes via measuring the similarity of its
incident nodes' prediction vectors produced by a GNN model. Such attacks pose
severe security and privacy threats to the training graph used in GNN models.
In this work, we propose a novel solution, called Graph Link Disguise (GRID),
to defend against link stealing attacks with the formal guarantee of GNN model
utility for retaining prediction accuracy. The key idea of GRID is to add
carefully crafted noises to the nodes' prediction vectors for disguising
adjacent nodes as n-hop indirect neighboring nodes. We take into account the
graph topology and select only a subset of nodes (called core nodes) covering
all links for adding noises, which can avert the noises offset and have the
further advantages of reducing both the distortion loss and the computation
cost. Our crafted noises can ensure 1) the noisy prediction vectors of any two
adjacent nodes have their similarity level like that of two non-adjacent nodes
and 2) the model prediction is unchanged to ensure zero utility loss. Extensive
experiments on five datasets are conducted to show the effectiveness of our
proposed GRID solution against different representative link-stealing attacks
under transductive settings and inductive settings respectively, as well as two
influence-based attacks. Meanwhile, it achieves a much better privacy-utility
trade-off than existing methods when extended to GNNs.

</details>


### [367] [Decoding Futures Price Dynamics: A Regularized Sparse Autoencoder for Interpretable Multi-Horizon Forecasting and Factor Discovery](https://arxiv.org/pdf/2505.06795)
*Abhijit Gupta*

Main category: cs.LG

TL;DR: The paper introduces a Regularized Sparse Autoencoder (RSAE) for multi-horizon commodity price forecasting, combining accuracy with interpretability by learning sparse latent market drivers.


<details>
  <summary>Details</summary>
Motivation: Commodity price volatility and the lack of transparency in current models necessitate accurate and interpretable forecasting methods.

Method: The RSAE uses L1 regularization on latent vectors to enforce sparsity, enabling interpretable market driver discovery while forecasting prices at multiple horizons.

Result: The RSAE achieves competitive forecasting accuracy for Copper and Crude Oil and provides insights into price dynamics through its interpretable latent space.

Conclusion: The RSAE outperforms traditional black-box models by offering both accurate predictions and transparent insights into market drivers.

Abstract: Commodity price volatility creates economic challenges, necessitating
accurate multi-horizon forecasting. Predicting prices for commodities like
copper and crude oil is complicated by diverse interacting factors
(macroeconomic, supply/demand, geopolitical, etc.). Current models often lack
transparency, limiting strategic use. This paper presents a Regularized Sparse
Autoencoder (RSAE), a deep learning framework for simultaneous multi-horizon
commodity price prediction and discovery of interpretable latent market
drivers. The RSAE forecasts prices at multiple horizons (e.g., 1-day, 1-week,
1-month) using multivariate time series. Crucially, L1 regularization
($\|\mathbf{z}\|_1$) on its latent vector $\mathbf{z}$ enforces sparsity,
promoting parsimonious explanations of market dynamics through learned factors
representing underlying drivers (e.g., demand, supply shocks). Drawing from
energy-based models and sparse coding, the RSAE optimizes predictive accuracy
while learning sparse representations. Evaluated on historical Copper and Crude
Oil data with numerous indicators, our findings indicate the RSAE offers
competitive multi-horizon forecasting accuracy and data-driven insights into
price dynamics via its interpretable latent space, a key advantage over
traditional black-box approaches.

</details>


### [368] [Functional Complexity-adaptive Temporal Tensor Decomposition](https://arxiv.org/pdf/2502.06164)
*Panqi Chen, Lei Cheng, Jianlong Li, Weichang Li, Weiqing Liu, Jiang Bian, Shikai Fang*

Main category: cs.LG

TL;DR: The paper introduces CATTE, a method for functional temporal tensor decomposition that adapts model complexity and handles continuous indexes beyond just temporal modes.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with continuous indexes in non-temporal modes (e.g., spatial) and lack adaptive model complexity.

Method: CATTE uses learnable Fourier features for spatial indexes, neural ODEs for temporal trajectories, and a sparsity-inducing prior for adaptive complexity.

Result: CATTE outperforms existing methods in prediction, robustness, and uncovering underlying tensor ranks.

Conclusion: CATTE addresses key limitations in functional temporal tensor decomposition, offering improved performance and adaptability.

Abstract: Tensor decomposition is a fundamental tool for analyzing multi-dimensional
data by learning low-rank factors to represent high-order interactions. While
recent works on temporal tensor decomposition have made significant progress by
incorporating continuous timestamps in latent factors, they still struggle with
general tensor data with continuous indexes not only in the temporal mode but
also in other modes, such as spatial coordinates in climate data. Moreover, the
challenge of self-adapting model complexity is largely unexplored in functional
temporal tensor models, with existing methods being inapplicable in this
setting. To address these limitations, we propose functional
\underline{C}omplexity-\underline{A}daptive \underline{T}emporal
\underline{T}ensor d\underline{E}composition (\textsc{Catte}).
  Our approach encodes continuous spatial indexes as learnable Fourier features
and employs neural ODEs in latent space to learn the temporal trajectories of
factors. To enable automatic adaptation of model complexity, we introduce a
sparsity-inducing prior over the factor trajectories.
  We develop an efficient variational inference scheme with an analytical
evidence lower bound, enabling sampling-free optimization. Through extensive
experiments on both synthetic and real-world datasets, we demonstrate that
\textsc{Catte} not only reveals the underlying ranks of functional temporal
tensors but also significantly outperforms existing methods in prediction
performance and robustness against noise.

</details>


### [369] [Joint Metric Space Embedding by Unbalanced OT with Gromov-Wasserstein Marginal Penalization](https://arxiv.org/pdf/2502.07510)
*Florian Beier, Moritz Piening, Robert Beinert, Gabriele Steidl*

Main category: cs.LG

TL;DR: A new unsupervised method aligns heterogeneous datasets by mapping them to a common metric space using unbalanced optimal transport with Gromov-Wasserstein penalization.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of aligning datasets from different domains without known correspondences, leveraging optimal transport theory.

Method: Uses an unbalanced optimal transport problem with Gromov-Wasserstein marginal penalization, reformulated as a quadratic, multi-marginal problem, solved via block-coordinate descent.

Result: Proves existence of a minimizer and convergence to the embedded Wasserstein distance. Demonstrates effectiveness in Euclidean and non-Euclidean spaces.

Conclusion: The method provides a robust framework for unsupervised dataset alignment, with theoretical guarantees and practical applicability.

Abstract: We propose a new approach for unsupervised alignment of heterogeneous
datasets, which maps data from two different domains without any known
correspondences to a common metric space. Our method is based on an unbalanced
optimal transport problem with Gromov-Wasserstein marginal penalization. It can
be seen as a counterpart to the recently introduced joint multidimensional
scaling method. We prove that there exists a minimizer of our functional and
that for penalization parameters going to infinity, the corresponding sequence
of minimizers converges to a minimizer of the so-called embedded Wasserstein
distance. Our model can be reformulated as a quadratic, multi-marginal,
unbalanced optimal transport problem, for which a bi-convex relaxation admits a
numerical solver via block-coordinate descent. We provide numerical examples
for joint embeddings in Euclidean as well as non-Euclidean spaces.

</details>


### [370] [Prototype Augmented Hypernetworks for Continual Learning](https://arxiv.org/pdf/2505.07450)
*Neil De La Fuente, Maria Pilligua, Daniel Vidal, Albin Soutiff, Cecilia Curreli, Daniel Cremers, Andrey Barsky*

Main category: cs.LG

TL;DR: PAH introduces Prototype-Augmented Hypernetworks to mitigate catastrophic forgetting in continual learning by dynamically generating task-specific heads and using dual distillation losses.


<details>
  <summary>Details</summary>
Motivation: Address catastrophic forgetting in continual learning where gradient updates for new tasks overwrite prior knowledge.

Method: Uses a hypernetwork conditioned on task prototypes to generate task-specific classifier heads, combining cross-entropy with dual distillation losses for logit and prototype alignment.

Result: Achieves 74.5% and 63.7% accuracy on Split-CIFAR100 and TinyImageNet with minimal forgetting (1.7% and 4.4%).

Conclusion: PAH outperforms prior methods without storing samples or heads, demonstrating effective mitigation of catastrophic forgetting.

Abstract: Continual learning (CL) aims to learn a sequence of tasks without forgetting
prior knowledge, but gradient updates for a new task often overwrite the
weights learned earlier, causing catastrophic forgetting (CF). We propose
Prototype-Augmented Hypernetworks (PAH), a framework where a single
hypernetwork, conditioned on learnable task prototypes, dynamically generates
task-specific classifier heads on demand. To mitigate forgetting, PAH combines
cross-entropy with dual distillation losses, one to align logits and another to
align prototypes, ensuring stable feature representations across tasks.
Evaluations on Split-CIFAR100 and TinyImageNet demonstrate that PAH achieves
state-of-the-art performance, reaching 74.5 % and 63.7 % accuracy with only 1.7
% and 4.4 % forgetting, respectively, surpassing prior methods without storing
samples or heads.

</details>


### [371] [Emotional EEG Classification using Upscaled Connectivity Matrices](https://arxiv.org/pdf/2502.07843)
*Chae-Won Lee, Jong-Seok Lee*

Main category: cs.LG

TL;DR: Proposes upscaling connectivity matrices in EEG classification to preserve important patterns lost in CNNs, improving performance.


<details>
  <summary>Details</summary>
Motivation: Limitation of CNNs losing important patterns in connectivity matrices during convolution.

Method: Upscaling connectivity matrices to strengthen local patterns before CNN processing.

Result: Significant enhancement in classification performance.

Conclusion: Upscaling connectivity matrices is a simple yet effective solution for better EEG classification.

Abstract: In recent studies of emotional EEG classification, connectivity matrices have
been successfully employed as input to convolutional neural networks (CNNs),
which can effectively consider inter-regional interaction patterns in EEG.
However, we find that such an approach has a limitation that important patterns
in connectivity matrices may be lost during the convolutional operations in
CNNs. To resolve this issue, we propose and validate an idea to upscale the
connectivity matrices to strengthen the local patterns. Experimental results
demonstrate that this simple idea can significantly enhance the classification
performance.

</details>


### [372] [ALinFiK: Learning to Approximate Linearized Future Influence Kernel for Scalable Third-Party LLM Data Valuation](https://arxiv.org/pdf/2503.01052)
*Yanzhou Pan, Huawei Lin, Yide Ran, Jiamin Chen, Xiaodong Yu, Weijie Zhao, Denghui Zhang, Zhaozhuo Xu*

Main category: cs.LG

TL;DR: A scalable data valuation method (LinFiK and ALinFiK) for LLMs improves performance and benefits both data providers and developers.


<details>
  <summary>Details</summary>
Motivation: Optimizing LLM performance with limited budgets requires high-quality training data, necessitating effective data valuation.

Method: Introduces LinFiK for data sample valuation and ALinFiK, a scalable learning strategy to approximate LinFiK.

Result: Outperforms existing baselines in effectiveness, efficiency, and scalability, especially with larger LLMs.

Conclusion: The proposed approach offers a practical solution for data valuation in LLM training, benefiting stakeholders.

Abstract: Large Language Models (LLMs) heavily rely on high-quality training data,
making data valuation crucial for optimizing model performance, especially when
working within a limited budget. In this work, we aim to offer a third-party
data valuation approach that benefits both data providers and model developers.
We introduce a linearized future influence kernel (LinFiK), which assesses the
value of individual data samples in improving LLM performance during training.
We further propose ALinFiK, a learning strategy to approximate LinFiK, enabling
scalable data valuation. Our comprehensive evaluations demonstrate that this
approach surpasses existing baselines in effectiveness and efficiency,
demonstrating significant scalability advantages as LLM parameters increase.

</details>


### [373] [DPR: Diffusion Preference-based Reward for Offline Reinforcement Learning](https://arxiv.org/pdf/2503.01143)
*Teng Pang, Bingzheng Wang, Guoqiang Wu, Yilong Yin*

Main category: cs.LG

TL;DR: The paper introduces Diffusion Preference-based Reward (DPR) and its conditional variant (C-DPR) to improve reward modeling in offline PbRL, outperforming traditional MLP and Transformer methods.


<details>
  <summary>Details</summary>
Motivation: Current MLP and Transformer-based methods in offline PbRL may inadequately model reward functions due to insufficient modeling capabilities, limiting the effectiveness of preference-driven rewards.

Method: The authors propose DPR and C-DPR, using diffusion models to directly model preference distributions for state-action pairs, leveraging relative preference information for enhanced modeling.

Result: Experiments show that diffusion-based reward acquisition (DPR and C-DPR) outperforms MLP and Transformer-based methods in offline reinforcement learning tasks.

Conclusion: Diffusion models (DPR and C-DPR) offer a superior approach for reward modeling in offline PbRL, addressing limitations of existing methods.

Abstract: Offline preference-based reinforcement learning (PbRL) mitigates the need for
reward definition, aligning with human preferences via preference-driven reward
feedback without interacting with the environment. However, the effectiveness
of preference-driven reward functions depends on the modeling ability of the
learning model, which current MLP-based and Transformer-based methods may fail
to adequately provide. To alleviate the failure of the reward function caused
by insufficient modeling, we propose a novel preference-based reward
acquisition method: Diffusion Preference-based Reward (DPR). Unlike previous
methods using Bradley-Terry models for trajectory preferences, we use diffusion
models to directly model preference distributions for state-action pairs,
allowing rewards to be discriminatively obtained from these distributions. In
addition, considering the particularity of preference data that only know the
internal relationships of paired trajectories, we further propose Conditional
Diffusion Preference-based Reward (C-DPR), which leverages relative preference
information to enhance the construction of the diffusion model. We apply the
above methods to existing offline reinforcement learning algorithms and a
series of experiment results demonstrate that the diffusion-based reward
acquisition approach outperforms previous MLP-based and Transformer-based
methods.

</details>


### [374] [Early Detection of Forest Calamities in Homogeneous Stands -- Deep Learning Applied to Bark-Beetle Outbreaks](https://arxiv.org/pdf/2503.12883)
*Maximilian Kirsch, Jakob Wernicke, Pawan Datta, Christine Preisach*

Main category: cs.LG

TL;DR: The study proposes an LSTM Autoencoder for early anomaly detection in forest health using Sentinel-2 time series, achieving 87% accuracy and outperforming BFAST.


<details>
  <summary>Details</summary>
Motivation: Climate change has increased forest vulnerability to insect damage, necessitating efficient monitoring systems without labeled data.

Method: Utilizes a Deep Learning LSTM Autoencoder on Sentinel-2 time series data for anomaly detection, requiring minimal storage.

Result: Achieved 87% accuracy, detecting 61% of anomalies early (over a month before visible degradation), outperforming BFAST.

Conclusion: LSTM Autoencoders offer a resource-efficient, timely solution for forest health monitoring.

Abstract: Climate change has increased the vulnerability of forests to insect-related
damage, resulting in widespread forest loss in Central Europe and highlighting
the need for effective, continuous monitoring systems. Remote sensing based
forest health monitoring, oftentimes, relies on supervised machine learning
algorithms that require labeled training data. Monitoring temporal patterns
through time series analysis offers a potential alternative for earlier
detection of disturbance but requires substantial storage resources. This study
investigates the potential of a Deep Learning algorithm based on a Long Short
Term Memory (LSTM) Autoencoder for the detection of anomalies in forest health
(e.g. bark beetle outbreaks), utilizing Sentinel-2 time series data. This
approach is an alternative to supervised machine learning methods, avoiding the
necessity for labeled training data. Furthermore, it is more memory-efficient
than other time series analysis approaches, as a robust model can be created
using only a 26-week-long time series as input. In this study, we monitored
pure stands of spruce in Thuringia, Germany, over a 7-year period from 2018 to
the end of 2024. Our best model achieved a detection accuracy of 87% on test
data and was able to detect 61% of all anomalies at a very early stage (more
than a month before visible signs of forest degradation). Compared to another
widely used time series break detection algorithm - BFAST (Breaks For Additive
Season and Trend), our approach consistently detected higher percentage of
anomalies at an earlier stage. These findings suggest that LSTM-based
Autoencoders could provide a promising, resource-efficient approach to forest
health monitoring, enabling more timely responses to emerging threats.

</details>


### [375] [Sample-Efficient Reinforcement Learning of Koopman eNMPC](https://arxiv.org/pdf/2503.18787)
*Daniel Mayfrank, Mehmet Velioglu, Alexander Mitsos, Manuel Dahmen*

Main category: cs.LG

TL;DR: Combining model-based RL with differentiable Koopman (e)NMPCs improves sample efficiency and control performance in tuning data-driven eNMPCs, outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: To enhance the sample efficiency of RL in tuning data-driven eNMPCs for optimal control performance.

Method: Combines model-based RL with differentiable Koopman (e)NMPCs and applies it to a CSTR case study.

Result: Outperforms benchmarks in control performance and sample efficiency, especially with physics-informed learning.

Conclusion: The approach effectively improves RL-based tuning of eNMPCs, leveraging prior knowledge for better efficiency.

Abstract: Reinforcement learning (RL) can be used to tune data-driven (economic)
nonlinear model predictive controllers ((e)NMPCs) for optimal performance in a
specific control task by optimizing the dynamic model or parameters in the
policy's objective function or constraints, such as state bounds. However, the
sample efficiency of RL is crucial, and to improve it, we combine a model-based
RL algorithm with our published method that turns Koopman (e)NMPCs into
automatically differentiable policies. We apply our approach to an eNMPC case
study of a continuous stirred-tank reactor (CSTR) model from the literature.
The approach outperforms benchmark methods, i.e., data-driven eNMPCs using
models based on system identification without further RL tuning of the
resulting policy, and neural network controllers trained with model-based RL,
by achieving superior control performance and higher sample efficiency.
Furthermore, utilizing partial prior knowledge about the system dynamics via
physics-informed learning further increases sample efficiency.

</details>


### [376] [From S4 to Mamba: A Comprehensive Survey on Structured State Space Models](https://arxiv.org/pdf/2503.18970)
*Shriyank Somvanshi, Md Monzurul Islam, Mahmuda Sultana Mimi, Sazzad Bin Bashar Polock, Gaurab Chhetri, Subasish Das*

Main category: cs.LG

TL;DR: Structured State Space Models (SSMs) offer efficient long-range dependency modeling with linear complexity, surpassing RNNs and Transformers. This survey reviews SSM advancements, applications, and challenges.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies of RNNs (vanishing gradients) and Transformers (quadratic complexity) by leveraging SSMs for better long-sequence processing.

Method: Review evolution of SSMs from S4 to models like Mamba, S5, and Jamba, comparing their performance in NLP, speech, vision, and time-series tasks.

Result: SSMs show superior computational efficiency, memory optimization, and inference speed while handling long-range dependencies.

Conclusion: SSMs are promising but face challenges in training, hybrid modeling, and interpretability. The survey guides future research and applications.

Abstract: Recent advancements in sequence modeling have led to the emergence of
Structured State Space Models (SSMs) as an efficient alternative to Recurrent
Neural Networks (RNNs) and Transformers, addressing challenges in long-range
dependency modeling and computational efficiency. While RNNs suffer from
vanishing gradients and sequential inefficiencies, and Transformers face
quadratic complexity, SSMs leverage structured recurrence and state-space
representations to achieve superior long-sequence processing with linear or
near-linear complexity. This survey provides a comprehensive review of SSMs,
tracing their evolution from the foundational S4 model to its successors like
Mamba, Simplified Structured State Space Sequence Model (S5), and Jamba,
highlighting their improvements in computational efficiency, memory
optimization, and inference speed. By comparing SSMs with traditional sequence
models across domains such as natural language processing (NLP), speech
recognition, vision, and time-series forecasting, we demonstrate their
advantages in handling long-range dependencies while reducing computational
overhead. Despite their potential, challenges remain in areas such as training
optimization, hybrid modeling, and interpretability. This survey serves as a
structured guide for researchers and practitioners, detailing the advancements,
trade-offs, and future directions of SSM-based architectures in AI and deep
learning.

</details>


### [377] [Brain Hematoma Marker Recognition Using Multitask Learning: SwinTransformer and Swin-Unet](https://arxiv.org/pdf/2505.06185)
*Kodai Hirata, Tsuyoshi Okita*

Main category: cs.LG

TL;DR: MTL-Swin-Unet uses multi-task learning with transformers for classification and semantic segmentation, improving performance in both covariate and non-covariate shift settings.


<details>
  <summary>Details</summary>
Motivation: Address spurious-correlation problems by enhancing image representation with semantic segmentation and reconstruction.

Method: Multi-task learning with transformers (MTL-Swin-Unet) combining classification, semantic segmentation, and image reconstruction.

Result: Outperformed in F-value (no covariate shift) and AUC (covariate shift) compared to other classifiers.

Conclusion: MTL-Swin-Unet effectively handles covariate shifts and improves performance in medical image analysis tasks.

Abstract: This paper proposes a method MTL-Swin-Unet which is multi-task learning using
transformers for classification and semantic segmentation. For
spurious-correlation problems, this method allows us to enhance the image
representation with two other image representations: representation obtained by
semantic segmentation and representation obtained by image reconstruction. In
our experiments, the proposed method outperformed in F-value measure than other
classifiers when the test data included slices from the same patient (no
covariate shift). Similarly, when the test data did not include slices from the
same patient (covariate shift setting), the proposed method outperformed in AUC
measure.

</details>


### [378] [Transformer representation learning is necessary for dynamic multi-modal physiological data on small-cohort patients](https://arxiv.org/pdf/2504.04120)
*Bingxu Wang, Min Ge, Kunzhi Cai, Yuqi Zhang, Zeyi Zhou, Wenjiao Li, Yachong Guo, Wei Wang, Qing Zhou*

Main category: cs.LG

TL;DR: A Transformer-based framework improves early POD diagnosis using multi-modal physiological data, showing better sensitivity and Youden index.


<details>
  <summary>Details</summary>
Motivation: POD is underdiagnosed due to subjective methods; early, accurate diagnosis is critical.

Method: Uses Transformer representation models and traditional ML on multi-modal data (aEEG, vital signs, etc.).

Result: Improved sensitivity and Youden index, especially with Pathformer fusion adaptation.

Conclusion: Multi-modal data and Transformer architectures are promising for clinical POD diagnosis.

Abstract: Postoperative delirium (POD), a severe neuropsychiatric complication
affecting nearly 50% of high-risk surgical patients, is defined as an acute
disorder of attention and cognition, It remains significantly underdiagnosed in
the intensive care units (ICUs) due to subjective monitoring methods. Early and
accurate diagnosis of POD is critical and achievable. Here, we propose a POD
prediction framework comprising a Transformer representation model followed by
traditional machine learning algorithms. Our approaches utilizes multi-modal
physiological data, including amplitude-integrated electroencephalography
(aEEG), vital signs, electrocardiographic monitor data as well as hemodynamic
parameters. We curated the first multi-modal POD dataset encompassing two
patient types and evaluated the various Transformer architectures for
representation learning. Empirical results indicate a consistent improvements
of sensitivity and Youden index in patient TYPE I using Transformer
representations, particularly our fusion adaptation of Pathformer. By enabling
effective delirium diagnosis from postoperative day 1 to 3, our extensive
experimental findings emphasize the potential of multi-modal physiological data
and highlight the necessity of representation learning via multi-modal
Transformer architecture in clinical diagnosis.

</details>


### [379] [DrivAer Transformer: A high-precision and fast prediction method for vehicle aerodynamic drag coefficient based on the DrivAerNet++ dataset](https://arxiv.org/pdf/2504.08217)
*Jiaqi He, Xiangwen Luo, Yiping Wang*

Main category: cs.LG

TL;DR: A Transformer-based framework (DAT) is proposed for accurate aerodynamic drag prediction from 3D vehicle meshes, addressing limitations of deep learning methods in handling complex geometries.


<details>
  <summary>Details</summary>
Motivation: Deep learning methods for aerodynamic performance evaluation face challenges with complex 3D vehicle models due to limited datasets and geometric diversity.

Method: The study introduces DrivAer Transformer (DAT), leveraging Transformer models and the DrivAerNet++ dataset for direct 3D mesh processing.

Result: DAT enables fast and accurate drag prediction, overcoming traditional method limitations like 2D rendering or SDF.

Conclusion: DAT accelerates vehicle design and improves efficiency, laying a foundation for data-driven automotive design.

Abstract: At the current stage, deep learning-based methods have demonstrated excellent
capabilities in evaluating aerodynamic performance, significantly reducing the
time and cost required for traditional computational fluid dynamics (CFD)
simulations. However, when faced with the task of processing extremely complex
three-dimensional (3D) vehicle models, the lack of large-scale datasets and
training resources, coupled with the inherent diversity and complexity of the
geometry of different vehicle models, means that the prediction accuracy and
versatility of these networks are still not up to the level required for
current production. In view of the remarkable success of Transformer models in
the field of natural language processing and their strong potential in the
field of image processing, this study innovatively proposes a point cloud
learning framework called DrivAer Transformer (DAT). The DAT structure uses the
DrivAerNet++ dataset, which contains high-fidelity CFD data of
industrial-standard 3D vehicle shapes. enabling accurate estimation of air drag
directly from 3D meshes, thus avoiding the limitations of traditional methods
such as 2D image rendering or signed distance fields (SDF). DAT enables fast
and accurate drag prediction, driving the evolution of the aerodynamic
evaluation process and laying the critical foundation for introducing a
data-driven approach to automotive design. The framework is expected to
accelerate the vehicle design process and improve development efficiency.

</details>


### [380] [A Generalized Meta Federated Learning Framework with Theoretical Convergence Guarantees](https://arxiv.org/pdf/2504.21327)
*Mohammad Vahid Jamali, Hamid Saber, Jung Hyun Bae*

Main category: cs.LG

TL;DR: Meta federated learning (FL) generalizes conventional FL by optimizing for multiple fine-tuning steps, improving model personalization under data heterogeneity. A new FedAvg variant shows better accuracy and convergence.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of conventional meta FL, which only considers one fine-tuning step, especially under highly heterogeneous data distributions.

Method: Proposes a generalized meta FL framework minimizing average loss after any number of fine-tuning steps. Introduces a FedAvg variant and provides theoretical convergence analysis.

Result: Experiments on real-world datasets show superior accuracy and faster convergence compared to conventional methods.

Conclusion: The generalized meta FL framework and FedAvg variant effectively improve model personalization and performance under heterogeneous data.

Abstract: Meta federated learning (FL) is a personalized variant of FL, where multiple
agents collaborate on training an initial shared model without exchanging raw
data samples. The initial model should be trained in a way that current or new
agents can easily adapt it to their local datasets after one or a few
fine-tuning steps, thus improving the model personalization. Conventional meta
FL approaches minimize the average loss of agents on the local models obtained
after one step of fine-tuning. In practice, agents may need to apply several
fine-tuning steps to adapt the global model to their local data, especially
under highly heterogeneous data distributions across agents. To this end, we
present a generalized framework for the meta FL by minimizing the average loss
of agents on their local model after any arbitrary number $\nu$ of fine-tuning
steps. For this generalized framework, we present a variant of the well-known
federated averaging (FedAvg) algorithm and conduct a comprehensive theoretical
convergence analysis to characterize the convergence speed as well as behavior
of the meta loss functions in both the exact and approximated cases. Our
experiments on real-world datasets demonstrate superior accuracy and faster
convergence for the proposed scheme compared to conventional approaches.

</details>


### [381] [Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws](https://arxiv.org/pdf/2505.06699)
*Xiyuan Wei, Ming Lin, Fanjiang Ye, Fengguang Song, Liangliang Cao, My T. Thai, Tianbao Yang*

Main category: cs.LG

TL;DR: The paper introduces a theory-driven framework for model steering, called DRRho risk minimization, rooted in DRO, and applies it to CLIP, showing improved generalization and efficiency.


<details>
  <summary>Details</summary>
Motivation: To formalize and enhance the understanding of model steering, which uses a reference model to guide training, addressing sub-optimal performance in ad-hoc methods.

Method: Proposes DRRho risk minimization, a DRO-based framework, and applies it to CLIP (DRRho-CLIP) for contrastive learning with a reference model.

Result: Theoretical insights confirm improved generalization and data efficiency; experiments show superior scaling and performance over heuristic methods.

Conclusion: The work advances the understanding and practice of model steering, providing a robust theoretical foundation and effective application in CLIP.

Abstract: This paper formalizes an emerging learning paradigm that uses a trained model
as a reference to guide and enhance the training of a target model through
strategic data selection or weighting, named $\textbf{model steering}$. While
ad-hoc methods have been used in various contexts, including the training of
large foundation models, its underlying principles remain insufficiently
understood, leading to sub-optimal performance. In this work, we propose a
theory-driven framework for model steering called $\textbf{DRRho risk
minimization}$, which is rooted in Distributionally Robust Optimization (DRO).
Through a generalization analysis, we provide theoretical insights into why
this approach improves generalization and data efficiency compared to training
without a reference model. To the best of our knowledge, this is the first time
such theoretical insights are provided for the new learning paradigm, which
significantly enhance our understanding and practice of model steering.
Building on these insights and the connection between contrastive learning and
DRO, we introduce a novel method for Contrastive Language-Image Pretraining
(CLIP) with a reference model, termed DRRho-CLIP. Extensive experiments
validate the theoretical insights, reveal a superior scaling law compared to
CLIP without a reference model, and demonstrate its strength over existing
heuristic approaches.

</details>


### [382] [COMRECGC: Global Graph Counterfactual Explainer through Common Recourse](https://arxiv.org/pdf/2505.07081)
*Gregoire Fournier, Sourav Medya*

Main category: cs.LG

TL;DR: The paper introduces COMRECGC, an algorithm for finding common recourse explanations in GNNs, addressing a gap in global counterfactual explanations. It outperforms baselines on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Existing work on GNN explanations focuses on local counterfactuals, leaving global counterfactual explanations, particularly common recourse, unexplored.

Method: The paper formalizes the common recourse problem and designs COMRECGC, an algorithm to solve it.

Result: COMRECGC outperforms baselines on four real-world datasets and shows comparable or superior performance to graph counterfactual explanations.

Conclusion: Common recourse explanations are valuable for applications like drug discovery, making COMRECGC a promising solution.

Abstract: Graph neural networks (GNNs) have been widely used in various domains such as
social networks, molecular biology, or recommendation systems. Concurrently,
different explanations methods of GNNs have arisen to complement its black-box
nature. Explanations of the GNNs' predictions can be categorized into two
types--factual and counterfactual. Given a GNN trained on binary classification
into ''accept'' and ''reject'' classes, a global counterfactual explanation
consists in generating a small set of ''accept'' graphs relevant to all of the
input ''reject'' graphs. The transformation of a ''reject'' graph into an
''accept'' graph is called a recourse. A common recourse explanation is a small
set of recourse, from which every ''reject'' graph can be turned into an
''accept'' graph. Although local counterfactual explanations have been studied
extensively, the problem of finding common recourse for global counterfactual
explanation remains unexplored, particularly for GNNs. In this paper, we
formalize the common recourse explanation problem, and design an effective
algorithm, COMRECGC, to solve it. We benchmark our algorithm against strong
baselines on four different real-world graphs datasets and demonstrate the
superior performance of COMRECGC against the competitors. We also compare the
common recourse explanations to the graph counterfactual explanation, showing
that common recourse explanations are either comparable or superior, making
them worth considering for applications such as drug discovery or computational
biology.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [383] [Scalable UAV Multi-Hop Networking via Multi-Agent Reinforcement Learning with Large Language Models](https://arxiv.org/pdf/2505.08448)
*Yanggang Xu, Weijie Hong, Jirong Zha, Geng Chen, Jianfeng Zheng, Chen-Chun Hsia, Xinlei Chen*

Main category: cs.MA

TL;DR: MRLMN integrates MARL and LLMs to optimize UAV networks in disasters, improving scalability and robustness through reward decomposition, behavioral constraints, and knowledge distillation.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in organizing UAVs for multi-hop networks in dynamic disaster scenarios, focusing on algorithmic scalability and coordinated decision-making.

Method: Proposes MRLMN, combining MARL and LLMs with reward decomposition, behavioral constraints, and a Hungarian algorithm-based knowledge distillation module.

Result: Simulations show improved network performance, including better coverage and communication quality.

Conclusion: MRLMN effectively enhances UAV network performance in disaster scenarios through integrated MARL and LLM techniques.

Abstract: In disaster scenarios, establishing robust emergency communication networks
is critical, and unmanned aerial vehicles (UAVs) offer a promising solution to
rapidly restore connectivity. However, organizing UAVs to form multi-hop
networks in large-scale dynamic environments presents significant challenges,
including limitations in algorithmic scalability and the vast exploration space
required for coordinated decision-making. To address these issues, we propose
MRLMN, a novel framework that integrates multi-agent reinforcement learning
(MARL) and large language models (LLMs) to jointly optimize UAV agents toward
achieving optimal networking performance. The framework incorporates a grouping
strategy with reward decomposition to enhance algorithmic scalability and
balance decision-making across UAVs. In addition, behavioral constraints are
applied to selected key UAVs to improve the robustness of the network.
Furthermore, the framework integrates LLM agents, leveraging knowledge
distillation to transfer their high-level decision-making capabilities to MARL
agents. This enhances both the efficiency of exploration and the overall
training process. In the distillation module, a Hungarian algorithm-based
matching scheme is applied to align the decision outputs of the LLM and MARL
agents and define the distillation loss. Extensive simulation results validate
the effectiveness of our approach, demonstrating significant improvements in
network performance, including enhanced coverage and communication quality.

</details>


### [384] [LLM Multi-Agent Systems: Challenges and Open Problems](https://arxiv.org/pdf/2402.03578)
*Shanshan Han, Qifan Zhang, Yuhang Yao, Weizhao Jin, Zhaozhuo Xu*

Main category: cs.MA

TL;DR: The paper examines challenges in multi-agent systems, focusing on task allocation, reasoning, context management, and memory. It also explores applications in blockchain.


<details>
  <summary>Details</summary>
Motivation: To address inadequately solved challenges in multi-agent systems and highlight their potential in distributed systems like blockchain.

Method: Leveraging diverse agent roles and capabilities, optimizing task allocation, iterative debates for reasoning, and improving context and memory management.

Result: Identifies key challenges and proposes solutions for multi-agent systems, with potential applications in blockchain.

Conclusion: Multi-agent systems hold promise for complex tasks and distributed systems, but challenges like task allocation and reasoning need further attention.

Abstract: This paper explores multi-agent systems and identify challenges that remain
inadequately addressed. By leveraging the diverse capabilities and roles of
individual agents, multi-agent systems can tackle complex tasks through agent
collaboration. We discuss optimizing task allocation, fostering robust
reasoning through iterative debates, managing complex and layered context
information, and enhancing memory management to support the intricate
interactions within multi-agent systems. We also explore potential applications
of multi-agent systems in blockchain systems to shed light on their future
development and application in real-world distributed systems.

</details>


### [385] [MobA: Multifaceted Memory-Enhanced Adaptive Planning for Efficient Mobile Task Automation](https://arxiv.org/pdf/2410.13757)
*Zichen Zhu, Hao Tang, Yansi Li, Dingye Liu, Hongshen Xu, Kunyao Lan, Danyang Zhang, Yixuan Jiang, Hao Zhou, Chenrun Wang, Situo Zhang, Liangtai Sun, Yixiao Wang, Yuheng Sun, Lu Chen, Kai Yu*

Main category: cs.MA

TL;DR: MobA is a new MLLM-based mobile assistant system designed to handle complex GUI interactions by incorporating adaptive planning, error recovery, and memory support.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM-based agents struggle with dynamic and structured GUI environments, which involve text, images, and spatial relationships, as well as variable action spaces.

Method: MobA introduces an adaptive planning module with reflection for error recovery and dynamic plan adjustment, along with a multifaceted memory module for enhanced adaptability.

Result: Experiments on MobBench and AndroidArena show MobA effectively handles dynamic GUI environments and complex tasks.

Conclusion: MobA addresses the limitations of current MLLM-based agents in GUI interactions, demonstrating improved performance and adaptability.

Abstract: Existing Multimodal Large Language Model (MLLM)-based agents face significant
challenges in handling complex GUI (Graphical User Interface) interactions on
devices. These challenges arise from the dynamic and structured nature of GUI
environments, which integrate text, images, and spatial relationships, as well
as the variability in action spaces across different pages and tasks. To
address these limitations, we propose MobA, a novel MLLM-based mobile assistant
system. MobA introduces an adaptive planning module that incorporates a
reflection mechanism for error recovery and dynamically adjusts plans to align
with the real environment contexts and action module's execution capacity.
Additionally, a multifaceted memory module provides comprehensive memory
support to enhance adaptability and efficiency. We also present MobBench, a
dataset designed for complex mobile interactions. Experimental results on
MobBench and AndroidArena demonstrate MobA's ability to handle dynamic GUI
environments and perform complex mobile tasks.

</details>


### [386] [Capability-Aware Shared Hypernetworks for Flexible Heterogeneous Multi-Robot Coordination](https://arxiv.org/pdf/2501.06058)
*Kevin Fu, Shalin Anand Jain, Pierce Howell, Harish Ravichandar*

Main category: cs.MA

TL;DR: CASH is a hypernetwork-based architecture for heterogeneous multi-robot teams, balancing expressivity and efficiency by dynamically adapting policies to robot capabilities, enabling zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: Existing neural architectures for heterogeneous robot teams force a trade-off between expressivity (diverse behaviors) and efficiency (sample efficiency). CASH aims to avoid this trade-off.

Method: Proposes Capability-Aware Shared Hypernetworks (CASH), a soft weight-sharing architecture using hypernetworks to dynamically adapt policies to each robot's capabilities.

Result: CASH outperforms baselines in performance and sample efficiency, with 60%-80% fewer parameters, and enables zero-shot generalization to unseen robots or teams.

Conclusion: CASH successfully balances expressivity and efficiency, demonstrating superior performance and generalization across diverse tasks and learning paradigms.

Abstract: Recent advances have enabled heterogeneous multi-robot teams to learn complex
and effective coordination skills. However, existing neural architectures that
support heterogeneous teaming tend to force a trade-off between expressivity
and efficiency. Shared-parameter designs prioritize sample efficiency by
enabling a single network to be shared across all or a pre-specified subset of
robots (via input augmentations), but tend to limit behavioral diversity. In
contrast, recent designs employ a separate policy for each robot, enabling
greater diversity and expressivity at the cost of efficiency and
generalization. Our key insight is that such tradeoffs can be avoided by
viewing these design choices as ends of a broad spectrum. Inspired by recent
work in transfer and meta learning, and building on prior work in multi-robot
task allocation, we propose Capability-Aware Shared Hypernetworks (CASH), a
soft weight sharing architecture that uses hypernetworks to efficiently learn a
flexible shared policy that dynamically adapts to each robot post-training. By
explicitly encoding the impact of robot capabilities (e.g., speed and payload)
on collective behavior, CASH enables zero-shot generalization to unseen robots
or team compositions. Our experiments involve multiple heterogeneous tasks,
three learning paradigms (imitation learning, value-based, and policy-gradient
RL), and SOTA multi-robot simulation (JaxMARL) and hardware (Robotarium)
platforms. Across all conditions, we find that CASH generates
appropriately-diverse behaviors and consistently outperforms baseline
architectures in terms of performance and sample efficiency during both
training and zero-shot generalization, all with 60%-80% fewer learnable
parameters.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)



<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [387] [MiniMax-Speech: Intrinsic Zero-Shot Text-to-Speech with a Learnable Speaker Encoder](https://arxiv.org/pdf/2505.07916)
*Bowen Zhang, Congchao Guo, Geng Yang, Hang Yu, Haozhe Zhang, Heidi Lei, Jialong Mai, Junjie Yan, Kaiyue Yang, Mingqi Yang, Peikai Huang, Ruiyang Jin, Sitan Jiang, Weihua Cheng, Yawei Li, Yichen Xiao, Yiying Zhou, Yongmao Zhang, Yuan Lu, Yucen He*

Main category: eess.AS

TL;DR: MiniMax-Speech is a Transformer-based TTS model with a learnable speaker encoder for zero-shot expressive speech and one-shot voice cloning, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: To create a high-quality TTS model capable of zero-shot expressive speech and one-shot voice cloning without requiring reference audio transcription.

Method: Uses an autoregressive Transformer with a learnable speaker encoder and Flow-VAE for enhanced audio quality. Supports 32 languages.

Result: Achieves SOTA in voice cloning metrics (WER, Speaker Similarity) and tops the TTS Arena leaderboard.

Conclusion: MiniMax-Speech is highly extensible, enabling applications like voice emotion control and professional voice cloning, without modifying the base model.

Abstract: We introduce MiniMax-Speech, an autoregressive Transformer-based
Text-to-Speech (TTS) model that generates high-quality speech. A key innovation
is our learnable speaker encoder, which extracts timbre features from a
reference audio without requiring its transcription. This enables
MiniMax-Speech to produce highly expressive speech with timbre consistent with
the reference in a zero-shot manner, while also supporting one-shot voice
cloning with exceptionally high similarity to the reference voice. In addition,
the overall quality of the synthesized audio is enhanced through the proposed
Flow-VAE. Our model supports 32 languages and demonstrates excellent
performance across multiple objective and subjective evaluations metrics.
Notably, it achieves state-of-the-art (SOTA) results on objective voice cloning
metrics (Word Error Rate and Speaker Similarity) and has secured the top
position on the public TTS Arena leaderboard. Another key strength of
MiniMax-Speech, granted by the robust and disentangled representations from the
speaker encoder, is its extensibility without modifying the base model,
enabling various applications such as: arbitrary voice emotion control via
LoRA; text to voice (T2V) by synthesizing timbre features directly from text
description; and professional voice cloning (PVC) by fine-tuning timbre
features with additional data. We encourage readers to visit
https://minimax-ai.github.io/tts_tech_report for more examples.

</details>


### [388] [Investigating self-supervised features for expressive, multilingual voice conversion](https://arxiv.org/pdf/2505.08278)
*Álvaro Martín-Cortinas, Daniel Sáez-Trigueros, Grzegorz Beringer, Iván Vallés-Pérez, Roberto Barra-Chicote, Biel Tura-Vecino, Adam Gabryś, Piotr Bilinski, Thomas Merritt, Jaime Lorenzo-Trueba*

Main category: eess.AS

TL;DR: The paper explores voice conversion using self-supervised learning (SSL) to address challenges in disentangling content and speaker information, achieving results comparable to phonetic posteriorgram-based systems.


<details>
  <summary>Details</summary>
Motivation: Voice conversion (VC) systems face challenges in disentangling content and speaker information, leading to issues like speaker leakage or prosodic loss. Supervised methods require costly parallel data, while unsupervised methods struggle with reconstruction.

Method: The approach combines latent representations from SSL models with speaker embeddings, feeding them to a vocoder trained for input reconstruction.

Result: Zero-shot VC results show the method preserves source speaker prosody and content while matching speaker similarity of phonetic posteriorgram-based systems.

Conclusion: Leveraging SSL for VC effectively balances content retention and speaker similarity, offering a promising alternative to traditional methods.

Abstract: Voice conversion (VC) systems are widely used for several applications, from
speaker anonymisation to personalised speech synthesis. Supervised approaches
learn a mapping between different speakers using parallel data, which is
expensive to produce. Unsupervised approaches are typically trained to
reconstruct the input signal, which is composed of the content and the speaker
information. Disentangling these components is a challenge and often leads to
speaker leakage or prosodic information removal. In this paper, we explore
voice conversion by leveraging the potential of self-supervised learning (SSL).
A combination of the latent representations of SSL models, concatenated with
speaker embeddings, is fed to a vocoder which is trained to reconstruct the
input. Zero-shot voice conversion results show that this approach allows to
keep the prosody and content of the source speaker while matching the speaker
similarity of a VC system based on phonetic posteriorgrams (PPGs).

</details>


### [389] [A Survey of Deep Learning for Complex Speech Spectrograms](https://arxiv.org/pdf/2505.08694)
*Yuying Xie, Zheng-Hua Tan*

Main category: eess.AS

TL;DR: A survey on deep learning techniques for processing complex spectrograms in speech signal processing, covering architectures, training strategies, and applications like phase retrieval and speech enhancement.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of state-of-the-art deep learning methods for analyzing and manipulating complex spectrograms in speech processing.

Method: Explores complex-valued neural networks, training strategies, and loss functions tailored for complex spectrogram processing.

Result: Highlights advancements in applications such as phase retrieval, speech enhancement, and speech separation using deep learning.

Conclusion: The survey serves as a resource for researchers and practitioners in speech signal processing and complex-valued neural networks.

Abstract: Recent advancements in deep learning have significantly impacted the field of
speech signal processing, particularly in the analysis and manipulation of
complex spectrograms. This survey provides a comprehensive overview of the
state-of-the-art techniques leveraging deep neural networks for processing
complex spectrograms, which encapsulate both magnitude and phase information.
We begin by introducing complex spectrograms and their associated features for
various speech processing tasks. Next, we explore the key components and
architectures of complex-valued neural networks, which are specifically
designed to handle complex-valued data and have been applied for complex
spectrogram processing. We then discuss various training strategies and loss
functions tailored for training neural networks to process and model complex
spectrograms. The survey further examines key applications, including phase
retrieval, speech enhancement, and speech separation, where deep learning has
achieved significant progress by leveraging complex spectrograms or their
derived feature representations. Additionally, we examine the intersection of
complex spectrograms with generative models. This survey aims to serve as a
valuable resource for researchers and practitioners in the field of speech
signal processing and complex-valued neural networks.

</details>


### [390] [Granite-speech: open-source speech-aware LLMs with strong English ASR capabilities](https://arxiv.org/pdf/2505.08699)
*George Saon, Avihu Dekel, Alexander Brooks, Tohru Nagano, Abraham Daniels, Aharon Satt, Ashish Mittal, Brian Kingsbury, David Haws, Edmilson Morais, Gakuto Kurata, Hagai Aronowitz, Ibrahim Ibrahim, Jeff Kuo, Kate Soule, Luis Lastras, Masayuki Suzuki, Ron Hoory, Samuel Thomas, Sashi Novitasari, Takashi Fukuda, Vishal Sunder, Xiaodong Cui, Zvi Kons*

Main category: eess.AS

TL;DR: Granite-speech LLMs are compact, efficient models for English ASR and AST, outperforming competitors with less data and matching performance in multilingual AST.


<details>
  <summary>Details</summary>
Motivation: To create efficient speech language models for ASR and AST that perform well despite limited training data.

Method: Modality alignment of 2B/8B parameter variants, conformer acoustic encoder, transformer adapter, and LoRA fine-tuning.

Result: Outperforms competitors in English ASR and matches performance in AST for major languages.

Conclusion: Granite-speech LLMs are effective, freely available, and preserve text LLM capabilities in text mode.

Abstract: Granite-speech LLMs are compact and efficient speech language models
specifically designed for English ASR and automatic speech translation (AST).
The models were trained by modality aligning the 2B and 8B parameter variants
of granite-3.3-instruct to speech on publicly available open-source corpora
containing audio inputs and text targets consisting of either human transcripts
for ASR or automatically generated translations for AST. Comprehensive
benchmarking shows that on English ASR, which was our primary focus, they
outperform several competitors' models that were trained on orders of magnitude
more proprietary data, and they keep pace on English-to-X AST for major
European languages, Japanese, and Chinese. The speech-specific components are:
a conformer acoustic encoder using block attention and self-conditioning
trained with connectionist temporal classification, a windowed
query-transformer speech modality adapter used to do temporal downsampling of
the acoustic embeddings and map them to the LLM text embedding space, and LoRA
adapters to further fine-tune the text LLM. Granite-speech-3.3 operates in two
modes: in speech mode, it performs ASR and AST by activating the encoder,
projector, and LoRA adapters; in text mode, it calls the underlying
granite-3.3-instruct model directly (without LoRA), essentially preserving all
the text LLM capabilities and safety. Both models are freely available on
HuggingFace (https://huggingface.co/ibm-granite/granite-speech-3.3-2b and
https://huggingface.co/ibm-granite/granite-speech-3.3-8b) and can be used for
both research and commercial purposes under a permissive Apache 2.0 license.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [391] [Sub-diffraction terahertz backpropagation compressive imaging](https://arxiv.org/pdf/2505.07839)
*Yongsheng Zhu, Shaojing Liu, Ximiao Wang, Runli Li, Haili Yang, Jiali Wang, Hongjia Zhu, Yanlin Ke, Ningsheng Xu, Huanjun Chen, Shaozhi Deng*

Main category: eess.IV

TL;DR: A sub-diffraction THz backpropagation compressive imaging technique is proposed, using an untrained neural network and angular spectrum propagation theory to achieve high-resolution imaging with reduced sampling time.


<details>
  <summary>Details</summary>
Motivation: Overcome the limitations of Terahertz single-pixel imaging (TSPI), such as low resolution and harsh experimental conditions, by developing a more efficient method.

Method: Illuminates the object with THz radiation, modulates it with prearranged patterns on a silicon wafer, records with a single detector, and reconstructs the image using an untrained neural network under physical constraints.

Result: Achieves sub-diffraction imaging with a resolution of ~λ₀/7 and reduces sampling time, eliminating the need for ultrathin photomodulators.

Conclusion: This technique offers an efficient solution for THz microscopic imaging and other inverse imaging challenges.

Abstract: Terahertz single-pixel imaging (TSPI) has garnered significant attention due
to its simplicity and cost-effectiveness. However, the relatively long
wavelength of THz waves limits sub-diffraction-scale imaging resolution.
Although TSPI technique can achieve sub-wavelength resolution, it requires
harsh experimental conditions and time-consuming processes. Here, we propose a
sub-diffraction THz backpropagation compressive imaging technique. We
illuminate the object with monochromatic continuous-wave THz radiation. The
transmitted THz wave is modulated by prearranged patterns generated on the back
surface of a 500-{\mu}m-thick silicon wafer, realized through photoexcited
carriers using a 532-nm laser. The modulated THz wave is then recorded by a
single-element detector. An untrained neural network is employed to iteratively
reconstruct the object image with an ultralow compression ratio of 1.5625%
under a physical model constraint, thus reducing the long sampling times. To
further suppress the diffraction-field effects, embedded with the angular
spectrum propagation (ASP) theory to model the diffraction of THz waves during
propagation, the network retrieves near-field information from the object,
enabling sub-diffraction imaging with a spatial resolution of ~{\lambda}0/7
({\lambda}0 = 833.3 {\mu}m at 0.36 THz) and eliminating the need for ultrathin
photomodulators. This approach provides an efficient solution for advancing THz
microscopic imaging and addressing other inverse imaging challenges.

</details>


### [392] [Evaluation of UAV-Based RGB and Multispectral Vegetation Indices for Precision Agriculture in Palm Tree Cultivation](https://arxiv.org/pdf/2505.07840)
*Alavikunhu Panthakkan, S M Anzar, K. Sherin, Saeed Al Mansoori, Hussain Al-Ahmad*

Main category: eess.IV

TL;DR: UAV-based RGB imaging is a cost-effective alternative to multispectral imaging for vegetation health monitoring in precision farming, offering comparable accuracy.


<details>
  <summary>Details</summary>
Motivation: To enhance crop productivity and sustainability in agriculture by evaluating cost-effective UAV-based imaging for vegetation health assessment.

Method: Comparison of multispectral (NDVI, SAVI) and RGB-based (VARI, MGRVI) vegetation indices using UAVs in a palm tree cultivation region in Dubai.

Result: RGB-based indices performed comparably to multispectral indices in vegetation classification and stress detection.

Conclusion: RGB imagery is a practical, cost-effective tool for precision farming, enabling scalable and efficient agricultural monitoring.

Abstract: Precision farming relies on accurate vegetation monitoring to enhance crop
productivity and promote sustainable agricultural practices. This study
presents a comprehensive evaluation of UAV-based imaging for vegetation health
assessment in a palm tree cultivation region in Dubai. By comparing
multispectral and RGB image data, we demonstrate that RGBbased vegetation
indices offer performance comparable to more expensive multispectral indices,
providing a cost-effective alternative for large-scale agricultural monitoring.
Using UAVs equipped with multispectral sensors, indices such as NDVI and SAVI
were computed to categorize vegetation into healthy, moderate, and stressed
conditions. Simultaneously, RGB-based indices like VARI and MGRVI delivered
similar results in vegetation classification and stress detection. Our findings
highlight the practical benefits of integrating RGB imagery into precision
farming, reducing operational costs while maintaining accuracy in plant health
monitoring. This research underscores the potential of UAVbased RGB imaging as
a powerful tool for precision agriculture, enabling broader adoption of
data-driven decision-making in crop management. By leveraging the strengths of
both multispectral and RGB imaging, this work advances the state of UAV
applications in agriculture, paving the way for more efficient and scalable
farming solutions.

</details>


### [393] [Pose Estimation for Intra-cardiac Echocardiography Catheter via AI-Based Anatomical Understanding](https://arxiv.org/pdf/2505.07851)
*Jaeyoung Huh, Ankur Kapoor, Young-Ho Kim*

Main category: eess.IV

TL;DR: A novel AI-driven system uses Vision Transformers to estimate ICE catheter pose from images, eliminating external tracking and improving accuracy in cardiac procedures.


<details>
  <summary>Details</summary>
Motivation: Existing ICE navigation methods rely on error-prone EM tracking or manual adjustments, prompting the need for a more reliable, sensor-free solution.

Method: A ViT-based model processes ICE images into embeddings, predicting catheter position and orientation via separate linear layers, trained on a dataset of 851 subjects.

Result: Achieves average positional error of 9.48 mm and orientation errors of (16.13°, 8.98°, 10.47°), validated by qualitative 3D mesh alignment.

Conclusion: The system enhances procedural efficiency, reduces workload, and offers a transformative, tracking-free approach for ICE-guided interventions.

Abstract: Intra-cardiac Echocardiography (ICE) plays a crucial role in
Electrophysiology (EP) and Structural Heart Disease (SHD) interventions by
providing high-resolution, real-time imaging of cardiac structures. However,
existing navigation methods rely on electromagnetic (EM) tracking, which is
susceptible to interference and position drift, or require manual adjustments
based on operator expertise. To overcome these limitations, we propose a novel
anatomy-aware pose estimation system that determines the ICE catheter position
and orientation solely from ICE images, eliminating the need for external
tracking sensors. Our approach leverages a Vision Transformer (ViT)-based deep
learning model, which captures spatial relationships between ICE images and
anatomical structures. The model is trained on a clinically acquired dataset of
851 subjects, including ICE images paired with position and orientation labels
normalized to the left atrium (LA) mesh. ICE images are patchified into 16x16
embeddings and processed through a transformer network, where a [CLS] token
independently predicts position and orientation via separate linear layers. The
model is optimized using a Mean Squared Error (MSE) loss function, balancing
positional and orientational accuracy. Experimental results demonstrate an
average positional error of 9.48 mm and orientation errors of (16.13 deg, 8.98
deg, 10.47 deg) across x, y, and z axes, confirming the model accuracy.
Qualitative assessments further validate alignment between predicted and target
views within 3D cardiac meshes. This AI-driven system enhances procedural
efficiency, reduces operator workload, and enables real-time ICE catheter
localization for tracking-free procedures. The proposed method can function
independently or complement existing mapping systems like CARTO, offering a
transformative approach to ICE-guided interventions.

</details>


### [394] [Computationally Efficient Diffusion Models in Medical Imaging: A Comprehensive Review](https://arxiv.org/pdf/2505.07866)
*Abdullah, Tao Huang, Ickjai Lee, Euijoon Ahn*

Main category: eess.IV

TL;DR: The paper reviews diffusion models (DDPM, LDM, WDM) in generative AI, focusing on efficiency and applications in medical imaging, while addressing computational challenges.


<details>
  <summary>Details</summary>
Motivation: To address the high computational costs of diffusion models and explore their potential in medical imaging for fast, reliable image generation.

Method: Categorizes diffusion models into DDPM, LDM, and WDM, analyzing their frameworks and computational efficiencies in natural and medical imaging.

Result: Highlights the role of these models in medical imaging but notes current computational limitations.

Conclusion: Identifies opportunities for future research to improve efficiency and application in medical imaging.

Abstract: The diffusion model has recently emerged as a potent approach in computer
vision, demonstrating remarkable performances in the field of generative
artificial intelligence. Capable of producing high-quality synthetic images,
diffusion models have been successfully applied across a range of applications.
However, a significant challenge remains with the high computational cost
associated with training and generating these models. This study focuses on the
efficiency and inference time of diffusion-based generative models,
highlighting their applications in both natural and medical imaging. We present
the most recent advances in diffusion models by categorizing them into three
key models: the Denoising Diffusion Probabilistic Model (DDPM), the Latent
Diffusion Model (LDM), and the Wavelet Diffusion Model (WDM). These models play
a crucial role in medical imaging, where producing fast, reliable, and
high-quality medical images is essential for accurate analysis of abnormalities
and disease diagnosis. We first investigate the general framework of DDPM, LDM,
and WDM and discuss the computational complexity gap filled by these models in
natural and medical imaging. We then discuss the current limitations of these
models as well as the opportunities and future research directions in medical
imaging.

</details>


### [395] [Highly Undersampled MRI Reconstruction via a Single Posterior Sampling of Diffusion Models](https://arxiv.org/pdf/2505.08142)
*Jin Liu, Qing Lin, Zhuang Xiong, Shanshan Shan, Chunyi Liu, Min Li, Feng Liu, G. Bruce Pike, Hongfu Sun, Yang Gao*

Main category: eess.IV

TL;DR: SSDM-MRI, a single-step diffusion model, accelerates MRI reconstruction with high performance and speed, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Address the degradation of MRI reconstruction performance at high acceleration factors and the slow inference time of diffusion models.

Method: Proposes SSDM-MRI, a one-step reconstruction framework using a trained conditional diffusion model and iterative distillation.

Result: SSDM-MRI excels in numerical metrics, qualitative details, and speed (0.45s per slice), surpassing other methods.

Conclusion: SSDM-MRI is a highly effective solution for fast and high-quality MRI reconstruction.

Abstract: Incoherent k-space under-sampling and deep learning-based reconstruction
methods have shown great success in accelerating MRI. However, the performance
of most previous methods will degrade dramatically under high acceleration
factors, e.g., 8$\times$ or higher. Recently, denoising diffusion models (DM)
have demonstrated promising results in solving this issue; however, one major
drawback of the DM methods is the long inference time due to a dramatic number
of iterative reverse posterior sampling steps. In this work, a Single Step
Diffusion Model-based reconstruction framework, namely SSDM-MRI, is proposed
for restoring MRI images from highly undersampled k-space. The proposed method
achieves one-step reconstruction by first training a conditional DM and then
iteratively distilling this model. Comprehensive experiments were conducted on
both publicly available fastMRI images and an in-house multi-echo GRE (QSM)
subject. Overall, the results showed that SSDM-MRI outperformed other methods
in terms of numerical metrics (PSNR and SSIM), qualitative error maps, image
fine details, and latent susceptibility information hidden in MRI phase images.
In addition, the reconstruction time for a 320*320 brain slice of SSDM-MRI is
only 0.45 second, which is only comparable to that of a simple U-net, making it
a highly effective solution for MRI reconstruction tasks.

</details>


### [396] [Fault Detection Method for Power Conversion Circuits Using Thermal Image and Convolutional Autoencoder](https://arxiv.org/pdf/2505.08150)
*Noboru Katayama, Rintaro Ishida*

Main category: eess.IV

TL;DR: A method using thermal images and a convolutional autoencoder detects faults in power circuits with 100% accuracy under tested conditions.


<details>
  <summary>Details</summary>
Motivation: To improve fault detection in power conversion circuits using thermal imaging and deep learning.

Method: Train a convolutional autoencoder on augmented thermal images of normal operation, then test it on images with simulated faults.

Result: The autoencoder achieved 100% accuracy in detecting anomalies under tested conditions.

Conclusion: The method is effective for fault detection, with hyperparameters like convolutional layers and augmentation impacting accuracy.

Abstract: A fault detection method for power conversion circuits using thermal images
and a convolutional autoencoder is presented. The autoencoder is trained on
thermal images captured from a commercial power module at randomly varied load
currents and augmented image2 generated through image processing techniques
such as resizing, rotation, perspective transformation, and bright and contrast
adjustment. Since the autoencoder is trained to output images identical to
input only for normal samples, it reconstructs images similar to normal ones
even when the input images containing faults. A small heater is attached to the
circuit board to simulate a fault on a power module, and then thermal images
were captured from different angles and positions, as well as various load
currents to test the trained autoencoder model. The areas under the curve (AUC)
were obtained to evaluate the proposed method. The results show the autoencoder
model can detect anomalies with 100% accuracy under given conditions. The
influence of hyperparameters such as the number of convolutional layers and
image augmentation conditions on anomaly detection accuracy was also
investigated.

</details>


### [397] [Skeleton-Guided Diffusion Model for Accurate Foot X-ray Synthesis in Hallux Valgus Diagnosis](https://arxiv.org/pdf/2505.08247)
*Midi Wan, Pengfei Li, Yizhuo Liang, Di Wu, Yushan Pan, Guangzhen Zhu, Hao Wang*

Main category: eess.IV

TL;DR: The paper introduces SCCDM, a skeletal-constrained diffusion model for medical image synthesis, improving image quality and clinical applicability for hallux valgus assessment.


<details>
  <summary>Details</summary>
Motivation: Hallux valgus affects 19% of the global population, requiring frequent X-rays. Existing models lack skeletal guidance, leading to poor image fidelity and consistency.

Method: Proposes SCCDM with multi-scale feature extraction and attention mechanisms, combined with KCC for skeletal landmark evaluation.

Result: Improves SSIM by 5.72% (0.794) and PSNR by 18.34% (21.40 dB), achieving a clinical score of 0.85 with KCC.

Conclusion: SCCDM enhances medical image synthesis for hallux valgus, offering better accuracy and clinical utility.

Abstract: Medical image synthesis plays a crucial role in providing anatomically
accurate images for diagnosis and treatment. Hallux valgus, which affects
approximately 19% of the global population, requires frequent weight-bearing
X-rays for assessment, placing additional strain on both patients and
healthcare providers. Existing X-ray models often struggle to balance image
fidelity, skeletal consistency, and physical constraints, particularly in
diffusion-based methods that lack skeletal guidance. We propose the
Skeletal-Constrained Conditional Diffusion Model (SCCDM) and introduce KCC, a
foot evaluation method utilizing skeletal landmarks. SCCDM incorporates
multi-scale feature extraction and attention mechanisms, improving the
Structural Similarity Index (SSIM) by 5.72% (0.794) and Peak Signal-to-Noise
Ratio (PSNR) by 18.34% (21.40 dB). When combined with KCC, the model achieves
an average score of 0.85, demonstrating strong clinical applicability. The code
is available at https://github.com/midisec/SCCDM.

</details>


### [398] [An integrated language-vision foundation model for conversational diagnostics and triaging in primary eye care](https://arxiv.org/pdf/2505.08414)
*Zhi Da Soh, Yang Bai, Kai Yu, Yang Zhou, Xiaofeng Lei, Sahil Thakur, Zann Lee, Lee Ching Linette Phang, Qingsheng Peng, Can Can Xue, Rachel Shujuan Chong, Quan V. Hoang, Lavanya Raghavan, Yih Chung Tham, Charumathi Sabanayagam, Wei-Chi Wu, Ming-Chih Ho, Jiangnan He, Preeti Gupta, Ecosse Lamoureux, Seang Mei Saw, Vinay Nangia, Songhomitra Panda-Jonas, Jie Xu, Ya Xing Wang, Xinxing Xu, Jost B. Jonas, Tien Yin Wong, Rick Siow Mong Goh, Yong Liu, Ching-Yu Cheng*

Main category: eess.IV

TL;DR: Meta-EyeFM is a multi-function foundation model combining LLMs and VFMs for ocular disease assessment, achieving high accuracy in routing and disease detection, outperforming other models and matching ophthalmologist performance.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models lack user-friendly interfaces and task-specific integration for ocular disease assessment.

Method: Meta-EyeFM integrates LLMs with VFMs using a routing mechanism and Low Rank Adaptation for fine-tuning to detect diseases, differentiate severity, and identify signs.

Result: Achieved 100% routing accuracy, ≥82.2% disease detection, ≥89% severity differentiation, ≥76% sign identification, outperforming Gemini-1.5-flash and ChatGPT-4o by 11-43%.

Conclusion: Meta-EyeFM enhances usability and diagnostic performance, serving as a valuable tool for primary eye care and fundus evaluation.

Abstract: Current deep learning models are mostly task specific and lack a
user-friendly interface to operate. We present Meta-EyeFM, a multi-function
foundation model that integrates a large language model (LLM) with vision
foundation models (VFMs) for ocular disease assessment. Meta-EyeFM leverages a
routing mechanism to enable accurate task-specific analysis based on text
queries. Using Low Rank Adaptation, we fine-tuned our VFMs to detect ocular and
systemic diseases, differentiate ocular disease severity, and identify common
ocular signs. The model achieved 100% accuracy in routing fundus images to
appropriate VFMs, which achieved $\ge$ 82.2% accuracy in disease detection,
$\ge$ 89% in severity differentiation, $\ge$ 76% in sign identification.
Meta-EyeFM was 11% to 43% more accurate than Gemini-1.5-flash and ChatGPT-4o
LMMs in detecting various eye diseases and comparable to an ophthalmologist.
This system offers enhanced usability and diagnostic performance, making it a
valuable decision support tool for primary eye care or an online LLM for fundus
evaluation.

</details>


### [399] [GNCAF: A GNN-based Neighboring Context Aggregation Framework for Tertiary Lymphoid Structures Semantic Segmentation in WSI](https://arxiv.org/pdf/2505.08430)
*Lei Su*

Main category: eess.IV

TL;DR: The paper introduces GNCAF, a GNN-based framework for TLS semantic segmentation in WSIs, improving performance by aggregating multi-hop neighboring context.


<details>
  <summary>Details</summary>
Motivation: Existing methods for TLS assessment rely on proxy tasks and lack broader context integration, prompting the need for an end-to-end solution.

Method: Proposes GNCAF, which aggregates multi-hop neighboring context using GNNs and self-attention to guide target patch segmentation.

Result: GNCAF achieves up to 22.08% and 26.57% improvement in mF1 and mIoU on TCGA-COAD and INHOUSE-PAAD datasets.

Conclusion: GNCAF enhances TLS segmentation by leveraging broader context and shows scalability for other tasks like lymph node metastasis segmentation.

Abstract: Tertiary lymphoid structures (TLS) are organized clusters of immune cells,
whose maturity and area can be quantified in whole slide image (WSI) for
various prognostic tasks. Existing methods for assessing these characteristics
typically rely on cell proxy tasks and require additional post-processing
steps. In this work, We focus on a novel task-TLS Semantic Segmentation
(TLS-SS)-which segments both the regions and maturation stages of TLS in WSI in
an end-to-end manner. Due to the extensive scale of WSI and patch-based
segmentation strategies, TLS-SS necessitates integrating from neighboring
patches to guide target patch (target) segmentation. Previous techniques often
employ on multi-resolution approaches, constraining the capacity to leverage
the broader neighboring context while tend to preserve coarse-grained
information. To address this, we propose a GNN-based Neighboring Context
Aggregation Framework (GNCAF), which progressively aggregates multi-hop
neighboring context from the target and employs a self-attention mechanism to
guide the segmentation of the target. GNCAF can be integrated with various
segmentation models to enhance their ability to perceive contextual information
outside of the patch. We build two TLS-SS datasets, called TCGA-COAD and
INHOUSE-PAAD, and make the former (comprising 225 WSIs and 5041 TLSs) publicly
available. Experiments on these datasets demonstrate the superiority of GNCAF,
achieving a maximum of 22.08% and 26.57% improvement in mF1 and mIoU,
respectively. Additionally, we also validate the task scalability of GNCAF on
segmentation of lymph node metastases.

</details>


### [400] [SAR-GTR: Attributed Scattering Information Guided SAR Graph Transformer Recognition Algorithm](https://arxiv.org/pdf/2505.08547)
*Xuying Xiong, Xinyu Zhang, Weidong Jiang, Li Liu, Yongxiang Liu, Tianpeng Liu*

Main category: eess.IV

TL;DR: The paper proposes SAR-GTR, a Graph Transformer Recognition Algorithm for SAR interpretation, combining GNNs and Transformer to address challenges like limited samples and poor generalization.


<details>
  <summary>Details</summary>
Motivation: To improve SAR data interpretation by integrating electromagnetic scattering information and addressing limitations of GNNs in SAR applications.

Method: Proposes SAR-GTR, distinguishing mapping methods for discrete/continuous parameters, combining GNNs with Transformer, and enhancing edge information for robust feature learning.

Result: Validated on the ATRNet-STAR dataset, demonstrating effectiveness in capturing target structural characteristics.

Conclusion: SAR-GTR successfully integrates domain knowledge and enhances SAR interpretation by leveraging hierarchical structural information.

Abstract: Utilizing electromagnetic scattering information for SAR data interpretation
is currently a prominent research focus in the SAR interpretation domain. Graph
Neural Networks (GNNs) can effectively integrate domain-specific physical
knowledge and human prior knowledge, thereby alleviating challenges such as
limited sample availability and poor generalization in SAR interpretation. In
this study, we thoroughly investigate the electromagnetic inverse scattering
information of single-channel SAR and re-examine the limitations of applying
GNNs to SAR interpretation. We propose the SAR Graph Transformer Recognition
Algorithm (SAR-GTR). SAR-GTR carefully considers the attributes and
characteristics of different electromagnetic scattering parameters by
distinguishing the mapping methods for discrete and continuous parameters,
thereby avoiding information confusion and loss. Furthermore, the GTR combines
GNNs with the Transformer mechanism and introduces an edge information
enhancement channel to facilitate interactive learning of node and edge
features, enabling the capture of robust and global structural characteristics
of targets. Additionally, the GTR constructs a hierarchical topology-aware
system through global node encoding and edge position encoding, fully
exploiting the hierarchical structural information of targets. Finally, the
effectiveness of the algorithm is validated using the ATRNet-STAR large-scale
vehicle dataset.

</details>


### [401] [Towards Digital Twin in Flood Forecasting with Data Assimilation Satellite Earth Observations -- A Proof-of-Concept in the Alzette Catchment](https://arxiv.org/pdf/2505.08553)
*Thanh Huy Nguyen, Sukriti Bhattacharya, Jefferson S. Wong, Yoanne Didry, Duc Long Phan, Thomas Tamisier, Patrick Matgen*

Main category: eess.IV

TL;DR: A Digital Twin framework integrates satellite data and data assimilation to improve flood forecasting in Luxembourg, showing enhanced accuracy despite some uncertainties.


<details>
  <summary>Details</summary>
Motivation: Floods are a major risk; timely forecasting is crucial for mitigation. This study aims to improve flood predictions using advanced data integration.

Method: Combines Sentinel-1 flood maps with particle filter-based data assimilation, using GloFAS products and a LISFLOOD-FP model for 30-day forecasts.

Result: The framework improves forecast accuracy over open-loop simulations, though uncertainties in GloFAS and Sentinel-1 data remain.

Conclusion: The Digital Twin shows promise for real-time flood forecasting, with future work targeting reduced uncertainties for better predictions.

Abstract: Floods pose significant risks to human lives, infrastructure, and the
environment. Timely and accurate flood forecasting plays a pivotal role in
mitigating these risks. This study presents a proof-of-concept for a Digital
Twin framework aimed at improving flood forecasting in the Alzette Catchment,
Luxembourg. The approach integrates satellite-based Earth observations,
specifically Sentinel-1 flood probability maps, into a particle filter-based
data assimilation (DA) process to enhance flood predictions. By combining the
GloFAS global flood monitoring and GloFAS streamflow forecasts products with DA
using a high-resolution LISFLOOD-FP hydrodynamic model, the Digital Twin can
provide daily flood forecasts for up to 30 days with reduced prediction
uncertainties. Using the 2021 flood event as a case study, we evaluate the
performance of the Digital Twin in assimilating EO data to refine hydraulic
model simulations and issue accurate forecasts. While some limitations, such as
uncertainties in GloFAS discharge forecasts, remain large, the approach
successfully improves forecast accuracy compared to open-loop simulations.
Future developments will focus on constructing more adaptively the hazard
catalog, and reducing inherent uncertainties related to GloFAS streamflow
forecasts and Sentinel-1 flood maps, to further enhance predictive capability.
The framework demonstrates potential for advancing real-time flood forecasting
and strengthening flood resilience.

</details>


### [402] [A portable diagnosis model for Keratoconus using a smartphone](https://arxiv.org/pdf/2505.08616)
*Yifan Li, Myeongjun Kim, Yanjing Jin, Peter Ho, Jo Woon Chong*

Main category: eess.IV

TL;DR: A smartphone-based diagnostic framework for Keratoconus (KC) uses Placido disc reflections and a two-stage detection pipeline, achieving high accuracy and intuitive visualization.


<details>
  <summary>Details</summary>
Motivation: To address the accessibility limitations of standard Placido disc-based topography by leveraging smartphones for KC diagnosis.

Method: A two-stage pipeline: 1) KC stage classification using WSVM on reflection features, 2) visualization of protrusion regions via color maps. Validated on 3D-printed eyeball models.

Result: Achieved 92.93% accuracy, consistent across smartphones. ANOVA confirmed significant differentiation between KC stages (p < 10^-6, ω² up to 0.8398).

Conclusion: The framework offers a portable, accurate, and intuitive alternative for KC diagnosis, validated across devices and stages.

Abstract: Keratoconus (KC) is a progressive corneal disorder characterized by localized
thinning and protrusion, leading to visual distortion. While Placido disc-based
topography remains a standard in clinical diagnostics, its dependence on
specialized equipment limits accessibility. In this paper, we propose a
portable, smartphone-based diagnostic framework that captures corneal
reflections of a Placido disc displayed on a phone screen and applies a
two-stage detection pipeline, then validate on 3D-printed emulated eyeball
models that simulate normal, moderate, and severe KC stages based on anterior
chamber depth (ACD). The first step of the two-stage detection pipeline is
classifying different stages of KC with features including height and width of
extracted reflections using weighted support vector machine (WSVM). It achieves
a maximum accuracy of 92.93%, and maintains over 90% accuracy across multiple
smartphone models, including the Galaxy Z Flip 3, iPhone 15 Pro, and iPhone 16
Pro. For the second step, we visualize the KC-affected protrusion regions on
the corneas with color maps based on inter-disc distance, that provides an
intuitive representation of disease severity and localization. Moreover, we
validate the ability of the extracted features to differentiate between KC
stages with ANOVA and Omega Squared, with significant p-values (e.g., $p <
10^{-6}$) and large effect sizes ($\\omega^2$ up to 0.8398) among classes.

</details>


### [403] [VIViT: Variable-Input Vision Transformer Framework for 3D MR Image Segmentation](https://arxiv.org/pdf/2505.08693)
*Badhan Kumar Das, Ajay Singh, Gengyan Zhao, Han Liu, Thomas J. Re, Dorin Comaniciu, Eli Gibson, Andreas Maier*

Main category: eess.IV

TL;DR: VIViT, a transformer-based framework, enables self-supervised pretraining and segmentation finetuning for variable MR contrasts, improving adaptability and performance on downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Address challenges in deep learning for MR studies with varying contrasts due to different acquisition protocols, which current methods struggle with due to fixed input requirements.

Method: Proposes VIViT, a transformer-based framework for self-supervised pretraining and segmentation finetuning, accommodating variable contrasts in each study.

Result: Outperforms CNN and ViT-based models with mean Dice scores of 0.624 (brain infarct) and 0.883 (brain tumor segmentation).

Conclusion: VIViT enhances adaptability and performance for heterogeneous MR data, maximizing data availability and knowledge transfer.

Abstract: Self-supervised pretrain techniques have been widely used to improve the
downstream tasks' performance. However, real-world magnetic resonance (MR)
studies usually consist of different sets of contrasts due to different
acquisition protocols, which poses challenges for the current deep learning
methods on large-scale pretrain and different downstream tasks with different
input requirements, since these methods typically require a fixed set of input
modalities or, contrasts. To address this challenge, we propose variable-input
ViT (VIViT), a transformer-based framework designed for self-supervised
pretraining and segmentation finetuning for variable contrasts in each study.
With this ability, our approach can maximize the data availability in pretrain,
and can transfer the learned knowledge from pretrain to downstream tasks
despite variations in input requirements. We validate our method on brain
infarct and brain tumor segmentation, where our method outperforms current CNN
and ViT-based models with a mean Dice score of 0.624 and 0.883 respectively.
These results highlight the efficacy of our design for better adaptability and
performance on tasks with real-world heterogeneous MR data.

</details>


### [404] [Using Few-Shot Learning to Classify Primary Lung Cancer and Other Malignancy with Lung Metastasis in Cytological Imaging via Endobronchial Ultrasound Procedures](https://arxiv.org/pdf/2404.06080)
*Ching-Kai Lin, Di-Chun Wei, Yun-Chien Cheng*

Main category: eess.IV

TL;DR: A few-shot learning model for lung metastasis detection in EBUS procedures, achieving 49.59% accuracy and improving to 55.48% with 20 samples.


<details>
  <summary>Details</summary>
Motivation: Early detection of lung metastases is challenging due to limited cytology images and cell similarities, with existing research rarely addressing this directly.

Method: Proposes a few-shot learning model with a hybrid pretrained backbone, fine-grained classification, and contrastive learning, using parameter-efficient fine-tuning on augmented support sets.

Result: Achieved 49.59% accuracy, improving to 55.48% with 20 samples, outperforming existing methods.

Conclusion: The model shows strong potential for identifying rare or novel cancer types in low-data clinical settings, enabling timely treatment.

Abstract: This study presents a computer-aided diagnosis (CAD) system to assist early
detection of lung metastases during endobronchial ultrasound (EBUS) procedures,
significantly reducing follow-up time and enabling timely treatment. Due to
limited cytology images and morphological similarities among cells, classifying
lung metastases is challenging, and existing research rarely targets this issue
directly.To overcome data scarcity and improve classification, the authors
propose a few-shot learning model using a hybrid pretrained backbone with
fine-grained classification and contrastive learning. Parameter-efficient
fine-tuning on augmented support sets enhances generalization and
transferability. The model achieved 49.59% accuracy, outperforming existing
methods. With 20 image samples, accuracy improved to 55.48%, showing strong
potential for identifying rare or novel cancer types in low-data clinical
environments.

</details>


### [405] [Ptychographic Image Reconstruction from Limited Data via Score-Based Diffusion Models with Physics-Guidance](https://arxiv.org/pdf/2502.18767)
*Refik Mert Cam, Junjing Deng, Rajkumar Kettimuthu, Mathew J. Cherukara, Tekin Bicer*

Main category: eess.IV

TL;DR: A physics-guided score-based diffusion model reduces data requirements for ptychography, achieving high-fidelity reconstructions with only 20% overlap compared to conventional methods needing 62%.


<details>
  <summary>Details</summary>
Motivation: Conventional ptychography methods require substantial overlap, leading to high data volume and long acquisition times. The goal is to reduce these requirements while maintaining accuracy.

Method: The proposed method uses a diffusion model trained on object images to learn a prior distribution. During reconstruction, data consistency is enforced to guide the reverse diffusion process toward physically plausible solutions.

Result: The method achieves high-fidelity reconstructions with only 20% overlap, significantly reducing data needs compared to the 62% overlap required by conventional techniques.

Conclusion: The physics-guided diffusion model offers an efficient alternative to traditional ptychography, reducing data and time demands while maintaining reconstruction quality.

Abstract: Ptychography is a data-intensive computational imaging technique that
achieves high spatial resolution over large fields of view. The technique
involves scanning a coherent beam across overlapping regions and recording
diffraction patterns. Conventional reconstruction algorithms require
substantial overlap, increasing data volume and experimental time, reaching
PiB-scale experimental data and weeks to month-long data acquisition times. To
address this, we propose a reconstruction method employing a physics-guided
score-based diffusion model. Our approach trains a diffusion model on
representative object images to learn an object distribution prior. During
reconstruction, we modify the reverse diffusion process to enforce data
consistency, guiding reverse diffusion toward a physically plausible solution.
This method requires a single pretraining phase, allowing it to generalize
across varying scan overlap ratios and positions. Our results demonstrate that
the proposed method achieves high-fidelity reconstructions with only a 20%
overlap, while the widely employed rPIE method requires a 62% overlap to
achieve similar accuracy. This represents a significant reduction in data
requirements, offering an alternative to conventional techniques.

</details>


### [406] [An Analysis of Data Transformation Effects on Segment Anything 2](https://arxiv.org/pdf/2503.00042)
*Clayton Bromley, Alexander Moore, Amar Saini, Doug Poland, Carmen Carrano*

Main category: eess.IV

TL;DR: SAM 2's architecture excels in video object segmentation by filtering noise and emphasizing objects, analyzed through complex transformations.


<details>
  <summary>Details</summary>
Motivation: To understand how SAM 2 achieves high-quality VOS results and improve real-world applicability.

Method: Analyzing SAM 2's stages using complex video transformations and measuring their impact.

Result: Each stage filters noise and highlights objects, demonstrated via new datasets and visualizations.

Conclusion: Understanding SAM 2's stages aids in enhancing VOS for cluttered and obscured scenes.

Abstract: Video object segmentation (VOS) is a critical task in the development of
video perception and understanding. The Segment-Anything Model 2 (SAM 2),
released by Meta AI, is the current state-of-the-art architecture for
end-to-end VOS. SAM 2 performs very well on both clean video data and augmented
data, and completely intelligent video perception requires an understanding of
how this architecture is capable of achieving such quality results. To better
understand how each step within the SAM 2 architecture permits high-quality
video segmentation, a variety of complex video transformations are passed
through the architecture, and the impact at each stage of the process is
measured. It is observed that each progressive stage enables the filtering of
complex transformation noise and the emphasis of the object of interest.
Contributions include the creation of complex transformation video datasets, an
analysis of how each stage of the SAM 2 architecture interprets these
transformations, and visualizations of segmented objects through each stage. By
better understanding how each model structure impacts overall video
understanding, VOS development can work to improve real-world applicability and
performance tracking, localizing, and segmenting objects despite complex
cluttered scenes and obscurations.

</details>


### [407] [GBT-SAM: Adapting a Foundational Deep Learning Model for Generalizable Brain Tumor Segmentation via Efficient Integration of Multi-Parametric MRI Data](https://arxiv.org/pdf/2503.04325)
*Cecilia Diana-Albelda, Roberto Alcover-Couso, Álvaro García-Martín, Jesus Bescos, Marcos Escudero-Viñolo*

Main category: eess.IV

TL;DR: GBT-SAM is a parameter-efficient deep learning framework for glioma segmentation in mp-MRI, achieving high accuracy with minimal computational resources.


<details>
  <summary>Details</summary>
Motivation: Manual glioma segmentation is time-consuming and inconsistent, prompting the need for automated, efficient deep learning solutions.

Method: GBT-SAM adapts the Segment Anything Model (SAM) for volumetric mp-MRI, using a two-step fine-tuning strategy with depth-aware modules and lightweight adaptation layers.

Result: Achieves a Dice Score of 93.54 on BraTS Adult Glioma and robust performance on other datasets with just 6.5M trainable parameters.

Conclusion: GBT-SAM is a computationally efficient and domain-robust solution for brain tumor segmentation.

Abstract: Gliomas are aggressive brain tumors that require accurate imaging-based
diagnosis, with segmentation playing a critical role in evaluating morphology
and treatment decisions. Manual delineation of gliomas is time-consuming and
prone to variability, motivating the use of deep learning to improve
consistency and alleviate clinical workload. However, existing methods often
fail to fully exploit the information available in multi-parametric MRI
(mp-MRI), particularly inter-slice contextual features, and typically require
considerable computational resources while lacking robustness across tumor type
variations. We present GBT-SAM, a parameter-efficient deep learning framework
that adapts the Segment Anything Model (SAM), a large-scale vision model, to
volumetric mp-MRI data. GBT-SAM reduces input complexity by selecting fewer
than 2.6\% of slices per scan while incorporating all four MRI modalities,
preserving essential tumor-related information with minimal cost. Furthermore,
our model is trained by a two-step fine-tuning strategy that incorporates a
depth-aware module to capture inter-slice correlations and lightweight
adaptation layers, resulting in just 6.5M trainable parameters, which is the
lowest among SAM-based approaches. GBT-SAM achieves a Dice Score of 93.54 on
the BraTS Adult Glioma dataset and demonstrates robust performance on
Meningioma, Pediatric Glioma, and Sub-Saharan Glioma datasets. These results
highlight GBT-SAM's potential as a computationally efficient and domain-robust
framework for brain tumor segmentation using mp-MRI. Our code and models are
available at https://github.com/vpulab/med-sam-brain .

</details>


### [408] [Automatic quality control in multi-centric fetal brain MRI super-resolution reconstruction](https://arxiv.org/pdf/2503.10156)
*Thomas Sanchez, Vladyslav Zalevskyi, Angeline Mihailov, Gerard Martí-Juan, Elisenda Eixarch, Andras Jakab, Vincent Dunet, Mériam Koob, Guillaume Auzias, Meritxell Bach Cuadra*

Main category: eess.IV

TL;DR: The paper introduces FetMRQC$_{SR}$, a machine-learning method for automated quality control of super-resolution reconstruction (SRR) volumes in fetal brain MRI, achieving high performance (ROC AUC = 0.89) even in out-of-domain settings.


<details>
  <summary>Details</summary>
Motivation: Quality control is critical for fetal brain MRI due to less standardized acquisitions and processing compared to adult imaging. Automated QC is needed to ensure reliable SRR volumes.

Method: FetMRQC$_{SR}$ uses a random forest model to predict image quality scores by extracting over 100 image quality metrics, addressing high-dimensional, heterogeneous data with small datasets.

Result: The method achieves high performance (ROC AUC = 0.89) in out-of-domain validation, though 45% of failure cases arise from ambiguous expert ratings.

Conclusion: FetMRQC$_{SR}$ is effective for automated QC in fetal brain MRI, demonstrating suitability for multifaceted problems without deep learning. The tool and code are publicly available.

Abstract: Quality control (QC) has long been considered essential to guarantee the
reliability of neuroimaging studies. It is particularly important for fetal
brain MRI, where acquisitions and image processing techniques are less
standardized than in adult imaging. In this work, we focus on automated quality
control of super-resolution reconstruction (SRR) volumes of fetal brain MRI, an
important processing step where multiple stacks of thick 2D slices are
registered together and combined to build a single, isotropic and artifact-free
T2 weighted volume. We propose FetMRQC$_{SR}$, a machine-learning method that
extracts more than 100 image quality metrics to predict image quality scores
using a random forest model. This approach is well suited to a problem that is
high dimensional, with highly heterogeneous data and small datasets. We
validate FetMRQC$_{SR}$ in an out-of-domain (OOD) setting and report high
performance (ROC AUC = 0.89), even when faced with data from an unknown site or
SRR method. We also investigate failure cases and show that they occur in
$45\%$ of the images due to ambiguous configurations for which the rating from
the expert is arguable. These results are encouraging and illustrate how a non
deep learning-based method like FetMRQC$_{SR}$ is well suited to this
multifaceted problem. Our tool, along with all the code used to generate, train
and evaluate the model are available at
https://github.com/Medical-Image-Analysis-Laboratory/fetmrqc_sr/ .

</details>


### [409] [Deep Representation Learning for Unsupervised Clustering of Myocardial Fiber Trajectories in Cardiac Diffusion Tensor Imaging](https://arxiv.org/pdf/2504.01953)
*Mohini Anand, Xavier Tricoche*

Main category: eess.IV

TL;DR: A novel deep learning framework for unsupervised clustering of myocardial fibers from DTI data, combining BiLSTM and Transformer autoencoder for local and global feature learning, achieving detailed fiber delineation.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to accurately capture myocardial fiber structure due to lack of ground truth and complex fiber trajectories.

Method: Combines BiLSTM for local sequential information and Transformer autoencoder for global shape features, with anatomical context, followed by density-based clustering.

Result: Identifies 33 to 62 robust clusters, capturing subtle fiber trajectory distinctions with varying granularity.

Conclusion: Provides a flexible, quantitative approach for myocardial analysis, advancing personalized cardiac care and surgical planning.

Abstract: Understanding the complex myocardial architecture is critical for diagnosing
and treating heart disease. However, existing methods often struggle to
accurately capture this intricate structure from Diffusion Tensor Imaging (DTI)
data, particularly due to the lack of ground truth labels and the ambiguous,
intertwined nature of fiber trajectories. We present a novel deep learning
framework for unsupervised clustering of myocardial fibers, providing a
data-driven approach to identifying distinct fiber bundles. We uniquely combine
a Bidirectional Long Short-Term Memory network to capture local sequential
information along fibers, with a Transformer autoencoder to learn global shape
features, with pointwise incorporation of essential anatomical context.
Clustering these representations using a density-based algorithm identifies 33
to 62 robust clusters, successfully capturing the subtle distinctions in fiber
trajectories with varying levels of granularity. Our framework offers a new,
flexible, and quantitative way to analyze myocardial structure, achieving a
level of delineation that, to our knowledge, has not been previously achieved,
with potential applications in improving surgical planning, characterizing
disease-related remodeling, and ultimately, advancing personalized cardiac
care.

</details>


### [410] [TVC: Tokenized Video Compression with Ultra-Low Bitrate](https://arxiv.org/pdf/2504.16953)
*Lebin Zhou, Cihan Ruan, Nam Ling, Wei Wang, Wei Jiang*

Main category: eess.IV

TL;DR: Tokenized Video Compression (TVC) introduces a dual-stream framework for ultra-low bitrate video compression, combining discrete and continuous token streams for efficient and high-quality reconstruction.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the underexplored challenge of extending tokenized visual representations to video compression, especially under complex temporal dynamics and strict bitrate constraints.

Method: TVC uses the Cosmos video tokenizer to extract discrete and continuous token streams. Discrete tokens are masked and compressed losslessly, while continuous tokens are quantized and compressed. Both streams are fused at the decoder using ControlNet.

Result: TVC effectively operates at ultra-low bitrates, achieving high perceptual quality and strong fidelity in reconstruction.

Conclusion: This work validates the practicality of tokenized video compression and paves the way for semantics-aware, token-native video compression.

Abstract: Tokenized visual representations have shown great promise in image
compression, yet their extension to video remains underexplored due to the
challenges posed by complex temporal dynamics and stringent bitrate
constraints. In this paper, we propose Tokenized Video Compression (TVC), the
first token-based dual-stream video compression framework designed to operate
effectively at ultra-low bitrates. TVC leverages the powerful Cosmos video
tokenizer to extract both discrete and continuous token streams. The discrete
tokens (i.e., code maps generated by FSQ) are partially masked using a
strategic masking scheme, then compressed losslessly with a discrete
checkerboard context model to reduce transmission overhead. The masked tokens
are reconstructed by a decoder-only transformer with spatiotemporal token
prediction. Meanwhile, the continuous tokens, produced via an autoencoder (AE),
are quantized and compressed using a continuous checkerboard context model,
providing complementary continuous information at ultra-low bitrate. At the
Decoder side, both streams are fused using ControlNet, with multi-scale
hierarchical integration to ensure high perceptual quality alongside strong
fidelity in reconstruction. This work mitigates the long-standing skepticism
about the practicality of tokenized video compression and opens up new avenues
for semantics-aware, token-native video compression.

</details>


### [411] [Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video Generation Model](https://arxiv.org/pdf/2505.07449)
*Wei Li, Ming Hu, Guoan Wang, Lihao Liu, Kaijin Zhou, Junzhi Ning, Xin Guo, Zongyuan Ge, Lixu Gu, Junjun He*

Main category: eess.IV

TL;DR: Ophora is an AI model generating ophthalmic surgical videos from natural language instructions, addressing data scarcity and privacy issues.


<details>
  <summary>Details</summary>
Motivation: High-quality annotated ophthalmic surgical videos are hard to collect due to privacy and labor constraints, necessitating an alternative solution.

Method: Ophora uses a Comprehensive Data Curation pipeline to create a dataset (Ophora-160K) and a Progressive Video-Instruction Tuning scheme to adapt a T2V model for surgical video generation.

Result: Ophora generates realistic and reliable surgical videos, validated by quantitative analysis and ophthalmologist feedback, and aids downstream workflow understanding.

Conclusion: Ophora effectively addresses data scarcity in ophthalmic surgery by generating high-quality surgical videos from text instructions, with potential for broader applications.

Abstract: In ophthalmic surgery, developing an AI system capable of interpreting
surgical videos and predicting subsequent operations requires numerous
ophthalmic surgical videos with high-quality annotations, which are difficult
to collect due to privacy concerns and labor consumption. Text-guided video
generation (T2V) emerges as a promising solution to overcome this issue by
generating ophthalmic surgical videos based on surgeon instructions. In this
paper, we present Ophora, a pioneering model that can generate ophthalmic
surgical videos following natural language instructions. To construct Ophora,
we first propose a Comprehensive Data Curation pipeline to convert narrative
ophthalmic surgical videos into a large-scale, high-quality dataset comprising
over 160K video-instruction pairs, Ophora-160K. Then, we propose a Progressive
Video-Instruction Tuning scheme to transfer rich spatial-temporal knowledge
from a T2V model pre-trained on natural video-text datasets for
privacy-preserved ophthalmic surgical video generation based on Ophora-160K.
Experiments on video quality evaluation via quantitative analysis and
ophthalmologist feedback demonstrate that Ophora can generate realistic and
reliable ophthalmic surgical videos based on surgeon instructions. We also
validate the capability of Ophora for empowering downstream tasks of ophthalmic
surgical workflow understanding. Code is available at
https://github.com/mar-cry/Ophora.

</details>
