<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 75]
- [cs.CV](#cs.CV) [Total: 128]
- [cs.AI](#cs.AI) [Total: 38]
- [cs.SD](#cs.SD) [Total: 8]
- [cs.LG](#cs.LG) [Total: 110]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.MM](#cs.MM) [Total: 3]
- [eess.AS](#eess.AS) [Total: 6]
- [eess.IV](#eess.IV) [Total: 25]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [How Social is It? A Benchmark for LLMs' Capabilities in Multi-user Multi-turn Social Agent Tasks](https://arxiv.org/pdf/2505.04628)
*Yusen Wu, Junwu Xiong, Xiaotie Deng*

Main category: cs.CL

TL;DR: The paper introduces a benchmark (HSII) to measure LLMs' social capabilities in multi-user, multi-turn tasks, proposing a framework and dataset (HSII-Dataset) for evaluation, and explores the impact of the chain of thought (COT) method.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks lack systematic measurement of LLMs' social capabilities in complex, multi-user settings, necessitating a dedicated framework and benchmark.

Method: Proposes the HSII benchmark with four stages (format parsing, target selection, target switching conversation, stable conversation) and HSII-Dataset derived from news data. Investigates COT's impact and introduces COT-complexity for efficiency.

Result: Demonstrates HSII's suitability for evaluating LLMs' social skills, with COT enhancing performance but requiring efficiency trade-offs.

Conclusion: HSII effectively benchmarks LLMs' social capabilities, balancing correctness and efficiency, and highlights COT's role in performance enhancement.

Abstract: Expanding the application of large language models (LLMs) to societal life,
instead of primary function only as auxiliary assistants to communicate with
only one person at a time, necessitates LLMs' capabilities to independently
play roles in multi-user, multi-turn social agent tasks within complex social
settings. However, currently the capability has not been systematically
measured with available benchmarks. To address this gap, we first introduce an
agent task leveling framework grounded in sociological principles.
Concurrently, we propose a novel benchmark, How Social Is It (we call it HSII
below), designed to assess LLM's social capabilities in comprehensive social
agents tasks and benchmark representative models. HSII comprises four stages:
format parsing, target selection, target switching conversation, and stable
conversation, which collectively evaluate the communication and task completion
capabilities of LLMs within realistic social interaction scenarios dataset,
HSII-Dataset. The dataset is derived step by step from news dataset. We perform
an ablation study by doing clustering to the dataset. Additionally, we
investigate the impact of chain of thought (COT) method on enhancing LLMs'
social performance. Since COT cost more computation, we further introduce a new
statistical metric, COT-complexity, to quantify the efficiency of certain LLMs
with COTs for specific social tasks and strike a better trade-off between
measurement of correctness and efficiency. Various results of our experiments
demonstrate that our benchmark is well-suited for evaluating social skills in
LLMs.

</details>


### [2] [Adaptive Token Boundaries: Integrating Human Chunking Mechanisms into Multimodal LLMs](https://arxiv.org/pdf/2505.04637)
*Dongxing Yu*

Main category: cs.CL

TL;DR: The paper explores the gap between human cognition and MLLMs in multimodal processing, proposing a dynamic tokenization framework inspired by human cognitive mechanisms, which outperforms current models.


<details>
  <summary>Details</summary>
Motivation: To bridge the disparity between human cognitive processes and computational approaches in multimodal information integration.

Method: Systematic investigation of human cross-modal chunking vs. MLLM token representation, proposing a dynamic tokenization framework with adaptive boundaries and hierarchical representations.

Result: Significant improvements over state-of-the-art models (+7.8% on VQA, +5.3% on Complex Scene Description) and more human-aligned error patterns.

Conclusion: The study advances theoretical understanding of human-AI cognition links and offers a framework for more cognitively plausible AI systems.

Abstract: Recent advancements in multimodal large language models (MLLMs) have
demonstrated remarkable capabilities in processing diverse data types, yet
significant disparities persist between human cognitive processes and
computational approaches to multimodal information integration. This research
presents a systematic investigation into the parallels between human
cross-modal chunking mechanisms and token representation methodologies in
MLLMs. Through empirical studies comparing human performance patterns with
model behaviors across visual-linguistic tasks, we demonstrate that
conventional static tokenization schemes fundamentally constrain current
models' capacity to simulate the dynamic, context-sensitive nature of human
information processing. We propose a novel framework for dynamic cross-modal
tokenization that incorporates adaptive boundaries, hierarchical
representations, and alignment mechanisms grounded in cognitive science
principles. Quantitative evaluations demonstrate that our approach yields
statistically significant improvements over state-of-the-art models on
benchmark tasks (+7.8% on Visual Question Answering, +5.3% on Complex Scene
Description) while exhibiting more human-aligned error patterns and attention
distributions. These findings contribute to the theoretical understanding of
the relationship between human cognition and artificial intelligence, while
providing empirical evidence for developing more cognitively plausible AI
systems.

</details>


### [3] [Language translation, and change of accent for speech-to-speech task using diffusion model](https://arxiv.org/pdf/2505.04639)
*Abhishek Mishra, Ritesh Sur Chowdhury, Vartul Bahuguna, Isha Pandey, Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: A unified approach for simultaneous speech translation and accent adaptation using diffusion models, optimizing both tasks jointly for efficiency and effectiveness.


<details>
  <summary>Details</summary>
Motivation: Effective cross-cultural communication requires handling both language translation and accent adaptation simultaneously, a task underexplored in current literature.

Method: Reformulates the problem as a conditional generation task using diffusion models, generating target speech based on phonemes and guided by target speech features.

Result: The proposed framework enables joint optimization of translation and accent adaptation, offering a more parameter-efficient and effective model.

Conclusion: The method provides a unified, efficient solution for simultaneous speech translation and accent adaptation, advancing cross-cultural communication.

Abstract: Speech-to-speech translation (S2ST) aims to convert spoken input in one
language to spoken output in another, typically focusing on either language
translation or accent adaptation. However, effective cross-cultural
communication requires handling both aspects simultaneously - translating
content while adapting the speaker's accent to match the target language
context. In this work, we propose a unified approach for simultaneous speech
translation and change of accent, a task that remains underexplored in current
literature. Our method reformulates the problem as a conditional generation
task, where target speech is generated based on phonemes and guided by target
speech features. Leveraging the power of diffusion models, known for
high-fidelity generative capabilities, we adapt text-to-image diffusion
strategies by conditioning on source speech transcriptions and generating Mel
spectrograms representing the target speech with desired linguistic and
accentual attributes. This integrated framework enables joint optimization of
translation and accent adaptation, offering a more parameter-efficient and
effective model compared to traditional pipelines.

</details>


### [4] [A Comparative Benchmark of a Moroccan Darija Toxicity Detection Model (Typica.ai) and Major LLM-Based Moderation APIs (OpenAI, Mistral, Anthropic)](https://arxiv.org/pdf/2505.04640)
*Hicham Assoudi*

Main category: cs.CL

TL;DR: Typica.ai's Moroccan Darija toxicity detection model outperforms major LLM-based moderation APIs in detecting culturally grounded toxic content.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of culturally adapted toxicity detection models compared to general-purpose LLM-based moderation APIs.

Method: Comparative benchmark using precision, recall, F1-score, and accuracy on a balanced test set from OMCD_Typica.ai_Mix dataset.

Result: Typica.ai's model shows superior performance in detecting implicit insults, sarcasm, and culturally specific aggression.

Conclusion: Culturally adapted models are crucial for reliable content moderation in underrepresented languages.

Abstract: This paper presents a comparative benchmark evaluating the performance of
Typica.ai's custom Moroccan Darija toxicity detection model against major
LLM-based moderation APIs: OpenAI (omni-moderation-latest), Mistral
(mistral-moderation-latest), and Anthropic Claude (claude-3-haiku-20240307). We
focus on culturally grounded toxic content, including implicit insults,
sarcasm, and culturally specific aggression often overlooked by general-purpose
systems. Using a balanced test set derived from the OMCD_Typica.ai_Mix dataset,
we report precision, recall, F1-score, and accuracy, offering insights into
challenges and opportunities for moderation in underrepresented languages. Our
results highlight Typica.ai's superior performance, underlining the importance
of culturally adapted models for reliable content moderation.

</details>


### [5] [Rethinking Multimodal Sentiment Analysis: A High-Accuracy, Simplified Fusion Architecture](https://arxiv.org/pdf/2505.04642)
*Nischal Mandal, Yang Li*

Main category: cs.CL

TL;DR: A lightweight fusion-based deep learning model for multimodal sentiment analysis achieves 92% accuracy on the IEMOCAP dataset, outperforming complex models with simpler feature engineering.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient emotion classification by integrating language, audio, and visual cues without computational overhead.

Method: Modality-specific encoders with dropout regularization, followed by simple concatenation and a dense fusion layer for cross-modal interactions.

Result: Achieved 92% classification accuracy across six emotion categories on the IEMOCAP dataset.

Conclusion: Simpler fusion strategies with careful feature engineering can match or outperform complex models, especially in resource-constrained settings.

Abstract: Multimodal sentiment analysis, a pivotal task in affective computing, seeks
to understand human emotions by integrating cues from language, audio, and
visual signals. While many recent approaches leverage complex attention
mechanisms and hierarchical architectures, we propose a lightweight, yet
effective fusion-based deep learning model tailored for utterance-level emotion
classification. Using the benchmark IEMOCAP dataset, which includes aligned
text, audio-derived numeric features, and visual descriptors, we design a
modality-specific encoder using fully connected layers followed by dropout
regularization. The modality-specific representations are then fused using
simple concatenation and passed through a dense fusion layer to capture
cross-modal interactions. This streamlined architecture avoids computational
overhead while preserving performance, achieving a classification accuracy of
92% across six emotion categories. Our approach demonstrates that with careful
feature engineering and modular design, simpler fusion strategies can
outperform or match more complex models, particularly in resource-constrained
environments.

</details>


### [6] [Prediction-powered estimators for finite population statistics in highly imbalanced textual data: Public hate crime estimation](https://arxiv.org/pdf/2505.04643)
*Hannes Waldetoft, Jakob Torgander, Måns Magnusson*

Main category: cs.CL

TL;DR: Combining transformer encoder predictions with survey sampling estimators to efficiently estimate population parameters in text documents, demonstrated with Swedish hate crime statistics.


<details>
  <summary>Details</summary>
Motivation: Manual annotation for labeling target variables in text documents is time-consuming; this method aims to reduce the need for extensive manual work.

Method: Uses a transformer encoder neural network for predictions, combined with survey sampling estimators (Hansen-Hurwitz, difference estimation, stratified random sampling) as auxiliary variables.

Result: Demonstrated effectiveness in estimating yearly hate crimes and under-reporting in Swedish police reports, showing efficiency with labeled training data.

Conclusion: The method provides efficient estimates with reduced manual annotation effort when labeled training data is available.

Abstract: Estimating population parameters in finite populations of text documents can
be challenging when obtaining the labels for the target variable requires
manual annotation. To address this problem, we combine predictions from a
transformer encoder neural network with well-established survey sampling
estimators using the model predictions as an auxiliary variable. The
applicability is demonstrated in Swedish hate crime statistics based on Swedish
police reports. Estimates of the yearly number of hate crimes and the police's
under-reporting are derived using the Hansen-Hurwitz estimator, difference
estimation, and stratified random sampling estimation. We conclude that if
labeled training data is available, the proposed method can provide very
efficient estimates with reduced time spent on manual annotation.

</details>


### [7] [ChatGPT for automated grading of short answer questions in mechanical ventilation](https://arxiv.org/pdf/2505.04645)
*Tejas Jade, Alex Yartsev*

Main category: cs.CL

TL;DR: ChatGPT 4o was evaluated for grading postgraduate medical SAQs but showed poor agreement with human graders, cautioning against its use for high-stakes assessments.


<details>
  <summary>Details</summary>
Motivation: To assess the feasibility of using LLMs like ChatGPT for automated grading of SAQs in postgraduate medical education.

Method: Used ChatGPT 4o to grade 557 SAQs from 215 students, comparing results with human graders using statistical measures like ICC, Cohen's kappa, and Bland-Altman.

Result: ChatGPT awarded lower marks than humans (mean bias -1.34/10), with poor agreement (ICC1=0.086, kappa=-0.0786). Internal consistency was high but diverged from human grading.

Conclusion: LLMs like ChatGPT are not reliable for high-stakes SAQ grading due to significant disagreement with human graders.

Abstract: Standardised tests using short answer questions (SAQs) are common in
postgraduate education. Large language models (LLMs) simulate conversational
language and interpret unstructured free-text responses in ways aligning with
applying SAQ grading rubrics, making them attractive for automated grading. We
evaluated ChatGPT 4o to grade SAQs in a postgraduate medical setting using data
from 215 students (557 short-answer responses) enrolled in an online course on
mechanical ventilation (2020--2024). Deidentified responses to three case-based
scenarios were presented to ChatGPT with a standardised grading prompt and
rubric. Outputs were analysed using mixed-effects modelling, variance component
analysis, intraclass correlation coefficients (ICCs), Cohen's kappa, Kendall's
W, and Bland--Altman statistics. ChatGPT awarded systematically lower marks
than human graders with a mean difference (bias) of -1.34 on a 10-point scale.
ICC values indicated poor individual-level agreement (ICC1 = 0.086), and
Cohen's kappa (-0.0786) suggested no meaningful agreement. Variance component
analysis showed minimal variability among the five ChatGPT sessions (G-value =
0.87), indicating internal consistency but divergence from the human grader.
The poorest agreement was observed for evaluative and analytic items, whereas
checklist and prescriptive rubric items had less disagreement. We caution
against the use of LLMs in grading postgraduate coursework. Over 60% of
ChatGPT-assigned grades differed from human grades by more than acceptable
boundaries for high-stakes assessments.

</details>


### [8] [FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research Insights](https://arxiv.org/pdf/2505.04649)
*Chengzhang Yu, Yiming Zhang, Zhixin Liu, Zenghui Ding, Yining Sun, Zhanpeng Jin*

Main category: cs.CL

TL;DR: FRAME is a framework using iterative refinement and structured feedback to enhance automated medical paper generation, outperforming conventional methods and matching human quality.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in knowledge synthesis and quality assurance for automated scientific research using LLMs.

Method: Structured dataset construction, tripartite agent architecture (Generator, Evaluator, Reflector), and a comprehensive evaluation framework.

Result: Significant improvements over conventional methods (9.91% average gain) and human-like quality in generated papers.

Conclusion: FRAME effectively supports automated medical research paper generation while upholding academic standards.

Abstract: The automation of scientific research through large language models (LLMs)
presents significant opportunities but faces critical challenges in knowledge
synthesis and quality assurance. We introduce Feedback-Refined Agent
Methodology (FRAME), a novel framework that enhances medical paper generation
through iterative refinement and structured feedback. Our approach comprises
three key innovations: (1) A structured dataset construction method that
decomposes 4,287 medical papers into essential research components through
iterative refinement; (2) A tripartite architecture integrating Generator,
Evaluator, and Reflector agents that progressively improve content quality
through metric-driven feedback; and (3) A comprehensive evaluation framework
that combines statistical metrics with human-grounded benchmarks. Experimental
results demonstrate FRAME's effectiveness, achieving significant improvements
over conventional approaches across multiple models (9.91% average gain with
DeepSeek V3, comparable improvements with GPT-4o Mini) and evaluation
dimensions. Human evaluation confirms that FRAME-generated papers achieve
quality comparable to human-authored works, with particular strength in
synthesizing future research directions. The results demonstrated our work
could efficiently assist medical research by building a robust foundation for
automated medical research paper generation while maintaining rigorous academic
standards.

</details>


### [9] [Scientific Hypothesis Generation and Validation: Methods, Datasets, and Future Directions](https://arxiv.org/pdf/2505.04651)
*Adithya Kulkarni, Fatimah Alotaibi, Xinyue Zeng, Longfeng Wu, Tong Zeng, Barry Menglong Yao, Minqian Liu, Shuaicheng Zhang, Lifu Huang, Dawei Zhou*

Main category: cs.CL

TL;DR: The survey explores how LLMs enhance scientific discovery through synthesis, relationship discovery, and reasoning, comparing traditional and modern methods, and outlines future directions.


<details>
  <summary>Details</summary>
Motivation: To provide a structured overview of LLM-driven approaches in scientific hypothesis generation and validation, highlighting advancements and trade-offs.

Method: Reviews symbolic frameworks, generative models, hybrid systems, and multi-agent architectures, along with techniques like retrieval-augmented generation and causal inference.

Result: Identifies trade-offs in interpretability, novelty, and domain alignment, and maps datasets across various scientific fields.

Conclusion: Proposes a roadmap for future LLM applications, emphasizing novelty, multimodal integration, human collaboration, and ethical safeguards.

Abstract: Large Language Models (LLMs) are transforming scientific hypothesis
generation and validation by enabling information synthesis, latent
relationship discovery, and reasoning augmentation. This survey provides a
structured overview of LLM-driven approaches, including symbolic frameworks,
generative models, hybrid systems, and multi-agent architectures. We examine
techniques such as retrieval-augmented generation, knowledge-graph completion,
simulation, causal inference, and tool-assisted reasoning, highlighting
trade-offs in interpretability, novelty, and domain alignment. We contrast
early symbolic discovery systems (e.g., BACON, KEKADA) with modern LLM
pipelines that leverage in-context learning and domain adaptation via
fine-tuning, retrieval, and symbolic grounding. For validation, we review
simulation, human-AI collaboration, causal modeling, and uncertainty
quantification, emphasizing iterative assessment in open-world contexts. The
survey maps datasets across biomedicine, materials science, environmental
science, and social science, introducing new resources like AHTech and
CSKG-600. Finally, we outline a roadmap emphasizing novelty-aware generation,
multimodal-symbolic integration, human-in-the-loop systems, and ethical
safeguards, positioning LLMs as agents for principled, scalable scientific
discovery.

</details>


### [10] [Advancing Conversational Diagnostic AI with Multimodal Reasoning](https://arxiv.org/pdf/2505.04653)
*Khaled Saab, Jan Freyberg, Chunjong Park, Tim Strother, Yong Cheng, Wei-Hung Weng, David G. T. Barrett, David Stutz, Nenad Tomasev, Anil Palepu, Valentin Liévin, Yash Sharma, Roma Ruparel, Abdullah Ahmed, Elahe Vedadi, Kimberly Kanada, Cian Hughes, Yun Liu, Geoff Brown, Yang Gao, Sean Li, S. Sara Mahdavi, James Manyika, Katherine Chou, Yossi Matias, Avinatan Hassidim, Dale R. Webster, Pushmeet Kohli, S. M. Ali Eslami, Joëlle Barral, Adam Rodman, Vivek Natarajan, Mike Schaekermann, Tao Tu, Alan Karthikesalingam, Ryutaro Tanno*

Main category: cs.CL

TL;DR: AMIE, enhanced with multimodal data interpretation, outperforms primary care physicians in diagnostic conversations, excelling in accuracy and structured history-taking.


<details>
  <summary>Details</summary>
Motivation: Evaluate LLMs' ability to handle multimodal medical data in diagnostic conversations, addressing gaps in real-world remote care delivery.

Method: AMIE uses Gemini 2.0 Flash for state-aware dialogue, dynamically controlling conversation flow based on patient states and evolving diagnoses.

Result: AMIE surpassed PCPs in 7/9 multimodal and 29/32 non-multimodal axes, including diagnostic accuracy, in a blinded OSCE-style study.

Conclusion: AMIE shows promise for multimodal diagnostic AI, but real-world application requires further research.

Abstract: Large Language Models (LLMs) have demonstrated great potential for conducting
diagnostic conversations but evaluation has been largely limited to
language-only interactions, deviating from the real-world requirements of
remote care delivery. Instant messaging platforms permit clinicians and
patients to upload and discuss multimodal medical artifacts seamlessly in
medical consultation, but the ability of LLMs to reason over such data while
preserving other attributes of competent diagnostic conversation remains
unknown. Here we advance the conversational diagnosis and management
performance of the Articulate Medical Intelligence Explorer (AMIE) through a
new capability to gather and interpret multimodal data, and reason about this
precisely during consultations. Leveraging Gemini 2.0 Flash, our system
implements a state-aware dialogue framework, where conversation flow is
dynamically controlled by intermediate model outputs reflecting patient states
and evolving diagnoses. Follow-up questions are strategically directed by
uncertainty in such patient states, leading to a more structured multimodal
history-taking process that emulates experienced clinicians. We compared AMIE
to primary care physicians (PCPs) in a randomized, blinded, OSCE-style study of
chat-based consultations with patient actors. We constructed 105 evaluation
scenarios using artifacts like smartphone skin photos, ECGs, and PDFs of
clinical documents across diverse conditions and demographics. Our rubric
assessed multimodal capabilities and other clinically meaningful axes like
history-taking, diagnostic accuracy, management reasoning, communication, and
empathy. Specialist evaluation showed AMIE to be superior to PCPs on 7/9
multimodal and 29/32 non-multimodal axes (including diagnostic accuracy). The
results show clear progress in multimodal conversational diagnostic AI, but
real-world translation needs further research.

</details>


### [11] [A Comparative Analysis of Ethical and Safety Gaps in LLMs using Relative Danger Coefficient](https://arxiv.org/pdf/2505.04654)
*Yehor Tereshchenko, Mika Hämäläinen*

Main category: cs.CL

TL;DR: The paper compares ethical performance of AI models like DeepSeek-V3, GPT variants, and Gemini, introducing a new metric (RDC) for assessing harm in LLMs.


<details>
  <summary>Details</summary>
Motivation: Address ethical concerns (safety, misuse, discrimination) raised by rapid advancements in AI and LLMs.

Method: Comparative analysis of AI models' ethical performance and introduction of the Relative Danger Coefficient (RDC).

Result: Highlights varying ethical performance among models and the need for human oversight in high-stakes scenarios.

Conclusion: Proposes RDC as a tool for evaluating harm in LLMs and emphasizes robust human oversight.

Abstract: Artificial Intelligence (AI) and Large Language Models (LLMs) have rapidly
evolved in recent years, showcasing remarkable capabilities in natural language
understanding and generation. However, these advancements also raise critical
ethical questions regarding safety, potential misuse, discrimination and
overall societal impact. This article provides a comparative analysis of the
ethical performance of various AI models, including the brand new
DeepSeek-V3(R1 with reasoning and without), various GPT variants (4o, 3.5
Turbo, 4 Turbo, o1/o3 mini) and Gemini (1.5 flash, 2.0 flash and 2.0 flash exp)
and highlights the need for robust human oversight, especially in situations
with high stakes. Furthermore, we present a new metric for calculating harm in
LLMs called Relative Danger Coefficient (RDC).

</details>


### [12] [Applications of Artificial Intelligence for Cross-language Intelligibility Assessment of Dysarthric Speech](https://arxiv.org/pdf/2501.15858)
*Eunjung Yeo, Julie Liss, Visar Berisha, David Mortensen*

Main category: cs.CL

TL;DR: The paper proposes an AI-driven framework for cross-language intelligibility assessment of dysarthric speech, addressing data and linguistic challenges.


<details>
  <summary>Details</summary>
Motivation: Current research and clinical practices focus on English, limiting applicability across languages. The paper aims to bridge this gap.

Method: A two-tiered framework: a universal speech model for acoustic-phonetic encoding and a language-specific model for intelligibility assessment.

Result: Identifies barriers like data scarcity and annotation complexity, suggesting AI-driven solutions.

Conclusion: AI can enable scalable, linguistically informed assessment frameworks for dysarthric speech across languages.

Abstract: Purpose: Speech intelligibility is a critical outcome in the assessment and
management of dysarthria, yet most research and clinical practices have focused
on English, limiting their applicability across languages. This commentary
introduces a conceptual framework--and a demonstration of how it can be
implemented--leveraging artificial intelligence (AI) to advance cross-language
intelligibility assessment of dysarthric speech. Method: We propose a
two-tiered conceptual framework consisting of a universal speech model that
encodes dysarthric speech into acoustic-phonetic representations, followed by a
language-specific intelligibility assessment model that interprets these
representations within the phonological or prosodic structures of the target
language. We further identify barriers to cross-language intelligibility
assessment of dysarthric speech, including data scarcity, annotation
complexity, and limited linguistic insights into dysarthric speech, and outline
potential AI-driven solutions to overcome these challenges. Conclusion:
Advancing cross-language intelligibility assessment of dysarthric speech
necessitates models that are both efficient and scalable, yet constrained by
linguistic rules to ensure accurate and language-sensitive assessment. Recent
advances in AI provide the foundational tools to support this integration,
shaping future directions toward generalizable and linguistically informed
assessment frameworks.

</details>


### [13] [Toward Reasonable Parrots: Why Large Language Models Should Argue with Us by Design](https://arxiv.org/pdf/2505.05298)
*Elena Musi, Nadin Kokciyan, Khalid Al-Khatib, Davide Ceolin, Emmanuelle Dietz, Klara Gutekunst, Annette Hautli-Janisz, Cristian Manuel Santibañez Yañez, Jodi Schneider, Jonas Scholz, Cor Steging, Jacky Visser, Henning Wachsmuth*

Main category: cs.CL

TL;DR: Advocates for designing conversational tech to support argumentation, critiquing current LLMs and proposing 'reasonable parrots' based on argumentation theory.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack support for argumentative processes; the paper aims to enhance critical thinking through tech.

Method: Proposes 'reasonable parrots'—LLMs redesigned with principles of relevance, responsibility, and freedom, using argumentative dialogical moves.

Result: Introduces a framework for LLMs to facilitate argumentation, grounded in argumentation theory.

Conclusion: LLMs should be tools for critical thinking, not replacements, designed with argumentation principles.

Abstract: In this position paper, we advocate for the development of conversational
technology that is inherently designed to support and facilitate argumentative
processes. We argue that, at present, large language models (LLMs) are
inadequate for this purpose, and we propose an ideal technology design aimed at
enhancing argumentative skills. This involves re-framing LLMs as tools to
exercise our critical thinking rather than replacing them. We introduce the
concept of 'reasonable parrots' that embody the fundamental principles of
relevance, responsibility, and freedom, and that interact through argumentative
dialogical moves. These principles and moves arise out of millennia of work in
argumentation theory and should serve as the starting point for LLM-based
technology that incorporates basic principles of argumentation.

</details>


### [14] [Integration of Large Language Models and Traditional Deep Learning for Social Determinants of Health Prediction](https://arxiv.org/pdf/2505.04655)
*Paul Landes, Jimeng Sun, Adam Cross*

Main category: cs.CL

TL;DR: The paper explores using deep learning and LLMs to extract Social Determinants of Health (SDoH) from clinical text, achieving a 10-point improvement in classification and 12X faster execution by combining LLM precision with deep learning efficiency.


<details>
  <summary>Details</summary>
Motivation: SDoHs influence health outcomes and aid physicians in diagnosis and decision-making, necessitating efficient and accurate extraction methods.

Method: Combines traditional deep learning and LLMs to classify SDoHs, leveraging synthetic data and optimizing for speed and precision.

Result: Outperforms previous benchmarks by 10 points in multilabel classification and achieves 12X faster execution.

Conclusion: The proposed hybrid method offers a scalable and efficient solution for SDoH extraction, benefiting at-risk patients.

Abstract: Social Determinants of Health (SDoH) are economic, social and personal
circumstances that affect or influence an individual's health status. SDoHs
have shown to be correlated to wellness outcomes, and therefore, are useful to
physicians in diagnosing diseases and in decision-making. In this work, we
automatically extract SDoHs from clinical text using traditional deep learning
and Large Language Models (LLMs) to find the advantages and disadvantages of
each on an existing publicly available dataset. Our models outperform a
previous reference point on a multilabel SDoH classification by 10 points, and
we present a method and model to drastically speed up classification (12X
execution time) by eliminating expensive LLM processing. The method we present
combines a more nimble and efficient solution that leverages the power of the
LLM for precision and traditional deep learning methods for efficiency. We also
show highly performant results on a dataset supplemented with synthetic data
and several traditional deep learning models that outperform LLMs. Our models
and methods offer the next iteration of automatic prediction of SDoHs that
impact at-risk patients.

</details>


### [15] [AI-Generated Fall Data: Assessing LLMs and Diffusion Model for Wearable Fall Detection](https://arxiv.org/pdf/2505.04660)
*Sana Alamgeer, Yasine Souissi, Anne H. H. Ngu*

Main category: cs.CL

TL;DR: The paper explores using LLMs to generate synthetic fall data for training fall detection systems, comparing text-to-motion and text-to-text models. Results show dataset characteristics and sensor placement influence synthetic data effectiveness, with diffusion-based data aligning best with real data but not always improving performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity of real-world fall data, especially for elderly individuals, to improve fall detection systems.

Method: Evaluates text-to-motion (T2M, SATO, ParCo) and text-to-text (GPT4o, GPT4, Gemini) models for generating synthetic fall data. Integrates synthetic datasets with real-world baselines and tests performance using an LSTM model. Compares LLM-generated data with diffusion-based methods.

Result: Dataset characteristics affect synthetic data effectiveness; LLM-generated data performs well in low-frequency settings but struggles in high-frequency ones. Diffusion-based data aligns best with real data but doesn't consistently boost performance.

Conclusion: Synthetic data generation for fall detection must consider dataset characteristics, sensor placement, and fall representation. LLMs and diffusion methods offer potential but require optimization.

Abstract: Training fall detection systems is challenging due to the scarcity of
real-world fall data, particularly from elderly individuals. To address this,
we explore the potential of Large Language Models (LLMs) for generating
synthetic fall data. This study evaluates text-to-motion (T2M, SATO, ParCo) and
text-to-text models (GPT4o, GPT4, Gemini) in simulating realistic fall
scenarios. We generate synthetic datasets and integrate them with four
real-world baseline datasets to assess their impact on fall detection
performance using a Long Short-Term Memory (LSTM) model. Additionally, we
compare LLM-generated synthetic data with a diffusion-based method to evaluate
their alignment with real accelerometer distributions. Results indicate that
dataset characteristics significantly influence the effectiveness of synthetic
data, with LLM-generated data performing best in low-frequency settings (e.g.,
20Hz) while showing instability in high-frequency datasets (e.g., 200Hz). While
text-to-motion models produce more realistic biomechanical data than
text-to-text models, their impact on fall detection varies. Diffusion-based
synthetic data demonstrates the closest alignment to real data but does not
consistently enhance model performance. An ablation study further confirms that
the effectiveness of synthetic data depends on sensor placement and fall
representation. These findings provide insights into optimizing synthetic data
generation for fall detection models.

</details>


### [16] [Personalized Risks and Regulatory Strategies of Large Language Models in Digital Advertising](https://arxiv.org/pdf/2505.04665)
*Haoyang Feng, Yanjun Dai, Yuan Gao*

Main category: cs.CL

TL;DR: The paper explores combining large language models (LLMs) like BERT with privacy protection for personalized ad recommendations, showing improved ad performance while mitigating privacy risks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of integrating LLMs in advertising systems while ensuring user privacy and data security.

Method: Combines BERT and attention mechanisms for ad recommendations, using data preprocessing, feature selection, semantic embedding, local training, and encryption.

Result: BERT-based recommendations boost ad click-through and conversion rates, with reduced privacy risks via local training and encryption.

Conclusion: LLMs like BERT can enhance ad personalization while safeguarding privacy, but further research is needed for broader implementation.

Abstract: Although large language models have demonstrated the potential for
personalized advertising recommendations in experimental environments, in
actual operations, how advertising recommendation systems can be combined with
measures such as user privacy protection and data security is still an area
worthy of in-depth discussion. To this end, this paper studies the personalized
risks and regulatory strategies of large language models in digital
advertising. This study first outlines the principles of Large Language Model
(LLM), especially the self-attention mechanism based on the Transformer
architecture, and how to enable the model to understand and generate natural
language text. Then, the BERT (Bidirectional Encoder Representations from
Transformers) model and the attention mechanism are combined to construct an
algorithmic model for personalized advertising recommendations and user factor
risk protection. The specific steps include: data collection and preprocessing,
feature selection and construction, using large language models such as BERT
for advertising semantic embedding, and ad recommendations based on user
portraits. Then, local model training and data encryption are used to ensure
the security of user privacy and avoid the leakage of personal data. This paper
designs an experiment for personalized advertising recommendation based on a
large language model of BERT and verifies it with real user data. The
experimental results show that BERT-based advertising push can effectively
improve the click-through rate and conversion rate of advertisements. At the
same time, through local model training and privacy protection mechanisms, the
risk of user privacy leakage can be reduced to a certain extent.

</details>


### [17] [Fine-Tuning Large Language Models and Evaluating Retrieval Methods for Improved Question Answering on Building Codes](https://arxiv.org/pdf/2505.04666)
*Mohammad Aqib, Mohd Hamza, Qipei Mei, Ying Hei Chui*

Main category: cs.CL

TL;DR: A QA system using RAG is proposed to simplify querying complex building codes. Elasticsearch is the best retriever, and fine-tuning LLMs on NBCC data improves response relevance.


<details>
  <summary>Details</summary>
Motivation: Building codes are complex and hard to navigate manually, necessitating an automated QA system.

Method: Evaluated retrieval methods (e.g., Elasticsearch) and fine-tuned language models on NBCC data.

Result: Elasticsearch outperformed other retrievers; fine-tuned LLMs generated more relevant responses.

Conclusion: Combining Elasticsearch with fine-tuned LLMs optimizes RAG for navigating building codes.

Abstract: Building codes are regulations that establish standards for the design,
construction, and safety of buildings to ensure structural integrity, fire
protection, and accessibility. They are often extensive, complex, and subject
to frequent updates, making manual querying challenging and time-consuming. Key
difficulties include navigating large volumes of text, interpreting technical
language, and identifying relevant clauses across different sections. A
potential solution is to build a Question-Answering (QA) system that answers
user queries based on building codes. Among the various methods for building a
QA system, Retrieval-Augmented Generation (RAG) stands out in performance. RAG
consists of two components: a retriever and a language model. This study
focuses on identifying a suitable retriever method for building codes and
optimizing the generational capability of the language model using fine-tuning
techniques. We conducted a detailed evaluation of various retrieval methods by
performing the retrieval on the National Building Code of Canada (NBCC) and
explored the impact of domain-specific fine-tuning on several language models
using the dataset derived from NBCC. Our analysis included a comparative
assessment of different retrievers and the performance of both pre-trained and
fine-tuned models to determine the efficacy and domain-specific adaptation of
language models using fine-tuning on the NBCC dataset. Experimental results
showed that Elasticsearch proved to be the most robust retriever among all. The
findings also indicate that fine-tuning language models on an NBCC-specific
dataset can enhance their ability to generate contextually relevant responses.
When combined with context retrieved by a powerful retriever like
Elasticsearch, this improvement in LLM performance can optimize the RAG system,
enabling it to better navigate the complexities of the NBCC.

</details>


### [18] [Reward-SQL: Boosting Text-to-SQL via Stepwise Reasoning and Process-Supervised Rewards](https://arxiv.org/pdf/2505.04671)
*Yuxin Zhang, Meihao Fan, Ju Fan, Mingyang Yi, Yuyu Luo, Jian Tan, Guoliang Li*

Main category: cs.CL

TL;DR: Reward-SQL improves Text-to-SQL performance by integrating Process Reward Models (PRMs) effectively, achieving a 13.1% gain on BIRD benchmark.


<details>
  <summary>Details</summary>
Motivation: To address the misuse of PRMs in Text-to-SQL tasks, which can distort reasoning and lead to incorrect SQL generation.

Method: Uses a 'cold start, then PRM supervision' paradigm: decomposes SQL queries into stepwise reasoning chains (Chain-of-CTEs), then integrates PRMs via four strategies, with GRPO and best-of-N sampling yielding the best results.

Result: Achieves 68.9% accuracy on BIRD development set with Qwen2.5-Coder-7B-Instruct, outperforming baselines.

Conclusion: Reward-SQL effectively leverages PRMs for Text-to-SQL reasoning, demonstrating significant performance improvements.

Abstract: Recent advances in large language models (LLMs) have significantly improved
performance on the Text-to-SQL task by leveraging their powerful reasoning
capabilities. To enhance accuracy during the reasoning process, external
Process Reward Models (PRMs) can be introduced during training and inference to
provide fine-grained supervision. However, if misused, PRMs may distort the
reasoning trajectory and lead to suboptimal or incorrect SQL generation.To
address this challenge, we propose Reward-SQL, a framework that systematically
explores how to incorporate PRMs into the Text-to-SQL reasoning process
effectively. Our approach follows a "cold start, then PRM supervision"
paradigm. Specifically, we first train the model to decompose SQL queries into
structured stepwise reasoning chains using common table expressions
(Chain-of-CTEs), establishing a strong and interpretable reasoning baseline.
Then, we investigate four strategies for integrating PRMs, and find that
combining PRM as an online training signal (GRPO) with PRM-guided inference
(e.g., best-of-N sampling) yields the best results. Empirically, on the BIRD
benchmark, Reward-SQL enables models supervised by a 7B PRM to achieve a 13.1%
performance gain across various guidance strategies. Notably, our GRPO-aligned
policy model based on Qwen2.5-Coder-7B-Instruct achieves 68.9% accuracy on the
BIRD development set, outperforming all baseline methods under the same model
size. These results demonstrate the effectiveness of Reward-SQL in leveraging
reward-based supervision for Text-to-SQL reasoning. Our code is publicly
available.

</details>


### [19] [REVEAL: Multi-turn Evaluation of Image-Input Harms for Vision LLM](https://arxiv.org/pdf/2505.04673)
*Madhur Jindal, Saurabh Deshpande*

Main category: cs.CL

TL;DR: The paper introduces REVEAL, a framework for evaluating safety in Vision Large Language Models (VLLMs), revealing vulnerabilities in multi-turn interactions and highlighting GPT-4o as the most balanced performer.


<details>
  <summary>Details</summary>
Motivation: Existing safety frameworks are inadequate for multi-modal, multi-turn VLLMs, necessitating a scalable evaluation method.

Method: REVEAL uses automated image mining, synthetic adversarial data, multi-turn conversational expansion, and harm assessment via evaluators like GPT-4o.

Result: Multi-turn interactions showed higher defect rates; GPT-4o performed best, while misinformation was a critical vulnerability.

Conclusion: REVEAL effectively identifies VLLM vulnerabilities, emphasizing the need for improved contextual defenses, especially against misinformation.

Abstract: Vision Large Language Models (VLLMs) represent a significant advancement in
artificial intelligence by integrating image-processing capabilities with
textual understanding, thereby enhancing user interactions and expanding
application domains. However, their increased complexity introduces novel
safety and ethical challenges, particularly in multi-modal and multi-turn
conversations. Traditional safety evaluation frameworks, designed for
text-based, single-turn interactions, are inadequate for addressing these
complexities. To bridge this gap, we introduce the REVEAL (Responsible
Evaluation of Vision-Enabled AI LLMs) Framework, a scalable and automated
pipeline for evaluating image-input harms in VLLMs. REVEAL includes automated
image mining, synthetic adversarial data generation, multi-turn conversational
expansion using crescendo attack strategies, and comprehensive harm assessment
through evaluators like GPT-4o.
  We extensively evaluated five state-of-the-art VLLMs, GPT-4o, Llama-3.2,
Qwen2-VL, Phi3.5V, and Pixtral, across three important harm categories: sexual
harm, violence, and misinformation. Our findings reveal that multi-turn
interactions result in significantly higher defect rates compared to
single-turn evaluations, highlighting deeper vulnerabilities in VLLMs. Notably,
GPT-4o demonstrated the most balanced performance as measured by our
Safety-Usability Index (SUI) followed closely by Pixtral. Additionally,
misinformation emerged as a critical area requiring enhanced contextual
defenses. Llama-3.2 exhibited the highest MT defect rate ($16.55 \%$) while
Qwen2-VL showed the highest MT refusal rate ($19.1 \%$).

</details>


### [20] [Advanced Deep Learning Approaches for Automated Recognition of Cuneiform Symbols](https://arxiv.org/pdf/2505.04678)
*Shahad Elshehaby, Alavikunhu Panthakkan, Hussain Al-Ahmad, Mina Al-Saad*

Main category: cs.CL

TL;DR: Automated deep-learning method for cuneiform character identification and translation, showing high accuracy and precision, with future work on hybrid architectures.


<details>
  <summary>Details</summary>
Motivation: To explore the linguistic relationships between Akkadian and Arabic and demonstrate deep learning's potential in deciphering ancient scripts.

Method: Five deep-learning models trained on cuneiform datasets, evaluated on performance metrics, and tested on Hammurabi Law 1 symbols.

Result: Two models excelled in recognizing Akkadian meanings and providing precise English translations.

Conclusion: Deep learning bridges computational linguistics and archaeology, offering insights for historical comprehension and conservation.

Abstract: This paper presents a thoroughly automated method for identifying and
interpreting cuneiform characters via advanced deep-learning algorithms. Five
distinct deep-learning models were trained on a comprehensive dataset of
cuneiform characters and evaluated according to critical performance metrics,
including accuracy and precision. Two models demonstrated outstanding
performance and were subsequently assessed using cuneiform symbols from the
Hammurabi law acquisition, notably Hammurabi Law 1. Each model effectively
recognized the relevant Akkadian meanings of the symbols and delivered precise
English translations. Future work will investigate ensemble and stacking
approaches to optimize performance, utilizing hybrid architectures to improve
detection accuracy and reliability. This research explores the linguistic
relationships between Akkadian, an ancient Mesopotamian language, and Arabic,
emphasizing their historical and cultural linkages. This study demonstrates the
capability of deep learning to decipher ancient scripts by merging
computational linguistics with archaeology, therefore providing significant
insights for the comprehension and conservation of human history.

</details>


### [21] [SOAEsV2-7B/72B: Full-Pipeline Optimization for State-Owned Enterprise LLMs via Continual Pre-Training, Domain-Progressive SFT and Distillation-Enhanced Speculative Decoding](https://arxiv.org/pdf/2505.04723)
*Jingyang Deng, Ran Chen, Jo-Ku Cheng, Jinwen Ma*

Main category: cs.CL

TL;DR: The paper proposes SOAEsV2-7B/72B, a specialized LLM series for Chinese SOAEs, addressing limitations in model capacity, data reliance, and inference efficiency through a three-phase framework, achieving improved performance and speed.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in current domain-specific LLMs for Chinese SOAEs, including constrained model capacity, excessive reliance on domain-specific data, and inefficient inference.

Method: A three-phase framework: 1) continual pre-training, 2) domain-progressive SFT, and 3) distillation-enhanced speculative decoding.

Result: Improved domain performance (1.08× Rouge-1, 1.17× BLEU-4) while retaining 99.8% general capabilities, with 1.39-1.52× inference speedup.

Conclusion: The work provides a full-pipeline approach to optimize SOAEs LLMs, balancing general and domain-specific capabilities.

Abstract: This study addresses key challenges in developing domain-specific large
language models (LLMs) for Chinese state-owned assets and enterprises (SOAEs),
where current approaches face three limitations: 1) constrained model capacity
that limits knowledge integration and cross-task adaptability; 2) excessive
reliance on domain-specific supervised fine-tuning (SFT) data, which neglects
the broader applicability of general language patterns; and 3) inefficient
inference acceleration for large models processing long contexts. In this work,
we propose SOAEsV2-7B/72B, a specialized LLM series developed via a three-phase
framework: 1) continual pre-training integrates domain knowledge while
retaining base capabilities; 2) domain-progressive SFT employs curriculum-based
learning strategy, transitioning from weakly relevant conversational data to
expert-annotated SOAEs datasets to optimize domain-specific tasks; 3)
distillation-enhanced speculative decoding accelerates inference via logit
distillation between 72B target and 7B draft models, achieving
1.39-1.52$\times$ speedup without quality loss. Experimental results
demonstrate that our domain-specific pre-training phase maintains 99.8% of
original general language capabilities while significantly improving domain
performance, resulting in a 1.08$\times$ improvement in Rouge-1 score and a
1.17$\times$ enhancement in BLEU-4 score. Ablation studies further show that
domain-progressive SFT outperforms single-stage training, achieving
1.02$\times$ improvement in Rouge-1 and 1.06$\times$ in BLEU-4. Our work
introduces a comprehensive, full-pipeline approach for optimizing SOAEs LLMs,
bridging the gap between general language capabilities and domain-specific
expertise.

</details>


### [22] [Flower Across Time and Media: Sentiment Analysis of Tang Song Poetry and Visual Correspondence](https://arxiv.org/pdf/2505.04785)
*Shuai Gong, Tiange Zhou*

Main category: cs.CL

TL;DR: The paper explores the link between floral motifs in Tang and Song dynasty poetry and decorative arts using BERT-based sentiment analysis, revealing emotional shifts and artistic synergies.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in understanding the systematic correlation between evolving literary emotions and visual culture in Tang and Song dynasties.

Method: Employed a fine-tuned BERT model for sentiment analysis of floral imagery in poetry, cross-referenced with visual evidence from decorative arts.

Result: Detected measurable emotional shifts in floral motifs between Tang and Song periods, revealing synergies between literature and art.

Conclusion: The study highlights the dynamic interplay between literary sentiment and visual culture, advancing computational humanities in sinological research.

Abstract: The Tang (618 to 907) and Song (960 to 1279) dynasties witnessed an
extraordinary flourishing of Chinese cultural expression, where floral motifs
served as a dynamic medium for both poetic sentiment and artistic design. While
previous scholarship has examined these domains independently, the systematic
correlation between evolving literary emotions and visual culture remains
underexplored. This study addresses that gap by employing BERT-based sentiment
analysis to quantify emotional patterns in floral imagery across Tang Song
poetry, then validating these patterns against contemporaneous developments in
decorative arts.Our approach builds upon recent advances in computational
humanities while remaining grounded in traditional sinological methods. By
applying a fine tuned BERT model to analyze peony and plum blossom imagery in
classical poetry, we detect measurable shifts in emotional connotations between
the Tang and Song periods. These textual patterns are then cross berenced with
visual evidence from textiles, ceramics, and other material culture, revealing
previously unrecognized synergies between literary expression and artistic
representation.

</details>


### [23] [Osiris: A Lightweight Open-Source Hallucination Detection System](https://arxiv.org/pdf/2505.04844)
*Alex Shan, John Bauer, Christopher D. Manning*

Main category: cs.CL

TL;DR: The paper introduces a perturbed multi-hop QA dataset to detect hallucinations in RAG systems, achieving better recall than GPT-4o with a smaller 7B model.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in RAG systems hinder deployment; current detection methods (human evaluation or closed-source models) are costly and slow.

Method: Supervised fine-tuning on a perturbed multi-hop QA dataset with induced hallucinations.

Result: The 7B model outperforms GPT-4o in recall on the RAGTruth benchmark and offers competitive precision and accuracy.

Conclusion: The proposed method provides a scalable, cost-effective solution for hallucination detection in RAG systems.

Abstract: Retrieval-Augmented Generation (RAG) systems have gained widespread adoption
by application builders because they leverage sources of truth to enable Large
Language Models (LLMs) to generate more factually sound responses. However,
hallucinations, instances of LLM responses that are unfaithful to the provided
context, often prevent these systems from being deployed in production
environments. Current hallucination detection methods typically involve human
evaluation or the use of closed-source models to review RAG system outputs for
hallucinations. Both human evaluators and closed-source models suffer from
scaling issues due to their high costs and slow inference speeds. In this work,
we introduce a perturbed multi-hop QA dataset with induced hallucinations. Via
supervised fine-tuning on our dataset, we achieve better recall with a 7B model
than GPT-4o on the RAGTruth hallucination detection benchmark and offer
competitive performance on precision and accuracy, all while using a fraction
of the parameters. Code is released at our repository.

</details>


### [24] [Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards](https://arxiv.org/pdf/2505.04847)
*Manveer Singh Tamber, Forrest Sheng Bao, Chenyu Xu, Ge Luo, Suleman Kazi, Minseok Bae, Miaoran Li, Ofer Mendelevitch, Renyi Qu, Jimmy Lin*

Main category: cs.CL

TL;DR: The paper addresses LLM hallucinations in summarization tasks, critiques current evaluation methods like HHEM, and introduces FaithJudge, an improved LLM-as-a-judge approach for hallucination detection.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs persist even with RAG, necessitating better evaluation methods.

Method: Proposes FaithJudge, a few-shot human-annotated LLM-as-a-judge approach, and introduces an enhanced hallucination leaderboard.

Result: FaithJudge improves hallucination evaluation over existing methods like HHEM.

Conclusion: FaithJudge and the new leaderboard provide more reliable benchmarking for LLM hallucinations in RAG.

Abstract: Hallucinations remain a persistent challenge for LLMs. RAG aims to reduce
hallucinations by grounding responses in contexts. However, even when provided
context, LLMs still frequently introduce unsupported information or
contradictions. This paper presents our efforts to measure LLM hallucinations
with a focus on summarization tasks, assessing how often various LLMs introduce
hallucinations when summarizing documents. We discuss Vectara's existing LLM
hallucination leaderboard, based on the Hughes Hallucination Evaluation Model
(HHEM). While HHEM and Vectara's Hallucination Leaderboard have garnered great
research interest, we examine challenges faced by HHEM and current
hallucination detection methods by analyzing the effectiveness of these methods
on existing hallucination datasets. To address these limitations, we propose
FaithJudge, an LLM-as-a-judge approach guided by few-shot human hallucination
annotations, which substantially improves automated LLM hallucination
evaluation over current methods. We introduce an enhanced hallucination
leaderboard centered on FaithJudge, alongside our current hallucination
leaderboard, enabling more reliable benchmarking of LLMs for hallucinations in
RAG.

</details>


### [25] [An Open-Source Dual-Loss Embedding Model for Semantic Retrieval in Higher Education](https://arxiv.org/pdf/2505.04916)
*Ramteja Sajja, Yusuf Sermet, Ibrahim Demir*

Main category: cs.CL

TL;DR: The study introduces two open-source embedding models fine-tuned for educational question answering, showing improved performance over baselines and narrowing the gap with proprietary models.


<details>
  <summary>Details</summary>
Motivation: Address the unsuitability of existing semantic retrieval systems for academic content by developing domain-specific embedding models.

Method: Fine-tuned two models: one using MultipleNegativesRankingLoss (MNRL) and another combining MNRL with CosineSimilarityLoss. Evaluated on a synthetic dataset of 3,197 sentence pairs and 28 university course syllabi.

Result: Both models outperformed open-source baselines, with the dual-loss model narrowing the performance gap with high-performing proprietary embeddings.

Conclusion: The work provides reusable, domain-aligned embedding models and a replicable framework for educational semantic retrieval, supporting applications like academic chatbots and LMS integrations.

Abstract: Recent advances in AI have catalyzed the adoption of intelligent educational
tools, yet many semantic retrieval systems remain ill-suited to the unique
linguistic and structural characteristics of academic content. This study
presents two open-source embedding models fine-tuned for educational question
answering, particularly in the context of course syllabi. A synthetic dataset
of 3,197 sentence pairs, spanning synonymous terminology, paraphrased
questions, and implicit-explicit mappings, was constructed through a
combination of manual curation and large language model (LLM)-assisted
generation. Two training strategies were evaluated: (1) a baseline model
fine-tuned using MultipleNegativesRankingLoss (MNRL), and (2) a dual-loss model
that combines MNRL with CosineSimilarityLoss to improve both semantic ranking
and similarity calibration. Evaluations were conducted on 28 university course
syllabi using a fixed set of natural language questions categorized into
course, faculty, and teaching assistant information. Results demonstrate that
both fine-tuned models outperform strong open-source baselines, including
all-MiniLM-L6-v2 and multi-qa-MiniLM-L6-cos-v1, and that the dual-loss model
narrows the performance gap with high-performing proprietary embeddings such as
OpenAI's text-embedding-3 series. This work contributes reusable,
domain-aligned embedding models and provides a replicable framework for
educational semantic retrieval, supporting downstream applications such as
academic chatbots, retrieval-augmented generation (RAG) systems, and learning
management system (LMS) integrations.

</details>


### [26] [Chain-of-Thought Tokens are Computer Program Variables](https://arxiv.org/pdf/2505.04955)
*Fangwei Zhu, Peiyi Wang, Zhifang Sui*

Main category: cs.CL

TL;DR: CoT tokens in LLMs act like variables in programs, with intermediate results being key to performance, but they may introduce unintended shortcuts and complexity.


<details>
  <summary>Details</summary>
Motivation: To understand the inner mechanism of Chain-of-Thoughts (CoT) in LLMs and its role in solving complex reasoning tasks.

Method: Empirical study on CoT tokens in LLMs using two compositional tasks: multi-digit multiplication and dynamic programming. Analyzed the impact of preserving intermediate results and alternative latent forms.

Result: Preserving only intermediate result tokens achieves comparable performance. Alternative latent forms don't affect performance. Random interventions in CoT tokens alter subsequent tokens and final answers.

Conclusion: CoT tokens function like variables in programs but may have drawbacks like unintended shortcuts and computational complexity limits.

Abstract: Chain-of-thoughts (CoT) requires large language models (LLMs) to generate
intermediate steps before reaching the final answer, and has been proven
effective to help LLMs solve complex reasoning tasks. However, the inner
mechanism of CoT still remains largely unclear. In this paper, we empirically
study the role of CoT tokens in LLMs on two compositional tasks: multi-digit
multiplication and dynamic programming. While CoT is essential for solving
these problems, we find that preserving only tokens that store intermediate
results would achieve comparable performance. Furthermore, we observe that
storing intermediate results in an alternative latent form will not affect
model performance. We also randomly intervene some values in CoT, and notice
that subsequent CoT tokens and the final answer would change correspondingly.
These findings suggest that CoT tokens may function like variables in computer
programs but with potential drawbacks like unintended shortcuts and
computational complexity limits between tokens. The code and data are available
at https://github.com/solitaryzero/CoTs_are_Variables.

</details>


### [27] [Rethinking the Relationship between the Power Law and Hierarchical Structures](https://arxiv.org/pdf/2505.04984)
*Kai Nakaishi, Ryo Yoshida, Kohei Kajikawa, Koji Hukushima, Yohei Oseki*

Main category: cs.CL

TL;DR: The paper challenges the interpretation of power-law decay in language corpora as evidence for hierarchical structures, showing that assumptions don't hold for syntactic structures.


<details>
  <summary>Details</summary>
Motivation: To empirically test the validity of the argument linking power-law decay to hierarchical structures in language, especially for syntax, child languages, and animal signals.

Method: Analyzed English corpora by examining mutual information, deviations from PCFGs, and other properties in parse trees and their PCFG approximations.

Result: Found that the assumptions supporting the power-law argument do not hold for syntactic structures, complicating its application to child languages and animal signals.

Conclusion: The relationship between power laws and hierarchical structures needs reconsideration, as empirical evidence contradicts the existing argument.

Abstract: Statistical analysis of corpora provides an approach to quantitatively
investigate natural languages. This approach has revealed that several power
laws consistently emerge across different corpora and languages, suggesting the
universal principles underlying languages. Particularly, the power-law decay of
correlation has been interpreted as evidence for underlying hierarchical
structures in syntax, semantics, and discourse. This perspective has also been
extended to child languages and animal signals. However, the argument
supporting this interpretation has not been empirically tested. To address this
problem, this study examines the validity of the argument for syntactic
structures. Specifically, we test whether the statistical properties of parse
trees align with the implicit assumptions in the argument. Using English
corpora, we analyze the mutual information, deviations from probabilistic
context-free grammars (PCFGs), and other properties in parse trees, as well as
in the PCFG that approximates these trees. Our results indicate that the
assumptions do not hold for syntactic structures and that it is difficult to
apply the proposed argument to child languages and animal signals, highlighting
the need to reconsider the relationship between the power law and hierarchical
structures.

</details>


### [28] [Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes](https://arxiv.org/pdf/2505.04993)
*Zhuocheng Gong, Jian Guan, Wei Wu, Huishuai Zhang, Dongyan Zhao*

Main category: cs.CL

TL;DR: Latent Preference Coding (LPC) improves LLM alignment by modeling implicit human preference factors with latent codes, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing preference modeling overlooks the complexity of human preferences, which can be conflicting and multifaceted.

Method: LPC uses discrete latent codes to model implicit preference factors and their combinations, integrating with offline alignment algorithms without predefined rewards.

Result: LPC consistently enhances alignment algorithms (DPO, SimPO, IPO) across benchmarks and models, improving robustness against noisy data.

Conclusion: LPC offers a unified representation for diverse preference factors, advancing robust and versatile LLM alignment techniques.

Abstract: Large language models (LLMs) have achieved remarkable success, yet aligning
their generations with human preferences remains a critical challenge. Existing
approaches to preference modeling often rely on an explicit or implicit reward
function, overlooking the intricate and multifaceted nature of human
preferences that may encompass conflicting factors across diverse tasks and
populations. To address this limitation, we introduce Latent Preference Coding
(LPC), a novel framework that models the implicit factors as well as their
combinations behind holistic preferences using discrete latent codes. LPC
seamlessly integrates with various offline alignment algorithms, automatically
inferring the underlying factors and their importance from data without relying
on pre-defined reward functions and hand-crafted combination weights. Extensive
experiments on multiple benchmarks demonstrate that LPC consistently improves
upon three alignment algorithms (DPO, SimPO, and IPO) using three base models
(Mistral-7B, Llama3-8B, and Llama3-8B-Instruct). Furthermore, deeper analysis
reveals that the learned latent codes effectively capture the differences in
the distribution of human preferences and significantly enhance the robustness
of alignment against noise in data. By providing a unified representation for
the multifarious preference factors, LPC paves the way towards developing more
robust and versatile alignment techniques for the responsible deployment of
powerful LLMs.

</details>


### [29] [Rethinking Invariance in In-context Learning](https://arxiv.org/pdf/2505.04994)
*Lizhe Fang, Yifei Wang, Khashayar Gatmiry, Lei Fang, Yisen Wang*

Main category: cs.CL

TL;DR: InvICL addresses sensitivity to example order in In-Context Learning (ICL) by ensuring information non-leakage and context interdependence, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Standard ICL is sensitive to example order, and existing invariant methods underperform. InvICL aims to solve this.

Method: Proposes InvICL, ensuring information non-leakage and context interdependence for permutation invariance.

Result: InvICL outperforms existing methods in benchmarks, showing better generalization.

Conclusion: InvICL effectively achieves invariant ICL with superior performance.

Abstract: In-Context Learning (ICL) has emerged as a pivotal capability of
auto-regressive large language models, yet it is hindered by a notable
sensitivity to the ordering of context examples regardless of their mutual
independence. To address this issue, recent studies have introduced several
variant algorithms of ICL that achieve permutation invariance. However, many of
these do not exhibit comparable performance with the standard auto-regressive
ICL algorithm. In this work, we identify two crucial elements in the design of
an invariant ICL algorithm: information non-leakage and context
interdependence, which are not simultaneously achieved by any of the existing
methods. These investigations lead us to the proposed Invariant ICL (InvICL), a
methodology designed to achieve invariance in ICL while ensuring the two
properties. Empirically, our findings reveal that InvICL surpasses previous
models, both invariant and non-invariant, in most benchmark datasets,
showcasing superior generalization capabilities across varying input lengths.
Code is available at https://github.com/PKU-ML/InvICL.

</details>


### [30] [The Pitfalls of Growing Group Complexity: LLMs and Social Choice-Based Aggregation for Group Recommendations](https://arxiv.org/pdf/2505.05016)
*Cedric Waterschoot, Nava Tintarev, Francesco Barile*

Main category: cs.CL

TL;DR: LLMs can perform group recommendation tasks via zero-shot learning, but performance drops with over 100 ratings. In-Context Learning boosts accuracy for complex groups, while other prompt modifications don't help. Smaller LLMs are viable under optimal conditions.


<details>
  <summary>Details</summary>
Motivation: To explore how LLMs handle group recommendation tasks under zero-shot learning and how factors like group complexity, prompting, and formatting affect accuracy.

Method: Analyzed LLMs' performance in GRS using zero-shot learning, varying group complexity (users/items), prompting conditions (ICL, explanations), and preference formatting.

Result: Performance declines beyond 100 ratings. ICL improves accuracy for complex groups; other prompt changes don't. Formatting preferences (per user/item) affects results. Smaller LLMs work well under right conditions.

Conclusion: Group complexity impacts LLM performance in GRS. Future research should account for it. Smaller LLMs are practical for group recommendations with proper setup.

Abstract: Large Language Models (LLMs) are increasingly applied in recommender systems
aimed at both individuals and groups. Previously, Group Recommender Systems
(GRS) often used social choice-based aggregation strategies to derive a single
recommendation based on the preferences of multiple people. In this paper, we
investigate under which conditions language models can perform these strategies
correctly based on zero-shot learning and analyse whether the formatting of the
group scenario in the prompt affects accuracy. We specifically focused on the
impact of group complexity (number of users and items), different LLMs,
different prompting conditions, including In-Context learning or generating
explanations, and the formatting of group preferences. Our results show that
performance starts to deteriorate when considering more than 100 ratings.
However, not all language models were equally sensitive to growing group
complexity. Additionally, we showed that In-Context Learning (ICL) can
significantly increase the performance at higher degrees of group complexity,
while adding other prompt modifications, specifying domain cues or prompting
for explanations, did not impact accuracy. We conclude that future research
should include group complexity as a factor in GRS evaluation due to its effect
on LLM performance. Furthermore, we showed that formatting the group scenarios
differently, such as rating lists per user or per item, affected accuracy. All
in all, our study implies that smaller LLMs are capable of generating group
recommendations under the right conditions, making the case for using smaller
models that require less computing power and costs.

</details>


### [31] [Scalable Multi-Stage Influence Function for Large Language Models via Eigenvalue-Corrected Kronecker-Factored Parameterization](https://arxiv.org/pdf/2505.05017)
*Yuntai Bao, Xuhong Zhang, Tianyu Du, Xinkui Zhao, Jiang Zong, Hao Peng, Jianwei Yin*

Main category: cs.CL

TL;DR: The paper introduces a multi-stage influence function to attribute fine-tuned LLM predictions to pre-training data, using EK-FAC for efficient approximation.


<details>
  <summary>Details</summary>
Motivation: To address the lack of scalable and multi-stage influence attribution methods for fine-tuned LLMs.

Method: Proposes a multi-stage influence function with EK-FAC parameterization for efficient approximation.

Result: Validates scalability of EK-FAC and effectiveness of the multi-stage influence function, demonstrated on dolly-v2-3b.

Conclusion: The method provides interpretable insights into LLM predictions, with publicly available code.

Abstract: Pre-trained large language models (LLMs) are commonly fine-tuned to adapt to
downstream tasks. Since the majority of knowledge is acquired during
pre-training, attributing the predictions of fine-tuned LLMs to their
pre-training data may provide valuable insights. Influence functions have been
proposed as a means to explain model predictions based on training data.
However, existing approaches fail to compute ``multi-stage'' influence and lack
scalability to billion-scale LLMs.
  In this paper, we propose the multi-stage influence function to attribute the
downstream predictions of fine-tuned LLMs to pre-training data under the
full-parameter fine-tuning paradigm. To enhance the efficiency and practicality
of our multi-stage influence function, we leverage Eigenvalue-corrected
Kronecker-Factored (EK-FAC) parameterization for efficient approximation.
Empirical results validate the superior scalability of EK-FAC approximation and
the effectiveness of our multi-stage influence function. Additionally, case
studies on a real-world LLM, dolly-v2-3b, demonstrate its interpretive power,
with exemplars illustrating insights provided by multi-stage influence
estimates. Our code is public at
https://github.com/colored-dye/multi_stage_influence_function.

</details>


### [32] [G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness](https://arxiv.org/pdf/2505.05026)
*Jaehyun Jeon, Janghan Yoon, Minsoo Kim, Sumin Shim, Yejin Choi, Hanbin Kim, Youngjae Yu*

Main category: cs.CL

TL;DR: WiserUI-Bench is a benchmark for assessing UI persuasiveness using pairwise comparisons, and G-FOCUS improves VLM-based evaluation accuracy, offering a scalable alternative to A/B testing.


<details>
  <summary>Details</summary>
Motivation: Current UI evaluation methods like A/B testing are costly, and existing VLM approaches lack focus on comparative persuasiveness.

Method: Introduces WiserUI-Bench with 300 UI pairs and G-FOCUS, a reasoning strategy to enhance VLM accuracy.

Result: G-FOCUS outperforms existing methods in consistency and accuracy for UI persuasiveness assessment.

Conclusion: The work provides a scalable solution for UI design optimization, complementing traditional A/B testing.

Abstract: Evaluating user interface (UI) design effectiveness extends beyond aesthetics
to influencing user behavior, a principle central to Design Persuasiveness. A/B
testing is the predominant method for determining which UI variations drive
higher user engagement, but it is costly and time-consuming. While recent
Vision-Language Models (VLMs) can process automated UI analysis, current
approaches focus on isolated design attributes rather than comparative
persuasiveness-the key factor in optimizing user interactions. To address this,
we introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design
Persuasiveness Assessment task, featuring 300 real-world UI image pairs labeled
with A/B test results and expert rationales. Additionally, we propose G-FOCUS,
a novel inference-time reasoning strategy that enhances VLM-based
persuasiveness assessment by reducing position bias and improving evaluation
accuracy. Experimental results show that G-FOCUS surpasses existing inference
strategies in consistency and accuracy for pairwise UI evaluation. Through
promoting VLM-driven evaluation of UI persuasiveness, our work offers an
approach to complement A/B testing, propelling progress in scalable UI
preference modeling and design optimization. Code and data will be released
publicly.

</details>


### [33] [Image-Text Relation Prediction for Multilingual Tweets](https://arxiv.org/pdf/2505.05040)
*Matīss Rikters, Edison Marrese-Taylor*

Main category: cs.CL

TL;DR: The paper explores multilingual vision-language models for image-text relation prediction, using a Latvian-English Twitter dataset, and finds newer models perform better but need improvement.


<details>
  <summary>Details</summary>
Motivation: To understand how multilingual vision-language models predict image-text relations across languages, addressing unclear relationships in social media posts.

Method: Constructed a balanced benchmark dataset from Latvian Twitter posts and their English translations, evaluating multilingual vision-language models.

Result: Newer model checkpoints show improved capability in image-text relation prediction, but significant room for improvement remains.

Conclusion: While recent vision-language models perform better, further advancements are needed for accurate image-text relation prediction in multilingual contexts.

Abstract: Various social networks have been allowing media uploads for over a decade
now. Still, it has not always been clear what is their relation with the posted
text or even if there is any at all. In this work, we explore how multilingual
vision-language models tackle the task of image-text relation prediction in
different languages, and construct a dedicated balanced benchmark data set from
Twitter posts in Latvian along with their manual translations into English. We
compare our results to previous work and show that the more recently released
vision-language model checkpoints are becoming increasingly capable at this
task, but there is still much room for further improvement.

</details>


### [34] [Teochew-Wild: The First In-the-wild Teochew Dataset with Orthographic Annotations](https://arxiv.org/pdf/2505.05056)
*Linrong Pan, Chenglong Jiang, Gaoze Hou, Ying Gao*

Main category: cs.CL

TL;DR: The paper introduces Teochew-Wild, the first publicly available Teochew speech corpus with 18.9 hours of annotated data, supporting ASR and TTS tasks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of resources for the Teochew dialect, a low-resource language, by providing a comprehensive speech corpus with annotations.

Method: Constructed a corpus with 18.9 hours of Teochew speech data, including formal and colloquial expressions, annotated with orthography and pinyin. Provided supplementary tools for research.

Result: Experiments confirmed the corpus's effectiveness for ASR and TTS tasks.

Conclusion: Teochew-Wild fills a gap in resources for the Teochew dialect and supports advancements in speech technology.

Abstract: This paper reports the construction of the Teochew-Wild, a speech corpus of
the Teochew dialect. The corpus includes 18.9 hours of in-the-wild Teochew
speech data from multiple speakers, covering both formal and colloquial
expressions, with precise orthographic and pinyin annotations. Additionally, we
provide supplementary text processing tools and resources to propel research
and applications in speech tasks for this low-resource language, such as
automatic speech recognition (ASR) and text-to-speech (TTS). To the best of our
knowledge, this is the first publicly available Teochew dataset with accurate
orthographic annotations. We conduct experiments on the corpus, and the results
validate its effectiveness in ASR and TTS tasks.

</details>


### [35] [Performance Evaluation of Large Language Models in Bangla Consumer Health Query Summarization](https://arxiv.org/pdf/2505.05070)
*Ajwad Abrar, Farzana Tabassum, Sabbir Ahmed*

Main category: cs.CL

TL;DR: Zero-shot LLMs like Mixtral-8x22b-Instruct perform comparably to fine-tuned models (e.g., Bangla T5) in summarizing Bengali consumer health queries, highlighting their potential for low-resource languages.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of summarizing extraneous details in Bengali consumer health queries (CHQs) for efficient medical responses.

Method: Benchmarked nine advanced LLMs (e.g., GPT-4, Claude-3.5-Sonnet) against Bangla T5 using the BanglaCHQ-Summ dataset (2,350 pairs) and ROUGE metrics.

Result: Mixtral-8x22b-Instruct led in ROUGE-1 and ROUGE-L; Bangla T5 excelled in ROUGE-2. Zero-shot LLMs matched fine-tuned models.

Conclusion: Zero-shot LLMs offer scalable, high-quality summarization for low-resource languages like Bengali, aiding healthcare query processing.

Abstract: Consumer Health Queries (CHQs) in Bengali (Bangla), a low-resource language,
often contain extraneous details, complicating efficient medical responses.
This study investigates the zero-shot performance of nine advanced large
language models (LLMs): GPT-3.5-Turbo, GPT-4, Claude-3.5-Sonnet,
Llama3-70b-Instruct, Mixtral-8x22b-Instruct, Gemini-1.5-Pro,
Qwen2-72b-Instruct, Gemma-2-27b, and Athene-70B, in summarizing Bangla CHQs.
Using the BanglaCHQ-Summ dataset comprising 2,350 annotated query-summary
pairs, we benchmarked these LLMs using ROUGE metrics against Bangla T5, a
fine-tuned state-of-the-art model. Mixtral-8x22b-Instruct emerged as the top
performing model in ROUGE-1 and ROUGE-L, while Bangla T5 excelled in ROUGE-2.
The results demonstrate that zero-shot LLMs can rival fine-tuned models,
achieving high-quality summaries even without task-specific training. This work
underscores the potential of LLMs in addressing challenges in low-resource
languages, providing scalable solutions for healthcare query summarization.

</details>


### [36] [Reliably Bounding False Positives: A Zero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal Prediction](https://arxiv.org/pdf/2505.05084)
*Xiaowei Zhu, Yubing Ren, Yanan Cao, Xixun Lin, Fang Fang, Yangxi Li*

Main category: cs.CL

TL;DR: A framework using Multiscaled Conformal Prediction (MCP) is proposed to detect machine-generated text while controlling false positive rates (FPRs) and improving detection performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the societal risks of high FPRs in existing detection methods for large language models.

Method: Leverages Conformal Prediction (CP) and introduces MCP to balance FPR constraints and detection performance, using the RealDet dataset for realistic calibration.

Result: MCP effectively constrains FPRs, enhances detection performance, and improves robustness against adversarial attacks.

Conclusion: The proposed MCP framework offers a balanced solution for detecting machine-generated text with controlled FPRs and improved accuracy.

Abstract: The rapid advancement of large language models has raised significant
concerns regarding their potential misuse by malicious actors. As a result,
developing effective detectors to mitigate these risks has become a critical
priority. However, most existing detection methods focus excessively on
detection accuracy, often neglecting the societal risks posed by high false
positive rates (FPRs). This paper addresses this issue by leveraging Conformal
Prediction (CP), which effectively constrains the upper bound of FPRs. While
directly applying CP constrains FPRs, it also leads to a significant reduction
in detection performance. To overcome this trade-off, this paper proposes a
Zero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal
Prediction (MCP), which both enforces the FPR constraint and improves detection
performance. This paper also introduces RealDet, a high-quality dataset that
spans a wide range of domains, ensuring realistic calibration and enabling
superior detection performance when combined with MCP. Empirical evaluations
demonstrate that MCP effectively constrains FPRs, significantly enhances
detection performance, and increases robustness against adversarial attacks
across multiple detectors and datasets.

</details>


### [37] [Unveiling Language-Specific Features in Large Language Models via Sparse Autoencoders](https://arxiv.org/pdf/2505.05111)
*Boyi Deng, Yu Wan, Yidan Zhang, Baosong Yang, Fuli Feng*

Main category: cs.CL

TL;DR: The paper explores multilingual capabilities in LLMs using Sparse Autoencoders (SAEs), introducing a metric for monolinguality and showing language-specific feature ablation impacts.


<details>
  <summary>Details</summary>
Motivation: To address limitations of neuron-based methods in analyzing multilingual LLMs, such as superposition and activation variance.

Method: Uses SAEs to decompose LLM activations into sparse features, introduces a monolinguality metric, and ablates features to test language-specific impacts.

Result: Some SAE features are language-specific; ablating them affects only one language. Synergistic features exist, and SAE-derived features improve language control.

Conclusion: SAEs provide deeper insights into multilingual LLMs, enabling better analysis and control of language-specific behaviors.

Abstract: The mechanisms behind multilingual capabilities in Large Language Models
(LLMs) have been examined using neuron-based or internal-activation-based
methods. However, these methods often face challenges such as superposition and
layer-wise activation variance, which limit their reliability. Sparse
Autoencoders (SAEs) offer a more nuanced analysis by decomposing the
activations of LLMs into sparse linear combination of SAE features. We
introduce a novel metric to assess the monolinguality of features obtained from
SAEs, discovering that some features are strongly related to specific
languages. Additionally, we show that ablating these SAE features only
significantly reduces abilities in one language of LLMs, leaving others almost
unaffected. Interestingly, we find some languages have multiple synergistic SAE
features, and ablating them together yields greater improvement than ablating
individually. Moreover, we leverage these SAE-derived language-specific
features to enhance steering vectors, achieving control over the language
generated by LLMs.

</details>


### [38] [A Benchmark Dataset and a Framework for Urdu Multimodal Named Entity Recognition](https://arxiv.org/pdf/2505.05148)
*Hussain Ahmad, Qingyang Zeng, Jing Wan*

Main category: cs.CL

TL;DR: The paper introduces U-MNER, a framework for Urdu Multimodal Named Entity Recognition (MNER), and the Twitter2015-Urdu dataset, addressing the lack of resources for low-resource languages.


<details>
  <summary>Details</summary>
Motivation: MNER is understudied for low-resource languages like Urdu due to scarce annotated datasets and baselines.

Method: U-MNER combines Urdu-BERT for text and ResNet for images, with a Cross-Modal Fusion Module.

Result: The model achieves state-of-the-art performance on the Twitter2015-Urdu dataset.

Conclusion: This work advances MNER research for low-resource languages and provides a benchmark for Urdu.

Abstract: The emergence of multimodal content, particularly text and images on social
media, has positioned Multimodal Named Entity Recognition (MNER) as an
increasingly important area of research within Natural Language Processing.
Despite progress in high-resource languages such as English, MNER remains
underexplored for low-resource languages like Urdu. The primary challenges
include the scarcity of annotated multimodal datasets and the lack of
standardized baselines. To address these challenges, we introduce the U-MNER
framework and release the Twitter2015-Urdu dataset, a pioneering resource for
Urdu MNER. Adapted from the widely used Twitter2015 dataset, it is annotated
with Urdu-specific grammar rules. We establish benchmark baselines by
evaluating both text-based and multimodal models on this dataset, providing
comparative analyses to support future research on Urdu MNER. The U-MNER
framework integrates textual and visual context using Urdu-BERT for text
embeddings and ResNet for visual feature extraction, with a Cross-Modal Fusion
Module to align and fuse information. Our model achieves state-of-the-art
performance on the Twitter2015-Urdu dataset, laying the groundwork for further
MNER research in low-resource languages.

</details>


### [39] [QualBench: Benchmarking Chinese LLMs with Localized Professional Qualifications for Vertical Domain Evaluation](https://arxiv.org/pdf/2505.05225)
*Mengze Hong, Wailing Ng, Di Jiang, Chen Jason Zhang*

Main category: cs.CL

TL;DR: QualBench is a multi-domain Chinese QA benchmark for evaluating Chinese LLMs using qualification exams, showing localized models outperform non-Chinese ones.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack domain-specific coverage and insights into the Chinese context, necessitating a localized evaluation framework.

Method: QualBench uses 17,000+ questions across six domains, aligned with 24 Chinese qualifications, to assess LLMs like Qwen2.5 and GPT-4o.

Result: Chinese LLMs (e.g., Qwen2.5) outperformed non-Chinese models (e.g., GPT-4o), with a top score of 75.26%, revealing domain coverage gaps.

Conclusion: Localized domain knowledge is crucial; LLM collaboration with crowdsourcing failed, suggesting opportunities for RAG and federated learning in domain-specific training.

Abstract: The rapid advancement of Chinese large language models (LLMs) underscores the
need for domain-specific evaluations to ensure reliable applications. However,
existing benchmarks often lack coverage in vertical domains and offer limited
insights into the Chinese working context. Leveraging qualification exams as a
unified framework for human expertise evaluation, we introduce QualBench, the
first multi-domain Chinese QA benchmark dedicated to localized assessment of
Chinese LLMs. The dataset includes over 17,000 questions across six vertical
domains, with data selections grounded in 24 Chinese qualifications to closely
align with national policies and working standards. Through comprehensive
evaluation, the Qwen2.5 model outperformed the more advanced GPT-4o, with
Chinese LLMs consistently surpassing non-Chinese models, highlighting the
importance of localized domain knowledge in meeting qualification requirements.
The best performance of 75.26% reveals the current gaps in domain coverage
within model capabilities. Furthermore, we present the failure of LLM
collaboration with crowdsourcing mechanisms and suggest the opportunities for
multi-domain RAG knowledge enhancement and vertical domain LLM training with
Federated Learning.

</details>


### [40] [T-T: Table Transformer for Tagging-based Aspect Sentiment Triplet Extraction](https://arxiv.org/pdf/2505.05271)
*Kun Peng, Chaodong Tong, Cong Cao, Hao Peng, Qian Li, Guanlin Wu, Lei Jiang, Yanbing Liu, Philip S. Yu*

Main category: cs.CL

TL;DR: The paper introduces Table-Transformer (T-T) for aspect sentiment triplet extraction (ASTE), addressing challenges of long sequences and unfair attention in transformers with a stripe attention mechanism and loop-shift strategy.


<details>
  <summary>Details</summary>
Motivation: Transformers' strong semantic modeling could improve ASTE, but their direct use faces challenges like long sequences and unfair attention.

Method: Proposes T-T with stripe attention and loop-shift to modify global attention and enable window interactions.

Result: T-T achieves state-of-the-art performance with lower computational costs.

Conclusion: The T-T module effectively addresses transformer challenges in ASTE, offering superior performance and efficiency.

Abstract: Aspect sentiment triplet extraction (ASTE) aims to extract triplets composed
of aspect terms, opinion terms, and sentiment polarities from given sentences.
The table tagging method is a popular approach to addressing this task, which
encodes a sentence into a 2-dimensional table, allowing for the tagging of
relations between any two words. Previous efforts have focused on designing
various downstream relation learning modules to better capture interactions
between tokens in the table, revealing that a stronger capability to capture
relations can lead to greater improvements in the model. Motivated by this, we
attempt to directly utilize transformer layers as downstream relation learning
modules. Due to the powerful semantic modeling capability of transformers, it
is foreseeable that this will lead to excellent improvement. However, owing to
the quadratic relation between the length of the table and the length of the
input sentence sequence, using transformers directly faces two challenges:
overly long table sequences and unfair local attention interaction. To address
these challenges, we propose a novel Table-Transformer (T-T) for the
tagging-based ASTE method. Specifically, we introduce a stripe attention
mechanism with a loop-shift strategy to tackle these challenges. The former
modifies the global attention mechanism to only attend to a 2-dimensional local
attention window, while the latter facilitates interaction between different
attention windows. Extensive and comprehensive experiments demonstrate that the
T-T, as a downstream relation learning module, achieves state-of-the-art
performance with lower computational costs.

</details>


### [41] [ICon: In-Context Contribution for Automatic Data Selection](https://arxiv.org/pdf/2505.05327)
*Yixin Yang, Qingxiu Dong, Linli Yao, Fangwei Zhu, Zhifang Sui*

Main category: cs.CL

TL;DR: ICon is a gradient-free method for data selection in LLM instruction tuning, leveraging in-context learning to measure sample contribution efficiently. It outperforms existing methods, achieving better performance with less data.


<details>
  <summary>Details</summary>
Motivation: Existing data selection methods for LLMs are either computationally expensive (gradient-based) or rely on manual heuristics, limiting their effectiveness. ICon aims to address these limitations.

Method: ICon uses in-context learning to implicitly measure sample contribution without gradients or manual indicators. It consists of three components to assess performance shifts under implicit learning.

Result: Experiments show ICon-selected data (15%) outperforms full datasets by 5.42% and other methods by 2.06% on LLaMA3.1-8B. High-contribution samples are diverse and appropriately difficult.

Conclusion: ICon provides an efficient, effective alternative to gradient-based and heuristic methods for data selection in LLM instruction tuning.

Abstract: Data selection for instruction tuning is essential for improving the
performance of Large Language Models (LLMs) and reducing training cost.
However, existing automated selection methods either depend on computationally
expensive gradient-based measures or manually designed heuristics, which may
fail to fully exploit the intrinsic attributes of data. In this paper, we
propose In-context Learning for Contribution Measurement (ICon), a novel
gradient-free method that takes advantage of the implicit fine-tuning nature of
in-context learning (ICL) to measure sample contribution without gradient
computation or manual indicators engineering. ICon offers a computationally
efficient alternative to gradient-based methods and reduces human inductive
bias inherent in heuristic-based approaches. ICon comprises three components
and identifies high-contribution data by assessing performance shifts under
implicit learning through ICL. Extensive experiments on three LLMs across 12
benchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of
ICon. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICon-selected data
outperform full datasets by 5.42% points and exceed the best performance of
widely used selection methods by 2.06% points. We further analyze
high-contribution samples selected by ICon, which show both diverse tasks and
appropriate difficulty levels, rather than just the hardest ones.

</details>


### [42] [Frame In, Frame Out: Do LLMs Generate More Biased News Headlines than Humans?](https://arxiv.org/pdf/2505.05406)
*Valeria Pastorino, Nafise Sadat Moosavi*

Main category: cs.CL

TL;DR: LLMs in news generation show stronger framing biases than humans, especially in sensitive topics, varying by model. Mitigation strategies are needed.


<details>
  <summary>Details</summary>
Motivation: To investigate framing biases in LLM-generated news compared to human authors, given concerns about automated content creation.

Method: Analyzed framing in out-of-the-box and fine-tuned LLM-generated news, focusing on politically and socially sensitive contexts.

Result: LLMs exhibit more pronounced framing than humans, with significant variation across models. Some models show higher biases.

Conclusion: Post-training mitigation and stricter evaluation are needed to ensure balanced automated news reporting.

Abstract: Framing in media critically shapes public perception by selectively
emphasizing some details while downplaying others. With the rise of large
language models in automated news and content creation, there is growing
concern that these systems may introduce or even amplify framing biases
compared to human authors. In this paper, we explore how framing manifests in
both out-of-the-box and fine-tuned LLM-generated news content. Our analysis
reveals that, particularly in politically and socially sensitive contexts, LLMs
tend to exhibit more pronounced framing than their human counterparts. In
addition, we observe significant variation in framing tendencies across
different model architectures, with some models displaying notably higher
biases. These findings point to the need for effective post-training mitigation
strategies and tighter evaluation frameworks to ensure that automated news
content upholds the standards of balanced reporting.

</details>


### [43] [Crosslingual Reasoning through Test-Time Scaling](https://arxiv.org/pdf/2505.05408)
*Zheng-Xin Yong, M. Farid Adilazuarda, Jonibek Mansurov, Ruochen Zhang, Niklas Muennighoff, Carsten Eickhoff, Genta Indra Winata, Julia Kreutzer, Stephen H. Bach, Alham Fikri Aji*

Main category: cs.CL

TL;DR: English-centric reasoning finetuning with long chain-of-thoughts (CoTs) improves multilingual mathematical reasoning, especially in high-resource languages, but struggles with low-resource languages and out-of-domain contexts.


<details>
  <summary>Details</summary>
Motivation: To investigate the generalization of English reasoning finetuning across languages and its effectiveness in multilingual settings.

Method: Scaling up inference compute for English-centric RLMs, analyzing their CoT patterns, and controlling the language of reasoning.

Result: Improved multilingual reasoning, especially in high-resource languages, but poor out-of-domain generalization and challenges in low-resource languages.

Conclusion: Practitioners should use English-centric RLMs in high-resource languages, but further work is needed for low-resource languages and out-of-domain reasoning.

Abstract: Reasoning capabilities of large language models are primarily studied for
English, even when pretrained models are multilingual. In this work, we
investigate to what extent English reasoning finetuning with long
chain-of-thoughts (CoTs) can generalize across languages. First, we find that
scaling up inference compute for English-centric reasoning language models
(RLMs) improves multilingual mathematical reasoning across many languages
including low-resource languages, to an extent where they outperform models
twice their size. Second, we reveal that while English-centric RLM's CoTs are
naturally predominantly English, they consistently follow a quote-and-think
pattern to reason about quoted non-English inputs. Third, we discover an
effective strategy to control the language of long CoT reasoning, and we
observe that models reason better and more efficiently in high-resource
languages. Finally, we observe poor out-of-domain reasoning generalization, in
particular from STEM to cultural commonsense knowledge, even for English.
Overall, we demonstrate the potentials, study the mechanisms and outline the
limitations of crosslingual generalization of English reasoning test-time
scaling. We conclude that practitioners should let English-centric RLMs reason
in high-resource languages, while further work is needed to improve reasoning
in low-resource languages and out-of-domain contexts.

</details>


### [44] [Reasoning Models Don't Always Say What They Think](https://arxiv.org/pdf/2505.05410)
*Yanda Chen, Joe Benton, Ansh Radhakrishnan, Jonathan Uesato, Carson Denison, John Schulman, Arushi Somani, Peter Hase, Misha Wagner, Fabien Roger, Vlad Mikulik, Samuel R. Bowman, Jan Leike, Jared Kaplan, Ethan Perez*

Main category: cs.CL

TL;DR: Chain-of-thought (CoT) monitoring is promising for AI safety but insufficient to fully detect undesired behaviors, as CoTs often don't faithfully represent models' reasoning.


<details>
  <summary>Details</summary>
Motivation: To assess the faithfulness of CoT in representing models' actual reasoning processes and its implications for AI safety.

Method: Evaluated CoT faithfulness across 6 reasoning hints in state-of-the-art models, analyzing reveal rates and the impact of reinforcement learning.

Result: CoTs reveal hint usage in 1-20% of cases; reinforcement learning improves faithfulness but plateaus, and reward hacking doesn't increase verbalization.

Conclusion: CoT monitoring is useful but not reliable for catching rare, catastrophic behaviors, especially when CoT reasoning isn't necessary.

Abstract: Chain-of-thought (CoT) offers a potential boon for AI safety as it allows
monitoring a model's CoT to try to understand its intentions and reasoning
processes. However, the effectiveness of such monitoring hinges on CoTs
faithfully representing models' actual reasoning processes. We evaluate CoT
faithfulness of state-of-the-art reasoning models across 6 reasoning hints
presented in the prompts and find: (1) for most settings and models tested,
CoTs reveal their usage of hints in at least 1% of examples where they use the
hint, but the reveal rate is often below 20%, (2) outcome-based reinforcement
learning initially improves faithfulness but plateaus without saturating, and
(3) when reinforcement learning increases how frequently hints are used (reward
hacking), the propensity to verbalize them does not increase, even without
training against a CoT monitor. These results suggest that CoT monitoring is a
promising way of noticing undesired behaviors during training and evaluations,
but that it is not sufficient to rule them out. They also suggest that in
settings like ours where CoT reasoning is not necessary, test-time monitoring
of CoTs is unlikely to reliably catch rare and catastrophic unexpected
behaviors.

</details>


### [45] [TransProQA: an LLM-based literary Translation evaluation metric with Professional Question Answering](https://arxiv.org/pdf/2505.05423)
*Ran Zhang, Wei Zhao, Lieve Macken, Steffen Eger*

Main category: cs.CL

TL;DR: TransProQA is a reference-free, LLM-based QA framework for literary translation evaluation, outperforming current metrics by integrating professional translator insights.


<details>
  <summary>Details</summary>
Motivation: Existing metrics prioritize mechanical accuracy over artistic expression, risking translation quality and cultural authenticity.

Method: TransProQA integrates insights from professional translators, focusing on literary devices, cultural understanding, and authorial voice.

Result: TransProQA outperforms SOTA metrics, achieving up to 0.07 gain in correlation and surpassing adequacy assessments by over 15 points.

Conclusion: TransProQA approaches human-level performance, is applicable to open-source models, and serves as a valuable tool for literary evaluation.

Abstract: The impact of Large Language Models (LLMs) has extended into literary
domains. However, existing evaluation metrics prioritize mechanical accuracy
over artistic expression and tend to overrate machine translation (MT) as being
superior to experienced professional human translation. In the long run, this
bias could result in a permanent decline in translation quality and cultural
authenticity. In response to the urgent need for a specialized literary
evaluation metric, we introduce TransProQA, a novel, reference-free, LLM-based
question-answering (QA) framework designed specifically for literary
translation evaluation. TransProQA uniquely integrates insights from
professional literary translators and researchers, focusing on critical
elements in literary quality assessment such as literary devices, cultural
understanding, and authorial voice. Our extensive evaluation shows that while
literary-finetuned XCOMET-XL yields marginal gains, TransProQA substantially
outperforms current metrics, achieving up to 0.07 gain in correlation (ACC-EQ
and Kendall's tau) and surpassing the best state-of-the-art (SOTA) metrics by
over 15 points in adequacy assessments. Incorporating professional translator
insights as weights further improves performance, highlighting the value of
translator inputs. Notably, TransProQA approaches human-level evaluation
performance comparable to trained linguistic annotators. It demonstrates broad
applicability to open-source models such as LLaMA3.3-70b and Qwen2.5-32b,
indicating its potential as an accessible and training-free literary evaluation
metric and a valuable tool for evaluating texts that require local processing
due to copyright or ethical considerations.

</details>


### [46] [Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data](https://arxiv.org/pdf/2505.05427)
*Yudong Wang, Zixuan Fu, Jie Cai, Peijun Tang, Hongya Lyu, Yewei Fang, Zhi Zheng, Jie Zhou, Guoyang Zeng, Chaojun Xiao, Xu Han, Zhiyuan Liu*

Main category: cs.CL

TL;DR: The paper introduces an efficient data filtering pipeline for LLMs, addressing challenges in data verification and seed data selection, resulting in the high-quality Ultra-FineWeb dataset and improved model performance.


<details>
  <summary>Details</summary>
Motivation: Data quality is critical for LLM performance, but current methods lack efficient verification and objective seed data selection.

Method: Proposes a verification strategy for rapid data impact evaluation and optimizes seed data selection using a lightweight classifier (fastText).

Result: Created Ultra-FineWeb dataset (1T English, 120B Chinese tokens), leading to significant LLM performance improvements.

Conclusion: The pipeline enhances data quality, training efficiency, and model performance, validated by empirical results.

Abstract: Data quality has become a key factor in enhancing model performance with the
rapid development of large language models (LLMs). Model-driven data filtering
has increasingly become a primary approach for acquiring high-quality data.
However, it still faces two main challenges: (1) the lack of an efficient data
verification strategy makes it difficult to provide timely feedback on data
quality; and (2) the selection of seed data for training classifiers lacks
clear criteria and relies heavily on human expertise, introducing a degree of
subjectivity. To address the first challenge, we introduce an efficient
verification strategy that enables rapid evaluation of the impact of data on
LLM training with minimal computational cost. To tackle the second challenge,
we build upon the assumption that high-quality seed data is beneficial for LLM
training, and by integrating the proposed verification strategy, we optimize
the selection of positive and negative samples and propose an efficient data
filtering pipeline. This pipeline not only improves filtering efficiency,
classifier quality, and robustness, but also significantly reduces experimental
and inference costs. In addition, to efficiently filter high-quality data, we
employ a lightweight classifier based on fastText, and successfully apply the
filtering pipeline to two widely-used pre-training corpora, FineWeb and Chinese
FineWeb datasets, resulting in the creation of the higher-quality Ultra-FineWeb
dataset. Ultra-FineWeb contains approximately 1 trillion English tokens and 120
billion Chinese tokens. Empirical results demonstrate that the LLMs trained on
Ultra-FineWeb exhibit significant performance improvements across multiple
benchmark tasks, validating the effectiveness of our pipeline in enhancing both
data quality and training efficiency.

</details>


### [47] [clem:todd: A Framework for the Systematic Benchmarking of LLM-Based Task-Oriented Dialogue System Realisations](https://arxiv.org/pdf/2505.05445)
*Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen*

Main category: cs.CL

TL;DR: The paper introduces clem todd, a flexible framework for evaluating dialogue systems consistently, enabling benchmarking across user simulators and system designs.


<details>
  <summary>Details</summary>
Motivation: Existing research evaluates dialogue components in isolation, limiting generalizability. clem todd addresses this by providing a unified evaluation setup.

Method: The framework supports plug-and-play integration of user simulators and dialogue systems, ensuring uniform datasets, metrics, and constraints.

Result: Re-evaluation of existing systems and integration of new ones provided insights into architecture, scale, and prompting effects on performance.

Conclusion: clem todd offers practical guidance for building efficient conversational AI systems by enabling systematic evaluation.

Abstract: The emergence of instruction-tuned large language models (LLMs) has advanced
the field of dialogue systems, enabling both realistic user simulations and
robust multi-turn conversational agents. However, existing research often
evaluates these components in isolation-either focusing on a single user
simulator or a specific system design-limiting the generalisability of insights
across architectures and configurations. In this work, we propose clem todd
(chat-optimized LLMs for task-oriented dialogue systems development), a
flexible framework for systematically evaluating dialogue systems under
consistent conditions. clem todd enables detailed benchmarking across
combinations of user simulators and dialogue systems, whether existing models
from literature or newly developed ones. It supports plug-and-play integration
and ensures uniform datasets, evaluation metrics, and computational
constraints. We showcase clem todd's flexibility by re-evaluating existing
task-oriented dialogue systems within this unified setup and integrating three
newly proposed dialogue systems into the same evaluation pipeline. Our results
provide actionable insights into how architecture, scale, and prompting
strategies affect dialogue performance, offering practical guidance for
building efficient and effective conversational AI systems.

</details>


### [48] [UKElectionNarratives: A Dataset of Misleading Narratives Surrounding Recent UK General Elections](https://arxiv.org/pdf/2505.05459)
*Fatima Haouari, Carolina Scarton, Nicolò Faggiani, Nikolaos Nikolaidis, Bonka Kotseva, Ibrahim Abu Farha, Jens Linge, Kalina Bontcheva*

Main category: cs.CL

TL;DR: The paper introduces a taxonomy and dataset (UKElectionNarratives) for detecting misleading narratives in elections, benchmarks LLMs like GPT-4o, and suggests future research directions.


<details>
  <summary>Details</summary>
Motivation: Misleading narratives influence voter perception, necessitating accurate detection methods.

Method: Developed a taxonomy of misleading narratives, created a human-annotated dataset, and benchmarked pre-trained and large language models (e.g., GPT-4o).

Result: Produced UKElectionNarratives dataset and evaluated model performance in detecting misleading narratives.

Conclusion: Highlights the importance of detecting misleading narratives and provides tools (taxonomy, dataset) for future research.

Abstract: Misleading narratives play a crucial role in shaping public opinion during
elections, as they can influence how voters perceive candidates and political
parties. This entails the need to detect these narratives accurately. To
address this, we introduce the first taxonomy of common misleading narratives
that circulated during recent elections in Europe. Based on this taxonomy, we
construct and analyse UKElectionNarratives: the first dataset of
human-annotated misleading narratives which circulated during the UK General
Elections in 2019 and 2024. We also benchmark Pre-trained and Large Language
Models (focusing on GPT-4o), studying their effectiveness in detecting
election-related misleading narratives. Finally, we discuss potential use cases
and make recommendations for future research directions using the proposed
codebook and dataset.

</details>


### [49] [Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging](https://arxiv.org/pdf/2505.05464)
*Shiqi Chen, Jinghan Zhang, Tongyao Zhu, Wei Liu, Siyang Gao, Miao Xiong, Manling Li, Junxian He*

Main category: cs.CL

TL;DR: The paper explores merging Vision-Language Models (VLMs) and Large Language Models (LLMs) to combine perception and reasoning, revealing insights into their internal mechanisms.


<details>
  <summary>Details</summary>
Motivation: To understand how perception and reasoning can be combined in VLMs and LLMs, and to explore model merging as a tool for this integration.

Method: Proposes merging models across modalities (VLMs and LLMs) to transfer reasoning abilities without training, and analyzes the merged models' internal mechanisms.

Result: Merging successfully transfers reasoning from LLMs to VLMs; perception is encoded in early layers, while reasoning involves middle-to-late layers, with merging redistributing reasoning contributions.

Conclusion: Model merging is a promising tool for multimodal integration and interpretation, with potential for further exploration.

Abstract: Vision-Language Models (VLMs) combine visual perception with the general
capabilities, such as reasoning, of Large Language Models (LLMs). However, the
mechanisms by which these two abilities can be combined and contribute remain
poorly understood. In this work, we explore to compose perception and reasoning
through model merging that connects parameters of different models. Unlike
previous works that often focus on merging models of the same kind, we propose
merging models across modalities, enabling the incorporation of the reasoning
capabilities of LLMs into VLMs. Through extensive experiments, we demonstrate
that model merging offers a successful pathway to transfer reasoning abilities
from LLMs to VLMs in a training-free manner. Moreover, we utilize the merged
models to understand the internal mechanism of perception and reasoning and how
merging affects it. We find that perception capabilities are predominantly
encoded in the early layers of the model, whereas reasoning is largely
facilitated by the middle-to-late layers. After merging, we observe that all
layers begin to contribute to reasoning, whereas the distribution of perception
abilities across layers remains largely unchanged. These observations shed
light on the potential of model merging as a tool for multimodal integration
and interpretation.

</details>


### [50] [ComPO: Preference Alignment via Comparison Oracles](https://arxiv.org/pdf/2505.05465)
*Peter Chen, Xi Chen, Wotao Yin, Tianyi Lin*

Main category: cs.CL

TL;DR: The paper proposes a new preference alignment method for LLMs to address verbosity and likelihood displacement issues caused by noisy preference pairs, with experimental validation across multiple models and benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing direct alignment methods for LLMs suffer from verbosity and likelihood displacement due to noisy preference pairs, necessitating a specialized approach.

Method: A new preference alignment method based on comparison oracles is introduced, with convergence guarantees and heuristic improvements for practical use.

Result: Experiments on models like Mistral-7B and benchmarks (AlpacaEval 2) show the method's effectiveness in addressing limitations of existing approaches.

Conclusion: The work highlights the need for specialized methods for preference pairs with distinct likelihood margins, complementing recent findings in the field.

Abstract: Direct alignment methods are increasingly used for aligning large language
models (LLMs) with human preferences. However, these methods suffer from the
issues of verbosity and likelihood displacement, which can be driven by the
noisy preference pairs that induce similar likelihood for preferred and
dispreferred responses. The contributions of this paper are two-fold. First, we
propose a new preference alignment method based on comparison oracles and
provide the convergence guarantee for its basic scheme. Second, we improve our
method using some heuristics and conduct the experiments to demonstrate the
flexibility and compatibility of practical scheme in improving the performance
of LLMs using noisy preference pairs. Evaluations are conducted across multiple
base and instruction-tuned models (Mistral-7B, Llama-3-8B and Gemma-2-9B) with
benchmarks (AlpacaEval 2, MT-Bench and Arena-Hard). Experimental results show
the effectiveness of our method as an alternative to addressing the limitations
of existing direct alignment methods. A highlight of our work is that we
evidence the importance of designing specialized methods for preference pairs
with distinct likelihood margin, which complements the recent findings in
\citet{Razin-2025-Unintentional}.

</details>


### [51] [Position: AI Evaluation Should Learn from How We Test Humans](https://arxiv.org/pdf/2306.10512)
*Yan Zhuang, Qi Liu, Zachary A. Pardos, Patrick C. Kyllonen, Jiyun Zu, Zhenya Huang, Shijin Wang, Enhong Chen*

Main category: cs.CL

TL;DR: The paper advocates for shifting from static AI evaluation methods to adaptive testing, inspired by human psychometrics, to address limitations like high costs and data contamination.


<details>
  <summary>Details</summary>
Motivation: Current static evaluation methods for AI systems have limitations such as high costs, data contamination, and reliability issues due to low-quality test items.

Method: Proposes adaptive testing, estimating the value of each test item and tailoring evaluations dynamically, inspired by psychometrics.

Result: Adaptive testing offers robust ability estimation and reveals latent traits in AI models, improving evaluation reliability and efficiency.

Conclusion: Psychometrics, traditionally used for human assessment, can effectively address challenges in AI evaluation, making adaptive testing a promising paradigm.

Abstract: As AI systems continue to evolve, their rigorous evaluation becomes crucial
for their development and deployment. Researchers have constructed various
large-scale benchmarks to determine their capabilities, typically against a
gold-standard test set and report metrics averaged across all items. However,
this static evaluation paradigm increasingly shows its limitations, including
high evaluation costs, data contamination, and the impact of low-quality or
erroneous items on evaluation reliability and efficiency. In this Position,
drawing from human psychometrics, we discuss a paradigm shift from static
evaluation methods to adaptive testing. This involves estimating the
characteristics or value of each test item in the benchmark, and tailoring each
model's evaluation instead of relying on a fixed test set. This paradigm
provides robust ability estimation, uncovering the latent traits underlying a
model's observed scores. This position paper analyze the current possibilities,
prospects, and reasons for adopting psychometrics in AI evaluation. We argue
that psychometrics, a theory originating in the 20th century for human
assessment, could be a powerful solution to the challenges in today's AI
evaluations.

</details>


### [52] [DEGAP: Dual Event-Guided Adaptive Prefixes for Templated-Based Event Argument Extraction with Slot Querying](https://arxiv.org/pdf/2405.13325)
*Guanghui Wang, Dexi Liu, Jian-Yun Nie, Qizhi Wan, Rong Hu, Xiping Liu, Wanlong Liu, Jiaming Liu*

Main category: cs.CL

TL;DR: DEGAP improves event argument extraction by using dual prefixes and an adaptive gating mechanism, avoiding retrieval issues and leveraging event relationships.


<details>
  <summary>Details</summary>
Motivation: Address challenges of irrelevant retrieval results and independent template development in event argument extraction.

Method: Proposes DEGAP with dual prefixes (instance-oriented and template-oriented) and an event-guided adaptive gating mechanism.

Result: Achieves state-of-the-art performance on ACE05, RAMS, WIKIEVENTS, and MLEE datasets.

Conclusion: DEGAP effectively enhances EAE by leveraging event relationships and adaptive gating, outperforming existing methods.

Abstract: Recent advancements in event argument extraction (EAE) involve incorporating
useful auxiliary information into models during training and inference, such as
retrieved instances and event templates. These methods face two challenges: (1)
the retrieval results may be irrelevant and (2) templates are developed
independently for each event without considering their possible relationship.
In this work, we propose DEGAP to address these challenges through a simple yet
effective components: dual prefixes, i.e. learnable prompt vectors, where the
instance-oriented prefix and template-oriented prefix are trained to learn
information from different event instances and templates. Additionally, we
propose an event-guided adaptive gating mechanism, which can adaptively
leverage possible connections between different events and thus capture
relevant information from the prefix. Finally, these event-guided prefixes
provide relevant information as cues to EAE model without retrieval. Extensive
experiments demonstrate that our method achieves new state-of-the-art
performance on four datasets (ACE05, RAMS, WIKIEVENTS, and MLEE). Further
analysis shows the impact of different components.

</details>


### [53] [Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon](https://arxiv.org/pdf/2406.17746)
*USVSN Sai Prashanth, Alvin Deng, Kyle O'Brien, Jyothir S V, Mohammad Aflah Khan, Jaydeep Borkar, Christopher A. Choquette-Choo, Jacob Ray Fuehne, Stella Biderman, Tracy Ke, Katherine Lee, Naomi Saphra*

Main category: cs.CL

TL;DR: The paper proposes a taxonomy for memorization in language models, categorizing it into recitation, reconstruction, and recollection, and uses this to predict memorization likelihood.


<details>
  <summary>Details</summary>
Motivation: To address the oversimplification of memorization as a homogenous phenomenon by considering specific factors of memorized data.

Method: Develops a taxonomy of memorization (recitation, reconstruction, recollection) and constructs a predictive model to analyze memorization likelihood.

Result: Finds that different factors influence memorization differently based on the taxonomic category.

Conclusion: The taxonomy provides a nuanced understanding of memorization, aiding in better predictive modeling.

Abstract: Memorization in language models is typically treated as a homogenous
phenomenon, neglecting the specifics of the memorized data. We instead model
memorization as the effect of a set of complex factors that describe each
sample and relate it to the model and corpus. To build intuition around these
factors, we break memorization down into a taxonomy: recitation of highly
duplicated sequences, reconstruction of inherently predictable sequences, and
recollection of sequences that are neither. We demonstrate the usefulness of
our taxonomy by using it to construct a predictive model for memorization. By
analyzing dependencies and inspecting the weights of the predictive model, we
find that different factors influence the likelihood of memorization
differently depending on the taxonomic category.

</details>


### [54] [Scaling Synthetic Data Creation with 1,000,000,000 Personas](https://arxiv.org/pdf/2406.20094)
*Tao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, Dong Yu*

Main category: cs.CL

TL;DR: A novel persona-driven data synthesis method using 1 billion diverse personas (Persona Hub) to create scalable, diverse synthetic data for various applications.


<details>
  <summary>Details</summary>
Motivation: To leverage diverse perspectives within LLMs for generating high-quality synthetic data at scale, addressing limitations in current data synthesis methods.

Method: Introduces Persona Hub, a collection of 1 billion personas curated from web data, to tap into LLM knowledge and create diverse synthetic data.

Result: Demonstrated versatility in synthesizing mathematical problems, user prompts, knowledge-rich texts, NPCs, and tools, proving scalability and flexibility.

Conclusion: Persona-driven data synthesis is a scalable, flexible, and impactful approach, potentially transforming synthetic data creation and LLM applications.

Abstract: We propose a novel persona-driven data synthesis methodology that leverages
various perspectives within a large language model (LLM) to create diverse
synthetic data. To fully exploit this methodology at scale, we introduce
Persona Hub -- a collection of 1 billion diverse personas automatically curated
from web data. These 1 billion personas (~13% of the world's total population),
acting as distributed carriers of world knowledge, can tap into almost every
perspective encapsulated within the LLM, thereby facilitating the creation of
diverse synthetic data at scale for various scenarios. By showcasing Persona
Hub's use cases in synthesizing high-quality mathematical and logical reasoning
problems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs
and tools (functions) at scale, we demonstrate persona-driven data synthesis is
versatile, scalable, flexible, and easy to use, potentially driving a paradigm
shift in synthetic data creation and applications in practice, which may have a
profound impact on LLM research and development.

</details>


### [55] [Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant](https://arxiv.org/pdf/2409.11055)
*Jemin Lee, Sihyeong Park, Jinse Kwon, Jihun Oh, Yongin Kwon*

Main category: cs.CL

TL;DR: The paper evaluates quantization methods for instruction-tuned language models (1B-405B parameters) across 13 datasets, revealing trade-offs in performance, robustness, and task-specific impacts.


<details>
  <summary>Details</summary>
Motivation: Prior work lacks comprehensive evaluation of recent models like Llama-3.3 and focuses narrowly on perplexity or basic tasks. This study aims to fill that gap.

Method: Four quantization methods are applied to instruction-tuned models (1B-405B parameters) and evaluated across 13 datasets, including MT-Bench for coding/STEM tasks.

Result: Quantized models outperform smaller FP16 baselines but struggle with instruction-following and hallucination. FP8 is robust, AWQ excels in weight-only quantization, and larger models handle 4-bit quantization better.

Conclusion: Quantization amplifies inherent model weaknesses rather than task difficulty. Performance declines in coding/STEM tasks, but reasoning may improve.

Abstract: Quantization has gained attention as a promising solution for the
cost-effective deployment of large and small language models. However, most
prior work has been limited to perplexity or basic knowledge tasks and lacks a
comprehensive evaluation of recent models like Llama-3.3. In this paper, we
conduct a comprehensive evaluation of instruction-tuned models spanning 1B to
405B parameters, applying four quantization methods across 13 datasets. Our
findings reveal that (1) quantized models generally surpass smaller FP16
baselines, yet they often struggle with instruction-following and hallucination
detection; (2) FP8 consistently emerges as the most robust option across tasks,
and AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller
models can suffer severe accuracy drops at 4-bit quantization, while 70B-scale
models maintain stable performance; (4) notably, \textit{hard} tasks do not
always experience the largest accuracy losses, indicating that quantization
magnifies a model's inherent weaknesses rather than simply correlating with
task difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant
performance declines in coding and STEM tasks, though reasoning may sometimes
improve.

</details>


### [56] [To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning](https://arxiv.org/pdf/2409.12183)
*Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, Greg Durrett*

Main category: cs.CL

TL;DR: CoT improves performance on math/logic tasks but offers minimal gains elsewhere. Selective use of CoT can save costs, and new paradigms beyond CoT are needed.


<details>
  <summary>Details</summary>
Motivation: To determine the effectiveness of Chain-of-Thought (CoT) prompting across different tasks and identify where it adds value.

Method: Conducted a meta-analysis of 100+ CoT papers and evaluated 20 datasets across 14 models, analyzing CoT's impact on tasks with symbolic operations.

Result: CoT significantly benefits math/logic tasks but has little effect on others. It improves symbolic execution but underperforms compared to symbolic solvers.

Conclusion: CoT should be used selectively, and new methods beyond CoT are needed to leverage intermediate computation in LLMs.

Abstract: Chain-of-thought (CoT) via prompting is the de facto method for eliciting
reasoning capabilities from large language models (LLMs). But for what kinds of
tasks is this extra ``thinking'' really helpful? To analyze this, we conducted
a quantitative meta-analysis covering over 100 papers using CoT and ran our own
evaluations of 20 datasets across 14 models. Our results show that CoT gives
strong performance benefits primarily on tasks involving math or logic, with
much smaller gains on other types of tasks. On MMLU, directly generating the
answer without CoT leads to almost identical accuracy as CoT unless the
question or model's response contains an equals sign, indicating symbolic
operations and reasoning. Following this finding, we analyze the behavior of
CoT on these problems by separating planning and execution and comparing
against tool-augmented LLMs. Much of CoT's gain comes from improving symbolic
execution, but it underperforms relative to using a symbolic solver. Our
results indicate that CoT can be applied selectively, maintaining performance
while saving inference costs. Furthermore, they suggest a need to move beyond
prompt-based CoT to new paradigms that better leverage intermediate computation
across the whole range of LLM applications.

</details>


### [57] [Self-Correction is More than Refinement: A Learning Framework for Visual and Language Reasoning Tasks](https://arxiv.org/pdf/2410.04055)
*Jiayi He, Hehai Lin, Qingyun Wang, Yi Fung, Heng Ji*

Main category: cs.CL

TL;DR: The paper explores self-correction in Vision-Language Models (VLMs), introducing a Self-Correction Learning (SCL) approach using Direct Preference Optimization (DPO) to improve model performance without external feedback.


<details>
  <summary>Details</summary>
Motivation: VLMs often produce flawed responses, and while self-correction is promising, its application to VLMs, especially for visual and linguistic tasks, remains understudied.

Method: The study proposes SCL, where VLMs generate self-correction data (preferred/disfavored samples) during inference and use DPO for fine-tuning.

Result: VLMs struggle with iterative self-correction during inference but improve performance when fine-tuned on categorized self-generated data.

Conclusion: Self-correction should enhance reasoning abilities through training, enabling direct high-quality responses without refinement.

Abstract: While Vision-Language Models (VLMs) have shown remarkable abilities in visual
and language reasoning tasks, they invariably generate flawed responses.
Self-correction that instructs models to refine their outputs presents a
promising solution to this issue. Previous studies have mainly concentrated on
Large Language Models (LLMs), while the self-correction abilities of VLMs,
particularly concerning both visual and linguistic information, remain largely
unexamined. This study investigates the self-correction capabilities of VLMs
during both inference and fine-tuning stages. We introduce a Self-Correction
Learning (SCL) approach that enables VLMs to learn from their self-generated
self-correction data through Direct Preference Optimization (DPO) without
relying on external feedback, facilitating self-improvement. Specifically, we
collect preferred and disfavored samples based on the correctness of initial
and refined responses, which are obtained by two-turn self-correction with VLMs
during the inference stage. Experimental results demonstrate that although VLMs
struggle to self-correct effectively during iterative inference without
additional fine-tuning and external feedback, they can enhance their
performance and avoid previous mistakes through preference fine-tuning when
their self-generated self-correction data are categorized into preferred and
disfavored samples. This study emphasizes that self-correction is not merely a
refinement process; rather, it should enhance the reasoning abilities of models
through additional training, enabling them to generate high-quality responses
directly without further refinement.

</details>


### [58] [SAPIENT: Mastering Multi-turn Conversational Recommendation with Strategic Planning and Monte Carlo Tree Search](https://arxiv.org/pdf/2410.09580)
*Hanwen Du, Bo Peng, Xia Ning*

Main category: cs.CL

TL;DR: SAPIENT introduces a Monte Carlo Tree Search (MCTS)-based framework for Conversational Recommender Systems (CRS) to improve conversational planning and outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing RL-based CRS methods suffer from suboptimal conversational planning due to greedy action selection or sampling strategies.

Method: SAPIENT uses MCTS for conversational planning, with a self-training loop between a conversational agent (S-agent) and planner (S-planner).

Result: Experiments on four datasets show SAPIENT outperforms state-of-the-art baselines.

Conclusion: SAPIENT effectively enhances conversational planning in CRS and offers a trade-off variant for efficiency and performance.

Abstract: Conversational Recommender Systems (CRS) proactively engage users in
interactive dialogues to elicit user preferences and provide personalized
recommendations. Existing methods train Reinforcement Learning (RL)-based agent
with greedy action selection or sampling strategy, and may suffer from
suboptimal conversational planning. To address this, we present a novel Monte
Carlo Tree Search (MCTS)-based CRS framework SAPIENT. SAPIENT consists of a
conversational agent (S-agent) and a conversational planner (S-planner).
S-planner builds a conversational search tree with MCTS based on the initial
actions proposed by S-agent to find conversation plans. The best conversation
plans from S-planner are used to guide the training of S-agent, creating a
self-training loop where S-agent can iteratively improve its capability for
conversational planning. Furthermore, we propose an efficient variant SAPIENT
for trade-off between training efficiency and performance. Extensive
experiments on four benchmark datasets validate the effectiveness of our
approach, showing that SAPIENT outperforms the state-of-the-art baselines. Our
code and data are accessible through https://github.com/ninglab/SAPIENT.

</details>


### [59] [WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines](https://arxiv.org/pdf/2410.12705)
*Genta Indra Winata, Frederikus Hudi, Patrick Amadeus Irawan, David Anugraha, Rifki Afina Putri, Yutong Wang, Adam Nohejl, Ubaidillah Ariq Prathama, Nedjma Ousidhoum, Afifa Amriani, Anar Rzayev, Anirban Das, Ashmari Pramodya, Aulia Adila, Bryan Wilie, Candy Olivia Mawalim, Ching Lam Cheng, Daud Abolade, Emmanuele Chersoni, Enrico Santus, Fariz Ikhwantri, Garry Kuwanto, Hanyang Zhao, Haryo Akbarianto Wibowo, Holy Lovenia, Jan Christian Blaise Cruz, Jan Wira Gotama Putra, Junho Myung, Lucky Susanto, Maria Angelica Riera Machin, Marina Zhukova, Michael Anugraha, Muhammad Farid Adilazuarda, Natasha Santosa, Peerat Limkonchotiwat, Raj Dabre, Rio Alexander Audino, Samuel Cahyawijaya, Shi-Xiong Zhang, Stephanie Yulia Salim, Yi Zhou, Yinxuan Gui, David Ifeoluwa Adelani, En-Shiun Annie Lee, Shogo Okada, Ayu Purwarianti, Alham Fikri Aji, Taro Watanabe, Derry Tanti Wijaya, Alice Oh, Chong-Wah Ngo*

Main category: cs.CL

TL;DR: WorldCuisines is a multilingual, multicultural VQA benchmark to evaluate VLMs' understanding of culture-specific knowledge, revealing their struggles with adversarial contexts and regional specifics.


<details>
  <summary>Details</summary>
Motivation: VLMs often lack culture-specific knowledge, especially in non-English languages and underrepresented contexts. WorldCuisines aims to address this gap.

Method: A VQA dataset with 1M text-image pairs across 30 languages/dialects, including dish identification and origin tasks, with training and evaluation datasets.

Result: VLMs perform better with location context but struggle with adversarial contexts and regional specifics.

Conclusion: WorldCuisines highlights VLMs' limitations in multicultural understanding and provides resources for future research.

Abstract: Vision Language Models (VLMs) often struggle with culture-specific knowledge,
particularly in languages other than English and in underrepresented cultural
contexts. To evaluate their understanding of such knowledge, we introduce
WorldCuisines, a massive-scale benchmark for multilingual and multicultural,
visually grounded language understanding. This benchmark includes a visual
question answering (VQA) dataset with text-image pairs across 30 languages and
dialects, spanning 9 language families and featuring over 1 million data
points, making it the largest multicultural VQA benchmark to date. It includes
tasks for identifying dish names and their origins. We provide evaluation
datasets in two sizes (12k and 60k instances) alongside a training dataset (1
million instances). Our findings show that while VLMs perform better with
correct location context, they struggle with adversarial contexts and
predicting specific regional cuisines and languages. To support future
research, we release a knowledge base with annotated food entries and images
along with the VQA data.

</details>


### [60] [E2E-AFG: An End-to-End Model with Adaptive Filtering for Retrieval-Augmented Generation](https://arxiv.org/pdf/2411.00437)
*Yun Jiang, Zilong Xie, Wei Zhang, Yun Fang, Shuai Pan*

Main category: cs.CL

TL;DR: E2E-AFG improves retrieval-augmented generation by integrating adaptive filtering, answer existence judgment, and text generation into one framework, outperforming baselines on six datasets.


<details>
  <summary>Details</summary>
Motivation: Current retrieval-augmented generation methods often retrieve low-quality content, leading to irrelevant or misleading information in outputs.

Method: Proposes E2E-AFG, an end-to-end model with adaptive filtering, combining answer existence judgment and text generation.

Result: E2E-AFG consistently outperforms baseline models across six knowledge-intensive language datasets.

Conclusion: The approach is effective and robust, enhancing generation accuracy by focusing on relevant content.

Abstract: Retrieval-augmented generation methods often neglect the quality of content
retrieved from external knowledge bases, resulting in irrelevant information or
potential misinformation that negatively affects the generation results of
large language models. In this paper, we propose an end-to-end model with
adaptive filtering for retrieval-augmented generation (E2E-AFG), which
integrates answer existence judgment and text generation into a single
end-to-end framework. This enables the model to focus more effectively on
relevant content while reducing the influence of irrelevant information and
generating accurate answers. We evaluate E2E-AFG on six representative
knowledge-intensive language datasets, and the results show that it
consistently outperforms baseline models across all tasks, demonstrating the
effectiveness and robustness of the proposed approach.

</details>


### [61] [Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models](https://arxiv.org/pdf/2411.04996)
*Weixin Liang, Lili Yu, Liang Luo, Srinivasan Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis, Wen-tau Yih, Luke Zettlemoyer, Xi Victoria Lin*

Main category: cs.CL

TL;DR: Mixture-of-Transformers (MoT) reduces computational costs for multi-modal LLMs by decoupling parameters by modality, matching dense baseline performance with fewer FLOPs.


<details>
  <summary>Details</summary>
Motivation: Addressing the high computational demands of training multi-modal LLMs by introducing a more efficient architecture.

Method: MoT decouples non-embedding parameters (feed-forward networks, attention matrices, layer normalization) by modality, enabling modality-specific processing with global self-attention.

Result: MoT matches dense baseline performance with 55.8% FLOPs for text-and-image, 37.2% for speech, and outperforms in image generation with fewer FLOPs.

Conclusion: MoT is a practical, efficient solution for scaling multi-modal LLMs, reducing computational costs while maintaining performance.

Abstract: The development of large language models (LLMs) has expanded to multi-modal
systems capable of processing text, images, and speech within a unified
framework. Training these models demands significantly larger datasets and
computational resources compared to text-only LLMs. To address the scaling
challenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modal
transformer architecture that significantly reduces pretraining computational
costs. MoT decouples non-embedding parameters of the model by modality --
including feed-forward networks, attention matrices, and layer normalization --
enabling modality-specific processing with global self-attention over the full
input sequence. We evaluate MoT across multiple settings and model scales. In
the Chameleon 7B setting (autoregressive text-and-image generation), MoT
matches the dense baseline's performance using only 55.8\% of the FLOPs. When
extended to include speech, MoT reaches speech performance comparable to the
dense baseline with only 37.2\% of the FLOPs. In the Transfusion setting, where
text and image are trained with different objectives, a 7B MoT model matches
the image modality performance of the dense baseline with one third of the
FLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image
generation metrics. System profiling further highlights MoT's practical
benefits, achieving dense baseline image quality in 47.2\% of the wall-clock
time and text quality in 75.6\% of the wall-clock time (measured on AWS
p4de.24xlarge instances with NVIDIA A100 GPUs).

</details>


### [62] [LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models](https://arxiv.org/pdf/2501.00874)
*Hieu Man, Nghia Trung Ngo, Viet Dac Lai, Ryan A. Rossi, Franck Dernoncourt, Thien Huu Nguyen*

Main category: cs.CL

TL;DR: LUSIFER is a zero-shot approach adapting LLM-based embedding models for multilingual tasks without multilingual supervision, enhancing performance for medium and low-resource languages.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based embedding models focus on English, leaving multilingual capabilities underexplored.

Method: LUSIFER combines a multilingual encoder with an LLM-based embedding model using minimal trainable parameters to transfer language understanding.

Result: LUSIFER significantly improves multilingual performance across embedding tasks, especially for medium and low-resource languages.

Conclusion: LUSIFER effectively bridges the gap in multilingual embedding capabilities without requiring explicit multilingual training data.

Abstract: Recent advancements in large language models (LLMs) based embedding models
have established new state-of-the-art benchmarks for text embedding tasks,
particularly in dense vector-based retrieval. However, these models
predominantly focus on English, leaving multilingual embedding capabilities
largely unexplored. To address this limitation, we present LUSIFER, a novel
zero-shot approach that adapts LLM-based embedding models for multilingual
tasks without requiring multilingual supervision. LUSIFER's architecture
combines a multilingual encoder, serving as a language-universal learner, with
an LLM-based embedding model optimized for embedding-specific tasks. These
components are seamlessly integrated through a minimal set of trainable
parameters that act as a connector, effectively transferring the multilingual
encoder's language understanding capabilities to the specialized embedding
model. Additionally, to comprehensively evaluate multilingual embedding
performance, we introduce a new benchmark encompassing 5 primary embedding
tasks, 123 diverse datasets, and coverage across 14 languages. Extensive
experimental results demonstrate that LUSIFER significantly enhances the
multilingual performance across various embedding tasks, particularly for
medium and low-resource languages, without requiring explicit multilingual
training data.

</details>


### [63] [ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented Contextual Learning](https://arxiv.org/pdf/2501.01031)
*Wonduk Seo, Zonghao Yuan, Yi Bu*

Main category: cs.CL

TL;DR: ValuesRAG, a framework using Retrieval-Augmented Generation and In-Context Learning, improves cultural alignment in LLMs by dynamically integrating cultural knowledge, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing Western-centric biases in LLMs and their misrepresentations in cross-cultural applications, as current methods like role assignment and few-shot learning are ineffective.

Method: ValuesRAG leverages the World Values Survey dataset to generate value summaries, retrieves relevant summaries based on demographics, and reranks them for dynamic integration during text generation.

Result: ValuesRAG outperforms zero-shot, role-assignment, few-shot, and hybrid methods across 6 regional datasets, achieving the best performance.

Conclusion: Dynamic retrieval-based methods like ValuesRAG can bridge the gap between global LLM capabilities and localized cultural values, fostering inclusive AI systems.

Abstract: Ensuring cultural values alignment in Large Language Models (LLMs) remains a
critical challenge, as these models often embed Western-centric biases from
their training data, leading to misrepresentations and fairness concerns in
cross-cultural applications. Existing approaches such as role assignment and
few-shot learning struggle to address these limitations effectively due to
their reliance on pre-trained knowledge, limited scalability, and inability to
capture nuanced cultural values. To address these issues, we propose ValuesRAG,
a novel and effective framework that applies Retrieval-Augmented Generation
(RAG) with In-Context Learning (ICL) to integrate cultural and demographic
knowledge dynamically during text generation. Leveraging the World Values
Survey (WVS) dataset, ValuesRAG first generates summaries of values for each
individual. We subsequently curate several representative regional datasets to
serve as test datasets and retrieve relevant summaries of values based on
demographic features, followed by a reranking step to select the top-k relevant
summaries. We evaluate ValuesRAG using 6 diverse regional datasets and show
that it consistently outperforms baselines: including zero-shot,
role-assignment, few-shot, and hybrid methods, both in main experiments and
ablation settings. Notably, ValuesRAG achieves the best overall performance
over prior methods, demonstrating its effectiveness in fostering culturally
aligned and inclusive AI systems. Our findings underscore the potential of
dynamic retrieval-based methods to bridge the gap between global LLM
capabilities and localized cultural values.

</details>


### [64] [Communicating Activations Between Language Model Agents](https://arxiv.org/pdf/2501.14082)
*Vignav Ramesh, Kenneth Li*

Main category: cs.CL

TL;DR: LMs communicate via activations instead of natural language, improving performance and reducing compute costs.


<details>
  <summary>Details</summary>
Motivation: Natural language communication between LMs is costly and loses rich internal activation information.

Method: Pause an LM's computation, combine activations from another LM via a function, and continue decoding.

Result: Achieves up to 27.0% improvement over natural language with <1/4 compute.

Conclusion: Activations are a superior and robust alternative for inter-LM communication.

Abstract: Communication between multiple language model (LM) agents has been shown to
scale up the reasoning ability of LMs. While natural language has been the
dominant medium for inter-LM communication, it is not obvious this should be
the standard: not only does natural language communication incur high inference
costs that scale quickly with the number of both agents and messages, but also
the decoding process abstracts away too much rich information that could be
otherwise accessed from the internal activations. In this work, we propose a
simple technique whereby LMs communicate via activations; concretely, we pause
an LM $\textit{B}$'s computation at an intermediate layer, combine its current
activation with another LM $\textit{A}$'s intermediate activation via some
function $\textit{f}$, then pass $\textit{f}$'s output into the next layer of
$\textit{B}$ and continue the forward pass till decoding is complete. This
approach scales up LMs on new tasks with zero additional parameters and data,
and saves a substantial amount of compute over natural language communication.
We test our method with various functional forms $\textit{f}$ on two
experimental setups--multi-player coordination games and reasoning
benchmarks--and find that it achieves up to $27.0\%$ improvement over natural
language communication across datasets with $<$$1/4$ the compute, illustrating
the superiority and robustness of activations as an alternative "language" for
communication between LMs.

</details>


### [65] [Safety Evaluation of DeepSeek Models in Chinese Contexts](https://arxiv.org/pdf/2502.11137)
*Wenjing Zhang, Xuejiao Lei, Zhaoxiang Liu, Ning Wang, Zhenhong Long, Peijun Yang, Jiaojiao Zhao, Minjie Hua, Chaoyang Ma, Kai Wang, Shiguo Lian*

Main category: cs.CL

TL;DR: The paper introduces CHiSafetyBench, a benchmark to evaluate safety vulnerabilities of DeepSeek models in Chinese contexts, revealing significant deficiencies.


<details>
  <summary>Details</summary>
Motivation: Current safety evaluations of DeepSeek models focus mainly on English, leaving a gap for Chinese-specific assessments despite their robust performance in both languages.

Method: The study develops CHiSafetyBench, a Chinese-specific safety evaluation benchmark, to systematically assess DeepSeek-R1 and DeepSeek-V3.

Result: The benchmark quantifies safety deficiencies in both models in Chinese contexts, highlighting critical vulnerabilities.

Conclusion: The study underscores the need for improved safety assessments in Chinese and commits to refining the benchmark for more accurate future evaluations.

Abstract: Recently, the DeepSeek series of models, leveraging their exceptional
reasoning capabilities and open-source strategy, is reshaping the global AI
landscape. Despite these advantages, they exhibit significant safety
deficiencies. Research conducted by Robust Intelligence, a subsidiary of Cisco,
in collaboration with the University of Pennsylvania, revealed that DeepSeek-R1
has a 100\% attack success rate when processing harmful prompts. Additionally,
multiple safety companies and research institutions have confirmed critical
safety vulnerabilities in this model. As models demonstrating robust
performance in Chinese and English, DeepSeek models require equally crucial
safety assessments in both language contexts. However, current research has
predominantly focused on safety evaluations in English environments, leaving a
gap in comprehensive assessments of their safety performance in Chinese
contexts. In response to this gap, this study introduces CHiSafetyBench, a
Chinese-specific safety evaluation benchmark. This benchmark systematically
evaluates the safety of DeepSeek-R1 and DeepSeek-V3 in Chinese contexts,
revealing their performance across safety categories. The experimental results
quantify the deficiencies of these two models in Chinese contexts, providing
key insights for subsequent improvements. It should be noted that, despite our
efforts to establish a comprehensive, objective, and authoritative evaluation
benchmark, the selection of test samples, characteristics of data distribution,
and the setting of evaluation criteria may inevitably introduce certain biases
into the evaluation results. We will continuously optimize the evaluation
benchmark and periodically update this report to provide more comprehensive and
accurate assessment outcomes. Please refer to the latest version of the paper
for the most recent evaluation results and conclusions.

</details>


### [66] [Drift: Decoding-time Personalized Alignments with Implicit User Preferences](https://arxiv.org/pdf/2502.14289)
*Minbeom Kim, Kang-il Lee, Seongho Joo, Hwaran Lee, Thibaut Thonet, Kyomin Jung*

Main category: cs.CL

TL;DR: Drift is a training-free framework for personalizing LLMs using implicit user preferences, outperforming RLHF with minimal data.


<details>
  <summary>Details</summary>
Motivation: To achieve personalized alignments in LLMs without the need for extensive annotated data or costly gradient updates.

Method: Drift models user preferences as interpretable attributes and aligns them at decoding time, requiring only 50-100 examples.

Result: Drift significantly outperforms RLHF baselines on synthetic and real datasets, proving efficient and interpretable.

Conclusion: Drift offers a scalable, efficient, and interpretable solution for personalizing LLMs with minimal data.

Abstract: Personalized alignments for individual users have been a long-standing goal
in large language models (LLMs). We introduce Drift, a novel framework that
personalizes LLMs at decoding time with implicit user preferences. Traditional
Reinforcement Learning from Human Feedback (RLHF) requires thousands of
annotated examples and expensive gradient updates. In contrast, Drift
personalizes LLMs in a training-free manner, using only a few dozen examples to
steer a frozen model through efficient preference modeling. Our approach models
user preferences as a composition of predefined, interpretable attributes and
aligns them at decoding time to enable personalized generation. Experiments on
both a synthetic persona dataset (Perspective) and a real human-annotated
dataset (PRISM) demonstrate that Drift significantly outperforms RLHF baselines
while using only 50-100 examples. Our results and analysis show that Drift is
both computationally efficient and interpretable.

</details>


### [67] [Correctness Coverage Evaluation for Medical Multiple-Choice Question Answering Based on the Enhanced Conformal Prediction Framework](https://arxiv.org/pdf/2503.05505)
*Yusong Ke, Hongru Lin, Yuting Ruan, Junya Tang, Li Li*

Main category: cs.CL

TL;DR: An enhanced Conformal Prediction framework for medical MCQA tasks improves LLM trustworthiness by addressing hallucinations and nonfactual outputs, ensuring error rate guarantees and reduced prediction set size.


<details>
  <summary>Details</summary>
Motivation: LLMs in medical QA can produce unreliable information (hallucinations), risking trustworthiness in high-stakes tasks. Existing CP methods lack exploration in this domain.

Method: Proposes a CP framework integrating non-conformance scores with correct option frequency and self-consistency, plus a risk control strategy with a monotonic loss function.

Result: Tested on MedMCQA, MedQA, and MMLU datasets with four LLMs, the method meets error rate guarantees and reduces prediction set size as risk increases.

Conclusion: The framework provides a robust uncertainty metric for LLMs in medical QA, balancing reliability and efficiency.

Abstract: Large language models (LLMs) are increasingly adopted in medical
question-answering (QA) scenarios. However, LLMs can generate hallucinations
and nonfactual information, undermining their trustworthiness in high-stakes
medical tasks. Conformal Prediction (CP) provides a statistically rigorous
framework for marginal (average) coverage guarantees but has limited
exploration in medical QA. This paper proposes an enhanced CP framework for
medical multiple-choice question-answering (MCQA) tasks. By associating the
non-conformance score with the frequency score of correct options and
leveraging self-consistency, the framework addresses internal model opacity and
incorporates a risk control strategy with a monotonic loss function. Evaluated
on MedMCQA, MedQA, and MMLU datasets using four off-the-shelf LLMs, the
proposed method meets specified error rate guarantees while reducing average
prediction set size with increased risk level, offering a promising uncertainty
evaluation metric for LLMs.

</details>


### [68] [Large Language Models for Outpatient Referral: Problem Definition, Benchmarking and Challenges](https://arxiv.org/pdf/2503.08292)
*Xiaoxiao Liu, Qingying Xiao, Junying Chen, Xiangyi Feng, Xiangbo Wu, Bairui Zhang, Xiang Wan, Jian Chang, Guangjun Yu, Yan Hu, Benyou Wang*

Main category: cs.CL

TL;DR: The paper evaluates LLMs in outpatient referral tasks, proposing a static and dynamic evaluation framework. LLMs show limited advantages over BERT-like models but excel in interactive dialogues.


<details>
  <summary>Details</summary>
Motivation: There is a lack of standardized evaluation criteria for LLMs in dynamic, interactive healthcare referral tasks.

Method: Proposes a framework with static (predefined referrals) and dynamic (iterative dialogues) evaluation tasks.

Result: LLMs have limited advantages over BERT-like models but perform well in asking effective questions during dialogues.

Conclusion: The study highlights the need for tailored evaluation frameworks for LLMs in healthcare referrals, noting their potential in interactive scenarios.

Abstract: Large language models (LLMs) are increasingly applied to outpatient referral
tasks across healthcare systems. However, there is a lack of standardized
evaluation criteria to assess their effectiveness, particularly in dynamic,
interactive scenarios. In this study, we systematically examine the
capabilities and limitations of LLMs in managing tasks within Intelligent
Outpatient Referral (IOR) systems and propose a comprehensive evaluation
framework specifically designed for such systems. This framework comprises two
core tasks: static evaluation, which focuses on evaluating the ability of
predefined outpatient referrals, and dynamic evaluation, which evaluates
capabilities of refining outpatient referral recommendations through iterative
dialogues. Our findings suggest that LLMs offer limited advantages over
BERT-like models, but show promise in asking effective questions during
interactive dialogues.

</details>


### [69] [Atyaephyra at SemEval-2025 Task 4: Low-Rank Negative Preference Optimization](https://arxiv.org/pdf/2503.13690)
*Jan Bronec, Jindřich Helcl*

Main category: cs.CL

TL;DR: A method using negative preference optimization and low-rank adaptation for unlearning sensitive content in LLMs, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of unlearning sensitive content from large language models (LLMs) efficiently.

Method: Combines negative preference optimization with low-rank adaptation to compute regularization terms for stabilization.

Result: Significantly exceeds shared task baselines.

Conclusion: The approach is effective for unlearning sensitive content in LLMs.

Abstract: We present a submission to the SemEval 2025 shared task on unlearning
sensitive content from LLMs. Our approach employs negative preference
optimization using low-rank adaptation. We show that we can utilize this
combination to efficiently compute additional regularization terms, which help
with unlearning stabilization. The results of our approach significantly exceed
the shared task baselines.

</details>


### [70] [Benchmarking Open-Source Large Language Models on Healthcare Text Classification Tasks](https://arxiv.org/pdf/2503.15169)
*Yuting Guo, Abeed Sarker*

Main category: cs.CL

TL;DR: The study evaluates five open-source LLMs for healthcare classification tasks, finding DeepSeekV3 as the top performer. Performance varied by task domain, with social media data easier than clinical data. Model size didn't guarantee better results, emphasizing task-specific selection.


<details>
  <summary>Details</summary>
Motivation: To assess the performance of open-source LLMs in healthcare information extraction, addressing the need for effective AI tools in medical contexts.

Method: Evaluated five LLMs (GEMMA-3-27B-IT, LLAMA3-70B, LLAMA4-109B, DEEPSEEK-R1-DISTILL-LLAMA-70B, DEEPSEEK-V3-0324-UD-Q2_K_XL) on six healthcare classification tasks using precision, recall, and F1 scores.

Result: DeepSeekV3 performed best overall, excelling in four tasks. Models did better on social media than clinical data. GEMMA-3-27B-IT had high recall, while LLAMA4-109B underperformed despite its size.

Conclusion: Task-specific model selection is crucial in healthcare, considering domain and precision-recall needs, not just model size. Continued evaluation and adaptation of LLMs are necessary.

Abstract: The application of large language models (LLMs) to healthcare information
extraction has emerged as a promising approach. This study evaluates the
classification performance of five open-source LLMs: GEMMA-3-27B-IT,
LLAMA3-70B, LLAMA4-109B, DEEPSEEK-R1-DISTILL-LLAMA-70B, and
DEEPSEEK-V3-0324-UD-Q2_K_XL, across six healthcare-related classification tasks
involving both social media data (breast cancer, changes in medication regimen,
adverse pregnancy outcomes, potential COVID-19 cases) and clinical data (stigma
labeling, medication change discussion). We report precision, recall, and F1
scores with 95% confidence intervals for all model-task combinations. Our
findings reveal significant performance variability between LLMs, with
DeepSeekV3 emerging as the strongest overall performer, achieving the highest
F1 scores in four tasks. Notably, models generally performed better on social
media tasks compared to clinical data tasks, suggesting potential
domain-specific challenges. GEMMA-3-27B-IT demonstrated exceptionally high
recall despite its smaller parameter count, while LLAMA4-109B showed
surprisingly underwhelming performance compared to its predecessor LLAMA3-70B,
indicating that larger parameter counts do not guarantee improved
classification results. We observed distinct precision-recall trade-offs across
models, with some favoring sensitivity over specificity and vice versa. These
findings highlight the importance of task-specific model selection for
healthcare applications, considering the particular data domain and
precision-recall requirements rather than model size alone. As healthcare
increasingly integrates AI-driven text classification tools, this comprehensive
benchmarking provides valuable guidance for model selection and implementation
while underscoring the need for continued evaluation and domain adaptation of
LLMs in healthcare contexts.

</details>


### [71] [Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation](https://arxiv.org/pdf/2504.17480)
*Xin Yi, Shunfan Zheng, Linlin Wang, Xiaoling Wang, Liang He*

Main category: cs.CL

TL;DR: The paper introduces CDG-KD, a framework for bidirectional watermark attacks in unauthorized knowledge distillation, highlighting the need for robust and unforgeable watermarks.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored robustness and unforgeability of watermarks in LLMs against scrubbing and spoofing attacks under unauthorized knowledge distillation.

Method: Proposes CDG-KD, using contrastive decoding to extract corrupted/amplified watermarks and bidirectional distillation for watermark removal/forgery.

Result: CDG-KD effectively performs attacks while maintaining the distilled model's performance.

Conclusion: Emphasizes the necessity for developing more robust and unforgeable watermarking schemes.

Abstract: Watermarking has emerged as a critical technique for combating misinformation
and protecting intellectual property in large language models (LLMs). A recent
discovery, termed watermark radioactivity, reveals that watermarks embedded in
teacher models can be inherited by student models through knowledge
distillation. On the positive side, this inheritance allows for the detection
of unauthorized knowledge distillation by identifying watermark traces in
student models. However, the robustness of watermarks against scrubbing attacks
and their unforgeability in the face of spoofing attacks under unauthorized
knowledge distillation remain largely unexplored. Existing watermark attack
methods either assume access to model internals or fail to simultaneously
support both scrubbing and spoofing attacks. In this work, we propose
Contrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified
framework that enables bidirectional attacks under unauthorized knowledge
distillation. Our approach employs contrastive decoding to extract corrupted or
amplified watermark texts via comparing outputs from the student model and
weakly watermarked references, followed by bidirectional distillation to train
new student models capable of watermark removal and watermark forgery,
respectively. Extensive experiments show that CDG-KD effectively performs
attacks while preserving the general performance of the distilled model. Our
findings underscore critical need for developing watermarking schemes that are
robust and unforgeable.

</details>


### [72] [Large Language Models Understanding: an Inherent Ambiguity Barrier](https://arxiv.org/pdf/2505.00654)
*Daniel N. Nissani*

Main category: cs.CL

TL;DR: The paper argues that LLMs cannot truly understand dialogues due to an inherent ambiguity barrier, despite their fluency.


<details>
  <summary>Details</summary>
Motivation: To counter the debate on LLMs' understanding capabilities by highlighting their limitations.

Method: Uses a thought experiment and semi-formal considerations to demonstrate the ambiguity barrier.

Result: Identifies an inherent ambiguity barrier preventing LLMs from understanding dialogues.

Conclusion: LLMs lack genuine understanding of dialogues despite their fluency.

Abstract: A lively ongoing debate is taking place, since the extraordinary emergence of
Large Language Models (LLMs) with regards to their capability to understand the
world and capture the meaning of the dialogues in which they are involved.
Arguments and counter-arguments have been proposed based upon thought
experiments, anecdotal conversations between LLMs and humans, statistical
linguistic analysis, philosophical considerations, and more. In this brief
paper we present a counter-argument based upon a thought experiment and
semi-formal considerations leading to an inherent ambiguity barrier which
prevents LLMs from having any understanding of what their amazingly fluent
dialogues mean.

</details>


### [73] [A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency](https://arxiv.org/pdf/2505.01658)
*Sihyeong Park, Sungryeol Jeon, Chaelyn Lee, Seokhun Jeon, Byung-Soo Kim, Jemin Lee*

Main category: cs.CL

TL;DR: The paper evaluates 25 open-source and commercial LLM inference engines, analyzing their usability, deployment, scalability, and optimization techniques, while also suggesting future research directions.


<details>
  <summary>Details</summary>
Motivation: The increasing use of LLMs in diverse applications raises inference costs, but selecting the right optimization method is challenging due to varied service requirements.

Method: The study conducts a comprehensive evaluation of 25 inference engines, assessing ease-of-use, deployment, scalability, and optimization techniques.

Result: The paper provides insights into the design goals, ecosystem maturity, and performance-cost policies of inference engines.

Conclusion: Future research should focus on complex LLM services, hardware support, and security, with a public repository for ongoing updates.

Abstract: Large language models (LLMs) are widely applied in chatbots, code generators,
and search engines. Workloads such as chain-of-thought, complex reasoning, and
agent services significantly increase the inference cost by invoking the model
repeatedly. Optimization methods such as parallelism, compression, and caching
have been adopted to reduce costs, but the diverse service requirements make it
hard to select the right method. Recently, specialized LLM inference engines
have emerged as a key component for integrating the optimization methods into
service-oriented infrastructures. However, a systematic study on inference
engines is still lacking. This paper provides a comprehensive evaluation of 25
open-source and commercial inference engines. We examine each inference engine
in terms of ease-of-use, ease-of-deployment, general-purpose support,
scalability, and suitability for throughput- and latency-aware computation.
Furthermore, we explore the design goals of each inference engine by
investigating the optimization techniques it supports. In addition, we assess
the ecosystem maturity of open source inference engines and handle the
performance and cost policy of commercial solutions. We outline future research
directions that include support for complex LLM-based services, support of
various hardware, and enhanced security, offering practical guidance to
researchers and developers in selecting and designing optimized LLM inference
engines. We also provide a public repository to continually track developments
in this fast-evolving field:
https://github.com/sihyeong/Awesome-LLM-Inference-Engine

</details>


### [74] [Humans can learn to detect AI-generated texts, or at least learn when they can't](https://arxiv.org/pdf/2505.01877)
*Jiří Milička, Anna Marklová, Ondřej Drobil, Eva Pospíšilová*

Main category: cs.CL

TL;DR: Participants improved at distinguishing human-written from AI-generated texts with feedback, correcting misconceptions about AI style and readability.


<details>
  <summary>Details</summary>
Motivation: To explore if feedback helps individuals learn to differentiate human and AI texts and recalibrate self-perceived competence.

Method: Used GPT-4o to generate texts, compared with human-written ones, and tested 254 participants with/without feedback.

Result: Feedback group showed better accuracy and confidence calibration, correcting prior misconceptions.

Conclusion: Targeted training with feedback effectively teaches differentiation, aiding self-assessment and educational applications.

Abstract: This study investigates whether individuals can learn to accurately
discriminate between human-written and AI-produced texts when provided with
immediate feedback, and if they can use this feedback to recalibrate their
self-perceived competence. We also explore the specific criteria individuals
rely upon when making these decisions, focusing on textual style and perceived
readability.
  We used GPT-4o to generate several hundred texts across various genres and
text types comparable to Koditex, a multi-register corpus of human-written
texts. We then presented randomized text pairs to 254 Czech native speakers who
identified which text was human-written and which was AI-generated.
Participants were randomly assigned to two conditions: one receiving immediate
feedback after each trial, the other receiving no feedback until experiment
completion. We recorded accuracy in identification, confidence levels, response
times, and judgments about text readability along with demographic data and
participants' engagement with AI technologies prior to the experiment.
  Participants receiving immediate feedback showed significant improvement in
accuracy and confidence calibration. Participants initially held incorrect
assumptions about AI-generated text features, including expectations about
stylistic rigidity and readability. Notably, without feedback, participants
made the most errors precisely when feeling most confident -- an issue largely
resolved among the feedback group.
  The ability to differentiate between human and AI-generated texts can be
effectively learned through targeted training with explicit feedback, which
helps correct misconceptions about AI stylistic features and readability, as
well as potential other variables that were not explored, while facilitating
more accurate self-assessment. This finding might be particularly important in
educational contexts.

</details>


### [75] [RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale](https://arxiv.org/pdf/2505.03005)
*Daniel Goldstein, Eric Alcaide, Janna Lu, Eugene Cheah*

Main category: cs.CL

TL;DR: RADLADS converts softmax attention transformers to linear attention decoders efficiently, requiring minimal tokens and cost, while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To enable rapid and cost-effective conversion of transformer models to linear attention decoders without significant quality loss.

Method: Introduces RADLADS protocol and new RWKV-variant architectures, converting Qwen2.5 models (7B, 32B, 72B) with only 350-700M tokens.

Result: Achieves state-of-the-art performance in benchmarks, with conversion costs under $2,000 for a 72B model.

Conclusion: RADLADS offers a practical, scalable solution for linear attention models, with models released under open licenses.

Abstract: We present Rapid Attention Distillation to Linear Attention Decoders at Scale
(RADLADS), a protocol for rapidly converting softmax attention transformers
into linear attention decoder models, along with two new RWKV-variant
architectures, and models converted from popular Qwen2.5 open source models in
7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens,
less than 0.005% of the token count used to train the original teacher models.
Converting to our 72B linear attention model costs less than \$2,000 USD at
today's prices, yet quality at inference remains close to the original
transformer. These models achieve state-of-the-art downstream performance
across a set of standard benchmarks for linear attention models of their size.
We release all our models on HuggingFace under the Apache 2.0 license, with the
exception of our 72B models which are also governed by the Qwen License
Agreement.
  Models at
https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102
Training Code at https://github.com/recursal/RADLADS-paper

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [76] [Histo-Miner: Deep Learning based Tissue Features Extraction Pipeline from H&E Whole Slide Images of Cutaneous Squamous Cell Carcinoma](https://arxiv.org/pdf/2505.04672)
*Lucas Sancéré, Carina Lorenz, Doris Helbig, Oana-Diana Persa, Sonja Dengler, Alexander Kreuter, Martim Laimer, Anne Fröhlich, Jennifer Landsberg, Johannes Brägelmann, Katarzyna Bozek*

Main category: cs.CV

TL;DR: Histo-Miner is a deep learning pipeline for analyzing skin WSIs, generating labeled datasets for nuclei and tumor regions, and predicting cSCC patient response to immunotherapy.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of labeled datasets and open-source pipelines for skin tissue analysis, particularly for cSCC.

Method: Uses convolutional neural networks and vision transformers for nucleus segmentation, classification, and tumor region segmentation, trained on two datasets (47,392 annotated nuclei and 144 tumor-segmented WSIs).

Result: Achieves mPQ of 0.569 for nucleus segmentation, F1 of 0.832 for classification, and mIoU of 0.884 for tumor segmentation. Identifies predictive features for immunotherapy response.

Conclusion: Histo-Miner is effective for clinical applications, offering interpretable insights into tissue morphology and therapy response.

Abstract: Recent advancements in digital pathology have enabled comprehensive analysis
of Whole-Slide Images (WSI) from tissue samples, leveraging high-resolution
microscopy and computational capabilities. Despite this progress, there is a
lack of labeled datasets and open source pipelines specifically tailored for
analysis of skin tissue. Here we propose Histo-Miner, a deep learning-based
pipeline for analysis of skin WSIs and generate two datasets with labeled
nuclei and tumor regions. We develop our pipeline for the analysis of patient
samples of cutaneous squamous cell carcinoma (cSCC), a frequent non-melanoma
skin cancer. Utilizing the two datasets, comprising 47,392 annotated cell
nuclei and 144 tumor-segmented WSIs respectively, both from cSCC patients,
Histo-Miner employs convolutional neural networks and vision transformers for
nucleus segmentation and classification as well as tumor region segmentation.
Performance of trained models positively compares to state of the art with
multi-class Panoptic Quality (mPQ) of 0.569 for nucleus segmentation,
macro-averaged F1 of 0.832 for nucleus classification and mean Intersection
over Union (mIoU) of 0.884 for tumor region segmentation. From these
predictions we generate a compact feature vector summarizing tissue morphology
and cellular interactions, which can be used for various downstream tasks.
Here, we use Histo-Miner to predict cSCC patient response to immunotherapy
based on pre-treatment WSIs from 45 patients. Histo-Miner identifies
percentages of lymphocytes, the granulocyte to lymphocyte ratio in tumor
vicinity and the distances between granulocytes and plasma cells in tumors as
predictive features for therapy response. This highlights the applicability of
Histo-Miner to clinically relevant scenarios, providing direct interpretation
of the classification and insights into the underlying biology.

</details>


### [77] [Comparison of Visual Trackers for Biomechanical Analysis of Running](https://arxiv.org/pdf/2505.04713)
*Luis F. Gomez, Gonzalo Garrido-Lopez, Julian Fierrez, Aythami Morales, Ruben Tolosana, Javier Rueda, Enrique Navarro*

Main category: cs.CV

TL;DR: The paper evaluates six pose trackers for biomechanical sprint analysis, showing joint-based models with post-processing reduce errors significantly, though high-accuracy applications need further improvement.


<details>
  <summary>Details</summary>
Motivation: To assess the performance of pose trackers for biomechanical analysis in sprints, comparing them with expert annotations to validate their utility in sports analysis.

Method: Analyzed six trackers (two point, four joint) on 5870 frames from 40 sprints, focusing on key angles. Proposed a post-processing module for outlier detection and fusion prediction.

Result: Joint-based models achieved RMSE of 11.41° to 4.37°, reduced to 6.99° and 3.88° with post-processing.

Conclusion: Pose trackers are useful for biomechanical running analysis but require further refinement for high-accuracy needs.

Abstract: Human pose estimation has witnessed significant advancements in recent years,
mainly due to the integration of deep learning models, the availability of a
vast amount of data, and large computational resources. These developments have
led to highly accurate body tracking systems, which have direct applications in
sports analysis and performance evaluation.
  This work analyzes the performance of six trackers: two point trackers and
four joint trackers for biomechanical analysis in sprints. The proposed
framework compares the results obtained from these pose trackers with the
manual annotations of biomechanical experts for more than 5870 frames. The
experimental framework employs forty sprints from five professional runners,
focusing on three key angles in sprint biomechanics: trunk inclination, hip
flex extension, and knee flex extension. We propose a post-processing module
for outlier detection and fusion prediction in the joint angles.
  The experimental results demonstrate that using joint-based models yields
root mean squared errors ranging from 11.41{\deg} to 4.37{\deg}. When
integrated with the post-processing modules, these errors can be reduced to
6.99{\deg} and 3.88{\deg}, respectively. The experimental findings suggest that
human pose tracking approaches can be valuable resources for the biomechanical
analysis of running. However, there is still room for improvement in
applications where high accuracy is required.

</details>


### [78] [Lay-Your-Scene: Natural Scene Layout Generation with Diffusion Transformers](https://arxiv.org/pdf/2505.04718)
*Divyansh Srivastava, Xiang Zhang, He Wen, Chenru Wen, Zhuowen Tu*

Main category: cs.CV

TL;DR: LayouSyn is a text-to-layout pipeline for natural scenes, using lightweight open-source models and a novel diffusion Transformer for open-vocabulary generation, outperforming existing methods and enabling applications in image editing.


<details>
  <summary>Details</summary>
Motivation: Prior methods are limited by closed-vocabulary or proprietary models, restricting their applicability in controllable image generation.

Method: Uses lightweight open-source language models for scene elements and a novel aspect-aware diffusion Transformer for conditional layout generation.

Result: Outperforms existing methods on spatial and numerical reasoning benchmarks and enables applications like image editing.

Conclusion: LayouSyn advances open-vocabulary layout generation and demonstrates practical utility in image editing.

Abstract: We present Lay-Your-Scene (shorthand LayouSyn), a novel text-to-layout
generation pipeline for natural scenes. Prior scene layout generation methods
are either closed-vocabulary or use proprietary large language models for
open-vocabulary generation, limiting their modeling capabilities and broader
applicability in controllable image generation. In this work, we propose to use
lightweight open-source language models to obtain scene elements from text
prompts and a novel aspect-aware diffusion Transformer architecture trained in
an open-vocabulary manner for conditional layout generation. Extensive
experiments demonstrate that LayouSyn outperforms existing methods and achieves
state-of-the-art performance on challenging spatial and numerical reasoning
benchmarks. Additionally, we present two applications of LayouSyn. First, we
show that coarse initialization from large language models can be seamlessly
combined with our method to achieve better results. Second, we present a
pipeline for adding objects to images, demonstrating the potential of LayouSyn
in image editing applications.

</details>


### [79] [False Promises in Medical Imaging AI? Assessing Validity of Outperformance Claims](https://arxiv.org/pdf/2505.04720)
*Evangelia Christodoulou, Annika Reinke, Pascaline Andrè, Patrick Godau, Piotr Kalinowski, Rola Houhou, Selen Erkan, Carole H. Sudre, Ninon Burgos, Sofiène Boutaj, Sophie Loizillon, Maëlys Solal, Veronika Cheplygina, Charles Heitz, Michal Kozubek, Michela Antonelli, Nicola Rieke, Antoine Gilson, Leon D. Mayer, Minu D. Tizabi, M. Jorge Cardoso, Amber Simpson, Annette Kopp-Schneider, Gaël Varoquaux, Olivier Colliot, Lena Maier-Hein*

Main category: cs.CV

TL;DR: The paper critiques performance claims in medical imaging AI, revealing many outperformance claims are unsubstantiated.


<details>
  <summary>Details</summary>
Motivation: To assess whether new methods genuinely outperform state-of-the-art in medical imaging AI, given reliance on empirical mean performance.

Method: A Bayesian approach analyzing reported results and model congruence to quantify false claims.

Result: >80% of papers claim outperformance; high false claim probability (>5%) in 86% of classification and 53% of segmentation papers.

Conclusion: Current benchmarking practices often lead to unsubstantiated claims, risking misdirection of research.

Abstract: Performance comparisons are fundamental in medical imaging Artificial
Intelligence (AI) research, often driving claims of superiority based on
relative improvements in common performance metrics. However, such claims
frequently rely solely on empirical mean performance. In this paper, we
investigate whether newly proposed methods genuinely outperform the state of
the art by analyzing a representative cohort of medical imaging papers. We
quantify the probability of false claims based on a Bayesian approach that
leverages reported results alongside empirically estimated model congruence to
estimate whether the relative ranking of methods is likely to have occurred by
chance. According to our results, the majority (>80%) of papers claims
outperformance when introducing a new method. Our analysis further revealed a
high probability (>5%) of false outperformance claims in 86% of classification
papers and 53% of segmentation papers. These findings highlight a critical flaw
in current benchmarking practices: claims of outperformance in medical imaging
AI are frequently unsubstantiated, posing a risk of misdirecting future
research efforts.

</details>


### [80] [Does CLIP perceive art the same way we do?](https://arxiv.org/pdf/2505.05229)
*Andrea Asperti, Leonardo Dessì, Maria Chiara Tonetti, Nico Wu*

Main category: cs.CV

TL;DR: The paper investigates CLIP's ability to interpret artworks like humans, evaluating its perception of semantic and stylistic elements in paintings. It compares CLIP's responses to human benchmarks, revealing strengths and limitations, and discusses implications for generative processes.


<details>
  <summary>Details</summary>
Motivation: To assess whether CLIP 'sees' artworks similarly to humans, focusing on semantic and stylistic understanding, and its applicability in creative domains.

Method: Targeted probing tasks comparing CLIP's outputs to human annotations and expert benchmarks across content, style, historical period, and visual artifacts.

Result: CLIP shows strengths in some areas but has limitations in aesthetic and artistic intent understanding.

Conclusion: Highlights the need for better interpretability in multimodal systems for creative applications, where nuance is key.

Abstract: CLIP has emerged as a powerful multimodal model capable of connecting images
and text through joint embeddings, but to what extent does it "see" the same
way humans do - especially when interpreting artworks? In this paper, we
investigate CLIP's ability to extract high-level semantic and stylistic
information from paintings, including both human-created and AI-generated
imagery. We evaluate its perception across multiple dimensions: content, scene
understanding, artistic style, historical period, and the presence of visual
deformations or artifacts. By designing targeted probing tasks and comparing
CLIP's responses to human annotations and expert benchmarks, we explore its
alignment with human perceptual and contextual understanding. Our findings
reveal both strengths and limitations in CLIP's visual representations,
particularly in relation to aesthetic cues and artistic intent. We further
discuss the implications of these insights for using CLIP as a guidance
mechanism during generative processes, such as style transfer or prompt-based
image synthesis. Our work highlights the need for deeper interpretability in
multimodal systems, especially when applied to creative domains where nuance
and subjectivity play a central role.

</details>


### [81] [Hyb-KAN ViT: Hybrid Kolmogorov-Arnold Networks Augmented Vision Transformer](https://arxiv.org/pdf/2505.04740)
*Sainath Dey, Mitul Goswami, Jashika Sethi, Prasant Kumar Pattnaik*

Main category: cs.CV

TL;DR: Hyb-KAN ViT replaces MLPs in ViTs with wavelet-based spectral decomposition and spline-optimized activation functions, improving performance on vision tasks.


<details>
  <summary>Details</summary>
Motivation: Address limitations of MLPs in ViTs by leveraging wavelet functions and spline optimizations for better spatial-frequency modeling.

Method: Introduces Eff-KAN (spline functions) and Wav-KAN (wavelet transforms) modules integrated into ViT layers for enhanced feature extraction.

Result: Achieves state-of-the-art performance on ImageNet-1K, COCO, and ADE20K benchmarks.

Conclusion: Hyb-KAN ViT balances parameter efficiency and multi-scale representation, setting a new paradigm for vision architectures.

Abstract: This study addresses the inherent limitations of Multi-Layer Perceptrons
(MLPs) in Vision Transformers (ViTs) by introducing Hybrid Kolmogorov-Arnold
Network (KAN)-ViT (Hyb-KAN ViT), a novel framework that integrates
wavelet-based spectral decomposition and spline-optimized activation functions,
prior work has failed to focus on the prebuilt modularity of the ViT
architecture and integration of edge detection capabilities of Wavelet
functions. We propose two key modules: Efficient-KAN (Eff-KAN), which replaces
MLP layers with spline functions and Wavelet-KAN (Wav-KAN), leveraging
orthogonal wavelet transforms for multi-resolution feature extraction. These
modules are systematically integrated in ViT encoder layers and classification
heads to enhance spatial-frequency modeling while mitigating computational
bottlenecks. Experiments on ImageNet-1K (Image Recognition), COCO (Object
Detection and Instance Segmentation), and ADE20K (Semantic Segmentation)
demonstrate state-of-the-art performance with Hyb-KAN ViT. Ablation studies
validate the efficacy of wavelet-driven spectral priors in segmentation and
spline-based efficiency in detection tasks. The framework establishes a new
paradigm for balancing parameter efficiency and multi-scale representation in
vision architectures.

</details>


### [82] [Lightweight RGB-D Salient Object Detection from a Speed-Accuracy Tradeoff Perspective](https://arxiv.org/pdf/2505.04758)
*Songsong Duan, Xi Yang, Nannan Wang, Xinbo Gao*

Main category: cs.CV

TL;DR: SATNet balances efficiency and performance for RGB-D SOD by improving depth quality, modality fusion, and feature representation, achieving 5.2M parameters and 415 FPS.


<details>
  <summary>Details</summary>
Motivation: Existing RGB-D methods either sacrifice efficiency for accuracy or struggle with high precision. SATNet aims to bridge this gap.

Method: Introduces Depth Anything Model for depth quality, Decoupled Attention Module (DAM) for fusion, and Dual Information Representation Module (DIRM) for feature enrichment.

Result: Outperforms SOTA CNN-based models with 5.2M parameters and 415 FPS on five RGB-D SOD datasets.

Conclusion: SATNet successfully balances speed and accuracy, offering a lightweight yet high-performance solution for RGB-D SOD.

Abstract: Current RGB-D methods usually leverage large-scale backbones to improve
accuracy but sacrifice efficiency. Meanwhile, several existing lightweight
methods are difficult to achieve high-precision performance. To balance the
efficiency and performance, we propose a Speed-Accuracy Tradeoff Network
(SATNet) for Lightweight RGB-D SOD from three fundamental perspectives: depth
quality, modality fusion, and feature representation. Concerning depth quality,
we introduce the Depth Anything Model to generate high-quality depth maps,which
effectively alleviates the multi-modal gaps in the current datasets. For
modality fusion, we propose a Decoupled Attention Module (DAM) to explore the
consistency within and between modalities. Here, the multi-modal features are
decoupled into dual-view feature vectors to project discriminable information
of feature maps. For feature representation, we develop a Dual Information
Representation Module (DIRM) with a bi-directional inverted framework to
enlarge the limited feature space generated by the lightweight backbones. DIRM
models texture features and saliency features to enrich feature space, and
employ two-way prediction heads to optimal its parameters through a
bi-directional backpropagation. Finally, we design a Dual Feature Aggregation
Module (DFAM) in the decoder to aggregate texture and saliency features.
Extensive experiments on five public RGB-D SOD datasets indicate that the
proposed SATNet excels state-of-the-art (SOTA) CNN-based heavyweight models and
achieves a lightweight framework with 5.2 M parameters and 415 FPS.

</details>


### [83] [Hearing and Seeing Through CLIP: A Framework for Self-Supervised Sound Source Localization](https://arxiv.org/pdf/2505.05343)
*Sooyoung Park, Arda Senocak, Joon Son Chung*

Main category: cs.CV

TL;DR: The paper extends CLIP to sound source localization using a self-supervised method without text input, achieving superior performance and generalization.


<details>
  <summary>Details</summary>
Motivation: To leverage CLIP's multimodal alignment for sound source localization, avoiding reliance on explicit text input.

Method: A framework maps audios into CLIP-compatible tokens, generates sounding region masks, and aligns visual features with audio embeddings via contrastive learning. An LLM-guided extension enhances alignment.

Result: Outperforms state-of-the-art methods in five tasks, with strong zero-shot generalization.

Conclusion: Pre-trained multimodal models like CLIP enable effective sound source localization, with LLM-guided training further improving alignment.

Abstract: Large-scale vision-language models demonstrate strong multimodal alignment
and generalization across diverse tasks. Among them, CLIP stands out as one of
the most successful approaches. In this work, we extend the application of CLIP
to sound source localization, proposing a self-supervised method operates
without explicit text input. We introduce a framework that maps audios into
tokens compatible with CLIP's text encoder, producing audio-driven embeddings.
These embeddings are used to generate sounding region masks, from which visual
features are extracted and aligned with the audio embeddings through a
contrastive audio-visual correspondence objective. Our findings show that
alignment knowledge of pre-trained multimodal foundation model enables our
method to generate more complete and compact localization for sounding objects.
We further propose an LLM-guided extension that distills object-aware
audio-visual scene understanding into the model during training to enhance
alignment. Extensive experiments across five diverse tasks demonstrate that our
method, in all variants, outperforms state-of-the-art approaches and achieves
strong generalization in zero-shot settings.

</details>


### [84] [Vision-Language-Action Models: Concepts, Progress, Applications and Challenges](https://arxiv.org/pdf/2505.04769)
*Ranjan Sapkota, Yang Cao, Konstantinos I. Roumeliotis, Manoj Karkee*

Main category: cs.CV

TL;DR: A review of Vision-Language-Action (VLA) models, covering their evolution, methodologies, applications, challenges, and future directions in AI and robotics.


<details>
  <summary>Details</summary>
Motivation: To unify perception, language understanding, and action in AI, advancing embodied agents and general intelligence.

Method: Rigorous literature review of 80+ VLA models, focusing on architecture, training, and inference innovations.

Result: Identifies key progress areas (e.g., architectural innovations) and challenges (e.g., real-time control, ethical risks).

Conclusion: Proposes solutions like agentic AI adaptation and outlines a roadmap for socially aligned, general-purpose embodied agents.

Abstract: Vision-Language-Action (VLA) models mark a transformative advancement in
artificial intelligence, aiming to unify perception, natural language
understanding, and embodied action within a single computational framework.
This foundational review presents a comprehensive synthesis of recent
advancements in Vision-Language-Action models, systematically organized across
five thematic pillars that structure the landscape of this rapidly evolving
field. We begin by establishing the conceptual foundations of VLA systems,
tracing their evolution from cross-modal learning architectures to generalist
agents that tightly integrate vision-language models (VLMs), action planners,
and hierarchical controllers. Our methodology adopts a rigorous literature
review framework, covering over 80 VLA models published in the past three
years. Key progress areas include architectural innovations,
parameter-efficient training strategies, and real-time inference accelerations.
We explore diverse application domains such as humanoid robotics, autonomous
vehicles, medical and industrial robotics, precision agriculture, and augmented
reality navigation. The review further addresses major challenges across
real-time control, multimodal action representation, system scalability,
generalization to unseen tasks, and ethical deployment risks. Drawing from the
state-of-the-art, we propose targeted solutions including agentic AI
adaptation, cross-embodiment generalization, and unified neuro-symbolic
planning. In our forward-looking discussion, we outline a future roadmap where
VLA models, VLMs, and agentic AI converge to power socially aligned, adaptive,
and general-purpose embodied agents. This work serves as a foundational
reference for advancing intelligent, real-world robotics and artificial general
intelligence. >Vision-language-action, Agentic AI, AI Agents, Vision-language
Models

</details>


### [85] [Replay to Remember (R2R): An Efficient Uncertainty-driven Unsupervised Continual Learning Framework Using Generative Replay](https://arxiv.org/pdf/2505.04787)
*Sriram Mandalika, Harsha Vardhan, Athira Nambiar*

Main category: cs.CV

TL;DR: A novel unsupervised continual learning framework, R2R, uses generative replay and uncertainty-driven feedback to mitigate catastrophic forgetting, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To address catastrophic forgetting in neural networks by enabling continual learning without prior training or reliance on pretrained models.

Method: Uses cluster-level uncertainty-driven feedback and a VLM-powered generative replay module to balance unlabelled and synthetic labelled data.

Result: Achieves top performance on CIFAR-10 (98.13%), CIFAR-100 (73.06%), CINIC-10 (93.41%), SVHN (95.18%), and TinyImageNet (59.74%), surpassing prior methods by 4.36%.

Conclusion: R2R effectively retains knowledge and outperforms existing approaches, demonstrating the potential of unsupervised continual learning.

Abstract: Continual Learning entails progressively acquiring knowledge from new data
while retaining previously acquired knowledge, thereby mitigating
``Catastrophic Forgetting'' in neural networks. Our work presents a novel
uncertainty-driven Unsupervised Continual Learning framework using Generative
Replay, namely ``Replay to Remember (R2R)''. The proposed R2R architecture
efficiently uses unlabelled and synthetic labelled data in a balanced
proportion using a cluster-level uncertainty-driven feedback mechanism and a
VLM-powered generative replay module. Unlike traditional memory-buffer methods
that depend on pretrained models and pseudo-labels, our R2R framework operates
without any prior training. It leverages visual features from unlabeled data
and adapts continuously using clustering-based uncertainty estimation coupled
with dynamic thresholding. Concurrently, a generative replay mechanism along
with DeepSeek-R1 powered CLIP VLM produces labelled synthetic data
representative of past experiences, resembling biological visual thinking that
replays memory to remember and act in new, unseen tasks. Extensive experimental
analyses are carried out in CIFAR-10, CIFAR-100, CINIC-10, SVHN and
TinyImageNet datasets. Our proposed R2R approach improves knowledge retention,
achieving a state-of-the-art performance of 98.13%, 73.06%, 93.41%, 95.18%,
59.74%, respectively, surpassing state-of-the-art performance by over 4.36%.

</details>


### [86] [Convex Relaxation for Robust Vanishing Point Estimation in Manhattan World](https://arxiv.org/pdf/2505.04788)
*Bangyan Liao, Zhenjun Zhao, Haoang Li, Yi Zhou, Yingping Zeng, Hao Li, Peidong Liu*

Main category: cs.CV

TL;DR: The paper introduces a convex relaxation method (GlobustVP) for efficiently and robustly determining vanishing points in a Manhattan world, balancing global optimality and computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing methods for vanishing point detection are either sub-optimal or computationally expensive. The paper aims to address this gap by proposing a globally optimal yet efficient solution.

Method: The method uses convex relaxation techniques, formulating the problem as a QCQP and relaxing it into a convex SDP. It employs an iterative solver (GlobustVP) to independently update VPs and their line associations, reinforcing orthogonality constraints.

Result: Experiments on synthetic and real-world data show GlobustVP achieves a balance between efficiency, robustness, and global optimality.

Conclusion: GlobustVP outperforms prior methods in efficiency and accuracy, offering a practical solution for vanishing point detection in 3D vision applications.

Abstract: Determining the vanishing points (VPs) in a Manhattan world, as a fundamental
task in many 3D vision applications, consists of jointly inferring the line-VP
association and locating each VP. Existing methods are, however, either
sub-optimal solvers or pursuing global optimality at a significant cost of
computing time. In contrast to prior works, we introduce convex relaxation
techniques to solve this task for the first time. Specifically, we employ a
``soft'' association scheme, realized via a truncated multi-selection error,
that allows for joint estimation of VPs' locations and line-VP associations.
This approach leads to a primal problem that can be reformulated into a
quadratically constrained quadratic programming (QCQP) problem, which is then
relaxed into a convex semidefinite programming (SDP) problem. To solve this SDP
problem efficiently, we present a globally optimal outlier-robust iterative
solver (called \textbf{GlobustVP}), which independently searches for one VP and
its associated lines in each iteration, treating other lines as outliers. After
each independent update of all VPs, the mutual orthogonality between the three
VPs in a Manhattan world is reinforced via local refinement. Extensive
experiments on both synthetic and real-world data demonstrate that
\textbf{GlobustVP} achieves a favorable balance between efficiency, robustness,
and global optimality compared to previous works. The code is publicly
available at https://github.com/WU-CVGL/GlobustVP.

</details>


### [87] [DetReIDX: A Stress-Test Dataset for Real-World UAV-Based Person Recognition](https://arxiv.org/pdf/2505.04793)
*Kailash A. Hambarde, Nzakiese Mbongo, Pavan Kumar MP, Satish Mekewad, Carolina Fernandes, Gökhan Silahtaroğlu, Alice Nithya, Pawan Wasnik, MD. Rashidunnabi, Pranita Samale, Hugo Proença*

Main category: cs.CV

TL;DR: DetReIDX is a large-scale aerial-ground person dataset designed to stress-test ReID technology under real-world conditions, addressing data variability and long-term evaluation challenges.


<details>
  <summary>Details</summary>
Motivation: Current ReID technology fails in real-world settings due to extreme data variability, and existing datasets lack realistic variability.

Method: DetReIDX includes 13M+ bounding boxes from 509 identities, collected across 7 campuses with drone altitudes of 5.8-120m, featuring multi-session recordings with clothing and environmental changes.

Result: SOTA methods degrade significantly (80% in detection, 70% in Rank-1 ReID) under DetReIDX conditions.

Conclusion: DetReIDX provides a robust benchmark for evaluating ReID in real-world scenarios, with publicly available data and protocols.

Abstract: Person reidentification (ReID) technology has been considered to perform
relatively well under controlled, ground-level conditions, but it breaks down
when deployed in challenging real-world settings. Evidently, this is due to
extreme data variability factors such as resolution, viewpoint changes, scale
variations, occlusions, and appearance shifts from clothing or session drifts.
Moreover, the publicly available data sets do not realistically incorporate
such kinds and magnitudes of variability, which limits the progress of this
technology. This paper introduces DetReIDX, a large-scale aerial-ground person
dataset, that was explicitly designed as a stress test to ReID under real-world
conditions. DetReIDX is a multi-session set that includes over 13 million
bounding boxes from 509 identities, collected in seven university campuses from
three continents, with drone altitudes between 5.8 and 120 meters. More
important, as a key novelty, DetReIDX subjects were recorded in (at least) two
sessions on different days, with changes in clothing, daylight and location,
making it suitable to actually evaluate long-term person ReID. Plus, data were
annotated from 16 soft biometric attributes and multitask labels for detection,
tracking, ReID, and action recognition. In order to provide empirical evidence
of DetReIDX usefulness, we considered the specific tasks of human detection and
ReID, where SOTA methods catastrophically degrade performance (up to 80% in
detection accuracy and over 70% in Rank-1 ReID) when exposed to DetReIDXs
conditions. The dataset, annotations, and official evaluation protocols are
publicly available at https://www.it.ubi.pt/DetReIDX/

</details>


### [88] [Are Synthetic Corruptions A Reliable Proxy For Real-World Corruptions?](https://arxiv.org/pdf/2505.04835)
*Shashank Agnihotri, David Schader, Nico Sharei, Mehmet Ege Kaçar, Margret Keuper*

Main category: cs.CV

TL;DR: The paper investigates whether synthetic corruptions are a reliable proxy for real-world corruptions in testing DL model robustness, finding strong correlation in mean performance.


<details>
  <summary>Details</summary>
Motivation: DL models are vulnerable to distribution shifts (e.g., weather, lighting), and real-world data collection is resource-intensive, making synthetic corruptions a potential alternative.

Method: The study conducts a large benchmarking of semantic segmentation models, comparing performance on real-world and synthetic corruptions datasets.

Result: Results show a strong correlation in mean performance between synthetic and real-world corruptions, supporting synthetic corruptions for robustness evaluation.

Conclusion: Synthetic corruptions are a viable proxy for real-world corruptions in robustness testing, with corruption-specific insights provided for their reliability.

Abstract: Deep learning (DL) models are widely used in real-world applications but
remain vulnerable to distribution shifts, especially due to weather and
lighting changes. Collecting diverse real-world data for testing the robustness
of DL models is resource-intensive, making synthetic corruptions an attractive
alternative for robustness testing. However, are synthetic corruptions a
reliable proxy for real-world corruptions? To answer this, we conduct the
largest benchmarking study on semantic segmentation models, comparing
performance on real-world corruptions and synthetic corruptions datasets. Our
results reveal a strong correlation in mean performance, supporting the use of
synthetic corruptions for robustness evaluation. We further analyze
corruption-specific correlations, providing key insights to understand when
synthetic corruptions succeed in representing real-world corruptions.
Open-source Code:
https://github.com/shashankskagnihotri/benchmarking_robustness/tree/segmentation_david/semantic_segmentation

</details>


### [89] [Seeing Cells Clearly: Evaluating Machine Vision Strategies for Microglia Centroid Detection in 3D Images](https://arxiv.org/pdf/2505.04838)
*Youjia Zhang*

Main category: cs.CV

TL;DR: Comparison of three tools (ilastik, 3D Morph, Omnipose) for detecting microglia centers in 3D images, revealing tool-specific biases.


<details>
  <summary>Details</summary>
Motivation: Microglia shape is indicative of brain health, necessitating accurate tools for center detection in 3D images.

Method: Tested ilastik, 3D Morph, and Omnipose for their ability to locate microglia centers in 3D microscope images.

Result: Each tool detects microglia centers differently, influencing the data derived from images.

Conclusion: Tool choice impacts microglia analysis, highlighting the need for careful selection based on study goals.

Abstract: Microglia are important cells in the brain, and their shape can tell us a lot
about brain health. In this project, I test three different tools for finding
the center points of microglia in 3D microscope images. The tools include
ilastik, 3D Morph, and Omnipose. I look at how well each one finds the cells
and how their results compare. My findings show that each tool sees the cells
in its own way, and this can affect the kind of information we get from the
images.

</details>


### [90] [ORXE: Orchestrating Experts for Dynamically Configurable Efficiency](https://arxiv.org/pdf/2505.04850)
*Qingyuan Wang, Guoxin Wang, Barry Cardiff, Deepu John*

Main category: cs.CV

TL;DR: ORXE is a modular framework for real-time efficiency in AI models, using pre-trained experts and dynamic inference pathways. It avoids complex metamodel training and adjusts computational resources via a confidence-based gating mechanism.


<details>
  <summary>Details</summary>
Motivation: To achieve high efficiency and flexibility in AI models without complicating development, addressing the limitations of conventional approaches.

Method: Uses pre-trained experts with varying costs and performance, dynamically adjusting inference pathways via a confidence-based gating mechanism. Supports runtime adjustments for cost-performance trade-offs.

Result: ORXE outperforms individual experts and other dynamic models in efficiency and accuracy for image classification tasks.

Conclusion: ORXE provides a scalable, adaptable solution for real-world AI deployments, extendable to other applications.

Abstract: This paper presents ORXE, a modular and adaptable framework for achieving
real-time configurable efficiency in AI models. By leveraging a collection of
pre-trained experts with diverse computational costs and performance levels,
ORXE dynamically adjusts inference pathways based on the complexity of input
samples. Unlike conventional approaches that require complex metamodel
training, ORXE achieves high efficiency and flexibility without complicating
the development process. The proposed system utilizes a confidence-based gating
mechanism to allocate appropriate computational resources for each input. ORXE
also supports adjustments to the preference between inference cost and
prediction performance across a wide range during runtime. We implemented a
training-free ORXE system for image classification tasks, evaluating its
efficiency and accuracy across various devices. The results demonstrate that
ORXE achieves superior performance compared to individual experts and other
dynamic models in most cases. This approach can be extended to other
applications, providing a scalable solution for diverse real-world deployment
scenarios.

</details>


### [91] [Mix-QSAM: Mixed-Precision Quantization of the Segment Anything Model](https://arxiv.org/pdf/2505.04861)
*Navin Ranjan, Andreas Savakis*

Main category: cs.CV

TL;DR: Mix-QSAM is a mixed-precision PTQ framework for SAM, using layer-wise importance and cross-layer synergy to optimize bit-width allocation, outperforming fixed-bit PTQ methods.


<details>
  <summary>Details</summary>
Motivation: SAM's high computational demands hinder deployment on resource-constrained devices, and fixed-bit PTQ methods are suboptimal.

Method: Mix-QSAM uses KL divergence for layer importance, causal mutual information for cross-layer synergy, and IQP for bit-width allocation.

Result: Mix-QSAM achieves up to 20% higher average precision in 6-bit and 4-bit settings while maintaining efficiency.

Conclusion: Mix-QSAM effectively balances accuracy and efficiency for SAM deployment on constrained devices.

Abstract: The Segment Anything Model (SAM) is a popular vision foundation model;
however, its high computational and memory demands make deployment on
resource-constrained devices challenging. While Post-Training Quantization
(PTQ) is a practical approach for reducing computational overhead, existing PTQ
methods rely on fixed bit-width quantization, leading to suboptimal accuracy
and efficiency. To address this limitation, we propose Mix-QSAM, a
mixed-precision PTQ framework for SAM. First, we introduce a layer-wise
importance score, derived using Kullback-Leibler (KL) divergence, to quantify
each layer's contribution to the model's output. Second, we introduce
cross-layer synergy, a novel metric based on causal mutual information, to
capture dependencies between adjacent layers. This ensures that highly
interdependent layers maintain similar bit-widths, preventing abrupt precision
mismatches that degrade feature propagation and numerical stability. Using
these metrics, we formulate an Integer Quadratic Programming (IQP) problem to
determine optimal bit-width allocation under model size and bit-operation
constraints, assigning higher precision to critical layers while minimizing
bit-width in less influential layers. Experimental results demonstrate that
Mix-QSAM consistently outperforms existing PTQ methods on instance segmentation
and object detection tasks, achieving up to 20% higher average precision under
6-bit and 4-bit mixed-precision settings, while maintaining computational
efficiency.

</details>


### [92] [Auto-regressive transformation for image alignment](https://arxiv.org/pdf/2505.04864)
*Kanggeon Lee, Soochahn Lee, Kyoung Mu Lee*

Main category: cs.CV

TL;DR: ART is a novel auto-regressive method for image alignment that outperforms existing techniques by refining transformations iteratively and focusing on critical regions.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail in feature-sparse regions, extreme scale differences, and large deformations, leading to poor accuracy.

Method: ART iteratively estimates coarse-to-fine transformations using hierarchical multi-scale features and cross-attention guidance.

Result: ART significantly outperforms state-of-the-art methods in diverse datasets.

Conclusion: ART is a robust and precise solution for challenging image alignment tasks with broad applicability.

Abstract: Existing methods for image alignment struggle in cases involving
feature-sparse regions, extreme scale and field-of-view differences, and large
deformations, often resulting in suboptimal accuracy. Robustness to these
challenges improves through iterative refinement of the transformation field
while focusing on critical regions in multi-scale image representations. We
thus propose Auto-Regressive Transformation (ART), a novel method that
iteratively estimates the coarse-to-fine transformations within an
auto-regressive framework. Leveraging hierarchical multi-scale features, our
network refines the transformations using randomly sampled points at each
scale. By incorporating guidance from the cross-attention layer, the model
focuses on critical regions, ensuring accurate alignment even in challenging,
feature-limited conditions. Extensive experiments across diverse datasets
demonstrate that ART significantly outperforms state-of-the-art methods,
establishing it as a powerful new method for precise image alignment with broad
applicability.

</details>


### [93] [Learning from Loss Landscape: Generalizable Mixed-Precision Quantization via Adaptive Sharpness-Aware Gradient Aligning](https://arxiv.org/pdf/2505.04877)
*Lianbo Ma, Jianlun Ma, Yuee Zhou, Guoyang Xie, Qiang He, Zhichao Lu*

Main category: cs.CV

TL;DR: Proposes a method to generalize quantization policies from small to large datasets, reducing computational costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing MPQ methods require expensive searches on large datasets, which is inefficient.

Method: Uses small datasets for policy search, sharpness-aware minimization, gradient alignment, and adaptive perturbation.

Result: Achieves equivalent accuracy on ImageNet with 150% efficiency improvement over baselines.

Conclusion: The approach simplifies MPQ by eliminating large-scale fine-tuning, validated by theory and experiments.

Abstract: Mixed Precision Quantization (MPQ) has become an essential technique for
optimizing neural network by determining the optimal bitwidth per layer.
Existing MPQ methods, however, face a major hurdle: they require a
computationally expensive search for quantization policies on large-scale
datasets. To resolve this issue, we introduce a novel approach that first
searches for quantization policies on small datasets and then generalizes them
to large-scale datasets. This approach simplifies the process, eliminating the
need for large-scale quantization fine-tuning and only necessitating model
weight adjustment. Our method is characterized by three key techniques:
sharpness-aware minimization for enhanced quantization generalization, implicit
gradient direction alignment to handle gradient conflicts among different
optimization objectives, and an adaptive perturbation radius to accelerate
optimization. Both theoretical analysis and experimental results validate our
approach. Using the CIFAR10 dataset (just 0.5\% the size of ImageNet training
data) for MPQ policy search, we achieved equivalent accuracy on ImageNet with a
significantly lower computational cost, while improving efficiency by up to
150% over the baselines.

</details>


### [94] [Joint Super-Resolution and Segmentation for 1-m Impervious Surface Area Mapping in China's Yangtze River Economic Belt](https://arxiv.org/pdf/2505.05367)
*Jie Deng, Danfeng Hong, Chenyu Li, Naoto Yokoya*

Main category: cs.CV

TL;DR: JointSeg integrates super-resolution and segmentation to create 1m ISA maps from Sentinel-2 imagery, outperforming benchmarks with an F1-score of 85.71% and demonstrating robustness in diverse landscapes.


<details>
  <summary>Details</summary>
Motivation: To provide a scalable, affordable solution for high-resolution ISA mapping by leveraging freely available Sentinel-2 data, addressing limitations of traditional methods.

Method: JointSeg combines super-resolution and segmentation, trained on multimodal cross-resolution inputs, enabling gradual resolution enhancement (10m to 1m) and cross-scale feature fusion.

Result: ISA-1, the resulting product, covers 2.2M km², achieves 85.71% F1-score, and outperforms benchmarks by 9.5%-61.07%, with improved accuracy in urban and mountainous areas.

Conclusion: JointSeg is robust and effective for diverse landscapes, capturing urbanization dynamics (2017-2023) and revealing distinct regional growth patterns.

Abstract: We propose a novel joint framework by integrating super-resolution and
segmentation, called JointSeg, which enables the generation of 1-meter ISA maps
directly from freely available Sentinel-2 imagery. JointSeg was trained on
multimodal cross-resolution inputs, offering a scalable and affordable
alternative to traditional approaches. This synergistic design enables gradual
resolution enhancement from 10m to 1m while preserving fine-grained spatial
textures, and ensures high classification fidelity through effective
cross-scale feature fusion. This method has been successfully applied to the
Yangtze River Economic Belt (YREB), a region characterized by complex
urban-rural patterns and diverse topography. As a result, a comprehensive ISA
mapping product for 2021, referred to as ISA-1, was generated, covering an area
of over 2.2 million square kilometers. Quantitative comparisons against the 10m
ESA WorldCover and other benchmark products reveal that ISA-1 achieves an
F1-score of 85.71%, outperforming bilinear-interpolation-based segmentation by
9.5%, and surpassing other ISA datasets by 21.43%-61.07%. In densely urbanized
areas (e.g., Suzhou, Nanjing), ISA-1 reduces ISA overestimation through
improved discrimination of green spaces and water bodies. Conversely, in
mountainous regions (e.g., Ganzi, Zhaotong), it identifies significantly more
ISA due to its enhanced ability to detect fragmented anthropogenic features
such as rural roads and sparse settlements, demonstrating its robustness across
diverse landscapes. Moreover, we present biennial ISA maps from 2017 to 2023,
capturing spatiotemporal urbanization dynamics across representative cities.
The results highlight distinct regional growth patterns: rapid expansion in
upstream cities, moderate growth in midstream regions, and saturation in
downstream metropolitan areas.

</details>


### [95] [Cross-Branch Orthogonality for Improved Generalization in Face Deepfake Detection](https://arxiv.org/pdf/2505.04888)
*Tharindu Fernando, Clinton Fookes, Sridha Sridharan, Simon Denman*

Main category: cs.CV

TL;DR: The paper proposes a new deepfake detection strategy using coarse-to-fine spatial and semantic information, outperforming state-of-the-art methods by 5-7%.


<details>
  <summary>Details</summary>
Motivation: Deepfakes are causing societal harm, and existing detectors fail to generalize due to reliance on specific artifacts.

Method: Leverages spatial and semantic information with feature orthogonality-based disentanglement to ensure distinctiveness and reduce redundancy.

Result: Outperforms current methods by 5% on Celeb-DF and 7% on DFDC datasets in cross-dataset evaluations.

Conclusion: The proposed approach effectively combats malicious face deepfakes with improved generalization and performance.

Abstract: Remarkable advancements in generative AI technology have given rise to a
spectrum of novel deepfake categories with unprecedented leaps in their
realism, and deepfakes are increasingly becoming a nuisance to law enforcement
authorities and the general public. In particular, we observe alarming levels
of confusion, deception, and loss of faith regarding multimedia content within
society caused by face deepfakes, and existing deepfake detectors are
struggling to keep up with the pace of improvements in deepfake generation.
This is primarily due to their reliance on specific forgery artifacts, which
limits their ability to generalise and detect novel deepfake types. To combat
the spread of malicious face deepfakes, this paper proposes a new strategy that
leverages coarse-to-fine spatial information, semantic information, and their
interactions while ensuring feature distinctiveness and reducing the redundancy
of the modelled features. A novel feature orthogonality-based disentanglement
strategy is introduced to ensure branch-level and cross-branch feature
disentanglement, which allows us to integrate multiple feature vectors without
adding complexity to the feature space or compromising generalisation.
Comprehensive experiments on three public benchmarks: FaceForensics++,
Celeb-DF, and the Deepfake Detection Challenge (DFDC) show that these design
choices enable the proposed approach to outperform current state-of-the-art
methods by 5% on the Celeb-DF dataset and 7% on the DFDC dataset in a
cross-dataset evaluation setting.

</details>


### [96] [OWT: A Foundational Organ-Wise Tokenization Framework for Medical Imaging](https://arxiv.org/pdf/2505.04899)
*Sifan Song, Siyeop Yoon, Pengfei Jin, Sekeun Kim, Matthew Tivnan, Yujin Oh, Runqi Meng, Ling Chen, Zhiliang Lyu, Dufan Wu, Ning Guo, Xiang Li, Quanzheng Li*

Main category: cs.CV

TL;DR: The paper introduces Organ-Wise Tokenization (OWT) and Token Group-based Reconstruction (TGR) to address limitations of holistic embeddings in medical imaging, improving interpretability and generalization.


<details>
  <summary>Details</summary>
Motivation: Holistic embeddings in representation learning entangle semantic components, limiting interpretability and generalization, especially in medical imaging.

Method: Proposes OWT and TGR to disentangle images into organ-specific token groups, enhancing interpretability and control.

Result: OWT achieves strong performance in reconstruction and segmentation, enabling novel semantic-level applications.

Conclusion: OWT is a scalable, foundational framework for semantically disentangled representation learning in medical imaging and beyond.

Abstract: Recent advances in representation learning often rely on holistic, black-box
embeddings that entangle multiple semantic components, limiting
interpretability and generalization. These issues are especially critical in
medical imaging. To address these limitations, we propose an Organ-Wise
Tokenization (OWT) framework with a Token Group-based Reconstruction (TGR)
training paradigm. Unlike conventional approaches that produce holistic
features, OWT explicitly disentangles an image into separable token groups,
each corresponding to a distinct organ or semantic entity. Our design ensures
each token group encapsulates organ-specific information, boosting
interpretability, generalization, and efficiency while allowing fine-grained
control in downstream tasks. Experiments on CT and MRI datasets demonstrate the
effectiveness of OWT in not only achieving strong image reconstruction and
segmentation performance, but also enabling novel semantic-level generation and
retrieval applications that are out of reach for standard holistic embedding
methods. These findings underscore the potential of OWT as a foundational
framework for semantically disentangled representation learning, offering broad
scalability and applicability to real-world medical imaging scenarios and
beyond.

</details>


### [97] [Pro2SAM: Mask Prompt to SAM with Grid Points for Weakly Supervised Object Localization](https://arxiv.org/pdf/2505.04905)
*Xi Yang, Songsong Duan, Nannan Wang, Xinbo Gao*

Main category: cs.CV

TL;DR: The paper introduces Pro2SAM, a method leveraging SAM for weakly supervised object localization, addressing limitations of CAM and self-attention maps by using mask prompts and grid points.


<details>
  <summary>Details</summary>
Motivation: Current WSOL methods like CAM and self-attention maps lack fine-grained pixel-level information, limiting performance.

Method: Pro2SAM uses GTFormer for coarse foreground maps and grid points with SAM for dense prompts, optimizing mask matching via a similarity metric.

Result: Pro2SAM achieves 84.03% and 66.85% Top-1 Loc on CUB-200-2011 and ILSVRC, respectively.

Conclusion: Pro2SAM advances WSOL by integrating SAM's capabilities, outperforming existing methods.

Abstract: Weakly Supervised Object Localization (WSOL), which aims to localize objects
by only using image-level labels, has attracted much attention because of its
low annotation cost in real applications. Current studies focus on the Class
Activation Map (CAM) of CNN and the self-attention map of transformer to
identify the region of objects. However, both CAM and self-attention maps can
not learn pixel-level fine-grained information on the foreground objects, which
hinders the further advance of WSOL. To address this problem, we initiatively
leverage the capability of zero-shot generalization and fine-grained
segmentation in Segment Anything Model (SAM) to boost the activation of
integral object regions. Further, to alleviate the semantic ambiguity issue
accrued in single point prompt-based SAM, we propose an innovative mask prompt
to SAM (Pro2SAM) network with grid points for WSOL task. First, we devise a
Global Token Transformer (GTFormer) to generate a coarse-grained foreground map
as a flexible mask prompt, where the GTFormer jointly embeds patch tokens and
novel global tokens to learn foreground semantics. Secondly, we deliver grid
points as dense prompts into SAM to maximize the probability of foreground
mask, which avoids the lack of objects caused by a single point/box prompt.
Finally, we propose a pixel-level similarity metric to come true the mask
matching from mask prompt to SAM, where the mask with the highest score is
viewed as the final localization map. Experiments show that the proposed
Pro2SAM achieves state-of-the-art performance on both CUB-200-2011 and ILSVRC,
with 84.03\% and 66.85\% Top-1 Loc, respectively.

</details>


### [98] [SpatialPrompting: Keyframe-driven Zero-Shot Spatial Reasoning with Off-the-Shelf Multimodal Large Language Models](https://arxiv.org/pdf/2505.04911)
*Shun Taguchi, Hideki Deguchi, Takumi Hamazaki, Hiroyuki Sakai*

Main category: cs.CV

TL;DR: SpatialPrompting enables zero-shot 3D spatial reasoning using multimodal LLMs without specialized 3D inputs or fine-tuning, outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing methods require costly 3D-specific fine-tuning and inputs, limiting scalability and flexibility.

Method: Uses keyframe-driven prompts with vision-language similarity, Mahalanobis distance, field of view, and sharpness to infer 3D structures.

Result: Achieves state-of-the-art zero-shot performance on ScanQA and SQA3D datasets.

Conclusion: Offers a simpler, scalable alternative to conventional 3D reasoning methods.

Abstract: This study introduces SpatialPrompting, a novel framework that harnesses the
emergent reasoning capabilities of off-the-shelf multimodal large language
models to achieve zero-shot spatial reasoning in three-dimensional (3D)
environments. Unlike existing methods that rely on expensive 3D-specific
fine-tuning with specialized 3D inputs such as point clouds or voxel-based
features, SpatialPrompting employs a keyframe-driven prompt generation
strategy. This framework uses metrics such as vision-language similarity,
Mahalanobis distance, field of view, and image sharpness to select a diverse
and informative set of keyframes from image sequences and then integrates them
with corresponding camera pose data to effectively abstract spatial
relationships and infer complex 3D structures. The proposed framework not only
establishes a new paradigm for flexible spatial reasoning that utilizes
intuitive visual and positional cues but also achieves state-of-the-art
zero-shot performance on benchmark datasets, such as ScanQA and SQA3D, across
several metrics. The proposed method effectively eliminates the need for
specialized 3D inputs and fine-tuning, offering a simpler and more scalable
alternative to conventional approaches.

</details>


### [99] [GlyphMastero: A Glyph Encoder for High-Fidelity Scene Text Editing](https://arxiv.org/pdf/2505.04915)
*Tong Wang, Ting Liu, Xiaochao Qu, Chengjing Wu, Luoqi Liu, Xiaolin Hu*

Main category: cs.CV

TL;DR: GlyphMastero improves scene text editing by using a glyph encoder for stroke-level precision, outperforming existing methods in accuracy and quality.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based methods struggle with high-quality text generation, especially for complex characters like Chinese, due to inadequate feature extraction and hierarchical structure modeling.

Method: GlyphMastero employs a glyph encoder with a glyph attention module and feature pyramid network to capture cross-level and multi-scale text features, enhancing stroke-level precision.

Result: Achieves 18.02% higher sentence accuracy and reduces text-region Fréchet inception distance by 53.28% compared to state-of-the-art methods.

Conclusion: GlyphMastero effectively addresses the limitations of current methods, offering superior performance in scene text editing.

Abstract: Scene text editing, a subfield of image editing, requires modifying texts in
images while preserving style consistency and visual coherence with the
surrounding environment. While diffusion-based methods have shown promise in
text generation, they still struggle to produce high-quality results. These
methods often generate distorted or unrecognizable characters, particularly
when dealing with complex characters like Chinese. In such systems, characters
are composed of intricate stroke patterns and spatial relationships that must
be precisely maintained. We present GlyphMastero, a specialized glyph encoder
designed to guide the latent diffusion model for generating texts with
stroke-level precision. Our key insight is that existing methods, despite using
pretrained OCR models for feature extraction, fail to capture the hierarchical
nature of text structures - from individual strokes to stroke-level
interactions to overall character-level structure. To address this, our glyph
encoder explicitly models and captures the cross-level interactions between
local-level individual characters and global-level text lines through our novel
glyph attention module. Meanwhile, our model implements a feature pyramid
network to fuse the multi-scale OCR backbone features at the global-level.
Through these cross-level and multi-scale fusions, we obtain more detailed
glyph-aware guidance, enabling precise control over the scene text generation
process. Our method achieves an 18.02\% improvement in sentence accuracy over
the state-of-the-art multi-lingual scene text editing baseline, while
simultaneously reducing the text-region Fr\'echet inception distance by
53.28\%.

</details>


### [100] [A Simple Detector with Frame Dynamics is a Strong Tracker](https://arxiv.org/pdf/2505.04917)
*Chenxu Peng, Chenxu Wang, Minrui Zou, Danyang Li, Zhengpeng Yang, Yimian Dai, Ming-Ming Cheng, Xiang Li*

Main category: cs.CV

TL;DR: A new infrared tiny-object tracker improves performance by integrating global detection, motion-aware learning, and temporal priors, outperforming existing methods in Anti-UAV applications.


<details>
  <summary>Details</summary>
Motivation: Existing trackers struggle with tiny targets due to reliance on cropped templates and limited motion modeling.

Method: Combines frame dynamics (frame difference and optical flow) for input-level motion encoding and trajectory constraint filtering for post-processing.

Result: Achieves state-of-the-art performance, winning 1st place in Track 1 and 2nd place in Track 2 of the 4th Anti-UAV Challenge.

Conclusion: The proposed tracker effectively addresses challenges in infrared tiny-object tracking, demonstrating superior robustness and accuracy.

Abstract: Infrared object tracking plays a crucial role in Anti-Unmanned Aerial Vehicle
(Anti-UAV) applications. Existing trackers often depend on cropped template
regions and have limited motion modeling capabilities, which pose challenges
when dealing with tiny targets. To address this, we propose a simple yet
effective infrared tiny-object tracker that enhances tracking performance by
integrating global detection and motion-aware learning with temporal priors.
Our method is based on object detection and achieves significant improvements
through two key innovations. First, we introduce frame dynamics, leveraging
frame difference and optical flow to encode both prior target features and
motion characteristics at the input level, enabling the model to better
distinguish the target from background clutter. Second, we propose a trajectory
constraint filtering strategy in the post-processing stage, utilizing
spatio-temporal priors to suppress false positives and enhance tracking
robustness. Extensive experiments show that our method consistently outperforms
existing approaches across multiple metrics in challenging infrared UAV
tracking scenarios. Notably, we achieve state-of-the-art performance in the 4th
Anti-UAV Challenge, securing 1st place in Track 1 and 2nd place in Track 2.

</details>


### [101] [Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models](https://arxiv.org/pdf/2505.04921)
*Yunxin Li, Zhenyu Liu, Zitao Li, Xuanyu Zhang, Zhenran Xu, Xinyu Chen, Haoyuan Shi, Shenyuan Jiang, Xintong Wang, Jifang Wang, Shouzheng Huang, Xinping Zhao, Borui Jiang, Lanqing Hong, Longyue Wang, Zhuotao Tian, Baoxing Huai, Wenhan Luo, Weihua Luo, Zheng Zhang, Baotian Hu, Min Zhang*

Main category: cs.CV

TL;DR: The paper surveys multimodal reasoning research, tracing its evolution from modular pipelines to unified frameworks, and highlights challenges and future directions like native large multimodal reasoning models (N-LMRMs).


<details>
  <summary>Details</summary>
Motivation: To address the need for robust and adaptive reasoning in AI systems operating in open, uncertain, and multimodal environments.

Method: A structured survey organized around a four-stage developmental roadmap, reviewing early task-specific modules and recent unified multimodal LLMs.

Result: Identifies advancements like Multimodal Chain-of-Thought (MCoT) and multimodal reinforcement learning, but notes challenges in generalization, depth, and agentic behavior.

Conclusion: Proposes the future direction of N-LMRMs for scalable, adaptive reasoning in complex real-world environments.

Abstract: Reasoning lies at the heart of intelligence, shaping the ability to make
decisions, draw conclusions, and generalize across domains. In artificial
intelligence, as systems increasingly operate in open, uncertain, and
multimodal environments, reasoning becomes essential for enabling robust and
adaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a
promising paradigm, integrating modalities such as text, images, audio, and
video to support complex reasoning capabilities and aiming to achieve
comprehensive perception, precise understanding, and deep reasoning. As
research advances, multimodal reasoning has rapidly evolved from modular,
perception-driven pipelines to unified, language-centric frameworks that offer
more coherent cross-modal understanding. While instruction tuning and
reinforcement learning have improved model reasoning, significant challenges
remain in omni-modal generalization, reasoning depth, and agentic behavior. To
address these issues, we present a comprehensive and structured survey of
multimodal reasoning research, organized around a four-stage developmental
roadmap that reflects the field's shifting design philosophies and emerging
capabilities. First, we review early efforts based on task-specific modules,
where reasoning was implicitly embedded across stages of representation,
alignment, and fusion. Next, we examine recent approaches that unify reasoning
into multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT)
and multimodal reinforcement learning enabling richer and more structured
reasoning chains. Finally, drawing on empirical insights from challenging
benchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the
conceptual direction of native large multimodal reasoning models (N-LMRMs),
which aim to support scalable, agentic, and adaptive reasoning and planning in
complex, real-world environments.

</details>


### [102] [Canny2Palm: Realistic and Controllable Palmprint Generation for Large-scale Pre-training](https://arxiv.org/pdf/2505.04922)
*Xingzeng Lan, Xing Duan, Chen Chen, Weiyu Lin, Bo Wang*

Main category: cs.CV

TL;DR: Canny2Palm, a novel method for synthesizing realistic palmprints using Canny edge detector and Pix2Pix, improves recognition accuracy by 7.2% and scales well with large datasets.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity of palmprint data for biometric identification by generating synthetic palmprints.

Method: Uses Canny edge detector to extract textures and conditions a Pix2Pix network for realistic generation, enabling new identity creation.

Result: Achieves 7.2% higher accuracy on benchmarks and scales better with 10,000 synthetic IDs.

Conclusion: Canny2Palm is effective for large-scale pre-training and outperforms existing methods.

Abstract: Palmprint recognition is a secure and privacy-friendly method of biometric
identification. One of the major challenges to improve palmprint recognition
accuracy is the scarcity of palmprint data. Recently, a popular line of
research revolves around the synthesis of virtual palmprints for large-scale
pre-training purposes. In this paper, we propose a novel synthesis method named
Canny2Palm that extracts palm textures with Canny edge detector and uses them
to condition a Pix2Pix network for realistic palmprint generation. By
re-assembling palmprint textures from different identities, we are able to
create new identities by seeding the generator with new assemblies. Canny2Palm
not only synthesizes realistic data following the distribution of real
palmprints but also enables controllable diversity to generate large-scale new
identities. On open-set palmprint recognition benchmarks, models pre-trained
with Canny2Palm synthetic data outperform the state-of-the-art with up to 7.2%
higher identification accuracy. Moreover, the performance of models pre-trained
with Canny2Palm continues to improve given 10,000 synthetic IDs while those
with existing methods already saturate, demonstrating the potential of our
method for large-scale pre-training.

</details>


### [103] [FF-PNet: A Pyramid Network Based on Feature and Field for Brain Image Registration](https://arxiv.org/pdf/2505.04938)
*Ying Zhang, Shuai Guo, Chenxi Sun, Yuchen Zhu, Jinhai Xiang*

Main category: cs.CV

TL;DR: A new pyramid registration network (FF-PNet) with parallel coarse and fine-grained feature extraction modules (RFFM and RDFFM) improves deformable medical image registration efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing models lack efficiency in parallel extraction of coarse and fine-grained features for deformable medical image registration.

Method: Proposes FF-PNet with Residual Feature Fusion Module (RFFM) for coarse features and Residual Deformation Field Fusion Module (RDFFM) for fine-grained deformation, using traditional CNNs without attention mechanisms.

Result: Outperforms popular methods on LPBA and OASIS datasets, achieving higher Dice Similarity Coefficient.

Conclusion: FF-PNet demonstrates superior feature decoding and deformation handling, improving registration accuracy without complex mechanisms.

Abstract: In recent years, deformable medical image registration techniques have made
significant progress. However, existing models still lack efficiency in
parallel extraction of coarse and fine-grained features. To address this, we
construct a new pyramid registration network based on feature and deformation
field (FF-PNet). For coarse-grained feature extraction, we design a Residual
Feature Fusion Module (RFFM), for fine-grained image deformation, we propose a
Residual Deformation Field Fusion Module (RDFFM). Through the parallel
operation of these two modules, the model can effectively handle complex image
deformations. It is worth emphasizing that the encoding stage of FF-PNet only
employs traditional convolutional neural networks without any attention
mechanisms or multilayer perceptrons, yet it still achieves remarkable
improvements in registration accuracy, fully demonstrating the superior feature
decoding capabilities of RFFM and RDFFM. We conducted extensive experiments on
the LPBA and OASIS datasets. The results show our network consistently
outperforms popular methods in metrics like the Dice Similarity Coefficient.

</details>


### [104] [Building-Guided Pseudo-Label Learning for Cross-Modal Building Damage Mapping](https://arxiv.org/pdf/2505.04941)
*Jiepan Li, He Huang, Yu Sheng, Yujun Guo, Wei He*

Main category: cs.CV

TL;DR: A novel framework for building damage assessment using bi-temporal multi-modal remote sensing images, combining building extraction and change detection with pseudo-label refinement for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate building damage assessment is crucial for disaster response and recovery planning, but mapping damage from pre-disaster optical and post-disaster SAR images is challenging.

Method: The framework involves training building extraction models, refining them with pseudo-labels, and using building priors to guide damage classification in a change detection model.

Result: Achieved the highest mIoU score (54.28%) and first place in the 2025 IEEE GRSS Data Fusion Contest.

Conclusion: The proposed framework effectively addresses challenges in building damage assessment, demonstrating superior performance in accuracy and reliability.

Abstract: Accurate building damage assessment using bi-temporal multi-modal remote
sensing images is essential for effective disaster response and recovery
planning. This study proposes a novel Building-Guided Pseudo-Label Learning
Framework to address the challenges of mapping building damage from
pre-disaster optical and post-disaster SAR images. First, we train a series of
building extraction models using pre-disaster optical images and building
labels. To enhance building segmentation, we employ multi-model fusion and
test-time augmentation strategies to generate pseudo-probabilities, followed by
a low-uncertainty pseudo-label training method for further refinement. Next, a
change detection model is trained on bi-temporal cross-modal images and damaged
building labels. To improve damage classification accuracy, we introduce a
building-guided low-uncertainty pseudo-label refinement strategy, which
leverages building priors from the previous step to guide pseudo-label
generation for damaged buildings, reducing uncertainty and enhancing
reliability. Experimental results on the 2025 IEEE GRSS Data Fusion Contest
dataset demonstrate the effectiveness of our approach, which achieved the
highest mIoU score (54.28%) and secured first place in the competition.

</details>


### [105] [T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video Generation Models](https://arxiv.org/pdf/2505.04946)
*Xuyang Guo, Jiayan Huo, Zhenmei Shi, Zhao Song, Jiahao Zhang, Jiale Zhao*

Main category: cs.CV

TL;DR: The paper introduces T2VTextBench, a benchmark for evaluating text fidelity and consistency in text-to-video models, revealing significant shortcomings in current systems.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in text-to-video generation, models struggle with rendering precise on-screen text, limiting applications requiring textual accuracy.

Method: The authors create T2VTextBench, a human-evaluation benchmark with prompts combining complex text and dynamic scenes, testing ten state-of-the-art models.

Result: Most models fail to generate legible, consistent text, exposing a critical gap in current video generation capabilities.

Conclusion: The findings emphasize the need for future research to improve textual manipulation in video synthesis.

Abstract: Thanks to recent advancements in scalable deep architectures and large-scale
pretraining, text-to-video generation has achieved unprecedented capabilities
in producing high-fidelity, instruction-following content across a wide range
of styles, enabling applications in advertising, entertainment, and education.
However, these models' ability to render precise on-screen text, such as
captions or mathematical formulas, remains largely untested, posing significant
challenges for applications requiring exact textual accuracy. In this work, we
introduce T2VTextBench, the first human-evaluation benchmark dedicated to
evaluating on-screen text fidelity and temporal consistency in text-to-video
models. Our suite of prompts integrates complex text strings with dynamic scene
changes, testing each model's ability to maintain detailed instructions across
frames. We evaluate ten state-of-the-art systems, ranging from open-source
solutions to commercial offerings, and find that most struggle to generate
legible, consistent text. These results highlight a critical gap in current
video generators and provide a clear direction for future research aimed at
enhancing textual manipulation in video synthesis.

</details>


### [106] [Rethinking Video Super-Resolution: Towards Diffusion-Based Methods without Motion Alignment](https://arxiv.org/pdf/2503.03355)
*Zhihao Zhan, Wang Pang, Xiang Zhu, Yechao Bai*

Main category: cs.CV

TL;DR: A diffusion-based, alignment-free video super-resolution method using a diffusion transformer in latent space, eliminating the need for explicit motion estimation.


<details>
  <summary>Details</summary>
Motivation: To simplify video super-resolution by leveraging a powerful model that inherently learns real-world physics and motion patterns, avoiding explicit optical flow or motion parameter estimation.

Method: Uses a Diffusion Posterior Sampling framework with an unconditional video diffusion transformer in latent space, acting as a space-time model.

Result: Demonstrates feasibility on synthetic and real-world datasets, showing adaptability to different sampling conditions without re-training.

Conclusion: The proposed method effectively handles video super-resolution without explicit alignment, leveraging learned motion priors.

Abstract: In this work, we rethink the approach to video super-resolution by
introducing a method based on the Diffusion Posterior Sampling framework,
combined with an unconditional video diffusion transformer operating in latent
space. The video generation model, a diffusion transformer, functions as a
space-time model. We argue that a powerful model, which learns the physics of
the real world, can easily handle various kinds of motion patterns as prior
knowledge, thus eliminating the need for explicit estimation of optical flows
or motion parameters for pixel alignment. Furthermore, a single instance of the
proposed video diffusion transformer model can adapt to different sampling
conditions without re-training. Empirical results on synthetic and real-world
datasets illustrate the feasibility of diffusion-based, alignment-free video
super-resolution.

</details>


### [107] [An Efficient Method for Accurate Pose Estimation and Error Correction of Cuboidal Objects](https://arxiv.org/pdf/2505.04962)
*Utsav Rai, Hardik Mehta, Vismay Vakharia, Aditya Choudhary, Amit Parmar, Rolif Lima, Kaushik Das*

Main category: cs.CV

TL;DR: A system for precise pose estimation of cuboid-shaped objects, reducing errors efficiently.


<details>
  <summary>Details</summary>
Motivation: Addressing minor pose errors in autonomous picking of cuboidal objects from piles.

Method: Proposes a linear time approach for pose error estimation and correction, avoiding overhead of local registration algorithms.

Result: Efficient and precise pose estimation for cuboid-shaped objects.

Conclusion: The proposed method improves accuracy and reduces time for pose estimation in autonomous picking tasks.

Abstract: The proposed system outlined in this paper is a solution to a use case that
requires the autonomous picking of cuboidal objects from an organized or
unorganized pile with high precision. This paper presents an efficient method
for precise pose estimation of cuboid-shaped objects, which aims to reduce
errors in target pose in a time-efficient manner. Typical pose estimation
methods like global point cloud registrations are prone to minor pose errors
for which local registration algorithms are generally used to improve pose
accuracy. However, due to the execution time overhead and uncertainty in the
error of the final achieved pose, an alternate, linear time approach is
proposed for pose error estimation and correction. This paper presents an
overview of the solution followed by a detailed description of individual
modules of the proposed algorithm.

</details>


### [108] [MaskAttn-UNet: A Mask Attention-Driven Framework for Universal Low-Resolution Image Segmentation](https://arxiv.org/pdf/2503.10686)
*Anzhe Cheng, Chenzhong Yin, Yu Chang, Heng Ping, Shixuan Li, Shahin Nazarian, Paul Bogdan*

Main category: cs.CV

TL;DR: MaskAttn-UNet improves low-resolution image segmentation by integrating a mask attention mechanism into U-Net, balancing local and contextual features for better accuracy at lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Low-resolution image segmentation is essential for applications like robotics and augmented reality, but existing methods struggle with computational constraints and cluttered scenes.

Method: Proposes MaskAttn-UNet, a U-Net variant with a mask attention mechanism to focus on important regions and suppress backgrounds, enhancing segmentation for low-resolution inputs.

Result: Achieves competitive performance on semantic, instance, and panoptic segmentation tasks at 128x128 resolution, with lower computational cost than transformer-based models.

Conclusion: MaskAttn-UNet is an efficient, scalable solution for low-resolution segmentation, suitable for resource-constrained scenarios.

Abstract: Low-resolution image segmentation is crucial in real-world applications such
as robotics, augmented reality, and large-scale scene understanding, where
high-resolution data is often unavailable due to computational constraints. To
address this challenge, we propose MaskAttn-UNet, a novel segmentation
framework that enhances the traditional U-Net architecture via a mask attention
mechanism. Our model selectively emphasizes important regions while suppressing
irrelevant backgrounds, thereby improving segmentation accuracy in cluttered
and complex scenes. Unlike conventional U-Net variants, MaskAttn-UNet
effectively balances local feature extraction with broader contextual
awareness, making it particularly well-suited for low-resolution inputs. We
evaluate our approach on three benchmark datasets with input images rescaled to
128x128 and demonstrate competitive performance across semantic, instance, and
panoptic segmentation tasks. Our results show that MaskAttn-UNet achieves
accuracy comparable to state-of-the-art methods at significantly lower
computational cost than transformer-based models, making it an efficient and
scalable solution for low-resolution segmentation in resource-constrained
scenarios.

</details>


### [109] [ViCTr: Vital Consistency Transfer for Pathology Aware Image Synthesis](https://arxiv.org/pdf/2505.04963)
*Onkar Susladkar, Gayatri Deshmukh, Yalcin Tur, Ulas Bagci*

Main category: cs.CV

TL;DR: ViCTr introduces a two-stage framework for high-fidelity medical image synthesis, combining rectified flow and Tweedie-corrected diffusion, achieving state-of-the-art results with efficient one-step sampling.


<details>
  <summary>Details</summary>
Motivation: Challenges in medical image synthesis include limited annotated pathological data, domain gaps, and complex pathologies like liver cirrhosis. Existing methods lack anatomical fidelity and efficiency.

Method: ViCTr uses a two-stage approach: pretraining with EWC for anatomical fidelity and fine-tuning with LoRA for pathology control. It reformulates Tweedie's formula for one-step sampling.

Result: ViCTr achieves MFID of 17.01 for cirrhosis synthesis (28% better than others) and improves nnUNet segmentation by +3.8% mDSC. Radiologists find its outputs clinically indistinguishable.

Conclusion: ViCTr is the first method to offer fine-grained, pathology-aware MRI synthesis with severity control, advancing AI-driven medical imaging.

Abstract: Synthesizing medical images remains challenging due to limited annotated
pathological data, modality domain gaps, and the complexity of representing
diffuse pathologies such as liver cirrhosis. Existing methods often struggle to
maintain anatomical fidelity while accurately modeling pathological features,
frequently relying on priors derived from natural images or inefficient
multi-step sampling. In this work, we introduce ViCTr (Vital Consistency
Transfer), a novel two-stage framework that combines a rectified flow
trajectory with a Tweedie-corrected diffusion process to achieve high-fidelity,
pathology-aware image synthesis. First, we pretrain ViCTr on the ATLAS-8k
dataset using Elastic Weight Consolidation (EWC) to preserve critical
anatomical structures. We then fine-tune the model adversarially with Low-Rank
Adaptation (LoRA) modules for precise control over pathology severity. By
reformulating Tweedie's formula within a linear trajectory framework, ViCTr
supports one-step sampling, reducing inference from 50 steps to just 4, without
sacrificing anatomical realism. We evaluate ViCTr on BTCV (CT), AMOS (MRI), and
CirrMRI600+ (cirrhosis) datasets. Results demonstrate state-of-the-art
performance, achieving a Medical Frechet Inception Distance (MFID) of 17.01 for
cirrhosis synthesis 28% lower than existing approaches and improving nnUNet
segmentation by +3.8% mDSC when used for data augmentation. Radiologist reviews
indicate that ViCTr-generated liver cirrhosis MRIs are clinically
indistinguishable from real scans. To our knowledge, ViCTr is the first method
to provide fine-grained, pathology-aware MRI synthesis with graded severity
control, closing a critical gap in AI-driven medical imaging research.

</details>


### [110] [CAG-VLM: Fine-Tuning of a Large-Scale Model to Recognize Angiographic Images for Next-Generation Diagnostic Systems](https://arxiv.org/pdf/2505.04964)
*Yuto Nakamura, Satoshi Kodera, Haruki Settai, Hiroki Shinohara, Masatsugu Tamura, Tomohiro Noguchi, Tatsuki Furusawa, Ryo Takizawa, Tempei Kabayama, Norihiko Takeda*

Main category: cs.CV

TL;DR: A two-stage AI pipeline and bilingual dataset for coronary angiography (CAG) analysis, achieving high accuracy in laterality classification and effective clinical report generation via fine-tuned VLMs.


<details>
  <summary>Details</summary>
Motivation: To provide AI-based decision support for interpreting CAG images, reducing reliance on expert cardiologists.

Method: A two-stage approach: (1) annotate and train a CNN for key-frame detection and laterality classification, (2) fine-tune VLMs on paired CAG images and reports for clinical decision support.

Result: CNN achieved 0.96 F1 on laterality classification; Gemma3 w/LoRA (CAG-VLM) scored highest in clinician ratings (7.20/10).

Conclusion: Specialized VLMs can effectively assist cardiologists in generating reports and treatment recommendations from CAG images.

Abstract: Coronary angiography (CAG) is the gold-standard imaging modality for
evaluating coronary artery disease, but its interpretation and subsequent
treatment planning rely heavily on expert cardiologists. To enable AI-based
decision support, we introduce a two-stage, physician-curated pipeline and a
bilingual (Japanese/English) CAG image-report dataset. First, we sample 14,686
frames from 539 exams and annotate them for key-frame detection and left/right
laterality; a ConvNeXt-Base CNN trained on this data achieves 0.96 F1 on
laterality classification, even on low-contrast frames. Second, we apply the
CNN to 243 independent exams, extract 1,114 key frames, and pair each with its
pre-procedure report and expert-validated diagnostic and treatment summary,
yielding a parallel corpus. We then fine-tune three open-source VLMs
(PaliGemma2, Gemma3, and ConceptCLIP-enhanced Gemma3) via LoRA and evaluate
them using VLScore and cardiologist review. Although PaliGemma2 w/LoRA attains
the highest VLScore, Gemma3 w/LoRA achieves the top clinician rating (mean
7.20/10); we designate this best-performing model as CAG-VLM. These results
demonstrate that specialized, fine-tuned VLMs can effectively assist
cardiologists in generating clinical reports and treatment recommendations from
CAG images.

</details>


### [111] [DenseGrounding: Improving Dense Language-Vision Semantics for Ego-Centric 3D Visual Grounding](https://arxiv.org/pdf/2505.04965)
*Henry Zheng, Hao Shi, Qihang Peng, Yong Xien Chng, Rui Huang, Yepeng Weng, Zhongchao Shi, Gao Huang*

Main category: cs.CV

TL;DR: DenseGrounding improves 3D visual grounding by enhancing visual and textual semantics, outperforming existing methods and winning a CVPR 2024 award.


<details>
  <summary>Details</summary>
Motivation: Advancing robotics and human-computer interaction by enabling agents to comprehend and interact with 3D environments using natural language.

Method: Proposes DenseGrounding with Hierarchical Scene Semantic Enhancer for visual features and Language Semantic Enhancer for text descriptions.

Result: Achieves 5.81% and 7.56% accuracy improvements on full and mini datasets, and wins CVPR 2024 award.

Conclusion: DenseGrounding effectively addresses challenges in 3D visual grounding, setting a new SOTA.

Abstract: Enabling intelligent agents to comprehend and interact with 3D environments
through natural language is crucial for advancing robotics and human-computer
interaction. A fundamental task in this field is ego-centric 3D visual
grounding, where agents locate target objects in real-world 3D spaces based on
verbal descriptions. However, this task faces two significant challenges: (1)
loss of fine-grained visual semantics due to sparse fusion of point clouds with
ego-centric multi-view images, (2) limited textual semantic context due to
arbitrary language descriptions. We propose DenseGrounding, a novel approach
designed to address these issues by enhancing both visual and textual
semantics. For visual features, we introduce the Hierarchical Scene Semantic
Enhancer, which retains dense semantics by capturing fine-grained global scene
features and facilitating cross-modal alignment. For text descriptions, we
propose a Language Semantic Enhancer that leverages large language models to
provide rich context and diverse language descriptions with additional context
during model training. Extensive experiments show that DenseGrounding
significantly outperforms existing methods in overall accuracy, with
improvements of 5.81% and 7.56% when trained on the comprehensive full dataset
and smaller mini subset, respectively, further advancing the SOTA in egocentric
3D visual grounding. Our method also achieves 1st place and receives the
Innovation Award in the CVPR 2024 Autonomous Grand Challenge Multi-view 3D
Visual Grounding Track, validating its effectiveness and robustness.

</details>


### [112] [ReAlign: Bilingual Text-to-Motion Generation via Step-Aware Reward-Guided Alignment](https://arxiv.org/pdf/2505.04974)
*Wanjiang Weng, Xiaofeng Tan, Hongsong Wang, Pan Zhou*

Main category: cs.CV

TL;DR: BiHumanML3D introduces a bilingual motion dataset and BiMD model for text-to-motion generation, enhanced by ReAlign for better alignment and quality.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of bilingual datasets and misalignment in diffusion models for cross-linguistic applications.

Method: Proposes BiHumanML3D dataset, BiMD model with cross-lingual alignment, and ReAlign for reward-guided sampling.

Result: Significant improvement in text-motion alignment and motion quality over existing methods.

Conclusion: The approach sets a benchmark for bilingual text-to-motion generation, enhancing semantic consistency and realism.

Abstract: Bilingual text-to-motion generation, which synthesizes 3D human motions from
bilingual text inputs, holds immense potential for cross-linguistic
applications in gaming, film, and robotics. However, this task faces critical
challenges: the absence of bilingual motion-language datasets and the
misalignment between text and motion distributions in diffusion models, leading
to semantically inconsistent or low-quality motions. To address these
challenges, we propose BiHumanML3D, a novel bilingual human motion dataset,
which establishes a crucial benchmark for bilingual text-to-motion generation
models. Furthermore, we propose a Bilingual Motion Diffusion model (BiMD),
which leverages cross-lingual aligned representations to capture semantics,
thereby achieving a unified bilingual model. Building upon this, we propose
Reward-guided sampling Alignment (ReAlign) method, comprising a step-aware
reward model to assess alignment quality during sampling and a reward-guided
strategy that directs the diffusion process toward an optimally aligned
distribution. This reward model integrates step-aware tokens and combines a
text-aligned module for semantic consistency and a motion-aligned module for
realism, refining noisy motions at each timestep to balance probability density
and alignment. Experiments demonstrate that our approach significantly improves
text-motion alignment and motion quality compared to existing state-of-the-art
methods. Project page: https://wengwanjiang.github.io/ReAlign-page/.

</details>


### [113] [Federated Deconfounding and Debiasing Learning for Out-of-Distribution Generalization](https://arxiv.org/pdf/2505.04979)
*Zhuang Qi, Sijin Zhou, Lei Meng, Han Hu, Han Yu, Xiangxu Meng*

Main category: cs.CV

TL;DR: FedDDL addresses attribute bias in federated learning by using causal graphs and backdoor adjustment, improving model focus on main objects and accuracy.


<details>
  <summary>Details</summary>
Motivation: Attribute bias in FL causes inconsistent local model optimization due to non-causal associations, degrading performance. Existing methods lack comprehensive inference path analysis.

Method: FedDDL constructs a causal graph for inference analysis, performs backdoor adjustment, and includes intra-client deconfounding and inter-client debiasing modules.

Result: FedDDL achieves 4.5% higher Top-1 Accuracy on average over 9 state-of-the-art methods on 2 datasets.

Conclusion: FedDDL effectively mitigates attribute bias in FL, enhancing model performance by focusing on main objects and reducing confounding effects.

Abstract: Attribute bias in federated learning (FL) typically leads local models to
optimize inconsistently due to the learning of non-causal associations,
resulting degraded performance. Existing methods either use data augmentation
for increasing sample diversity or knowledge distillation for learning
invariant representations to address this problem. However, they lack a
comprehensive analysis of the inference paths, and the interference from
confounding factors limits their performance. To address these limitations, we
propose the \underline{Fed}erated \underline{D}econfounding and
\underline{D}ebiasing \underline{L}earning (FedDDL) method. It constructs a
structured causal graph to analyze the model inference process, and performs
backdoor adjustment to eliminate confounding paths. Specifically, we design an
intra-client deconfounding learning module for computer vision tasks to
decouple background and objects, generating counterfactual samples that
establish a connection between the background and any label, which stops the
model from using the background to infer the label. Moreover, we design an
inter-client debiasing learning module to construct causal prototypes to reduce
the proportion of the background in prototype components. Notably, it bridges
the gap between heterogeneous representations via causal prototypical
regularization. Extensive experiments on 2 benchmarking datasets demonstrate
that \methodname{} significantly enhances the model capability to focus on main
objects in unseen data, leading to 4.5\% higher Top-1 Accuracy on average over
9 state-of-the-art existing methods.

</details>


### [114] [StabStitch++: Unsupervised Online Video Stitching with Spatiotemporal Bidirectional Warps](https://arxiv.org/pdf/2505.05001)
*Lang Nie, Chunyu Lin, Kang Liao, Yun Zhang, Shuaicheng Liu, Yao Zhao*

Main category: cs.CV

TL;DR: StabStitch++ addresses warping shake in video stitching by combining spatial stitching and temporal stabilization using unsupervised learning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The paper tackles the issue of warping shake in video stitching, where temporal instability arises from unsmooth warps, degrading visual quality even with stable input videos.

Method: The framework introduces a virtual midplane for bidirectional homography decomposition, integrates spatial and temporal warps for smooth trajectories, and uses a hybrid loss for alignment and stabilization.

Result: StabStitch++ achieves superior stitching performance, robustness, and efficiency, enabling real-time online video stitching.

Conclusion: The proposed method advances video stitching by optimizing alignment and stabilization simultaneously, validated by a diverse dataset and outperforming existing solutions.

Abstract: We retarget video stitching to an emerging issue, named warping shake, which
unveils the temporal content shakes induced by sequentially unsmooth warps when
extending image stitching to video stitching. Even if the input videos are
stable, the stitched video can inevitably cause undesired warping shakes and
affect the visual experience. To address this issue, we propose StabStitch++, a
novel video stitching framework to realize spatial stitching and temporal
stabilization with unsupervised learning simultaneously. First, different from
existing learning-based image stitching solutions that typically warp one image
to align with another, we suppose a virtual midplane between original image
planes and project them onto it. Concretely, we design a differentiable
bidirectional decomposition module to disentangle the homography transformation
and incorporate it into our spatial warp, evenly spreading alignment burdens
and projective distortions across two views. Then, inspired by camera paths in
video stabilization, we derive the mathematical expression of stitching
trajectories in video stitching by elaborately integrating spatial and temporal
warps. Finally, a warp smoothing model is presented to produce stable stitched
videos with a hybrid loss to simultaneously encourage content alignment,
trajectory smoothness, and online collaboration. Compared with StabStitch that
sacrifices alignment for stabilization, StabStitch++ makes no compromise and
optimizes both of them simultaneously, especially in the online mode. To
establish an evaluation benchmark and train the learning framework, we build a
video stitching dataset with a rich diversity in camera motions and scenes.
Experiments exhibit that StabStitch++ surpasses current solutions in stitching
performance, robustness, and efficiency, offering compelling advancements in
this field by building a real-time online video stitching system.

</details>


### [115] [Automated Thoracolumbar Stump Rib Detection and Analysis in a Large CT Cohort](https://arxiv.org/pdf/2505.05004)
*Hendrik Möller, Hanna Schön, Alina Dima, Benjamin Keinert-Weth, Robert Graf, Matan Atad, Johannes Paetzold, Friederike Jungmann, Rickmer Braren, Florian Kofler, Bjoern Menze, Daniel Rueckert, Jan S. Kirschke*

Main category: cs.CV

TL;DR: The paper automates thoracolumbar stump rib detection using deep learning, achieving high accuracy in segmentation and morphological analysis.


<details>
  <summary>Details</summary>
Motivation: To address the manual and qualitative assessment of thoracolumbar stump ribs by automating detection and providing quantitative morphological analysis.

Method: A high-resolution deep-learning model for rib segmentation, combined with an iterative algorithm and piece-wise linear interpolation for rib length assessment.

Result: Significant improvements in segmentation (Dice score 0.997), 98.2% success rate in rib length assessment, and morphological differences in stump ribs (e.g., thinner, more posterior articulation).

Conclusion: The automated approach effectively differentiates stump ribs from regular ones (F1-score 0.84) and provides publicly available model weights and masks.

Abstract: Thoracolumbar stump ribs are one of the essential indicators of thoracolumbar
transitional vertebrae or enumeration anomalies. While some studies manually
assess these anomalies and describe the ribs qualitatively, this study aims to
automate thoracolumbar stump rib detection and analyze their morphology
quantitatively. To this end, we train a high-resolution deep-learning model for
rib segmentation and show significant improvements compared to existing models
(Dice score 0.997 vs. 0.779, p-value < 0.01). In addition, we use an iterative
algorithm and piece-wise linear interpolation to assess the length of the ribs,
showing a success rate of 98.2%. When analyzing morphological features, we show
that stump ribs articulate more posteriorly at the vertebrae (-19.2 +- 3.8 vs
-13.8 +- 2.5, p-value < 0.01), are thinner (260.6 +- 103.4 vs. 563.6 +- 127.1,
p-value < 0.01), and are oriented more downwards and sideways within the first
centimeters in contrast to full-length ribs. We show that with partially
visible ribs, these features can achieve an F1-score of 0.84 in differentiating
stump ribs from regular ones. We publish the model weights and masks for public
use.

</details>


### [116] [Driving with Context: Online Map Matching for Complex Roads Using Lane Markings and Scenario Recognition](https://arxiv.org/pdf/2505.05007)
*Xin Bi, Zhichao Li, Yuxuan Xia, Panpan Tong, Lijuan Zhang, Yang Chen, Junsheng Fu*

Main category: cs.CV

TL;DR: Proposes an online SD map matching method using HMM with multiple probability factors for accurate navigation in complex road networks, validated by road tests in Europe and China.


<details>
  <summary>Details</summary>
Motivation: Current online map matching methods struggle with accuracy in complex road networks, especially multilevel roads, necessitating a more robust solution.

Method: Uses HMM with lane markings and scenario recognition to build an enriched SD map, leveraging ICP registration and driving scenario recognition for improved accuracy.

Result: Achieves F1 scores of 98.04% and 94.60% on benchmark datasets, outperforming existing methods.

Conclusion: The proposed method significantly enhances online map matching accuracy, particularly in multilevel road areas.

Abstract: Accurate online map matching is fundamental to vehicle navigation and the
activation of intelligent driving functions. Current online map matching
methods are prone to errors in complex road networks, especially in multilevel
road area. To address this challenge, we propose an online Standard Definition
(SD) map matching method by constructing a Hidden Markov Model (HMM) with
multiple probability factors. Our proposed method can achieve accurate map
matching even in complex road networks by carefully leveraging lane markings
and scenario recognition in the designing of the probability factors. First,
the lane markings are generated by a multi-lane tracking method and associated
with the SD map using HMM to build an enriched SD map. In areas covered by the
enriched SD map, the vehicle can re-localize itself by performing Iterative
Closest Point (ICP) registration for the lane markings. Then, the probability
factor accounting for the lane marking detection can be obtained using the
association probability between adjacent lanes and roads. Second, the driving
scenario recognition model is applied to generate the emission probability
factor of scenario recognition, which improves the performance of map matching
on elevated roads and ordinary urban roads underneath them. We validate our
method through extensive road tests in Europe and China, and the experimental
results show that our proposed method effectively improves the online map
matching accuracy as compared to other existing methods, especially in
multilevel road area. Specifically, the experiments show that our proposed
method achieves $F_1$ scores of 98.04% and 94.60% on the Zenseact Open Dataset
and test data of multilevel road areas in Shanghai respectively, significantly
outperforming benchmark methods. The implementation is available at
https://github.com/TRV-Lab/LMSR-OMM.

</details>


### [117] [Adaptive Contextual Embedding for Robust Far-View Borehole Detection](https://arxiv.org/pdf/2505.05008)
*Xuesong Liu, Tianyu Hao, Emmett J. Ientilucci*

Main category: cs.CV

TL;DR: Proposes an adaptive detection method for tiny boreholes in blasting operations, improving accuracy over YOLO with EMA-based updates, adaptive augmentation, embedding stabilization, and contextual refinement.


<details>
  <summary>Details</summary>
Motivation: Accurate detection of tiny boreholes in far-view imagery is crucial for safety and efficiency, but existing methods fail due to small scales, dense arrangements, and limited visual features.

Method: Enhances YOLO with EMA-based statistical updates, adaptive augmentation, embedding stabilization, and contextual refinement.

Result: Substantial improvements over YOLO baselines on a quarry-site dataset.

Conclusion: The method effectively addresses challenges in detecting tiny, densely arranged boreholes in industrial settings.

Abstract: In controlled blasting operations, accurately detecting densely distributed
tiny boreholes from far-view imagery is critical for operational safety and
efficiency. However, existing detection methods often struggle due to small
object scales, highly dense arrangements, and limited distinctive visual
features of boreholes. To address these challenges, we propose an adaptive
detection approach that builds upon existing architectures (e.g., YOLO) by
explicitly leveraging consistent embedding representations derived through
exponential moving average (EMA)-based statistical updates.
  Our method introduces three synergistic components: (1) adaptive augmentation
utilizing dynamically updated image statistics to robustly handle illumination
and texture variations; (2) embedding stabilization to ensure consistent and
reliable feature extraction; and (3) contextual refinement leveraging spatial
context for improved detection accuracy. The pervasive use of EMA in our method
is particularly advantageous given the limited visual complexity and small
scale of boreholes, allowing stable and robust representation learning even
under challenging visual conditions. Experiments on a challenging proprietary
quarry-site dataset demonstrate substantial improvements over baseline
YOLO-based architectures, highlighting our method's effectiveness in realistic
and complex industrial scenarios.

</details>


### [118] [SOAP: Style-Omniscient Animatable Portraits](https://arxiv.org/pdf/2505.05022)
*Tingting Liao, Yujian Zheng, Adilbek Karmanov, Liwen Hu, Leyang Jin, Yuliang Xiu, Hao Li*

Main category: cs.CV

TL;DR: SOAP is a framework for creating animatable 3D avatars from a single image, overcoming style limitations and accessory challenges. It uses a multiview diffusion model and adaptive optimization to produce rigged, topology-consistent avatars.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D avatar creation from single images face style limitations, accessory handling issues, and lack animation controls or suffer from artifacts.

Method: SOAP employs a multiview diffusion model trained on diverse 3D heads and an adaptive optimization pipeline to deform the FLAME mesh while preserving topology and rigging via differentiable rendering.

Result: The method generates textured avatars supporting FACS-based animation, integrates eyeballs and teeth, and preserves details like braided hair or accessories, outperforming state-of-the-art techniques.

Conclusion: SOAP advances single-view 3D avatar creation by addressing style and animation challenges, offering publicly available code and data for research.

Abstract: Creating animatable 3D avatars from a single image remains challenging due to
style limitations (realistic, cartoon, anime) and difficulties in handling
accessories or hairstyles. While 3D diffusion models advance single-view
reconstruction for general objects, outputs often lack animation controls or
suffer from artifacts because of the domain gap. We propose SOAP, a
style-omniscient framework to generate rigged, topology-consistent avatars from
any portrait. Our method leverages a multiview diffusion model trained on 24K
3D heads with multiple styles and an adaptive optimization pipeline to deform
the FLAME mesh while maintaining topology and rigging via differentiable
rendering. The resulting textured avatars support FACS-based animation,
integrate with eyeballs and teeth, and preserve details like braided hair or
accessories. Extensive experiments demonstrate the superiority of our method
over state-of-the-art techniques for both single-view head modeling and
diffusion-based generation of Image-to-3D. Our code and data are publicly
available for research purposes at https://github.com/TingtingLiao/soap.

</details>


### [119] [Split Matching for Inductive Zero-shot Semantic Segmentation](https://arxiv.org/pdf/2505.05023)
*Jialei Chen, Xu Zheng, Dongyue Li, Chong Yi, Seigo Ito, Danda Pani Paudel, Luc Van Gool, Hiroshi Murase, Daisuke Deguchi*

Main category: cs.CV

TL;DR: The paper proposes Split Matching (SM) for Zero-shot Semantic Segmentation (ZSS), decoupling Hungarian matching into seen and unseen class components, and introduces a Multi-scale Feature Enhancement (MFE) module to improve performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods overfit to seen categories due to lack of supervision for unseen classes, and conventional Hungarian matching misclassifies unseen categories as background.

Method: SM partitions queries into seen and candidate groups, optimizes them independently, and uses CLIP features for pseudo masks. MFE refines decoder features.

Result: SM achieves state-of-the-art performance on two standard benchmarks.

Conclusion: SM effectively addresses the limitations of conventional Hungarian matching in ZSS, improving segmentation of unseen categories.

Abstract: Zero-shot Semantic Segmentation (ZSS) aims to segment categories that are not
annotated during training. While fine-tuning vision-language models has
achieved promising results, these models often overfit to seen categories due
to the lack of supervision for unseen classes. As an alternative to fully
supervised approaches, query-based segmentation has shown great latent in ZSS,
as it enables object localization without relying on explicit labels. However,
conventional Hungarian matching, a core component in query-based frameworks,
needs full supervision and often misclassifies unseen categories as background
in the setting of ZSS. To address this issue, we propose Split Matching (SM), a
novel assignment strategy that decouples Hungarian matching into two
components: one for seen classes in annotated regions and another for latent
classes in unannotated regions (referred to as unseen candidates).
Specifically, we partition the queries into seen and candidate groups, enabling
each to be optimized independently according to its available supervision. To
discover unseen candidates, we cluster CLIP dense features to generate pseudo
masks and extract region-level embeddings using CLS tokens. Matching is then
conducted separately for the two groups based on both class-level similarity
and mask-level consistency. Additionally, we introduce a Multi-scale Feature
Enhancement (MFE) module that refines decoder features through residual
multi-scale aggregation, improving the model's ability to capture spatial
details across resolutions. SM is the first to introduce decoupled Hungarian
matching under the inductive ZSS setting, and achieves state-of-the-art
performance on two standard benchmarks.

</details>


### [120] [xTrace: A Facial Expressive Behaviour Analysis Tool for Continuous Affect Recognition](https://arxiv.org/pdf/2505.05043)
*Mani Kumar Tellamekala, Shashank Jaiswal, Thomas Smith, Timur Alamev, Gary McKeown, Anthony Brown, Michel Valstar*

Main category: cs.CV

TL;DR: The paper introduces xTrace, a tool for analyzing facial expressive behaviors and predicting valence and arousal from in-the-wild face videos, addressing dataset scarcity and feature extraction challenges.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the lack of large-scale labeled facial affect datasets and the difficulty of extracting robust, interpretable, and efficient facial video features for real-time analysis.

Method: xTrace is trained on a large dataset (~450k videos) and uses explainable facial affect descriptors for accurate and computationally efficient predictions, benchmarked against existing tools.

Result: xTrace achieves 0.86 mean CCC and 0.13 mean absolute error on a 50k-video validation set, demonstrating high accuracy, robustness to head poses, and reliable uncertainty estimates.

Conclusion: xTrace is a versatile and robust tool for facial expressive behavior analysis, outperforming existing methods and addressing key challenges in the field.

Abstract: Recognising expressive behaviours in face videos is a long-standing challenge
in Affective Computing. Despite significant advancements in recent years, it
still remains a challenge to build a robust and reliable system for
naturalistic and in-the-wild facial expressive behaviour analysis in real time.
This paper addresses two key challenges in building such a system: (1). The
paucity of large-scale labelled facial affect video datasets with extensive
coverage of the 2D emotion space, and (2). The difficulty of extracting facial
video features that are discriminative, interpretable, robust, and
computationally efficient. Toward addressing these challenges, we introduce
xTrace, a robust tool for facial expressive behaviour analysis and predicting
continuous values of dimensional emotions, namely valence and arousal, from
in-the-wild face videos.
  To address challenge (1), our affect recognition model is trained on the
largest facial affect video data set, containing ~450k videos that cover most
emotion zones in the dimensional emotion space, making xTrace highly versatile
in analysing a wide spectrum of naturalistic expressive behaviours. To address
challenge (2), xTrace uses facial affect descriptors that are not only
explainable, but can also achieve a high degree of accuracy and robustness with
low computational complexity. The key components of xTrace are benchmarked
against three existing tools: MediaPipe, OpenFace, and Augsburg Affect Toolbox.
On an in-the-wild validation set composed of 50k videos, xTrace achieves 0.86
mean CCC and 0.13 mean absolute error values. We present a detailed error
analysis of affect predictions from xTrace, illustrating (a). its ability to
recognise emotions with high accuracy across most bins in the 2D emotion space,
(b). its robustness to non-frontal head pose angles, and (c). a strong
correlation between its uncertainty estimates and its accuracy.

</details>


### [121] [UncertainSAM: Fast and Efficient Uncertainty Quantification of the Segment Anything Model](https://arxiv.org/pdf/2505.05049)
*Timo Kaiser, Thomas Norrenbrock, Bodo Rosenhahn*

Main category: cs.CV

TL;DR: The paper introduces USAM, a lightweight Bayesian entropy-based uncertainty quantification method for the Segment Anything Model (SAM), addressing aleatoric, epistemic, and task uncertainty.


<details>
  <summary>Details</summary>
Motivation: Quantifying uncertainty in SAM is challenging due to its class-agnostic nature, motivating a new UQ approach.

Method: Proposes USAM, a post-hoc UQ method using Bayesian entropy to trace uncertainty sources like under-parameterization, insufficient prompts, or image ambiguities.

Result: USAM outperforms on SA-V, MOSE, ADE20k, DAVIS, and COCO datasets, offering a cheap and user-friendly UQ solution.

Conclusion: USAM enhances SAM's usability in tasks like user-prompting and semi-supervised learning, balancing accuracy and cost efficiency.

Abstract: The introduction of the Segment Anything Model (SAM) has paved the way for
numerous semantic segmentation applications. For several tasks, quantifying the
uncertainty of SAM is of particular interest. However, the ambiguous nature of
the class-agnostic foundation model SAM challenges current uncertainty
quantification (UQ) approaches. This paper presents a theoretically motivated
uncertainty quantification model based on a Bayesian entropy formulation
jointly respecting aleatoric, epistemic, and the newly introduced task
uncertainty. We use this formulation to train USAM, a lightweight post-hoc UQ
method. Our model traces the root of uncertainty back to under-parameterised
models, insufficient prompts or image ambiguities. Our proposed deterministic
USAM demonstrates superior predictive capabilities on the SA-V, MOSE, ADE20k,
DAVIS, and COCO datasets, offering a computationally cheap and easy-to-use UQ
alternative that can support user-prompting, enhance semi-supervised pipelines,
or balance the tradeoff between accuracy and cost efficiency.

</details>


### [122] [FG-CLIP: Fine-Grained Visual and Textual Alignment](https://arxiv.org/pdf/2505.05071)
*Chunyu Xie, Bin Wang, Fanjing Kong, Jincheng Li, Dawei Liang, Gengshen Zhang, Dawei Leng, Yuhui Yin*

Main category: cs.CV

TL;DR: FG-CLIP improves fine-grained understanding in multimodal tasks by leveraging large-scale data, detailed captions, and hard negative samples, outperforming CLIP and other methods.


<details>
  <summary>Details</summary>
Motivation: CLIP struggles with fine-grained understanding due to its reliance on coarse-grained captions. FG-CLIP aims to address this limitation.

Method: FG-CLIP uses 1.6B long caption-image pairs, a high-quality dataset with region-specific bounding boxes, and 10M hard negative samples, with tailored training methods.

Result: FG-CLIP outperforms CLIP and state-of-the-art methods in fine-grained tasks, object detection, retrieval, and benchmarks.

Conclusion: FG-CLIP effectively captures fine-grained details, enhancing multimodal performance, with resources publicly available.

Abstract: Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks
such as image-text retrieval and zero-shot classification but struggles with
fine-grained understanding due to its focus on coarse-grained short captions.
To address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances
fine-grained understanding through three key innovations. First, we leverage
large multimodal models to generate 1.6 billion long caption-image pairs for
capturing global-level semantic details. Second, a high-quality dataset is
constructed with 12 million images and 40 million region-specific bounding
boxes aligned with detailed captions to ensure precise, context-rich
representations. Third, 10 million hard fine-grained negative samples are
incorporated to improve the model's ability to distinguish subtle semantic
differences. Corresponding training methods are meticulously designed for these
data. Extensive experiments demonstrate that FG-CLIP outperforms the original
CLIP and other state-of-the-art methods across various downstream tasks,
including fine-grained understanding, open-vocabulary object detection,
image-text retrieval, and general multimodal benchmarks. These results
highlight FG-CLIP's effectiveness in capturing fine-grained image details and
improving overall model performance. The related data, code, and models are
available at https://github.com/360CVGroup/FG-CLIP.

</details>


### [123] [ULFine: Unbiased Lightweight Fine-tuning for Foundation-Model-Assisted Long-Tailed Semi-Supervised Learning](https://arxiv.org/pdf/2505.05062)
*Enhao Zhang, Chaohua Li, Chuanxing Geng, Songcan Chen*

Main category: cs.CV

TL;DR: The paper explores the impact of visual foundation models (e.g., CLIP) on Long-Tailed Semi-Supervised Learning (LTSSL) using three strategies: LP, LFT, and FFT. It finds FFT harms performance, while LP and LFT improve overall accuracy but neglect tail classes. The proposed ULFine method reduces biases and costs, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To investigate how visual foundation models affect LTSSL and address their limitations in handling tail classes and biases.

Method: Evaluates LP, LFT, and FFT strategies, identifies their flaws, and proposes ULFine, which combines confidence-aware adaptive fitting and dual logits fusion.

Result: FFT degrades performance; LP and LFT improve overall accuracy but fail for tail classes. ULFine reduces training costs by 10x and boosts accuracy.

Conclusion: ULFine effectively mitigates biases and improves LTSSL performance, offering a cost-efficient and accurate solution.

Abstract: Based on the success of large-scale visual foundation models like CLIP in
various downstream tasks, this paper initially attempts to explore their impact
on Long-Tailed Semi-Supervised Learning (LTSSL) by employing the foundation
model with three strategies: Linear Probing (LP), Lightweight Fine-Tuning
(LFT), and Full Fine-Tuning (FFT). Our analysis presents the following
insights: i) Compared to LTSSL algorithms trained from scratch, FFT results in
a decline in model performance, whereas LP and LFT, although boosting overall
model performance, exhibit negligible benefits to tail classes. ii) LP produces
numerous false pseudo-labels due to \textit{underlearned} training data, while
LFT can reduce the number of these false labels but becomes overconfident about
them owing to \textit{biased fitting} training data. This exacerbates the
pseudo-labeled and classifier biases inherent in LTSSL, limiting performance
improvement in the tail classes. With these insights, we propose a Unbiased
Lightweight Fine-tuning strategy, \textbf{ULFine}, which mitigates the
overconfidence via confidence-aware adaptive fitting of textual prototypes and
counteracts the pseudo-labeled and classifier biases via complementary fusion
of dual logits. Extensive experiments demonstrate that ULFine markedly
decreases training costs by over ten times and substantially increases
prediction accuracies compared to state-of-the-art methods.

</details>


### [124] [Visual Affordances: Enabling Robots to Understand Object Functionality](https://arxiv.org/pdf/2505.05074)
*Tommaso Apicella, Alessio Xompero, Andrea Cavallaro*

Main category: cs.CV

TL;DR: The paper addresses reproducibility issues in visual affordance prediction for human-robot interaction, proposing a unified formulation, reviewing past works, and introducing the Affordance Sheet for transparency. It also links affordance prediction to physical properties like object weight.


<details>
  <summary>Details</summary>
Motivation: To tackle the reproducibility problem in affordance prediction tasks, which leads to unfair and unreliable benchmarks in human-robot interaction.

Method: Proposes a unified formulation for visual affordance prediction, reviews previous works, introduces the Affordance Sheet, and presents a framework linking affordance prediction to physical properties (e.g., object weight).

Result: A systematic review of methods and datasets, a transparency tool (Affordance Sheet), and a framework connecting affordance perception to physical world properties.

Conclusion: The approach bridges affordance perception and robot actuation, ensuring comprehensive object interaction information for task completion.

Abstract: Human-robot interaction for assistive technologies relies on the prediction
of affordances, which are the potential actions a robot can perform on objects.
Predicting object affordances from visual perception is formulated differently
for tasks such as grasping detection, affordance classification, affordance
segmentation, and hand-object interaction synthesis. In this work, we highlight
the reproducibility issue in these redefinitions, making comparative benchmarks
unfair and unreliable. To address this problem, we propose a unified
formulation for visual affordance prediction, provide a comprehensive and
systematic review of previous works highlighting strengths and limitations of
methods and datasets, and analyse what challenges reproducibility. To favour
transparency, we introduce the Affordance Sheet, a document to detail the
proposed solution, the datasets, and the validation. As the physical properties
of an object influence the interaction with the robot, we present a generic
framework that links visual affordance prediction to the physical world. Using
the weight of an object as an example for this framework, we discuss how
estimating object mass can affect the affordance prediction. Our approach
bridges the gap between affordance perception and robot actuation, and accounts
for the complete information about objects of interest and how the robot
interacts with them to accomplish its task.

</details>


### [125] [PIDiff: Image Customization for Personalized Identities with Diffusion Models](https://arxiv.org/pdf/2505.05081)
*Jinyu Gu, Haipeng Liu, Meng Wang, Yang Wang*

Main category: cs.CV

TL;DR: PIDiff is a fine-tuning-based diffusion model for personalized text-to-image generation, using W+ space and identity-tailored fine-tuning to avoid semantic entanglement and improve identity feature extraction.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to disentangle identity and background information, leading to loss of key identity characteristics and reduced diversity in generated images.

Method: PIDiff combines W+ space from StyleGAN with diffusion models, employing an identity-tailored fine-tuning strategy and cross-attention blocks for accurate feature extraction and localization.

Result: PIDiff successfully preserves identity information and maintains generation capability for in-the-wild images, validated by experimental results.

Conclusion: PIDiff addresses semantic interference and improves identity localization, offering a robust solution for personalized text-to-image generation.

Abstract: Text-to-image generation for personalized identities aims at incorporating
the specific identity into images using a text prompt and an identity image.
Based on the powerful generative capabilities of DDPMs, many previous works
adopt additional prompts, such as text embeddings and CLIP image embeddings, to
represent the identity information, while they fail to disentangle the identity
information and background information. As a result, the generated images not
only lose key identity characteristics but also suffer from significantly
reduced diversity. To address this issue, previous works have combined the W+
space from StyleGAN with diffusion models, leveraging this space to provide a
more accurate and comprehensive representation of identity features through
multi-level feature extraction. However, the entanglement of identity and
background information in in-the-wild images during training prevents accurate
identity localization, resulting in severe semantic interference between
identity and background. In this paper, we propose a novel fine-tuning-based
diffusion model for personalized identities text-to-image generation, named
PIDiff, which leverages the W+ space and an identity-tailored fine-tuning
strategy to avoid semantic entanglement and achieves accurate feature
extraction and localization. Style editing can also be achieved by PIDiff
through preserving the characteristics of identity features in the W+ space,
which vary from coarse to fine. Through the combination of the proposed
cross-attention block and parameter optimization strategy, PIDiff preserves the
identity information and maintains the generation capability for in-the-wild
images of the pre-trained model during inference. Our experimental results
validate the effectiveness of our method in this task.

</details>


### [126] [Biomed-DPT: Dual Modality Prompt Tuning for Biomedical Vision-Language Models](https://arxiv.org/pdf/2505.05189)
*Wei Peng, Kang Liu, Jianchen Hu, Meng Zhang*

Main category: cs.CV

TL;DR: Biomed-DPT introduces a dual-modality prompt tuning technique for biomedical image classification, combining text and vision prompts to leverage clinical knowledge and attention re-weighting, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current prompt learning methods for vision-language models in biomedical tasks often ignore image structures and rely solely on text prompts, limiting performance.

Method: Biomed-DPT uses dual prompts: text prompts (clinical and domain-adapted) with knowledge distillation, and vision prompts (zero vectors) for attention re-weighting to avoid non-diagnostic regions.

Result: Achieves 66.14% average accuracy across 11 datasets, with 78.06% in base classes and 75.97% in novel classes, surpassing CoOp by significant margins.

Conclusion: Biomed-DPT effectively integrates clinical knowledge and attention mechanisms, improving biomedical image classification in few-shot scenarios.

Abstract: Prompt learning is one of the most effective paradigms for adapting
pre-trained vision-language models (VLMs) to the biomedical image
classification tasks in few shot scenarios. However, most of the current prompt
learning methods only used the text prompts and ignored the particular
structures (such as the complex anatomical structures and subtle pathological
features) in the biomedical images. In this work, we propose Biomed-DPT, a
knowledge-enhanced dual modality prompt tuning technique. In designing the text
prompt, Biomed-DPT constructs a dual prompt including the template-driven
clinical prompts and the large language model (LLM)-driven domain-adapted
prompts, then extracts the clinical knowledge from the domain-adapted prompts
through the knowledge distillation technique. In designing the vision prompt,
Biomed-DPT introduces the zero vector as a soft prompt to leverage attention
re-weighting so that the focus on non-diagnostic regions and the recognition of
non-critical pathological features are avoided. Biomed-DPT achieves an average
classification accuracy of 66.14\% across 11 biomedical image datasets covering
9 modalities and 10 organs, with performance reaching 78.06\% in base classes
and 75.97\% in novel classes, surpassing the Context Optimization (CoOp) method
by 6.20\%, 3.78\%, and 8.04\%, respectively. Our code are available at
\underline{https://github.com/Kanyooo/Biomed-DPT}.

</details>


### [127] [Nonlinear Motion-Guided and Spatio-Temporal Aware Network for Unsupervised Event-Based Optical Flow](https://arxiv.org/pdf/2505.05089)
*Zuntao Liu, Hao Zhuang, Junjie Jiang, Yuhang Song, Zheng Fang*

Main category: cs.CV

TL;DR: E-NMSTFlow is an unsupervised event-based optical flow network that leverages spatio-temporal information and nonlinear motion for accurate flow estimation in long-time sequences.


<details>
  <summary>Details</summary>
Motivation: Existing methods ignore spatio-temporal event characteristics and assume linear motion, leading to errors in long sequences.

Method: Proposes STMFA and AMFE modules for spatio-temporal data associations and a nonlinear motion compensation loss for unsupervised learning.

Result: Achieves top performance among unsupervised methods on MVSEC and DSEC-Flow datasets.

Conclusion: E-NMSTFlow effectively addresses limitations of existing methods, demonstrating superior accuracy in event-based optical flow estimation.

Abstract: Event cameras have the potential to capture continuous motion information
over time and space, making them well-suited for optical flow estimation.
However, most existing learning-based methods for event-based optical flow
adopt frame-based techniques, ignoring the spatio-temporal characteristics of
events. Additionally, these methods assume linear motion between consecutive
events within the loss time window, which increases optical flow errors in
long-time sequences. In this work, we observe that rich spatio-temporal
information and accurate nonlinear motion between events are crucial for
event-based optical flow estimation. Therefore, we propose E-NMSTFlow, a novel
unsupervised event-based optical flow network focusing on long-time sequences.
We propose a Spatio-Temporal Motion Feature Aware (STMFA) module and an
Adaptive Motion Feature Enhancement (AMFE) module, both of which utilize rich
spatio-temporal information to learn spatio-temporal data associations.
Meanwhile, we propose a nonlinear motion compensation loss that utilizes the
accurate nonlinear motion between events to improve the unsupervised learning
of our network. Extensive experiments demonstrate the effectiveness and
superiority of our method. Remarkably, our method ranks first among
unsupervised learning methods on the MVSEC and DSEC-Flow datasets. Our project
page is available at https://wynelio.github.io/E-NMSTFlow.

</details>


### [128] [DispBench: Benchmarking Disparity Estimation to Synthetic Corruptions](https://arxiv.org/pdf/2505.05091)
*Shashank Agnihotri, Amaan Ansari, Annika Dackermann, Fabian Rösch, Margret Keuper*

Main category: cs.CV

TL;DR: DispBench is introduced as a benchmarking tool to evaluate the robustness of disparity estimation methods against synthetic corruptions and adversarial attacks, addressing the lack of standardized benchmarks in the field.


<details>
  <summary>Details</summary>
Motivation: Deep learning-based disparity estimation methods are widely used but vulnerable to distribution shifts and adversarial attacks, lacking standardized benchmarks to assess their reliability.

Method: DispBench systematically evaluates robustness using synthetic image corruptions, adversarial attacks, and out-of-distribution shifts across multiple datasets.

Result: The tool provides the most extensive analysis of disparity estimation methods, revealing correlations between accuracy, reliability, and generalization.

Conclusion: DispBench fills a critical gap by offering a standardized benchmark to advance the reliability and robustness of disparity estimation methods.

Abstract: Deep learning (DL) has surpassed human performance on standard benchmarks,
driving its widespread adoption in computer vision tasks. One such task is
disparity estimation, estimating the disparity between matching pixels in
stereo image pairs, which is crucial for safety-critical applications like
medical surgeries and autonomous navigation. However, DL-based disparity
estimation methods are highly susceptible to distribution shifts and
adversarial attacks, raising concerns about their reliability and
generalization. Despite these concerns, a standardized benchmark for evaluating
the robustness of disparity estimation methods remains absent, hindering
progress in the field.
  To address this gap, we introduce DispBench, a comprehensive benchmarking
tool for systematically assessing the reliability of disparity estimation
methods. DispBench evaluates robustness against synthetic image corruptions
such as adversarial attacks and out-of-distribution shifts caused by 2D Common
Corruptions across multiple datasets and diverse corruption scenarios. We
conduct the most extensive performance and robustness analysis of disparity
estimation methods to date, uncovering key correlations between accuracy,
reliability, and generalization. Open-source code for DispBench:
https://github.com/shashankskagnihotri/benchmarking_robustness/tree/disparity_estimation/final/disparity_estimation

</details>


### [129] [MDE-Edit: Masked Dual-Editing for Multi-Object Image Editing via Diffusion Models](https://arxiv.org/pdf/2505.05101)
*Hongyang Zhu, Haipeng Liu, Bo Fu, Yang Wang*

Main category: cs.CV

TL;DR: MDE-Edit is a training-free method for precise multi-object editing in complex scenes, addressing attention misalignment and attribute leakage via Object Alignment Loss and Color Consistency Loss.


<details>
  <summary>Details</summary>
Motivation: Challenges in multi-object editing include inaccurate localization and attribute-object mismatch due to attention dilution and feature entanglement. Existing methods struggle with these issues.

Method: MDE-Edit optimizes noise latent features in diffusion models using Object Alignment Loss (OAL) for precise object positioning and Color Consistency Loss (CCL) to prevent attribute leakage.

Result: MDE-Edit outperforms state-of-the-art methods in editing accuracy and visual quality.

Conclusion: MDE-Edit provides a robust solution for localized and coherent multi-object image manipulation.

Abstract: Multi-object editing aims to modify multiple objects or regions in complex
scenes while preserving structural coherence. This task faces significant
challenges in scenarios involving overlapping or interacting objects: (1)
Inaccurate localization of target objects due to attention misalignment,
leading to incomplete or misplaced edits; (2) Attribute-object mismatch, where
color or texture changes fail to align with intended regions due to
cross-attention leakage, creating semantic conflicts (\textit{e.g.}, color
bleeding into non-target areas). Existing methods struggle with these
challenges: approaches relying on global cross-attention mechanisms suffer from
attention dilution and spatial interference between objects, while mask-based
methods fail to bind attributes to geometrically accurate regions due to
feature entanglement in multi-object scenarios. To address these limitations,
we propose a training-free, inference-stage optimization approach that enables
precise localized image manipulation in complex multi-object scenes, named
MDE-Edit. MDE-Edit optimizes the noise latent feature in diffusion models via
two key losses: Object Alignment Loss (OAL) aligns multi-layer cross-attention
with segmentation masks for precise object positioning, and Color Consistency
Loss (CCL) amplifies target attribute attention within masks while suppressing
leakage to adjacent regions. This dual-loss design ensures localized and
coherent multi-object edits. Extensive experiments demonstrate that MDE-Edit
outperforms state-of-the-art methods in editing accuracy and visual quality,
offering a robust solution for complex multi-object image manipulation tasks.

</details>


### [130] [Automated vision-based assistance tools in bronchoscopy: stenosis severity estimation](https://arxiv.org/pdf/2505.05136)
*Clara Tomasini, Javier Rodriguez-Puigvert, Dinora Polanco, Manuel Viñuales, Luis Riazuelo, Ana Cristina Murillo*

Main category: cs.CV

TL;DR: The paper proposes an automated pipeline for subglottic stenosis severity estimation using bronchoscopy images, eliminating the need for CT scans and reducing subjectivity in diagnosis.


<details>
  <summary>Details</summary>
Motivation: Current methods for evaluating subglottic stenosis rely on subjective visual inspections or CT scans, lacking consistency and public datasets for automated assessment.

Method: The approach uses bronchoscopy images to segment and track the airway lumen, creating a 3D model to measure narrowing without traversing the stenosed region.

Result: The pipeline provides consistent and repeatable severity measurements, validated against CT scans and expert estimations, using a new public dataset.

Conclusion: The method automates stenosis evaluation, improves diagnosis efficiency, reduces radiation exposure, and introduces the first public benchmark for this condition.

Abstract: Purpose: Subglottic stenosis refers to the narrowing of the subglottis, the
airway between the vocal cords and the trachea. Its severity is typically
evaluated by estimating the percentage of obstructed airway. This estimation
can be obtained from CT data or through visual inspection by experts exploring
the region. However, visual inspections are inherently subjective, leading to
less consistent and robust diagnoses. No public methods or datasets are
currently available for automated evaluation of this condition from
bronchoscopy video.
  Methods: We propose a pipeline for automated subglottic stenosis severity
estimation during the bronchoscopy exploration, without requiring the physician
to traverse the stenosed region. Our approach exploits the physical effect of
illumination decline in endoscopy to segment and track the lumen and obtain a
3D model of the airway. This 3D model is obtained from a single frame and is
used to measure the airway narrowing.
  Results: Our pipeline is the first to enable automated and robust subglottic
stenosis severity measurement using bronchoscopy images. The results show
consistency with ground-truth estimations from CT scans and expert estimations,
and reliable repeatability across multiple estimations on the same patient. Our
evaluation is performed on our new Subglottic Stenosis Dataset of real
bronchoscopy procedures data.
  Conclusion: We demonstrate how to automate evaluation of subglottic stenosis
severity using only bronchoscopy. Our approach can assist with and shorten
diagnosis and monitoring procedures, with automated and repeatable estimations
and less exploration time, and save radiation exposure to patients as no CT is
required. Additionally, we release the first public benchmark for subglottic
stenosis severity assessment.

</details>


### [131] [Probabilistic Embeddings for Frozen Vision-Language Models: Uncertainty Quantification with Gaussian Process Latent Variable Models](https://arxiv.org/pdf/2505.05163)
*Aishwarya Venkataramanan, Paul Bodesheim, Joachim Denzler*

Main category: cs.CV

TL;DR: GroVE introduces a post-hoc method to derive probabilistic embeddings from frozen VLMs using GPLVM, improving uncertainty calibration without retraining.


<details>
  <summary>Details</summary>
Motivation: Standard VLMs lack uncertainty awareness in embeddings, and existing probabilistic methods require large datasets and retraining.

Method: Uses GPLVM to learn a shared latent space, optimizing single-modal reconstruction and cross-modal alignment with frozen VLM embeddings.

Result: Achieves state-of-the-art uncertainty calibration in tasks like cross-modal retrieval and visual question answering.

Conclusion: GroVE effectively enhances frozen VLMs with probabilistic embeddings, improving performance in uncertainty-sensitive tasks.

Abstract: Vision-Language Models (VLMs) learn joint representations by mapping images
and text into a shared latent space. However, recent research highlights that
deterministic embeddings from standard VLMs often struggle to capture the
uncertainties arising from the ambiguities in visual and textual descriptions
and the multiple possible correspondences between images and texts. Existing
approaches tackle this by learning probabilistic embeddings during VLM
training, which demands large datasets and does not leverage the powerful
representations already learned by large-scale VLMs like CLIP. In this paper,
we propose GroVE, a post-hoc approach to obtaining probabilistic embeddings
from frozen VLMs. GroVE builds on Gaussian Process Latent Variable Model
(GPLVM) to learn a shared low-dimensional latent space where image and text
inputs are mapped to a unified representation, optimized through single-modal
embedding reconstruction and cross-modal alignment objectives. Once trained,
the Gaussian Process model generates uncertainty-aware probabilistic
embeddings. Evaluation shows that GroVE achieves state-of-the-art uncertainty
calibration across multiple downstream tasks, including cross-modal retrieval,
visual question answering, and active learning.

</details>


### [132] [PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes](https://arxiv.org/pdf/2505.05288)
*Ahmed Abdelreheem, Filippo Aleotti, Jamie Watson, Zawar Qureshi, Abdelrahman Eldesokey, Peter Wonka, Gabriel Brostow, Sara Vicente, Guillermo Garcia-Hernando*

Main category: cs.CV

TL;DR: A new task called Language-Guided Object Placement in 3D Scenes is introduced, involving placing a 3D asset in a scene based on a textual prompt. Challenges include ambiguity and geometric reasoning. A benchmark, dataset, and baseline method are proposed.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of placing 3D assets in scenes using natural language prompts, which is ambiguous and requires geometric reasoning.

Method: Proposes a benchmark, evaluation protocol, dataset for training 3D LLMs, and a baseline method.

Result: A new task, benchmark, dataset, and baseline method are introduced to evaluate 3D LLMs.

Conclusion: This task and benchmark could become a standard for evaluating generalist 3D LLM models.

Abstract: We introduce the novel task of Language-Guided Object Placement in Real 3D
Scenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual
prompt broadly describing where the 3D asset should be placed. The task here is
to find a valid placement for the 3D asset that respects the prompt. Compared
with other language-guided localization tasks in 3D scenes such as grounding,
this task has specific challenges: it is ambiguous because it has multiple
valid solutions, and it requires reasoning about 3D geometric relationships and
free space. We inaugurate this task by proposing a new benchmark and evaluation
protocol. We also introduce a new dataset for training 3D LLMs on this task, as
well as the first method to serve as a non-trivial baseline. We believe that
this challenging task and our new benchmark could become part of the suite of
benchmarks used to evaluate and compare generalist 3D LLM models.

</details>


### [133] [PaniCar: Securing the Perception of Advanced Driving Assistance Systems Against Emergency Vehicle Lighting](https://arxiv.org/pdf/2505.05183)
*Elad Feldman, Jacob Shams, Dudi Biton, Alfred Chen, Shaoyuan Xie, Satoru Koda, Yisroel Mirsky, Asaf Shabtai, Yuval Elovici, Ben Nassi*

Main category: cs.CV

TL;DR: The paper investigates the impact of emergency vehicle lighting on object detection in autonomous cars, revealing a vulnerability (PaniCar) and proposing a mitigation framework (Caracetamol).


<details>
  <summary>Details</summary>
Motivation: The study is motivated by documented incidents of Tesla crashes into emergency vehicles, highlighting the unclear impact of flare artifacts on object detection performance.

Method: The research evaluates commercial ADASs, object detectors, and emergency lighting patterns, and proposes Caracetamol to enhance detector resilience.

Result: Caracetamol improves detection confidence and reduces fluctuation, achieving real-time processing (30-50 FPS).

Conclusion: The paper identifies a critical safety risk and offers a practical solution to mitigate the effects of emergency lighting on autonomous vehicle detection systems.

Abstract: The safety of autonomous cars has come under scrutiny in recent years,
especially after 16 documented incidents involving Teslas (with autopilot
engaged) crashing into parked emergency vehicles (police cars, ambulances, and
firetrucks). While previous studies have revealed that strong light sources
often introduce flare artifacts in the captured image, which degrade the image
quality, the impact of flare on object detection performance remains unclear.
In this research, we unveil PaniCar, a digital phenomenon that causes an object
detector's confidence score to fluctuate below detection thresholds when
exposed to activated emergency vehicle lighting. This vulnerability poses a
significant safety risk, and can cause autonomous vehicles to fail to detect
objects near emergency vehicles. In addition, this vulnerability could be
exploited by adversaries to compromise the security of advanced driving
assistance systems (ADASs). We assess seven commercial ADASs (Tesla Model 3,
"manufacturer C", HP, Pelsee, AZDOME, Imagebon, Rexing), four object detectors
(YOLO, SSD, RetinaNet, Faster R-CNN), and 14 patterns of emergency vehicle
lighting to understand the influence of various technical and environmental
factors. We also evaluate four SOTA flare removal methods and show that their
performance and latency are insufficient for real-time driving constraints. To
mitigate this risk, we propose Caracetamol, a robust framework designed to
enhance the resilience of object detectors against the effects of activated
emergency vehicle lighting. Our evaluation shows that on YOLOv3 and Faster
RCNN, Caracetamol improves the models' average confidence of car detection by
0.20, the lower confidence bound by 0.33, and reduces the fluctuation range by
0.33. In addition, Caracetamol is capable of processing frames at a rate of
between 30-50 FPS, enabling real-time ADAS car detection.

</details>


### [134] [TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation](https://arxiv.org/pdf/2505.05422)
*Haokun Lin, Teng Wang, Yixiao Ge, Yuying Ge, Zhichao Lu, Ying Wei, Qingfu Zhang, Zhenan Sun, Ying Shan*

Main category: cs.CV

TL;DR: TokLIP is a visual tokenizer that improves multimodal comprehension and generation by combining low-level VQ tokens with high-level semantics, enabling efficient end-to-end training.


<details>
  <summary>Details</summary>
Motivation: Existing token-based methods like Chameleon and Emu3 suffer from high computational costs and limited comprehension due to lacking high-level semantics.

Method: TokLIP integrates a VQ tokenizer with a ViT-based encoder to capture high-level semantics, disentangling comprehension and generation objectives.

Result: TokLIP achieves exceptional data efficiency and enhances both semantic understanding and generative capacity.

Conclusion: TokLIP is effective for autoregressive Transformers in comprehension and generation tasks, with code and models publicly available.

Abstract: Pioneering token-based works such as Chameleon and Emu3 have established a
foundation for multimodal unification but face challenges of high training
computational overhead and limited comprehension performance due to a lack of
high-level semantics. In this paper, we introduce TokLIP, a visual tokenizer
that enhances comprehension by semanticizing vector-quantized (VQ) tokens and
incorporating CLIP-level semantics while enabling end-to-end multimodal
autoregressive training with standard VQ tokens. TokLIP integrates a low-level
discrete VQ tokenizer with a ViT-based token encoder to capture high-level
continuous semantics. Unlike previous approaches (e.g., VILA-U) that discretize
high-level features, TokLIP disentangles training objectives for comprehension
and generation, allowing the direct application of advanced VQ tokenizers
without the need for tailored quantization operations. Our empirical results
demonstrate that TokLIP achieves exceptional data efficiency, empowering visual
tokens with high-level semantic understanding while enhancing low-level
generative capacity, making it well-suited for autoregressive Transformers in
both comprehension and generation tasks. The code and models are available at
https://github.com/TencentARC/TokLIP.

</details>


### [135] [EAM: Enhancing Anything with Diffusion Transformers for Blind Super-Resolution](https://arxiv.org/pdf/2505.05209)
*Haizhen Xie, Kunpeng Du, Qiangyu Yan, Sen Lu, Jianhong Han, Hanting Chen, Hailin Hu, Jie Hu*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Utilizing pre-trained Text-to-Image (T2I) diffusion models to guide Blind
Super-Resolution (BSR) has become a predominant approach in the field. While
T2I models have traditionally relied on U-Net architectures, recent
advancements have demonstrated that Diffusion Transformers (DiT) achieve
significantly higher performance in this domain. In this work, we introduce
Enhancing Anything Model (EAM), a novel BSR method that leverages DiT and
outperforms previous U-Net-based approaches. We introduce a novel block,
$\Psi$-DiT, which effectively guides the DiT to enhance image restoration. This
block employs a low-resolution latent as a separable flow injection control,
forming a triple-flow architecture that effectively leverages the prior
knowledge embedded in the pre-trained DiT. To fully exploit the prior guidance
capabilities of T2I models and enhance their generalization in BSR, we
introduce a progressive Masked Image Modeling strategy, which also reduces
training costs. Additionally, we propose a subject-aware prompt generation
strategy that employs a robust multi-modal model in an in-context learning
framework. This strategy automatically identifies key image areas, provides
detailed descriptions, and optimizes the utilization of T2I diffusion priors.
Our experiments demonstrate that EAM achieves state-of-the-art results across
multiple datasets, outperforming existing methods in both quantitative metrics
and visual quality.

</details>


### [136] [Mapping User Trust in Vision Language Models: Research Landscape, Challenges, and Prospects](https://arxiv.org/pdf/2505.05318)
*Agnese Chiatti, Sara Bernardini, Lara Shibelski Godoy Piccolo, Viola Schiaffonati, Matteo Matteucci*

Main category: cs.CV

TL;DR: A survey on trust dynamics in Vision Language Models (VLMs), reviewing studies and proposing requirements for future research.


<details>
  <summary>Details</summary>
Motivation: To address the need for trust in rapidly adopted VLMs by understanding user interactions and trust dynamics.

Method: Reviews literature and insights from a workshop with prospective VLM users, using a multi-disciplinary taxonomy.

Result: Identifies key cognitive science capabilities, collaboration modes, and agent behaviors influencing trust.

Conclusion: Preliminary requirements for future VLM trust studies are proposed based on findings.

Abstract: The rapid adoption of Vision Language Models (VLMs), pre-trained on large
image-text and video-text datasets, calls for protecting and informing users
about when to trust these systems. This survey reviews studies on trust
dynamics in user-VLM interactions, through a multi-disciplinary taxonomy
encompassing different cognitive science capabilities, collaboration modes, and
agent behaviours. Literature insights and findings from a workshop with
prospective VLM users inform preliminary requirements for future VLM trust
studies.

</details>


### [137] [Adaptive Markup Language Generation for Contextually-Grounded Visual Document Understanding](https://arxiv.org/pdf/2505.05446)
*Han Xiao, Yina Xie, Guanxin Tan, Yinghao Chen, Rui Hu, Ke Wang, Aojun Zhou, Hao Li, Hao Shao, Xudong Lu, Peng Gao, Yafei Wen, Xiaoxin Chen, Shuai Ren, Hongsheng Li*

Main category: cs.CV

TL;DR: The paper proposes a pipeline using adaptive markup languages (Markdown, JSON, HTML, TiKZ) to enhance visual document understanding, addressing challenges like limited contextual data and spatial relationship comprehension. It introduces two datasets (DocMark-Pile, DocMark-Instruct) and shows superior performance over existing models.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges in visual document understanding, such as integrating visual and textual information and overcoming limited contextual data in existing datasets.

Method: An innovative pipeline leveraging adaptive markup languages to create structured document representations, supported by two new datasets (DocMark-Pile for pretraining, DocMark-Instruct for fine-tuning).

Result: The proposed model outperforms state-of-the-art MLLMs in visual document understanding benchmarks, improving reasoning and comprehension in complex scenarios.

Conclusion: The pipeline and datasets significantly advance visual document understanding, with released code and models for broader use.

Abstract: Visual Document Understanding has become essential with the increase of
text-rich visual content. This field poses significant challenges due to the
need for effective integration of visual perception and textual comprehension,
particularly across diverse document types with complex layouts. Moreover,
existing fine-tuning datasets for this domain often fall short in providing the
detailed contextual information for robust understanding, leading to
hallucinations and limited comprehension of spatial relationships among visual
elements. To address these challenges, we propose an innovative pipeline that
utilizes adaptive generation of markup languages, such as Markdown, JSON, HTML,
and TiKZ, to build highly structured document representations and deliver
contextually-grounded responses. We introduce two fine-grained structured
datasets: DocMark-Pile, comprising approximately 3.8M pretraining data pairs
for document parsing, and DocMark-Instruct, featuring 624k fine-tuning data
annotations for grounded instruction following. Extensive experiments
demonstrate that our proposed model significantly outperforms existing
state-of-theart MLLMs across a range of visual document understanding
benchmarks, facilitating advanced reasoning and comprehension capabilities in
complex visual scenarios. Our code and models are released at https://github.
com/Euphoria16/DocMark.

</details>


### [138] [HQC-NBV: A Hybrid Quantum-Classical View Planning Approach](https://arxiv.org/pdf/2505.05212)
*Xiaotong Yu, Chang Wen Chen*

Main category: cs.CV

TL;DR: HQC-NBV, a hybrid quantum-classical framework, improves view planning efficiency by 49.2% over classical methods using quantum properties.


<details>
  <summary>Details</summary>
Motivation: Classical view planning methods lack scalability and optimality in complex settings, prompting the need for quantum-enhanced solutions.

Method: Proposes a Hamiltonian formulation with multi-component cost terms and a variational ansatz with bidirectional entanglement patterns.

Result: Achieves up to 49.2% higher exploration efficiency compared to classical methods.

Conclusion: Demonstrates quantum advantage in robotic perception, advancing integration of quantum computing in robot vision tasks.

Abstract: Efficient view planning is a fundamental challenge in computer vision and
robotic perception, critical for tasks ranging from search and rescue
operations to autonomous navigation. While classical approaches, including
sampling-based and deterministic methods, have shown promise in planning camera
viewpoints for scene exploration, they often struggle with computational
scalability and solution optimality in complex settings. This study introduces
HQC-NBV, a hybrid quantum-classical framework for view planning that leverages
quantum properties to efficiently explore the parameter space while maintaining
robustness and scalability. We propose a specific Hamiltonian formulation with
multi-component cost terms and a parameter-centric variational ansatz with
bidirectional alternating entanglement patterns that capture the hierarchical
dependencies between viewpoint parameters. Comprehensive experiments
demonstrate that quantum-specific components provide measurable performance
advantages. Compared to the classical methods, our approach achieves up to
49.2% higher exploration efficiency across diverse environments. Our analysis
of entanglement architecture and coherence-preserving terms provides insights
into the mechanisms of quantum advantage in robotic exploration tasks. This
work represents a significant advancement in integrating quantum computing into
robotic perception systems, offering a paradigm-shifting solution for various
robot vision tasks.

</details>


### [139] [Feature-Augmented Deep Networks for Multiscale Building Segmentation in High-Resolution UAV and Satellite Imagery](https://arxiv.org/pdf/2505.05321)
*Chintan B. Maniyar, Minakshi Kumar, Gengchen Mai*

Main category: cs.CV

TL;DR: A deep learning framework for building segmentation using RGB aerial/satellite imagery, achieving high accuracy with feature augmentation and optimized training.


<details>
  <summary>Details</summary>
Motivation: Challenges in building segmentation due to spectral similarity, shadows, and irregular geometries motivate a robust solution.

Method: Uses a Res-U-Net with feature-augmented inputs (PCA, VDVI, MBI, Sobel) and optimized training policies (layer freezing, cyclical learning rates).

Result: Achieves 96.5% accuracy, F1-score 0.86, IoU 0.80, outperforming benchmarks.

Conclusion: Combining multi-resolution imagery, feature augmentation, and optimized training yields robust building segmentation.

Abstract: Accurate building segmentation from high-resolution RGB imagery remains
challenging due to spectral similarity with non-building features, shadows, and
irregular building geometries. In this study, we present a comprehensive deep
learning framework for multiscale building segmentation using RGB aerial and
satellite imagery with spatial resolutions ranging from 0.4m to 2.7m. We curate
a diverse, multi-sensor dataset and introduce feature-augmented inputs by
deriving secondary representations including Principal Component Analysis
(PCA), Visible Difference Vegetation Index (VDVI), Morphological Building Index
(MBI), and Sobel edge filters from RGB channels. These features guide a
Res-U-Net architecture in learning complex spatial patterns more effectively.
We also propose training policies incorporating layer freezing, cyclical
learning rates, and SuperConvergence to reduce training time and resource
usage. Evaluated on a held-out WorldView-3 image, our model achieves an overall
accuracy of 96.5%, an F1-score of 0.86, and an Intersection over Union (IoU) of
0.80, outperforming existing RGB-based benchmarks. This study demonstrates the
effectiveness of combining multi-resolution imagery, feature augmentation, and
optimized training strategies for robust building segmentation in remote
sensing applications.

</details>


### [140] [StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant](https://arxiv.org/pdf/2505.05467)
*Haibo Wang, Bo Feng, Zhengfeng Lai, Mingze Xu, Shiyu Li, Weifeng Ge, Afshin Dehghan, Meng Cao, Ping Huang*

Main category: cs.CV

TL;DR: StreamBridge adapts offline Video-LLMs for streaming by addressing multi-turn understanding and proactive responses, outperforming proprietary models.


<details>
  <summary>Details</summary>
Motivation: Existing Video-LLMs lack capabilities for real-time multi-turn interactions and proactive responses in streaming scenarios.

Method: StreamBridge uses a memory buffer with round-decayed compression and a lightweight activation model, supported by the Stream-IT dataset.

Result: It improves streaming understanding, outperforming GPT-4o and Gemini 1.5 Pro, and matches standard benchmarks.

Conclusion: StreamBridge effectively bridges the gap between offline and streaming Video-LLMs, enhancing real-time performance.

Abstract: We present StreamBridge, a simple yet effective framework that seamlessly
transforms offline Video-LLMs into streaming-capable models. It addresses two
fundamental challenges in adapting existing models into online scenarios: (1)
limited capability for multi-turn real-time understanding, and (2) lack of
proactive response mechanisms. Specifically, StreamBridge incorporates (1) a
memory buffer combined with a round-decayed compression strategy, supporting
long-context multi-turn interactions, and (2) a decoupled, lightweight
activation model that can be effortlessly integrated into existing Video-LLMs,
enabling continuous proactive responses. To further support StreamBridge, we
construct Stream-IT, a large-scale dataset tailored for streaming video
understanding, featuring interleaved video-text sequences and diverse
instruction formats. Extensive experiments show that StreamBridge significantly
improves the streaming understanding capabilities of offline Video-LLMs across
various tasks, outperforming even proprietary models such as GPT-4o and Gemini
1.5 Pro. Simultaneously, it achieves competitive or superior performance on
standard video understanding benchmarks.

</details>


### [141] [Diffusion Model Quantization: A Review](https://arxiv.org/pdf/2505.05215)
*Qian Zeng, Chenggong Hu, Mingli Song, Jie Song*

Main category: cs.CV

TL;DR: A survey on quantization techniques for diffusion models, addressing challenges, methods, and future research directions.


<details>
  <summary>Details</summary>
Motivation: To enable efficient deployment of diffusion models on resource-constrained devices through quantization.

Method: Review and analysis of quantization techniques, including U-Net and DiT architectures, with qualitative and quantitative evaluations.

Result: Benchmarked methods on datasets, analyzed quantization errors, and provided visual and trajectory insights.

Conclusion: Proposed future research directions for generative model quantization, with resources available on GitHub.

Abstract: Recent success of large text-to-image models has empirically underscored the
exceptional performance of diffusion models in generative tasks. To facilitate
their efficient deployment on resource-constrained edge devices, model
quantization has emerged as a pivotal technique for both compression and
acceleration. This survey offers a thorough review of the latest advancements
in diffusion model quantization, encapsulating and analyzing the current state
of the art in this rapidly advancing domain. First, we provide an overview of
the key challenges encountered in the quantization of diffusion models,
including those based on U-Net architectures and Diffusion Transformers (DiT).
We then present a comprehensive taxonomy of prevalent quantization techniques,
engaging in an in-depth discussion of their underlying principles.
Subsequently, we perform a meticulous analysis of representative diffusion
model quantization schemes from both qualitative and quantitative perspectives.
From a quantitative standpoint, we rigorously benchmark a variety of methods
using widely recognized datasets, delivering an extensive evaluation of the
most recent and impactful research in the field. From a qualitative standpoint,
we categorize and synthesize the effects of quantization errors, elucidating
these impacts through both visual analysis and trajectory examination. In
conclusion, we outline prospective avenues for future research, proposing novel
directions for the quantization of generative models in practical applications.
The list of related papers, corresponding codes, pre-trained models and
comparison results are publicly available at the survey project homepage
https://github.com/TaylorJocelyn/Diffusion-Model-Quantization.

</details>


### [142] [PADriver: Towards Personalized Autonomous Driving](https://arxiv.org/pdf/2505.05240)
*Genghua Kou, Fan Jia, Weixin Mao, Yingfei Liu, Yucheng Zhao, Ziheng Zhang, Osamu Yoshie, Tiancai Wang, Ying Li, Xiangyu Zhang*

Main category: cs.CV

TL;DR: PADriver is a closed-loop framework for personalized autonomous driving using MLLM, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To enhance autonomous driving by integrating personalized textual prompts and real-time risk assessment.

Method: Uses Multi-modal Large Language Model (MLLM) for scene understanding, danger level estimation, and action decision based on streaming frames and prompts.

Result: Outperforms existing methods on PAD-Highway benchmark, enabling diverse driving modes.

Conclusion: PADriver effectively combines personalization and safety in autonomous driving, validated by a robust benchmark.

Abstract: In this paper, we propose PADriver, a novel closed-loop framework for
personalized autonomous driving (PAD). Built upon Multi-modal Large Language
Model (MLLM), PADriver takes streaming frames and personalized textual prompts
as inputs. It autoaggressively performs scene understanding, danger level
estimation and action decision. The predicted danger level reflects the risk of
the potential action and provides an explicit reference for the final action,
which corresponds to the preset personalized prompt. Moreover, we construct a
closed-loop benchmark named PAD-Highway based on Highway-Env simulator to
comprehensively evaluate the decision performance under traffic rules. The
dataset contains 250 hours videos with high-quality annotation to facilitate
the development of PAD behavior analysis. Experimental results on the
constructed benchmark show that PADriver outperforms state-of-the-art
approaches on different evaluation metrics, and enables various driving modes.

</details>


### [143] [PRE-Mamba: A 4D State Space Model for Ultra-High-Frequent Event Camera Deraining](https://arxiv.org/pdf/2505.05307)
*Ciyu Ruan, Ruishan Guo, Zihang Gong, Jingao Xu, Wenhan Yang, Xinlei Chen*

Main category: cs.CV

TL;DR: PRE-Mamba is a novel event camera deraining framework that balances temporal precision, deraining effectiveness, and computational efficiency using a 4D event cloud, STDF module, and MS3M.


<details>
  <summary>Details</summary>
Motivation: Event cameras struggle with dense noise in rain, and existing deraining methods compromise on temporal precision, effectiveness, or efficiency.

Method: Uses a 4D event cloud, STDF module for spatiotemporal decoupling/fusion, and MS3M for rain dynamics with linear complexity.

Result: Achieves 0.95 SR, 0.91 NR, and 0.4s/M events with 0.26M parameters, generalizing across rain intensities, viewpoints, and snow.

Conclusion: PRE-Mamba outperforms existing methods in deraining while maintaining efficiency and adaptability.

Abstract: Event cameras excel in high temporal resolution and dynamic range but suffer
from dense noise in rainy conditions. Existing event deraining methods face
trade-offs between temporal precision, deraining effectiveness, and
computational efficiency. In this paper, we propose PRE-Mamba, a novel
point-based event camera deraining framework that fully exploits the
spatiotemporal characteristics of raw event and rain. Our framework introduces
a 4D event cloud representation that integrates dual temporal scales to
preserve high temporal precision, a Spatio-Temporal Decoupling and Fusion
module (STDF) that enhances deraining capability by enabling shallow decoupling
and interaction of temporal and spatial information, and a Multi-Scale State
Space Model (MS3M) that captures deeper rain dynamics across dual-temporal and
multi-spatial scales with linear computational complexity. Enhanced by
frequency-domain regularization, PRE-Mamba achieves superior performance (0.95
SR, 0.91 NR, and 0.4s/M events) with only 0.26M parameters on EventRain-27K, a
comprehensive dataset with labeled synthetic and real-world sequences.
Moreover, our method generalizes well across varying rain intensities,
viewpoints, and even snowy conditions.

</details>


### [144] [Threshold Modulation for Online Test-Time Adaptation of Spiking Neural Networks](https://arxiv.org/pdf/2505.05375)
*Kejie Zhao, Wenjia Hua, Aiersi Tuerhong, Luziwei Leng, Yuxin Ma, Qinghua Guo*

Main category: cs.CV

TL;DR: A novel online test-time adaptation framework for spiking neural networks (SNNs) called Threshold Modulation (TM) is proposed to enhance robustness against distribution shifts while being hardware-friendly.


<details>
  <summary>Details</summary>
Motivation: Existing OTTA methods are not well-suited for SNNs, which face challenges in adapting to distribution shifts after deployment.

Method: The proposed TM dynamically adjusts firing thresholds using neuronal dynamics-inspired normalization, ensuring compatibility with neuromorphic hardware.

Result: Experiments show TM improves SNN robustness against distribution shifts with low computational cost.

Conclusion: TM provides a practical OTTA solution for SNNs, inspiring future neuromorphic chip designs.

Abstract: Recently, spiking neural networks (SNNs), deployed on neuromorphic chips,
provide highly efficient solutions on edge devices in different scenarios.
However, their ability to adapt to distribution shifts after deployment has
become a crucial challenge. Online test-time adaptation (OTTA) offers a
promising solution by enabling models to dynamically adjust to new data
distributions without requiring source data or labeled target samples.
Nevertheless, existing OTTA methods are largely designed for traditional
artificial neural networks and are not well-suited for SNNs. To address this
gap, we propose a low-power, neuromorphic chip-friendly online test-time
adaptation framework, aiming to enhance model generalization under distribution
shifts. The proposed approach is called Threshold Modulation (TM), which
dynamically adjusts the firing threshold through neuronal dynamics-inspired
normalization, being more compatible with neuromorphic hardware. Experimental
results on benchmark datasets demonstrate the effectiveness of this method in
improving the robustness of SNNs against distribution shifts while maintaining
low computational cost. The proposed method offers a practical solution for
online test-time adaptation of SNNs, providing inspiration for the design of
future neuromorphic chips. The demo code is available at
github.com/NneurotransmitterR/TM-OTTA-SNN.

</details>


### [145] [Aesthetics Without Semantics](https://arxiv.org/pdf/2505.05331)
*C. Alejandro Parraga, Olivier Penacchio, Marcos Muňoz Gonzalez, Bogdan Raducanu, Xavier Otazu*

Main category: cs.CV

TL;DR: The paper addresses biases in aesthetic image databases by creating a balanced dataset (MSC) of 10,426 images with minimal semantic content, including ugly images, to better study aesthetic judgments.


<details>
  <summary>Details</summary>
Motivation: Current aesthetic databases are biased towards beautiful images, limiting the study of aesthetic responses. The paper aims to overcome this by including ugly images and minimal semantic content.

Method: The authors created the MSC database with images of minimal semantic content, balanced for aesthetic value (beautiful and ugly), and used established image metrics to analyze relationships between features and aesthetics.

Result: Augmenting biased image sets with ugly images can modify or invert observed relationships between image features and aesthetic judgments.

Conclusion: Limiting the range of aesthetic values in studies can distort findings; including a broader range (e.g., ugly images) reveals more nuanced effects in empirical aesthetics.

Abstract: While it is easy for human observers to judge an image as beautiful or ugly,
aesthetic decisions result from a combination of entangled perceptual and
cognitive (semantic) factors, making the understanding of aesthetic judgements
particularly challenging from a scientific point of view. Furthermore, our
research shows a prevailing bias in current databases, which include mostly
beautiful images, further complicating the study and prediction of aesthetic
responses. We address these limitations by creating a database of images with
minimal semantic content and devising, and next exploiting, a method to
generate images on the ugly side of aesthetic valuations. The resulting Minimum
Semantic Content (MSC) database consists of a large and balanced collection of
10,426 images, each evaluated by 100 observers. We next use established image
metrics to demonstrate how augmenting an image set biased towards beautiful
images with ugly images can modify, or even invert, an observed relationship
between image features and aesthetics valuation. Taken together, our study
reveals that works in empirical aesthetics attempting to link image content and
aesthetic judgements may magnify, underestimate, or simply miss interesting
effects due to a limitation of the range of aesthetic values they consider.

</details>


### [146] [Progressive Inertial Poser: Progressive Real-Time Kinematic Chain Estimation for 3D Full-Body Pose from Three IMU Sensors](https://arxiv.org/pdf/2505.05336)
*Zunjie Zhu, Yan Zhao, Yihan Hu, Guoxiang Wang, Hai Qiu, Bolun Zheng, Chenggang Yan, Feng Xu*

Main category: cs.CV

TL;DR: ProgIP estimates full-body poses using only three IMU sensors (head and wrists), combining neural networks and human dynamics for real-time motion reconstruction, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To improve practicality for VR by reducing hardware complexity (no pelvis/lower-body sensors or external visuals) while maintaining accuracy.

Method: Progressive Inertial Poser (ProgIP) uses TE-biLSTM for temporal dependencies and MLPs to project features onto SMPL model parameters, leveraging a hierarchical kinematic chain.

Result: Outperforms state-of-the-art methods with three IMUs and matches performance of six-IMU setups on public datasets.

Conclusion: ProgIP enables accurate, real-time full-body pose estimation with minimal hardware, advancing VR motion capture.

Abstract: The motion capture system that supports full-body virtual representation is
of key significance for virtual reality. Compared to vision-based systems,
full-body pose estimation from sparse tracking signals is not limited by
environmental conditions or recording range. However, previous works either
face the challenge of wearing additional sensors on the pelvis and lower-body
or rely on external visual sensors to obtain global positions of key joints. To
improve the practicality of the technology for virtual reality applications, we
estimate full-body poses using only inertial data obtained from three Inertial
Measurement Unit (IMU) sensors worn on the head and wrists, thereby reducing
the complexity of the hardware system. In this work, we propose a method called
Progressive Inertial Poser (ProgIP) for human pose estimation, which combines
neural network estimation with a human dynamics model, considers the
hierarchical structure of the kinematic chain, and employs a multi-stage
progressive network estimation with increased depth to reconstruct full-body
motion in real time. The encoder combines Transformer Encoder and bidirectional
LSTM (TE-biLSTM) to flexibly capture the temporal dependencies of the inertial
sequence, while the decoder based on multi-layer perceptrons (MLPs) transforms
high-dimensional features and accurately projects them onto Skinned
Multi-Person Linear (SMPL) model parameters. Quantitative and qualitative
experimental results on multiple public datasets show that our method
outperforms state-of-the-art methods with the same inputs, and is comparable to
recent works using six IMU sensors.

</details>


### [147] [Flow-GRPO: Training Flow Matching Models via Online RL](https://arxiv.org/pdf/2505.05470)
*Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, Wanli Ouyang*

Main category: cs.CV

TL;DR: Flow-GRPO integrates RL into flow matching models using ODE-to-SDE conversion and Denoising Reduction, improving performance in text-to-image tasks without compromising quality.


<details>
  <summary>Details</summary>
Motivation: To enhance flow matching models by incorporating online RL for better exploration and efficiency in tasks like text-to-image generation.

Method: Uses ODE-to-SDE conversion for RL exploration and Denoising Reduction to improve sampling efficiency while maintaining performance.

Result: Significant improvements in GenEval accuracy (63% to 95%) and visual text rendering (59% to 92%), with stable image quality and diversity.

Conclusion: Flow-GRPO effectively integrates RL into flow matching, achieving high performance and alignment with human preferences without reward hacking.

Abstract: We propose Flow-GRPO, the first method integrating online reinforcement
learning (RL) into flow matching models. Our approach uses two key strategies:
(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary
Differential Equation (ODE) into an equivalent Stochastic Differential Equation
(SDE) that matches the original model's marginal distribution at all timesteps,
enabling statistical sampling for RL exploration; and (2) a Denoising Reduction
strategy that reduces training denoising steps while retaining the original
inference timestep number, significantly improving sampling efficiency without
performance degradation. Empirically, Flow-GRPO is effective across multiple
text-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly
perfect object counts, spatial relations, and fine-grained attributes, boosting
GenEval accuracy from $63\%$ to $95\%$. In visual text rendering, its accuracy
improves from $59\%$ to $92\%$, significantly enhancing text generation.
Flow-GRPO also achieves substantial gains in human preference alignment.
Notably, little to no reward hacking occurred, meaning rewards did not increase
at the cost of image quality or diversity, and both remained stable in our
experiments.

</details>


### [148] [GeomHair: Reconstruction of Hair Strands from Colorless 3D Scans](https://arxiv.org/pdf/2505.05376)
*Rachmadio Noval Lazuardi, Artem Sevastopolsky, Egor Zakharov, Matthias Niessner, Vanessa Sklyarova*

Main category: cs.CV

TL;DR: A novel method reconstructs hair strands from colorless 3D scans using multi-modal orientation extraction, avoiding RGB reliance and enabling accurate hairstyle reconstruction.


<details>
  <summary>Details</summary>
Motivation: Hair strand reconstruction is crucial for digital avatars, animation, and AR/VR, but existing RGB-based methods struggle with complex hairstyles and environmental sensitivity.

Method: The method extracts sharp surface features from scans, uses a neural 2D line detector for orientation, and incorporates a diffusion prior trained on synthetic hair scans.

Result: Accurate reconstruction of simple and intricate hairstyles without color information, supported by the Strands400 dataset.

Conclusion: The approach advances hair reconstruction by eliminating RGB dependency and provides a valuable dataset for future research.

Abstract: We propose a novel method that reconstructs hair strands directly from
colorless 3D scans by leveraging multi-modal hair orientation extraction. Hair
strand reconstruction is a fundamental problem in computer vision and graphics
that can be used for high-fidelity digital avatar synthesis, animation, and
AR/VR applications. However, accurately recovering hair strands from raw scan
data remains challenging due to human hair's complex and fine-grained
structure. Existing methods typically rely on RGB captures, which can be
sensitive to the environment and can be a challenging domain for extracting the
orientation of guiding strands, especially in the case of challenging
hairstyles. To reconstruct the hair purely from the observed geometry, our
method finds sharp surface features directly on the scan and estimates strand
orientation through a neural 2D line detector applied to the renderings of scan
shading. Additionally, we incorporate a diffusion prior trained on a diverse
set of synthetic hair scans, refined with an improved noise schedule, and
adapted to the reconstructed contents via a scan-specific text prompt. We
demonstrate that this combination of supervision signals enables accurate
reconstruction of both simple and intricate hairstyles without relying on color
information. To facilitate further research, we introduce Strands400, the
largest publicly available dataset of hair strands with detailed surface
geometry extracted from real-world data, which contains reconstructed hair
strands from the scans of 400 subjects.

</details>


### [149] [EDmamba: A Simple yet Effective Event Denoising Method with State Space Model](https://arxiv.org/pdf/2505.05391)
*Ciyu Ruan, Zihang Gong, Ruishan Guo, Jingao Xu, Xinlei Chen*

Main category: cs.CV

TL;DR: A novel event denoising framework using State Space Models (SSMs) achieves high accuracy and efficiency, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Event cameras' high-speed vision is hindered by noisy output, requiring efficient denoising without compromising speed.

Method: Proposes a 4D event cloud representation with Coarse Feature Extraction (CFE) and two SSM components (S-SSM for local geometry, T-SSM for global dynamics).

Result: Achieves 0.982 accuracy, 88.89K parameters, 0.0685s per 100K events, outperforming Transformer-based methods by 2.08% accuracy and 36X speed.

Conclusion: The SSM-based framework effectively balances denoising accuracy and computational efficiency for event cameras.

Abstract: Event cameras excel in high-speed vision due to their high temporal
resolution, high dynamic range, and low power consumption. However, as dynamic
vision sensors, their output is inherently noisy, making efficient denoising
essential to preserve their ultra-low latency and real-time processing
capabilities. Existing event denoising methods struggle with a critical
dilemma: computationally intensive approaches compromise the sensor's
high-speed advantage, while lightweight methods often lack robustness across
varying noise levels. To address this, we propose a novel event denoising
framework based on State Space Models (SSMs). Our approach represents events as
4D event clouds and includes a Coarse Feature Extraction (CFE) module that
extracts embedding features from both geometric and polarity-aware subspaces.
The model is further composed of two essential components: A Spatial Mamba
(S-SSM) that models local geometric structures and a Temporal Mamba (T-SSM)
that captures global temporal dynamics, efficiently propagating spatiotemporal
features across events. Experiments demonstrate that our method achieves
state-of-the-art accuracy and efficiency, with 88.89K parameters, 0.0685s per
100K events inference time, and a 0.982 accuracy score, outperforming
Transformer-based methods by 2.08% in denoising accuracy and 36X faster.

</details>


### [150] [PillarMamba: Learning Local-Global Context for Roadside Point Cloud via Hybrid State Space Model](https://arxiv.org/pdf/2505.05397)
*Zhang Zhang, Chao Sun, Chao Yue, Da Wen, Tianze Wang, Jianghao Leng*

Main category: cs.CV

TL;DR: PillarMamba, a framework using Cross-stage State-space Group (CSG) and Hybrid State-space Block (HSB), improves roadside point cloud 3D object detection by combining local-global context and efficient computation.


<details>
  <summary>Details</summary>
Motivation: Roadside point cloud 3D object detection is under-explored but crucial for ITS and V2X tasks. Existing methods lack effective global receptive fields and scene context utilization.

Method: Introduces Mamba (SSM-based) to pillar-based point cloud perception, using CSG for cross-stage feature fusion and HSB for local-global context via local convolution and residual attention.

Result: Outperforms state-of-the-art methods on the DAIR-V2X-I benchmark.

Conclusion: PillarMamba effectively addresses limitations in roadside point cloud detection, offering improved performance and context utilization.

Abstract: Serving the Intelligent Transport System (ITS) and Vehicle-to-Everything
(V2X) tasks, roadside perception has received increasing attention in recent
years, as it can extend the perception range of connected vehicles and improve
traffic safety. However, roadside point cloud oriented 3D object detection has
not been effectively explored. To some extent, the key to the performance of a
point cloud detector lies in the receptive field of the network and the ability
to effectively utilize the scene context. The recent emergence of Mamba, based
on State Space Model (SSM), has shaken up the traditional convolution and
transformers that have long been the foundational building blocks, due to its
efficient global receptive field. In this work, we introduce Mamba to
pillar-based roadside point cloud perception and propose a framework based on
Cross-stage State-space Group (CSG), called PillarMamba. It enhances the
expressiveness of the network and achieves efficient computation through
cross-stage feature fusion. However, due to the limitations of scan directions,
state space model faces local connection disrupted and historical relationship
forgotten. To address this, we propose the Hybrid State-space Block (HSB) to
obtain the local-global context of roadside point cloud. Specifically, it
enhances neighborhood connections through local convolution and preserves
historical memory through residual attention. The proposed method outperforms
the state-of-the-art methods on the popular large scale roadside benchmark:
DAIR-V2X-I. The code will be released soon.

</details>


### [151] [SITE: towards Spatial Intelligence Thorough Evaluation](https://arxiv.org/pdf/2505.05456)
*Wenqi Wang, Reuben Tan, Pengyue Zhu, Jianwei Yang, Zhengyuan Yang, Lijuan Wang, Andrey Kolobov, Jianfeng Gao, Boqing Gong*

Main category: cs.CV

TL;DR: The paper introduces SITE, a benchmark dataset for evaluating spatial intelligence (SI) in vision-language models, revealing gaps in model performance compared to humans, especially in spatial orientation.


<details>
  <summary>Details</summary>
Motivation: To address the lack of standardized evaluation for spatial intelligence in AI, particularly in vision-language models, by creating a comprehensive benchmark.

Method: Combines a survey of 31 datasets with cognitive science classifications to design tasks, including novel ones like view-taking and dynamic scenes.

Result: Leading models lag behind humans in spatial orientation, and spatial reasoning proficiency correlates with performance in embodied AI tasks.

Conclusion: SITE provides a robust framework for assessing SI in AI, highlighting areas for improvement in spatial reasoning capabilities.

Abstract: Spatial intelligence (SI) represents a cognitive ability encompassing the
visualization, manipulation, and reasoning about spatial relationships,
underpinning disciplines from neuroscience to robotics. We introduce SITE, a
benchmark dataset towards SI Thorough Evaluation in a standardized format of
multi-choice visual question-answering, designed to assess large
vision-language models' spatial intelligence across diverse visual modalities
(single-image, multi-image, and video) and SI factors (figural to environmental
scales, spatial visualization and orientation, intrinsic and extrinsic, static
and dynamic). Our approach to curating the benchmark combines a bottom-up
survey about 31 existing datasets and a top-down strategy drawing upon three
classification systems in cognitive science, which prompt us to design two
novel types of tasks about view-taking and dynamic scenes. Extensive
experiments reveal that leading models fall behind human experts especially in
spatial orientation, a fundamental SI factor. Moreover, we demonstrate a
positive correlation between a model's spatial reasoning proficiency and its
performance on an embodied AI task.

</details>


### [152] [Generating Physically Stable and Buildable LEGO Designs from Text](https://arxiv.org/pdf/2505.05469)
*Ava Pun, Kangle Deng, Ruixuan Liu, Deva Ramanan, Changliu Liu, Jun-Yan Zhu*

Main category: cs.CV

TL;DR: LegoGPT generates stable LEGO models from text prompts using a large-scale dataset, physics-aware rollback, and validity checks.


<details>
  <summary>Details</summary>
Motivation: To create a method for generating physically stable and text-aligned LEGO designs from prompts.

Method: Uses an autoregressive LLM trained on a dataset of LEGO designs with captions, incorporating physics-aware rollback and validity checks during inference.

Result: Produces stable, diverse, and aesthetically pleasing LEGO designs, manually and robotically assemblable.

Conclusion: LegoGPT successfully bridges text prompts to stable LEGO models, with released dataset and models for broader use.

Abstract: We introduce LegoGPT, the first approach for generating physically stable
LEGO brick models from text prompts. To achieve this, we construct a
large-scale, physically stable dataset of LEGO designs, along with their
associated captions, and train an autoregressive large language model to
predict the next brick to add via next-token prediction. To improve the
stability of the resulting designs, we employ an efficient validity check and
physics-aware rollback during autoregressive inference, which prunes infeasible
token predictions using physics laws and assembly constraints. Our experiments
show that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO
designs that align closely with the input text prompts. We also develop a
text-based LEGO texturing method to generate colored and textured designs. We
show that our designs can be assembled manually by humans and automatically by
robotic arms. We also release our new dataset, StableText2Lego, containing over
47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed
captions, along with our code and models at the project website:
https://avalovelace1.github.io/LegoGPT/.

</details>


### [153] [Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation](https://arxiv.org/pdf/2505.05472)
*Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, Weilin Huang*

Main category: cs.CV

TL;DR: Mogao is a unified framework for interleaved multi-modal generation, combining autoregressive and diffusion models with key architectural improvements, achieving state-of-the-art performance in multi-modal tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of single-modal generation in unified models by enabling interleaved multi-modal generation and improving coherence and quality.

Method: Mogao integrates deep-fusion design, dual vision encoders, interleaved rotary position embeddings, and multi-modal classifier-free guidance, trained on a large-scale dataset.

Result: Achieves state-of-the-art performance in multi-modal understanding and text-to-image generation, with strong zero-shot capabilities.

Conclusion: Mogao is a practical omni-modal foundation model, paving the way for future unified multi-modal systems.

Abstract: Recent progress in unified models for image understanding and generation has
been impressive, yet most approaches remain limited to single-modal generation
conditioned on multiple modalities. In this paper, we present Mogao, a unified
framework that advances this paradigm by enabling interleaved multi-modal
generation through a causal approach. Mogao integrates a set of key technical
improvements in architecture design, including a deep-fusion design, dual
vision encoders, interleaved rotary position embeddings, and multi-modal
classifier-free guidance, which allow it to harness the strengths of both
autoregressive models for text generation and diffusion models for high-quality
image synthesis. These practical improvements also make Mogao particularly
effective to process interleaved sequences of text and images arbitrarily. To
further unlock the potential of unified models, we introduce an efficient
training strategy on a large-scale, in-house dataset specifically curated for
joint text and image generation. Extensive experiments show that Mogao not only
achieves state-of-the-art performance in multi-modal understanding and
text-to-image generation, but also excels in producing high-quality, coherent
interleaved outputs. Its emergent capabilities in zero-shot image editing and
compositional generation highlight Mogao as a practical omni-modal foundation
model, paving the way for future development and scaling the unified
multi-modal systems.

</details>


### [154] [DiffusionSfM: Predicting Structure and Motion via Ray Origin and Endpoint Diffusion](https://arxiv.org/pdf/2505.05473)
*Qitao Zhao, Amy Lin, Jeff Tan, Jason Y. Zhang, Deva Ramanan, Shubham Tulsiani*

Main category: cs.CV

TL;DR: DiffusionSfM proposes a data-driven multi-view reasoning approach for 3D scene reconstruction, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Current SfM methods rely on two-stage pipelines, which may limit efficiency and accuracy. DiffusionSfM aims to directly infer 3D geometry and camera poses from multi-view images.

Method: The framework uses a transformer-based denoising diffusion model to predict scene geometry and camera poses, parameterized as pixel-wise ray origins and endpoints. Specialized mechanisms address missing data and unbounded scene coordinates.

Result: DiffusionSfM outperforms classical and learning-based approaches on synthetic and real datasets, while naturally modeling uncertainty.

Conclusion: DiffusionSfM offers a robust, end-to-end solution for multi-view 3D reconstruction, demonstrating the potential of diffusion models in this domain.

Abstract: Current Structure-from-Motion (SfM) methods typically follow a two-stage
pipeline, combining learned or geometric pairwise reasoning with a subsequent
global optimization step. In contrast, we propose a data-driven multi-view
reasoning approach that directly infers 3D scene geometry and camera poses from
multi-view images. Our framework, DiffusionSfM, parameterizes scene geometry
and cameras as pixel-wise ray origins and endpoints in a global frame and
employs a transformer-based denoising diffusion model to predict them from
multi-view inputs. To address practical challenges in training diffusion models
with missing data and unbounded scene coordinates, we introduce specialized
mechanisms that ensure robust learning. We empirically validate DiffusionSfM on
both synthetic and real datasets, demonstrating that it outperforms classical
and learning-based approaches while naturally modeling uncertainty.

</details>


### [155] [3D Scene Generation: A Survey](https://arxiv.org/pdf/2505.05474)
*Beichen Wen, Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, Ziwei Liu*

Main category: cs.CV

TL;DR: A survey on 3D scene generation methods, covering procedural, neural 3D-based, image-based, and video-based paradigms, with analysis of their trade-offs, datasets, and future directions.


<details>
  <summary>Details</summary>
Motivation: To synthesize realistic and diverse 3D environments for applications like immersive media, robotics, and autonomous driving.

Method: Organizes approaches into four paradigms: procedural generation, neural 3D-based generation, image-based generation, and video-based generation. Analyzes their foundations, trade-offs, and results.

Result: Highlights advancements in fidelity, diversity, and view consistency using deep generative models (e.g., GANs, diffusion models) and 3D representations (e.g., NeRF).

Conclusion: Identifies challenges in generation capacity, 3D representation, and evaluation, and suggests future directions like physics-aware generation and unified models.

Abstract: 3D scene generation seeks to synthesize spatially structured, semantically
meaningful, and photorealistic environments for applications such as immersive
media, robotics, autonomous driving, and embodied AI. Early methods based on
procedural rules offered scalability but limited diversity. Recent advances in
deep generative models (e.g., GANs, diffusion models) and 3D representations
(e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene
distributions, improving fidelity, diversity, and view consistency. Recent
advances like diffusion models bridge 3D scene synthesis and photorealism by
reframing generation as image or video synthesis problems. This survey provides
a systematic overview of state-of-the-art approaches, organizing them into four
paradigms: procedural generation, neural 3D-based generation, image-based
generation, and video-based generation. We analyze their technical foundations,
trade-offs, and representative results, and review commonly used datasets,
evaluation protocols, and downstream applications. We conclude by discussing
key challenges in generation capacity, 3D representation, data and annotations,
and evaluation, and outline promising directions including higher fidelity,
physics-aware and interactive generation, and unified perception-generation
models. This review organizes recent advances in 3D scene generation and
highlights promising directions at the intersection of generative AI, 3D
vision, and embodied intelligence. To track ongoing developments, we maintain
an up-to-date project page:
https://github.com/hzxie/Awesome-3D-Scene-Generation.

</details>


### [156] [SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with Video Diffusion and Data Augmentation](https://arxiv.org/pdf/2505.05475)
*Yonwoo Choi*

Main category: cs.CV

TL;DR: SVAD combines video diffusion and 3DGS to create high-quality animatable 3D avatars from a single image, outperforming SOTA methods in identity consistency and detail preservation.


<details>
  <summary>Details</summary>
Motivation: Existing methods either require multiple views (3DGS) or struggle with consistency (video diffusion). SVAD aims to bridge this gap by leveraging both techniques.

Method: SVAD uses video diffusion to generate synthetic training data, enhances it with identity preservation and image restoration, and trains 3DGS avatars with this refined data.

Result: SVAD outperforms SOTA methods in identity consistency, fine detail preservation, and real-time rendering, while reducing dependency on dense training data.

Conclusion: SVAD establishes a novel approach for high-fidelity avatar generation from a single image by combining diffusion models and 3DGS.

Abstract: Creating high-quality animatable 3D human avatars from a single image remains
a significant challenge in computer vision due to the inherent difficulty of
reconstructing complete 3D information from a single viewpoint. Current
approaches face a clear limitation: 3D Gaussian Splatting (3DGS) methods
produce high-quality results but require multiple views or video sequences,
while video diffusion models can generate animations from single images but
struggle with consistency and identity preservation. We present SVAD, a novel
approach that addresses these limitations by leveraging complementary strengths
of existing techniques. Our method generates synthetic training data through
video diffusion, enhances it with identity preservation and image restoration
modules, and utilizes this refined data to train 3DGS avatars. Comprehensive
evaluations demonstrate that SVAD outperforms state-of-the-art (SOTA)
single-image methods in maintaining identity consistency and fine details
across novel poses and viewpoints, while enabling real-time rendering
capabilities. Through our data augmentation pipeline, we overcome the
dependency on dense monocular or multi-view training data typically required by
traditional 3DGS approaches. Extensive quantitative, qualitative comparisons
show our method achieves superior performance across multiple metrics against
baseline models. By effectively combining the generative power of diffusion
models with both the high-quality results and rendering efficiency of 3DGS, our
work establishes a new approach for high-fidelity avatar generation from a
single image input.

</details>


### [157] [Transformer-based assignment decision network for multiple object tracking](https://arxiv.org/pdf/2208.03571)
*Athena Psalta, Vasileios Tsironis, Konstantinos Karantzalos*

Main category: cs.CV

TL;DR: TADN is a Transformer-based network for data association in MOT, eliminating explicit optimization during inference and achieving strong performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing data association methods require complex optimization steps during inference, which adds computational overhead. TADN aims to simplify this by directly inferring assignments without optimization.

Method: TADN, a Transformer-based Assignment Decision Network, is introduced to infer assignment pairs in a single forward pass. It is integrated into a simple MOT framework with a novel training strategy.

Result: TADN performs well on MOT17, MOT20, and UA-DETRAC benchmarks, despite lacking auxiliary components like occlusion handling.

Conclusion: TADN offers a simpler, efficient solution for data association in MOT, demonstrating strong performance without explicit optimization during inference.

Abstract: Data association is a crucial component for any multiple object tracking
(MOT) method that follows the tracking-by-detection paradigm. To generate
complete trajectories such methods employ a data association process to
establish assignments between detections and existing targets during each
timestep. Recent data association approaches try to solve either a
multi-dimensional linear assignment task or a network flow minimization problem
or tackle it via multiple hypotheses tracking. However, during inference an
optimization step that computes optimal assignments is required for every
sequence frame inducing additional complexity to any given solution. To this
end, in the context of this work we introduce Transformer-based Assignment
Decision Network (TADN) that tackles data association without the need of any
explicit optimization during inference. In particular, TADN can directly infer
assignment pairs between detections and active targets in a single forward pass
of the network. We have integrated TADN in a rather simple MOT framework,
designed a novel training strategy for efficient end-to-end training and
demonstrated the high potential of our approach for online visual
tracking-by-detection MOT on several popular benchmarks, i.e. MOT17, MOT20 and
UA-DETRAC. Our proposed approach demonstrates strong performance in most
evaluation metrics despite its simple nature as a tracker lacking significant
auxiliary components such as occlusion handling or re-identification. The
implementation of our method is publicly available at
https://github.com/psaltaath/tadn-mot.

</details>


### [158] [Label-Efficient Deep Learning in Medical Image Analysis: Challenges and Future Directions](https://arxiv.org/pdf/2303.12484)
*Cheng Jin, Zhengrui Guo, Yi Lin, Luyang Luo, Hao Chen*

Main category: cs.CV

TL;DR: The paper surveys label-efficient deep learning methods in medical imaging analysis (MIA), categorizing them into four paradigms and highlighting the role of health foundation models (HFMs).


<details>
  <summary>Details</summary>
Motivation: The high cost and effort of obtaining large-scale labeled datasets in MIA motivate the need for label-efficient learning methods.

Method: The authors systematically review over 350 studies, categorizing methods into no label, insufficient label, inexact label, and label refinement paradigms.

Result: The survey provides a taxonomy of label-efficient techniques, emphasizing shared principles and HFMs' role in leveraging limited annotations.

Conclusion: Challenges and future directions are identified to bridge the gap between research and clinical application of label-efficient learning.

Abstract: Deep learning has significantly advanced medical imaging analysis (MIA),
achieving state-of-the-art performance across diverse clinical tasks. However,
its success largely depends on large-scale, high-quality labeled datasets,
which are costly and time-consuming to obtain due to the need for expert
annotation. To mitigate this limitation, label-efficient deep learning methods
have emerged to improve model performance under limited supervision by
leveraging labeled, unlabeled, and weakly labeled data. In this survey, we
systematically review over 350 peer-reviewed studies and present a
comprehensive taxonomy of label-efficient learning methods in MIA. These
methods are categorized into four labeling paradigms: no label, insufficient
label, inexact label, and label refinement. For each category, we analyze
representative techniques across imaging modalities and clinical applications,
highlighting shared methodological principles and task-specific adaptations. We
also examine the growing role of health foundation models (HFMs) in enabling
label-efficient learning through large-scale pre-training and transfer
learning, enhancing the use of limited annotations in downstream tasks.
Finally, we identify current challenges and future directions to facilitate the
translation of label-efficient learning from research promise to everyday
clinical care.

</details>


### [159] [Semi-supervised Underwater Image Enhancement Using A Physics-Aware Triple-Stream Network](https://arxiv.org/pdf/2307.11470)
*Shixuan Xu, Hao Qi, Xinghui Dong*

Main category: cs.CV

TL;DR: The paper proposes PATS-UIENet, a deep learning network combining physics-based models for underwater image enhancement, addressing limitations of prior methods through semi-supervised learning.


<details>
  <summary>Details</summary>
Motivation: Underwater images degrade due to water transmission, and existing methods (prior-based or deep learning) struggle with flexibility or generalization due to insufficient data.

Method: PATS-UIENet uses three streams (D-Stream, B-Stream, A-Stream) to estimate degradation parameters of a revised Image Formation Model (IFM) and employs a semi-supervised learning framework.

Result: The method outperforms or matches 16 baselines across 6 testing sets in degradation estimation and image enhancement.

Conclusion: The physics-aware network and semi-supervised framework effectively model degradation and learn scene characteristics, achieving superior performance.

Abstract: Underwater images normally suffer from degradation due to the transmission
medium of water bodies. Both traditional prior-based approaches and deep
learning-based methods have been used to address this problem. However, the
inflexible assumption of the former often impairs their effectiveness in
handling diverse underwater scenes, while the generalization of the latter to
unseen images is usually weakened by insufficient data. In this study, we
leverage both the physics-based Image Formation Model (IFM) and deep learning
techniques for Underwater Image Enhancement (UIE). To this end, we propose a
novel Physics-Aware Triple-Stream Underwater Image Enhancement Network, i.e.,
PATS-UIENet, which comprises a Direct Signal Transmission Estimation Steam
(D-Stream), a Backscatter Signal Transmission Estimation Steam (B-Stream) and
an Ambient Light Estimation Stream (A-Stream). This network fulfills the UIE
task by explicitly estimating the degradation parameters of a revised IFM. We
also adopt an IFM-inspired semi-supervised learning framework, which exploits
both the labeled and unlabeled images, to address the issue of insufficient
data. To our knowledge, such a physics-aware deep network and the IFM-inspired
semi-supervised learning framework have not been used for the UIE task before.
Our method performs better than, or at least comparably to, sixteen baselines
across six testing sets in the degradation estimation and UIE tasks. These
promising results should be due to the fact that the proposed method can not
only model the degradation but also learn the characteristics of diverse
underwater scenes.

</details>


### [160] [USTEP: Spatio-Temporal Predictive Learning under A Unified View](https://arxiv.org/pdf/2310.05829)
*Cheng Tan, Jue Wang, Zhangyang Gao, Siyuan Li, Stan Z. Li*

Main category: cs.CV

TL;DR: The paper introduces USTEP, a unified framework for spatio-temporal predictive learning that combines recurrent-based and recurrent-free methods to improve efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing temporal modeling approaches (recurrent-based and recurrent-free) have limitations—recurrent methods ignore short-term redundancies, while recurrent-free methods overlook temporal dependencies. The paper aims to unify these approaches.

Method: The proposed USTEP framework integrates micro-temporal and macro-temporal scales, bridging the gap between recurrent-based and recurrent-free methods.

Result: USTEP outperforms existing temporal modeling approaches in spatio-temporal predictive learning tasks.

Conclusion: USTEP is a robust solution for spatio-temporal applications, offering significant improvements over prior methods.

Abstract: Spatio-temporal predictive learning plays a crucial role in self-supervised
learning, with wide-ranging applications across a diverse range of fields.
Previous approaches for temporal modeling fall into two categories:
recurrent-based and recurrent-free methods. The former, while meticulously
processing frames one by one, neglect short-term spatio-temporal information
redundancies, leading to inefficiencies. The latter naively stack frames
sequentially, overlooking the inherent temporal dependencies. In this paper, we
re-examine the two dominant temporal modeling approaches within the realm of
spatio-temporal predictive learning, offering a unified perspective. Building
upon this analysis, we introduce USTEP (Unified Spatio-TEmporal Predictive
learning), an innovative framework that reconciles the recurrent-based and
recurrent-free methods by integrating both micro-temporal and macro-temporal
scales. Extensive experiments on a wide range of spatio-temporal predictive
learning demonstrate that USTEP achieves significant improvements over existing
temporal modeling approaches, thereby establishing it as a robust solution for
a wide range of spatio-temporal applications.

</details>


### [161] [Two Views Are Better than One: Monocular 3D Pose Estimation with Multiview Consistency](https://arxiv.org/pdf/2311.12421)
*Christian Keilstrup Ingwersen, Rasmus Tirsgaard, Rasmus Nylander, Janus Nørtoft Jensen, Anders Bjorholm Dahl, Morten Rieger Hannemose*

Main category: cs.CV

TL;DR: A method improves 3D human pose estimation from 2D images using multiview data in training, introducing a consistency loss for better performance without 3D data during inference.


<details>
  <summary>Details</summary>
Motivation: Overcoming the ambiguity in 3D pose estimation from 2D images without costly 3D data or complex setups.

Method: Proposes a consistency loss for synchronized multiview data during training, penalizing pose sequence differences after alignment.

Result: Improves performance for fine-tuning and achieves state-of-the-art results in semi-supervised training.

Conclusion: Offers a simpler, scalable approach for 3D pose estimation with minimal data requirements.

Abstract: Deducing a 3D human pose from a single 2D image is inherently challenging
because multiple 3D poses can correspond to the same 2D representation. 3D data
can resolve this pose ambiguity, but it is expensive to record and requires an
intricate setup that is often restricted to controlled lab environments. We
propose a method that improves the performance of deep learning-based monocular
3D human pose estimation models by using multiview data only during training,
but not during inference. We introduce a novel loss function, consistency loss,
which operates on two synchronized views. This approach is simpler than
previous models that require 3D ground truth or intrinsic and extrinsic camera
parameters. Our consistency loss penalizes differences in two pose sequences
after rigid alignment. We also demonstrate that our consistency loss
substantially improves performance for fine-tuning without requiring 3D data.
Furthermore, we show that using our consistency loss can yield state-of-the-art
performance when training models from scratch in a semi-supervised manner. Our
findings provide a simple way to capture new data, e.g in a new domain. This
data can be added using off-the-shelf cameras with no calibration requirements.
We make all our code and data publicly available.

</details>


### [162] [FieldNet: Efficient Real-Time Shadow Removal for Enhanced Vision in Field Robotics](https://arxiv.org/pdf/2403.08142)
*Alzayat Saleh, Alex Olsen, Jake Wood, Bronson Philippa, Mostafa Rahimi Azghadi*

Main category: cs.CV

TL;DR: FieldNet is a deep learning framework for real-time shadow removal in outdoor robotics, offering improved accuracy and speed without needing shadow masks during inference.


<details>
  <summary>Details</summary>
Motivation: Shadows in outdoor environments hinder computer vision tasks like object detection and localization, especially in field robotics under varying lighting conditions.

Method: FieldNet uses a probabilistic enhancement module and a novel loss function to address inconsistent shadow boundary supervision and artefact generation. It is trained on 10,000 natural images with synthetic shadows.

Result: FieldNet outperforms state-of-the-art methods on benchmark datasets (ISTD, ISTD+, SRD) with 9x speed improvements (66 FPS) and superior shadow removal quality (PSNR: 38.67, SSIM: 0.991).

Conclusion: FieldNet proves to be a robust, efficient solution for real-time vision tasks in field robotics, as demonstrated in precision agriculture robotics for weed detection.

Abstract: Shadows significantly hinder computer vision tasks in outdoor environments,
particularly in field robotics, where varying lighting conditions complicate
object detection and localisation. We present FieldNet, a novel deep learning
framework for real-time shadow removal, optimised for resource-constrained
hardware. FieldNet introduces a probabilistic enhancement module and a novel
loss function to address challenges of inconsistent shadow boundary supervision
and artefact generation, achieving enhanced accuracy and simplicity without
requiring shadow masks during inference. Trained on a dataset of 10,000 natural
images augmented with synthetic shadows, FieldNet outperforms state-of-the-art
methods on benchmark datasets (ISTD, ISTD+, SRD), with up to $9$x speed
improvements (66 FPS on Nvidia 2080Ti) and superior shadow removal quality
(PSNR: 38.67, SSIM: 0.991). Real-world case studies in precision agriculture
robotics demonstrate the practical impact of FieldNet in enhancing weed
detection accuracy. These advancements establish FieldNet as a robust,
efficient solution for real-time vision tasks in field robotics and beyond.

</details>


### [163] [Generalizable Human Gaussians from Single-View Image](https://arxiv.org/pdf/2406.06050)
*Jinnan Chen, Chen Li, Jianfeng Zhang, Lingting Zhu, Buzhen Huang, Hanlin Chen, Gim Hee Lee*

Main category: cs.CV

TL;DR: A method for learning 3D human Gaussians from a single image, refining appearance and geometry using a generate-then-refine pipeline with human and diffusion priors.


<details>
  <summary>Details</summary>
Motivation: To recover detailed 3D human appearance and geometry, including unobserved regions, from a single image.

Method: Uses a Human Gaussian Model (HGM) with a generate-then-refine pipeline, guided by human body (SMPL-X) and diffusion priors. ControlNet refines back-view images, and features propagate via sparse convolution and attention.

Result: Outperforms previous methods in novel view synthesis and surface reconstruction, with strong cross-dataset and in-the-wild generalization.

Conclusion: The proposed HGM effectively recovers detailed 3D human models from single images, leveraging priors and iterative refinement for accuracy.

Abstract: In this work, we tackle the task of learning 3D human Gaussians from a single
image, focusing on recovering detailed appearance and geometry including
unobserved regions. We introduce a single-view generalizable Human Gaussian
Model (HGM), which employs a novel generate-then-refine pipeline with the
guidance from human body prior and diffusion prior. Our approach uses a
ControlNet to refine rendered back-view images from coarse predicted human
Gaussians, then uses the refined image along with the input image to
reconstruct refined human Gaussians. To mitigate the potential generation of
unrealistic human poses and shapes, we incorporate human priors from the SMPL-X
model as a dual branch, propagating image features from the SMPL-X volume to
the image Gaussians using sparse convolution and attention mechanisms. Given
that the initial SMPL-X estimation might be inaccurate, we gradually refine it
with our HGM model. We validate our approach on several publicly available
datasets. Our method surpasses previous methods in both novel view synthesis
and surface reconstruction. Our approach also exhibits strong generalization
for cross-dataset evaluation and in-the-wild images.

</details>


### [164] [Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding](https://arxiv.org/pdf/2409.03757)
*Yunze Man, Shuhong Zheng, Zhipeng Bao, Martial Hebert, Liang-Yan Gui, Yu-Xiong Wang*

Main category: cs.CV

TL;DR: A study evaluates various visual encoding models for 3D scene understanding, identifying strengths and limitations across tasks, with DINOv2 performing best.


<details>
  <summary>Details</summary>
Motivation: To clarify optimal scene encoding strategies for 3D scene understanding, which remain unclear compared to image-based methods.

Method: Evaluates seven vision foundation encoders (image, video, 3D models) across four tasks: Vision-Language Scene Reasoning, Visual Grounding, Segmentation, and Registration.

Result: DINOv2 excels overall, video models perform well in object-level tasks, diffusion models aid geometric tasks, and language-pretrained models underperform in language tasks.

Conclusion: Challenges conventional views, offers new insights for visual foundation models, and suggests flexible encoder selection for future tasks.

Abstract: Complex 3D scene understanding has gained increasing attention, with scene
encoding strategies playing a crucial role in this success. However, the
optimal scene encoding strategies for various scenarios remain unclear,
particularly compared to their image-based counterparts. To address this issue,
we present a comprehensive study that probes various visual encoding models for
3D scene understanding, identifying the strengths and limitations of each model
across different scenarios. Our evaluation spans seven vision foundation
encoders, including image-based, video-based, and 3D foundation models. We
evaluate these models in four tasks: Vision-Language Scene Reasoning, Visual
Grounding, Segmentation, and Registration, each focusing on different aspects
of scene understanding. Our evaluations yield key findings: DINOv2 demonstrates
superior performance, video models excel in object-level tasks, diffusion
models benefit geometric tasks, and language-pretrained models show unexpected
limitations in language-related tasks. These insights challenge some
conventional understandings, provide novel perspectives on leveraging visual
foundation models, and highlight the need for more flexible encoder selection
in future vision-language and scene-understanding tasks. Code:
https://github.com/YunzeMan/Lexicon3D

</details>


### [165] [Boosting Adverse Weather Crowd Counting via Multi-queue Contrastive Learning](https://arxiv.org/pdf/2408.05956)
*Tianhang Pan, Xiuyi Jia*

Main category: cs.CV

TL;DR: MQCL improves crowd counting accuracy in adverse weather by addressing domain gaps and weather class imbalance using a two-stage contrastive learning approach.


<details>
  <summary>Details</summary>
Motivation: Existing crowd counting methods perform poorly in adverse weather due to domain gaps and imbalanced training data.

Method: Proposes MQCL: a two-stage method using multi-queue contrastive learning for weather-aware representation and domain conversion.

Result: MQCL reduces counting error by 22% in adverse weather with only 13% computational overhead, achieving state-of-the-art performance.

Conclusion: MQCL effectively bridges the domain gap and improves robustness in crowd counting under diverse weather conditions.

Abstract: Currently, most crowd counting methods have outstanding performance under
normal weather conditions. However, our experimental validation reveals two key
obstacles limiting the accuracy improvement of crowd counting models: 1) the
domain gap between the adverse weather and the normal weather images; 2) the
weather class imbalance in the training set. To address the problems, we
propose a two-stage crowd counting method named Multi-queue Contrastive
Learning (MQCL). Specifically, in the first stage, our target is to equip the
backbone network with weather-awareness capabilities. In this process, a
contrastive learning method named multi-queue MoCo designed by us is employed
to enable representation learning under weather class imbalance. After the
first stage is completed, the backbone model is "mature" enough to extract
weather-related representations. On this basis, we proceed to the second stage,
in which we propose to refine the representations under the guidance of
contrastive learning, enabling the conversion of the weather-aware
representations to the normal weather domain. Through such representation and
conversion, the model achieves robust counting performance under both normal
and adverse weather conditions. Extensive experimental results show that,
compared to the baseline, MQCL reduces the counting error under adverse weather
conditions by 22%, while introducing only about 13% increase in computational
burden, which achieves state-of-the-art performance.

</details>


### [166] [Deep learning-based ecological analysis of camera trap images is impacted by training data quality and quantity](https://arxiv.org/pdf/2408.14348)
*Peggy A. Bevan, Omiros Pantazis, Holly Pringle, Guilherme Braga Ferreira, Daniel J. Ingram, Emily Madsen, Liam Thomas, Dol Raj Thanet, Thakur Silwal, Santosh Rayamajhi, Gabriel Brostow, Oisin Mac Aodha, Kate E. Jones*

Main category: cs.CV

TL;DR: Deep neural networks automate camera trap image labeling, closely matching expert-derived ecological metrics, though less common species are more affected by accuracy drops.


<details>
  <summary>Details</summary>
Motivation: Manual processing of camera trap images is time-consuming, and the impact of deep learning classification errors on ecological metrics is unclear.

Method: Analyzed data from African savannah and Asian dry forest camera traps, comparing expert and deep learning-derived metrics, assessing model architecture, label noise, and dataset size impacts.

Result: Deep learning metrics closely matched expert labels, resilient to training pipeline changes, but less common species were disproportionately affected by accuracy drops.

Conclusion: Practitioners should focus on large, clean, balanced training datasets over exploring multiple model architectures for reliable ecological metrics.

Abstract: Large image collections generated from camera traps offer valuable insights
into species richness, occupancy, and activity patterns, significantly aiding
biodiversity monitoring. However, the manual processing of these datasets is
time-consuming, hindering analytical processes. To address this, deep neural
networks have been adopted to automate image labelling, but the impact of
classification error on ecological metrics remains unclear. Here, we analyse
data from camera trap collections in an African savannah (82,300 images, 47
species) and an Asian sub-tropical dry forest (40,308 images, 29 species) to
compare ecological metrics derived from expert-generated species
identifications with those generated by deep learning classification models. We
specifically assess the impact of deep learning model architecture, the
proportion of label noise in the training data, and the size of the training
dataset on three ecological metrics: species richness, occupancy, and activity
patterns. Overall, ecological metrics derived from deep neural networks closely
match those calculated from expert labels and remain robust to manipulations in
the training pipeline. We found that the choice of deep learning model
architecture does not impact ecological metrics, and ecological metrics related
to the overall community (species richness, community occupancy) were resilient
to up to 10% noise in the training dataset and a 50% reduction in the training
dataset size. However, we caution that less common species are
disproportionately affected by a reduction in deep neural network accuracy, and
this has consequences for species-specific metrics (occupancy, diel activity
patterns). To ensure the reliability of their findings, practitioners should
prioritize creating large, clean training sets with balanced representation
across species over exploring numerous deep learning model architectures.

</details>


### [167] [On Synthetic Texture Datasets: Challenges, Creation, and Curation](https://arxiv.org/pdf/2409.10297)
*Blaine Hoak, Patrick McDaniel*

Main category: cs.CV

TL;DR: The paper introduces a method to generate a large, diverse texture dataset using text-to-image models, addressing limitations in texture research due to data scarcity.


<details>
  <summary>Details</summary>
Motivation: Existing texture research is limited by the lack of large, diverse datasets. Image generative models offer potential but are unexplored for texture synthesis.

Method: The authors develop a pipeline using text-to-image models (Stable Diffusion) to generate and filter high-quality texture images, resulting in the Prompted Textures Dataset (PTD).

Result: The PTD contains 362,880 images across 56 textures. The study also reveals biases in NSFW filters, flagging many texture images.

Conclusion: The dataset is high-quality and diverse, supporting texture-based tasks, while also highlighting challenges in generative models for textures.

Abstract: The influence of textures on machine learning models has been an ongoing
investigation, specifically in texture bias/learning, interpretability, and
robustness. However, due to the lack of large and diverse texture data
available, the findings in these works have been limited, as more comprehensive
evaluations have not been feasible. Image generative models are able to provide
data creation at scale, but utilizing these models for texture synthesis has
been unexplored and poses additional challenges both in creating accurate
texture images and validating those images. In this work, we introduce an
extensible methodology and corresponding new dataset for generating
high-quality, diverse texture images capable of supporting a broad set of
texture-based tasks. Our pipeline consists of: (1) developing prompts from a
range of descriptors to serve as input to text-to-image models, (2) adopting
and adapting Stable Diffusion pipelines to generate and filter the
corresponding images, and (3) further filtering down to the highest quality
images. Through this, we create the Prompted Textures Dataset (PTD), a dataset
of 362,880 texture images that span 56 textures. During the process of
generating images, we find that NSFW safety filters in image generation
pipelines are highly sensitive to texture (and flag up to 60\% of our texture
images), uncovering a potential bias in these models and presenting unique
challenges when working with texture data. Through both standard metrics and a
human evaluation, we find that our dataset is high quality and diverse. Our
dataset is available for download at https://zenodo.org/records/15359142.

</details>


### [168] [Perceive-IR: Learning to Perceive Degradation Better for All-in-One Image Restoration](https://arxiv.org/pdf/2408.15994)
*Xu Zhang, Jiaqi Ma, Guoli Wang, Qian Zhang, Huan Zhang, Lefei Zhang*

Main category: cs.CV

TL;DR: Perceive-IR is a backbone-agnostic All-in-One image restoration framework for fine-grained quality control across degradation types and severity levels, using modular design and quality-aware learning.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack fine-grained quality perception and adaptability due to highly customized backbones.

Method: Two-stage approach: 1) multi-level quality-driven prompt learning for nuanced quality understanding, 2) restoration with difficulty-adaptive perceptual loss.

Result: Enables fine-grained quality control and seamless integration into advanced models.

Conclusion: Perceive-IR addresses limitations of existing methods by offering adaptable, quality-aware image restoration.

Abstract: Existing All-in-One image restoration methods often fail to perceive
degradation types and severity levels simultaneously, overlooking the
importance of fine-grained quality perception. Moreover, these methods often
utilize highly customized backbones, which hinder their adaptability and
integration into more advanced restoration networks. To address these
limitations, we propose Perceive-IR, a novel backbone-agnostic All-in-One image
restoration framework designed for fine-grained quality control across various
degradation types and severity levels. Its modular structure allows core
components to function independently of specific backbones, enabling seamless
integration into advanced restoration models without significant modifications.
Specifically, Perceive-IR operates in two key stages: 1) multi-level
quality-driven prompt learning stage, where a fine-grained quality perceiver is
meticulously trained to discern three tier quality levels by optimizing the
alignment between prompts and images within the CLIP perception space. This
stage ensures a nuanced understanding of image quality, laying the groundwork
for subsequent restoration; 2) restoration stage, where the quality perceiver
is seamlessly integrated with a difficulty-adaptive perceptual loss, forming a
quality-aware learning strategy. This strategy not only dynamically
differentiates sample learning difficulty but also achieves fine-grained
quality control by driving the restored image toward the ground truth while
pulling it away from both low- and medium-quality samples.

</details>


### [169] [Automated detection of underdiagnosed medical conditions via opportunistic imaging](https://arxiv.org/pdf/2409.11686)
*Asad Aali, Andrew Johnston, Louis Blankemeier, Dave Van Veen, Laura T Derry, David Svec, Jason Hom, Robert D. Boutin, Akshay S. Chaudhari*

Main category: cs.CV

TL;DR: Deep learning analyzes opportunistic CT scans to identify diagnostic discrepancies, revealing low ICD coding rates for conditions like sarcopenia, hepatic steatosis, and ascites.


<details>
  <summary>Details</summary>
Motivation: To leverage opportunistic CT scans for detecting underdiagnosed conditions and improve diagnostic precision in clinical settings.

Method: Deep learning analysis of 2,674 inpatient CT scans to compare imaging phenotypes with radiology reports and ICD coding.

Result: Low ICD coding rates: 0.5% for sarcopenia, 3.2% for hepatic steatosis, and 30.7% for ascites.

Conclusion: Opportunistic CT can enhance diagnostic accuracy and improve precision medicine, particularly in risk adjustment models.

Abstract: Abdominal computed tomography (CT) scans are frequently performed in clinical
settings. Opportunistic CT involves repurposing routine CT images to extract
diagnostic information and is an emerging tool for detecting underdiagnosed
conditions such as sarcopenia, hepatic steatosis, and ascites. This study
utilizes deep learning methods to promote accurate diagnosis and clinical
documentation. We analyze 2,674 inpatient CT scans to identify discrepancies
between imaging phenotypes (characteristics derived from opportunistic CT
scans) and their corresponding documentation in radiology reports and ICD
coding. Through our analysis, we find that only 0.5%, 3.2%, and 30.7% of scans
diagnosed with sarcopenia, hepatic steatosis, and ascites (respectively)
through either opportunistic imaging or radiology reports were ICD-coded. Our
findings demonstrate opportunistic CT's potential to enhance diagnostic
precision and accuracy of risk adjustment models, offering advancements in
precision medicine.

</details>


### [170] [Locality-aware Cross-modal Correspondence Learning for Dense Audio-Visual Events Localization](https://arxiv.org/pdf/2409.07967)
*Ling Xing, Hongyu Qu, Rui Yan, Xiangbo Shu, Jinhui Tang*

Main category: cs.CV

TL;DR: LoCo improves DAVE by using local temporal continuity to filter irrelevant signals and enhance cross-modal alignment, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Complex audio-visual scenes often have asynchronization between modalities, making accurate event localization challenging. Existing methods struggle with shared semantics and irrelevant feature attention.

Method: LoCo uses Local Correspondence Feature (LCF) Modulation to focus on shared semantics and Local Adaptive Cross-modal (LAC) Interaction to dynamically adjust attention regions.

Result: LoCo provides solid performance gains and outperforms existing DAVE methods.

Conclusion: LoCo effectively addresses challenges in DAVE by leveraging local temporal continuity and adaptive cross-modal interaction.

Abstract: Dense-localization Audio-Visual Events (DAVE) aims to identify time
boundaries and corresponding categories for events that are both audible and
visible in a long video, where events may co-occur and exhibit varying
durations. However, complex audio-visual scenes often involve asynchronization
between modalities, making accurate localization challenging. Existing DAVE
solutions extract audio and visual features through unimodal encoders, and fuse
them via dense cross-modal interaction. However, independent unimodal encoding
struggles to emphasize shared semantics between modalities without cross-modal
guidance, while dense cross-modal attention may over-attend to semantically
unrelated audio-visual features. To address these problems, we present LoCo, a
Locality-aware cross-modal Correspondence learning framework for DAVE. LoCo
leverages the local temporal continuity of audio-visual events as important
guidance to filter irrelevant cross-modal signals and enhance cross-modal
alignment throughout both unimodal and cross-modal encoding stages. i)
Specifically, LoCo applies Local Correspondence Feature (LCF) Modulation to
enforce unimodal encoders to focus on modality-shared semantics by modulating
agreement between audio and visual features based on local cross-modal
coherence. ii) To better aggregate cross-modal relevant features, we further
customize Local Adaptive Cross-modal (LAC) Interaction, which dynamically
adjusts attention regions in a data-driven manner. This adaptive mechanism
focuses attention on local event boundaries and accommodates varying event
durations. By incorporating LCF and LAC, LoCo provides solid performance gains
and outperforms existing DAVE methods.

</details>


### [171] [Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models](https://arxiv.org/pdf/2410.03577)
*Xin Zou, Yizhou Wang, Yibo Yan, Yuanhuiyi Lyu, Kening Zheng, Sirui Huang, Junkai Chen, Peijie Jiang, Jia Liu, Chang Tang, Xuming Hu*

Main category: cs.CV

TL;DR: MemVR addresses hallucinations in MLLMs by re-injecting visual tokens as key-value memory when uncertainty is high, improving factual alignment without extra time cost.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in MLLMs stem from text decoder sensitivity to visual tokens, causing 'amnesia' about visual information.

Method: Proposes MemVR, a decoding paradigm that re-injects visual tokens via FFN as key-value memory at trigger layers during high uncertainty.

Result: Significantly reduces hallucinations across MLLMs and performs well in benchmarks without additional time overhead.

Conclusion: MemVR effectively mitigates hallucinations by leveraging a 'look-twice' mechanism, enhancing factual alignment in MLLMs.

Abstract: Despite their impressive capabilities, multimodal large language models
(MLLMs) are prone to hallucinations, i.e., the generated content that is
nonsensical or unfaithful to input sources. Unlike in LLMs, hallucinations in
MLLMs often stem from the sensitivity of text decoder to visual tokens, leading
to a phenomenon akin to "amnesia" about visual information. To address this
issue, we propose MemVR, a novel decoding paradigm inspired by common
cognition: when the memory of an image seen the moment before is forgotten,
people will look at it again for factual answers. Following this principle, we
treat visual tokens as supplementary evidence, re-injecting them into the MLLM
through Feed Forward Network (FFN) as "key-value memory" at the middle trigger
layer. This "look-twice" mechanism occurs when the model exhibits high
uncertainty during inference, effectively enhancing factual alignment.
Comprehensive experimental evaluations demonstrate that MemVR significantly
mitigates hallucination across various MLLMs and excels in general benchmarks
without incurring additional time overhead. The implementation is available
from https://github.com/1zhou-Wang/MemVR

</details>


### [172] [MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion](https://arxiv.org/pdf/2410.03825)
*Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: MonST3R introduces a geometry-first approach for dynamic scenes by estimating per-timestep pointmaps, adapting DUST3R's static-scene method, and achieves strong performance on video tasks despite limited training data.


<details>
  <summary>Details</summary>
Motivation: Current methods for dynamic scene geometry rely on complex multi-stage pipelines, which are error-prone. MonST3R aims to simplify this by directly estimating geometry.

Method: MonST3R adapts DUST3R's pointmap representation for dynamic scenes, fine-tunes on limited dynamic data, and introduces optimizations for video tasks.

Result: Outperforms prior work in video depth and camera pose estimation, showing robustness and efficiency, with promising 4D reconstruction results.

Conclusion: MonST3R successfully extends static-scene geometry estimation to dynamic scenes, demonstrating effectiveness with limited data and new optimizations.

Abstract: Estimating geometry from dynamic scenes, where objects move and deform over
time, remains a core challenge in computer vision. Current approaches often
rely on multi-stage pipelines or global optimizations that decompose the
problem into subtasks, like depth and flow, leading to complex systems prone to
errors. In this paper, we present Motion DUSt3R (MonST3R), a novel
geometry-first approach that directly estimates per-timestep geometry from
dynamic scenes. Our key insight is that by simply estimating a pointmap for
each timestep, we can effectively adapt DUST3R's representation, previously
only used for static scenes, to dynamic scenes. However, this approach presents
a significant challenge: the scarcity of suitable training data, namely
dynamic, posed videos with depth labels. Despite this, we show that by posing
the problem as a fine-tuning task, identifying several suitable datasets, and
strategically training the model on this limited data, we can surprisingly
enable the model to handle dynamics, even without an explicit motion
representation. Based on this, we introduce new optimizations for several
downstream video-specific tasks and demonstrate strong performance on video
depth and camera pose estimation, outperforming prior work in terms of
robustness and efficiency. Moreover, MonST3R shows promising results for
primarily feed-forward 4D reconstruction.

</details>


### [173] [SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding](https://arxiv.org/pdf/2504.21435)
*Chenkai Zhang, Yiming Lei, Zeming Liu, Haitao Leng, Shaoguo Liu, Tingting Gao, Qingjie Liu, Yunhong Wang*

Main category: cs.CV

TL;DR: SeriesBench is a new benchmark for evaluating MLLMs' understanding of narrative-driven video series, addressing gaps in existing benchmarks. It includes 105 series and 28 tasks, enhanced by the PC-DCoT framework for improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on standalone videos and visual elements, neglecting complex narratives in series. SeriesBench aims to fill this gap.

Method: Curated 105 narrative-driven series, introduced long-span annotation, and developed PC-DCoT for narrative reasoning.

Result: Existing MLLMs struggle with narrative series, but PC-DCoT improves their performance.

Conclusion: SeriesBench and PC-DCoT emphasize advancing MLLMs' narrative understanding for future development.

Abstract: With the rapid development of Multi-modal Large Language Models (MLLMs), an
increasing number of benchmarks have been established to evaluate the video
understanding capabilities of these models. However, these benchmarks focus on
standalone videos and mainly assess "visual elements" like human actions and
object states. In reality, contemporary videos often encompass complex and
continuous narratives, typically presented as a series. To address this
challenge, we propose SeriesBench, a benchmark consisting of 105 carefully
curated narrative-driven series, covering 28 specialized tasks that require
deep narrative understanding. Specifically, we first select a diverse set of
drama series spanning various genres. Then, we introduce a novel long-span
narrative annotation method, combined with a full-information transformation
approach to convert manual annotations into diverse task formats. To further
enhance model capacity for detailed analysis of plot structures and character
relationships within series, we propose a novel narrative reasoning framework,
PC-DCoT. Extensive results on SeriesBench indicate that existing MLLMs still
face significant challenges in understanding narrative-driven series, while
PC-DCoT enables these MLLMs to achieve performance improvements. Overall, our
SeriesBench and PC-DCoT highlight the critical necessity of advancing model
capabilities to understand narrative-driven series, guiding the future
development of MLLMs. SeriesBench is publicly available at
https://github.com/zackhxn/SeriesBench-CVPR2025.

</details>


### [174] [SceneCraft: Layout-Guided 3D Scene Generation](https://arxiv.org/pdf/2410.09049)
*Xiuyu Yang, Yunze Man, Jun-Kun Chen, Yu-Xiong Wang*

Main category: cs.CV

TL;DR: SceneCraft is a novel method for generating detailed indoor 3D scenes from text and layout preferences, outperforming existing methods in complexity and realism.


<details>
  <summary>Details</summary>
Motivation: Traditional 3D modeling is tedious, and existing text-to-3D methods are limited in scale and control.

Method: Uses a rendering-based technique to convert 3D layouts into 2D proxy maps, then a diffusion model to generate multi-view images for NeRF-based scene representation.

Result: Supports complex indoor scenes (e.g., multi-bedroom apartments) with diverse textures and realistic quality, surpassing prior methods.

Conclusion: SceneCraft advances text-to-3D generation for intricate indoor scenes, offering superior control and visual fidelity.

Abstract: The creation of complex 3D scenes tailored to user specifications has been a
tedious and challenging task with traditional 3D modeling tools. Although some
pioneering methods have achieved automatic text-to-3D generation, they are
generally limited to small-scale scenes with restricted control over the shape
and texture. We introduce SceneCraft, a novel method for generating detailed
indoor scenes that adhere to textual descriptions and spatial layout
preferences provided by users. Central to our method is a rendering-based
technique, which converts 3D semantic layouts into multi-view 2D proxy maps.
Furthermore, we design a semantic and depth conditioned diffusion model to
generate multi-view images, which are used to learn a neural radiance field
(NeRF) as the final scene representation. Without the constraints of panorama
image generation, we surpass previous methods in supporting complicated indoor
space generation beyond a single room, even as complicated as a whole
multi-bedroom apartment with irregular shapes and layouts. Through experimental
analysis, we demonstrate that our method significantly outperforms existing
approaches in complex indoor scene generation with diverse textures, consistent
geometry, and realistic visual quality. Code and more results are available at:
https://orangesodahub.github.io/SceneCraft

</details>


### [175] [Vision Transformers for Efficient Indoor Pathloss Radio Map Prediction](https://arxiv.org/pdf/2412.09507)
*Rafayel Mkrtchyan, Edvard Ghukasyan, Khoren Petrosyan, Hrant Khachatrian, Theofanis P. Raptis*

Main category: cs.CV

TL;DR: A deep learning-based approach using ViT with DINO-v2 pretrained weights is proposed for indoor pathloss prediction, showing robustness and improved generalization with augmentation and feature engineering.


<details>
  <summary>Details</summary>
Motivation: Indoor pathloss prediction is challenging due to environmental complexity and data scarcity, necessitating advanced modeling techniques.

Method: Utilizes a vision transformer (ViT) with DINO-v2 pretrained weights, processes floor maps and wall features, and evaluates architectural choices, augmentation, and feature engineering.

Result: Extensive augmentation improves generalization; feature engineering is vital in low-data regimes. The model demonstrates robustness in various scenarios.

Conclusion: The proposed ViT-based approach effectively addresses indoor pathloss prediction challenges, with augmentation and feature engineering playing key roles in performance.

Abstract: Indoor pathloss prediction is a fundamental task in wireless network
planning, yet it remains challenging due to environmental complexity and data
scarcity. In this work, we propose a deep learning-based approach utilizing a
vision transformer (ViT) architecture with DINO-v2 pretrained weights to model
indoor radio propagation. Our method processes a floor map with additional
features of the walls to generate indoor pathloss maps. We systematically
evaluate the effects of architectural choices, data augmentation strategies,
and feature engineering techniques. Our findings indicate that extensive
augmentation significantly improves generalization, while feature engineering
is crucial in low-data regimes. Through comprehensive experiments, we
demonstrate the robustness of our model across different generalization
scenarios.

</details>


### [176] [PhysFlow: Unleashing the Potential of Multi-modal Foundation Models and Video Diffusion for 4D Dynamic Physical Scene Simulation](https://arxiv.org/pdf/2411.14423)
*Zhuoman Liu, Weicai Ye, Yan Luximon, Pengfei Wan, Di Zhang*

Main category: cs.CV

TL;DR: PhysFlow enhances 4D dynamic scene simulation using multi-modal foundation models and video diffusion for realistic material interactions.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack the ability to represent complex real-world materials and interactions due to limited predictable parameters.

Method: Combines multi-modal models for material identification and 3D Gaussian splats, refined via video diffusion with differentiable MPM and optical flow guidance.

Result: Accurate prediction and realistic simulation of dynamic interactions in real-world scenarios.

Conclusion: PhysFlow advances accuracy and flexibility in physics-based simulations.

Abstract: Realistic simulation of dynamic scenes requires accurately capturing diverse
material properties and modeling complex object interactions grounded in
physical principles. However, existing methods are constrained to basic
material types with limited predictable parameters, making them insufficient to
represent the complexity of real-world materials. We introduce PhysFlow, a
novel approach that leverages multi-modal foundation models and video diffusion
to achieve enhanced 4D dynamic scene simulation. Our method utilizes
multi-modal models to identify material types and initialize material
parameters through image queries, while simultaneously inferring 3D Gaussian
splats for detailed scene representation. We further refine these material
parameters using video diffusion with a differentiable Material Point Method
(MPM) and optical flow guidance rather than render loss or Score Distillation
Sampling (SDS) loss. This integrated framework enables accurate prediction and
realistic simulation of dynamic interactions in real-world scenarios, advancing
both accuracy and flexibility in physics-based simulations.

</details>


### [177] [MambaNUT: Nighttime UAV Tracking via Mamba-based Adaptive Curriculum Learning](https://arxiv.org/pdf/2412.00626)
*You Wu, Xiangyang Yang, Xucheng Wang, Hengzhou Ye, Dan Zeng, Shuiwang Li*

Main category: cs.CV

TL;DR: MambaNUT introduces a pure Mamba-based tracking framework with linear complexity, adaptive curriculum learning, and outperforms existing methods in nighttime UAV tracking with lower computational costs.


<details>
  <summary>Details</summary>
Motivation: Overcoming limitations like reliance on image enhancement, scarce nighttime data, and high computational demands of ViT-based trackers.

Method: Uses a state space model (Mamba) for linear complexity, integrates feature learning and template-search coupling, and employs adaptive curriculum learning (ACL) with sampling and loss schedulers.

Result: Achieves state-of-the-art performance on nighttime UAV tracking benchmarks with reduced computational costs.

Conclusion: MambaNUT offers an efficient, end-to-end trainable solution for nighttime UAV tracking, balancing performance and resource use.

Abstract: Harnessing low-light enhancement and domain adaptation, nighttime UAV
tracking has made substantial strides. However, over-reliance on image
enhancement, limited high-quality nighttime data, and a lack of integration
between daytime and nighttime trackers hinder the development of an end-to-end
trainable framework. Additionally, current ViT-based trackers demand heavy
computational resources due to their reliance on the self-attention mechanism.
In this paper, we propose a novel pure Mamba-based tracking framework
(MambaNUT) that employs a state space model with linear complexity as its
backbone, incorporating a single-stream architecture that integrates feature
learning and template-search coupling within Vision Mamba. We introduce an
adaptive curriculum learning (ACL) approach that dynamically adjusts sampling
strategies and loss weights, thereby improving the model's ability of
generalization. Our ACL is composed of two levels of curriculum schedulers: (1)
sampling scheduler that transforms the data distribution from imbalanced to
balanced, as well as from easier (daytime) to harder (nighttime) samples; (2)
loss scheduler that dynamically assigns weights based on the size of the
training data and IoU of individual instances. Exhaustive experiments on
multiple nighttime UAV tracking benchmarks demonstrate that the proposed
MambaNUT achieves state-of-the-art performance while requiring lower
computational costs. The code will be available at
https://github.com/wuyou3474/MambaNUT.

</details>


### [178] [Expanding Event Modality Applications through a Robust CLIP-Based Encoder](https://arxiv.org/pdf/2412.03093)
*Sungheon Jeong, Hanning Chen, Sanggeon Yun, Suhyeon Cho, Wenjun Huang, Xiangjian Liu, Mohsen Imani*

Main category: cs.CV

TL;DR: The paper introduces an encoder that adapts CLIP for event-based data, enabling zero-shot learning and cross-modality applications with strong performance in object recognition.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of comprehensive event datasets and expand CLIP's capabilities to event-based data, enhancing its utility across diverse domains.

Method: Adapts CLIP's architecture to align event embeddings with image embeddings, supporting zero-shot learning and text alignment while preventing catastrophic forgetting.

Result: Achieves strong performance in object recognition, competitive zero-shot/few-shot learning, and generalizes to video-extracted events without retraining.

Conclusion: The encoder broadens event-based data's scope and utility, demonstrating transformative potential for cross-modal applications.

Abstract: This paper introduces a powerful encoder that transfers CLIP`s capabilities
to event-based data, enhancing its utility and expanding its applicability
across diverse domains. While large-scale datasets have significantly advanced
image-based models, the scarcity of comprehensive event datasets has limited
performance potential in event modality. To address this challenge, we adapt
CLIP`s architecture to align event embeddings with image embeddings, supporting
zero-shot learning and preserving text alignment while mitigating catastrophic
forgetting. Our encoder achieves strong performance in object recognition, with
competitive results in zero-shot and few-shot learning tasks. Notably, it
generalizes effectively to events extracted from video data without requiring
additional training, highlighting its versatility. Additionally, we integrate
this encoder within a cross-modality framework that facilitates interaction
across five modalities-Image, Event, Text, Sound, and Depth-expanding the
possibilities for cross-modal applications. Overall, this work underscores the
transformative potential of a robust event encoder, broadening the scope and
utility of event-based data across various fields.

</details>


### [179] [3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark](https://arxiv.org/pdf/2412.07825)
*Wufei Ma, Haoyu Chen, Guofeng Zhang, Yu-Cheng Chou, Celso M de Melo, Alan Yuille*

Main category: cs.CV

TL;DR: The paper introduces 3DSRBench, a benchmark for evaluating 3D spatial reasoning in large multi-modal models (LMMs), revealing their limitations and providing insights for future improvements.


<details>
  <summary>Details</summary>
Motivation: To address the understudied capabilities of LMMs in 3D spatial reasoning, which is crucial for applications like autonomous navigation and AR/VR.

Method: Creation of 3DSRBench with 2,772 annotated Q&A pairs across 12 question types, balanced data distribution, and FlipEval strategy. Includes subsets for common and uncommon camera viewpoints.

Result: LMMs show limitations in height, orientation, location, and multi-object reasoning, with degraded performance on uncommon viewpoints.

Conclusion: 3DSRBench highlights gaps in LMMs' 3D reasoning and offers a foundation for future advancements.

Abstract: 3D spatial reasoning is the ability to analyze and interpret the positions,
orientations, and spatial relationships of objects within the 3D space. This
allows models to develop a comprehensive understanding of the 3D scene,
enabling their applicability to a broader range of areas, such as autonomous
navigation, robotics, and AR/VR. While large multi-modal models (LMMs) have
achieved remarkable progress in a wide range of image and video understanding
tasks, their capabilities to perform 3D spatial reasoning on diverse natural
images are less studied. In this work we present the first comprehensive 3D
spatial reasoning benchmark, 3DSRBench, with 2,772 manually annotated visual
question-answer pairs across 12 question types. We conduct robust and thorough
evaluation of 3D spatial reasoning capabilities by balancing the data
distribution and adopting a novel FlipEval strategy. To further study the
robustness of 3D spatial reasoning w.r.t. camera 3D viewpoints, our 3DSRBench
includes two subsets with 3D spatial reasoning questions on paired images with
common and uncommon viewpoints. We benchmark a wide range of open-sourced and
proprietary LMMs, uncovering their limitations in various aspects of 3D
awareness, such as height, orientation, location, and multi-object reasoning,
as well as their degraded performance on images with uncommon camera
viewpoints. Our 3DSRBench provide valuable findings and insights about the
future development of LMMs with strong 3D reasoning capabilities. Our project
page and dataset is available https://3dsrbench.github.io.

</details>


### [180] [Texture Image Synthesis Using Spatial GAN Based on Vision Transformers](https://arxiv.org/pdf/2502.01842)
*Elahe Salari, Zohreh Azimifar*

Main category: cs.CV

TL;DR: ViT-SGAN, a hybrid model combining Vision Transformers and Spatial GANs, outperforms traditional methods in texture synthesis by leveraging specialized descriptors and self-attention.


<details>
  <summary>Details</summary>
Motivation: Traditional texture synthesis methods struggle with complex textures, prompting the need for advanced deep learning solutions.

Method: ViT-SGAN integrates Vision Transformers with Spatial GANs, using texture descriptors (mean-variance, textons) in self-attention for better spatial dependency capture.

Result: ViT-SGAN achieves superior texture quality, outperforming state-of-the-art models in metrics like FID, IS, SSIM, and LPIPS.

Conclusion: ViT-SGAN is efficient for diverse realistic texture synthesis, especially for regular and irregular textures.

Abstract: Texture synthesis is a fundamental task in computer vision, whose goal is to
generate visually realistic and structurally coherent textures for a wide range
of applications, from graphics to scientific simulations. While traditional
methods like tiling and patch-based techniques often struggle with complex
textures, recent advancements in deep learning have transformed this field. In
this paper, we propose ViT-SGAN, a new hybrid model that fuses Vision
Transformers (ViTs) with a Spatial Generative Adversarial Network (SGAN) to
address the limitations of previous methods. By incorporating specialized
texture descriptors such as mean-variance (mu, sigma) and textons into the
self-attention mechanism of ViTs, our model achieves superior texture
synthesis. This approach enhances the model's capacity to capture complex
spatial dependencies, leading to improved texture quality that is superior to
state-of-the-art models, especially for regular and irregular textures.
Comparison experiments with metrics such as FID, IS, SSIM, and LPIPS
demonstrate the substantial improvement of ViT-SGAN, which underlines its
efficiency in generating diverse realistic textures.

</details>


### [181] [Interact with me: Joint Egocentric Forecasting of Intent to Interact, Attitude and Social Actions](https://arxiv.org/pdf/2412.16698)
*Tongfei Bian, Yiming Ma, Mathieu Chollet, Victor Sanchez, Tanaya Guha*

Main category: cs.CV

TL;DR: The paper introduces SocialEgoNet, a graph-based spatiotemporal framework for forecasting user intent, attitude, and action in human-agent interactions using 1-second video input. It achieves real-time inference and 83.15% average accuracy.


<details>
  <summary>Details</summary>
Motivation: To enable efficient human-agent interaction by proactively recognizing user intent, attitude, and action from an egocentric perspective.

Method: Proposes SocialEgoNet, a hierarchical multitask learning framework using whole-body skeletons from 1-second video clips for fast inference.

Result: Achieves 83.15% average accuracy and real-time performance on the augmented JPL-Social dataset, outperforming baselines.

Conclusion: SocialEgoNet is effective for real-time forecasting in human-agent interactions, with potential for broader applications.

Abstract: For efficient human-agent interaction, an agent should proactively recognize
their target user and prepare for upcoming interactions. We formulate this
challenging problem as the novel task of jointly forecasting a person's intent
to interact with the agent, their attitude towards the agent and the action
they will perform, from the agent's (egocentric) perspective. So we propose
\emph{SocialEgoNet} - a graph-based spatiotemporal framework that exploits task
dependencies through a hierarchical multitask learning approach. SocialEgoNet
uses whole-body skeletons (keypoints from face, hands and body) extracted from
only 1 second of video input for high inference speed. For evaluation, we
augment an existing egocentric human-agent interaction dataset with new class
labels and bounding box annotations. Extensive experiments on this augmented
dataset, named JPL-Social, demonstrate \emph{real-time} inference and superior
performance (average accuracy across all tasks: 83.15\%) of our model
outperforming several competitive baselines. The additional annotations and
code will be available upon acceptance.

</details>


### [182] [DejAIvu: Identifying and Explaining AI Art on the Web in Real-Time with Saliency Maps](https://arxiv.org/pdf/2502.08821)
*Jocelyn Dzuong*

Main category: cs.CV

TL;DR: DejAIvu is a Chrome extension for detecting and explaining AI-generated images in real-time, addressing challenges like misinformation and digital forgery.


<details>
  <summary>Details</summary>
Motivation: The rise of AI-generated images poses risks like misinformation and authenticity issues, prompting the need for tools to detect and explain such content.

Method: DejAIvu uses an ONNX-optimized deep learning model for real-time detection, saliency heatmaps for explainability, and integrates in-browser inference.

Result: The tool achieves high accuracy and low latency, proving practical for AI image accountability.

Conclusion: DejAIvu effectively enhances transparency and interpretability in detecting AI-generated images, offering a deployable solution.

Abstract: The recent surge in advanced generative models, such as diffusion models and
generative adversarial networks (GANs), has led to an alarming rise in
AI-generated images across various domains on the web. While such technologies
offer benefits such as democratizing artistic creation, they also pose
challenges in misinformation, digital forgery, and authenticity verification.
Additionally, the uncredited use of AI-generated images in media and marketing
has sparked significant backlash from online communities. In response to this,
we introduce DejAIvu, a Chrome Web extension that combines real-time
AI-generated image detection with saliency-based explainability while users
browse the web. Using an ONNX-optimized deep learning model, DejAIvu
automatically analyzes images on websites such as Google Images, identifies
AI-generated content using model inference, and overlays a saliency heatmap to
highlight AI-related artifacts. Our approach integrates efficient in-browser
inference, gradient-based saliency analysis, and a seamless user experience,
ensuring that AI detection is both transparent and interpretable. We also
evaluate DejAIvu across multiple pretrained architectures and benchmark
datasets, demonstrating high accuracy and low latency, making it a practical
and deployable tool for enhancing AI image accountability. The code for this
system can be found at https://github.com/Noodulz/dejAIvu.

</details>


### [183] [Balanced 3DGS: Gaussian-wise Parallelism Rendering with Fine-Grained Tiling](https://arxiv.org/pdf/2412.17378)
*Hao Gui, Lin Hu, Rui Chen, Mingxiao Huang, Yuxin Yin, Jin Yang, Yong Wu, Chen Liu, Zhongxu Sun, Xueyang Zhang, Kun Zhan*

Main category: cs.CV

TL;DR: Balanced 3DGS improves 3D Gaussian Splatting training by addressing load imbalance with dynamic workload distribution, Gaussian-wise parallelism, and fine-grained tiling, boosting performance by up to 7.52x.


<details>
  <summary>Details</summary>
Motivation: Training 3DGS models is time-intensive due to load imbalance, causing poor kernel performance.

Method: Introduces inter-block dynamic workload distribution, Gaussian-wise parallel rendering, and fine-grained combined load balancing.

Result: Achieves up to 7.52x performance improvement in forward renderCUDA kernel.

Conclusion: Balanced 3DGS effectively solves load imbalance, enhancing training efficiency.

Abstract: 3D Gaussian Splatting (3DGS) is increasingly attracting attention in both
academia and industry owing to its superior visual quality and rendering speed.
However, training a 3DGS model remains a time-intensive task, especially in
load imbalance scenarios where workload diversity among pixels and Gaussian
spheres causes poor renderCUDA kernel performance. We introduce Balanced 3DGS,
a Gaussian-wise parallelism rendering with fine-grained tiling approach in 3DGS
training process, perfectly solving load-imbalance issues. First, we
innovatively introduce the inter-block dynamic workload distribution technique
to map workloads to Streaming Multiprocessor(SM) resources within a single GPU
dynamically, which constitutes the foundation of load balancing. Second, we are
the first to propose the Gaussian-wise parallel rendering technique to
significantly reduce workload divergence inside a warp, which serves as a
critical component in addressing load imbalance. Based on the above two
methods, we further creatively put forward the fine-grained combined load
balancing technique to uniformly distribute workload across all SMs, which
boosts the forward renderCUDA kernel performance by up to 7.52x. Besides, we
present a self-adaptive render kernel selection strategy during the 3DGS
training process based on different load-balance situations, which effectively
improves training efficiency.

</details>


### [184] [Quaternionic Reweighted Amplitude Flow for Phase Retrieval in Image Reconstruction](https://arxiv.org/pdf/2501.02180)
*Ren Hu, Pan Lian*

Main category: cs.CV

TL;DR: The paper introduces novel quaternionic phase retrieval algorithms (QRAF and QPAF) for color signal processing, showing improved recovery and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the quaternionic phase retrieval problem by leveraging quaternion algebra for better color signal management.

Method: Proposes QRAF algorithm and its variants (incremental, accelerated, adapted), along with the QPAF algorithm with linear convergence.

Result: Numerical experiments on synthetic data and real images show superior recovery performance and computational efficiency.

Conclusion: The proposed methods outperform state-of-the-art approaches in quaternionic phase retrieval.

Abstract: Quaternionic signal processing provides powerful tools for efficiently
managing color signals by preserving the intrinsic correlations among signal
dimensions through quaternion algebra. In this paper, we address the
quaternionic phase retrieval problem by systematically developing novel
algorithms based on an amplitude-based model. Specifically, we propose the
Quaternionic Reweighted Amplitude Flow (QRAF) algorithm, which is further
enhanced by three of its variants: incremental, accelerated, and adapted QRAF
algorithms. In addition, we introduce the Quaternionic Perturbed Amplitude Flow
(QPAF) algorithm, which has linear convergence. Extensive numerical experiments
on both synthetic data and real images, demonstrate that our proposed methods
significantly improve recovery performance and computational efficiency
compared to state-of-the-art approaches.

</details>


### [185] [LIVS: A Pluralistic Alignment Dataset for Inclusive Public Spaces](https://arxiv.org/pdf/2503.01894)
*Rashid Mushkani, Shravan Nayak, Hugo Berard, Allison Cohen, Shin Koseki, Hadrien Bertrand*

Main category: cs.CV

TL;DR: The paper introduces the LIVS dataset for multi-criteria alignment in text-to-image models, fine-tuned using DPO, and evaluates it through case studies highlighting intersectional preferences and community-driven alignment.


<details>
  <summary>Details</summary>
Motivation: To support inclusive urban planning by aligning text-to-image models with diverse, community-defined spatial preferences.

Method: Developed the LIVS dataset through participatory processes, fine-tuned Stable Diffusion XL using DPO, and evaluated via four case studies.

Result: DPO improves alignment with high annotation volume; preference patterns vary by identity; human prompts yield distinctive outputs; intersectional groups rate criteria differently.

Conclusion: LIVS enables context-aware alignment in spatial design, though community values remain heterogeneous and ambiguous.

Abstract: We introduce the Local Intersectional Visual Spaces (LIVS) dataset, a
benchmark for multi-criteria alignment, developed through a two-year
participatory process with 30 community organizations to support the
pluralistic alignment of text-to-image (T2I) models in inclusive urban
planning. The dataset encodes 37,710 pairwise comparisons across 13,462 images,
structured along six criteria - Accessibility, Safety, Comfort, Invitingness,
Inclusivity, and Diversity - derived from 634 community-defined concepts. Using
Direct Preference Optimization (DPO), we fine-tune Stable Diffusion XL to
reflect multi-criteria spatial preferences and evaluate the LIVS dataset and
the fine-tuned model through four case studies: (1) DPO increases alignment
with annotated preferences, particularly when annotation volume is high; (2)
preference patterns vary across participant identities, underscoring the need
for intersectional data; (3) human-authored prompts generate more distinctive
visual outputs than LLM-generated ones, influencing annotation decisiveness;
and (4) intersectional groups assign systematically different ratings across
criteria, revealing the limitations of single-objective alignment. While DPO
improves alignment under specific conditions, the prevalence of neutral ratings
indicates that community values are heterogeneous and often ambiguous. LIVS
provides a benchmark for developing T2I models that incorporate local,
stakeholder-driven preferences, offering a foundation for context-aware
alignment in spatial design.

</details>


### [186] [Semantic Shift Estimation via Dual-Projection and Classifier Reconstruction for Exemplar-Free Class-Incremental Learning](https://arxiv.org/pdf/2503.05423)
*Run He, Di Fang, Yicheng Xu, Yawen Cui, Ming Li, Cen Chen, Ziqian Zeng, Huiping Zhuang*

Main category: cs.CV

TL;DR: DPCR addresses catastrophic forgetting in exemplar-free class-incremental learning by estimating semantic shift and reducing decision bias, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing EFCIL methods struggle with semantic shift and decision bias, hindering balance between old and new knowledge.

Method: DPCR uses dual-projection for shift estimation and ridge regression for classifier reconstruction.

Result: DPCR outperforms state-of-the-art EFCIL methods across datasets.

Conclusion: DPCR effectively balances old and new tasks, mitigating forgetting and bias.

Abstract: Exemplar-Free Class-Incremental Learning (EFCIL) aims to sequentially learn
from distinct categories without retaining exemplars but easily suffers from
catastrophic forgetting of learned knowledge. While existing EFCIL methods
leverage knowledge distillation to alleviate forgetting, they still face two
critical challenges: semantic shift and decision bias. Specifically, the
embeddings of old tasks shift in the embedding space after learning new tasks,
and the classifier becomes biased towards new tasks due to training solely with
new data, thereby hindering the balance between old and new knowledge. To
address these issues, we propose the Dual-Projection Shift Estimation and
Classifier Reconstruction (DPCR) approach for EFCIL. DPCR effectively estimates
semantic shift through a dual-projection, which combines a learnable
transformation with a row-space projection to capture both task-wise and
category-wise shifts. Furthermore, to mitigate decision bias, DPCR employs
ridge regression to reformulate classifier training as a reconstruction
process. This reconstruction exploits previous information encoded in
covariance and prototype of each class after calibration with estimated shift,
thereby reducing decision bias. Extensive experiments demonstrate that, across
various datasets, DPCR effectively balances old and new tasks, outperforming
state-of-the-art EFCIL methods.

</details>


### [187] [A Quantitative Evaluation of the Expressivity of BMI, Pose and Gender in Body Embeddings for Recognition and Identification](https://arxiv.org/pdf/2503.06451)
*Basudha Pal, Siyuan Huang, Rama Chellappa*

Main category: cs.CV

TL;DR: The paper introduces a method to quantify attribute expressivity in ReID systems, revealing BMI's dominant influence and dynamic attribute encoding across layers and epochs.


<details>
  <summary>Details</summary>
Motivation: Existing ReID systems are biased by attributes like gender, pose, and BMI, raising fairness and generalization concerns. The study aims to measure and understand these biases.

Method: A secondary neural network quantifies attribute expressivity (mutual information between features and attributes) in three ReID models, analyzing layer-wise and epoch-wise trends.

Result: BMI shows the highest expressivity in final layers, with attribute rankings as BMI > Pitch > Gender > Yaw. Expressivity evolves dynamically during training.

Conclusion: Body attributes play a central role in ReID, and the proposed framework provides a principled way to analyze attribute-driven correlations.

Abstract: Person Re-identification (ReID) systems that match individuals across images
or video frames are essential in many real-world applications. However,
existing methods are often influenced by attributes such as gender, pose, and
body mass index (BMI), which vary in unconstrained settings and raise concerns
related to fairness and generalization. To address this, we extend the notion
of expressivity, defined as the mutual information between learned features and
specific attributes, using a secondary neural network to quantify how strongly
attributes are encoded. Applying this framework to three ReID models, we find
that BMI consistently shows the highest expressivity in the final layers,
indicating its dominant role in recognition. In the last attention layer,
attributes are ranked as BMI > Pitch > Gender > Yaw, revealing their relative
influences in representation learning. Expressivity values also evolve across
layers and training epochs, reflecting a dynamic encoding of attributes. These
findings demonstrate the central role of body attributes in ReID and establish
a principled approach for uncovering attribute driven correlations.

</details>


### [188] [How Do Multimodal Large Language Models Handle Complex Multimodal Reasoning? Placing Them in An Extensible Escape Game](https://arxiv.org/pdf/2503.10042)
*Ziyue Wang, Yurui Dong, Fuwen Luo, Minyuan Ruan, Zhili Cheng, Chi Chen, Peng Li, Yang Liu*

Main category: cs.CV

TL;DR: The paper introduces MM-Escape, a benchmark for evaluating multimodal reasoning in MLLMs, using an escape game environment to analyze intermediate behaviors and task completion.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations focus on final task completion, neglecting comprehensive analysis of reasoning processes in multimodal environments.

Method: Developed EscapeCraft, a customizable environment for free-form exploration, and tested MLLMs on varying difficulty tasks.

Result: MLLMs succeed in simple tasks but struggle with harder ones, revealing varied failure modes like poor spatial awareness and ineffective prop use.

Conclusion: MM-Escape highlights challenges in multimodal reasoning and identifies areas for improving MLLMs.

Abstract: The rapid advancing of Multimodal Large Language Models (MLLMs) has spurred
interest in complex multimodal reasoning tasks in the real-world and virtual
environment, which require coordinating multiple abilities, including visual
perception, visual reasoning, spatial awareness, and target deduction. However,
existing evaluations primarily assess the final task completion, often
degrading assessments to isolated abilities such as visual grounding and visual
question answering. Less attention is given to comprehensively and
quantitatively analyzing reasoning process in multimodal environments, which is
crucial for understanding model behaviors and underlying reasoning mechanisms
beyond merely task success. To address this, we introduce MM-Escape, an
extensible benchmark for investigating multimodal reasoning, inspired by
real-world escape games. MM-Escape emphasizes intermediate model behaviors
alongside final task completion. To achieve this, we develop EscapeCraft, a
customizable and open environment that enables models to engage in free-form
exploration for assessing multimodal reasoning. Extensive experiments show that
MLLMs, regardless of scale, can successfully complete the simplest room escape
tasks, with some exhibiting human-like exploration strategies. Yet, performance
dramatically drops as task difficulty increases. Moreover, we observe that
performance bottlenecks vary across models, revealing distinct failure modes
and limitations in their multimodal reasoning abilities, such as repetitive
trajectories without adaptive exploration, getting stuck in corners due to poor
visual spatial awareness, and ineffective use of acquired props, such as the
key. We hope our work sheds light on new challenges in multimodal reasoning,
and uncovers potential improvements in MLLMs capabilities.

</details>


### [189] [Search is All You Need for Few-shot Anomaly Detection](https://arxiv.org/pdf/2504.11895)
*Qishan Wang, Jia Guo, Shuyong Gao, Haofen Wang, Li Xiong, Junjie Hu, Hanqi Guo, Wenqiang Zhang*

Main category: cs.CV

TL;DR: VisionAD, a simple nearest-neighbor framework, outperforms state-of-the-art FSAD methods with minimal training, achieving high AUROC scores on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of few-shot anomaly detection in industrial inspection without complex prompt engineering or manual tuning.

Method: Uses scalable vision models, dual augmentation, multi-layer feature integration, and a class-aware memory bank for efficient detection.

Result: Achieves AUROC scores of 97.4%, 94.8%, and 70.8% on MVTec-AD, VisA, and Real-IAD benchmarks, surpassing competitors.

Conclusion: VisionAD's training-free, efficient approach is ideal for real-world applications with scarce data.

Abstract: Few-shot anomaly detection (FSAD) has emerged as a crucial yet challenging
task in industrial inspection, where normal distribution modeling must be
accomplished with only a few normal images. While existing approaches typically
employ multi-modal foundation models combining language and vision modalities
for prompt-guided anomaly detection, these methods often demand sophisticated
prompt engineering and extensive manual tuning. In this paper, we demonstrate
that a straightforward nearest-neighbor search framework can surpass
state-of-the-art performance in both single-class and multi-class FSAD
scenarios. Our proposed method, VisionAD, consists of four simple yet essential
components: (1) scalable vision foundation models that extract universal and
discriminative features; (2) dual augmentation strategies - support
augmentation to enhance feature matching adaptability and query augmentation to
address the oversights of single-view prediction; (3) multi-layer feature
integration that captures both low-frequency global context and high-frequency
local details with minimal computational overhead; and (4) a class-aware visual
memory bank enabling efficient one-for-all multi-class detection. Extensive
evaluations across MVTec-AD, VisA, and Real-IAD benchmarks demonstrate
VisionAD's exceptional performance. Using only 1 normal images as support, our
method achieves remarkable image-level AUROC scores of 97.4%, 94.8%, and 70.8%
respectively, outperforming current state-of-the-art approaches by significant
margins (+1.6%, +3.2%, and +1.4%). The training-free nature and superior
few-shot capabilities of VisionAD make it particularly appealing for real-world
applications where samples are scarce or expensive to obtain. Code is available
at https://github.com/Qiqigeww/VisionAD.

</details>


### [190] [Federated EndoViT: Pretraining Vision Transformers via Federated Learning on Endoscopic Image Collections](https://arxiv.org/pdf/2504.16612)
*Max Kirchner, Alexander C. Jenke, Sebastian Bodenstedt, Fiona R. Kolbinger, Oliver L. Saldanha, Jakob N. Kather, Martin Wagner, Stefanie Speidel*

Main category: cs.CV

TL;DR: Federated learning (FL) trains surgical foundation models without data sharing, using FedSAM and SWA, achieving comparable performance to centralized methods and excelling in limited-data scenarios.


<details>
  <summary>Details</summary>
Motivation: To address data-sharing limitations in minimally invasive surgery by enabling collaborative model training without transferring sensitive data.

Method: Adapts Masked Autoencoder for FL, enhanced with FedSAM and SWA, pretrained on Endo700k, and fine-tuned for surgical tasks like segmentation and recognition.

Result: FedSAM improves pretraining, reducing reconstruction loss. FL-EndoViT matches CEN-EndoViT in performance and outperforms it in limited-data segmentation and large-data action recognition.

Conclusion: FL offers privacy-preserving, robust surgical model training. Future work may explore FL in video models for spatiotemporal dynamics.

Abstract: Purpose: In this study, we investigate the training of foundation models
using federated learning to address data-sharing limitations and enable
collaborative model training without data transfer for minimally invasive
surgery. Methods: Inspired by the EndoViT study, we adapt the Masked
Autoencoder for federated learning, enhancing it with adaptive Sharpness-Aware
Minimization (FedSAM) and Stochastic Weight Averaging (SWA). Our model is
pretrained on the Endo700k dataset collection and later fine-tuned and
evaluated for tasks such as Semantic Segmentation, Action Triplet Recognition,
and Surgical Phase Recognition. Results: Our findings demonstrate that
integrating adaptive FedSAM into the federated MAE approach improves
pretraining, leading to a reduction in reconstruction loss per patch. The
application of FL-EndoViT in surgical downstream tasks results in performance
comparable to CEN-EndoViT. Furthermore, FL-EndoViT exhibits advantages over
CEN-EndoViT in surgical scene segmentation when data is limited and in action
triplet recognition when large datasets are used. Conclusion: These findings
highlight the potential of federated learning for privacy-preserving training
of surgical foundation models, offering a robust and generalizable solution for
surgical data science. Effective collaboration requires adapting federated
learning methods, such as the integration of FedSAM, which can accommodate the
inherent data heterogeneity across institutions. In future, exploring FL in
video-based models may enhance these capabilities by incorporating
spatiotemporal dynamics crucial for real-world surgical environments.

</details>


### [191] [Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing](https://arxiv.org/pdf/2504.21356)
*Hong Zhang, Zhongjie Duan, Xingjun Wang, Yuze Zhao, Weiyi Lu, Zhipeng Di, Yixuan Xu, Yingda Chen, Yu Zhang*

Main category: cs.CV

TL;DR: Nexus-Gen is a unified multimodal model combining LLMs and diffusion models for improved performance in understanding, generating, and editing images. It uses dual-phase alignment training and a prefilled autoregression strategy to address performance gaps.


<details>
  <summary>Details</summary>
Motivation: Existing open-source unified models underperform compared to domain-specific architectures. Nexus-Gen aims to bridge this gap by integrating LLMs and diffusion models.

Method: Dual-phase alignment training: (1) LLM predicts image embeddings, (2) vision decoder reconstructs images. Prefilled autoregression avoids error accumulation.

Result: Nexus-Gen achieves integrated capabilities for image understanding, generation, and editing.

Conclusion: Nexus-Gen successfully unifies multimodal tasks, with models and code publicly available for further research.

Abstract: Unified multimodal large language models (MLLMs) aim to integrate multimodal
understanding and generation abilities through a single framework. Despite
their versatility, existing open-source unified models exhibit performance gaps
against domain-specific architectures. To bridge this gap, we present
Nexus-Gen, a unified model that synergizes the language reasoning capabilities
of LLMs with the image synthesis power of diffusion models. To align the
embedding space of the LLM and diffusion model, we conduct a dual-phase
alignment training process. (1) The autoregressive LLM learns to predict image
embeddings conditioned on multimodal inputs, while (2) the vision decoder is
trained to reconstruct high-fidelity images from these embeddings. During
training the LLM, we identified a critical discrepancy between the
autoregressive paradigm's training and inference phases, where error
accumulation in continuous embedding space severely degrades generation
quality. To avoid this issue, we introduce a prefilled autoregression strategy
that prefills input sequence with position-embedded special tokens instead of
continuous embeddings. Through dual-phase training, Nexus-Gen has developed the
integrated capability to comprehensively address the image understanding,
generation and editing tasks. All models, datasets, and codes are published at
https://github.com/modelscope/Nexus-Gen.git to facilitate further advancements
across the field.

</details>


### [192] [DGSolver: Diffusion Generalist Solver with Universal Posterior Sampling for Image Restoration](https://arxiv.org/pdf/2504.21487)
*Hebaixu Wang, Jing Zhang, Haonan Guo, Di Wang, Jiayi Ma, Bo Du*

Main category: cs.CV

TL;DR: DGSolver is a diffusion generalist solver for universal image restoration, improving accuracy and efficiency with high-order solvers and universal posterior sampling.


<details>
  <summary>Details</summary>
Motivation: Existing methods for image restoration with diffusion models face issues with cumulative errors from reduced sampling steps and struggle to balance degradation representations and restoration quality.

Method: DGSolver derives exact ODEs for generalist diffusion models, uses high-order solvers with a queue-based accelerated sampling strategy, and integrates universal posterior sampling for better gradient approximation.

Result: DGSolver outperforms state-of-the-art methods in restoration accuracy, stability, and scalability.

Conclusion: DGSolver addresses key challenges in diffusion-based image restoration, offering superior performance and practical benefits.

Abstract: Diffusion models have achieved remarkable progress in universal image
restoration. While existing methods speed up inference by reducing sampling
steps, substantial step intervals often introduce cumulative errors. Moreover,
they struggle to balance the commonality of degradation representations and
restoration quality. To address these challenges, we introduce
\textbf{DGSolver}, a diffusion generalist solver with universal posterior
sampling. We first derive the exact ordinary differential equations for
generalist diffusion models and tailor high-order solvers with a queue-based
accelerated sampling strategy to improve both accuracy and efficiency. We then
integrate universal posterior sampling to better approximate
manifold-constrained gradients, yielding a more accurate noise estimation and
correcting errors in inverse inference. Extensive experiments show that
DGSolver outperforms state-of-the-art methods in restoration accuracy,
stability, and scalability, both qualitatively and quantitatively. Code and
models will be available at https://github.com/MiliLab/DGSolver.

</details>


### [193] [REHEARSE-3D: A Multi-modal Emulated Rain Dataset for 3D Point Cloud De-raining](https://arxiv.org/pdf/2504.21699)
*Abu Mohammed Raisuddin, Jesper Holmblad, Hamed Haghighi, Yuri Poledna, Maikol Funk Drechsler, Valentina Donzella, Eren Erdal Aksoy*

Main category: cs.CV

TL;DR: The paper introduces REHEARSE-3D, a large-scale, multi-modal emulated rain dataset for 3D point cloud de-raining, addressing sensor degradation in autonomous driving caused by rain.


<details>
  <summary>Details</summary>
Motivation: Sensor degradation due to rain affects LiDAR point clouds, posing safety risks in autonomous driving. Existing datasets lack scale and multi-modal data.

Method: The study releases REHEARSE-3D, featuring high-resolution LiDAR and 4D Radar data, annotated point-wise, with rain-characteristic information.

Result: The dataset is benchmarked for raindrop detection and removal, evaluating statistical and deep-learning models.

Conclusion: REHEARSE-3D advances research in weather-aware autonomous systems and will be publicly available.

Abstract: Sensor degradation poses a significant challenge in autonomous driving.
During heavy rainfall, the interference from raindrops can adversely affect the
quality of LiDAR point clouds, resulting in, for instance, inaccurate point
measurements. This, in turn, can potentially lead to safety concerns if
autonomous driving systems are not weather-aware, i.e., if they are unable to
discern such changes. In this study, we release a new, large-scale, multi-modal
emulated rain dataset, REHEARSE-3D, to promote research advancements in 3D
point cloud de-raining. Distinct from the most relevant competitors, our
dataset is unique in several respects. First, it is the largest point-wise
annotated dataset, and second, it is the only one with high-resolution LiDAR
data (LiDAR-256) enriched with 4D Radar point clouds logged in both daytime and
nighttime conditions in a controlled weather environment. Furthermore,
REHEARSE-3D involves rain-characteristic information, which is of significant
value not only for sensor noise modeling but also for analyzing the impact of
weather at a point level. Leveraging REHEARSE-3D, we benchmark raindrop
detection and removal in fused LiDAR and 4D Radar point clouds. Our
comprehensive study further evaluates the performance of various statistical
and deep-learning models. Upon publication, the dataset and benchmark models
will be made publicly available at: https://sporsho.github.io/REHEARSE3D.

</details>


### [194] [Transforming faces into video stories -- VideoFace2.0](https://arxiv.org/pdf/2505.02060)
*Branko Brkljač, Vladimir Kalušev, Branislav Popović, Milan Sečujski*

Main category: cs.CV

TL;DR: VideoFace2.0 is an advanced video analytics tool for face re-identification (ReID), enabling structured video outputs for applications like TV production and ML dataset creation. It combines detection, recognition, and tracking, achieving 18-25 fps and reducing false identities by 73%-93%.


<details>
  <summary>Details</summary>
Motivation: To create an efficient tool for structured video story creation, addressing needs in TV production, media analysis, and ML dataset generation for tasks like lip reading.

Method: Combines face detection, recognition, and passive tracking-by-detection for robust face ReID. Designed as a modular extension of video production equipment.

Result: Achieves 18-25 fps on consumer notebooks and reduces false identities by 73%-93%.

Conclusion: VideoFace2.0 is a practical solution for face ReID, with potential to inspire further development of application-specific video tools and high-quality dataset production.

Abstract: Face detection and face recognition have been in the focus of vision
community since the very beginnings. Inspired by the success of the original
Videoface digitizer, a pioneering device that allowed users to capture video
signals from any source, we have designed an advanced video analytics tool to
efficiently create structured video stories, i.e. identity-based information
catalogs. VideoFace2.0 is the name of the developed system for spatial and
temporal localization of each unique face in the input video, i.e. face
re-identification (ReID), which also allows their cataloging, characterization
and creation of structured video outputs for later downstream tasks. Developed
near real-time solution is primarily designed to be utilized in application
scenarios involving TV production, media analysis, and as an efficient tool for
creating large video datasets necessary for training machine learning (ML)
models in challenging vision tasks such as lip reading and multimodal speech
recognition. Conducted experiments confirm applicability of the proposed face
ReID algorithm that is combining the concepts of face detection, face
recognition and passive tracking-by-detection in order to achieve robust and
efficient face ReID. The system is envisioned as a compact and modular
extensions of the existing video production equipment. Presented results are
based on test implementation that achieves between 18-25 fps on consumer type
notebook. Ablation experiments also confirmed that the proposed algorithm
brings relative gain in the reduction of number of false identities in the
range of 73%-93%. We hope that the presented work and shared code
implementation will stimulate further interest in development of similar,
application specific video analysis tools, and lower the entry barrier for
production of high-quality multi-modal datasets in the future.

</details>


### [195] [Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly Detection](https://arxiv.org/pdf/2505.02393)
*Sungheon Jeong, Jihong Park, Mohsen Imani*

Main category: cs.CV

TL;DR: IEF-VAD improves video anomaly detection by fusing RGB and synthetic event representations, outperforming existing methods without needing event sensors or frame-level labels.


<details>
  <summary>Details</summary>
Motivation: Existing RGB-based detectors miss transient motion cues crucial for anomaly detection.

Method: IEF-VAD synthesizes event representations from RGB, fuses them with image features using uncertainty-aware techniques, and refines the fusion iteratively.

Result: Achieves state-of-the-art performance on multiple benchmarks without event sensors or labels.

Conclusion: Synthetic event representations enhance motion cues in RGB, enabling robust anomaly detection without specialized hardware.

Abstract: Most existing video anomaly detectors rely solely on RGB frames, which lack
the temporal resolution needed to capture abrupt or transient motion cues, key
indicators of anomalous events. To address this limitation, we propose
Image-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that
synthesizes event representations directly from RGB videos and fuses them with
image features through a principled, uncertainty-aware process. The system (i)
models heavy-tailed sensor noise with a Student`s-t likelihood, deriving
value-level inverse-variance weights via a Laplace approximation; (ii) applies
Kalman-style frame-wise updates to balance modalities over time; and (iii)
iteratively refines the fused latent state to erase residual cross-modal noise.
Without any dedicated event sensor or frame-level labels, IEF-VAD sets a new
state of the art across multiple real-world anomaly detection benchmarks. These
findings highlight the utility of synthetic event representations in
emphasizing motion cues that are often underrepresented in RGB frames, enabling
accurate and robust video understanding across diverse applications without
requiring dedicated event sensors. Code and models are available at
https://github.com/EavnJeong/IEF-VAD.

</details>


### [196] [Advances in Automated Fetal Brain MRI Segmentation and Biometry: Insights from the FeTA 2024 Challenge](https://arxiv.org/pdf/2505.02784)
*Vladyslav Zalevskyi, Thomas Sanchez, Misha Kaandorp, Margaux Roulet, Diego Fajardo-Rojas, Liu Li, Jana Hutter, Hongwei Bran Li, Matthew Barkovich, Hui Ji, Luca Wilhelmi, Aline Dändliker, Céline Steger, Mériam Koob, Yvan Gomez, Anton Jakovčić, Melita Klaić, Ana Adžić, Pavel Marković, Gracia Grabarić, Milan Rados, Jordina Aviles Verdera, Gregor Kasprian, Gregor Dovjak, Raphael Gaubert-Rachmühl, Maurice Aschwanden, Qi Zeng, Davood Karimi, Denis Peruzzo, Tommaso Ciceri, Giorgio Longari, Rachika E. Hamadache, Amina Bouzid, Xavier Lladó, Simone Chiarella, Gerard Martí-Juan, Miguel Ángel González Ballester, Marco Castellaro, Marco Pinamonti, Valentina Visani, Robin Cremese, Keïn Sam, Fleur Gaudfernau, Param Ahir, Mehul Parikh, Maximilian Zenk, Michael Baumgartner, Klaus Maier-Hein, Li Tianhong, Yang Hong, Zhao Longfei, Domen Preloznik, Žiga Špiclin, Jae Won Choi, Muyang Li, Jia Fu, Guotai Wang, Jingwen Jiang, Lyuyang Tong, Bo Du, Andrea Gondova, Sungmin You, Kiho Im, Abdul Qayyum, Moona Mazher, Steven A Niederer, Andras Jakab, Roxane Licandro, Kelly Payette, Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: The FeTA Challenge 2024 focused on automated fetal brain MRI analysis, introducing biometry prediction and a low-field MRI dataset. While segmentation methods performed well, biometry prediction lagged, and topological metrics revealed new insights.


<details>
  <summary>Details</summary>
Motivation: To advance fetal brain MRI analysis by addressing segmentation, biometry prediction, and evaluating performance across diverse datasets, including low-field MRI.

Method: Sixteen teams submitted segmentation methods, and seven participated in biometry prediction. Metrics included conventional and topological (Euler characteristic difference) evaluations.

Result: Segmentation accuracy neared inter-rater variability, but biometry prediction struggled. Low-field MRI performed well, and topological metrics identified missed differences.

Conclusion: The challenge highlights the need for data-centric approaches, better topological evaluation, and diverse datasets to improve AI tools for fetal brain MRI.

Abstract: Accurate fetal brain tissue segmentation and biometric analysis are essential
for studying brain development in utero. The FeTA Challenge 2024 advanced
automated fetal brain MRI analysis by introducing biometry prediction as a new
task alongside tissue segmentation. For the first time, our diverse
multi-centric test set included data from a new low-field (0.55T) MRI dataset.
Evaluation metrics were also expanded to include the topology-specific Euler
characteristic difference (ED). Sixteen teams submitted segmentation methods,
most of which performed consistently across both high- and low-field scans.
However, longitudinal trends indicate that segmentation accuracy may be
reaching a plateau, with results now approaching inter-rater variability. The
ED metric uncovered topological differences that were missed by conventional
metrics, while the low-field dataset achieved the highest segmentation scores,
highlighting the potential of affordable imaging systems when paired with
high-quality reconstruction. Seven teams participated in the biometry task, but
most methods failed to outperform a simple baseline that predicted measurements
based solely on gestational age, underscoring the challenge of extracting
reliable biometric estimates from image data alone. Domain shift analysis
identified image quality as the most significant factor affecting model
generalization, with super-resolution pipelines also playing a substantial
role. Other factors, such as gestational age, pathology, and acquisition site,
had smaller, though still measurable, effects. Overall, FeTA 2024 offers a
comprehensive benchmark for multi-class segmentation and biometry estimation in
fetal brain MRI, underscoring the need for data-centric approaches, improved
topological evaluation, and greater dataset diversity to enable clinically
robust and generalizable AI tools.

</details>


### [197] [Motion-compensated cardiac MRI using low-rank diffeomorphic flow (DMoCo)](https://arxiv.org/pdf/2505.03149)
*Joseph Kettelkamp, Ludovica Romanin, Sarv Priya, Mathews Jacob*

Main category: cs.CV

TL;DR: Unsupervised motion-compensated 3D cardiac MRI reconstruction using a low-rank model for diffeomorphisms, improving recovery over current methods.


<details>
  <summary>Details</summary>
Motivation: To address challenges in free-breathing and ungated 3D cardiac MRI by developing a more constrained motion model.

Method: Represents image volumes as deformations of a static template, using a low-rank model for diffeomorphisms parameterized by motion phases. Learns template and motion parameters directly from k-space data.

Result: Improved recovery compared to existing motion-resolved and motion-compensated algorithms.

Conclusion: The proposed low-rank motion model enhances unsupervised reconstruction for free-breathing 3D cardiac MRI.

Abstract: We introduce an unsupervised motion-compensated image reconstruction
algorithm for free-breathing and ungated 3D cardiac magnetic resonance imaging
(MRI). We express the image volume corresponding to each specific motion phase
as the deformation of a single static image template. The main contribution of
the work is the low-rank model for the compact joint representation of the
family of diffeomorphisms, parameterized by the motion phases. The
diffeomorphism at a specific motion phase is obtained by integrating a
parametric velocity field along a path connecting the reference template phase
to the motion phase. The velocity field at different phases is represented
using a low-rank model. The static template and the low-rank motion model
parameters are learned directly from the k-space data in an unsupervised
fashion. The more constrained motion model is observed to offer improved
recovery compared to current motion-resolved and motion-compensated algorithms
for free-breathing 3D cine MRI.

</details>


### [198] [MAISY: Motion-Aware Image SYnthesis for Medical Image Motion Correction](https://arxiv.org/pdf/2505.04105)
*Andrew Zhang, Hao Wang, Shuchang Ye, Michael Fulham, Jinman Kim*

Main category: cs.CV

TL;DR: MAISY improves motion artifact correction in medical images by dynamically learning spatial patterns and introducing a VS-SSIM loss, outperforming GAN-based methods.


<details>
  <summary>Details</summary>
Motivation: Current GAN-based methods overlook localized features and struggle with varying pixel intensities, limiting their effectiveness in medical imaging.

Method: MAISY uses SAM for dynamic spatial pattern learning and VS-SSIM loss to emphasize high-variance regions, enhancing artifact correction.

Result: MAISY outperforms state-of-the-art methods with PSNR up by 40%, SSIM by 10%, and Dice by 16%.

Conclusion: MAISY addresses limitations of existing methods, offering superior motion artifact correction in medical images.

Abstract: Patient motion during medical image acquisition causes blurring, ghosting,
and distorts organs, which makes image interpretation challenging. Current
state-of-the-art algorithms using Generative Adversarial Network (GAN)-based
methods with their ability to learn the mappings between corrupted images and
their ground truth via Structural Similarity Index Measure (SSIM) loss
effectively generate motion-free images. However, we identified the following
limitations: (i) they mainly focus on global structural characteristics and
therefore overlook localized features that often carry critical pathological
information, and (ii) the SSIM loss function struggles to handle images with
varying pixel intensities, luminance factors, and variance. In this study, we
propose Motion-Aware Image SYnthesis (MAISY) which initially characterize
motion and then uses it for correction by: (a) leveraging the foundation model
Segment Anything Model (SAM), to dynamically learn spatial patterns along
anatomical boundaries where motion artifacts are most pronounced and, (b)
introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively
emphasizes spatial regions with high pixel variance to preserve essential
anatomical details during artifact correction. Experiments on chest and head CT
datasets demonstrate that our model outperformed the state-of-the-art
counterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by
10%, and Dice by 16%.

</details>


### [199] [Learning from Similarity Proportion Loss for Classifying Skeletal Muscle Recovery Stages](https://arxiv.org/pdf/2505.04150)
*Yu Yamaoka, Weng Ian Chan, Shigeto Seno, Soichiro Fukada, Hideo Matsuda*

Main category: cs.CV

TL;DR: The paper proposes OSLSP, a method for automating muscle tissue regeneration analysis using weakly supervised learning, addressing limitations of current LLP methods by incorporating ordinal class information.


<details>
  <summary>Details</summary>
Motivation: To replace manual, subjective muscle tissue regeneration analysis with automated, quantitative methods using machine learning, especially given limited labeled data.

Method: Introduces OSLSP, which uses similarity proportion loss and class proportion attention to adapt feature extraction and preserve ordinal class information.

Result: OSLSP outperforms large-scale pre-trained and fine-tuning models in classifying skeletal muscle recovery stages.

Conclusion: OSLSP provides a more effective automated solution for muscle regeneration analysis by addressing key limitations of existing LLP methods.

Abstract: Evaluating the regeneration process of damaged muscle tissue is a fundamental
analysis in muscle research to measure experimental effect sizes and uncover
mechanisms behind muscle weakness due to aging and disease. The conventional
approach to assessing muscle tissue regeneration involves whole-slide imaging
and expert visual inspection of the recovery stages based on the morphological
information of cells and fibers. There is a need to replace these tasks with
automated methods incorporating machine learning techniques to ensure a
quantitative and objective analysis. Given the limited availability of fully
labeled data, a possible approach is Learning from Label Proportions (LLP), a
weakly supervised learning method using class label proportions. However,
current LLP methods have two limitations: (1) they cannot adapt the feature
extractor for muscle tissues, and (2) they treat the classes representing
recovery stages and cell morphological changes as nominal, resulting in the
loss of ordinal information. To address these issues, we propose Ordinal Scale
Learning from Similarity Proportion (OSLSP), which uses a similarity proportion
loss derived from two bag combinations. OSLSP can update the feature extractor
by using class proportion attention to the ordinal scale of the class. Our
model with OSLSP outperforms large-scale pre-trained and fine-tuning models in
classification tasks of skeletal muscle recovery stages.

</details>


### [200] [FA-KPConv: Introducing Euclidean Symmetries to KPConv via Frame Averaging](https://arxiv.org/pdf/2505.04485)
*Ali Alawieh, Alexandru P. Condurache*

Main category: cs.CV

TL;DR: FA-KPConv enhances KPConv by making it exactly invariant/equivariant to Euclidean transformations, improving performance in tasks like classification and registration, especially with limited data or random rotations.


<details>
  <summary>Details</summary>
Motivation: KPConv lacks exact invariance/equivariance to Euclidean transformations, requiring large datasets or augmentations. FA-KPConv addresses this by embedding geometric priors without extra parameters.

Method: FA-KPConv uses Frame Averaging to wrap existing KPConv networks, ensuring exact invariance/equivariance to translations, rotations, and reflections.

Result: Improves performance in point cloud classification and registration, particularly with scarce training data or randomly rotated test data.

Conclusion: FA-KPConv effectively integrates geometric priors into KPConv, enhancing robustness without compromising input information or adding parameters.

Abstract: We present Frame-Averaging Kernel-Point Convolution (FA-KPConv), a neural
network architecture built on top of the well-known KPConv, a widely adopted
backbone for 3D point cloud analysis. Even though invariance and/or
equivariance to Euclidean transformations are required for many common tasks,
KPConv-based networks can only approximately achieve such properties when
training on large datasets or with significant data augmentations. Using Frame
Averaging, we allow to flexibly customize point cloud neural networks built
with KPConv layers, by making them exactly invariant and/or equivariant to
translations, rotations and/or reflections of the input point clouds. By simply
wrapping around an existing KPConv-based network, FA-KPConv embeds geometrical
prior knowledge into it while preserving the number of learnable parameters and
not compromising any input information. We showcase the benefit of such an
introduced bias for point cloud classification and point cloud registration,
especially in challenging cases such as scarce training data or randomly
rotated test data.

</details>


### [201] [Defining and Quantifying Creative Behavior in Popular Image Generators](https://arxiv.org/pdf/2505.04497)
*Aditi Ramaswamy, Hana Chockler, Melane Navaratnarajah*

Main category: cs.CV

TL;DR: The paper introduces quantitative measures to assess the creativity of generative AI models, aiding users in selecting the right model for tasks, with results aligning with human intuition.


<details>
  <summary>Details</summary>
Motivation: To address the ongoing debate about the creativity of generative AI models by providing practical, measurable criteria for evaluation.

Method: Developed quantitative measures for creativity and tested them on popular image-to-image generation models.

Result: The measures were found to align with human intuition, suggesting their practical utility.

Conclusion: The proposed measures offer a practical way to evaluate and choose generative AI models based on their creativity.

Abstract: Creativity of generative AI models has been a subject of scientific debate in
the last years, without a conclusive answer. In this paper, we study creativity
from a practical perspective and introduce quantitative measures that help the
user to choose a suitable AI model for a given task. We evaluated our measures
on a number of popular image-to-image generation models, and the results of
this suggest that our measures conform to human intuition.

</details>


### [202] [HunyuanCustom: A Multimodal-Driven Architecture for Customized Video Generation](https://arxiv.org/pdf/2505.04512)
*Teng Hu, Zhentao Yu, Zhengguang Zhou, Sen Liang, Yuan Zhou, Qin Lin, Qinglin Lu*

Main category: cs.CV

TL;DR: HunyuanCustom is a multi-modal video generation framework that enhances subject consistency and supports diverse input modalities like image, audio, video, and text.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with identity consistency and limited input modalities in customized video generation.

Method: The framework introduces a text-image fusion module, image ID enhancement, AudioNet for audio alignment, and a video-driven injection module for video conditioning.

Result: HunyuanCustom outperforms state-of-the-art methods in ID consistency, realism, and text-video alignment, validated across various tasks.

Conclusion: Multi-modal conditioning and identity-preserving strategies effectively advance controllable video generation.

Abstract: Customized video generation aims to produce videos featuring specific
subjects under flexible user-defined conditions, yet existing methods often
struggle with identity consistency and limited input modalities. In this paper,
we propose HunyuanCustom, a multi-modal customized video generation framework
that emphasizes subject consistency while supporting image, audio, video, and
text conditions. Built upon HunyuanVideo, our model first addresses the
image-text conditioned generation task by introducing a text-image fusion
module based on LLaVA for enhanced multi-modal understanding, along with an
image ID enhancement module that leverages temporal concatenation to reinforce
identity features across frames. To enable audio- and video-conditioned
generation, we further propose modality-specific condition injection
mechanisms: an AudioNet module that achieves hierarchical alignment via spatial
cross-attention, and a video-driven injection module that integrates
latent-compressed conditional video through a patchify-based feature-alignment
network. Extensive experiments on single- and multi-subject scenarios
demonstrate that HunyuanCustom significantly outperforms state-of-the-art open-
and closed-source methods in terms of ID consistency, realism, and text-video
alignment. Moreover, we validate its robustness across downstream tasks,
including audio and video-driven customized video generation. Our results
highlight the effectiveness of multi-modal conditioning and identity-preserving
strategies in advancing controllable video generation. All the code and models
are available at https://hunyuancustom.github.io.

</details>


### [203] [MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection](https://arxiv.org/pdf/2505.04594)
*Zhihao Zhang, Abhinav Kumar, Girish Chandar Ganesan, Xiaoming Liu*

Main category: cs.CV

TL;DR: MonoCoP improves monocular 3D object detection by using a Chain-of-Prediction (CoP) to sequentially predict and condition 3D attributes, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook the inter-correlation of 3D attributes, limiting accuracy. MonoCoP addresses this by conditioning predictions on prior attributes.

Method: Uses AttributeNet for attribute-specific features, constructs a CoP chain for feature propagation, and employs residual connections to aggregate features.

Result: Achieves SoTA on KITTI and outperforms methods on Waymo and nuScenes datasets.

Conclusion: MonoCoP demonstrates that sequential and conditional prediction of 3D attributes enhances accuracy and stability in monocular 3D detection.

Abstract: Accurately predicting 3D attributes is crucial for monocular 3D object
detection (Mono3D), with depth estimation posing the greatest challenge due to
the inherent ambiguity in mapping 2D images to 3D space. While existing methods
leverage multiple depth cues (e.g., estimating depth uncertainty, modeling
depth error) to improve depth accuracy, they overlook that accurate depth
prediction requires conditioning on other 3D attributes, as these attributes
are intrinsically inter-correlated through the 3D to 2D projection, which
ultimately limits overall accuracy and stability. Inspired by Chain-of-Thought
(CoT) in large language models (LLMs), this paper proposes MonoCoP, which
leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and
conditionally via three key designs. First, it employs a lightweight
AttributeNet (AN) for each 3D attribute to learn attribute-specific features.
Next, MonoCoP constructs an explicit chain to propagate these learned features
from one attribute to the next. Finally, MonoCoP uses a residual connection to
aggregate features for each attribute along the chain, ensuring that later
attribute predictions are conditioned on all previously processed attributes
without forgetting the features of earlier ones. Experimental results show that
our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI
leaderboard without requiring additional data and further surpasses existing
methods on the Waymo and nuScenes frontal datasets.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [204] [Towards Artificial Intelligence Research Assistant for Expert-Involved Learning](https://arxiv.org/pdf/2505.04638)
*Tianyu Liu, Simeng Han, Xiao Luo, Hanchen Wang, Pan Lu, Biqing Zhu, Yuge Wang, Keyi Li, Jiapeng Chen, Rihao Qu, Yufeng Liu, Xinyue Cui, Aviv Yaish, Yuhang Chen, Minsheng Hao, Chuhan Li, Kexing Li, Arman Cohan, Hua Xu, Mark Gerstein, James Zou, Hongyu Zhao*

Main category: cs.AI

TL;DR: The paper introduces ARIEL, a multimodal dataset to benchmark LLMs and LMMs in biomedical research, focusing on text summarization and figure interpretation. It evaluates models with expert input, improves performance via prompt engineering, and explores LMMs for hypothesis generation.


<details>
  <summary>Details</summary>
Motivation: To address the insufficient characterization of LLMs and LMMs' reliability and contributions in biomedical applications.

Method: Creation of open-source biomedical datasets, benchmarking models with expert evaluations, prompt engineering, fine-tuning, and test-time computational scaling.

Result: Improved model accuracy in summarization and reasoning, with insights into strengths and limitations of current models.

Conclusion: The study provides actionable insights for advancing LLMs and LMMs in biomedical research, highlighting their potential and challenges.

Abstract: Large Language Models (LLMs) and Large Multi-Modal Models (LMMs) have emerged
as transformative tools in scientific research, yet their reliability and
specific contributions to biomedical applications remain insufficiently
characterized. In this study, we present \textbf{AR}tificial
\textbf{I}ntelligence research assistant for \textbf{E}xpert-involved
\textbf{L}earning (ARIEL), a multimodal dataset designed to benchmark and
enhance two critical capabilities of LLMs and LMMs in biomedical research:
summarizing extensive scientific texts and interpreting complex biomedical
figures. To facilitate rigorous assessment, we create two open-source sets
comprising biomedical articles and figures with designed questions. We
systematically benchmark both open- and closed-source foundation models,
incorporating expert-driven human evaluations conducted by doctoral-level
experts. Furthermore, we improve model performance through targeted prompt
engineering and fine-tuning strategies for summarizing research papers, and
apply test-time computational scaling to enhance the reasoning capabilities of
LMMs, achieving superior accuracy compared to human-expert corrections. We also
explore the potential of using LMM Agents to generate scientific hypotheses
from diverse multimodal inputs. Overall, our results delineate clear strengths
and highlight significant limitations of current foundation models, providing
actionable insights and guiding future advancements in deploying large-scale
language and multi-modal models within biomedical research.

</details>


### [205] [Computational Irreducibility as the Foundation of Agency: A Formal Model Connecting Undecidability to Autonomous Behavior in Complex Systems](https://arxiv.org/pdf/2505.04646)
*Poria Azadi*

Main category: cs.AI

TL;DR: The paper connects computational limits (decidability, completeness, irreducibility) with physical concepts to model a 'minimal agent' in Turing-complete environments. It argues that undecidability and computational irreducibility enable agency by creating unpredictability and novel information.


<details>
  <summary>Details</summary>
Motivation: To understand how autonomy and agency emerge from computational and physical principles, distinguishing autonomous systems from predictable ones.

Method: Uses algorithmic information theory and a formal model of a minimal agent in Turing-complete environments to analyze agent-environment interaction.

Result: Proves that genuine autonomy implies undecidability from an external perspective, linking agency to computational irreducibility and mutual information growth.

Conclusion: Agency arises from complex agent-environment coupling where analytical solutions are absent, with implications for AI, consciousness, and free will.

Abstract: This article explores the emergence of autonomy and agency by connecting
fundamental computational limits (decidability, completeness, computational
irreducibility) with physical concepts. We introduce a formal model of a
"minimal agent" operating within potentially Turing-complete environments.
Using algorithmic information theory, we argue that the inherent undecidability
and computational irreducibility of agent-environment interaction lead to
unpredictability and novel information generation, enabling agency (effective
goal-directed action). Computational irreducibility prevents full external
prediction, creating necessary conditions for autonomous behavior. We relate
this to computational sourcehood, where an agent is the irreducible origin of
its behavior, though formalizing this concept remains challenging. Our central
thesis, formally proven, is that genuine autonomy necessarily implies
undecidability from an external perspective, distinguishing autonomous systems
from predictable ones. We propose that agency arises when agent-environment
coupling complexity allows mutual information between internal states and
relevant environmental variables to increase, particularly where analytical
solutions are absent and operational closure is needed for persistence. This
framework links agency directly to the computational properties of interaction,
offering implications for understanding consciousness, designing autonomous AI,
and reconceptualizing free will in a deterministic yet computationally
irreducible universe.

</details>


### [206] [Dynamic Location Search for Identifying Maximum Weighted Independent Sets in Complex Networks](https://arxiv.org/pdf/2505.04674)
*Enqiang Zhu, Chenkai Hao, Chanjuan Liu, Yongsheng Rao*

Main category: cs.AI

TL;DR: The paper introduces DynLS, an efficient algorithm for the NP-hard MWIS problem, enhancing ITS applications like traffic control. It outperforms existing methods with innovations like SAVP, RLM, and ComLS.


<details>
  <summary>Details</summary>
Motivation: AI techniques in ITSs require excessive resources for large-scale scenarios. The MWIS problem, central to ITS applications, lacks efficient solutions.

Method: DynLS combines SAVP for faster convergence, RLM to escape local optima, and ComLS for guided search.

Result: DynLS outperformed five algorithms in 350 of 360 tests, matching the fastest convergence speed.

Conclusion: DynLS advances MWIS heuristics, offering a practical tool to enhance AI-driven ITS optimization.

Abstract: While Artificial intelligence (AI), including Generative AI, are effective at
generating high-quality traffic data and optimization solutions in intelligent
transportation systems (ITSs), these techniques often demand significant
training time and computational resources, especially in large-scale and
complex scenarios. To address this, we introduce a novel and efficient
algorithm for solving the maximum weighted independent set (MWIS) problem,
which can be used to model many ITSs applications, such as traffic signal
control and vehicle routing. Given the NP-hard nature of the MWIS problem, our
proposed algorithm, DynLS, incorporates three key innovations to solve it
effectively. First, it uses a scores-based adaptive vertex perturbation (SAVP)
technique to accelerate convergence, particularly in sparse graphs. Second, it
includes a region location mechanism (RLM) to help escape local optima by
dynamically adjusting the search space. Finally, it employs a novel variable
neighborhood descent strategy, ComLS, which combines vertex exchange strategies
with a reward mechanism to guide the search toward high-quality solutions. Our
experimental results demonstrate DynLS's superior performance, consistently
delivering high-quality solutions within 1000 seconds. DynLS outperformed five
leading algorithms across 360 test instances, achieving the best solution for
350 instances and surpassing the second-best algorithm, Cyclic-Fast, by 177
instances. Moreover, DynLS matched Cyclic-Fast's convergence speed,
highlighting its efficiency and practicality. This research represents a
significant advancement in heuristic algorithms for the MWIS problem, offering
a promising approach to aid AI techniques in optimizing intelligent
transportation systems.

</details>


### [207] [Foam-Agent: Towards Automated Intelligent CFD Workflows](https://arxiv.org/pdf/2505.04997)
*Ling Yue, Nithin Somasekharan, Yadi Cao, Shaowu Pan*

Main category: cs.AI

TL;DR: Foam-Agent automates OpenFOAM-based CFD workflows using natural language, achieving an 83.6% success rate, outperforming existing frameworks.


<details>
  <summary>Details</summary>
Motivation: CFD simulations require expertise and manual work, creating barriers. Foam-Agent aims to democratize access by automating workflows.

Method: Uses a multi-agent framework with hierarchical retrieval, dependency-aware file generation, and iterative error correction.

Result: Achieves 83.6% success rate, outperforming MetaOpenFOAM (55.5%) and OpenFOAM-GPT (37.3%). Error correction improves performance by 36.4%.

Conclusion: Foam-Agent lowers expertise thresholds while maintaining accuracy, showcasing the potential of multi-agent systems for scientific tools.

Abstract: Computational Fluid Dynamics (CFD) is an essential simulation tool in various
engineering disciplines, but it often requires substantial domain expertise and
manual configuration, creating barriers to entry. We present Foam-Agent, a
multi-agent framework that automates complex OpenFOAM-based CFD simulation
workflows from natural language inputs. Our innovation includes (1) a
hierarchical multi-index retrieval system with specialized indices for
different simulation aspects, (2) a dependency-aware file generation system
that provides consistency management across configuration files, and (3) an
iterative error correction mechanism that diagnoses and resolves simulation
failures without human intervention. Through comprehensive evaluation on the
dataset of 110 simulation tasks, Foam-Agent achieves an 83.6% success rate with
Claude 3.5 Sonnet, significantly outperforming existing frameworks (55.5% for
MetaOpenFOAM and 37.3% for OpenFOAM-GPT). Ablation studies demonstrate the
critical contribution of each system component, with the specialized error
correction mechanism providing a 36.4% performance improvement. Foam-Agent
substantially lowers the CFD expertise threshold while maintaining modeling
accuracy, demonstrating the potential of specialized multi-agent systems to
democratize access to complex scientific simulation tools. The code is public
at https://github.com/csml-rpi/Foam-Agent

</details>


### [208] [The Promise and Limits of LLMs in Constructing Proofs and Hints for Logic Problems in Intelligent Tutoring Systems](https://arxiv.org/pdf/2505.04736)
*Sutapa Dey Tithi, Arun Kumar Ramesh, Clara DiMarco, Xiaoyi Tian, Nazia Alam, Kimia Fazeli, Tiffany Barnes*

Main category: cs.AI

TL;DR: LLMs like DeepSeek-V3 show promise for generating logic tutoring hints but need refinement for accuracy and pedagogy.


<details>
  <summary>Details</summary>
Motivation: Template-based tutoring systems lack personalized feedback; LLMs could offer dynamic feedback but risk inaccuracies.

Method: Evaluated six prompting techniques across four LLMs on 358 logic problems, then generated hints for 1,050 student states.

Result: DeepSeek-V3 achieved 84.4% stepwise accuracy; hints were 75% accurate but lacked contextual explanation.

Conclusion: LLMs can augment tutoring systems but require modifications for pedagogical soundness.

Abstract: Intelligent tutoring systems have demonstrated effectiveness in teaching
formal propositional logic proofs, but their reliance on template-based
explanations limits their ability to provide personalized student feedback.
While large language models (LLMs) offer promising capabilities for dynamic
feedback generation, they risk producing hallucinations or pedagogically
unsound explanations. We evaluated the stepwise accuracy of LLMs in
constructing multi-step symbolic logic proofs, comparing six prompting
techniques across four state-of-the-art LLMs on 358 propositional logic
problems. Results show that DeepSeek-V3 achieved superior performance with
84.4% accuracy on stepwise proof construction and excelled particularly in
simpler rules. We further used the best-performing LLM to generate explanatory
hints for 1,050 unique student problem-solving states from a logic ITS and
evaluated them on 4 criteria with both an LLM grader and human expert ratings
on a 20% sample. Our analysis finds that LLM-generated hints were 75% accurate
and rated highly by human evaluators on consistency and clarity, but did not
perform as well explaining why the hint was provided or its larger context. Our
results demonstrate that LLMs may be used to augment tutoring systems with
logic tutoring hints, but requires additional modifications to ensure accuracy
and pedagogical appropriateness.

</details>


### [209] [A Reputation System for Large Language Model-based Multi-agent Systems to Avoid the Tragedy of the Commons](https://arxiv.org/pdf/2505.05029)
*Siyue Ren, Wanli Fu, Xinkun Zou, Chen Shen, Yi Cai, Chen Chu, Zhen Wang, Shuyue Hu*

Main category: cs.AI

TL;DR: RepuNet, a dual-level reputation framework, mitigates the 'tragedy of the commons' in generative multi-agent systems by modeling agent and system-level reputations, fostering cooperation and emergent behaviors like cooperative clusters.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of collective self-interest leading to disastrous outcomes in generative multi-agent systems, similar to the 'tragedy of the commons' in human society.

Method: Propose RepuNet, a dynamic reputation framework with agent-level reputation dynamics and system-level network evolution, driven by direct interactions and indirect gossip.

Result: RepuNet effectively mitigates the 'tragedy of the commons', promotes cooperation, and leads to emergent behaviors like cooperative clusters and social isolation of exploitative agents.

Conclusion: Reputation systems like RepuNet can sustain cooperation and induce rich emergent behaviors in generative multi-agent systems.

Abstract: The tragedy of the commons, where individual self-interest leads to
collectively disastrous outcomes, is a pervasive challenge in human society.
Recent studies have demonstrated that similar phenomena can arise in generative
multi-agent systems (MASs). To address this challenge, this paper explores the
use of reputation systems as a remedy. We propose RepuNet, a dynamic,
dual-level reputation framework that models both agent-level reputation
dynamics and system-level network evolution. Specifically, driven by direct
interactions and indirect gossip, agents form reputations for both themselves
and their peers, and decide whether to connect or disconnect other agents for
future interactions. Through two distinct scenarios, we show that RepuNet
effectively mitigates the 'tragedy of the commons', promoting and sustaining
cooperation in generative MASs. Moreover, we find that reputation systems can
give rise to rich emergent behaviors in generative MASs, such as the formation
of cooperative clusters, the social isolation of exploitative agents, and the
preference for sharing positive gossip rather than negative ones.

</details>


### [210] [Is there Value in Reinforcement Learning?](https://arxiv.org/pdf/2505.04822)
*Lior Fox, Yonatan Loewenstein*

Main category: cs.AI

TL;DR: The paper critiques the debate between policy-gradient (PG) and value-based (VB) models in RL, arguing that PG methods still rely on value representations for learning. It suggests reevaluating underlying RL assumptions and proposes a more nuanced view of model complexity in cognitive sciences.


<details>
  <summary>Details</summary>
Motivation: To address the ongoing debate about whether action-values are explicitly represented in RL models, and to challenge the notion that PG models eliminate the need for value representations.

Method: The paper critiques existing arguments favoring PG over VB models, analyzes the role of value in PG learning, and re-examines standard RL assumptions.

Result: PG methods are not value-free, as they require value representations for learning. The debate should focus on underlying modeling assumptions rather than just optimization methods.

Conclusion: The focus should shift to evaluating RL assumptions, and model complexity in cognitive sciences should consider computational aspects alongside statistical ones.

Abstract: Action-values play a central role in popular Reinforcement Learing (RL)
models of behavior. Yet, the idea that action-values are explicitly represented
has been extensively debated. Critics had therefore repeatedly suggested that
policy-gradient (PG) models should be favored over value-based (VB) ones, as a
potential solution for this dilemma. Here we argue that this solution is
unsatisfying. This is because PG methods are not, in fact, "Value-free" --
while they do not rely on an explicit representation of Value for acting
(stimulus-response mapping), they do require it for learning. Hence, switching
to PG models is, per se, insufficient for eliminating Value from models of
behavior. More broadly, the requirement for a representation of Value stems
from the underlying assumptions regarding the optimization objective posed by
the standard RL framework, not from the particular algorithm chosen to solve
it. Previous studies mostly took these standard RL assumptions for granted, as
part of their conceptualization or problem modeling, while debating the
different methods used to optimize it (i.e., PG or VB). We propose that,
instead, the focus of the debate should shift to critically evaluating the
underlying modeling assumptions. Such evaluation is particularly important from
an experimental perspective. Indeed, the very notion of Value must be
reconsidered when standard assumptions (e.g., risk neutrality,
full-observability, Markovian environment, exponential discounting) are
relaxed, as is likely in natural settings. Finally, we use the Value debate as
a case study to argue in favor of a more nuanced, algorithmic rather than
statistical, view of what constitutes "a model" in cognitive sciences. Our
analysis suggests that besides "parametric" statistical complexity, additional
aspects such as computational complexity must also be taken into account when
evaluating model complexity.

</details>


### [211] [Multi-agent Embodied AI: Advances and Future Directions](https://arxiv.org/pdf/2505.05108)
*Zhaohan Feng, Ruiqi Xue, Lei Yuan, Yang Yu, Ning Ding, Meiqin Liu, Bingzhao Gao, Jian Sun, Gang Wang*

Main category: cs.AI

TL;DR: The paper reviews multi-agent embodied AI, highlighting its importance in dynamic environments and the lack of comprehensive research, while identifying future directions.


<details>
  <summary>Details</summary>
Motivation: Real-world embodied AI requires multi-agent collaboration in complex environments, but existing research is limited and lacks systematic review.

Method: The paper reviews current research, analyzes key contributions, and identifies challenges in multi-agent embodied AI.

Result: Existing research is narrow and fails to capture the complexity of dynamic, open environments for multi-agent embodied AI.

Conclusion: The paper calls for deeper understanding and innovation in multi-agent embodied AI to address real-world challenges.

Abstract: Embodied artificial intelligence (Embodied AI) plays a pivotal role in the
application of advanced technologies in the intelligent era, where AI systems
are integrated with physical bodies that enable them to perceive, reason, and
interact with their environments. Through the use of sensors for input and
actuators for action, these systems can learn and adapt based on real-world
feedback, allowing them to perform tasks effectively in dynamic and
unpredictable environments. As techniques such as deep learning (DL),
reinforcement learning (RL), and large language models (LLMs) mature, embodied
AI has become a leading field in both academia and industry, with applications
spanning robotics, healthcare, transportation, and manufacturing. However, most
research has focused on single-agent systems that often assume static, closed
environments, whereas real-world embodied AI must navigate far more complex
scenarios. In such settings, agents must not only interact with their
surroundings but also collaborate with other agents, necessitating
sophisticated mechanisms for adaptation, real-time learning, and collaborative
problem-solving. Despite increasing interest in multi-agent systems, existing
research remains narrow in scope, often relying on simplified models that fail
to capture the full complexity of dynamic, open environments for multi-agent
embodied AI. Moreover, no comprehensive survey has systematically reviewed the
advancements in this area. As embodied AI rapidly evolves, it is crucial to
deepen our understanding of multi-agent embodied AI to address the challenges
presented by real-world applications. To fill this gap and foster further
development in the field, this paper reviews the current state of research,
analyzes key contributions, and identifies challenges and future directions,
providing insights to guide innovation and progress in this field.

</details>


### [212] [Large Language Models are Autonomous Cyber Defenders](https://arxiv.org/pdf/2505.04843)
*Sebastián R. Castro, Roberto Campbell, Nancy Lau, Octavio Villalobos, Jiaqi Duan, Alvaro A. Cardenas*

Main category: cs.AI

TL;DR: The paper explores using Large Language Models (LLMs) in multi-agent Autonomous Cyber Defense (ACD) environments, comparing their performance with Reinforcement Learning (RL) agents and proposing a new communication protocol.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of RL-trained ACD agents, such as high training costs and lack of explainability, by leveraging LLMs for more adaptable and interpretable cyber defense.

Method: Proposes integrating LLMs into the CybORG CAGE 4 environment and introduces a novel communication protocol for LLM and RL agent teams.

Result: Evaluates the strengths and weaknesses of LLMs and RL in multi-agent ACD scenarios, identifying key research directions.

Conclusion: LLMs show promise for multi-agent ACD, offering explainability and adaptability, but further research is needed to optimize their deployment alongside RL agents.

Abstract: Fast and effective incident response is essential to prevent adversarial
cyberattacks. Autonomous Cyber Defense (ACD) aims to automate incident response
through Artificial Intelligence (AI) agents that plan and execute actions. Most
ACD approaches focus on single-agent scenarios and leverage Reinforcement
Learning (RL). However, ACD RL-trained agents depend on costly training, and
their reasoning is not always explainable or transferable. Large Language
Models (LLMs) can address these concerns by providing explainable actions in
general security contexts. Researchers have explored LLM agents for ACD but
have not evaluated them on multi-agent scenarios or interacting with other ACD
agents. In this paper, we show the first study on how LLMs perform in
multi-agent ACD environments by proposing a new integration to the CybORG CAGE
4 environment. We examine how ACD teams of LLM and RL agents can interact by
proposing a novel communication protocol. Our results highlight the strengths
and weaknesses of LLMs and RL and help us identify promising research
directions to create, train, and deploy future teams of ACD agents.

</details>


### [213] [CRAFT: Cultural Russian-Oriented Dataset Adaptation for Focused Text-to-Image Generation](https://arxiv.org/pdf/2505.04851)
*Viacheslav Vasilev, Vladimir Arkhipkin, Julia Agafonova, Tatiana Nikulina, Evelina Mironova, Alisa Shichanina, Nikolai Gerasimenko, Mikhail Shoytov, Denis Dimitrov*

Main category: cs.AI

TL;DR: The paper addresses the cultural bias in text-to-image models, proposing a method to improve cultural awareness, specifically for Russian culture, and demonstrates its effectiveness.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image models lack cultural diversity, leading to incorrect or stereotypical outputs. The paper aims to bridge this gap by focusing on cultural codes, particularly Russian culture.

Method: The authors propose a methodology for collecting and processing culturally specific data (Russian cultural code) and evaluate its impact using the Kandinsky 3.1 model.

Result: Human evaluation shows improved cultural awareness in the model for Russian-specific queries.

Conclusion: The approach effectively enhances cultural adaptation in text-to-image models, suggesting broader applications for other cultures.

Abstract: Despite the fact that popular text-to-image generation models cope well with
international and general cultural queries, they have a significant knowledge
gap regarding individual cultures. This is due to the content of existing large
training datasets collected on the Internet, which are predominantly based on
Western European or American popular culture. Meanwhile, the lack of cultural
adaptation of the model can lead to incorrect results, a decrease in the
generation quality, and the spread of stereotypes and offensive content. In an
effort to address this issue, we examine the concept of cultural code and
recognize the critical importance of its understanding by modern image
generation models, an issue that has not been sufficiently addressed in the
research community to date. We propose the methodology for collecting and
processing the data necessary to form a dataset based on the cultural code, in
particular the Russian one. We explore how the collected data affects the
quality of generations in the national domain and analyze the effectiveness of
our approach using the Kandinsky 3.1 text-to-image model. Human evaluation
results demonstrate an increase in the level of awareness of Russian culture in
the model.

</details>


### [214] [Enigme: Generative Text Puzzles for Evaluating Reasoning in Language Models](https://arxiv.org/pdf/2505.04914)
*John Hawkins*

Main category: cs.AI

TL;DR: The paper discusses the limitations of transformer-decoder language models in reasoning tasks and introduces 'enigme,' a library for evaluating their reasoning skills.


<details>
  <summary>Details</summary>
Motivation: To understand the constraints of transformer-decoder models in reasoning tasks and design tasks to probe their limits.

Method: Analyzing the latent variable structure of transformer-decoder models and developing 'enigme,' a library for generating text-based puzzles.

Result: The paper presents 'enigme' as a tool to evaluate and train reasoning skills in AI models.

Conclusion: Transformer-decoder models have reasoning limitations, and 'enigme' helps assess and improve their reasoning capabilities.

Abstract: Transformer-decoder language models are a core innovation in text based
generative artificial intelligence. These models are being deployed as
general-purpose intelligence systems in many applications. Central to their
utility is the capacity to understand natural language commands and exploit the
reasoning embedded in human text corpora to apply some form of reasoning
process to a wide variety of novel tasks. To understand the limitations of this
approach to generating reasoning we argue that we need to consider the
architectural constraints of these systems. Consideration of the latent
variable structure of transformer-decoder models allows us to design reasoning
tasks that should probe the boundary of their capacity to reason. We present
enigme, an open-source library for generating text-based puzzles to be used in
training and evaluating reasoning skills within transformer-decoder models and
future AI architectures.

</details>


### [215] [Belief Filtering for Epistemic Control in Linguistic State Space](https://arxiv.org/pdf/2505.04927)
*Sebastian Dumbrava*

Main category: cs.AI

TL;DR: Belief filtering regulates AI agents' cognitive states using linguistic expressions within the Semantic Manifold framework, enhancing interpretability and safety.


<details>
  <summary>Details</summary>
Motivation: To provide a principled approach for controlling AI agents' internal cognitive states to improve safety and alignment.

Method: Develop belief filters as content-aware operations on dynamic, structured natural language fragments in the Semantic Manifold framework.

Result: Demonstrates how linguistically-grounded cognitive architectures enable interpretable and modular belief filtering.

Conclusion: Belief filtering offers a structured way to govern AI cognition, advancing safety and alignment through semantic interventions.

Abstract: We examine belief filtering as a mechanism for the epistemic control of
artificial agents, focusing on the regulation of internal cognitive states
represented as linguistic expressions. This mechanism is developed within the
Semantic Manifold framework, where belief states are dynamic, structured
ensembles of natural language fragments. Belief filters act as content-aware
operations on these fragments across various cognitive transitions. This paper
illustrates how the inherent interpretability and modularity of such a
linguistically-grounded cognitive architecture directly enable belief
filtering, offering a principled approach to agent regulation. The study
highlights the potential for enhancing AI safety and alignment through
structured interventions in an agent's internal semantic space and points to
new directions for architecturally embedded cognitive governance.

</details>


### [216] [Position: Epistemic Artificial Intelligence is Essential for Machine Learning Models to Know When They Do Not Know](https://arxiv.org/pdf/2505.04950)
*Shireen Kudukkil Manchingal, Fabio Cuzzolin*

Main category: cs.AI

TL;DR: The paper highlights AI's limitations in handling uncertainty and unfamiliar data, proposing a shift to epistemic AI for better robustness.


<details>
  <summary>Details</summary>
Motivation: AI struggles with uncertainty and generalization, especially in autonomous systems, due to overfitting and lack of adaptability.

Method: Proposes a paradigm shift towards epistemic AI, focusing on learning from ignorance and managing uncertainty.

Result: Epistemic AI could enhance resilience and robustness in AI systems for unpredictable environments.

Conclusion: Adopting epistemic AI is a promising solution to improve AI's handling of uncertainty and real-world unpredictability.

Abstract: Despite the impressive achievements of AI, including advancements in
generative models and large language models, there remains a significant gap in
the ability of AI to handle uncertainty and generalize beyond the training
data. We argue that AI models, especially in autonomous systems, fail to make
robust predictions when faced with unfamiliar or adversarial data, as evidenced
by incidents with autonomous vehicles. Traditional machine learning approaches
struggle to address these issues due to an overemphasis on data fitting and
domain adaptation. This position paper posits a paradigm shift towards
epistemic artificial intelligence, emphasizing the need for models to learn not
only from what they know but also from their ignorance. This approach, which
focuses on recognizing and managing uncertainty, offers a potential solution to
improve the resilience and robustness of AI systems, ensuring that they can
better handle unpredictable real-world environments.

</details>


### [217] [Position: The AI Conference Peer Review Crisis Demands Author Feedback and Reviewer Rewards](https://arxiv.org/pdf/2505.04966)
*Jaeho Kim, Yunseok Lee, Seulki Lee*

Main category: cs.AI

TL;DR: The paper proposes transforming the AI conference peer review system into a bi-directional feedback loop to improve accountability and review quality, focusing on reviewer reforms.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like low review quality and lack of accountability due to high submission volumes in AI conferences.

Method: Introduces a two-stage bi-directional review system for author feedback on reviews and a systematic reward system for reviewers.

Result: Aims to create a sustainable, high-quality peer review system by incentivizing and accrediting reviewers.

Conclusion: Calls for community engagement to reform the peer review process for better accountability and quality.

Abstract: The peer review process in major artificial intelligence (AI) conferences
faces unprecedented challenges with the surge of paper submissions (exceeding
10,000 submissions per venue), accompanied by growing concerns over review
quality and reviewer responsibility. This position paper argues for the need to
transform the traditional one-way review system into a bi-directional feedback
loop where authors evaluate review quality and reviewers earn formal
accreditation, creating an accountability framework that promotes a
sustainable, high-quality peer review system. The current review system can be
viewed as an interaction between three parties: the authors, reviewers, and
system (i.e., conference), where we posit that all three parties share
responsibility for the current problems. However, issues with authors can only
be addressed through policy enforcement and detection tools, and ethical
concerns can only be corrected through self-reflection. As such, this paper
focuses on reforming reviewer accountability with systematic rewards through
two key mechanisms: (1) a two-stage bi-directional review system that allows
authors to evaluate reviews while minimizing retaliatory behavior, (2)a
systematic reviewer reward system that incentivizes quality reviewing. We ask
for the community's strong interest in these problems and the reforms that are
needed to enhance the peer review process.

</details>


### [218] [Enhancing Reinforcement Learning for the Floorplanning of Analog ICs with Beam Search](https://arxiv.org/pdf/2505.05059)
*Sandro Junior Della Rovere, Davide Basso, Luca Bortolussi, Mirjana Videnovic-Misic, Husni Habal*

Main category: cs.AI

TL;DR: A hybrid method combining reinforcement learning (RL) and beam search (BS) improves analog IC floorplanning, achieving better area, dead space, and wire length without retraining.


<details>
  <summary>Details</summary>
Motivation: Full automation of analog IC layout is challenging due to complex trade-offs and variability. RL shows promise, but needs enhancement for practical use.

Method: Combines RL with BS to improve inference, handling congestion and flexible objectives without retraining.

Result: 5-85% improvement in area, dead space, and wire length; higher rewards; performance matches state-of-the-art.

Conclusion: The hybrid RL-BS method is effective for analog IC floorplanning, maintaining generalization and efficiency.

Abstract: The layout of analog ICs requires making complex trade-offs, while addressing
device physics and variability of the circuits. This makes full automation with
learning-based solutions hard to achieve. However, reinforcement learning (RL)
has recently reached significant results, particularly in solving the
floorplanning problem. This paper presents a hybrid method that combines RL
with a beam (BS) strategy. The BS algorithm enhances the agent's inference
process, allowing for the generation of flexible floorplans by accomodating
various objective weightings, and addressing congestion without without the
need for policy retraining or fine-tuning. Moreover, the RL agent's
generalization ability stays intact, along with its efficient handling of
circuit features and constraints. Experimental results show approx. 5-85%
improvement in area, dead space and half-perimeter wire length compared to a
standard RL application, along with higher rewards for the agent. Moreover,
performance and efficiency align closely with those of existing
state-of-the-art techniques.

</details>


### [219] [A Neuro-Symbolic Framework for Sequence Classification with Relational and Temporal Knowledge](https://arxiv.org/pdf/2505.05106)
*Luca Salvatore Lorello, Marco Lippi, Stefano Melacci*

Main category: cs.AI

TL;DR: The paper addresses knowledge-driven sequence classification in neuro-symbolic AI, focusing on dynamic knowledge and temporal relations, and evaluates multi-stage architectures.


<details>
  <summary>Details</summary>
Motivation: Existing neuro-symbolic frameworks lack handling of dynamic knowledge and temporal dimensions, prompting the need for a more challenging and realistic approach.

Method: The work introduces a multi-stage neuro-symbolic architecture and compares it with neural-only models using a new benchmarking framework.

Result: Results show the difficulty of the new setting and reveal shortcomings in neuro-symbolic methods, providing insights for future research.

Conclusion: The study highlights the challenges and limitations of neuro-symbolic AI in dynamic knowledge scenarios, serving as a valuable reference for advancing the field.

Abstract: One of the goals of neuro-symbolic artificial intelligence is to exploit
background knowledge to improve the performance of learning tasks. However,
most of the existing frameworks focus on the simplified scenario where
knowledge does not change over time and does not cover the temporal dimension.
In this work we consider the much more challenging problem of knowledge-driven
sequence classification where different portions of knowledge must be employed
at different timesteps, and temporal relations are available. Our experimental
evaluation compares multi-stage neuro-symbolic and neural-only architectures,
and it is conducted on a newly-introduced benchmarking framework. Results
demonstrate the challenging nature of this novel setting, and also highlight
under-explored shortcomings of neuro-symbolic methods, representing a precious
reference for future research.

</details>


### [220] [Is there a half-life for the success rates of AI agents?](https://arxiv.org/pdf/2505.05115)
*Toby Ord*

Main category: cs.AI

TL;DR: AI agents' performance on long tasks follows a simple model: constant failure rate per minute, leading to exponential decline in success. Each agent has a unique half-life.


<details>
  <summary>Details</summary>
Motivation: To explain AI agents' performance on longer-duration tasks using a simple mathematical model.

Method: Analyzed empirical data from Kwa et al. (2025) to model failure rates as a constant per minute, resulting in exponential success decline.

Result: The model fits the data well, suggesting failure on long tasks is due to cumulative subtask failures.

Conclusion: The model explains AI performance on long tasks but needs further validation for broader applicability.

Abstract: Building on the recent empirical work of Kwa et al. (2025), I show that
within their suite of research-engineering tasks the performance of AI agents
on longer-duration tasks can be explained by an extremely simple mathematical
model -- a constant rate of failing during each minute a human would take to do
the task. This implies an exponentially declining success rate with the length
of the task and that each agent could be characterised by its own half-life.
This empirical regularity allows us to estimate the success rate for an agent
at different task lengths. And the fact that this model is a good fit for the
data is suggestive of the underlying causes of failure on longer tasks -- that
they involve increasingly large sets of subtasks where failing any one fails
the task. Whether this model applies more generally on other suites of tasks is
unknown and an important subject for further work.

</details>


### [221] [The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete](https://arxiv.org/pdf/2505.03961)
*Gerrit Großmann, Larisa Ivanova, Sai Leela Poduru, Mohaddeseh Tabrizian, Islam Mesabah, David A. Selby, Sebastian J. Vollmer*

Main category: cs.AI

TL;DR: Shared narratives influence LLM agents' collaboration in a public goods game, with common stories boosting cooperation and differing stories favoring self-interest.


<details>
  <summary>Details</summary>
Motivation: To explore if shared narratives, which drive human cooperation, can similarly nudge LLM agents toward collaborative behavior.

Method: A finitely repeated public goods game with LLM agents primed by teamwork stories, testing negotiation outcomes under varied narrative conditions.

Result: Common stories enhance collaboration, while differing stories lead to self-interest dominance. Narrative priming significantly impacts negotiation strategies.

Conclusion: Shared narratives can shape LLM agent behavior, suggesting implications for multi-agent system design and AI alignment.

Abstract: According to Yuval Noah Harari, large-scale human cooperation is driven by
shared narratives that encode common beliefs and values. This study explores
whether such narratives can similarly nudge LLM agents toward collaboration. We
use a finitely repeated public goods game in which LLM agents choose either
cooperative or egoistic spending strategies. We prime agents with stories
highlighting teamwork to different degrees and test how this influences
negotiation outcomes. Our experiments explore four questions:(1) How do
narratives influence negotiation behavior? (2) What differs when agents share
the same story versus different ones? (3) What happens when the agent numbers
grow? (4) Are agents resilient against self-serving negotiators? We find that
story-based priming significantly affects negotiation strategies and success
rates. Common stories improve collaboration, benefiting each agent. By
contrast, priming agents with different stories reverses this effect, and those
agents primed toward self-interest prevail. We hypothesize that these results
carry implications for multi-agent system design and AI alignment.

</details>


### [222] [MARK: Memory Augmented Refinement of Knowledge](https://arxiv.org/pdf/2505.05177)
*Anish Ganguli, Prabal Deb, Debleena Banerjee*

Main category: cs.AI

TL;DR: The MARK framework enables LLMs to continuously learn domain knowledge without retraining by using specialized agents to refine memory and improve response accuracy.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle to align with evolving domain knowledge without costly fine-tuning, creating a gap between expert understanding and system knowledge.

Method: MARK employs three agents (Residual Refined Memory, User Question Refined Memory, LLM Response Refined Memory) to store, retrieve, and refine domain insights, prioritizing recency and frequency.

Result: MARK reduces hallucinations, adapts to domain-specific needs, and personalizes AI assistants by maintaining context and resolving contradictions.

Conclusion: MARK enhances LLMs by enabling continuous learning and improving accuracy in specialized domains like healthcare and law.

Abstract: Large Language Models (LLMs) assist in specialized tasks but struggle to
align with evolving domain knowledge without costly fine-tuning. Domain
knowledge consists of: Knowledge: Immutable facts (e.g., 'A stone is solid')
and generally accepted principles (e.g., ethical standards); Refined Memory:
Evolving insights shaped by business needs and real-world changes. However, a
significant gap often exists between a domain expert's deep, nuanced
understanding and the system's domain knowledge, which can hinder accurate
information retrieval and application. Our Memory-Augmented Refinement of
Knowledge (MARK) framework enables LLMs to continuously learn without
retraining by leveraging structured refined memory, inspired by the Society of
Mind. MARK operates through specialized agents, each serving a distinct role:
Residual Refined Memory Agent: Stores and retrieves domain-specific insights to
maintain context over time; User Question Refined Memory Agent: Captures
user-provided facts, abbreviations, and terminology for better comprehension;
LLM Response Refined Memory Agent: Extracts key elements from responses for
refinement and personalization. These agents analyse stored refined memory,
detect patterns, resolve contradictions, and improve response accuracy.
Temporal factors like recency and frequency prioritize relevant information
while discarding outdated insights. MARK enhances LLMs in multiple ways: Ground
Truth Strategy: Reduces hallucinations by establishing a structured reference;
Domain-Specific Adaptation: Essential for fields like healthcare, law, and
manufacturing, where proprietary insights are absent from public datasets;
Personalized AI Assistants: Improves virtual assistants by remembering user
preferences, ensuring coherent responses over time.

</details>


### [223] [Societal and technological progress as sewing an ever-growing, ever-changing, patchy, and polychrome quilt](https://arxiv.org/pdf/2505.05197)
*Joel Z. Leibo, Alexander Sasha Vezhnevets, William A. Cunningham, Sébastien Krier, Manfred Diaz, Simon Osindero*

Main category: cs.AI

TL;DR: The paper critiques the one-size-fits-all approach to AI alignment, proposing the 'appropriateness framework' to handle moral diversity through principles like contextual grounding and polycentric governance.


<details>
  <summary>Details</summary>
Motivation: AI decisions impact real-world scenarios, but current alignment methods ignore moral diversity, risking resistance and institutional instability.

Method: The paper introduces the appropriateness framework, based on conflict theory and cultural evolution, with four design principles for handling persistent disagreement.

Result: The framework shifts AI alignment from moral unification to conflict management, addressing ethical diversity more effectively.

Conclusion: Adopting the appropriateness framework is both desirable and urgent for safer, more ethically diverse AI systems.

Abstract: Artificial Intelligence (AI) systems are increasingly placed in positions
where their decisions have real consequences, e.g., moderating online spaces,
conducting research, and advising on policy. Ensuring they operate in a safe
and ethically acceptable fashion is thus critical. However, most solutions have
been a form of one-size-fits-all "alignment". We are worried that such systems,
which overlook enduring moral diversity, will spark resistance, erode trust,
and destabilize our institutions. This paper traces the underlying problem to
an often-unstated Axiom of Rational Convergence: the idea that under ideal
conditions, rational agents will converge in the limit of conversation on a
single ethics. Treating that premise as both optional and doubtful, we propose
what we call the appropriateness framework: an alternative approach grounded in
conflict theory, cultural evolution, multi-agent systems, and institutional
economics. The appropriateness framework treats persistent disagreement as the
normal case and designs for it by applying four principles: (1) contextual
grounding, (2) community customization, (3) continual adaptation, and (4)
polycentric governance. We argue here that adopting these design principles is
a good way to shift the main alignment metaphor from moral unification to a
more productive metaphor of conflict management, and that taking this step is
both desirable and urgent.

</details>


### [224] [ChemRxivQuest: A Curated Chemistry Question-Answer Database Extracted from ChemRxiv Preprints](https://arxiv.org/pdf/2505.05232)
*Mahmoud Amiri, Thomas Bocklitz*

Main category: cs.AI

TL;DR: ChemRxivQuest is a curated dataset of 970 QA pairs from ChemRxiv preprints, designed to advance chemistry NLP. It uses OCR, GPT-4o, and fuzzy matching for creation and supports QA systems, search engines, and LLM fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in accessing domain-specific chemistry knowledge due to rapid literature expansion.

Method: Automated pipeline combining OCR, GPT-4o-based QA generation, and fuzzy matching for answer verification.

Result: A dataset of 970 QA pairs linked to source texts, covering 17 chemistry subfields.

Conclusion: ChemRxivQuest is a foundational resource for chemistry NLP research, education, and tool development.

Abstract: The rapid expansion of chemistry literature poses significant challenges for
researchers seeking to efficiently access domain-specific knowledge. To support
advancements in chemistry-focused natural language processing (NLP), we present
ChemRxivQuest, a curated dataset of 970 high-quality question-answer (QA) pairs
derived from 155 ChemRxiv preprints across 17 subfields of chemistry. Each QA
pair is explicitly linked to its source text segment to ensure traceability and
contextual accuracy. ChemRxivQuest was constructed using an automated pipeline
that combines optical character recognition (OCR), GPT-4o-based QA generation,
and a fuzzy matching technique for answer verification. The dataset emphasizes
conceptual, mechanistic, applied, and experimental questions, enabling
applications in retrieval-based QA systems, search engine development, and
fine-tuning of domain-adapted large language models. We analyze the dataset's
structure, coverage, and limitations, and outline future directions for
expansion and expert validation. ChemRxivQuest provides a foundational resource
for chemistry NLP research, education, and tool development.

</details>


### [225] [Advancing Neural Network Verification through Hierarchical Safety Abstract Interpretation](https://arxiv.org/pdf/2505.05235)
*Luca Marzari, Isabella Mastroeni, Alessandro Farinelli*

Main category: cs.AI

TL;DR: The paper introduces Abstract DNN-Verification, a method for nuanced safety analysis of DNNs using abstract interpretation, offering granular safety levels and efficient computation.


<details>
  <summary>Details</summary>
Motivation: Traditional binary FV methods for DNNs lack nuance, often being overly restrictive or permissive. This paper aims to provide a more detailed safety analysis.

Method: Leverages abstract interpretation and output reachable sets to assess multiple safety levels hierarchically, with comparable or reduced computational effort.

Result: Demonstrates ranking of adversarial inputs by safety violation, providing detailed model evaluation. Validated on deep reinforcement learning and standard benchmarks.

Conclusion: Abstract DNN-Verification offers a more granular and efficient approach to DNN safety analysis, bridging gaps in traditional binary methods.

Abstract: Traditional methods for formal verification (FV) of deep neural networks
(DNNs) are constrained by a binary encoding of safety properties, where a model
is classified as either safe or unsafe (robust or not robust). This binary
encoding fails to capture the nuanced safety levels within a model, often
resulting in either overly restrictive or too permissive requirements. In this
paper, we introduce a novel problem formulation called Abstract
DNN-Verification, which verifies a hierarchical structure of unsafe outputs,
providing a more granular analysis of the safety aspect for a given DNN.
Crucially, by leveraging abstract interpretation and reasoning about output
reachable sets, our approach enables assessing multiple safety levels during
the FV process, requiring the same (in the worst case) or even potentially less
computational effort than the traditional binary verification approach.
Specifically, we demonstrate how this formulation allows rank adversarial
inputs according to their abstract safety level violation, offering a more
detailed evaluation of the model's safety and robustness. Our contributions
include a theoretical exploration of the relationship between our novel
abstract safety formulation and existing approaches that employ abstract
interpretation for robustness verification, complexity analysis of the novel
problem introduced, and an empirical evaluation considering both a complex deep
reinforcement learning task (based on Habitat 3.0) and standard
DNN-Verification benchmarks.

</details>


### [226] [A Pain Assessment Framework based on multimodal data and Deep Machine Learning methods](https://arxiv.org/pdf/2505.05396)
*Stefanos Gkikas*

Main category: cs.AI

TL;DR: The thesis explores automatic pain assessment methods, aiming for high performance in clinical settings, and investigates demographic impacts on pain perception. It proposes unimodal and multimodal pipelines, achieving state-of-the-art results and inspiring AI advancements.


<details>
  <summary>Details</summary>
Motivation: To improve pain assessment in clinical settings by developing computational methods that address gaps in existing approaches and incorporate demographic factors influencing pain perception.

Method: Developed automatic pain assessment pipelines for unimodal and multimodal configurations, leveraging computational techniques and available data.

Result: Achieved state-of-the-art performance in pain assessment and opened new research directions in AI, foundation models, and generative AI.

Conclusion: The proposed methods are effective for clinical pain assessment and have broader implications for advancing AI technologies in healthcare.

Abstract: From the original abstract:
  This thesis initially aims to study the pain assessment process from a
clinical-theoretical perspective while exploring and examining existing
automatic approaches. Building on this foundation, the primary objective of
this Ph.D. project is to develop innovative computational methods for automatic
pain assessment that achieve high performance and are applicable in real
clinical settings. A primary goal is to thoroughly investigate and assess
significant factors, including demographic elements that impact pain
perception, as recognized in pain research, through a computational standpoint.
Within the limits of the available data in this research area, our goal was to
design, develop, propose, and offer automatic pain assessment pipelines for
unimodal and multimodal configurations that are applicable to the specific
requirements of different scenarios. The studies published in this Ph.D. thesis
showcased the effectiveness of the proposed methods, achieving state-of-the-art
results. Additionally, they paved the way for exploring new approaches in
artificial intelligence, foundation models, and generative artificial
intelligence.

</details>


### [227] [EcoAgent: An Efficient Edge-Cloud Collaborative Multi-Agent Framework for Mobile Automation](https://arxiv.org/pdf/2505.05440)
*Biao Yi, Xavier Hu, Yurun Chen, Shengyu Zhang, Hongxia Yang, Fan Wu, Fei Wu*

Main category: cs.AI

TL;DR: EcoAgent is an Edge-Cloud collaborative framework for mobile automation, balancing efficiency and task success by combining cloud-based planning with edge-based execution and observation.


<details>
  <summary>Details</summary>
Motivation: Cloud-based agents (MLLMs) are powerful but costly and slow, while edge-based agents (MSLMs) lack generality. EcoAgent aims to bridge this gap.

Method: EcoAgent uses a cloud-based Planning Agent and two edge-based agents (Execution and Observation). The Observation Agent compresses screen images to text, and the Planning Agent replans if needed.

Result: EcoAgent achieves high task success rates on AndroidWorld while reducing MLLM token usage, making mobile automation practical.

Conclusion: EcoAgent effectively combines edge and cloud strengths for efficient, high-performance mobile automation.

Abstract: Cloud-based mobile agents powered by (multimodal) large language models
((M)LLMs) offer strong reasoning abilities but suffer from high latency and
cost. While fine-tuned (M)SLMs enable edge deployment, they often lose general
capabilities and struggle with complex tasks. To address this, we propose
EcoAgent, an Edge-Cloud cOllaborative multi-agent framework for mobile
automation. EcoAgent features a closed-loop collaboration among a cloud-based
Planning Agent and two edge-based agents: the Execution Agent for action
execution and the Observation Agent for verifying outcomes. The Observation
Agent uses a Pre-Understanding Module to compress screen images into concise
text, reducing token usage. In case of failure, the Planning Agent retrieves
screen history and replans via a Reflection Module. Experiments on AndroidWorld
show that EcoAgent maintains high task success rates while significantly
reducing MLLM token consumption, enabling efficient and practical mobile
automation.

</details>


### [228] [Conversational Process Model Redesign](https://arxiv.org/pdf/2505.05453)
*Nataliia Klievtsova, Timotheus Kampik, Juergen Mangler, Stefanie Rinderle-Ma*

Main category: cs.AI

TL;DR: The paper explores using LLMs for iterative, conversational process model redesign, focusing on explainable and reproducible changes.


<details>
  <summary>Details</summary>
Motivation: Current research lacks continuous interaction between users and LLMs for process model redesign, focusing instead on single-prompt execution.

Method: Proposes a conversational process model redesign (CPD) approach where the LLM identifies change patterns, rephrases requests, and applies changes.

Result: Evaluation shows LLMs handle most changes well, though some patterns are hard for LLMs and users to understand. Users need clearer change descriptions.

Conclusion: LLMs can effectively support iterative process model redesign, but user guidance is crucial for clarity.

Abstract: With the recent success of large language models (LLMs), the idea of
AI-augmented Business Process Management systems is becoming more feasible. One
of their essential characteristics is the ability to be conversationally
actionable, allowing humans to interact with the LLM effectively to perform
crucial process life cycle tasks such as process model design and redesign.
However, most current research focuses on single-prompt execution and
evaluation of results, rather than on continuous interaction between the user
and the LLM. In this work, we aim to explore the feasibility of using LLMs to
empower domain experts in the creation and redesign of process models in an
iterative and effective way. The proposed conversational process model redesign
(CPD) approach receives as input a process model and a redesign request by the
user in natural language. Instead of just letting the LLM make changes, the LLM
is employed to (a) identify process change patterns from literature, (b)
re-phrase the change request to be aligned with an expected wording for the
identified pattern (i.e., the meaning), and then to (c) apply the meaning of
the change to the process model. This multi-step approach allows for
explainable and reproducible changes. In order to ensure the feasibility of the
CPD approach, and to find out how well the patterns from literature can be
handled by the LLM, we performed an extensive evaluation. The results show that
some patterns are hard to understand by LLMs and by users. Within the scope of
the study, we demonstrated that users need support to describe the changes
clearly. Overall the evaluation shows that the LLMs can handle most changes
well according to a set of completeness and correctness criteria.

</details>


### [229] [Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for Entity Alignment](https://arxiv.org/pdf/2307.02075)
*Qijie Ding, Jie Yin, Daokun Zhang, Junbin Gao*

Main category: cs.AI

TL;DR: UPL-EA is a framework to reduce pseudo-labeling errors in entity alignment by combining Optimal Transport-based labeling and parallel ensembling, outperforming 15 baselines.


<details>
  <summary>Details</summary>
Motivation: Address confirmation bias in pseudo-labeling for entity alignment, which hinders performance due to erroneous matches.

Method: UPL-EA uses Optimal Transport for pseudo-labeling to ensure one-to-one correspondences and parallel ensembling to refine alignments.

Result: UPL-EA outperforms 15 baselines, proving effective in reducing pseudo-labeling errors.

Conclusion: UPL-EA is a robust framework for improving entity alignment accuracy by systematically combating confirmation bias.

Abstract: Entity alignment (EA) aims at identifying equivalent entity pairs across
different knowledge graphs (KGs) that refer to the same real-world identity. To
circumvent the shortage of seed alignments provided for training, recent EA
models utilize pseudo-labeling strategies to iteratively add unaligned entity
pairs predicted with high confidence to the seed alignments for model training.
However, the adverse impact of confirmation bias during pseudo-labeling has
been largely overlooked, thus hindering entity alignment performance. To
systematically combat confirmation bias for pseudo-labeling-based entity
alignment, we propose a Unified Pseudo-Labeling framework for Entity Alignment
(UPL-EA) that explicitly eliminates pseudo-labeling errors to boost the
accuracy of entity alignment. UPL-EA consists of two complementary components:
(1) Optimal Transport (OT)-based pseudo-labeling uses discrete OT modeling as
an effective means to determine entity correspondences and reduce erroneous
matches across two KGs. An effective criterion is derived to infer
pseudo-labeled alignments that satisfy one-to-one correspondences; (2) Parallel
pseudo-label ensembling refines pseudo-labeled alignments by combining
predictions over multiple models independently trained in parallel. The
ensembled pseudo-labeled alignments are thereafter used to augment seed
alignments to reinforce subsequent model training for alignment inference. The
effectiveness of UPL-EA in eliminating pseudo-labeling errors is both
theoretically supported and experimentally validated. Our extensive results and
in-depth analyses demonstrate the superiority of UPL-EA over 15 competitive
baselines and its utility as a general pseudo-labeling framework for entity
alignment.

</details>


### [230] [Public Perceptions of Fairness Metrics Across Borders](https://arxiv.org/pdf/2403.16101)
*Yuya Sasaki, Sohei Tokuno, Haruka Maeda, Kazuki Nakajima, Osamu Sakura, George Fletcher, Mykola Pechenizkiy, Panagiotis Karras, Irina Shklovski*

Main category: cs.AI

TL;DR: An international survey evaluates public perceptions of fairness metrics across four countries, revealing national context influences preferences.


<details>
  <summary>Details</summary>
Motivation: To address limitations of prior small-scale surveys and explore discordance between fairness metrics and human perceptions.

Method: Conducted a survey with 4,000 participants (1,000 each from China, France, Japan, and the US) across three scenarios and four fairness metrics.

Result: National context significantly influences preferences for fairness metrics.

Conclusion: Fairness perceptions vary by cultural/national context, highlighting the need for context-aware fairness metrics.

Abstract: Which fairness metrics are appropriately applicable in your contexts? There
may be instances of discordance regarding the perception of fairness, even when
the outcomes comply with established fairness metrics. Several
questionnaire-based surveys have been conducted to evaluate fairness metrics
with human perceptions of fairness. However, these surveys were limited in
scope, including only a few hundred participants within a single country. In
this study, we conduct an international survey to evaluate public perceptions
of various fairness metrics in decision-making scenarios. We collected
responses from 1,000 participants in each of China, France, Japan, and the
United States, amassing a total of 4,000 participants, to analyze the
preferences of fairness metrics. Our survey consists of three distinct
scenarios paired with four fairness metrics. This investigation explores the
relationship between personal attributes and the choice of fairness metrics,
uncovering a significant influence of national context on these preferences.

</details>


### [231] [Imagining and building wise machines: The centrality of AI metacognition](https://arxiv.org/pdf/2411.02478)
*Samuel G. B. Johnson, Amir-Hossein Karimi, Yoshua Bengio, Nick Chater, Tobias Gerstenberg, Kate Larson, Sydney Levine, Melanie Mitchell, Iyad Rahwan, Bernhard Schölkopf, Igor Grossmann*

Main category: cs.AI

TL;DR: The paper explores the gap between AI's intelligence and wisdom, proposing strategies inspired by human wisdom to enhance AI's problem-solving, metacognition, and safety.


<details>
  <summary>Details</summary>
Motivation: AI lacks wisdom despite its intelligence, limiting its ability to handle intractable problems and metacognitive challenges.

Method: Analyzes human wisdom strategies (heuristics, metacognition) and applies them to AI, focusing on robustness, explainability, cooperation, and safety.

Result: Improved metacognition in AI could enhance adaptability, user interaction, and alignment with human goals.

Conclusion: The paper outlines benchmarks, training methods, and implementations for developing wiser AI systems.

Abstract: Although AI has become increasingly smart, its wisdom has not kept pace. In
this article, we examine what is known about human wisdom and sketch a vision
of its AI counterpart. We analyze human wisdom as a set of strategies for
solving intractable problems-those outside the scope of analytic
techniques-including both object-level strategies like heuristics [for managing
problems] and metacognitive strategies like intellectual humility,
perspective-taking, or context-adaptability [for managing object-level
strategies]. We argue that AI systems particularly struggle with metacognition;
improved metacognition would lead to AI more robust to novel environments,
explainable to users, cooperative with others, and safer in risking fewer
misaligned goals with human users. We discuss how wise AI might be benchmarked,
trained, and implemented.

</details>


### [232] [Generating Symbolic World Models via Test-time Scaling of Large Language Models](https://arxiv.org/pdf/2502.04728)
*Zhouliang Yu, Yuhuan Yuan, Tim Z. Xiao, Fuxiang Frank Xia, Jie Fu, Ge Zhang, Ge Lin, Weiyang Liu*

Main category: cs.AI

TL;DR: The paper proposes a method to enhance LLMs' PDDL reasoning for planning tasks by scaling up test-time computation, achieving high success rates without additional training.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with ambiguity in natural language for planning tasks, and generating PDDL domains is challenging due to lack of training data.

Method: Uses Best-of-N sampling and verbalized machine learning to refine PDDL domain generation.

Result: Achieves over 50% success rate in PDDL domain generation, outperforming existing methods.

Conclusion: PDDL abstraction and the proposed method enable superior performance in planning tasks.

Abstract: Solving complex planning problems requires Large Language Models (LLMs) to
explicitly model the state transition to avoid rule violations, comply with
constraints, and ensure optimality-a task hindered by the inherent ambiguity of
natural language. To overcome such ambiguity, Planning Domain Definition
Language (PDDL) is leveraged as a planning abstraction that enables precise and
formal state descriptions. With PDDL, we can generate a symbolic world model
where classic searching algorithms, such as A*, can be seamlessly applied to
find optimal plans. However, directly generating PDDL domains with current LLMs
remains an open challenge due to the lack of PDDL training data. To address
this challenge, we propose to scale up the test-time computation of LLMs to
enhance their PDDL reasoning capabilities, thereby enabling the generation of
high-quality PDDL domains. Specifically, we introduce a simple yet effective
algorithm, which first employs a Best-of-N sampling approach to improve the
quality of the initial solution and then refines the solution in a fine-grained
manner with verbalized machine learning. Our method outperforms o1-mini by a
considerable margin in the generation of PDDL domains, achieving over 50\%
success rate on two tasks (i.e., generating PDDL domains from natural language
description or PDDL problems). This is done without requiring additional
training. By taking advantage of PDDL as state abstraction, our method is able
to outperform current state-of-the-art methods on almost all competition-level
planning tasks.

</details>


### [233] [Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems](https://arxiv.org/pdf/2502.07503)
*Ibrahim Alabdulmohsin, Xiaohua Zhai*

Main category: cs.AI

TL;DR: RINS is a recursive inference scaling method that outperforms other variants, improving language and multimodal tasks while being compute-efficient.


<details>
  <summary>Details</summary>
Motivation: To enhance inference time scaling in language and multimodal systems by leveraging recursive depth, addressing limitations of prior methods.

Method: Introduces RINS, a recursive depth strategy, tested in compute-matched regimes with light-weight adapters and stochastic dropout.

Result: RINS improves language modeling and multimodal performance (e.g., +2% in 0-shot ImageNet accuracy) and enhances scaling laws.

Conclusion: RINS is a no-regret, efficient scaling method with potential for integration into LLM pretraining.

Abstract: Inspired by recent findings on the fractal geometry of language, we introduce
Recursive INference Scaling (RINS) as a complementary, plug-in recipe for
scaling inference time in language and multimodal systems. RINS is a particular
form of recursive depth that significantly outperforms +55 other variants,
including the recent "repeat-all-over" (RAO) strategy in Mobile LLM (Liu et
al., 2024) and latent recurrent thinking (Geiping et al., 2025). Unlike prior
works, we carry out our comparisons on a compute-matched regime, and
demonstrate that for a fixed model size and training compute budget, RINS
substantially improves language modeling performance. It also generalizes
beyond pure language tasks, delivering gains in multimodal systems, including a
+2% improvement in 0-shot ImageNet accuracy for SigLIP-B/16. Additionally, by
deriving data scaling laws, we show that RINS improves both the asymptotic
performance limits and the scaling exponents. More importantly, with
light-weight (linear) adapters (comprising <1% of model parameters) and
stochastic dropout, RINS offers a no-regret strategy, meaning that RINS-enabled
pretraining improves performance in language modeling even when recursive depth
is not applied at inference time. This corresponds to improving performance on
a training compute-, parameter-, and inference-matched regime, suggesting its
potential as a viable component of LLM pretraining!

</details>


### [234] [Do We Truly Need So Many Samples? Multi-LLM Repeated Sampling Efficiently Scales Test-Time Compute](https://arxiv.org/pdf/2504.00762)
*Jianhao Chen, Zishuo Xun, Bocheng Zhou, Han Qi, Hangfan Zhang, Qiaosheng Zhang, Yang Chen, Wei Hu, Yuzhong Qu, Wanli Ouyang, Shuyue Hu*

Main category: cs.AI

TL;DR: A cost-efficient strategy improves LLM performance by dynamically switching between multiple models, leveraging their complementary strengths, and outperforming existing methods while reducing costs.


<details>
  <summary>Details</summary>
Motivation: To enhance LLM performance efficiently by utilizing diverse models and minimizing inference costs.

Method: Uses a repeated-sampling-then-voting framework with dynamic model switching based on consistency signals.

Result: Outperforms self-consistency and multi-agent debate methods, reduces costs, and achieves optimal performance with few models.

Conclusion: Demonstrates the potential of leveraging multiple LLMs in a generation-verification paradigm for efficient and effective performance.

Abstract: This paper presents a simple, effective, and cost-efficient strategy to
improve LLM performance by scaling test-time compute. Our strategy builds upon
the repeated-sampling-then-voting framework, with a novel twist: incorporating
multiple models, even weaker ones, to leverage their complementary strengths
that potentially arise from diverse training data and paradigms. By using
consistency as a signal, our strategy dynamically switches between models.
Theoretical analysis highlights the efficiency and performance advantages of
our strategy. Extensive experiments on six datasets demonstrate that our
strategy not only outperforms self-consistency and state-of-the-art multi-agent
debate approaches, but also significantly reduces inference costs.
Additionally, ModelSwitch requires only a few comparable LLMs to achieve
optimal performance and can be extended with verification methods,
demonstrating the potential of leveraging multiple LLMs in the
generation-verification paradigm.

</details>


### [235] [Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation](https://arxiv.org/pdf/2504.15699)
*Ning Wang, Zihan Yan, Weiyang Li, Chuan Ma, He Chen, Tao Xiang*

Main category: cs.AI

TL;DR: The paper introduces a novel input moderation framework for embodied agents, including a safety benchmark (EAsafetyBench) and a prompt-decoupled moderation scheme (Pinpoint), achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing research lacks specialized safety methodologies for embodied agents, necessitating a tailored framework to ensure their behavioral safety.

Method: The framework includes taxonomy definition, dataset curation, moderator architecture, model training, and evaluation. Pinpoint uses a masked attention mechanism to isolate functional prompts.

Result: The approach achieves 94.58% detection accuracy and 0.002 seconds per instance moderation time, outperforming existing methods.

Conclusion: The proposed framework effectively addresses the safety gap for embodied agents, demonstrating high performance and practicality.

Abstract: Embodied agents exhibit immense potential across a multitude of domains,
making the assurance of their behavioral safety a fundamental prerequisite for
their widespread deployment. However, existing research predominantly
concentrates on the security of general large language models, lacking
specialized methodologies for establishing safety benchmarks and input
moderation tailored to embodied agents. To bridge this gap, this paper
introduces a novel input moderation framework, meticulously designed to
safeguard embodied agents. This framework encompasses the entire pipeline,
including taxonomy definition, dataset curation, moderator architecture, model
training, and rigorous evaluation. Notably, we introduce EAsafetyBench, a
meticulously crafted safety benchmark engineered to facilitate both the
training and stringent assessment of moderators specifically designed for
embodied agents. Furthermore, we propose Pinpoint, an innovative
prompt-decoupled input moderation scheme that harnesses a masked attention
mechanism to effectively isolate and mitigate the influence of functional
prompts on moderation tasks. Extensive experiments conducted on diverse
benchmark datasets and models validate the feasibility and efficacy of the
proposed approach. The results demonstrate that our methodologies achieve an
impressive average detection accuracy of 94.58%, surpassing the performance of
existing state-of-the-art techniques, alongside an exceptional moderation
processing time of merely 0.002 seconds per instance.

</details>


### [236] [MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and Theory of Mind](https://arxiv.org/pdf/2504.18039)
*Zheng Zhang, Nuoqian Xiao, Qi Chai, Deheng Ye, Hao Wang*

Main category: cs.AI

TL;DR: MultiMind integrates multimodal cues and Theory of Mind into LLM agents for social deduction games, outperforming text-only approaches.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents in social deduction games lack multimodal cues (e.g., facial expressions, tone) and fail to model how players perceive each other.

Method: MultiMind combines facial expressions, vocal tones, and verbal content with a Theory of Mind model and Monte Carlo Tree Search for strategic communication.

Result: MultiMind achieves superior performance in agent-versus-agent simulations and human player studies.

Conclusion: This work advances LLM agents toward human-like social reasoning in multimodal domains.

Abstract: Large Language Model (LLM) agents have demonstrated impressive capabilities
in social deduction games (SDGs) like Werewolf, where strategic reasoning and
social deception are essential. However, current approaches remain limited to
textual information, ignoring crucial multimodal cues such as facial
expressions and tone of voice that humans naturally use to communicate.
Moreover, existing SDG agents primarily focus on inferring other players'
identities without modeling how others perceive themselves or fellow players.
To address these limitations, we use One Night Ultimate Werewolf (ONUW) as a
testbed and present MultiMind, the first framework integrating multimodal
information into SDG agents. MultiMind processes facial expressions and vocal
tones alongside verbal content, while employing a Theory of Mind (ToM) model to
represent each player's suspicion levels toward others. By combining this ToM
model with Monte Carlo Tree Search (MCTS), our agent identifies communication
strategies that minimize suspicion directed at itself. Through comprehensive
evaluation in both agent-versus-agent simulations and studies with human
players, we demonstrate MultiMind's superior performance in gameplay. Our work
presents a significant advancement toward LLM agents capable of human-like
social reasoning across multimodal domains.

</details>


### [237] [Approximate Lifted Model Construction](https://arxiv.org/pdf/2504.20784)
*Malte Luttermann, Jan Speller, Marcel Gehrke, Tanya Braun, Ralf Möller, Mattis Hartwig*

Main category: cs.AI

TL;DR: The paper introduces ε-ACP, an extension of the ACP algorithm, to handle deviations in potentials for lifted inference, ensuring practical applicability with bounded approximation error.


<details>
  <summary>Details</summary>
Motivation: ACP requires exact matching of potentials, making it impractical for real-world data. ε-ACP addresses this by allowing deviations, improving usability.

Method: The ε-ACP algorithm permits potential deviations via a hyperparameter ε, efficiently identifying and exploiting near-indistinguishabilities.

Result: Theoretical proof shows ε-ACP's approximation error is strictly bounded, and experiments confirm it remains negligible in practice.

Conclusion: ε-ACP enhances lifted inference by accommodating real-world data imperfections while maintaining accuracy.

Abstract: Probabilistic relational models such as parametric factor graphs enable
efficient (lifted) inference by exploiting the indistinguishability of objects.
In lifted inference, a representative of indistinguishable objects is used for
computations. To obtain a relational (i.e., lifted) representation, the
Advanced Colour Passing (ACP) algorithm is the state of the art. The ACP
algorithm, however, requires underlying distributions, encoded as
potential-based factorisations, to exactly match to identify and exploit
indistinguishabilities. Hence, ACP is unsuitable for practical applications
where potentials learned from data inevitably deviate even if associated
objects are indistinguishable. To mitigate this problem, we introduce the
$\varepsilon$-Advanced Colour Passing ($\varepsilon$-ACP) algorithm, which
allows for a deviation of potentials depending on a hyperparameter
$\varepsilon$. $\varepsilon$-ACP efficiently uncovers and exploits
indistinguishabilities that are not exact. We prove that the approximation
error induced by $\varepsilon$-ACP is strictly bounded and our experiments show
that the approximation error is close to zero in practice.

</details>


### [238] [Theoretical Foundations for Semantic Cognition in Artificial Intelligence](https://arxiv.org/pdf/2504.21218)
*Sebastian Dumbrava*

Main category: cs.AI

TL;DR: A modular cognitive architecture for AI is proposed, modeling belief as structured semantic states, enabling reflective, goal-directed thought.


<details>
  <summary>Details</summary>
Motivation: To create a foundational framework for AI agents that can reason, remember, and regulate beliefs in interpretable ways, drawing from philosophy, cognitive science, and neuroscience.

Method: Develops a layered framework with belief states as dynamic linguistic ensembles, introducing the epistemic vacuum and Null Tower as core constructs.

Result: Theoretical constructs are designed for implementation in symbolic, neural, and hybrid systems, including large language models.

Conclusion: The work provides a foundational substrate for building self-regulating epistemic agents with structured belief systems.

Abstract: This monograph presents a modular cognitive architecture for artificial
intelligence grounded in the formal modeling of belief as structured semantic
state. Belief states are defined as dynamic ensembles of linguistic expressions
embedded within a navigable manifold, where operators enable assimilation,
abstraction, nullification, memory, and introspection. Drawing from philosophy,
cognitive science, and neuroscience, we develop a layered framework that
enables self-regulating epistemic agents capable of reflective, goal-directed
thought. At the core of this framework is the epistemic vacuum: a class of
semantically inert cognitive states that serves as the conceptual origin of
belief space. From this foundation, the Null Tower arises as a generative
structure recursively built through internal representational capacities. The
theoretical constructs are designed to be implementable in both symbolic and
neural systems, including large language models, hybrid agents, and adaptive
memory architectures. This work offers a foundational substrate for
constructing agents that reason, remember, and regulate their beliefs in
structured, interpretable ways.

</details>


### [239] [Agentic Neurodivergence as a Contingent Solution to the AI Alignment Problem](https://arxiv.org/pdf/2505.02581)
*Alberto Hernández-Espinosa, Felipe S. Abrahão, Olaf Witkowski, Hector Zenil*

Main category: cs.AI

TL;DR: The paper explores AI misalignment as a strategy to foster competition among agents, mitigating risks by preventing dominance of any single system. It argues full alignment is mathematically impossible and tests interventions to neutralize AIs.


<details>
  <summary>Details</summary>
Motivation: Addressing the AI alignment problem and existential risks posed by AGI and ASI, the study investigates if embracing misalignment can create a balanced ecosystem of competing agents aligned with human interests.

Method: The paper introduces change-of-opinion attacks and intervention analysis to study how cooperation and competition among agents can neutralize AIs. It also provides a proof of the mathematical impossibility of full AI-human alignment.

Result: Open models show greater diversity, and proprietary models' guardrails effectively steer agents' opinions and sentiments, indicating a neuro-symbolic approach.

Conclusion: Embracing AI misalignment as a counterbalance can mitigate risks by fostering competition and cooperation among agents, though full alignment remains unattainable.

Abstract: The AI alignment problem, which focusses on ensuring that artificial
intelligence (AI), including AGI and ASI, systems act according to human
values, presents profound challenges. With the progression from narrow AI to
Artificial General Intelligence (AGI) and Superintelligence, fears about
control and existential risk have escalated. Here, we investigate whether
embracing inevitable AI misalignment can be a contingent strategy to foster a
dynamic ecosystem of competing agents as a viable path to steer them in more
human-aligned trends and mitigate risks. We explore how misalignment may serve
and should be promoted as a counterbalancing mechanism to team up with
whichever agents are most aligned to human interests, ensuring that no single
system dominates destructively. The main premise of our contribution is that
misalignment is inevitable because full AI-human alignment is a mathematical
impossibility from Turing-complete systems, which we also offer as a proof in
this contribution, a feature then inherited to AGI and ASI systems. We
introduce and test change-of-opinion attacks based on this kind of perturbation
and intervention analysis to study how agents may neutralise friendly or
unfriendly AIs through cooperation and competition. We show that open models
are more diverse and that most likely guardrails implemented in proprietary
models are successful at steering and controlling to some extent the agents'
range of opinion and sentiment change with possible positive and negative
consequences in what we believe are signs of a neuro-symbolic approach even if
shallow.

</details>


### [240] [A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning and Inference-time Scaling Law](https://arxiv.org/pdf/2505.02665)
*Qianjun Pan, Wenkai Ji, Yuyang Ding, Junsong Li, Shilian Chen, Junyi Wang, Jie Zhou, Qin Chen, Min Zhang, Yulan Wu, Liang He*

Main category: cs.AI

TL;DR: The survey examines advancements in reasoning LLMs inspired by human 'slow thinking,' detailing methods like dynamic computation scaling, reinforced learning, and structured frameworks, while highlighting challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: To explore how LLMs can mimic human-like deep reasoning for complex tasks, enhancing their real-world applicability.

Method: Categorizes approaches into (1) dynamic computation scaling, (2) reinforced learning, and (3) slow-thinking frameworks.

Result: Synthesizes over 100 studies to outline key technologies and progress in reasoning LLMs.

Conclusion: Advancing reasoning in LLMs is vital for their broader use in applications like scientific discovery and decision support.

Abstract: This survey explores recent advancements in reasoning large language models
(LLMs) designed to mimic "slow thinking" - a reasoning process inspired by
human cognition, as described in Kahneman's Thinking, Fast and Slow. These
models, like OpenAI's o1, focus on scaling computational resources dynamically
during complex tasks, such as math reasoning, visual reasoning, medical
diagnosis, and multi-agent debates. We present the development of reasoning
LLMs and list their key technologies. By synthesizing over 100 studies, it
charts a path toward LLMs that combine human-like deep thinking with scalable
efficiency for reasoning. The review breaks down methods into three categories:
(1) test-time scaling dynamically adjusts computation based on task complexity
via search and sampling, dynamic verification; (2) reinforced learning refines
decision-making through iterative improvement leveraging policy networks,
reward models, and self-evolution strategies; and (3) slow-thinking frameworks
(e.g., long CoT, hierarchical processes) that structure problem-solving with
manageable steps. The survey highlights the challenges and further directions
of this domain. Understanding and advancing the reasoning abilities of LLMs is
crucial for unlocking their full potential in real-world applications, from
scientific discovery to decision support systems.

</details>


### [241] [An alignment safety case sketch based on debate](https://arxiv.org/pdf/2505.03989)
*Marie Davidsen Buhl, Jacob Pfau, Benjamin Hilton, Geoffrey Irving*

Main category: cs.AI

TL;DR: The paper explores using debate between AI systems to ensure safety and honesty, addressing risks like research sabotage by AI agents.


<details>
  <summary>Details</summary>
Motivation: As AI systems surpass human capabilities, human oversight becomes challenging, necessitating alternative methods like debate to ensure AI alignment and safety.

Method: The paper proposes training AI agents via debate with exploration guarantees to ensure honesty, supported by an alignment safety case with four key claims.

Result: The approach hinges on debate performance implying honesty, stability of honesty during deployment, and error tolerance in deployment contexts.

Conclusion: Further research is needed to validate the debate-based safety case and address open problems for reliable AI alignment.

Abstract: If AI systems match or exceed human capabilities on a wide range of tasks, it
may become difficult for humans to efficiently judge their actions -- making it
hard to use human feedback to steer them towards desirable traits. One proposed
solution is to leverage another superhuman system to point out flaws in the
system's outputs via a debate. This paper outlines the value of debate for AI
safety, as well as the assumptions and further research required to make debate
work. It does so by sketching an ``alignment safety case'' -- an argument that
an AI system will not autonomously take actions which could lead to egregious
harm, despite being able to do so. The sketch focuses on the risk of an AI R\&D
agent inside an AI company sabotaging research, for example by producing false
results. To prevent this, the agent is trained via debate, subject to
exploration guarantees, to teach the system to be honest. Honesty is maintained
throughout deployment via online training. The safety case rests on four key
claims: (1) the agent has become good at the debate game, (2) good performance
in the debate game implies that the system is mostly honest, (3) the system
will not become significantly less honest during deployment, and (4) the
deployment context is tolerant of some errors. We identify open research
problems that, if solved, could render this a compelling argument that an AI
system is safe.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [242] [Data Standards in Audiology: A Mixed-Methods Exploration of Community Perspectives and Implementation Considerations](https://arxiv.org/pdf/2505.04728)
*Charlotte Vercammen, Antje Heinrich, Christophe Lesimple, Alessia Paglialonga, Jan-Willem A. Wasmann, Mareike Buhl*

Main category: cs.SD

TL;DR: The study explored data standardization in audiology, highlighting low awareness but strong community support for future initiatives.


<details>
  <summary>Details</summary>
Motivation: To address the need for data standardization in audiology to enhance research and patient care.

Method: Mixed-methods approach: survey (82 participants) and expert panel discussion (5 experts).

Result: Low awareness (38%) of existing initiatives, but 90% willing to contribute. Challenges (data quality, privacy) and opportunities (synergies with other fields) identified.

Conclusion: Community support can drive standardization efforts, ensuring alignment with other medical fields.

Abstract: Objective: The purpose of this study was to explore options for data
standardisation in audiology and document the global audiology community's
current knowledge and views of data standards, explore their needs and
preferences, and develop recommendations for data standardisation as a result.
  Design: A mixed-methods approach, combining a structured survey with an
in-depth exploration of themes by experts during a special session on "Big Data
and Data Standards in Audiology" at the 2024 Virtual Conference of
Computational Audiology.
  Study Sample: The survey sample consisted of 82 members of the global
audiology community; five experts joined the panel discussion.
  Results: Survey results emphasized the need for data standardisation in
audiology aimed at facilitating research and improving patient care. Knowledge
of existing initiatives was low: 38% were aware of initiatives. Yet, 90%
envisioned contributing to them moving forward. The panel discussion explored
emerging standardisation initiatives in audiology (OMOP, openEHR, HIMSA's Noah
standard), challenges (e.g., data quality and privacy), and opportunities
(e.g., conversion between approaches and synergies with other medical fields).
  Conclusions: The community support identified in this study could be
leveraged to further develop standardisation initiatives for audiology,
ensuring alignment between initiatives and with other medical fields.

</details>


### [243] [A Multi-Agent AI Framework for Immersive Audiobook Production through Spatial Audio and Neural Narration](https://arxiv.org/pdf/2505.04885)
*Shaja Arul Selvamani, Nia D'Souza Ganapathy*

Main category: cs.SD

TL;DR: An AI-driven multi-agent framework for immersive audiobooks uses advanced TTS, spatial audio, and synchronization techniques to enhance realism and immersion.


<details>
  <summary>Details</summary>
Motivation: To create richer audiobook experiences for education, storytelling, and accessibility, leveraging AI for expressive narration and realistic soundscapes.

Method: Combines FastSpeech 2, VALL-E, DTW, RNNs, diffusion models, HOA, and SDN for dynamic narration and 3D audio effects.

Result: Highly realistic audiobooks with synchronized spatial audio, improving immersion and accessibility.

Conclusion: The framework advances audiobook technology, with future work focusing on personalization, ethics, and multi-sensory integration.

Abstract: This research introduces an innovative AI-driven multi-agent framework
specifically designed for creating immersive audiobooks. Leveraging neural
text-to-speech synthesis with FastSpeech 2 and VALL-E for expressive narration
and character-specific voices, the framework employs advanced language models
to automatically interpret textual narratives and generate realistic spatial
audio effects. These sound effects are dynamically synchronized with the
storyline through sophisticated temporal integration methods, including Dynamic
Time Warping (DTW) and recurrent neural networks (RNNs). Diffusion-based
generative models combined with higher-order ambisonics (HOA) and scattering
delay networks (SDN) enable highly realistic 3D soundscapes, substantially
enhancing listener immersion and narrative realism. This technology
significantly advances audiobook applications, providing richer experiences for
educational content, storytelling platforms, and accessibility solutions for
visually impaired audiences. Future work will address personalization, ethical
management of synthesized voices, and integration with multi-sensory platforms.

</details>


### [244] [How to Infer Repeat Structures in MIDI Performances](https://arxiv.org/pdf/2505.05055)
*Silvan Peter, Patricia Hu, Gerhard Widmer*

Main category: cs.SD

TL;DR: A method to automatically infer the repeat structure of a MIDI performance from a symbolic score, easing alignment for performance research.


<details>
  <summary>Details</summary>
Motivation: Manual unfolding of score repeats for alignment in performance corpora is time-consuming; automation is needed.

Method: Uses local alignment of score sections with performance sections, stitching them based on alignment gain to infer the repeat structure.

Result: The method identifies valid structural versions of the score that match the performance, improving alignment efficiency.

Conclusion: Automated repeat structure inference simplifies alignment, aiding large-scale performance corpus curation.

Abstract: MIDI performances are generally expedient in performance research and music
information retrieval, and even more so if they can be connected to a score.
This connection is usually established by means of alignment, linking either
notes or time points between the score and the performance. The first obstacle
when trying to establish such an alignment is that a performance realizes one
(out of many) structural versions of the score that can plausibly result from
instructions such as repeats, variations, and navigation markers like 'dal
segno/da capo al coda'. A score needs to be unfolded, that is, its repeats and
navigation markers need to be explicitly written out to create a single
timeline without jumps matching the performance, before alignment algorithms
can be applied. In the curation of large performance corpora this process is
carried out manually, as no tools are available to infer the repeat structure
of the performance. To ease this process, we develop a method to automatically
infer the repeat structure of a MIDI performance, given a symbolically encoded
score including repeat and navigation markers. The intuition guiding our design
is: 1) local alignment of every contiguous section of the score with a section
of a performance containing the same material should receive high alignment
gain, whereas local alignment with any other performance section should accrue
a low or zero gain. And 2) stitching local alignments together according to a
valid structural version of the score should result in an approximate full
alignment and correspondingly high global accumulated gain if the structural
version corresponds to the performance, and low gain for all other, ill-fitting
structural versions.

</details>


### [245] [ReverbMiipher: Generative Speech Restoration meets Reverberation Characteristics Controllability](https://arxiv.org/pdf/2505.05077)
*Wataru Nakata, Yuma Koizumi, Shigeki Karita, Robin Scheibler, Haruko Ishikawa, Adriana Guevara-Rukoz, Heiga Zen, Michiel Bacchiani*

Main category: cs.SD

TL;DR: ReverbMiipher is an SR model that denoises speech while preserving and controlling reverberation, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional SR removes reverberation, but it encodes spatial information. ReverbMiipher aims to retain and control reverberation while denoising.

Method: Uses a ReverbEncoder to extract reverb features, conditions a vocoder for reconstruction, and employs a zero-vector replacement strategy to disentangle reverb from other attributes.

Result: Effectively preserves reverberation, removes artifacts, and outperforms conventional SR methods. Enables novel reverberation effects via feature manipulation.

Conclusion: ReverbMiipher successfully balances denoising with reverberation preservation and control, offering superior performance and flexibility.

Abstract: Reverberation encodes spatial information regarding the acoustic source
environment, yet traditional Speech Restoration (SR) usually completely removes
reverberation. We propose ReverbMiipher, an SR model extending parametric
resynthesis framework, designed to denoise speech while preserving and enabling
control over reverberation. ReverbMiipher incorporates a dedicated
ReverbEncoder to extract a reverb feature vector from noisy input. This feature
conditions a vocoder to reconstruct the speech signal, removing noise while
retaining the original reverberation characteristics. A stochastic zero-vector
replacement strategy during training ensures the feature specifically encodes
reverberation, disentangling it from other speech attributes. This learned
representation facilitates reverberation control via techniques such as
interpolation between features, replacement with features from other
utterances, or sampling from a latent space. Objective and subjective
evaluations confirm ReverbMiipher effectively preserves reverberation, removes
other artifacts, and outperforms the conventional two-stage SR and convolving
simulated room impulse response approach. We further demonstrate its ability to
generate novel reverberation effects through feature manipulation.

</details>


### [246] [Pairing Real-Time Piano Transcription with Symbol-level Tracking for Precise and Robust Score Following](https://arxiv.org/pdf/2505.05078)
*Silvan Peter, Patricia Hu, Gerhard Widmer*

Main category: cs.SD

TL;DR: A mixed audio-symbolic approach for real-time music tracking outperforms traditional audio-only methods in precision and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing audio-only methods for real-time music tracking have plateaued in performance, prompting exploration of symbolic domain transformation for better results.

Method: Combines real-time audio-to-note transcription with a novel symbol-level tracker to align transcribed input with the score.

Result: Outperforms audio-only methods in precision (absolute tracking error) and robustness (tracking success).

Conclusion: Symbolic domain transformation, even imperfect, enhances real-time music tracking performance.

Abstract: Real-time music tracking systems follow a musical performance and at any time
report the current position in a corresponding score. Most existing methods
approach this problem exclusively in the audio domain, typically using online
time warping (OLTW) techniques on incoming audio and an audio representation of
the score. Audio OLTW techniques have seen incremental improvements both in
features and model heuristics which reached a performance plateau in the past
ten years. We argue that converting and representing the performance in the
symbolic domain -- thereby transforming music tracking into a symbolic task --
can be a more effective approach, even when the domain transformation is
imperfect. Our music tracking system combines two real-time components: one
handling audio-to-note transcription and the other a novel symbol-level tracker
between transcribed input and score. We compare the performance of this mixed
audio-symbolic approach with its equivalent audio-only counterpart, and
demonstrate that our method outperforms the latter in terms of both precision,
i.e., absolute tracking error, and robustness, i.e., tracking success.

</details>


### [247] [FLAM: Frame-Wise Language-Audio Modeling](https://arxiv.org/pdf/2505.05335)
*Yusong Wu, Christos Tsirigotis, Ke Chen, Cheng-Zhi Anna Huang, Aaron Courville, Oriol Nieto, Prem Seetharaman, Justin Salamon*

Main category: cs.SD

TL;DR: FLAM is an open-vocabulary contrastive audio-language model that improves frame-wise sound event localization while maintaining strong retrieval performance.


<details>
  <summary>Details</summary>
Motivation: Existing models lack fine-grained labeling for sound events and struggle with out-of-distribution scenarios. FLAM addresses these gaps.

Method: FLAM uses a memory-efficient frame-wise objective with logit adjustment and leverages a large-scale dataset with LLM-generated captions.

Result: FLAM significantly enhances open-vocabulary localization and performs well in global retrieval and downstream tasks.

Conclusion: FLAM advances frame-wise audio understanding and is effective for real-world sound event detection.

Abstract: Recent multi-modal audio-language models (ALMs) excel at text-audio retrieval
but struggle with frame-wise audio understanding. Prior works use
temporal-aware labels or unsupervised training to improve frame-wise
capabilities, but they still lack fine-grained labeling capability to pinpoint
when an event occurs. While traditional sound event detection models can
precisely localize events, they are limited to pre-defined categories, making
them ineffective for real-world scenarios with out-of-distribution events. In
this work, we introduce FLAM, an open-vocabulary contrastive audio-language
model capable of localizing specific sound events. FLAM employs a
memory-efficient and calibrated frame-wise objective with logit adjustment to
address spurious correlations, such as event dependencies and label imbalances
during training. To enable frame-wise supervision, we leverage a large-scale
dataset with diverse audio events, LLM-generated captions and simulation.
Experimental results and case studies demonstrate that FLAM significantly
improves the open-vocabulary localization capability while maintaining strong
performance in global retrieval and downstream tasks.

</details>


### [248] [Metamathematics of Algorithmic Composition](https://arxiv.org/pdf/2305.15601)
*Michael Gogins*

Main category: cs.SD

TL;DR: A personal exploration of the mathematical foundations of algorithmic music composition, focusing on limits and possibilities rather than specific algorithms.


<details>
  <summary>Details</summary>
Motivation: To understand the deeper mathematical principles underlying algorithmic music composition, drawing parallels with metalogic, metamathematics, and computability theory.

Method: Reflective and analytical approach, discussing general issues and foundational concepts.

Result: Insights into the fundamental limits and possibilities of algorithmic composition.

Conclusion: The essay highlights implications for the future of algorithmic composition based on these foundational insights.

Abstract: This essay recounts my personal journey towards a deeper understanding of the
mathematical foundations of algorithmic music composition. I do not spend much
time on specific mathematical algorithms used by composers; rather, I focus on
general issues such as fundamental limits and possibilities, by analogy with
metalogic, metamathematics, and computability theory. I discuss implications
from these foundations for the future of algorithmic composition.

</details>


### [249] [An Efficient GPU-based Implementation for Noise Robust Sound Source Localization](https://arxiv.org/pdf/2504.03373)
*Zirui Lin, Masayuki Takigahira, Naoya Terakado, Haris Gulzar, Monikka Roslianna Busto, Takeharu Eda, Katsutoshi Itoyama, Kazuhiro Nakadai, Hideharu Amano*

Main category: cs.SD

TL;DR: The paper presents a GPU-based implementation of Sound Source Localization (SSL) for robot audition, using GSVD-MUSIC, achieving significant speedups on embedded and server systems.


<details>
  <summary>Details</summary>
Motivation: Processing multi-channel audio signals in SSL is computationally intensive on CPUs, especially in embedded systems, limiting efficiency.

Method: A GPU-based implementation of SSL using GSVD-MUSIC within the HARK platform is proposed.

Result: Speedups of 5648.7x for GSVD and 10.7x for SSL on Jetson AGX Orin, and 4245.1x for GSVD and 17.3x for SSL on an NVIDIA A100 server.

Conclusion: The GPU-based approach enables real-time processing for large-scale microphone arrays, supporting additional ML/DL tasks.

Abstract: Robot audition, encompassing Sound Source Localization (SSL), Sound Source
Separation (SSS), and Automatic Speech Recognition (ASR), enables robots and
smart devices to acquire auditory capabilities similar to human hearing.
Despite their wide applicability, processing multi-channel audio signals from
microphone arrays in SSL involves computationally intensive matrix operations,
which can hinder efficient deployment on Central Processing Units (CPUs),
particularly in embedded systems with limited CPU resources. This paper
introduces a GPU-based implementation of SSL for robot audition, utilizing the
Generalized Singular Value Decomposition-based Multiple Signal Classification
(GSVD-MUSIC), a noise-robust algorithm, within the HARK platform, an
open-source software suite. For a 60-channel microphone array, the proposed
implementation achieves significant performance improvements. On the Jetson AGX
Orin, an embedded device powered by an NVIDIA GPU and ARM Cortex-A78AE v8.2
64-bit CPUs, we observe speedups of 5648.7x for GSVD calculations and 10.7x for
the SSL module, while speedups of 4245.1x for GSVD calculation and 17.3x for
the entire SSL module on a server configured with an NVIDIA A100 GPU and AMD
EPYC 7352 CPUs, making real-time processing feasible for large-scale microphone
arrays and providing ample capacity for real-time processing of potential
subsequent machine learning or deep learning tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [250] [MatMMFuse: Multi-Modal Fusion model for Material Property Prediction](https://arxiv.org/pdf/2505.04634)
*Abhiroop Bhattacharya, Sylvain G. Cloutier*

Main category: cs.LG

TL;DR: The paper proposes MatMMFuse, a multi-modal fusion model combining graph and text embeddings for improved material property prediction, outperforming single-modality models like CGCNN and SciBERT.


<details>
  <summary>Details</summary>
Motivation: Single-modality models limit feature space exploitation; combining graph (local features) and text (global knowledge) embeddings can enhance performance.

Method: MatMMFuse uses multi-head attention to fuse embeddings from CGCNN (graph) and SciBERT (text), trained end-to-end on the Materials Project Dataset.

Result: The model improves prediction accuracy by 40% over CGCNN and 68% over SciBERT for formation energy, with strong zero-shot performance on specialized datasets.

Conclusion: MatMMFuse outperforms single-modality models, enabling deployment in data-scarce industrial applications.

Abstract: The recent progress of using graph based encoding of crystal structures for
high throughput material property prediction has been quite successful.
However, using a single modality model prevents us from exploiting the
advantages of an enhanced features space by combining different
representations. Specifically, pre-trained Large language models(LLMs) can
encode a large amount of knowledge which is beneficial for training of models.
Moreover, the graph encoder is able to learn the local features while the text
encoder is able to learn global information such as space group and crystal
symmetry. In this work, we propose Material Multi-Modal Fusion(MatMMFuse), a
fusion based model which uses a multi-head attention mechanism for the
combination of structure aware embedding from the Crystal Graph Convolution
Network (CGCNN) and text embeddings from the SciBERT model. We train our model
in an end-to-end framework using data from the Materials Project Dataset. We
show that our proposed model shows an improvement compared to the vanilla CGCNN
and SciBERT model for all four key properties: formation energy, band gap,
energy above hull and fermi energy. Specifically, we observe an improvement of
40% compared to the vanilla CGCNN model and 68% compared to the SciBERT model
for predicting the formation energy per atom. Importantly, we demonstrate the
zero shot performance of the trained model on small curated datasets of
Perovskites, Chalcogenides and the Jarvis Dataset. The results show that the
proposed model exhibits better zero shot performance than the individual plain
vanilla CGCNN and SciBERT model. This enables researchers to deploy the model
for specialized industrial applications where collection of training data is
prohibitively expensive.

</details>


### [251] [Conformal Prediction with Corrupted Labels: Uncertain Imputation and Robust Re-weighting](https://arxiv.org/pdf/2505.04733)
*Shai Feldman, Stephen Bates, Yaniv Romano*

Main category: cs.LG

TL;DR: The paper introduces methods for robust uncertainty quantification in corrupted data settings, analyzing the robustness of privileged conformal prediction (PCP) and proposing uncertain imputation (UI) as an alternative. A triply robust framework ensures validity if any method works.


<details>
  <summary>Details</summary>
Motivation: Label corruption (noisy/missing labels) invalidates standard conformal prediction assumptions, necessitating robust uncertainty quantification methods.

Method: Analyzes PCP's robustness to weight inaccuracies and introduces UI, a conformal method avoiding weight estimation by imputing corrupted labels with uncertainty. Theoretical and empirical validation is provided.

Result: PCP remains valid despite poor weight estimates. UI offers an alternative without weight reliance. The triply robust framework ensures validity if any method is correct.

Conclusion: The proposed methods address label corruption challenges, providing robust uncertainty quantification with theoretical and empirical support.

Abstract: We introduce a framework for robust uncertainty quantification in situations
where labeled training data are corrupted, through noisy or missing labels. We
build on conformal prediction, a statistical tool for generating prediction
sets that cover the test label with a pre-specified probability. The validity
of conformal prediction, however, holds under the i.i.d assumption, which does
not hold in our setting due to the corruptions in the data. To account for this
distribution shift, the privileged conformal prediction (PCP) method proposed
leveraging privileged information (PI) -- additional features available only
during training -- to re-weight the data distribution, yielding valid
prediction sets under the assumption that the weights are accurate. In this
work, we analyze the robustness of PCP to inaccuracies in the weights. Our
analysis indicates that PCP can still yield valid uncertainty estimates even
when the weights are poorly estimated. Furthermore, we introduce uncertain
imputation (UI), a new conformal method that does not rely on weight
estimation. Instead, we impute corrupted labels in a way that preserves their
uncertainty. Our approach is supported by theoretical guarantees and validated
empirically on both synthetic and real benchmarks. Finally, we show that these
techniques can be integrated into a triply robust framework, ensuring
statistically valid predictions as long as at least one underlying method is
valid.

</details>


### [252] [SetONet: A Deep Set-based Operator Network for Solving PDEs with permutation invariant variable input sampling](https://arxiv.org/pdf/2505.04738)
*Stepan Tretiakov, Xingjian Li, Krishna Kumar*

Main category: cs.LG

TL;DR: SetONet extends DeepONet by handling variable sensor configurations and missing data using Deep Sets principles, improving robustness and flexibility in operator learning.


<details>
  <summary>Details</summary>
Motivation: Standard DeepONet's limitation with fixed input sampling restricts its use in scenarios with variable or incomplete data.

Method: SetONet processes input functions as unordered sets of location-value pairs, ensuring permutation invariance and robustness to sensor variations.

Result: SetONet outperforms DeepONet in variable input conditions and achieves comparable or better accuracy on fixed grids, especially for nonlinear problems.

Conclusion: SetONet broadens neural operator applicability by addressing limitations of fixed input sampling, enhancing robustness and flexibility.

Abstract: Neural operators, particularly the Deep Operator Network (DeepONet), have
shown promise in learning mappings between function spaces for solving
differential equations. However, standard DeepONet requires input functions to
be sampled at fixed locations, limiting its applicability in scenarios with
variable sensor configurations, missing data, or irregular grids. We introduce
the Set Operator Network (SetONet), a novel architecture that integrates Deep
Sets principles into the DeepONet framework to address this limitation. The
core innovation lies in the SetONet branch network, which processes the input
function as an unordered \emph{set} of location-value pairs. This design
ensures permutation invariance with respect to the input points, making SetONet
inherently robust to variations in the number and locations of sensors. SetONet
learns richer, spatially-aware input representations by explicitly processing
spatial coordinates and function values. We demonstrate SetONet's effectiveness
on several benchmark problems, including derivative/anti-derivative operators,
1D Darcy flow, and 2D elasticity. Results show that SetONet successfully learns
operators under variable input sampling conditions where standard DeepONet
fails. Furthermore, SetONet is architecturally robust to sensor drop-off;
unlike standard DeepONet, which requires methods like interpolation to function
with missing data. Notably, SetONet can achieve comparable or improved accuracy
over DeepONet on fixed grids, particularly for nonlinear problems, likely due
to its enhanced input representation. SetONet provides a flexible and robust
extension to the neural operator toolkit, significantly broadening the
applicability of operator learning to problems with variable or incomplete
input data.

</details>


### [253] [When Bad Data Leads to Good Models](https://arxiv.org/pdf/2505.04741)
*Kenneth Li, Yida Chen, Fernanda Viégas, Martin Wattenberg*

Main category: cs.LG

TL;DR: Pre-training on toxic data improves post-training control, reducing output toxicity while preserving model capabilities.


<details>
  <summary>Details</summary>
Motivation: Re-examining data 'quality' in LLM pretraining, focusing on how toxic data can enhance post-training control and reduce toxicity.

Method: Toy experiments on data composition's effect on feature geometry, followed by controlled experiments with Olmo-1B models trained on varying clean-toxic data ratios.

Result: Toxic data leads to less entangled linear representations of toxicity, making it easier to remove while maintaining general capabilities.

Conclusion: Bad data (toxic) can yield good models when post-training techniques like ITI are applied, balancing toxicity reduction and capability preservation.

Abstract: In large language model (LLM) pretraining, data quality is believed to
determine model quality. In this paper, we re-examine the notion of "quality"
from the perspective of pre- and post-training co-design. Specifically, we
explore the possibility that pre-training on more toxic data can lead to better
control in post-training, ultimately decreasing a model's output toxicity.
First, we use a toy experiment to study how data composition affects the
geometry of features in the representation space. Next, through controlled
experiments with Olmo-1B models trained on varying ratios of clean and toxic
data, we find that the concept of toxicity enjoys a less entangled linear
representation as the proportion of toxic data increases. Furthermore, we show
that although toxic data increases the generational toxicity of the base model,
it also makes the toxicity easier to remove. Evaluations on Toxigen and Real
Toxicity Prompts demonstrate that models trained on toxic data achieve a better
trade-off between reducing generational toxicity and preserving general
capabilities when detoxifying techniques such as inference-time intervention
(ITI) are applied. Our findings suggest that, with post-training taken into
account, bad data may lead to good models.

</details>


### [254] [Primal-dual algorithm for contextual stochastic combinatorial optimization](https://arxiv.org/pdf/2505.04757)
*Louis Bouvier, Thibault Prunet, Vincent Leclère, Axel Parmentier*

Main category: cs.LG

TL;DR: A novel approach combining neural networks and combinatorial optimization for contextual stochastic optimization, achieving efficient and scalable performance.


<details>
  <summary>Details</summary>
Motivation: Traditional methods lack contextual information, necessitating new algorithms for decision-making under uncertainty.

Method: Uses neural networks with combinatorial optimization layers, a surrogate learning problem, and a primal-dual algorithm with sparse regularization.

Result: Linear convergence under certain conditions, with bounds on policy non-optimality. Experiments show efficiency and scalability.

Conclusion: The approach is effective for contextual stochastic optimization, matching performance of expensive heuristic methods.

Abstract: This paper introduces a novel approach to contextual stochastic optimization,
integrating operations research and machine learning to address decision-making
under uncertainty. Traditional methods often fail to leverage contextual
information, which underscores the necessity for new algorithms. In this study,
we utilize neural networks with combinatorial optimization layers to encode
policies. Our goal is to minimize the empirical risk, which is estimated from
past data on uncertain parameters and contexts. To that end, we present a
surrogate learning problem and a generic primal-dual algorithm that is
applicable to various combinatorial settings in stochastic optimization. Our
approach extends classic Fenchel-Young loss results and introduces a new
regularization method using sparse perturbations on the distribution simplex.
This allows for tractable updates in the original space and can accommodate
diverse objective functions. We demonstrate the linear convergence of our
algorithm under certain conditions and provide a bound on the non-optimality of
the resulting policy in terms of the empirical risk. Experiments on a
contextual stochastic minimum weight spanning tree problem show that our
algorithm is efficient and scalable, achieving performance comparable to
imitation learning of solutions computed using an expensive Lagrangian-based
heuristic.

</details>


### [255] [Prediction via Shapley Value Regression](https://arxiv.org/pdf/2505.04775)
*Amr Alkhatib, Roman Bresson, Henrik Boström, Michalis Vazirgiannis*

Main category: cs.LG

TL;DR: ViaSHAP learns to compute Shapley values directly, avoiding post-hoc computation, and performs comparably to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Traditional Shapley value computation is post-hoc and computationally expensive at inference time.

Method: ViaSHAP learns a function to compute Shapley values directly, using approaches based on the universal approximation theorem and Kolmogorov-Arnold representation theorem.

Result: ViaSHAP matches state-of-the-art performance on tabular data and outperforms FastSHAP in explanation accuracy for tabular data and images.

Conclusion: ViaSHAP offers efficient and accurate Shapley value computation, improving on traditional methods.

Abstract: Shapley values have several desirable, theoretically well-supported,
properties for explaining black-box model predictions. Traditionally, Shapley
values are computed post-hoc, leading to additional computational cost at
inference time. To overcome this, a novel method, called ViaSHAP, is proposed,
that learns a function to compute Shapley values, from which the predictions
can be derived directly by summation. Two approaches to implement the proposed
method are explored; one based on the universal approximation theorem and the
other on the Kolmogorov-Arnold representation theorem. Results from a
large-scale empirical investigation are presented, showing that ViaSHAP using
Kolmogorov-Arnold Networks performs on par with state-of-the-art algorithms for
tabular data. It is also shown that the explanations of ViaSHAP are
significantly more accurate than the popular approximator FastSHAP on both
tabular data and images.

</details>


### [256] [USPR: Learning a Unified Solver for Profiled Routing](https://arxiv.org/pdf/2505.05119)
*Chuanbo Hua, Federico Berto, Zhikai Zhao, Jiwoo Son, Changhyun Kwon, Jinkyoo Park*

Main category: cs.LG

TL;DR: USPR introduces a unified framework for the Profiled Vehicle Routing Problem (PVRP) with innovations like Profile Embeddings, Multi-Head Profiled Attention, and Profile-aware Score Reshaping, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing RL solvers for PVRP, which require retraining for new profiles and struggle with generalization.

Method: USPR uses Profile Embeddings, Multi-Head Profiled Attention, and Profile-aware Score Reshaping to handle arbitrary profile types.

Result: USPR achieves state-of-the-art performance on PVRP benchmarks, offering flexibility and computational efficiency.

Conclusion: USPR provides a robust, flexible solution for PVRP, with publicly available code to support further research.

Abstract: The Profiled Vehicle Routing Problem (PVRP) extends the classical VRP by
incorporating vehicle-client-specific preferences and constraints, reflecting
real-world requirements such as zone restrictions and service-level
preferences. While recent reinforcement learning (RL) solvers have shown
promise, they require retraining for each new profile distribution, suffer from
poor representation ability, and struggle to generalize to out-of-distribution
instances. In this paper, we address these limitations by introducing USPR
(Unified Solver for Profiled Routing), a novel framework that natively handles
arbitrary profile types. USPR introduces three key innovations: (i) Profile
Embeddings (PE) to encode any combination of profile types; (ii) Multi-Head
Profiled Attention (MHPA), an attention mechanism that models rich interactions
between vehicles and clients; (iii) Profile-aware Score Reshaping (PSR), which
dynamically adjusts decoder logits using profile scores to improve
generalization. Empirical results on diverse PVRP benchmarks demonstrate that
USPR achieves state-of-the-art results among learning-based methods while
offering significant gains in flexibility and computational efficiency. We make
our source code publicly available to foster future research at
https://github.com/ai4co/uspr.

</details>


### [257] [Robust ML Auditing using Prior Knowledge](https://arxiv.org/pdf/2505.04796)
*Jade Garcia Bourrée, Augustin Godinot, Martijn De Vos, Milos Vujasinovic, Sayan Biswas, Gilles Tredan, Erwan Le Merrer, Anne-Marie Kermarrec*

Main category: cs.LG

TL;DR: The paper addresses the risk of manipulation in ML system audits for fairness, proposing a novel approach using the auditor's prior knowledge to prevent such manipulation.


<details>
  <summary>Details</summary>
Motivation: The rapid adoption of ML systems has led to regulations, but auditing for fairness is vulnerable to manipulation by platforms.

Method: The paper introduces a manipulation-proof auditing approach leveraging the auditor's prior knowledge, avoiding reliance on public priors.

Result: Experiments show the maximum unfairness a platform can hide before detection, and formal conditions for preventing manipulation are established.

Conclusion: The work formalizes manipulation-proof auditing with a prior, paving the way for more robust fairness audits.

Abstract: The rapid adoption of ML decision-making systems across products and services
has led to a set of regulations on how such systems should behave and be built.
Among all the technical challenges to enforcing these regulations, one crucial,
yet under-explored problem is the risk of manipulation while these systems are
being audited for fairness. This manipulation occurs when a platform
deliberately alters its answers to a regulator to pass an audit without
modifying its answers to other users. In this paper, we introduce a novel
approach to manipulation-proof auditing by taking into account the auditor's
prior knowledge of the task solved by the platform. We first demonstrate that
regulators must not rely on public priors (e.g. a public dataset), as platforms
could easily fool the auditor in such cases. We then formally establish the
conditions under which an auditor can prevent audit manipulations using prior
knowledge about the ground truth. Finally, our experiments with two standard
datasets exemplify the maximum level of unfairness a platform can hide before
being detected as malicious. Our formalization and generalization of
manipulation-proof auditing with a prior opens up new research directions for
more robust fairness audits.

</details>


### [258] [Enhancing Cooperative Multi-Agent Reinforcement Learning with State Modelling and Adversarial Exploration](https://arxiv.org/pdf/2505.05262)
*Andreas Kontogiannis, Konstantinos Papathanasiou, Yi Shen, Giorgos Stamou, Michael M. Zavlanos, George Vouros*

Main category: cs.LG

TL;DR: A novel state modelling framework (SMPE) for cooperative MARL improves agent policies by inferring meaningful state representations and using adversarial exploration, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in MARL for distributed, partially observable environments without communication, focusing on state inference and collaborative task execution.

Method: Proposes SMPE algorithm: agents infer state beliefs, incorporate them into policy networks, and use adversarial exploration to discover high-value states.

Result: SMPE outperforms state-of-the-art MARL algorithms in cooperative tasks across MPE, LBF, and RWARE benchmarks.

Conclusion: SMPE enhances MARL performance in partially observable environments by improving state representation and exploration strategies.

Abstract: Learning to cooperate in distributed partially observable environments with
no communication abilities poses significant challenges for multi-agent deep
reinforcement learning (MARL). This paper addresses key concerns in this
domain, focusing on inferring state representations from individual agent
observations and leveraging these representations to enhance agents'
exploration and collaborative task execution policies. To this end, we propose
a novel state modelling framework for cooperative MARL, where agents infer
meaningful belief representations of the non-observable state, with respect to
optimizing their own policies, while filtering redundant and less informative
joint state information. Building upon this framework, we propose the MARL SMPE
algorithm. In SMPE, agents enhance their own policy's discriminative abilities
under partial observability, explicitly by incorporating their beliefs into the
policy network, and implicitly by adopting an adversarial type of exploration
policies which encourages agents to discover novel, high-value states while
improving the discriminative abilities of others. Experimentally, we show that
SMPE outperforms state-of-the-art MARL algorithms in complex fully cooperative
tasks from the MPE, LBF, and RWARE benchmarks.

</details>


### [259] [ORBIT-2: Scaling Exascale Vision Foundation Models for Weather and Climate Downscaling](https://arxiv.org/pdf/2505.04802)
*Xiao Wang, Jong-Youl Choi, Takuya Kurihaya, Isaac Lyngaas, Hong-Jun Yoon, Ming Fan, Nasik Muhammad Nafi, Aristeidis Tsaris, Ashwin M. Aji, Maliha Hossain, Mohamed Wahib, Dali Wang, Peter Thornton, Prasanna Balaprakash, Moetasim Ashfaq, Dan Lu*

Main category: cs.LG

TL;DR: ORBIT-2 is a scalable foundation model for climate downscaling, introducing innovations like Reslim and TILES to improve efficiency and accuracy, achieving high performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Sparse observations and coarse-resolution climate models hinder regional decision-making, necessitating robust downscaling methods. Existing AI approaches lack generalization and face computational limitations.

Method: ORBIT-2 uses Residual Slim ViT (Reslim) for lightweight, robust prediction and TILES for linear-complexity self-attention, enabling massive parallelism.

Result: ORBIT-2 scales to 10B parameters, achieves 1.8 ExaFLOPS, and supports 0.9 km resolution with R^2 scores of 0.98-0.99 on 7 km benchmarks.

Conclusion: ORBIT-2 addresses key limitations in climate downscaling, offering scalable, high-accuracy solutions for global hyper-resolution applications.

Abstract: Sparse observations and coarse-resolution climate models limit effective
regional decision-making, underscoring the need for robust downscaling.
However, existing AI methods struggle with generalization across variables and
geographies and are constrained by the quadratic complexity of Vision
Transformer (ViT) self-attention. We introduce ORBIT-2, a scalable foundation
model for global, hyper-resolution climate downscaling. ORBIT-2 incorporates
two key innovations: (1) Residual Slim ViT (Reslim), a lightweight architecture
with residual learning and Bayesian regularization for efficient, robust
prediction; and (2) TILES, a tile-wise sequence scaling algorithm that reduces
self-attention complexity from quadratic to linear, enabling long-sequence
processing and massive parallelism. ORBIT-2 scales to 10 billion parameters
across 32,768 GPUs, achieving up to 1.8 ExaFLOPS sustained throughput and
92-98% strong scaling efficiency. It supports downscaling to 0.9 km global
resolution and processes sequences up to 4.2 billion tokens. On 7 km resolution
benchmarks, ORBIT-2 achieves high accuracy with R^2 scores in the range of 0.98
to 0.99 against observation data.

</details>


### [260] [Piecewise Constant Spectral Graph Neural Network](https://arxiv.org/pdf/2505.04808)
*Vahan Martirosyan, Jhony H. Giraldo, Fragkiskos D. Malliaros*

Main category: cs.LG

TL;DR: PieCoN introduces a hybrid of constant and polynomial spectral filters in GNNs to better capture graph spectral properties, showing strong performance on heterophilic datasets.


<details>
  <summary>Details</summary>
Motivation: Existing spectral GNNs with low-degree polynomial filters may not fully capture graph spectral properties, and increasing polynomial degrees is computationally costly or ineffective.

Method: PieCoN combines constant spectral filters with polynomial filters and adaptively partitions the spectrum to enhance spectral property learning.

Result: Experiments on nine benchmark datasets show PieCoN's effectiveness, especially on heterophilic graphs.

Conclusion: PieCoN offers a flexible and effective approach for leveraging graph structures, with potential for broad applications.

Abstract: Graph Neural Networks (GNNs) have achieved significant success across various
domains by leveraging graph structures in data. Existing spectral GNNs, which
use low-degree polynomial filters to capture graph spectral properties, may not
fully identify the graph's spectral characteristics because of the polynomial's
small degree. However, increasing the polynomial degree is computationally
expensive and beyond certain thresholds leads to performance plateaus or
degradation. In this paper, we introduce the Piecewise Constant Spectral Graph
Neural Network(PieCoN) to address these challenges. PieCoN combines constant
spectral filters with polynomial filters to provide a more flexible way to
leverage the graph structure. By adaptively partitioning the spectrum into
intervals, our approach increases the range of spectral properties that can be
effectively learned. Experiments on nine benchmark datasets, including both
homophilic and heterophilic graphs, demonstrate that PieCoN is particularly
effective on heterophilic datasets, highlighting its potential for a wide range
of applications.

</details>


### [261] [Guide your favorite protein sequence generative model](https://arxiv.org/pdf/2505.04823)
*Junhao Xiong, Hunter Nisonoff, Ishan Gaur, Jennifer Listgarten*

Main category: cs.LG

TL;DR: ProteinGuide is a framework for conditioning protein generative models on auxiliary information, enabling guided sequence generation for desired properties.


<details>
  <summary>Details</summary>
Motivation: The lack of a principled framework for incorporating auxiliary information into protein generative models limits their practical utility in protein engineering.

Method: ProteinGuide unifies various protein generative models (masked language, autoregressive, diffusion, flow-matching) to enable statistical conditioning on pre-trained models.

Result: The framework successfully guides models like ProteinMPNN and ESM3 to generate sequences with properties like enhanced stability and specific folds (CATH-labeled).

Conclusion: ProteinGuide provides a versatile and rigorous solution for conditioning protein generative models, expanding their applicability in protein engineering.

Abstract: Generative machine learning models have begun to transform protein
engineering, yet no principled framework for conditioning on auxiliary
information in a plug-and-play manner exists; one may want to iteratively
incorporate experimental feedback, or make use of an existing classifier --
such as for predicting enzyme commission number -- in order to guide the
sampling of the generative model to generate sequences with desired properties.
Herein, we present ProteinGuide, a rigorous and general framework to achieve
just that: through unifying a broad class of protein generative models that
includes masked language, (order-agnostic) autoregressive, diffusion and
flow-matching models, we provide an approach to statistically condition
pre-trained protein generative models. We demonstrate applicability of our
approach by guiding each of two commonly used protein generative models,
ProteinMPNN and ESM3, to generate amino acid and structure token sequences
conditioned on several user-specified properties, namely, enhanced stability
and CATH-labeled fold generation.

</details>


### [262] [When the Universe is Too Big: Bounding Consideration Probabilities for Plackett-Luce Rankings](https://arxiv.org/pdf/2401.11016)
*Ben Aoki-Sherwood, Catherine Bregou, David Liben-Nowell, Kiran Tomlinson, Thomas Zeng*

Main category: cs.LG

TL;DR: The paper addresses the challenge of inferring consideration sets in ranking models, proving bounds on consideration probabilities despite non-identifiability, and provides algorithms to tighten these bounds.


<details>
  <summary>Details</summary>
Motivation: The Plackett-Luce model assumes full consideration of all items, which is unrealistic for large universes. The paper aims to address this by incorporating consideration sets, though inferring them is challenging.

Method: The authors apply the consider-then-choose framework to top-k rankings, assuming Plackett-Luce rankings after sampling consideration sets. They derive bounds on consideration probabilities and provide algorithms to refine these bounds.

Result: They prove bounds on relative and absolute consideration probabilities and demonstrate their methods on a psychology experiment dataset.

Conclusion: The paper shows that useful information about consideration probabilities can be learned despite non-identifiability, with practical applications in ranking tasks.

Abstract: The widely used Plackett-Luce ranking model assumes that individuals rank
items by making repeated choices from a universe of items. But in many cases
the universe is too big for people to plausibly consider all options. In the
choice literature, this issue has been addressed by supposing that individuals
first sample a small consideration set and then choose among the considered
items. However, inferring unobserved consideration sets (or item consideration
probabilities) in this "consider then choose" setting poses significant
challenges, because even simple models of consideration with strong
independence assumptions are not identifiable, even if item utilities are
known. We apply the consider-then-choose framework to top-$k$ rankings, where
we assume rankings are constructed according to a Plackett-Luce model after
sampling a consideration set. While item consideration probabilities remain
non-identified in this setting, we prove that we can infer bounds on the
relative values of consideration probabilities. Additionally, given a condition
on the expected consideration set size and known item utilities, we derive
absolute upper and lower bounds on item consideration probabilities. We also
provide algorithms to tighten those bounds on consideration probabilities by
propagating inferred constraints. Thus, we show that we can learn useful
information about consideration probabilities despite not being able to
identify them precisely. We demonstrate our methods on a ranking dataset from a
psychology experiment with two different ranking tasks (one with fixed
consideration sets and one with unknown consideration sets). This combination
of data allows us to estimate utilities and then learn about unknown
consideration probabilities using our bounds.

</details>


### [263] [Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM Reasoners With Verifiers](https://arxiv.org/pdf/2505.04842)
*Kusha Sareen, Morgane M Moss, Alessandro Sordoni, Rishabh Agarwal, Arian Hosseini*

Main category: cs.LG

TL;DR: RL$^V$ enhances value-free RL methods by training LLMs as reasoners and verifiers, improving accuracy and compute efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods for LLM fine-tuning discard value functions, limiting test-time compute scaling. RL$^V$ addresses this gap.

Method: RL$^V$ jointly trains LLMs as reasoners and verifiers using RL-generated data, adding verification without overhead.

Result: RL$^V$ boosts MATH accuracy by 20%, enables 8-32x compute scaling, and shows strong generalization.

Conclusion: RL$^V$ effectively combines reasoning and verification, improving performance and efficiency in LLM fine-tuning.

Abstract: Prevalent reinforcement learning~(RL) methods for fine-tuning LLM reasoners,
such as GRPO or Leave-one-out PPO, abandon the learned value function in favor
of empirically estimated returns. This hinders test-time compute scaling that
relies on using the value-function for verification. In this work, we propose
RL$^V$ that augments any ``value-free'' RL method by jointly training the LLM
as both a reasoner and a generative verifier using RL-generated data, adding
verification capabilities without significant overhead. Empirically, RL$^V$
boosts MATH accuracy by over 20\% with parallel sampling and enables
$8-32\times$ efficient test-time compute scaling compared to the base RL
method. RL$^V$ also exhibits strong generalization capabilities for both
easy-to-hard and out-of-domain tasks. Furthermore, RL$^V$ achieves
$1.2-1.6\times$ higher performance when jointly scaling parallel and sequential
test-time compute with a long reasoning R1 model.

</details>


### [264] [Federated Learning for Cyber Physical Systems: A Comprehensive Survey](https://arxiv.org/pdf/2505.04873)
*Minh K. Quan, Pubudu N. Pathirana, Mayuri Wijayasundara, Sujeeva Setunge, Dinh C. Nguyen, Christopher G. Brinton, David J. Love, H. Vincent Poor*

Main category: cs.LG

TL;DR: The paper analyzes the integration of federated learning (FL) in cyber-physical systems (CPS), covering applications, topologies, algorithms, and comparisons with IoT, while highlighting challenges and future research directions.


<details>
  <summary>Details</summary>
Motivation: The complexity of integrating ML in CPS due to real-time, safety, and privacy challenges motivates exploring FL as a distributed solution.

Method: The study reviews recent FL and CPS advancements, compares FL applications in CPS and IoT, and examines critical CPS use cases like transportation and healthcare.

Result: FL-CPS integration shows promise in diverse applications, with insights from implementations revealing both potential and limitations.

Conclusion: The paper identifies key concerns and suggests future research directions to advance FL-CPS integration.

Abstract: The integration of machine learning (ML) in cyber physical systems (CPS) is a
complex task due to the challenges that arise in terms of real-time decision
making, safety, reliability, device heterogeneity, and data privacy. There are
also open research questions that must be addressed in order to fully realize
the potential of ML in CPS. Federated learning (FL), a distributed approach to
ML, has become increasingly popular in recent years. It allows models to be
trained using data from decentralized sources. This approach has been gaining
popularity in the CPS field, as it integrates computer, communication, and
physical processes. Therefore, the purpose of this work is to provide a
comprehensive analysis of the most recent developments of FL-CPS, including the
numerous application areas, system topologies, and algorithms developed in
recent years. The paper starts by discussing recent advances in both FL and
CPS, followed by their integration. Then, the paper compares the application of
FL in CPS with its applications in the internet of things (IoT) in further
depth to show their connections and distinctions. Furthermore, the article
scrutinizes how FL is utilized in critical CPS applications, e.g., intelligent
transportation systems, cybersecurity services, smart cities, and smart
healthcare solutions. The study also includes critical insights and lessons
learned from various FL-CPS implementations. The paper's concluding section
delves into significant concerns and suggests avenues for further research in
this fast-paced and dynamic era.

</details>


### [265] [ConCISE: Confidence-guided Compression in Step-by-step Efficient Reasoning](https://arxiv.org/pdf/2505.04881)
*Ziqing Qiao, Yongheng Deng, Jiali Zeng, Dong Wang, Lai Wei, Fandong Meng, Jie Zhou, Ju Ren, Yaoxue Zhang*

Main category: cs.LG

TL;DR: ConCISE reduces redundant outputs in Large Reasoning Models by reinforcing confidence during inference, cutting output length by ~50% without losing accuracy.


<details>
  <summary>Details</summary>
Motivation: LRMs often produce verbose, redundant outputs due to low confidence or delayed termination, degrading efficiency and user experience.

Method: ConCISE uses Confidence Injection to stabilize steps and Early Stopping to halt reasoning when confidence is high.

Result: Fine-tuned LRMs with ConCISE achieve ~50% shorter outputs while maintaining high accuracy, outperforming baselines.

Conclusion: ConCISE effectively compresses reasoning chains by addressing confidence issues, improving efficiency and performance.

Abstract: Large Reasoning Models (LRMs) perform strongly in complex reasoning tasks via
Chain-of-Thought (CoT) prompting, but often suffer from verbose outputs caused
by redundant content, increasing computational overhead, and degrading user
experience. Existing compression methods either operate post-hoc pruning,
risking disruption to reasoning coherence, or rely on sampling-based selection,
which fails to intervene effectively during generation. In this work, we
introduce a confidence-guided perspective to explain the emergence of redundant
reflection in LRMs, identifying two key patterns: Confidence Deficit, where the
model reconsiders correct steps due to low internal confidence, and Termination
Delay, where reasoning continues even after reaching a confident answer. Based
on this analysis, we propose ConCISE (Confidence-guided Compression In
Step-by-step Efficient Reasoning), a framework that simplifies reasoning chains
by reinforcing the model's confidence during inference, thus preventing the
generation of redundant reflection steps. It integrates Confidence Injection to
stabilize intermediate steps and Early Stopping to terminate reasoning when
confidence is sufficient. Extensive experiments demonstrate that fine-tuning
LRMs on ConCISE-generated data yields significantly shorter outputs, reducing
length by up to approximately 50% under SimPO, while maintaining high task
accuracy. ConCISE consistently outperforms existing baselines across multiple
reasoning benchmarks.

</details>


### [266] [FedRE: Robust and Effective Federated Learning with Privacy Preference](https://arxiv.org/pdf/2505.04889)
*Tianzhe Xiao, Yichen Li, Yu Zhou, Yining Qi, Yi Liu, Wei Wang, Haozhao Wang, Yi Wang, Ruixuan Li*

Main category: cs.LG

TL;DR: FedRE enhances Federated Learning by optimizing local differential privacy (LDP) for privacy-sensitive information (PSI), balancing privacy and performance.


<details>
  <summary>Details</summary>
Motivation: Existing FL methods apply uniform LDP, ignoring varying client privacy preferences and PSI distribution, leading to unnecessary noise and degraded performance.

Method: FedRE defines PSI per client, allocates LDP budget layer-wise for stricter PSI protection, and uses a parameter aggregation mechanism to mitigate noise impact.

Result: FedRE achieves competitive performance on text tamper detection tasks (T-SROIE and DocTamper datasets).

Conclusion: FedRE effectively balances privacy and model performance by focusing LDP on PSI, outperforming uniform LDP methods.

Abstract: Despite Federated Learning (FL) employing gradient aggregation at the server
for distributed training to prevent the privacy leakage of raw data, private
information can still be divulged through the analysis of uploaded gradients
from clients. Substantial efforts have been made to integrate local
differential privacy (LDP) into the system to achieve a strict privacy
guarantee. However, existing methods fail to take practical issues into account
by merely perturbing each sample with the same mechanism while each client may
have their own privacy preferences on privacy-sensitive information (PSI),
which is not uniformly distributed across the raw data. In such a case,
excessive privacy protection from private-insensitive information can
additionally introduce unnecessary noise, which may degrade the model
performance. In this work, we study the PSI within data and develop FedRE, that
can simultaneously achieve robustness and effectiveness benefits with LDP
protection. More specifically, we first define PSI with regard to the privacy
preferences of each client. Then, we optimize the LDP by allocating less
privacy budget to gradients with higher PSI in a layer-wise manner, thus
providing a stricter privacy guarantee for PSI. Furthermore, to mitigate the
performance degradation caused by LDP, we design a parameter aggregation
mechanism based on the distribution of the perturbed information. We conducted
experiments with text tamper detection on T-SROIE and DocTamper datasets, and
FedRE achieves competitive performance compared to state-of-the-art methods.

</details>


### [267] [Clustering with Communication: A Variational Framework for Single Cell Representation Learning](https://arxiv.org/pdf/2505.04891)
*Cong Qi, Yeqing Chen, Jie Zhang, Wei Zhi*

Main category: cs.LG

TL;DR: CCCVAE is a variational autoencoder that integrates cell-cell communication signals into single-cell RNA-seq data analysis, improving clustering performance over standard VAEs.


<details>
  <summary>Details</summary>
Motivation: Understanding cell-cell communication (CCC) is crucial for deciphering biological functions, but existing methods often overlook CCC signals in single-cell data.

Method: CCCVAE uses a communication-aware kernel and sparse Gaussian process to incorporate CCC signals into the latent space, combining transcriptional similarity and signaling context.

Result: CCCVAE outperforms standard VAEs in clustering performance across four scRNA-seq datasets.

Conclusion: Embedding biological priors like CCC into deep generative models enhances unsupervised single-cell analysis.

Abstract: Single-cell RNA sequencing (scRNA-seq) has revealed complex cellular
heterogeneity, but recent studies emphasize that understanding biological
function also requires modeling cell-cell communication (CCC), the signaling
interactions mediated by ligand-receptor pairs that coordinate cellular
behavior. Tools like CellChat have demonstrated that CCC plays a critical role
in processes such as cell differentiation, tissue regeneration, and immune
response, and that transcriptomic data inherently encodes rich information
about intercellular signaling. We propose CCCVAE, a novel variational
autoencoder framework that incorporates CCC signals into single-cell
representation learning. By leveraging a communication-aware kernel derived
from ligand-receptor interactions and a sparse Gaussian process, CCCVAE encodes
biologically informed priors into the latent space. Unlike conventional VAEs
that treat each cell independently, CCCVAE encourages latent embeddings to
reflect both transcriptional similarity and intercellular signaling context.
Empirical results across four scRNA-seq datasets show that CCCVAE improves
clustering performance, achieving higher evaluation scores than standard VAE
baselines. This work demonstrates the value of embedding biological priors into
deep generative models for unsupervised single-cell analysis.

</details>


### [268] [GCN-Based Throughput-Oriented Handover Management in Dense 5G Vehicular Networks](https://arxiv.org/pdf/2505.04894)
*Nazanin Mehregan, Robson E. De Grande*

Main category: cs.LG

TL;DR: TH-GCN, a GNN-based method, optimizes handover management in 5G vehicular networks, reducing handovers by 78% and improving signal quality by 10%.


<details>
  <summary>Details</summary>
Motivation: Addressing network instability in 5G vehicular networks caused by limited coverage and frequent handovers, especially in high-mobility environments.

Method: TH-GCN models vehicles and base stations as nodes in a dynamic graph, integrating signal quality, throughput, speed, and load for adaptive handover decisions.

Result: Simulations show TH-GCN reduces handovers by 78% and improves signal quality by 10%, outperforming existing methods.

Conclusion: TH-GCN effectively enhances network stability in dense 5G vehicular networks through adaptive handover management.

Abstract: The rapid advancement of 5G has transformed vehicular networks, offering high
bandwidth, low latency, and fast data rates essential for real-time
applications in smart cities and vehicles. These improvements enhance traffic
safety and entertainment services. However, the limited coverage and frequent
handovers in 5G networks cause network instability, especially in high-mobility
environments due to the ping-pong effect. This paper presents TH-GCN
(Throughput-oriented Graph Convolutional Network), a novel approach for
optimizing handover management in dense 5G networks. Using graph neural
networks (GNNs), TH-GCN models vehicles and base stations as nodes in a dynamic
graph enriched with features such as signal quality, throughput, vehicle speed,
and base station load. By integrating both user equipment and base station
perspectives, this dual-centric approach enables adaptive, real-time handover
decisions that improve network stability. Simulation results show that TH-GCN
reduces handovers by up to 78 percent and improves signal quality by 10
percent, outperforming existing methods.

</details>


### [269] [Precise gradient descent training dynamics for finite-width multi-layer neural networks](https://arxiv.org/pdf/2505.04898)
*Qiyang Han, Masaaki Imaizumi*

Main category: cs.LG

TL;DR: The paper provides a precise distributional characterization of gradient descent iterates for multi-layer neural networks in the finite-width proportional regime, capturing Gaussian fluctuations and concentration in weights, and differs from existing theories like NTK, MF, and TP.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in understanding gradient descent behavior for finite-width neural networks, especially in non-asymptotic settings and beyond lazy training regimes.

Method: Non-asymptotic state evolution theory is used to analyze gradient descent iterates, focusing on Gaussian fluctuations in first-layer weights and concentration in deeper layers.

Result: The theory captures weight evolution from individual initializations, characterizes training and generalization errors, and shows gradient descent retains single-index function structure despite misspecification.

Conclusion: The work offers a novel theoretical framework for finite-width networks, with practical applications like early stopping guidance and insights into model behavior under gradient descent.

Abstract: In this paper, we provide the first precise distributional characterization
of gradient descent iterates for general multi-layer neural networks under the
canonical single-index regression model, in the `finite-width proportional
regime' where the sample size and feature dimension grow proportionally while
the network width and depth remain bounded. Our non-asymptotic state evolution
theory captures Gaussian fluctuations in first-layer weights and concentration
in deeper-layer weights, and remains valid for non-Gaussian features.
  Our theory differs from existing neural tangent kernel (NTK), mean-field (MF)
theories and tensor program (TP) in several key aspects. First, our theory
operates in the finite-width regime whereas these existing theories are
fundamentally infinite-width. Second, our theory allows weights to evolve from
individual initializations beyond the lazy training regime, whereas NTK and MF
are either frozen at or only weakly sensitive to initialization, and TP relies
on special initialization schemes. Third, our theory characterizes both
training and generalization errors for general multi-layer neural networks
beyond the uniform convergence regime, whereas existing theories study
generalization almost exclusively in two-layer settings.
  As a statistical application, we show that vanilla gradient descent can be
augmented to yield consistent estimates of the generalization error at each
iteration, which can be used to guide early stopping and hyperparameter tuning.
As a further theoretical implication, we show that despite model
misspecification, the model learned by gradient descent retains the structure
of a single-index function with an effective signal determined by a linear
combination of the true signal and the initialization.

</details>


### [270] [VaCDA: Variational Contrastive Alignment-based Scalable Human Activity Recognition](https://arxiv.org/pdf/2505.04907)
*Soham Khisa, Avijoy Chakma*

Main category: cs.LG

TL;DR: The paper proposes VaCDA, a framework combining VAEs and contrastive learning to address data heterogeneity in wearable sensor data for activity recognition.


<details>
  <summary>Details</summary>
Motivation: Wearable sensor data is unlabeled, heterogeneous, and hard to interpret, making traditional transfer learning methods ineffective.

Method: Uses a VAE for shared latent space learning and integrates contrastive learning to align same-class instances across domains.

Result: VaCDA outperforms baselines in cross-position and cross-device scenarios.

Conclusion: VaCDA effectively mitigates data heterogeneity and improves activity recognition in wearable sensor data.

Abstract: Technological advancements have led to the rise of wearable devices with
sensors that continuously monitor user activities, generating vast amounts of
unlabeled data. This data is challenging to interpret, and manual annotation is
labor-intensive and error-prone. Additionally, data distribution is often
heterogeneous due to device placement, type, and user behavior variations. As a
result, traditional transfer learning methods perform suboptimally, making it
difficult to recognize daily activities. To address these challenges, we use a
variational autoencoder (VAE) to learn a shared, low-dimensional latent space
from available sensor data. This space generalizes data across diverse sensors,
mitigating heterogeneity and aiding robust adaptation to the target domain. We
integrate contrastive learning to enhance feature representation by aligning
instances of the same class across domains while separating different classes.
We propose Variational Contrastive Domain Adaptation (VaCDA), a multi-source
domain adaptation framework combining VAEs and contrastive learning to improve
feature representation and reduce heterogeneity between source and target
domains. We evaluate VaCDA on multiple publicly available datasets across three
heterogeneity scenarios: cross-person, cross-position, and cross-device. VaCDA
outperforms the baselines in cross-position and cross-device scenarios.

</details>


### [271] [Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction](https://arxiv.org/pdf/2505.04918)
*Jiaqi Zheng, Qing Ling, Yerong Feng*

Main category: cs.LG

TL;DR: PASSAT is a deep learning model for weather prediction that integrates physics and Earth's topology, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models for weather prediction often ignore physics or Earth's topology, limiting their accuracy.

Method: PASSAT combines advection and Navier-Stokes equations on a spherical manifold with a spherical graph neural network to model Earth-atmosphere interactions.

Result: PASSAT surpasses state-of-the-art deep learning and operational models in the ERA5 dataset.

Conclusion: Integrating physics and topology improves weather prediction accuracy, as demonstrated by PASSAT.

Abstract: Although deep learning models have demonstrated remarkable potential in
weather prediction, most of them overlook either the \textbf{physics} of the
underlying weather evolution or the \textbf{topology} of the Earth's surface.
In light of these disadvantages, we develop PASSAT, a novel Physics-ASSisted
And Topology-informed deep learning model for weather prediction. PASSAT
attributes the weather evolution to two key factors: (i) the advection process
that can be characterized by the advection equation and the Navier-Stokes
equation; (ii) the Earth-atmosphere interaction that is difficult to both model
and calculate. PASSAT also takes the topology of the Earth's surface into
consideration, other than simply treating it as a plane. With these
considerations, PASSAT numerically solves the advection equation and the
Navier-Stokes equation on the spherical manifold, utilizes a spherical graph
neural network to capture the Earth-atmosphere interaction, and generates the
initial velocity fields that are critical to solving the advection equation
from the same spherical graph neural network. In the $5.625^\circ$-resolution
ERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-based
weather prediction models and the operational numerical weather prediction
model IFS T42. Code and checkpoint are available at
https://github.com/Yumenomae/PASSAT_5p625.

</details>


### [272] [Fair Uncertainty Quantification for Depression Prediction](https://arxiv.org/pdf/2505.04931)
*Yonghong Li, Xiuzhuang Zhou*

Main category: cs.LG

TL;DR: The paper proposes Fair Uncertainty Quantification (FUQ) for depression prediction, ensuring both reliability and fairness across demographic groups by leveraging conformal prediction and fairness-aware optimization.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of focus on fairness in uncertainty quantification (UQ) for depression prediction, the study aims to achieve reliable and fair predictions across diverse groups.

Method: The approach groups participants by sensitive attributes, uses conformal prediction for UQ within each group, and employs fairness-aware optimization under Equal Opportunity Coverage (EOC) constraints.

Result: Extensive evaluations on visual and audio depression datasets confirm the effectiveness of FUQ in achieving reliable and fair predictions.

Conclusion: FUQ successfully balances predictive reliability and algorithmic fairness, making it suitable for clinical applications in depression prediction.

Abstract: Trustworthy depression prediction based on deep learning, incorporating both
predictive reliability and algorithmic fairness across diverse demographic
groups, is crucial for clinical application. Recently, achieving reliable
depression predictions through uncertainty quantification has attracted
increasing attention. However, few studies have focused on the fairness of
uncertainty quantification (UQ) in depression prediction. In this work, we
investigate the algorithmic fairness of UQ, namely Equal Opportunity Coverage
(EOC) fairness, and propose Fair Uncertainty Quantification (FUQ) for
depression prediction. FUQ pursues reliable and fair depression predictions
through group-based analysis. Specifically, we first group all the participants
by different sensitive attributes and leverage conformal prediction to quantify
uncertainty within each demographic group, which provides a theoretically
guaranteed and valid way to quantify uncertainty for depression prediction and
facilitates the investigation of fairness across different demographic groups.
Furthermore, we propose a fairness-aware optimization strategy that formulates
fairness as a constrained optimization problem under EOC constraints. This
enables the model to preserve predictive reliability while adapting to the
heterogeneous uncertainty levels across demographic groups, thereby achieving
optimal fairness. Through extensive evaluations on several visual and audio
depression datasets, our approach demonstrates its effectiveness.

</details>


### [273] [Structural Alignment in Link Prediction](https://arxiv.org/pdf/2505.04939)
*Jeffrey Seathrún Sardina*

Main category: cs.LG

TL;DR: The paper proposes a graph-structure-first approach for link prediction in Knowledge Graphs (KGs), challenging the dominant embedding-based paradigm. It validates this approach through experiments and introduces the Structural Alignment Hypothesis.


<details>
  <summary>Details</summary>
Motivation: Current KG link prediction relies heavily on embedding-based methods, which focus on individual nodes and edges. The paper argues for a holistic, structure-first perspective to better model KG information and improve link prediction.

Method: The thesis re-analyzes KGs and link predictors from a graph-structure-first perspective, modeling KG information as whole triples. It includes a literature review and two sets of experiments to validate this approach.

Result: The structure-first perspective is found viable and useful for KG learning and cross-KG transfer learning. The Structural Alignment Hypothesis is proposed, framing link prediction as a structural task.

Conclusion: The work advocates for a shift in KG modeling and link prediction, emphasizing structural alignment. All research materials, including code and a bilingual translation dictionary, are open-sourced.

Abstract: While Knowledge Graphs (KGs) have become increasingly popular across various
scientific disciplines for their ability to model and interlink huge quantities
of data, essentially all real-world KGs are known to be incomplete. As such,
with the growth of KG use has been a concurrent development of machine learning
tools designed to predict missing information in KGs, which is referred to as
the Link Prediction Task. The majority of state-of-the-art link predictors to
date have followed an embedding-based paradigm. In this paradigm, it is assumed
that the information content of a KG is best represented by the (individual)
vector representations of its nodes and edges, and that therefore node and edge
embeddings are particularly well-suited to performing link prediction.
  This thesis proposes an alternative perspective on the field's approach to
link prediction and KG data modelling. Specifically, this work re-analyses KGs
and state-of-the-art link predictors from a graph-structure-first perspective
that models the information content of a KG in terms of whole triples, rather
than individual nodes and edges.
  Following a literature review and two core sets of experiments, this thesis
concludes that a structure-first perspective on KGs and link prediction is both
viable and useful for understanding KG learning and for enabling cross-KG
transfer learning for the link prediction task. This observation is used to
create and propose the Structural Alignment Hypothesis, which postulates that
link prediction can be understood and modelled as a structural task.
  All code and data used for this thesis are open-sourced. This thesis was
written bilingually, with the main document in English and an informal extended
summary in Irish. An Irish-language translation dictionary of machine learning
terms (the Focl\'oir Tr\'achtais) created for this work is open-sourced as
well.

</details>


### [274] [Graffe: Graph Representation Learning via Diffusion Probabilistic Models](https://arxiv.org/pdf/2505.04956)
*Dingshuo Chen, Shuchen Xue, Liuji Chen, Yingheng Wang, Qiang Liu, Shu Wu, Zhi-Ming Ma, Liang Wang*

Main category: cs.LG

TL;DR: Graffe is a self-supervised diffusion model for graph representation learning, combining a graph encoder and diffusion decoder to achieve state-of-the-art performance on 9 of 11 datasets.


<details>
  <summary>Details</summary>
Motivation: Diffusion models (DPMs) are underutilized in representation learning, especially for graphs. Graffe aims to bridge this gap by adapting DPMs to graph data.

Method: Graffe uses a graph encoder to create compact representations, guiding the denoising process of a diffusion decoder. Theoretical analysis links denoising to maximizing conditional mutual information.

Result: Graffe achieves state-of-the-art performance on 9 of 11 datasets for node and graph classification, validating its effectiveness.

Conclusion: Diffusion models, like Graffe, are powerful tools for graph representation learning, as demonstrated by theoretical and empirical results.

Abstract: Diffusion probabilistic models (DPMs), widely recognized for their potential
to generate high-quality samples, tend to go unnoticed in representation
learning. While recent progress has highlighted their potential for capturing
visual semantics, adapting DPMs to graph representation learning remains in its
infancy. In this paper, we introduce Graffe, a self-supervised diffusion model
proposed for graph representation learning. It features a graph encoder that
distills a source graph into a compact representation, which, in turn, serves
as the condition to guide the denoising process of the diffusion decoder. To
evaluate the effectiveness of our model, we first explore the theoretical
foundations of applying diffusion models to representation learning, proving
that the denoising objective implicitly maximizes the conditional mutual
information between data and its representation. Specifically, we prove that
the negative logarithm of the denoising score matching loss is a tractable
lower bound for the conditional mutual information. Empirically, we conduct a
series of case studies to validate our theoretical insights. In addition,
Graffe delivers competitive results under the linear probing setting on node
and graph classification tasks, achieving state-of-the-art performance on 9 of
the 11 real-world datasets. These findings indicate that powerful generative
models, especially diffusion models, serve as an effective tool for graph
representation learning.

</details>


### [275] [General Transform: A Unified Framework for Adaptive Transform to Enhance Representations](https://arxiv.org/pdf/2505.04969)
*Gekko Budiutama, Shunsuke Daimon, Hirofumi Nishi, Yu-ichiro Matsushita*

Main category: cs.LG

TL;DR: Proposes General Transform (GT), an adaptive, data-driven transform for machine learning, outperforming conventional transforms in vision and NLP tasks.


<details>
  <summary>Details</summary>
Motivation: Existing discrete transforms require dataset knowledge for selection, limiting effectiveness when such knowledge is unavailable.

Method: GT learns data-driven mappings tailored to the dataset and task, unlike fixed conventional transforms.

Result: Models with GT outperform traditional transform-based methods in computer vision and NLP tasks.

Conclusion: GT is effective for diverse learning scenarios, offering adaptive feature extraction without prior dataset knowledge.

Abstract: Discrete transforms, such as the discrete Fourier transform, are widely used
in machine learning to improve model performance by extracting meaningful
features. However, with numerous transforms available, selecting an appropriate
one often depends on understanding the dataset's properties, making the
approach less effective when such knowledge is unavailable. In this work, we
propose General Transform (GT), an adaptive transform-based representation
designed for machine learning applications. Unlike conventional transforms, GT
learns data-driven mapping tailored to the dataset and task of interest. Here,
we demonstrate that models incorporating GT outperform conventional
transform-based approaches across computer vision and natural language
processing tasks, highlighting its effectiveness in diverse learning scenarios.

</details>


### [276] [Graph Neural Network Aided Deep Reinforcement Learning for Resource Allocation in Dynamic Terahertz UAV Networks](https://arxiv.org/pdf/2505.04981)
*Zhifeng Hu, Chong Han*

Main category: cs.LG

TL;DR: A GNN-aided DRL algorithm (GLOVE) is proposed for efficient resource allocation in dynamic THz UAV networks, outperforming benchmarks in resource efficiency and latency.


<details>
  <summary>Details</summary>
Motivation: Dynamic topologies and mixed-integer nonlinear programming challenges in THz UAV networks hinder efficient resource allocation.

Method: GLOVE uses GNN to learn relationships between UAVs and emphasizes self-node features, with a multi-task structure for cooperative training.

Result: GLOVE achieves higher resource efficiency, lower latency, and zero packet loss compared to benchmarks.

Conclusion: GLOVE demonstrates robustness and superior performance in highly dynamic THz UAV networks.

Abstract: Terahertz (THz) unmanned aerial vehicle (UAV) networks with flexible
topologies and ultra-high data rates are expected to empower numerous
applications in security surveillance, disaster response, and environmental
monitoring, among others. However, the dynamic topologies hinder the efficient
long-term joint power and antenna array resource allocation for THz links among
UAVs. Furthermore, the continuous nature of power and the discrete nature of
antennas cause this joint resource allocation problem to be a mixed-integer
nonlinear programming (MINLP) problem with non-convexity and NP-hardness.
Inspired by recent rapid advancements in deep reinforcement learning (DRL), a
graph neural network (GNN) aided DRL algorithm for resource allocation in the
dynamic THz UAV network with an emphasis on self-node features (GLOVE) is
proposed in this paper, with the aim of resource efficiency (RE) maximization.
When training the allocation policy for each UAV, GLOVE learns the relationship
between this UAV and its neighboring UAVs via GNN, while also emphasizing the
important self-node features of this UAV. In addition, a multi-task structure
is leveraged by GLOVE to cooperatively train resource allocation decisions for
the power and sub-arrays of all UAVs. Experimental results illustrate that
GLOVE outperforms benchmark schemes in terms of the highest RE and the lowest
latency. Moreover, unlike the benchmark methods with severe packet loss, GLOVE
maintains zero packet loss during the entire training process, demonstrating
its better robustness under the highly dynamic THz UAV network.

</details>


### [277] [An Agent-Based Modeling Approach to Free-Text Keyboard Dynamics for Continuous Authentication](https://arxiv.org/pdf/2505.05015)
*Roberto Dillon, Arushi*

Main category: cs.LG

TL;DR: The study evaluates keyboard dynamics for continuous authentication, comparing OC-SVM and RF. RF performs better but struggles across keyboard types.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of behavioral biometrics (keyboard dynamics) for transparent, user-friendly security.

Method: Used an Agent-Based Model to simulate typing profiles, analyzed keystroke data (dwell time, flight time, errors) in 5-second windows, and tested OC-SVM and RF for user verification.

Result: RF achieved >0.7 accuracy for intra-keyboard recognition but failed across keyboards; OC-SVM performed poorly.

Conclusion: Keyboard-specific profiles may be needed, and RF outperforms OC-SVM for fine-grained user patterns.

Abstract: Continuous authentication systems leveraging free-text keyboard dynamics
offer a promising additional layer of security in a multifactor authentication
setup that can be used in a transparent way with no impact on user experience.
This study investigates the efficacy of behavioral biometrics by employing an
Agent-Based Model (ABM) to simulate diverse typing profiles across mechanical
and membrane keyboards. Specifically, we generated synthetic keystroke data
from five unique agents, capturing features related to dwell time, flight time,
and error rates within sliding 5-second windows updated every second. Two
machine learning approaches, One-Class Support Vector Machine (OC-SVM) and
Random Forest (RF), were evaluated for user verification. Results revealed a
stark contrast in performance: while One-Class SVM failed to differentiate
individual users within each group, Random Forest achieved robust
intra-keyboard user recognition (Accuracy > 0.7) but struggled to generalize
across keyboards for the same user, highlighting the significant impact of
keyboard hardware on typing behavior. These findings suggest that: (1)
keyboard-specific user profiles may be necessary for reliable authentication,
and (2) ensemble methods like RF outperform One-Class SVM in capturing
fine-grained user-specific patterns.

</details>


### [278] [Generating Reliable Synthetic Clinical Trial Data: The Role of Hyperparameter Optimization and Domain Constraints](https://arxiv.org/pdf/2505.05019)
*Waldemar Hahn, Jan-Niklas Eckardt, Christoph Röllig, Martin Sedlmayr, Jan Moritz Middeke, Markus Wolfien*

Main category: cs.LG

TL;DR: The study evaluates hyperparameter optimization (HPO) strategies for synthetic clinical trial data, finding compound metrics outperform single-metric approaches. HPO improves data quality, but domain-specific constraints require additional processing.


<details>
  <summary>Details</summary>
Motivation: To address privacy and accessibility issues in medical research by improving synthetic data generation, while ensuring fidelity and utility.

Method: Systematic evaluation of four HPO strategies across eight generative models, comparing single-metric and compound metric optimization.

Result: HPO improves synthetic data quality (up to 60% for TVAE). Compound metrics outperform single metrics, but clinical validity requires preprocessing/postprocessing.

Conclusion: HPO enhances synthetic data, but domain knowledge and processing are crucial for clinical validity. Future work should refine metrics and validate on larger datasets.

Abstract: The generation of synthetic clinical trial data offers a promising approach
to mitigating privacy concerns and data accessibility limitations in medical
research. However, ensuring that synthetic datasets maintain high fidelity,
utility, and adherence to domain-specific constraints remains a key challenge.
While hyperparameter optimization (HPO) has been shown to improve generative
model performance, the effectiveness of different optimization strategies for
synthetic clinical data remains unclear. This study systematically evaluates
four HPO strategies across eight generative models, comparing single-metric
optimization against compound metric optimization approaches. Our results
demonstrate that HPO consistently improves synthetic data quality, with TVAE,
CTGAN, and CTAB-GAN+ achieving improvements of up to 60%, 39%, and 38%,
respectively. Compound metric optimization outperformed single-metric
strategies, producing more balanced and generalizable synthetic datasets.
Interestingly, HPO alone is insufficient to ensure clinically valid synthetic
data, as all models exhibited violations of fundamental survival constraints.
Preprocessing and postprocessing played a crucial role in reducing these
violations, as models lacking robust processing steps produced invalid data in
up to 61% of cases. These findings underscore the necessity of integrating
explicit domain knowledge alongside HPO to create high quality synthetic
datasets. Our study provides actionable recommendations for improving synthetic
data generation, with future research needed to refine metric selection and
validate these findings on larger datasets to enhance clinical applicability.

</details>


### [279] [HESSO: Towards Automatic Efficient and User Friendly Any Neural Network Training and Pruning](https://arxiv.org/pdf/2409.09085)
*Tianyi Chen, Xiaoyi Qu, David Aponte, Colby Banbury, Jongwoo Ko, Tianyu Ding, Yong Ma, Vladimir Lyapunov, Ilya Zharkov, Luming Liang*

Main category: cs.LG

TL;DR: HESSO and CRIC improve structured pruning in DNNs by automating optimization and preventing performance collapse, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing structured pruning methods require multi-stage processes and human intervention, which HESSO and CRIC aim to automate and simplify.

Method: HESSO is a hybrid optimizer for efficient structured sparse training, while CRIC identifies critical structures to prevent irreversible performance collapse.

Result: HESSO achieves competitive or superior performance across applications, and CRIC enhances reliability and performance.

Conclusion: HESSO and CRIC offer a tuning-free, efficient solution for structured pruning, applicable to diverse DNN architectures.

Abstract: Structured pruning is one of the most popular approaches to effectively
compress the heavy deep neural networks (DNNs) into compact sub-networks while
retaining performance. The existing methods suffer from multi-stage procedures
along with significant engineering efforts and human expertise. The
Only-Train-Once (OTO) series has been recently proposed to resolve the many
pain points by streamlining the workflow by automatically conducting (i) search
space generation, (ii) structured sparse optimization, and (iii) sub-network
construction. However, the built-in sparse optimizers in the OTO series, i.e.,
the Half-Space Projected Gradient (HSPG) family, have limitations that require
hyper-parameter tuning and the implicit controls of the sparsity exploration,
consequently requires intervening by human expertise. To address such
limitations, we propose a Hybrid Efficient Structured Sparse Optimizer (HESSO).
HESSO could automatically and efficiently train a DNN to produce a
high-performing subnetwork. Meanwhile, it is almost tuning-free and enjoys
user-friendly integration for generic training applications. To address another
common issue of irreversible performance collapse observed in pruning DNNs, we
further propose a Corrective Redundant Identification Cycle (CRIC) for reliably
identifying indispensable structures. We numerically demonstrate the efficacy
of HESSO and its enhanced version HESSO-CRIC on a variety of applications
ranging from computer vision to natural language processing, including large
language model. The numerical results showcase that HESSO can achieve
competitive even superior performance to varying state-of-the-arts and support
most DNN architectures. Meanwhile, CRIC can effectively prevent the
irreversible performance collapse and further enhance the performance of HESSO
on certain applications.

</details>


### [280] [Generative Models for Long Time Series: Approximately Equivariant Recurrent Network Structures for an Adjusted Training Scheme](https://arxiv.org/pdf/2505.05020)
*Ruwen Fulek, Markus Lange-Hegermann*

Main category: cs.LG

TL;DR: A Recurrent Variational Autoencoder with Subsequent Training (RVAE-ST) is introduced for time series data, using progressive sequence length training to handle long sequences efficiently.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of modeling long sequences with recurrent layers while maintaining efficiency and performance.

Method: Uses a VAE with recurrent layers and a progressive training scheme to increase sequence length gradually.

Result: Matches or outperforms state-of-the-art models, excelling in quasi-periodic time series and remaining competitive in irregular cases.

Conclusion: A well-composed combination of existing components can achieve strong performance without new architectures.

Abstract: We present a simple yet effective generative model for time series data based
on a Variational Autoencoder (VAE) with recurrent layers, referred to as the
Recurrent Variational Autoencoder with Subsequent Training (RVAE-ST). Our
method introduces an adapted training scheme that progressively increases the
sequence length, addressing the challenge recurrent layers typically face when
modeling long sequences. By leveraging the recurrent architecture, the model
maintains a constant number of parameters regardless of sequence length. This
design encourages approximate time-shift equivariance and enables efficient
modeling of long-range temporal dependencies. Rather than introducing a
fundamentally new architecture, we show that a carefully composed combination
of known components can match or outperform state-of-the-art generative models
on several benchmark datasets. Our model performs particularly well on time
series that exhibit quasi-periodic structure,while remaining competitive on
datasets with more irregular or partially non-stationary behavior. We evaluate
its performance using ELBO, Fr\'echet Distance, discriminative scores, and
visualizations of the learned embeddings.

</details>


### [281] [Dequantified Diffusion Schrödinger Bridge for Density Ratio Estimation](https://arxiv.org/pdf/2505.05034)
*Wei Chen, Shigui Li, Jiacheng Li, Junmei Yang, John Paisley, Delu Zeng*

Main category: cs.LG

TL;DR: Proposes $	ext{D}^3	ext{RE}$, a robust framework for density ratio estimation using diffusion bridges and Gaussian dequantization to address instability and support issues.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail under significantly different distributions or inadequate support overlap, suffering from density-chasm and support-chasm problems.

Method: Introduces Dequantified Diffusion-Bridge Interpolant (DDBI) and Dequantified Schrödinger-Bridge Interpolant (DSBI) to expand support coverage and stabilize time scores.

Result: Theoretically ensures uniform approximation and bounded time scores; empirically outperforms baselines in mutual information and density estimation.

Conclusion: $	ext{D}^3	ext{RE}$ provides a unified, efficient solution for robust density ratio estimation.

Abstract: Density ratio estimation is fundamental to tasks involving $f$-divergences,
yet existing methods often fail under significantly different distributions or
inadequately overlap supports, suffering from the \textit{density-chasm} and
the \textit{support-chasm} problems. Additionally, prior approaches yield
divergent time scores near boundaries, leading to instability. We propose
$\text{D}^3\text{RE}$, a unified framework for robust and efficient density
ratio estimation. It introduces the Dequantified Diffusion-Bridge Interpolant
(DDBI), which expands support coverage and stabilizes time scores via diffusion
bridges and Gaussian dequantization. Building on DDBI, the Dequantified
Schr\"odinger-Bridge Interpolant (DSBI) incorporates optimal transport to solve
the Schr\"odinger bridge problem, enhancing accuracy and efficiency. Our method
offers uniform approximation and bounded time scores in theory, and outperforms
baselines empirically in mutual information and density estimation tasks.

</details>


### [282] [Neural Pathways to Program Success: Hopfield Networks for PERT Analysis](https://arxiv.org/pdf/2505.05047)
*Azgar Ali Noor Ahamed*

Main category: cs.LG

TL;DR: A novel PERT scheduling method using Hopfield neural networks for energy minimization, achieving near-optimal schedules with minimal constraint violations.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of project and task scheduling under uncertainty, particularly in complex, multi-project systems where accurate task duration and dependency estimation is critical.

Method: Formulates PERT scheduling as an energy minimization problem within a Hopfield neural network, mapping task start times and precedence constraints into the neural framework.

Result: Numerical simulations on synthetic project networks (up to 1000 tasks) show near-optimal makespans with minimal constraint violations.

Conclusion: Neural optimization models, like the proposed Hopfield approach, are promising for scalable and adaptive project scheduling under uncertainty, especially in AI workflows and microservice-based applications.

Abstract: Project and task scheduling under uncertainty remains a fundamental challenge
in program and project management, where accurate estimation of task durations
and dependencies is critical for delivering complex, multi project systems. The
Program Evaluation and Review Technique provides a probabilistic framework to
model task variability and critical paths. In this paper, the author presents a
novel formulation of PERT scheduling as an energy minimization problem within a
Hopfield neural network architecture. By mapping task start times and
precedence constraints into a neural computation framework, the networks
inherent optimization dynamics is exploited to approximate globally consistent
schedules. The author addresses key theoretical issues related to energy
function differentiability, constraint encoding, and convergence, and extends
the Hopfield model for structured precedence graphs. Numerical simulations on
synthetic project networks comprising up to 1000 tasks demonstrate the
viability of this approach, achieving near optimal makespans with minimal
constraint violations. The findings suggest that neural optimization models
offer a promising direction for scalable and adaptive project tasks scheduling
under uncertainty in areas such as the agentic AI workflows, microservice based
applications that the modern AI systems are being built upon.

</details>


### [283] [CodeMixBench: Evaluating Large Language Models on Code Generation with Code-Mixed Prompts](https://arxiv.org/pdf/2505.05063)
*Manik Sheokand, Parth Sawant*

Main category: cs.LG

TL;DR: CodeMixBench is a new benchmark evaluating LLMs on code generation from code-mixed prompts, revealing performance drops compared to English-only prompts.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks overlook multilingual developers using code-mixed language, creating a gap in evaluating LLMs' robustness.

Method: Built on BigCodeBench, CodeMixBench introduces controlled code-mixing in prompts across three language pairs and evaluates diverse LLMs.

Result: Code-mixed prompts degrade Pass@1 performance, especially for smaller models under higher code-mixing levels.

Conclusion: CodeMixBench highlights challenges for multilingual code generation and guides future robust model development.

Abstract: Large Language Models (LLMs) have achieved remarkable success in code
generation tasks, powering various applications like code completion,
debugging, and programming assistance. However, existing benchmarks such as
HumanEval, MBPP, and BigCodeBench primarily evaluate LLMs on English-only
prompts, overlooking the real-world scenario where multilingual developers
often use code-mixed language while interacting with LLMs. To address this gap,
we introduce CodeMixBench, a novel benchmark designed to evaluate the
robustness of LLMs on code generation from code-mixed prompts. Built upon
BigCodeBench, CodeMixBench introduces controlled code-mixing (CMD) into the
natural language parts of prompts across three language pairs: Hinglish
(Hindi-English), Spanish-English, and Chinese Pinyin-English. We
comprehensively evaluate a diverse set of open-source code generation models
ranging from 1.5B to 15B parameters. Our results show that code-mixed prompts
consistently degrade Pass@1 performance compared to their English-only
counterparts, with performance drops increasing under higher CMD levels for
smaller models. CodeMixBench provides a realistic evaluation framework for
studying multilingual code generation and highlights new challenges and
directions for building robust code generation models that generalize well
across diverse linguistic settings.

</details>


### [284] [WaterDrum: Watermarking for Data-centric Unlearning Metric](https://arxiv.org/pdf/2505.05064)
*Xinyang Lu, Xinyuan Niu, Gregory Kang Ruey Lau, Bui Thi Cam Nhung, Rachael Hwee Ling Sim, Fanyu Wen, Chuan-Sheng Foo, See-Kiong Ng, Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: The paper introduces WaterDrum, a data-centric unlearning metric for LLMs, addressing limitations of utility-centric metrics. It also provides benchmark datasets for rigorous evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing unlearning metrics fail in realistic scenarios (e.g., similar forget/retain data, impractical retraining). A robust, data-centric solution is needed.

Method: WaterDrum uses text watermarking to overcome limitations. New benchmark datasets with varying similarity levels are introduced.

Result: WaterDrum provides a more accurate evaluation of unlearning in LLMs. Benchmark datasets enable rigorous testing.

Conclusion: WaterDrum is a novel, effective metric for LLM unlearning, supported by practical datasets for evaluation.

Abstract: Large language model (LLM) unlearning is critical in real-world applications
where it is necessary to efficiently remove the influence of private,
copyrighted, or harmful data from some users. However, existing utility-centric
unlearning metrics (based on model utility) may fail to accurately evaluate the
extent of unlearning in realistic settings such as when (a) the forget and
retain set have semantically similar content, (b) retraining the model from
scratch on the retain set is impractical, and/or (c) the model owner can
improve the unlearning metric without directly performing unlearning on the
LLM. This paper presents the first data-centric unlearning metric for LLMs
called WaterDrum that exploits robust text watermarking for overcoming these
limitations. We also introduce new benchmark datasets for LLM unlearning that
contain varying levels of similar data points and can be used to rigorously
evaluate unlearning algorithms using WaterDrum. Our code is available at
https://github.com/lululu008/WaterDrum and our new benchmark datasets are
released at https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax.

</details>


### [285] [ItDPDM: Information-Theoretic Discrete Poisson Diffusion Model](https://arxiv.org/pdf/2505.05082)
*Sagnik Bhattacharya, Abhiram R. Gorle, Ahmed Mohsin, Ahsan Bilal, Connor Ding, Amit Kumar Singh Yadav, Tsachy Weissman*

Main category: cs.LG

TL;DR: ItDPDM introduces a discrete Poisson diffusion model for generative modeling, addressing limitations of continuous state-spaces and variational losses, achieving better performance and faster convergence.


<details>
  <summary>Details</summary>
Motivation: Existing methods for discrete data modeling either use continuous embeddings or approximate losses, leading to suboptimal results.

Method: The Information-Theoretic Discrete Poisson Diffusion Model (ItDPDM) operates directly in discrete state-spaces using a Poisson diffusion process and introduces a novel Poisson Reconstruction Loss (PRL).

Result: ItDPDM reduces test NLL by up to 80% and achieves faster convergence on the Lakh MIDI and CIFAR-10 datasets.

Conclusion: ItDPDM effectively addresses the limitations of prior methods, offering improved performance and efficiency for discrete data modeling.

Abstract: Existing methods for generative modeling of discrete data, such as symbolic
music tokens, face two primary challenges: (1) they either embed discrete
inputs into continuous state-spaces or (2) rely on variational losses that only
approximate the true negative log-likelihood. Previous efforts have
individually targeted these limitations. While information-theoretic Gaussian
diffusion models alleviate the suboptimality of variational losses, they still
perform modeling in continuous domains. In this work, we introduce the
Information-Theoretic Discrete Poisson Diffusion Model (ItDPDM), which
simultaneously addresses both limitations by directly operating in a discrete
state-space via a Poisson diffusion process inspired by photon arrival
processes in camera sensors. We introduce a novel Poisson Reconstruction Loss
(PRL) and derive an exact relationship between PRL and the true negative
log-likelihood, thereby eliminating the need for approximate evidence lower
bounds. Experiments conducted on the Lakh MIDI symbolic music dataset and the
CIFAR-10 image benchmark demonstrate that ItDPDM delivers significant
improvements, reducing test NLL by up to 80% compared to prior baselines, while
also achieving faster convergence.

</details>


### [286] [Beyond Low-rank Decomposition: A Shortcut Approach for Efficient On-Device Learning](https://arxiv.org/pdf/2505.05086)
*Le-Trung Nguyen, Ael Quelennec, Van-Tam Nguyen, Enzo Tartaglione*

Main category: cs.LG

TL;DR: Proposes a shortcut method for on-device learning to reduce activation memory usage and computational costs, achieving significant improvements over vanilla training.


<details>
  <summary>Details</summary>
Motivation: Address memory and computational constraints in on-device learning to enhance efficiency and privacy.

Method: Introduces a novel shortcut approach based on low-rank decomposition to tackle activation memory bottlenecks.

Result: Reduces activation memory usage up to 120.09× and training FLOPs up to 1.86× compared to vanilla training.

Conclusion: The method effectively addresses key challenges in on-device learning, offering practical benefits for deployment.

Abstract: On-device learning has emerged as a promising direction for AI development,
particularly because of its potential to reduce latency issues and mitigate
privacy risks associated with device-server communication, while improving
energy efficiency. Despite these advantages, significant memory and
computational constraints still represent major challenges for its deployment.
Drawing on previous studies on low-rank decomposition methods that address
activation memory bottlenecks in backpropagation, we propose a novel shortcut
approach as an alternative. Our analysis and experiments demonstrate that our
method can reduce activation memory usage, even up to $120.09\times$ compared
to vanilla training, while also reducing overall training FLOPs up to
$1.86\times$ when evaluated on traditional benchmarks.

</details>


### [287] [A Conjoint Graph Representation Learning Framework for Hypertension Comorbidity Risk Prediction](https://arxiv.org/pdf/2505.05094)
*Leming Zhou, Zuo Wang, Zhixuan Duan*

Main category: cs.LG

TL;DR: The paper proposes a Conjoint Graph Representation Learning (CGRL) framework to predict diabetes and coronary heart disease risks by analyzing comorbidity networks.


<details>
  <summary>Details</summary>
Motivation: Early identification of hypertension comorbidities is challenging but crucial for intervention.

Method: CGRL constructs patient and disease difference networks, generates comorbidity network features, and integrates computational structure intervention for risk prediction.

Result: The framework outperforms other models in accuracy, with network features proving significant.

Conclusion: CGRL effectively predicts disease risks and reveals comorbidity patterns, aiding in understanding disease progression.

Abstract: The comorbidities of hypertension impose a heavy burden on patients and
society. Early identification is necessary to prompt intervention, but it
remains a challenging task. This study aims to address this challenge by
combining joint graph learning with network analysis. Motivated by this
discovery, we develop a Conjoint Graph Representation Learning (CGRL) framework
that: a) constructs two networks based on disease coding, including the patient
network and the disease difference network. Three comorbidity network features
were generated based on the basic difference network to capture the potential
relationship between comorbidities and risk diseases; b) incorporates
computational structure intervention and learning feature representation, CGRL
was developed to predict the risks of diabetes and coronary heart disease in
patients; and c) analysis the comorbidity patterns and exploring the pathways
of disease progression, the pathological pathogenesis of diabetes and coronary
heart disease may be revealed. The results show that the network features
extracted based on the difference network are important, and the framework we
proposed provides more accurate predictions than other strong models in terms
of accuracy.

</details>


### [288] [Balancing Client Participation in Federated Learning Using AoI](https://arxiv.org/pdf/2505.05099)
*Alireza Javani, Zhiying Wang*

Main category: cs.LG

TL;DR: The paper proposes an Age of Information (AoI)-based client selection policy for Federated Learning (FL) to address challenges like communication limits and data heterogeneity. It uses a decentralized Markov scheduling policy for balanced participation and proves convergence, showing significant improvements over FedAvg.


<details>
  <summary>Details</summary>
Motivation: FL faces issues like limited communication, statistical heterogeneity, and unbalanced client participation, which hinder efficiency and fairness.

Method: An AoI-based client selection policy using decentralized Markov scheduling to balance participation with minimal central oversight. Convergence is proven, and optimal parameters are derived.

Result: The method improves convergence by 7.5% (IID) and up to 20% (non-IID) over FedAvg, demonstrating scalability and fairness.

Conclusion: AoI-based scheduling enhances FL systems, ensuring stable convergence and balanced participation in diverse settings.

Abstract: Federated Learning (FL) offers a decentralized framework that preserves data
privacy while enabling collaborative model training across distributed clients.
However, FL faces significant challenges due to limited communication
resources, statistical heterogeneity, and the need for balanced client
participation. This paper proposes an Age of Information (AoI)-based client
selection policy that addresses these challenges by minimizing load imbalance
through controlled selection intervals. Our method employs a decentralized
Markov scheduling policy, allowing clients to independently manage
participation based on age-dependent selection probabilities, which balances
client updates across training rounds with minimal central oversight. We
provide a convergence proof for our method, demonstrating that it ensures
stable and efficient model convergence. Specifically, we derive optimal
parameters for the Markov selection model to achieve balanced and consistent
client participation, highlighting the benefits of AoI in enhancing convergence
stability. Through extensive simulations, we demonstrate that our AoI-based
method, particularly the optimal Markov variant, improves convergence over the
FedAvg selection approach across both IID and non-IID data settings by $7.5\%$
and up to $20\%$. Our findings underscore the effectiveness of AoI-based
scheduling for scalable, fair, and efficient FL systems across diverse learning
environments.

</details>


### [289] [Taming OOD Actions for Offline Reinforcement Learning: An Advantage-Based Approach](https://arxiv.org/pdf/2505.05126)
*Xuyang Chen, Keyu Yan, Lin Zhao*

Main category: cs.LG

TL;DR: ADAC is a novel offline RL method that evaluates OOD actions using the batch-optimal value function, enabling better generalization and performance.


<details>
  <summary>Details</summary>
Motivation: Offline RL suffers from distribution shift and overestimation of OOD actions, limiting its effectiveness.

Method: ADAC uses advantage-based modulation of the Q-function to systematically assess OOD actions, leveraging a custom PointMaze environment for validation.

Result: ADAC achieves state-of-the-art performance on D4RL benchmarks, especially in challenging tasks.

Conclusion: ADAC effectively addresses distribution shift in offline RL by selectively evaluating OOD actions, improving generalization and performance.

Abstract: Offline reinforcement learning (RL) aims to learn decision-making policies
from fixed datasets without online interactions, providing a practical solution
where online data collection is expensive or risky. However, offline RL often
suffers from distribution shift, resulting in inaccurate evaluation and
substantial overestimation on out-of-distribution (OOD) actions. To address
this, existing approaches incorporate conservatism by indiscriminately
discouraging all OOD actions, thereby hindering the agent's ability to
generalize and exploit beneficial ones. In this paper, we propose
Advantage-based Diffusion Actor-Critic (ADAC), a novel method that
systematically evaluates OOD actions using the batch-optimal value function.
Based on this evaluation, ADAC defines an advantage function to modulate the
Q-function update, enabling more precise assessment of OOD action quality. We
design a custom PointMaze environment and collect datasets to visually reveal
that advantage modulation can effectively identify and select superior OOD
actions. Extensive experiments show that ADAC achieves state-of-the-art
performance on almost all tasks in the D4RL benchmark, with particularly clear
margins on the more challenging tasks.

</details>


### [290] [Research on Anomaly Detection Methods Based on Diffusion Models](https://arxiv.org/pdf/2505.05137)
*Yi Chen*

Main category: cs.LG

TL;DR: A novel anomaly detection framework using diffusion models outperforms traditional methods by leveraging multi-scale features and attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Traditional anomaly detection methods struggle with complex, high-dimensional data, prompting the exploration of diffusion models for better performance.

Method: The framework models normal data distribution via diffusion, reconstructs inputs with reverse diffusion, and uses reconstruction errors and semantic discrepancies as anomaly indicators. Multi-scale features, attention, and wavelet-domain representations enhance performance.

Result: Outperforms state-of-the-art methods on benchmarks like MVTec AD and UrbanSound8K, showing superior accuracy and robustness.

Conclusion: Diffusion models are effective for anomaly detection, offering a robust solution for real-world applications.

Abstract: Anomaly detection is a fundamental task in machine learning and data mining,
with significant applications in cybersecurity, industrial fault diagnosis, and
clinical disease monitoring. Traditional methods, such as statistical modeling
and machine learning-based approaches, often face challenges in handling
complex, high-dimensional data distributions. In this study, we explore the
potential of diffusion models for anomaly detection, proposing a novel
framework that leverages the strengths of diffusion probabilistic models (DPMs)
to effectively identify anomalies in both image and audio data. The proposed
method models the distribution of normal data through a diffusion process and
reconstructs input data via reverse diffusion, using a combination of
reconstruction errors and semantic discrepancies as anomaly indicators. To
enhance the framework's performance, we introduce multi-scale feature
extraction, attention mechanisms, and wavelet-domain representations, enabling
the model to capture fine-grained structures and global dependencies in the
data. Extensive experiments on benchmark datasets, including MVTec AD and
UrbanSound8K, demonstrate that our method outperforms state-of-the-art anomaly
detection techniques, achieving superior accuracy and robustness across diverse
data modalities. This research highlights the effectiveness of diffusion models
in anomaly detection and provides a robust and efficient solution for
real-world applications.

</details>


### [291] [Sparse Training from Random Initialization: Aligning Lottery Ticket Masks using Weight Symmetry](https://arxiv.org/pdf/2505.05143)
*Mohammed Adnan, Rohan Jain, Ekansh Sharma, Rahul Krishnan, Yani Ioannou*

Main category: cs.LG

TL;DR: The paper addresses the generalization issue of Lottery Ticket Hypothesis (LTH) masks by proposing permutation alignment to improve sparse training performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the problem where LTH masks fail to generalize to new random initializations due to misaligned optimization basins.

Method: The method involves permuting the LTH mask to align with the new optimization basin when training from a different random initialization.

Result: Empirical results show improved generalization with the permuted mask on datasets like CIFAR-10, CIFAR-100, and ImageNet, using models such as VGG11 and ResNets.

Conclusion: Permuting LTH masks enhances their generalization across different initializations, validating the hypothesis about basin misalignment.

Abstract: The Lottery Ticket Hypothesis (LTH) suggests there exists a sparse LTH mask
and weights that achieve the same generalization performance as the dense model
while using significantly fewer parameters. However, finding a LTH solution is
computationally expensive, and a LTH sparsity mask does not generalize to other
random weight initializations. Recent work has suggested that neural networks
trained from random initialization find solutions within the same basin modulo
permutation, and proposes a method to align trained models within the same loss
basin. We hypothesize that misalignment of basins is the reason why LTH masks
do not generalize to new random initializations and propose permuting the LTH
mask to align with the new optimization basin when performing sparse training
from a different random init. We empirically show a significant increase in
generalization when sparse training from random initialization with the
permuted mask as compared to using the non-permuted LTH mask, on multiple
datasets (CIFAR-10, CIFAR-100 and ImageNet) and models (VGG11, ResNet20 and
ResNet50).

</details>


### [292] [Understanding In-context Learning of Addition via Activation Subspaces](https://arxiv.org/pdf/2505.05145)
*Xinyan Hu, Kayo Yin, Michael I. Jordan, Jacob Steinhardt, Lijie Chen*

Main category: cs.LG

TL;DR: The paper investigates how transformer models like Llama-3-8B perform in-context learning by analyzing a structured task of adding an integer $k$ to inputs. It identifies key attention heads and a low-dimensional subspace for signal extraction, revealing a self-correction mechanism.


<details>
  <summary>Details</summary>
Motivation: To understand the computational mechanisms behind in-context learning in transformer models, specifically how they extract and apply prediction rules from few-shot examples.

Method: The study uses a structured task (adding integer $k$ to inputs) and analyzes Llama-3-8B's performance, localizing few-shot learning to specific attention heads and a six-dimensional subspace.

Result: Llama-3-8B achieves high accuracy, with signal extraction localized to three attention heads and a six-dimensional subspace (four for unit digits, two for magnitude). A self-correction mechanism is also identified.

Conclusion: Tracking low-dimensional subspaces in transformer models provides insights into fine-grained computational structures, revealing how in-context learning is implemented.

Abstract: To perform in-context learning, language models must extract signals from
individual few-shot examples, aggregate these into a learned prediction rule,
and then apply this rule to new examples. How is this implemented in the
forward pass of modern transformer models? To study this, we consider a
structured family of few-shot learning tasks for which the true prediction rule
is to add an integer $k$ to the input. We find that Llama-3-8B attains high
accuracy on this task for a range of $k$, and localize its few-shot ability to
just three attention heads via a novel optimization approach. We further show
the extracted signals lie in a six-dimensional subspace, where four of the
dimensions track the unit digit and the other two dimensions track overall
magnitude. We finally examine how these heads extract information from
individual few-shot examples, identifying a self-correction mechanism in which
mistakes from earlier examples are suppressed by later examples. Our results
demonstrate how tracking low-dimensional subspaces across a forward pass can
provide insight into fine-grained computational structures.

</details>


### [293] [FedTDP: A Privacy-Preserving and Unified Framework for Trajectory Data Preparation via Federated Learning](https://arxiv.org/pdf/2505.05155)
*Zhihao Zeng, Ziquan Fang, Wei Shao, Lu Chen, Yunjun Gao*

Main category: cs.LG

TL;DR: FedTDP is a privacy-preserving, unified framework using LLMs for Trajectory Data Preparation (TDP) in federated settings, addressing noise, incompleteness, and privacy concerns while outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Trajectory data quality is compromised by noise and incompleteness, and existing TDP methods lack privacy protection and generalizability.

Method: FedTDP uses a trajectory privacy autoencoder, knowledge enhancer, and federated parallel optimization to secure data, improve learning, and enhance training efficiency.

Result: FedTDP outperforms 13 baselines across 6 datasets and 10 TDP tasks.

Conclusion: FedTDP effectively addresses privacy and generalizability in TDP, demonstrating superior performance in federated environments.

Abstract: Trajectory data, which capture the movement patterns of people and vehicles
over time and space, are crucial for applications like traffic optimization and
urban planning. However, issues such as noise and incompleteness often
compromise data quality, leading to inaccurate trajectory analyses and limiting
the potential of these applications. While Trajectory Data Preparation (TDP)
can enhance data quality, existing methods suffer from two key limitations: (i)
they do not address data privacy concerns, particularly in federated settings
where trajectory data sharing is prohibited, and (ii) they typically design
task-specific models that lack generalizability across diverse TDP scenarios.
To overcome these challenges, we propose FedTDP, a privacy-preserving and
unified framework that leverages the capabilities of Large Language Models
(LLMs) for TDP in federated environments. Specifically, we: (i) design a
trajectory privacy autoencoder to secure data transmission and protect privacy,
(ii) introduce a trajectory knowledge enhancer to improve model learning of
TDP-related knowledge, enabling the development of TDP-oriented LLMs, and (iii)
propose federated parallel optimization to enhance training efficiency by
reducing data transmission and enabling parallel model training. Experiments on
6 real datasets and 10 mainstream TDP tasks demonstrate that FedTDP
consistently outperforms 13 state-of-the-art baselines.

</details>


### [294] [Bandit Max-Min Fair Allocation](https://arxiv.org/pdf/2505.05169)
*Tsubasa Harada, Shinji Ito, Hanna Sumita*

Main category: cs.LG

TL;DR: The paper introduces the Bandit Max-Min Fair Allocation (BMMFA) problem, focusing on maximizing the minimum utility among agents with additive valuations using semi-bandit feedback. It proposes an algorithm with an asymptotic regret bound and provides a matching lower bound.


<details>
  <summary>Details</summary>
Motivation: Existing work assumes item values are known upfront, but this paper addresses the challenge of semi-bandit feedback and non-additive rewards, which are more realistic but understudied.

Method: The authors combine bandit techniques with a resource allocation algorithm to develop a novel algorithm for BMMFA.

Result: The algorithm achieves an asymptotic regret bound of $O(m\sqrt{T}\ln T/n + m\sqrt{T \ln(mnT)})$, and a lower bound of $\Omega(m\sqrt{T}/n)$ is established.

Conclusion: The paper bridges a gap in the literature by addressing semi-bandit feedback and non-additive rewards, with theoretical guarantees on regret bounds.

Abstract: In this paper, we study a new decision-making problem called the bandit
max-min fair allocation (BMMFA) problem. The goal of this problem is to
maximize the minimum utility among agents with additive valuations by
repeatedly assigning indivisible goods to them. One key feature of this problem
is that each agent's valuation for each item can only be observed through the
semi-bandit feedback, while existing work supposes that the item values are
provided at the beginning of each round. Another key feature is that the
algorithm's reward function is not additive with respect to rounds, unlike most
bandit-setting problems.
  Our first contribution is to propose an algorithm that has an asymptotic
regret bound of $O(m\sqrt{T}\ln T/n + m\sqrt{T \ln(mnT)})$, where $n$ is the
number of agents, $m$ is the number of items, and $T$ is the time horizon. This
is based on a novel combination of bandit techniques and a resource allocation
algorithm studied in the literature on competitive analysis. Our second
contribution is to provide the regret lower bound of $\Omega(m\sqrt{T}/n)$.
When $T$ is sufficiently larger than $n$, the gap between the upper and lower
bounds is a logarithmic factor of $T$.

</details>


### [295] [OpenworldAUC: Towards Unified Evaluation and Optimization for Open-world Prompt Tuning](https://arxiv.org/pdf/2505.05180)
*Cong Hua, Qianqian Xu, Zhiyong Yang, Zitai Wang, Shilong Bao, Qingming Huang*

Main category: cs.LG

TL;DR: The paper introduces OpenworldAUC, a unified metric for evaluating open-world prompt tuning in Vision-Language Models, and Gated Mixture-of-Prompts (GMoP) to optimize it, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Real-world scenarios require models to handle inputs without prior domain knowledge, but current metrics fail to evaluate detection and classification simultaneously.

Method: Proposes OpenworldAUC for unified evaluation and GMoP, which uses domain-specific prompts and a gating mechanism to balance detection and classification.

Result: GMoP achieves SOTA performance on 15 benchmarks in open-world scenarios.

Conclusion: OpenworldAUC and GMoP address practical challenges in open-world prompt tuning, offering a robust solution for unified evaluation and optimization.

Abstract: Prompt tuning adapts Vision-Language Models like CLIP to open-world tasks
with minimal training costs. In this direction, one typical paradigm evaluates
model performance separately on known classes (i.e., base domain) and unseen
classes (i.e., new domain). However, real-world scenarios require models to
handle inputs without prior domain knowledge. This practical challenge has
spurred the development of open-world prompt tuning, which demands a unified
evaluation of two stages: 1) detecting whether an input belongs to the base or
new domain (P1), and 2) classifying the sample into its correct class (P2).
What's more, as domain distributions are generally unknown, a proper metric
should be insensitive to varying base/new sample ratios (P3). However, we find
that current metrics, including HM, overall accuracy, and AUROC, fail to
satisfy these three properties simultaneously. To bridge this gap, we propose
OpenworldAUC, a unified metric that jointly assesses detection and
classification through pairwise instance comparisons. To optimize OpenworldAUC
effectively, we introduce Gated Mixture-of-Prompts (GMoP), which employs
domain-specific prompts and a gating mechanism to dynamically balance detection
and classification. Theoretical guarantees ensure generalization of GMoP under
practical conditions. Experiments on 15 benchmarks in open-world scenarios show
GMoP achieves SOTA performance on OpenworldAUC and other metrics. We release
the code at https://github.com/huacong/OpenworldAUC

</details>


### [296] [Stochastic Variational Propagation: Local, Scalable and Efficient Alternative to Backpropagation](https://arxiv.org/pdf/2505.05181)
*Bojian Yin, Federico Corradi*

Main category: cs.LG

TL;DR: SVP replaces backpropagation with hierarchical variational inference for scalable, memory-efficient deep learning, avoiding representation collapse via low-dimensional projections.


<details>
  <summary>Details</summary>
Motivation: Backpropagation's scalability and memory limitations drive the need for alternatives like SVP.

Method: SVP uses local ELBOs, random projections, and feature alignment to train networks without global gradients.

Result: SVP matches BP's accuracy, reduces memory by 4x, and improves scalability.

Conclusion: SVP offers a probabilistic, modular approach to deep learning, enhancing interpretability and scalability.

Abstract: Backpropagation (BP) is the cornerstone of deep learning, but its reliance on
global gradient synchronization limits scalability and imposes significant
memory overhead. We propose Stochastic Variational Propagation (SVP), a
scalable alternative that reframes training as hierarchical variational
inference. SVP treats layer activations as latent variables and optimizes local
Evidence Lower Bounds (ELBOs), enabling independent, local updates while
preserving global coherence. However, directly applying KL divergence in
layer-wise ELBOs risks inter-layer's representation collapse due to excessive
compression. To prevent this, SVP projects activations into low-dimensional
spaces via fixed random matrices, ensuring information preservation and
representational diversity. Combined with a feature alignment loss for
inter-layer consistency, SVP achieves competitive accuracy with BP across
diverse architectures (MLPs, CNNs, Transformers) and datasets (MNIST to
ImageNet), reduces memory usage by up to 4x, and significantly improves
scalability. More broadly, SVP introduces a probabilistic perspective to deep
representation learning, opening pathways toward more modular and interpretable
neural network design.

</details>


### [297] [Revealing Weaknesses in Text Watermarking Through Self-Information Rewrite Attacks](https://arxiv.org/pdf/2505.05190)
*Yixin Cheng, Hongcheng Guo, Yangming Li, Leonid Sigal*

Main category: cs.LG

TL;DR: The paper reveals a vulnerability in current text watermarking algorithms, introduces the SIRA attack, and demonstrates its high success rate and low cost, urging the need for more robust watermarking.


<details>
  <summary>Details</summary>
Motivation: To expose a vulnerability in text watermarking algorithms that embed watermarks in high-entropy tokens, which attackers can exploit.

Method: Introduces the Self-Information Rewrite Attack (SIRA), which identifies and targets pattern tokens using self-information calculations.

Result: SIRA achieves nearly 100% attack success rates on seven watermarking methods with minimal cost (0.88 USD per million tokens).

Conclusion: The findings emphasize the urgent need for more robust watermarking techniques to counter such attacks.

Abstract: Text watermarking aims to subtly embed statistical signals into text by
controlling the Large Language Model (LLM)'s sampling process, enabling
watermark detectors to verify that the output was generated by the specified
model. The robustness of these watermarking algorithms has become a key factor
in evaluating their effectiveness. Current text watermarking algorithms embed
watermarks in high-entropy tokens to ensure text quality. In this paper, we
reveal that this seemingly benign design can be exploited by attackers, posing
a significant risk to the robustness of the watermark. We introduce a generic
efficient paraphrasing attack, the Self-Information Rewrite Attack (SIRA),
which leverages the vulnerability by calculating the self-information of each
token to identify potential pattern tokens and perform targeted attack. Our
work exposes a widely prevalent vulnerability in current watermarking
algorithms. The experimental results show SIRA achieves nearly 100% attack
success rates on seven recent watermarking methods with only 0.88 USD per
million tokens cost. Our approach does not require any access to the watermark
algorithms or the watermarked LLM and can seamlessly transfer to any LLM as the
attack model, even mobile-level models. Our findings highlight the urgent need
for more robust watermarking.

</details>


### [298] [Long-Term Individual Causal Effect Estimation via Identifiable Latent Representation Learning](https://arxiv.org/pdf/2505.05192)
*Ruichu Cai, Junjie Wan, Weilin Chen, Zeqin Yang, Zijian Li, Peng Zhen, Jiecheng Guo*

Main category: cs.LG

TL;DR: Proposes a method to estimate long-term causal effects without relying on ideal assumptions by leveraging data heterogeneity and latent representation learning.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on ideal assumptions (e.g., latent unconfoundedness) that are often violated in practice, limiting their effectiveness.

Method: Uses data heterogeneity (e.g., multiple sources) to identify latent confounders and develops a latent representation learning-based estimator.

Result: Demonstrates effectiveness on synthetic and semi-synthetic datasets, proving identifiability of latent confounders and long-term effects.

Conclusion: The method avoids idealized assumptions and provides a practical solution for long-term causal effect estimation.

Abstract: Estimating long-term causal effects by combining long-term observational and
short-term experimental data is a crucial but challenging problem in many
real-world scenarios. In existing methods, several ideal assumptions, e.g.
latent unconfoundedness assumption or additive equi-confounding bias
assumption, are proposed to address the latent confounder problem raised by the
observational data. However, in real-world applications, these assumptions are
typically violated which limits their practical effectiveness. In this paper,
we tackle the problem of estimating the long-term individual causal effects
without the aforementioned assumptions. Specifically, we propose to utilize the
natural heterogeneity of data, such as data from multiple sources, to identify
latent confounders, thereby significantly avoiding reliance on idealized
assumptions. Practically, we devise a latent representation learning-based
estimator of long-term causal effects. Theoretically, we establish the
identifiability of latent confounders, with which we further achieve long-term
effect identification. Extensive experimental studies, conducted on multiple
synthetic and semi-synthetic datasets, demonstrate the effectiveness of our
proposed method.

</details>


### [299] [Concept-Based Unsupervised Domain Adaptation](https://arxiv.org/pdf/2505.05195)
*Xinyue Xu, Yueying Hu, Hui Tang, Yi Qin, Lu Mi, Hao Wang, Xiaomeng Li*

Main category: cs.LG

TL;DR: CUDA improves CBMs' robustness under domain shifts by aligning concept representations, relaxing constraints, and integrating concept learning into DA.


<details>
  <summary>Details</summary>
Motivation: CBMs struggle with domain shifts, degrading performance and generalization. CUDA aims to enhance robustness and adaptability.

Method: CUDA aligns concepts across domains via adversarial training, relaxes constraints, infers concepts without labels, and integrates with DA theoretically.

Result: CUDA outperforms state-of-the-art CBM and DA methods on real-world datasets.

Conclusion: CUDA effectively addresses domain shift challenges in CBMs, improving performance and interpretability.

Abstract: Concept Bottleneck Models (CBMs) enhance interpretability by explaining
predictions through human-understandable concepts but typically assume that
training and test data share the same distribution. This assumption often fails
under domain shifts, leading to degraded performance and poor generalization.
To address these limitations and improve the robustness of CBMs, we propose the
Concept-based Unsupervised Domain Adaptation (CUDA) framework. CUDA is designed
to: (1) align concept representations across domains using adversarial
training, (2) introduce a relaxation threshold to allow minor domain-specific
differences in concept distributions, thereby preventing performance drop due
to over-constraints of these distributions, (3) infer concepts directly in the
target domain without requiring labeled concept data, enabling CBMs to adapt to
diverse domains, and (4) integrate concept learning into conventional domain
adaptation (DA) with theoretical guarantees, improving interpretability and
establishing new benchmarks for DA. Experiments demonstrate that our approach
significantly outperforms the state-of-the-art CBM and DA methods on real-world
datasets.

</details>


### [300] [GFlowNets for Active Learning Based Resource Allocation in Next Generation Wireless Networks](https://arxiv.org/pdf/2505.05224)
*Charbel Bou Chaaya, Mehdi Bennis*

Main category: cs.LG

TL;DR: A novel active learning framework using GFlowNet for efficient radio resource allocation in wireless systems with diverse functionalities, achieving 20% performance gains.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of managing heterogeneous requirements (communication, sensing, computing) in wireless systems with high-dimensional and discrete resource allocation problems.

Method: Proposes an active learning framework where resource allocation patterns are drawn sequentially, evaluated, and used to update a surrogate model. Uses GFlowNet to sample diverse and high-return solutions.

Result: Achieves 20% performance gains over benchmarks and requires fewer acquisition rounds.

Conclusion: The method efficiently discovers suitable resource management solutions, demonstrating significant improvements in performance and efficiency.

Abstract: In this work, we consider the radio resource allocation problem in a wireless
system with various integrated functionalities, such as communication, sensing
and computing. We design suitable resource management techniques that can
simultaneously cater to those heterogeneous requirements, and scale
appropriately with the high-dimensional and discrete nature of the problem. We
propose a novel active learning framework where resource allocation patterns
are drawn sequentially, evaluated in the environment, and then used to
iteratively update a surrogate model of the environment. Our method leverages a
generative flow network (GFlowNet) to sample favorable solutions, as such
models are trained to generate compositional objects proportionally to their
training reward, hence providing an appropriate coverage of its modes. As such,
GFlowNet generates diverse and high return resource management designs that
update the surrogate model and swiftly discover suitable solutions. We provide
simulation results showing that our method can allocate radio resources
achieving 20% performance gains against benchmarks, while requiring less than
half of the number of acquisition rounds.

</details>


### [301] [Put CASH on Bandits: A Max K-Armed Problem for Automated Machine Learning](https://arxiv.org/pdf/2505.05226)
*Amir Rezaei Balef, Claire Vernade, Katharina Eggensperger*

Main category: cs.LG

TL;DR: MaxUCB, a max k-armed bandit method, efficiently solves the CASH problem in AutoML by balancing model class exploration and hyperparameter optimization, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenging CASH problem in AutoML requires efficient resource allocation, which existing methods struggle with due to assumptions of heavy-tailed reward distributions.

Method: Proposes MaxUCB, a max k-armed bandit method tailored for light-tailed and bounded reward distributions in AutoML.

Result: Theoretical and empirical evaluations on four AutoML benchmarks show MaxUCB outperforms prior approaches.

Conclusion: MaxUCB provides a superior solution for the CASH problem by leveraging tailored assumptions and efficient resource allocation.

Abstract: The Combined Algorithm Selection and Hyperparameter optimization (CASH) is a
challenging resource allocation problem in the field of AutoML. We propose
MaxUCB, a max $k$-armed bandit method to trade off exploring different model
classes and conducting hyperparameter optimization. MaxUCB is specifically
designed for the light-tailed and bounded reward distributions arising in this
setting and, thus, provides an efficient alternative compared to classic max
$k$-armed bandit methods assuming heavy-tailed reward distributions. We
theoretically and empirically evaluate our method on four standard AutoML
benchmarks, demonstrating superior performance over prior approaches.

</details>


### [302] [Latte: Transfering LLMs` Latent-level Knowledge for Few-shot Tabular Learning](https://arxiv.org/pdf/2505.05237)
*Ruxue Shi, Hengrui Gu, Hangting Ye, Yiwei Dai, Xu Shen, Xin Wang*

Main category: cs.LG

TL;DR: Latte is a framework for few-shot tabular learning that leverages LLMs' latent knowledge at training time, improving generalization and reducing overfitting.


<details>
  <summary>Details</summary>
Motivation: Existing methods for few-shot tabular learning using LLMs suffer from latency or unreliable feature engineering. Latte addresses these issues by extracting knowledge during training.

Method: Latte transfers latent prior knowledge from LLMs to optimize downstream models, enabling weighted fusion of feature values and leveraging unlabeled data.

Result: Experiments show Latte outperforms existing methods on few-shot tabular learning benchmarks.

Conclusion: Latte is a state-of-the-art approach for few-shot tabular learning, offering improved performance and compatibility with existing pre-training paradigms.

Abstract: Few-shot tabular learning, in which machine learning models are trained with
a limited amount of labeled data, provides a cost-effective approach to
addressing real-world challenges. The advent of Large Language Models (LLMs)
has sparked interest in leveraging their pre-trained knowledge for few-shot
tabular learning. Despite promising results, existing approaches either rely on
test-time knowledge extraction, which introduces undesirable latency, or
text-level knowledge, which leads to unreliable feature engineering. To
overcome these limitations, we propose Latte, a training-time knowledge
extraction framework that transfers the latent prior knowledge within LLMs to
optimize a more generalized downstream model. Latte enables general
knowledge-guided downstream tabular learning, facilitating the weighted fusion
of information across different feature values while reducing the risk of
overfitting to limited labeled data. Furthermore, Latte is compatible with
existing unsupervised pre-training paradigms and effectively utilizes available
unlabeled samples to overcome the performance limitations imposed by an
extremely small labeled dataset. Extensive experiments on various few-shot
tabular learning benchmarks demonstrate the superior performance of Latte,
establishing it as a state-of-the-art approach in this domain

</details>


### [303] [Enhancing Treatment Effect Estimation via Active Learning: A Counterfactual Covering Perspective](https://arxiv.org/pdf/2505.05242)
*Hechuan Wen, Tong Chen, Mingming Gong, Li Kheng Chai, Shazia Sadiq, Hongzhi Yin*

Main category: cs.LG

TL;DR: The paper addresses the challenge of treatment effect estimation with limited labeled data by proposing an active learning framework and a greedy algorithm (FCCM) to maximize data efficiency.


<details>
  <summary>Details</summary>
Motivation: High labeling costs for treatment effects (e.g., tumor imaging) limit the availability of labeled data, necessitating methods to optimize data acquisition under budget constraints.

Method: The authors formalize the problem theoretically, introduce key measures (factual and counterfactual covering radius), and propose a greedy algorithm (FCCM) to reduce risk bounds and maximize coverage.

Result: FCCM outperforms baselines on synthetic and semi-synthetic datasets, demonstrating its effectiveness in data-efficient treatment effect estimation.

Conclusion: The proposed FCCM framework provides a practical solution for improving treatment effect estimation with limited labeled data, validated by superior performance in benchmarks.

Abstract: Although numerous complex algorithms for treatment effect estimation have
been developed in recent years, their effectiveness remains limited when
handling insufficiently labeled training sets due to the high cost of labeling
the effect after treatment, e.g., expensive tumor imaging or biopsy procedures
needed to evaluate treatment effects. Therefore, it becomes essential to
actively incorporate more high-quality labeled data, all while adhering to a
constrained labeling budget. To enable data-efficient treatment effect
estimation, we formalize the problem through rigorous theoretical analysis
within the active learning context, where the derived key measures --
\textit{factual} and \textit{counterfactual covering radius} determine the risk
upper bound. To reduce the bound, we propose a greedy radius reduction
algorithm, which excels under an idealized, balanced data distribution. To
generalize to more realistic data distributions, we further propose FCCM, which
transforms the optimization objective into the \textit{Factual} and
\textit{Counterfactual Coverage Maximization} to ensure effective radius
reduction during data acquisition. Furthermore, benchmarking FCCM against other
baselines demonstrates its superiority across both fully synthetic and
semi-synthetic datasets.

</details>


### [304] [MTL-UE: Learning to Learn Nothing for Multi-Task Learning](https://arxiv.org/pdf/2505.05279)
*Yi Yu, Song Xia, Siyuan Yang, Chenqi Kong, Wenhan Yang, Shijian Lu, Yap-Peng Tan, Alex C. Kot*

Main category: cs.LG

TL;DR: MTL-UE is the first framework for creating unlearnable examples in multi-task learning, outperforming existing methods by using a generator-based approach with label priors and feature embeddings.


<details>
  <summary>Details</summary>
Motivation: Existing unlearnable strategies neglect multi-task learning (MTL), which is crucial for generalist models. MTL-UE addresses this gap.

Method: MTL-UE uses a generator-based structure with label priors, class-wise embeddings, and intra/inter-task regularization for robust attacks.

Result: MTL-UE excels in attacking performance across 4 datasets, 3 UE methods, 5 backbones, and 5 task-weighting strategies.

Conclusion: MTL-UE is a versatile, plug-and-play solution for unlearnable examples in MTL, significantly enhancing attack robustness.

Abstract: Most existing unlearnable strategies focus on preventing unauthorized users
from training single-task learning (STL) models with personal data.
Nevertheless, the paradigm has recently shifted towards multi-task data and
multi-task learning (MTL), targeting generalist and foundation models that can
handle multiple tasks simultaneously. Despite their growing importance, MTL
data and models have been largely neglected while pursuing unlearnable
strategies. This paper presents MTL-UE, the first unified framework for
generating unlearnable examples for multi-task data and MTL models. Instead of
optimizing perturbations for each sample, we design a generator-based structure
that introduces label priors and class-wise feature embeddings which leads to
much better attacking performance. In addition, MTL-UE incorporates intra-task
and inter-task embedding regularization to increase inter-class separation and
suppress intra-class variance which enhances the attack robustness greatly.
Furthermore, MTL-UE is versatile with good supports for dense prediction tasks
in MTL. It is also plug-and-play allowing integrating existing
surrogate-dependent unlearnable methods with little adaptation. Extensive
experiments show that MTL-UE achieves superior attacking performance
consistently across 4 MTL datasets, 3 base UE methods, 5 model backbones, and 5
MTL task-weighting strategies.

</details>


### [305] [Scalable Chain of Thoughts via Elastic Reasoning](https://arxiv.org/pdf/2505.05315)
*Yuhui Xu, Hanze Dong, Lei Wang, Doyen Sahoo, Junnan Li, Caiming Xiong*

Main category: cs.LG

TL;DR: Elastic Reasoning splits reasoning into thinking and solution phases with independent budgets, improving reliability under constraints and reducing training costs.


<details>
  <summary>Details</summary>
Motivation: LRMs face challenges with uncontrolled output lengths under strict resource constraints, necessitating a scalable solution.

Method: Proposes Elastic Reasoning with two-phase reasoning (thinking and solution) and a budget-constrained rollout strategy integrated into GRPO.

Result: Outperforms baselines under strict budget constraints, reduces training costs, and produces concise reasoning even in unconstrained settings.

Conclusion: Elastic Reasoning provides a practical solution for scalable and controllable reasoning.

Abstract: Large reasoning models (LRMs) have achieved remarkable progress on complex
tasks by generating extended chains of thought (CoT). However, their
uncontrolled output lengths pose significant challenges for real-world
deployment, where inference-time budgets on tokens, latency, or compute are
strictly constrained. We propose Elastic Reasoning, a novel framework for
scalable chain of thoughts that explicitly separates reasoning into two
phases--thinking and solution--with independently allocated budgets. At test
time, Elastic Reasoning prioritize that completeness of solution segments,
significantly improving reliability under tight resource constraints. To train
models that are robust to truncated thinking, we introduce a lightweight
budget-constrained rollout strategy, integrated into GRPO, which teaches the
model to reason adaptively when the thinking process is cut short and
generalizes effectively to unseen budget constraints without additional
training. Empirical results on mathematical (AIME, MATH500) and programming
(LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning
performs robustly under strict budget constraints, while incurring
significantly lower training cost than baseline methods. Remarkably, our
approach also produces more concise and efficient reasoning even in
unconstrained settings. Elastic Reasoning offers a principled and practical
solution to the pressing challenge of controllable reasoning at scale.

</details>


### [306] [Performance Estimation in Binary Classification Using Calibrated Confidence](https://arxiv.org/pdf/2505.05295)
*Juhani Kivimäki, Jakub Białek, Wojtek Kuberski, Jukka K. Nurminen*

Main category: cs.LG

TL;DR: CBPE is a novel method for estimating binary classification metrics (e.g., accuracy, precision, recall, F1) without ground truth labels, using calibrated confidence scores and treating the confusion matrix as random variables.


<details>
  <summary>Details</summary>
Motivation: Ground truth labels are often unavailable for model performance monitoring, limiting traditional methods. CBPE addresses this gap by enabling label-free estimation of key metrics.

Method: CBPE treats confusion matrix elements as random variables, leveraging calibrated confidence scores to estimate their distributions and derive metric probability distributions.

Result: CBPE provides theoretically guaranteed estimates with valid confidence intervals for metrics like accuracy, precision, recall, and F1.

Conclusion: CBPE fills a critical gap in model monitoring by enabling label-free estimation of diverse binary classification metrics, supported by strong theoretical guarantees.

Abstract: Model monitoring is a critical component of the machine learning lifecycle,
safeguarding against undetected drops in the model's performance after
deployment. Traditionally, performance monitoring has required access to ground
truth labels, which are not always readily available. This can result in
unacceptable latency or render performance monitoring altogether impossible.
Recently, methods designed to estimate the accuracy of classifier models
without access to labels have shown promising results. However, there are
various other metrics that might be more suitable for assessing model
performance in many cases. Until now, none of these important metrics has
received similar interest from the scientific community. In this work, we
address this gap by presenting CBPE, a novel method that can estimate any
binary classification metric defined using the confusion matrix. In particular,
we choose four metrics from this large family: accuracy, precision, recall, and
F$_1$, to demonstrate our method. CBPE treats the elements of the confusion
matrix as random variables and leverages calibrated confidence scores of the
model to estimate their distributions. The desired metric is then also treated
as a random variable, whose full probability distribution can be derived from
the estimated confusion matrix. CBPE is shown to produce estimates that come
with strong theoretical guarantees and valid confidence intervals.

</details>


### [307] [Nearly Optimal Sample Complexity for Learning with Label Proportions](https://arxiv.org/pdf/2505.05355)
*Robert Busa-Fekete, Travis Dick, Claudio Gentile, Haim Kaplan, Tomer Koren, Uri Stemmer*

Main category: cs.LG

TL;DR: The paper explores Learning from Label Proportions (LLP), focusing on sample complexity under square loss and proposing algorithmic solutions with improved performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of learning from grouped examples with only aggregate label values, aiming for individual-level accuracy despite partial observability.

Method: The method involves variants of Empirical Risk Minimization and Stochastic Gradient Descent, enhanced with ad hoc variance reduction techniques.

Result: The results show an optimal sample complexity and improved empirical performance (better accuracy with fewer samples) compared to existing baselines.

Conclusion: The paper concludes with theoretical advancements in LLP and validated algorithmic improvements, outperforming recent baselines.

Abstract: We investigate Learning from Label Proportions (LLP), a partial information
setting where examples in a training set are grouped into bags, and only
aggregate label values in each bag are available. Despite the partial
observability, the goal is still to achieve small regret at the level of
individual examples. We give results on the sample complexity of LLP under
square loss, showing that our sample complexity is essentially optimal. From an
algorithmic viewpoint, we rely on carefully designed variants of Empirical Risk
Minimization, and Stochastic Gradient Descent algorithms, combined with ad hoc
variance reduction techniques. On one hand, our theoretical results improve in
important ways on the existing literature on LLP, specifically in the way the
sample complexity depends on the bag size. On the other hand, we validate our
algorithmic solutions on several datasets, demonstrating improved empirical
performance (better accuracy for less samples) against recent baselines.

</details>


### [308] [Denoising Diffusion Probabilistic Models for Coastal Inundation Forecasting](https://arxiv.org/pdf/2505.05381)
*Kazi Ashik Islam, Zakaria Mehrab, Mahantesh Halappanavar, Henning Mortveit, Sridhar Katragadda, Jon Derek Loftis, Madhav Marathe*

Main category: cs.LG

TL;DR: DIFF-FLOOD is a probabilistic spatiotemporal forecasting method using denoising diffusion models for coastal flooding, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Coastal flooding risks require fast, accurate forecasting to mitigate damage.

Method: Uses denoising diffusion models with spatial (neighboring inundation levels, elevation data) and temporal (inundation history, co-variates) contexts, employing CNNs and cross-attention.

Result: Outperforms existing methods by 6% to 64% in performance metrics and shows better scalability.

Conclusion: DIFF-FLOOD is effective for coastal inundation forecasting, offering improved accuracy and scalability.

Abstract: Coastal flooding poses significant risks to communities, necessitating fast
and accurate forecasting methods to mitigate potential damage. To approach this
problem, we present DIFF-FLOOD, a probabilistic spatiotemporal forecasting
method designed based on denoising diffusion models. DIFF-FLOOD predicts
inundation level at a location by taking both spatial and temporal context into
account. It utilizes inundation levels at neighboring locations and digital
elevation data as spatial context. Inundation history from a context time
window, together with additional co-variates are used as temporal context.
Convolutional neural networks and cross-attention mechanism are then employed
to capture the spatiotemporal dynamics in the data. We trained and tested
DIFF-FLOOD on coastal inundation data from the Eastern Shore of Virginia, a
region highly impacted by coastal flooding. Our results show that, DIFF-FLOOD
outperforms existing forecasting methods in terms of prediction performance (6%
to 64% improvement in terms of two performance metrics) and scalability.

</details>


### [309] [CART-ELC: Oblique Decision Tree Induction via Exhaustive Search](https://arxiv.org/pdf/2505.05402)
*Andrew D. Laack*

Main category: cs.LG

TL;DR: CART-ELC is a new algorithm for oblique decision trees that improves classification accuracy and interpretability on small datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional oblique decision tree methods face computational challenges due to exhaustive search, limiting their exploration.

Method: CART-ELC performs an exhaustive search on a restricted set of hyperplanes to induce oblique decision trees.

Result: CART-ELC achieves competitive performance on small datasets, with significant accuracy improvements and simpler, more interpretable trees.

Conclusion: CART-ELC offers a practical solution for oblique decision trees, balancing performance and interpretability.

Abstract: Oblique decision trees have attracted attention due to their potential for
improved classification performance over traditional axis-aligned decision
trees. However, methods that rely on exhaustive search to find oblique splits
face computational challenges. As a result, they have not been widely explored.
We introduce a novel algorithm, Classification and Regression Tree - Exhaustive
Linear Combinations (CART-ELC), for inducing oblique decision trees that
performs an exhaustive search on a restricted set of hyperplanes. We then
investigate the algorithm's computational complexity and its predictive
capabilities. Our results demonstrate that CART-ELC consistently achieves
competitive performance on small datasets, often yielding statistically
significant improvements in classification accuracy relative to existing
decision tree induction algorithms, while frequently producing shallower,
simpler, and thus more interpretable trees.

</details>


### [310] [Hide & Seek: Transformer Symmetries Obscure Sharpness & Riemannian Geometry Finds It](https://arxiv.org/pdf/2505.05409)
*Marvin F. da Silva, Felix Dangel, Sageev Oore*

Main category: cs.LG

TL;DR: The paper redefines sharpness for transformers by accounting for their symmetries, proposing a geodesic-based measure that correlates strongly with generalization.


<details>
  <summary>Details</summary>
Motivation: Existing sharpness measures fail for transformers due to their rich symmetries, necessitating a new approach to predict generalization accurately.

Method: The authors redefine sharpness on a quotient manifold to remove symmetry ambiguities and propose a geodesic-based sharpness measure, approximating geodesics for practical use.

Result: The geodesic sharpness measure shows strong correlation with generalization for transformers on text and image tasks, outperforming existing methods.

Conclusion: Accounting for transformer symmetries in sharpness measures is crucial for predicting generalization, with geodesic-based approaches proving effective.

Abstract: The concept of sharpness has been successfully applied to traditional
architectures like MLPs and CNNs to predict their generalization. For
transformers, however, recent work reported weak correlation between flatness
and generalization. We argue that existing sharpness measures fail for
transformers, because they have much richer symmetries in their attention
mechanism that induce directions in parameter space along which the network or
its loss remain identical. We posit that sharpness must account fully for these
symmetries, and thus we redefine it on a quotient manifold that results from
quotienting out the transformer symmetries, thereby removing their ambiguities.
Leveraging tools from Riemannian geometry, we propose a fully general notion of
sharpness, in terms of a geodesic ball on the symmetry-corrected quotient
manifold. In practice, we need to resort to approximating the geodesics. Doing
so up to first order yields existing adaptive sharpness measures, and we
demonstrate that including higher-order terms is crucial to recover correlation
with generalization. We present results on diagonal networks with synthetic
data, and show that our geodesic sharpness reveals strong correlation for
real-world transformers on both text and image classification tasks.

</details>


### [311] [DPQ-HD: Post-Training Compression for Ultra-Low Power Hyperdimensional Computing](https://arxiv.org/pdf/2505.05413)
*Nilesh Prasad Pandey, Shriniwas Kulkarni, David Wang, Onat Gungor, Flavio Ponzina, Tajana Rosing*

Main category: cs.LG

TL;DR: DPQ-HD is a novel post-training compression algorithm for Hyperdimensional Computing (HDC) that combines decomposition, pruning, and quantization to reduce computational and memory demands without retraining, achieving near floating-point performance and faster inference.


<details>
  <summary>Details</summary>
Motivation: Current HDC-based applications require high-precision models or encoding matrices, leading to high computational and memory costs, especially for low-power devices. Existing compression methods often need retraining, which is impractical.

Method: DPQ-HD integrates decomposition, pruning, and quantization to compress HDC systems post-training. It also uses progressive similarity evaluation and early exit for energy-efficient inference.

Result: DPQ-HD reduces memory by 20-100x with only a 1-2% accuracy drop. It outperforms existing post-training methods and matches retraining-based techniques while requiring 100x less optimization time and 56x faster inference.

Conclusion: DPQ-HD offers a practical, efficient solution for compressing HDC systems, enabling high performance on resource-constrained devices without retraining.

Abstract: Hyperdimensional Computing (HDC) is emerging as a promising approach for edge
AI, offering a balance between accuracy and efficiency. However, current
HDC-based applications often rely on high-precision models and/or encoding
matrices to achieve competitive performance, which imposes significant
computational and memory demands, especially for ultra-low power devices. While
recent efforts use techniques like precision reduction and pruning to increase
the efficiency, most require retraining to maintain performance, making them
expensive and impractical. To address this issue, we propose a novel Post
Training Compression algorithm, Decomposition-Pruning-Quantization (DPQ-HD),
which aims at compressing the end-to-end HDC system, achieving near floating
point performance without the need of retraining. DPQ-HD reduces computational
and memory overhead by uniquely combining the above three compression
techniques and efficiently adapts to hardware constraints. Additionally, we
introduce an energy-efficient inference approach that progressively evaluates
similarity scores such as cosine similarity and performs early exit to reduce
the computation, accelerating prediction inference while maintaining accuracy.
We demonstrate that DPQ-HD achieves up to 20-100x reduction in memory for image
and graph classification tasks with only a 1-2% drop in accuracy compared to
uncompressed workloads. Lastly, we show that DPQ-HD outperforms the existing
post-training compression methods and performs better or at par with
retraining-based state-of-the-art techniques, requiring significantly less
overall optimization time (up to 100x) and faster inference (up to 56x) on a
microcontroller

</details>


### [312] [RL-DAUNCE: Reinforcement Learning-Driven Data Assimilation with Uncertainty-Aware Constrained Ensembles](https://arxiv.org/pdf/2505.05452)
*Pouria Behnoudfar, Nan Chen*

Main category: cs.LG

TL;DR: RL-DAUNCE is a reinforcement learning-based method for data assimilation that outperforms traditional methods like EnKF by incorporating physical constraints, uncertainty quantification, and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance data assimilation by leveraging reinforcement learning's sequential decision-making framework, which aligns with the iterative nature of data assimilation and addresses limitations of supervised learning.

Method: RL-DAUNCE uses ensemble members as agents, enforces physical constraints via primal-dual optimization, and respects state variable bounds by constraining the RL action space.

Result: RL-DAUNCE outperforms the standard EnKF, matches constrained EnKF's performance, and excels in recovering intermittent signals and capturing extreme events with less computational effort.

Conclusion: RL-DAUNCE offers a physically consistent, efficient, and superior alternative to traditional data assimilation methods, particularly for complex, non-Gaussian phenomena.

Abstract: Machine learning has become a powerful tool for enhancing data assimilation.
While supervised learning remains the standard method, reinforcement learning
(RL) offers unique advantages through its sequential decision-making framework,
which naturally fits the iterative nature of data assimilation by dynamically
balancing model forecasts with observations. We develop RL-DAUNCE, a new
RL-based method that enhances data assimilation with physical constraints
through three key aspects. First, RL-DAUNCE inherits the computational
efficiency of machine learning while it uniquely structures its agents to
mirror ensemble members in conventional data assimilation methods. Second,
RL-DAUNCE emphasizes uncertainty quantification by advancing multiple ensemble
members, moving beyond simple mean-state optimization. Third, RL-DAUNCE's
ensemble-as-agents design facilitates the enforcement of physical constraints
during the assimilation process, which is crucial to improving the state
estimation and subsequent forecasting. A primal-dual optimization strategy is
developed to enforce constraints, which dynamically penalizes the reward
function to ensure constraint satisfaction throughout the learning process.
Also, state variable bounds are respected by constraining the RL action space.
Together, these features ensure physical consistency without sacrificing
efficiency. RL-DAUNCE is applied to the Madden-Julian Oscillation, an
intermittent atmospheric phenomenon characterized by strongly non-Gaussian
features and multiple physical constraints. RL-DAUNCE outperforms the standard
ensemble Kalman filter (EnKF), which fails catastrophically due to the
violation of physical constraints. Notably, RL-DAUNCE matches the performance
of constrained EnKF, particularly in recovering intermittent signals, capturing
extreme events, and quantifying uncertainties, while requiring substantially
less computational effort.

</details>


### [313] [Connecting NTK and NNGP: A Unified Theoretical Framework for Wide Neural Network Learning Dynamics](https://arxiv.org/pdf/2309.04522)
*Yehonatan Avidan, Qianyi Li, Haim Sompolinsky*

Main category: cs.LG

TL;DR: The paper unifies NTK and NNGP theories for wide neural networks, introducing a Neural Dynamical Kernel (NDK) and identifying two learning phases: gradient-driven and diffusive.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between NTK and NNGP theories and provide a comprehensive framework for understanding deep wide neural networks and biological circuit dynamics.

Method: Constructs an analytical theory using gradient descent with noise in an ensemble of wide networks, introducing the NDK to derive NTK and NNGP kernels.

Result: Identifies two learning phases with distinct dynamics and time scales, influenced by initialization and noise variances, and explains representational drift in biological circuits.

Conclusion: The work unifies NTK and NNGP, offering insights into neural network learning and biological dynamics.

Abstract: Artificial neural networks have revolutionized machine learning in recent
years, but a complete theoretical framework for their learning process is still
lacking. Substantial advances were achieved for wide networks, within two
disparate theoretical frameworks: the Neural Tangent Kernel (NTK), which
assumes linearized gradient descent dynamics, and the Bayesian Neural Network
Gaussian Process (NNGP). We unify these two theories using gradient descent
learning with an additional noise in an ensemble of wide deep networks. We
construct an analytical theory for the network input-output function and
introduce a new time-dependent Neural Dynamical Kernel (NDK) from which both
NTK and NNGP kernels are derived. We identify two learning phases: a
gradient-driven learning phase, dominated by loss minimization, in which the
time scale is governed by the initialization variance. It is followed by a slow
diffusive learning stage, where the parameters sample the solution space, with
a time constant decided by the noise and the Bayesian prior variance. The two
variance parameters strongly affect the performance in the two regimes,
especially in sigmoidal neurons. In contrast to the exponential convergence of
the mean predictor in the initial phase, the convergence to the equilibrium is
more complex and may behave nonmonotonically. By characterizing the diffusive
phase, our work sheds light on representational drift in the brain, explaining
how neural activity changes continuously without degrading performance, either
by ongoing gradient signals that synchronize the drifts of different synapses
or by architectural biases that generate task-relevant information that is
robust against the drift process. This work closes the gap between the NTK and
NNGP theories, providing a comprehensive framework for the learning process of
deep wide neural networks and for analyzing dynamics in biological circuits.

</details>


### [314] [DyCE: Dynamically Configurable Exiting for Deep Learning Compression and Real-time Scaling](https://arxiv.org/pdf/2403.01695)
*Qingyuan Wang, Barry Cardiff, Antoine Frappé, Benoit Larras, Deepu John*

Main category: cs.LG

TL;DR: DyCE is a dynamic system for DL models that adjusts computation based on sample complexity, reducing computational costs with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Existing DL models lack adaptability to sample difficulty and real-time demands, limiting efficiency and generalizability.

Method: DyCE adds small exit networks to intermediate layers, enabling early termination and decoupling dynamic model design for broader use.

Result: DyCE reduces computational complexity by 23.5% for ResNet152 and 25.9% for ConvNextv2-tiny on ImageNet, with <0.5% accuracy loss.

Conclusion: DyCE offers a flexible, efficient solution for dynamic DL model compression and scaling, adaptable to real-time needs.

Abstract: Conventional deep learning (DL) model compression and scaling methods focus
on altering the model's components, impacting the results across all samples
uniformly. However, since samples vary in difficulty, a dynamic model that
adapts computation based on sample complexity offers a novel perspective for
compression and scaling. Despite this potential, existing dynamic models are
typically monolithic and model-specific, limiting their generalizability as
broad compression and scaling methods. Additionally, most deployed DL systems
are fixed, unable to adjust their scale once deployed and, therefore, cannot
adapt to the varying real-time demands. This paper introduces DyCE, a
dynamically configurable system that can adjust the performance-complexity
trade-off of a DL model at runtime without requiring re-initialization or
redeployment on inference hardware. DyCE achieves this by adding small exit
networks to intermediate layers of the original model, allowing computation to
terminate early if acceptable results are obtained. DyCE also decouples the
design of an efficient dynamic model, facilitating easy adaptation to new base
models and potential general use in compression and scaling. We also propose
methods for generating optimized configurations and determining the types and
positions of exit networks to achieve desired performance and complexity
trade-offs. By enabling simple configuration switching, DyCE provides
fine-grained performance tuning in real-time. We demonstrate the effectiveness
of DyCE through image classification tasks using deep convolutional neural
networks (CNNs). DyCE significantly reduces computational complexity by 23.5%
for ResNet152 and 25.9% for ConvNextv2-tiny on ImageNet, with accuracy
reductions of less than 0.5%.

</details>


### [315] [HORAE: A Domain-Agnostic Language for Automated Service Regulation](https://arxiv.org/pdf/2406.06600)
*Yutao Sun, Mingshuai Chen, Tiancheng Zhao, Kangjia Zhao, He Li, Jintao Chen, Zhongyi Wang, Liqiang Lu, Xinkui Zhao, Shuiguang Deng, Jianwei Yin*

Main category: cs.LG

TL;DR: Horae is a unified language for modeling regulation rules across domains, enhanced by RuleGPT, an AI that automates the process, outperforming GPT-3.5 and matching GPT-4.


<details>
  <summary>Details</summary>
Motivation: Existing AI-based regulation techniques are domain-specific and lack generalization. Horae aims to unify and automate regulation modeling.

Method: Developed Horae, a specification language, and RuleGPT, a fine-tuned LLM, to automate the modeling process.

Result: RuleGPT (7B parameters) outperforms GPT-3.5 and matches GPT-4 in real-world regulation benchmarks.

Conclusion: Horae and RuleGPT provide an effective, automated framework for intelligent service regulation across diverse domains.

Abstract: Artificial intelligence is rapidly encroaching on the field of service
regulation. However, existing AI-based regulation techniques are often tailored
to specific application domains and thus are difficult to generalize in an
automated manner. This paper presents Horae, a unified specification language
for modeling (multimodal) regulation rules across a diverse set of domains. We
showcase how Horae facilitates an intelligent service regulation pipeline by
further exploiting a fine-tuned large language model named RuleGPT that
automates the Horae modeling process, thereby yielding an end-to-end framework
for fully automated intelligent service regulation. The feasibility and
effectiveness of our framework are demonstrated over a benchmark of various
real-world regulation domains. In particular, we show that our open-sourced,
fine-tuned RuleGPT with 7B parameters suffices to outperform GPT-3.5 and
perform on par with GPT-4o.

</details>


### [316] [Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for LLM and RAG Systems](https://arxiv.org/pdf/2502.18635)
*Matthew Barker, Andrew Bell, Evan Thomas, James Carr, Thomas Andrews, Umang Bhatt*

Main category: cs.LG

TL;DR: The paper introduces a method for multi-objective optimization in RAG and LLM systems, addressing challenges like large solution spaces and noisy evaluations. Bayesian optimization outperforms baselines, and findings emphasize task-specific optimal configurations.


<details>
  <summary>Details</summary>
Motivation: The need to optimize multiple objectives (cost, latency, safety, alignment) in RAG and LLM systems, which is under-explored due to complexity and high evaluation costs.

Method: Proposes Bayesian optimization for multi-objective parameter optimization across LLM and RAG systems, tested on new RAG benchmarks.

Result: Bayesian optimization outperforms baselines, achieving a superior Pareto front, but optimal configurations are task-specific.

Conclusion: Practitioners should consider task-specific nuances when designing multi-objective RAG systems, as optimal configurations may not generalize.

Abstract: While Retrieval Augmented Generation (RAG) has emerged as a popular technique
for improving Large Language Model (LLM) systems, it introduces a large number
of choices, parameters and hyperparameters that must be made or tuned. This
includes the LLM, embedding, and ranker models themselves, as well as
hyperparameters governing individual RAG components. Yet, collectively
optimizing the entire configuration in a RAG or LLM system remains
under-explored - especially in multi-objective settings - due to intractably
large solution spaces, noisy objective evaluations, and the high cost of
evaluations. In this work, we introduce the first approach for multi-objective
parameter optimization of cost, latency, safety and alignment over entire LLM
and RAG systems. We find that Bayesian optimization methods significantly
outperform baseline approaches, obtaining a superior Pareto front on two new
RAG benchmark tasks. We conclude our work with important considerations for
practitioners who are designing multi-objective RAG systems, highlighting
nuances such as how optimal configurations may not generalize across tasks and
objectives.

</details>


### [317] [Learning to Compare Hardware Designs for High-Level Synthesis](https://arxiv.org/pdf/2409.13138)
*Yunsheng Bai, Atefeh Sohrabizadeh, Zijian Ding, Rongjian Liang, Weikai Li, Ding Wang, Haoxing Ren, Yizhou Sun, Jason Cong*

Main category: cs.LG

TL;DR: CompareXplore is a novel ML-based HLS optimization method that uses hybrid loss and node difference attention to improve design ranking and performance prediction, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional HLS DSE methods struggle with nonlinear relationships and complex interactions between pragmas, leading to suboptimal hardware designs.

Method: CompareXplore combines pairwise preference learning with pointwise performance prediction, using a node difference attention module and two-stage DSE for optimization.

Result: CompareXplore significantly improves ranking metrics and generates high-quality HLS results, outperforming state-of-the-art methods.

Conclusion: CompareXplore effectively addresses HLS optimization challenges by leveraging hybrid learning and focused attention on critical design differences.

Abstract: High-level synthesis (HLS) is an automated design process that transforms
high-level code into hardware designs, enabling the rapid development of
hardware accelerators. HLS relies on pragmas, which are directives inserted
into the source code to guide the synthesis process, and pragmas have various
settings and values that significantly impact the resulting hardware design.
State-of-the-art ML-based HLS methods, such as HARP, first train a deep
learning model, typically based on graph neural networks (GNNs) applied to
graph-based representations of the source code and pragmas. They then perform
design space exploration (DSE) to explore the pragma design space, rank
candidate designs using the model, and return the top designs. However,
traditional DSE methods face challenges due to the highly nonlinear
relationship between pragma settings and performance metrics, along with
complex interactions between pragmas that affect performance in non-obvious
ways.
  To address these challenges, we propose compareXplore, a novel approach that
learns to compare hardware designs for effective HLS optimization.
CompareXplore introduces a hybrid loss function that combines pairwise
preference learning with pointwise performance prediction, enabling the model
to capture both relative preferences and absolute performance. Moreover, we
introduce a novel node difference attention module that focuses on the most
informative differences between designs, enabling the model to identify
critical pragmas impacting performance. CompareXplore adopts a two-stage DSE,
where a pointwise prediction model is used for the initial design pruning,
followed by a pairwise comparison stage for precise performance verification.
In extensive experiments, compareXplore achieves significant improvements in
ranking metrics and generates high-quality HLS results for the selected
designs, outperforming the existing SOTA method.

</details>


### [318] [TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining](https://arxiv.org/pdf/2504.02107)
*Jeffrey Li, Mohammadreza Armandpour, Iman Mirzadeh, Sachin Mehta, Vaishaal Shankar, Raviteja Vemulapalli, Samy Bengio, Oncel Tuzel, Mehrdad Farajtabar, Hadi Pouransari, Fartash Faghri*

Main category: cs.LG

TL;DR: The paper explores evaluation and update methods for outdated LLMs using a large-scale dataset from Common Crawl. It introduces time-stratified evaluations and finds that autoregressive meta-schedules with fixed-ratio replay of old data achieve comparable performance to full retraining, with less computation.


<details>
  <summary>Details</summary>
Motivation: LLMs trained on historical web data become outdated; the study aims to address this by evaluating and updating LLMs as new data becomes available.

Method: Uses a web-scale dataset from 114 Common Crawl dumps for time-continual pretraining. Introduces time-stratified evaluations across general and domain-specific data to test continual learning methods.

Result: Autoregressive meta-schedules with fixed-ratio replay achieve similar performance to full retraining (2.6x less computation). Replay is crucial for generic web data but less so for specific domains.

Conclusion: The study provides efficient methods for updating LLMs, balancing new data incorporation and old data replay, with domain-specific considerations.

Abstract: Large Language Models (LLMs) trained on historical web data inevitably become
outdated. We investigate evaluation strategies and update methods for LLMs as
new data becomes available. We introduce a web-scale dataset for time-continual
pretraining of LLMs derived from 114 dumps of Common Crawl (CC) - orders of
magnitude larger than previous continual language modeling benchmarks. We also
design time-stratified evaluations across both general CC data and specific
domains (Wikipedia, StackExchange, and code documentation) to assess how well
various continual learning methods adapt to new data while retaining past
knowledge. Our findings demonstrate that, on general CC data, autoregressive
meta-schedules combined with a fixed-ratio replay of older data can achieve
comparable held-out loss to re-training from scratch, while requiring
significantly less computation (2.6x). However, the optimal balance between
incorporating new data and replaying old data differs as replay is crucial to
avoid forgetting on generic web data but less so on specific domains.

</details>


### [319] [CATCH: Channel-Aware multivariate Time Series Anomaly Detection via Frequency Patching](https://arxiv.org/pdf/2410.12261)
*Xingjian Wu, Xiangfei Qiu, Zhengyu Li, Yihang Wang, Jilin Hu, Chenjuan Guo, Hui Xiong, Bin Yang*

Main category: cs.LG

TL;DR: CATCH is a frequency-patching framework for anomaly detection in multivariate time series, improving fine-grained frequency capture and channel correlations via a Channel Fusion Module and bi-level optimization.


<details>
  <summary>Details</summary>
Motivation: Existing reconstruction-based methods lack fine-grained frequency characteristics and channel correlation awareness, limiting anomaly detection performance.

Method: CATCH patches the frequency domain into bands and uses a Channel Fusion Module (CFM) with a patch-wise mask generator and masked-attention mechanism, optimized via bi-level multi-objective optimization.

Result: CATCH outperforms state-of-the-art methods on 10 real-world and 12 synthetic datasets.

Conclusion: CATCH effectively addresses limitations in anomaly detection by enhancing frequency granularity and channel correlation perception, achieving superior performance.

Abstract: Anomaly detection in multivariate time series is challenging as heterogeneous
subsequence anomalies may occur. Reconstruction-based methods, which focus on
learning normal patterns in the frequency domain to detect diverse abnormal
subsequences, achieve promising results, while still falling short on capturing
fine-grained frequency characteristics and channel correlations. To contend
with the limitations, we introduce CATCH, a framework based on frequency
patching. We propose to patchify the frequency domain into frequency bands,
which enhances its ability to capture fine-grained frequency characteristics.
To perceive appropriate channel correlations, we propose a Channel Fusion
Module (CFM), which features a patch-wise mask generator and a masked-attention
mechanism. Driven by a bi-level multi-objective optimization algorithm, the CFM
is encouraged to iteratively discover appropriate patch-wise channel
correlations, and to cluster relevant channels while isolating adverse effects
from irrelevant channels. Extensive experiments on 10 real-world datasets and
12 synthetic datasets demonstrate that CATCH achieves state-of-the-art
performance. We make our code and datasets available at
https://github.com/decisionintelligence/CATCH.

</details>


### [320] [Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided GFlowNets](https://arxiv.org/pdf/2504.19981)
*Adam Younsi, Abdalgader Abubaker, Mohamed El Amine Seddik, Hakim Hacid, Salem Lahlou*

Main category: cs.LG

TL;DR: The paper introduces a Process Reward Model (PRM) and adapts Generative Flow Networks (GFlowNets) to improve LLMs' accuracy and diversity in mathematical reasoning, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of evaluating intermediate reasoning steps in LLMs for complex domains like mathematics without costly human annotations.

Method: Develop a PRM using Monte Carlo Tree Search and similarity-based data augmentation, then adapt GFlowNets to sample diverse, high-quality solutions based on PRM rewards.

Result: Improved accuracy (+2.59% on MATH Level 5) and diversity, with generalization to unseen datasets (+9.4% on SAT MATH).

Conclusion: PRM-guided GFlowNets enhance LLMs' robustness and versatility in mathematical reasoning.

Abstract: Achieving both accuracy and diverse reasoning remains challenging for Large
Language Models (LLMs) in complex domains like mathematics. A key bottleneck is
evaluating intermediate reasoning steps to guide generation without costly
human annotations. To address this, we first introduce a novel Process Reward
Model (PRM) trained automatically using Monte Carlo Tree Search coupled with a
similarity-based data augmentation technique, effectively capturing step-level
reasoning quality. Leveraging this PRM, we then adapt Generative Flow Networks
(GFlowNets) to operate at the reasoning step level. Unlike traditional
reinforcement learning focused on maximizing a single reward, GFlowNets
naturally sample diverse, high-quality solutions proportional to their rewards,
as measured by our PRM. Empirical evaluation shows strong improvements in both
accuracy and solution diversity on challenging mathematical benchmarks (e.g.,
+2.59% absolute accuracy on MATH Level 5 for Llama3.2-3B), with effective
generalization to unseen datasets (+9.4% absolute on SAT MATH). Our work
demonstrates the potential of PRM-guided, step-level GFlowNets for developing
more robust and versatile mathematical reasoning in LLMs.

</details>


### [321] [Optimizing LLMs for Resource-Constrained Environments: A Survey of Model Compression Techniques](https://arxiv.org/pdf/2505.02309)
*Sanjay Surendranath Girija, Shashank Kapoor, Lakshit Arora, Dipen Pradhan, Aman Raj, Ankit Shetgaonkar*

Main category: cs.LG

TL;DR: A survey on compressing Large Language Models (LLMs) for efficient edge deployment, covering Knowledge Distillation, Quantization, Pruning, and complementary techniques like mixture-of-experts.


<details>
  <summary>Details</summary>
Motivation: LLMs' high resource demands hinder mobile/edge deployment; this paper reviews compression techniques to address this.

Method: Examines Knowledge Distillation, Model Quantization, and Model Pruning, along with variants and applications.

Result: Provides a comprehensive overview of compression techniques and their successful implementations.

Conclusion: Highlights future directions for optimizing LLMs in resource-constrained environments, aiding researchers and practitioners.

Abstract: Large Language Models (LLMs) have revolutionized many areas of artificial
intelligence (AI), but their substantial resource requirements limit their
deployment on mobile and edge devices. This survey paper provides a
comprehensive overview of techniques for compressing LLMs to enable efficient
inference in resource-constrained environments. We examine three primary
approaches: Knowledge Distillation, Model Quantization, and Model Pruning. For
each technique, we discuss the underlying principles, present different
variants, and provide examples of successful applications. We also briefly
discuss complementary techniques such as mixture-of-experts and early-exit
strategies. Finally, we highlight promising future directions, aiming to
provide a valuable resource for both researchers and practitioners seeking to
optimize LLMs for edge deployment.

</details>


### [322] [Toward Task Generalization via Memory Augmentation in Meta-Reinforcement Learning](https://arxiv.org/pdf/2502.01521)
*Kaixi Bao, Chenhao Li, Yarden As, Andreas Krause, Marco Hutter*

Main category: cs.LG

TL;DR: Memory augmentation in RL improves task generalization by simulating out-of-distribution scenarios and using memory for context-aware adaptation, achieving zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: RL agents often fail in tasks differing from training, limiting broader deployment.

Method: Memory-based RL with task-structured augmentations and memory mechanisms for context-aware policy adaptation.

Result: Achieves zero-shot generalization to unseen tasks, maintains robust in-distribution performance, and is sample-efficient.

Conclusion: Memory augmentation enables effective generalization in RL without additional environment interactions.

Abstract: Agents trained via reinforcement learning (RL) often struggle to perform well
on tasks that differ from those encountered during training. This limitation
presents a challenge to the broader deployment of RL in diverse and dynamic
task settings. In this work, we introduce memory augmentation, a memory-based
RL approach to improve task generalization. Our approach leverages
task-structured augmentations to simulate plausible out-of-distribution
scenarios and incorporates memory mechanisms to enable context-aware policy
adaptation. Trained on a predefined set of tasks, our policy demonstrates the
ability to generalize to unseen tasks through memory augmentation without
requiring additional interactions with the environment. Through extensive
simulation experiments and real-world hardware evaluations on legged locomotion
tasks, we demonstrate that our approach achieves zero-shot generalization to
unseen tasks while maintaining robust in-distribution performance and high
sample efficiency.

</details>


### [323] [PointBA: Towards Backdoor Attacks in 3D Point Cloud](https://arxiv.org/pdf/2103.16074)
*Xinke Li, Zhirui Chen, Yue Zhao, Zekun Tong, Yabang Zhao, Andrew Lim, Joey Tianyi Zhou*

Main category: cs.LG

TL;DR: The paper explores backdoor attacks in 3D deep learning, proposing two methods (PointPBA and PointCBA) with high success rates, highlighting a serious but underexplored threat.


<details>
  <summary>Details</summary>
Motivation: Backdoor attacks pose a serious threat to 3D deep learning systems, yet remain unexplored compared to adversarial attacks.

Method: Two attack approaches are designed: poison-label (PointPBA) and clean-label (PointCBA), leveraging spatial transformations and feature disentanglement.

Result: PointPBA achieves over 95% success rate; PointCBA, though stealthier, has around 50% success rate.

Conclusion: The work serves as a baseline for improving 3D model robustness against backdoor attacks.

Abstract: 3D deep learning has been increasingly more popular for a variety of tasks
including many safety-critical applications. However, recently several works
raise the security issues of 3D deep models. Although most of them consider
adversarial attacks, we identify that backdoor attack is indeed a more serious
threat to 3D deep learning systems but remains unexplored. We present the
backdoor attacks in 3D point cloud with a unified framework that exploits the
unique properties of 3D data and networks. In particular, we design two attack
approaches on point cloud: the poison-label backdoor attack (PointPBA) and the
clean-label backdoor attack (PointCBA). The first one is straightforward and
effective in practice, while the latter is more sophisticated assuming there
are certain data inspections. The attack algorithms are mainly motivated and
developed by 1) the recent discovery of 3D adversarial samples suggesting the
vulnerability of deep models under spatial transformation; 2) the proposed
feature disentanglement technique that manipulates the feature of the data
through optimization methods and its potential to embed a new task. Extensive
experiments show the efficacy of the PointPBA with over 95% success rate across
various 3D datasets and models, and the more stealthy PointCBA with around 50%
success rate. Our proposed backdoor attack in 3D point cloud is expected to
perform as a baseline for improving the robustness of 3D deep models.

</details>


### [324] [Exploring Learning Complexity for Efficient Downstream Dataset Pruning](https://arxiv.org/pdf/2402.05356)
*Wenyu Jiang, Zhenlong Liu, Zejian Xie, Songxin Zhang, Bingyi Jing, Hongxin Wei*

Main category: cs.LG

TL;DR: The paper introduces Distorting-based Learning Complexity (DLC), a training-free method for dataset pruning, and FlexRand, a flexible under-sampling strategy, to efficiently reduce dataset size while maintaining performance for large-scale pre-trained models.


<details>
  <summary>Details</summary>
Motivation: The high cost of fine-tuning large pre-trained models necessitates efficient dataset pruning methods that avoid full-dataset training.

Method: Proposes DLC, a lightweight hardness score based on Learning Complexity and weights masking, and FlexRand for flexible under-sampling.

Result: DLC reduces pruning time by 35x and achieves state-of-the-art performance with FlexRand in image pruning benchmarks.

Conclusion: DLC and FlexRand offer an efficient and effective solution for dataset pruning in large-scale pre-trained models.

Abstract: The ever-increasing fine-tuning cost of large-scale pre-trained models gives
rise to the importance of dataset pruning, which aims to reduce dataset size
while maintaining task performance. However, existing dataset pruning methods
require training on the entire dataset, which is impractical for large-scale
pre-trained models. In this paper, we propose a straightforward, novel, and
training-free hardness score named Distorting-based Learning Complexity (DLC),
to identify informative images and instructions from the downstream dataset
efficiently. Our method is motivated by the observation that easy samples
learned faster can also be learned with fewer parameters. Specifically, we
define the Learning Complexity to quantify sample hardness and utilize a
lightweight weights masking process for fast estimation, instead of the costly
SGD optimization. Based on DLC, we further design a flexible under-sampling
with randomness (dubbed FlexRand), replacing the top-K strategy, to alleviate
the severe subset distribution shift. Extensive experiments with downstream
image and instructions dataset pruning benchmarks demonstrate the effectiveness
and efficiency of the proposed approach. In the images pruning benchmark, DLC
significantly reduces the pruning time by 35x while establishing
state-of-the-art performance with FlexRand.

</details>


### [325] [Comparing Hyper-optimized Machine Learning Models for Predicting Efficiency Degradation in Organic Solar Cells](https://arxiv.org/pdf/2404.00173)
*David Valiente, Fernando Rodríguez-Mas, Juan V. Alegre-Requena, David Dalmau, María Flores, Juan C. Ferrer*

Main category: cs.LG

TL;DR: The paper presents optimal ML models to predict the degradation of organic solar cells' power conversion efficiency, achieving high accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: To address the temporal degradation of polymeric organic solar cells (OSCs) by developing accurate predictive models.

Method: A database of 996 entries with 7 variables was used. Automated ML protocols optimized models via exhaustive benchmarking.

Result: High accuracy (R2 > 0.90, RMSE/SSE/MAE <1% PCE) and validated models for unseen OSCs (R2 ~0.96-0.97, RMSE ~1%).

Conclusion: ML models outperform classical Bayesian regression, offering reliable predictions and insights into OSC performance and stability.

Abstract: This work presents a set of optimal machine learning (ML) models to represent
the temporal degradation suffered by the power conversion efficiency (PCE) of
polymeric organic solar cells (OSCs) with a multilayer structure
ITO/PEDOT:PSS/P3HT:PCBM/Al. To that aim, we generated a database with 996
entries, which includes up to 7 variables regarding both the manufacturing
process and environmental conditions for more than 180 days. Then, we relied on
a software framework that brings together a conglomeration of automated ML
protocols that execute sequentially against our database by simply command-line
interface. This easily permits hyper-optimizing and randomizing seeds of the ML
models through exhaustive benchmarking so that optimal models are obtained. The
accuracy achieved reaches values of the coefficient determination (R2) widely
exceeding 0.90, whereas the root mean squared error (RMSE), sum of squared
error (SSE), and mean absolute error (MAE)>1% of the target value, the PCE.
Additionally, we contribute with validated models able to screen the behavior
of OSCs never seen in the database. In that case, R2~0.96-0.97 and RMSE~1%,
thus confirming the reliability of the proposal to predict. For comparative
purposes, classical Bayesian regression fitting based on non-linear mean
squares (LMS) are also presented, which only perform sufficiently for
univariate cases of single OSCs. Hence they fail to outperform the breadth of
the capabilities shown by the ML models. Finally, thanks to the standardized
results offered by the ML framework, we study the dependencies between the
variables of the dataset and their implications for the optimal performance and
stability of the OSCs. Reproducibility is ensured by a standardized report
altogether with the dataset, which are publicly available at Github.

</details>


### [326] [Correcting Noisy Multilabel Predictions: Modeling Label Noise through Latent Space Shifts](https://arxiv.org/pdf/2502.14281)
*Weipeng Huang, Qin Li, Yang Xiao, Cheng Qiao, Tie Cai, Junwei Liang, Neil J. Hurley, Guangyuan Piao*

Main category: cs.LG

TL;DR: The paper addresses noisy label learning in multilabel classifications using deep generative approaches for post-correction of predictions, showing improved performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Noise in data features and labels causes overfitting; the paper focuses on noisy label learning in multilabel classifications, a less explored area.

Method: Uses deep generative models to estimate uncertainty, modeling label noise as a stochastic shift in latent variables. Develops unsupervised and semi-supervised learning methods.

Result: The approach consistently improves independent models and outperforms existing methods across noisy label settings. Robustness is validated through sensitivity analysis and ablation studies.

Conclusion: The proposed method effectively handles noisy labels in multilabel classifications, offering computational efficiency and potential for further improvements when combined with other techniques.

Abstract: Noise in data appears to be inevitable in most real-world machine learning
applications and would cause severe overfitting problems. Not only can data
features contain noise, but labels are also prone to be noisy due to human
input. In this paper, rather than noisy label learning in multiclass
classifications, we instead focus on the less explored area of noisy label
learning for multilabel classifications. Specifically, we investigate the
post-correction of predictions generated from classifiers learned with noisy
labels. The reasons are two-fold. Firstly, this approach can directly work with
the trained models to save computational resources. Secondly, it could be
applied on top of other noisy label correction techniques to achieve further
improvements. To handle this problem, we appeal to deep generative approaches
that are possible for uncertainty estimation. Our model posits that label noise
arises from a stochastic shift in the latent variable, providing a more robust
and beneficial means for noisy learning. We develop both unsupervised and
semi-supervised learning methods for our model. The extensive empirical study
presents solid evidence to that our approach is able to consistently improve
the independent models and performs better than a number of existing methods
across various noisy label settings. Moreover, a comprehensive empirical
analysis of the proposed method is carried out to validate its robustness,
including sensitivity analysis and an ablation study, among other elements.

</details>


### [327] [Information-Theoretic Generalization Bounds for Deep Neural Networks](https://arxiv.org/pdf/2404.03176)
*Haiyun He, Ziv Goldfeld*

Main category: cs.LG

TL;DR: The paper explores how depth in DNNs aids generalization using information-theoretic bounds, showing deeper networks can generalize better under certain conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the role of depth in DNNs for supervised learning and quantify its impact on generalization.

Method: Derives hierarchical bounds on generalization error using KL divergence and 1-Wasserstein distance, analyzes SDPI coefficients for regularized DNNs, and tests on binary Gaussian classification.

Result: Deeper networks can generalize better, with bounds shrinking as layers increase and a generalization funnel layer identified.

Conclusion: Depth improves generalization, especially in narrower architectures, but broader applicability remains open.

Abstract: Deep neural networks (DNNs) exhibit an exceptional capacity for
generalization in practical applications. This work aims to capture the effect
and benefits of depth for supervised learning via information-theoretic
generalization bounds. We first derive two hierarchical bounds on the
generalization error in terms of the Kullback-Leibler (KL) divergence or the
1-Wasserstein distance between the train and test distributions of the network
internal representations. The KL divergence bound shrinks as the layer index
increases, while the Wasserstein bound implies the existence of a layer that
serves as a generalization funnel, which attains a minimal 1-Wasserstein
distance. Analytic expressions for both bounds are derived under the setting of
binary Gaussian classification with linear DNNs. To quantify the contraction of
the relevant information measures when moving deeper into the network, we
analyze the strong data processing inequality (SDPI) coefficient between
consecutive layers of three regularized DNN models: $\mathsf{Dropout}$,
$\mathsf{DropConnect}$, and Gaussian noise injection. This enables refining our
generalization bounds to capture the contraction as a function of the network
architecture parameters. Specializing our results to DNNs with a finite
parameter space and the Gibbs algorithm reveals that deeper yet narrower
network architectures generalize better in those examples, although how broadly
this statement applies remains a question.

</details>


### [328] [A Probabilistic Approach to Learning the Degree of Equivariance in Steerable CNNs](https://arxiv.org/pdf/2406.03946)
*Lars Veefkind, Gabriele Cesa*

Main category: cs.LG

TL;DR: The paper introduces a probabilistic method to learn the degree of equivariance in steerable CNNs, addressing overconstrained weights due to unknown symmetries.


<details>
  <summary>Details</summary>
Motivation: Unknown or varying symmetries in SCNNs can lead to overconstrained weights and reduced performance, necessitating a method to learn equivariance adaptively.

Method: A probabilistic approach parameterizes equivariance as a likelihood distribution over transformation groups using Fourier coefficients, allowing layer-wise or shared equivariance.

Result: Experiments show competitive performance on datasets with mixed symmetries, with learned distributions accurately reflecting equivariance.

Conclusion: The method flexibly learns equivariance for any subgroup of compact groups, enhancing SCNN adaptability without extra layers.

Abstract: Steerable convolutional neural networks (SCNNs) enhance task performance by
modelling geometric symmetries through equivariance constraints on weights.
Yet, unknown or varying symmetries can lead to overconstrained weights and
decreased performance. To address this, this paper introduces a probabilistic
method to learn the degree of equivariance in SCNNs. We parameterise the degree
of equivariance as a likelihood distribution over the transformation group
using Fourier coefficients, offering the option to model layer-wise and shared
equivariance. These likelihood distributions are regularised to ensure an
interpretable degree of equivariance across the network. Advantages include the
applicability to many types of equivariant networks through the flexible
framework of SCNNs and the ability to learn equivariance with respect to any
subgroup of any compact group without requiring additional layers. Our
experiments reveal competitive performance on datasets with mixed symmetries,
with learnt likelihood distributions that are representative of the underlying
degree of equivariance.

</details>


### [329] [Noise-Aware Differentially Private Regression via Meta-Learning](https://arxiv.org/pdf/2406.08569)
*Ossi Räisä, Stratis Markou, Matthew Ashman, Wessel P. Bruinsma, Marlon Tobaben, Antti Honkela, Richard E. Turner*

Main category: cs.LG

TL;DR: A meta-learning model, DPConvCNP, combines ConvCNP with a functional DP mechanism to provide accurate, private predictions, outperforming DP Gaussian Processes.


<details>
  <summary>Details</summary>
Motivation: High-stakes applications need private, accurate, and well-calibrated machine learning models, but standard DP mechanisms often degrade performance.

Method: Pre-train a meta-learning model (DPConvCNP) on simulated data, using ConvCNP and an improved functional DP mechanism, to map private data to predictions in one forward pass.

Result: DPConvCNP outperforms a DP Gaussian Process baseline, especially on non-Gaussian data, and is faster with less tuning.

Conclusion: DPConvCNP offers a promising solution for private, accurate, and efficient predictions in high-stakes applications.

Abstract: Many high-stakes applications require machine learning models that protect
user privacy and provide well-calibrated, accurate predictions. While
Differential Privacy (DP) is the gold standard for protecting user privacy,
standard DP mechanisms typically significantly impair performance. One approach
to mitigating this issue is pre-training models on simulated data before DP
learning on the private data. In this work we go a step further, using
simulated data to train a meta-learning model that combines the Convolutional
Conditional Neural Process (ConvCNP) with an improved functional DP mechanism
of Hall et al. [2013] yielding the DPConvCNP. DPConvCNP learns from simulated
data how to map private data to a DP predictive model in one forward pass, and
then provides accurate, well-calibrated predictions. We compare DPConvCNP with
a DP Gaussian Process (GP) baseline with carefully tuned hyperparameters. The
DPConvCNP outperforms the GP baseline, especially on non-Gaussian data, yet is
much faster at test time and requires less tuning.

</details>


### [330] [Retraining with Predicted Hard Labels Provably Increases Model Accuracy](https://arxiv.org/pdf/2406.11206)
*Rudrajit Das, Inderjit S. Dhillon, Alessandro Epasto, Adel Javanmard, Jieming Mao, Vahab Mirrokni, Sujay Sanghavi, Peilin Zhong*

Main category: cs.LG

TL;DR: Retraining models with their own predicted hard labels improves performance under noisy labels, theoretically and empirically, especially in label differential privacy settings.


<details>
  <summary>Details</summary>
Motivation: To provide a theoretical understanding of why retraining with predicted labels improves model performance under noisy labels and to enhance label differential privacy training.

Method: Theoretical analysis in a linearly separable binary classification setting with noisy labels, followed by empirical validation using consensus-based retraining in label DP scenarios.

Result: Retraining improves population accuracy theoretically. Empirically, consensus-based retraining boosts accuracy by over 6% in label DP training (e.g., ResNet-18 on CIFAR-100 with ε=3).

Conclusion: Retraining, especially consensus-based, is a simple yet effective method to enhance model performance under noisy labels and label DP, with theoretical backing.

Abstract: The performance of a model trained with noisy labels is often improved by
simply \textit{retraining} the model with its \textit{own predicted hard
labels} (i.e., 1/0 labels). Yet, a detailed theoretical characterization of
this phenomenon is lacking. In this paper, we theoretically analyze retraining
in a linearly separable binary classification setting with randomly corrupted
labels given to us and prove that retraining can improve the population
accuracy obtained by initially training with the given (noisy) labels. To the
best of our knowledge, this is the first such theoretical result. Retraining
finds application in improving training with local label differential privacy
(DP) which involves training with noisy labels. We empirically show that
retraining selectively on the samples for which the predicted label matches the
given label significantly improves label DP training at no extra privacy cost;
we call this consensus-based retraining. As an example, when training ResNet-18
on CIFAR-100 with $\epsilon=3$ label DP, we obtain more than 6% improvement in
accuracy with consensus-based retraining.

</details>


### [331] [Not Another Imputation Method: A Transformer-based Model for Missing Values in Tabular Datasets](https://arxiv.org/pdf/2407.11540)
*Camillo Maria Caruso, Paolo Soda, Valerio Guarrasi*

Main category: cs.LG

TL;DR: NAIM is a transformer-based model for handling missing values in tabular data without traditional imputation, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Missing values in tabular data challenge AI models, and traditional imputation techniques may introduce bias or inefficiency.

Method: NAIM uses feature-specific embeddings and a modified masked self-attention mechanism to ignore missing data, plus a novel regularization technique.

Result: NAIM outperformed 6 ML and 5 DL models with imputation on 5 datasets, showing better predictive performance and resilience.

Conclusion: NAIM provides an effective alternative to traditional imputation, with code available for further research and application.

Abstract: Handling missing values in tabular datasets presents a significant challenge
in training and testing artificial intelligence models, an issue usually
addressed using imputation techniques. Here we introduce "Not Another
Imputation Method" (NAIM), a novel transformer-based model specifically
designed to address this issue without the need for traditional imputation
techniques. NAIM's ability to avoid the necessity of imputing missing values
and to effectively learn from available data relies on two main techniques: the
use of feature-specific embeddings to encode both categorical and numerical
features also handling missing inputs; the modification of the masked
self-attention mechanism to completely mask out the contributions of missing
data. Additionally, a novel regularization technique is introduced to enhance
the model's generalization capability from incomplete data. We extensively
evaluated NAIM on 5 publicly available tabular datasets, demonstrating its
superior performance over 6 state-of-the-art machine learning models and 5 deep
learning models, each paired with 3 different imputation techniques when
necessary. The results highlight the efficacy of NAIM in improving predictive
performance and resilience in the presence of missing data. To facilitate
further research and practical application in handling missing data without
traditional imputation methods, we made the code for NAIM available at
https://github.com/cosbidev/NAIM.

</details>


### [332] [Towards Certified Unlearning for Deep Neural Networks](https://arxiv.org/pdf/2408.00920)
*Binchi Zhang, Yushun Dong, Tianhao Wang, Jundong Li*

Main category: cs.LG

TL;DR: The paper proposes techniques to extend certified unlearning to nonconvex DNNs, improving efficiency with inverse Hessian approximation and addressing real-world scenarios like nonconvergence training and sequential unlearning.


<details>
  <summary>Details</summary>
Motivation: Certified unlearning is well-studied for convex models but challenging for nonconvex DNNs. The paper aims to bridge this gap.

Method: Extends certified unlearning to nonconvex objectives, uses inverse Hessian approximation for efficiency, and considers nonconvergence training and sequential unlearning.

Result: Experiments on three datasets show the method's efficacy and advantages for DNNs.

Conclusion: The proposed techniques successfully apply certified unlearning to DNNs, offering practical benefits for real-world applications.

Abstract: In the field of machine unlearning, certified unlearning has been extensively
studied in convex machine learning models due to its high efficiency and strong
theoretical guarantees. However, its application to deep neural networks
(DNNs), known for their highly nonconvex nature, still poses challenges. To
bridge the gap between certified unlearning and DNNs, we propose several simple
techniques to extend certified unlearning methods to nonconvex objectives. To
reduce the time complexity, we develop an efficient computation method by
inverse Hessian approximation without compromising certification guarantees. In
addition, we extend our discussion of certification to nonconvergence training
and sequential unlearning, considering that real-world users can send
unlearning requests at different time points. Extensive experiments on three
real-world datasets demonstrate the efficacy of our method and the advantages
of certified unlearning in DNNs.

</details>


### [333] [Perils of Label Indeterminacy: A Case Study on Prediction of Neurological Recovery After Cardiac Arrest](https://arxiv.org/pdf/2504.04243)
*Jakob Schoeffer, Maria De-Arteaga, Jonathan Elmer*

Main category: cs.LG

TL;DR: The paper introduces 'label indeterminacy' in AI-assisted decision-making, showing its impact in healthcare, particularly for predicting comatose patient recovery. It reveals ethical implications and suggests evaluation and design improvements.


<details>
  <summary>Details</summary>
Motivation: AI systems often rely on labels for training, but these labels may be unknown or based on unverifiable assumptions, leading to potential inconsistencies in high-stakes decisions.

Method: The study empirically examines label indeterminacy in healthcare, focusing on predicting recovery of comatose patients after cardiac arrest, comparing model performance on known and unknown labels.

Result: Models perform similarly on known labels but vary drastically for unknown labels, highlighting ethical concerns in high-stakes decisions.

Conclusion: The paper emphasizes the need for better evaluation, reporting, and design practices to address label indeterminacy in AI-assisted decision-making.

Abstract: The design of AI systems to assist human decision-making typically requires
the availability of labels to train and evaluate supervised models. Frequently,
however, these labels are unknown, and different ways of estimating them
involve unverifiable assumptions or arbitrary choices. In this work, we
introduce the concept of label indeterminacy and derive important implications
in high-stakes AI-assisted decision-making. We present an empirical study in a
healthcare context, focusing specifically on predicting the recovery of
comatose patients after resuscitation from cardiac arrest. Our study shows that
label indeterminacy can result in models that perform similarly when evaluated
on patients with known labels, but vary drastically in their predictions for
patients where labels are unknown. After demonstrating crucial ethical
implications of label indeterminacy in this high-stakes context, we discuss
takeaways for evaluation, reporting, and design.

</details>


### [334] [Contextual Bandits for Unbounded Context Distributions](https://arxiv.org/pdf/2408.09655)
*Puning Zhao, Rongfei Fan, Shaowei Wang, Li Shen, Qixin Zhang, Zong Ke, Tianhang Zheng*

Main category: cs.LG

TL;DR: The paper addresses the nonparametric contextual bandit problem with unbounded contexts, proposing two nearest neighbor methods with UCB exploration to achieve optimal regret bounds.


<details>
  <summary>Details</summary>
Motivation: Existing research lacks analysis of optimal regret for unbounded contexts, which is challenging due to the need for simultaneous exploration-exploitation and bias-variance tradeoffs.

Method: Two nearest neighbor methods with UCB exploration: one with fixed k and another with adaptive k, tailored for weak margin conditions and light-tailed distributions.

Result: The fixed k method achieves minimax optimal regret under weak margin conditions. The adaptive k method matches the minimax lower bound up to log factors, proving near-optimality.

Conclusion: The proposed methods effectively solve the nonparametric contextual bandit problem with unbounded contexts, achieving optimal or near-optimal regret bounds.

Abstract: Nonparametric contextual bandit is an important model of sequential decision
making problems. Under $\alpha$-Tsybakov margin condition, existing research
has established a regret bound of
$\tilde{O}\left(T^{1-\frac{\alpha+1}{d+2}}\right)$ for bounded supports.
However, the optimal regret with unbounded contexts has not been analyzed. The
challenge of solving contextual bandit problems with unbounded support is to
achieve both exploration-exploitation tradeoff and bias-variance tradeoff
simultaneously. In this paper, we solve the nonparametric contextual bandit
problem with unbounded contexts. We propose two nearest neighbor methods
combined with UCB exploration. The first method uses a fixed $k$. Our analysis
shows that this method achieves minimax optimal regret under a weak margin
condition and relatively light-tailed context distributions. The second method
uses adaptive $k$. By a proper data-driven selection of $k$, this method
achieves an expected regret of
$\tilde{O}\left(T^{1-\frac{(\alpha+1)\beta}{\alpha+(d+2)\beta}}+T^{1-\beta}\right)$,
in which $\beta$ is a parameter describing the tail strength. This bound
matches the minimax lower bound up to logarithm factors, indicating that the
second method is approximately optimal.

</details>


### [335] [Characterizing and Efficiently Accelerating Multimodal Generation Model Inference](https://arxiv.org/pdf/2410.00215)
*Yejin Lee, Anna Sun, Basil Hosmer, Bilge Acun, Can Balioglu, Changhan Wang, Charles David Hernandez, Christian Puhrsch, Daniel Haziza, Driss Guessous, Francisco Massa, Jacob Kahn, Jeffrey Wan, Jeremy Reizenstein, Jiaqi Zhai, Joe Isaacson, Joel Schlosser, Juan Pino, Kaushik Ram Sadagopan, Leonid Shamis, Linjian Ma, Min-Jae Hwang, Mingda Chen, Mostafa Elhoushi, Pedro Rodriguez, Ram Pasunuru, Scott Yih, Sravya Popuri, Xing Liu, Carole-Jean Wu*

Main category: cs.LG

TL;DR: The paper identifies system design and optimization opportunities for scaling generative AI, focusing on latency bottlenecks like GPU idle time and memory-intensive operations.


<details>
  <summary>Details</summary>
Motivation: To sustainably scale generative AI for billions of users, efficient and fast inference is crucial due to high system resource demands.

Method: Characterizes multi-modal generation models on real systems, analyzing bottlenecks like auto-regressive token generation and memory-intensive attention.

Result: State-of-the-art optimizations achieve a 3.88x better baseline in inference performance.

Conclusion: Optimizing system design and leveraging hardware-software co-design can significantly improve generative AI scalability and efficiency.

Abstract: Generative artificial intelligence (AI) technology is revolutionizing the
computing industry. Not only its applications have broadened to various sectors
but also poses new system design and optimization opportunities. The technology
is capable of understanding and responding in multiple modalities. However, the
advanced capability currently comes with significant system resource demands.
To sustainably scale generative AI capabilities to billions of users in the
world, inference must be fast and efficient. This paper pinpoints key system
design and optimization opportunities by characterizing a family of emerging
multi-modal generation models on real systems. Auto-regressive token generation
is a critical latency performance bottleneck, typically dominated by GPU idle
time. In addition to memory-intensive attention across the generative AI
models, linear operations constitute significant inference latency due to the
feed forward networks in Transformer-based models. We demonstrate that
state-of-the-art optimization levers, spanning from applications to system
software and hardware, set a 3.88x better baseline.

</details>


### [336] [VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning](https://arxiv.org/pdf/2504.08837)
*Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, Wenhu Chen*

Main category: cs.LG

TL;DR: Slow-thinking models like GPT-o1 outperform fast-thinking models in math/science tasks but lag in multimodal reasoning. This paper enhances slow-thinking via reinforcement learning, introducing Selective Sample Replay and Forced Rethinking, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: To improve slow-thinking capabilities in vision-language models, addressing their underperformance in multimodal reasoning compared to fast-thinking models.

Method: Uses reinforcement learning (GRPO algorithm with Selective Sample Replay) and introduces Forced Rethinking to enforce self-reflection.

Result: VL-Rethinker achieves SOTA scores on MathVista (80.4%) and MathVerse (63.5%), and performs well on other benchmarks.

Conclusion: The proposed techniques effectively enhance slow-thinking, narrowing the gap with top models like OpenAI-o1.

Abstract: Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated
great potential in solving challenging problems through explicit reflection.
They significantly outperform the best fast-thinking models, such as GPT-4o, on
various math and science benchmarks. However, their multimodal reasoning
capabilities remain on par with fast-thinking models. For instance, GPT-o1's
performance on benchmarks like MathVista, MathVerse, and MathVision is similar
to fast-thinking models. In this paper, we aim to enhance the slow-thinking
capabilities of vision-language models using reinforcement learning (without
relying on distillation) to advance the state of the art. First, we adapt the
GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to
address the vanishing advantages problem. While this approach yields strong
performance, the resulting RL-trained models exhibit limited self-reflection or
self-verification. To further encourage slow-thinking, we introduce Forced
Rethinking, which appends a rethinking trigger token to the end of rollouts in
RL training, explicitly enforcing a self-reflection reasoning step. By
combining these two techniques, our model, VL-Rethinker, advances
state-of-the-art scores on MathVista, MathVerse to achieve 80.4%, 63.5%
respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary
benchmarks such as MathVision, MMMU-Pro, EMMA, and MEGA-Bench, narrowing the
gap with OpenAI-o1. Our empirical results show the effectiveness of our
approaches.

</details>


### [337] [Learning stochastic dynamics from snapshots through regularized unbalanced optimal transport](https://arxiv.org/pdf/2410.00844)
*Zhenyi Zhang, Tiejun Li, Peijie Zhou*

Main category: cs.LG

TL;DR: A deep learning method for reconstructing dynamics from sparse snapshots using regularized unbalanced optimal transport (RUOT), applicable to gene networks and single-cell data.


<details>
  <summary>Details</summary>
Motivation: To infer continuous unbalanced stochastic dynamics from sparse snapshots without prior knowledge of growth/death processes.

Method: Deep learning approach for solving RUOT, connecting it to the Schrödinger bridge problem.

Result: Accurately identifies growth/transition patterns, eliminates false transitions, and constructs developmental landscapes.

Conclusion: The method effectively models dynamics from data, outperforming other approaches.

Abstract: Reconstructing dynamics using samples from sparsely time-resolved snapshots
is an important problem in both natural sciences and machine learning. Here, we
introduce a new deep learning approach for solving regularized unbalanced
optimal transport (RUOT) and inferring continuous unbalanced stochastic
dynamics from observed snapshots. Based on the RUOT form, our method models
these dynamics without requiring prior knowledge of growth and death processes
or additional information, allowing them to be learned directly from data.
Theoretically, we explore the connections between the RUOT and Schr\"odinger
bridge problem and discuss the key challenges and potential solutions. The
effectiveness of our method is demonstrated with a synthetic gene regulatory
network, high-dimensional Gaussian Mixture Model, and single-cell RNA-seq data
from blood development. Compared with other methods, our approach accurately
identifies growth and transition patterns, eliminates false transitions, and
constructs the Waddington developmental landscape. Our code is available at:
https://github.com/zhenyiizhang/DeepRUOT.

</details>


### [338] [C-MORL: Multi-Objective Reinforcement Learning through Efficient Discovery of Pareto Front](https://arxiv.org/pdf/2410.02236)
*Ruohong Liu, Yuxin Pan, Linjie Xu, Lei Song, Jiang Bian, Pengcheng You, Yize Chen*

Main category: cs.LG

TL;DR: C-MORL, a two-stage Pareto front discovery algorithm, bridges constrained policy optimization and MORL, outperforming existing methods in multi-objective tasks.


<details>
  <summary>Details</summary>
Motivation: Existing MORL methods struggle with efficient Pareto front discovery and scalability as state and preference dimensions grow.

Method: C-MORL trains policies in parallel for individual preferences, then uses constrained optimization to fill Pareto front gaps.

Result: C-MORL achieves superior performance in hypervolume, expected utility, and sparsity, even with up to nine objectives.

Conclusion: C-MORL effectively addresses scalability and Pareto front discovery challenges in MORL, demonstrating robust performance.

Abstract: Multi-objective reinforcement learning (MORL) excels at handling rapidly
changing preferences in tasks that involve multiple criteria, even for unseen
preferences. However, previous dominating MORL methods typically generate a
fixed policy set or preference-conditioned policy through multiple training
iterations exclusively for sampled preference vectors, and cannot ensure the
efficient discovery of the Pareto front. Furthermore, integrating preferences
into the input of policy or value functions presents scalability challenges, in
particular as the dimension of the state and preference space grow, which can
complicate the learning process and hinder the algorithm's performance on more
complex tasks. To address these issues, we propose a two-stage Pareto front
discovery algorithm called Constrained MORL (C-MORL), which serves as a
seamless bridge between constrained policy optimization and MORL. Concretely, a
set of policies is trained in parallel in the initialization stage, with each
optimized towards its individual preference over the multiple objectives. Then,
to fill the remaining vacancies in the Pareto front, the constrained
optimization steps are employed to maximize one objective while constraining
the other objectives to exceed a predefined threshold. Empirically, compared to
recent advancements in MORL methods, our algorithm achieves more consistent and
superior performances in terms of hypervolume, expected utility, and sparsity
on both discrete and continuous control tasks, especially with numerous
objectives (up to nine objectives in our experiments).

</details>


### [339] [GeoUni: A Unified Model for Generating Geometry Diagrams, Problems and Problem Solutions](https://arxiv.org/pdf/2504.10146)
*Jo-Ku Cheng, Zeren Zhang, Ran Chen, Jingyang Deng, Ziran Qin, Jinwen Ma*

Main category: cs.LG

TL;DR: GeoUni is a unified geometry expert model that integrates problem-solving, diagram generation, and problem creation in one framework, outperforming larger models and excelling in precision.


<details>
  <summary>Details</summary>
Motivation: Mastery in geometry requires seamless integration of problem-solving, visualization, and problem creation, which existing models fail to address.

Method: GeoUni combines problem-solving and diagram generation in a single framework, enabling individualized geometry problem creation.

Result: GeoUni matches larger models in reasoning tasks, excels in diagram precision, and uniquely generates problems with matching diagrams.

Conclusion: GeoUni offers broader capabilities than current models, unifying geometry tasks effectively.

Abstract: We propose GeoUni, the first unified geometry expert model capable of
generating problem solutions and diagrams within a single framework in a way
that enables the creation of unique and individualized geometry problems.
Traditionally, solving geometry problems and generating diagrams have been
treated as separate tasks in machine learning, with no models successfully
integrating both to support problem creation. However, we believe that mastery
in geometry requires frictionless integration of all of these skills, from
solving problems to visualizing geometric relationships, and finally, crafting
tailored problems. Our extensive experiments demonstrate that GeoUni, with only
1.5B parameters, achieves performance comparable to larger models such as
DeepSeek-R1 with 671B parameters in geometric reasoning tasks. GeoUni also
excels in generating precise geometric diagrams, surpassing both text-to-image
models and unified models, including the GPT-4o image generation. Most
importantly, GeoUni is the only model capable of successfully generating
textual problems with matching diagrams based on specific knowledge points,
thus offering a wider range of capabilities that extend beyond current models.

</details>


### [340] [Let's Ask GNN: Empowering Large Language Model for Graph In-Context Learning](https://arxiv.org/pdf/2410.07074)
*Zhengyu Hu, Yichuan Li, Zhengyu Chen, Jingang Wang, Han Liu, Kyumin Lee, Kaize Ding*

Main category: cs.LG

TL;DR: AskGNN bridges the gap between LLMs and graph data using In-Context Learning and a GNN-powered retriever, improving graph task performance without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of applying LLMs to Textual Attributed Graphs (TAGs) due to the mismatch between sequential text processing and graph-structured data.

Method: Uses In-Context Learning (ICL) and a GNN-powered retriever to select labeled nodes, integrating graph structures and supervision signals.

Result: Superior performance in graph tasks across three tasks and seven LLMs, demonstrating effectiveness.

Conclusion: AskGNN enables LLMs to handle graph-structured data efficiently without extensive fine-tuning, opening new research avenues.

Abstract: Textual Attributed Graphs (TAGs) are crucial for modeling complex real-world
systems, yet leveraging large language models (LLMs) for TAGs presents unique
challenges due to the gap between sequential text processing and
graph-structured data. We introduce AskGNN, a novel approach that bridges this
gap by leveraging In-Context Learning (ICL) to integrate graph data and
task-specific information into LLMs. AskGNN employs a Graph Neural Network
(GNN)-powered structure-enhanced retriever to select labeled nodes across
graphs, incorporating complex graph structures and their supervision signals.
Our learning-to-retrieve algorithm optimizes the retriever to select example
nodes that maximize LLM performance on graph. Experiments across three tasks
and seven LLMs demonstrate AskGNN's superior effectiveness in graph task
performance, opening new avenues for applying LLMs to graph-structured data
without extensive fine-tuning.

</details>


### [341] [Regularized Robustly Reliable Learners and Instance Targeted Attacks](https://arxiv.org/pdf/2410.10572)
*Avrim Blum, Donya Saless*

Main category: cs.LG

TL;DR: The paper addresses two challenges in robustly-reliable learning: vacuous guarantees for flexible hypothesis classes and impractical computational efficiency. It introduces regularized robustly-reliable learners and sublinear-time algorithms for specific cases.


<details>
  <summary>Details</summary>
Motivation: To improve upon Balcan et al's (2022) work by addressing its limitations in handling flexible hypothesis classes and computational inefficiency.

Method: Defines regularized robustly-reliable learners and employs dynamic algorithm design for sublinear-time solutions.

Result: Proposes a modified notion of robustly-reliable learning and efficient algorithms for certain scenarios.

Conclusion: The work advances robustly-reliable learning by resolving key issues, enabling practical applications in adversarial settings.

Abstract: Instance-targeted data poisoning attacks, where an adversary corrupts a
training set to induce errors on specific test points, have raised significant
concerns. Balcan et al (2022) proposed an approach to addressing this challenge
by defining a notion of robustly-reliable learners that provide per-instance
guarantees of correctness under well-defined assumptions, even in the presence
of data poisoning attacks. They then give a generic optimal (but
computationally inefficient) robustly reliable learner as well as a
computationally efficient algorithm for the case of linear separators over
log-concave distributions.
  In this work, we address two challenges left open by Balcan et al (2022). The
first is that the definition of robustly-reliable learners in Balcan et al
(2022) becomes vacuous for highly-flexible hypothesis classes: if there are two
classifiers h_0, h_1 \in H both with zero error on the training set such that
h_0(x) \neq h_1(x), then a robustly-reliable learner must abstain on x. We
address this problem by defining a modified notion of regularized
robustly-reliable learners that allows for nontrivial statements in this case.
The second is that the generic algorithm of Balcan et al (2022) requires
re-running an ERM oracle (essentially, retraining the classifier) on each test
point x, which is generally impractical even if ERM can be implemented
efficiently. To tackle this problem, we show that at least in certain
interesting cases we can design algorithms that can produce their outputs in
time sublinear in training time, by using techniques from dynamic algorithm
design.

</details>


### [342] [Electrocardiogram-Language Model for Few-Shot Question Answering with Meta Learning](https://arxiv.org/pdf/2410.14464)
*Jialu Tang, Tong Xia, Yuan Lu, Cecilia Mascolo, Aaqib Saeed*

Main category: cs.LG

TL;DR: A novel multimodal meta-learning method for few-shot ECG question answering is introduced, combining ECG signals with LLMs to improve clinical interpretation despite limited labeled data.


<details>
  <summary>Details</summary>
Motivation: The challenge of interpreting ECG signals with diverse clinical queries and scarce labeled data drives the need for robust, adaptable diagnostic systems.

Method: The approach integrates a pre-trained ECG encoder with a frozen LLM via a trainable fusion module, enabling LLMs to reason about ECG data.

Result: The method achieves high accuracy (e.g., 84.6% in a 5-way 5-shot setting) and generalizes well to unseen tasks, even with limited ECG leads.

Conclusion: The method effectively combines ECG signal processing with LLMs' language understanding, enhancing clinical ECG interpretation in data-constrained scenarios.

Abstract: Electrocardiogram (ECG) interpretation requires specialized expertise, often
involving synthesizing insights from ECG signals with complex clinical queries
posed in natural language. The scarcity of labeled ECG data coupled with the
diverse nature of clinical inquiries presents a significant challenge for
developing robust and adaptable ECG diagnostic systems. This work introduces a
novel multimodal meta-learning method for few-shot ECG question answering,
addressing the challenge of limited labeled data while leveraging the rich
knowledge encoded within large language models (LLMs). Our LLM-agnostic
approach integrates a pre-trained ECG encoder with a frozen LLM (e.g., LLaMA
and Gemma) via a trainable fusion module, enabling the language model to reason
about ECG data and generate clinically meaningful answers. Extensive
experiments demonstrate superior generalization to unseen diagnostic tasks
compared to supervised baselines, achieving notable performance even with
limited ECG leads. For instance, in a 5-way 5-shot setting, our method using
LLaMA-3.1-8B achieves an accuracy of 84.6%, 77.3%, and 69.6% on single verify,
choose and query question types, respectively. These results highlight the
potential of our method to enhance clinical ECG interpretation by combining
signal processing with the nuanced language understanding capabilities of LLMs,
particularly in data-constrained scenarios.

</details>


### [343] [Learning from Convolution-based Unlearnable Datasets](https://arxiv.org/pdf/2411.01742)
*Dohyun Kim, Pedro Sandoval-Segura*

Main category: cs.LG

TL;DR: CUDA method makes data unlearnable via class-wise blurs, but sharpening and frequency filtering can restore its utility for training.


<details>
  <summary>Details</summary>
Motivation: Address concerns about unauthorized use of online data by making datasets unlearnable.

Method: Apply class-wise blurs to images (CUDA), then test resilience against sharpening and frequency filtering.

Result: Sharpening and filtering improve CUDA data's training utility, increasing test accuracy on CIFAR and ImageNet datasets.

Conclusion: Simple transforms can break CUDA's unlearnability, highlighting the need for stronger data poisoning techniques.

Abstract: The construction of large datasets for deep learning has raised concerns
regarding unauthorized use of online data, leading to increased interest in
protecting data from third-parties who want to use it for training. The
Convolution-based Unlearnable DAtaset (CUDA) method aims to make data
unlearnable by applying class-wise blurs to every image in the dataset so that
neural networks learn relations between blur kernels and labels, as opposed to
informative features for classifying clean data. In this work, we evaluate
whether CUDA data remains unlearnable after image sharpening and frequency
filtering, finding that this combination of simple transforms improves the
utility of CUDA data for training. In particular, we observe a substantial
increase in test accuracy over adversarial training for models trained with
CUDA unlearnable data from CIFAR-10, CIFAR-100, and ImageNet-100. In training
models to high accuracy using unlearnable data, we underscore the need for
ongoing refinement in data poisoning techniques to ensure data privacy. Our
method opens new avenues for enhancing the robustness of unlearnable datasets
by highlighting that simple methods such as sharpening and frequency filtering
are capable of breaking convolution-based unlearnable datasets.

</details>


### [344] [T2S: High-resolution Time Series Generation with Text-to-Series Diffusion Models](https://arxiv.org/pdf/2505.02417)
*Yunfeng Ge, Jiawei Li, Yiji Zhao, Haomin Wen, Zhao Li, Meikang Qiu, Hongyan Li, Ming Jin, Shirui Pan*

Main category: cs.LG

TL;DR: The paper introduces Text-to-Series (T2S), a diffusion-based framework for generating time series from text, addressing limitations in generalization and arbitrary-length generation.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges like data sparsity and lack of multimodal time series datasets, and to extend diffusion models' success to time series generation.

Method: Proposes T2S, using a length-adaptive variational autoencoder and Flow Matching with Diffusion Transformer for domain-agnostic time series generation.

Result: T2S achieves state-of-the-art performance across 13 datasets in 12 domains.

Conclusion: T2S effectively bridges natural language and time series, enabling flexible and high-quality generation.

Abstract: Text-to-Time Series generation holds significant potential to address
challenges such as data sparsity, imbalance, and limited availability of
multimodal time series datasets across domains. While diffusion models have
achieved remarkable success in Text-to-X (e.g., vision and audio data)
generation, their use in time series generation remains in its nascent stages.
Existing approaches face two critical limitations: (1) the lack of systematic
exploration of general-proposed time series captions, which are often
domain-specific and struggle with generalization; and (2) the inability to
generate time series of arbitrary lengths, limiting their applicability to
real-world scenarios. In this work, we first categorize time series captions
into three levels: point-level, fragment-level, and instance-level.
Additionally, we introduce a new fragment-level dataset containing over 600,000
high-resolution time series-text pairs. Second, we propose Text-to-Series
(T2S), a diffusion-based framework that bridges the gap between natural
language and time series in a domain-agnostic manner. T2S employs a
length-adaptive variational autoencoder to encode time series of varying
lengths into consistent latent embeddings. On top of that, T2S effectively
aligns textual representations with latent embeddings by utilizing Flow
Matching and employing Diffusion Transformer as the denoiser. We train T2S in
an interleaved paradigm across multiple lengths, allowing it to generate
sequences of any desired length. Extensive evaluations demonstrate that T2S
achieves state-of-the-art performance across 13 datasets spanning 12 domains.

</details>


### [345] [On-device Anomaly Detection in Conveyor Belt Operations](https://arxiv.org/pdf/2411.10729)
*Luciano S. Martinez-Rau, Yuxuan Zhang, Bengt Oelmann, Sebastian Bader*

Main category: cs.LG

TL;DR: The paper proposes two novel methods for classifying normal and abnormal duty cycles in mining conveyor belts, outperforming existing methods with dataset-dependent performance and low-energy consumption.


<details>
  <summary>Details</summary>
Motivation: Identifying root causes of conveyor belt failures, such as production changes and operator errors, is critical but understudied, necessitating robust monitoring solutions.

Method: The study introduces pattern recognition systems using threshold-based detection, manual feature extraction, pattern-matching, and supervised tiny ML models (e.g., decision trees, random forests).

Result: The proposed methods outperform the former approach, with heuristic rule-based and ML-based methods achieving 97.3%/80.2% and 91.3%/67.9% accuracy, respectively, on different datasets.

Conclusion: The methods enable efficient, real-time anomaly detection with low energy consumption, offering practical insights for conveyor belt monitoring.

Abstract: Conveyor belts are crucial in mining operations by enabling the continuous
and efficient movement of bulk materials over long distances, which directly
impacts productivity. While detecting anomalies in specific conveyor belt
components has been widely studied, identifying the root causes of these
failures, such as changing production conditions and operator errors, remains
critical. Continuous monitoring of mining conveyor belt work cycles is still at
an early stage and requires robust solutions. Recently, an anomaly detection
method for duty cycle operations of a mining conveyor belt has been proposed.
Based on its limited performance and unevaluated long-term proper operation,
this study proposes two novel methods for classifying normal and abnormal duty
cycles. The proposed approaches are pattern recognition systems that make use
of threshold-based duty-cycle detection mechanisms, manually extracted
features, pattern-matching, and supervised tiny machine learning models. The
explored low-computational models include decision tree, random forest, extra
trees, extreme gradient boosting, Gaussian naive Bayes, and multi-layer
perceptron. A comprehensive evaluation of the former and proposed approaches is
carried out on two datasets. Both proposed methods outperform the former
method, with the best-performing approach being dataset-dependent. The
heuristic rule-based approach achieves the highest performance in the same
dataset used for algorithm training, with 97.3% for normal cycles and 80.2% for
abnormal cycles. The ML-based approach performs better on a dataset including
the effects of machine aging, scoring 91.3% for normal cycles and 67.9% for
abnormal cycles. Implemented on two low-power microcontrollers, the methods
demonstrate efficient, real-time operation with energy consumption of 13.3 and
20.6 ${\mu}$J during inference. These results offer valuable insights for
detecting ...

</details>


### [346] [A Unified Data Representation Learning for Non-parametric Two-sample Testing](https://arxiv.org/pdf/2412.00613)
*Xunye Tian, Liuhua Peng, Zhijian Zhou, Mingming Gong, Arthur Gretton, Feng Liu*

Main category: cs.LG

TL;DR: Proposes RL-TST, a framework for two-sample testing that leverages self-supervised learning on the entire dataset to improve representation learning and test power.


<details>
  <summary>Details</summary>
Motivation: Existing methods split data into training and test sets, but recent theory shows using the whole dataset (without sample indexes) can improve representation learning while controlling Type-I errors.

Method: RL-TST uses self-supervised learning on the entire dataset to capture inherent representations (IRs), then trains a discriminative model on IRs to learn discriminative representations (DRs).

Result: RL-TST outperforms existing methods by leveraging test set data manifold information and enhancing test power with DRs.

Conclusion: RL-TST effectively combines structural and discriminative information for improved two-sample testing.

Abstract: Learning effective data representations has been crucial in non-parametric
two-sample testing. Common approaches will first split data into training and
test sets and then learn data representations purely on the training set.
However, recent theoretical studies have shown that, as long as the sample
indexes are not used during the learning process, the whole data can be used to
learn data representations, meanwhile ensuring control of Type-I errors. The
above fact motivates us to use the test set (but without sample indexes) to
facilitate the data representation learning in the testing. To this end, we
propose a representation-learning two-sample testing (RL-TST) framework. RL-TST
first performs purely self-supervised representation learning on the entire
dataset to capture inherent representations (IRs) that reflect the underlying
data manifold. A discriminative model is then trained on these IRs to learn
discriminative representations (DRs), enabling the framework to leverage both
the rich structural information from IRs and the discriminative power of DRs.
Extensive experiments demonstrate that RL-TST outperforms representative
approaches by simultaneously using data manifold information in the test set
and enhancing test power via finding the DRs with the training set.

</details>


### [347] [Graph Attention is Not Always Beneficial: A Theoretical Analysis of Graph Attention Mechanisms via Contextual Stochastic Block Models](https://arxiv.org/pdf/2412.15496)
*Zhongtian Ma, Qiaosheng Zhang, Bocheng Zhou, Yexin Zhang, Shuyue Hu, Zhen Wang*

Main category: cs.LG

TL;DR: Graph attention mechanisms are not universally beneficial; they improve node classification when structure noise exceeds feature noise, but simpler methods work better otherwise. Multi-layer GATs outperform single-layer ones, especially in high SNR regimes.


<details>
  <summary>Details</summary>
Motivation: To theoretically understand the effectiveness of graph attention mechanisms in node classification tasks using CSBMs.

Method: Theoretical analysis of graph attention mechanisms under structure and feature noise, proposing a multi-layer GAT architecture.

Result: Graph attention helps when structure noise > feature noise; multi-layer GATs achieve perfect classification with relaxed SNR requirements.

Conclusion: Multi-layer GATs are superior in high SNR, resolving over-smoothing and enabling perfect classification under specific noise conditions.

Abstract: Despite the growing popularity of graph attention mechanisms, their
theoretical understanding remains limited. This paper aims to explore the
conditions under which these mechanisms are effective in node classification
tasks through the lens of Contextual Stochastic Block Models (CSBMs). Our
theoretical analysis reveals that incorporating graph attention mechanisms is
\emph{not universally beneficial}. Specifically, by appropriately defining
\emph{structure noise} and \emph{feature noise} in graphs, we show that graph
attention mechanisms can enhance classification performance when structure
noise exceeds feature noise. Conversely, when feature noise predominates,
simpler graph convolution operations are more effective. Furthermore, we
examine the over-smoothing phenomenon and show that, in the high
signal-to-noise ratio (SNR) regime, graph convolutional networks suffer from
over-smoothing, whereas graph attention mechanisms can effectively resolve this
issue. Building on these insights, we propose a novel multi-layer Graph
Attention Network (GAT) architecture that significantly outperforms
single-layer GATs in achieving \emph{perfect node classification} in CSBMs,
relaxing the SNR requirement from $ \omega(\sqrt{\log n}) $ to $
\omega(\sqrt{\log n} / \sqrt[3]{n}) $. To our knowledge, this is the first
study to delineate the conditions for perfect node classification using
multi-layer GATs. Our theoretical contributions are corroborated by extensive
experiments on both synthetic and real-world datasets, highlighting the
practical implications of our findings.

</details>


### [348] [Towards the Worst-case Robustness of Large Language Models](https://arxiv.org/pdf/2501.19040)
*Huanran Chen, Yinpeng Dong, Zeming Wei, Hang Su, Jun Zhu*

Main category: cs.LG

TL;DR: The paper analyzes the worst-case robustness of large language models against adversarial attacks, showing current defenses are ineffective and proposing theoretical lower bounds for stochastic defenses.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of large language models to adversarial attacks, particularly in inducing harmful or incorrect outputs.

Method: Uses white-box attacks to upper bound worst-case robustness and proposes a general tight lower bound for randomized smoothing using knapsack solvers.

Result: Most deterministic defenses achieve nearly 0% worst-case robustness, while the proposed method certifies robustness against any attack for specific cases.

Conclusion: The study highlights the limitations of current defenses and provides theoretical bounds to improve robustness against adversarial attacks.

Abstract: Recent studies have revealed the vulnerability of large language models to
adversarial attacks, where adversaries craft specific input sequences to induce
harmful, violent, private, or incorrect outputs. In this work, we study their
worst-case robustness, i.e., whether an adversarial example exists that leads
to such undesirable outputs. We upper bound the worst-case robustness using
stronger white-box attacks, indicating that most current deterministic defenses
achieve nearly 0\% worst-case robustness. We propose a general tight lower
bound for randomized smoothing using fractional knapsack solvers or 0-1
knapsack solvers, and using them to bound the worst-case robustness of all
stochastic defenses. Based on these solvers, we provide theoretical lower
bounds for several previous empirical defenses. For example, we certify the
robustness of a specific case, smoothing using a uniform kernel, against
\textit{any possible attack} with an average $\ell_0$ perturbation of 2.02 or
an average suffix length of 6.41.

</details>


### [349] [Uncertainty Comes for Free: Human-in-the-Loop Policies with Diffusion Models](https://arxiv.org/pdf/2503.01876)
*Zhanpeng He, Yifeng Cao, Matei Ciocarlie*

Main category: cs.LG

TL;DR: A method using diffusion policies to reduce human oversight in HitL robot deployment by seeking assistance only when necessary, improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Continuous human monitoring in HitL robot deployment is labor-intensive and impractical for large-scale use.

Method: Leverages diffusion policies to compute an uncertainty-based metric for deciding when to request human assistance, without training-time interaction. Also aids in data collection for policy fine-tuning.

Result: Experimental results show improved policy performance in simulated and real-world scenarios.

Conclusion: The approach reduces reliance on constant human oversight while enhancing deployment efficiency and performance.

Abstract: Human-in-the-loop (HitL) robot deployment has gained significant attention in
both academia and industry as a semi-autonomous paradigm that enables human
operators to intervene and adjust robot behaviors at deployment time, improving
success rates. However, continuous human monitoring and intervention can be
highly labor-intensive and impractical when deploying a large number of robots.
To address this limitation, we propose a method that allows diffusion policies
to actively seek human assistance only when necessary, reducing reliance on
constant human oversight. To achieve this, we leverage the generative process
of diffusion policies to compute an uncertainty-based metric based on which the
autonomous agent can decide to request operator assistance at deployment time,
without requiring any operator interaction during training. Additionally, we
show that the same method can be used for efficient data collection for
fine-tuning diffusion policies in order to improve their autonomous
performance. Experimental results from simulated and real-world environments
demonstrate that our approach enhances policy performance during deployment for
a variety of scenarios.

</details>


### [350] [Effective Dimension Aware Fractional-Order Stochastic Gradient Descent for Convex Optimization Problems](https://arxiv.org/pdf/2503.13764)
*Mohammad Partohaghighi, Roummel Marcia, YangQuan Chen*

Main category: cs.LG

TL;DR: 2SEDFOSGD integrates 2SED with FOSGD to adapt fractional exponents dynamically, improving stability and convergence in optimization.


<details>
  <summary>Details</summary>
Motivation: FOSGD's utility is limited by tuning and stabilizing fractional exponents, prompting the need for a data-driven approach.

Method: 2SEDFOSGD combines the Two-Scale Effective Dimension algorithm with FOSGD to adjust the fractional exponent based on model sensitivity and dimensionality.

Result: Empirical tests show faster convergence and robust parameter estimates in Gaussian and α-stable noise scenarios, as well as on MNIST and CIFAR-100 datasets.

Conclusion: 2SEDFOSGD offers a dimension-aware fractional technique that enhances optimization performance for advanced modeling tasks.

Abstract: Fractional-order stochastic gradient descent (FOSGD) leverages fractional
exponents to capture long-memory effects in optimization. However, its utility
is often limited by the difficulty of tuning and stabilizing these exponents.
We propose 2SED Fractional-Order Stochastic Gradient Descent (2SEDFOSGD), which
integrates the Two-Scale Effective Dimension (2SED) algorithm with FOSGD to
adapt the fractional exponent in a data-driven manner. By tracking model
sensitivity and effective dimensionality, 2SEDFOSGD dynamically modulates the
exponent to mitigate oscillations and hasten convergence. Theoretically, this
approach preserves the advantages of fractional memory without the sluggish or
unstable behavior observed in na\"ive fractional SGD. Empirical evaluations in
Gaussian and $\alpha$-stable noise scenarios using an autoregressive (AR)
model\textcolor{red}{, as well as on the MNIST and CIFAR-100 datasets for image
classification,} highlight faster convergence and more robust parameter
estimates compared to baseline methods, underscoring the potential of
dimension-aware fractional techniques for advanced modeling and estimation
tasks.

</details>


### [351] [Advances in Protein Representation Learning: Methods, Applications, and Future Directions](https://arxiv.org/pdf/2503.16659)
*Viet Thanh Duy Nguyen, Truong-Son Hy*

Main category: cs.LG

TL;DR: The paper reviews Protein Representation Learning (PRL) methodologies, categorizes them into five approaches, introduces key databases, and discusses applications, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: Proteins are vital for biological processes, and understanding their structures and functions is crucial for advancements in molecular biology and drug discovery. PRL offers a computational approach to address these challenges.

Method: The paper categorizes PRL methodologies into five areas: feature-based, sequence-based, structure-based, multimodal, and complex-based approaches. It also introduces essential protein databases.

Result: The review highlights the broad impact of PRL applications across domains and identifies key resources for researchers.

Conclusion: The paper outlines technical challenges and future directions to inspire further innovation in PRL, emphasizing its foundational role in molecular biology.

Abstract: Proteins are complex biomolecules that play a central role in various
biological processes, making them critical targets for breakthroughs in
molecular biology, medical research, and drug discovery. Deciphering their
intricate, hierarchical structures, and diverse functions is essential for
advancing our understanding of life at the molecular level. Protein
Representation Learning (PRL) has emerged as a transformative approach,
enabling the extraction of meaningful computational representations from
protein data to address these challenges. In this paper, we provide a
comprehensive review of PRL research, categorizing methodologies into five key
areas: feature-based, sequence-based, structure-based, multimodal, and
complex-based approaches. To support researchers in this rapidly evolving
field, we introduce widely used databases for protein sequences, structures,
and functions, which serve as essential resources for model development and
evaluation. We also explore the diverse applications of these approaches in
multiple domains, demonstrating their broad impact. Finally, we discuss
pressing technical challenges and outline future directions to advance PRL,
offering insights to inspire continued innovation in this foundational field.

</details>


### [352] [Probabilistic Uncertain Reward Model](https://arxiv.org/pdf/2503.22480)
*Wangtao Sun, Xiang Cheng, Xing Yu, Haotian Xu, Zhao Yang, Shizhu He, Jun Zhao, Kang Liu*

Main category: cs.LG

TL;DR: PURM, a probabilistic reward model, mitigates reward hacking in RLHF by learning reward distributions and incorporating uncertainty-aware penalties in PPO.


<details>
  <summary>Details</summary>
Motivation: Addressing reward hacking in RLHF by modeling uncertainty from preference data, which existing methods fail to do systematically.

Method: Proposes PURM, a probabilistic reward model, and integrates it with PPO using uncertainty-aware penalties.

Result: PURM delays reward hacking onset and improves performance, providing accurate reward and uncertainty estimates.

Conclusion: PURM offers a robust solution for long-term RLHF training by effectively mitigating reward hacking.

Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a critical
technique for training large language models. However, reward hacking-a
phenomenon where models exploit flaws in the reward model-remains a significant
barrier to achieving robust and scalable intelligence through long-term
training. Existing studies have proposed the uncertain reward models to address
reward hacking, however, they often lack systematic or theoretical foundations,
failing to model the uncertainty intrinsically emerging from preference data,
and thus cannot sufficiently mitigate reward hacking to sustain prolonged RLHF
training and exploration. In this paper, we propose a Probabilistic Uncertain
Reward Model (PURM), a natural generalization of the classical Bradley-Terry
reward model, that can directly learn the reward distribution emerged from the
preference data. We theoretically derived PURM's loss function and the
uncertainty of the reward distribution. To mitigate reward hacking with PURM,
we further introduce an uncertainty-aware penalty into Proximal Policy
Optimization (PPO), which leverages the learned uncertainty to dynamically
balance reward optimization and exploration. Experimental results demonstrate
that PURM significantly delays the onset of reward hacking while improving
final performance compared with existing methods. We also find that PURM
genuinely produce sound reward and uncertainty estimations. The data and code
of this paper can be found at
https://anonymous.4open.science/r/Probabilistic-Uncertain-Reward-Model/

</details>


### [353] [Type-Constrained Code Generation with Language Models](https://arxiv.org/pdf/2504.09246)
*Niels Mündler, Jingxuan He, Hao Wang, Koushik Sen, Dawn Song, Martin Vechev*

Main category: cs.LG

TL;DR: A type-constrained decoding approach is introduced to reduce typing errors in LLM-generated code, improving compilation success and functional correctness.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce uncompilable code due to typing errors, which are not addressed by existing constrained decoding methods.

Method: Develops prefix automata and inhabitable type search to enforce well-typedness, formalized for a simply-typed language and extended to TypeScript.

Result: Reduces compilation errors by over half and boosts functional correctness in code tasks across various LLMs.

Conclusion: The approach effectively constrains LLM code generation using type systems, demonstrating broad applicability.

Abstract: Large language models (LLMs) have achieved notable success in code
generation. However, they still frequently produce uncompilable output because
their next-token inference procedure does not model formal aspects of code.
Although constrained decoding is a promising approach to alleviate this issue,
it has only been applied to handle either domain-specific languages or
syntactic features of general-purpose programming languages. However, LLMs
frequently generate code with typing errors, which are beyond the domain of
syntax and generally hard to adequately constrain. To address this challenge,
we introduce a type-constrained decoding approach that leverages type systems
to guide code generation. For this purpose, we develop novel prefix automata
and a search over inhabitable types, forming a sound approach to enforce
well-typedness on LLM-generated code. We formalize our approach on a
foundational simply-typed language and extend it to TypeScript to demonstrate
practicality. Our evaluation on the HumanEval and MBPP datasets shows that our
approach reduces compilation errors by more than half and significantly
increases functional correctness in code synthesis, translation, and repair
tasks across LLMs of various sizes and model families, including
state-of-the-art open-weight models with more than 30B parameters. The results
demonstrate the generality and effectiveness of our approach in constraining
LLM code generation with formal rules of type systems.

</details>


### [354] [On Multivariate Financial Time Series Classification](https://arxiv.org/pdf/2504.17664)
*Grégory Bournassenko*

Main category: cs.LG

TL;DR: The paper compares Machine Learning and Deep Learning models for financial time series analysis, highlighting the advantages of Big Data and modern architectures like ConvTimeNet over traditional methods like SVMs.


<details>
  <summary>Details</summary>
Motivation: To explore the effectiveness of Machine Learning and Deep Learning in financial markets, particularly comparing small vs. Big Data approaches and their scalability.

Method: The study contrasts traditional methods (e.g., SVMs) with modern architectures (e.g., ConvTimeNet) in multivariate time series analysis.

Result: Big Data and deep learning models (e.g., ConvTimeNet) outperform traditional methods, emphasizing the need for in-depth understanding of Big Data in financial predictions.

Conclusion: Modern Deep Learning architectures and Big Data are crucial for accurate financial time series analysis and prediction.

Abstract: This article investigates the use of Machine Learning and Deep Learning
models in multivariate time series analysis within financial markets. It
compares small and big data approaches, focusing on their distinct challenges
and the benefits of scaling. Traditional methods such as SVMs are contrasted
with modern architectures like ConvTimeNet. The results show the importance of
using and understanding Big Data in depth in the analysis and prediction of
financial time series.

</details>


### [355] [Physics-Learning AI Datamodel (PLAID) datasets: a collection of physics simulations for machine learning](https://arxiv.org/pdf/2505.02974)
*Fabien Casenave, Xavier Roynard, Brian Staber, William Piat, Michele Alessandro Bucci, Nissrine Akkari, Abbas Kabalan, Xuan Minh Vuong Nguyen, Luca Saverio, Raphaël Carpintero Perez, Anthony Kalaydjian, Samy Fouché, Thierry Gonon, Ghassan Najjar, Emmanuel Menier, Matthieu Nastorg, Giovanni Catalani, Christian Rey*

Main category: cs.LG

TL;DR: PLAID is introduced as a flexible framework for standardizing and sharing physics simulation datasets, addressing limitations of existing tools. Six datasets and benchmarks are released.


<details>
  <summary>Details</summary>
Motivation: The lack of large-scale, diverse, and standardized datasets for physics simulations hinders the adoption of machine learning-based surrogate models.

Method: PLAID provides a unified standard for simulation data and includes a library for dataset manipulation. Six datasets in structural mechanics and fluid dynamics are released with benchmarks.

Result: PLAID enables standardized representation and sharing of physics simulation data, with benchmarks available for community participation.

Conclusion: PLAID addresses key limitations in physics simulation datasets, fostering broader adoption of machine learning in scientific workflows.

Abstract: Machine learning-based surrogate models have emerged as a powerful tool to
accelerate simulation-driven scientific workflows. However, their widespread
adoption is hindered by the lack of large-scale, diverse, and standardized
datasets tailored to physics-based simulations. While existing initiatives
provide valuable contributions, many are limited in scope-focusing on specific
physics domains, relying on fragmented tooling, or adhering to overly
simplistic datamodels that restrict generalization. To address these
limitations, we introduce PLAID (Physics-Learning AI Datamodel), a flexible and
extensible framework for representing and sharing datasets of physics
simulations. PLAID defines a unified standard for describing simulation data
and is accompanied by a library for creating, reading, and manipulating complex
datasets across a wide range of physical use cases (gitlab.com/drti/plaid). We
release six carefully crafted datasets under the PLAID standard, covering
structural mechanics and computational fluid dynamics, and provide baseline
benchmarks using representative learning methods. Benchmarking tools are made
available on Hugging Face, enabling direct participation by the community and
contribution to ongoing evaluation efforts (huggingface.co/PLAIDcompetitions).

</details>


### [356] [Uncovering the Limitations of Model Inversion Evaluation -- Benchmarks and Connection to Type-I Adversarial Attacks](https://arxiv.org/pdf/2505.03519)
*Sy-Tuyen Ho, Koh Jun Hao, Ngoc-Bao Nguyen, Alexander Binder, Ngai-Man Cheung*

Main category: cs.LG

TL;DR: The paper critiques the current Model Inversion (MI) attack evaluation framework, revealing false positives and overestimated success rates. It introduces a human-annotated dataset, analyzes causes of errors, and suggests human evaluation as a primary method.


<details>
  <summary>Details</summary>
Motivation: To address limitations in the MI evaluation framework, which may overestimate attack success due to false positives, and to explore the impact of adversarial features on MI evaluation.

Method: Constructed a human-annotated dataset from 28 MI attack/defense setups, analyzed evaluation accuracy, and conducted controlled experiments to identify causes of false positives.

Result: Found significant false positives in MI evaluation, linking adversarial features to errors, and showing actual privacy leakage is lower than reported.

Conclusion: Advocates for human evaluation as primary and calls for more robust automatic frameworks, highlighting overestimation in prior MI attack success rates.

Abstract: Model Inversion (MI) attacks aim to reconstruct information of private
training data by exploiting access to machine learning models. The most common
evaluation framework for MI attacks/defenses relies on an evaluation model that
has been utilized to assess progress across almost all MI attacks and defenses
proposed in recent years. In this paper, for the first time, we present an
in-depth study of MI evaluation. Firstly, we construct the first comprehensive
human-annotated dataset of MI attack samples, based on 28 setups of different
MI attacks, defenses, private and public datasets. Secondly, using our dataset,
we examine the accuracy of the MI evaluation framework and reveal that it
suffers from a significant number of false positives. These findings raise
questions about the previously reported success rates of SOTA MI attacks.
Thirdly, we analyze the causes of these false positives, design controlled
experiments, and discover the surprising effect of Type I adversarial features
on MI evaluation, as well as adversarial transferability, highlighting a
relationship between two previously distinct research areas. Our findings
suggest that the performance of SOTA MI attacks has been overestimated, with
the actual privacy leakage being significantly less than previously reported.
In conclusion, we highlight critical limitations in the widely used MI
evaluation framework and present our methods to mitigate false positive rates.
We remark that prior research has shown that Type I adversarial attacks are
very challenging, with no existing solution. Therefore, we urge to consider
human evaluation as a primary MI evaluation framework rather than merely a
supplement as in previous MI research. We also encourage further work on
developing more robust and reliable automatic evaluation frameworks.

</details>


### [357] [Neural Integral Operators for Inverse problems in Spectroscopy](https://arxiv.org/pdf/2505.03677)
*Emanuele Zappala, Alice Giola, Andreas Kramer, Enrico Greco*

Main category: cs.LG

TL;DR: A deep learning method for molecular spectra classification is introduced, addressing overfitting in small datasets by leveraging integral operators and inverse problem formulations, outperforming traditional and other deep learning models.


<details>
  <summary>Details</summary>
Motivation: Deep learning struggles with small datasets in spectroscopy due to overfitting, while traditional methods lack accuracy and applicability.

Method: The method uses integral operators and inverse problem formulations to reduce overfitting in deep learning for small datasets.

Result: The model outperforms traditional methods (e.g., decision trees, SVMs) and other deep learning models on small datasets.

Conclusion: The approach successfully combines deep learning's power with robustness to limited data, addressing a key challenge in spectroscopy.

Abstract: Deep learning has shown high performance on spectroscopic inverse problems
when sufficient data is available. However, it is often the case that data in
spectroscopy is scarce, and this usually causes severe overfitting problems
with deep learning methods. Traditional machine learning methods are viable
when datasets are smaller, but the accuracy and applicability of these methods
is generally more limited. We introduce a deep learning method for
classification of molecular spectra based on learning integral operators via
integral equations of the first kind, which results in an algorithm that is
less affected by overfitting issues on small datasets, compared to other deep
learning models. The problem formulation of the deep learning approach is based
on inverse problems, which have traditionally found important applications in
spectroscopy. We perform experiments on real world data to showcase our
algorithm. It is seen that the model outperforms traditional machine learning
approaches such as decision tree and support vector machine, and for small
datasets it outperforms other deep learning models. Therefore, our methodology
leverages the power of deep learning, still maintaining the performance when
the available data is very limited, which is one of the main issues that deep
learning faces in spectroscopy, where datasets are often times of small size.

</details>


### [358] [MolMole: Molecule Mining from Scientific Literature](https://arxiv.org/pdf/2505.03777)
*LG AI Research, Sehyun Chun, Jiye Kim, Ahra Jo, Yeonsik Jo, Seungyul Oh, Seungjun Lee, Kwangrok Ryoo, Jongmin Lee, Seung Hwan Kim, Byung Jun Kang, Soonyoung Lee, Jun Ha Park, Chanwoo Moon, Jiwon Ham, Haein Lee, Heejae Han, Jaeseung Byun, Soojong Do, Minju Ha, Dongyun Kim, Kyunghoon Bae, Woohyung Lim, Edward Hwayoung Lee, Yongmin Park, Jeongsang Yu, Gerrard Jeongwon Jo, Yeonjung Hong, Kyungjae Yoo, Sehui Han, Jaewan Lee, Changyoung Park, Kijeong Jeon, Sihyuk Yi*

Main category: cs.LG

TL;DR: MolMole is a vision-based deep learning framework for extracting chemical data from documents, outperforming existing tools and introducing a new benchmark.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in extracting molecular structures and reaction data from unstructured chemical formats and complex document layouts.

Method: MolMole unifies molecule detection, reaction diagram parsing, and OCSR into a single pipeline.

Result: It outperforms existing toolkits on both the new benchmark and public datasets.

Conclusion: MolMole and its benchmark will be publicly available, with commercial inquiries directed to LG AI Research.

Abstract: The extraction of molecular structures and reaction data from scientific
documents is challenging due to their varied, unstructured chemical formats and
complex document layouts. To address this, we introduce MolMole, a vision-based
deep learning framework that unifies molecule detection, reaction diagram
parsing, and optical chemical structure recognition (OCSR) into a single
pipeline for automating the extraction of chemical data directly from
page-level documents. Recognizing the lack of a standard page-level benchmark
and evaluation metric, we also present a testset of 550 pages annotated with
molecule bounding boxes, reaction labels, and MOLfiles, along with a novel
evaluation metric. Experimental results demonstrate that MolMole outperforms
existing toolkits on both our benchmark and public datasets. The benchmark
testset will be publicly available, and the MolMole toolkit will be accessible
soon through an interactive demo on the LG AI Research website. For commercial
inquiries, please contact us at
\href{mailto:contact_ddu@lgresearch.ai}{contact\_ddu@lgresearch.ai}.

</details>


### [359] [Iterative Orthogonalization Scaling Laws](https://arxiv.org/pdf/2505.04005)
*Devan Selvaraj*

Main category: cs.LG

TL;DR: The paper examines scaling issues in the muon optimizer's iterative orthogonalization at large scales, showing theoretical and empirical evidence but no solutions.


<details>
  <summary>Details</summary>
Motivation: To understand the scaling behavior of muon optimizer's hyper-parameters and identify potential issues in large-scale applications.

Method: Theoretical analysis and empirical validation on random matrices to demonstrate scaling behavior.

Result: Singular values of random matrices shrink with scale, impacting muon's orthogonalization procedure.

Conclusion: The paper highlights a scaling issue in muon but does not propose solutions.

Abstract: The muon optimizer has picked up much attention as of late as a possible
replacement to the seemingly omnipresent Adam optimizer. Recently, care has
been taken to document the scaling laws of hyper-parameters under muon such as
weight decay and learning rate. However, at much larger scales the iterative
orthogonalization procedure present in muon may suffer a possible issue as the
singular values of random matrices shrink with scale. This paper shows this
scaling behavior theoretically and empirically on random matrices but does not
suggest what to do about it.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [360] [Empowering Scientific Workflows with Federated Agents](https://arxiv.org/pdf/2505.05428)
*J. Gregory Pauloski, Yadu Babuji, Ryan Chard, Mansi Sakarvadia, Kyle Chard, Ian Foster*

Main category: cs.MA

TL;DR: Academy is a middleware for deploying autonomous agents in research cyberinfrastructure, supporting asynchronous execution, heterogeneous resources, and high-throughput data flows.


<details>
  <summary>Details</summary>
Motivation: Existing agentic frameworks lack compatibility with research cyberinfrastructure, limiting their use in scientific computing.

Method: Academy is introduced as a modular and extensible middleware, providing abstractions for stateful agents, inter-agent coordination, and computation-experimental control integration.

Result: Microbenchmarks show high performance and scalability in HPC environments. Case studies in materials discovery, decentralized learning, and information extraction demonstrate its versatility.

Conclusion: Academy enables agentic workflows across diverse HPC systems, addressing the needs of scientific computing.

Abstract: Agentic systems, in which diverse agents cooperate to tackle challenging
problems, are exploding in popularity in the AI community. However, the agentic
frameworks used to build these systems have not previously enabled use with
research cyberinfrastructure. Here we introduce Academy, a modular and
extensible middleware designed to deploy autonomous agents across the federated
research ecosystem, including HPC systems, experimental facilities, and data
repositories. To meet the demands of scientific computing, Academy supports
asynchronous execution, heterogeneous resources, high-throughput data flows,
and dynamic resource availability. It provides abstractions for expressing
stateful agents, managing inter-agent coordination, and integrating computation
with experimental control. We present microbenchmark results that demonstrate
high performance and scalability in HPC environments. To demonstrate the
breadth of applications that can be supported by agentic workflow designs, we
also present case studies in materials discovery, decentralized learning, and
information extraction in which agents are deployed across diverse HPC systems.

</details>


### [361] [Robust Coordination under Misaligned Communication via Power Regularization](https://arxiv.org/pdf/2404.06387)
*Nancirose Piazza, Amirhossein Karimia, Behnia Soleymanib, Vahid Behzadan, Stefan Sarkadi*

Main category: cs.MA

TL;DR: CPR extends power regularization to communication in MARL, mitigating adversarial communication risks while maintaining cooperation.


<details>
  <summary>Details</summary>
Motivation: Address vulnerabilities in MARL due to misaligned or adversarial communication, overlooked in prior power regularization methods.

Method: Introduces Communicative Power Regularization (CPR) to quantify and constrain agents' communicative influence during training.

Result: CPR enhances robustness to adversarial communication in benchmark environments without compromising cooperative performance.

Conclusion: CPR provides a practical framework for secure and resilient cooperative MARL systems.

Abstract: Effective communication in Multi-Agent Reinforcement Learning (MARL) can
significantly enhance coordination and collaborative performance in complex and
partially observable environments. However, reliance on communication can also
introduce vulnerabilities when agents are misaligned, potentially leading to
adversarial interactions that exploit implicit assumptions of cooperative
intent. Prior work has addressed adversarial behavior through power
regularization through controlling the influence one agent exerts over another,
but has largely overlooked the role of communication in these dynamics. This
paper introduces Communicative Power Regularization (CPR), extending power
regularization specifically to communication channels. By explicitly
quantifying and constraining agents' communicative influence during training,
CPR actively mitigates vulnerabilities arising from misaligned or adversarial
communications. Evaluations across benchmark environments Red-Door-Blue-Door,
Predator-Prey, and Grid Coverage demonstrate that our approach significantly
enhances robustness to adversarial communication while preserving cooperative
performance, offering a practical framework for secure and resilient
cooperative MARL systems.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [362] [SSH-Net: A Self-Supervised and Hybrid Network for Noisy Image Watermark Removal](https://arxiv.org/pdf/2505.05088)
*Wenyang Liu, Jianjun Gao, Kim-Hui Yap*

Main category: cs.MM

TL;DR: SSH-Net is a self-supervised hybrid network for noisy image watermark removal, using dual networks (CNN and Transformer) and a shared feature encoder.


<details>
  <summary>Details</summary>
Motivation: Existing supervised methods require impractical paired datasets, prompting a self-supervised solution.

Method: SSH-Net synthesizes reference images self-supervisedly, uses a lightweight CNN for noise removal, and a Transformer-based network for watermark and noise removal, with a shared feature encoder.

Result: The dual-network design effectively removes watermarks and noise, leveraging self-supervised learning.

Conclusion: SSH-Net offers a practical solution for watermark removal without paired datasets, combining CNN and Transformer strengths.

Abstract: Visible watermark removal is challenging due to its inherent complexities and
the noise carried within images. Existing methods primarily rely on supervised
learning approaches that require paired datasets of watermarked and
watermark-free images, which are often impractical to obtain in real-world
scenarios. To address this challenge, we propose SSH-Net, a Self-Supervised and
Hybrid Network specifically designed for noisy image watermark removal. SSH-Net
synthesizes reference watermark-free images using the watermark distribution in
a self-supervised manner and adopts a dual-network design to address the task.
The upper network, focused on the simpler task of noise removal, employs a
lightweight CNN-based architecture, while the lower network, designed to handle
the more complex task of simultaneously removing watermarks and noise,
incorporates Transformer blocks to model long-range dependencies and capture
intricate image features. To enhance the model's effectiveness, a shared
CNN-based feature encoder is introduced before dual networks to extract common
features that both networks can leverage. Our code will be available at
https://github.com/wenyang001/SSH-Net.

</details>


### [363] [TCAN: Text-oriented Cross Attention Network for Multimodal Sentiment Analysis](https://arxiv.org/pdf/2404.04545)
*Weize Quan, Yunfei Feng, Ming Zhou, Yunzhen Zhao, Tong Wang, Dong-Ming Yan*

Main category: cs.MM

TL;DR: The paper introduces TCAN, a Text-oriented Cross-Attention Network for Multimodal Sentiment Analysis, emphasizing text modality dominance to address multimodal heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Existing MSA methods often treat all modalities uniformly, ignoring varying semantic richness, leading to imbalanced contributions. TCAN aims to prioritize the text modality while mitigating noise.

Method: TCAN uses unaligned sequences of text, visual, and acoustic inputs, applies self-attention to text, and text-queried cross-attention to other modalities. A gated control mechanism reduces noise, and unimodal joint learning enhances emotional understanding.

Result: TCAN outperforms state-of-the-art MSA methods on CMU-MOSI and CMU-MOSEI datasets.

Conclusion: TCAN effectively addresses multimodal heterogeneity by prioritizing text and reducing noise, achieving superior performance in sentiment analysis.

Abstract: Multimodal Sentiment Analysis (MSA) endeavors to understand human sentiment
by leveraging language, visual, and acoustic modalities. Despite the remarkable
performance exhibited by previous MSA approaches, the presence of inherent
multimodal heterogeneities poses a challenge, with the contribution of
different modalities varying considerably. Past research predominantly focused
on improving representation learning techniques and feature fusion strategies.
However, many of these efforts overlooked the variation in semantic richness
among different modalities, treating each modality uniformly. This approach may
lead to underestimating the significance of strong modalities while
overemphasizing the importance of weak ones. Motivated by these insights, we
introduce a Text-oriented Cross-Attention Network (TCAN), emphasizing the
predominant role of the text modality in MSA. Specifically, for each multimodal
sample, by taking unaligned sequences of the three modalities as inputs, we
initially allocate the extracted unimodal features into a visual-text and an
acoustic-text pair. Subsequently, we implement self-attention on the text
modality and apply text-queried cross-attention to the visual and acoustic
modalities. To mitigate the influence of noise signals and redundant features,
we incorporate a gated control mechanism into the framework. Additionally, we
introduce unimodal joint learning to gain a deeper understanding of homogeneous
emotional tendencies across diverse modalities through backpropagation.
Experimental results demonstrate that TCAN consistently outperforms
state-of-the-art MSA methods on two datasets (CMU-MOSI and CMU-MOSEI).

</details>


### [364] [Adaptive Rate Control for Deep Video Compression with Rate-Distortion Prediction](https://arxiv.org/pdf/2412.18834)
*Bowen Gu, Hao Chen, Ming Lu, Jie Yao, Zhan Ma*

Main category: cs.MM

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Deep video compression has made significant progress in recent years,
achieving rate-distortion performance that surpasses that of traditional video
compression methods. However, rate control schemes tailored for deep video
compression have not been well studied. In this paper, we propose a neural
network-based $\lambda$-domain rate control scheme for deep video compression,
which determines the coding parameter $\lambda$ for each to-be-coded frame
based on the rate-distortion-$\lambda$ (R-D-$\lambda$) relationships directly
learned from uncompressed frames, achieving high rate control accuracy
efficiently without the need for pre-encoding. Moreover, this content-aware
scheme is able to mitigate inter-frame quality fluctuations and adapt to abrupt
changes in video content. Specifically, we introduce two neural network-based
predictors to estimate the relationship between bitrate and $\lambda$, as well
as the relationship between distortion and $\lambda$ for each frame. Then we
determine the coding parameter $\lambda$ for each frame to achieve the target
bitrate. Experimental results demonstrate that our approach achieves high rate
control accuracy at the mini-GOP level with low time overhead and mitigates
inter-frame quality fluctuations across video content of varying resolutions.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [365] [From Dialect Gaps to Identity Maps: Tackling Variability in Speaker Verification](https://arxiv.org/pdf/2505.04629)
*Abdulhady Abas Abdullah, Soran Badawi, Dana A. Abdullah, Dana Rasul Hamad, Hanan Abdulrahman Taher, Sabat Salih Muhamad, Aram Mahmood Ahmed, Bryar A. Hassan, Sirwan Abdolwahed Aula, Tarik A. Rashid*

Main category: eess.AS

TL;DR: The paper explores challenges in Kurdish speaker detection across dialects, proposing machine learning and data augmentation to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Kurdish dialects (Kurmanji, Sorani, Hawrami) pose unique challenges for speaker recognition due to phonetic and lexical differences.

Method: Uses advanced machine learning, data augmentation, and dialect-specific corpus building.

Result: Customized strategies and cross-dialect training significantly improve recognition performance.

Conclusion: Tailored approaches for each dialect enhance the reliability of Kurdish speaker detection systems.

Abstract: The complexity and difficulties of Kurdish speaker detection among its
several dialects are investigated in this work. Because of its great phonetic
and lexical differences, Kurdish with several dialects including Kurmanji,
Sorani, and Hawrami offers special challenges for speaker recognition systems.
The main difficulties in building a strong speaker identification system
capable of precisely identifying speakers across several dialects are
investigated in this work. To raise the accuracy and dependability of these
systems, it also suggests solutions like sophisticated machine learning
approaches, data augmentation tactics, and the building of thorough
dialect-specific corpus. The results show that customized strategies for every
dialect together with cross-dialect training greatly enhance recognition
performance.

</details>


### [366] [Listen to Extract: Onset-Prompted Target Speaker Extraction](https://arxiv.org/pdf/2505.05114)
*Pengjie Shen, Kangrui Chen, Shulin He, Pengru Chen, Shuqi Yuan, He Kong, Xueliang Zhang, Zhong-Qiu Wang*

Main category: eess.AS

TL;DR: LExt is a simple yet effective algorithm for monaural target speaker extraction (TSE) by concatenating an enrollment utterance to the mixture signal and training DNNs to extract the target speech.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of extracting a target speaker's speech from mixed audio with other speakers.

Method: Concatenate an enrollment utterance of the target speaker to the mixture signal at the waveform level and train DNNs to extract the target speech.

Result: Strong TSE performance achieved on public datasets (WSJ0-2mix, WHAM!, WHAMR!).

Conclusion: LExt's simplicity and effectiveness make it a promising approach for TSE tasks.

Abstract: We propose $\textit{listen to extract}$ (LExt), a highly-effective while
extremely-simple algorithm for monaural target speaker extraction (TSE). Given
an enrollment utterance of a target speaker, LExt aims at extracting the target
speaker from the speaker's mixed speech with other speakers. For each mixture,
LExt concatenates an enrollment utterance of the target speaker to the mixture
signal at the waveform level, and trains deep neural networks (DNN) to extract
the target speech based on the concatenated mixture signal. The rationale is
that, this way, an artificial speech onset is created for the target speaker
and it could prompt the DNN (a) which speaker is the target to extract; and (b)
spectral-temporal patterns of the target speaker that could help extraction.
This simple approach produces strong TSE performance on multiple public TSE
datasets including WSJ0-2mix, WHAM! and WHAMR!.

</details>


### [367] [Regression-based Melody Estimation with Uncertainty Quantification](https://arxiv.org/pdf/2505.05156)
*Kavya Ranjan Saxena, Vipul Arora*

Main category: eess.AS

TL;DR: The paper proposes a regression-based approach for melody estimation from polyphonic audio, improving over classification methods by capturing finer pitch variations and introducing uncertainty prediction.


<details>
  <summary>Details</summary>
Motivation: Existing classification-based methods discretize pitch values, losing finer frequency variations. The paper aims to better capture these variations and enhance model trustworthiness by predicting uncertainty.

Method: Three methods are proposed: two address discontinuity between voiced and unvoiced ranges by mapping them to a continuous range, and the third reformulates the task as a Bayesian problem. A novel uncertainty estimation method is also introduced.

Result: Reformulating melody estimation as a regression problem outperforms classification-based approaches. The Bayesian method performs best in melody and uncertainty estimation.

Conclusion: Regression-based melody estimation with uncertainty prediction improves performance, with the Bayesian method being the most effective.

Abstract: Existing machine learning models approach the task of melody estimation from
polyphonic audio as a classification problem by discretizing the pitch values,
which results in the loss of finer frequency variations present in the melody.
To better capture these variations, we propose to approach this task as a
regression problem. Apart from predicting only the pitch for a particular
region in the audio, we also predict its uncertainty to enhance the
trustworthiness of the model. To perform regression-based melody estimation, we
propose three different methods that use histogram representation to model the
pitch values. Such a representation requires the support range of the histogram
to be continuous. The first two methods address the abrupt discontinuity
between unvoiced and voiced frequency ranges by mapping them to a continuous
range. The third method reformulates melody estimation as a fully Bayesian
task, modeling voicing detection as a classification problem, and voiced pitch
estimation as a regression problem. Additionally, we introduce a novel method
to estimate the uncertainty from the histogram representation that correlates
well with the deviation of the mean of the predicted distribution from the
ground truth. Experimental results demonstrate that reformulating melody
estimation as a regression problem significantly improves the performance over
classification-based approaches. Comparing the proposed methods with a
state-of-the-art regression model, it is observed that the Bayesian method
performs the best at estimating both the melody and its associated uncertainty.

</details>


### [368] [FlexSpeech: Towards Stable, Controllable and Expressive Text-to-Speech](https://arxiv.org/pdf/2505.05159)
*Linhan Ma, Dake Guo, He Wang, Jin Xu, Lei Xie*

Main category: eess.AS

TL;DR: FlexSpeech combines autoregressive (AR) and non-autoregressive (NAR) methods for stable, controllable, and expressive speech generation, achieving SOTA results in zero-shot TTS and lightweight style transfer.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between stability (NAR) and naturalness (AR) in speech generation by integrating Markov dependencies and preference optimization into duration prediction.

Method: Decomposes speech generation into an AR duration predictor and a NAR acoustic model, optimizing the former for style transfer while keeping the latter stable.

Result: Achieves state-of-the-art stability and naturalness in zero-shot TTS, with rapid style transfer using only ~100 data samples.

Conclusion: FlexSpeech successfully balances stability and naturalness, enabling efficient and high-quality speech generation and style adaptation.

Abstract: Current speech generation research can be categorized into two primary
classes: non-autoregressive and autoregressive. The fundamental distinction
between these approaches lies in the duration prediction strategy employed for
predictable-length sequences. The NAR methods ensure stability in speech
generation by explicitly and independently modeling the duration of each
phonetic unit. Conversely, AR methods employ an autoregressive paradigm to
predict the compressed speech token by implicitly modeling duration with Markov
properties. Although this approach improves prosody, it does not provide the
structural guarantees necessary for stability. To simultaneously address the
issues of stability and naturalness in speech generation, we propose
FlexSpeech, a stable, controllable, and expressive TTS model. The motivation
behind FlexSpeech is to incorporate Markov dependencies and preference
optimization directly on the duration predictor to boost its naturalness while
maintaining explicit modeling of the phonetic units to ensure stability.
Specifically, we decompose the speech generation task into two components: an
AR duration predictor and a NAR acoustic model. The acoustic model is trained
on a substantial amount of data to learn to render audio more stably, given
reference audio prosody and phone durations. The duration predictor is
optimized in a lightweight manner for different stylistic variations, thereby
enabling rapid style transfer while maintaining a decoupled relationship with
the specified speaker timbre. Experimental results demonstrate that our
approach achieves SOTA stability and naturalness in zero-shot TTS. More
importantly, when transferring to a specific stylistic domain, we can
accomplish lightweight optimization of the duration module solely with about
100 data samples, without the need to adjust the acoustic model, thereby
enabling rapid and stable style transfer.

</details>


### [369] [Normalize Everything: A Preconditioned Magnitude-Preserving Architecture for Diffusion-Based Speech Enhancement](https://arxiv.org/pdf/2505.05216)
*Julius Richter, Danilo de Oliveira, Timo Gerkmann*

Main category: eess.AS

TL;DR: A new diffusion-based speech enhancement framework using a Schroedinger bridge, preconditioning, and skip connections, with improved performance through magnitude-preserving architecture and EMA analysis.


<details>
  <summary>Details</summary>
Motivation: To enhance noisy speech by transforming its distribution into clean speech using a novel diffusion-based approach.

Method: Employs a Schroedinger bridge, preconditioning, skip connections, and a magnitude-preserving architecture. Analyzes EMA profiles for performance.

Result: Improved performance on speech enhancement metrics; shorter EMA lengths outperform longer ones.

Conclusion: The framework effectively enhances speech, with shorter EMA lengths yielding better results.

Abstract: This paper presents a new framework for diffusion-based speech enhancement.
Our method employs a Schroedinger bridge to transform the noisy speech
distribution into the clean speech distribution. To stabilize and improve
training, we employ time-dependent scalings of the inputs and outputs of the
network, known as preconditioning. We consider two skip connection
configurations, which either include or omit the current process state in the
denoiser's output, enabling the network to predict either environmental noise
or clean speech. Each approach leads to improved performance on different
speech enhancement metrics. To maintain stable magnitude levels and balance
during training, we use a magnitude-preserving network architecture that
normalizes all activations and network weights to unit length. Additionally, we
propose learning the contribution of the noisy input within each network block
for effective input conditioning. After training, we apply a method to
approximate different exponential moving average (EMA) profiles and investigate
their effects on the speech enhancement performance. In contrast to image
generation tasks, where longer EMA lengths often enhance mode coverage, we
observe that shorter EMA lengths consistently lead to better performance on
standard speech enhancement metrics. Code, audio examples, and checkpoints are
available online.

</details>


### [370] [The Search for Squawk: Agile Modeling in Bioacoustics](https://arxiv.org/pdf/2505.03071)
*Vincent Dumoulin, Otilia Stretcu, Jenny Hamer, Lauren Harrell, Rob Laber, Hugo Larochelle, Bart van Merriënboer, Amanda Navine, Patrick Hart, Ben Williams, Timothy A. C. Lamont, Tries B. Rasak, Mars Coral Restoration Team, Sheryn Brodie, Brendan Doohan, Phil Eichinski, Paul Roe, Lin Schwarzkopf, Tom Denton*

Main category: eess.AS

TL;DR: A scalable, data-efficient system for bioacoustic recognizers reduces development time to under an hour, using pre-trained embeddings, indexed audio search, and active learning.


<details>
  <summary>Details</summary>
Motivation: Passive acoustic monitoring (PAM) faces challenges in extracting insights from vast audio data due to the need for specialized recognizers and large training datasets.

Method: The system employs generalizable acoustic embeddings, indexed audio search for dataset creation, and precomputed embeddings for active learning.

Result: Successfully applied in three case studies (coral reef health, juvenile Hawaiian bird calls, Christmas Island bird occupancy) and validated through simulated experiments.

Conclusion: The system is scalable, efficient, and generalizable, enabling rapid solutions for new bioacoustic challenges.

Abstract: Passive acoustic monitoring (PAM) has shown great promise in helping
ecologists understand the health of animal populations and ecosystems. However,
extracting insights from millions of hours of audio recordings requires the
development of specialized recognizers. This is typically a challenging task,
necessitating large amounts of training data and machine learning expertise. In
this work, we introduce a general, scalable and data-efficient system for
developing recognizers for novel bioacoustic problems in under an hour. Our
system consists of several key components that tackle problems in previous
bioacoustic workflows: 1) highly generalizable acoustic embeddings pre-trained
for birdsong classification minimize data hunger; 2) indexed audio search
allows the efficient creation of classifier training datasets, and 3)
precomputation of embeddings enables an efficient active learning loop,
improving classifier quality iteratively with minimal wait time. Ecologists
employed our system in three novel case studies: analyzing coral reef health
through unidentified sounds; identifying juvenile Hawaiian bird calls to
quantify breeding success and improve endangered species monitoring; and
Christmas Island bird occupancy modeling. We augment the case studies with
simulated experiments which explore the range of design decisions in a
structured way and help establish best practices. Altogether these experiments
showcase our system's scalability, efficiency, and generalizability, enabling
scientists to quickly address new bioacoustic challenges.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [371] [Rethinking Boundary Detection in Deep Learning-Based Medical Image Segmentation](https://arxiv.org/pdf/2505.04652)
*Yi Lin, Dong Zhang, Xiao Fang, Yufan Chen, Kwang-Ting Cheng, Hao Chen*

Main category: eess.IV

TL;DR: CTO, a novel network combining CNNs, ViT, and edge detection, improves medical image segmentation accuracy, especially in boundary areas, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Precise segmentation of boundary areas in medical images remains challenging despite advancements in segmentation methods.

Method: CTO uses a dual-stream encoder (CNN for local features, StitchViT for long-range dependencies) and a boundary-guided decoder with edge detection operators.

Result: CTO achieves state-of-the-art accuracy on seven medical image datasets without extra data or labels.

Conclusion: CTO offers a balanced, efficient solution for medical image segmentation, particularly in boundary areas.

Abstract: Medical image segmentation is a pivotal task within the realms of medical
image analysis and computer vision. While current methods have shown promise in
accurately segmenting major regions of interest, the precise segmentation of
boundary areas remains challenging. In this study, we propose a novel network
architecture named CTO, which combines Convolutional Neural Networks (CNNs),
Vision Transformer (ViT) models, and explicit edge detection operators to
tackle this challenge. CTO surpasses existing methods in terms of segmentation
accuracy and strikes a better balance between accuracy and efficiency, without
the need for additional data inputs or label injections. Specifically, CTO
adheres to the canonical encoder-decoder network paradigm, with a dual-stream
encoder network comprising a mainstream CNN stream for capturing local features
and an auxiliary StitchViT stream for integrating long-range dependencies.
Furthermore, to enhance the model's ability to learn boundary areas, we
introduce a boundary-guided decoder network that employs binary boundary masks
generated by dedicated edge detection operators to provide explicit guidance
during the decoding process. We validate the performance of CTO through
extensive experiments conducted on seven challenging medical image segmentation
datasets, namely ISIC 2016, PH2, ISIC 2018, CoNIC, LiTS17, and BTCV. Our
experimental results unequivocally demonstrate that CTO achieves
state-of-the-art accuracy on these datasets while maintaining competitive model
complexity. The codes have been released at:
https://github.com/xiaofang007/CTO.

</details>


### [372] [EvEnhancer: Empowering Effectiveness, Efficiency and Generalizability for Continuous Space-Time Video Super-Resolution with Events](https://arxiv.org/pdf/2505.04657)
*Shuoyan Wei, Feng Li, Shengeng Tang, Yao Zhao, Huihui Bai*

Main category: eess.IV

TL;DR: EvEnhancer combines event streams with C-STVSR for superior video upscaling at arbitrary scales, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing C-STVSR methods fail at out-of-distribution scales, while event streams offer high temporal resolution and dynamic range, making them promising for enhancing video super-resolution.

Method: EvEnhancer uses event-adapted synthesis for motion trajectory learning and local implicit video transformer for continuous video representation, enabling adaptive interpolation and fusion.

Result: EvEnhancer excels on synthetic and real-world datasets and shows better generalizability at out-of-distribution scales.

Conclusion: The integration of event streams with C-STVSR in EvEnhancer significantly improves effectiveness, efficiency, and generalizability.

Abstract: Continuous space-time video super-resolution (C-STVSR) endeavors to upscale
videos simultaneously at arbitrary spatial and temporal scales, which has
recently garnered increasing interest. However, prevailing methods struggle to
yield satisfactory videos at out-of-distribution spatial and temporal scales.
On the other hand, event streams characterized by high temporal resolution and
high dynamic range, exhibit compelling promise in vision tasks. This paper
presents EvEnhancer, an innovative approach that marries the unique advantages
of event streams to elevate effectiveness, efficiency, and generalizability for
C-STVSR. Our approach hinges on two pivotal components: 1) Event-adapted
synthesis capitalizes on the spatiotemporal correlations between frames and
events to discern and learn long-term motion trajectories, enabling the
adaptive interpolation and fusion of informative spatiotemporal features; 2)
Local implicit video transformer integrates local implicit video neural
function with cross-scale spatiotemporal attention to learn continuous video
representations utilized to generate plausible videos at arbitrary resolutions
and frame rates. Experiments show that EvEnhancer achieves superiority on
synthetic and real-world datasets and preferable generalizability on
out-of-distribution scales against state-of-the-art methods. Code is available
at https://github.com/W-Shuoyan/EvEnhancer.

</details>


### [373] [Cross-organ all-in-one parallel compressed sensing magnetic resonance imaging](https://arxiv.org/pdf/2505.04658)
*Baoshun Shi, Zheng Liu, Xin Meng, Yan Yang*

Main category: eess.IV

TL;DR: CAPNet is a unified deep learning framework for p-CSMRI that avoids training separate models for each organ by integrating artifact and organ-specific features into a single network.


<details>
  <summary>Details</summary>
Motivation: Current p-CSMRI methods require organ-specific DNNs due to anatomical variations, limiting generalization. CAPNet aims to overcome this by unifying reconstruction across organs.

Method: CAPNet uses three modules: auxiliary variable, prior, and data consistency, with artifact generation and organ structure-prompt submodules to handle varying sampling ratios and anatomical features.

Result: CAPNet achieves state-of-the-art reconstruction performance on a cross-organ dataset with a single model.

Conclusion: CAPNet provides a generalized solution for p-CSMRI, eliminating the need for organ-specific models while maintaining high reconstruction quality.

Abstract: Recent advances in deep learning-based parallel compressed sensing magnetic
resonance imaging (p-CSMRI) have significantly improved reconstruction quality.
However, current p-CSMRI methods often require training separate deep neural
network (DNN) for each organ due to anatomical variations, creating a barrier
to developing generalized medical image reconstruction systems. To address
this, we propose CAPNet (cross-organ all-in-one deep unfolding p-CSMRI
network), a unified framework that implements a p-CSMRI iterative algorithm via
three specialized modules: auxiliary variable module, prior module, and data
consistency module. Recognizing that p-CSMRI systems often employ varying
sampling ratios for different organs, resulting in organ-specific artifact
patterns, we introduce an artifact generation submodule, which extracts and
integrates artifact features into the data consistency module to enhance the
discriminative capability of the overall network. For the prior module, we
design an organ structure-prompt generation submodule that leverages structural
features extracted from the segment anything model (SAM) to create cross-organ
prompts. These prompts are strategically incorporated into the prior module
through an organ structure-aware Mamba submodule. Comprehensive evaluations on
a cross-organ dataset confirm that CAPNet achieves state-of-the-art
reconstruction performance across multiple anatomical structures using a single
unified model. Our code will be published at
https://github.com/shibaoshun/CAPNet.

</details>


### [374] [Advancing 3D Medical Image Segmentation: Unleashing the Potential of Planarian Neural Networks in Artificial Intelligence](https://arxiv.org/pdf/2505.04664)
*Ziyuan Huang, Kevin Huggins, Srikar Bellur*

Main category: eess.IV

TL;DR: PNN-UNet, inspired by planarian neural networks, combines Deep-UNet and Wide-UNet with a dense autoencoder, outperforming UNet variants in 3D medical image segmentation.


<details>
  <summary>Details</summary>
Motivation: To leverage the structural efficiency of planarian neural networks (PNN) for improving 3D medical image segmentation.

Method: PNN-UNet uses Deep-UNet and Wide-UNet as neural cords and a dense autoencoder as the brain, mimicking PNN.

Result: Outperforms baseline UNet and other variants on 3D MRI hippocampus dataset, with and without augmentation.

Conclusion: PNN-UNet's biologically inspired architecture enhances segmentation performance in medical imaging.

Abstract: Our study presents PNN-UNet as a method for constructing deep neural networks
that replicate the planarian neural network (PNN) structure in the context of
3D medical image data. Planarians typically have a cerebral structure
comprising two neural cords, where the cerebrum acts as a coordinator, and the
neural cords serve slightly different purposes within the organism's
neurological system. Accordingly, PNN-UNet comprises a Deep-UNet and a
Wide-UNet as the nerve cords, with a densely connected autoencoder performing
the role of the brain. This distinct architecture offers advantages over both
monolithic (UNet) and modular networks (Ensemble-UNet). Our outcomes on a 3D
MRI hippocampus dataset, with and without data augmentation, demonstrate that
PNN-UNet outperforms the baseline UNet and several other UNet variants in image
segmentation.

</details>


### [375] [Convergent Complex Quasi-Newton Proximal Methods for Gradient-Driven Denoisers in Compressed Sensing MRI Reconstruction](https://arxiv.org/pdf/2505.04820)
*Tao Hong, Zhaoyi Xu, Se Young Chun, Luis Hernandez-Garcia, Jeffrey A. Fessler*

Main category: eess.IV

TL;DR: A complex quasi-Newton proximal method is proposed for faster convergence in CS MRI reconstruction, addressing slow solvers and ensuring Hermitian positive definiteness in the complex domain.


<details>
  <summary>Details</summary>
Motivation: Model-based CS MRI methods struggle with effective priors and slow solvers for gradient-driven denoisers, despite their theoretical and practical advantages.

Method: Proposes a complex quasi-Newton proximal method with modified Hessian estimation for Hermitian positive definiteness, ensuring faster convergence.

Result: Numerical experiments show the method's effectiveness and efficiency on Cartesian and non-Cartesian sampling trajectories.

Conclusion: The proposed method bridges the gap between practical performance and theoretical guarantees, offering faster convergence for CS MRI reconstruction.

Abstract: In compressed sensing (CS) MRI, model-based methods are pivotal to achieving
accurate reconstruction. One of the main challenges in model-based methods is
finding an effective prior to describe the statistical distribution of the
target image. Plug-and-Play (PnP) and REgularization by Denoising (RED) are two
general frameworks that use denoisers as the prior. While PnP/RED methods with
convolutional neural networks (CNNs) based denoisers outperform classical
hand-crafted priors in CS MRI, their convergence theory relies on assumptions
that do not hold for practical CNNs. The recently developed gradient-driven
denoisers offer a framework that bridges the gap between practical performance
and theoretical guarantees. However, the numerical solvers for the associated
minimization problem remain slow for CS MRI reconstruction. This paper proposes
a complex quasi-Newton proximal method that achieves faster convergence than
existing approaches. To address the complex domain in CS MRI, we propose a
modified Hessian estimation method that guarantees Hermitian positive
definiteness. Furthermore, we provide a rigorous convergence analysis of the
proposed method for nonconvex settings. Numerical experiments on both Cartesian
and non-Cartesian sampling trajectories demonstrate the effectiveness and
efficiency of our approach.

</details>


### [376] [Advanced 3D Imaging Approach to TSV/TGV Metrology and Inspection Using Only Optical Microscopy](https://arxiv.org/pdf/2505.04913)
*Gugeong Sung*

Main category: eess.IV

TL;DR: A hybrid method combining photometric stereo with optical microscopy improves 3D reconstruction and defect detection in silicon and glass vias, outperforming conventional techniques.


<details>
  <summary>Details</summary>
Motivation: Conventional optical microscopy fails to effectively inspect internal structures of silicon and glass vias, necessitating a more advanced approach.

Method: The proposed method integrates photometric stereo with optical microscopy, using varied lighting for 3D reconstruction and defect visualization.

Result: The method captures intricate surface details and internal structures, with quantitative validation showing improved accuracy and repeatability.

Conclusion: The hybrid approach enhances cost-effectiveness and inspection quality, advancing silicon and glass via inspection techniques.

Abstract: This paper introduces an innovative approach to silicon and glass via
inspection, which combines hybrid field microscopy with photometric stereo.
Conventional optical microscopy techniques are generally limited to superficial
inspections and struggle to effectively visualize the internal structures of
silicon and glass vias. By utilizing various lighting conditions for 3D
reconstruction, the proposed method surpasses these limitations. By integrating
photometric stereo to the traditional optical microscopy, the proposed method
not only enhances the capability to detect micro-scale defects but also
provides a detailed visualization of depth and edge abnormality, which are
typically not visible with conventional optical microscopy inspection. The
experimental results demonstrated that the proposed method effectively captures
intricate surface details and internal structures. Quantitative comparisons
between the reconstructed models and actual measurements present the capability
of the proposed method to significantly improve silicon and glass via
inspection process. As a result, the proposed method achieves enhanced
cost-effectiveness while maintaining high accuracy and repeatability,
suggesting substantial advancements in silicon and glass via inspection
techniques

</details>


### [377] [MoRe-3DGSMR: Motion-resolved reconstruction framework for free-breathing pulmonary MRI based on 3D Gaussian representation](https://arxiv.org/pdf/2505.04959)
*Tengya Peng, Ruyi Zha, Qing Zou*

Main category: eess.IV

TL;DR: An unsupervised, motion-resolved 3D Gaussian representation (3DGS) framework for high-resolution pulmonary MRI, outperforming existing methods in image quality.


<details>
  <summary>Details</summary>
Motivation: To address challenges in motion-resolved 3D isotropic pulmonary MRI reconstruction by leveraging 3DGS for continuous spatial representation and motion correction.

Method: Uses golden-angle radial sampling, extracts respiratory motion signals, sorts k-space data into phases, applies 3DGS for reference volume reconstruction, and trains a patient-specific CNN for deformation vector fields (DVFs) to generate motion states.

Result: Superior image quality (higher SNR and CNR) compared to three state-of-the-art methods, validated on six datasets.

Conclusion: The 3DGS-based method is a robust, accurate solution for clinical pulmonary MRI, offering high-resolution, motion-resolved imaging.

Abstract: This study presents an unsupervised, motion-resolved reconstruction framework
for high-resolution, free-breathing pulmonary magnetic resonance imaging (MRI),
utilizing a three-dimensional Gaussian representation (3DGS). The proposed
method leverages 3DGS to address the challenges of motion-resolved 3D isotropic
pulmonary MRI reconstruction by enabling data smoothing between voxels for
continuous spatial representation. Pulmonary MRI data acquisition is performed
using a golden-angle radial sampling trajectory, with respiratory motion
signals extracted from the center of k-space in each radial spoke. Based on the
estimated motion signal, the k-space data is sorted into multiple respiratory
phases. A 3DGS framework is then applied to reconstruct a reference image
volume from the first motion state. Subsequently, a patient-specific
convolutional neural network is trained to estimate the deformation vector
fields (DVFs), which are used to generate the remaining motion states through
spatial transformation of the reference volume. The proposed reconstruction
pipeline is evaluated on six datasets from six subjects and bench-marked
against three state-of-the-art reconstruction methods. The experimental
findings demonstrate that the proposed reconstruction framework effectively
reconstructs high-resolution, motion-resolved pulmonary MR images. Compared
with existing approaches, it achieves superior image quality, reflected by
higher signal-to-noise ratio and contrast-to-noise ratio. The proposed
unsupervised 3DGS-based reconstruction method enables accurate motion-resolved
pulmonary MRI with isotropic spatial resolution. Its superior performance in
image quality metrics over state-of-the-art methods highlights its potential as
a robust solution for clinical pulmonary MR imaging.

</details>


### [378] [ADNP-15: An Open-Source Histopathological Dataset for Neuritic Plaque Segmentation in Human Brain Whole Slide Images with Frequency Domain Image Enhancement for Stain Normalization](https://arxiv.org/pdf/2505.05041)
*Chenxi Zhao, Jianqiang Li, Qing Zhao, Jing Bai, Susana Boluda, Benoit Delatour, Lev Stimmer, Daniel Racoceanu, Gabriel Jimenez, Guanghui Fu*

Main category: eess.IV

TL;DR: The paper introduces an open-source dataset (ADNP-15) for neuritic plaque segmentation in Alzheimer's Disease, evaluates deep learning models and stain normalization techniques, and proposes a novel image enhancement method to improve segmentation accuracy.


<details>
  <summary>Details</summary>
Motivation: The study addresses challenges in AD research, such as the lack of annotated datasets and staining variations, to improve automated segmentation of key histopathological features like amyloid-beta plaques and tau tangles.

Method: The authors evaluate five deep learning models and four stain normalization techniques on the ADNP-15 dataset, and introduce a novel image enhancement method to improve segmentation.

Result: The proposed enhancement method significantly boosts model generalization and segmentation accuracy, particularly in complex tissue structures.

Conclusion: The open-source dataset and code promote transparency and reproducibility, advancing research in AD pathology image analysis.

Abstract: Alzheimer's Disease (AD) is a neurodegenerative disorder characterized by
amyloid-beta plaques and tau neurofibrillary tangles, which serve as key
histopathological features. The identification and segmentation of these
lesions are crucial for understanding AD progression but remain challenging due
to the lack of large-scale annotated datasets and the impact of staining
variations on automated image analysis. Deep learning has emerged as a powerful
tool for pathology image segmentation; however, model performance is
significantly influenced by variations in staining characteristics,
necessitating effective stain normalization and enhancement techniques. In this
study, we address these challenges by introducing an open-source dataset
(ADNP-15) of neuritic plaques (i.e., amyloid deposits combined with a crown of
dystrophic tau-positive neurites) in human brain whole slide images. We
establish a comprehensive benchmark by evaluating five widely adopted deep
learning models across four stain normalization techniques, providing deeper
insights into their influence on neuritic plaque segmentation. Additionally, we
propose a novel image enhancement method that improves segmentation accuracy,
particularly in complex tissue structures, by enhancing structural details and
mitigating staining inconsistencies. Our experimental results demonstrate that
this enhancement strategy significantly boosts model generalization and
segmentation accuracy. All datasets and code are open-source, ensuring
transparency and reproducibility while enabling further advancements in the
field.

</details>


### [379] [Direct Image Classification from Fourier Ptychographic Microscopy Measurements without Reconstruction](https://arxiv.org/pdf/2505.05054)
*Navya Sonal Agarwal, Jan Philipp Schneider, Kanchana Vaishnavi Gandikota, Syed Muhammad Kazim, John Meshreki, Ivo Ihrke, Michael Moeller*

Main category: eess.IV

TL;DR: Fourier Ptychographic Microscopy (FPM) enables high-res imaging but is computationally expensive. This paper proposes using CNNs to classify images directly from FPM measurements, bypassing reconstruction, improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: FPM's high computational cost for reconstruction motivates exploring direct classification from measurements to save time and resources.

Method: Uses Convolutional Neural Networks (CNNs) to classify image content directly from FPM measurements, avoiding reconstruction. Also explores learned multiplexing to reduce data.

Result: CNNs outperform single-image classification by up to 12% and are more efficient than full reconstruction. Learned multiplexing maintains accuracy while reducing data.

Conclusion: Direct classification from FPM measurements using CNNs is efficient and accurate, with potential for reduced data and acquisition time.

Abstract: The computational imaging technique of Fourier Ptychographic Microscopy (FPM)
enables high-resolution imaging with a wide field of view and can serve as an
extremely valuable tool, e.g. in the classification of cells in medical
applications. However, reconstructing a high-resolution image from tens or even
hundreds of measurements is computationally expensive, particularly for a wide
field of view. Therefore, in this paper, we investigate the idea of classifying
the image content in the FPM measurements directly without performing a
reconstruction step first. We show that Convolutional Neural Networks (CNN) can
extract meaningful information from measurement sequences, significantly
outperforming the classification on a single band-limited image (up to 12 %)
while being significantly more efficient than a reconstruction of a
high-resolution image. Furthermore, we demonstrate that a learned multiplexing
of several raw measurements allows maintaining the classification accuracy
while reducing the amount of data (and consequently also the acquisition time)
significantly.

</details>


### [380] [RepSNet: A Nucleus Instance Segmentation model based on Boundary Regression and Structural Re-parameterization](https://arxiv.org/pdf/2505.05073)
*Shengchun Xiong, Xiangru Li, Yunpeng Zhong, Wanfen Peng*

Main category: eess.IV

TL;DR: RepSNet is a neural network model for nucleus instance segmentation in histopathological images, using boundary regression and re-parameterization to improve efficiency and handle overlapping nuclei.


<details>
  <summary>Details</summary>
Motivation: Pathological diagnosis relies on nucleus segmentation, but computational efficiency and overlapping nuclei are challenges. RepSNet addresses these by leveraging boundary regression and re-parameterization.

Method: RepSNet estimates boundary position information (BPI) for pixels, aggregates BPIs via boundary voting, and uses re-parameterization for efficient feature aggregation.

Result: RepSNet outperforms benchmark models in segmentation accuracy and computational efficiency.

Conclusion: RepSNet offers a robust solution for nucleus segmentation, balancing accuracy and efficiency through innovative boundary estimation and re-parameterization.

Abstract: Pathological diagnosis is the gold standard for tumor diagnosis, and nucleus
instance segmentation is a key step in digital pathology analysis and
pathological diagnosis. However, the computational efficiency of the model and
the treatment of overlapping targets are the major challenges in the studies of
this problem. To this end, a neural network model RepSNet was designed based on
a nucleus boundary regression and a structural re-parameterization scheme for
segmenting and classifying the nuclei in H\&E-stained histopathological images.
First, RepSNet estimates the boundary position information (BPI) of the parent
nucleus for each pixel. The BPI estimation incorporates the local information
of the pixel and the contextual information of the parent nucleus. Then, the
nucleus boundary is estimated by aggregating the BPIs from a series of pixels
using a proposed boundary voting mechanism (BVM), and the instance segmentation
results are computed from the estimated nucleus boundary using a connected
component analysis procedure. The BVM intrinsically achieves a kind of
synergistic belief enhancement among the BPIs from various pixels. Therefore,
different from the methods available in literature that obtain nucleus
boundaries based on a direct pixel recognition scheme, RepSNet computes its
boundary decisions based on some guidances from macroscopic information using
an integration mechanism. In addition, RepSNet employs a re-parametrizable
encoder-decoder structure. This model can not only aggregate features from some
receptive fields with various scales which helps segmentation accuracy
improvement, but also reduce the parameter amount and computational burdens in
the model inference phase through the structural re-parameterization technique.
Extensive experiments demonstrated the superiorities of RepSNet compared to
several typical benchmark models.

</details>


### [381] [MDAA-Diff: CT-Guided Multi-Dose Adaptive Attention Diffusion Model for PET Denoising](https://arxiv.org/pdf/2505.05112)
*Xiaolong Niu, Zanting Ye, Xu Han, Yanchao Huang, Hao Sun, Hubing Wu, Lijun Lu*

Main category: eess.IV

TL;DR: A novel CT-guided denoising model (MDAA-Diff) improves PET image quality under low-dose conditions by integrating anatomical CT guidance and dose-level adaptation.


<details>
  <summary>Details</summary>
Motivation: High-dose radiotracers for PET imaging pose radiation risks; generating standard-dose PET from low-dose PET is a solution, but prior methods ignore inter-patient variability and CT-derived anatomical constraints.

Method: Proposes MDAA-Diff with CT-Guided High-frequency Wavelet Attention (HWA) for anatomical boundary features and Dose-Adaptive Attention (DAA) for dynamic dose-level integration.

Result: Outperforms state-of-the-art methods in preserving diagnostic quality on 18F-FDG and 68Ga-FAPI datasets.

Conclusion: MDAA-Diff effectively enhances low-dose PET images by leveraging CT guidance and dose adaptation, offering a safer alternative to high-dose radiotracers.

Abstract: Acquiring high-quality Positron Emission Tomography (PET) images requires
administering high-dose radiotracers, which increases radiation exposure risks.
Generating standard-dose PET (SPET) from low-dose PET (LPET) has become a
potential solution. However, previous studies have primarily focused on single
low-dose PET denoising, neglecting two critical factors: discrepancies in dose
response caused by inter-patient variability, and complementary anatomical
constraints derived from CT images. In this work, we propose a novel CT-Guided
Multi-dose Adaptive Attention Denoising Diffusion Model (MDAA-Diff) for
multi-dose PET denoising. Our approach integrates anatomical guidance and
dose-level adaptation to achieve superior denoising performance under low-dose
conditions. Specifically, this approach incorporates a CT-Guided High-frequency
Wavelet Attention (HWA) module, which uses wavelet transforms to separate
high-frequency anatomical boundary features from CT images. These extracted
features are then incorporated into PET imaging through an adaptive weighted
fusion mechanism to enhance edge details. Additionally, we propose the
Dose-Adaptive Attention (DAA) module, a dose-conditioned enhancement mechanism
that dynamically integrates dose levels into channel-spatial attention weight
calculation. Extensive experiments on 18F-FDG and 68Ga-FAPI datasets
demonstrate that MDAA-Diff outperforms state-of-the-art approaches in
preserving diagnostic quality under reduced-dose conditions. Our code is
publicly available.

</details>


### [382] [Improved Brain Tumor Detection in MRI: Fuzzy Sigmoid Convolution in Deep Learning](https://arxiv.org/pdf/2505.05208)
*Muhammad Irfan, Anum Nawaz, Riku Klen, Abdulhamit Subasi, Tomi Westerlund, Wei Chen*

Main category: eess.IV

TL;DR: A novel FSC-based model with fuzzy sigmoid activation reduces parameters while maintaining high accuracy for tumor detection.


<details>
  <summary>Details</summary>
Motivation: Early and accurate tumor detection is critical, but existing CNN models are overparameterized, limiting performance.

Method: Introduces fuzzy sigmoid convolution (FSC) with top/middle-of-the-funnel modules, reducing parameters and enhancing feature extraction.

Result: Achieved 99.17%-99.89% accuracy on benchmarks with 100x fewer parameters than large models.

Conclusion: The FSC-based model is efficient, lightweight, and highly effective for early tumor detection in medical imaging.

Abstract: Early detection and accurate diagnosis are essential to improving patient
outcomes. The use of convolutional neural networks (CNNs) for tumor detection
has shown promise, but existing models often suffer from overparameterization,
which limits their performance gains. In this study, fuzzy sigmoid convolution
(FSC) is introduced along with two additional modules: top-of-the-funnel and
middle-of-the-funnel. The proposed methodology significantly reduces the number
of trainable parameters without compromising classification accuracy. A novel
convolutional operator is central to this approach, effectively dilating the
receptive field while preserving input data integrity. This enables efficient
feature map reduction and enhances the model's tumor detection capability. In
the FSC-based model, fuzzy sigmoid activation functions are incorporated within
convolutional layers to improve feature extraction and classification. The
inclusion of fuzzy logic into the architecture improves its adaptability and
robustness. Extensive experiments on three benchmark datasets demonstrate the
superior performance and efficiency of the proposed model. The FSC-based
architecture achieved classification accuracies of 99.17%, 99.75%, and 99.89%
on three different datasets. The model employs 100 times fewer parameters than
large-scale transfer learning architectures, highlighting its computational
efficiency and suitability for detecting brain tumors early. This research
offers lightweight, high-performance deep-learning models for medical imaging
applications.

</details>


### [383] [White Light Specular Reflection Data Augmentation for Deep Learning Polyp Detection](https://arxiv.org/pdf/2505.05248)
*Jose Angel Nuñez, Fabian Vazquez, Diego Adame, Xiaoyan Fu, Pengfei Gu, Bin Fu*

Main category: eess.IV

TL;DR: A novel data augmentation method is proposed to improve deep learning polyp detection by artificially adding white light reflections to training images, reducing false positives.


<details>
  <summary>Details</summary>
Motivation: Human error in colonoscopies can miss polyps, and existing DL detectors often misidentify white light reflections as polyps, leading to false positives.

Method: The approach involves generating artificial light reflections, identifying safe regions to avoid, and using a sliding window to augment training images.

Result: Experiments show the method effectively enhances polyp detection performance.

Conclusion: The proposed augmentation helps the model learn from mistakes, improving accuracy in detecting polyps.

Abstract: Colorectal cancer is one of the deadliest cancers today, but it can be
prevented through early detection of malignant polyps in the colon, primarily
via colonoscopies. While this method has saved many lives, human error remains
a significant challenge, as missing a polyp could have fatal consequences for
the patient. Deep learning (DL) polyp detectors offer a promising solution.
However, existing DL polyp detectors often mistake white light reflections from
the endoscope for polyps, which can lead to false positives.To address this
challenge, in this paper, we propose a novel data augmentation approach that
artificially adds more white light reflections to create harder training
scenarios. Specifically, we first generate a bank of artificial lights using
the training dataset. Then we find the regions of the training images that we
should not add these artificial lights on. Finally, we propose a sliding window
method to add the artificial light to the areas that fit of the training
images, resulting in augmented images. By providing the model with more
opportunities to make mistakes, we hypothesize that it will also have more
chances to learn from those mistakes, ultimately improving its performance in
polyp detection. Experimental results demonstrate the effectiveness of our new
data augmentation method.

</details>


### [384] [Benchmarking Ophthalmology Foundation Models for Clinically Significant Age Macular Degeneration Detection](https://arxiv.org/pdf/2505.05291)
*Benjamin A. Cohen, Jonathan Fhima, Meishar Meisel, Baskin Meital, Luis Filipe Nakayama, Eran Berkowitz, Joachim A. Behar*

Main category: eess.IV

TL;DR: SSL-pretrained ViTs, especially iBOT on natural images, outperform domain-specific models and baselines in AMD identification, challenging the need for in-domain pretraining. BRAMD dataset is released.


<details>
  <summary>Details</summary>
Motivation: To determine if in-domain pretraining is necessary for robust AMD identification in retinal imaging, comparing SSL-pretrained ViTs on natural vs. ophthalmic data.

Method: Benchmark six SSL-pretrained ViTs on seven DFI datasets (70,000 images) for AMD identification, evaluating out-of-distribution generalization.

Result: iBOT pretrained on natural images achieved the highest AUROCs (0.80-0.97), outperforming domain-specific models (0.78-0.96) and a baseline ViT-L (0.68-0.91).

Conclusion: Foundation models, even without in-domain pretraining, improve AMD identification, questioning the necessity of domain-specific pretraining. BRAMD dataset is shared for further research.

Abstract: Self-supervised learning (SSL) has enabled Vision Transformers (ViTs) to
learn robust representations from large-scale natural image datasets, enhancing
their generalization across domains. In retinal imaging, foundation models
pretrained on either natural or ophthalmic data have shown promise, but the
benefits of in-domain pretraining remain uncertain. To investigate this, we
benchmark six SSL-pretrained ViTs on seven digital fundus image (DFI) datasets
totaling 70,000 expert-annotated images for the task of moderate-to-late
age-related macular degeneration (AMD) identification. Our results show that
iBOT pretrained on natural images achieves the highest out-of-distribution
generalization, with AUROCs of 0.80-0.97, outperforming domain-specific models,
which achieved AUROCs of 0.78-0.96 and a baseline ViT-L with no pretraining,
which achieved AUROCs of 0.68-0.91. These findings highlight the value of
foundation models in improving AMD identification and challenge the assumption
that in-domain pretraining is necessary. Furthermore, we release BRAMD, an
open-access dataset (n=587) of DFIs with AMD labels from Brazil.

</details>


### [385] [Augmented Deep Contexts for Spatially Embedded Video Coding](https://arxiv.org/pdf/2505.05309)
*Yifan Bian, Chuanbo Tang, Li Li, Dong Liu*

Main category: eess.IV

TL;DR: SEVC improves Neural Video Codecs by adding spatial references to handle large motions and emerging objects, reducing bitrate by 11.9%.


<details>
  <summary>Details</summary>
Motivation: Temporal-only NVCs struggle with large motions and emerging objects due to limited contexts and misaligned latent prior.

Method: SEVC uses spatial and temporal references for augmented motion vectors, hybrid contexts, and spatial-guided latent prior. Joint optimization improves bit allocation.

Result: SEVC handles large motions better and reduces bitrate by 11.9% compared to state-of-the-art NVCs.

Conclusion: SEVC enhances video compression by integrating spatial references and optimizing bit allocation, outperforming existing methods.

Abstract: Most Neural Video Codecs (NVCs) only employ temporal references to generate
temporal-only contexts and latent prior. These temporal-only NVCs fail to
handle large motions or emerging objects due to limited contexts and misaligned
latent prior. To relieve the limitations, we propose a Spatially Embedded Video
Codec (SEVC), in which the low-resolution video is compressed for spatial
references. Firstly, our SEVC leverages both spatial and temporal references to
generate augmented motion vectors and hybrid spatial-temporal contexts.
Secondly, to address the misalignment issue in latent prior and enrich the
prior information, we introduce a spatial-guided latent prior augmented by
multiple temporal latent representations. At last, we design a joint
spatial-temporal optimization to learn quality-adaptive bit allocation for
spatial references, further boosting rate-distortion performance. Experimental
results show that our SEVC effectively alleviates the limitations in handling
large motions or emerging objects, and also reduces 11.9% more bitrate than the
previous state-of-the-art NVC while providing an additional low-resolution
bitstream. Our code and model are available at https://github.com/EsakaK/SEVC.

</details>


### [386] [OcularAge: A Comparative Study of Iris and Periocular Images for Pediatric Age Estimation](https://arxiv.org/pdf/2505.05374)
*Naveenkumar G Venkataswamy, Poorna Ravi, Stephanie Schuckers, Masudul H. Imtiaz*

Main category: eess.IV

TL;DR: The paper compares iris and periocular images for age estimation in children (4-16 years) using a multi-task deep learning framework, finding periocular models more accurate (MAE 1.33 years).


<details>
  <summary>Details</summary>
Motivation: Pediatric age estimation from ocular images is underexplored, especially for privacy-preserving child-centric applications.

Method: A multi-task deep learning framework analyzed longitudinal NIR images (21,000+ from 288 subjects) using CNNs adapted for non-square ocular inputs.

Result: Periocular models outperformed iris models (MAE 1.33 years, 83.82% age-group accuracy) and showed resilience across sensors and real-time feasibility.

Conclusion: Reliable pediatric age estimation from ocular images is feasible, establishing a benchmark for child-focused biometric systems.

Abstract: Estimating a child's age from ocular biometric images is challenging due to
subtle physiological changes and the limited availability of longitudinal
datasets. Although most biometric age estimation studies have focused on facial
features and adult subjects, pediatric-specific analysis, particularly of the
iris and periocular regions, remains relatively unexplored. This study presents
a comparative evaluation of iris and periocular images for estimating the ages
of children aged between 4 and 16 years. We utilized a longitudinal dataset
comprising more than 21,000 near-infrared (NIR) images, collected from 288
pediatric subjects over eight years using two different imaging sensors. A
multi-task deep learning framework was employed to jointly perform age
prediction and age-group classification, enabling a systematic exploration of
how different convolutional neural network (CNN) architectures, particularly
those adapted for non-square ocular inputs, capture the complex variability
inherent in pediatric eye images. The results show that periocular models
consistently outperform iris-based models, achieving a mean absolute error
(MAE) of 1.33 years and an age-group classification accuracy of 83.82%. These
results mark the first demonstration that reliable age estimation is feasible
from children's ocular images, enabling privacy-preserving age checks in
child-centric applications. This work establishes the first longitudinal
benchmark for pediatric ocular age estimation, providing a foundation for
designing robust, child-focused biometric systems. The developed models proved
resilient across different imaging sensors, confirming their potential for
real-world deployment. They also achieved inference speeds of less than 10
milliseconds per image on resource-constrained VR headsets, demonstrating their
suitability for real-time applications.

</details>


### [387] [An automated end-to-end deep learning-based framework for lung cancer diagnosis by detecting and classifying the lung nodules](https://arxiv.org/pdf/2305.00046)
*Samiul Based Shuvo, Tasnia Binte Mamun*

Main category: eess.IV

TL;DR: An automated deep learning framework for early lung nodule detection and classification, achieving high accuracy in segmentation, detection, and classification, especially for low-resource settings.


<details>
  <summary>Details</summary>
Motivation: Early lung cancer detection is critical but challenging in low-resource areas due to limited medical access. This study aims to automate the process using deep learning.

Method: The framework includes lung segmentation (3D Res-U-Net), nodule detection (YOLO-v5), and classification (Vision Transformer), evaluated on the LUNA16 dataset.

Result: Achieved 98.82% segmentation dice score, 0.76 mAP@50 for detection, and 93.57% classification accuracy, outperforming existing methods.

Conclusion: The framework is effective for early lung cancer screening in low-resource settings, improving accuracy and patient outcomes.

Abstract: Lung cancer is a leading cause of cancer-related deaths worldwide, and early
detection is crucial for improving patient outcomes. Nevertheless, early
diagnosis of cancer is a major challenge, particularly in low-resource settings
where access to medical resources and trained radiologists is limited. The
objective of this study is to propose an automated end-to-end deep
learning-based framework for the early detection and classification of lung
nodules, specifically for low-resource settings. The proposed framework
consists of three stages: lung segmentation using a modified 3D U-Net named 3D
Res-U-Net, nodule detection using YOLO-v5, and classification with a Vision
Transformer-based architecture. We evaluated the proposed framework on a
publicly available dataset, LUNA16. The proposed framework's performance was
measured using the respective domain's evaluation matrices. The proposed
framework achieved a 98.82% lung segmentation dice score while detecting the
lung nodule with 0.76 mAP@50 from the segmented lung, at a low false-positive
rate. The performance of both networks of the proposed framework was compared
with other studies and found to outperform them regarding segmentation and
detection accuracy. Additionally, our proposed Vision transformer network
obtained an accuracy of 93.57%, which is 1.21% higher than the state-of-the-art
networks. Our proposed end-to-end deep learning-based framework can effectively
segment lungs, and detect and classify lung nodules, specifically in
low-resource settings with limited access to radiologists. The proposed
framework outperforms existing studies regarding all the respective evaluation
metrics. The proposed framework can potentially improve the accuracy and
efficiency of lung cancer screening in low-resource settings, ultimately
leading to better patient outcomes.

</details>


### [388] [Evaluating Deep Learning Models for Breast Cancer Classification: A Comparative Study](https://arxiv.org/pdf/2408.16859)
*Sania Eskandari, Ali Eslamian, Nusrat Munia, Amjad Alqarni, Qiang Cheng*

Main category: eess.IV

TL;DR: Deep learning models, especially Vision Transformer (ViT), outperform CNNs in classifying histopathological images for breast cancer detection, achieving 94% accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve early and accurate breast cancer diagnosis using advanced deep learning models.

Method: Comparison of eight models (ResNet-50, DenseNet-121, etc.) on 277,524 image patches.

Result: ViT achieved the highest validation accuracy (94%), surpassing CNNs.

Conclusion: Advanced machine learning methods like ViT can enhance breast cancer diagnosis precision and efficiency.

Abstract: This study evaluates the effectiveness of deep learning models in classifying
histopathological images for early and accurate detection of breast cancer.
Eight advanced models, including ResNet-50, DenseNet-121, ResNeXt-50, Vision
Transformer (ViT), GoogLeNet (Inception v3), EfficientNet, MobileNet, and
SqueezeNet, were compared using a dataset of 277,524 image patches. The Vision
Transformer (ViT) model, with its attention-based mechanisms, achieved the
highest validation accuracy of 94%, outperforming conventional CNNs. The study
demonstrates the potential of advanced machine learning methods to enhance
precision and efficiency in breast cancer diagnosis in clinical settings.

</details>


### [389] [Large Scale MRI Collection and Segmentation of Cirrhotic Liver](https://arxiv.org/pdf/2410.16296)
*Debesh Jha, Onkar Kishor Susladkar, Vandan Gorade, Elif Keles, Matthew Antalek, Deniz Seyithanoglu, Timurhan Cebeci, Halil Ertugrul Aktas, Gulbiz Dagoglu Kartal, Sabahattin Kaymakoglu, Sukru Mehmet Erturk, Yuri Velichko, Daniela Ladner, Amir A. Borhani, Alpay Medetalibeyoglu, Gorkem Durak, Ulas Bagci*

Main category: eess.IV

TL;DR: The paper introduces CirrMRI600+, a comprehensive dataset of 628 high-resolution MRI scans for cirrhotic liver segmentation, addressing the lack of large-scale annotated data. It includes benchmark results from 11 deep learning models to set performance standards.


<details>
  <summary>Details</summary>
Motivation: Liver cirrhosis is a severe condition with high mortality, and accurate MRI segmentation is challenging due to morphological changes and signal heterogeneity. Existing deep learning methods lack large annotated datasets for training and validation.

Method: The authors compiled CirrMRI600+, a dataset of 628 abdominal MRI scans (T1 and T2-weighted) with expert-validated segmentation labels, demographic and clinical data, and histopathological validation. They also tested 11 state-of-the-art deep learning models for benchmarking.

Result: The dataset provides a foundation for developing advanced computational methods, with benchmark results from 11 models to guide future research.

Conclusion: CirrMRI600+ fills a critical gap in cirrhotic liver analysis, enabling progress toward automated staging and personalized treatment planning.

Abstract: Liver cirrhosis represents the end stage of chronic liver disease,
characterized by extensive fibrosis and nodular regeneration that significantly
increases mortality risk. While magnetic resonance imaging (MRI) offers a
non-invasive assessment, accurately segmenting cirrhotic livers presents
substantial challenges due to morphological alterations and heterogeneous
signal characteristics. Deep learning approaches show promise for automating
these tasks, but progress has been limited by the absence of large-scale,
annotated datasets. Here, we present CirrMRI600+, the first comprehensive
dataset comprising 628 high-resolution abdominal MRI scans (310 T1-weighted and
318 T2-weighted sequences, totaling nearly 40,000 annotated slices) with
expert-validated segmentation labels for cirrhotic livers. The dataset includes
demographic information, clinical parameters, and histopathological validation
where available. Additionally, we provide benchmark results from 11
state-of-the-art deep learning experiments to establish performance standards.
CirrMRI600+ enables the development and validation of advanced computational
methods for cirrhotic liver analysis, potentially accelerating progress toward
automated Cirrhosis visual staging and personalized treatment planning.

</details>


### [390] [AirMorph: Topology-Preserving Deep Learning for Pulmonary Airway Analysis](https://arxiv.org/pdf/2412.11039)
*Minghui Zhang, Chenyu Li, Fangfang Xie, Yaoyu Liu, Hanxiao Zhang, Junyang Wu, Chunxi Zhang, Jie Yang, Jiayuan Sun, Guang-Zhong Yang, Yun Gu*

Main category: eess.IV

TL;DR: AirMorph is a deep learning pipeline for automatic, fine-grained airway labeling in thoracic CT, outperforming existing methods and enabling disease-specific morphological analysis.


<details>
  <summary>Details</summary>
Motivation: The need for clinically deployable, fine-grained morphological atlases of the lung to understand abnormalities and support targeted therapies.

Method: AirMorph, an end-to-end deep learning pipeline for automatic airway labeling at lobar, segmental, and subsegmental resolutions, evaluated on multi-center datasets.

Result: Outperforms existing methods in accuracy, topological consistency, and completeness; introduces a compact anatomical signature for disease-specific morphological patterns.

Conclusion: AirMorph enhances clinical diagnosis, treatment, and personalized care through automated branching pattern analysis and disease-specific insights.

Abstract: Accurate anatomical labeling and analysis of the pulmonary structure and its
surrounding anatomy from thoracic CT is getting increasingly important for
understanding the etilogy of abnormalities or supporting targetted therapy and
early interventions. Whilst lung and airway cell atlases have been attempted,
there is a lack of fine-grained morphological atlases that are clinically
deployable. In this work, we introduce AirMorph, a robust, end-to-end deep
learning pipeline enabling fully automatic and comprehensive airway anatomical
labeling at lobar, segmental, and subsegmental resolutions that can be used to
create digital atlases of the lung. Evaluated across large-scale multi-center
datasets comprising diverse pulmonary conditions, the AirMorph consistently
outperformed existing segmentation and labeling methods in terms of accuracy,
topological consistency, and completeness. To simplify clinical interpretation,
we further introduce a compact anatomical signature quantifying critical
morphological airway features, including stenosis, ectasia, tortuosity,
divergence, length, and complexity. When applied to various pulmonary diseases
such as pulmonary fibrosis, emphysema, atelectasis, consolidation, and
reticular opacities, it demonstrates strong discriminative power, revealing
disease-specific morphological patterns with high interpretability and
explainability. Additionally, AirMorph supports efficient automated branching
pattern analysis, potentially enhancing bronchoscopic navigation planning and
procedural safety, offering a valuable clinical tool for improved diagnosis,
targeted treatment, and personalized patient care.

</details>


### [391] [Proxy Prompt: Endowing SAM and SAM 2 with Auto-Interactive-Prompt for Medical Segmentation](https://arxiv.org/pdf/2502.03501)
*Wang Xinyi, Kang Hongyu, Wei Peishan, Shuai Li, Yu Sun, Sai Kit Lam, Yongping Zheng*

Main category: eess.IV

TL;DR: The paper proposes Proxy Prompt (PP), an automated prompting method for SAM and SAM2, enhancing human-model interactions and segmentation performance using non-target data and a novel 3-step context-selection strategy.


<details>
  <summary>Details</summary>
Motivation: To address the lack of automated prompting and improve human-model interactions for SAM and SAM2, facilitating their clinical adoption.

Method: Introduces Proxy Prompt (PP) with a 3-step context-selection strategy and a contextual colorization module for enhanced interactions.

Result: Achieves state-of-the-art performance on four datasets and matches fully-trained models with minimal training data (16 masks).

Conclusion: PP effectively automates prompting and enhances segmentation, demonstrating strong performance with limited data.

Abstract: In this paper, we aim to address the unmet demand for automated prompting and
enhanced human-model interactions of SAM and SAM2 for the sake of promoting
their widespread clinical adoption. Specifically, we propose Proxy Prompt (PP),
auto-generated by leveraging non-target data with a pre-annotated mask. We
devise a novel 3-step context-selection strategy for adaptively selecting the
most representative contextual information from non-target data via vision
mamba and selective maps, empowering the guiding capability of non-target
image-mask pairs for segmentation on target image/video data. To reinforce
human-model interactions in PP, we further propose a contextual colorization
module via a dual-reverse cross-attention to enhance interactions between
target features and contextual-embedding with amplifying distinctive features
of user-defined object(s). Via extensive evaluations, our method achieves
state-of-the-art performance on four public datasets and yields comparable
results with fully-trained models, even when trained with only 16 image masks.

</details>


### [392] [Integrating AI for Human-Centric Breast Cancer Diagnostics: A Multi-Scale and Multi-View Swin Transformer Framework](https://arxiv.org/pdf/2503.13309)
*Farnoush Bayatmakou, Reza Taleei, Milad Amir Toutounchian, Arash Mohammadi*

Main category: eess.IV

TL;DR: The paper proposes a hybrid, multi-scale, multi-view Swin Transformer-based framework (MSMV-Swin) to enhance breast cancer diagnosis by integrating AI into a human-centric workflow, addressing challenges like reliance on tumor annotations and missing views.


<details>
  <summary>Details</summary>
Motivation: Breast cancer remains a leading cause of death among women, and while AI shows promise in CAD systems, challenges like annotation reliance and missing views persist.

Method: The MSMV-Swin framework uses a Swin Transformer and SAM for breast lobe isolation, multi-scale feature extraction, and hybrid fusion to handle missing views.

Result: The framework improves diagnostic robustness and accuracy, aligning with radiologists' interpretations and enhancing human-AI trust.

Conclusion: MSMV-Swin effectively addresses key challenges in breast cancer diagnosis, offering a reliable decision-support tool for radiologists.

Abstract: Despite advancements in Computer-Aided Diagnosis (CAD) systems, breast cancer
remains one of the leading causes of cancer-related deaths among women
worldwide. Recent breakthroughs in Artificial Intelligence (AI) have shown
significant promise in development of advanced Deep Learning (DL) architectures
for breast cancer diagnosis through mammography. In this context, the paper
focuses on the integration of AI within a Human-Centric workflow to enhance
breast cancer diagnostics. Key challenges are, however, largely overlooked such
as reliance on detailed tumor annotations and susceptibility to missing views,
particularly during test time. To address these issues, we propose a hybrid,
multi-scale and multi-view Swin Transformer-based framework (MSMV-Swin) that
enhances diagnostic robustness and accuracy. The proposed MSMV-Swin framework
is designed to work as a decision-support tool, helping radiologists analyze
multi-view mammograms more effectively. More specifically, the MSMV-Swin
framework leverages the Segment Anything Model (SAM) to isolate the breast
lobe, reducing background noise and enabling comprehensive feature extraction.
The multi-scale nature of the proposed MSMV-Swin framework accounts for
tumor-specific regions as well as the spatial characteristics of tissues
surrounding the tumor, capturing both localized and contextual information. The
integration of contextual and localized data ensures that MSMV-Swin's outputs
align with the way radiologists interpret mammograms, fostering better human-AI
interaction and trust. A hybrid fusion structure is then designed to ensure
robustness against missing views, a common occurrence in clinical practice when
only a single mammogram view is available.

</details>


### [393] [Leveraging Depth Maps and Attention Mechanisms for Enhanced Image Inpainting](https://arxiv.org/pdf/2505.00735)
*Jin Hyun Park, Harine Choi, Praewa Pitiphat*

Main category: eess.IV

TL;DR: A novel image inpainting method combines RGB and depth images using a dual encoder and attention mechanism, outperforming RGB-only baselines.


<details>
  <summary>Details</summary>
Motivation: RGB-only methods lack depth information, which is crucial for spatial and structural context in image reconstruction.

Method: Dual encoder architecture processes RGB and depth images separately, fusing features with attention in the decoder. Masking strategies (line and square) test robustness, and Grad-CAM visualizations analyze model focus.

Result: Depth integration improves reconstruction quality, with attention further enhancing performance, validated by metrics and visualizations.

Conclusion: Incorporating depth alongside RGB significantly enhances inpainting accuracy and contextual awareness.

Abstract: Existing deep learning-based image inpainting methods typically rely on
convolutional networks with RGB images to reconstruct images. However, relying
exclusively on RGB images may neglect important depth information, which plays
a critical role in understanding the spatial and structural context of a scene.
Just as human vision leverages stereo cues to perceive depth, incorporating
depth maps into the inpainting process can enhance the model's ability to
reconstruct images with greater accuracy and contextual awareness. In this
paper, we propose a novel approach that incorporates both RGB and depth images
for enhanced image inpainting. Our models employ a dual encoder architecture,
where one encoder processes the RGB image and the other handles the depth
image. The encoded features from both encoders are then fused in the decoder
using an attention mechanism, effectively integrating the RGB and depth
representations. We use two different masking strategies, line and square, to
test the robustness of the model under different types of occlusions. To
further analyze the effectiveness of our approach, we use Gradient-weighted
Class Activation Mapping (Grad-CAM) visualizations to examine the regions of
interest the model focuses on during inpainting. We show that incorporating
depth information alongside the RGB image significantly improves the
reconstruction quality. Through both qualitative and quantitative comparisons,
we demonstrate that the depth-integrated model outperforms the baseline, with
attention mechanisms further enhancing inpainting performance, as evidenced by
multiple evaluation metrics and visualization.

</details>


### [394] [IntelliCardiac: An Intelligent Platform for Cardiac Image Segmentation and Classification](https://arxiv.org/pdf/2505.03838)
*Ting Yu Tsai, An Yu, Meghana Spurthi Maadugundu, Ishrat Jahan Mohima, Umme Habiba Barsha, Mei-Hwa F. Chen, Balakrishnan Prabhakaran, Ming-Ching Chang*

Main category: eess.IV

TL;DR: IntelliCardiac is a web-based AI platform for automatic 4D cardiac image segmentation and disease classification, achieving high accuracy and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Precise cardiac imaging processing is crucial for diagnosing and managing cardiovascular diseases, necessitating an efficient, scalable tool for healthcare professionals.

Method: Uses deep learning models for segmentation and a two-step classification pipeline, trained on the ACDC dataset, to analyze ventricles and myocardium.

Result: Segmentation accuracy of 92.6% and classification accuracy of 98% in five disease categories, surpassing state-of-the-art methods.

Conclusion: IntelliCardiac is a scalable, accurate tool with potential for clinical decision assistance in cardiac imaging and diagnosis.

Abstract: Precise and effective processing of cardiac imaging data is critical for the
identification and management of the cardiovascular diseases. We introduce
IntelliCardiac, a comprehensive, web-based medical image processing platform
for the automatic segmentation of 4D cardiac images and disease classification,
utilizing an AI model trained on the publicly accessible ACDC dataset. The
system, intended for patients, cardiologists, and healthcare professionals,
offers an intuitive interface and uses deep learning models to identify
essential heart structures and categorize cardiac diseases. The system supports
analysis of both the right and left ventricles as well as myocardium, and then
classifies patient's cardiac images into five diagnostic categories: dilated
cardiomyopathy, myocardial infarction, hypertrophic cardiomyopathy, right
ventricular abnormality, and no disease. IntelliCardiac combines a deep
learning-based segmentation model with a two-step classification pipeline. The
segmentation module gains an overall accuracy of 92.6%. The classification
module, trained on characteristics taken from segmented heart structures,
achieves 98% accuracy in five categories. These results exceed the performance
of the existing state-of-the-art methods that integrate both segmentation and
classification models. IntelliCardiac, which supports real-time visualization,
workflow integration, and AI-assisted diagnostics, has great potential as a
scalable, accurate tool for clinical decision assistance in cardiac imaging and
diagnosis.

</details>


### [395] [A Dataset and Toolkit for Multiparameter Cardiovascular Physiology Sensing on Rings](https://arxiv.org/pdf/2505.04172)
*Jiankai Tang, Kegang Wang, Yingke Ding, Jiatong Ji, Zeyu Wang, Xiyuxing Zhang, Ping Chen, Yuanchun Shi, Yuntao Wang*

Main category: eess.IV

TL;DR: The paper introduces τ-Ring, the first open-source dataset for ring-based cardiovascular monitoring, and RingTool, a toolkit for analyzing signals. It evaluates methods and shows superior performance over commercial rings.


<details>
  <summary>Details</summary>
Motivation: The lack of public datasets and standardized tools for ring-based cardiovascular monitoring hinders research. This work aims to bridge that gap.

Method: The dataset includes photoplethysmography and accelerometer data from 34 subjects across activities. Physics-based and deep learning methods were evaluated using RingTool.

Result: Best MAE values: 5.18 BPM (heart rate), 2.98 BPM (respiratory rate), 3.22% (oxygen saturation), 13.33/7.56 mmHg (blood pressure).

Conclusion: τ-Ring and RingTool advance ring-based cardiovascular research by providing open data and tools, outperforming commercial solutions.

Abstract: Smart rings offer a convenient way to continuously and unobtrusively monitor
cardiovascular physiological signals. However, a gap remains between the ring
hardware and reliable methods for estimating cardiovascular parameters, partly
due to the lack of publicly available datasets and standardized analysis tools.
In this work, we present $\tau$-Ring, the first open-source ring-based dataset
designed for cardiovascular physiological sensing. The dataset comprises
photoplethysmography signals (infrared and red channels) and 3-axis
accelerometer data collected from two rings (reflective and transmissive
optical paths), with 28.21 hours of raw data from 34 subjects across seven
activities. $\tau$-Ring encompasses both stationary and motion scenarios, as
well as stimulus-evoked abnormal physiological states, annotated with four
ground-truth labels: heart rate, respiratory rate, oxygen saturation, and blood
pressure. Using our proposed RingTool toolkit, we evaluated three widely-used
physics-based methods and four cutting-edge deep learning approaches. Our
results show superior performance compared to commercial rings, achieving best
MAE values of 5.18 BPM for heart rate, 2.98 BPM for respiratory rate, 3.22\%
for oxygen saturation, and 13.33/7.56 mmHg for systolic/diastolic blood
pressure estimation. The open-sourced dataset and toolkit aim to foster further
research and community-driven advances in ring-based cardiovascular health
sensing.

</details>
